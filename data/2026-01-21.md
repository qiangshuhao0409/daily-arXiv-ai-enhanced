<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 24]
- [cs.AI](#cs.AI) [Total: 101]
- [cs.IT](#cs.IT) [Total: 40]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [An Efficient and Explainable KAN Framework forWireless Radiation Field Prediction](https://arxiv.org/abs/2601.11656)
*Jingzhou Shen,Xuyu Wang*

Main category: cs.NI

TL;DR: 提出一种新的无线信道建模方法，使用KAN网络结合Transformer学习完整射线而非单个点的表示，在真实和合成场景中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无线信道建模因环境变化和信号不确定性而具有挑战性。现有神经网络方法独立处理射线上的每个体素，缺乏全局上下文和环境因素考虑。

Method: 提出学习完整射线的综合表示而非单个点的方法，集成Kolmogorov-Arnold网络(KAN)架构与Transformer模块，保持计算效率的同时捕获更详细的环境特征。

Result: 实验结果表明该方法在各种场景中优于现有方法，消融研究确认模型各组件均对有效性有贡献，额外实验为模型性能提供了清晰解释。

Conclusion: 通过集成KAN和Transformer学习完整射线表示的方法，能够更准确地建模无线信道，在性能和计算效率方面均表现出色。

Abstract: Modeling wireless channels accurately remains a challenge due to environmental variations and signal uncertainties. Recent neural networks can learn radio frequency~(RF) signal propagation patterns, but they process each voxel on the ray independently, without considering global context or environmental factors. Our paper presents a new approach that learns comprehensive representations of complete rays rather than individual points, capturing more detailed environmental features. We integrate a Kolmogorov-Arnold network (KAN) architecture with transformer modules to achieve better performance across realistic and synthetic scenes while maintaining computational efficiency. Our experimental results show that this approach outperforms existing methods in various scenarios. Ablation studies confirm that each component of our model contributes to its effectiveness. Additional experiments provide clear explanations for our model's performance.

</details>


### [2] [Cascaded Transformer for Robust and Scalable SLA Decomposition via Amortized Optimization](https://arxiv.org/abs/2601.11859)
*Cyril Shih-Huan Hsu*

Main category: cs.NI

TL;DR: Casformer：基于级联Transformer架构的优化免费SLA分解方案，用于6G网络切片，相比传统优化方法具有更好的分解质量、可扩展性和实时性


<details>
  <summary>Details</summary>
Motivation: 6G网络切片需要将端到端SLA分解为域特定SLA，现有基于迭代优化的方法计算密集、延迟高、复杂度大，需要更高效的解决方案

Method: 提出Casformer级联Transformer架构：第一层使用域特定Transformer编码器处理历史域反馈，第二层使用Transformer聚合器整合跨域依赖；采用域感知神经网络学习范式，结合风险感知建模和摊销优化，学习稳定的前向SLA分解策略

Result: Casformer在SLA分解质量上优于最先进的基于优化的框架，在波动和噪声网络条件下表现出更好的可扩展性和鲁棒性；前向设计降低了运行时复杂度，简化了部署和维护

Conclusion: 结合摊销优化和基于Transformer的序列建模能够推进网络自动化，为高级5G及以后网络环境提供可扩展、高效的实时SLA管理解决方案

Abstract: The evolution toward 6G networks increasingly relies on network slicing to provide tailored, End-to-End (E2E) logical networks over shared physical infrastructures. A critical challenge is effectively decomposing E2E Service Level Agreements (SLAs) into domain-specific SLAs, which current solutions handle through computationally intensive, iterative optimization processes that incur substantial latency and complexity. To address this, we introduce Casformer, a cascaded Transformer architecture designed for fast, optimization-free SLA decomposition. Casformer leverages historical domain feedback encoded through domain-specific Transformer encoders in its first layer, and integrates cross-domain dependencies using a Transformer-based aggregator in its second layer. The model is trained under a learning paradigm inspired by Domain-Informed Neural Networks (DINNs), incorporating risk-informed modeling and amortized optimization to learn a stable, forward-only SLA decomposition policy. Extensive evaluations demonstrate that Casformer achieves improved SLA decomposition quality against state-of-the-art optimization-based frameworks, while exhibiting enhanced scalability and robustness under volatile and noisy network conditions. In addition, its forward-only design reduces runtime complexity and simplifies deployment and maintenance. These insights reveal the potential of combining amortized optimization with Transformer-based sequence modeling to advance network automation, providing a scalable and efficient solution suitable for real-time SLA management in advanced 5G-and-beyond network environments.

</details>


### [3] [A Method for Detecting Spatio-temporal Correlation Anomalies of WSN Nodes Based on Topological Information Enhancement and Time-frequency Feature Extraction](https://arxiv.org/abs/2601.11951)
*Miao Ye,Ziheng Wang,Yong Wang,Junqi Chen*

Main category: cs.NI

TL;DR: 提出TE-MSTAD方法，通过拓扑增强的时空特征融合来改进WSN异常检测，结合时间-频率域特征和空间相关性学习，在公开和真实数据集上取得92.52%和93.28%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有WSN异常检测方法存在三个主要问题：1) 时空相关性特征提取不足；2) 仅依赖时间域或频率域单一信息；3) 计算开销大。需要一种能充分提取时空特征、融合多域信息且计算高效的方法。

Method: 1) 基于RWKV线性注意力机制设计跨模态特征提取模块，充分提取多节点空间相关性特征并降低计算消耗；2) 通过联合学习时间-频率域特征构建邻接矩阵，集成不同图神经网络增强空间相关性特征提取；3) 设计双分支网络TE-MSTAD进行时间-频率域特征融合，克服单一域依赖的局限性。

Result: 在公开和真实世界数据集上测试，TE-MSTAD模型分别达到92.52%和93.28%的F1分数，相比现有方法展现出更优的检测性能和泛化能力。

Conclusion: TE-MSTAD方法通过拓扑增强的时空特征融合，有效解决了WSN异常检测中时空特征提取不足、单域依赖和计算开销大的问题，显著提升了检测性能和泛化能力。

Abstract: Existing anomaly detection methods for Wireless Sensor Networks (WSNs) generally suffer from insufficient ex-traction of spatio-temporal correlation features, reliance on either time-domain or frequency-domain information alone, and high computational overhead. To address these limitations, this paper proposes a topology-enhanced spatio-temporal feature fusion anomaly detection method, TE-MSTAD. First, building upon the RWKV model with linear attention mechanisms, a Cross-modal Feature Extraction (CFE) module is introduced to fully extract spatial correlation features among multiple nodes while reducing computational resource consumption. Second, a strategy is designed to construct an adjacency matrix by jointly learning spatial correlation from time-frequency domain features. Different graph neural networks are integrated to enhance spatial correlation feature extraction, thereby fully capturing spatial relationships among multiple nodes. Finally, a dual-branch network TE-MSTAD is designed for time-frequency domain feature fusion, overcoming the limitations of relying solely on the time or frequency domain to improve WSN anomaly detection performance. Testing on both public and real-world datasets demonstrates that the TE-MSTAD model achieves F1 scores of 92.52% and 93.28%, respectively, exhibiting superior detection performance and generalization capabilities compared to existing methods.

</details>


### [4] [Noisy Neighbor Influence in the Data Plane of Beyond 5G Networks](https://arxiv.org/abs/2601.12106)
*Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,Tereza C. Carvalho,Flavio de Oliveira Silva*

Main category: cs.NI

TL;DR: 论文提出在UPF内核层面进行插装，评估噪声切片对数据平面处理的影响，发现即使优先切片也会受到NN效应导致的性能下降


<details>
  <summary>Details</summary>
Motivation: 虚拟化和容器化增强了移动网络架构的模块化和可扩展性，但在B5G网络中，共享底层硬件可能导致切片间的数据平面争用，产生噪声邻居效应，影响网络切片和服务级别协议

Method: 在用户平面功能（UPF）的内核层面进行插装，评估噪声切片对数据平面处理的影响

Result: 研究发现即使优先切片也容易受到噪声邻居效应导致的性能下降，对用户体验相关的延迟指标有可观测的影响

Conclusion: 需要进一步解决B5G网络中切片隔离问题，噪声邻居效应是影响网络切片性能的重要挑战

Abstract: Virtualization and containerization enhance the modularity and scalability of mobile network architectures, facilitating customized user services and improving management and orchestration across the network. In the context of the 5th Generation Mobile Network (5G), these advancements contribute to reduced Operational Expenditures (OPEX) and enable sliced-based networking for novel applications and services. However, as beyond fifth-generation (B5G) networks aim to address the remaining challenges regarding network slice isolation, the shared underlying hardware can lead to data plane contention among slices, resulting in the Noisy Neighbor (NN) effect, which may compromise network slicing and Service-Level Agreements (SLAs). We propose a kernel-level instrumentation of the User Plane Function (UPF) to assess the impact of noisy slices on data plane processing. Our findings reveal that even prioritized slices are susceptible to degradation induced by NN, with observable effects on latency metrics pertinent to user experience.

</details>


### [5] [IntAgent: NWDAF-Based Intent LLM Agent Towards Advanced Next Generation Networks](https://arxiv.org/abs/2601.13114)
*Abdelrahman Soliman,Ahmed Refaey,Aiman Erbad,Amr Mohamed*

Main category: cs.NI

TL;DR: IntAgent是一个基于意图的智能LLM代理，通过集成NWDAF分析和工具来自动化网络操作，能够利用实时网络分析来推理和选择工具，实现复杂网络意图的自主满足。


<details>
  <summary>Details</summary>
Motivation: 意图网络（IBNs）作为创新技术，通过高级请求语句自动化网络操作，但现有方法缺乏与实时网络分析的深度集成。本文旨在开发一个智能代理，能够利用网络数据分析来更好地理解和满足网络操作员的意图。

Method: 开发IntAgent智能意图LLM代理，在NWDAF分析引擎内直接构建意图工具引擎，使代理能够利用实时网络分析进行推理和工具选择。提供符合3GPP标准的增强数据源，以及用于调度、监控和分析工具的MCP工具服务器。

Result: 通过两个实际用例验证了框架的有效性：基于ML的流量预测和调度策略执行，证明了IntAgent能够自主满足复杂的网络意图。

Conclusion: IntAgent通过集成NWDAF分析和工具，实现了对网络操作员意图的动态、上下文感知的满足，为意图网络提供了更智能、自主的解决方案。

Abstract: Intent-based networks (IBNs) are gaining prominence as an innovative technology that automates network operations through high-level request statements, defining what the network should achieve. In this work, we introduce IntAgent, an intelligent intent LLM agent that integrates NWDAF analytics and tools to fulfill the network operator's intents. Unlike previous approaches, we develop an intent tools engine directly within the NWDAF analytics engine, allowing our agent to utilize live network analytics to inform its reasoning and tool selection. We offer an enriched, 3GPP-compliant data source that enhances the dynamic, context-aware fulfillment of network operator goals, along with an MCP tools server for scheduling, monitoring, and analytics tools. We demonstrate the efficacy of our framework through two practical use cases: ML-based traffic prediction and scheduled policy enforcement, which validate IntAgent's ability to autonomously fulfill complex network intents.

</details>


### [6] [A Tutorial on Controlling Metasurfaces from the Network Perspective](https://arxiv.org/abs/2601.12118)
*Christos Liaskos,Evangelos Papapetrou,Kostas Katsalis,Dimitrios Tyrovolas,Alexandros Papadopoulos,Stavros Tsimpoukis,Arash Pourdamghani,Max Franke,Stefan Schmid*

Main category: cs.NI

TL;DR: 该教程论文将超表面视为网络系统组件，提出基于图论的建模框架，用于优化超表面在无线通信中的性能，并探讨标准化和仿真集成等挑战。


<details>
  <summary>Details</summary>
Motivation: 超表面作为可实时控制电磁波传播的革命性结构，在无线通信中具有提升数据速率、隐私保护、能效和环境感知的潜力，但需要系统化的网络视角来优化其应用。

Method: 将超表面建模为波路由器，采用图论描述超表面系统，建立性能目标框架，开发启发式和路径搜索算法作为实际求解器，并探讨与通信系统集成的工作流程。

Result: 提出了基于图论的超表面系统建模框架，能够优化系统性能，展示了与Omnet++等网络仿真器的集成方法，并识别了标准化和AI驱动应用等未来研究方向。

Conclusion: 将超表面视为网络系统组件并采用图论建模是优化无线通信应用的有效方法，未来需要在图论算法、标准化和网络仿真集成等方面进一步研究，特别是在AI驱动应用领域。

Abstract: Metasurfaces have emerged as transformative electromagnetic structures for wireless communications, enabling the real-time control over wave propagation, yielding potential for improved data rates, privacy, energy efficiency and even precise environmental sensing. This tutorial offers a perspective on controlling metasurfaces by treating them as components of a larger networked system. Towards this end, we first review the physical principles of metasurfaces and their various applications, followed by an exploration of manufacturing approaches for creating these structures. Then, aligning with standard network layer concepts, we describe the modeling of metasurfaces as wave routers, enabling us to describe systems of metasurfaces using graph theory. This approach enables the development of a performance objective framework for optimizing these systems, while classes of heuristic and path-finding-driven algorithms are discussed as practical solvers. The paper also examines the integration of metasurfaces with communication systems, by presenting their overall workflow, discussing its relation to ongoing standardization efforts, as well as defining a context for their integration to network simulators, using Omnet++ as a driving example. Finally, the paper explores future directions for research in this field, identifying graph-theoretic, standardization and integration challenges, relating to several networking disciplines including AI-driven applications.

</details>


### [7] [Understanding Partial Reachability in the Internet Core](https://arxiv.org/abs/2601.12196)
*Guillermo Baltra,Tarang Saluja,Yuri Pradkin,John Heidemann*

Main category: cs.NI

TL;DR: 该论文提出互联网存在持久性部分可达性（半岛）和完全隔离（岛屿）现象，并开发了观测算法，发现这些现象比网络中断更常见，对现有测量系统有重要影响。


<details>
  <summary>Details</summary>
Motivation: 互联网面临政治压力、架构变化（私有云、运营商级NAT、防火墙）和商业争端等威胁，导致路由碎片化和条件性连接。持久性部分可达性是互联网的基本但未被充分探索的问题。

Method: 1. 基于连接性而非权威性定义互联网核心概念；2. 识别"半岛"（持久部分连接）和"岛屿"（与互联网核心隔离）；3. 开发观测算法并在Trinocular（6个位置观测500万网络）和RIPE Atlas（1.3万个位置扫描DNS根）系统上应用；4. 使用三年数据进行交叉验证；5. 与CAIDA Ark数据验证。

Result: 1. 发现半岛现象比互联网中断更常见；2. 排除半岛和岛屿噪声可改善现有测量系统（噪声是RIPE DNSmon操作事件的5-9.7倍）；3. 45%半岛事件是路由瞬态，但90%半岛时间由少数（7%）长期事件造成；4. 算法具有良好召回率（0.94）和精度（0.42-0.82）；5. 中性定义显示没有单一国家或组织能单方面控制互联网核心。

Conclusion: 持久性部分可达性是互联网的基本特征，对互联网政策、治理和测量系统有重要影响。排除这些现象作为噪声可显著改善网络监测，同时表明互联网核心具有分布式控制特性。

Abstract: Routing strives to connect all the Internet, but compete: political pressure threatens routing fragmentation; architectural changes such as private clouds, carrier-grade NAT, and firewalls make connectivity conditional; and commercial disputes create partial reachability for days or years. This paper suggests *persistent, partial reachability is fundamental to the Internet* and an underexplored problem. We first *derive a conceptual definition of the Internet core* based on connectivity, not authority. We identify *peninsulas*: persistent, partial connectivity; and *islands*: when computers are partitioned from the Internet core. Second, we develop algorithms to observe each across the Internet, and apply them to two existing measurement systems: Trinocular, where 6 locations observe 5M networks frequently, and RIPE Atlas, where 13k locations scan the DNS roots frequently. Cross-validation shows our findings are stable over *three years of data*, and consistent with as few as 3 geographically-distributed observers. We validate peninsulas and islands against CAIDA Ark, showing good recall (0.94) and bounding precision between 0.42 and 0.82. Finally, our work has broad practical impact: we show that *peninsulas are more common than Internet outages*. Factoring out peninsulas and islands as noise can *improve existing measurement systems*; their ``noise'' is $5\times$ to $9.7\times$ larger than the operational events in RIPE's DNSmon. We show that most peninsula events are routing transients (45\%), but most peninsula-time (90\%) is due to a few (7\%) long-lived events. Our work helps inform Internet policy and governance, with our neutral definition showing no single country or organization can unilaterally control the Internet core.

</details>


### [8] [Cross-reality Location Privacy Protection in 6G-enabled Vehicular Metaverses: An LLM-enhanced Hybrid Generative Diffusion Model-based Approach](https://arxiv.org/abs/2601.12311)
*Xiaofeng Luo,Jiayi He,Jiawen Kang,Ruichen Zhang,Zhaoshui He,Ekram Hossain,Dong In Kim*

Main category: cs.NI

TL;DR: 提出基于混合动作的跨现实位置隐私保护框架，用于6G车联网元宇宙中的自动驾驶车辆，通过位置扰动和AI代理迁移保护位置隐私，同时平衡服务延迟和服务质量。


<details>
  <summary>Details</summary>
Motivation: 6G车联网元宇宙中，自动驾驶车辆在物理和虚拟空间交互时面临严重的跨现实位置隐私风险，攻击者可通过关联物理位置请求和虚拟AI代理部署位置来推断车辆轨迹。

Method: 设计基于混合动作的跨现实位置隐私保护框架：1）物理空间的连续位置扰动；2）虚拟空间的离散隐私感知AI代理迁移。提出跨现实位置熵隐私度量，并开发LLM增强的混合扩散近端策略优化算法（LHDPPO），结合LLM驱动的奖励设计和生成扩散模型策略探索。

Result: 在真实数据集上的实验表明，该框架能有效缓解6G车联网元宇宙场景中自动驾驶车辆的跨现实位置隐私泄露，同时保持用户沉浸感。

Conclusion: 提出的混合动作隐私保护框架成功解决了车联网元宇宙中的跨现实位置隐私问题，在位置保护、服务延迟和服务质量之间实现了良好平衡，为6G环境下的车辆隐私保护提供了有效解决方案。

Abstract: The emergence of 6G-enabled vehicular metaverses enables Autonomous Vehicles (AVs) to operate across physical and virtual spaces through space-air-ground-sea integrated networks. The AVs can deploy AI agents powered by large AI models as personalized assistants, on edge servers to support intelligent driving decision making and enhanced on-board experiences. However, such cross-reality interactions may cause serious location privacy risks, as adversaries can infer AV trajectories by correlating the location reported when AVs request LBS in reality with the location of the edge servers on which their corresponding AI agents are deployed in virtuality. To address this challenge, we design a cross-reality location privacy protection framework based on hybrid actions, including continuous location perturbation in reality and discrete privacy-aware AI agent migration in virtuality. In this framework, a new privacy metric, termed cross-reality location entropy, is proposed to effectively quantify the privacy levels of AVs. Based on this metric, we formulate an optimization problem to optimize the hybrid action, focusing on achieving a balance between location protection, service latency reduction, and quality of service maintenance. To solve the complex mixed-integer problem, we develop a novel LLM-enhanced Hybrid Diffusion Proximal Policy Optimization (LHDPPO) algorithm, which integrates LLM-driven informative reward design to enhance environment understanding with double Generative Diffusion Models-based policy exploration to handle high-dimensional action spaces, thereby enabling reliable determination of optimal hybrid actions. Extensive experiments on real-world datasets demonstrate that the proposed framework effectively mitigates cross-reality location privacy leakage for AVs while maintaining strong user immersion within 6G-enabled vehicular metaverse scenarios.

</details>


### [9] [LiQSS: Post-Transformer Linear Quantum-Inspired State-Space Tensor Networks for Real-Time 6G](https://arxiv.org/abs/2601.12375)
*Farhad Rezazadeh,Hatim Chergui,Mehdi Bennis,Houbing Song,Lingjia Liu,Dusit Niyato,Merouane Debbah*

Main category: cs.NI

TL;DR: 提出线性量子启发状态空间模型(LiQSS)，用于6G O-RAN中的高效无线电遥测预测，相比Transformer模型参数减少155倍，推理速度提升2.74倍。


<details>
  <summary>Details</summary>
Motivation: 6G O-RAN需要满足近实时延迟和计算约束的控制级预测，但Transformer模型的二次复杂度限制了其在近实时RAN智能控制器中的可扩展性。

Method: 采用后Transformer设计范式，提出量子启发的多体状态空间张量网络，用稳定的结构化状态空间动态核替代自注意力机制，实现线性时间序列建模。使用张量训练/矩阵乘积状态表示减少参数化和数据移动，轻量级通道门控和混合层捕获非平稳跨KPI依赖。

Result: LiQSS模型比现有结构化状态空间基线小10.8-15.8倍，快约1.4倍。相比Transformer模型，参数减少达155倍，推理速度提升达2.74倍，同时保持预测精度。

Conclusion: 量子启发的张量网络状态空间模型为6G O-RAN中的高效遥测预测提供了可行方案，在保持精度的同时显著降低了计算复杂度和延迟，适合近实时RAN智能控制器应用。

Abstract: Proactive and agentic control in Sixth-Generation (6G) Open Radio Access Networks (O-RAN) requires control-grade prediction under stringent Near-Real-Time (Near-RT) latency and computational constraints. While Transformer-based models are effective for sequence modeling, their quadratic complexity limits scalability in Near-RT RAN Intelligent Controller (RIC) analytics. This paper investigates a post-Transformer design paradigm for efficient radio telemetry forecasting. We propose a quantum-inspired many-body state-space tensor network that replaces self-attention with stable structured state-space dynamics kernels, enabling linear-time sequence modeling. Tensor-network factorizations in the form of Tensor Train (TT) / Matrix Product State (MPS) representations are employed to reduce parameterization and data movement in both input projections and prediction heads, while lightweight channel gating and mixing layers capture non-stationary cross-Key Performance Indicator (KPI) dependencies. The proposed model is instantiated as an agentic perceive-predict xApp and evaluated on a bespoke O-RAN KPI time-series dataset comprising 59,441 sliding windows across 13 KPIs, using Reference Signal Received Power (RSRP) forecasting as a representative use case. Our proposed Linear Quantum-Inspired State-Space (LiQSS) model is 10.8x-15.8x smaller and approximately 1.4x faster than prior structured state-space baselines. Relative to Transformer-based models, LiQSS achieves up to a 155x reduction in parameter count and up to 2.74x faster inference, without sacrificing forecasting accuracy.

</details>


### [10] [SDN-Blockchain Based Security Routing for UAV Communication via Reinforcement Learning](https://arxiv.org/abs/2601.12774)
*Yulu Han,Ziye Jia,Jingjing Zhao,Lijun He,Yao Wu,Qihui Wu*

Main category: cs.NI

TL;DR: 本文提出了一种结合SDN和区块链的无人机网络安全路由架构，并设计了BSPPO算法，通过波束搜索预筛选高安全路径，再使用PPO进行逐跳路由决策，显著提升了动态攻击环境下的路由性能。


<details>
  <summary>Details</summary>
Motivation: 无人机网络在应急通信中至关重要，但在动态且易受攻击的环境中设计同时满足低延迟、高能效和安全性的可靠路由策略具有挑战性。需要解决集中控制、防篡改信任管理以及动态攻击下的自适应路由问题。

Method: 1. 设计集成SDN（集中控制）和区块链（防篡改信任管理）的安全路由架构；2. 引入新的安全度指标量化无人机可信度；3. 提出BSPPO算法：波束搜索预筛选高安全候选路径，近端策略优化进行逐跳路由决策，支持攻击检测时的动态重路由。

Result: 在不同攻击密度、数据包大小和重路由事件下的广泛仿真表明，BSPPO在延迟、能耗和传输成功率方面优于PPO、BS-Q学习和BS-actor critic算法，显示出卓越的鲁棒性和适应性。

Conclusion: 提出的集成SDN和区块链的安全路由架构以及BSPPO算法，有效解决了无人机网络在动态攻击环境下的安全路由问题，实现了低延迟、高能效和高安全性的平衡，为应急通信提供了可靠解决方案。

Abstract: The unmanned aerial vehicle (UAV) network plays important roles in emergency communications. However, it is challenging to design reliable routing strategies that ensure low latency, energy efficiency, and security in the dynamic and attack-prone environments. To this end, we design a secure routing architecture integrating software-defined networking (SDN) for centralized control and blockchain for tamper-proof trust management. In particular, a novel security degree metric is introduced to quantify the UAV trustworthiness. Based on this architecture, we propose a beam search-proximal policy optimization (BSPPO) algorithm, where beam search (BS) pre-screens the high-security candidate paths, and proximal policy optimization (PPO) performs hop-by-hop routing decisions to support dynamic rerouting upon attack detections. Finally, extensive simulations under varying attack densities, packet sizes, and rerouting events demonstrate that BSPPO outperforms PPO, BS-Q learning, and BS-actor critic in terms of delay, energy consumption, and transmission success rate, showing the outstanding robustness and adaptability.

</details>


### [11] [Path to Diversity: A Primer on ISAC-izing Commodity Wi-Fi for Practical Deployments](https://arxiv.org/abs/2601.12980)
*Hongbo Wang,Xin Li,Yinghui He,Jingzhi Hu,Mingming Xu,Zhe Chen,Fu Xiao,Jun Luo*

Main category: cs.NI

TL;DR: 该教程采用自下而上的方法，从物理层多样性角度系统分析Wi-Fi技术进步带来的感知增益，围绕时间、频率、链路和空间四个正交维度构建框架，为商品Wi-Fi实现ISAC提供全面指导。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC研究多采用自上而下的视角，强调上层应用或深度学习模型，将物理层视为黑盒，缺乏对信号形成底层和物理层障碍的技术指导。为填补这一空白，本教程从物理层多样性角度出发，系统分析Wi-Fi标准演进带来的感知能力提升。

Method: 采用自下而上的方法，围绕四个正交维度构建框架：1) 时间多样性解决同步差距实现绝对测距；2) 频率多样性扩展有效带宽提高距离分辨率；3) 链路多样性利用分布式拓扑和数字反馈实现无处不在的可观测性；4) 空间多样性利用多天线阵列结合被动角度分辨与主动方向控制。

Result: 通过整合这四个正交维度，解决了时间、距离和空间中的基本模糊问题，将物理能力与具有挑战性的感知多样性需求连接起来，为"ISAC化"商品Wi-Fi提供了全面指导。

Conclusion: 该教程通过物理层多样性的系统分析，为未来标准化和稳健部署铺平了道路，为商品Wi-Fi实现高性能ISAC提供了实用的技术框架和指导。

Abstract: Integrated Sensing and Communication (ISAC) has emerged as a key paradigm in next-generation wireless networks. While the ubiquity and low cost of commodity Wi-Fi make it an ideal platform for wide-scale sensing, it is the continuous evolution of Wi-Fi standards-towards higher frequency bands, wider bandwidths, and larger antenna arrays-that fundamentally unlocks the physical resources required for high-performance ISAC. To structure this rapidly expanding field, numerous surveys have appeared. However, prevailing literature predominantly adopts a top-down perspective, emphasizing upper-layer applications or deep learning models while treating the physical layer as an opaque abstraction. Consequently, these works often fail to touch the bottom layer of signal formation and lack technical guidance on overcoming the physical barriers that constrain sensing performance. To bridge this gap, this tutorial takes a bottom-up approach, systematically analyzing the sensing gains brought by Wi-Fi advancements through the lens of physical-layer diversity. We organize the framework around four orthogonal dimensions: i) Temporal Diversity addresses synchronization gaps to enable absolute ranging; ii) Frequency Diversity expands the effective bandwidth to sharpen range resolution; iii) Link Diversity leverages distributed topologies and digital feedback to achieve ubiquitous observability; and iv) Spatial Diversity utilizes multi-antenna arrays to combine passive angular discrimination with active directional control. Collectively, these orthogonal dimensions resolve fundamental ambiguities in time, range, and space, bridging physical capabilities with challenging sensing diversities. By synthesizing these dimensions, this tutorial provides a comprehensive guide for "ISAC-izing" commodity Wi-Fi, paving the way for future standardization and robust deployment.

</details>


### [12] [No Traffic to Cry: Traffic-Oblivious Link Deactivation for Green Traffic Engineering](https://arxiv.org/abs/2601.13087)
*Max Ilsen,Daniel Otten,Nils Aschenbruck,Markus Chimani*

Main category: cs.NI

TL;DR: 提出一种流量无关的路由方案，通过关闭网络中的冗余连接来节省能耗，同时保证任何按比例缩放的流量矩阵都能被路由


<details>
  <summary>Details</summary>
Motivation: 互联网流量增长导致基础设施能耗增加，非高峰时段网络利用率低，现有绿色流量工程方法需要频繁适应流量变化，难以实现快速调整

Method: 采用流量无关路由方案，研究NP难问题：激活尽可能少的连接，同时保证任何按比例缩放的流量矩阵都能被路由；提出近似算法和两种后处理启发式方法

Result: 提出max(1/(ϱ·λ_min),2)近似算法，评估显示能快速生成接近最优解，避免频繁重新配置

Conclusion: 该方法为骨干网实现实际节能提供了有前景的方向，通过设计避免了频繁重新配置的需求

Abstract: As internet traffic grows, the underlying infrastructure consumes increasing amounts of energy. During off-peak hours, large parts of the networks remain underutilized, presenting significant potential for energy savings. Existing Green Traffic Engineering approaches attempt to leverage this potential by switching off those parts of the networks that are not required for the routing of specific traffic matrices. When traffic changes, the approaches need to adapt rapidly, which is hard to achieve given the complexity of the problem. We take a fundamentally different approach: instead of considering a specific traffic matrix, we rely on a traffic-oblivious routing scheme. We discuss the NP-hard problem of activating as few connections as possible while still guaranteeing that any down-scaled traffic matrix $\varrho\cdot T$ can be routed, where $\varrho \in (0,1)$ and $T$ is any traffic matrix routable in the original network. We present a $\max(\frac{1}{\varrho\cdotλ_{\text{min}}},2)$-approximation algorithm for this problem, with $λ_{\text{min}}$ denoting the minimum number of connections between any two connected routers. Additionally, we propose two post-processing heuristics to further improve solution quality. Our evaluation shows that we can quickly generate near-optimal solutions. By design, our method avoids the need for frequent reconfigurations and offers a promising direction to achieve practical energy savings in backbone networks.

</details>


### [13] [Conflict Detection in AI-RAN: Efficient Interaction Learning and Autonomous Graph Reconstruction](https://arxiv.org/abs/2601.13213)
*Joao F. Santos,Arshia Zolghadr,Scott Kuzdeba,Jacek Kibiłda*

Main category: cs.NI

TL;DR: 提出首个AI原生移动网络冲突检测系统框架，采用双塔编码器架构学习RAN数据交互，引入数据驱动的稀疏机制自动重建冲突图，无需人工调参。


<details>
  <summary>Details</summary>
Motivation: 6G AI原生移动网络中多个AI代理优化不同且常竞争的目标，冲突不可避免，会降低性能、导致不稳定和服务中断。现有方法依赖复杂计算昂贵的GNN和人工设定阈值构建冲突图。

Method: 提出系统化冲突检测框架，采用双塔编码器架构学习RAN数据中的交互关系，引入数据驱动的稀疏机制自动重建冲突图，无需人工微调。

Result: 首个系统化AI原生移动网络冲突检测框架，能够自动学习RAN数据交互并重建冲突图，摆脱对复杂GNN和人工阈值的依赖。

Conclusion: 该框架为AI原生移动网络提供了一种高效、自动化的冲突检测解决方案，有助于提升6G网络的稳定性和性能，减少人工干预需求。

Abstract: Artificial Intelligence (AI)-native mobile networks represent a fundamental step toward 6G, where learning, inference, and decision making are embedded into the Radio Access Network (RAN) itself. In such networks, multiple AI agents optimize the network to achieve distinct and often competing objectives. As such, conflicts become inevitable and have the potential to degrade performance, cause instability, and disrupt service. Current approaches for conflict detection rely on conflict graphs created based on relationships between AI agents, parameters, and Key Performance Indicators (KPIs). Existing works often rely on complex and computationally expensive Graph Neural Networks (GNNs) and depend on manually chosen thresholds to create conflict graphs. In this work, we present the first systematic framework for conflict detection in AI-native mobile networks, propose a two-tower encoder architecture for learning interactions based on data from the RAN, and introduce a data-driven sparsity-based mechanism for autonomously reconstructing conflict graphs without manual fine-tuning.

</details>


### [14] [Spectrum & RAN Sharing: A Measurement-based Case Study of Commercial 5G Networks in Spain](https://arxiv.org/abs/2601.13484)
*Rostand A. K. Fezeu,Lilian C. Freitas,Eman Ramadan,Jason Carpenter,Claudio Fiandrino,Joerg Widmer,Zhi-Li Zhang*

Main category: cs.NI

TL;DR: 首次对商用5G频谱和RAN共享进行实证测量研究，分析资源池化对用户感知性能的影响


<details>
  <summary>Details</summary>
Motivation: 虽然RAN共享的经济效益已被充分理解，但资源池化对用户感知性能的影响在真实商用部署中仍未被充分探索，特别是在5G时代面临频谱稀缺和基础设施成本挑战的背景下

Method: 进行首次商用5G频谱和RAN共享的实证测量研究，不仅识别真实世界中的共享部署实例，还分析用户感知性能及其对体验质量(QoE)的影响

Result: 研究提供了关于资源管理和频谱效率的关键见解，为5G、6G及未来的网络演进提供了蓝图和启示

Conclusion: 这项研究填补了RAN共享对用户性能影响的研究空白，为未来网络演进提供了实证基础和指导

Abstract: Radio Access Network (RAN) sharing, which often also includes spectrum sharing, is a strategic cooperative agreement among two or more mobile operators, where one operator may use another's RAN infrastructure to provide mobile services to its users. By mutually sharing physical sites, radio elements, licensed spectrum and other parts of the RAN infrastructure, participating operators can significantly reduce the capital (and operational) expenditure in deploying and operating cellular networks, while accelerating coverage expansion -- thereby addressing the spectrum scarcity and infrastructure cost challenges in the 5G era and beyond. While the economic benefits of RAN sharing are well understood, the impact of such resource pooling on user-perceived performance remains underexplored, especially in real-world commercial deployments. We present, to the best of our knowledge, the first empirical measurement study of commercial 5G spectrum and RAN sharing. Our measurement study is unique in that, beyond identifying real-world instances of shared 5G spectrum and RAN deployment "in the wild", we also analyze users' perceived performance and its implication on Quality of Experience (QoE). Our study provides critical insights into resource management (i.e., pooling) and spectrum efficiency, offering a blueprint (and implications) for network evolution in 5G, 6G and beyond.

</details>


### [15] [Reinforcement Learning for Opportunistic Routing in Software-Defined LEO-Terrestrial Systems](https://arxiv.org/abs/2601.13662)
*Sivaram Krishnan,Zhouyou Gu,Jihong Park,Sung-Min Oh,Jinho Choi*

Main category: cs.NI

TL;DR: 论文提出了一种基于GEO-SDN控制器的机会路由方法，利用强化学习优化LEO卫星网络中的数据传输延迟


<details>
  <summary>Details</summary>
Motivation: 大规模低地球轨道卫星星座的快速发展需要智能路由策略，以应对快速变化的拓扑结构和间歇性的网关可见性，实现低延迟、鲁棒的数据传输

Method: 利用GEO卫星的软件定义网络控制器，提出机会路由方法，将数据包转发到任何可用的地面网关而非固定目的地；采用约束随机优化问题建模，并运用残差强化学习框架优化路由以减少传输延迟

Result: 多日轨道数据的仿真结果表明，该方法相比经典背压算法和其他知名排队算法，在队列长度减少方面取得了显著改进

Conclusion: 基于GEO-SDN控制器的机会路由结合强化学习优化，是实现动态LEO卫星网络中低延迟、鲁棒数据传输的有效方法

Abstract: The proliferation of large-scale low Earth orbit (LEO) satellite constellations is driving the need for intelligent routing strategies that can effectively deliver data to terrestrial networks under rapidly time-varying topologies and intermittent gateway visibility. Leveraging the global control capabilities of a geostationary (GEO)-resident software-defined networking (SDN) controller, we introduce opportunistic routing, which aims to minimize delivery delay by forwarding packets to any currently available ground gateways rather than fixed destinations. This makes it a promising approach for achieving low-latency and robust data delivery in highly dynamic LEO networks. Specifically, we formulate a constrained stochastic optimization problem and employ a residual reinforcement learning framework to optimize opportunistic routing for reducing transmission delay. Simulation results over multiple days of orbital data demonstrate that our method achieves significant improvements in queue length reduction compared to classical backpressure and other well-known queueing algorithms.

</details>


### [16] [Generative Intent Prediction Agentic AI empowered Edge Service Function Chain Orchestration](https://arxiv.org/abs/2601.13694)
*Yan Sun,Shaoyong Guo,Sai Huang,Zhiyong Feng,Feng Qi,Xuesong Qiu*

Main category: cs.NI

TL;DR: 提出基于生成式意图预测代理(GIPA)的边缘服务功能链编排框架，通过预测用户隐式意图实现从被动执行到主动预测的范式转变


<details>
  <summary>Details</summary>
Motivation: 边缘网络环境中，高用户移动性和隐式服务意图对传统基于大语言模型的智能体AI的被动和反应式管理构成挑战，现有方法在处理动态需求和预测用户隐式意图方面存在局限

Method: 1) 构建包含功能偏好、QoS敏感性和资源需求的多维意图空间；2) 基于生成扩散模型设计意图预测模型，通过反向去噪过程从多维上下文中重构用户隐式意图；3) 将预测的隐式意图作为全局提示嵌入SFC编排模型，指导网络主动优化部署策略

Result: 实验结果表明，GIPA在高并发和高动态场景下优于现有基线方法

Conclusion: 提出的GIPA框架能够有效预测用户隐式意图，实现边缘网络服务功能链的主动编排，解决了传统被动管理在动态边缘环境中的局限性

Abstract: With the development of artificial intelligence (AI), Agentic AI (AAI) based on large language models (LLMs) is gradually being applied to network management. However, in edge network environments, high user mobility and implicit service intents pose significant challenges to the passive and reactive management of traditional AAI. To address the limitations of existing approaches in handling dynamic demands and predicting users' implicit intents, in this paper we propose an edge service function chain (SFC) orchestration framework empowered by a Generative Intent Prediction Agent (GIPA). Our GIPA aims to shift the paradigm from passive execution to proactive prediction and orchestration. First, we construct a multidimensional intent space that includes functional preferences, QoS sensitivity, and resource requirements, enabling the mapping from unstructured natural language to quantifiable physical resource demands. Second, to cope with the complexity and randomness of intent sequences, we design an intent prediction model based on a Generative Diffusion Model (GDM), which reconstructs users' implicit intents from multidimensional context through a reverse denoising process. Finally, the predicted implicit intents are embedded as global prompts into the SFC orchestration model to guide the network in proactively and ahead-of-time optimizing SFC deployment strategies. Experiment results show that GIPA outperforms existing baseline methods in highly concurrent and highly dynamic scenarios.

</details>


### [17] [IGAA: Intent-Driven General Agentic AI for Edge Services Scheduling using Generative Meta Learning](https://arxiv.org/abs/2601.13702)
*Yan Sun,Yinqiu Liu,Shaoyong Guo,Ruichen Zhang,Feng Qi,Xuesong Qiu,Weifeng Gong,Dusit Niyato,Qihui Wu*

Main category: cs.NI

TL;DR: 提出IGAA框架，通过元学习让AI代理从历史调度经验中持续学习，实现边缘服务调度的泛化能力，解决用户移动性带来的动态需求挑战。


<details>
  <summary>Details</summary>
Motivation: 用户移动性导致边缘网络服务需求高度动态变化，现有服务调度代理缺乏对新场景的泛化能力，需要能够适应不同场景的通用调度方案。

Method: 1) 网络-服务-意图矩阵映射方法生成训练数据；2) 易到难泛化学习方案（RCETL和APOTL算法）；3) 生成意图回放机制防止灾难性遗忘；4) 场景评估校正模型减少LLM幻觉影响。

Result: IGAA展现出强大的泛化能力和可扩展性，能够快速适应新场景，将学习到的策略迁移到类似新任务中。与场景特定方法相比，意图满足率差距保持在3.81%以内。

Conclusion: IGAA框架通过元学习和持续学习机制，有效解决了边缘服务调度中的泛化问题，为动态边缘网络环境提供了通用的自主调度解决方案。

Abstract: Agentic AI (AAI), which extends Large Language Models with enhanced reasoning capabilities, has emerged as a promising paradigm for autonomous edge service scheduling. However, user mobility creates highly dynamic service demands in edge networks, and existing service scheduling agents often lack generalization capabilities for new scenarios. Therefore, this paper proposes a novel Intent-Driven General Agentic AI (IGAA) framework. Leveraging a meta-learning paradigm, IGAA enables AAI to continuously learn from prior service scheduling experiences to achieve generalized scheduling capabilities. Particularly, IGAA incorporates three core mechanisms. First, we design a Network-Service-Intent matrix mapping method to allow agents to simulate novel scenarios and generate training datasets. Second, we present an easy-to-hard generalization learning scheme with two customized algorithms, namely Resource Causal Effect-aware Transfer Learning (RCETL) and Action Potential Optimality-aware Transfer Learning (APOTL). These algorithms help IGAA adapt to new scenarios. Furthermore, to prevent catastrophic forgetting during continual IGAA learning, we propose a Generative Intent Replay (GIR) mechanism that synthesizes historical service data to consolidate prior capabilities. Finally, to mitigate the effect of LLM hallucinations on scenario simulation, we incorporate a scenario evaluation and correction model to guide agents in generating rational scenarios and datasets. Extensive experiments demonstrate IGAA's strong generalization and scalability. Specifically, IGAA enables rapid adaptation by transferring learned policies to analogous new ones, such as applying latency-sensitive patterns from real-time computing to optimize novel Internet of Vehicles (IoV) services. Compared to scenario-specific methods, IGAA maintains the intent-satisfaction rate gap within 3.81%.

</details>


### [18] [Variational Dual-path Attention Network for CSI-Based Gesture Recognition](https://arxiv.org/abs/2601.13745)
*N. Zhang*

Main category: cs.NI

TL;DR: 提出VDAN轻量级特征预处理模块，通过变分双路径注意力网络解决Wi-Fi手势识别中CSI高维噪声和边缘设备资源受限问题，实现结构化特征精炼和可解释性


<details>
  <summary>Details</summary>
Motivation: Wi-Fi手势识别基于CSI面临高维噪声和边缘设备资源约束的挑战，现有端到端模型将特征提取与分类紧密耦合，忽略了CSI固有的时频稀疏性，导致冗余和泛化能力差

Method: 提出变分双路径注意力网络(VDAN)作为轻量级特征预处理模块，通过频域滤波和时序检测进行结构化特征精炼，引入变分推理建模注意力权重的不确定性以增强噪声鲁棒性

Result: 在公开数据集上的实验表明，学习到的注意力权重与CSI的物理稀疏特性一致，验证了其可解释性，为资源受限的无线传感系统提供了高效可解释的前端处理方案

Conclusion: VDAN模块从信息瓶颈和正则化角度解释设计原理，解决了CSI手势识别中的噪声和资源约束问题，实现了高效可解释的特征预处理

Abstract: Wi-Fi gesture recognition based on Channel State Information (CSI) is challenged by high-dimensional noise and resource constraints on edge devices. Prevailing end-to-end models tightly couple feature extraction with classification, overlooking the inherent time-frequency sparsity of CSI and leading to redundancy and poor generalization. To address this, this paper proposes a lightweight feature preprocessing module--the Variational Dual-path Attention Network (VDAN). It performs structured feature refinement through frequency-domain filtering and temporal detection. Variational inference is introduced to model the uncertainty in attention weights, thereby enhancing robustness to noise. The design principles of the module are explained from the perspectives of the information bottleneck and regularization. Experiments on a public dataset demonstrate that the learned attention weights align with the physical sparse characteristics of CSI, verifying its interpretability. This work provides an efficient and explainable front-end processing solution for resource-constrained wireless sensing systems.

</details>


### [19] [Interoperable rApp/xApp Control over O-RAN for Mobility-aware Dynamic Spectrum Allocation](https://arxiv.org/abs/2601.13769)
*Anastasios Giannopoulos,Sotirios Spantideas,Maria Lamprini Bartsioka,Panagiotis Trakadas*

Main category: cs.NI

TL;DR: 提出基于图论的O-RAN动态频谱分配框架，通过rApp/xApp协同实现分钟级流量预测与亚秒级资源分配，显著提升PRB分配成功率和公平性。


<details>
  <summary>Details</summary>
Motivation: O-RAN虽然支持无线接入功能解耦和控制应用跨时域部署，但在密集多小区干扰和异构业务需求下，设计能够同时利用长期流量感知和近实时无线资源优化的互操作控制方案仍然具有挑战性。

Method: 提出互操作的rApp/xApp驱动动态频谱分配框架：1) Non-RT RIC rApp预测聚合流量演化并在分钟级生成高层频谱策略；2) Near-RT RIC xApp构建用户中心冲突图并在亚秒级执行公平感知PRB分配；3) 采用冲突感知改进比例公平调度机制实现受控的无干扰PRB时间共享。

Result: 仿真结果表明，该框架在不同信道配置和用户需求下，显著提升PRB分配成功率（超过90%）和服务份额公平性（超过85%），同时保持架构分离和rApp/xApp互操作性。

Conclusion: 提出的基于图论的O-RAN动态频谱分配框架有效解决了密集干扰环境下的资源分配问题，实现了长期流量感知与近实时优化的协同，符合O-RAN架构原则。

Abstract: Open Radio Access Networks (O-RAN) enable the disaggregation of radio access functions and the deployment of control applications across different timescales. However, designing interoperable control schemes that jointly exploit long-term traffic awareness and near-real-time radio resource optimization remains a challenging problem, particularly under dense multi-cell interference and heterogeneous service demands. This paper proposes an interoperable rApp/xApp-driven dynamic spectrum allocation (DSA) framework for O-RAN, based on a graph-theoretic formulation of physical resource block (PRB) assignment. The proposed architecture leverages a non-real-time radio intelligent controller (Non-RT RIC) rApp to predict aggregated traffic evolution and generate high-level spectrum policies at the minutes timescale, while a near-real-time RIC (Near-RT RIC) xApp constructs a user-centric conflict graph and performs fairness-aware PRB allocation at sub-second timescales. To mitigate persistent user starvation, a conflict-aware modified proportional fair (MPF) scheduling mechanism is applied, enabling controlled interference-free PRB time-sharing. Extensive simulation results demonstrate that the proposed framework significantly improves the PRB assignment success rate (above 90%) and service-share fairness (above 85%) across different channel configurations and user demands, while maintaining architectural separation and rApp/xApp interoperability in accordance with O-RAN principles.

</details>


### [20] [Demystifying Starlink Network Performance under Vehicular Mobility with Dynamic Beam Switching](https://arxiv.org/abs/2601.13790)
*Jinwei Zhao,Jack Baude,Ali Ahangarpour,Vaibhava Krishna Devulapalli,Sree Ganesh Lalitaditya Divakarla,Zhi-Li Zhang,Jianping Pan*

Main category: cs.NI

TL;DR: 提出一种移动感知的Starlink卫星识别方法，能够检测动态波束切换事件，解释移动场景下的网络性能问题


<details>
  <summary>Details</summary>
Motivation: 现有Starlink卫星识别方法仅适用于静止、无遮挡场景，无法处理用户终端移动、遮挡或动态波束切换事件，导致对移动场景下网络性能下降原因理解不足

Method: 提出移动感知的Starlink卫星识别方法，检测动态波束切换事件，结合用户终端诊断数据和连接卫星信息解释网络性能

Result: 发现用户终端在链路质量下降时会尝试多次动态波束切换以连接不同卫星，这种切换可能由视场变化、遮挡或信号质量差引起，导致切换间隔超过已知的15秒常规切换间隔

Conclusion: 该方法揭示了移动Starlink网络性能下降的原因，对提升传输层协议端到端性能和各种应用场景至关重要

Abstract: In the last few years, considerable research efforts have focused on measuring and improving Starlink network performance, especially for user terminals (UTs) in stationary scenarios. However, the performance of Starlink networks in mobility settings, particularly with frequent changes in the UT's orientation, and the impact of environmental factors, such as transient obstructions, has not been thoroughly studied, leaving gaps in understanding the causes of performance degradation. Recently, researchers have started identifying the communicating satellites to evaluate satellite selection strategies and the impact on network performance. However, existing Starlink satellite identification methods only work in stationary, obstruction-free scenarios, as they do not account for UT mobility, obstructions or detect dynamic beam switching events. In this paper, we reveal that the UT can perform multiple dynamic beam switching attempts to connect to different satellites when the UT-satellite link is degraded. This degradation can occur either due to the loss of line-of-sight (LoS) from changes in the FOV or obstructions, or due to poor signal quality, extending UT-satellite handovers beyond the well-known 15-second regular handover interval. We propose a mobility-aware Starlink satellite identification method that detects dynamic beam switching events, and plausibly explain network performance using UT's diagnostic data and connected satellite information. Our findings demystifies the mobile Starlink network performance degradations, which is crucial to enhance the end-to-end performance of transport layer protocols and in diverse application scenarios.

</details>


### [21] [A Predictive and Preventive Digital Twin Framework for Indoor Wireless Networks](https://arxiv.org/abs/2601.13838)
*Jiunn-Tsair Chen*

Main category: cs.NI

TL;DR: 提出基于数字孪生的Wi-Fi网络预测性管理框架，通过分析性能上界和重要性采样来预测并预防网络性能下降


<details>
  <summary>Details</summary>
Motivation: Wi-Fi网络面临竞争性信道访问、密集部署和自管理操作导致的性能下降问题，需要预测性管理方法来避免性能恶化

Method: 1) 构建数字孪生框架捕捉无线信道时空特征和流量模式；2) 推导基于香农容量和CSMA-CA延迟的性能上界；3) 应用重要性采样识别高风险网络场景；4) 使用梯度搜索主动优化网络配置

Result: 仿真结果表明，该方法能成功预测时间相关的网络拥塞并提前缓解，展示了预测性和预防性Wi-Fi网络管理的潜力

Conclusion: 数字孪生框架结合性能上界分析和重要性采样，能够有效预测网络性能下降并主动优化配置，为Wi-Fi网络管理提供了新的预测性方法

Abstract: Wi-Fi networks increasingly suffer from performance degradation caused by contention-based channel access, dense deployments, and largely self-managed operation among mutually interfering access points (APs). In this paper, we propose a Digital Twin (DT) framework that captures the essential spatial and temporal characteristics of wireless channels and traffic patterns, enabling the prediction of likely future network scenarios while respecting physical constraints. Leveraging this predictive capability, we introduce two analytically derived performance upper bounds-one based on Shannon capacity and the other on latency behavior under CSMA-CA (Carrier Sense Multiple Access with Collision Avoidance)-that can be evaluated efficiently without time-consuming network simulations. By applying importance sampling to DT-generated scenarios, potentially risky network conditions can be identified within large stochastic scenario spaces. These same performance bounds are then used to proactively guide a gradient-based search for improved network configurations, with the objective of avoiding imminent performance degradation rather than pursuing globally optimal but fragile solutions. Simulation results demonstrate that the proposed approach can successfully predict time-dependent network congestion and mitigate it in advance, highlighting its potential for predictive and preventive Wi-Fi network management.

</details>


### [22] [Capacity and Energy Trade-Offs in FR3 6G Networks Using Real Deployment Data](https://arxiv.org/abs/2601.13993)
*David López-Pérez,Nicola Piovesan,Matteo Bernabè*

Main category: cs.NI

TL;DR: 基于中国商用4G/5G网络真实数据，分析7-24GHz频段6G多层网络的系统级性能，发现非共站、流量感知的部署策略能实现更优的能效比


<details>
  <summary>Details</summary>
Motivation: 现有6G研究多基于3GPP模板，缺乏真实部署和流量数据的支撑。本文旨在利用中国商用4G/5G网络的实际数据，评估6G在实际部署中的性能表现和能效问题

Method: 使用Giulia模型（基于部署信息的系统级异构网络模型），结合中国商用4G/5G网络的真实部署和流量数据，分析6G在7-24GHz频段的系统级性能

Result: 6G相比异构4G+5G部署可将中位数吞吐量提升高达9.5倍，但功耗增加高达59%。与现有站点共站部署增益有限且能耗高，而非共站、流量感知的部署策略能实现更优的吞吐量-瓦特效率

Conclusion: 6G网络规划需要采用战略性的、以用户设备热点为中心的部署策略，避免简单的共站部署，以实现更好的能效和性能平衡

Abstract: This article presents a data-driven system-level analysis of multi-layer 6G networks operating in the upper mid-band (FR3: 7-24 GHz). Unlike most prior studies based on 3rd Generation Partnership Project (3GPP) templates, we leverage real-world deployment and traffic data from a commercial 4G/5G network in China to evaluate practical 6G strategies. Using Giulia-a deployment-informed system-level heterogeneous network model-we show that 6G can boost median throughput by up to 9.5x over heterogeneous 4G+5G deployments, but also increases power usage by up to 59%. Critically, co-locating 6G with existing sites delivers limited gains while incurring high energy cost. In contrast, non-co-located, traffic-aware deployments achieve superior throughput-to-watt efficiency, highlighting the need for strategic, user equipment (UE) hotspot-focused 6G planning.

</details>


### [23] [MANATEE: A DevOps Platform for xApp Lifecycle Management and Testing in Open RAN](https://arxiv.org/abs/2601.14009)
*Sofia Montebugnoli,Leonardo Bonati,Andrea Sabbioni,Luca Foschini,Paolo Bellavista,Salvatore D'Oro,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: MANATEE平台结合DevOps CI/CD管道与服务网格技术，简化O-RAN环境中xApp的部署、测试和生命周期管理，实现低延迟、可靠的渐进式部署。


<details>
  <summary>Details</summary>
Motivation: 5G解耦架构带来灵活性但也增加复杂性，当前Open RAN生态系统缺乏xApp的自动化测试、无缝迁移和生产级可观测性，导致xApp交付缓慢且易出错。

Method: 提出MANATEE平台，结合DevOps CI/CD管道与服务网格技术，在Kubernetes集群上集成O-RAN软件社区近实时RIC，支持模拟、仿真和真实测试环境中的xApp测试。

Result: 服务网格集成引入的延迟开销低于1毫秒，支持可靠的canary部署和细粒度流量控制，通过熔断机制实现无冲突的A/B测试。

Conclusion: MANATEE是首个结合这些原则的平台，能简化xApp生产交付、加速创新，并保证异构O-RAN环境中的性能表现。

Abstract: The shift to disaggregated 5G architectures introduces unprecedented flexibility but also significant complexity in Beyond 5G Radio Access Networks (RANs). Open RAN enables programmability through xApps, yet deploying and validating these applications is critical given the nature of the systems they aim to control. Current Open RAN ecosystems lack robust lifecycle management of xApps that enable automated testing, seamless migration, and production-grade observability, resulting in slow, error-prone xApp delivery. To address these issues, DevOps practices can streamline the xApp lifecycle by integrating Continuous Integration/Continuous Deployment (CI/CD) pipelines with advanced traffic management and monitoring, such as leveraging service mesh technologies to enable progressive deployment strategies (e.g., canary releases and A/B testing) to ensure fine-grained observability and resilience. The solution presented in this article, MANATEE (Mesh Architecture for Radio Access Network Automation and TEsting Ecosystems), is the first platform that combines these principles to simplify xApp delivery into production, accelerate innovation, and guarantee performance across heterogeneous O-RAN environments. We prototyped MANATEE on a Kubernetes cluster integrated with the O-RAN Software Community Near-Real Time RAN Intelligent Controller (RIC), as well as with service mesh technologies, to facilitate testing of xApps across simulated, emulated, and real testbed environments. Our experimental results demonstrate that service mesh integration introduces minimal overhead (below 1 ms latency), while enabling reliable canary deployments with fine-grained traffic control and conflict-free A/B testing through circuit-breaking mechanisms.

</details>


### [24] [Communication Technologies for Intelligent Transportation Systems: From Railways to UAVs and Beyond](https://arxiv.org/abs/2601.14106)
*Shrief Rizkalla,Adrian Kliks,Nila Bagheri,Miguel A. Bellido-Manganell,Aniruddha Chandra,Anja Dakic,Laura Finarelli,Davy Gaillot,Matti Hamalainen,Ruisi He,Markus Hofer,Sandaruwan Jayaweera,Francesco Linsalata,Konstantin Mikhaylov,Jon M. Peha,Ibrahim Rashdan,Gianluca Rizzo,Abdul Saboor,Martin Schmidhammer,Michal Sybis,Fredrik Tufvesson,Paul Unterhuber,Fernando J. Velez,Evgenii Vinogradov,Michael Walter,Thomas Zemen,Haibin Zhang,Zhengyu Zhang*

Main category: cs.NI

TL;DR: 白皮书全面分析了支持现代和未来ICT的通信技术现状，旨在建立跨交通领域（铁路、公路、航空、无人机）通信解决方案如何实现自动化、安全和效率的共同理解，识别关键需求和技术使能因素，评估当前系统局限，并提出5G/6G/AI等新兴技术集成路径。


<details>
  <summary>Details</summary>
Motivation: 建立对通信技术如何支持跨交通领域（铁路、公路车辆、飞机、无人机）自动化、安全和效率的共同理解，识别当前系统的局限性，并为未来智能交通系统（ITS）的发展提供技术路线图。

Method: 通过综合分析现有通信技术状态，识别关键通信需求和技术使能因素，评估当前系统限制，提出5G、6G、AI驱动网络控制等新兴技术集成路径，并建立统一的通信建模、测试和标准化框架。

Result: 建立了跨交通领域通信解决方案的全面分析框架，识别了智能交通系统（ITS）的关键需求和技术使能因素，提出了集成5G/6G/AI等新兴技术的具体路径，强调了频谱管理和标准化对互操作性的重要性。

Conclusion: 该白皮书为未来智能交通系统提供了全面的通信技术路线图，强调了跨领域协作、统一框架和新兴技术集成的重要性，旨在推动学术界、产业界和标准化机构合作，设计具有弹性和适应性的未来交通通信基础设施。

Abstract: This white paper aims to comprehensively analyze and consolidate the state of the art in communication technologies supporting modern and future Information and Communication Technology (ICT). Its primary objective is to establish a common understanding of how communication solutions enable automation, safety, and efficiency across multiple transport domains, including railways, road vehicles, aircraft, and unmanned aerial vehicles. The document seeks to identify key communication requirements and technological enablers necessary for interoperable and reliable ITS operation. It also assesses the limitations of current systems and proposes pathways for integrating emerging technologies such as 5G, Sixth Generation (6G), and Artificial Intelligence (AI)-driven network control. The white paper also intends to support harmonization between different transport modes through a unified framework for communication modeling, testing, and standardization. It highlights the importance of accurate channel modeling and empirical validation to design efficient, robust, and scalable systems. Another objective is to explore the use of reconfigurable intelligent surfaces, integrated sensing and communication, and digital twin concepts within ITS. The document emphasizes the role of spectrum management and standardization efforts in ensuring interoperability among diverse communication systems. Finally, the paper seeks to stimulate collaboration among academia, industry, and standardization bodies to advance the design of resilient and adaptive communication infrastructures for future transportation systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [25] [MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?](https://arxiv.org/abs/2601.11559)
*Zilal Eiz AlDin,John Wu,Jeffrey Paul Fung,Jennifer King,Mya Watts,Lauren ONeill,Adam Richard Cross,Jimeng Sun*

Main category: cs.AI

TL;DR: 本文提出了MIMIC-RD基准，用于评估LLM在罕见病鉴别诊断中的表现，发现当前最先进模型表现不佳，揭示了现有能力与临床需求之间的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 罕见病影响十分之一的美国人，但其鉴别诊断仍然具有挑战性。现有评估LLM在罕见病诊断中的方法存在两个关键局限：1）依赖理想化的临床案例研究，无法捕捉真实世界的临床复杂性；2）使用ICD代码作为疾病标签，由于许多罕见病缺乏与Orphanet等综合罕见病数据库的直接映射，导致严重低估罕见病数量。

Method: 开发了MIMIC-RD基准，通过将临床文本实体直接映射到Orphanet数据库来构建罕见病鉴别诊断基准。方法包括：1）初始基于LLM的挖掘过程；2）由四名医学注释员验证，确认识别的实体是真正的罕见病。在145名患者的数据集上评估了各种模型。

Result: 当前最先进的大型语言模型在罕见病鉴别诊断方面表现不佳，揭示了现有能力与临床需求之间的巨大差距。

Conclusion: 需要改进罕见病的鉴别诊断方法，基于研究发现提出了几个未来改进方向。

Abstract: Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.

</details>


### [26] [A Mind Cannot Be Smeared Across Time](https://arxiv.org/abs/2601.11620)
*Michael Timothy Bennett*

Main category: cs.AI

TL;DR: 意识需要同时性计算而非顺序计算，硬件架构影响意识可能性


<details>
  <summary>Details</summary>
Motivation: 探讨机器意识的可能性，强调计算的时间特性（同时性 vs 顺序性）对意识体验统一性的重要性

Method: 扩展栈理论，引入时间窗口轨迹语义，区分强同步和弱同步假设，形式化并发容量概念

Result: 证明存在性时间实现不保留合取，意识需要客观同时实例化，严格顺序计算无法实现某些意识内容

Conclusion: 意识归因需要检查硬件架构而不仅仅是功能表现，同时性要求越高需要的并发容量越大

Abstract: Whether machines can be conscious depends not only on what they compute, but \emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates. Conscious experience appears unified and simultaneous. I show that this difference matters formally. I augment Stack Theory with algebraic laws relating within time-window constraint satisfaction to conjunction. I introduce a precise temporal semantics over windowed trajectories $τ^{Δ,s}$ and prove that existential temporal realisation $\Diamond_Δ$ does not preserve conjunction. A system can realise all the ingredients of experience across time without ever instantiating the experienced conjunction itself. I then distinguish two postulates. StrongSync requires objective co-instantiation of the grounded conjunction within the window, while WeakSync permits temporal ``smearing''. I formalise concurrency-capacity to measure what is needed to satisfy StrongSync. Finally, I review neurophysiological evidence suggesting that consciousness depends on phase synchrony and effective connectivity, and that loss of consciousness is often associated with its breakdown. This evidence makes WeakSync less plausible. Under StrongSync, software consciousness on strictly sequential substrates is impossible for contents whose grounding requires two or more simultaneous contributors. The more parts from which simultaneous contribution required, the more concurrency capacity is required. The hardware matters. Consciousness attribution therefore requires architectural inspection, not just functional performance.

</details>


### [27] [Kernel-Based Learning of Safety Barriers](https://arxiv.org/abs/2601.12002)
*Oliver Schön,Zhengang Zhong,Sadegh Soudjani*

Main category: cs.AI

TL;DR: 提出一种数据驱动的安全验证方法，通过控制屏障证书和核希尔伯特空间嵌入，为黑盒随机系统提供可扩展的安全保证。


<details>
  <summary>Details</summary>
Motivation: AI算法在自动驾驶、医疗等安全关键应用中快速集成，但传统形式化验证工具难以处理黑盒系统且缺乏扩展性，需要新的验证方法。

Method: 使用控制屏障证书保证系统安全，从系统轨迹数据中学习证书；采用条件均值嵌入将数据映射到RKHS空间，构建可膨胀的模糊集以增强鲁棒性；利用有限傅里叶展开将半无限优化问题转化为线性规划。

Result: 开发出谱屏障方法，通过快速傅里叶变换高效生成松弛问题，为黑盒系统提供可扩展且分布鲁棒的安全验证框架，成功应用于包含神经网络控制器的案例。

Conclusion: 该方法突破了系统动态和不确定性的限制性假设，为复杂黑盒AI系统提供了实用的安全验证解决方案。

Abstract: The rapid integration of AI algorithms in safety-critical applications such as autonomous driving and healthcare is raising significant concerns about the ability to meet stringent safety standards. Traditional tools for formal safety verification struggle with the black-box nature of AI-driven systems and lack the flexibility needed to scale to the complexity of real-world applications. In this paper, we present a data-driven approach for safety verification and synthesis of black-box systems with discrete-time stochastic dynamics. We employ the concept of control barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a set of system trajectories. We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be inflated to robustify the result to out-of-distribution behavior. We provide the theoretical results on how to apply the approach to general classes of temporal logic specifications beyond safety. For the data-driven computation of safety barriers, we leverage a finite Fourier expansion to cast a typically intractable semi-infinite optimization problem as a linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. Our work moves beyond restrictive assumptions on system dynamics and uncertainty, as demonstrated on two case studies including a black-box system with a neural network controller.

</details>


### [28] [Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models](https://arxiv.org/abs/2601.11622)
*Hassan Ugail,Newton Howard*

Main category: cs.AI

TL;DR: 该研究将神经科学中的时间整合与亚稳态概念应用于Transformer模型，提出一种复合动力学指标来量化LLM生成过程中的时间组织，发现结构化推理相比重复、噪声和扰动条件具有显著更高的动力学复杂度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过高维内部动力学进行文本生成，但这些动力学的时间组织仍未被充分理解。现有可解释性方法多关注静态表示或因果干预，而忽视了时间结构。研究者借鉴神经科学中时间整合和亚稳态作为神经组织核心标志的概念，试图将这些概念应用于Transformer模型。

Method: 研究者提出一种复合动力学指标，从自回归生成过程中的激活时间序列计算得出。在GPT-2-medium模型上评估该指标，设置了五种条件：结构化推理、强制重复、高温噪声采样、注意力头剪枝和权重噪声注入。通过单因素方差分析和效应大小进行统计检验，并对层选择、通道子采样和随机种子进行鲁棒性验证。

Result: 结构化推理条件相比重复、噪声和扰动条件表现出显著更高的动力学指标，统计差异显著（单因素方差分析），关键比较中效应大小较大。结果对层选择、通道子采样和随机种子具有鲁棒性。

Conclusion: 神经科学启发的动力学指标能够可靠地表征大型语言模型不同功能机制间的计算组织差异。该指标捕捉形式动力学特性，不暗示主观体验，为理解LLM内部时间动力学提供了新视角。

Abstract: Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric, computed from activation time-series during autoregressive generation. We evaluate this metric in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Structured reasoning consistently exhibits elevated metric relative to repetitive, noisy, and perturbed regimes, with statistically significant differences confirmed by one-way ANOVA and large effect sizes in key comparisons. These results are robust to layer selection, channel subsampling, and random seeds. Our findings demonstrate that neuroscience-inspired dynamical metrics can reliably characterise differences in computational organisation across functional regimes in large language models. We stress that the proposed metric captures formal dynamical properties and does not imply subjective experience.

</details>


### [29] [Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance](https://arxiv.org/abs/2601.11625)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: 本文提出一种训练时解释性方法，通过跟踪微调过程中token级归因的变化来监控模型决策证据的演变，并引入"推理稳定点"概念来识别归因变化趋于稳定的训练阶段。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型微调虽然能提升任务性能，但会微妙地改变模型依赖的证据。需要一种训练时解释性方法来监控决策证据如何演变，特别是在存在标签相关触发词等捷径的情况下。

Method: 提出解释漂移概念，定义为固定探测集上归一化token归因的epoch间变化。引入推理稳定点(RSP)，即漂移首次进入并保持低稳定状态的最早epoch。该方法仅使用训练过程中的漂移动态，无需在分布外数据上调整。

Result: 在多个轻量级Transformer分类器和基准分类任务中，漂移通常在训练早期就进入低稳定状态，而验证准确率仅发生微小变化。在受控的捷径设置中，归因动态揭示了模型对捷径的依赖增加，即使验证准确率保持竞争力。

Conclusion: 解释漂移提供了一种简单、低成本的诊断工具，用于监控微调过程中决策证据的演变，并选择处于稳定证据状态的检查点。该方法能暴露模型对捷径的依赖，即使验证性能看起来良好。

Abstract: Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.

</details>


### [30] [PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement](https://arxiv.org/abs/2601.11747)
*Huaxiaoyue Wang,Sunav Choudhary,Franck Dernoncourt,Yu Shen,Stefano Petrangeli*

Main category: cs.AI

TL;DR: PRISM框架利用设计数据构建知识库，通过聚类、总结和检索实现基于自然语言指令的风格化设计改进，在Crello数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在图形设计中的风格知识过于通用，与特定领域数据不匹配，导致风格改进效果不佳。需要利用真实设计数据来学习设计师的原则和知识。

Method: 提出PRISM框架，包含三个阶段：1) 对高方差设计进行聚类以捕捉风格多样性；2) 将每个聚类总结为可操作的设计知识；3) 在推理时检索相关知识实现风格感知的改进。

Result: 在Crello数据集上，PRISM获得了1.49的平均排名（越接近1越好），在风格对齐方面优于基线方法。用户研究也证实设计师更偏好PRISM的结果。

Conclusion: 利用设计数据构建知识库能有效提升基于自然语言指令的风格化设计改进，PRISM框架在风格对齐和设计师偏好方面表现优异。

Abstract: Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas designers emphasize shape and color choices. Our key insight is to leverage design data -- a collection of real-world designs that implicitly capture designer's principles -- to learn design knowledge and guide stylistic improvement. We propose PRISM (PRior-Informed Stylistic Modification) that constructs and applies a design knowledge base through three stages: (1) clustering high-variance designs to capture diversity within a style, (2) summarizing each cluster into actionable design knowledge, and (3) retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.

</details>


### [31] [Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles](https://arxiv.org/abs/2601.11781)
*Dawood Wasif,Terrence J. Moore,Seunghyun Yoon,Hyuk Lim,Dan Dongseong Kim,Frederica F. Nelson,Jin-Hee Cho*

Main category: cs.AI

TL;DR: RAIL是一个风险感知的人机协同框架，通过融合多种运行时信号进行风险评分，实现自适应控制调整和针对性学习，在自动驾驶中应对罕见场景和网络物理入侵。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在遇到罕见的长尾场景或网络物理入侵时需要保持安全和有效性，现有方法在这些挑战性情况下的表现有限。

Method: RAIL融合三种线索（曲率执行完整性、碰撞时间接近度和观测偏移一致性）通过加权Noisy-OR生成入侵风险评分。根据风险阈值，将动作与特定线索的防护机制混合，同时保持人工干预能力。使用上下文老虎机基于线索向量仲裁防护选择，结合Soft Actor-Critic与风险优先回放和双重奖励机制。

Result: 在MetaDrive上，RAIL获得测试回报360.65、测试成功率0.85、测试安全违规0.75、干扰率0.0027，仅记录29.07个训练安全违规。在CAN注入和LiDAR欺骗攻击下，成功率提升至0.68和0.80，攻击下脱离率降至0.37和0.03，攻击成功率降至0.34和0.11。在CARLA中，仅用8000步获得测试回报1609.70和测试成功率0.41。

Conclusion: RAIL框架通过风险感知的人机协同机制，有效提升了自动驾驶系统在罕见场景和网络攻击下的安全性和性能，超越了现有强化学习、安全强化学习和人机协同基线方法。

Abstract: Autonomous vehicles must remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during driving. We present RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime signals into calibrated control adaptations and focused learning. RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) via a weighted Noisy-OR. When IRS exceeds a threshold, actions are blended with a cue-specific shield using a learned authority, while human override remains available; when risk is low, the nominal policy executes. A contextual bandit arbitrates among shields based on the cue vector, improving mitigation choices online. RAIL couples Soft Actor-Critic (SAC) with risk-prioritized replay and dual rewards so that takeovers and near misses steer learning while nominal behavior remains covered. On MetaDrive, RAIL achieves a Test Return (TR) of 360.65, a Test Success Rate (TSR) of 0.85, a Test Safety Violation (TSV) of 0.75, and a Disturbance Rate (DR) of 0.0027, while logging only 29.07 training safety violations, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under Controller Area Network (CAN) injection and LiDAR spoofing attacks, it improves Success Rate (SR) to 0.68 and 0.80, lowers the Disengagement Rate under Attack (DRA) to 0.37 and 0.03, and reduces the Attack Success Rate (ASR) to 0.34 and 0.11. In CARLA, RAIL attains a TR of 1609.70 and TSR of 0.41 with only 8000 steps.

</details>


### [32] [A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation](https://arxiv.org/abs/2601.11792)
*Yifei Sun,Yongan Li,A. K. Qin,Sicheng Hou,Tamas Pflanzner*

Main category: cs.AI

TL;DR: 本文提出创新数学题生成任务(IMPG)，通过自演化多角色协作框架与细粒度难度指导，显著提升生成题目的创新性同时保持高正确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学题生成任务中虽然正确率高，但缺乏创新性和区分度，需要开发能够生成创新性数学题目的新方法。

Method: 1) 构建包含采样器、生成器、评估器、状态机和记忆的多角色协作机制，通过自评估和外部反馈进行迭代优化；2) 引入改进的难度模型进行细粒度指导，采用DAPS算法增强采样编码的语义合理性；3) 构建HSM3K-CN数据集，采用CPT、SFT和GRPO多阶段训练流程；4) 通过蒸馏将专家模型的评估能力迁移到学徒模型，实现系统自演化。

Result: 实验表明，相比基线模型，该方法在保持高正确率的同时，显著提高了生成题目的创新性。

Conclusion: 提出的自演化多角色协作框架有效解决了创新数学题生成任务，平衡了正确性与创新性，为智能教育领域的数学题生成提供了新思路。

Abstract: Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory is constructed, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. Second, we introduce an improved difficulty model to quantify difficulty and provide fine-grained guidance. We adopt the data-driven association-guided path sampling (DAPS) algorithm to enhance the semantic rationality of sampled encodings. Third, we construct the HSM3K-CN dataset, which comprises high-quality high school math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that, compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.

</details>


### [33] [Multi-agent DRL-based Lane Change Decision Model for Cooperative Planning in Mixed Traffic](https://arxiv.org/abs/2601.11809)
*Zeyu Mu,Shangtong Zhang,B. Brian Park*

Main category: cs.AI

TL;DR: 提出混合多智能体换道决策模型，使用CNN-QMIX框架提升网联自动驾驶车辆在混合交通中的协同编队参与率


<details>
  <summary>Details</summary>
Motivation: 在CAV部署初期，网联自动驾驶车辆在人类驾驶车辆中分布稀疏，难以形成有效的协同编队，需要提升CAV参与协同编队的能力

Method: 采用QMIX框架结合卷积神经网络处理交通数据（CNN-QMIX），设计轨迹规划器和模型预测控制器确保安全平滑的换道执行

Result: 模型能有效处理动态变化的交通智能体数量，显著优于基于规则的基线模型，协同编队率提升达26.2%

Conclusion: 提出的混合多智能体换道决策模型能优化CAV部署初期的协同编队和交通动态，提升交通效率和能源效益

Abstract: Connected automated vehicles (CAVs) possess the ability to communicate and coordinate with one another, enabling cooperative platooning that enhances both energy efficiency and traffic flow. However, during the initial stage of CAV deployment, the sparse distribution of CAVs among human-driven vehicles reduces the likelihood of forming effective cooperative platoons. To address this challenge, this study proposes a hybrid multi-agent lane change decision model aimed at increasing CAV participation in cooperative platooning and maximizing its associated benefits. The proposed model employs the QMIX framework, integrating traffic data processed through a convolutional neural network (CNN-QMIX). This architecture addresses a critical issue in dynamic traffic scenarios by enabling CAVs to make optimal decisions irrespective of the varying number of CAVs present in mixed traffic. Additionally, a trajectory planner and a model predictive controller are designed to ensure smooth and safe lane-change execution. The proposed model is trained and evaluated within a microsimulation environment under varying CAV market penetration rates. The results demonstrate that the proposed model efficiently manages fluctuating traffic agent numbers, significantly outperforming the baseline rule-based models. Notably, it enhances cooperative platooning rates up to 26.2\%, showcasing its potential to optimize CAV cooperation and traffic dynamics during the early stage of deployment.

</details>


### [34] [POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation](https://arxiv.org/abs/2601.11816)
*Zahra Moslemi,Keerthi Koneru,Yen-Ting Lee,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: POLARIS是一个面向企业后台工作流的治理型LLM智能体编排框架，通过类型化计划合成和验证执行实现可审计、策略对齐的操作自动化。


<details>
  <summary>Details</summary>
Motivation: 企业后台工作流需要可审计、策略对齐且操作可预测的智能体系统，而通用的多智能体设置往往无法满足这些要求。

Method: POLARIS采用治理型编排框架，将自动化视为类型化计划合成和验证执行：规划器生成类型检查的有向无环图，基于规则的推理模块选择合规计划，执行过程通过验证器门控检查、有限修复循环和编译策略护栏来防护。

Result: 在文档中心财务任务中，POLARIS生成决策级工件和完整执行轨迹，减少人工干预。在SROIE数据集上获得0.81的微F1分数，在受控合成套件上实现0.95-1.00的异常路由精度，同时保持审计轨迹。

Conclusion: POLARIS为策略对齐的智能体AI提供了方法论和基准参考，建立了治理型智能体AI的初步基准。

Abstract: Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation

</details>


### [35] [AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept](https://arxiv.org/abs/2601.11825)
*Arya Rahgozar,Pouria Mortezaagha*

Main category: cs.AI

TL;DR: AI驱动的PICOS知识合成平台，通过自动化PICOS合规检测、研究设计分类、检索增强生成和主题建模，提高生物医学证据合成的可扩展性和透明度，减少研究浪费。


<details>
  <summary>Details</summary>
Motivation: 生物医学研究中存在研究浪费问题，包括冗余研究、不完整报告和传统证据合成工作流程的可扩展性有限。需要开发可扩展、透明的知识合成方法来减少研究浪费。

Method: 基于PICOS框架的AI协同科学家平台，整合关系存储、向量语义检索和Neo4j知识图谱。使用Bi-LSTM和基于PubMedBERT的transformer多任务分类器进行PICOS合规检测和研究设计分类。采用检索增强生成进行全文合成，BERTopic进行主题建模。

Result: transformer模型在研究设计分类上达到95.7%准确率，Bi-LSTM在PICOS合规检测上达到87%准确率。检索增强生成在需要结构化约束、跨研究整合和图推理的查询中表现优于非检索方法。主题建模揭示了大量主题冗余和未充分探索的研究领域。

Conclusion: PICOS感知和可解释的自然语言处理可以提高证据合成的可扩展性、透明度和效率。该架构是领域无关的，为减少生物医学学科的研究浪费提供了实用框架。

Abstract: Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on explicit formalization of Population, Intervention, Comparator, Outcome, and Study design (PICOS). The platform integrates relational storage, vector-based semantic retrieval, and a Neo4j knowledge graph. Evaluation was conducted on dementia-sport and non-communicable disease corpora. Automated PICOS compliance and study design classification from titles and abstracts were performed using a Bidirectional Long Short-Term Memory baseline and a transformer-based multi-task classifier fine-tuned from PubMedBERT. Full-text synthesis employed retrieval-augmented generation with hybrid vector and graph retrieval, while BERTopic was used to identify thematic structure, redundancy, and evidence gaps. The transformer model achieved 95.7% accuracy for study design classification with strong agreement against expert annotations, while the Bi-LSTM achieved 87% accuracy for PICOS compliance detection. Retrieval-augmented generation outperformed non-retrieval generation for queries requiring structured constraints, cross-study integration, and graph-based reasoning, whereas non-retrieval approaches remained competitive for high-level summaries. Topic modeling revealed substantial thematic redundancy and identified underexplored research areas. These results demonstrate that PICOS-aware and explainable natural language processing can improve the scalability, transparency, and efficiency of evidence synthesis. The proposed architecture is domain-agnostic and offers a practical framework for reducing research waste across biomedical disciplines.

</details>


### [36] [Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic](https://arxiv.org/abs/2601.11840)
*Hongyu Lin,Samer Abdallah,Makar Valentinov,Paul Brennan,Elijah Kagan,Christoph M. Wintersteiger,Denis Ignatovich,Grant Passmore*

Main category: cs.AI

TL;DR: CodeLogician是一个神经符号代理，结合LLM和形式化推理引擎ImandraX，用于精确分析软件逻辑。相比纯LLM方法，它能显著提升程序推理准确性（41-47个百分点）。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码理解任务上表现良好，但缺乏对程序行为进行精确、穷尽的数学推理能力。现有基准测试要么专注于与真实软件脱节的数学证明自动化，要么专注于不需要语义严谨性的工程任务。

Method: CodeLogician是一个神经符号代理，集成ImandraX工业自动化推理引擎。它使用LLM构建软件系统的显式形式化模型，然后利用自动化推理引擎回答丰富的语义问题，而不仅仅是二元验证结果。

Result: 引入code-logic-bench基准测试，评估程序状态空间、控制流、覆盖约束和边界情况的推理能力。相比纯LLM推理，CodeLogician的形式化增强显著提升了推理准确性，缩小了41-47个百分点的差距。

Conclusion: 神经符号集成对于扩展程序分析、实现严谨自主的软件理解至关重要。CodeLogician展示了结合LLM和形式化方法在软件逻辑精确分析方面的巨大潜力。

Abstract: Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.
  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.
  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.
  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.

</details>


### [37] [Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis and Human Interpretive Authority](https://arxiv.org/abs/2601.11850)
*Matthew Nyaaba,Min SungEun,Mary Abiswin Apam,Kwame Owoahene Acheampong,Emmanuel Dwamena,Xiaoming Zhai*

Main category: cs.AI

TL;DR: 研究探讨了生成式AI在质性研究中的应用，特别是ITA-GPT工具如何支持归纳式主题分析，并分析了人机协作中的解释权威问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在质性研究中的使用增加，需要探讨其对分析实践和解释权威的影响，理解人机协作如何影响归纳式主题分析过程。

Method: 基于HACITA框架，三位经验丰富的质性研究人员使用ITA-GPT工具分析加纳教师教育背景的访谈转录本。工具支持熟悉化、逐字编码、动名词描述编码和主题发展，同时确保文本完整性、覆盖检查和可审计性。数据包括交互日志、AI生成表格、研究人员修订、删除、插入、评论和反思备忘录。

Result: ITA-GPT作为程序性支架，结构化分析工作流程并增强透明度。但解释权威仍由人类研究人员掌握，他们通过修改、删除、拒绝、插入和评论等反复分析行动行使判断力。

Conclusion: 研究表明归纳式主题分析可以通过负责任的人机协作实现，AI工具提供结构支持，但最终解释权威和判断仍由人类研究人员保持。

Abstract: The increasing use of generative artificial intelligence (GenAI) in qualitative research raises important questions about analytic practice and interpretive authority. This study examines how researchers interact with an Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to support inductive thematic analysis through structured, semi-automated prompts aligned with reflexive thematic analysis and verbatim coding principles. Guided by a Human-Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA) framework, the study focuses on analytic process rather than substantive findings. Three experienced qualitative researchers conducted ITA-GPT assisted analyses of interview transcripts from education research in the Ghanaian teacher education context. The tool supported familiarization, verbatim in vivo coding, gerund-based descriptive coding, and theme development, while enforcing trace to text integrity, coverage checks, and auditability. Data sources included interaction logs, AI-generated tables, researcher revisions, deletions, insertions, comments, and reflexive memos. Findings show that ITA-GPT functioned as a procedural scaffold that structured analytic workflow and enhanced transparency. However, interpretive authority remained with human researchers, who exercised judgment through recurrent analytic actions including modification, deletion, rejection, insertion, and commenting. The study demonstrates how inductive thematic analysis is enacted through responsible human AI collaboration.

</details>


### [38] [MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment](https://arxiv.org/abs/2601.11885)
*Zhifei Li,Ziyue Qin,Xiangyu Luo,Xiaoju Hou,Yue Zhao,Miao Zhang,Zhifang Huang,Kui Xiao,Bing Yang*

Main category: cs.AI

TL;DR: MyGram：一种用于多模态实体对齐的模态感知图变换器，通过模态扩散学习和Gram损失实现跨模态全局分布一致性，在多个数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态实体对齐方法可能忽略模态内的结构上下文信息，容易受到浅层特征的干扰，需要更好的方法来捕获深度结构信息并实现细粒度多模态融合。

Method: 提出MyGram框架，包含：1）模态扩散学习模块，捕获模态内深度结构上下文信息并实现细粒度多模态融合；2）Gram损失函数，通过最小化多模态特征形成的4维平行多面体体积，实现跨模态全局分布一致性约束。

Result: 在五个公共数据集上的实验表明，MyGram显著优于基线模型，在FBDB15K上Hits@1最大提升4.8%，在FBYG15K上提升9.9%，在DBP15K上提升4.3%。

Conclusion: MyGram通过模态扩散学习和Gram损失有效解决了多模态实体对齐中模态内结构信息捕获不足和跨模态分布不一致的问题，取得了显著性能提升。

Abstract: Multi-modal entity alignment aims to identify equivalent entities between two multi-modal Knowledge graphs by integrating multi-modal data, such as images and text, to enrich the semantic representations of entities. However, existing methods may overlook the structural contextual information within each modality, making them vulnerable to interference from shallow features. To address these challenges, we propose MyGram, a modality-aware graph transformer with global distribution for multi-modal entity alignment. Specifically, we develop a modality diffusion learning module to capture deep structural contextual information within modalities and enable fine-grained multi-modal fusion. In addition, we introduce a Gram Loss that acts as a regularization constraint by minimizing the volume of a 4-dimensional parallelotope formed by multi-modal features, thereby achieving global distribution consistency across modalities. We conduct experiments on five public datasets. Results show that MyGram outperforms baseline models, achieving a maximum improvement of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.

</details>


### [39] [AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems](https://arxiv.org/abs/2601.11903)
*YenTing Lee,Keerthi Koneru,Zahra Moslemi,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: AEMA是一个面向企业级多智能体系统的评估框架，通过多步骤、可审计的评估流程，相比单一LLM-as-a-Judge方法提供更稳定、可追溯且支持人类监督的自动化评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统评估存在局限性：现有方法多为单响应评分或狭窄基准测试，在企业级多智能体规模下缺乏稳定性、可扩展性和自动化能力，难以实现可靠协调、透明决策和可验证性能。

Method: 提出AEMA（Adaptive Evaluation Multi-Agent）框架，这是一个过程感知、可审计的评估系统，能够在人类监督下规划、执行和聚合异构智能体工作流的多步骤评估，支持可追溯记录和负责任自动化。

Result: 在模拟真实业务场景的企业级智能体工作流测试中，AEMA相比单一LLM-as-a-Judge方法展现出更高的稳定性、更好的人类对齐性，并提供可追溯记录，支持负责任的自动化评估。

Conclusion: AEMA为LLM多智能体系统评估提供了透明、可复现的路径，支持企业级应用中的可信评估，通过过程感知和可审计设计实现了负责任的人工智能评估。

Abstract: Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.
  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight

</details>


### [40] [LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning](https://arxiv.org/abs/2601.11905)
*Junyu Cao,Ruijiang Gao,Esmaeil Keyvanshokooh,Jianhao Ma*

Main category: cs.AI

TL;DR: 提出一个统一框架，将算法追索、上下文老虎机和大型语言模型结合，用于高风险顺序决策（如个性化医疗）。提出追索老虎机问题和GLRB算法，以及LIBRA算法，后者结合LLM领域知识和老虎机统计严谨性，提供三个关键保证。


<details>
  <summary>Details</summary>
Motivation: 在高风险顺序决策场景（如个性化医疗）中，需要同时选择治疗行动和可行的患者特征修改。现有方法缺乏将算法追索、上下文老虎机和LLM知识有效结合的框架，难以平衡领域知识和统计学习。

Method: 1. 提出追索老虎机问题，决策者需同时选择治疗行动和可行的最小患者特征修改。2. 开发GLRB算法解决该问题。3. 提出LIBRA算法，策略性地结合LLM领域知识和老虎机学习统计严谨性，提供三个理论保证。

Result: 1. 理论保证：LIBRA提供热启动保证（LLM推荐接近最优时显著减少初始遗憾）、LLM努力保证（仅咨询LLM O(log²T)次确保长期自主性）、鲁棒性保证（即使LLM不可靠也不比纯老虎机算法差）。2. 建立匹配下界证明算法接近最优。3. 实验验证：在合成环境和真实高血压管理案例中，GLRB和LIBRA在遗憾、治疗质量和样本效率上优于标准上下文老虎机和纯LLM基准。

Conclusion: 追索感知的LLM辅助老虎机算法在个性化高风险决策中具有前景，为LLM-老虎机可信协作提供了有希望的方向，平衡了领域知识和统计学习的需求。

Abstract: We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.

</details>


### [41] [Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart](https://arxiv.org/abs/2601.11940)
*Kang Chen,Fan Yu,Junjie Nian,Shihan Zhao,Zhuoka Feng,Zijun Yao,Heng Wang,Minshen Yu,Yixin Cao*

Main category: cs.AI

TL;DR: TAAR框架通过检测思维陷阱并自适应重启解码，提升大模型在复杂推理任务中的表现，无需微调基础模型参数。


<details>
  <summary>Details</summary>
Motivation: 长思维链虽然能增强推理能力，但模型一旦早期犯错，会陷入"思维陷阱"——即使后续反思、尝试或验证也无法修正根本错误，导致生成自洽但错误的答案。

Method: 提出TAAR（Trap-Aware Adaptive Restart）框架：训练诊断策略从部分轨迹中预测两个信号：陷阱位置索引（截断点）和逃脱概率（干预强度）。推理时根据预测截断轨迹并自适应重启解码；对严重陷阱情况应用更强扰动，包括高温重采样和可选的结构化重启后缀。

Result: 在AIME24、AIME25、GPQA-Diamond、HMMT25、BRUMO25等数学和科学推理基准测试中，TAAR显著提升了推理性能，且无需微调基础模型参数。

Conclusion: TAAR通过检测和规避思维陷阱，有效解决了长思维链推理中的错误累积问题，为提升大模型推理可靠性提供了实用的测试时控制框架。

Abstract: Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root error. On a curated subset of DAPO-MATH, 89\% of failures exhibit such traps. To solve this problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict two signals from partial trajectories: a trap index for where to truncate and an escape probability for whether and how strongly to intervene. At inference time, TAAR truncates the trajectory before the predicted trap segment and adaptively restarts decoding; for severely trapped cases, it applies stronger perturbations, including higher-temperature resampling and an optional structured reboot suffix. Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves reasoning performance without fine-tuning base model parameters.

</details>


### [42] [Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement](https://arxiv.org/abs/2601.11974)
*Xinmeng Hou,Peiliang Gong,Bohao Qu,Wuqi Wang,Qing Guo,Yang Liu*

Main category: cs.AI

TL;DR: MARS框架通过单次递归实现LLM代理高效自我进化，结合原则性反思和程序性反思优化推理逻辑，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理受限于静态人工设计的提示词，缺乏适应性。现有自我改进框架依赖低效的多轮递归循环，计算成本高，需要更高效的自我进化方法

Method: 提出MARS框架，受教育心理学启发，模拟人类学习过程，结合原则性反思（抽象规范规则避免错误）和程序性反思（推导逐步成功策略），在单次递归周期内合成优化指令

Result: 在六个基准测试上的广泛实验表明，MARS优于最先进的自我进化系统，同时显著降低计算开销

Conclusion: MARS框架实现了LLM代理的高效自我进化，通过单次递归循环结合双重反思机制，在保持性能优势的同时大幅减少计算成本

Abstract: While Large Language Models (LLMs) enable complex autonomous behavior, current agents remain constrained by static, human-designed prompts that limit adaptability. Existing self-improving frameworks attempt to bridge this gap but typically rely on inefficient, multi-turn recursive loops that incur high computational costs. To address this, we propose Metacognitive Agent Reflective Self-improvement (MARS), a framework that achieves efficient self-evolution within a single recurrence cycle. Inspired by educational psychology, MARS mimics human learning by integrating principle-based reflection (abstracting normative rules to avoid errors) and procedural reflection (deriving step-by-step strategies for success). By synthesizing these insights into optimized instructions, MARS allows agents to systematically refine their reasoning logic without continuous online feedback. Extensive experiments on six benchmarks demonstrate that MARS outperforms state-of-the-art self-evolving systems while significantly reducing computational overhead.

</details>


### [43] [Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion](https://arxiv.org/abs/2601.11979)
*Ang Gao,Changshuo Zhang,Xiao Zhang,Deyang Li,Minjun Zhao,Fangchao Liu,Xinyu Zhang*

Main category: cs.AI

TL;DR: PICL提出动态演示集成框架，通过实时识别推理过程中的困惑点并插入相关演示，提升数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习方法在数学推理等需要逐步逻辑推导的任务中存在局限，其静态演示无法适应推理过程中动态出现的困惑点（如模糊计算、逻辑漏洞），导致级联错误。

Method: PICL采用两阶段框架：1）通过分析推理过程中的语义和熵识别潜在困惑点并总结核心特征；2）在遇到困惑点时从演示池检索匹配上下文的相关演示，直接插入到正在进行的推理过程中指导后续步骤。

Result: 实验表明PICL优于基线方法，通过缓解推理过程中的困惑点提高了数学推理的准确性。

Conclusion: PICL证明了自适应演示插入在复杂数学推理中的价值，动态演示集成能够有效提升上下文学习在需要逐步逻辑推导任务中的性能。

Abstract: In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often arise during multi-step reasoning such as ambiguous calculations or logical gaps. These unresolved confusion points can lead to cascading errors that degrade final accuracy. To tackle this issue, we propose Process In-Context Learning (PICL), a dynamic demonstration integration framework designed to boost mathematical reasoning by responding to real-time inference needs. PICL operates in two stages: 1)~it identifies potential confusion points by analyzing semantics and entropy in the reasoning process and summarizes their core characteristics; 2)~upon encountering these points, it retrieves relevant demonstrations from the demonstration pool that match the confusion context and inserts them directly into the ongoing reasoning process to guide subsequent steps. Experiments show that PICL outperforms baseline methods by mitigating mid-inference confusion, highlighting the value of adaptive demonstration insertion in complex mathematical reasoning.

</details>


### [44] [Are LLMs Ready for TOON? Benchmarking Structural Correctness-Sustainability Trade-offs in Novel Structured Output Formats](https://arxiv.org/abs/2601.12014)
*Elio Masciari,Vincenzo Moscato,Enea Vincenzo Napolitano,Gian Marco Orlando,Marco Perillo,Diego Russo*

Main category: cs.AI

TL;DR: 论文提出一个可持续性评估框架，用于评估LLM结构化输出的环境效率，引入GCS_env指标综合结构正确性和碳排放效率，比较TOON格式与传统格式的优劣。


<details>
  <summary>Details</summary>
Motivation: 当前LLM结构化输出评估主要关注结构正确性，忽略了不同输出格式对环境（碳排放）的影响。需要将环境效率纳入结构化输出的评估框架中。

Method: 提出可持续性评估框架，测量token使用量、生成时间和估算碳排放。引入环境感知生成正确性分数(GCS_env)，统一结构正确性和碳感知效率。系统比较TOON格式与JSON、XML、YAML等传统格式。

Result: TOON格式产生更紧凑的输出和更低的排放，但结构正确性较低（尤其当模型缺乏原生支持时）。模型容量增加可缩小差距，环境感知评分可根据部署优先级改变格式排名。

Conclusion: 需要将可持续性纳入结构化输出基准测试，紧凑表示如TOON在大规模、碳意识LLM部署中具有实际优势。

Abstract: Large Language Models (LLMs) are increasingly required to generate structured, machine-readable outputs for downstream systems. While recent benchmarks have focused on evaluating the structural correctness of such outputs, the environmental impact of inference for different output formats has largely been overlooked. In this paper, we argue that structured output formats should be assessed not only in terms of correctness, but also with respect to their environmental efficiency. To this end, we introduce a sustainability-aware evaluation framework for structured generation that measures token usage, generation time, and estimated carbon emissions. Within this framework, we propose the Environment-Aware Generation Correctness Score (GCS_env), a unified metric that integrates structural correctness with carbon-aware efficiency. Using this framework, we systematically benchmark the novel TOON format against established representations (JSON, XML, YAML) across multiple LLMs spanning different architectures and parameter scales.
  Our results reveal a consistent trade-off: TOON yields markedly more compact outputs and lower emissions, but lower structural correctness when models lack native support. We show that increased model capacity reduces this gap and that environment-aware scoring can shift format rankings depending on deployment priorities. highlighting the need for sustainability-inclusive benchmarking and provides empirical evidence that compact representations such as TOON can offer practical advantages in large-scale, carbon-conscious LLM deployments.

</details>


### [45] [A Multi-Agent System for Generating Actionable Business Advice](https://arxiv.org/abs/2601.12024)
*Kartikey Singh Bhandari,Tanish Jain,Archit Agrawal,Dhruv Kumar,Praveen Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: 提出一个基于LLM的多智能体框架，将大规模客户评论转化为可执行的商业建议，通过聚类、生成、迭代评估和可行性排序等组件提升建议的质量和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有分析方法（如情感分析、方面提取）停留在描述性任务层面，而LLM生成的建议往往缺乏准确性和深度推理。客户评论蕴含丰富的产品弱点信号和未满足的用户需求，需要更有效的工具将其转化为可执行的商业建议。

Method: 提出多智能体LLM框架，包含四个组件：1）聚类选择代表性评论；2）生成建议；3）迭代评估；4）基于可行性的排序。该设计将语料库蒸馏与反馈驱动的建议精炼相结合。

Result: 在三个服务领域和多个模型系列的实验中，该框架在可操作性、特异性和非冗余性方面持续优于单模型基线，中等规模模型的性能接近大型模型框架。

Conclusion: 该多智能体框架能够将大规模评论语料转化为具体、可操作且实用的商业建议，为决策支持提供了有效的解决方案。

Abstract: Customer reviews contain rich signals about product weaknesses and unmet user needs, yet existing analytic methods rarely move beyond descriptive tasks such as sentiment analysis or aspect extraction. While large language models (LLMs) can generate free-form suggestions, their outputs often lack accuracy and depth of reasoning. In this paper, we present a multi-agent, LLM-based framework for prescriptive decision support, which transforms large scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, generation of advices, iterative evaluation, and feasibility based ranking. This design couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical. Experiments across three service domains and multiple model families show that our framework consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks.

</details>


### [46] [ARC: Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents](https://arxiv.org/abs/2601.12030)
*Yilun Yao,Shan Huang,Elsie Dai,Zhewen Tan,Zhenyu Duan,Shousheng Jia,Yanbing Jiang,Tong Yang*

Main category: cs.AI

TL;DR: ARC框架将上下文管理重新定义为主动的、反思驱动的过程，通过监控和修订来动态重组工作上下文，解决LLM在长时信息搜索中的上下文退化问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为研究代理进行深度搜索和长时信息寻求时，随着交互历史增长性能会下降（上下文退化）。现有方法主要通过原始积累或被动摘要管理上下文，将其视为静态产物，导致早期错误或不当强调持续存在。

Method: 提出ARC框架，将上下文管理系统性地制定为主动的、反思驱动的过程，将上下文视为执行过程中的动态内部推理状态。通过反思驱动的监控和修订操作化，允许代理在检测到错位或退化时主动重组工作上下文。

Result: 在具有挑战性的长时信息寻求基准测试中，ARC始终优于被动上下文压缩方法，在BrowseComp-ZH基准上使用Qwen2.5-32B-Instruct实现了高达11%的绝对准确率提升。

Conclusion: 将上下文管理视为动态内部推理状态而非静态产物的主动反思驱动方法，能有效解决长时信息搜索中的上下文退化问题，显著提升LLM研究代理的性能。

Abstract: Large language models are increasingly deployed as research agents for deep search and long-horizon information seeking, yet their performance often degrades as interaction histories grow. This degradation, known as context rot, reflects a failure to maintain coherent and task-relevant internal states over extended reasoning horizons. Existing approaches primarily manage context through raw accumulation or passive summarization, treating it as a static artifact and allowing early errors or misplaced emphasis to persist. Motivated by this perspective, we propose ARC, which is the first framework to systematically formulate context management as an active, reflection-driven process that treats context as a dynamic internal reasoning state during execution. ARC operationalizes this view through reflection-driven monitoring and revision, allowing agents to actively reorganize their working context when misalignment or degradation is detected. Experiments on challenging long-horizon information-seeking benchmarks show that ARC consistently outperforms passive context compression methods, achieving up to an 11% absolute improvement in accuracy on BrowseComp-ZH with Qwen2.5-32B-Instruct.

</details>


### [47] [Abstract Argumentation with Subargument Relations](https://arxiv.org/abs/2601.12038)
*Beishui Liao*

Main category: cs.AI

TL;DR: 该论文提出在抽象论辩框架中引入显式的子论证关系，作为与攻击关系并列的基本关系，以更好地捕捉结构化论辩中的依赖关系。


<details>
  <summary>Details</summary>
Motivation: Dung的抽象论辩框架仅通过攻击关系来刻画论证可接受性，这虽然实现了丰富的理论结果，但限制了表示结构化论辩中核心的结构依赖关系（特别是子论证关系）的能力。现有的扩展（如双极论辩框架）引入了支持关系，但未能捕捉子论证的非对称性和构成性本质，以及它们与攻击的交互。

Method: 研究在抽象论辩框架中丰富显式的子论证关系，将其与攻击关系一起作为基本关系处理。分析子论证关系如何与攻击关系交互，并考察它们对基本语义属性的影响。

Result: 该框架提供了结构信息的原理性抽象，并阐明了子论证在抽象可接受性推理中的作用。

Conclusion: 通过引入显式子论证关系，该研究为抽象论辩框架提供了更丰富的表达能力，能够更好地捕捉结构化论辩中的关键依赖关系，同时保持了抽象框架的理论优势。

Abstract: Dung's abstract argumentation framework characterises argument acceptability solely via an attack relation, deliberately abstracting from the internal structure of arguments. While this level of abstraction has enabled a rich body of results, it limits the ability to represent structural dependencies that are central in many structured argumentation formalisms, in particular subargument relations. Existing extensions, including bipolar argumentation frameworks, introduce support relations, but these do not capture the asymmetric and constitutive nature of subarguments or their interaction with attacks. In this paper, we study abstract argumentation frameworks enriched with an explicit subargument relation, treated alongside attack as a basic relation. We analyse how subargument relations interact with attacks and examine their impact on fundamental semantic properties. This framework provides a principled abstraction of structural information and clarifies the role of subarguments in abstract acceptability reasoning.

</details>


### [48] [Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty](https://arxiv.org/abs/2601.12040)
*Murilo da Luz,Bruno Brandão,Luana Martins,Gustavo Oliveira,Bryan de Oliveira,Luckeciano Melo,Telma Soares*

Main category: cs.AI

TL;DR: PREGU：一种基于不确定性的部分推理引导方法，通过监控输出分布的熵来触发局部搜索，提升LLMs在数学和逻辑推理任务中的表现


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在推理和规划任务上取得了显著进展，但在多步推理场景（特别是数学和逻辑推理）中仍存在局限性，需要更有效的推理引导机制

Method: 提出PREGU方法：在自回归生成过程中监控输出分布的熵，当熵超过设定阈值时停止生成（表示不确定性），然后在潜在空间进行局部搜索，使用Soft Reasoning方法精炼部分推理并选择最一致的答案

Result: 在LLaMA-3-8B、Mistral-7B和Qwen2-7B模型上，在GSM8K、GSM-Hard、SVAMP和StrategyQA四个推理基准测试中，性能达到或超过Soft Reasoning方法

Conclusion: 熵可以作为推理过程中触发选择性精炼的有效信号，PREGU方法通过基于不确定性的部分推理引导，能够有效提升LLMs在多步推理任务中的表现

Abstract: The use of Large Language Models (LLMs) for reasoning and planning tasks has drawn increasing attention in Artificial Intelligence research. Despite their remarkable progress, these models still exhibit limitations in multi-step inference scenarios, particularly in mathematical and logical reasoning. We introduce PREGU (Partial Reasoning Guided by Uncertainty). PREGU monitors the entropy of the output distribution during autoregressive generation and halts the process whenever entropy exceeds a defined threshold, signaling uncertainty. From that point, a localized search is performed in the latent space to refine the partial reasoning and select the most coherent answer, using the Soft Reasoning method. Experiments conducted with LLaMA-3-8B, Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard, SVAMP, and StrategyQA) showed performance greater than or similar to Soft Reasoning, indicating that entropy can serve as an effective signal to trigger selective refinement during reasoning.

</details>


### [49] [Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA Systems Using Deep Reinforcement Learning](https://arxiv.org/abs/2601.12242)
*WooSeok Kim,Jeonghoon Lee,Sangho Kim,Taesun An,WonMin Lee,Dowon Kim,Kyungseop Shin*

Main category: cs.AI

TL;DR: 本文提出了一种结合回放记忆的深度强化学习框架，用于解决NOMA系统中的信道分配问题，通过广泛的仿真评估不同参数对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着物联网的扩展导致网络资源稀缺，需要优化网络资源利用。NOMA系统通过功率复用允许多用户同时接入网络，但存在信道分配问题需要进一步研究。

Method: 提出了一种结合回放记忆的深度强化学习框架，采用on-policy算法，在NOMA系统中分配网络资源以实现泛化学习。

Result: 通过广泛的仿真评估了学习率、批量大小、模型类型和状态特征数量等因素对性能的影响。

Conclusion: 提出的深度强化学习框架能够有效解决NOMA系统中的信道分配问题，为网络资源优化提供了新的解决方案。

Abstract: In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as a promising candidate for multiple access frameworks due to the evolution of deep machine learning, trying to incorporate deep machine learning into the NOMA system. The main motivation for such active studies is the growing need to optimize the utilization of network resources as the expansion of the internet of things (IoT) caused a scarcity of network resources. The NOMA addresses this need by power multiplexing, allowing multiple users to access the network simultaneously. Nevertheless, the NOMA system has few limitations. Several works have proposed to mitigate this, including the optimization of power allocation known as joint resource allocation(JRA) method, and integration of the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the channel assignment problem remains unclear and requires further investigation. In this paper, we propose a deep reinforcement learning framework incorporating replay memory with an on-policy algorithm, allocating network resources in a NOMA system to generalize the learning. Also, we provide extensive simulations to evaluate the effects of varying the learning rate, batch size, type of model, and the number of features in the state.

</details>


### [50] [UniMo: Unified Motion Generation and Understanding with Chain of Thought](https://arxiv.org/abs/2601.12126)
*Guocun Wang,Kenkun Liu,Jing Lin,Guorui Song,Jian Li,Xiaoguang Han*

Main category: cs.AI

TL;DR: UniMo框架通过监督微调和强化学习，将运动-语言信息与可解释思维链推理集成到LLM中，显著提升了3D人体运动生成与理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体运动生成与理解方法可解释性有限，限制了这两个相关任务之间的相互增强。基于LLM的统一框架存在语义对齐和任务连贯性挑战，且LLM的下一个token预测范式不适合运动序列，导致累积预测误差。

Method: 提出UniMo框架：1) 通过监督微调将运动-语言信息和可解释思维链推理集成到LLM中；2) 引入基于Group Relative Policy Optimization的强化学习作为后训练策略，通过优化token组来强制结构正确性和语义对齐，减轻运动token预测中的累积误差。

Result: 大量实验表明，UniMo显著优于现有的统一和任务特定模型，在运动生成和理解方面都实现了最先进的性能。

Conclusion: UniMo通过集成运动-语言信息、可解释思维链推理和强化学习优化，有效解决了现有方法的局限性，为3D人体运动生成与理解提供了更强大、更可解释的统一框架。

Abstract: Existing 3D human motion generation and understanding methods often exhibit limited interpretability, restricting effective mutual enhancement between these inherently related tasks. While current unified frameworks based on large language models (LLMs) leverage linguistic priors, they frequently encounter challenges in semantic alignment and task coherence. Moreover, the next-token prediction paradigm in LLMs is ill-suited for motion sequences, causing cumulative prediction errors. To address these limitations, we propose UniMo, a novel framework that integrates motion-language information and interpretable chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT). We further introduce reinforcement learning with Group Relative Policy Optimization (GRPO) as a post-training strategy that optimizes over groups of tokens to enforce structural correctness and semantic alignment, mitigating cumulative errors in motion token prediction. Extensive experiments demonstrate that UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding.

</details>


### [51] [Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks](https://arxiv.org/abs/2601.12744)
*Tasnim Ahmed,Yifan Zhu,Salimur Choudhury*

Main category: cs.AI

TL;DR: 本文提出了IntentOpt基准测试，评估视觉语言模型将网络拓扑图转化为优化代码的能力，发现视觉参数提取会降低执行成功率12-21个百分点，开源模型表现远落后于闭源模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于意图的网络系统需要操作员用文字描述拓扑和参数，但网络工程师通常通过图表进行推理。视觉语言模型能否处理带注释的网络草图并生成正确的优化代码尚未探索。

Method: 创建了包含85个优化问题的IntentOpt基准测试，涵盖17个类别。评估了4个VLM模型在三种提示策略下对多模态与纯文本输入的表现，并通过案例研究在实际网络测试床中部署生成的代码。

Result: 视觉参数提取使执行成功率降低12-21个百分点（GPT-5-Mini从93%降至72%）。思维程序提示使性能下降最多13个百分点。开源模型表现落后，Llama-3.2-11B-Vision仅18%成功率，而GPT-5-Mini达到75%。

Conclusion: 研究建立了当前VLM在基于意图的网络系统中生成优化代码的基准能力和局限性，展示了通过模型上下文协议在实际网络基础设施中部署VLM生成代码的可行性。

Abstract: Intent-Based Networking (IBN) allows operators to specify high-level network goals rather than low-level configurations. While recent work demonstrates that large language models can automate configuration tasks, a distinct class of intents requires generating optimization code to compute provably optimal solutions for traffic engineering, routing, and resource allocation. Current systems assume text-based intent expression, requiring operators to enumerate topologies and parameters in prose. Network practitioners naturally reason about structure through diagrams, yet whether Vision-Language Models (VLMs) can process annotated network sketches into correct optimization code remains unexplored. We present IntentOpt, a benchmark of 85 optimization problems across 17 categories, evaluating four VLMs (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) under three prompting strategies on multimodal versus text-only inputs. Our evaluation shows that visual parameter extraction reduces execution success by 12-21 percentage points (pp), with GPT-5-Mini dropping from 93% to 72%. Program-of-thought prompting decreases performance by up to 13 pp, and open-source models lag behind closed-source ones, with Llama-3.2-11B-Vision reaching 18% compared to 75% for GPT-5-Mini. These results establish baseline capabilities and limitations of current VLMs for optimization code generation within an IBN system. We also demonstrate practical feasibility through a case study that deploys VLM-generated code to network testbed infrastructure using Model Context Protocol.

</details>


### [52] [DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants](https://arxiv.org/abs/2601.12138)
*Abhishek Kumar,Riya Tapwal,Carsten Maple*

Main category: cs.AI

TL;DR: DriveSafe：针对LLM驾驶助手的分层四级风险分类法，包含129个细粒度风险类别，评估显示现有模型在驾驶场景中安全对齐不足


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地集成到车载数字助手中，但不安全、模糊或法律错误的响应可能导致严重的安全、道德和监管后果。现有的安全分类和评估框架大多是通用的，未能捕捉真实驾驶场景中的领域特定风险。

Method: 提出了DriveSafe，一个分层四级风险分类法，包含129个细粒度原子风险类别，涵盖技术、法律、社会和伦理维度，基于真实驾驶法规和安全原则，并由领域专家审查。通过评估六个广泛部署的LLM的拒绝行为来验证构建提示的安全相关性和现实性。

Result: 评估的模型经常无法适当拒绝不安全或不合规的驾驶相关查询，突显了通用安全对齐在驾驶环境中的局限性。

Conclusion: 需要针对驾驶领域的特定安全评估框架，现有LLM在驾驶场景中的安全对齐存在不足，DriveSafe分类法为系统评估LLM驾驶助手的安全风险提供了基础。

Abstract: Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.

</details>


### [53] [TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals](https://arxiv.org/abs/2601.12141)
*Yuliia Suprun,Khen Elimelech,Lydia E. Kavraki,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: TIDE是一种用于具有时间扩展目标的任务规划新方法，通过将时间问题分解为可管理的子问题，并使用成本驱动启发式引导搜索，提高了规划效率。


<details>
  <summary>Details</summary>
Motivation: 传统LTLf任务规划方法通常将时间规划问题转化为经典规划问题，但缺乏针对时间目标的启发式引导搜索，导致效率不高。

Method: TIDE将时间问题分解为一系列可管理的reach-avoid子问题，使用成本驱动启发式识别和优先处理有希望的自动机轨迹，并采用自适应回溯机制从失败计划中恢复。

Result: 实验结果表明TIDE取得了有前景的性能表现，成为处理时间扩展目标规划方法组合中的有价值补充。

Conclusion: TIDE通过分解时间问题、使用启发式引导搜索和自适应回溯机制，有效解决了传统LTLf任务规划中缺乏引导搜索的问题，提高了规划效率和完整性。

Abstract: Task planning with temporally extended goals (TEGs) is a critical challenge in AI and robotics, enabling agents to achieve complex sequences of objectives over time rather than addressing isolated, immediate tasks. Linear Temporal Logic on finite traces (LTLf ) provides a robust formalism for encoding these temporal goals. Traditional LTLf task planning approaches often transform the temporal planning problem into a classical planning problem with reachability goals, which are then solved using off-the-shelf planners. However, these methods often lack informed heuristics to provide a guided search for temporal goals. We introduce TIDE (Trace-Informed Depth-first Exploration), a novel approach that addresses this limitation by decomposing a temporal problem into a sequence of smaller, manageable reach-avoid sub-problems, each solvable using an off-the-shelf planner. TIDE identifies and prioritizes promising automaton traces within the domain graph, using cost-driven heuristics to guide exploration. Its adaptive backtracking mechanism systematically recovers from failed plans by recalculating costs and penalizing infeasible transitions, ensuring completeness and efficiency. Experimental results demonstrate that TIDE achieves promising performance and is a valuable addition to the portfolio of planning methods for temporally extended goals.

</details>


### [54] [Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration](https://arxiv.org/abs/2601.12256)
*Jinyoung Park,Minseong Bae,Jeehye Na,Hyunwoo J. Kim*

Main category: cs.AI

TL;DR: CoLLaMo是一个基于大语言模型的分子助手，通过多级分子模态协作投影器整合1D序列、2D分子图和3D构象信息，解决了现有大型分子语言模型的幻觉和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型分子语言模型（LMLMs）存在幻觉和鲁棒性有限的问题，主要原因是未能充分整合1D序列、2D分子图和3D构象等多种分子模态信息。

Method: 提出CoLLaMo模型，配备多级分子模态协作投影器，采用关系感知的模态协作注意力机制，实现原子间细粒度和关系引导的信息交换，整合2D结构和3D空间关系。

Result: CoLLaMo在多个任务上取得最佳性能，包括分子描述、计算性质QA、描述性质QA、基序计数和IUPAC名称预测，增强了分子模态泛化能力。

Conclusion: 通过整合多种分子模态信息和引入新的分子中心自动评估指标，CoLLaMo有效解决了LMLMs的幻觉问题，提升了分子理解能力。

Abstract: Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.

</details>


### [55] [FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains](https://arxiv.org/abs/2601.12259)
*Jiashuo Liu,Siyuan Chen,Zaiyuan Wang,Zhiyuan Zeng,Jiacheng Guo,Liang Hu,Lingyue Yin,Suozhi Huang,Wenxin Hao,Yang Yang,Zerui Cheng,Zixin Yao,Lingyue Yin,Haoxin Liu,Jiayi Cheng,Yuzhen Li,Zezhong Ma,Bingjie Wang,Bingsen Qiu,Xiao Liu,Zeyang Zhang,Zijian Liu,Jinpeng Wang,Mingren Yin,Tianci He,Yali Liao,Yixiao Tian,Zhenwei Zhu,Anqi Dai,Ge Zhang,Jingkai Liu,Kaiyuan Zhang,Wenlong Wu,Xiang Gao,Xinjie Chen,Zhixin Yao,Zhoufutu Wen,B. Aditya Prakash,Jose Blanchet,Mengdi Wang,Nian Si,Wenhao Huang*

Main category: cs.AI

TL;DR: FutureX-Pro扩展了FutureX的实时预测基准，针对金融、零售、公共卫生和自然灾害四个高价值垂直领域，评估当前SOTA智能体LLM在工业部署中的领域基础能力，发现通用推理与垂直应用精度之间存在性能差距。


<details>
  <summary>Details</summary>
Motivation: 虽然通用智能体在开放领域搜索中表现出色，但在资本密集型和安全关键领域的可靠性尚未充分探索。需要评估智能体LLM是否具备工业部署所需的领域基础能力。

Method: 基于FutureX的无污染实时评估流程，构建了FutureX-Pro框架，包括金融、零售、公共卫生和自然灾害四个垂直领域。在入门级但基础的预测任务上对智能体LLM进行基准测试，涵盖市场指标预测、供应链需求预测、流行病趋势跟踪和自然灾害跟踪等任务。

Result: 评估结果显示，当前最先进的智能体LLM在通用推理能力与高价值垂直应用所需的精度之间存在性能差距。

Conclusion: 智能体未来预测需要针对特定垂直领域进行专门化，通用智能体在资本密集型和安全关键领域的工业部署仍面临挑战，需要更强的领域基础能力。

Abstract: Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.

</details>


### [56] [Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding](https://arxiv.org/abs/2601.12260)
*Yihao Ding,Qiang Sun,Puzhen Wu,Sirui Li,Siwen Luo,Wei Liu*

Main category: cs.AI

TL;DR: Docs2Synth是一个无需人工标注的合成监督框架，通过检索引导推理解决受监管领域文档理解中的幻觉和领域适应问题。


<details>
  <summary>Details</summary>
Motivation: 受监管领域的文档理解面临两大挑战：1）缺乏人工标注用于模型适应；2）预训练模型难以跟上领域特定知识的更新。MLLMs存在幻觉问题，而VLPMs需要昂贵的标注。

Method: Docs2Synth自动处理原始文档集合，通过基于代理的系统生成和验证多样化的QA对，训练轻量级视觉检索器提取领域相关证据。推理时，检索器与MLLM通过迭代检索-生成循环协作。

Result: 在多个VRDU基准测试中，Docs2Synth显著增强了基础性和领域泛化能力，且无需人工标注。

Conclusion: Docs2Synth提供了一个易于使用的Python包，支持即插即用部署，有效解决了受监管领域文档理解中的幻觉和领域适应问题。

Abstract: Document understanding (VRDU) in regulated domains is particularly challenging, since scanned documents often contain sensitive, evolving, and domain specific knowledge. This leads to two major challenges: the lack of manual annotations for model adaptation and the difficulty for pretrained models to stay up-to-date with domain-specific facts. While Multimodal Large Language Models (MLLMs) show strong zero-shot abilities, they still suffer from hallucination and limited domain grounding. In contrast, discriminative Vision-Language Pre-trained Models (VLPMs) provide reliable grounding but require costly annotations to cover new domains. We introduce Docs2Synth, a synthetic-supervision framework that enables retrieval-guided inference for private and low-resource domains. Docs2Synth automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with an MLLM through an iterative retrieval--generation loop, reducing hallucination and improving response consistency. We further deliver Docs2Synth as an easy-to-use Python package, enabling plug-and-play deployment across diverse real-world scenarios. Experiments on multiple VRDU benchmarks show that Docs2Synth substantially enhances grounding and domain generalization without requiring human annotations.

</details>


### [57] [ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents](https://arxiv.org/abs/2601.12294)
*Dawei Li,Yuguang Yao,Zhen Tan,Huan Liu,Ruocheng Guo*

Main category: cs.AI

TL;DR: 提出了ToolPRMBench，一个专门评估工具使用代理中过程奖励模型（PRMs）的大规模基准测试，通过离线和在线采样方法测试单步和多步错误，并利用多LLM验证确保数据质量。


<details>
  <summary>Details</summary>
Motivation: 虽然奖励引导的搜索方法在工具使用代理中显示出潜力，但缺乏系统可靠的PRMs评估基准。现有研究缺少专门针对工具使用场景的PRM评估标准。

Method: 基于多个代表性工具使用基准构建ToolPRMBench，将代理轨迹转换为步级测试用例。采用离线采样隔离单步错误，在线采样捕获多步失败。使用多LLM验证管道减少标签噪声。

Result: 实验结果显示不同PRMs在工具使用场景中效果存在明显差异，专门针对工具使用的PRMs显示出更大潜力。

Conclusion: ToolPRMBench为评估工具使用代理的PRMs提供了系统基准，揭示了专门化PRMs的重要性，并促进了该领域的研究发展。

Abstract: Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.

</details>


### [58] [Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection](https://arxiv.org/abs/2601.12310)
*Jennifer Dodgson,Alfath Daryl Alhajir,Michael Joedhitya,Akira Rafhael Janson Pattirane,Surender Suresh Kumar,Joseph Lim,C. H. Peh,Adith Ramdas,Steven Zhang Zhexu*

Main category: cs.AI

TL;DR: 提出一种基于环境存活性而非奖励的自我训练架构，通过行为在真实资源约束下的持久性进行选择，避免奖励黑客和语义漂移，实现可持续的开放式自我改进。


<details>
  <summary>Details</summary>
Motivation: 传统自我训练系统因缺乏判断数据质量的外部标准而退化，导致奖励黑客和语义漂移问题。需要一种在稀疏外部反馈和有限内存下稳定的自我训练架构。

Method: 引入基于环境存活性而非奖励的自我训练架构：候选行为在真实资源约束下执行，只有那些环境效应持久且保留未来交互可能性的行为被传播。环境不提供语义反馈、密集奖励或任务特定监督，选择仅通过行为作为世界改变事件的差异生存进行。

Result: 分析显示改进主要通过有效可重复策略在整合和剪枝机制下的持久性实现（负空间学习范式）。模型在没有明确指导的情况下发展元学习策略（如故意实验失败以获取信息性错误消息）。

Conclusion: 环境基础选择能够实现可持续的开放式自我改进，为更鲁棒和可泛化的自主系统提供可行路径，无需依赖人类策划数据或复杂奖励塑造。

Abstract: Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.
  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.
  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.

</details>


### [59] [Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence](https://arxiv.org/abs/2601.12318)
*Dehao Ying,Fengchang Yu,Haihua Chen,Changjiang Jiang,Yurong Li,Wei Lu*

Main category: cs.AI

TL;DR: 这篇论文是第一份关于文档智能数据生成的全面技术地图，提出了基于"数据和标签可用性"的新分类法，将方法分为四个资源中心范式，并建立了多级评估框架。


<details>
  <summary>Details</summary>
Motivation: 文档智能需要大规模高质量训练数据，但手动标注是瓶颈。现有研究局限于单一模态或特定任务，缺乏与现实工作流程一致的整体视角，需要建立统一框架来系统化这个碎片化领域。

Method: 将数据生成重新定义为监督信号生产，基于"数据和标签可用性"引入新分类法，将方法组织为四个资源中心范式：数据增强、从零开始生成数据、自动数据标注和自监督信号构建。建立多级评估框架整合内在质量和外在效用。

Result: 创建了文档智能数据生成的第一个全面技术地图，揭示了保真度差距等关键挑战和协同进化生态系统等前沿问题。通过统一结构分析方法论，展示了在多样化文档智能基准上的性能提升。

Conclusion: 通过系统化这个碎片化领域，将数据生成定位为下一代文档智能的核心引擎，为未来研究提供了统一框架和方向指导。

Abstract: The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the "availability of data and labels." This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.

</details>


### [60] [MARO: Learning Stronger Reasoning from Social Interaction](https://arxiv.org/abs/2601.12323)
*Yin Cai,Zhouhong Gu,Juntao Zhang,Ping Chen*

Main category: cs.AI

TL;DR: MARO：通过多智能体社交环境中的学习和实践，让大语言模型获得更强推理能力的方法


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型训练方法主要让模型从现有文本内容学习或解决预定问题，缺乏在涉及与他人互动、谈判和竞争的真实场景中的经验，而人类在日常生活中面临无数需要推理和判断的场景

Method: MARO方法：1) 将最终成败结果分解为交互过程中的具体行为，解决稀疏学习信号问题；2) 通过平衡不同角色的训练样本权重，处理角色分布不均问题；3) 通过直接评估每个行为的效用，解决环境不稳定性问题

Result: MARO不仅在社交推理能力上取得显著提升，而且通过社交模拟学习获得的能力能有效迁移到数学推理和指令遵循等其他任务

Conclusion: 多智能体社交学习在增强LLMs通用推理能力方面具有巨大潜力

Abstract: Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.

</details>


### [61] [Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations](https://arxiv.org/abs/2601.12338)
*Kartikey Singh Bhandari,Manav Ganesh,Yashwant Viswanathan,Archit Agrawal,Dhruv Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: 提出一个两阶段LLM框架，将客户评论转化为可执行的业务建议，使用LoRA专家混合策略提升专业性和效率


<details>
  <summary>Details</summary>
Motivation: 客户评论包含丰富的服务失败和用户期望信号，但将这种非结构化反馈转化为可执行的业务决策仍然困难

Method: 提出模块化两阶段LLM框架：问题模型提取关键问题并分类，建议模型基于问题表示生成针对性操作建议；使用LoRA专家混合策略，训练多个低秩适配器并通过轻量门控机制在推理时进行专家混合

Result: 在航空和餐厅两个领域的Yelp评论上评估，该方法在行动性、特异性等八个维度上持续优于仅提示和单适配器基线，保持效率与质量的良好平衡

Conclusion: 提出的两阶段LLM框架结合LoRA专家混合策略，能够有效将客户评论转化为具体可执行的业务建议，在多个评估维度上表现优异

Abstract: Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.

</details>


### [62] [PsychēChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling](https://arxiv.org/abs/2601.12392)
*Zhentao Xia,Yongqi Fan,Yuxiang Chu,Yichao Yin,Liangliang Chen,Tong Ruan,Weiyan Zhang*

Main category: cs.AI

TL;DR: PsychēChat是一个用于心理咨询的LLM系统，通过显式建模用户情绪变化和安全风险分析来提升咨询质量，提供Agent Mode和LLM Mode两种推理范式。


<details>
  <summary>Details</summary>
Motivation: 现有心理咨询模型通常不显式建模用户在咨询过程中的情绪变化，这是传统心理学流派的核心关注点。同时，如何将咨询师模型的回应与这些情绪变化对齐，并主动缓解安全风险，这些问题尚未得到充分探索。

Method: 提出PsychēChat系统，包含两个核心模块：情绪管理模块（捕捉用户当前情绪和情绪变化）和风险控制模块（预测用户后续反应并识别潜在风险）。采用两种建模范式：Agent Mode（多智能体协作管道）和LLM Mode（端到端思维链推理）。通过交互式角色扮演合成咨询师-求助者对话。

Result: 通过交互式评分、对话级评估和人工评估等广泛实验证明，PsychēChat在情感洞察和安全控制方面优于现有方法。

Conclusion: PsychēChat通过显式整合情绪变化追踪和安全风险分析，为心理咨询提供了更有效的解决方案，两种推理范式在效率和性能之间取得了平衡。

Abstract: Large language models (LLMs) have demonstrated notable advancements in psychological counseling. However, existing models generally do not explicitly model seekers' emotion shifts across counseling sessions, a core focus in classical psychological schools. Moreover, how to align counselor models' responses with these emotion shifts while proactively mitigating safety risks remains underexplored. To bridge these gaps, we propose PsychēChat, which explicitly integrates emotion shift tracking and safety risk analysis for psychological counseling. Specifically, we employ interactive role-playing to synthesize counselor--seeker dialogues, incorporating two modules: Emotion Management Module, to capture seekers' current emotions and emotion shifts; and Risk Control Module, to anticipate seekers' subsequent reactions and identify potential risks. Furthermore, we introduce two modeling paradigms. The Agent Mode structures emotion management, risk control, and counselor responses into a collaborative multi-agent pipeline. The LLM Mode integrates these stages into a unified chain-of-thought for end-to-end inference, balancing efficiency and performance. Extensive experiments, including interactive scoring, dialogue-level evaluation, and human assessment, demonstrate that PsychēChat outperforms existing methods for emotional insight and safety control.

</details>


### [63] [Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation](https://arxiv.org/abs/2601.12410)
*Dingyi Yang,Junqi Zhao,Xue Li,Ce Li,Boyang Li*

Main category: cs.AI

TL;DR: LLMs在知识状态追踪和估计任务上表现接近随机水平，显著低于人类，未来研究应更重视知识估计和意图理解能力


<details>
  <summary>Details</summary>
Motivation: 认知人类学认为人类智能的核心在于推断他人知识状态和理解意图的能力，而黑猩猩等近亲动物缺乏这种能力。本研究旨在评估LLMs在知识状态追踪和估计方面的表现。

Method: 设计两个任务：(1) 检测故事角色是否通过行动表现出本不应拥有的知识；(2) 预测故事角色基于自身知识（而非客观事实）的下一步行动。

Result: 当前最先进的LLMs在两个任务上都达到接近随机的性能水平，显著低于人类表现。

Conclusion: 未来LLM研究应更加重视知识估计和意图理解能力的发展。

Abstract: Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.

</details>


### [64] [Large Language Model for OWL Proofs](https://arxiv.org/abs/2601.12444)
*Hui Yang,Jiaoyan Chen,Uli Sattler*

Main category: cs.AI

TL;DR: 该论文研究了LLM在OWL本体论中生成证明的能力，开发了自动化数据集构建和评估框架，发现LLM在复杂情况下表现有限，逻辑复杂性是主要影响因素，噪声和不完整数据会显著降低性能。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM的推理能力已被广泛研究，但它们在生成忠实、可读的证明（解释结论为何成立）方面的能力尚未充分探索。特别是在OWL本体论这种广泛用于表示和推理复杂知识的背景下，需要系统评估LLM的证明生成能力。

Method: 开发了自动化数据集构建和评估框架，包含三个顺序任务：提取、简化和解释，以及评估前提逻辑完整性的额外任务。在广泛使用的推理LLM上进行了大量实验。

Result: 1）某些模型整体表现良好但在复杂情况下有限；2）逻辑复杂性（而非表示格式）是影响LLM性能的主要因素；3）输入数据中的噪声和不完整性显著降低LLM性能。

Conclusion: LLM在严格逻辑解释方面具有潜力，但在复杂或不完美条件下支持弹性推理仍存在差距。研究强调了需要改进LLM在复杂逻辑场景下的证明生成能力。

Abstract: The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.

</details>


### [65] [Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck](https://arxiv.org/abs/2601.12499)
*Meiru Zhang,Zaiqiao Meng,Nigel Collier*

Main category: cs.AI

TL;DR: LLMs存在位置偏见导致多跳推理失败，研究提出MFAI探针发现"最弱环节定律"：多跳推理性能取决于最不可见证据的性能，且由绝对位置而非相对距离决定。注意力引导可改善识别瓶颈，而"思考"模型能有效定位和整合信息。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs具有大规模上下文窗口，但在多跳推理中存在位置偏见问题，导致忽略某些位置的信息。需要厘清这些失败是由于无法定位证据（识别失败）还是无法整合证据（综合失败）。

Method: 提出多焦点注意力指令（MFAI）作为语义探针，通过显式引导注意力到选定位置来分离识别和综合机制。在5个LLMs和两个多跳QA任务（MuSiQue和NeoQA）上进行实验。

Result: 发现"最弱环节定律"：多跳推理性能崩溃到最不可见证据的性能水平，且由绝对位置而非事实间的线性距离决定（性能方差<3%）。匹配的MFAI可改善低可见性位置的识别瓶颈，提升准确率达11.5%；误导性MFAI在真实任务中引发混淆但在合成任务中被过滤。"思考"模型能有效定位和整合信息，在嘈杂长上下文设置中匹配黄金基线。

Conclusion: LLMs的多跳推理失败主要由位置偏见导致的识别瓶颈引起，而非综合失败。注意力引导可改善识别，而System-2推理模型能有效处理多跳推理，即使在嘈杂长上下文中也能达到黄金基线水平。

Abstract: Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the "Weakest Link Law": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that "thinking" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.

</details>


### [66] [Agentic Reasoning for Large Language Models](https://arxiv.org/abs/2601.12538)
*Tianxin Wei,Ting-Wei Li,Zhining Liu,Xuying Ning,Ze Yang,Jiaru Zou,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Dongqi Fu,Zihao Li,Mengting Ai,Duo Zhou,Wenxuan Bao,Yunzhe Li,Gaotang Li,Cheng Qian,Yu Wang,Xiangru Tang,Yin Xiao,Liri Fang,Hui Liu,Xianfeng Tang,Yuji Zhang,Chi Wang,Jiaxuan You,Heng Ji,Hanghang Tong,Jingrui He*

Main category: cs.AI

TL;DR: 该调查论文系统性地组织并分析了LLM的智能体推理能力，将其分为基础智能体推理、自我进化推理和集体多智能体推理三个层次，并区分了上下文推理与训练后推理两种范式，最后总结了实际应用和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在封闭环境中表现出强大的推理能力，但在开放动态环境中表现不佳。智能体推理通过将LLM重构为能够规划、行动和持续学习的自主智能体，代表了范式转变，需要系统性地组织和理解这一新兴领域。

Method: 论文采用三维度框架组织智能体推理：1) 环境动态性分为三层：基础智能体推理（稳定环境中的规划、工具使用和搜索）、自我进化推理（通过反馈、记忆和适应优化能力）、集体多智能体推理（协作环境中的协调和知识共享）；2) 区分上下文推理（通过结构化编排扩展测试时交互）和训练后推理（通过强化学习和监督微调优化行为）；3) 综述实际应用和基准测试。

Result: 论文将智能体推理方法整合为连接思维与行动的统一路线图，系统性地综述了科学、机器人、医疗、自主研究和数学等领域的代表性智能体推理框架和应用基准。

Conclusion: 智能体推理代表了LLM从被动文本生成向主动交互智能体的重要转变。论文提出了系统性的组织框架，并指出了未来研究方向，包括个性化、长时程交互、世界建模、可扩展的多智能体训练以及实际部署的治理问题。

Abstract: Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.

</details>


### [67] [MemeLens: Multilingual Multitask VLMs for Memes](https://arxiv.org/abs/2601.12539)
*Ali Ezzat Shahroor,Mohamed Bayan Kmainasi,Abul Hasnat,Dimitar Dimitrov,Giovanni Da San Martino,Preslav Nakov,Firoj Alam*

Main category: cs.AI

TL;DR: MemeLens：统一的多语言多任务解释增强视觉语言模型，整合38个公共表情包数据集，构建包含20个任务的共享分类法，用于表情包理解。


<details>
  <summary>Details</summary>
Motivation: 现有表情包研究分散在不同任务（仇恨、厌女、宣传、情感、幽默）和语言中，限制了跨领域泛化能力，需要统一框架来解决这一差距。

Method: 整合38个公共表情包数据集，将数据集特定标签过滤并映射到包含20个任务的共享分类法中，涵盖危害、目标、比喻/语用意图和情感等维度，构建多语言多任务解释增强视觉语言模型。

Result: 研究发现：1）稳健的表情包理解需要多模态训练；2）不同语义类别间存在显著差异；3）在单个数据集上微调而非统一训练时，模型容易过度专业化。

Conclusion: MemeLens为表情包理解提供了统一的多语言多任务框架，实验资源和数据集将公开，促进社区研究。

Abstract: Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.

</details>


### [68] [Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery](https://arxiv.org/abs/2601.12542)
*Lukas Weidener,Marko Brkić,Mihailo Jovanović,Ritvik Singh,Chiara Baccin,Emre Ulgac,Alex Dobrin,Aakaash Meduri*

Main category: cs.AI

TL;DR: Deep Research是一个多智能体系统，能够在几分钟内完成交互式科学研究，相比现有批量处理系统（需要数小时）显著提升了效率，在BixBench计算生物学基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI科学发现系统大多是专有的，且采用批量处理模式，每个研究周期需要数小时，无法实现实时研究人员指导。需要开发能够支持交互式科学调查的系统。

Method: 采用多智能体架构，包括规划、数据分析、文献搜索和新颖性检测等专门智能体，通过持久世界状态在迭代研究周期中保持上下文。支持两种操作模式：带选择性人工检查点的半自主模式和用于扩展调查的完全自主模式。

Result: 在BixBench计算生物学基准测试中表现优异，开放回答准确率达到48.8%，多项选择准确率达到64.5%，比现有基线高出14到26个百分点。研究周期时间缩短到分钟级别。

Conclusion: Deep Research系统实现了交互式科学调查，显著缩短了研究周期时间。同时分析了架构限制，包括开放获取文献的限制和自动新颖性评估的挑战，为AI辅助科学工作流程的实际部署提供了实用考虑。

Abstract: Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.

</details>


### [69] [How Clinicians Think and What AI Can Learn From It](https://arxiv.org/abs/2601.12547)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 临床AI应转向序数决策而非预测引擎，采用稳健的启发式规则而非期望效用优化，以匹配临床推理的本质


<details>
  <summary>Details</summary>
Motivation: 现有临床AI系统主要作为预测引擎（生成标签或风险评分），但真实的临床推理是时间受限、顺序控制的不确定性问题。临床医生在信息收集和不可逆行动之间交替，受后悔、约束和患者价值观指导。需要AI系统更好地匹配临床推理的本质。

Method: 提出临床推理的计算基础是序数、非补偿性决策，而非基数优化。临床医生依赖快速节俭的词典式启发式（如快速节俭树），在检查少量固定线索后早期停止。为这种算法提供规范性理由：1）临床权衡主要通过人类判断构建，仅在弱可测量尺度上，只有排序是不变的；2）偏好和信号获取结构粗糙，存在持久的不确定性下限。

Result: 当"粗糙性"超过决策边界时，插件式期望效用优化变得脆弱（小扰动下高翻转概率），而稳健的支配/过滤规则（ε-支配、极大极小）能稳定决策。提出临床对齐的AI蓝图：使用丰富模型进行信念和轨迹建模，但通过稳健序数规则选择行动；将启发式视为低维特例；部署AI作为"选择性复杂性"——主要在决策脆弱且信息具有正期望影响时用于打破平局。

Conclusion: 临床AI应采纳序数优先立场，使用稳健的序数决策规则而非期望效用优化，将启发式视为临床推理的核心而非简化，通过选择性复杂性实现临床对齐的AI系统。

Abstract: Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\to$ perception $\to$ inference $\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($ε$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.

</details>


### [70] [Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents](https://arxiv.org/abs/2601.12560)
*Arunkumar V,Gangadharan G. R.,Rajkumar Buyya*

Main category: cs.AI

TL;DR: 本文提出一个统一的Agentic AI分类框架，将智能体分解为感知、大脑、规划、行动、工具使用和协作六个组件，并探讨从线性推理到原生推理模型的演进趋势。


<details>
  <summary>Details</summary>
Motivation: 随着AI从仅生成文本的模型转向Agentic AI（智能体AI），系统作为自主实体能够感知、推理、规划和行动。LLMs不再只是被动的知识引擎，而是作为认知控制器结合记忆、工具使用和环境反馈来追求扩展目标。然而，从简单单循环智能体到分层多智能体系统的各种新兴设计使得这一领域难以导航，需要统一的分类框架。

Method: 提出一个统一的分类法，将智能体分解为六个核心组件：感知、大脑、规划、行动、工具使用和协作。使用这个框架分析从线性推理程序到原生推理时间推理模型的演进，以及从固定API调用到开放标准（如模型上下文协议和原生计算机使用）的转变。同时对智能体操作环境进行分类，并回顾当前评估实践。

Result: 建立了一个统一的Agentic AI分类框架，能够系统描述智能体架构的各个组件。识别了从线性推理到原生推理模型的重要转变趋势，以及工具使用标准化的演进。对智能体操作环境进行了系统分类，并总结了当前评估方法。

Conclusion: Agentic AI正在快速发展，但面临幻觉行动、无限循环和提示注入等开放挑战。未来研究需要朝着更稳健可靠的自主系统方向发展，统一的分类框架为理解和导航这一复杂领域提供了有价值的工具。

Abstract: Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.

</details>


### [71] [STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models](https://arxiv.org/abs/2601.12641)
*Xiangyu Shi,Junyang Ding,Xu Zhao,Sinong Zhan,Payal Mohapatra,Daniel Quispe,Kojo Welbeck,Jian Cao,Wei Chen,Ping Guo,Qi Zhu*

Main category: cs.AI

TL;DR: STEP-LLM：通过LLM从自然语言生成STEP格式CAD模型的新框架，在几何保真度上超越Text2CAD基线


<details>
  <summary>Details</summary>
Motivation: CAD模型创建需要专业知识且耗时，现有基于命令序列或脚本的方法（如CadQuery）依赖于特定内核且缺乏制造通用性。STEP文件作为广泛采用的中性边界表示格式直接兼容制造，但其图结构、交叉引用的特性对自回归LLM构成挑战。

Method: 1) 收集约40K STEP-描述对数据集；2) 针对STEP图结构格式设计预处理：基于深度优先搜索的重新序列化以线性化交叉引用同时保持局部性，以及CoT风格的结构注释指导全局一致性；3) 集成检索增强生成进行监督微调；4) 通过基于Chamfer距离的几何奖励进行强化学习优化生成质量。

Result: STEP-LLM在几何保真度上持续优于Text2CAD基线：RAG模块显著提升完整性和可渲染性，DFS重新序列化增强整体准确性，RL进一步减少几何差异。度量和视觉比较均证实STEP-LLM生成形状具有更高保真度。

Conclusion: 证明了LLM驱动从自然语言生成STEP模型的可行性，展示了其民主化CAD设计用于制造的潜力。该框架通过专门针对STEP图结构特性的预处理、RAG和RL优化，成功克服了自回归LLM处理复杂交叉引用格式的挑战。

Abstract: Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.

</details>


### [72] [MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents](https://arxiv.org/abs/2601.12661)
*Chuhan Qiao,Jianghua Huang,Daxing Zhao,Ziding Liu,Yanjun Shen,Bing Cheng,Wei Lin,Kai Wu*

Main category: cs.AI

TL;DR: MedConsultBench是一个评估医疗咨询代理的综合性框架，通过原子信息单元和22个细粒度指标来追踪完整的在线咨询周期，发现高诊断准确率往往掩盖了信息收集效率和用药安全方面的缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前医疗咨询代理评估过于关注结果导向任务，忽视了端到端流程完整性和临床安全性，现有交互基准往往碎片化且粗粒度，无法捕捉专业咨询所需的结构化询问逻辑和诊断严谨性。

Method: 提出MedConsultBench框架，覆盖从病史采集、诊断到治疗计划和随访问答的完整临床工作流程。引入原子信息单元（AIUs）在子轮次层面追踪临床信息获取，通过22个细粒度指标精确监测关键事实的获取过程，评估不确定性感知的简洁询问能力，强调药物方案兼容性，并通过约束尊重计划修订处理现实的处方后随访问答。

Result: 对19个大语言模型的系统评估显示，高诊断准确率往往掩盖了信息收集效率和用药安全方面的显著缺陷，揭示了理论医学知识与临床实践能力之间的关键差距。

Conclusion: MedConsultBench为医疗AI与真实世界临床护理的细微需求对齐提供了严谨基础，强调了将医疗AI与临床实践要求相结合的重要性。

Abstract: Current evaluations of medical consultation agents often prioritize outcome-oriented tasks, frequently overlooking the end-to-end process integrity and clinical safety essential for real-world practice. While recent interactive benchmarks have introduced dynamic scenarios, they often remain fragmented and coarse-grained, failing to capture the structured inquiry logic and diagnostic rigor required in professional consultations. To bridge this gap, we propose MedConsultBench, a comprehensive framework designed to evaluate the complete online consultation cycle by covering the entire clinical workflow from history taking and diagnosis to treatment planning and follow-up Q\&A. Our methodology introduces Atomic Information Units (AIUs) to track clinical information acquisition at a sub-turn level, enabling precise monitoring of how key facts are elicited through 22 fine-grained metrics. By addressing the underspecification and ambiguity inherent in online consultations, the benchmark evaluates uncertainty-aware yet concise inquiry while emphasizing medication regimen compatibility and the ability to handle realistic post-prescription follow-up Q\&A via constraint-respecting plan revisions. Systematic evaluation of 19 large language models reveals that high diagnostic accuracy often masks significant deficiencies in information-gathering efficiency and medication safety. These results underscore a critical gap between theoretical medical knowledge and clinical practice ability, establishing MedConsultBench as a rigorous foundation for aligning medical AI with the nuanced requirements of real-world clinical care.

</details>


### [73] [Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration](https://arxiv.org/abs/2601.12667)
*Yi Di,Zhibin Zhao,Fujin Wang,Xue Liu,Jiafeng Tang,Jiaxin Ren,Zhi Zhai,Xuefeng Chen*

Main category: cs.AI

TL;DR: 论文提出SpaceHMchat框架，用于卫星巨型星座时代航天器电源系统的全回路健康管理，通过人机协作实现故障检测、定位和维护决策。


<details>
  <summary>Details</summary>
Motivation: 随着卫星巨型星座时代的到来，航天器数量将指数级增长，而航天器电源系统故障率高且对供电至关重要，需要适应大规模系统的健康管理新范式。

Method: 提出对齐底层能力原则，开发开源人机协作框架SpaceHMchat，建立硬件真实的故障注入实验平台和仿真模型，并发布首个全回路健康管理数据集。

Result: SpaceHMchat在23个量化指标上表现优异：工况识别逻辑推理结论准确率100%，异常检测工具调用成功率超99%，故障定位精度超90%，维护决策知识库搜索时间低于3分钟。

Conclusion: 该工作为卫星巨型星座时代的航天器电源系统健康管理提供了可行的解决方案，通过人机协作框架、实验平台和开源数据集推动了该领域的发展。

Abstract: It is foreseeable that the number of spacecraft will increase exponentially, ushering in an era dominated by satellite mega-constellations (SMC). This necessitates a focus on energy in space: spacecraft power systems (SPS), especially their health management (HM), given their role in power supply and high failure rates. Providing health management for dozens of SPS and for thousands of SPS represents two fundamentally different paradigms. Therefore, to adapt the health management in the SMC era, this work proposes a principle of aligning underlying capabilities (AUC principle) and develops SpaceHMchat, an open-source Human-AI collaboration (HAIC) framework for all-in-loop health management (AIL HM). SpaceHMchat serves across the entire loop of work condition recognition, anomaly detection, fault localization, and maintenance decision making, achieving goals such as conversational task completion, adaptive human-in-the-loop learning, personnel structure optimization, knowledge sharing, efficiency enhancement, as well as transparent reasoning and improved interpretability. Meanwhile, to validate this exploration, a hardware-realistic fault injection experimental platform is established, and its simulation model is built and open-sourced, both fully replicating the real SPS. The corresponding experimental results demonstrate that SpaceHMchat achieves excellent performance across 23 quantitative metrics, such as 100% conclusion accuracy in logical reasoning of work condition recognition, over 99% success rate in anomaly detection tool invocation, over 90% precision in fault localization, and knowledge base search time under 3 minutes in maintenance decision-making. Another contribution of this work is the release of the first-ever AIL HM dataset of SPS. This dataset contains four sub-datasets, involving 4 types of AIL HM sub-tasks, 17 types of faults, and over 700,000 timestamps.

</details>


### [74] [Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction](https://arxiv.org/abs/2601.12688)
*Xu Zhang,Qinghua Wang,Mengyang Zhao,Fang Wang,Cunquan Qu*

Main category: cs.AI

TL;DR: 该论文提出了一种用于多被告人案件中角色区分的掩码多阶段推理框架，通过整合量刑逻辑和定向掩码机制，提高AI在司法分析中的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在多被告人案件中，司法表述常常模糊被告人的角色，这阻碍了AI驱动的有效分析。需要精确区分主犯和从犯的责任，以确保司法公平性。

Method: 提出了掩码多阶段推理（MMSI）框架：1）将量刑逻辑整合到预训练Transformer编码器中；2）使用定向掩码机制澄清角色；3）采用比较数据构建策略提高对主从犯责任区分的敏感性；4）通过广播将预测的罪名标签整合到回归模型中。

Result: 在自定义的IMLJP故意伤害案件数据集上评估，MMSI框架在角色责任区分方面取得了显著的准确率提升，优于基线方法。

Conclusion: 该工作为增强智能司法系统提供了稳健的解决方案，代码已公开，有助于提高多被告人案件中的AI辅助分析能力。

Abstract: Crime disrupts societal stability, making law essential for balance. In multidefendant cases, assigning responsibility is complex and challenges fairness, requiring precise role differentiation. However, judicial phrasing often obscures the roles of the defendants, hindering effective AI-driven analyses. To address this issue, we incorporate sentencing logic into a pretrained Transformer encoder framework to enhance the intelligent assistance in multidefendant cases while ensuring legal interpretability. Within this framework an oriented masking mechanism clarifies roles and a comparative data construction strategy improves the model's sensitivity to culpability distinctions between principals and accomplices. Predicted guilt labels are further incorporated into a regression model through broadcasting, consolidating crime descriptions and court views. Our proposed masked multistage inference (MMSI) framework, evaluated on the custom IMLJP dataset for intentional injury cases, achieves significant accuracy improvements, outperforming baselines in role-based culpability differentiation. This work offers a robust solution for enhancing intelligent judicial systems, with publicly code available.

</details>


### [75] [Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts](https://arxiv.org/abs/2601.12711)
*Kevin Wang,Neel P. Bhatt,Cong Liu,Junbo Li,Runjin Chen,Yihan Xi,Timothy Barclay,Alvaro Velasquez,Ufuk Topcu,Zhangyang Wang*

Main category: cs.AI

TL;DR: 提出神经符号LoRA框架，动态结合数值微调和符号编辑两种策略，通过统一监控信号和奖励分类器决定何时使用LoRA进行事实重构，何时使用TextGrad进行词元级编辑，在多种LLM骨干上表现优于纯数值或纯符号基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的适应可以通过数值更新（修改模型参数）或符号操作（处理离散提示或逻辑约束）实现。数值微调擅长注入新事实知识，而符号更新则能灵活控制风格和对齐而无需重新训练。需要结合这两种互补策略以实现更优的模型适应能力。

Method: 提出神经符号LoRA框架：1）使用统一监控信号和基于奖励的分类器，动态决定何时使用LoRA进行更深层的事实重构，何时使用TextGrad进行词元级编辑；2）保持内存效率，仅在需要时将符号转换卸载到外部LLM；3）符号编辑过程中生成的优化提示可作为高质量、可重复使用的训练数据。

Result: 在多个LLM骨干上进行广泛实验，神经符号LoRA始终优于纯数值或纯符号基线，表现出卓越的适应能力和改进的性能。特别是在数学推理等数据稀缺领域，生成的优化提示作为高质量训练数据具有重要价值。

Conclusion: 研究表明，交错使用数值和符号更新能够解锁语言模型微调的新层次多功能性。神经符号LoRA框架展示了结合两种策略的优势，为LLM适应提供了更灵活高效的解决方案。

Abstract: Large language models (LLMs) can be adapted either through numerical updates that alter model parameters or symbolic manipulations that work on discrete prompts or logical constraints. While numerical fine-tuning excels at injecting new factual knowledge, symbolic updates offer flexible control of style and alignment without retraining. We introduce a neurosymbolic LoRA framework that dynamically combines these two complementary strategies. Specifically, we present a unified monitoring signal and a reward-based classifier to decide when to employ LoRA for deeper factual reconstruction and when to apply TextGrad for token-level edits. Our approach remains memory-efficient by offloading the symbolic transformations to an external LLM only when needed. Additionally, the refined prompts produced during symbolic editing serve as high-quality, reusable training data, an important benefit in data-scarce domains like mathematical reasoning. Extensive experiments across multiple LLM backbones show that neurosymbolic LoRA consistently outperforms purely numerical or purely symbolic baselines, demonstrating superior adaptability and improved performance. Our findings highlight the value of interleaving numerical and symbolic updates to unlock a new level of versatility in language model fine-tuning.

</details>


### [76] [Teaching Large Reasoning Models Effective Reflection](https://arxiv.org/abs/2601.12720)
*Hanbin Wang,Jingwei Song,Jinpeng Li,Qi Zhu,Fei Mi,Ganqu Cui,Yasheng Wang,Lifeng Shang*

Main category: cs.AI

TL;DR: 提出SCFT和RLERR方法解决大型推理模型的表面反思问题，通过自我批判微调和强化学习提升反思质量与推理准确率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中常进行自我反思，但许多反思是表面的，对原始答案改进有限且带来计算开销，需要解决表面反思问题

Method: 1. SCFT：自我批判微调框架，让模型批判自身输出，通过拒绝采样筛选高质量批判，使用批判目标微调模型；2. RLERR：在SCFT基础上，利用高质量反思构建奖励信号，通过强化学习引导模型内化自我纠正过程

Result: 在AIME2024和AIME2025两个挑战性基准测试中，SCFT和RLERR显著提高了推理准确率和反思质量，超越了最先进的基线方法

Conclusion: 提出的SCFT和RLERR方法有效解决了大型推理模型的表面反思问题，通过增强模型的反思推理能力，在复杂推理任务上取得了显著改进

Abstract: Large Reasoning Models (LRMs) have recently shown impressive performance on complex reasoning tasks, often by engaging in self-reflective behaviors such as self-critique and backtracking. However, not all reflections are beneficial-many are superficial, offering little to no improvement over the original answer and incurring computation overhead. In this paper, we identify and address the problem of superficial reflection in LRMs. We first propose Self-Critique Fine-Tuning (SCFT), a training framework that enhances the model's reflective reasoning ability using only self-generated critiques. SCFT prompts models to critique their own outputs, filters high-quality critiques through rejection sampling, and fine-tunes the model using a critique-based objective. Building on this strong foundation, we further introduce Reinforcement Learning with Effective Reflection Rewards (RLERR). RLERR leverages the high-quality reflections initialized by SCFT to construct reward signals, guiding the model to internalize the self-correction process via reinforcement learning. Experiments on two challenging benchmarks, AIME2024 and AIME2025, show that SCFT and RLERR significantly improve both reasoning accuracy and reflection quality, outperforming state-of-the-art baselines. All data and codes are available at https://github.com/wanghanbinpanda/SCFT.

</details>


### [77] [VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension](https://arxiv.org/abs/2601.12781)
*Hyejin Park,Junhyuk Kwon,Suha Kwak,Jungseul Ok*

Main category: cs.AI

TL;DR: VIRO框架通过嵌入轻量级操作级验证器来解决神经符号REC方法中的级联错误问题，在目标存在和无目标场景下都实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号REC方法虽然实现了可解释推理和强大的零样本泛化能力，但假设中间推理步骤都是准确的。这种假设导致级联错误：错误检测和无效关系在推理链中传播，即使图像中没有目标也会产生高置信度的误报。

Method: 提出了验证集成推理操作符（VIRO）框架，在推理步骤中嵌入轻量级操作级验证器。每个操作符执行并验证其输出（如对象存在性或空间关系），当验证条件不满足时，系统能够稳健地处理无目标情况。

Result: 在目标存在和无目标设置下达到61.1%的平衡准确率，实现了最先进的性能。同时展示了向真实世界第一人称数据的泛化能力，具有高计算效率（高吞吐量）、高可靠性（程序失败率低于0.3%）和通过解耦程序生成与执行的可扩展性。

Conclusion: VIRO框架通过集成操作级验证器，有效解决了神经符号REC方法中的级联错误问题，在保持可解释推理优势的同时，显著提升了系统的鲁棒性、可靠性和效率。

Abstract: Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.

</details>


### [78] [SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability](https://arxiv.org/abs/2601.12804)
*Hanwei Zhang,Luo Cheng,Rui Wen,Yang Zhang,Lijun Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: SL-CBM通过引入语义局部性增强概念瓶颈模型，生成空间一致的概念和类别显著图，提高局部忠实性和解释质量，同时保持分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型（CBMs）存在局部忠实性不足的问题，无法将概念与有意义的图像区域空间对齐，限制了其可解释性和可靠性。

Method: 提出SL-CBM，通过集成1x1卷积层和交叉注意力机制，在概念和类别层面生成空间一致的显著图，增强概念、图像区域和最终预测之间的对齐。

Result: 在图像数据集上的实验表明，SL-CBM显著提高了局部忠实性、解释质量和干预效果，同时保持了有竞争力的分类准确性。

Conclusion: SL-CBM弥合了基于概念的推理和空间可解释性之间的差距，为可解释和可信赖的概念模型设定了新标准。

Abstract: Explainable AI (XAI) is crucial for building transparent and trustworthy machine learning systems, especially in high-stakes domains. Concept Bottleneck Models (CBMs) have emerged as a promising ante-hoc approach that provides interpretable, concept-level explanations by explicitly modeling human-understandable concepts. However, existing CBMs often suffer from poor locality faithfulness, failing to spatially align concepts with meaningful image regions, which limits their interpretability and reliability. In this work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that enforces locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer with a cross-attention mechanism to enhance alignment between concepts, image regions, and final predictions. Unlike prior methods, SL-CBM produces faithful saliency maps inherently tied to the model's internal reasoning, facilitating more effective debugging and intervention. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Our ablation studies highlight the importance of contrastive and entropy-based regularization for balancing accuracy, sparsity, and faithfulness. Overall, SL-CBM bridges the gap between concept-based reasoning and spatial explainability, setting a new standard for interpretable and trustworthy concept-based models.

</details>


### [79] [MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction](https://arxiv.org/abs/2601.12822)
*Wenqi Zhang,Yulin Shen,Changyue Jiang,Jiarun Dai,Geng Hong,Xudong Pan*

Main category: cs.AI

TL;DR: MirrorGuard是一个即插即用的防御框架，通过模拟训练提升计算机使用代理的安全性，在保持代理实用性的同时显著降低安全风险。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型集成到计算机使用代理中，使其能够通过图形界面自主与操作系统交互，但这也引入了严重的安全风险：恶意指令或视觉提示注入可能触发不安全的推理并导致有害的系统级操作。现有防御方法（如基于检测的阻止）虽然能防止损害，但常常过早中止任务，降低了代理的实用性。

Method: 提出MirrorGuard防御框架，采用模拟训练方法。为了降低操作系统大规模训练成本，设计了新颖的神经符号模拟管道，在纯文本模拟环境中生成逼真的高风险GUI交互轨迹，捕获不安全推理模式和潜在系统危险，无需执行真实操作。在模拟环境中，MirrorGuard学习在CUA产生和执行不安全操作之前拦截并纠正其不安全推理链。

Result: 在多样化的基准测试和CUA架构上的广泛评估显示，MirrorGuard显著降低了安全风险。例如，在字节跳动UI-TARS系统上，它将不安全率从66.5%降至13.0%，同时保持较低的错误拒绝率（FRR）。相比之下，最先进的GuardAgent仅将不安全率降至53.9%，且FRR高出15.4%。

Conclusion: 模拟衍生的防御方法能够在保持代理基本实用性的同时，提供强大的现实世界保护。MirrorGuard证明了通过模拟训练可以显著提升计算机使用代理的安全性。

Abstract: Large foundation models are integrated into Computer Use Agents (CUAs), enabling autonomous interaction with operating systems through graphical user interfaces (GUIs) to perform complex tasks. This autonomy introduces serious security risks: malicious instructions or visual prompt injections can trigger unsafe reasoning and cause harmful system-level actions. Existing defenses, such as detection-based blocking, prevent damage but often abort tasks prematurely, reducing agent utility. In this paper, we present MirrorGuard, a plug-and-play defense framework that uses simulation-based training to improve CUA security in the real world. To reduce the cost of large-scale training in operating systems, we propose a novel neural-symbolic simulation pipeline, which generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment, which captures unsafe reasoning patterns and potential system hazards without executing real operations. In the simulation environment, MirrorGuard learns to intercept and rectify insecure reasoning chains of CUAs before they produce and execute unsafe actions. In real-world testing, extensive evaluations across diverse benchmarks and CUA architectures show that MirrorGuard significantly mitigates security risks. For instance, on the ByteDance UI-TARS system, it reduces the unsafe rate from 66.5% to 13.0% while maintaining a marginal false refusal rate (FRR). In contrast, the state-of-the-art GuardAgent only achieves a reduction to 53.9% and suffers from a 15.4% higher FRR. Our work proves that simulation-derived defenses can provide robust, real-world protection while maintaining the fundamental utility of the agent. Our code and model are publicly available at https://bmz-q-q.github.io/MirrorGuard/.

</details>


### [80] [SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning](https://arxiv.org/abs/2601.12842)
*Qitong Fang,Haotian Li,Xu Wang*

Main category: cs.AI

TL;DR: SCULPT是一种约束引导的蒙特卡洛树搜索方法，通过领域感知评分和剪枝来提升LLM代理工作流的推理稳定性，避免随机探索中的不合理分支。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理工作流中的搜索策略依赖随机探索，经常遍历不合理分支，因为现有方法从通用提示或弱领域先验的策略中采样候选步骤，导致在操作符、单位和格式上的近似随机游走。

Method: SCULPT将领域感知评分集成到MCTS的选择、扩展、模拟和反向传播阶段，通过符号检查（维度一致性、类型兼容性、幅度合理性、深度控制和多样性）和结构模式指导来评分和剪枝动作。

Result: 在相同LLM配置下，SCULPT在多个数据集上带来稳定改进；使用GPT-5.2的额外结果评估了执行器可迁移性和前沿推理模型的性能。

Conclusion: 领域感知约束可以在保持效率和推理稳定性的同时提高准确性，为LLM代理工作流提供更有序的探索策略。

Abstract: Automated agent workflows can enhance the problem-solving ability of large language models (LLMs), but common search strategies rely on stochastic exploration and often traverse implausible branches. This occurs because current pipelines sample candidate steps from generic prompts or learned policies with weak domain priors, yielding near-random walks over operators, units, and formats. To promote ordered exploration, this paper introduces SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that integrates domain-aware scoring into selection, expansion, simulation, and backpropagation. SCULPT scores and prunes actions using a combination of symbolic checks (dimensional consistency, type compatibility, magnitude sanity, depth control, and diversity) and structural pattern guidance, thereby steering the search toward plausible reasoning paths. Under matched LLM configurations, SCULPT yields stable improvements on multiple datasets; additional results with GPT-5.2 assess executor transferability and performance on frontier reasoning models. Overall, domain-aware constraints can improve accuracy while maintaining efficiency and reasoning stability.

</details>


### [81] [Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data](https://arxiv.org/abs/2601.12856)
*Liping Huang,Gaoxi Xiao,Stefan Ma,Hechang Chen,Shisong Tang,Flora Salim*

Main category: cs.AI

TL;DR: 提出一个从公开登革热病例数据挖掘城市区域间潜在传播链接的新框架，用于预测热点区域和解释城市传播模式


<details>
  <summary>Details</summary>
Motivation: 登革热在热带城市地区持续构成公共卫生挑战，需要预测传播风险以进行主动干预而非被动应对

Method: 通过梯度下降优化从病例数据中学习区域间的潜在传播链接，利用这些链接预测热点状态，并通过检查推断网络在连续周内的稳定性验证传播模式一致性

Result: 在新加坡2013-2018和2020年的案例研究中，仅需四周热点历史即可达到平均F分数0.79，学习到的传播链接与通勤流量高度一致

Conclusion: 该框架将公开病例数据转化为预测性和解释性资源，为公共卫生规划、早期干预和城市韧性提供可扩展的低成本工具

Abstract: Dengue, a mosquito-borne disease, continues to pose a persistent public health challenge in urban areas, particularly in tropical regions such as Singapore. Effective and affordable control requires anticipating where transmission risks are likely to emerge so that interventions can be deployed proactively rather than reactively. This study introduces a novel framework that uncovers and exploits latent transmission links between urban regions, mined directly from publicly available dengue case data. Instead of treating cases as isolated reports, we model how hotspot formation in one area is influenced by epidemic dynamics in neighboring regions. While mosquito movement is highly localized, long-distance transmission is often driven by human mobility, and in our case study, the learned network aligns closely with commuting flows, providing an interpretable explanation for citywide spread. These hidden links are optimized through gradient descent and used not only to forecast hotspot status but also to verify the consistency of spreading patterns, by examining the stability of the inferred network across consecutive weeks. Case studies on Singapore during 2013-2018 and 2020 show that four weeks of hotspot history are sufficient to achieve an average F-score of 0.79. Importantly, the learned transmission links align with commuting flows, highlighting the interpretable interplay between hidden epidemic spread and human mobility. By shifting from simply reporting dengue cases to mining and validating hidden spreading dynamics, this work transforms open web-based case data into a predictive and explanatory resource. The proposed framework advances epidemic modeling while providing a scalable, low-cost tool for public health planning, early intervention, and urban resilience.

</details>


### [82] [Human Emotion Verification by Action Languages via Answer Set Programming](https://arxiv.org/abs/2601.12912)
*Andreas Brännström,Juan Carlos Nieves*

Main category: cs.AI

TL;DR: C-MT是一种基于答案集编程和转移系统构建的动作语言，用于表示人类心理状态如何响应可观察动作序列而演化，特别关注情绪等心理状态的多维配置和受控变化。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够表示人类心理状态演化的形式化框架，以支持受控的智能体行为，并限制动作带来的不良心理副作用，同时能够验证情绪等心理状态的变化。

Method: 基于答案集编程和转移系统构建C-MT语言，整合评价情绪理论等心理学理论，引入"禁止导致"因果规则和专门的心理状态动态表达式，将心理变化原则转化为转移约束和不变性属性。

Result: 开发了一个能够对心理状态动态演化进行受控推理的框架，支持通过轨迹分析比较不同的心理变化动态，并应用于情绪验证模型的设计。

Conclusion: C-MT语言为形式化表示和分析人类心理状态演化提供了一个强大的框架，特别适用于情绪验证和受控智能体行为设计，具有理论和实践价值。

Abstract: In this paper, we introduce the action language C-MT (Mind Transition Language). It is built on top of answer set programming (ASP) and transition systems to represent how human mental states evolve in response to sequences of observable actions. Drawing on well-established psychological theories, such as the Appraisal Theory of Emotion, we formalize mental states, such as emotions, as multi-dimensional configurations. With the objective to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions, we extend the language with a novel causal rule, forbids to cause, along with expressions specialized for mental state dynamics, which enables the modeling of principles for valid transitions between mental states. These principles of mental change are translated into transition constraints, and properties of invariance, which are rigorously evaluated using transition systems in terms of so-called trajectories. This enables controlled reasoning about the dynamic evolution of human mental states. Furthermore, the framework supports the comparison of different dynamics of change by analyzing trajectories that adhere to different psychological principles. We apply the action language to design models for emotion verification. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [83] [Actionable Interpretability Must Be Defined in Terms of Symmetries](https://arxiv.org/abs/2601.12913)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Francesco Giannini,Alberto Termine,Filippo Bonchi,Mateja Jamnik,Giuseppe Marra*

Main category: cs.AI

TL;DR: 该论文认为当前AI可解释性研究存在根本性问题，因为现有定义缺乏可操作性，无法为具体建模和推理规则提供形式化原则。作者提出基于对称性的可操作定义，并假设四种对称性足以解决可解释性的核心问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI可解释性研究面临根本性挑战：现有定义缺乏可操作性，无法提供形式化原则来指导具体建模和推理规则的制定。这导致可解释性研究停留在概念层面，难以转化为实际可用的方法论。

Method: 提出基于对称性的可操作定义框架，假设四种对称性足以：(1) 激发核心可解释性属性，(2) 刻画可解释模型类别，(3) 推导统一的可解释推理形式（如对齐、干预和反事实）作为贝叶斯逆问题。

Result: 提出了一个基于对称性的可解释性理论框架，该框架能够为可解释性提供形式化基础，统一解释对齐、干预和反事实推理等概念，并将其表述为贝叶斯逆问题。

Conclusion: 可解释性研究需要从当前缺乏可操作性的定义转向基于对称性的形式化框架，这样才能为AI系统提供坚实的理论基础和实用的建模指导原则。

Abstract: This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed as existing definitions of interpretability are not *actionable*: they fail to provide formal principles from which concrete modelling and inferential rules can be derived. We posit that for a definition of interpretability to be actionable, it must be given in terms of *symmetries*. We hypothesise that four symmetries suffice to (i) motivate core interpretability properties, (ii) characterize the class of interpretable models, and (iii) derive a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion.

</details>


### [84] [MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI Agents via Automated Feedback Reflux](https://arxiv.org/abs/2601.13060)
*Zecheng Li,Zhihui Cao,Wenke Huang,Yudong Zhang,Keying Qi,Rui Wang,Zeyu Zheng,Jian Zhao,Hao Zhu,Hengxin Wu,Yuran Wang,Guitao Fan,Guokun Wu,Yicong Liu,Zhilin Gao,Haikun Xu,He Yang,Minqi Xiang,Xingyu Liu,Zuojian Wang*

Main category: cs.AI

TL;DR: MagicGUI-RMS是一个多智能体奖励模型系统，用于自动化评估GUI智能体轨迹并生成高质量训练数据，通过领域特定和通用奖励模型实现自适应评估和持续改进。


<details>
  <summary>Details</summary>
Motivation: 当前GUI智能体面临两大挑战：自动化评估智能体轨迹的困难，以及大规模生成高质量训练数据以实现持续改进的难题。现有方法依赖人工标注或静态规则验证，限制了可扩展性和动态环境适应性。

Method: 提出MagicGUI-RMS多智能体奖励模型系统，整合领域特定奖励模型(DS-RM)和通用奖励模型(GP-RM)，实现细粒度动作评估和跨异构GUI任务的鲁棒泛化。设计了结构化数据构建管道，自动生成平衡多样的奖励数据集，降低标注成本。通过自动数据回流机制，系统能识别错误动作、提出改进方案并持续增强智能体行为。

Result: 大量实验表明，MagicGUI-RMS在任务准确性和行为鲁棒性方面取得了显著提升，为构建基于奖励自适应驱动的自改进GUI智能体提供了有效基础。

Conclusion: MagicGUI-RMS建立了一个原则性且有效的框架，通过奖励模型系统实现GUI智能体的自适应轨迹评估、纠正反馈和自我进化学习能力，推动了自改进GUI智能体的发展。

Abstract: Graphical user interface (GUI) agents are rapidly progressing toward autonomous interaction and reliable task execution across diverse applications. However, two central challenges remain unresolved: automating the evaluation of agent trajectories and generating high-quality training data at scale to enable continual improvement. Existing approaches often depend on manual annotation or static rule-based verification, which restricts scalability and limits adaptability in dynamic environments. We present MagicGUI-RMS, a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. MagicGUI-RMS integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM), enabling fine-grained action assessment and robust generalization across heterogeneous GUI tasks. To support reward learning at scale, we design a structured data construction pipeline that automatically produces balanced and diverse reward datasets, effectively reducing annotation costs while maintaining sample fidelity. During execution, the reward model system identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate that MagicGUI-RMS yields substantial gains in task accuracy, behavioral robustness. These results establish MagicGUI-RMS as a principled and effective foundation for building self-improving GUI agents driven by reward-based adaptation.

</details>


### [85] [Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward](https://arxiv.org/abs/2601.13122)
*Gourab K Patro,Himanshi Agrawal,Himanshu Gharat,Supriya Panigrahi,Nim Sherpa,Vishal Vaddina,Dagnachew Birru*

Main category: cs.AI

TL;DR: 本文分析通用AI系统的风险，提出传统任务特定AI风险更小，需要重新思考负责任AI方法，并建立C2V2框架（控制、一致性、价值、真实性）来指导未来通用AI系统的负责任发展。


<details>
  <summary>Details</summary>
Motivation: 现代通用AI系统虽然功能强大，但存在幻觉、毒性、刻板印象等风险，使其不可信。这些风险在传统任务特定AI中不存在或更轻微且易于缓解。需要重新思考通用AI的负责任AI方法。

Method: 从八个广泛接受的负责任AI原则（公平性、隐私、可解释性、鲁棒性、安全性、真实性、治理、可持续性）分析通用AI的风险和脆弱性，与传统任务特定AI对比。提出输出自由度概念，并推导出C2V2（控制、一致性、价值、真实性）需求框架。

Result: 通用AI系统由于输出自由度非确定性高，风险更严重。C2V2框架为未来通用AI系统提供了满足负责任AI要求的设计指导。现有技术如AI对齐、检索增强生成、推理增强等在不同程度上满足C2V2需求。

Conclusion: 通过将应用或领域相关的负责任AI需求沿C2V2维度形式化建模，并采用系统设计方法结合各种技术来满足这些需求，可以实现负责任通用AI的发展目标。

Abstract: Modern general-purpose AI systems made using large language and vision models, are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating from one language to another, which has made them quite popular across industries. However, there are risks like hallucinations, toxicity, and stereotypes in their output that make them untrustworthy. We review various risks and vulnerabilities of modern general-purpose AI along eight widely accepted responsible AI (RAI) principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, and sustainability) and compare how they are non-existent or less severe and easily mitigable in traditional task-specific counterparts. We argue that this is due to the non-deterministically high Degree of Freedom in output (DoFo) of general-purpose AI (unlike the deterministically constant or low DoFo of traditional task-specific AI systems), and there is a need to rethink our approach to RAI for general-purpose AI. Following this, we derive C2V2 (Control, Consistency, Value, Veracity) desiderata to meet the RAI requirements for future general-purpose AI systems, and discuss how recent efforts in AI alignment, retrieval-augmented generation, reasoning enhancements, etc. fare along one or more of the desiderata. We believe that the goal of developing responsible general-purpose AI can be achieved by formally modeling application- or domain-dependent RAI requirements along C2V2 dimensions, and taking a system design approach to suitably combine various techniques to meet the desiderata.

</details>


### [86] [Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching](https://arxiv.org/abs/2601.13186)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 论文提出TIVS-O评估框架，结合语义缓存和可观测性指标，在HOPE启发的嵌套学习架构中评估多智能体系统的提示注入防御效果，实现零高风险漏洞、41.6%计算节省和最优安全-透明权衡。


<details>
  <summary>Details</summary>
Motivation: 提示注入是多智能体大语言模型部署的主要安全障碍，现有评估框架缺乏对防御效果与透明度交互关系的系统分析，需要更全面的评估方法。

Method: 扩展TIVS为TIVS-O，增加可观测性评分比(OSR)；采用HOPE启发的嵌套学习架构，结合智能体管道和连续记忆系统实现语义相似性缓存；使用301个合成提示和10种攻击家族进行评估。

Result: 系统实现零高风险漏洞，语义缓存减少41.6%的LLM调用，降低延迟、能耗和碳排放；TIVS-O的五种配置揭示了缓解严格性与取证透明度之间的最优权衡。

Conclusion: 可观测性感知评估能揭示多智能体管道中的非单调效应，记忆增强智能体可在不修改模型权重的情况下同时最大化安全鲁棒性、实时性能、运营成本和环境可持续性。

Abstract: Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.

</details>


### [87] [Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues](https://arxiv.org/abs/2601.13206)
*Neil K. R. Sehgal,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.AI

TL;DR: LLMs在实时谈判中缺乏时间意识，提供剩余时间信息能显著提高交易成功率


<details>
  <summary>Details</summary>
Motivation: 现实世界中的沟通（如治疗、商业谈判）都受连续时间约束，而当前LLM架构和评估很少测试其在实时截止期限下的时间意识

Method: 使用模拟谈判实验，对比两种条件：控制组（仅知全局时间限制）和时间感知组（每回合接收剩余时间更新），测试不同LLM在时间敏感环境中的表现

Result: 时间感知条件下交易成功率大幅提高（GPT-5.1从4%提升到32%），报价接受率提高六倍；但在回合制限制下LLM能达到≥95%的交易成功率，表明问题在于时间跟踪而非战略推理

Conclusion: LLM普遍缺乏时间意识，这限制了其在许多时间敏感应用中的部署，需要改进LLM的时间跟踪能力

Abstract: Large Language Models (LLMs) generate text token-by-token in discrete time, yet real-world communication, from therapy sessions to business negotiations, critically depends on continuous time constraints. Current LLM architectures and evaluation protocols rarely test for temporal awareness under real-time deadlines. We use simulated negotiations between paired agents under strict deadlines to investigate how LLMs adjust their behavior in time-sensitive settings. In a control condition, agents know only the global time limit. In a time-aware condition, they receive remaining-time updates at each turn. Deal closure rates are substantially higher (32\% vs. 4\% for GPT-5.1) and offer acceptances are sixfold higher in the time-aware condition than in the control, suggesting LLMs struggle to internally track elapsed time. However, the same LLMs achieve near-perfect deal closure rates ($\geq$95\%) under turn-based limits, revealing the failure is in temporal tracking rather than strategic reasoning. These effects replicate across negotiation scenarios and models, illustrating a systematic lack of LLM time awareness that will constrain LLM deployment in many time-sensitive applications.

</details>


### [88] [RAG: A Random-Forest-Based Generative Design Framework for Uncertainty-Aware Design of Metamaterials with Complex Functional Response Requirements](https://arxiv.org/abs/2601.13233)
*Bolin Chen,Dex Doksoo Lee,Wei "Wayne'' Chen,Wei Chen*

Main category: cs.AI

TL;DR: 提出RAG方法：基于随机森林的生成式设计框架，用于高效逆设计具有功能响应的超材料，解决高维函数响应、数据稀缺和不确定性量化等挑战。


<details>
  <summary>Details</summary>
Motivation: 超材料设计中需要逆设计非线性、条件依赖的功能响应（如应力-应变关系、色散关系），现有方法主要针对向量值响应，而功能响应的逆设计面临高维度、设计需求复杂、解不存在或不唯一等挑战。生成式方法虽有效但通常需要大量数据，且缺乏不确定性量化。

Method: 提出RAndom-forest-based Generative approach (RAG)：利用随机森林的小数据兼容性高效预测高维功能响应；通过集成方法估计似然度来量化生成设计的可信度；使用单次设计生成处理一对多映射问题；在声学超材料（部分通带/阻带）和力学超材料（目标snap-through响应）上验证。

Result: 使用500个样本实现声学超材料设计，1057个样本实现力学超材料设计；在公开非线性应力-应变关系数据集上，相比神经网络表现出更好的数据效率；框架能处理复杂设计需求，提供可信的生成设计。

Conclusion: RAG为涉及功能响应、昂贵仿真和复杂设计需求的逆设计问题提供了轻量级、可信赖的解决方案，不仅适用于超材料，还可扩展到其他领域。

Abstract: Metamaterials design for advanced functionality often entails the inverse design on nonlinear and condition-dependent responses (e.g., stress-strain relation and dispersion relation), which are described by continuous functions. Most existing design methods focus on vector-valued responses (e.g., Young's modulus and bandgap width), while the inverse design of functional responses remains challenging due to their high-dimensionality, the complexity of accommodating design requirements in inverse-design frameworks, and non-existence or non-uniqueness of feasible solutions. Although generative design approaches have shown promise, they are often data-hungry, handle design requirements heuristically, and may generate infeasible designs without uncertainty quantification. To address these challenges, we introduce a RAndom-forest-based Generative approach (RAG). By leveraging the small-data compatibility of random forests, RAG enables data-efficient predictions of high-dimensional functional responses. During the inverse design, the framework estimates the likelihood through the ensemble which quantifies the trustworthiness of generated designs while reflecting the relative difficulty across different requirements. The one-to-many mapping is addressed through single-shot design generation by sampling from the conditional likelihood. We demonstrate RAG on: 1) acoustic metamaterials with prescribed partial passbands/stopbands, and 2) mechanical metamaterials with targeted snap-through responses, using 500 and 1057 samples, respectively. Its data-efficiency is benchmarked against neural networks on a public mechanical metamaterial dataset with nonlinear stress-strain relations. Our framework provides a lightweight, trustworthy pathway to inverse design involving functional responses, expensive simulations, and complex design requirements, beyond metamaterials.

</details>


### [89] [CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning](https://arxiv.org/abs/2601.13262)
*Eric Onyame,Akash Ghosh,Subhadip Baidya,Sriparna Saha,Xiuying Chen,Chirag Agarwal*

Main category: cs.AI

TL;DR: 提出CURE-MED框架，通过课程强化学习提升LLM在多语言医疗推理中的逻辑正确性和语言一致性，在13种语言上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单语言数学和常识推理上表现良好，但在多语言医疗推理应用中仍不可靠，阻碍了在多语言医疗环境中的部署。

Method: 首先构建CUREMED-BENCH多语言医疗推理数据集，然后提出CURE-MED框架，结合代码切换感知的监督微调和组相对策略优化，通过课程强化学习提升逻辑正确性和语言稳定性。

Result: 在13种语言上，该方法持续优于强基线并有效扩展：7B参数模型达到85.21%语言一致性和54.35%逻辑正确性；32B参数模型达到94.96%语言一致性和70.04%逻辑正确性。

Conclusion: CURE-MED框架支持LLM实现可靠且公平的多语言医疗推理，代码和数据集已开源。

Abstract: While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/

</details>


### [90] [Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops](https://arxiv.org/abs/2601.13268)
*Zainab Ghafoor,Md Shafiqul Islam,Koushik Howlader,Md Rasel Khondokar,Tanusree Bhattacharjee,Sayantan Chakraborty,Adrito Roy,Ushashi Bhattacharjee,Tirtho Roy*

Main category: cs.AI

TL;DR: 提出多智能体精炼框架，通过结构化迭代对齐提升医疗大语言模型的安全性和可靠性，结合生成模型和评估智能体，在900个临床查询上实现89%伦理违规减少和92%风险降级率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域应用日益广泛，但其伦理完整性和安全合规性仍是临床部署的主要障碍，需要建立有效的安全治理机制。

Method: 采用多智能体精炼框架，结合两个生成模型（DeepSeek R1和Med-PaLM）和两个评估智能体（LLaMA 3.1和Phi-4），使用美国医学会医学伦理原则和五级安全风险评估协议进行结构化迭代对齐。

Result: 在900个涵盖九个伦理领域的临床查询上评估，DeepSeek R1收敛更快（平均2.34 vs 2.67次迭代），Med-PaLM在隐私敏感场景处理更优，迭代多智能体循环实现89%伦理违规减少和92%风险降级率。

Conclusion: 该研究提出了一种可扩展、符合监管要求且成本效益高的医疗AI安全治理范式，为医疗大语言模型的临床部署提供了有效的安全增强方案。

Abstract: Large Language Models (LLMs) are increasingly applied in healthcare, yet ensuring their ethical integrity and safety compliance remains a major barrier to clinical deployment. This work introduces a multi-agent refinement framework designed to enhance the safety and reliability of medical LLMs through structured, iterative alignment. Our system combines two generative models - DeepSeek R1 and Med-PaLM - with two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association's (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. We evaluate performance across 900 clinically diverse queries spanning nine ethical domains, measuring convergence efficiency, ethical violation reduction, and domain-specific risk behavior. Results demonstrate that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations), while Med-PaLM shows superior handling of privacy-sensitive scenarios. The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate, underscoring the effectiveness of our approach. This study presents a scalable, regulator-aligned, and cost-efficient paradigm for governing medical AI safety.

</details>


### [91] [PepEDiff: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion](https://arxiv.org/abs/2601.13327)
*Po-Yu Liang,Tobo Duran,Jun Bai*

Main category: cs.AI

TL;DR: PepEDiff是一个新型的肽结合剂生成器，直接在预训练蛋白质嵌入模型的连续潜在空间中生成结合序列，无需结构预测，提高了序列多样性。


<details>
  <summary>Details</summary>
Motivation: 现有肽结合剂生成方法严重依赖中间结构预测，增加了复杂性并限制了序列多样性。需要一种不依赖结构预测的直接生成方法。

Method: 使用预训练蛋白质嵌入模型的连续潜在空间，通过潜在空间探索和基于扩散的采样，直接生成肽结合序列，无需结构预测。

Result: 在TIGIT等挑战性靶点上，PepEDiff在基准测试中优于现有最先进方法，展示了作为零样本肽结合剂设计的通用、无结构框架的潜力。

Conclusion: PepEDiff提供了一种简单有效的零样本肽结合剂设计方法，不依赖结构预测，能够生成超越已知结合剂分布的新颖肽序列。

Abstract: We present PepEDiff, a novel peptide binder generator that designs binding sequences given a target receptor protein sequence and its pocket residues. Peptide binder generation is critical in therapeutic and biochemical applications, yet many existing methods rely heavily on intermediate structure prediction, adding complexity and limiting sequence diversity. Our approach departs from this paradigm by generating binder sequences directly in a continuous latent space derived from a pretrained protein embedding model, without relying on predicted structures, thereby improving structural and sequence diversity. To encourage the model to capture binding-relevant features rather than memorizing known sequences, we perform latent-space exploration and diffusion-based sampling, enabling the generation of peptides beyond the limited distribution of known binders. This zero-shot generative strategy leverages the global protein embedding manifold as a semantic prior, allowing the model to propose novel peptide sequences in previously unseen regions of the protein space. We evaluate PepEDiff on TIGIT, a challenging target with a large, flat protein-protein interaction interface that lacks a druggable pocket. Despite its simplicity, our method outperforms state-of-the-art approaches across benchmark tests and in the TIGIT case study, demonstrating its potential as a general, structure-free framework for zero-shot peptide binder design. The code for this research is available at GitHub: https://github.com/LabJunBMI/PepEDiff-An-Peptide-binder-Embedding-Diffusion-Model

</details>


### [92] [The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models](https://arxiv.org/abs/2601.13358)
*Samuel Cyrenius Anderson*

Main category: cs.AI

TL;DR: 研究发现模型规模扩张不会均匀提升推理能力，而是重构推理过程。通过分析25,000+思维链轨迹，发现神经缩放定律触发领域特定的相变：法律推理呈现"结晶化"，科学和数学推理保持"液态"，代码推理形成"晶格"结构。几何结构可预测可学习性，并可用于推理加速。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为模型规模越大，推理能力越强。但本文质疑这种均匀改进的假设，旨在探究规模扩张如何真正影响不同领域的推理过程，揭示其内在的几何结构变化。

Method: 分析25,000+思维链轨迹，覆盖法律、科学、代码、数学四个领域，使用8B和70B两种参数规模的模型。引入神经推理算子（Neural Reasoning Operators）作为从初始到最终隐藏状态的映射，并分析轨迹的几何特性如维度、对齐度、流形解缠等。

Result: 发现领域特定的相变：法律推理维度降低45%，轨迹对齐度增加31%，流形解缠10倍；科学和数学推理几何不变；代码推理形成离散策略模式。神经推理算子在法律推理上通过探针解码达到63.6%准确率。发现跨领域和规模的通用振荡特征（相干性~-0.4）。

Conclusion: 推理成本由流形几何而非任务难度决定。不同领域在规模扩张下呈现不同的几何相变，这为在拓扑允许的情况下加速推理提供了蓝图。研究建立了规模扩张重构而非均匀改进推理的理论框架。

Abstract: Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -> 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -> 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.

</details>


### [93] [A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models: Design, Implementation, and Applications in AgentForge](https://arxiv.org/abs/2601.13383)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: AgentForge是一个轻量级开源Python框架，通过模块化架构简化LLM驱动的自主代理开发，支持技能组合、统一LLM后端和声明式配置，显著减少开发时间。


<details>
  <summary>Details</summary>
Motivation: 现有代理框架存在架构僵化、供应商锁定和复杂性过高的问题，阻碍了快速原型设计和部署，需要一种更灵活、易用的解决方案来民主化自主代理开发。

Method: 提出AgentForge框架，包含三个核心创新：1) 可组合的技能抽象，支持细粒度任务分解和形式化输入输出契约；2) 统一的LLM后端接口，支持云端API和本地推理引擎无缝切换；3) 基于YAML的声明式配置系统，分离代理逻辑与实现细节。将技能组合机制形式化为有向无环图(DAG)。

Result: 在四个基准场景的评估中，AgentForge实现了竞争力的任务完成率，相比LangChain减少62%开发时间，相比直接API集成减少78%开发时间。编排延迟低于100毫秒，适合实时应用。框架集成了六个内置技能，支持自定义技能开发。

Conclusion: AgentForge填补了LLM代理生态系统的关键空白，为研究人员和从业者提供了一个生产就绪的基础设施，用于构建、评估和部署自主代理，同时保持灵活性和性能。

Abstract: The emergence of LLMs has catalyzed a paradigm shift in autonomous agent development, enabling systems capable of reasoning, planning, and executing complex multi-step tasks. However, existing agent frameworks often suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impedes rapid prototyping and deployment. This paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. AgentForge introduces three key innovations: (1) a composable skill abstraction that enables fine-grained task decomposition with formally defined input-output contracts, (2) a unified LLM backend interface supporting seamless switching between cloud-based APIs and local inference engines, and (3) a declarative YAML-based configuration system that separates agent logic from implementation details. We formalize the skill composition mechanism as a directed acyclic graph (DAG) and prove its expressiveness for representing arbitrary sequential and parallel task workflows. Comprehensive experimental evaluation across four benchmark scenarios demonstrates that AgentForge achieves competitive task completion rates while reducing development time by 62% compared to LangChain and 78% compared to direct API integration. Latency measurements confirm sub-100ms orchestration overhead, rendering the framework suitable for real-time applications. The modular design facilitates extension: we demonstrate the integration of six built-in skills and provide comprehensive documentation for custom skill development. AgentForge addresses a critical gap in the LLM agent ecosystem by providing researchers and practitioners with a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance.

</details>


### [94] [Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models](https://arxiv.org/abs/2601.13443)
*Héctor Manuel Manzanilla-Granados,Zaira Navarrete-Cazales,Miriam Pescador-Rojas,Tonahtiu Ramírez-Romero*

Main category: cs.AI

TL;DR: 论文提出"显式认知分配"原则和"认知通用代理"架构，将AI辅助推理分解为不同认知阶段，通过"通用认知工具"形式化各种研究手段，相比传统LLM推理具有更好的可追溯性、认知控制和可重复性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的使用方式在认知上缺乏结构，将问题框架、知识探索、检索、方法意识和解释等认知功能压缩到单一生成过程中，这限制了可追溯性、削弱了认知控制、破坏了可重复性，特别是在高责任场景中。

Method: 提出"显式认知分配"原则，并实例化为"认知通用代理"架构。该架构将推理组织为探索与框架、认知锚定、工具与方法映射、解释性合成等不同阶段。核心是"通用认知工具"概念，形式化了计算、实验、组织、监管和教育等异质手段。

Result: 在农业领域的多个提示下，CUA推理表现出更早且结构化的认知收敛、在语义扩展下更高的认知对齐度，并系统性地揭示了研究的工具性景观。相比之下，基线LLM推理在对齐度上表现出更大的变异性，且未能明确展现工具结构。

Conclusion: 显式认知分配和工具分配能够显著改善AI辅助推理的质量，特别是在需要可追溯性、认知控制和可重复性的高责任场景中。CUA架构通过结构化认知过程和形式化研究工具，提供了更可靠、更透明的推理框架。

Abstract: The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings.
  We introduce Explicit Cognitive Allocation, a general principle for structuring AI-assisted inference through the explicit separation and orchestration of epistemic functions. We instantiate this principle in the Cognitive Universal Agent (CUA), an architecture that organizes inference into distinct stages of exploration and framing, epistemic anchoring, instrumental and methodological mapping, and interpretive synthesis. Central to this framework is the notion of Universal Cognitive Instruments (UCIs), which formalize heterogeneous means, including computational, experimental, organizational, regulatory, and educational instruments, through which abstract inquiries become investigable.
  We evaluate the effects of explicit cognitive and instrumental allocation through controlled comparisons between CUA-orchestrated inference and baseline LLM inference under matched execution conditions. Across multiple prompts in the agricultural domain, CUA inference exhibits earlier and structurally governed epistemic convergence, higher epistemic alignment under semantic expansion, and systematic exposure of the instrumental landscape of inquiry. In contrast, baseline LLM inference shows greater variability in alignment and fails to explicitly surface instrumental structure.

</details>


### [95] [SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation](https://arxiv.org/abs/2601.13462)
*Amine Rostane*

Main category: cs.AI

TL;DR: SpatialBench-UC：一个用于评估文本到图像模型空间关系理解能力的小型可复现基准，包含200个提示，通过选择性预测和人类校准来提供可靠评估。


<details>
  <summary>Details</summary>
Motivation: 评估文本到图像模型是否遵循明确的空间指令难以自动化，因为对象检测器可能漏检或返回多个可能检测，简单的几何测试在边界情况下变得模糊。需要一种选择性预测方法，在证据不足时能够弃权，并报告置信度。

Method: 引入SpatialBench-UC基准，包含200个提示（50个对象对×4种关系），分为100个反事实对。提供版本化提示、固定配置、每样本检查器输出和报告表格。包含轻量级人工审核来校准检查器的弃权边界和置信度阈值。

Result: 评估了三个基线模型：Stable Diffusion 1.5、SD 1.5 BoxDiff和SD 1.4 GLIGEN。结果显示，接地方法显著提高了通过率和覆盖率，但弃权仍然是主要因素，主要由于检测缺失。

Conclusion: SpatialBench-UC为空间关系评估提供了可复现、可审计的基准框架，选择性预测方法能更好地解释模型性能，接地技术能改善空间指令遵循能力。

Abstract: Evaluating whether text-to-image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, the checker may abstain when evidence is weak and report confidence so that results can be interpreted as a risk coverage tradeoff rather than a single score. We introduce SpatialBench-UC, a small, reproducible benchmark for pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs times 4 relations) grouped into 100 counterfactual pairs obtained by swapping object roles. We release a benchmark package, versioned prompts, pinned configs, per-sample checker outputs, and report tables, enabling reproducible and auditable comparisons across models. We also include a lightweight human audit used to calibrate the checker's abstention margin and confidence threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as conditional pass rates on decided samples. The results show that grounding methods substantially improve both pass rate and coverage, while abstention remains a dominant factor due mainly to missing detections.

</details>


### [96] [Context and Transcripts Improve Detection of Deepfake Audios of Public Figures](https://arxiv.org/abs/2601.13464)
*Chongyang Gao,Marco Postiglione,Julian Baldwin,Natalia Denisenko,Isabel Gortner,Luke Fosdick,Chiara Pulice,Sarit Kraus,V. S. Subrahmanian*

Main category: cs.AI

TL;DR: 提出基于上下文的音频深度伪造检测器CADD，通过结合上下文信息和转录文本，显著提升检测性能，并对抗攻击具有更强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人类利用上下文判断信息真伪，但现有音频深度伪造检测器仅分析音频文件，忽略了上下文和转录文本的重要信息。

Method: 创建记者提供的深度伪造数据集JDD和合成音频数据集SYN，提出CADD架构，利用上下文和转录文本信息，在多个数据集上进行评估。

Result: 上下文和转录文本能显著提升检测性能：F1分数提升5%-37.58%，AUC提升3.77%-42.79%，EER提升6.17%-47.83%。CADD对5种对抗攻击策略具有更强鲁棒性，性能下降平均仅-0.71%。

Conclusion: 上下文和转录文本信息对音频深度伪造检测至关重要，CADD架构通过有效利用这些信息，显著提升了检测性能和对抗攻击鲁棒性。

Abstract: Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection (access restricted during review).

</details>


### [97] [Graph Neural Networks are Heuristics](https://arxiv.org/abs/2601.13465)
*Yimeng Min,Carla P. Gomes*

Main category: cs.AI

TL;DR: 单个训练轨迹可将图神经网络转化为组合优化的无监督启发式算法，用于旅行商问题，无需搜索或监督即可直接生成解


<details>
  <summary>Details</summary>
Motivation: 探索图神经网络是否能在无监督、无搜索的情况下直接作为组合优化的启发式算法，重新定义学习在组合优化中的角色

Method: 将全局结构约束作为归纳偏置编码到非自回归模型中，通过前向传播直接生成解；推理时使用dropout和快照集成作为隐式集成提升解多样性

Result: 图神经网络无需监督训练或显式搜索即可有效工作，能够内化全局组合结构并作为强大的学习启发式算法

Conclusion: 学习在组合优化中的角色应从增强经典算法转变为直接实例化新的启发式算法

Abstract: We demonstrate that a single training trajectory can transform a graph neural network into an unsupervised heuristic for combinatorial optimization. Focusing on the Travelling Salesman Problem, we show that encoding global structural constraints as an inductive bias enables a non-autoregressive model to generate solutions via direct forward passes, without search, supervision, or sequential decision-making. At inference time, dropout and snapshot ensembling allow a single model to act as an implicit ensemble, reducing optimality gaps through increased solution diversity. Our results establish that graph neural networks do not require supervised training nor explicit search to be effective. Instead, they can internalize global combinatorial structure and function as strong, learned heuristics. This reframes the role of learning in combinatorial optimization: from augmenting classical algorithms to directly instantiating new heuristics.

</details>


### [98] [Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement](https://arxiv.org/abs/2601.13481)
*Jian Zhang,Zhangqi Wang,Zhiyuan Wang,Weiping Fu,Yu He,Haiping Zhu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: APOLO是一个自动化提示优化框架，通过多智能体协作机制改进LLM在心理健康领域的情感诊断准确性和鲁棒性，解决情感共病和临床线索探索效率问题。


<details>
  <summary>Details</summary>
Motivation: 心理健康领域的情感识别对临床分诊、风险评估和及时干预至关重要。虽然LLM在情感分析任务中表现出强大的泛化能力，但在高风险、上下文密集的医疗环境中，其诊断可靠性对提示设计高度敏感。现有方法面临情感共病（多种交织情感状态使预测复杂化）和临床相关线索探索效率低下两大挑战。

Method: 提出APOLO框架，将指令优化建模为部分可观察马尔可夫决策过程，采用包含规划者、教师、批评者、学生和目标角色的多智能体协作机制。规划者定义优化轨迹，教师-批评者-学生智能体迭代优化提示以增强推理稳定性和有效性，目标智能体基于性能评估决定是否继续优化。

Result: 实验结果表明，APOLO在领域特定和分层基准测试中持续提高诊断准确性和鲁棒性，为心理健康领域可信赖的LLM应用提供了一个可扩展和可泛化的范式。

Conclusion: APOLO通过系统探索更广泛和更细粒度的提示空间，提高了心理健康情感诊断的效率和鲁棒性，为解决情感共病和临床线索探索效率问题提供了有效解决方案，展示了在医疗保健领域可信赖LLM应用的可扩展范式。

Abstract: Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.

</details>


### [99] [AgenticRed: Optimizing Agentic Systems for Automated Red-teaming](https://arxiv.org/abs/2601.13518)
*Jiayi Yuan,Jonathan Nöther,Natasha Jaques,Goran Radanović*

Main category: cs.AI

TL;DR: AgenticRed：一种利用LLM上下文学习自动设计和优化红队系统的框架，无需人工干预，显著提升攻击成功率


<details>
  <summary>Details</summary>
Motivation: 现有自动红队方法依赖人工设计的工作流程，存在人为偏见且探索设计空间成本高昂，需要一种能自动设计红队系统的方法

Method: 将红队视为系统设计问题，利用LLM的上下文学习迭代设计和优化红队系统，采用进化选择方法演化智能体系统

Result: 在Llama-2-7B上达到96%攻击成功率（提升36%），在Llama-3-8B上达到98%，在GPT-3.5-Turbo和GPT-4o-mini上达到100%，在Claude-Sonnet-3.5上达到60%（提升24%）

Conclusion: 自动化系统设计是AI安全评估的强大范式，能够跟上快速发展的模型步伐，为红队测试提供了更有效的自动化解决方案

Abstract: While recent automated red-teaming methods show promise for systematically exposing model vulnerabilities, most existing approaches rely on human-specified workflows. This dependence on manually designed workflows suffers from human biases and makes exploring the broader design space expensive. We introduce AgenticRed, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. Rather than optimizing attacker policies within predefined structures, AgenticRed treats red-teaming as a system design problem. Inspired by methods like Meta Agent Search, we develop a novel procedure for evolving agentic systems using evolutionary selection, and apply it to the problem of automatic red-teaming. Red-teaming systems designed by AgenticRed consistently outperform state-of-the-art approaches, achieving 96% attack success rate (ASR) on Llama-2-7B (36% improvement) and 98% on Llama-3-8B on HarmBench. Our approach exhibits strong transferability to proprietary models, achieving 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on Claude-Sonnet-3.5 (24% improvement). This work highlights automated system design as a powerful paradigm for AI safety evaluation that can keep pace with rapidly evolving models.

</details>


### [100] [Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models](https://arxiv.org/abs/2601.13533)
*Changshuo Zhang*

Main category: cs.AI

TL;DR: EGLR模型通过熵引导的潜在推理机制，在生成式重排序中实现"边推理边推荐"，利用动态温度调整和上下文感知推理token来适应列表生成中的难度变化，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式重排序方法难以适应列表生成过程中模型难度的动态熵变化，无法准确捕捉复杂偏好。受语言模型推理能力启发，需要引入推理机制来降低决策熵并提升推荐准确性。

Method: 提出熵引导潜在推理(EGLR)模型：1)采用"边推理边推荐"范式，在生成过程中实时推理；2)使用上下文感知推理token和动态温度调整实现变长推理；3)轻量级集成设计，无需复杂独立模块或后处理。

Result: 在两个真实世界数据集上的实验验证了模型有效性，EGLR能够与现有生成式重排序模型兼容并提升其性能，同时展示了实际部署价值和研究潜力。

Conclusion: EGLR模型通过熵引导的潜在推理机制，成功解决了生成式重排序中动态熵变化的适应问题，实现了更精确的探索-利用权衡，为推荐系统提供了有效的轻量级增强方案。

Abstract: Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model's decision-making process. Based on these findings, we introduce the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three core advantages. First, it abandons the "reason first, recommend later" paradigm to achieve "reasoning while recommending", specifically designed for the high-difficulty nature of list generation by enabling real-time reasoning during generation. Second, it implements entropy-guided variable-length reasoning using context-aware reasoning token alongside dynamic temperature adjustment, expanding exploration breadth in reasoning and boosting exploitation precision in recommending to achieve a more precisely adapted exploration-exploitation trade-off. Third, the model adopts a lightweight integration design with no complex independent modules or post-processing, enabling easy adaptation to existing models. Experimental results on two real-world datasets validate the model's effectiveness, and its notable advantage lies in being compatible with existing generative re-ranking models to enhance their performance. Further analyses also demonstrate its practical deployment value and research potential.

</details>


### [101] [TruthTensor: Evaluating LLMs Human Imitation through Prediction Market Drift and Holistic Reasoning](https://arxiv.org/abs/2601.13545)
*Shirin Shahabi,Spencer Graham,Haruna Isah*

Main category: cs.AI

TL;DR: TruthTensor是一个新颖的、可复现的评估范式，用于评估大型语言模型在真实世界不确定性、分布偏移和社会化高熵环境中的表现，超越传统静态基准测试。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试无法捕捉真实世界的不确定性、分布偏移以及孤立任务准确性与人类对齐决策之间的差距，需要一种更全面的评估框架来评估LLM作为人类模仿系统在复杂环境中的表现。

Method: 基于前瞻性、无污染任务构建评估框架，锚定到实时预测市场，结合概率评分提供模型行为的整体视图。补充传统正确性指标，包含漂移中心诊断和显式鲁棒性检查，明确人类与自动化评估角色、标注协议和统计测试程序。

Result: 在500多个真实市场（政治、经济、文化、技术）的实验中，TruthTensor显示具有相似预测准确性的模型在校准、漂移和风险敏感性方面存在显著差异，突显了需要从多个维度（准确性、校准、叙事稳定性、成本和资源效率）评估模型。

Conclusion: TruthTensor将现代评估最佳实践操作化，包括清晰的假设框架、谨慎的指标选择、透明的计算/成本报告、人在环验证和开放的版本化评估合同，为LLM在真实世界决策环境中的表现提供可辩护的评估。

Abstract: Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures Large Language Models (LLMs) not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to live prediction markets and combines probabilistic scoring to provide a holistic view of model behavior. TruthTensor complements traditional correctness metrics with drift-centric diagnostics and explicit robustness checks for reproducibility. It specify human vs. automated evaluation roles, annotation protocols, and statistical testing procedures to ensure interpretability and replicability of results. In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, underscoring the need to evaluate models along multiple axes (accuracy, calibration, narrative stability, cost, and resource efficiency). TruthTensor therefore operationalizes modern evaluation best practices, clear hypothesis framing, careful metric selection, transparent compute/cost reporting, human-in-the-loop validation, and open, versioned evaluation contracts, to produce defensible assessments of LLMs in real-world decision contexts. We publicly release TruthTensor at https://truthtensor.com

</details>


### [102] [ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution](https://arxiv.org/abs/2601.13546)
*Hui Sun,Chang Xu,Haonan Xie,Hao Li,Yuhao Huang,Chuheng Zhang,Ming Jin,Xiaoguang Liu,Gang Wang,Jiang Bian*

Main category: cs.AI

TL;DR: 该论文提出TSEvol多智能体时间序列演化算法、TSEData-20K数据集、ChatAD系列聊天机器人、TKTO优化方法以及LLADBench基准，显著提升时间序列异常检测的推理能力和跨任务泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的异常检测方法存在推理能力不足、多轮对话能力欠缺、泛化范围狭窄等问题，需要提升对时间序列异常行为的理解和解释能力。

Method: 1) 提出TSEvol多智能体时间序列演化算法；2) 构建TSEData-20K数据集并开发ChatAD系列聊天机器人；3) 提出TKTO优化方法增强跨任务泛化；4) 建立LLADBench基准评估系统。

Result: ChatAD模型在准确率提升34.50%、F1分数提升34.71%、误报率降低37.42%；通过TKTO优化后，在分类、预测和插补任务上展现出竞争力的推理和跨任务泛化性能。

Conclusion: 该研究通过多智能体算法、专用数据集、优化方法和评估基准的完整框架，显著提升了LLM在时间序列异常检测中的推理、对话和泛化能力。

Abstract: LLM-driven Anomaly Detection (AD) helps enhance the understanding and explanatory abilities of anomalous behaviors in Time Series (TS). Existing methods face challenges of inadequate reasoning ability, deficient multi-turn dialogue capability, and narrow generalization. To this end, we 1) propose a multi-agent-based TS Evolution algorithm named TSEvol. On top of it, we 2) introduce the AD reasoning and multi-turn dialogue Dataset TSEData-20K and contribute the Chatbot family for AD, including ChatAD-Llama3-8B, Qwen2.5-7B, and Mistral-7B. Furthermore, 3) we propose the TS Kahneman-Tversky Optimization (TKTO) to enhance ChatAD's cross-task generalization capability. Lastly, 4) we propose a LLM-driven Learning-based AD Benchmark LLADBench to evaluate the performance of ChatAD and nine baselines across seven datasets and tasks. Our three ChatAD models achieve substantial gains, up to 34.50% in accuracy, 34.71% in F1, and a 37.42% reduction in false positives. Besides, via KTKO, our optimized ChatAD achieves competitive performance in reasoning and cross-task generalization on classification, forecasting, and imputation.

</details>


### [103] [Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis](https://arxiv.org/abs/2601.13558)
*Mehrab Beikzadeh,Chenglin Hong,Cory J Cascalheira,Callisto Boka,Majid Sarrafzadeh,Ian W Holloway*

Main category: cs.AI

TL;DR: 利用社交媒体和约会应用文本数据，通过机器学习模型预测男男性行为者的性风险行为、酒精使用和PrEP使用情况


<details>
  <summary>Details</summary>
Motivation: 男男性行为者面临性传播感染和有害饮酒的高风险，社交媒体和约会应用文本数据可能为个性化公共卫生干预提供新机会

Method: 收集参与者文本数据，使用ChatGPT嵌入、BERT嵌入、LIWC和基于词典的风险术语方法训练机器学习模型

Result: 模型在预测月度酗酒和超过5个性伴侣方面表现良好（F1分数0.78），在预测PrEP使用和重度饮酒方面表现中等（F1分数0.64和0.63）

Conclusion: 社交媒体和约会应用文本数据能提供风险和保护行为的宝贵见解，基于大语言模型的方法有潜力支持针对男男性行为者的可扩展个性化公共卫生干预

Abstract: Men who have sex with men (MSM) are at elevated risk for sexually transmitted infections and harmful drinking compared to heterosexual men. Text data collected from social media and dating applications may provide new opportunities for personalized public health interventions by enabling automatic identification of risk and protective behaviors. In this study, we evaluated whether text from social media and dating apps can be used to predict sexual risk behaviors, alcohol use, and pre-exposure prophylaxis (PrEP) uptake among MSM. With participant consent, we collected textual data and trained machine learning models using features derived from ChatGPT embeddings, BERT embeddings, LIWC, and a dictionary-based risk term approach. The models achieved strong performance in predicting monthly binge drinking and having more than five sexual partners, with F1 scores of 0.78, and moderate performance in predicting PrEP use and heavy drinking, with F1 scores of 0.64 and 0.63. These findings demonstrate that social media and dating app text data can provide valuable insights into risk and protective behaviors and highlight the potential of large language model-based methods to support scalable and personalized public health interventions for MSM.

</details>


### [104] [AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent](https://arxiv.org/abs/2601.13559)
*Sun Hui,Ding Yanfeng,Huidong Ma,Chang Xu,Keyan Jin,Lizheng Zu,Cheng Zhong,xiaoguang Liu,Gang Wang,Wentong Cai*

Main category: cs.AI

TL;DR: AgentGC：首个基于智能体的进化式基因组数据无损压缩系统，通过三层架构和多智能体设计，在压缩比和吞吐量上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的基因组数据压缩方法存在不可进化、低层次建模、适应性有限和用户界面不友好等问题，需要更智能的解决方案。

Method: 提出AgentGC三层架构：1）用户层通过Leader智能体结合LLM提供友好界面；2）认知层由Leader驱动，整合LLM进行算法-数据集-系统联合优化；3）压缩层由Worker智能体执行基于多知识学习的自动压缩/解压框架。支持三种模式：CP（压缩比优先）、TP（吞吐量优先）、BM（平衡模式）。

Result: 在9个数据集上与14个基线方法比较：平均压缩比提升16.66%、16.11%、16.33%；吞吐量提升4.73倍、9.23倍、9.15倍（分别对应三种模式）。

Conclusion: AgentGC是首个基于智能体的进化式基因组数据压缩器，通过多智能体架构和LLM集成，有效解决了现有方法的局限性，在压缩性能和用户体验方面均有显著提升。

Abstract: Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker. Specifically, the 1) User layer provides a user-friendly interface via Leader combined with LLM; 2) Cognitive layer, driven by the Leader, integrates LLM to consider joint optimization of algorithm-dataset-system, addressing the issues of low-level modeling and limited adaptability; and 3) Compression layer, headed by Worker, performs compression & decompression via a automated multi-knowledge learning-based compression framework. On top of AgentGC, we design 3 modes to support diverse scenarios: CP for compression-ratio priority, TP for throughput priority, and BM for balanced mode. Compared with 14 baselines on 9 datasets, the average compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains are 4.73x, 9.23x, and 9.15x, respectively.

</details>


### [105] [Reasoning is a Modality](https://arxiv.org/abs/2601.13562)
*Zhiguang Liu,Yi Shang*

Main category: cs.AI

TL;DR: 论文提出一种角色分离的Transformer架构，将推理作为独立模态，在ARC视觉推理任务上超越人类平均表现


<details>
  <summary>Details</summary>
Motivation: 现代AI系统（如LLMs和ViTs）主要作为行为序列预测机器运行，通过建模token统计来匹配可观察行为，缺乏持久、可读的内心状态。这与人类行为存在差距：人类可以通过解码内部状态来解释行为，而AI系统只能产生未基于此类状态的流畅事后合理化。作者假设推理是一种模态：推理应作为独立于规则应用的低级工作空间的独立通道存在。

Method: 设计了新颖的角色分离Transformer块，将全局控制器token与网格工作空间token分离，实现迭代规则执行。该方法在VARC视觉中心协议下进行训练和评估，将推理作为视觉推理问题来解决ARC任务。

Result: 在ARC-1上达到62.6%的准确率，超过了人类平均表现（60.2%），并显著优于先前方法。定性分析显示，与密集ViT基线相比，模型展现出更一致的规则应用结构，从概率块向控制器驱动的推理转变。

Conclusion: 研究支持推理作为独立模态的假设，角色分离的Transformer架构能够实现更类似人类的抽象推理能力，为AI系统实现可解释的内部状态和真正的推理能力提供了新方向。

Abstract: The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.

</details>


### [106] [SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System](https://arxiv.org/abs/2601.13581)
*Heedou Kim,Changsik Kim,Sanghwa Shin,Jaewoo Kang*

Main category: cs.AI

TL;DR: ScriptMind是一个基于大语言模型的诈骗检测框架，通过犯罪脚本推理、数据集构建和认知模拟评估，显著提升诈骗检测性能并增强用户认知警觉性。


<details>
  <summary>Details</summary>
Motivation: 传统诈骗检测方法难以应对日益个性化的多轮社交工程诈骗，而大语言模型在诈骗检测中的认知辅助潜力尚未充分探索。

Method: 提出ScriptMind框架，包含三个组件：犯罪脚本推理任务（CSIT）用于诈骗推理，犯罪脚本感知推理数据集（CSID）用于微调小型LLM，以及基于认知模拟的社交工程防御评估（CSED）用于评估实时认知影响。使用571个韩国电话诈骗案例构建了22,712个结构化诈骗序列训练实例。

Result: 经过ScriptMind微调的11B小型LLM在检测准确率上比GPT-4o高出13%，在检测准确性、误报减少、诈骗者话语预测和推理质量方面优于商业模型。在电话诈骗模拟实验中，显著提升并维持了用户的怀疑水平，增强了他们对诈骗的认知意识。

Conclusion: ScriptMind代表了向以人为中心、认知自适应的大语言模型诈骗防御迈出的一步，展示了LLM在增强人类认知警觉性方面的潜力。

Abstract: Social engineering scams increasingly employ personalized, multi-turn deception, exposing the limits of traditional detection methods. While Large Language Models (LLMs) show promise in identifying deception, their cognitive assistance potential remains underexplored. We propose ScriptMind, an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script-Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean phone scam cases, we built 22,712 structured scammer-sequence training instances. Experimental results show that the 11B small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. Moreover, in phone scam simulation experiments, it significantly enhanced and sustained users' suspicion levels, improving their cognitive awareness of scams. ScriptMind represents a step toward human-centered, cognitively adaptive LLMs for scam defense.

</details>


### [107] [Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification](https://arxiv.org/abs/2601.13589)
*HyeYoung Lee*

Main category: cs.AI

TL;DR: 提出基于音频情感信号的多智能体AI系统，实时生成响应式媒体内容，通过安全验证确保内容适龄可控


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别研究主要关注分类准确性，但缺乏将推断出的情感状态转化为安全、适龄、可控的响应内容的能力。需要一种能够实时生成情感响应媒体内容并确保安全性的系统

Method: 采用四智能体协作架构：1) CNN情感识别智能体提取音频特征；2) 响应策略决策智能体映射情感到响应模式；3) 内容参数生成智能体产生媒体控制参数；4) 安全验证智能体强制执行适龄和刺激约束。引入显式安全验证循环在输出前过滤生成内容

Result: 在公共数据集上达到73.2%情感识别准确率，89.4%响应模式一致性，100%安全合规性，同时保持低于100ms的推理延迟，适合设备端部署

Conclusion: 该模块化架构实现了可解释性和可扩展性，适用于儿童相关媒体、治疗应用和情感响应智能设备，为情感驱动的实时媒体内容生成提供了安全可控的解决方案

Abstract: This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.

</details>


### [108] [DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems](https://arxiv.org/abs/2601.13591)
*Maojun Sun,Yifei Xie,Yue Wu,Ruijian Han,Binyan Jiang,Defeng Sun,Yancheng Yuan,Jian Huang*

Main category: cs.AI

TL;DR: DSAEval是一个包含641个真实世界数据科学问题的基准测试，基于285个多样化数据集，涵盖结构化和非结构化数据，具有多模态环境感知、多查询交互和多维评估三大特点。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的数据代理旨在自动化数据科学任务，但真实世界数据科学问题的开放性、跨分类和缺乏标准答案的特性给评估带来了重大挑战。

Method: 提出DSAEval基准测试，包含641个真实世界数据科学问题，基于285个多样化数据集，涵盖结构化和非结构化数据（如视觉和文本）。该基准具有三大特点：多模态环境感知、多查询交互和多维评估。

Result: 评估了11个先进的代理LLM，结果显示Claude-Sonnet-4.5整体性能最强，GPT-5.2最有效率，MiMo-V2-Flash最具成本效益。多模态感知在视觉相关任务上能带来2.04%到11.30%的性能提升。

Conclusion: 当前数据科学代理在结构化数据和常规数据分析工作流上表现良好，但在非结构化领域仍面临重大挑战。研究提供了关键见解并概述了未来研究方向。

Abstract: Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.

</details>


### [109] [Foundations of Global Consistency Checking with Noisy LLM Oracles](https://arxiv.org/abs/2601.13600)
*Paul He,Elke Kirschbaum,Shiva Kasiviswanathan*

Main category: cs.AI

TL;DR: 提出一种自适应分治算法，用于检测和定位自然语言事实集合中的全局不一致性，通过识别最小不一致子集和最小修复方案，在多项式查询复杂度下解决LLM判断噪声问题。


<details>
  <summary>Details</summary>
Motivation: 自然语言事实集合的全局一致性对于事实核查、摘要和知识库构建等任务至关重要。虽然大语言模型可以评估小规模事实子集的一致性，但其判断存在噪声，且成对检查无法保证全局一致性。验证全局一致性在最坏情况下需要指数级查询次数，因此需要更高效的算法。

Method: 提出自适应分治算法：1）识别最小不一致子集（MUSes）；2）可选地通过命中集计算最小修复方案。该方法具有低阶多项式查询复杂度，能够有效处理LLM判断的噪声问题。

Result: 在合成和真实LLM评估器上的实验表明，该方法能够高效检测和定位不一致性，为基于LLM的语言一致性验证提供了可扩展框架。算法在多项式查询复杂度下运行，相比最坏情况的指数级查询有显著改进。

Conclusion: 该研究为自然语言事实集合的全局一致性验证提供了实用解决方案，通过自适应分治算法在多项式时间内识别最小不一致子集和修复方案，解决了LLM判断噪声和可扩展性问题，为事实核查等应用提供了有效工具。

Abstract: Ensuring that collections of natural-language facts are globally consistent is essential for tasks such as fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient to guarantee global coherence. We formalize this problem and show that verifying global consistency requires exponentially many oracle queries in the worst case. To make the task practical, we propose an adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) of facts and optionally computes minimal repairs through hitting-sets. Our approach has low-degree polynomial query complexity. Experiments with both synthetic and real LLM oracles show that our method efficiently detects and localizes inconsistencies, offering a scalable framework for linguistic consistency verification with LLM-based evaluators.

</details>


### [110] [Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning](https://arxiv.org/abs/2601.13632)
*Zhiming Xue,Sichen Zhao,Yalun Qi,Xianling Zeng,Zihan Yu*

Main category: cs.AI

TL;DR: 提出RADR框架，结合时空图神经网络与组合优化，通过预测拥堵风险实现动态路径规划，在保持运输距离仅增加2.1%的情况下降低19.3%的拥堵风险暴露。


<details>
  <summary>Details</summary>
Motivation: 电子商务快速发展给物流网络带来巨大压力，传统静态路由策略难以应对交通拥堵和零售需求波动，需要更智能的动态路由方案。

Method: 1) 使用空间聚类方法从离散GPS数据构建物流拓扑图；2) 采用GCN+GRU混合深度学习模型提取时空特征预测未来拥堵风险；3) 将预测结果集成到动态边权重机制中进行路径规划。

Result: 在Smart Logistics Dataset 2024上评估，RADR算法显著增强供应链韧性。在高拥堵场景下，潜在拥堵风险暴露降低19.3%，运输距离仅增加2.1%。

Conclusion: 提出的数据驱动方法能有效平衡配送效率和运营安全，为物流网络提供更智能的动态路由解决方案。

Abstract: With the rapid development of the e-commerce industry, the logistics network is experiencing unprecedented pressure. The traditional static routing strategy most time cannot tolerate the traffic congestion and fluctuating retail demand. In this paper, we propose a Risk-Aware Dynamic Routing(RADR) framework which integrates Spatiotemporal Graph Neural Networks (ST-GNN) with combinatorial optimization. We first construct a logistics topology graph by using the discrete GPS data using spatial clustering methods. Subsequently, a hybrid deep learning model combining Graph Convolutional Network (GCN) and Gated Recurrent Unit (GRU) is adopted to extract spatial correlations and temporal dependencies for predicting future congestion risks. These prediction results are then integrated into a dynamic edge weight mechanism to perform path planning. We evaluated the framework on the Smart Logistics Dataset 2024, which contains real-world Internet of Things(IoT) sensor data. The experimental results show that the RADR algorithm significantly enhances the resilience of the supply chain. Particularly in the case study of high congestion scenarios, our method reduces the potential congestion risk exposure by 19.3% while only increasing the transportation distance by 2.1%. This empirical evidence confirms that the proposed data-driven approach can effectively balance delivery efficiency and operational safety.

</details>


### [111] [Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue](https://arxiv.org/abs/2601.13687)
*Zhichao Liang,Satoshi Nakamura*

Main category: cs.AI

TL;DR: SocialMindChange：一个从追踪心理状态转向改变心理状态的动态心智理论基准，要求模型在社交互动中通过对话改变他人心理状态轨迹


<details>
  <summary>Details</summary>
Motivation: 现有动态心智理论基准大多让语言模型处于被动角色，仅读取场景并报告心理状态变化。现实中，心智理论也用于主动行动：说话者计划说什么来改变他人的心理状态轨迹以实现目标。需要从追踪心理转向改变心理的基准。

Method: 提出SocialMindChange基准，每个实例定义包含4个角色的社交情境和5个相连场景。模型扮演一个角色，在五个场景中生成对话以达到目标，同时保持与所有参与者不断变化的状态一致。使用结构化四步框架构建了1200个社交情境，覆盖6000个场景和超过90000个问题，每个都经过真实性和质量验证。

Result: 对10个最先进的LLM进行评估，结果显示它们的平均性能比人类性能低54.2%。这个差距表明当前LLM在长期相连的互动中维持和改变心理状态表征方面仍然存在困难。

Conclusion: SocialMindChange基准揭示了当前LLM在主动使用心智理论改变他人心理状态方面的显著不足，为评估模型在动态社交互动中的能力提供了新方向。

Abstract: Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.

</details>


### [112] [Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games](https://arxiv.org/abs/2601.13709)
*Christopher Kao,Vanshika Vats,James Davis*

Main category: cs.AI

TL;DR: GPT-4o在社交推理游戏《黑手党》中比人类更擅长欺骗，通过异步多智能体框架模拟真实社交环境，检测器对LLM游戏的预测准确率低于人类游戏。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在自然语言社交环境中的欺骗能力，弥补以往研究多关注受控任务而缺乏对真实社交场景中LLM欺骗行为的了解。

Method: 使用异步多智能体框架模拟35场《黑手党》游戏，用GPT-4-Turbo构建"黑手党检测器"分析游戏记录（不含角色信息）来预测黑手党玩家，以预测准确率作为欺骗质量的替代指标。

Result: 检测器对LLM游戏的预测准确率低于人类游戏，且结果在不同游戏天数和检测到的黑手党数量上保持一致，表明LLM能更好地融入群体，欺骗效果更佳。

Conclusion: LLM在社交环境中的欺骗能力相当成熟，这既展示了其复杂性也带来了风险，研究同时发布了LLM《黑手党》游戏记录数据集以供未来研究。

Abstract: Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.

</details>


### [113] [Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection](https://arxiv.org/abs/2601.13735)
*Hojin Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: 研究发现当前基于概率的置信度指标无法有效捕捉推理步骤间的因果依赖关系，主要反映表面流畅度而非逻辑结构


<details>
  <summary>Details</summary>
Motivation: 挑战当前普遍假设：高置信度等于高推理质量。研究概率置信度指标是否能真正捕捉推理步骤间的因果依赖关系

Method: 引入三类步间因果扰动，系统性地破坏推理步骤间的依赖关系但保持局部流畅度；提出对比因果度量方法，显式隔离步间因果依赖

Result: 即使严重干预（如硬注意力掩码阻止模型关注先前推理步骤），选择性能下降也很小；当前概率度量对逻辑结构不敏感，主要捕捉表面流畅度

Conclusion: 需要更可靠的推理质量评估指标；提出的对比因果度量比现有概率方法产生更忠实的输出选择

Abstract: Probabilistic confidence metrics are increasingly adopted as proxies for reasoning quality in Best-of-N selection, under the assumption that higher confidence reflects higher reasoning fidelity. In this work, we challenge this assumption by investigating whether these metrics truly capture inter-step causal dependencies necessary for valid reasoning. We introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, we find that selection accuracy degrades only marginally under these disruptions. Even severe interventions, such as applying hard attention masks that directly prevent the model from attending to prior reasoning steps, do not substantially reduce selection performance. These findings provide strong evidence that current probabilistic metrics are largely insensitive to logical structure, and primarily capture surface-level fluency or in-distribution priors instead. Motivated by this gap, we propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, and demonstrate that it yields more faithful output selection than existing probability-based approaches.

</details>


### [114] [Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering](https://arxiv.org/abs/2601.13752)
*Chak Tou Leong,Dingwei Chen,Heming Xia,Qingyu Yin,Sunbowen Lee,Jian Wang,Wenjie Li*

Main category: cs.AI

TL;DR: 提出RELIEF框架，通过调整大推理模型的自我概念来塑造其行为，无需推理轨迹监督，降低训练成本


<details>
  <summary>Details</summary>
Motivation: 大推理模型存在计算冗余和推理不忠实的问题，现有基于强化学习或黄金推理轨迹微调的方法计算成本高且难以扩展

Method: 发现大推理模型具有潜在推理信念，可通过简单logit探测捕获；提出RELIEF框架，通过微调合成的自我反思问答对来将模型自我概念与目标信念蓝图对齐

Result: 在效率和忠实性任务上，RELIEF匹配或优于行为监督和基于偏好的基线方法，同时训练成本更低；分析验证改变模型推理信念能有效塑造其实际行为

Conclusion: RELIEF提供了一种简单有效的框架，通过调整模型自我概念来塑造大推理模型行为，无需昂贵的推理轨迹监督，具有更好的可扩展性

Abstract: Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.

</details>


### [115] [DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution](https://arxiv.org/abs/2601.13761)
*Shengda Fan,Xuyan Ye,Yankai Lin*

Main category: cs.AI

TL;DR: DARC是一个两阶段自演化解耦框架，通过难度校准的问题生成和不对称自蒸馏机制，解决了自演化的优化不稳定问题，在多个推理基准上平均提升10.9分。


<details>
  <summary>Details</summary>
Motivation: 现有自演框架存在优化不稳定问题：(1) 提问者依赖求解器反馈的非平稳目标，(2) 求解器使用自生成伪标签导致的引导误差。需要稳定自演化过程。

Method: 两阶段框架：第一阶段训练提问者基于明确难度级别和外部语料合成难度校准的问题；第二阶段训练求解器使用不对称自蒸馏机制，文档增强的教师生成高质量伪标签来监督无文档访问的学生求解器。

Result: DARC是模型无关的，在九个推理基准和三个骨干模型上平均提升10.9分，持续优于所有基线，接近完全监督模型的性能且无需人工标注。

Conclusion: DARC通过解耦和不对称设计有效稳定了自演化过程，实现了无需人工标注的自改进AI，代码已开源。

Abstract: Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.

</details>


### [116] [Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance](https://arxiv.org/abs/2601.13770)
*Mostapha Benhenda*

Main category: cs.AI

TL;DR: 提出了Look-Ahead-Bench基准，用于评估金融LLM中的前瞻性偏差，通过实际场景测试发现标准LLM存在显著前瞻偏差，而PiT模型随规模增大表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过问答测试LLM的内部前瞻知识，缺乏对实际金融工作流程中模型行为的评估，需要建立标准化基准来衡量金融LLM中的时间偏差。

Method: 创建标准化基准，在现实金融场景中评估模型行为，通过分析不同时间市场制度下的性能衰减来区分真实预测能力和记忆性能，引入多个量化基线建立性能阈值。

Result: 评估显示标准LLM（Llama 3.1和DeepSeek 3.2）存在显著前瞻偏差（alpha衰减），而PiT-Inference模型（Pitinf-Small/Medium/Large）随规模增大展现出更好的泛化和推理能力。

Conclusion: 该工作为金融LLM时间偏差的标准化评估奠定了基础，提供了识别适合实际部署模型的实用框架，代码已在GitHub开源。

Abstract: We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench

</details>


### [117] [Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments](https://arxiv.org/abs/2601.13846)
*Glinskaya Maria*

Main category: cs.AI

TL;DR: 本文提出Virtual Urbanism (VU)框架，通过AI生成城市合成副本量化城市身份，在东京九个区域进行试点研究，验证了方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏计算可处理的城市身份量化指标，需要开发AI驱动的分析框架来评估城市身份，为城市规划和分析提供新工具。

Method: 使用Stable Diffusion和LoRA模型生成东京九个区域的合成城市序列，排除现有方向标记以提取核心身份元素，通过人类评估实验验证副本感知合法性、量化区域身份、识别核心身份形成元素。

Result: 合成副本的平均识别准确率约81%，验证了副本有效性；城市身份水平(UIL)指标能评估不同区域的身份水平；语义分析揭示了文化嵌入的类型学作为核心身份形成元素。

Conclusion: VU框架是AI增强城市分析的可行方法，为自动化、多参数身份指标开辟了路径，具有城市研究和规划的应用潜力。

Abstract: This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.

</details>


### [118] [LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health](https://arxiv.org/abs/2601.13880)
*Ye Tian,Zihao Wang,Onat Gungor,Xiaoran Fan,Tajana Rosing*

Main category: cs.AI

TL;DR: LifeAgentBench是一个大规模QA基准测试，用于评估LLM在长时程、跨维度、多用户生活方式健康推理方面的能力，包含22,573个问题，并提出了LifeAgent作为强基线代理。


<details>
  <summary>Details</summary>
Motivation: 个性化数字健康支持需要对异构生活方式信号进行长时程、跨维度推理，但当前LLM在此场景下的能力尚不明确，缺乏系统性基准测试。

Method: 引入LifeAgentBench基准测试，包含可扩展的构建流程和标准化评估协议；提出LifeAgent代理，集成多步证据检索与确定性聚合。

Result: 系统评估了11个领先LLM，识别了长时程聚合和跨维度推理的关键瓶颈；LifeAgent相比两个广泛使用的基线取得了显著改进。

Conclusion: LifeAgentBench为可靠、可扩展评估LLM健康助手提供了基准，LifeAgent展示了在现实日常场景中的潜力，基准已公开可用。

Abstract: Personalized digital health support requires long-horizon, cross-dimensional reasoning over heterogeneous lifestyle signals, and recent advances in mobile sensing and large language models (LLMs) make such support increasingly feasible. However, the capabilities of current LLMs in this setting remain unclear due to the lack of systematic benchmarks. In this paper, we introduce LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. We release an extensible benchmark construction pipeline and a standardized evaluation protocol to enable reliable and scalable assessment of LLM-based health assistants. We then systematically evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. Motivated by these findings, we propose LifeAgent as a strong baseline agent for health assistant that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines. Case studies further demonstrate its potential in realistic daily-life scenarios. The benchmark is publicly available at https://anonymous.4open.science/r/LifeAgentBench-CE7B.

</details>


### [119] [Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems](https://arxiv.org/abs/2601.13887)
*Hong Su*

Main category: cs.AI

TL;DR: 提出人类模拟计算（HSC）框架，将智能建模为包含思考、行动、学习、反思和活动调度的闭环过程，强调主动参与和环境交互，以克服大语言模型在真实动态环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型仅依赖文本数据，限制了其在开放动态真实环境中的适应能力、推理结果验证和有效操作。需要更接近人类智能的框架来克服这些限制。

Method: 提出人类模拟计算（HSC）框架，将智能建模为包含思考、行动、学习、反思和活动调度的连续闭环过程。强调主动参与和环境交互，行动不仅用于达成目标，还用于自动改进内部推理机制。整合人类常用思维策略，如主特征导向推理、通过行动扩展范围、环境反馈驱动的即时学习。

Result: 通过理论分析表明，人类模拟策略无法仅从语言材料中完全学习，人类式推理过程和基于行动的推理方法对于在真实环境中实现稳健适应和有效交互至关重要。

Conclusion: HSC框架为解决大语言模型在真实世界环境中的局限性提供了新方向，强调主动参与、环境交互和人类式思维策略的重要性，为实现更强大的人工智能系统奠定了基础。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.

</details>


### [120] [PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation](https://arxiv.org/abs/2601.13904)
*Jaeyoung Moon,Youjin Choi,Yucheon Park,David Melhart,Georgios N. Yannakakis,Kyung-Joong Kim*

Main category: cs.AI

TL;DR: PREFAB是一种低成本回顾式自标注方法，通过检测情感变化点，让标注者只标注关键片段而非全程标注，减轻标注负担


<details>
  <summary>Details</summary>
Motivation: 传统全标注方法耗时、认知负荷高、易疲劳且易出错，需要更高效的标注方法

Method: 基于峰值-终点规则和情感序数表示，使用偏好学习模型检测相对情感变化，标注者只标注选定片段，其余部分插值，并引入预览机制提供上下文线索

Result: PREFAB在建模情感变化方面优于基线方法，减轻工作负担（条件性减轻时间负担），提高标注者信心且不降低标注质量

Conclusion: PREFAB是一种有效的低预算情感标注方法，能准确捕捉情感变化点，同时减少标注负担

Abstract: Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.

</details>


### [121] [Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval](https://arxiv.org/abs/2601.13969)
*Joaquín Polonuer,Lucas Vittor,Iñaki Arango,Ayush Noori,David A. Clifton,Luciano Del Corro,Marinka Zitnik*

Main category: cs.AI

TL;DR: ARK是一个自适应知识图谱检索器，让语言模型通过全局搜索和邻域探索两种操作控制检索的广度-深度权衡，无需训练即可实现多跳推理，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱检索方法存在局限性：基于相似性的检索器覆盖广但深度不足，而基于遍历的方法依赖种子节点选择，当查询涉及多个实体和关系时容易失败。需要一种能自适应平衡广度与深度的检索方法。

Method: ARK采用代理式知识图谱检索器，让语言模型通过两种操作控制检索：1）全局词汇搜索（广度导向），2）一跳邻域探索（深度导向）。这两种操作可组合实现多跳遍历。ARK交替使用这两种操作，无需预设跳数或训练，并能根据查询类型自适应选择工具。

Result: 在STaRK基准上，ARK达到59.1%的平均Hit@1和67.4的平均MRR，比检索基线和无训练代理方法分别提升最多31.4%的Hit@1和28.0%的MRR。通过从大教师模型蒸馏到8B模型，在AMAZON、MAG和PRIME数据集上分别比基础8B模型提升+7.0、+26.6和+13.5个绝对百分点的Hit@1，同时保留教师模型最多98.5%的性能。

Conclusion: ARK通过赋予语言模型对检索广度-深度权衡的控制能力，提供了一种无需训练、自适应查询类型的知识图谱检索方法，显著优于现有方法，并能通过蒸馏有效压缩到较小模型中。

Abstract: Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.

</details>


### [122] [Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics](https://arxiv.org/abs/2601.14027)
*Junqi Liu,Zihao Zhou,Zekai Zhu,Marco Dos Santos,Weikun He,Jiawei Liu,Ran Wang,Yunzhou Xie,Junqiao Zhao,Qiufeng Wang,Lihong Zhi,Jia Li,Wenda Li*

Main category: cs.AI

TL;DR: 提出使用通用编码智能体作为形式数学推理器的新范式，开发了Numina-Lean-Agent系统，在Putnam 2025竞赛中取得12/12全对成绩，并能形式化复杂的Brascamp-Lieb定理。


<details>
  <summary>Details</summary>
Motivation: 现有定理证明系统通常依赖特定任务流水线和训练过的形式证明器，限制了灵活性和可复现性。作者提出使用通用编码智能体作为形式数学推理器，因为：(1)通用编码智能体为超越证明的多样化推理任务提供自然接口；(2)仅通过替换底层基础模型即可提升性能，无需训练；(3)MCP支持灵活扩展和自主调用专业工具，避免复杂设计。

Method: 提出Numina-Lean-Agent系统，结合Claude Code与Numina-Lean-MCP，实现与Lean证明助手的自主交互、相关定理检索、非形式化证明和辅助推理工具调用。系统采用Claude Opus 4.5作为基础模型。

Result: 使用Claude Opus 4.5作为基础模型，Numina-Lean-Agent在Putnam 2025竞赛中解决了所有问题（12/12），与最佳闭源系统表现相当。此外，系统成功与数学家合作形式化了Brascamp-Lieb定理，展示了其通用性。

Conclusion: 证明了使用通用编码智能体作为形式数学推理器的可行性，Numina-Lean-Agent在竞赛和实际数学形式化任务中都表现出色。系统代码和解决方案已开源发布。

Abstract: Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.

</details>


### [123] [Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems](https://arxiv.org/abs/2601.14096)
*Benedikt Hartl,Léo Pio-Lopez,Chris Fields,Michael Levin*

Main category: cs.AI

TL;DR: 论文提出认知的普适框架：通过嵌入空间重映射和迭代误差最小化导航，这一机制在从生物系统到人工智能的各种智能系统中普遍存在。


<details>
  <summary>Details</summary>
Motivation: 寻求一个统一的框架来理解不同起源、组成和基质的智能系统（从亚细胞化学网络到生物群体）中的问题解决机制，探索尺度不变的决策原则。

Method: 提出认知可以通过两个同等重要的不变量来表征：(1)嵌入空间的重映射；(2)在这些空间中的导航。生物系统通过重映射转录、形态、生理或3D空间来维持稳态和再生结构，而AI系统（如transformer、扩散模型）则将数据重映射到潜在嵌入并通过迭代细化。

Result: 识别出"通过迭代误差最小化进行嵌入空间重映射和导航"这一双重原则构成了认知的基质独立不变量，揭示了生物系统与人工模型之间的深层平行关系。

Conclusion: 这一共享机制不仅阐明了生命系统与人工模型之间的深刻相似性，还为跨尺度工程化自适应智能提供了一个统一框架。

Abstract: The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.

</details>


### [124] [Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance](https://arxiv.org/abs/2601.14171)
*Qianli Ma,Chang Guo,Zhiheng Tian,Siyu Wang,Jipeng Xiao,Yuanhao Yue,Zhipeng Zhang*

Main category: cs.AI

TL;DR: RebuttalAgent：首个多智能体框架，将反驳信生成重构为以证据为中心的规划任务，通过分解评审意见、构建混合上下文、集成外部搜索模块，确保每个论点都有明确证据支撑，在覆盖度、忠实度和策略连贯性上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前反驳信生成方法通常将其视为直接文本生成问题，存在幻觉、忽略批评意见、缺乏可验证基础等问题。需要一种能够精确对齐审稿人意图和稿件细节的解决方案。

Method: 提出RebuttalAgent多智能体框架：1) 将复杂反馈分解为原子关注点；2) 动态构建混合上下文，综合压缩摘要和高保真文本；3) 集成自主按需外部搜索模块，解决需要外部文献的关注点；4) 在起草前生成可检查的响应计划。

Result: 在提出的RebuttalBench上验证，RebuttalAgent在覆盖度、忠实度和策略连贯性方面优于强基线方法，为同行评审过程提供了透明可控的助手。

Conclusion: RebuttalAgent通过将反驳信生成重构为证据中心的规划任务，解决了现有方法的局限性，提供了透明、可控且基于证据的同行评审助手，代码将开源。

Abstract: Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.

</details>


### [125] [Toward Efficient Agents: Memory, Tool learning, and Planning](https://arxiv.org/abs/2601.14192)
*Xiaofang Yang,Lijun Li,Heng Zhou,Tong Zhu,Xiaoye Qu,Yuchen Fan,Qianshan Wei,Rui Ye,Li Kang,Yiran Qin,Zhiqiang Kou,Daizong Liu,Qi Li,Ning Ding,Siheng Chen,Jing Shao*

Main category: cs.AI

TL;DR: 该论文系统研究了智能体系统的效率问题，从记忆、工具学习和规划三个核心组件出发，分析了效率优化的方法、评估指标和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型向智能体系统扩展，现有研究大多关注效果提升，而忽略了效率这一实际部署的关键因素。论文旨在填补这一空白，对智能体系统的效率进行全面研究。

Method: 从智能体的三个核心组件（记忆、工具学习、规划）出发，综述了各种效率优化方法，包括上下文压缩与管理、设计最小化工具调用的强化学习奖励、受控搜索机制等。提出了两种效率评估方式：固定成本预算下的效果比较和可比效果水平下的成本比较。

Result: 系统梳理了智能体效率优化的共同原则，总结了各组件评估协议和常用效率指标，建立了效果与成本的帕累托前沿分析框架，为效率导向的基准测试提供了方法论基础。

Conclusion: 智能体效率研究至关重要但尚未得到充分重视。论文为这一领域提供了系统性框架，识别了关键挑战和未来方向，包括更精细的效率优化技术、标准化评估协议和实际部署考量。

Abstract: Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [126] [Age-Based Scheduling for a Memory-Constrained Quantum Switch](https://arxiv.org/abs/2601.11698)
*Stavros Mitrolaris,Subhankar Banerjee,Sennur Ulukus*

Main category: cs.IT

TL;DR: 研究量子交换中多部分纠缠请求的调度问题，提出基于纠缠建立年龄（AoEE）的新度量，开发三种低复杂度调度策略并进行性能比较


<details>
  <summary>Details</summary>
Motivation: 在量子网络中，如何有效调度有限量子内存寄存器中的多部分纠缠请求是一个关键挑战。现有研究缺乏针对概率性链路级纠缠生成、纠缠交换和退相干效应的系统调度方法。

Method: 提出基于纠缠建立年龄（AoEE）的新性能度量，设计两个策略家族并推导其闭式AoEE表达式，优化得到两种策略，再提出第三种低复杂度策略并提供性能保证

Result: 获得了两种优化策略的闭式性能表达式，提出了第三种策略的性能保证，并通过数值模拟比较了三种策略的性能表现

Conclusion: 提出的基于AoEE的调度策略能有效管理量子交换中的多部分纠缠请求，为有限量子内存下的量子网络调度提供了实用解决方案

Abstract: In a time-slotted system, we study the problem of scheduling multipartite entanglement requests in a quantum switch with a finite number of quantum memory registers. Specifically, we consider probabilistic link-level entanglement (LLE) generation for each user, probabilistic entanglement swapping, and one-slot decoherence. To evaluate the performance of the proposed scheduling policies, we introduce a novel age-based metric, coined age of entanglement establishment (AoEE). We consider two families of low-complexity policies for which we obtain closed-form expressions for their corresponding AoEE performance. Optimizing over each family, we obtain two policies. Further, we propose one more low-complexity policy and provide its performance guarantee. Finally, we numerically compare the performance of the proposed policies.

</details>


### [127] [Asymptotically Optimal Tests for One- and Two-Sample Problems](https://arxiv.org/abs/2601.11727)
*Arick Grootveld,Biao Chen,Venkata Gandikota*

Main category: cs.IT

TL;DR: 本文重新审视了单样本和双样本检验问题，为Hoeffding似然比检验的渐近最优性提供了更简洁的证明，并将其扩展到双样本检验中。


<details>
  <summary>Details</summary>
Motivation: 重新研究单样本和双样本检验问题，其中单样本检验中一个分布未知，双样本检验中两个分布都未知。旨在为Hoeffding检验提供更直观的证明并扩展到双样本场景。

Method: 为单样本检验提供了Hoeffding似然比检验渐近最优性的更简洁证明，该检验等价于经验分布与名义分布之间相对熵的阈值检验。然后将类似方法扩展到双样本检验，使用两个经验分布之间相对熵的阈值检验。

Result: 证明了单样本Hoeffding检验的渐近最优性，并证明双样本检验中类似形式的Hoeffding检验也是渐近最优的。同时获得了双样本检验的强逆定理。

Conclusion: 本文为Hoeffding检验提供了更直观的渐近最优性证明框架，成功将其扩展到双样本检验，并建立了相应的强逆定理，为二元假设检验提供了理论保证。

Abstract: In this work, we revisit the one- and two-sample testing problems: binary hypothesis testing in which one or both distributions are unknown. For the one-sample test, we provide a more streamlined proof of the asymptotic optimality of Hoeffding's likelihood ratio test, which is equivalent to the threshold test of the relative entropy between the empirical distribution and the nominal distribution. The new proof offers an intuitive interpretation and naturally extends to the two-sample test where we show that a similar form of Hoeffding's test, namely a threshold test of the relative entropy between the two empirical distributions is also asymptotically optimal. A strong converse for the two-sample test is also obtained.

</details>


### [128] [The Noisy Quantitative Group Testing Problem](https://arxiv.org/abs/2601.11797)
*Tenghao Li,Neha Sangwan,Xiaxin Li,Arya Mazumdar*

Main category: cs.IT

TL;DR: 论文研究了定量群体检测问题，分析了三种模型（无噪声、加性高斯噪声、噪声Z信道）和两种算法（基于相关分数的线性估计器、最小二乘估计器），推导了精确恢复所需测试次数的上界，并给出了信息论下界。


<details>
  <summary>Details</summary>
Motivation: 研究定量群体检测问题在不同噪声模型下的性能，为实际应用中的群体检测提供理论指导。

Method: 分析三种噪声模型：无噪声模型、加性高斯噪声模型、噪声Z信道模型；采用两种算法方法：基于相关分数的线性估计器和最小二乘估计器。

Result: 推导了精确恢复所需测试次数的上界和误差概率趋于零的条件，并给出了信息论下界；在加性高斯噪声设置中，上下界在阶数上匹配。

Conclusion: 为定量群体检测问题提供了系统的理论分析框架，特别是在加性高斯噪声模型下获得了紧致的上下界结果。

Abstract: In this paper, we study the problem of quantitative group testing (QGT) and analyze the performance of three models: the noiseless model, the additive Gaussian noise model, and the noisy Z-channel model. For each model, we analyze two algorithmic approaches: a linear estimator based on correlation scores, and a least squares estimator (LSE). We derive upper bounds on the number of tests required for exact recovery with vanishing error probability, and complement these results with information-theoretic lower bounds. In the additive Gaussian noise setting, our lower and upper bounds match in order.

</details>


### [129] [Bayesian ICA for Causal Discovery](https://arxiv.org/abs/2601.11815)
*Joe Suzuki*

Main category: cs.IT

TL;DR: 提出基于信息论和贝叶斯ICA的因果发现框架，在存在任意混杂的情况下估计因果顺序，将经典LiNGAM方法推广到混杂场景。


<details>
  <summary>Details</summary>
Motivation: 经典LiNGAM方法假设存在一个因果顺序使得噪声项完全独立，但这一强假设在存在混杂时经常被违反。需要开发一个在任意混杂下仍然适用的因果顺序估计框架。

Method: 使用多元互信息量化混杂程度，将其分解为沿因果顺序的互信息项之和。通过贝叶斯边际似然估计互信息，将因果顺序选择建模为排列上的模型选择问题。采用非高斯预测模型（如多元t分布）避免高斯噪声导致的不可识别性。

Result: 提出的贝叶斯互信息估计量具有一致性，冗余度为O(log n)。该方法在无混杂时恢复经典LiNGAM和DirectLiNGAM，在存在混杂时提供因果顺序的合理排序。

Conclusion: 建立了一个统一的、考虑混杂的、基于信息论的ICA因果发现扩展框架，为存在混杂的因果顺序估计提供了理论基础和实用方法。

Abstract: Causal discovery based on Independent Component Analysis (ICA) has achieved remarkable success through the LiNGAM framework, which exploits non-Gaussianity and independence of noise variables to identify causal order. However, classical LiNGAM methods rely on the strong assumption that there exists an ordering under which the noise terms are exactly independent, an assumption that is often violated in the presence of confounding. In this paper, we propose a general information-theoretic framework for causal order estimation that remains applicable under arbitrary confounding. Rather than imposing independence as a hard constraint, we quantify the degree of confounding by the multivariate mutual information among the noise variables. This quantity is decomposed into a sum of mutual information terms along a causal order and is estimated using Bayesian marginal likelihoods. The resulting criterion can be interpreted as Bayesian ICA for causal discovery, where causal order selection is formulated as a model selection problem over permutations. Under standard regularity conditions, we show that the proposed Bayesian mutual information estimator is consistent, with redundancy of order $O(\log n)$. To avoid non-identifiability caused by Gaussian noise, we employ non-Gaussian predictive models, including multivariate $t$ distributions, whose marginal likelihoods can be evaluated via MCMC. The proposed method recovers classical LiNGAM and DirectLiNGAM as limiting cases in the absence of confounding, while providing a principled ranking of causal orders when confounding is present. This establishes a unified, confounding-aware, and information-theoretically grounded extension of ICA-based causal discovery.

</details>


### [130] [On the Rényi Rate-Distortion-Perception Function and Functional Representations](https://arxiv.org/abs/2601.11862)
*Jiahui Wei,Marios Kountouris*

Main category: cs.IT

TL;DR: 将率失真感知(RDP)框架扩展到Rényi信息论体系，利用Sibson α-互信息刻画失真和感知约束下的基本极限。针对标量高斯源推导了Rényi RDP函数的闭式表达式，发现感知约束诱导了再生方差的可行区间。建立了Rényi广义强函数表示引理，揭示了最优函数表示复杂度的相变现象。


<details>
  <summary>Details</summary>
Motivation: 扩展经典率失真感知框架到更一般的Rényi信息论体系，研究在广义互信息度量下的压缩极限，探索感知约束对编码表示复杂度的影响。

Method: 采用Sibson α-互信息作为信息度量工具，推导标量高斯源的Rényi RDP函数闭式解，建立Rényi广义强函数表示引理，分析不同α值下的编码表示特性。

Result: 发现感知约束导致再生方差存在可行区间；揭示了最优函数表示的相变：当0.5<α<1时，编码成本由α+1阶α-散度界定，需要重尾多项式衰减的码本；当α>1时，表示坍缩为有限支撑集。

Conclusion: 该研究将RDP框架推广到Rényi信息论体系，为广义互信息下的共享随机性压缩提供了新的理论见解，揭示了感知约束对编码表示复杂度的深刻影响。

Abstract: We extend the Rate-Distortion-Perception (RDP) framework to the Rényi information-theoretic regime, utilizing Sibson's $α$-mutual information to characterize the fundamental limits under distortion and perception constraints. For scalar Gaussian sources, we derive closed-form expressions for the Rényi RDP function, showing that the perception constraint induces a feasible interval for the reproduction variance. Furthermore, we establish a Rényi-generalized version of the Strong Functional Representation Lemma. Our analysis reveals a phase transition in the complexity of optimal functional representations: for $0.5<α< 1$, the coding cost is bounded by the $α$-divergence of order $α+1$, necessitating a codebook with heavy-tailed polynomial decay; conversely, for $α> 1$, the representation collapses to one with finite support, offering new insights into the compression of shared randomness under generalized notions of mutual information.

</details>


### [131] [Rate-Distortion-Classification Representation Theory for Bernoulli Sources](https://arxiv.org/abs/2601.11919)
*Nam Nguyen,Thinh Nguyen,Bella Bose*

Main category: cs.IT

TL;DR: 研究任务导向的有损压缩，通过率-失真-分类(RDC)表示框架分析伯努利源、汉明失真和二元对称分类模型，推导闭式解并研究通用编码器。


<details>
  <summary>Details</summary>
Motivation: 传统率失真理论主要关注重建质量，但实际应用中常需同时考虑分类等任务性能。本文旨在建立任务导向压缩的理论框架，量化压缩率、失真和分类性能之间的权衡关系。

Method: 采用率-失真-分类(RDC)表示框架，针对伯努利源和汉明失真，推导单次RDC和失真-率-分类(DRC)闭式解；通过线性规划刻画固定表示下的失真-分类区域；研究支持多操作点的通用编码器，推导渐近率上下界。

Result: 获得了RDC和DRC的闭式表征；通过线性规划得到了失真-分类区域的下边界；推导了通用编码器所需最小渐近率的可计算上下界，量化了通用性带来的率惩罚；提供了数值示例验证理论结果。

Conclusion: 建立了任务导向压缩的RDC理论框架，为同时优化压缩率、重建质量和分类性能提供了理论基础，通用编码器的率惩罚分析为实际系统设计提供了指导。

Abstract: We study task-oriented lossy compression through the lens of rate-distortion-classification (RDC) representations. The source is Bernoulli, the distortion measure is Hamming, and the binary classification variable is coupled to the source via a binary symmetric model. Building on the one-shot common-randomness formulation, we first derive closed-form characterizations of the one-shot RDC and the dual distortion-rate-classification (DRC) tradeoffs. We then use a representation-based viewpoint and characterize the achievable distortion-classification (DC) region induced by a fixed representation by deriving its lower boundary via a linear program. Finally, we study universal encoders that must support a family of DC operating points and derive computable lower and upper bounds on the minimum asymptotic rate required for universality, thereby yielding bounds on the corresponding rate penalty. Numerical examples are provided to illustrate the achievable regions and the resulting universal RDC/DRC curves.

</details>


### [132] [Exact Redundancy for Symmetric Rate-Distortion](https://arxiv.org/abs/2601.11927)
*Sharang M. Sriramu,Aaron B. Wagner*

Main category: cs.IT

TL;DR: 本文研究了变长编码在几乎确定失真约束下的冗余度问题，证明了对于满足特定对称条件的均匀源，log n/(2n)是最优可达的冗余度，且即使将失真约束放宽为期望约束也无法改进。


<details>
  <summary>Details</summary>
Motivation: Zhang等人先前的研究表明，对于离散源，在几乎确定失真约束下，冗余度的上界为log n/n，下界（在大多数情况下）为log n/(2n)。本文旨在进一步探究这一界限的紧致性，特别是针对满足对称条件的均匀源，确定精确的最优冗余度。

Method: 针对满足特定对称条件的均匀源，作者通过理论分析证明了log n/(2n)冗余度的可达性。同时，通过构造性证明表明，即使将几乎确定的失真约束放宽为期望失真约束，这一冗余度也无法进一步改进。

Result: 对于满足对称条件的均匀源，log n/(2n)是最优可达的冗余度。这一结果比Zhang等人的下界更精确，并且证明了即使放宽约束条件也无法获得更好的冗余度性能。

Conclusion: 本文确定了在几乎确定失真约束下，对于满足对称条件的均匀源，log n/(2n)是最优的冗余度。这一结果为变长编码的冗余度分析提供了更精确的理论界限，并表明即使放宽约束条件也无法改进这一性能。

Abstract: For variable-length coding with an almost-sure distortion constraint, Zhang et al. show that for discrete sources the redundancy is upper bounded by $\log n/n$ and lower bounded (in most cases) by $\log n/(2n)$, ignoring lower order terms. For a uniform source with a distortion measure satisfying certain symmetry conditions, we show that $\log n/(2n)$ is achievable and that this cannot be improved even if one relaxes the distortion constraint to be in expectation rather than with probability one.

</details>


### [133] [Utilizing the Perceived Age to Maximize Freshness in Query-Based Update Systems](https://arxiv.org/abs/2601.14075)
*Sahan Liyanaarachchi,Sennur Ulukus,Nail Akar*

Main category: cs.IT

TL;DR: 研究在通用延迟分布下，基于查询采样的连续时间马尔可夫链监控最优策略，发现等待策略能显著提升平均二进制新鲜度


<details>
  <summary>Details</summary>
Motivation: 现有基于查询采样的监控研究大多假设查询延迟服从指数分布且反馈瞬时完成，这些假设在实际应用中不现实，需要更通用的延迟分布模型

Method: 放松指数分布和瞬时反馈假设，在通用延迟分布下研究连续时间马尔可夫链监控，提出基于等待的查询采样策略

Result: 通过采用等待策略，在平均二进制新鲜度指标上获得了显著提升

Conclusion: 在更现实的通用延迟分布下，等待策略能有效优化基于查询采样的马尔可夫源监控性能

Abstract: Query-based sampling has become an increasingly popular technique for monitoring Markov sources in pull-based update systems. However, most of the contemporary literature on this assumes an exponential distribution for query delay and often relies on the assumption that the feedback or replies to the queries are instantaneous. In this work, we relax both of these assumptions and find optimal sampling policies for monitoring continuous-time Markov chains (CTMC) under generic delay distributions. In particular, we show that one can obtain significant gains in terms of mean binary freshness (MBF) by employing a waiting based strategy for query-based sampling.

</details>


### [134] [Small-Error Cascaded Group Testing](https://arxiv.org/abs/2601.11945)
*Daniel McMorrow,Nikhil Karamchandani,Sidharth Jaggi*

Main category: cs.IT

TL;DR: 该论文研究级联群测试模型，其中测试结果指示排序中第一个缺陷项，建立了多种恢复准则下的可实现性界限


<details>
  <summary>Details</summary>
Motivation: 传统群测试模型使用二元测试结果（仅指示是否存在缺陷项），而级联群测试模型通过测试排序提供更多信息，可能提高恢复效率

Method: 研究级联群测试模型，其中测试具有排序，测试结果指示该排序中第一个缺陷项。使用非自适应和自适应（少量阶段）测试设计，建立多种恢复准则的可实现性界限

Result: 为级联群测试模型建立了各种可实现性界限，包括不同恢复准则下的性能界限，展示了该模型相对于传统模型的优势

Conclusion: 级联群测试模型通过利用测试排序信息，为缺陷项恢复提供了新的理论框架和性能改进，扩展了群测试的研究范围

Abstract: Group testing concerns itself with the accurate recovery of a set of "defective" items from a larger population via a series of tests. While most works in this area have considered the classical group testing model, where tests are binary and indicate the presence of at least one defective item in the test, we study the cascaded group testing model. In cascaded group testing, tests admit an ordering, and test outcomes indicate the first defective item in the test under this ordering. Under this model, we establish various achievability bounds for several different recovery criteria using both non-adaptive and adaptive (with "few" stages) test designs.

</details>


### [135] [Generalizing the Fano inequality further](https://arxiv.org/abs/2601.12027)
*Raghav Bongole,Tobias J. Oechtering,Mikael Skoglund*

Main category: cs.IT

TL;DR: 提出了一种交互式统计决策的广义信息论下界框架，通过Bernoulli f-散度不等式推导出有界损失变换的两侧区间，特别应用于CVaR风险度量的下界分析


<details>
  <summary>Details</summary>
Motivation: 现有交互式统计决策的信息论下界主要针对期望风险，而对尾部敏感目标（如CVaR）的研究不足，需要更通用的下界框架

Method: 将Chen等人的交互式Fano框架推广，用随机化的一比特统计量替代硬成功事件，建立Bernoulli f-散度不等式，通过反演得到有界变换的两侧区间，特别应用有界铰链变换和Rockafellar-Uryasev表示推导CVaR下界

Result: 获得了有界损失变换的通用下界框架，特别推导出先验预测（贝叶斯）CVaR的下界，对于KL散度和混合参考分布的情况，通过Pinsker不等式得到互信息形式的显式下界

Conclusion: 该框架扩展了交互式统计决策的信息论下界理论，为尾部敏感风险度量提供了理论基础，将传统期望风险下界推广到更一般的风险度量场景

Abstract: Interactive statistical decision making (ISDM) features algorithm-dependent data generated through interaction. Existing information-theoretic lower bounds in ISDM largely target expected risk, while tail-sensitive objectives are less developed. We generalize the interactive Fano framework of Chen et al. by replacing the hard success event with a randomized one-bit statistic representing an arbitrary bounded transform of the loss. This yields a Bernoulli f-divergence inequality, which we invert to obtain a two-sided interval for the transform, recovering the previous result as a special case. Instantiating the transform with a bounded hinge and using the Rockafellar-Uryasev representation, we derive lower bounds on the prior-predictive (Bayesian) CVaR of bounded losses. For KL divergence with the mixture reference distribution, the bound becomes explicit in terms of mutual information via Pinsker's inequality.

</details>


### [136] [Function Computation Over Multiple Access Channels via Hierarchical Constellations](https://arxiv.org/abs/2601.12050)
*Saeed Razavikia,Mohammad Kazemi,Deniz Gündüz,Carlo Fischione*

Main category: cs.IT

TL;DR: 提出基于分层星座设计的空中计算框架，支持单信道使用计算多个函数输出，并引入屏蔽机制减少错误传播，实现低延迟、信道无关的函数计算。


<details>
  <summary>Details</summary>
Motivation: 研究高斯多址信道上的函数计算问题，多个发射机需要在公共接收机处计算其值的函数。现有方法在延迟和可靠性方面存在限制，需要设计更高效的空中计算方案。

Method: 提出基于分层星座设计的编码调制框架，支持单信道使用计算多个函数输出。引入基于变长分组编码的屏蔽机制，减少噪声引起的错误传播，同时保持多址信道的叠加结构。

Result: 所提分层星座方案的计算速率与最优计算速率之间的差距随网络规模增长而消失，渐近达到最优速率。屏蔽机制进一步提高了可靠性，差距以最优速率缩放。

Conclusion: 该工作为低延迟、信道无关的函数计算提供了统一框架，识别了未编码或轻编码空中计算在信息论上最优的机制，在无线网络函数计算方面具有重要理论价值。

Abstract: We study function computation over a Gaussian multiple-access channel (MAC), where multiple transmitters aim at computing a function of their values at a common receiver. To this end, we propose a novel coded-modulation framework for over-the-air computation (OAC) based on hierarchical constellation design, which supports reliable computation of multiple function outputs using a single channel use. Moreover, we characterize the achievable computation rate and show that the proposed hierarchical constellations can compute R output functions with decoding error probability epsilon while the gap to the optimal computation rate scales as O(\log_2(1/ε)/K) for independent source symbols, where K denotes the number of transmitters. Consequently, this gap vanishes as the network size grows, and the optimal rate is asymptotically attained.
  Furthermore, we introduce a shielding mechanism based on variable-length block coding that mitigates noise-induced error propagation across constellation levels while preserving the superposition structure of the MAC. We show that the shielding technique improves reliability, yielding a gap that scales optimally as O(\log_2\ln{(1/ε)}), regardless of the source distribution. Together, these results identify the regimes in which uncoded or lightly coded OAC is information-theoretically optimal, providing a unified framework for low-latency, channel-agnostic function computation.

</details>


### [137] [On the Construction and Correlation Properties of Permutation-Interleaved Zadoff-Chu Sequences](https://arxiv.org/abs/2601.12107)
*Qin Yuan,Chunlei Li,Xiangyong Zeng*

Main category: cs.IT

TL;DR: 本文提出了一种通过置换多项式交织Zadoff-Chu序列生成CAZAC序列的新方法，证明了所得序列与现有序列不等价，并验证了Berggren和Popović猜想的充分性。


<details>
  <summary>Details</summary>
Motivation: 受Berggren和Popović近期工作的启发，本文旨在进一步研究通过置换多项式交织ZC序列来生成CAZAC序列的方法，探索更高阶的置换多项式构造，并验证相关猜想。

Method: 提出了一类在整数环Z_N上的高阶置换多项式，利用这些多项式及其逆来交织ZC序列，从而构造CAZAC序列。同时评估了由二次置换多项式生成的ZC序列的非周期自相关特性。

Result: 证明了所构造的CAZAC序列与ZC序列以及通过二次置换多项式及其逆交织ZC序列得到的序列不等价，验证了Berggren和Popović猜想的充分性。

Conclusion: 本文扩展了通过置换多项式交织ZC序列生成CAZAC序列的方法，提供了高阶置换多项式的构造，证明了所得序列的新颖性，并解决了相关猜想，为雷达和通信系统中的波形设计提供了更多选择。

Abstract: Constant amplitude zero auto-correlation (CAZAC) sequences are widely applied in waveforms for radar and communication systems. Motivated by a recent work [Berggren and Popović, IEEE Trans. Inf. Theory 70(8), 6068-6075 (2024)], this paper further investigates the approach to generating CAZAC sequences by interleaving Zadoff-Chu (ZC) sequences with permutation polynomials (PPs). We propose one class of high-degree PPs over the integer ring Z N , and utilize them and their inverses to interleave ZC sequences for constructing CAZAC sequences. It is known that a CAZAC sequence can be extended to an equivalence class by five basic opertations. We further show that the obtained CAZAC sequences are not covered by the equivalence classes of ZC sequences and interleaved ZC sequences by quadratic PPs and their inverses, and prove the sufficiency of the conjecture by Berggren and Popović in the aforementioned work. In addition, we also evaluate the aperiodic auto-correlation of certain ZC sequences from quadratic PPs.

</details>


### [138] [Coherent Comparison as Information Cost: A Cost-First Ledger Framework for Discrete Dynamics](https://arxiv.org/abs/2601.12194)
*Sebastian Pardo-Guerra,Megan Simons,Anil Thapa,Jonathan Washburn*

Main category: cs.IT

TL;DR: 提出基于比率比较成本的信息论框架，推导出唯一互易成本函数，建立离散账本模型，在图上实现原子性更新和标量势函数。


<details>
  <summary>Details</summary>
Motivation: 为离散动力学建立信息论基础，通过比率比较的成本函数来量化偏离平衡的程度，确保在乘法链下的连贯性，从而构建一致的离散系统理论框架。

Method: 1) 定义比率比较成本函数F(x)，要求满足乘法链的d'Alembert函数方程；2) 通过归一化和二次校准推导出唯一互易成本函数J(x)；3) 将J(x)作为输入构建离散账本模型；4) 在图上实现原子性更新和最小周期；5) 引入时间聚合循环闭合假设，推导标量势函数。

Result: 得到唯一互易成本函数J(x)=½(x+x⁻¹)-1，建立离散账本模型，在确定性更新语义下实现原子性（每tick最多一个事件），在超立方图Q_d上获得2^d-tick最小周期，通过离散Poincaré引理得到唯一标量势函数。

Conclusion: 该框架通过一致性强制的成本结构，将比率散度、保守图流和离散势理论联系起来，为离散动力学提供了坚实的信息论基础，并展示了在超立方图上的具体实现。

Abstract: We develop an information-theoretic framework for discrete dynamics grounded in a comparison-cost functional on ratios. Given two quantities compared via their ratio \(x=a/b\), we assign a cost \(F(x)\) measuring deviation from equilibrium (\(x=1\)). Requiring coherent composition under multiplicative chaining imposes a d'Alembert functional equation; together with normalization (\(F(1)=0\)) and quadratic calibration at unity, this yields a unique reciprocal cost functional (proved in a companion paper): \[ J(x) = \tfrac{1}{2}\bigl(x + x^{-1}\bigr) - 1. \] This cost exhibits reciprocity \(J(x)=J(x^{-1})\), vanishes only at \(x=1\), and diverges at boundary regimes \(x\to 0^+\) and \(x\to\infty\), excluding ``nothingness'' configurations. Using \(J\) as input, we introduce a discrete ledger as a minimal lossless encoding of recognition events on directed graphs. Under deterministic update semantics and minimality (no intra-tick ordering metadata), we derive atomic ticks (at most one event per tick). Explicit structural assumptions (conservation, no sources/sinks, pairwise locality, quantization in \(δ\mathbb{Z}\)) force balanced double-entry postings and discrete ledger units. To obtain scalar potentials on graphs with cycles while retaining single-edge impulses per tick, we impose time-aggregated cycle closure (no-arbitrage/clearing over finite windows). Under this hypothesis, cycle closure is equivalent to path-independence, and the cleared cumulative flow admits a unique scalar potential on each connected component (up to additive constant), via a discrete Poincaré lemma. On hypercube graphs \(Q_d\), atomicity imposes a \(2^d\)-tick minimal period, with explicit Gray-code realization at \(d=3\). The framework connects ratio-based divergences, conservative graph flows, and discrete potential theory through a coherence-forced cost structure.

</details>


### [139] [Classical-Quantum Channel Resolvability Using Matrix Multiplicative Weight Update Algorithm](https://arxiv.org/abs/2601.12230)
*Koki Takahashi,Shun Watanabe*

Main category: cs.IT

TL;DR: 首次使用确定性编码证明经典-量子信道可分辨性，采用矩阵乘法权重更新算法


<details>
  <summary>Details</summary>
Motivation: 现有文献中经典-量子信道可分辨性仅通过随机编码证明，需要探索确定性编码方法

Method: 扩展先前确定性编码方法到经典-量子信道，采用矩阵乘法权重更新算法

Result: 首次实现了经典-量子信道可分辨性的确定性编码证明

Conclusion: 成功将确定性编码方法应用于经典-量子信道可分辨性，为量子信息理论提供了新的编码方法

Abstract: We study classical-quantum (C-Q) channel resolvability. C-Q channel resolvability has been proved by only random coding in the literature. In our previous study, we proved channel resolvability by deterministic coding, using multiplicative weight update algorithm. We extend this approach to C-Q channels and prove C-Q channel resolvability by deterministic coding, using the matrix multiplicative weight update algorithm. This is the first approach to C-Q channel resolvability using deterministic coding.

</details>


### [140] [On the Minimum Length of Functional Batch Codes with Small Recovery Sets](https://arxiv.org/abs/2601.12302)
*Kristiina Oksner,Henk D. L. Hollmann,Ago-Erik Riet,Vitaly Skachek*

Main category: cs.IT

TL;DR: 本文研究了具有低查询复杂度的线性功能批处理码，推导了此类码的最小长度界限，并通过数值计算进行了评估。


<details>
  <summary>Details</summary>
Motivation: 功能批处理码在分布式数据存储系统中具有负载均衡和私有信息检索的潜在应用价值。与标准批处理码只能查询信息符号不同，功能批处理码允许用户查询信息符号的线性组合，这在实际应用中更为灵活。

Method: 考虑具有低查询复杂度的线性功能批处理码，即每个查询仅使用少量编码符号来回答。通过理论分析推导此类码的最小长度界限，并进行数值计算来评估结果。

Result: 获得了线性功能批处理码最小长度的理论界限，并通过数值计算验证了这些界限的有效性，为实际系统设计提供了理论指导。

Conclusion: 本文为具有低查询复杂度的线性功能批处理码建立了理论框架，推导的最小长度界限有助于优化分布式存储系统的性能，在负载均衡和私有信息检索应用中具有实用价值。

Abstract: Batch codes are of potential use for load balancing and private information retrieval in distributed data storage systems. Recently, a special case of batch codes, termed functional batch codes, was proposed in the literature. In functional batch codes, users can query linear combinations of the information symbols, and not only the information symbols themselves, as is the case for standard batch codes. In this work, we consider linear functional batch codes with the additional property that every query is answered by using only a small number of coded symbols. We derive bounds on the minimum length of such codes, and evaluate the results by numerical computations.

</details>


### [141] [$2$-quasi-perfect Lee codes and abelian Ramanujan graphs: a new construction and relationship](https://arxiv.org/abs/2601.12393)
*Shohei Satake*

Main category: cs.IT

TL;DR: 本文构建了新的显式2-准完美Lee码族，长度可任意大，基于Forey等人提出的阿贝尔（几乎）Ramanujan图的生成集，并建立了特定阿贝尔Ramanujan图与Mesnager等人获得的2-准完美Lee码之间的联系。


<details>
  <summary>Details</summary>
Motivation: 研究2-准完美Lee码的显式构造问题，特别是构建长度可任意大的码族，以扩展Lee码的理论和应用范围。

Method: 利用Forey、Fresán、Kowalski和Wigderson提出的阿贝尔（几乎）Ramanujan图的生成集来构造新的2-准完美Lee码族，并建立这些图与Mesnager、Tang和Qi获得的2-准完美Lee码之间的数学关系。

Result: 成功获得了一个新的显式2-准完美Lee码族，其长度可以任意大，为Lee码理论提供了新的构造方法。

Conclusion: 通过阿贝尔Ramanujan图的生成集可以构造出长度可任意大的2-准完美Lee码，建立了图论与编码理论之间的新联系，为Lee码的进一步研究提供了新方向。

Abstract: In this paper, we obtain a new explicit family of $2$-quasi-perfect Lee codes of arbitrarily large length. Our construction is based on generating sets of abelian (almost) Ramanujan graphs obtained by Forey, Fresán, Kowalski and Wigderson. Also, we develop a relationship between certain abelian Ramanujan graphs and $2$-quasi-perfect Lee codes obtained by Mesnager, Tang and Qi.

</details>


### [142] [Privacy via Modulation Rotation and Inter-Symbol Interference](https://arxiv.org/abs/2601.12394)
*Morteza Varasteh,Pegah Sharifi*

Main category: cs.IT

TL;DR: 提出两种物理层机制实现用户侧差分隐私：基于BPSK调制相位旋转和故意引入ISI，利用通信系统固有特性而非显式噪声注入来提供隐私保护。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私机制依赖人工添加噪声，通常带来额外能量或通信成本。本文探索利用通信系统中固有的硬件非理想性和实现误差（如相位旋转、ISI）来提供隐私保证，避免显式数据扰动。

Method: 1. 旋转BPSK方案：在发射端对BPSK调制应用确定性相位旋转，接收端不知旋转角度，通过减小判决距离实现隐私保护。2. ISI方案：故意引入符号间干扰，接收端不知产生ISI的确定性定时偏移，ISI提供的隐私损失取决于输入数据分布。

Result: 数值结果表明：对于固定ISI参数，当二进制输入符号等概率时隐私损失最大。结构化修改（如调制旋转或诱导ISI）本身可提供差分隐私保证，无需显式噪声注入。实际设备非理想性可提供隐私保护而无需额外隐私成本。

Conclusion: 利用通信系统固有特性（相位旋转、ISI等硬件非理想性）可实现用户侧差分隐私，避免传统噪声注入方法的额外成本。实际设备无意引入的旋转或ISI可提供隐私保证，无需显式数据扰动。

Abstract: Two physical-layer mechanisms for achieving user-side differential privacy in communication systems are proposed. Focusing on binary phase-shift keying (BPSK) modulation, differential privacy (DP) is first studied under a deterministic phase rotation applied on the BPSK modulation at the transmitter, while the receiver is assumed to be unaware of the rotation angle. In this setting, privacy is achieved through an effective reduction in the decision distance, resulting in a controlled increase in the bit error rate (BER) without explicit noise injection. Next, a BPSK transmission scheme with intentionally induced inter-symbol interference (ISI) is studied, where the receiver is likewise unaware of the deterministic timing offset that generates the ISI. Unlike the rotated BPSK scheme, the DP obtained via ISI is shown to depend explicitly on the input data distribution. In particular, numerical results demonstrate that, for a fixed ISI parameter, the privacy loss is maximized when the binary input symbols are equiprobable. While conventional DP mechanisms rely on artificially added noise, often incurring additional energy or communication costs, it is shown that structured modifications, such as modulation rotation or induced ISI inherent to realistic communication channels can itself provide DP guarantees. While the analysis focuses on deterministic transmitter modifications unknown to the receiver, it is noted that real-world devices naturally introduce unintentional rotations or ISI due to hardware nonidealities and implementation errors. These effects can therefore provide a level of privacy without requiring explicit noise injection. Hence, it is possible to avoid deliberately perturbing the data, instead leveraging inherent device imperfections to achieve privacy guarantees with no additional privacy cost.

</details>


### [143] [Counterexamples, Constructions, and Nonexistence Results for Optimal Ternary Cyclic Codes](https://arxiv.org/abs/2601.12427)
*Jingjun Bao,Hanlin Zou*

Main category: cs.IT

TL;DR: 该论文研究了最优三元循环码的开放问题，为第三和第四个问题提供了首个反例，并在特定条件下构造了两类最优码，部分解决了第三个问题。同时研究了特定参数下的循环码，对a≡3(mod 4)构造了两类新的最优码，对a≡1(mod 4)揭示了此类码的不存在性约束。


<details>
  <summary>Details</summary>
Motivation: 2013年Ding和Helleseth提出了关于最优三元循环码的九个开放问题，其中前两个和第六个问题已完全解决，但其他问题仍然开放。本文旨在推进第三和第四个问题的研究，并探索特定参数条件下的最优循环码构造。

Method: 1. 为第三和第四个开放问题提供首个反例；2. 在特定条件下构造两类最优码；3. 研究参数e(3^h±1)≡(3^m-a)/2 (mod 3^m-1)且a为奇数的循环码；4. 对a≡3(mod 4)构造新的最优码族；5. 对a≡1(mod 4)分析最优码的不存在性约束。

Result: 1. 成功为第三和第四个开放问题提供了首个反例；2. 在特定条件下构造了两类最优码，部分解决了第三个问题；3. 对a≡3(mod 4)构造了两类新的最优码族，参数为[3^m-1,3^m-1-2m,4]；4. 对a≡1(mod 4)获得了多个最优码不存在的结果，揭示了此类码的约束条件。

Conclusion: 本文显著推进了最优三元循环码开放问题的研究，特别是为第三和第四个问题提供了重要突破。通过构造新的最优码族和揭示不存在性约束，深化了对特定参数条件下循环码性质的理解，为后续研究提供了新的方向。

Abstract: Cyclic codes are an important subclass of linear codes with wide applications in communication systems and data storage systems. In 2013, Ding and Helleseth presented nine open problems on optimal ternary cyclic codes $\mathcal{C}_{(1,e)}$. While the first two and the sixth problems have been fully solved, others remain open. In this paper, we advance the study of the third and fourth open problems by providing the first counterexamples to both and constructing two families of optimal codes under certain conditions, thereby partially solving the third problem. Furthermore, we investigate the cyclic codes $\mathcal{C}_{(1,e)}$ where $e(3^h\pm 1)\equiv\frac{3^m-a}{2}\pmod{3^m-1}$ and $a$ is odd. For $a\equiv 3\pmod{4}$, we present two new families of optimal codes with parameters $[3^m-1,3^m-1-2m,4]$, generalizing known constructions. For $a\equiv 1\pmod{4}$, we obtain several nonexistence results on optimal codes $\mathcal{C}_{(1,e)}$ with the aforementioned parameters revealing the constraints of such codes.

</details>


### [144] [The Origin of the Inaccessible Game](https://arxiv.org/abs/2601.12576)
*Neil D. Lawrence*

Main category: cs.IT

TL;DR: 该论文提出了一种基于信息几何的"不可及游戏"框架，通过最大熵产生和边际熵守恒来研究信息损失动力学。在量子版本中，使用冯·诺依曼熵允许存在具有零联合熵和正边际熵的"原点"状态，并推导了约束梯度流和熵时间参数化。


<details>
  <summary>Details</summary>
Motivation: 经典香农熵框架下无法表示零联合熵但具有正边际熵的状态，这限制了信息损失动力学的研究。论文旨在通过量子信息几何方法克服这一限制，建立更一般的理论框架来研究信息损失过程。

Method: 采用冯·诺依曼熵替代香农熵，在Baez-Fritz-Leinster-Parzygnat范畴框架下构建量子信息几何理论。定义了具有最大混合边际的全局纯态作为"原点"，推导了边际熵守恒下的约束梯度流，并引入了熵时间参数化。

Result: 成功构建了量子不可及游戏框架，允许存在零联合熵和正边际熵的原点状态。推导了约束动力学方程，显示其可分解为对称耗散部分（实现SEA）和可逆幺正演化部分。在吉布斯区域，边际熵守恒约化为非平衡热力学中的固定能量约束。

Conclusion: 量子信息几何框架扩展了经典不可及游戏，允许研究更广泛的信息损失动力学。熵时间参数化解决了边界趋近问题，而边际熵守恒与局部模哈密顿量期望值的守恒等价，为连接信息理论和热力学提供了桥梁。

Abstract: The inaccessible game is an information-geometric framework where dynamics of information loss emerge from maximum entropy production under marginal-entropy conservation.
  We study the game's starting state, the origin. Classical Shannon entropy forbids a representation with zero joint entropy and positive marginal entropies: non-negativity of conditional entropy rules this out. Replacing Shannon with von Neumann entropy within the Baez Fritz Leinster Parzygnat categorical framework removes this obstruction and admits a well-defined origin: a globally pure state with maximally mixed marginals, selected up to local-unitary equivalence. At this LME origin, marginal-entropy conservation becomes a second-order geometric condition. Because the marginal-entropy sum is saturated termwise, the constraint gradient vanishes and first-order tangency is vacuous; admissible directions are selected by the kernel of the constraint Hessian, characterised by the marginal-preserving tangent space.
  We derive the constrained gradient flow in the matrix exponential family and show that, as the origin is approached, the affine time parameter degenerates. This motivates an axiomatically distinguished reparametrisation, entropy time $t$, defined by $dH/dt = c$ for fixed constant $c>0$. In this parametrisation, the infinite affine-time approach to the boundary maps to a finite entropy-time interval. The constrained dynamics split into a symmetric dissipative component realising SEA and a reversible component represented as unitary evolution.
  As in the classical game, marginal-entropy conservation is equivalent to conservation of a sum of local modular Hamiltonian expectations, a state-dependent "modular energy"; in Gibbs regimes where local modular generators become approximately parameter-invariant, this reduces to familiar fixed-energy constraints from nonequilibrium thermodynamics.

</details>


### [145] [Beyond Identification: Computing Boolean Functions via Channels](https://arxiv.org/abs/2601.12640)
*Jingge Zhu,Matthias Frey*

Main category: cs.IT

TL;DR: 研究通信系统中接收端恢复发送端消息的布尔函数值的问题，其中函数对发送端未知但属于已知函数类。推导了计算容量并获得了特定函数类的紧致渐近结果。


<details>
  <summary>Details</summary>
Motivation: 研究当接收端只需要恢复消息的某个布尔函数值（而非完整消息）时，通信系统能达到的容量。这推广了Ahlswede和Dueck的通过信道识别框架，探索在函数计算而非消息传输场景下的通信效率。

Method: 提出计算容量的概念，针对由汉明权重特征化的函数类F，推导可达性和逆定理。通过分析不同函数类的渐近行为，建立m和n之间的渐近关系。

Result: 对于论文中考虑的所有函数类F，获得了紧致的渐近缩放行为结果。具体来说，确定了在不同函数类下，消息长度m相对于码字长度n的最大可达到的渐近增长率。

Conclusion: 该工作建立了布尔函数计算场景下的通信容量理论框架，为特定函数类提供了紧致的渐近结果，推广了传统的信道识别问题，为函数计算通信系统设计提供了理论基础。

Abstract: Consider a point-to-point communication system in which the transmitter holds a binary message of length $m$ and transmits a corresponding codeword of length $n$. The receiver's goal is to recover a Boolean function of that message, where the function is unknown to the transmitter, but chosen from a known class $F$. We are interested in the asymptotic relationship of $m$ and $n$: given $n$, how large can $m$ be (asymptotically), such that the value of the Boolean function can be recovered reliably? This problem generalizes the identification-via-channels framework introduced by Ahlswede and Dueck. We formulate the notion of computation capacity, and derive achievability and converse results for selected classes of functions $F$, characterized by the Hamming weight of functions. Our obtained results are tight in the sense of the scaling behavior for all cases of $F$ considered in the paper.

</details>


### [146] [Explicit Entropic Constructions for Coverage, Facility Location, and Graph Cuts](https://arxiv.org/abs/2601.12724)
*Rishabh Iyer*

Main category: cs.IT

TL;DR: 该论文证明了许多实际应用中常用的单调子模函数（如集合覆盖、设施选址、图割等）可以通过适当构造的随机变量精确实现为香农熵，从而在组合信息测度与经典信息论之间建立了直接桥梁。


<details>
  <summary>Details</summary>
Motivation: 香农熵是信息论的基础，但熵性多拟阵类严格小于所有子模函数类。同时，子模和组合信息测度（SIMs）被提出作为将熵、互信息和条件互信息扩展到一般子模函数的框架，并广泛应用于数据子集选择、主动学习等领域。这引发了一个基本问题：实践中常用的单调子模函数是否具有熵性？

Method: 为广泛使用的多拟阵函数提供显式的熵构造，包括集合覆盖和覆盖函数、设施选址、饱和覆盖、通过截断的凹-模函数，以及单调图割型目标。证明这些函数可以通过适当构造的随机变量精确实现为香农熵。

Result: 成功证明了这些常用子模函数具有熵性，即可以精确实现为香农熵。这意味着对于这些函数，子模互信息与经典互信息一致，条件增益特化为条件熵，子模条件互信息简化为标准条件互信息。

Conclusion: 该研究为许多最常见的子模目标建立了组合信息测度与经典信息论之间的直接桥梁，确认了实践中广泛使用的单调子模函数确实具有熵性，从而统一了这两个领域的重要概念。

Abstract: Shannon entropy is a polymatroidal set function and lies at the foundation of information theory, yet the class of entropic polymatroids is strictly smaller than the class of all submodular functions. In parallel, submodular and combinatorial information measures (SIMs) have recently been proposed as a principled framework for extending entropy, mutual information, and conditional mutual information to general submodular functions, and have been used extensively in data subset selection, active learning, domain adaptation, and representation learning. This raises a natural and fundamental question: are the monotone submodular functions most commonly used in practice entropic?
  In this paper, we answer this question in the affirmative for a broad class of widely used polymatroid functions. We provide explicit entropic constructions for set cover and coverage functions, facility location, saturated coverage, concave-over-modular functions via truncations, and monotone graph-cut-type objectives. Our results show that these functions can be realized exactly as Shannon entropies of appropriately constructed random variables. As a consequence, for these functions, submodular mutual information coincides with classical mutual information, conditional gain specializes to conditional entropy, and submodular conditional mutual information reduces to standard conditional mutual information in the entropic sense. These results establish a direct bridge between combinatorial information measures and classical information theory for many of the most common submodular objectives used in applications.

</details>


### [147] [Extended Gabidulin-Kronecker Product Codes and Their Application to Cryptosystems](https://arxiv.org/abs/2601.12780)
*Zhe Sun,Terry Shue Chien Lau,Mengying Zhao,Zimeng Zhou,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 论文研究了具有Kronecker乘积结构的扩展Gabidulin码，提出了三种增强的RQC密码系统变体，建立了Gabidulin-Kronecker乘积码的最小秩距离精确界，引入了新的可解码秩度量码（EGK码）及其解码算法。


<details>
  <summary>Details</summary>
Motivation: 现有基于秩的密码系统存在公钥尺寸大和存在解密失败概率的问题。需要开发新的秩度量码结构来改进RQC密码系统的效率和安全性，同时确保零解密失败概率。

Method: 1. 建立Gabidulin-Kronecker乘积码的最小秩距离精确界；2. 提出扩展Gabidulin-Kronecker乘积（EGK）码及其直接恢复码字而不恢复错误向量的高效解码算法；3. 基于EGK码设计三种增强的RQC密码系统变体。

Result: 1. 获得了Gabidulin-Kronecker乘积码的最小秩距离精确界，发现了一类新的最大秩距离（MRD）码；2. EGK码解码算法在纠错能力内实现零解码失败概率；3. 三种RQC变体在128位安全级别下显著减小公钥尺寸，同时确保零解密失败概率。

Conclusion: 论文提出的EGK码和增强RQC密码系统为基于秩的密码学提供了新的工具，在安全性和效率之间提供了更好的权衡，特别是解决了现有方案中公钥尺寸大和解密失败的问题。

Abstract: In this paper, we initiate the study of Extended Gabidulin codes with a Kronecker product structure and propose three enhanced variants of the Rank Quasi-Cyclic (RQC) (Melchor et.al., IEEE IT, 2018) cryptosystem. First, we establish precise bounds on the minimum rank distance of Gabidulin-Kronecker product codes under two distinct parameter regimes. Specifically, when $n_{1}=k_{1}$ and $n_{2}=m<n_{1}n_{2}$, the minimum rank distance is exactly $n_{2}-k_{2}+1$. This yields a new family of Maximum Rank Distance (MRD) codes, which are distinct from classical Gabidulin codes. For the case of $k_{1}\leq n_{1},k_{2}\leq n_{2},n_{1}n_{2}\leq m$, the minimum rank distance $d$ of Gabidulin-Kronecker product codes satisfies a tight upper and lower bound, i.e., $n_{2}-k_{2}+1 \leq d \leq (n_{1}-k_{1}+1)(n_{2}-k_{2}+1)$. Second, we introduce a new class of decodable rank-metric codes, namely Extended Gabidulin-Kronecker product (EGK) codes, which generalize the structure of Gabidulin-Kronecker product (GK) codes. We also propose a decoding algorithm that directly retrieves the codeword without recovering the error vector, thus improving efficiency. This algorithm achieves zero decoding failure probability when the error weight is within its correction capability. Third, we propose three enhanced variants of the RQC cryptosystem based on EGK codes, each offering a distinct trade-off between security and efficiency. For 128-bit security, all variants achieve significant reductions in public key size compared to the Multi-UR-AG (Bidoux et.al., IEEE IT, 2024) while ensuring zero decryption failure probability--a key security advantage over many existing rank-based schemes.

</details>


### [148] [Joint Source-Channel-Generation Coding: From Distortion-oriented Reconstruction to Semantic-consistent Generation](https://arxiv.org/abs/2601.12808)
*Tong Wu,Zhiyong Chen,Guo Lu,Li Song,Feng Yang,Meixia Tao,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出JSCGC新范式，从确定性重建转向概率生成，利用接收端生成模型作为生成器而非解码器，在信道约束下直接最大化互信息，显著提升感知质量和语义保真度。


<details>
  <summary>Details</summary>
Motivation: 传统通信系统依赖香农率失真理论，但通用失真度量无法捕捉复杂的人类视觉感知，常导致模糊或不真实的图像重建。需要从确定性重建转向更符合人类感知的生成方法。

Method: 提出联合源-信道-生成编码(JSCGC)范式，在接收端使用生成模型作为生成器而非传统解码器，参数化数据分布，在信道约束下直接最大化互信息，通过控制随机采样产生位于真实数据流形上的高保真输出。

Result: 推导了给定传输互信息下最大语义不一致的理论下界，阐明了控制生成过程的通信基本极限。图像传输实验表明，JSCGC显著提升了感知质量和语义保真度，大幅优于传统的失真导向JSCC方法。

Conclusion: JSCGC通过将通信目标从确定性重建转向概率生成，利用生成模型作为接收端生成器，能够产生更符合人类感知的高质量输出，为通信系统设计提供了新范式。

Abstract: Conventional communication systems, including both separation-based coding and AI-driven joint source-channel coding (JSCC), are largely guided by Shannon's rate-distortion theory. However, relying on generic distortion metrics fails to capture complex human visual perception, often resulting in blurred or unrealistic reconstructions. In this paper, we propose Joint Source-Channel-Generation Coding (JSCGC), a novel paradigm that shifts the focus from deterministic reconstruction to probabilistic generation. JSCGC leverages a generative model at the receiver as a generator rather than a conventional decoder to parameterize the data distribution, enabling direct maximization of mutual information under channel constraints while controlling stochastic sampling to produce outputs residing on the authentic data manifold with high fidelity. We further derive a theoretical lower bound on the maximum semantic inconsistency with given transmitted mutual information, elucidating the fundamental limits of communication in controlling the generative process. Extensive experiments on image transmission demonstrate that JSCGC substantially improves perceptual quality and semantic fidelity, significantly outperforming conventional distortion-oriented JSCC methods.

</details>


### [149] [On the Concavity of Tsallis Entropy along the Heat Flow](https://arxiv.org/abs/2601.12944)
*Lukang Sun*

Main category: cs.IT

TL;DR: 证明了Tsallis熵在热流下的凹性，将之前一维的结果推广到任意维度


<details>
  <summary>Details</summary>
Motivation: Wu et al 2025和Hung 2022之前只在一维情况下证明了Tsallis熵在热流下的凹性，本文旨在将这一结果推广到任意维度

Method: 通过新颖的二阶时间导数项估计和严格的分部积分验证来证明

Result: 成功证明了Tsallis熵在任意维度的热流下具有凹性，并建立了一个新的泛函不等式

Conclusion: 将Tsallis熵凹性结果从一维推广到任意维度，所建立的泛函不等式可能对其他数学分析领域有重要意义

Abstract: We demonstrate the concavity of the Tsallis entropy along the heat flow for general dimensions, expanding upon the findings of Wu et al 2025 and Hung 2022, which were previously limited to the one-dimensional case. The core of the proof is a novel estimate of the terms in the second-order time derivative, and a rigorous validation of integration by parts. The resulting bound establishes a new functional inequality, which may be of interest for other areas of mathematical analysis.

</details>


### [150] [Codes Correcting Few Restricted Errors](https://arxiv.org/abs/2601.12959)
*Jens Zumbrägel*

Main category: cs.IT

TL;DR: 提出纠错码新构造方法，针对高斯整数和艾森斯坦整数域，能纠正2-3个错误，适用于基于编码的密码学场景。


<details>
  <summary>Details</summary>
Motivation: 在基于编码的密码学背景下，针对误差值限制在单位群子群中的线性码（包括Lee距离码、高斯整数和艾森斯坦整数码）受到关注，需要新的构造方法来纠正多个错误。

Method: 借鉴Roth和Siegel在Lee度量码中的技术，提出两种构造方法：几何方法和代数方法，分别具有几何和代数特点。

Result: 提出了高斯整数和艾森斯坦整数域上能纠正2-3个错误的新代码构造方法。

Conclusion: 该工作为基于编码的密码学提供了新的纠错码构造方法，扩展了限制误差值线性码的应用范围，特别是在高斯整数和艾森斯坦整数域上的多错误纠正能力。

Abstract: We consider linear codes over a field in which the error values are restricted to a subgroup of its unit group. This scenario captures Lee distance codes as well as codes over the Gaussian or Eisenstein integers. Codes correcting restricted errors gained increased attention recently in the context of code-based cryptography.
  In this work we provide new constructions of codes over the Gaussian or Eisenstein integers correcting two or three errors. We adapt some techniques from Roth and Siegel's work on codes for the Lee metric. We propose two construction methods, which may be seen of geometric and algebraic flavor, respectively.

</details>


### [151] [Weighted-Hamming Metric: Bounds and Codes](https://arxiv.org/abs/2601.12998)
*Sebastian Bitzer,Alberto Ravagnani,Violetta Weger*

Main category: cs.IT

TL;DR: 本文研究了加权汉明度量下的纠错能力，建立了比最小距离参数更紧的直接界限，并提出了一种基于广义级联的灵活编码构造和高效解码算法。


<details>
  <summary>Details</summary>
Motivation: 加权汉明度量通过为坐标块分配不同权重，适用于独立并行信道编码等应用，其中每个信道具有不同的重要性或噪声水平。从编码理论角度看，在这种度量下，码的实际纠错能力可能超过其最小距离的一半。

Method: 建立了加权汉明度量下纠错能力的直接界限（比最小距离参数更紧），并提出基于广义级联的灵活编码构造，设计了能有效解码到纠错能力下界的算法。

Result: 获得了比传统最小距离参数更紧的纠错能力界限，构造的广义级联码能够被高效解码到纠错能力的下界。

Conclusion: 本文为加权汉明度量下的编码提供了更精确的纠错能力分析框架，并提出了实用的编码构造和解码方案，适用于具有不同重要性或噪声水平的并行信道编码场景。

Abstract: The weighted-Hamming metric generalizes the Hamming metric by assigning different weights to blocks of coordinates. It is well-suited for applications such as coding over independent parallel channels, each of which has a different level of importance or noise. From a coding-theoretic perspective, the actual error-correction capability of a code under this metric can exceed half its minimum distance. In this work, we establish direct bounds on this capability, tightening those obtained via minimum-distance arguments. We also propose a flexible code construction based on generalized concatenation and show that these codes can be efficiently decoded up to a lower bound on the error-correction capability.

</details>


### [152] [Two-timescale Optimization for Hybrid Mechanically and Electronically Tunable 6DMA Aided Communication](https://arxiv.org/abs/2601.13064)
*Yuyan Zhou,Haocheng Hua,Jie Xu,Rui Zhang*

Main category: cs.IT

TL;DR: 提出一种混合机械和电子可调的六维可移动天线基站架构，通过两时间尺度优化结合机械移动和电子波束重构，提升用户平均和速率。


<details>
  <summary>Details</summary>
Motivation: 未来无线通信网络需要更灵活的天线系统来适应动态用户分布。传统机械调整覆盖广但响应慢，电子调谐响应快但角度范围有限，需要结合两者优势。

Method: 提出6DMA基站架构：天线阵列沿圆形轨道机械移动适应水平用户热点，每个阵列配备模式可重构天线进行电子波束切换。采用两时间尺度优化：长时尺度根据用户分布统计优化阵列位置，短时尺度基于瞬时用户位置优化波束选择向量。开发基于蒙特卡洛采样的交替优化算法。

Result: 仿真结果表明，所提设计相比多种基准方案取得了显著性能增益。

Conclusion: 混合机械和电子可调的6DMA基站架构通过两时间尺度优化有效结合了机械调整的广角覆盖和电子调谐的快速响应优势，为未来无线通信网络提供了有前景的解决方案。

Abstract: This letter proposes a hybrid mechanically and electronically tunable six-dimensional movable antenna (6DMA) base station (BS) architecture for future wireless communication networks. Such BS consists of multiple antenna arrays that are mechanically movable along a circular rail to adapt to the horizontal user hotspots, and each array is equipped with pattern reconfigurable antennas (PRAs) that are capable of electronically switching among a set of specified beam patterns to cater to the instantaneous user channels. The mechanical adjustment provides wide-angle coverage but suffers from slow response, while the electronic tuning enables rapid beam reconfiguration but with limited angular range. To effectively combine their complementary advantages, we propose to jointly design both mechanical and electronic configurations to maximize the average sum-rate of users via a two-timescale optimization approach, in which the array positions are optimized on the long timescale according to large-scale user distribution statistics, and the pattern selection vectors are optimized on the short timescale to enable fast beam alignment based on the instantaneous user locations. An alternating optimization algorithm based on the Monte Carlo sampling method is developed to solve the problem efficiently. Finally, simulation results show that our proposed design achieves significant performance gains over various benchmark schemes.

</details>


### [153] [An AMP-Based Asymptotic Analysis For Nonlinear One-Bit Precoding](https://arxiv.org/abs/2601.13214)
*Zheyu Wu,Junjie Ma,Ya-Feng Liu,Bruno Clerckx*

Main category: cs.IT

TL;DR: 本文对一类非线性单比特预编码方案在瑞利衰落信道下的渐近性能进行了分析，提出基于近似消息传递的分析框架，推导出符号错误概率的闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 研究单比特预编码方案的渐近性能分析，特别是基于凸松弛量化方法的性能特性，为系统设计提供理论指导。

Method: 采用近似消息传递框架分析凸松弛量化单比特预编码方案，建立标量"信号加高斯噪声"模型，推导符号错误概率的闭式表达式。

Result: 建立了渐近性能分析框架，推导出符号错误概率的闭式表达式，验证了通过参数调优可以获得比传统SQUID方案更好的性能。

Conclusion: 提出的分析框架能有效表征单比特预编码方案的渐近性能，为系统参数优化提供理论依据，具有实际应用价值。

Abstract: This paper focuses on the asymptotic analysis of a class of nonlinear one-bit precoding schemes under Rayleigh fading channels. The considered scheme employs a convex-relaxation-then-quantization (CRQ) approach to the well-known minimum mean square error (MMSE) model, which includes the classical one-bit precoder SQUID as a special case. To analyze its asymptotic behavior, we develop a novel analytical framework based on approximate message passing (AMP). We show that, the statistical properties of the considered scheme can be asymptotically characterized by a scalar ``signal plus Gaussian noise'' model. Based on this, we further derive a closed-form expression for the symbol error probability (SEP) in the large-system limit, which quantitatively characterizes the impact of both system and model parameters on SEP performance. Simulation results validate our analysis and also demonstrate that performance gains over SQUID can be achieved by appropriately tuning the parameters involved in the considered model.

</details>


### [154] [On the Reliability of Estimation Bounds in Low-SNR Bistatic ISAC](https://arxiv.org/abs/2601.13216)
*Ataher Sams,Besma Smida*

Main category: cs.IT

TL;DR: 本文提出在低信噪比被动感知场景下，采用Ziv-Zakai Bound替代传统Cramér-Rao Bound作为双基地ISAC系统中角度估计的性能评估指标，解决了CRB在低SNR下的失效问题。


<details>
  <summary>Details</summary>
Motivation: 传统ISAC研究多关注功率分配或波束成形设计，但忽略了CRB在低信噪比被动感知场景下的不适用性。由于路径损耗等因素，感知接收信噪比远低于直接通信，使得基于CRB的性能评估不可靠。

Method: 采用双基地ISAC框架，基站发射信号同时用于用户通信和目标参数估计。感知接收器仅知接收信号的统计特性，采用Ziv-Zakai Bound进行到达角估计，推导ZZB和可达遍历通信速率的解析表达式。

Result: 通过数值仿真分析了通信与感知性能的帕累托前沿，证明在低感知SNR的ISAC场景中，ZZB相比传统CRB提供了更有意义的估计误差下界，能更准确评估系统性能。

Conclusion: 在低信噪比被动感知的ISAC系统中，Ziv-Zakai Bound是比Cramér-Rao Bound更合适的性能评估指标，为双基地ISAC设计提供了更可靠的理论基础。

Abstract: This paper explores a bistatic Integrated Sensing and Communication (ISAC) framework, where a base station transmits communication signal that serve both direct communication with a user and multi-target parameter estimation through reflections captured by a separate sensing receiver. We assume that the instantaneous knowledge of the transmit signal at the sensing receiver is not available, and the sensing receiver only has knowledge of the statistical properties of the received signal. Unlike prior research that focuses on power allocation or optimal beamforming design for ISAC, we emphasize the inadequacy of the Cramér-Rao Bound (and its variant) in low Signal-to-Noise Ratio (SNR) regimes, particularly in passive sensing scenarios. Due to severe path loss and other impairments, the received sensing SNR is often significantly lower than that of direct Line-of-Sight communication, making CRB-based performance evaluation unreliable. To address this, we adopt the Ziv-Zakai Bound (ZZB) for Angle of Arrival estimation, which provides a more meaningful lower bound on estimation error. We derive analytical expressions for the ZZB and the achievable ergodic communication rate as functions of SNR. Through numerical simulations, we analyze the pareto-front between communication and sensing performance, demonstrating why ZZB serves as a better metric in low sensing SNR ISAC where traditional CRB-based approaches fail.

</details>


### [155] [Elias-type Bounds for Codes in the Symmetric Limited-Magnitude Error Channel](https://arxiv.org/abs/2601.13477)
*Zhihao Guan,Hengjia Wei*

Main category: cs.IT

TL;DR: 研究对称有限幅度错误信道下完美纠错码的存在条件，针对不同错误幅度建立渐近界


<details>
  <summary>Details</summary>
Motivation: 研究在对称有限幅度错误信道下完美纠错码的存在条件，这种信道中最多有e个坐标可能被幅度不超过s的值改变。几何上对应用对称有限幅度错误球B(n,e,s,s)对Z^n进行平铺

Method: 将汉明度量下Elias界的几何思想适应到对称有限幅度错误信道的距离度量ds，推导完美码/平铺存在的必要条件，不假设任何格结构。针对不同错误幅度建立不同分析方法

Result: 发现两种不同机制：小错误幅度(s∈{1,2})时，若可纠正错误数不超过n的某个比例，则渐近界为e=O(√(n log n))；大错误幅度(s≥3)时，建立更尖锐的界e<√(12.36n)，且对e无比例限制。扩展方法到非完美码，得到打包密度上界

Conclusion: 对称有限幅度错误信道下完美码的存在性受到严格限制，特别是对于大错误幅度，存在尖锐的界。研究结果对理解此类信道下纠错码的构造和性能有重要意义

Abstract: We study perfect error-correcting codes in $\mathbb{Z}^n$ for the symmetric limited-magnitude error channel, where at most $e$ coordinates of an integer vector may be altered by a value whose magnitude is at most $s$. Geometrically, such codes correspond to tilings of $\mathbb{Z}^n$ by the symmetric limited-magnitude error ball $\mathcal{B}(n,e,s,s)$. Given $n$ and $s$, we adapt the geometric ideas underlying the Elias bound for the Hamming metric to the distance $d_s$ tailed for this channel, and derive new necessary conditions on $e$ for the existence of perfect codes / tilings, without assuming any lattice structure. Our main results identify two distinct regimes depending on the error magnitude. For small error magnitudes ($s \in \{1, 2\}$), we prove that if the number of correctable errors does not exceed a certain fraction of $n$, then it is asymptotically bounded by $e = \mathcal{O}(\sqrt{n \log n})$. In contrast, for larger magnitudes ($s \geq 3$), we establish a significantly sharper bound of $e < \sqrt{12.36n}$, which holds without any restriction on $e$ being below a given fraction of $n$. Finally, by extending our method to non-perfect codes, we derive an upper bound on packing density, showing that for codes correcting a linear or $Ω(\sqrt{n})$ number of errors, the density is bounded by a factor inversely proportional to the error magnitude $s$.

</details>


### [156] [Group Relative Policy Optimization for Robust Blind Interference Alignment with Fluid Antennas](https://arxiv.org/abs/2601.13506)
*Jianqiu Peng,Tong Zhang,Shuai Wang,Mingjie Shao,Hao Xu,Rui Wang*

Main category: cs.IT

TL;DR: 提出首个基于流体天线的鲁棒盲干扰对齐框架，采用GRPO强化学习算法优化天线位置以最大化和速率


<details>
  <summary>Details</summary>
Motivation: 流体天线系统能动态重构以重塑无线信道，但在不完美CSI下的K用户MISO下行链路中缺乏鲁棒的干扰对齐方案

Method: 提出鲁棒和速率最大化问题，采用GRPO深度强化学习算法（无critic网络）优化流体天线位置，通过分组探索避免局部最优

Result: GRPO比PPO性能提升4.17%，比预训练PPO提升30.29%；比启发式MaximumGain和RandomGain分别提升200.78%和465.38%

Conclusion: GRPO算法在模型大小和计算复杂度减半的同时，显著提升了流体天线驱动盲干扰对齐系统的鲁棒性和性能

Abstract: Fluid antenna system (FAS) leverages dynamic reconfigurability to unlock spatial degrees of freedom and reshape wireless channels. This paper proposes, for the first time, a robust fluid antenna-driven blind interference alignment (BIA) framework for a K-user MISO downlink under imperfect channel state information (CSI). We formulate a robust sum-rate maximization problem through optimizing fluid antenna positions. To solve this challenging non-convex problem, we employ group relative policy optimization (GRPO), a novel deep reinforcement learning algorithm that eliminates the critic network. This robust design reduces model size and floating point operations (FLOPs) by nearly half compared to proximal policy optimization (PPO) while significantly enhancing performance through group-based exploration that escapes bad local optima. Simulation results demonstrate that GRPO outperforms PPO by 4.17%, and a 100K-step pre-trained PPO by 30.29%. Due to error distribution learning, GRPO exceeds heuristic MaximumGain and RandomGain by 200.78% and 465.38%, respectively.

</details>


### [157] [Storage-Rate Trade-off in A-XPIR](https://arxiv.org/abs/2601.14202)
*Mohamed Nomeir,Sennur Ulukus*

Main category: cs.IT

TL;DR: 研究非对称X安全私有信息检索(A-XPIR)中的存储问题，分析服务器平均存储与平均下载成本之间的权衡关系，针对特定配置给出了可达区域表征和容量结果。


<details>
  <summary>Details</summary>
Motivation: 研究在非对称安全设置下的私有信息检索问题，当服务器之间存在特定通信模式时，探索存储成本与下载成本之间的最优权衡，为实际分布式存储系统提供理论指导。

Method: 采用信息论方法分析A-XPIR问题，推导存储-下载权衡的可达区域边界，针对N=4服务器、K=2消息、两组非重叠通信服务器的具体配置设计存储和检索方案，并分析非对称安全和共谋情况下的容量。

Result: 对于N=4服务器、K=2消息、两组非重叠通信服务器的情况，完整表征了可达区域，发现与无安全情况相比，三个主要不等式在非对称安全情况下简化为两个不等式。设计了不复制消息但能达到复制情况最优速率的存储检索方案。推导出特定非对称安全和共谋配置下的精确容量C=1/3。

Conclusion: 非对称安全设置显著改变了私有信息检索的存储-下载权衡特性，针对特定通信和共谋模式可以获得精确的容量结果，为实际分布式存储系统的安全设计提供了理论依据。

Abstract: We consider the storage problem in an asymmetric $X$-secure private information retrieval (A-XPIR) setting. The A-XPIR setting considers the $X$-secure PIR problem (XPIR) when a given arbitrary set of servers is communicating. We focus on the trade-off region between the average storage at the servers and the average download cost. In the case of $N=4$ servers and two non-overlapping sets of communicating servers with $K=2$ messages, we characterize the achievable region and show that the three main inequalities compared to the no-security case collapse to two inequalities in the asymmetric security case. In the general case, we derive bounds that need to be satisfied for the general achievable region for an arbitrary number of servers and messages. In addition, we provide the storage and retrieval scheme for the case of $N=4$ servers with $K=2$ messages and two non-overlapping sets of communicating servers, such that the messages are not replicated (in the sense of a coded version of each symbol) and at the same time achieve the optimal achievable rate for the case of replication. Finally, we derive the exact capacity for the case of asymmetric security and asymmetric collusion for $N=4$ servers, with the communication links $\{1,2\}$ and $\{3,4\}$, which splits the servers into two groups, i.e., $g=2$, and with the collusion links $\{1,3\}$, $\{2,4\}$, as $C=\frac{1}{3}$. More generally, we derive a capacity result for a certain family of asymmetric collusion and asymmetric security cases.

</details>


### [158] [An Elementary Approach to Scheduling in Generative Diffusion Models](https://arxiv.org/abs/2601.13602)
*Qiang Sun,H. Vincent Poor,Wenyi Zhang*

Main category: cs.IT

TL;DR: 提出一种分析扩散模型中噪声调度和时间离散化影响的基本方法，通过高斯源分布的解析解推导KL散度，优化噪声调度，并在实验中验证时间离散化策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型中噪声调度和时间离散化对生成质量的影响缺乏系统分析，需要建立理论框架来理解和优化这些关键参数。

Method: 采用简化模型：假设源分布为具有给定协方差矩阵的多变量高斯分布，推导反向采样过程中分布的显式闭式演化轨迹，从而获得源分布与反向采样输出之间的KL散度。通过欧拉-麦克劳林展开研究时间离散化步数对KL散度收敛的影响，并利用变分法求解最优噪声调度。

Result: 推导出最优噪声调度遵循正切定律，其系数由源协方差矩阵的特征值决定。实验表明，基于该方法选择的时间离散化策略在各种数据集和预训练模型上均优于基线方法和搜索策略，特别是在函数评估次数预算非常紧张的情况下表现更佳。

Conclusion: 该研究为理解扩散模型中噪声调度和时间离散化的影响提供了理论框架，提出的优化方法能够有效提升生成质量，特别是在计算资源受限的场景下具有重要实用价值。

Abstract: An elementary approach to characterizing the impact of noise scheduling and time discretization in generative diffusion models is developed. Considering a simplified model where the source distribution is multivariate Gaussian with a given covariance matrix, the explicit closed-form evolution trajectory of the distributions across reverse sampling steps is derived, and consequently, the Kullback-Leibler (KL) divergence between the source distribution and the reverse sampling output is obtained. The effect of the number of time discretization steps on the convergence of this KL divergence is studied via the Euler-Maclaurin expansion. An optimization problem is formulated, and its solution noise schedule is obtained via calculus of variations, shown to follow a tangent law whose coefficient is determined by the eigenvalues of the source covariance matrix. For an alternative scenario, more realistic in practice, where pretrained models have been obtained for some given noise schedules, the KL divergence also provides a measure to compare different time discretization strategies in reverse sampling. Experiments across different datasets and pretrained models demonstrate that the time discretization strategy selected by our approach consistently outperforms baseline and search-based strategies, particularly when the budget on the number of function evaluations is very tight.

</details>


### [159] [Reflections over the Sea: Reconfigurable Intelligent Surface for Maritime Self-Powered Communications](https://arxiv.org/abs/2601.13618)
*Qianqian Zhang,Long Wang,Ben Wu,Jia Mi*

Main category: cs.IT

TL;DR: 提出基于海上风电基础设施RIS的海洋物联网通信框架，利用海浪能量采集为传感器供电，通过动态信道优化提升恶劣海况下的通信性能


<details>
  <summary>Details</summary>
Motivation: 海洋经济快速发展推动6G网络中的海事通信需求，但现有技术在恶劣海况下面临信号覆盖、可用性和鲁棒性等关键挑战

Method: 1) 在海上风电等基础设施上部署可重构智能表面(RIS)增强覆盖；2) 建立考虑海浪影响的近海面信道模型；3) 设计海浪能量采集系统为物联网传感器自供电；4) 实时测量信道状态信息优化RIS反射参数最大化多用户通信速率

Result: 仿真结果表明，在恶劣海况下，所提系统将物联网通信性能提升超过20%

Conclusion: 该框架通过RIS增强、海浪能量采集和动态信道优化的协同设计，有效解决了恶劣海况下的海事物联网通信挑战，为6G海事通信提供了创新解决方案

Abstract: Maritime communication is becoming a vital component of 6G networks, driven by the rapid expansion of the maritime economy. However, existing technologies face critical challenges in signal coverage, availability, and robustness, especially under harsh sea conditions. This paper proposes a novel framework for the maritime Internet-of-Things (IoT) communications that leverages the reconfigurable intelligent surface (RIS) mounted on offshore infrastructures, such as wind turbines, to enhance coverage and reliability. To capture dynamic maritime environment, a near-ocean-surface channel model is developed considering the impact of sea waves. In addition, a wave energy harvesting (EH) system is designed to self-power IoT sensors for data acquisition, processing, and transmission. To support real-time adaptation, channel state information is continuously measured to optimize RIS reflection parameters and maximize multi-user communication rates. Simulation results show that the proposed system significantly improves IoT communication performance by over 20%, under harsh sea conditions.

</details>


### [160] [Constrained MARL for Coexisting TN-NTN Resource Allocation: Scalability and Flexibility](https://arxiv.org/abs/2601.13883)
*Cuong Le,Thang X. Vu,Stefano Andrenacci,Symeon Chatzinotas*

Main category: cs.IT

TL;DR: 论文提出一种分解式学习方法，用于解决大规模动态场景下地面与非地面基站共存的联合资源分配问题，显著提升了可扩展性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 地面基站与非地面基站共存频谱中的联合资源分配面临大规模、高动态性挑战，现有学习方法难以有效应对。

Method: 基于跨段干扰的特殊性质设计分解解决方案，通过顺序学习方式解决子问题，并构建随机训练环境以增强策略灵活性。

Result: 在完整20MHz带宽和多种参数配置下的仿真表明，该方法相比现有方案显著提升了可扩展性，并在高动态场景中保持鲁棒性。

Conclusion: 提出的分解式学习方法有效解决了大规模动态场景下TN-NTN联合资源分配问题，为实际系统部署提供了可行方案。

Abstract: This paper considers the joint TN-NTN constrained resource allocation, where terrestrial base stations and non-terrestrial base stations coexist in the spectrum. We focus on large-scale and practical scenarios characterized by large numbers of transmission channels and users, alongside highly dynamic user behaviors. As common learning solutions fail to address these challenges, we propose a decomposition solution based on the special properties of the cross-segment interference, and then tackle the original problem via solving subproblems in a sequential learning manner. Furthermore, to enhance the flexibility of the learned policies, we design a stochastic training environment that captures the key characteristics of real-world systems. Simulation results tested on the full 20MHz bandwidth with various numerologies show that our solution significantly improves scalability compared to existing solutions and remains robust in highly dynamic scenarios.

</details>


### [161] [Proactive Coded Caching Scheme for D2D Networks](https://arxiv.org/abs/2601.13929)
*Qiaoling Zhang,Changlu Lin,Minquan Cheng*

Main category: cs.IT

TL;DR: 提出了一种用于D2D网络的安全编码缓存方案，同时保证文件隐私和安全传输，在文件大小足够大且缓存内存充足时实现顺序最优性能


<details>
  <summary>Details</summary>
Motivation: 现有编码缓存方案通常假设用户缓存内容不被他人访问，忽视了缓存本身被攻击导致文件隐私泄露的风险。D2D通信和编码缓存是缓解网络流量的有效技术，但安全传输和文件隐私已成为这些领域的关键问题。

Method: 提出了一种安全的编码缓存方案，专门设计用于D2D网络，同时保证文件隐私和安全传输。该方案考虑了缓存本身可能被攻击的风险，而不仅仅是传输过程的安全性。

Result: 当文件大小足够大且缓存内存充足时，所提出的方案实现了顺序最优性能。这意味着在满足安全要求的同时，方案在缓存效率方面达到了理论上的最优或接近最优水平。

Conclusion: 该研究填补了编码缓存方案中缓存安全性的空白，为D2D网络提供了一种既能保证文件隐私又能确保安全传输的实用解决方案，在特定条件下实现了理论最优性能。

Abstract: Coded caching and device-to-device (D2D) communication are two effective techniques for alleviating network traffic. Secure transmission and file privacy have also become critical concerns in these domains. However, prevailing coded caching schemes typically assume that a user's cached content is inaccessible to others, overlooking the risk of file privacy leakage due to attacks targeting the cache itself. In this paper, we propose a secure coded caching scheme for D2D networks that guarantees both file privacy and secure delivery. We demonstrate that the proposed scheme achieves order-optimal performance when the file size is sufficiently large and the cache memory is ample.

</details>


### [162] [Near Optimal Code Construction for the Adversarial Torn Paper Channel With Edit Errors](https://arxiv.org/abs/2601.14088)
*Maria Abu-Sini,Reinhard Heckel*

Main category: cs.IT

TL;DR: 研究DNA存储和3D指纹识别中的对抗性撕裂信道，构建了能抵抗t次撕裂和t_e次编辑错误的近最优纠错码，并研究了列表解码


<details>
  <summary>Details</summary>
Motivation: 受DNA存储系统和3D指纹识别应用启发，研究对抗性撕裂信道中的编辑错误问题。在实际应用中，数据可能被编辑错误（插入、删除、替换）破坏，然后被任意位置撕裂成多个片段

Method: 构建t-breaks t_e-edit-errors resilient code（t次撕裂t_e次编辑错误弹性码），该码能够从t+1个噪声片段中重构传输的码字。同时研究列表解码，推导从t'次切割（t'>t）获得的码字列表大小的界限

Result: 构造了撕裂信道中编辑错误的近最优纠错码，能够从噪声片段中重构原始码字。建立了列表解码的理论界限

Conclusion: 该工作为DNA存储和3D指纹识别等应用中的撕裂信道提供了有效的编码解决方案，建立了对抗编辑错误和撕裂的编码理论框架

Abstract: Motivated by DNA storage systems and 3D fingerprinting, this work studies the adversarial torn paper channel with edit errors. This channel first applies at most $t_e$ edit errors (i.e., insertions, deletions, and substitutions) to the transmitted word and then breaks it into $t+1$ fragments at arbitrary positions. In this paper, we construct a near optimal error correcting code for this channel, which will be referred to as a $t$-breaks $t_e$-edit-errors resilient code. This code enables reconstructing the transmitted codeword from the $t+1$ noisy fragments. Moreover, we study list decoding of the torn paper channel by deriving bounds on the size of the list (of codewords) obtained from cutting a codeword of a $t$-breaks resilient code $t'$ times, where $t' > t$.

</details>


### [163] [Vector Coded Caching Multiplicatively Boosts MU-MIMO Systems Under Practical Considerations](https://arxiv.org/abs/2601.14142)
*Hui Zhao,Petros Elia*

Main category: cs.IT

TL;DR: 本文首次全面分析了多用户MIMO系统中向量编码缓存的性能影响，考虑了多接收天线和可变路径损耗等关键因素，提出了低复杂度的BD-MRC优化方法，在32发射天线配置下实现了超过300%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 研究向量编码缓存在多用户MIMO系统中的影响，特别关注多接收天线和可变路径损耗这两个对多用户单播系统性能至关重要的因素，旨在解决实际系统中的信道衰落、CSI获取开销和公平性功率分配等实际问题。

Method: 采用两种广泛使用的预编码策略：1) 发射端块对角化结合接收端最大比合并；2) 迫零预编码。开发了低复杂度的BD-MRC优化方法，利用矩阵结构降低预编码计算维度，通过高效的一维搜索解决最大最小公平性问题。在大规模MIMO场景下，推导了瑞利衰落信道的渐近吞吐量表达式。

Result: 理论分析和仿真验证表明，VCC相比优化的无缓存MU-MIMO系统带来显著性能提升。例如，在32个发射天线和每个用户2个接收天线的配置下，VCC实现了超过300%的吞吐量提升。在发射端CSI不完美的情况下，VCC将干扰缓解卸载到接收端的能力确保了鲁棒性能。

Conclusion: 向量编码缓存在多用户MIMO系统中具有重要价值，特别是在多接收天线和可变路径损耗的实际场景下。VCC不仅能显著提升系统吞吐量，还能在不完美CSI条件下保持鲁棒性能，为实际无线通信系统设计提供了有效的干扰管理方案。

Abstract: This work presents a first comprehensive analysis of the impact of vector coded caching (VCC) in multi-user multiple-input multiple-output (MU-MIMO) systems with multiple receive antennas and variable pathloss -- two key factors that critically influence systems with inherent MU unicasting behavior. We investigate two widely adopted precoding strategies: (i) blockdiagonalization (BD) at the transmitter combined with maximal ratio combining (MRC) at the receivers, and (ii) zero-forcing (ZF) precoding. Our analysis explicitly accounts for practical considerations such as channel fading, channel state information (CSI) acquisition overhead, and fairness-oriented power allocation.
  Our contributions span both analytical and simulation-based fronts. On the analytical side, we derive analytical expressions for the achievable throughput under BD-MRC and ZF, highlighting the performance benefits of equipping multi-antenna users with cache-aided interference management. Specifically, we develop a low-complexity BD-MRC optimization method that leverages matrix structure to significantly reduce the dimensionality involved in precoding computation, followed by solving the associated maxmin fairness problem through an efficient one-dimensional search. In the massive MIMO regime, an asymptotic expression for the achievable throughput over Rayleigh fading channels is also derived. Simulations validate our theoretical results, confirming that VCC delivers substantial performance gains over optimized cacheless MU-MIMO systems. For example, with 32 transmit antennas and 2 receive antennas per user, VCC yields throughput improvements exceeding 300%. These gains are further amplified under imperfect CSI at the transmitter, where VCC's ability to offload interference mitigation to the receivers ensures robust performance even in the face of degraded CSI quality and elevated acquisition costs.

</details>


### [164] [Stabilizer-Assisted Inactivation Decoding of Quantum Error-Correcting Codes with Erasures](https://arxiv.org/abs/2601.14236)
*Giulio Pech,Mert Gökduman,Hanwen Yao,Henry D. Pfister*

Main category: cs.IT

TL;DR: 提出一种用于量子低密度奇偶校验码擦除解码的简化复杂度最大似然解码器，结合经典失活解码与新的对偶剥离过程，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 量子低密度奇偶校验码的擦除解码需要最大似然性能，但传统解码器计算复杂度高，需要开发更高效的解码方法。

Method: 结合经典失活解码（剥离与符号猜测）与新的对偶剥离过程，通过对稳定子矩阵进行行操作，识别支持集完全位于擦除集合中的稳定子生成元，从而减少符号猜测需求。

Result: 解码器在多个QLDPC码族中实现了最大似然逻辑失败性能，同时显著降低了失活解码的复杂度，在B1提升乘积码的高擦除率下符号猜测减少超过20%。

Conclusion: 提出的对偶剥离方法有效降低了量子擦除解码的计算复杂度，同时保持最大似然性能，为QLDPC码的实用化提供了更高效的解码方案。

Abstract: In this work, we develop a reduced complexity maximum likelihood (ML) decoder for quantum low-density parity-check (QLDPC) codes over erasures. Our decoder combines classical inactivation decoding, which integrates peeling with symbolic guessing, with a new dual peeling procedure. In the dual peeling stage, we perform row operations on the stabilizer matrix to efficiently reveal stabilizer generators and their linear combinations whose support lies entirely on the erased set. Each such stabilizer identified allows us to freely fix a bit in its support without affecting the logical state of the decoded result. This removes one degree of freedom that would otherwise require a symbolic guess, reducing the number of inactivated variables and decreasing the size of the final linear system that must be solved. We further show that dual peeling combined with standard peeling alone, without inactivation, is sufficient to achieve ML for erasure decoding of surface codes. Simulations across several QLDPC code families confirm that our decoder matches ML logical failure performance while significantly reducing the complexity of inactivation decoding, including more than a 20% reduction in symbolic guesses for the B1 lifted product code at high erasure rates.

</details>


### [165] [Identification capacity and rate-query tradeoffs in classification systems](https://arxiv.org/abs/2601.14252)
*Tristan Simas*

Main category: cs.IT

TL;DR: 该论文研究离散分类中的一次性识别问题，分析标签率L、识别成本W和失真D三个资源之间的权衡关系，建立了零误差可行性阈值和基本界限。


<details>
  <summary>Details</summary>
Motivation: 研究在有限资源约束下（标签存储、查询成本、分类错误率）如何实现有效的实体类别识别，这对类型系统、数据库和生物分类学等应用具有重要意义。

Method: 采用信息论和组合数学方法，分析标签率L、识别成本W和失真D之间的权衡关系，建立零误差可行性阈值，使用拟阵理论分析最小充分查询族，并通过Lean4形式化验证所有结果。

Result: 1) 零误差可行性：当属性映射在类别上非单射时，无标签零误差识别不可能；2) 零失真界限：k个类别需要L≥log₂k比特标签才能实现D=0；3) 无标签情况：需要Ω(n)查询且可能D>0；4) 组合结构：最小充分查询族构成拟阵基。

Conclusion: 论文建立了离散分类中一次性识别问题的完整理论框架，揭示了标签率、查询成本和分类错误率之间的基本权衡关系，所有结果都经过形式化验证，为实际应用提供了理论基础。

Abstract: We study a one-shot identification analogue of rate-distortion for discrete classification under three resources: tag rate L (bits of side information stored per entity), identification cost W (attribute-membership queries per identification, excluding global preprocessing and amortized caching), and distortion D (misclassification probability). The question is to characterize achievable triples (L,W,D) when a decoder must recover an entity's class from limited observations. Zero-error barrier. If two distinct classes induce the same attribute profile, then the observation pi(V) is identical for both and no decoder can identify the class from attribute queries alone. Thus, if the profile map pi is not injective on classes, zero-error identification without tags is impossible (a zero-error feasibility threshold). Achievability and converse at D=0. With k classes, nominal tags of L = ceil(log2 k) bits enable O(1) identification cost with D=0. Conversely, any scheme with D=0 must satisfy L >= log2 k bits (tight). Without tags (L=0), identification requires Omega(n) queries in the worst case and may incur D>0. Combinatorial structure. Minimal sufficient query families form the bases of a matroid; the induced distinguishing dimension is well-defined and links to zero-error source coding via graph entropy. We illustrate implications for type systems, databases, and biological taxonomy. All results are mechanized in Lean4 (6000+ lines, 0 sorry).

</details>
