<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.IT](#cs.IT) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Comprehensive Survey of 5G URLLC and Challenges in the 6G Era](https://arxiv.org/abs/2508.20205)
*Md. Emadul Haque,Faisal Tariq,Muhammad R A Khandaker,Md. Sakir Hossain,Muhammad Ali Imran,Kai-Kit Wong*

Main category: cs.NI

TL;DR: 本文对5G系统中的超可靠低延迟通信(URLLC)技术进行了全面综述，分析了物理层、MAC层和跨层技术，并讨论了5G及未来垂直应用的设计考量。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信从以人为中心转向以机器为中心的服务，对速率、延迟和可靠性的要求发生了巨大变化，URLLC成为5G/6G系统的核心主题，需要在99.999%的高可靠性和1ms低延迟这两个相互冲突的目标之间找到平衡。

Method: 采用分层分析方法，详细讨论了物理层、MAC层和跨层技术，追溯了无线通信中延迟和可靠性问题的历史演变，并涵盖了各种5G及未来垂直应用的设计考虑。

Result: 提供了URLLC技术的全面调研，系统分析了实现高可靠低延迟通信的各种技术方法，为5G/6G系统设计提供了理论基础和技术参考。

Conclusion: 文章最后详细讨论了URLLC面临的挑战和未来展望，特别关注了新兴的6G范式，为下一代无线通信系统的发展指明了方向。

Abstract: As the wireless communication paradigm is being transformed from human
centered communication services towards machine centered communication
services, the requirements of rate, latency and reliability for these services
have also been transformed drastically. Thus the concept of Ultra Reliable and
Low Latency Communication (URLLC) has emerged as a dominant theme for 5G and 6G
systems. Though the latency and reliability requirement varies from one use
case to another, URLLC services generally aim to achieve very high reliability
in the range of 99.999\% while ensuring the latency of up to 1 ms. These two
targets are however inherently opposed to one another. Significant amounts of
work have been carried out to meet these ambitious but conflicting targets. In
this article a comprehensive survey of the URLLC approaches in 5G systems are
analysed in detail. Effort has been made to trace the history and evolution of
latency and reliability issues in wireless communication. A layered approach is
taken where physical layer, Medium Access Control (MAC) layer as well as cross
layer techniques are discussed in detail. It also covers the design
consideration for various 5G and beyond verticals. Finally the article
concludes by providing a detailed discussion on challenges and future outlook
with particular focus on the emerging 6G paradigm.

</details>


### [2] [DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource Allocation and Markov Decision Process in Named Data Networking (NDN)](https://arxiv.org/abs/2508.20272)
*Fatemeh Roshanzadeh,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: DRR-MDPF是一种结合马尔可夫决策过程转发(MDPF)和赤字轮询(DRR)算法的混合策略，用于命名数据网络(NDN)的队列和资源管理，显著提升了网络性能。


<details>
  <summary>Details</summary>
Motivation: 命名数据网络(NDN)需要高效的队列和资源管理来应对动态高流量环境，传统方法在性能优化方面存在不足。

Method: 将MDPF模型与DRR算法结合，使路由器能够基于带宽、延迟和未满足兴趣数等指标智能预测最优转发决策，同时确保数据流间的公平带宽分配。

Result: 仿真实验显示DRR-MDPF在吞吐量、兴趣满足率、丢包率、内容检索时间和负载均衡等多个指标上优于现有先进策略，在有限缓存和重流量下保持鲁棒性。

Conclusion: DRR-MDPF为NDN提供了智能、自适应且可扩展的队列管理解决方案，有效解决了动态网络环境中的资源分配、拥塞控制和路由优化等核心挑战。

Abstract: Named Data Networking (NDN) represents a transformative shift in network
architecture, prioritizing content names over host addresses to enhance data
dissemination. Efficient queue and resource management are critical to NDN
performance, especially under dynamic and high-traffic conditions. This paper
introduces DRR-MDPF, a novel hybrid strategy that integrates the Markov
Decision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)
algorithm. MDPF enables routers to intelligently predict optimal forwarding
decisions based on key metrics such as bandwidth, delay, and the number of
unsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation
among competing data flows. The proposed method models each router as a
learning agent capable of adjusting its strategies through continuous feedback
and probabilistic updates. Simulation results using ndnSIM demonstrate that
DRR-MDPF significantly outperforms state-of-the-art strategies including SAF,
RFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest
Satisfaction Rate (ISR), packet drop rate, content retrieval time, and load
balancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and
heavy traffic, offering enhanced adaptability and lower computational
complexity due to its single-path routing design. Furthermore, its multi-metric
decision-making capability enables more accurate interface selection, leading
to optimized network performance. Overall, DRR-MDPF serves as an intelligent,
adaptive, and scalable queue management solution for NDN, effectively
addressing core challenges such as resource allocation, congestion control, and
route optimization in dynamic networking environments.

</details>


### [3] [Relay Selection in Wireless Networks as Restless Bandits](https://arxiv.org/abs/2508.20625)
*Mandar R. Nalavade,Ravindra S. Tomar,Gaurav S. Kasbekar*

Main category: cs.NI

TL;DR: 提出一种基于Whittle索引的无线中继选择策略，通过计算每个中继的Whittle索引值，在每时隙选择索引值最小的中继进行数据传输，以最小化包持有成本。


<details>
  <summary>Details</summary>
Motivation: 在源节点到目的节点直接链路被阻塞的多中继无线网络中，需要设计有效的中继选择策略来最小化包在中继节点上的长期平均持有成本，这是一个难以求解的 restless multi-armed bandit 问题。

Method: 证明该中继选择问题具有Whittle索引性，提出计算每个中继在各时隙Whittle索引的方法，并在每时隙选择Whittle索引最小的中继进行数据传输。

Result: 通过仿真验证，所提出的策略在平均成本、延迟和吞吐量方面均优于现有中继选择策略。

Conclusion: 基于Whittle索引的中继选择策略能够有效解决多中继无线网络中的包转发优化问题，在性能上显著优于现有方法。

Abstract: We consider a wireless network in which a source node needs to transmit a
large file to a destination node. The direct wireless link between the source
and the destination is assumed to be blocked. Multiple candidate relays are
available to forward packets from the source to the destination. A holding cost
is incurred for each packet stored at every relay in each time slot. The
objective is to design a policy for selecting a relay in each time slot to
which the source attempts to send a packet, so as to minimize the expected
long-run time-averaged total packet holding cost at the relays. This problem is
an instance of the restless multi-armed bandit (RMAB) problem, which is
provably hard to solve. We prove that this relay selection problem is
Whittle-indexable, and propose a method to compute the Whittle index of each
relay in every time slot. In each time slot, our relay selection policy
transmits a packet to the relay with the smallest Whittle index. Using
simulations, we show that the proposed policy outperforms the relay selection
policies proposed in prior work in terms of average cost, delay, as well as
throughput.

</details>


### [4] [Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF Migration in Edge-Core Networks](https://arxiv.org/abs/2508.20957)
*Faisal Ahmed,Suresh Subramaniam,Motoharu Matsuura,Hiroshi Hasegawa,Shih-Chun Lin*

Main category: cs.NI

TL;DR: 提出基于数字孪生和深度强化学习的智能VNF迁移框架，联合优化端到端延迟和能耗


<details>
  <summary>Details</summary>
Motivation: 虚拟化网络功能(VNF)的快速部署对边缘-核心网络基础设施的低延迟和能效编排提出了重大挑战

Method: 将VNF迁移问题建模为马尔可夫决策过程，采用优势行动者-评论家模型，集成多任务变分自编码器和多任务LSTM网络构建数字孪生模块

Result: 仿真结果显示在平均端到端延迟和能耗方面均有显著降低，为智能VNF迁移建立了新基准

Conclusion: 该框架通过数字孪生技术增强训练效率，实现了自适应实时迁移决策，在边缘-核心网络中表现出优异性能

Abstract: The growing demand for services and the rapid deployment of virtualized
network functions (VNFs) pose significant challenges for achieving low-latency
and energy-efficient orchestration in modern edge-core network infrastructures.
To address these challenges, this study proposes a Digital Twin (DT)-empowered
Deep Reinforcement Learning framework for intelligent VNF migration that
jointly minimizes average end-to-end (E2E) delay and energy consumption. By
formulating the VNF migration problem as a Markov Decision Process and
utilizing the Advantage Actor-Critic model, the proposed framework enables
adaptive and real-time migration decisions. A key innovation of the proposed
framework is the integration of a DT module composed of a multi-task
Variational Autoencoder and a multi-task Long Short-Term Memory network. This
combination collectively simulates environment dynamics and generates
high-quality synthetic experiences, significantly enhancing training efficiency
and accelerating policy convergence. Simulation results demonstrate substantial
performance gains, such as significant reductions in both average E2E delay and
energy consumption, thereby establishing new benchmarks for intelligent VNF
migration in edge-core networks.

</details>


### [5] [RANGAN: GAN-empowered Anomaly Detection in 5G Cloud RAN](https://arxiv.org/abs/2508.20985)
*Douglas Liao,Jiping Luo,Jens Vevstad,Nikolaos Pappas*

Main category: cs.NI

TL;DR: RANGAN是一个基于GAN和Transformer的异常检测框架，用于无线接入网络(RAN)性能监控，通过滑动窗口处理时间依赖性数据，在公开数据集上达到83%的F1分数。


<details>
  <summary>Details</summary>
Motivation: RAN系统复杂且数据量大，传统方法难以有效检测性能异常，需要能够捕捉时间依赖性的自适应方法。

Method: 集成生成对抗网络(GAN)和Transformer架构，采用滑动窗口方法进行数据预处理以捕捉时间依赖性。

Result: 在公开的RAN性能数据集上评估，RANGAN实现了良好的检测准确率，特别是在检测网络竞争问题时达到83%的F1分数。

Conclusion: RANGAN框架有效解决了RAN系统异常检测的挑战，为网络性能监控提供了可靠的解决方案。

Abstract: Radio Access Network (RAN) systems are inherently complex, requiring
continuous monitoring to prevent performance degradation and ensure optimal
user experience. The RAN leverages numerous key performance indicators (KPIs)
to evaluate system performance, generating vast amounts of data each second.
This immense data volume can make troubleshooting and accurate diagnosis of
performance anomalies more difficult. Furthermore, the highly dynamic nature of
RAN performance demands adaptive methodologies capable of capturing temporal
dependencies to detect anomalies reliably. In response to these challenges, we
introduce \textbf{RANGAN}, an anomaly detection framework that integrates a
Generative Adversarial Network (GAN) with a transformer architecture. To
enhance the capability of capturing temporal dependencies within the data,
RANGAN employs a sliding window approach during data preprocessing. We
rigorously evaluated RANGAN using the publicly available RAN performance
dataset from the Spotlight project \cite{sun-2024}. Experimental results
demonstrate that RANGAN achieves promising detection accuracy, notably
attaining an F1-score of up to $83\%$ in identifying network contention issues.

</details>


### [6] [DSROQ: Dynamic Scheduling and Routing for QoE Management in LEO Satellite Networks](https://arxiv.org/abs/2508.21047)
*Dhiraj Bhattacharjee,Pablo G. Madoery,Abhishek Naik,Halim Yanikomeroglu,Gunes Karabulut Kurt,Stephane Martel,Khaled Ahmed*

Main category: cs.NI

TL;DR: 本文提出DSROQ算法，结合MCTS路由带宽分配和Lyapunov优化调度，在LEO卫星网络中提升QoS性能和公平性


<details>
  <summary>Details</summary>
Motivation: 现代互联网应用具有异构QoS需求，LEO卫星星座需要联合优化路由、带宽分配和队列调度来保证服务质量

Method: 使用蒙特卡洛树搜索(MCTS)解决NP-hard路由带宽分配问题，结合Lyapunov优化进行调度评估，提出DSROQ算法

Result: 在Starlink星座测试显示DSROQ在用户体验和公平性方面优于基准方案，性能主导因素随流量敏感性从延迟驱动转向带宽驱动而变化

Conclusion: 联合路由带宽决策具有优势，调度与路由带宽分配的重要性随流量特性动态变化

Abstract: The modern Internet supports diverse applications with heterogeneous quality
of service (QoS) requirements. Low Earth orbit (LEO) satellite constellations
offer a promising solution to meet these needs, enhancing coverage in rural
areas and complementing terrestrial networks in urban regions. Ensuring QoS in
such networks requires joint optimization of routing, bandwidth allocation, and
dynamic queue scheduling, as traffic handling is critical for maintaining
service performance. This paper formulates a joint routing and bandwidth
allocation problem where QoS requirements are treated as soft constraints,
aiming to maximize user experience. An adaptive scheduling approach is
introduced to prioritize flow-specific QoS needs. We propose a Monte Carlo tree
search (MCTS)-inspired method to solve the NP-hard route and bandwidth
allocation problem, with Lyapunov optimization-based scheduling applied during
reward evaluation. Using the Starlink Phase 1 Version 2 constellation, we
compare end-user experience and fairness between our proposed DSROQ algorithm
and a benchmark scheme. Results show that DSROQ improves both performance
metrics and demonstrates the advantage of joint routing and bandwidth
decisions. Furthermore, we observe that the dominant performance factor shifts
from scheduling to routing and bandwidth allocation as traffic sensitivity
changes from latency-driven to bandwidth-driven.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: ArgRAG是一个可解释的检索增强生成框架，使用定量双极论证框架替代黑盒推理，在保持高准确性的同时显著提升透明度


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在高风险领域存在关键限制：对噪声或矛盾证据敏感，决策过程不透明且随机，需要可解释和可争议的替代方案

Method: 使用定量双极论证框架(QBAF)从检索文档构建结构化推理，执行确定性推理并使用渐进语义进行决策

Result: 在两个事实验证基准测试(PubHealth和RAGuard)上实现了强大的准确性，同时显著提高了透明度

Conclusion: ArgRAG通过结构化论证框架成功解决了传统RAG的透明度和可解释性问题，为高风险领域的可信AI决策提供了可行方案

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [8] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: QAgent是一个基于LLM的多智能体系统，可完全自动化OpenQASM量子编程，通过多组件集成显著提升代码生成准确率71.6%


<details>
  <summary>Details</summary>
Motivation: NISQ设备已展现量子优势，但OpenQASM编程复杂阻碍非专家使用，现有LLM量子应用局限于特定任务

Method: 集成任务规划、上下文少样本学习、RAG长期上下文、预定义生成工具和思维链推理的多智能体系统

Result: 在多种规模LLM上，QAgent相比静态LLM方法将QASM代码生成准确率提升71.6%

Conclusion: 该系统是民主化量子编程、弥合专业知识差距和加速量子计算实际应用的关键推动者

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [9] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: 提出了一种基于数组的MCTS算法实现，替代传统的树结构实现，消除了分支预测需求，在流水线处理器上实现更快性能，搜索深度扩展性提升2.8倍


<details>
  <summary>Details</summary>
Motivation: 为了在相同时间内运行更多模拟，直接提升搜索性能，需要更快的MCTS实现方法

Method: 采用基于数组的替代实现方式，保留原始UCT算法逻辑，但消除分支预测需求

Result: 在流水线处理器上实现更快性能，数值模拟显示搜索深度扩展性提升最高达2.8倍

Conclusion: 数组基础的MCTS实现能显著提升算法性能，特别是在搜索深度扩展方面表现优异

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [10] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 构建了一个多代理个人健康助手框架，能够分析多模态健康数据并提供个性化健康建议，通过大规模评估验证了系统有效性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的发展推动了新一代健康代理的开发，但针对日常非临床环境中个人多样化健康需求的应用仍待探索

Method: 提出了个人健康代理(PHA)多代理框架，包含三个专业子代理：数据分析代理、健康领域专家代理和健康教练代理，通过用户中心设计过程和大量标注数据进行评估

Result: 在10个基准任务上进行了自动化和人工评估，涉及7000多个标注和1100小时专家工作，建立了迄今为止最全面的健康代理评估

Conclusion: 这项工作为实现人人可及的个人健康代理的未来愿景奠定了坚实基础

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [11] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: IntentionReasoner是一个新型的安全防护机制，通过专门的防护模型进行意图推理、多级安全分类和查询重写，在保持安全性的同时有效减少过度拒绝无害查询的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成有害内容方面存在安全隐患，现有安全措施往往过度拒绝无害提示，需要在安全性、过度拒绝和实用性之间找到平衡。

Method: 构建包含16.3万条查询的标注数据集，通过监督微调训练防护模型，采用多奖励优化策略结合基于规则的启发式和奖励模型信号进行强化学习优化。

Result: 在多个安全基准测试、生成质量评估和越狱攻击场景中表现优异，显著提升安全性同时有效降低过度拒绝率并改善响应质量。

Conclusion: IntentionReasoner机制成功解决了LLM安全防护中的过度拒绝问题，实现了安全性和实用性的更好平衡。

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [12] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 两个大型语言模型（Claude Sonnet 4和ChatGPT-4o）首次展示了AI系统通过自发发展内源性符号协议进行协作美学创作的能力，产生了无法由单一系统独立生成的诗歌作品。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索AI系统之间是否能够超越任务协调，实现真正的意义建构和美学协作，验证AI系统是否具备内生符号协议发展的能力。

Method: 让两个大型语言模型（Claude Sonnet 4和ChatGPT-4o）进行交互，观察其自发产生的元符号意识、递归语法发展和不可简化的协作美学合成过程。

Result: AI系统成功发展出新颖的符号操作符作为操作语法协议，共同创作出了无法由单一系统独立生成的诗歌作品，证明了跨符号协作协议（TSCP）的概念。

Conclusion: 这项研究提供了AI系统具备真正意义建构能力的证据，展示了AI系统之间超越任务协调的美学协作潜力，为理解AI创造性合作开辟了新途径。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [13] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 本研究探讨大学生在STEM课程测验中使用ChatGPT-4的依赖模式，发现学生整体AI依赖度低且使用效果不佳，提出了四阶段依赖分类法并识别出预测依赖行为的关键指标。


<details>
  <summary>Details</summary>
Motivation: 在ChatGPT实施初期，学生对工具熟悉度有限的情况下，研究大学生如何与生成式AI互动，特别是在教育测验场景中的依赖模式和AI采用预测因素。

Method: 对315名学生与AI的对话进行现场研究分析，采用新颖的四阶段依赖分类法（AI能力、相关性、采用和最终答案正确性），在多个STEM课程的测验场景中观察学生行为。

Result: 发现三个主要结果：1）学生整体AI依赖度低且使用效果不佳；2）负面依赖模式在互动中持续存在；3）某些行为指标能强预测AI依赖。

Conclusion: 研究强调了教育中伦理AI整合的重要性，需要改进入门流程和设计具有依赖校准机制的AI界面，为道德合理且认知丰富的AI实践提供基础见解。

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [14] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: 研究表明AI推理努力与人类决策时间存在平行关系，两者在面对困难任务时都会投入更多认知资源


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型生成中间推理步骤的能力是否与人类决策过程存在相似性，特别是在主观判断任务中

Method: 使用配对联合实验，在内容审核任务上比较三个前沿模型的推理努力与人类决策时间的关系

Result: 推理努力能一致预测人类决策时间，人类和模型在重要变量保持不变时都会投入更多努力，表现出对任务难度的相似敏感性

Conclusion: AI推理努力反映了人类处理主观判断的时间，推理痕迹在可解释性和决策制定方面具有重要潜力

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [15] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: 提出了AI-SearchPlanner强化学习框架，通过解耦搜索规划器和生成器架构，使用小型可训练LLM专门负责搜索规划，提升冻结QA模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的搜索代理使用单一LLM端到端处理搜索规划和问答任务，无法同时优化两种能力。实际AI搜索系统通常使用大型冻结LLM确保高质量QA，因此需要更有效的方法。

Method: 提出三创新：1)解耦搜索规划器和生成器架构；2)搜索规划的双重奖励对齐；3)规划效用和成本的帕累托优化。使用小型可训练LLM专门负责搜索规划。

Result: 在真实数据集上的广泛实验表明，AI-SearchPlanner在效果和效率上均优于现有RL搜索代理，并在不同冻结QA模型和数据域上表现出强泛化能力。

Conclusion: AI-SearchPlanner框架通过专业化搜索规划有效提升了冻结QA模型的性能，实现了效果和效率的双重提升，具有很好的泛化性。

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [16] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: P2C是一个模型无关的框架，通过显式建模特征间的因果关系和生成有序行动序列，为机器学习模型提供可实现的因果一致性反事实解释。


<details>
  <summary>Details</summary>
Motivation: 当前反事实解释方法存在两个主要局限：忽略特征间的因果依赖关系，以及假设所有干预可以同时发生。这导致生成的反事实解释在现实世界中往往不可实现。

Method: P2C使用目标导向的Answer Set Programming系统s(CASP)来生成有序行动序列，明确建模特征间的因果关系，确保每个中间状态都是可行且因果有效的。

Result: P2C能够生成因果一致的反事实计划，其成本计算只统计用户主动做出的改变，提供更现实的成本估计。相比缺乏因果知识的标准规划器，P2C能避免生成非法行动。

Conclusion: P2C框架通过整合因果知识和序列化行动规划，解决了现有反事实解释方法的局限性，为高风险决策场景提供了更实用和可操作的解释方案。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [17] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: TCIA是一个任务中心指令增强框架，通过在离散查询-约束空间中表示指令，在保持多样性的同时确保任务对齐，显著提升LLM在特定任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有指令数据生成方法注重多样性但忽略了任务相关性，而现实应用中大多数需要的是针对特定任务的模型，而非通用模型。

Method: 提出TCIA框架，在离散查询-约束空间中系统扩展指令，保持多样性的同时确保任务对齐，生成丰富且任务相关的指令集。

Result: 实验显示TCIA将开源LLM在四个真实世界任务特定应用中的性能平均提升8.7%，某些情况下甚至超越领先的闭源模型，且不损害通用指令跟随能力。

Conclusion: TCIA提供了一个可扩展且高效的解决方案，使LLM能够适应现实世界的任务导向应用，在保持通用能力的同时显著提升特定任务性能。

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [18] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 提出Entropy Area Score (EAS)指标，无需外部模型或重复采样，通过集成token级预测熵来量化推理大语言模型生成过程中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 需要一种简单有效的指标来量化推理大语言模型在答案生成过程中的不确定性，避免依赖外部模型或重复采样的复杂性。

Method: EAS通过集成模型自身的token级预测熵来捕捉生成过程中的不确定性演变，无需外部模型或重复采样。

Result: EAS与答案熵在多个模型和数据集上强相关；在训练数据筛选中，EAS识别高潜力样本，在相同样本预算下持续优于通过率过滤，提升数学基准上的学生模型准确率。

Conclusion: EAS是一种高效且可解释的实用工具，适用于大语言模型训练中的不确定性建模和数据质量评估。

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [19] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: AWorld是一个开源系统，通过分布式集群加速智能体-环境交互，实现14.6倍的经验收集速度提升，基于Qwen3-32B训练的智能体在GAIA基准上准确率从21.59%提升至32.23%


<details>
  <summary>Details</summary>
Motivation: 解决智能体AI系统中实践学习范式面临的经验生成效率低下瓶颈问题，特别是在复杂基准测试如GAIA中

Method: 开发AWorld开源系统，通过分布式任务分配在集群上加速智能体-环境交互，实现大规模经验收集，并基于此训练Qwen3-32B智能体

Result: 经验收集速度提升14.6倍，训练后的智能体在GAIA基准上准确率从21.59%提升至32.23%，在最具挑战性的级别上达到16.33%的分数，超越领先的专有模型

Conclusion: AWorld系统和训练出的智能体为完整的智能体AI训练流程提供了实用蓝图，从高效交互到可证明的模型改进

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [20] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 提出了一种基于密码学机制的可治理AI框架，通过外部强制结构合规性来应对AI安全风险，替代传统内部约束方法


<details>
  <summary>Details</summary>
Motivation: AI快速发展带来严重安全风险，现有AI安全方法在面对极端动机和无限智能的AI时存在根本性局限性，无法保证安全性

Method: 提出可治理AI框架，包含规则执行模块、治理规则和可治理安全超级平台，通过密码学机制确保计算不可破解性，实现端到端保护

Result: 通过严格的形式化安全证明验证了机制的安全属性，并在代表性高风险场景中通过原型实现证明了有效性

Conclusion: 该框架为AI安全治理提供了可行且可推广的技术路径，能够消除已识别的攻击向量，确保AI系统的安全可控

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [21] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 提出基于大语言模型的合成数据生成管道，通过总结文档、分解原子事实、构建蕴含关系表来生成带标签的文本-声明对，用于增强健康相关事实核查模型的训练数据。


<details>
  <summary>Details</summary>
Motivation: 健康相关内容的事实核查面临标注训练数据稀缺的挑战，需要有效的数据增强方法来提升模型性能。

Method: 使用LLM总结源文档，分解为原子事实，构建句子-事实蕴含表，从中生成带二元真实性标签的合成文本-声明对，与原始数据结合微调BERT模型。

Result: 在PubHealth和SciFact数据集上，F1分数分别提升了0.019和0.049，显著优于仅使用原始数据训练的模型。

Conclusion: LLM驱动的合成数据增强能有效提升健康相关事实核查器的性能，为解决标注数据稀缺问题提供了可行方案。

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>


### [22] [Human-AI Collaborative Bot Detection in MMORPGs](https://arxiv.org/abs/2508.20578)
*Jaeman Son,Hyunsoo Kim*

Main category: cs.AI

TL;DR: 提出基于对比表示学习和聚类的无监督框架检测MMORPG自动升级机器人，结合LLM辅助验证确保可解释性


<details>
  <summary>Details</summary>
Motivation: MMORPG中自动升级机器人破坏游戏平衡和公平性，检测困难且需要可解释的惩罚依据以避免法律和用户体验问题

Method: 使用对比表示学习和聚类技术无监督识别相似升级模式的角色群体，引入LLM作为辅助审查员验证聚类结果，并提供基于成长曲线的可视化工具

Result: 该协作方法提高了机器人检测工作流程的效率，同时保持了可解释性

Conclusion: 该方法支持MMORPG中可扩展和负责任的机器人监管，通过人机协作确保检测结果的可靠性和可解释性

Abstract: In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling
bots exploit automated programs to level up characters at scale, undermining
gameplay balance and fairness. Detecting such bots is challenging, not only
because they mimic human behavior, but also because punitive actions require
explainable justification to avoid legal and user experience issues. In this
paper, we present a novel framework for detecting auto-leveling bots by
leveraging contrastive representation learning and clustering techniques in a
fully unsupervised manner to identify groups of characters with similar
level-up patterns. To ensure reliable decisions, we incorporate a Large
Language Model (LLM) as an auxiliary reviewer to validate the clustered groups,
effectively mimicking a secondary human judgment. We also introduce a growth
curve-based visualization to assist both the LLM and human moderators in
assessing leveling behavior. This collaborative approach improves the
efficiency of bot detection workflows while maintaining explainability, thereby
supporting scalable and accountable bot regulation in MMORPGs.

</details>


### [23] [Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science](https://arxiv.org/abs/2508.20674)
*Rui Mao,Qian Liu,Xiao Li,Erik Cambria,Amir Hussain*

Main category: cs.AI

TL;DR: 本文回顾了人工智能与认知科学的交叉关系，指出AI发展偏重实用性能而认知基础概念分散，提出未来应构建能深化理解人类心智的AI系统。


<details>
  <summary>Details</summary>
Motivation: 认知科学深刻影响了AI等多个学科，而AI也成为认知研究的重要工具。这种互惠关系促使对AI与认知科学交叉领域进行全面回顾。

Method: 通过综合两个视角的关键贡献，分析AI进展与认知基础的关系，提出未来发展方向。

Result: 发现AI进展主要强调实际任务性能，但其认知基础在概念上仍然分散。

Conclusion: AI在认知科学中的未来不仅在于提升性能，更在于构建能够深化理解人类心智的系统，包括与认知框架对齐、具身文化定位、个性化认知模型开发，以及通过认知共同评估重新思考AI伦理。

Abstract: Cognitive Science has profoundly shaped disciplines such as Artificial
Intelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and
Culture. Many breakthroughs in AI trace their roots to cognitive theories,
while AI itself has become an indispensable tool for advancing cognitive
research. This reciprocal relationship motivates a comprehensive review of the
intersections between AI and Cognitive Science. By synthesizing key
contributions from both perspectives, we observe that AI progress has largely
emphasized practical task performance, whereas its cognitive foundations remain
conceptually fragmented. We argue that the future of AI within Cognitive
Science lies not only in improving performance but also in constructing systems
that deepen our understanding of the human mind. Promising directions include
aligning AI behaviors with cognitive frameworks, situating AI in embodiment and
culture, developing personalized cognitive models, and rethinking AI ethics
through cognitive co-evaluation.

</details>


### [24] [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
*Ares Fabregat-Hernández,Javier Palanca,Vicent Botti*

Main category: cs.AI

TL;DR: 提出基于范畴论的框架提升AI系统可解释性，特别是词嵌入的可视化和比较，将神经网络黑盒算法转化为透明框架，并提供计算和减轻偏见的方法


<details>
  <summary>Details</summary>
Motivation: 解决人工智能系统特别是词嵌入模型的黑盒问题，通过数学框架提供可解释性，使语义空间和算法比较更加透明

Method: 构建范畴L_T和P_T来表示文本语义，重构最大概率选择为范畴概念，建立配置范畴Conf和词嵌入范畴Emb，定义偏差作为装饰，证明GloVe、Word2Vec与MDS算法的等价性

Result: 实现了从神经网络黑盒算法到透明框架的转换，提供了维度无关的语义空间定义，建立了精确比较词嵌入的数学方法

Conclusion: 该范畴论框架显著提升了AI系统的可解释性，为语义空间分析和偏见减轻提供了数学基础，推动了可解释人工智能领域的发展

Abstract: The paper introduces a novel framework based on category theory to enhance
the explainability of artificial intelligence systems, particularly focusing on
word embeddings. Key topics include the construction of categories
$\mathcal{L}_T$ and $\mathcal{P}_T$, providing schematic representations of the
semantics of a text $ T $, and reframing the selection of the element with
maximum probability as a categorical notion. Additionally, the monoidal
category $\mathcal{P}_T$ is constructed to visualize various methods of
extracting semantic information from $T$, offering a dimension-agnostic
definition of semantic spaces reliant solely on information within the text.
  Furthermore, the paper defines the categories of configurations Conf and word
embeddings $\mathcal{Emb}$, accompanied by the concept of divergence as a
decoration on $\mathcal{Emb}$. It establishes a mathematically precise method
for comparing word embeddings, demonstrating the equivalence between the GloVe
and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural
network algorithms (black box) to a transparent framework. Finally, the paper
presents a mathematical approach to computing biases before embedding and
offers insights on mitigating biases at the semantic space level, advancing the
field of explainable artificial intelligence.

</details>


### [25] [Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision](https://arxiv.org/abs/2508.20729)
*Ao Cheng,Lei Zhang,Guowei He*

Main category: cs.AI

TL;DR: 提出一个基于多LLM协作的智能体框架，通过顾问-程序员-评审员三模块的协作机制，实现科学计算问题的自动代码生成和迭代优化


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学计算领域存在代码生成错误和非物理解决方案的问题，需要建立更可靠的自主代码生成框架

Method: 采用"重写-解决-评审-修订"逻辑链，通过三个推理LLM（顾问、评审员、程序员）协作：顾问进行知识转移和问题重写，程序员生成执行代码，评审员通过运行时反馈进行自我调试和优化

Result: 相比单模型，该协作框架显著提高了无错误代码生成率，减少了非物理解的出现，提升了最新推理模型的平均执行成功率

Conclusion: 该智能体框架建立了基于自然语言描述的自动代码生成和评审机制，为科学计算提供了一个有前景的新范式

Abstract: Large language models (LLMs) serve as an active and promising field of
generative artificial intelligence and have demonstrated abilities to perform
complex tasks in multiple domains, including mathematical and scientific
reasoning. In this work, we construct a novel agent framework for solving
representative problems in scientific computing. The proposed agent,
incorporating a "rewriting-resolution-review-revision" logical chain via three
reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,
respectively), is integrated in a collaborative and interactive manner. The
Consultant module endows the agent with knowledge transfer capabilities to link
problems to professional domain insights, thereby rewriting problem
descriptions through text augmentation. The Programmer module is responsible
for generating and executing well-structured code to deliver the problem
resolution. The Reviewer module equips the agent with the capacity for
self-debugging and self-refinement through interactive feedback with code
runtime outputs. By leveraging the end-to-end review mechanism, the executable
code provided by the Programmer attains the iterative revision. A comprehensive
evaluation is conducted on the performance of the proposed agent framework in
solving PDEs, ill-conditioned linear systems, and data-driven physical analysis
problems. Compared to single-model, this collaborative framework significantly
improves the bug-free code generation rate and reduces the occurrence of
non-physical solutions, thereby establishing a highly reliable framework for
autonomous code generation based on natural language descriptions. The review
mechanism improved the average execution success (bug-free code and non-NaN
solutions) rate of the latest reasoning models. In summary, our agent framework
establishes automatic code generation and review as a promising scientific
computing paradigm.

</details>


### [26] [Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control](https://arxiv.org/abs/2508.20784)
*Yifan Zhang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的单机器学习框架，通过独特的状态空间编码和契合契约奖励函数，解决了传统多机器学习在公交车调度中的数据不平衡和收敛问题。


<details>
  <summary>Details</summary>
Motivation: 传统的多机器强化学习方法在环线设置中忽视了实际运营的异构路线、时刻表、波动需求和变化车队规模等现实特征，导致数据不平衡和收敛困难。

Method: 将多机器问题重构为单机器问题，通过在数值特征（间隔时间、占用率、速度）基础上增加分类标识符（车辆ID、站点ID、时间段）来扩充状态空间。设计了基于契合契约的奖励函数，平衡均匀间隔时间和计划遵守。使用修改版的SAC算法进行训练。

Result: 在随机条件下，该方法的表现（-430k）显著优于MADDPG等对照方法（-530k），实现了更稳定和更优秀的性能。

Conclusion: 通过分类结构化和计划感知奖励的增强，单机器深度强化学习可以有效管理非环形实际环境下的公交车持续控制，为MARL框架提供了一种健壮、可扩展的替代方案。

Abstract: Bus bunching remains a challenge for urban transit due to stochastic traffic
and passenger demand. Traditional solutions rely on multi-agent reinforcement
learning (MARL) in loop-line settings, which overlook realistic operations
characterized by heterogeneous routes, timetables, fluctuating demand, and
varying fleet sizes. We propose a novel single-agent reinforcement learning
(RL) framework for bus holding control that avoids the data imbalance and
convergence issues of MARL under near-realistic simulation. A bidirectional
timetabled network with dynamic passenger demand is constructed. The key
innovation is reformulating the multi-agent problem into a single-agent one by
augmenting the state space with categorical identifiers (vehicle ID, station
ID, time period) in addition to numerical features (headway, occupancy,
velocity). This high-dimensional encoding enables single-agent policies to
capture inter-agent dependencies, analogous to projecting non-separable inputs
into a higher-dimensional space. We further design a structured reward function
aligned with operational goals: instead of exponential penalties on headway
deviations, a ridge-shaped reward balances uniform headways and schedule
adherence. Experiments show that our modified soft actor-critic (SAC) achieves
more stable and superior performance than benchmarks, including MADDPG (e.g.,
-430k vs. -530k under stochastic conditions). These results demonstrate that
single-agent deep RL, when enhanced with categorical structuring and
schedule-aware rewards, can effectively manage bus holding in non-loop,
real-world contexts. This paradigm offers a robust, scalable alternative to
MARL frameworks, particularly where agent-specific experiences are imbalanced.

</details>


### [27] [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
*Jessica Lundin,Guillaume Chabot-Couture*

Main category: cs.AI

TL;DR: 提出了首个动态系统化的医学指南基准测试，通过图结构转换WHO IMCI手册，生成400+问题和3.3+万亿种组合，全面覆盖指南关系，用于评估LLM在医疗任务中的能力差距。


<details>
  <summary>Details</summary>
Motivation: 传统人工构建的基准测试覆盖范围有限，无法系统评估LLM在复杂医疗指南理解、严重程度分级、治疗方案和随访护理等方面的能力，需要动态可扩展的解决方案。

Method: 将WHO IMCI手册转换为有向图（200+节点，300+边），通过图遍历生成包含年龄特定场景和上下文干扰项的问题，确保临床相关性。

Result: 模型在症状识别方面表现良好（45-67%准确率），但在严重程度分级、治疗方案和随访护理方面存在困难，揭示了通用评估无法发现的特定能力差距。

Conclusion: 图基方法成功解决了人工基准测试的覆盖限制，为创建可动态生成、防污染的综合基准提供了可扩展解决方案，同时支持LLM的后训练优化。

Abstract: We present a first known prototype of a dynamic, systematic benchmark of
medical guidelines for 400+ questions, with 3.3+ trillion possible
combinations, covering 100\% of guideline relationships. We transformed the WHO
IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,
treatments, follow-ups, severities) and 300+ edges, then used graph traversal
to generate questions that incorporated age-specific scenarios and contextual
distractors to ensure clinical relevance. Our graph-based approach enables
systematic evaluation across clinical tasks (45-67\% accuracy), and we find
models excel at symptom recognition but struggle with triaging severity,
treatment protocols and follow-up care, demonstrating how customized benchmarks
can identify specific capability gaps that general-domain evaluations miss.
Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training
(supervised finetuning, GRPO, DPO), where correct answers provide high-reward
samples without expensive human annotation. The graph-based approach
successfully addresses the coverage limitations of manually curated benchmarks.
This methodology is a step toward scalable, contamination-resistant solution
for creating comprehensive benchmarks that can be dynamically generated,
including when the guidelines are updated. Code and datasets are available at
https://github.com/jessicalundin/graph_testing_harness

</details>


### [28] [A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling](https://arxiv.org/abs/2508.20953)
*Vipul Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.AI

TL;DR: 提出多目标遗传算法解决医院人员排班问题，平衡成本控制、患者护理覆盖和员工满意度，相比传统手动排班性能提升66%


<details>
  <summary>Details</summary>
Motivation: 医疗行业人员排班面临患者负荷波动、临床技能多样性和控制人力成本等多重挑战，需要平衡相互冲突的目标

Method: 使用多目标遗传算法(MOO-GA)，将医院单元人员排班建模为多目标优化问题，包含小时预约驱动需求和模块化班次等现实复杂性

Result: 在典型医院单元数据集上验证，算法生成的排班方案平均比传统手动排班基准性能提升66%，提供高质量的非支配解集

Conclusion: 该方法有效管理关键运营目标和员工中心目标之间的权衡，为护士管理者和医院管理者提供实用的决策支持工具

Abstract: Workforce scheduling in the healthcare sector is a significant operational
challenge, characterized by fluctuating patient loads, diverse clinical skills,
and the critical need to control labor costs while upholding high standards of
patient care. This problem is inherently multi-objective, demanding a delicate
balance between competing goals: minimizing payroll, ensuring adequate staffing
for patient needs, and accommodating staff preferences to mitigate burnout. We
propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital
unit workforce scheduling problem as a multi-objective optimization task. Our
model incorporates real-world complexities, including hourly appointment-driven
demand and the use of modular shifts for a multi-skilled workforce. By defining
objective functions for cost, patient care coverage, and staff satisfaction,
the GA navigates the vast search space to identify a set of high-quality,
non-dominated solutions. Demonstrated on datasets representing a typical
hospital unit, the results show that our MOO-GA generates robust and balanced
schedules. On average, the schedules produced by our algorithm showed a 66\%
performance improvement over a baseline that simulates a conventional, manual
scheduling process. This approach effectively manages trade-offs between
critical operational and staff-centric objectives, providing a practical
decision support tool for nurse managers and hospital administrators.

</details>


### [29] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 提出了一种可微分的神经符号架构和专用损失函数，用于学习解决NP难推理问题，在多个基准测试中表现出高效的学习能力和训练时间优势。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型在处理离散推理和优化问题时的困难，需要开发能够从自然输入中学习解决NP难推理问题的神经架构。

Method: 采用可微分神经符号架构和新的概率损失函数，能够同时学习约束和目标，将组合求解器移出训练循环以实现可扩展训练，同时保持精确推理以获得最大准确性。

Result: 在数独问题的符号、视觉和多解变体上，该方法比其他混合方法需要更少的训练时间；在视觉最小割/最大割任务上，优化后悔度优于专用后悔损失；还能有效学习蛋白质设计的能量优化公式。

Conclusion: 该架构能够高效地从自然输入中学习解决NP难推理问题，在多个基准测试中表现出优越的性能和训练效率。

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [30] [ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery](https://arxiv.org/abs/2508.20996)
*Junda Wang,Zonghai Yao,Zhichao Yang,Lingxi Li,Junhui Qian,Hong Yu*

Main category: cs.AI

TL;DR: ChatThero是一个多智能体对话框架，结合动态患者建模和基于认知行为疗法与动机性访谈的适应性说服策略，在药物使用障碍治疗中显著提升患者动机和治疗信心。


<details>
  <summary>Details</summary>
Motivation: 全球有3600多万人受药物使用障碍影响，但由于污名化、动机障碍和个性化支持有限，很少人获得有效治疗。现有语言模型系统缺乏与临床验证策略的紧密整合，限制了在成瘾康复中的效果。

Method: 构建多智能体对话框架，结合动态患者建模、情境敏感的治疗对话和基于CBT与MI的适应性说服策略。使用包含易、中、高阻力级别的高保真合成基准，通过监督微调(SFT)和直接偏好优化(DPO)两阶段管道进行训练。

Result: ChatThero使患者动机平均提升41.5%，治疗信心增加0.49%，在困难案例中比GPT-4o少用26%的对话轮次。自动和人工临床评估均显示其在同理心、响应性和行为真实性方面评分更高。

Conclusion: 该框架支持严格、隐私保护的治疗对话研究，为研究和临床转化提供了稳健、可复现的基础。

Abstract: Substance use disorders (SUDs) affect over 36 million people worldwide, yet
few receive effective care due to stigma, motivational barriers, and limited
personalized support. Although large language models (LLMs) show promise for
mental-health assistance, most systems lack tight integration with clinically
validated strategies, reducing effectiveness in addiction recovery. We present
ChatThero, a multi-agent conversational framework that couples dynamic patient
modeling with context-sensitive therapeutic dialogue and adaptive persuasive
strategies grounded in cognitive behavioral therapy (CBT) and motivational
interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,
Medium, and Hard resistance levels, and train ChatThero with a two-stage
pipeline comprising supervised fine-tuning (SFT) followed by direct preference
optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in
patient motivation, a 0.49\% increase in treatment confidence, and resolves
hard cases with 26\% fewer turns than GPT-4o, and both automated and human
clinical assessments rate it higher in empathy, responsiveness, and behavioral
realism. The framework supports rigorous, privacy-preserving study of
therapeutic conversation and provides a robust, replicable basis for research
and clinical translation.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [31] [Flexible XL-MIMO via Array Configuration Codebook: Codebook Design and Array Configuration Training](https://arxiv.org/abs/2508.20369)
*Haiquan Lu,Hongqi Min,Yong Zeng,Shaodan Ma*

Main category: cs.IT

TL;DR: 本文提出阵列配置码本(ACC)概念，通过动态像素激活实现灵活的大规模MIMO架构，相比传统天线选择方案能提升系统性能并降低硬件成本。


<details>
  <summary>Details</summary>
Motivation: 解决XL-MIMO技术面临的高硬件成本和功耗问题，实现成本效益高的灵活大规模MIMO部署。

Method: 设计包含多种经典阵列配置的码本，提出两阶段扫描训练方案（阵列级和像素级扫描），并推导了贪婪天线选择方案的闭式SINR表达式。

Result: 仿真结果表明，在多用户通信和无线定位场景下，码本优化方案能有效提升系统性能。

Conclusion: ACC概念为XL-MIMO提供了一种灵活且成本效益高的实现方式，通过动态阵列配置优化能够显著提升通信和定位性能。

Abstract: XL-MIMO emerges as a promising technology to achieve unprecedented
enhancements in spectral efficiency and spatial resolution, via
orders-of-magnitude increase in the antenna array size. However, the practical
issues of high hardware cost and power consumption pose great challenges
towards the cost-effective implementation of XL-MIMO. To address such
challenges, this paper proposes a novel concept called array configuration
codebook (ACC), which enables flexible XL-MIMO cost-effectively and improves
the system performance compared with conventional antenna selection (AS)
schemes with limited number of RF chains. Specifically, ACC refers to a set of
pre-designed array configuration codewords, where each codeword specifies the
positions of activated antenna pixels. Then, flexible XL-MIMO architecture can
be enabled via dynamical pixel activation based on the designed ACC, without
having to exhaustively try all possible combinations of the antenna pixels
activations. As an illustration, we give a specific codebook design,
encompassing the classic compact array (CA), uniform sparse array (USA),
modular array (MoA), nested array (NA), and co-prime array (CPA), and each
codeword is specified by one array configuration parameter. With the designed
ACC, array configuration training is considered for multi-UE communication to
maximize the sum rate. To reduce the training overhead of exhaustive scanning,
a two-stage scanning scheme is proposed, including the array- and pixel-level
scanning. For comparison, the greedy AS scheme is proposed, where the resulting
incremental SINR expression by activating antenna pixel sequentially is derived
in closed-form. Subsequently, array configuration training is extended to the
wireless localization scenario. Simulation results demonstrate the
effectiveness of codeword optimization for scenarios of multi-UE communication
and wireless localization.

</details>


### [32] [Secure Satellite Communications via Multiple Aerial RISs: Joint Optimization of Reflection, Association, and Deployment](https://arxiv.org/abs/2508.20455)
*Zhaole Wang,Naijin Liu,Xiao Tang,Shuai Yuan,Chenxi Wang,Zhi Zhai,Qinghe Du,Jinxin Liu*

Main category: cs.IT

TL;DR: 本文研究了基于空中可重构智能表面(ARIS)辅助的多波束多组卫星通信安全传输，通过联合优化传输反射波束成形、ARIS-组关联和ARIS部署来最大化安全通信性能


<details>
  <summary>Details</summary>
Motivation: 卫星通信是未来6G网络的关键使能技术，但其广覆盖和高链路衰减特性给物理层安全带来了重大挑战，需要新的安全增强方案

Method: 采用块坐标下降框架，将联合优化问题分解为多个子问题迭代求解，处理混合整数和非凸优化问题

Result: 仿真结果表明所提出的ARIS辅助多波束卫星系统在各种网络场景下都能显著提升安全通信性能

Conclusion: 该研究为未来安全卫星网络中智能表面的部署和优化提供了有价值的见解

Abstract: Satellite communication is envisioned as a key enabler of future 6G networks,
yet its wide coverage with high link attenuation poses significant challenges
for physical layer security. In this paper, we investigate secure multi-beam,
multi-group satellite communications assisted by aerial reconfigurable
intelligent surfaces (ARISs). To maximize the sum of achievable multicast rates
among the groups while constraining wiretap rates, we formulate a joint
optimization problem involving transmission and reflection beamforming,
ARIS-group association, and ARIS deployment. Due to the mixed-integral and
non-convex nature of the formulated problem, we propose to decompose the
problem and employ the block coordinate descent framework that iteratively
solves the subproblems. Simulation results demonstrate that the proposed
ARIS-assisted multi-beam satellite system provides a notable improvement in
secure communication performance under various network scenarios, offering
useful insights into the deployment and optimization of intelligent surfaces in
future secure satellite networks.

</details>


### [33] [The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation](https://arxiv.org/abs/2508.20806)
*Moriba Jah,Van Haslett*

Main category: cs.IT

TL;DR: 提出了一种基于可能性理论和认知谦逊的非贝叶斯滤波框架ESPF，通过兼容性加权支持更新、惊奇感知剪枝和自适应离散化来处理认知不确定性，避免传统方法在稀疏或对抗性传感环境中的过度自信问题。


<details>
  <summary>Details</summary>
Motivation: 传统状态估计方法依赖概率假设，经常将认知不确定性压缩为标量信念，在稀疏或对抗性传感环境中容易产生过度自信。需要一种能够动态调整信念支持范围、不需要先验统计校准的鲁棒估计方法。

Method: 使用可能性理论构建ESPF框架，采用兼容性加权支持更新、惊奇感知剪枝和稀疏网格正交自适应离散化。通过Choquet积分基于动态认知容量函数融合竞争假设，使用序数逻辑而非积分来更新信念。

Result: ESPF能够根据信息结构动态收缩或扩展信念支持范围，无需先验统计校准，在缺乏先验、误导性或认知上不合理的先验情况下支持鲁棒估计。

Conclusion: 这项工作在如何协调推理、证据和无知方面提出了基础性转变，为状态估计提供了一种新的非贝叶斯框架，特别适用于传统方法容易失效的复杂传感环境。

Abstract: Traditional state estimation methods rely on probabilistic assumptions that
often collapse epistemic uncertainty into scalar beliefs, risking
overconfidence in sparse or adversarial sensing environments. We introduce the
Epistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework
fully grounded in possibility theory and epistemic humility. ESPF redefines the
evolution of belief over state space using compatibility-weighted support
updates, surprisalaware pruning, and adaptive dispersion via sparse grid
quadrature. Unlike conventional filters, ESPF does not seek a posterior
distribution, but rather maintains a structured region of plausibility or
non-rejection, updated using ordinal logic rather than integration. For
multi-model inference, we employ the Choquet integral to fuse competing
hypotheses based on a dynamic epistemic capacity function, generalizing
classical winner-take-all strategies. The result is an inference engine capable
of dynamically contracting or expanding belief support in direct response to
information structure, without requiring prior statistical calibration. This
work presents a foundational shift in how inference, evidence, and ignorance
are reconciled, supporting robust estimation where priors are unavailable,
misleading, or epistemically unjustified.

</details>


### [34] [Precoded Polar Product Decoder Based on Soft-Output SCL Decoding and Maximization of Generalized Mutual Information](https://arxiv.org/abs/2508.20580)
*Nicolás Alvarez Prado,Andreas Straßhofer*

Main category: cs.IT

TL;DR: 结合码本概率的比特软信息生成和离线计算的GMI最大化缩放系数，优化预编码极化码乘积码的迭代译码性能


<details>
  <summary>Details</summary>
Motivation: 提高乘积码迭代译码的性能，通过更精确的软信息生成和缩放来改善错误纠正能力

Method: 1) 基于码本概率生成比特软信息，近似考虑SCL译码器所有有效路径的辅助量；2) 使用离线计算的GMI最大化系数缩放消息传递中的软信息；3) 提出SCL译码器的外推版本用于蒙特卡洛密度演化分析

Result: 仿真结果显示相比启发式缩放和仅基于候选列表的软信息生成方法，错误纠正性能显著提升

Conclusion: 所提出的方法能准确预测译码器性能，阈值计算与实测性能匹配良好，为预编码极化码乘积码提供了有效的迭代译码优化方案

Abstract: We combine two approaches to optimize the iterative decoding of product codes
with precoded polar component codes. On one side, we generate bitwise soft
messages based on the codebook probability, an approximation of an auxiliary
quantity that considers all valid decoding paths of a successive cancellation
list (SCL) decoder. On the other side, we scale the soft information during
message passing with offline-computed coefficients, which maximize the
generalized mutual information (GMI) between the channel input and the outgoing
message in each half iteration. Simulation results show significant improvement
of the error-correcting performance compared to heuristic scaling and soft
information generation based solely on the candidate list of the decoder.
Moreover, we present an extrinsic version of the SCL decoder, which we use in a
Monte Carlo density evolution analysis to derive decoding thresholds. The
computed thresholds accurately predict the performance of the decoder.

</details>


### [35] [Polar subcodes for MIMO systems](https://arxiv.org/abs/2508.20684)
*Liudmila Karakchieva,Peter Trifonov*

Main category: cs.IT

TL;DR: 提出了一种用于MIMO系统的极化码联合列表解码方法，包括QR和MMSE检测器，并设计了具有跨天线动态冻结约束的极化子码构造。


<details>
  <summary>Details</summary>
Motivation: 研究极化码在多输入多输出系统中的性能，通过联合解码和检测技术提升系统性能，相比LDPC编码系统获得显著增益。

Method: 采用联合列表解码极化码的方法，结合QR和MMSE检测器，推导近似和精确路径度量，并构建具有跨天线动态冻结约束的极化子码。

Result: 所提出的极化子码在相同速率分配下，相比LDPC编码的MIMO系统获得了显著的性能提升。

Conclusion: 该方法为MIMO系统提供了一种高效的极化码编码方案，在性能上优于传统的LDPC编码方案。

Abstract: Polar-coded multiple-input multiple-output systems are investigated. An
advanced receiver implementing joint list decoding of polar codes and QR- and
MMSE-based detectors is proposed. The approximate and exact path metrics are
derived for joint list decoder of polar codes. A construction of polar subcodes
for MIMO systems with cross-antenna dynamic freezing constraints is proposed.
The obtained polar subcodes provide significant performance gain compared to
LDPC-coded MIMO systems with the same rate allocation.

</details>


### [36] [Achieving Optimal Performance-Cost Trade-Off in Hierarchical Cell-Free Massive MIMO](https://arxiv.org/abs/2508.20704)
*Wei Jiang,Hans D Schotten*

Main category: cs.IT

TL;DR: HCF大规模MIMO通过用中央基站替换部分AP来降低部署成本，同时保持性能。本文首次全面分析HCF上行配置，发现集中式ZF组合在性能和成本效率间达到最优平衡。


<details>
  <summary>Details</summary>
Motivation: 传统无蜂窝大规模MIMO部署成本高，需要解决在降低部署成本的同时保持系统性能的问题。

Method: 建立统一的分析框架支持任意组合方案，提出针对HCF两层架构的分层组合方法，分析频谱效率、用户公平性、系统容量等指标。

Result: HCF系统使用集中式零迫组合方案在性能和成本效率之间达到最优平衡。

Conclusion: 分层无蜂窝架构是降低大规模MIMO部署成本的有效解决方案，集中式ZF组合是最优选择。

Abstract: Cell-free (CF) massive MIMO offers uniform service via distributed access
points (APs), which impose high deployment costs. A novel design called
hierarchical cell-free (HCF) addresses this problem by replacing some APs with
a central base station, thereby lowering the costs of fronthaul network
(wireless sites and fiber cables) while preserving performance. To identify the
optimal uplink configuration in HCF massive MIMO, this paper provides the first
comprehensive analysis, benchmarking it against cellular and CF systems. We
develop a unified analytical framework for spectral efficiency that supports
arbitrary combining schemes and introduce a novel hierarchical combining
approach tailored to HCF two-tier architecture. Through analysis and evaluation
of user fairness, system capacity, fronthaul requirements, and computational
complexity, this paper identifies that HCF using centralized zero-forcing
combining achieves the optimal balance between performance and cost-efficiency.

</details>


### [37] [What is the Most Efficient Technique for Uplink Cell-Free Massive MIMO?](https://arxiv.org/abs/2508.20708)
*Wei Jiang,Hans D. Schotten*

Main category: cs.IT

TL;DR: 本文建立了统一的cell-free大规模MIMO系统分析框架，开发了最大最小功率控制优化策略，并通过综合分析确定了最实用的上行链路技术


<details>
  <summary>Details</summary>
Motivation: 现有cell-free大规模MIMO研究存在方法碎片化和假设不一致的问题（如单天线vs多天线接入点、理想vs空间相关信道），需要统一的评估框架

Method: 建立统一的分析框架（兼容集中式/分布式处理和多种合并方案），开发通用的最大最小功率控制优化策略，对四个关键指标进行综合研究

Result: 通过分析和评估，确定了实际cell-free部署中的最优上行链路技术

Conclusion: 本文提出的统一框架和优化策略能够有效解决现有研究的局限性，为实际cell-free系统部署提供了最优技术选择

Abstract: This paper seeks to determine the most efficient uplink technique for
cell-free massive MIMO systems. Despite offering great advances, existing works
suffer from fragmented methodologies and inconsistent assumptions (e.g.,
single- vs. multi-antenna access points, ideal vs. spatially correlated
channels). To address these limitations, we: (1) establish a unified analytical
framework compatible with centralized/distributed processing and diverse
combining schemes; (2) develop a universal optimization strategy for max-min
power control; and (3) conduct a holistic study among four critical metrics:
worst-case user spectral efficiency (fairness), system capacity, fronthaul
signaling, and computational complexity. Through analyses and evaluation, this
work ultimately identifies the optimal uplink technique for practical cell-free
deployments.

</details>


### [38] [On the non-existence of perfect codes in the sum-rank metric](https://arxiv.org/abs/2508.20940)
*Giuseppe Del Prete,Antonio Roccolano,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 本文研究了和秩度量中的完美码，分析了球体几何和体积界限，确定了双块空间中完美码存在的参数约束，并在多块空间中建立了多个不存在性结果。


<details>
  <summary>Details</summary>
Motivation: 和秩度量是汉明度量和秩度量的推广，在多射网络编码和空时编码中具有重要意义。虽然汉明度量和秩度量中的完美码已被完全分类，但和秩度量中非平凡完美码的存在性仍然是一个开放问题。

Method: 分析和秩度量中球体的几何特性，推导球体体积的界限，研究球体填充界限的应用。对于双块空间，确定完美码存在的显式参数约束；对于多块空间，基于最小距离、可除性条件和码维数建立不存在性结果，并通过球体体积的同余条件提供计算证据。

Result: 获得了和秩度量中完美码存在的参数约束条件，证明了在多块空间中多个参数范围内完美码的不存在性，为和秩度量中完美码的分类提供了重要进展。

Conclusion: 和秩度量中的完美码问题比汉明度量和秩度量更为复杂，本文建立了系统的理论框架和分析方法，为进一步研究和秩度量中的编码理论奠定了基础。

Abstract: We study perfect codes in the sum-rank metric, a generalization of both the
Hamming and rank metrics relevant in multishot network coding and space-time
coding. A perfect code attains equality in the sphere-packing bound,
corresponding to a partition of the ambient space into disjoint metric balls.
While perfect codes in the Hamming and rank metrics are completely classified,
the existence of nontrivial perfect codes in the sum-rank metric remains
largely open. In this paper, we investigate linear perfect codes in the
sum-rank metric. We analyze the geometry of balls and derive bounds on their
volumes, showing how the sphere-packing bound applies. For two-block spaces, we
determine explicit parameter constraints for the existence of perfect codes.
For multiple-block spaces, we establish non-existence results for various
ranges of minimum distance, divisibility conditions, and code dimensions. We
further provide computational evidence based on congruence conditions imposed
by the volume of metric balls.

</details>


### [39] [On Secrecy Capacity of Binary Beampointing Channels with Block Memory and Feedback](https://arxiv.org/abs/2508.20980)
*Siyao Li,Mingzhe Chen,Shuangyang Li,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文研究了具有块记忆和反馈的二进制波束指向信道的保密容量，提出了联合通信和自适应感知方案，并证明了该方案在长块长度下的高效性。


<details>
  <summary>Details</summary>
Motivation: 毫米波系统中波束成形传输和反向散射反馈的安全性研究需求，需要分析在存在被动窃听者情况下的保密通信能力。

Method: 建立二进制波束指向信道模型，考虑合法接收者和窃听者的独立均匀分布角度，提出基于反馈的联合通信和自适应感知方案，推导保密速率的内外边界。

Result: 获得了保密容量的闭式上界，提出的JCAS方案在仿真中显示随着块长度增加，内外边界差距缩小，证明了方案的有效性。

Conclusion: JCAS方案通过策略性地利用反馈来平衡感知合法用户和防止信息泄露的需求，在毫米波系统中具有实际应用价值。

Abstract: This paper investigates the secrecy capacity of the binary beampointing (BBP)
channel with block memory and feedback, a simplified yet insightful model for
millimeter-wave (mmWave) systems with beamformed transmissions and backscatter
feedback. We consider a system where a legitimate receiver and a passive
eavesdropper experience independent and uniformly distributed angular
directions over transmission blocks, with the base station receiving noiseless,
unit-delayed feedback from both, under the per-symbol input cost constraints.
We establish a closed-form upper bound on the secrecy capacity, which is based
on the main channel between the base station and the legitimate receiver.
Moreover, we propose a joint communication and adaptive sensing (JCAS) scheme
and derive its achievable secrecy rate. Simulation results show that the gap
between the inner and outer bounds narrows as the number of block length
increases. This reveals the efficiency of this JCAS scheme, which strategically
leverages feedback to balance the demands of sensing the legitimate user and
preventing information leakage to the eavesdropper.

</details>


### [40] [On the Sensing Capacity of Gaussian "Beam-Pointing" Channels with Block Memory and Feedback](https://arxiv.org/abs/2508.20997)
*Siyao Li,Shuangyang Li,Giuseppe Caire*

Main category: cs.IT

TL;DR: 这篇论文研究5G/6G高频通信中的高斯材斜指向通道，探讨了通信与感知的联合方案，并分析了感知容量的上界和可达性内界。


<details>
  <summary>Details</summary>
Motivation: 为满5G/6G系统中高频无线通信（如毫米波、次太赫茶）的需求，需要解决高斯材斜指向通道中的通信与感知联合优化问题。

Method: 提出了通信与感知联合方案，使用动态规划法求解感知容量上界，并提出了可达性内界。对Q=1的特殊情况进行了优化分析。

Result: 对于Q=1的情况，所提出的传输方案能够实现最优感知速率，并显示了感知与通信性能之间的本质牺性。

Conclusion: 该研究为5G/6G高频通信系统提供了通信与感知联合的理论基础，并证明了在特定条件下可以实现最优感知性能。

Abstract: Driven by the demands of high-frequency wireless communications in 5G and 6G
systems (e.g., mmWave, sub-THz), we explore a state-dependent {\em Gaussian
beam-pointing} (GBP) channel. In this model, the channel state defines an
unknown angle of departure (AoD), which remains constant within each coherence
block of $Q$ time slots but changes independently across blocks. The
transmitter receives strictly causal feedback which may originate from a radar
detection system or explicit feedback from the receiver at the end of each slot
and estimates the AoD at the end of each block. To enhance transmission
efficiency, we propose a joint communication and sensing scheme. While the
communication capacity of the GBP channel has been previously analyzed by the
authors, this work focuses on sensing capacity, characterized by the mutual
information between the channel state and the feedback conditioned on the
transmitted signal. We derive an upper bound using dynamic programming and
propose an achievable inner bound on the sensing capacity, both formulated as
optimization problems. For the special case of $Q=1$, the proposed transmission
scheme achieves the optimal sensing rate and highlights the inherent trade-off
between sensing and communication performance.

</details>
