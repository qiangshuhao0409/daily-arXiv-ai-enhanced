<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.IT](#cs.IT) [Total: 3]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Combined Push-Pull Access Framework for Digital Twin Alignment and Anomaly Reporting](https://arxiv.org/abs/2508.21516)
*Federico Chiariotti,Fabio Saggese,Andrea Munari,Leonardo Badia,Petar Popovski*

Main category: cs.NI

TL;DR: 提出了一种推拉调度器(PPS)介质访问框架，用于动态分配数字孪生中推送更新和拉取更新的通信资源，在保证异常检测性能的同时显著降低信息老化程度。


<details>
  <summary>Details</summary>
Motivation: 数字孪生的准确性依赖于与物理系统的实时同步，需要平衡正常状态下的对齐和异常报告的紧急需求，现有方案在资源分配和时效性方面存在不足。

Method: 设计了推拉调度器(PPS)框架，动态分配推送更新(紧急异常信息)和拉取更新(减少漂移的请求)的通信资源，优化资源使用效率。

Result: 相比最先进方案，在保持相同异常检测保证的前提下，将错误信息的老化年龄(AoII)降低了20%以上，最坏情况下的异常检测AoII从70ms降至20ms。

Conclusion: PPS框架有效解决了数字孪生系统中推送和拉取更新的资源分配问题，在保证系统性能的同时显著提升了信息时效性。

Abstract: A digital twin (DT) contains a set of virtual models of real systems and
processes that are synchronized to their physical counterparts. This enables
experimentation and examination of counterfactuals, simulating the consequences
of decisions in real time. However, the DT accuracy relies on timely updates
that maintain alignment with the real system. We can distinguish between: (i)
pull-updates, which follow a request from the DT to the sensors, to decrease
its drift from the physical state; (ii) push-updates, which are sent directly
by the sensors since they represent urgent information, such as anomalies. In
this work, we devise a push-pull scheduler (PPS) medium access framework, which
dynamically allocates the communication resources used for these two types of
updates. Our scheme strikes a balance in the trade-off between DT alignment in
normal conditions and anomaly reporting, optimizing resource usage and reducing
the drift age of incorrect information (AoII) by over 20% with respect to
state-of-the-art solutions, while maintaining the same anomaly detection
guarantees, as well as reducing the worst-case anomaly detection AoII from 70
ms to 20 ms when considering a 1 ms average drift AoII constraint.

</details>


### [2] [QoS-Aware Proportional Fairness Scheduling for Multi-Flow 5G UEs: A Smart Factory Perspective](https://arxiv.org/abs/2508.21783)
*Mohamed Seliem,Utz Roedig,Cormac Sreenan,Dirk Pesch*

Main category: cs.NI

TL;DR: 本文扩展Simu5G模拟框架支持QFI级多流建模，并提出QoS-PF调度器，在智能工厂场景中优化异构流资源分配，提升截止时间遵守率和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有模拟框架缺乏对5G网络中多并发流量流在QoS Flow Identifier级别的精细化建模能力，无法满足智能工厂中设备同时处理多种QoS要求流量的需求。

Method: 扩展Simu5G支持每QFI建模，设计QoS感知比例公平调度器(QoS-PF)，动态平衡延迟、保证比特率和优先级指标，在包含机器视觉、实时控制和大数据传输的智能工厂场景中进行评估。

Result: QoS-PF调度器在不牺牲吞吐量的情况下，显著提高了截止时间遵守率和公平性，所有扩展以模块化开源方式实现。

Conclusion: 该研究为工业5G部署中高级QoS策略的模拟和分析提供了方法和架构基础，支持未来研究发展。

Abstract: Private 5G networks are emerging as key enablers for smart factories, where a
single device often handles multiple concurrent traffic flows with distinct
Quality of Service (QoS) requirements. Existing simulation frameworks, however,
lack the fidelity to model such multi-flow behavior at the QoS Flow Identifier
(QFI) level. This paper addresses this gap by extending Simu5G to support
per-QFI modeling and by introducing a novel QoS-aware Proportional Fairness
(QoS-PF) scheduler. The scheduler dynamically balances delay, Guaranteed Bit
Rate (GBR), and priority metrics to optimize resource allocation across
heterogeneous flows. We evaluate the proposed approach in a realistic smart
factory scenario featuring edge-hosted machine vision, real-time control loops,
and bulk data transfer. Results show that QoS-PF improves deadline adherence
and fairness without compromising throughput. All extensions are implemented in
a modular and open-source manner to support future research. Our work provides
both a methodological and architectural foundation for simulating and analyzing
advanced QoS policies in industrial 5G deployments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 该研究探讨了架构归纳偏置如何影响大语言模型在指导性对话中的认知行为，通过引入符号支架机制和短期记忆模式来促进苏格拉底式辅导中的结构化推理。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型架构中的归纳偏置如何影响其在教学对话中的认知行为和推理能力，旨在通过结构化设计提升模型的教学效果。

Method: 采用符号支架机制配合短期记忆模式，通过五个系统变体的对照消融实验，使用专家设计的评分标准评估模型输出，包括支架使用、响应性、符号推理和对话记忆等方面。

Result: 完整系统在所有基线变体中表现最佳，移除记忆或符号结构会显著降低关键认知行为，包括抽象能力、适应性探询和概念连续性。

Conclusion: 架构支架能够可靠地塑造大语言模型中涌现的教学策略，支持处理层面的解释，表明结构化设计对模型认知行为具有重要影响。

Abstract: We study how architectural inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding mechanism paired with a short-term memory schema designed
to promote adaptive, structured reasoning in Socratic tutoring. Using
controlled ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which architectural scaffolds can
reliably shape emergent instructional strategies in LLMs.

</details>


### [4] [Addressing accuracy and hallucination of LLMs in Alzheimer's disease research through knowledge graphs](https://arxiv.org/abs/2508.21238)
*Tingxuan Xu,Jiarui Feng,Justin Melendez,Kaleigh Roberts,Donghong Cai,Mingfang Zhu,Donald Elbert,Yixin Chen,Randall J. Bateman*

Main category: cs.AI

TL;DR: 评估GraphRAG在阿尔茨海默病领域的应用效果，比较其与标准GPT-4o在回答专业问题时的质量和可追溯性差异


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学研究中的应用受到幻觉、领域知识有限和缺乏可追溯性等限制，GraphRAG作为改进方法在生物医学等知识密集型领域的评估研究不足

Method: 构建包含50篇论文和70个专家问题的阿尔茨海默病数据库，建立GraphRAG知识库，使用GPT-4o作为LLM进行查询回答，比较GraphRAG与标准GPT-4o的响应质量

Result: 论文评估了两种流行GraphRAG系统的质量和可追溯性，并提供了易于使用的界面和预构建数据库供研究人员测试标准RAG和GraphRAG性能

Conclusion: GraphRAG在生物医学领域具有应用潜力，但需要更多针对特定领域的评估研究，本研究为阿尔茨海默病研究提供了实用的测试平台

Abstract: In the past two years, large language model (LLM)-based chatbots, such as
ChatGPT, have revolutionized various domains by enabling diverse task
completion and question-answering capabilities. However, their application in
scientific research remains constrained by challenges such as hallucinations,
limited domain-specific knowledge, and lack of explainability or traceability
for the response. Graph-based Retrieval-Augmented Generation (GraphRAG) has
emerged as a promising approach to improving chatbot reliability by integrating
domain-specific contextual information before response generation, addressing
some limitations of standard LLMs. Despite its potential, there are only
limited studies that evaluate GraphRAG on specific domains that require
intensive knowledge, like Alzheimer's disease or other biomedical domains. In
this paper, we assess the quality and traceability of two popular GraphRAG
systems. We compile a database of 50 papers and 70 expert questions related to
Alzheimer's disease, construct a GraphRAG knowledge base, and employ GPT-4o as
the LLM for answering queries. We then compare the quality of responses
generated by GraphRAG with those from a standard GPT-4o model. Additionally, we
discuss and evaluate the traceability of several Retrieval-Augmented Generation
(RAG) and GraphRAG systems. Finally, we provide an easy-to-use interface with a
pre-built Alzheimer's disease database for researchers to test the performance
of both standard RAG and GraphRAG.

</details>


### [5] [MultiFluxAI Enhancing Platform Engineering with Advanced Agent-Orchestrated Retrieval Systems](https://arxiv.org/abs/2508.21307)
*Sri Ram Macharla,Sridhar Murthy J,Anjaneyulu Pasala*

Main category: cs.AI

TL;DR: MultiFluxAI是一个创新的AI平台，用于管理产品工程中的多源异构数据，通过生成式AI、向量化和智能代理编排技术提供动态的上下文感知响应。


<details>
  <summary>Details</summary>
Motivation: 解决产品工程领域中管理和整合大量异构数据源的挑战，同时处理现有和新服务相关的查询以提升用户在数字生态系统中的参与度。

Method: 采用先进的AI技术，包括生成式AI、向量化技术和智能代理编排，构建动态的上下文感知响应系统。

Result: 开发出了一个能够处理复杂用户查询的创新AI平台，增强了用户在数字生态系统中的参与体验。

Conclusion: MultiFluxAI平台通过整合多种先进AI技术，成功解决了产品工程中数据管理和查询响应的关键问题，为数字生态系统提供了更智能的服务支持。

Abstract: MultiFluxAI is an innovative AI platform developed to address the challenges
of managing and integrating vast, disparate data sources in product engineering
across application domains. It addresses both current and new service related
queries that enhance user engagement in the digital ecosystem. This platform
leverages advanced AI techniques, such as Generative AI, vectorization, and
agentic orchestration to provide dynamic and context-aware responses to complex
user queries.

</details>


### [6] [Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation](https://arxiv.org/abs/2508.21320)
*Mohsen Nayebi Kerdabadi,Arya Hadizadeh Moghaddam,Dongjie Wang,Zijun Yao*

Main category: cs.AI

TL;DR: LINKO是一个基于大语言模型的医学本体集成学习框架，通过双轴知识传播（垂直和水平）同时利用多个本体图谱，提升医学概念表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单一本体系统或孤立地使用多个本体系统，缺乏跨本体连接的整合，导致概念表示学习局限于本体内部关系。

Method: 1) 使用LLM进行图检索增强的初始化，包含概念描述和本体上下文；2) 双轴知识传播：垂直传播（层级内）和水平传播（跨本体）；3) 作为插件编码器与现有EHR预测模型兼容。

Result: 在两个公共数据集上的实验验证了LINKO优于现有最先进基线方法，在数据有限和罕见疾病预测场景中表现出更强的鲁棒性。

Conclusion: LINKO框架通过整合多个本体系统和双轴知识传播，显著提升了医学概念表示学习的效果，特别是在数据稀缺和罕见病预测方面表现出色。

Abstract: Medical ontology graphs map external knowledge to medical codes in electronic
health records via structured relationships. By leveraging domain-approved
connections (e.g., parent-child), predictive models can generate richer medical
concept representations by incorporating contextual information from related
concepts. However, existing literature primarily focuses on incorporating
domain knowledge from a single ontology system, or from multiple ontology
systems (e.g., diseases, drugs, and procedures) in isolation, without
integrating them into a unified learning structure. Consequently, concept
representation learning often remains limited to intra-ontology relationships,
overlooking cross-ontology connections. In this paper, we propose LINKO, a
large language model (LLM)-augmented integrative ontology learning framework
that leverages multiple ontology graphs simultaneously by enabling dual-axis
knowledge propagation both within and across heterogeneous ontology systems to
enhance medical concept representation learning. Specifically, LINKO first
employs LLMs to provide a graph-retrieval-augmented initialization for ontology
concept embedding, through an engineered prompt that includes concept
descriptions, and is further augmented with ontology context. Second, our
method jointly learns the medical concepts in diverse ontology graphs by
performing knowledge propagation in two axes: (1) intra-ontology vertical
propagation across hierarchical ontology levels and (2) inter-ontology
horizontal propagation within every level in parallel. Last, through extensive
experiments on two public datasets, we validate the superior performance of
LINKO over state-of-the-art baselines. As a plug-in encoder compatible with
existing EHR predictive models, LINKO further demonstrates enhanced robustness
in scenarios involving limited data availability and rare disease prediction.

</details>


### [7] [Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2508.21365)
*Yi Liao,Yu Gu,Yuan Sui,Zining Zhu,Yifan Lu,Guohua Tang,Zhongqian Sun,Wei Yang*

Main category: cs.AI

TL;DR: TiG框架通过将强化学习重新表述为语言建模任务，让大语言模型在游戏环境中直接交互学习程序性知识，同时保持其推理和解释能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务上表现出色，但在简单交互任务中表现不佳，这凸显了陈述性知识和程序性知识之间的关键差距

Method: 将基于强化学习的决策重新表述为语言建模任务：LLMs生成语言引导的策略，通过基于环境反馈的在线强化学习进行迭代优化

Result: TiG成功弥合了陈述性知识和程序性知识之间的差距，以显著更低的数据和计算需求实现了与传统RL方法相竞争的性能

Conclusion: TiG框架不仅提高了交互任务的性能，还通过自然语言解释大大提升了复杂交互任务的透明度和可解释性

Abstract: Large language models (LLMs) excel at complex reasoning tasks such as
mathematics and coding, yet they frequently struggle with simple interactive
tasks that young children perform effortlessly. This discrepancy highlights a
critical gap between declarative knowledge (knowing about something) and
procedural knowledge (knowing how to do something). Although traditional
reinforcement learning (RL) agents can acquire procedural knowledge through
environmental interaction, they often operate as black boxes and require
substantial training data. In contrast, LLMs possess extensive world knowledge
and reasoning capabilities, but are unable to effectively convert this static
knowledge into dynamic decision-making in interactive settings. To address this
challenge, we propose Think in Games (TiG), a novel framework that empowers
LLMs to develop procedural understanding through direct interaction with game
environments, while retaining their inherent reasoning and explanatory
abilities. Specifically, TiG reformulates RL-based decision-making as a
language modeling task: LLMs generate language-guided policies, which are
refined iteratively through online reinforcement learning based on
environmental feedback. Our experimental results show that TiG successfully
bridges the gap between declarative and procedural knowledge, achieving
competitive performance with dramatically lower data and computational demands
compared to conventional RL methods. Moreover, TiG provides step-by-step
natural language explanations for its decisions, greatly improving transparency
and interpretability in complex interactive tasks.

</details>


### [8] [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376)
*Tony Lee,Haoqin Tu,Chi Heem Wong,Zijun Wang,Siwei Yang,Yifan Mai,Yuyin Zhou,Cihang Xie,Percy Liang*

Main category: cs.AI

TL;DR: AHELM是一个新的音频-语言模型基准测试，整合了多个数据集来全面评估10个重要方面，包括音频感知、知识、推理、情感检测、偏见、公平性、多语言性、鲁棒性、毒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的音频-语言模型评估缺乏标准化基准，大多数基准只测试一两个能力，且忽略了公平性、安全性等重要方面，不同模型之间难以进行公平比较。

Method: 创建AHELM基准，整合各种数据集（包括2个新的合成音频-文本数据集PARADE和CoRe-Bench），标准化提示词、推理参数和评估指标，测试了14个开放权重和闭源API的ALM模型以及3个基线系统。

Result: Gemini 2.5 Pro在10个方面中的5个排名第一，但在ASR任务中表现出群体不公平性（p=0.01），而基线系统表现良好，其中一个仅具有语音转文本功能的系统总体排名第5。

Conclusion: AHELM提供了一个全面的音频-语言模型评估框架，揭示了现有模型的优势和不足，特别是公平性问题，该基准将持续更新以纳入新的数据集和模型。

Abstract: Evaluations of audio-language models (ALMs) -- multimodal models that take
interleaved audio and text as input and output text -- are hindered by the lack
of standardized benchmarks; most benchmarks measure only one or two
capabilities and omit evaluative aspects such as fairness or safety.
Furthermore, comparison across models is difficult as separate evaluations test
a limited number of models and use different prompting methods and inference
parameters. To address these shortfalls, we introduce AHELM, a benchmark that
aggregates various datasets -- including 2 new synthetic audio-text datasets
called PARADE, which evaluates the ALMs on avoiding stereotypes, and
CoRe-Bench, which measures reasoning over conversational audio through
inferential multi-turn question answering -- to holistically measure the
performance of ALMs across 10 aspects we have identified as important to the
development and usage of ALMs: audio perception, knowledge, reasoning, emotion
detection, bias, fairness, multilinguality, robustness, toxicity, and safety.
We also standardize the prompts, inference parameters, and evaluation metrics
to ensure equitable comparisons across models. We test 14 open-weight and
closed-API ALMs from 3 developers and 3 additional simple baseline systems each
consisting of an automatic speech recognizer and a language model. Our results
show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits
group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do
not. We also find that the baseline systems perform reasonably well on AHELM,
with one ranking 5th overall despite having only speech-to-text capabilities.
For transparency, all raw prompts, model generations, and outputs are available
on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is
intended to be a living benchmark and new datasets and models will be added
over time.

</details>


### [9] [AI Compute Architecture and Evolution Trends](https://arxiv.org/abs/2508.21394)
*Bor-Sung Liang*

Main category: cs.AI

TL;DR: 本文提出了AI计算的七层架构模型，分析了从底层硬件到上层应用的发展轨迹，并探讨了AI发展中的技术挑战和经济可持续性问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展从学术研究转向实际应用，面临多层次挑战，需要系统性的架构分析来理解AI计算的发展路径和未来趋势。

Method: 提出七层AI计算架构模型（物理层、链路层、神经网络层、上下文层、智能体层、编排层、应用层），通过分析大语言模型的三阶段演进，详细描述每层的发展轨迹和关键技术。

Result: 建立了完整的AI计算架构框架，识别了各层的关键技术挑战和发展趋势，特别是在计算架构扩展策略、LLM发展路径、上下文记忆、智能体生态系统等方面提供了深入分析。

Conclusion: AI发展需要同时解决技术架构和经济可持续性问题，基于互联网行业发展经验，文章为AI未来的发展轨迹提供了预测和指导框架。

Abstract: The focus of AI development has shifted from academic research to practical
applications. However, AI development faces numerous challenges at various
levels. This article will attempt to analyze the opportunities and challenges
of AI from several different perspectives using a structured approach. This
article proposes a seven-layer model for AI compute architecture, including
Physical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer,
Orchestrator Layer, and Application Layer, from bottom to top. It also explains
how AI computing has evolved into this 7-layer architecture through the
three-stage evolution on large-scale language models (LLMs). For each layer, we
describe the development trajectory and key technologies. In Layers 1 and 2 we
discuss AI computing issues and the impact of Scale-Up and Scale-Out strategies
on computing architecture. In Layer 3 we explore two different development
paths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs
and compares it to traditional processor memory. In Layers 5 to 7 we discuss
the trends of AI agents and explore the issues in evolution from a single AI
agent to an AI-based ecosystem, and their impact on the AI industry.
Furthermore, AI development involves not only technical challenges but also the
economic issues to build self-sustainable ecosystem. This article analyzes the
internet industry to provide predictions on the future trajectory of AI
development.

</details>


### [10] [CARJAN: Agent-Based Generation and Simulation of Traffic Scenarios with AJAN](https://arxiv.org/abs/2508.21411)
*Leonard Frank Neis,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: CARJAN是一个基于AJAN多智能体框架和CARLA驾驶模拟器的新型工具，用于半自动化生成和模拟包含行人、骑车人和自动驾驶车辆的城市交通场景。


<details>
  <summary>Details</summary>
Motivation: 当前城市交通场景中不同类型交互智能体（行人、骑车人、自动驾驶车辆）的建模和虚拟仿真仍然是一个挑战，需要用户友好的工具来支持场景生成和模拟。

Method: 基于AJAN多智能体工程框架和CARLA驾驶模拟器，提供可视化用户界面用于交通场景布局的建模、存储和维护，利用SPARQL行为树进行智能体决策和交互。

Result: 开发了CARJAN工具，实现了在CARLA中交互式、智能体驱动的虚拟交通场景生成和仿真的首个集成方法。

Conclusion: CARJAN为城市交通场景的建模和仿真提供了一个有效的半自动化解决方案，支持多种类型智能体的交互和动态场景模拟。

Abstract: User-friendly modeling and virtual simulation of urban traffic scenarios with
different types of interacting agents such as pedestrians, cyclists and
autonomous vehicles remains a challenge. We present CARJAN, a novel tool for
semi-automated generation and simulation of such scenarios based on the
multi-agent engineering framework AJAN and the driving simulator CARLA. CARJAN
provides a visual user interface for the modeling, storage and maintenance of
traffic scenario layouts, and leverages SPARQL Behavior Tree-based
decision-making and interactions for agents in dynamic scenario simulations in
CARLA. CARJAN provides a first integrated approach for interactive, intelligent
agent-based generation and simulation of virtual traffic scenarios in CARLA.

</details>


### [11] [A General Framework of Epistemic Forgetting and its Instantiation by Ranking Functions](https://arxiv.org/abs/2508.21441)
*Christoph Beierle,Alexander Hahn,Diana Howey,Gabriele Kern-Isberner,Kai Sauerwald*

Main category: cs.AI

TL;DR: 本文从认知角度研究知识管理中的遗忘操作，提出了五种通用类型的认知遗忘，并在Spohn排序函数中实例化了七种具体遗忘操作，通过丰富的公理体系评估了这些操作。


<details>
  <summary>Details</summary>
Motivation: 现有的遗忘操作主要基于经典逻辑，如变量消除和AGM信念修正理论中的收缩操作。本文旨在从认知状态的角度研究遗忘操作，探索在更丰富语义结构下的遗忘含义。

Method: 采用认知视角，在具有清晰命题逻辑联系的认知状态中研究遗忘操作。提出了五种通用类型的认知遗忘，并在Spohn排序函数中实例化了七种具体操作。借鉴逻辑编程和AGM理论中的遗忘公理，建立了丰富的评估公理体系。

Result: 对所有具体的遗忘操作进行了全面的公理评估，得出了新颖的综合概览，突显了不同遗忘算子之间的差异和共同点。

Conclusion: 本文通过将遗忘操作提升到认知层面，为知识管理中的遗忘操作提供了更全面的理论框架和评估体系，揭示了不同遗忘方法的特性和适用场景。

Abstract: Forgetting as a knowledge management operation deliberately ignores parts of
the knowledge and beliefs of an agent, for various reasons. Forgetting has many
facets, one may want to forget parts of the syntax, a proposition, or a
conditional. In the literature, two main operators suitable for performing
forgetting have been proposed and investigated in depth: First, variable
elimination is a syntactical method that blends out certain atomic variables to
focus on the rest of the language. It has been mainly used in the area of logic
programming and answer set programming. Second, contraction in AGM belief
revision theory effectively removes propositions from belief sets under logical
deduction. Both operations rely mainly on classical logics. In this article, we
take an epistemic perspective and study forgetting operations in epistemic
states with richer semantic structures, but with clear links to propositional
logic. This allows us to investigate what forgetting in the epistemic
background means, thereby lifting well-known and novel forgetting operations to
the epistemic level. We present five general types of epistemic forgetting and
instantiate them with seven concrete forgetting operations for Spohn's ranking
functions. We take inspiration from postulates of forgetting both from logic
programming and AGM theory to propose a rich landscape of axioms for evaluating
forgetting operations. Finally, we evaluate all concrete forgetting operations
according to all postulates, leading to a novel comprehensive overview
highlighting differences and commonalities among the forgetting operators.

</details>


### [12] [Learning Lifted Action Models From Traces of Incomplete Actions and States](https://arxiv.org/abs/2508.21449)
*Niklas Jansen,Jonas Gösgens,Hector Geffner*

Main category: cs.AI

TL;DR: 本文提出了一种从状态-动作轨迹中学习STRIPS+模型的新方法SYNTH，用于解决滑动拼图等领域的模型学习问题，其中状态和动作信息都不完整。


<details>
  <summary>Details</summary>
Motivation: 现有的模型学习方法大多假设动作是完整的STRIPS动作或所有域谓词都可观察，但在实际应用中，状态往往只包含部分信息，动作参数也不完整。本文旨在解决这种更现实的学习场景。

Method: 引入STRIPS+变体，允许动作参数在前提条件中隐式存在，并提出SYNTH算法通过构建分层的预处理表达式序列来学习STRIPS+模型。

Result: SYNTH算法被证明具有正确性和完备性，并在从现有STRIPS域派生的STRIPS+模型生成的状态-动作轨迹上测试了可扩展性。

Conclusion: 该方法能够有效处理状态和动作信息不完整的现实学习场景，为从部分观察数据中学习动作模型提供了可行的解决方案。

Abstract: Consider the problem of learning a lifted STRIPS model of the sliding-tile
puzzle from random state-action traces where the states represent the location
of the tiles only, and the actions are the labels up, down, left, and right,
with no arguments. Two challenges are involved in this problem. First, the
states are not full STRIPS states, as some predicates are missing, like the
atoms representing the position of the ``blank''. Second, the actions are not
full STRIPS either, as they do not reveal all the objects involved in the
actions effects and preconditions. Previous approaches have addressed different
versions of this model learning problem, but most assume that actions in the
traces are full STRIPS actions or that the domain predicates are all
observable. The new setting considered in this work is more ``realistic'', as
the atoms observed convey the state of the world but not full STRIPS states,
and the actions reveal the arguments needed for selecting the action but not
the ones needed for modeling it in STRIPS. For formulating and addressing the
learning problem, we introduce a variant of STRIPS, which we call STRIPS+,
where certain STRIPS action arguments can be left implicit in preconditions
which can also involve a limited form of existential quantification. The
learning problem becomes the problem of learning STRIPS+ models from STRIPS+
state-action traces. For this, the proposed learning algorithm, called SYNTH,
constructs a stratified sequence (conjunction) of precondition expressions or
``queries'' for each action, that denote unique objects in the state and ground
the implicit action arguments in STRIPS+. The correctness and completeness of
SYNTH is established, and its scalability is tested on state-action traces
obtained from STRIPS+ models derived from existing STRIPS domains.

</details>


### [13] [MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.21475)
*Xijia Tao,Yihua Teng,Xinxing Su,Xinyu Fu,Jihao Wu,Chaofan Tao,Ziru Liu,Haoli Bai,Rui Liu,Lingpeng Kong*

Main category: cs.AI

TL;DR: MMSearch-Plus是一个包含311个任务的基准测试，专门设计用于评估多模态语言模型在需要深度视觉推理、来源验证和长时程工具使用的复杂网络浏览任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态浏览基准测试往往可以通过浅层的固定工作流程解决，无法真正测试模型在细粒度视觉推理、来源验证和长时程工具使用方面的能力。

Method: 采用空间-时间外推法构建测试项，每个任务包含多个弱局部视觉信号，需要通过迭代文本-图像搜索进行提取和传播，并在检索噪声下进行交叉验证。

Result: 最佳模型(o3)在无搜索情况下达到15.1%准确率，在框架内搜索后达到36.0%；开源模型Qwen-2.5-VL-72B-Instruct在无搜索时为0.0%，20轮搜索后为6.9%。

Conclusion: 该基准测试揭示了当前多模态模型在来源验证、基于部件的推理和长时程规划方面的显著不足，为未来模型开发提供了重要指导。

Abstract: Large multimodal language models (MLLMs) are increasingly deployed as web
agents, yet many multimodal browsing benchmarks can be solved by shallow, fixed
workflows that lean on high-recall image search and nearby text-masking the
genuinely multimodal challenges of fine-grained visual reasoning, provenance
verification, and long-horizon tool use. We introduce MMSearch-Plus, a
benchmark of 311 tasks that highly demand multimodal understanding while
preserving the difficulty profile of strong text-only browsing suites. Each
item is constructed to contain multiple weak, localized visual signals that
must be extracted, propagated through iterative text-image search, and
cross-validated under retrieval noise before answering. Our curation procedure,
Spatial-Temporal Extrapolation, seeds questions whose answers require
extrapolating from spatial cues (micro-text, part-level appearance, layouts,
signage) and temporal traces (broadcast overlays, seasonal context) to
out-of-image facts such as events, dates, and venues. We provide a
model-agnostic agent framework with browsing tools and evaluate a range of
closed and open MLLMs. The strongest agent (o3) attains 15.1% without search
and 36.0% accuracy with rollout under our framework, while a strong open-source
model (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20
rounds of search. Beyond answer accuracy, we assess bounding-box production and
cropped-image search, and conduct an error analysis that surfaces failures in
source verification, part-based reasoning, and long-horizon planning.

</details>


### [14] [Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by Phronesis](https://arxiv.org/abs/2508.21517)
*Sweta Kaman,Ankita Sharma,Romi Banerjee*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于Z数的模糊推理系统，用于计算智慧评分和信心度，将智慧形式化为具有不确定性的多维构造。


<details>
  <summary>Details</summary>
Motivation: 当前智慧测量依赖自我报告，缺乏谦虚和不确定性的考虑，需要一种考虑多维度和信心度的计算框架来改善心理学测量并支持人道主义AI。

Method: 使用Z数模糊推理系统，将智慧表达为智慧分数（限制）和信心度分数（确定性）。通过100名参与者的语言响应分析，映射到5个智慧组成成分，使用21条规则和高斯核密度估计进行结合。

Result: 证明概念研究显示，系统产生的双属性智慧表征与现有量表呈现中等但显著相关，与无关特质关联微弱，支持了聚合效度和区别效度。

Conclusion: 该研究将智慧形式化为具有不确定性意识的多维构造，不仅推进心理学测量，还为AI系统提供了可解释、信心敏感的推理能力，在严格计算与人类判断之间找到安全中间地带。

Abstract: Background: Wisdom is a superordinate construct that embraces perspective
taking, reflectiveness, prosocial orientation, reflective empathetic action,
and intellectual humility. Unlike conventional models of reasoning that are
rigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,
requiring both graded evaluation and self-reflective humility. Current measures
depend on self-reports and seldom reflect the humility and uncertainty inherent
in wise reasoning. A computational framework that takes into account both
multidimensionality and confidence has the potential to improve psychological
science and allow humane AI. Method: We present a fuzzy inference system with Z
numbers, each of the decisions being expressed in terms of a wisdom score
(restriction) and confidence score (certainty). As part of this study,
participants (N = 100) were exposed to culturally neutral pictorial moral
dilemma tasks to which they generated think-aloud linguistic responses, which
were mapped into five theoretically based components of wisdom. The scores of
each individual component were combined using a base of 21 rules, with
membership functions tuned via Gaussian kernel density estimation. Results: In
a proof of concept study, the system produced dual attribute wisdom
representations that correlated modestly but significantly with established
scales while showing negligible relations with unrelated traits, supporting
convergent and divergent validity. Contribution: The contribution is to
formalize wisdom as a multidimensional, uncertainty-conscious construct,
operationalized in the form of Z-numbers. In addition to progressing
measurement in psychology, it calculates how fuzzy Z numbers can provide AI
systems with interpretable, confidence-sensitive reasoning that affords a safe,
middle ground between rigorous computation and human-like judgment.

</details>


### [15] [Counterfactual Scenarios for Automated Planning](https://arxiv.org/abs/2508.21521)
*Nicola Gigante,Francesco Leofante,Andrea Micheli*

Main category: cs.AI

TL;DR: 该论文提出了一种基于反事实场景的新解释范式，用于解释自动规划问题中的高级属性，而不仅仅是最小修改现有计划。


<details>
  <summary>Details</summary>
Motivation: 传统的反事实解释虽然能够帮助诊断故障和推理计划特征，但无法捐捕所解决问题的高级属性。

Method: 提出两种定性的反事实场景实例化方法，通过明确量化必须满足特定属性的计划，寻找对规划问题P的最小修改。

Result: 分析了不同类型修改下生成反事实场景的计算复杂度，并证明生成这些场景的成本通常仅与为P计算计划的成本相同。

Conclusion: 该方法实用可行，为构建实际算法提供了框架，最终提供了一种解释自动规划问题高级属性的实用方法。

Abstract: Counterfactual Explanations (CEs) are a powerful technique used to explain
Machine Learning models by showing how the input to a model should be minimally
changed for the model to produce a different output. Similar proposals have
been made in the context of Automated Planning, where CEs have been
characterised in terms of minimal modifications to an existing plan that would
result in the satisfaction of a different goal. While such explanations may
help diagnose faults and reason about the characteristics of a plan, they fail
to capture higher-level properties of the problem being solved. To address this
limitation, we propose a novel explanation paradigm that is based on
counterfactual scenarios. In particular, given a planning problem $P$ and an
\ltlf formula $\psi$ defining desired properties of a plan, counterfactual
scenarios identify minimal modifications to $P$ such that it admits plans that
comply with $\psi$. In this paper, we present two qualitative instantiations of
counterfactual scenarios based on an explicit quantification over plans that
must satisfy $\psi$. We then characterise the computational complexity of
generating such counterfactual scenarios when different types of changes are
allowed on $P$. We show that producing counterfactual scenarios is often only
as expensive as computing a plan for $P$, thus demonstrating the practical
viability of our proposal and ultimately providing a framework to construct
practical algorithms in this area.

</details>


### [16] [HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining](https://arxiv.org/abs/2508.21540)
*Eduardo Illueca-Fernandez,Kaile Chen,Fernando Seoane,Farhad Abtahi*

Main category: cs.AI

TL;DR: HealthProcessAI是一个基于GenAI的框架，通过集成多个大型语言模型(LLM)来自动解释过程挖掘结果并生成报告，简化了医疗保健领域过程挖掘的应用。


<details>
  <summary>Details</summary>
Motivation: 过程挖掘在医疗保健工作流分析中具有强大潜力，但面临技术复杂性、缺乏标准化方法和实践培训资源有限等障碍，需要更易用的解决方案。

Method: 开发了一个综合框架，包装现有的Python(PM4PY)和R(bupaR)库，集成多个LLM进行自动化过程图解释和报告生成，使用脓毒症进展数据作为概念验证。

Result: 框架成功处理了四个概念验证场景的脓毒症数据，技术性能稳健。LLM评估显示Claude Sonnet-4和Gemini 2.5-Pro获得最高一致性分数(3.79/4.0和3.65/4.0)。

Conclusion: 该框架通过结合结构化分析和AI驱动的解释，代表了将复杂过程挖掘结果转化为医疗保健应用可操作见解的新方法学进展。

Abstract: Process mining has emerged as a powerful analytical technique for
understanding complex healthcare workflows. However, its application faces
significant barriers, including technical complexity, a lack of standardized
approaches, and limited access to practical training resources. We introduce
HealthProcessAI, a GenAI framework designed to simplify process mining
applications in healthcare and epidemiology by providing a comprehensive
wrapper around existing Python (PM4PY) and R (bupaR) libraries. To address
unfamiliarity and improve accessibility, the framework integrates multiple
Large Language Models (LLMs) for automated process map interpretation and
report generation, helping translate technical analyses into outputs that
diverse users can readily understand. We validated the framework using sepsis
progression data as a proof-of-concept example and compared the outputs of five
state-of-the-art LLM models through the OpenRouter platform. To test its
functionality, the framework successfully processed sepsis data across four
proof-of-concept scenarios, demonstrating robust technical performance and its
capability to generate reports through automated LLM analysis. LLM evaluation
using five independent LLMs as automated evaluators revealed distinct model
strengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency
scores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By
integrating multiple Large Language Models (LLMs) for automated interpretation
and report generation, the framework addresses widespread unfamiliarity with
process mining outputs, making them more accessible to clinicians, data
scientists, and researchers. This structured analytics and AI-driven
interpretation combination represents a novel methodological advance in
translating complex process mining results into potentially actionable insights
for healthcare applications.

</details>


### [17] [Revisiting Landmarks: Learning from Previous Plans to Generalize over Problem Instances](https://arxiv.org/abs/2508.21564)
*Issa Hanou,Sebastijan Dumančić,Mathijs de Weerdt*

Main category: cs.AI

TL;DR: 提出了一种跨域自动发现通用地标的新框架，通过状态函数学习重复模式，构建有向通用地标图来指导规划问题求解


<details>
  <summary>Details</summary>
Motivation: 传统地标提取算法在处理具有重复子规划的规划问题时效果有限，需要能够自动捕捉领域重复模式的通用方法

Method: 使用独立于具体对象的状态函数学习通用地标，构建有向通用地标图来描述地标进展和循环可能性，并基于此设计启发式函数

Result: 从少量小规模实例学到的通用地标图对同领域的大规模实例同样有效，当识别到重复循环时，启发式性能相比基线有显著提升

Conclusion: 通用地标能够捕捉可解释的领域信息，可从少量规划中自动发现，对自动化规划器具有实用价值

Abstract: We propose a new framework for discovering landmarks that automatically
generalize across a domain. These generalized landmarks are learned from a set
of solved instances and describe intermediate goals for planning problems where
traditional landmark extraction algorithms fall short. Our generalized
landmarks extend beyond the predicates of a domain by using state functions
that are independent of the objects of a specific problem and apply to all
similar objects, thus capturing repetition. Based on these functions, we
construct a directed generalized landmark graph that defines the landmark
progression, including loop possibilities for repetitive subplans. We show how
to use this graph in a heuristic to solve new problem instances of the same
domain. Our results show that the generalized landmark graphs learned from a
few small instances are also effective for larger instances in the same domain.
If a loop that indicates repetition is identified, we see a significant
improvement in heuristic performance over the baseline. Generalized landmarks
capture domain information that is interpretable and useful to an automated
planner. This information can be discovered from a small set of plans for the
same domain.

</details>


### [18] [Scalable Solution Methods for Dec-POMDPs with Deterministic Dynamics](https://arxiv.org/abs/2508.21595)
*Yang You,Alex Schutz,Zhikun Li,Bruno Lacerda,Robert Skilton,Nick Hawes*

Main category: cs.AI

TL;DR: 提出了确定性分散POMDPs（Det-Dec-POMDPs）子类及其求解器IDPP，专门处理大规模确定性多智能体规划问题


<details>
  <summary>Details</summary>
Motivation: 许多高层次多智能体规划问题（如多机器人导航）可以用确定性动作和观察有效建模，但现有Dec-POMDP求解器无法高效处理大规模确定性场景

Method: 基于经典联合均衡策略搜索框架，提出迭代确定性POMDP规划（IDPP）方法，专门针对确定性转移和观察条件进行优化

Result: IDPP能够处理当前Dec-POMDP求解器无法高效解决的大规模Det-Dec-POMDP问题

Conclusion: Det-Dec-POMDPs为确定性多智能体规划提供了有效建模框架，IDPP方法为这类问题提供了实用的求解方案

Abstract: Many high-level multi-agent planning problems, including multi-robot
navigation and path planning, can be effectively modeled using deterministic
actions and observations.
  In this work, we focus on such domains and introduce the class of
Deterministic Decentralized POMDPs (Det-Dec-POMDPs). This is a subclass of
Dec-POMDPs characterized by deterministic transitions and observations
conditioned on the state and joint actions.
  We then propose a practical solver called Iterative Deterministic POMDP
Planning (IDPP). This method builds on the classic Joint Equilibrium Search for
Policies framework and is specifically optimized to handle large-scale
Det-Dec-POMDPs that current Dec-POMDP solvers are unable to address
efficiently.

</details>


### [19] [Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study](https://arxiv.org/abs/2508.21622)
*Saravanan Venkatachalam*

Main category: cs.AI

TL;DR: 整合传统网络优化模型与大语言模型，提供交互式、可解释性的供应链规划决策支持系统


<details>
  <summary>Details</summary>
Motivation: 缩小复杂运算研究输出与业务利益相关者理解之间的差距，提升供应链规划的可解释性和可访问性

Method: 采用混合整数规划模型处理多周期多物品的战术性库存重新分配，结合AI代理、RESTful API和动态用户界面，生成自然语言摘要、上下文可视化和定制KPI

Result: 案例研究证明系统能够防止缺货、降低成本并维持服务水平，改善规划结果

Conclusion: 案例验证了框架的有效性，未来将集成私有LLM、迁移学习、强化学习和贝叶斯神经网络来提升可解释性、适应性和实时决策能力

Abstract: This paper presents an integrated framework that combines traditional network
optimization models with large language models (LLMs) to deliver interactive,
explainable, and role-aware decision support for supply chain planning. The
proposed system bridges the gap between complex operations research outputs and
business stakeholder understanding by generating natural language summaries,
contextual visualizations, and tailored key performance indicators (KPIs). The
core optimization model addresses tactical inventory redistribution across a
network of distribution centers for multi-period and multi-item, using a
mixed-integer formulation. The technical architecture incorporates AI agents,
RESTful APIs, and a dynamic user interface to support real-time interaction,
configuration updates, and simulation-based insights. A case study demonstrates
how the system improves planning outcomes by preventing stockouts, reducing
costs, and maintaining service levels. Future extensions include integrating
private LLMs, transfer learning, reinforcement learning, and Bayesian neural
networks to enhance explainability, adaptability, and real-time
decision-making.

</details>


### [20] [A-MHA*: Anytime Multi-Heuristic A*](https://arxiv.org/abs/2508.21637)
*Ramkumar Natarajan,Muhammad Suhail Saleem,William Xiao,Sandip Aine,Howie Choset,Maxim Likhachev*

Main category: cs.AI

TL;DR: 将多启发式A*（MHA*）扩展为随时算法A-MHA*，能够快速找到可行解并持续改进，保持原有最优性保证


<details>
  <summary>Details</summary>
Motivation: MHA*虽然能利用多个不可采纳启发式函数加速搜索，但只能一次性求解且需要仔细设置膨胀因子，无法随时间持续改进解的质量

Method: 受ARA*算法启发，将ARA*的核心概念精确适配到MHA*框架中，开发出随时版本A-MHA*

Result: A-MHA*在3D路径规划和滑块拼图问题上表现良好，相比MHA*和其他随时算法具有竞争优势

Conclusion: A-MHA*成功扩展了MHA*的随时求解能力，保持了原有的次优性和完备性保证，在多个领域验证了其有效性

Abstract: Designing good heuristic functions for graph search requires adequate domain
knowledge. It is often easy to design heuristics that perform well and
correlate with the underlying true cost-to-go values in certain parts of the
search space but these may not be admissible throughout the domain thereby
affecting the optimality guarantees of the search. Bounded suboptimal search
using several such partially good but inadmissible heuristics was developed in
Multi-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible
heuristics to potentially generate a faster suboptimal solution, the original
version does not improve the solution over time. It is a one shot algorithm
that requires careful setting of inflation factors to obtain a desired one time
solution. In this work, we tackle this issue by extending MHA* to an anytime
version that finds a feasible suboptimal solution quickly and continually
improves it until time runs out. Our work is inspired from the Anytime
Repairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*
concepts in the MHA* framework preserves the original suboptimal and
completeness guarantees and enhances MHA* to perform in an anytime fashion.
Furthermore, we report the performance of A-MHA* in 3-D path planning domain
and sliding tiles puzzle and compare against MHA* and other anytime algorithms.

</details>


### [21] [Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI](https://arxiv.org/abs/2508.21648)
*Farhad Abtahi,Mehdi Astaraki,Fernando Seoane*

Main category: cs.AI

TL;DR: MEDLEY框架通过保留多个AI模型的多样性输出而非强制达成共识，将AI偏见视为潜在优势而非缺陷，为医疗AI系统提供透明化的诊断不确定性和偏见展示。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为医疗AI中的偏见是需要消除的缺陷，但人类推理本身就包含教育、文化和经验塑造的偏见，这些偏见可能具有价值且不可避免。

Method: 提出MEDLEY概念框架，协调多个AI模型并保留其多样性输出，将模型特定偏见记录为潜在优势，将幻觉视为需要临床医生验证的临时假设。开发了包含30多个大语言模型的概念验证演示器。

Result: 概念验证演示器在合成病例中成功保留了共识和少数观点，使诊断不确定性和潜在偏见对临床监督透明可见。

Conclusion: MEDLEY通过将AI不完美性重新定义为资源，为开发可信医疗AI系统开辟了新的监管、伦理和创新途径，展示了结构化多样性如何在临床医生监督下增强医疗推理。

Abstract: Bias in medical artificial intelligence is conventionally viewed as a defect
requiring elimination. However, human reasoning inherently incorporates biases
shaped by education, culture, and experience, suggesting their presence may be
inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble
Diagnostic system with Leveraged diversitY), a conceptual framework that
orchestrates multiple AI models while preserving their diverse outputs rather
than collapsing them into a consensus. Unlike traditional approaches that
suppress disagreement, MEDLEY documents model-specific biases as potential
strengths and treats hallucinations as provisional hypotheses for clinician
verification. A proof-of-concept demonstrator was developed using over 30 large
language models, creating a minimum viable product that preserved both
consensus and minority views in synthetic cases, making diagnostic uncertainty
and latent biases transparent for clinical oversight. While not yet a validated
clinical tool, the demonstration illustrates how structured diversity can
enhance medical reasoning under clinician supervision. By reframing AI
imperfection as a resource, MEDLEY offers a paradigm shift that opens new
regulatory, ethical, and innovation pathways for developing trustworthy medical
AI systems.

</details>


### [22] [PosterForest: Hierarchical Multi-Agent Collaboration for Scientific Poster Generation](https://arxiv.org/abs/2508.21720)
*Jiho Choi,Seojeong Park,Seongjong Song,Hyunjung Shim*

Main category: cs.AI

TL;DR: PosterForest是一个无需训练的科学海报生成框架，通过分层中间表示和多智能体协作，在保持内容保真度和视觉一致性的同时，自动生成高质量科学海报。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多忽视科学文档的层次结构和文本视觉元素的语义整合，需要一种能够同时解决这两个挑战的自动化海报生成方案。

Method: 提出Poster Tree分层中间表示来联合编码文档结构和视觉文本关系，采用多智能体协作策略，由内容摘要和布局规划专家智能体迭代协调并提供相互反馈。

Result: 在多个学术领域的广泛实验表明，该方法在定性和定量评估中都优于现有基线，生成的海报质量最接近专家设计的地面真实，具有更好的信息保存、结构清晰度和用户偏好。

Conclusion: PosterForest框架通过创新的分层表示和多智能体协作，成功实现了科学海报的自动化高质量生成，解决了内容与视觉元素的语义整合问题。

Abstract: We present a novel training-free framework, \textit{PosterForest}, for
automated scientific poster generation. Unlike prior approaches, which largely
neglect the hierarchical structure of scientific documents and the semantic
integration of textual and visual elements, our method addresses both
challenges directly. We introduce the \textit{Poster Tree}, a hierarchical
intermediate representation that jointly encodes document structure and
visual-textual relationships at multiple levels. Our framework employs a
multi-agent collaboration strategy, where agents specializing in content
summarization and layout planning iteratively coordinate and provide mutual
feedback. This approach enables the joint optimization of logical consistency,
content fidelity, and visual coherence. Extensive experiments on multiple
academic domains show that our method outperforms existing baselines in both
qualitative and quantitative evaluations. The resulting posters achieve quality
closest to expert-designed ground truth and deliver superior information
preservation, structural clarity, and user preference.

</details>


### [23] [Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem](https://arxiv.org/abs/2508.21730)
*Fabrizio Fagiolo,Nicolo' Vescera*

Main category: cs.AI

TL;DR: 提出了一种用于旅行商问题的变分量子算法，通过紧凑排列编码减少量子比特需求，采用优化-冻结-重用策略，在训练实例上优化电路结构后冻结并重用于新实例，仅需快速重新优化参数。


<details>
  <summary>Details</summary>
Motivation: 解决传统变分量子算法在NISQ设备上运行时需要为每个新问题实例重新进行结构搜索的高成本问题，旨在减少求解时间同时保持解的质量。

Method: 使用紧凑的排列编码减少量子比特需求；采用优化-冻结-重用策略：先用模拟退火优化训练实例的电路拓扑结构，然后冻结该结构，在新实例上仅重新优化电路参数。

Result: 在4-7个城市的40个随机对称实例上，4城市达到100%最优路径采样概率，5城市90%，6城市80%，7城市降至约20%，显示方法的可扩展性限制。

Conclusion: 该方法对中等规模问题展现出强大的泛化能力，冻结Ansatz能显著减少求解时间而不降低解质量，但存在可扩展性限制，有望扩展到车辆路径和作业车间调度等更复杂问题。

Abstract: In this paper we present a variational algorithm for the Traveling Salesman
Problem (TSP) that combines (i) a compact encoding of permutations, which
reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy:
where the circuit topology (``Ansatz'') is first optimized on a training
instance by Simulated Annealing (SA), then ``frozen'' and re-used on novel
instances, limited to a rapid re-optimization of only the circuit parameters.
This pipeline eliminates costly structural research in testing, making the
procedure immediately implementable on NISQ hardware.
  On a set of $40$ randomly generated symmetric instances that span $4 - 7$
cities, the resulting Ansatz achieves an average optimal trip sampling
probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for
6 city cases. With 7 cities the success rate drops markedly to an average of
$\sim 20\%$, revealing the onset of scalability limitations of the proposed
method.
  The results show robust generalization ability for moderate problem sizes and
indicate how freezing the Ansatz can dramatically reduce time-to-solution
without degrading solution quality. The paper also discusses scalability
limitations, the impact of ``warm-start'' initialization of parameters, and
prospects for extension to more complex problems, such as Vehicle Routing and
Job-Shop Scheduling.

</details>


### [24] [Orientability of Causal Relations in Time Series using Summary Causal Graphs and Faithful Distributions](https://arxiv.org/abs/2508.21742)
*Timothée Loranchet,Charles K. Assaad*

Main category: cs.AI

TL;DR: 提出了在给定高层摘要因果图的情况下，保证微观层面时间变量间边可定向性的条件，为复杂时间系统中的因果发现提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析中理解时间变量间的因果关系是一个核心挑战，特别是当完整因果结构未知时。专家通常能提供摘要因果图来捕捉主要因果关系，但如何利用这种高层知识来指导微观层面的因果发现需要理论支持。

Method: 提出了在给定摘要因果图背景知识和假设存在忠实且因果充分的分布的情况下，保证微观层面时间变量间边可定向性的条件。这些条件即使在宏观层面存在循环或双向边的情况下也适用。

Result: 研究结果为微观层面的边定向提供了理论保证，即使在宏观层面存在复杂结构（如循环或双向边）的情况下也能工作。

Conclusion: 这些发现为利用摘要因果图来指导复杂时间系统中的因果发现提供了实用指导，强调了将专家知识融入观测时间序列数据因果推理中的价值。

Abstract: Understanding causal relations between temporal variables is a central
challenge in time series analysis, particularly when the full causal structure
is unknown. Even when the full causal structure cannot be fully specified,
experts often succeed in providing a high-level abstraction of the causal
graph, known as a summary causal graph, which captures the main causal
relations between different time series while abstracting away micro-level
details. In this work, we present conditions that guarantee the orientability
of micro-level edges between temporal variables given the background knowledge
encoded in a summary causal graph and assuming having access to a faithful and
causally sufficient distribution with respect to the true unknown graph. Our
results provide theoretical guarantees for edge orientation at the micro-level,
even in the presence of cycles or bidirected edges at the macro-level. These
findings offer practical guidance for leveraging SCGs to inform causal
discovery in complex temporal systems and highlight the value of incorporating
expert knowledge to improve causal inference from observational time series
data.

</details>


### [25] [Tree-Guided Diffusion Planner](https://arxiv.org/abs/2508.21800)
*Hyeonseong Jeon,Cheolhong Min,Jaesik Park*

Main category: cs.AI

TL;DR: 提出Tree-guided Diffusion Planner (TDP)方法，通过树搜索和双层采样过程解决预训练扩散模型在测试时规划中的局限性，在非凸目标、不可微约束和多奖励场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度引导的扩散规划方法在凸可微奖励场景下表现良好，但在现实世界的非凸目标、不可微约束和多奖励结构中效果显著下降。同时，有监督规划方法需要任务特定训练或价值估计器，限制了测试时灵活性和零样本泛化能力。

Method: TDP将测试时规划构建为树搜索问题，采用双层采样过程：1) 通过无训练粒子引导生成多样化父轨迹以促进广泛探索；2) 通过任务目标引导的快速条件去噪精炼子轨迹。仅使用预训练模型和测试时奖励信号。

Result: 在三个不同任务上评估：迷宫金币收集、机械臂方块操作和AntMaze多目标探索。TDP在所有任务上都一致优于最先进方法。

Conclusion: TDP通过结构化轨迹生成平衡探索与利用，成功解决了梯度引导的局限性，在仅使用预训练模型和测试时奖励的情况下实现了优异的零样本规划性能。

Abstract: Planning with pretrained diffusion models has emerged as a promising approach
for solving test-time guided control problems. However, standard gradient
guidance typically performs optimally under convex and differentiable reward
landscapes, showing substantially reduced effectiveness in real-world scenarios
involving non-convex objectives, non-differentiable constraints, and
multi-reward structures. Furthermore, recent supervised planning approaches
require task-specific training or value estimators, which limits test-time
flexibility and zero-shot generalization. We propose a Tree-guided Diffusion
Planner (TDP), a zero-shot test-time planning framework that balances
exploration and exploitation through structured trajectory generation. We frame
test-time planning as a tree search problem using a bi-level sampling process:
(1) diverse parent trajectories are produced via training-free particle
guidance to encourage broad exploration, and (2) sub-trajectories are refined
through fast conditional denoising guided by task objectives. TDP addresses the
limitations of gradient guidance by exploring diverse trajectory regions and
harnessing gradient information across this expanded solution space using only
pretrained models and test-time reward signals. We evaluate TDP on three
diverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze
multi-goal exploration. TDP consistently outperforms state-of-the-art
approaches on all tasks. The project page can be found at:
tree-diffusion-planner.github.io.

</details>


### [26] [Automated Clinical Problem Detection from SOAP Notes using a Collaborative Multi-Agent LLM Architecture](https://arxiv.org/abs/2508.21803)
*Yeawon Lee,Xiaoyang Wang,Christopher C. Yang*

Main category: cs.AI

TL;DR: 提出了一种协作式多智能体系统来模拟临床团队会诊，通过分层迭代辩论的方式从SOAP笔记的S和O部分识别临床问题，相比单智能体基线在识别心力衰竭、急性肾损伤和脓毒症方面表现更优


<details>
  <summary>Details</summary>
Motivation: 临床叙述的准确解释对患者护理至关重要，但复杂性使得自动化具有挑战性。虽然大语言模型有潜力，但单模型方法缺乏高风险临床任务所需的鲁棒性

Method: 引入协作式多智能体系统，模拟临床会诊团队。Manager智能体协调动态分配的专业智能体团队，进行分层迭代辩论以达成共识，仅分析SOAP笔记的S和O部分来识别临床问题

Result: 在420份MIMIC-III笔记数据集上评估，动态多智能体配置在识别充血性心力衰竭、急性肾损伤和脓毒症方面表现出持续改进的性能。定性分析显示该结构能有效呈现和权衡冲突证据

Conclusion: 通过模拟临床团队的推理过程，该系统为开发更准确、鲁棒和可解释的临床决策支持工具提供了有前景的路径

Abstract: Accurate interpretation of clinical narratives is critical for patient care,
but the complexity of these notes makes automation challenging. While Large
Language Models (LLMs) show promise, single-model approaches can lack the
robustness required for high-stakes clinical tasks. We introduce a
collaborative multi-agent system (MAS) that models a clinical consultation team
to address this gap. The system is tasked with identifying clinical problems by
analyzing only the Subjective (S) and Objective (O) sections of SOAP notes,
simulating the diagnostic reasoning process of synthesizing raw data into an
assessment. A Manager agent orchestrates a dynamically assigned team of
specialist agents who engage in a hierarchical, iterative debate to reach a
consensus. We evaluated our MAS against a single-agent baseline on a curated
dataset of 420 MIMIC-III notes. The dynamic multi-agent configuration
demonstrated consistently improved performance in identifying congestive heart
failure, acute kidney injury, and sepsis. Qualitative analysis of the agent
debates reveals that this structure effectively surfaces and weighs conflicting
evidence, though it can occasionally be susceptible to groupthink. By modeling
a clinical team's reasoning process, our system offers a promising path toward
more accurate, robust, and interpretable clinical decision support tools.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [27] [A Flexible Design for Beam Squint Effect Suppression in IRS-Aided THz Communications](https://arxiv.org/abs/2508.21295)
*Yanze Zhu,Qingqing Wu,Wen Chen,Yang Liu,Ruiqi Liu*

Main category: cs.IT

TL;DR: 该论文研究在宽带太赫兹MISO系统中使用基站和智能反射面的可移动组件，通过优化MA和IRS子阵列位置来缓解双波束偏移效应，并提出了基于MM方法的有效算法。


<details>
  <summary>Details</summary>
Motivation: 解决宽带太赫兹通信中由基站和IRS波束偏移耦合引起的双波束偏移效应问题，提高系统性能。

Method: 采用可移动天线阵列和可移动IRS子阵列，使用majorization-minimization (MM) 方法来优化组件位置配置，最大化宽带太赫兹频谱中的最小接收功率。

Result: 数值结果表明所提算法有效，利用基站和IRS的可移动组件能够成功缓解宽带太赫兹通信中的双波束偏移效应。

Conclusion: 在宽带太赫兹MISO系统中部署可移动组件是缓解双波束偏移效应的有效方法，提出的MM算法能够有效优化系统性能。

Abstract: In this paper, we study employing movable components on both base station
(BS) and intelligent reflecting surface (IRS) in a wideband terahertz (THz)
multiple-input-single-output (MISO) system, where the BS is equipped with a
movable antenna (MA) array and the IRS consists of movable subarrays. To
alleviate double beam squint effect caused by the coupling of beam squint at
the BS and IRS, we propose to maximize the minimal received power across a wide
THz spectrum by delicately configuring the positions of MAs and IRS subarrays,
which is highly challenging. By adopting majorization-minimization (MM)
methodology, we develop an algorithm to tackle the aforementioned optimization.
Numerical results demonstrate the effectiveness of our proposed algorithm and
the benefit of utilizing movable components on the BS and IRS to mitigate
double beam squint effect in wideband THz communications.

</details>


### [28] [On the Weight Distribution of Concatenated Code Ensemble Based on the Plotkin Construction](https://arxiv.org/abs/2508.21515)
*Xiao Ma*

Main category: cs.IT

TL;DR: 本文揭示了基于Plotkin构造的级联码集合的重量分布与其分量码重量分布之间的关系


<details>
  <summary>Details</summary>
Motivation: 研究级联码集合的重量分布特性，为计算包括Reed-Muller类码在内的多种码的集合重量分布提供理论基础

Method: 通过分析基于Plotkin构造的级联码集合，建立其重量分布与分量码重量分布之间的数学关系

Result: 发现了级联码集合重量分布与分量码重量分布之间的明确关系，这一关系可用于计算多种码的集合重量分布

Conclusion: 提出的关系为计算Reed-Muller类码等码的集合重量分布提供了有效方法，具有重要的理论价值和应用前景

Abstract: In this note, we reveal a relation between the weight distribution of a
concatenated code ensemble based on the Plotkin construction and those of its
component codes. The relation may find applications in the calculation of the
ensemble weight distributions for many codes, including Reed-Muller (RM)-like
codes.

</details>


### [29] [Analysis of Semantic Communication for Logic-based Hypothesis Deduction](https://arxiv.org/abs/2508.21755)
*Ahmet Faruk Saz,Siheng Xiong,Faramarz Fekri*

Main category: cs.IT

TL;DR: 该论文分析了基于一阶逻辑推理的语义通信，提出了在传输器不了解真实世界状态的情况下，通过有限信息帮助接收器识别最符合真实状态的假设的优化通信策略。


<details>
  <summary>Details</summary>
Motivation: 研究在语义通信中，当传输器只有不完整证据而接收器持有假设集时，如何通过有限通信资源帮助接收器准确识别真实世界状态，解决传统通信中信息冗余的问题。

Method: 使用Stirling近似将后验分布近似问题转化为约束有限时域资源分配问题，应用KKT条件得到截断注水算法解，设计单轮和多轮消息选择策略，采用m元贝叶斯假设检验建模接收器推理。

Result: 在MAP规则下，提出的通信策略在预算约束内达到最优性能，实验验证了相比随机选择和现有方法错误率显著降低，并分析了收敛速率。

Conclusion: 该研究证明了基于FOL推理的语义通信框架的有效性，对称性和排列不变性保证了全局最优性，为资源受限的语义通信系统提供了理论依据和实用策略。

Abstract: This work presents an analysis of semantic communication in the context of
First-Order Logic (FOL)-based deduction. Specifically, the receiver holds a set
of hypotheses about the State of the World (SotW), while the transmitter has
incomplete evidence about the true SotW but lacks access to the ground truth.
The transmitter aims to communicate limited information to help the receiver
identify the hypothesis most consistent with true SotW. We formulate the
objective as approximating the posterior distribution at the transmitter to the
receiver. Using Stirling's approximation, this reduces to a constrained,
finite-horizon resource allocation problem. Applying the Karush-Kuhn-Tucker
conditions yields a truncated water-filling solution. Despite the problem's
non-convexity, symmetry and permutation invariance ensure global optimality.
Based on this, we design message selection strategies, both for single- and
multi-round communication, and model the receiver's inference as an m-ary
Bayesian hypothesis testing problem. Under the Maximum A Posteriori (MAP) rule,
our communication strategy achieves optimal performance within budget
constraints. We further analyze convergence rates and validate the theoretical
findings through experiments, demonstrating reduced error over random selection
and prior methods.

</details>
