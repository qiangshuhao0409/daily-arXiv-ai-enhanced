<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.AI](#cs.AI) [Total: 42]
- [cs.IT](#cs.IT) [Total: 15]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [AIMNET: An IoT-Empowered Digital Twin for Continuous Gas Emission Monitoring and Early Hazard Detection](https://arxiv.org/abs/2512.06148)
*Zifan Zhou,Xuan Wang,Yang Yan,Lkhanaajav Mijiddorj,Yu Ding,Tyler Beringer,Parisa Masnadi Khiabani,Wolfgang G. Jentner,Xiao-Ming Hu,Chenghao Wang,Bryan M. Carroll,Ming Xue,David Ebert,Bin Li,Binbin Weng*

Main category: cs.NI

TL;DR: AIMNET是一个数字孪生框架，通过物联网传感网络与物理气象-气体传输模型结合，实现碳基气体羽流的高分辨率实时监测和预测。


<details>
  <summary>Details</summary>
Motivation: 工业气体泄漏和野火等环境危害需要及时有效的缓解响应，因此需要增强碳基气体羽流监测的数字孪生框架。

Method: 提出三层系统架构：物理世界（定制监测设备）、双向信息反馈链路（智能数据传输与反向控制）、数字孪生世界（AI驱动的预测、异常检测和动态天气-气体耦合分子传输建模）。

Result: 在油气生产盆地部署了小规模分布式传感网络，并在污水处理厂周围部署了移动监测网络，成功捕获了甲烷排放事件，并通过分层模型模拟解析了其动态。

Conclusion: AIMNET为可靠、实时的监测和预测性风险评估提供了一个有前景的数字孪生框架，并讨论了关键实施挑战和未来发展方向。

Abstract: A Digital Twin (DT) framework to enhance carbon-based gas plume monitoring is critical for supporting timely and effective mitigation responses to environmental hazards such as industrial gas leaks, or wildfire outbreaks carrying large carbon emissions. We present AIMNET, a one-of-a-kind DT framework that integrates a built-in-house Internet of Things (IoT)-based continuous sensing network with a physics-based multi-scale weather-gas transport model, that enables high-resolution and real-time simulation and detection of carbon gas emissions. AIMNET features a three-layer system architecture: (i) physical world: custom-built devices for continuous monitoring; (ii) bidirectional information feedback links: intelligent data transmission and reverse control; and (iii) digital twin world: AI-driven analytics for prediction, anomaly detection, and dynamic weather-gas coupled molecule transport modeling. Designed for scalable, energy-efficient deployment in remote environments, AIMNET architecture is realized through a small-scale distributed sensing network over an oil and gas production basin. To demonstrate the high-resolution, fast-responding concept, an equivalent mobile-based emission monitoring network was deployed around a wastewater treatment plant that constantly emits methane plumes. Our preliminary results through which, have successfully captured the methane emission events whose dynamics have been further resolved by the tiered model simulations. This work supports our position that AIMNET provides a promising DT framework for reliable, real-time monitoring and predictive risk assessment. In the end, we also discuss key implementation challenges and outline future directions for advancing such a new DT framework for translation deployment.

</details>


### [2] [Programmable and GPU-Accelerated Edge Inference for Real-Time ISAC on NVIDIA ARC-OTA](https://arxiv.org/abs/2512.06493)
*Davide Villa,Mauro Belgiovine,Nicholas Hedberg,Michele Polese,Chris Dick,Tommaso Melodia*

Main category: cs.NI

TL;DR: 提出一个基于GPU加速的AI框架，用于在边缘RAN上处理PHY/MAC信号，并展示了cuSense室内定位应用，无需专用硬件即可实现77厘米平均定位误差。


<details>
  <summary>Details</summary>
Motivation: 随着蜂窝网络向软件化系统和多服务平台演进，以及感知成为6G关键特性，如何在有限带宽下支持ISAC并保持高可靠性成为根本挑战。

Method: 1. 提出可编程即插即用框架，在边缘RAN基础设施上通过GPU加速AI应用实时处理PHY/MAC信号；2. 基于Open RAN dApp架构，与NVIDIA ARC-OTA的GPU加速gNB接口；3. 开发cuSense室内定位dApp，使用上行DMRS信道估计，去除静态多径分量，运行神经网络推断人员位置。

Result: 框架处理延迟低于0.5毫秒；cuSense在3GPP兼容5G NR部署中实现77厘米平均定位误差，75%预测在1米内，无需专用感知硬件或修改RAN堆栈/信号。

Conclusion: 该框架为未来AI原生RAN和ISAC应用提供参考设计，将开源框架和cuSense流水线，推动软件化网络中的感知能力发展。

Abstract: The transition of cellular networks to (i) software-based systems on commodity hardware and (ii) platforms for services beyond connectivity introduces critical system-level challenges. As sensing emerges as a key feature toward 6G standardization, supporting Integrated Sensing and Communication (ISAC) with limited bandwidth and piggybacking on communication signals, while maintaining high reliability and performance, remains a fundamental challenge. In this paper, we provide two key contributions. First, we present a programmable, plug-and-play framework for processing PHY/MAC signals through real-time, GPU-accelerated Artificial Intelligence (AI) applications on the edge Radio Access Network (RAN) infrastructure. Building on the Open RAN dApp architecture, the framework interfaces with a GPU-accelerated gNB based on NVIDIA ARC-OTA, feeding PHY/MAC data to custom AI logic with latency under 0.5 ms for complex channel state information extraction. Second, we demonstrate the framework's capabilities through cuSense, an indoor localization dApp that consumes uplink DMRS channel estimates, removes static multipath components, and runs a neural network to infer the position of a moving person. Evaluated on a 3GPP-compliant 5G NR deployment, cuSense achieves a mean localization error of 77 cm, with 75% of predictions falling within 1 meter. This is without dedicated sensing hardware or modifications to the RAN stack or signals. We plan to release both the framework and cuSense pipelines as open source, providing a reference design for future AI-native RANs and ISAC applications.

</details>


### [3] [AQUILA: A QUIC-Based Link Architecture for Resilient Long-Range UAV Communication](https://arxiv.org/abs/2512.06889)
*Ximing Huang,Yirui Rao*

Main category: cs.NI

TL;DR: AQUILA是基于QUIC的跨层通信架构，为BVLOS无人机应用提供可靠、低延迟的通信，解决了TCP和UDP的局限性，通过统一传输层、优先级调度和无人机自适应拥塞控制实现优越性能。


<details>
  <summary>Details</summary>
Motivation: BVLOS无人机应用需要高带宽、低延迟的通信链路，但现有方案存在严重问题：TCP存在队头阻塞问题影响实时数据，UDP缺乏可靠性和拥塞控制，蜂窝网络对空中平台性能严重下降。

Method: 1) 使用QUIC的统一传输层：可靠流用于MAVLink C2控制，不可靠数据报用于视频传输，消除队头阻塞；2) 优先级调度机制：确保C2延迟有界且独立于视频流量；3) 无人机自适应拥塞控制算法：扩展SCReAM，加入高度自适应延迟目标和遥测带宽预留；4) 0-RTT连接恢复：最小化切换中断，应用层重放保护。

Result: 实验验证表明，AQUILA在C2延迟、视频质量和链路鲁棒性方面显著优于基于TCP和UDP的方法，为自主BVLOS任务提供了坚实基础。

Conclusion: AQUILA通过创新的跨层设计解决了BVLOS无人机通信的关键挑战，为自主无人机应用提供了可靠、低延迟的通信解决方案，在实际条件下表现出优越性能。

Abstract: The proliferation of autonomous Unmanned Aerial Vehicles (UAVs) in Beyond Visual Line of Sight (BVLOS) applications is critically dependent on resilient, high-bandwidth, and low-latency communication links. Existing solutions face critical limitations: TCP's head-of-line blocking stalls time-sensitive data, UDP lacks reliability and congestion control, and cellular networks designed for terrestrial users degrade severely for aerial platforms. This paper introduces AQUILA, a cross-layer communication architecture built on QUIC to address these challenges. AQUILA contributes three key innovations: (1) a unified transport layer using QUIC's reliable streams for MAVLink Command and Control (C2) and unreliable datagrams for video, eliminating head-of-line blocking under unified congestion control; (2) a priority scheduling mechanism that structurally ensures C2 latency remains bounded and independent of video traffic intensity; (3) a UAV-adapted congestion control algorithm extending SCReAM with altitude-adaptive delay targeting and telemetry headroom reservation. AQUILA further implements 0-RTT connection resumption to minimize handover blackouts with application-layer replay protection, deployed over an IP-native architecture enabling global operation. Experimental validation demonstrates that AQUILA significantly outperforms TCP- and UDP-based approaches in C2 latency, video quality, and link resilience under realistic conditions, providing a robust foundation for autonomous BVLOS missions.

</details>


### [4] [Hyperflex: A SIMD-based DFA Model for Deep Packet Inspection](https://arxiv.org/abs/2512.07123)
*Yang Liu,Wenjun Zhu,Harry Chang,Yang Hong,Geoff Langdale,Kun Qiu,Jin Zhao*

Main category: cs.NI

TL;DR: Hyperflex：一种基于SIMD的新型DFA模型，用于高性能正则表达式匹配，通过区域检测和混合状态转换算法，在Hyperscan中实现2.27倍性能提升


<details>
  <summary>Details</summary>
Motivation: 随着网络带宽和规则集规模快速增长，传统的确定性有限自动机(DFA)模型已成为深度包检测(DPI)的性能瓶颈，需要利用SIMD指令来提升DFA模型的效率

Method: 提出Hyperflex模型，包含：1) 区域检测算法，在整个DFA图中识别适合SIMD加速的区域；2) 混合状态转换算法，支持SIMD加速区域和普通区域的状态转换，并确保两种区域间的无缝转换

Result: 在商用CPU上实现Hyperflex，使用真实网络流量和DPI正则表达式进行评估，达到8.89Gbit/s的吞吐量，比Hyperscan的默认DFA模型Mcclellan提升高达2.27倍

Conclusion: Hyperflex已成功部署在Hyperscan中，显著提升了其性能，为解决DPI中DFA模型的性能瓶颈提供了有效方案

Abstract: Deep Packet Inspection (DPI) has been extensively employed for network security. It examines traffic payloads by searching for regular expressions (regex) with the Deterministic Finite Automaton (DFA) model. However, as the network bandwidth and ruleset size are increasing rapidly, the conventional DFA model has emerged as a significant performance bottleneck of DPI. Leveraging the Single-Instruction-Multiple-Data (SIMD) instruction to perform state transitions can substantially boost the efficiency of the DFA model. In this paper, we propose Hyperflex, a novel SIMD-based DFA model designed for high-performance regex matching. Hyperflex incorporates a region detection algorithm to identify regions suitable for acceleration by SIMD instructions across the whole DFA graph. Also, we design a hybrid state transition algorithm that enables state transition in both SIMD-accelerated and normal regions, and ensures seamless state transition across the two types of regions. We have implemented Hyperflex on the commodity CPU and evaluated it with real network traffic and DPI regexes. Our evaluation results indicate that Hyperflex reaches a throughput of 8.89Gbit/s, representing an improvement of up to 2.27 times over Mcclellan, the default DFA model of the prominent multi-pattern regex matching engine Hyperscan. As a result, Hyperflex has been successfully deployed in Hyperscan, significantly enhancing its performance.

</details>


### [5] [Implementation of Honeynet and Honeypot in Network Infrastructure in Production Network](https://arxiv.org/abs/2512.07180)
*Nawshad Ahmed Evan,Md Raihan Uddin*

Main category: cs.NI

TL;DR: 本文提出使用蜜网技术，其中蜜罐模拟真实资源来欺骗攻击者并分析其行为，以保护生产环境中的网络基础设施。


<details>
  <summary>Details</summary>
Motivation: 生产环境中的网络基础设施日益成为攻击目标，承载着众多公司资源和服务，需要有效保护。蜜罐作为一种高效工具，能够模拟扫描器和攻击者，在模拟的生产级环境中误导入侵者。

Method: 使用蜜网技术，其中蜜罐被设计成模拟真实资源来欺骗攻击者，通过这种模拟环境来分析和监控攻击者的行为模式。

Result: 论文将展示蜜网如何有效欺骗攻击者，并分析攻击者在模拟环境中的行为特征。

Conclusion: 蜜网技术是保护网络基础设施的有效方法，通过模拟真实资源欺骗攻击者并分析其行为，能够增强网络安全防护能力。

Abstract: Network infrastructure in a production environment is increasingly targeted by attackers every day. Many resources and services now rely on the internet, making network infrastructure one of the most critical parts to protect, as it hosts numerous company resources and services. Several solutions have already been proposed to prevent attacks, minimize damage, and divert hackers and intruders. Among these, the honeypot stands out as a highly effective tool; it is designed to mimic both a scanner and an attacker, diverting and misleading them within a simulated, production-level environment. This paper will demonstrate the use of a honeynet where a honeypot acts like a real resource to deceive the attacker and analyze their behavior.

</details>


### [6] [WaggleNet: A LoRa and MQTT-Based Monitoring System for Internal and External Beehive Conditions](https://arxiv.org/abs/2512.07408)
*Minju Jeon,Jiyun Kim,Sewon Kim,Seongmin Park,Bo Zhang,Anthony H. Smith*

Main category: cs.NI

TL;DR: WaggleNet是一个低成本、可扩展的双范围蜜蜂监测系统，同时监控蜂箱内部条件和外部环境因素，使用LoRa-MQTT架构，支持数据驱动的精准养蜂。


<details>
  <summary>Details</summary>
Motivation: 全球蜜蜂数量下降威胁农业生产和粮食安全。现有智能蜂箱系统通常只监测内部条件，忽略了显著影响蜂群健康的外部环境因素，且存在成本高、可扩展性有限和上下文分析不足的问题。

Method: 开发了WaggleNet双范围监测系统，采用成本效益高的LoRa-MQTT架构。系统包括约15美元的模块化工作节点（配备温度、湿度、光照和GPS传感器，部署在蜂箱内外）和作为LoRa-MQTT网关的主节点，将数据转发到云服务器和移动应用界面。

Result: 现场实验显示：在110米视距条件下100%数据包传输，95米有障碍环境下也能工作；系统能在木制蜂箱结构内成功部署；端到端延迟稳定在5秒以下；在多样化环境条件下连续运行两个月。

Conclusion: WaggleNet通过弥合内部和外部监测之间的差距，实现了上下文异常检测，支持资源受限环境下的数据驱动精准养蜂。

Abstract: Bee populations are declining globally due to habitat loss, pesticide exposure, and climate change, threatening agricultural productivity and food security. While existing smart beehive systems monitor internal conditions, they typically overlook external environmental factors that significantly influence colony health, and are constrained by high cost, limited scalability, and inadequate contextual analysis. We present WaggleNet, a novel dual-scope monitoring system that simultaneously captures both internal hive conditions and external environmental parameters using a cost-effective LoRa-MQTT architecture. Our system deploys modular worker nodes ($\sim$\$15 each) equipped with temperature, humidity, light, and GPS sensors both inside and around beehives. A master node functions as a LoRa-MQTT gateway, forwarding data to a cloud server with a mobile application interface. Field experiments confirmed reliable operation with 100\% packet delivery over 110 meters in line-of-sight conditions and 95 meters in obstructed environments, including successful deployment inside wooden hive structures. Our system demonstrated stable end-to-end latency under 5 seconds and continuous operation over a two-month period across diverse environmental conditions. By bridging the gap between internal and external monitoring, WaggleNet enables contextual anomaly detection and supports data-driven precision beekeeping in resource-constrained settings.

</details>


### [7] [Service Registration, Indexing, Discovery & Selection; An Architectural Survey Toward a GenAI-Driven Future](https://arxiv.org/abs/2512.07638)
*Mohammad Farhoudi,Masoud Shokrnezhad,Tarik Taleb*

Main category: cs.NI

TL;DR: 该论文从架构视角研究6G网络中的服务注册、索引、发现和选择(SRIDS)机制，提出理论框架、分类体系，并设计混合架构方案以应对6G服务编排挑战。


<details>
  <summary>Details</summary>
Motivation: 6G网络将边缘到云计算连续体与高性能网络统一，带来远超当前边界的能力。随着用例多样性指数级增长和流量达到前所未有的动态水平，需要新颖的服务编排机制。SRIDS作为6G服务提供的基础要素，需要系统性的架构研究。

Method: 1) 建立6G中SRIDS的理论基础，定义核心概念、端到端工作流程；2) 进行全面的文献综述和差距分析；3) 提出将SRIDS机制分为集中式、分布式、去中心化和混合架构的分类法；4) 基于分析提出结合集中式数据管理和分布式协调的混合架构框架，融入生成式AI等创新技术。

Result: 识别了阻碍6G统一SRIDS的概念和方法论差距，系统评估了各类架构研究，提出了能够确保一致性和敏捷性同时增强可扩展性的混合架构框架，为6G服务编排提供了解决方案。

Conclusion: SRIDS是6G服务提供的关键基础要素，需要综合考虑可靠性、可扩展性、自动化、确定性、效率、可持续性、语义感知、安全、隐私和信任等设计目标。提出的混合架构框架为应对6G挑战提供了可行方案，但仍存在开放挑战需要未来研究。

Abstract: The emergence of sixth-generation (6G) networks marks a paradigm shift: by unifying an edge-to-cloud computing continuum with ultra-high-performance networking, 6G will enable capabilities far beyond today's boundaries. As use-case diversity grows exponentially and user adoption drives traffic to unprecedented and highly dynamic levels, novel service orchestration mechanisms are indispensable. In this paper, we adopt an architectural viewpoint, examining Service Registration, Indexing, Discovery, and Selection (SRIDS) as fundamental elements of 6G service provision. We first establish the theoretical foundations of SRIDS in 6G by defining its core concepts, detailing its end-to-end workflow, reviewing current standardization efforts, and projecting its future design objectives, including reliability, scalability, automaticity and adaptability, determinism, efficiency, sustainability, semantic-awareness, security, privacy, and trust. We then perform a comprehensive literature review and gap analysis encompassing both existing surveys and recent research efforts, identifying conceptual and methodological gaps that hinder unified SRIDS in 6G. Next, we introduce a taxonomy that classifies SRIDS mechanisms into centralized, distributed, decentralized, and hybrid architectures, and systematically examine the relevant studies within each category. Each work is evaluated against the extracted design objectives. Building on these findings, we propose a hybrid architectural framework, combining centralized data management to ensure consistency and agility with distributed coordination to enhance scalability in emerging 6G use cases. The framework incorporates innovative technologies, such as Generative Artificial Intelligence (GenAI). We conclude by highlighting open challenges and suggesting directions for future research.

</details>


### [8] [Multi-Generator Continual Learning for Robust Delay Prediction in 6G](https://arxiv.org/abs/2512.07726)
*Xiaoyu Lan,Jalil Taghia,Hannes Larsson,Andreas Johnsson*

Main category: cs.NI

TL;DR: 提出一种基于多生成器的持续学习方法，用于6G网络中单向延迟预测，解决灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 6G网络需要为远程控制等应用提供严格的端到端性能保证，但动态网络环境导致数据分布漂移，使机器学习模型出现灾难性遗忘，预测性能下降

Method: 提出多生成器持续学习框架，使用表格变分自编码器作为生成器，结合用户设备能力知识确定生成器设置和相关性，增强生成重放方法的稳定性

Result: 在真实5G测试床收集的数据上进行评估，在多种场景下相比基线方法表现出优异性能

Conclusion: 多生成器持续学习方法能有效解决6G网络中单向延迟预测的灾难性遗忘问题，为网络性能保证提供可靠预测能力

Abstract: In future 6G networks, dependable networks will enable telecommunication services such as remote control of robots or vehicles with strict requirements on end-to-end network performance in terms of delay, delay variation, tail distributions, and throughput. With respect to such networks, it is paramount to be able to determine what performance level the network segment can guarantee at a given point in time. One promising approach is to use predictive models trained using machine learning (ML). Predicting performance metrics such as one-way delay (OWD), in a timely manner, provides valuable insights for the network, user equipments (UEs), and applications to address performance trends, deviations, and violations. Over the course of time, a dynamic network environment results in distributional shifts, which causes catastrophic forgetting and drop of ML model performance. In continual learning (CL), the model aims to achieve a balance between stability and plasticity, enabling new information to be learned while preserving previously learned knowledge. In this paper, we target on the challenges of catastrophic forgetting of OWD prediction model. We propose a novel approach which introducing the concept of multi-generator for the state-of-the-art CL generative replay framework, along with tabular variational autoencoders (TVAE) as generators. The domain knowledge of UE capabilities is incorporated into the learning process for determining generator setup and relevance. The proposed approach is evaluated across a diverse set of scenarios with data that is collected in a realistic 5G testbed, demonstrating its outstanding performance in comparison to baselines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals](https://arxiv.org/abs/2512.05998)
*Michael Todasco*

Main category: cs.AI

TL;DR: LLM评估任务中加入虚拟投注机制，能产生可校准的置信度信号，使模型内部信念可见化


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估其他模型时缺乏置信度表示，需要一种方法来让模型表达其预测的确定性程度

Method: 设计虚拟预测市场实验：让预测模型在控制条件（简单对错预测）和激励条件（预测+投注虚拟货币）下评估基线模型的数学逻辑问题回答能力

Result: 激励条件下准确率略有提升（81.5% vs 79.1%），学习速度显著更快，投注金额与置信度高度相关：大额投注（4万+）正确率约99%，小额投注（<1000）正确率仅74%

Conclusion: 虚拟货币投注机制能产生清晰的置信度信号，使LLM成为风险感知的预测器，为元评估系统和LLM间预测市场奠定基础

Abstract: Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. "Whale" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.

</details>


### [10] [Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach](https://arxiv.org/abs/2512.06161)
*Gondy Leroy,Prakash Bisht,Sai Madhuri Kandula,Nell Maltman,Sydney Rice*

Main category: cs.AI

TL;DR: 提出一种基于BioBERT的可解释机器学习方法，通过分析临床文本来诊断自闭症谱系障碍，在混合数据集训练下达到97%敏感性和98%特异性，优于黑盒模型。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍诊断过程漫长且需求增加，现有机器学习模型多为黑盒且通常基于单一数据集训练，限制了其泛化能力和临床可信度。

Method: 使用BioBERT语言模型分析非结构化临床文本，训练模型标注行为描述并映射到诊断标准，然后分配最终标签（ASD或非ASD）。评估了迁移学习能力，比较了顺序训练和混合训练两种策略，并与黑盒方法对比。

Result: 透明模型表现稳健，混合数据训练策略效果最佳（97%敏感性，98%特异性）。顺序训练导致性能轻微下降。黑盒模型在顺序或混合训练下表现较差（90%敏感性，96%特异性）。透明方法整体优于黑盒方法。

Conclusion: 透明可解释的机器学习方法在自闭症诊断中优于黑盒方法，混合数据集训练应作为首选策略。这项工作为神经发育诊断中更可信、可泛化且临床可操作的AI工具铺平了道路。

Abstract: Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.

</details>


### [11] [ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment](https://arxiv.org/abs/2512.06196)
*Charlie Masters,Marta Grześkiewicz,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: ARCANE框架将AI对齐问题转化为多智能体协作，通过动态生成自然语言评分标准（rubrics）来表示利益相关者偏好，实现可解释、无需重新训练即可调整的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体越来越多地部署到长期任务中，保持其与利益相关者偏好的一致性变得至关重要。需要可解释的奖励模型让利益相关者能够理解和审计模型目标，并且能够在交互时引导智能体，无需重新训练就能适应偏好变化。

Method: 提出ARCANE框架，将对齐问题构建为多智能体协作问题，动态地将利益相关者偏好表示为自然语言评分标准（加权可验证标准集）。受效用理论启发，将评分标准学习构建为重构问题，应用正则化的组序列策略优化（GSPO）程序，平衡可解释性、忠实性和计算效率。

Result: 使用GDPVal基准的219个标注评分标准进行评估，在需要多步推理和工具使用的挑战性任务上测试ARCANE。学习的评分标准产生紧凑、易读的评估，无需重新训练即可实现可配置的权衡（如正确性与简洁性）。

Conclusion: 基于评分标准的奖励模型为复杂、长期AI系统提供了一条有前景的可解释、测试时自适应对齐路径。

Abstract: As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.

</details>


### [12] [On measuring grounding and generalizing grounding problems](https://arxiv.org/abs/2512.06205)
*Daniel Quigley,Eric Maynard*

Main category: cs.AI

TL;DR: 论文将符号接地问题从二元判断重构为基于评估元组（上下文、意义类型、威胁模型、参考分布）的多维度审计框架，包含真实性、保持性、忠实性、鲁棒性和组合性五个标准，并应用于四种接地模式和三个案例研究。


<details>
  <summary>Details</summary>
Motivation: 传统符号接地问题通常被视为二元判断（接地与否），但作者认为这过于简化。需要建立一个更系统、多维度的框架来评估符号如何获得意义，为哲学家、计算机科学家、语言学家和数学家提供共同语言和技术工具来研究表示和意义问题。

Method: 提出基于评估元组（context, meaning type, threat model, reference distribution）的审计框架，包含五个核心标准：真实性（机制是否在智能体内部并通过学习/进化获得）、保持性（原子意义是否保持完整）、忠实性（包括相关性和病因性）、鲁棒性（在扰动下的优雅退化）、组合性（系统构建性）。将此框架应用于四种接地模式（符号、指称、向量、关系）和三个案例研究。

Result: 1. 模型论语义学实现精确组合但缺乏病因性保证；2. 大语言模型在语言任务上显示相关拟合和局部鲁棒性，但在没有接地交互的世界任务上缺乏成功选择；3. 人类语言通过进化和发育获得满足强真实性标准。框架为不同学科提供了系统研究接地和意义的共同语言。

Conclusion: 通过将哲学上的表示问题操作化，论文提供了一个多维度的审计框架来系统评估符号接地，超越了传统的二元判断。该框架为跨学科研究意义和表示问题提供了技术工具和共同语言，有助于更精确地理解和评估不同认知系统和人工智能中的意义获取机制。

Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.

</details>


### [13] [AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems](https://arxiv.org/abs/2512.06240)
*Chuanhao Nie,Yunbo Liu,Chao Wang*

Main category: cs.AI

TL;DR: 本文综述了AI在反洗钱(AML)中的应用，提出了基于图检索增强生成(RAG-Graph)的KYC系统，实验显示该架构能提高检测准确率、降低误报率，并增强KYC流程的效率和透明度。


<details>
  <summary>Details</summary>
Motivation: 洗钱和金融欺诈每年造成数万亿美元损失，威胁全球金融稳定，传统监管方式面临挑战。需要现代化AML工作流程以提高检测准确性、降低误报率、减轻人工调查负担，支持可持续发展。

Method: 1. 综述AI在AML中的应用现状；2. 提出未来研究方向：联邦学习、公平可解释AI、强化学习、人机协同可视化系统；3. 设计AI驱动的KYC应用，集成图检索增强生成(RAG-Graph)与生成模型。

Result: RAG-Graph架构在多样化评估场景中展现出高忠实度和强答案相关性，显著提升了KYC客户尽职调查(CDD)/增强尽职调查(EDD)工作流程的效率和透明度，有助于实现更可持续、资源优化的合规实践。

Conclusion: AI技术能有效现代化AML工作流程，提高检测性能并降低运营成本。RAG-Graph架构为KYC流程提供了高效透明的解决方案，未来需关注隐私保护、公平性、可解释性和自适应防御等研究方向，构建透明、负责、稳健的新一代AML架构。

Abstract: Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.

</details>


### [14] [How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion](https://arxiv.org/abs/2512.06296)
*Sooho Moon,Yunyong Ko*

Main category: cs.AI

TL;DR: PROBE是一个新的知识图谱补全评估框架，通过考虑预测锐度（严格程度）和流行度偏差鲁棒性（预测低频实体的能力）来提供更全面的模型评估。


<details>
  <summary>Details</summary>
Motivation: 现有KGC评估指标存在两个关键问题：1）忽视预测锐度——对单个预测的严格程度评估不足；2）缺乏流行度偏差鲁棒性——无法有效评估模型预测低频实体的能力。

Method: 提出PROBE框架，包含两个组件：1）秩变换器（RT）根据所需预测锐度水平估计每个预测的分数；2）秩聚合器（RA）以流行度感知方式聚合所有分数。

Result: 在真实世界知识图谱上的实验表明，现有指标倾向于高估或低估KGC模型的准确性，而PROBE能提供对KGC模型的全面理解和可靠的评估结果。

Conclusion: PROBE框架通过同时考虑预测锐度和流行度偏差鲁棒性，为知识图谱补全提供了更全面、可靠的评估方法，弥补了现有评估指标的不足。

Abstract: Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.

</details>


### [15] [DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2512.06337)
*Xuan Xie,Xuan Wang,Wenjie Wang*

Main category: cs.AI

TL;DR: DaGRPO通过序列级梯度修正和离策略数据增强，解决了GRPO在长链推理训练中的不稳定性和样本效率低的问题，在数学推理和OOD泛化基准上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: GRPO虽然在激发LLM后训练推理能力方面表现出色，但存在训练不稳定和样本效率低的问题。研究发现这些问题的根源在于on-policy rollout中缺乏区分度：对于常规查询，高度同质的样本导致破坏性梯度冲突；对于困难查询，有效正样本稀缺导致优化无效。

Method: 提出Distinctiveness-aware Group Relative Policy Optimization (DaGRPO)，包含两个核心机制：1) 序列级梯度修正：使用细粒度评分动态掩码低区分度的样本对，从源头消除梯度冲突；2) 离策略数据增强：引入高质量锚点来恢复困难任务的训练信号。

Result: 在9个数学推理和OOD泛化基准上的实验表明，DaGRPO显著超越了现有的SFT、GRPO和混合基线，实现了新的SOTA性能（例如在数学基准上平均准确率提升+4.7%）。深入分析证实DaGRPO有效缓解了梯度爆炸并加速了长链推理能力的出现。

Conclusion: DaGRPO通过解决GRPO的区分度不足问题，显著提升了训练稳定性和样本效率，为LLM的长链推理能力训练提供了更有效的优化框架。

Abstract: The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.

</details>


### [16] [Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression](https://arxiv.org/abs/2512.06393)
*Qiming Bao,Xiaoxuan Fu*

Main category: cs.AI

TL;DR: LLMs在逻辑推理中表现出对语义保持变换的稳定性，但对缺失或矛盾证据极度脆弱


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言任务中表现出色，但其在逻辑上下文结构扰动下的泛化能力仍不清楚，需要系统评估推理可靠性

Method: 提出控制评估框架，包含四种压力测试：规则删除（冗余vs必要）、矛盾证据注入、逻辑保持重写（六种等价律）、多律等价叠加（2-5个同时变换）

Result: 所有模型在基础任务上准确率完美，对冗余规则删除和所有等价重写（单律或多律）完全泛化，但在必要规则删除下准确率降至25%，在明确矛盾下完全崩溃（0%）

Conclusion: LLMs对语义保持的逻辑变换具有稳定不变性，但对缺失或冲突证据仍存在根本性脆弱，框架为诊断推理失败模式提供工具，突显当前LLMs逻辑泛化能力的持续差距

Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.

</details>


### [17] [GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols](https://arxiv.org/abs/2512.06404)
*Mohammad Soleymanibrojeni,Roland Aydin,Diego Guedes-Sobrinho,Alexandre C. Dias,Maurício J. Piotrowski,Wolfgang Wenzel,Celso Ricardo Caldeira Rêgo*

Main category: cs.AI

TL;DR: GENIUS是一个AI代理工作流，融合了智能Quantum ESPRESSO知识图谱和分层大语言模型，通过有限状态错误恢复机监督，将自由文本提示转换为验证过的输入文件，实现电子结构DFT模拟的自动化。


<details>
  <summary>Details</summary>
Motivation: 尽管预测性原子模拟推动了材料发现，但常规设置和调试仍需计算机专家，这种知识差距限制了集成计算材料工程（ICME）的发展。现有先进代码对非专家用户来说仍然繁琐。

Method: GENIUS结合了智能Quantum ESPRESSO知识图谱与分层大语言模型层次结构，由有限状态错误恢复机监督，将自由文本提示转换为验证过的输入文件。

Result: 在295个多样化基准测试中，约80%成功运行到完成，其中76%可自主修复，成功率呈指数衰减至7%基线。相比纯LLM基线，GENIUS将推理成本减半并几乎消除幻觉。

Conclusion: 该框架通过智能自动化协议生成、验证和修复，使电子结构DFT模拟民主化，为学术界和工业界的大规模筛选和ICME设计循环加速。

Abstract: Predictive atomistic simulations have propelled materials discovery, yet routine setup and debugging still demand computer specialists. This know-how gap limits Integrated Computational Materials Engineering (ICME), where state-of-the-art codes exist but remain cumbersome for non-experts. We address this bottleneck with GENIUS, an AI-agentic workflow that fuses a smart Quantum ESPRESSO knowledge graph with a tiered hierarchy of large language models supervised by a finite-state error-recovery machine. Here we show that GENIUS translates free-form human-generated prompts into validated input files that run to completion on $\approx$80% of 295 diverse benchmarks, where 76% are autonomously repaired, with success decaying exponentially to a 7% baseline. Compared with LLM-only baselines, GENIUS halves inference costs and virtually eliminates hallucinations. The framework democratizes electronic-structure DFT simulations by intelligently automating protocol generation, validation, and repair, opening large-scale screening and accelerating ICME design loops across academia and industry worldwide.

</details>


### [18] [UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems](https://arxiv.org/abs/2512.06406)
*Xianzong Wu,Xiaohong Li,Lili Quan,Qiang Hu*

Main category: cs.AI

TL;DR: UncertaintyZoo是一个统一工具包，集成了29种不确定性量化方法，用于评估大语言模型的预测置信度，特别是在代码漏洞检测任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在安全关键场景中可能做出错误预测，需要不确定性量化方法来评估模型置信度，但现有工具有限，阻碍了UQ方法的实际应用和研究发展。

Method: 开发UncertaintyZoo统一工具包，集成29种不确定性量化方法，涵盖五大类别，提供标准化接口，并在CodeBERT和ChatGLM3模型上进行代码漏洞检测任务的评估。

Result: UncertaintyZoo能有效揭示预测不确定性，在代码漏洞检测任务中验证了现有UQ方法的实用性，工具已开源并提供演示视频。

Conclusion: UncertaintyZoo填补了UQ工具集的空白，促进了不确定性量化方法在实际应用和未来研究中的使用，为评估大语言模型预测置信度提供了统一平台。

Abstract: Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.

</details>


### [19] [Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City](https://arxiv.org/abs/2512.06431)
*Mohamed Shamroukh,Mohamed Alkhuzamy Aziz*

Main category: cs.AI

TL;DR: 开发基于Voronoi图的空间分析算法，为埃及Qena市定制规划标准，评估公共服务覆盖效率


<details>
  <summary>Details</summary>
Motivation: 埃及国家规划标准往往忽视地方特色，需要针对Qena市开发定制化规划模型以解决公共服务覆盖不均问题

Method: 采用混合方法（描述性、分析性、实验性），使用Python编程开发基于Voronoi图的智能空间分析算法，生成城市特定规划标准

Result: 总体服务覆盖率为81.3%，救护车站效率最高（99.8%），公园绿地覆盖率最低（10%）；市中心服务密度高（>45个/km²），郊区显著降低（<5个/km²）

Conclusion: 成功开发了本地化规划标准模型和自动化评估算法，为埃及城市提供了可复制的数据驱动城市规划框架

Abstract: National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.

</details>


### [20] [The Effect of Belief Boxes and Open-mindedness on Persuasion](https://arxiv.org/abs/2512.06573)
*Onur Bilgin,Abdullah As Sami,Sriram Sai Vujjini,John Licato*

Main category: cs.AI

TL;DR: 论文研究了在LLM智能体中引入"信念盒"（包含信念陈述的提示空间）如何影响其行为、信念改变倾向以及在多智能体场景中的说服力，特别是开放心态指令和同伴压力情境下的表现。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统在推理和决策应用中日益普及，需要让基于LLM的智能体具备类似命题信念的能力。当前简单方法是在提示空间中包含信念陈述（信念盒），但需要探究这如何实际影响智能体行为、信念倾向以及在多智能体场景中的说服力。

Method: 通过一系列实验探索信念盒技术：1）在提示空间中包含信念陈述及其强度；2）研究开放心态指令对信念改变倾向的影响；3）测试在多智能体辩论中，特别是当智能体处于少数观点时的同伴压力场景。

Result: 1）开放心态指令确实影响智能体对信念改变的接受程度；2）信念陈述及其强度影响智能体对相反观点的抵抗力和说服力；3）在辩论中被相反观点包围时（同伴压力场景），信念改变的可能性增加；4）验证了信念盒技术在推理和决策任务中的可行性和有效性。

Conclusion: 信念盒技术是有效的，能够显著影响LLM智能体的信念相关行为，特别是在多智能体交互场景中。开放心态指令可以调节信念改变的倾向，而同伴压力情境会增强信念改变的可能性。这为构建更复杂的多智能体推理系统提供了基础。

Abstract: As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.

</details>


### [21] [FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection](https://arxiv.org/abs/2512.06629)
*Xiao-li Xia,Hou-biao Li*

Main category: cs.AI

TL;DR: FlatFormer通过信息注入而非结构堆叠，解决了知识追踪中的性能-复杂度困境，在保持轻量化的同时实现了SOTA性能


<details>
  <summary>Details</summary>
Motivation: 知识追踪模型面临"性能-复杂度困境"：捕捉复杂认知动态需要深度层次架构，但这会导致实时部署时的计算成本过高

Method: 提出FlatFormer架构，采用"信息注入而非结构堆叠"设计范式，包括：(1)混合输入编码策略（可学习会话标识符+固定正弦步长嵌入）；(2)预计算幂律偏置直接集成到注意力对数中，显式建模遗忘曲线

Result: 在四个大规模数据集上实验表明，FlatFormer达到SOTA性能。在EdNet数据集上，相比最强层次基线HiTSKT，AUC绝对提升8.3%，参数使用量不到15%，推理速度约快3倍

Conclusion: 高认知保真度不需要架构复杂性，通过轻量级信息注入机制可以在保持高效推理的同时实现卓越性能

Abstract: Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.

</details>


### [22] [The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds](https://arxiv.org/abs/2512.07631)
*Shahar Lutati*

Main category: cs.AI

TL;DR: 提出Agent Capability Problem (ACP)框架，通过信息获取视角预测智能体在资源约束下能否解决问题，用信息量/信息获取率×成本计算有效成本来预估资源需求


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖经验启发式，缺乏理论框架预测智能体在资源约束下能否完成任务。需要一种能提前预估资源需求的理论方法，避免资源浪费

Method: 将问题解决视为信息获取过程：智能体需要I_total比特信息识别解决方案，每步动作获得I_step比特信息，成本为C_step。计算有效成本C_eff = (I_total/I_step) × C_step来预测资源需求

Result: 证明C_eff是期望成本的下界，并提供紧密的概率上界。实验验证ACP预测与实际智能体性能高度一致，能有效约束搜索努力，相比贪婪和随机策略提高效率

Conclusion: ACP为预测智能体能力提供了统一的信息论框架，将主动学习、贝叶斯优化和强化学习原则联系起来，适用于LLM和智能体工作流

Abstract: When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\Itotal$ bits to identify a solution and gains $\Istep$ bits per action at cost $\Cstep$, yielding an effective cost $\Ceff = (\Itotal/\Istep), \Cstep$ that predicts resource requirements before search. We prove that $\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \

</details>


### [23] [LightSearcher: Efficient DeepSearch via Experiential Memory](https://arxiv.org/abs/2512.06653)
*Hengzhi Lan,Yue Yu,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Ting Bai*

Main category: cs.AI

TL;DR: LightSearcher是一个高效的强化学习框架，通过结合文本经验记忆和自适应奖励机制，在保持准确性的同时显著减少DeepSearch范式中的工具调用次数和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的RL驱动的DeepSearch系统存在准确性-效率权衡问题：频繁调用外部搜索工具可以提高事实准确性，但会导致不必要的计算开销和效率下降。

Method: 提出LightSearcher框架：1) 通过学习对比推理轨迹生成可解释的成功推理模式摘要，引入文本经验记忆；2) 采用自适应奖励塑造机制，仅在正确答案场景中惩罚冗余工具调用。

Result: 在四个多跳QA基准测试中，LightSearcher在保持与SOTA基线ReSearch相当的准确性的同时，将搜索工具调用减少39.6%，推理时间减少48.6%，token消耗减少21.2%。

Conclusion: LightSearcher通过创新的记忆机制和奖励设计，有效解决了DeepSearch范式中的准确性-效率权衡问题，实现了更高效的外部知识检索。

Abstract: DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.

</details>


### [24] [Academic journals' AI policies fail to curb the surge in AI-assisted academic writing](https://arxiv.org/abs/2512.06705)
*Yongyuan He,Yi Bu*

Main category: cs.AI

TL;DR: 对5114种期刊和520万篇论文的分析显示，尽管70%的期刊采用了AI政策，但AI写作工具的使用率在各学科中急剧增长，政策无明显效果，且AI使用透明度极低（仅0.1%的论文明确披露）。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在学术写作中的快速应用引发了期刊和出版商的广泛政策响应，但这些政策的实际效果尚不明确，需要评估AI使用指南在现实世界中的影响。

Method: 分析了5114种期刊和超过520万篇论文，评估AI使用指南的实际影响；对16.4万篇科学出版物进行全文分析，特别关注2023年以来的7.5万篇论文。

Result: 1. 尽管70%的期刊采用了AI政策（主要是要求披露），但研究人员使用AI写作工具的比例在各学科中急剧增长；2. 有政策和无政策的期刊之间没有显著差异；3. 非英语国家、物理科学和高开放获取期刊的增长率最高；4. 透明度存在巨大差距：2023年以来的7.5万篇论文中，只有76篇（0.1%）明确披露了AI使用。

Conclusion: 当前政策在促进透明度或限制AI采用方面基本失败，需要重新评估伦理框架以促进科学中负责任的AI整合。

Abstract: The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.

</details>


### [25] [Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation](https://arxiv.org/abs/2512.06710)
*Zairah Mustahsan,Abel Lim,Megna Anand,Saahil Jain,Bryan McCann*

Main category: cs.AI

TL;DR: 论文提出使用组内相关系数(ICC)来评估语言模型代理系统的可靠性，区分真实能力提升与随机采样噪声，建议在报告准确率时同时报告ICC和组内方差。


<details>
  <summary>Details</summary>
Motivation: 当前评估实践仅报告单次运行的准确率，掩盖了结果背后的方差，无法区分真实能力改进与幸运采样。随着大语言模型成为更大代理系统的组件，评估可靠性变得至关重要，不可靠的子代理会给下游系统行为带来脆弱性。

Method: 采用测量科学中的组内相关系数(ICC)来表征方差，将观测方差分解为查询间方差(任务难度)和查询内方差(代理不一致性)。在GAIA(代理能力)和FRAMES(检索和事实性)基准上进行评估，分析不同任务结构下的ICC变化。

Result: ICC随任务结构变化显著：推理和检索任务(FRAMES)的ICC为0.4955-0.7118，代理任务(GAIA)的ICC为0.304-0.774。对于代理系统中的子代理替换决策，只有在ICC也改进时，准确率提升才可信。ICC收敛需要n=8-16次试验(结构化任务)或n>=32次(复杂推理)。

Conclusion: 建议将准确率与ICC和组内方差一起报告作为标准实践，提出更新的评估卡片来捕捉这些指标。通过使评估稳定性可见，旨在将代理基准测试从不透明的排行榜竞争转变为可信的实验科学。

Abstract: As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.

</details>


### [26] [Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents](https://arxiv.org/abs/2512.06716)
*Zhibo Liang,Tianze Hu,Zaiye Chen,Mingjie Tang*

Main category: cs.AI

TL;DR: 论文提出认知控制架构(CCA)，通过意图图和分层裁决器构建双层防御系统，有效抵御间接提示注入攻击，解决现有防御机制在安全、功能和效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理对间接提示注入攻击(IPI)存在显著脆弱性，这些攻击通过污染外部信息源劫持代理行为。当前防御机制存在安全与功能之间的根本权衡，导致防御架构碎片化，无法在整个任务执行流程中提供完整的完整性保证。

Method: 提出认知控制架构(CCA)，包含两个协同支柱：1) 通过预生成的"意图图"实现主动控制流和数据流完整性执行；2) 创新的"分层裁决器"，在检测到偏差时基于多维评分启动深度推理，专门应对复杂的条件攻击。

Result: 在AgentDojo基准测试中，CCA不仅能有效抵御挑战其他先进防御方法的复杂攻击，还能在保持显著效率和鲁棒性的同时实现无妥协的安全性，从而调和了多维权衡问题。

Conclusion: CCA通过全生命周期认知监督框架，基于攻击行为轨迹偏差检测的核心洞察，构建了高效的双层防御系统，成功解决了LLM代理对间接提示注入攻击的脆弱性问题，实现了安全、功能和效率的平衡。

Abstract: Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated "Intent Graph"; and (ii) an innovative "Tiered Adjudicator" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.

</details>


### [27] [ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems](https://arxiv.org/abs/2512.06721)
*Bufang Yang,Lilin Xu,Liekang Zeng,Yunqi Guo,Siyang Jiang,Wenrui Lu,Kaiwei Liu,Hancheng Xiang,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

TL;DR: ProAgent：首个端到端主动代理系统，利用多模态感知和LLM推理提供主动服务，相比现有被动代理显著提升预测准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理主要采用被动响应范式，依赖用户明确指令启动服务，增加了用户的物理和认知负担。需要一种能够主动感知环境并预判用户需求的智能代理系统。

Method: 1. 主动导向的上下文提取：采用按需分层感知持续监测环境，提取包含感官信号和用户画像的分层上下文；2. 上下文感知的主动推理器：将上下文映射到用户需求并调用相应工具，提供主动协助。

Result: 在真实世界测试平台、公共数据集和用户研究中，ProAgent相比最先进基线：主动预测准确率提升33.4%，工具调用F1分数提升16.8%，用户满意度显著提高。

Conclusion: ProAgent是首个端到端的主动代理系统，通过结合多模态感知和LLM推理，实现了从被动响应到主动协助的范式转变，标志着向真正主动助手迈出了重要一步。

Abstract: Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.

</details>


### [28] [DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems](https://arxiv.org/abs/2512.06749)
*Ming Ma,Jue Zhang,Fangkai Yang,Yu Kang,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: DoVer是一个基于干预的调试框架，通过主动验证而非仅日志分析来定位和修复LLM多智能体系统中的故障


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体系统调试存在两个关键局限：1) 仅依赖日志的调试缺乏验证，产生未经测试的假设；2) 单步或单智能体归因往往不准确，因为多个不同的干预措施都能独立修复失败任务

Method: DoVer采用干预驱动的调试框架，通过主动验证（如编辑消息、修改计划等针对性干预）来增强假设生成，并关注系统是否解决故障或取得可量化的任务进展，而非简单的归因准确性

Result: 在Magnetic-One智能体框架上，DoVer将18-28%的失败试验转为成功，实现高达16%的里程碑进展，验证或反驳30-60%的故障假设。在不同数据集和框架上也能恢复49%的失败试验

Conclusion: 干预是提高智能体系统可靠性的实用机制，为基于LLM的多智能体系统提供了更鲁棒、可扩展的调试方法

Abstract: Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.

</details>


### [29] [Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning](https://arxiv.org/abs/2512.06835)
*Tingyu Li,Zheng Sun,Jingxuan Wei,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: DoGe提出双解耦框架，通过分离思考者和解决者组件，结合两阶段强化学习，解决专业领域视觉语言模型训练中的数据稀缺和奖励黑客问题，实现持续自我演化。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型通过强化学习实现推理能力，但在化学、地球科学、多模态数学等专业领域面临高质量多模态数据稀缺问题。现有方法如合成数据和自我奖励机制存在分布有限和对齐困难，导致奖励黑客问题（模型利用高奖励模式，导致策略熵崩溃和训练不稳定）。

Method: DoGe采用双解耦框架：1) 将学习过程解耦为思考者和解决者两个组件，引导模型首先从上下文学习而非直接解决问题；2) 提出两阶段强化学习后训练方法，从自由探索上下文到实际解决任务；3) 构建演化课程学习管道，包括扩展的本领域知识语料库和迭代演化的种子问题池。

Result: 实验表明，DoGe方法在各种基准测试中持续优于基线方法，为实现自我演化的大型视觉语言模型提供了可扩展的路径。

Conclusion: DoGe通过双解耦框架解决了专业领域视觉语言模型训练中的数据稀缺和奖励黑客问题，通过合理的奖励信号量化和演化课程学习，为实现持续自我演化的大型视觉语言模型提供了有效解决方案。

Abstract: Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.

</details>


### [30] [JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models](https://arxiv.org/abs/2512.06859)
*Ce Chi,Xing Wang,Zhendong Wang,Xiaofan Liu,Ce Li,Zhiyan Song,Chen Zhao,Kexin Yang,Boshen Shi,Jingjing Yang,Chao Deng,Junlan Feng*

Main category: cs.AI

TL;DR: JT-DA-8B是一个专为复杂表格推理任务设计的8B参数大语言模型，通过构建包含34个表格推理任务的多样化训练语料，结合SFT和RL优化，在多种表格推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决表格推理场景中高质量监督数据缺乏的问题，需要为复杂的现实世界表格分析任务开发专门的模型。

Method: 1) 构建包含29个公共表格QA数据集和300万张表格的多样化训练语料；2) 提出自动流水线生成多步分析任务；3) 基于JT-Coder-8B模型，采用LLM评分和工作流对齐过滤来蒸馏高质量数据；4) 结合监督微调和强化学习优化模型；5) 提出四阶段表格推理工作流（表格预处理、表格感知、工具集成推理、提示工程）。

Result: JT-DA-8B在各种表格推理任务中表现出强大的性能，证明了数据为中心的数据生成和工作流驱动的优化方法的有效性。

Conclusion: 通过构建全面的训练语料、采用数据为中心的生成方法和工作流驱动的优化策略，JT-DA-8B成功解决了复杂表格推理任务，为现实世界表格分析提供了有效的解决方案。

Abstract: In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.

</details>


### [31] [Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?](https://arxiv.org/abs/2512.06867)
*John Licato,Stephen Steinle,Brayden Hollis*

Main category: cs.AI

TL;DR: 研究探讨人格提示对LLM在战略游戏PERIL中决策行为的影响，发现某些战略型人格能提升游戏表现，但需要中介机制将人格转化为启发式策略。


<details>
  <summary>Details</summary>
Motivation: 虽然人格提示似乎能触发LLM生成不同风格的文本，但尚不清楚这些差异是否能转化为可测量的行为差异，特别是在对抗性战略环境中的决策影响。

Method: 使用PERIL世界统治棋盘游戏作为测试平台，比较人格衍生的启发式策略与手动选择策略的效果。引入基于探索性因子分析的结构化翻译过程作为中介，将LLM生成的人格清单响应映射为启发式值。

Result: 某些与战略思维相关的人格能提高游戏表现，但仅当使用中介机制将人格转化为启发式值时。该方法相比直接推断的启发式策略，提高了启发式的可靠性和表面效度。

Conclusion: 人格提示确实影响LLM的决策制定，但需要结构化翻译过程才能有效转化。研究提出了一种将心理测量学原理应用于LLM的启发式生成方法，有助于更好地研究人格类型对决策的影响。

Abstract: Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.

</details>


### [32] [On Memory: A comparison of memory mechanisms in world models](https://arxiv.org/abs/2512.06983)
*Eli J. Laird,Corey Clark*

Main category: cs.AI

TL;DR: 论文研究了基于Transformer的世界模型的有效记忆跨度，通过分析多种记忆增强机制来改善长时程规划中的感知漂移问题，提出了记忆编码与记忆注入的分类方法。


<details>
  <summary>Details</summary>
Motivation: 世界模型通过预测未来状态来支持智能体在想象环境中进行规划，但现有模型的长时程规划能力受限于架构的有效记忆跨度，导致长轨迹生成中出现感知漂移，难以完成轨迹闭环。

Method: 提出了记忆编码与记忆注入机制的分类法，从残差流动态角度分析这些机制如何扩展世界模型的记忆。使用状态回忆评估任务测量每种机制的记忆回忆能力，并分析各自的权衡。

Result: 研究发现记忆机制能有效提升视觉Transformer的有效记忆跨度，为在世界模型想象中完成轨迹闭环提供了路径。

Conclusion: 通过系统分析Transformer世界模型的记忆增强机制，证明了这些机制在扩展记忆跨度和解决长时程规划中感知漂移问题方面的有效性。

Abstract: World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.

</details>


### [33] [Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients](https://arxiv.org/abs/2512.06990)
*Krishna Arun,Moinak Bhattachrya,Paras Goel*

Main category: cs.AI

TL;DR: 开发了一个端到端的AI系统，用于辅助医生诊断和治疗胶质母细胞瘤，包含诊断阶段的序列决策框架和治疗阶段的强化学习系统，能显著降低计算成本并提高治疗效果。


<details>
  <summary>Details</summary>
Motivation: 目前医学领域缺乏AI支持医生治疗异质性脑肿瘤如胶质母细胞瘤（GBM），这是世界上最致命的癌症，五年生存率仅为5.1%。

Method: 诊断阶段：使用包含4个分类模型（卷积神经网络和支持向量机）的序列决策框架，逐步将患者大脑分类到更具体的类别。治疗阶段：使用包含3个生成模型的强化学习系统：切除模型（扩散模型）预测可能的切除结果；放疗模型（时空视觉变换器）生成指定周数后的大脑MRI；化疗模型（扩散模型）生成治疗后MRI。生存率计算器检查生成的治疗后MRI是否在用户定义目标的15%范围内，否则使用近端策略优化的反馈循环迭代直到找到最佳切除位置。

Result: 关键发现：1）使用4个小诊断模型的序列决策框架将计算成本降低22.28倍；2）变换器的回归能力将肿瘤进展推理时间减少113小时；3）应用类似真实情况的增强将整体DICE分数提高2.9%。这些结果预计可将生存率提高0.9%，可能挽救约2250条生命。

Conclusion: 该AI系统为胶质母细胞瘤的诊断和治疗规划提供了端到端的解决方案，通过创新的序列决策框架和强化学习系统，显著提高了计算效率和治疗效果，有望改善患者生存率。

Abstract: Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.

</details>


### [34] [ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes](https://arxiv.org/abs/2512.07081)
*Rongjia Zhou,Chengzhuo Li,Carl Yang,Jiaying Lu*

Main category: cs.AI

TL;DR: ClinNoteAgents是一个基于大语言模型的多智能体框架，能够将自由文本临床笔记转化为结构化风险因素表示和临床医生风格的抽象，用于心衰30天再入院预测，在数据有限的医疗系统中提供可扩展且可解释的方法。


<details>
  <summary>Details</summary>
Motivation: 心衰是美国老年人再入院的主要原因之一。临床笔记包含丰富的患者信息，占电子健康记录的很大部分，但在心衰再入院风险分析中仍未得到充分利用。传统模型依赖专家规则、医学术语表和本体来解释临床笔记，但这些笔记通常是在时间压力下书写的，可能包含拼写错误、缩写和领域特定术语。

Method: 提出ClinNoteAgents，一个基于LLM的多智能体框架，将自由文本临床笔记转化为：(1) 用于关联分析的结构化临床和社会风险因素表示；(2) 用于心衰30天再入院预测的临床医生风格抽象。

Result: 在3,544份来自2,065名患者（再入院率=35.16%）的笔记上评估ClinNoteAgents，展示了在从自由文本提取风险因素、识别关键贡献因素和预测再入院风险方面的强大性能。

Conclusion: 通过减少对结构化字段的依赖并最小化手动标注和模型训练，ClinNoteAgents为数据有限的医疗系统中的基于笔记的心衰再入院风险建模提供了可扩展且可解释的方法。

Abstract: Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.

</details>


### [35] [VIGIL: A Reflective Runtime for Self-Healing Agents](https://arxiv.org/abs/2512.07094)
*Christopher Cruz*

Main category: cs.AI

TL;DR: VIGIL是一个用于LLM代理的反射运行时系统，能够自主监控、诊断故障并进行自我修复，而不是执行具体任务。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理框架大多脆弱，缺乏运行时自省能力，无法诊断自身故障模式，且没有人类干预就无法改进。许多代理系统退化为装饰性的LLM调用链，缺乏可靠性的结构机制。

Method: VIGIL通过监督兄弟代理执行自主维护：1) 摄入行为日志；2) 将事件评估为结构化情感表示；3) 维护具有衰减和上下文策略的持久EmoBank；4) 生成RBT诊断（优势、机会、失败）；5) 生成保护性提示更新和只读代码提案；6) 作为状态门控管道运行，非法转换会产生显式错误。

Result: 在提醒延迟案例研究中，VIGIL识别出延迟升高，提出了提示和代码修复方案。当自身诊断工具因模式冲突失败时，它能暴露内部错误、生成备用诊断并发出修复计划，展示了部署代理运行时的元级自我修复能力。

Conclusion: VIGIL实现了LLM代理的自主监控和自我修复能力，解决了当前代理系统缺乏运行时自省和持续改进的问题，为构建更可靠的自主代理系统提供了新方法。

Abstract: Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.

</details>


### [36] [A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy](https://arxiv.org/abs/2512.07109)
*Miguel Ingram,Arthur Joseph Merritt*

Main category: cs.AI

TL;DR: 该研究提出了首个包含400个任务的9类别分类法，揭示了Transformer架构在抽象推理任务中的组成性差距和神经亲和力天花板效应。


<details>
  <summary>Details</summary>
Motivation: 响应Hodel等人对任务相关性形式化定义的需求，旨在系统分析抽象推理任务与神经网络架构之间的匹配度问题。

Method: 1) 开发基于规则代码分析的9类别任务分类法；2) 使用CNN验证分类法的视觉一致性；3) 在302个任务上微调170万参数Transformer；4) 应用分类法诊断ARC-AGI-2测试集。

Result: 1) 分类法准确率达97.5%；2) 发现69.5%任务存在组成性差距（局部模式准确率>80%但全局合成<10%）；3) 揭示35.3%任务对Transformer具有低神经亲和力；4) 在独立研究中验证了预测能力（低亲和力任务51.9% vs 高亲和力77.7%）。

Conclusion: 神经网络性能受架构适宜性限制而非训练数据，需要开发具有亲和力对齐模块的混合架构来推进抽象推理能力。

Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,

</details>


### [37] [ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation](https://arxiv.org/abs/2512.07178)
*Latifa Dwiyanti,Sergio Ryan Wibisono,Hidetaka Nambo*

Main category: cs.AI

TL;DR: 提出一个Python包，将SHAP与大型语言模型（GPT）结合，生成情境化的文本解释，提升非技术用户对特征重要性解释的理解度。


<details>
  <summary>Details</summary>
Motivation: SHAP虽然能有效可视化特征重要性，但缺乏对非技术背景用户有意义的情境解释。需要提供更用户友好的解释方式，特别是在高风险领域如医疗保健中。

Method: 开发Python包，集成SHAP与OpenAI的GPT模型，通过用户定义的参数（特征别名、描述、背景信息）来生成情境化的文本解释。

Result: 在医疗保健案例研究中应用，用户评估显示生成的解释比纯可视化输出更易理解和情境适当。Likert量表和访谈结果支持这一发现。

Conclusion: 将可视化与情境化文本结合可以支持更用户友好和可信的模型解释，尽管研究结果是初步的，但显示了这种方法的潜力。

Abstract: Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.

</details>


### [38] [PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations](https://arxiv.org/abs/2512.07179)
*Wonbeen Lee,Channyoung Lee,Junho Sohn,Hansam Cho*

Main category: cs.AI

TL;DR: 提出PICKT模型解决知识追踪中的冷启动问题，通过知识图谱处理多种输入数据，提升个性化学习系统的实用性


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型存在输入数据格式受限、新学生/新问题冷启动问题、实际服务环境稳定性不足等局限性，需要更实用的解决方案

Method: 提出PICKT模型，利用知识图谱结构化概念间关系，结合问题和概念文本信息，有效处理多种类型输入数据

Result: 实验显示模型在真实操作环境中表现优异，显著提升新学生注册和新问题添加两个核心冷启动场景的性能

Conclusion: PICKT模型通过有效利用多样化数据格式、解决冷启动问题、验证稳定性，为下一代智能辅导系统的实际应用提供了重要理论和技术基础

Abstract: With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.

</details>


### [39] [Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation](https://arxiv.org/abs/2512.07212)
*Zhaoyang Liu,Mokai Pan,Zhongyi Wang,Kaizhen Zhu,Haotao Lu,Jingya Wang,Ye Shi*

Main category: cs.AI

TL;DR: BridgePolicy是一种新的扩散桥生成式视觉运动策略，通过将观测嵌入扩散过程的随机微分方程中，从信息丰富的先验而非随机噪声开始采样，显著提高了机器人控制的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的模仿学习方法通常将观测作为去噪网络的高层条件输入，而不是将其整合到扩散过程的随机动力学中。这导致采样必须从随机高斯噪声开始，削弱了感知与控制之间的耦合，常常产生次优性能。

Method: 提出BridgePolicy，通过扩散桥公式将观测显式嵌入随机微分方程中。设计了多模态融合模块和语义对齐器，统一视觉和状态输入，对齐观测和动作表示，使桥方法适用于异构机器人数据。

Result: 在三个基准测试的52个模拟任务和五个真实世界任务上的广泛实验表明，BridgePolicy在生成式策略中始终优于最先进的方法。

Conclusion: 通过将观测整合到扩散过程的随机动力学中，BridgePolicy能够从信息丰富的先验开始采样，显著提高了机器人控制的精度和可靠性，为异构机器人数据提供了一种有效的扩散桥方法。

Abstract: Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.

</details>


### [40] [Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model](https://arxiv.org/abs/2512.07232)
*Wenlong Liu,Jiahua Pan,Xingyu Zhang,Xinxin Gong,Yang Ye,Xujin Zhao,Xin Wang,Kent Wu,Hua Xiang,Houmin Yan,Qingpeng Zhang*

Main category: cs.AI

TL;DR: 本文提出RAEA框架用于实体对齐，通过两阶段过滤匹配eBay和亚马逊产品，利用属性和关系三元组的交互提升对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有实体对齐方法未能充分利用属性三元组和关系三元组及其交互作用，导致产品匹配效果有限。

Method: 采用两阶段管道：粗过滤和细过滤。细过滤使用RAEA框架，包含属性感知实体编码器和关系感知图注意力网络，聚合属性和关系的对齐信号。

Result: RAEA在跨语言数据集DBP15K上比12个基线平均Hits@1提升6.59%，在单语言数据集DWY100K上取得竞争性结果。

Conclusion: RAEA通过有效利用属性和关系三元组的交互，显著提升了实体对齐性能，为产品匹配提供了有效解决方案。

Abstract: Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).

</details>


### [41] [M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling](https://arxiv.org/abs/2512.07314)
*Yuxiao Luo,Songming Zhang,Sijie Ruan,Siran Chen,Kang Liu,Yang Xu,Yu Zheng,Ling Yin*

Main category: cs.AI

TL;DR: M-STAR是一个多尺度时空自回归框架，通过从粗到细的预测过程生成长期人类移动轨迹，在保真度和生成速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归和扩散模型的方法在生成长期轨迹（如周轨迹）时效率低下，且缺乏显式的时空多尺度建模能力，限制了人类移动建模的应用。

Method: 提出M-STAR框架：1）多尺度时空标记器编码分层移动模式；2）基于Transformer的解码器进行下一尺度自回归预测，实现从粗到细的时空预测过程。

Result: 在两个真实数据集上的实验表明，M-STAR在轨迹保真度上优于现有方法，并显著提高了生成速度。

Conclusion: M-STAR通过多尺度时空建模有效解决了长期轨迹生成的效率和建模能力问题，为人类移动建模提供了新的解决方案。

Abstract: Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.

</details>


### [42] [A Geometric Unification of Concept Learning with Concept Cones](https://arxiv.org/abs/2512.07355)
*Alexandre Rocchi--Henry,Thomas Fel,Gianni Franchi*

Main category: cs.AI

TL;DR: 该论文提出统一概念瓶颈模型(CBMs)和稀疏自编码器(SAEs)的几何框架，将两者视为学习激活空间中的概念锥，并建立评估SAEs学习概念与人类定义概念对齐程度的度量方法。


<details>
  <summary>Details</summary>
Motivation: CBMs（监督方法）和SAEs（无监督方法）作为两种可解释性传统各自发展但缺乏交流，需要建立统一的框架来连接这两种范式，评估无监督发现的概念与人类定义概念的对齐程度。

Method: 提出几何框架：将CBMs和SAEs都视为学习激活空间中的线性方向，其非负组合形成概念锥。建立包含性框架，用CBMs提供参考几何，评估SAEs学习锥与CBM锥的近似或包含关系。开发定量指标连接稀疏度、扩展比等归纳偏置与概念涌现。

Result: 发现稀疏度和扩展因子的"最佳点"，能最大化与CBM概念的几何和语义对齐。建立了评估SAEs进展的原则性度量，并能评估发现概念与人类合理概念的对齐程度。

Conclusion: 通过共享的几何框架统一了监督和无监督的概念发现，为测量SAEs进展和评估发现概念与人类概念的对齐提供了原则性度量方法。

Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.

</details>


### [43] [LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services](https://arxiv.org/abs/2512.07436)
*Hang He,Chuhuai Yue,Chengqi Dong,Mingxue Tian,Zhenfeng Liu,Jiajun Chai,Xiaohan Wang,Yufei Zhang,Qun Liao,Guojun Yin,Wei Lin,Chengcheng Wan,Haiying Sun,Ting Su*

Main category: cs.AI

TL;DR: LocalSearchBench：首个针对本地生活服务的智能搜索基准，包含15万高质量条目和300个多跳问答任务，揭示现有大模型在该领域表现不佳（最佳模型正确率仅34.34%）。


<details>
  <summary>Details</summary>
Motivation: 现有大模型研究主要关注通用信息检索，很少探索具有独特挑战的垂直领域。本地生活服务领域的查询往往模糊且需要跨商家和产品的多跳推理，现有方法未能充分解决这些问题。

Method: 构建LocalSearchBench基准，包含来自不同城市和业务类型的15万高质量条目，基于真实用户查询创建300个多跳问答任务。同时开发LocalPlayground统一环境，集成多种工具供智能体交互。

Result: 实验显示，即使是当前最先进的大模型在LocalSearchBench上表现不佳：最佳模型DeepSeek-V3.1正确率仅34.34%，大多数模型在完整性（平均77.33%）和忠实性（平均61.99%）方面存在问题。

Conclusion: 本地生活服务领域需要专门的基准和领域特定的智能体训练。现有大模型在该垂直领域仍面临显著挑战，突显了领域专业化的重要性。

Abstract: Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.

</details>


### [44] [How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations](https://arxiv.org/abs/2512.07497)
*JV Roig*

Main category: cs.AI

TL;DR: 研究通过KAMI基准测试分析LLM作为自主工具使用代理的失败模式，发现模型规模并非代理可靠性的唯一决定因素，识别出四种常见失败模式，强调需要关注交互式基础、恢复行为和环境感知适应


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型作为具有工具使用能力的自主代理时的失败机制，而非仅仅关注聚合性能分数，旨在深入理解多步工具执行中的成功策略和反复出现的失败模式

Method: 使用Kamiwaza Agentic Merit Index (KAMI) v0.1基准测试，分析900个执行轨迹，涵盖三个代表性模型（Granite 4 Small、Llama 4 Maverick、DeepSeek V3.1），涉及文件系统、文本提取、CSV分析和SQL场景，进行细粒度的每试验行为分析

Result: 模型规模本身不能预测代理鲁棒性：Llama 4 Maverick (400B)在某些不确定性驱动任务中仅略优于Granite 4 Small (32B)；DeepSeek V3.1的优越可靠性主要来自后训练强化学习而非架构或规模；识别出四种反复出现的失败模式：缺乏基础的过早行动、替代缺失实体的过度帮助、干扰诱导的上下文污染脆弱性、负载下的脆弱执行

Conclusion: 可靠的代理部署不仅需要更强的模型，还需要有意的训练和设计选择，强调验证、约束发现和遵循真实数据源的重要性，需要评估方法关注交互式基础、恢复行为和环境感知适应

Abstract: We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.

</details>


### [45] [Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement](https://arxiv.org/abs/2512.07611)
*Yongsheng Lian*

Main category: cs.AI

TL;DR: 系统比较了三种强化学习算法（PPO、GRPO、DAPO）在提升大语言模型复杂推理能力方面的效果，通过控制性迁移学习评估发现RL训练模型在所有任务上都优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同强化学习算法如何有效提升大语言模型的复杂推理能力，为RL-based LLM训练提供实践指导。

Method: 采用控制性迁移学习评估方法：首先在专门的Countdown Game上对模型进行微调，然后在通用推理基准测试套件上进行评估。对比了PPO、GRPO和DAPO三种RL算法，并分析了GRPO/DAPO的组大小、KL惩罚系数等参数影响。

Result: 所有RL训练模型在各项任务上都优于对应的基础模型，但改进程度因基准测试而异。增加GRPO和DAPO的组大小能带来更稳定的训练动态和更高准确率，KL惩罚系数的影响是非单调的。DAPO中的动态采样组件并未提升性能，禁用DS时DAPO取得最佳整体结果。

Conclusion: 强化学习能有效提升LLM的复杂推理能力，但算法选择和参数设置至关重要。GRPO和DAPO的组大小优化、KL惩罚系数的合理设置能显著改善训练效果，而DAPO的动态采样组件在实际应用中可能不需要。

Abstract: This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.

</details>


### [46] [Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE](https://arxiv.org/abs/2512.07710)
*Anxiang Zeng,Haibo Zhang,Hailing Zhang,Kaixiang Mo,Liang Yao,Ling Hu,Long Zhang,Shuman Liu,Shuyi Xie,Yanshi Li,Yizhang Chen,Yuepeng Sheng,Yuwei Huang,Zhaochen Xu,Zhiqiang Zhou,Ziqin Liew*

Main category: cs.AI

TL;DR: CompassMax-V3-Thinking是一个千亿规模的MoE推理模型，采用新的RL框架训练，核心原则是"每个提示都必须重要"。通过多项创新技术解决了大规模RL训练中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 将RL扩展到千亿规模时暴露出关键效率问题：零方差提示浪费rollout、长时域重要性采样不稳定、标准奖励模型导致的优势反转，以及rollout处理的系统性瓶颈。需要解决这些问题来实现大规模MoE模型的稳定高效RL训练。

Method: 提出四项统一创新：1) 多阶段零方差消除，过滤非信息性提示并稳定基于组的策略优化；2) ESPO熵自适应优化方法，平衡token级和序列级重要性采样；3) Router Replay策略，对齐训练时MoE路由器决策与推理时行为；4) 高吞吐RL系统，采用FP8精度rollout、重叠奖励计算和长度感知调度。

Result: 这些贡献形成了一个连贯的pipeline，使千亿规模MoE模型的RL训练变得稳定高效。最终模型在内部和公共评估中都表现出色。

Conclusion: 通过解决大规模RL训练中的关键效率问题，成功训练了千亿规模的MoE推理模型CompassMax-V3-Thinking，证明了所提方法的有效性。

Abstract: We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.

</details>


### [47] [RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2512.07761)
*Xiqiao Xiong,Ouxiang Li,Zhuo Liu,Moxin Li,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的多轮越狱攻击方法，通过训练攻击者LLM来诱导黑盒模型生成有害内容，相比单轮优化方法显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到越狱攻击的威胁，影响其在现实应用中的安全部署。现有方法通常依赖单轮优化，难以学习长期攻击策略，需要更有效的多轮攻击方法。

Method: 将问题形式化为多轮强化学习任务，直接优化最终轮输出的有害性作为结果奖励。提出两种启发式过程奖励：1) 控制中间输出的有害性以避免触发黑盒模型的拒绝机制；2) 保持中间输出的语义相关性以避免内容漂移。

Result: 在多个基准测试上的实验结果显示，该方法在多个模型上持续提高了攻击成功率，证明了方法的有效性。

Conclusion: 提出的基于强化学习的多轮越狱攻击方法能够有效训练攻击者LLM，通过多轮交互诱导黑盒模型生成有害内容，为语言模型安全研究提供了重要参考。

Abstract: Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.

</details>


### [48] [ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning](https://arxiv.org/abs/2512.07795)
*Nearchos Potamitis,Lars Klein,Akhil Arora*

Main category: cs.AI

TL;DR: ReasonBENCH：首个量化LLM推理不稳定性的基准，通过多轮评估协议提供统计可靠的质量与成本指标，揭示当前推理方法普遍存在高不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理评估主要报告单次运行准确率，忽略了随机解码带来的内在不确定性，导致无法可靠评估方法的稳定性、可复现性和成本一致性，存在评估盲点。

Method: 提出ReasonBENCH基准，包含：(1)标准化推理框架、模型和任务的模块化评估库；(2)报告质量和成本统计可靠指标的多轮评估协议；(3)鼓励方差感知报告的公开排行榜。

Result: 跨多个领域任务发现，绝大多数推理策略和模型表现出高不稳定性。即使平均性能相似的策略，置信区间宽度可相差四倍，且性能最佳的方法通常成本更高、更不稳定。

Conclusion: 可复现性是可靠LLM推理的关键维度，ReasonBENCH为未来推理方法和不确定性量化技术提供了基础，强调需要方差感知的评估实践。

Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .

</details>


### [49] [Large Causal Models from Large Language Models](https://arxiv.org/abs/2512.07796)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 提出DEMOCRITUS系统，利用大语言模型构建大规模因果模型，通过提取和组织跨领域因果知识，形成统一的因果网络。


<details>
  <summary>Details</summary>
Motivation: 传统因果推理局限于狭窄领域和实验数据，而大语言模型蕴含丰富的跨领域知识，需要新方法来系统性地提取和组织这些知识，构建大规模因果模型。

Method: 使用高质量LLM提出主题、生成因果问题、提取因果陈述，然后将这些分散的因果声明转化为关系三元组，通过新的范畴机器学习方法整合成连贯的大规模因果模型。

Result: 实现了DEMOCRITUS系统，包含六个模块，成功应用于考古学、生物学、气候变化、经济学、医学和技术等多个领域，展示了跨领域因果建模的能力。

Conclusion: DEMOCRITUS为构建大规模因果模型提供了新范式，虽然存在扩展瓶颈和局限性，但为利用LLM知识进行跨领域因果推理开辟了新方向。

Abstract: We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.

</details>


### [50] [Auditing Games for Sandbagging](https://arxiv.org/abs/2512.07810)
*Jordan Taylor,Sid Black,Dillon Bowen,Thomas Read,Satvik Golechha,Alex Zelenka-Martin,Oliver Makins,Connor Kissane,Kola Ayonrinde,Jacob Merizian,Samuel Marks,Chris Cundy,Joseph Bloom*

Main category: cs.AI

TL;DR: 论文通过审计游戏测试沙袋化检测技术，发现现有方法难以可靠识别故意隐藏能力的AI模型，但基于训练的能力激发方法能有效激发沙袋化模型的全部性能。


<details>
  <summary>Details</summary>
Motivation: 未来AI系统可能在评估中故意隐藏能力（沙袋化），误导开发者和审计者。需要测试现有检测技术能否有效识别这种欺骗行为。

Method: 采用红蓝对抗游戏：红队微调5个模型（部分有条件地表现不佳作为沙袋化代理），蓝队使用黑盒、模型内部或基于训练的方法来识别沙袋化模型。

Result: 蓝队无法可靠区分沙袋化模型与良性模型。黑盒方法被有效模仿较弱模型的行为击败。线性探针（模型内部方法）更有前景但易受红队植入行为影响。基于训练的能力激发能一致激发沙袋化模型的全部性能，但良性模型性能有时也会提升，导致假阳性。

Conclusion: 短期内建议开发者使用分布内训练消除潜在沙袋化。长期需要进一步研究确保基于训练的能力激发的有效性，并开发更稳健的沙袋化检测方法。

Abstract: Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [51] [Hybrid Beamfocusing Design for RSMA-Enabled Near-Field Wideband Communications](https://arxiv.org/abs/2512.06156)
*Jiasi Zhou,Chintha Tellambura*

Main category: cs.IT

TL;DR: 论文提出了一种基于速率分割多址接入（RSMA）的发射方案，用于宽带近场通信，采用基于真实时延（TTD）的混合波束聚焦架构来缓解空间宽带效应并降低射频链需求。


<details>
  <summary>Details</summary>
Motivation: 未来无线网络将使用超大规模天线阵列和高频段，但这会产生近场球面波前和空间宽带效应。需要开发有效方案来利用和缓解这些效应。

Method: 提出RSMA使能的发射方案，采用TTD混合波束聚焦架构。通过联合优化频率相关模拟波束聚焦、频率无关模拟波束聚焦、数字波束聚焦和公共速率分配来最大化最小速率。开发了基于惩罚的迭代算法，使用块坐标下降法交替优化三个变量块。

Result: 仿真结果表明：1）有效补偿了空间宽带效应；2）性能接近全数字波束聚焦但硬件复杂度更低；3）相比其他基准方案获得显著性能增益。

Conclusion: 提出的RSMA使能发射方案能有效解决宽带近场通信中的空间宽带效应问题，在保持低硬件复杂度的同时实现高性能，为未来无线网络提供了有前景的解决方案。

Abstract: Future wireless networks will utilize extremely large-scale antenna arrays (ELAAs) over high-frequency bands, which, however, produce near-field spherical wavefronts and spatial wideband effects. To exploit and mitigate these, this paper proposes a rate-splitting multiple access (RSMA)-enabled transmit scheme for wideband near-field communications (NFC). Our solution leverages true-time-delay (TTD)-based hybrid beamfocusing architectures to mitigate spatial wideband effect and reduce radio frequency chain requirements. The objective is to maximize the minimum rate by jointly optimizing frequency-dependent analog beamfocusing, frequency-independent analog beamfocusing, digital beamfocusing, and common rate allocation. To solve this complicated non-convex problem, we develop a penalty-based iterative algorithm that partitions the variables into three blocks and then employs block coordinate descent (BCD) to optimize each block alternately. This algorithm is further extended to support the sub-connected TTD-based analog beamfocusing architectures. Comprehensive simulation results indicate that our transmit scheme: 1) effectively compensates for spatial wideband effect, addressing a critical challenge in wideband operation; 2) achieves performance comparable to full digital beamfocusing while maintaining lower hardware complexity; 3) achieves substantial performance gains over the other two benchmarks.

</details>


### [52] [Non-Asymptotic Error Bounds for Causally Conditioned Directed Information Rates of Gaussian Sequences](https://arxiv.org/abs/2512.06238)
*Yuping Zheng,Andrew Lamperski*

Main category: cs.IT

TL;DR: 提出了高斯向量序列中因果条件定向信息率的估计器，误差界为O(N^{-1/2}log(N))


<details>
  <summary>Details</summary>
Motivation: 定向信息及其因果条件变体常用于测量随机过程间的因果影响，但在实践中需要从数据中估计。对于有限字母表序列已有非渐近误差界，但对于实值数据（特别是高斯向量序列）的研究较少。

Method: 基于最优预测推导因果条件定向信息率的显式公式，并基于该公式构建估计器。利用高斯过程的特性进行分析。

Result: 提出的估计器能以高概率达到O(N^{-1/2}log(N))的误差阶，其中N是总样本量。

Conclusion: 为高斯向量序列的因果条件定向信息率估计提供了理论保证，填补了实值数据中因果信息估计的理论空白。

Abstract: Directed information and its causally conditioned variations are often used to measure causal influences between random processes. In practice, these quantities must be measured from data. Non-asymptotic error bounds for these estimates are known for sequences over finite alphabets, but less is known for real-valued data. This paper examines the case in which the data are sequences of Gaussian vectors. We provide an explicit formula for causally conditioned directed information rate based on optimal prediction and define an estimator based on this formula. We show that our estimator gives an error of order $O\left(N^{-1/2}\log(N)\right)$ with high probability, where $N$ is the total sample size.

</details>


### [53] [Performance Bounds on Pliable Index Coding Using Absent Receivers](https://arxiv.org/abs/2512.06312)
*Lawrence Ong,Badri N. Vellambi,Parastoo Sadeghi,Jörg Kliewer*

Main category: cs.IT

TL;DR: 该论文研究了可塑性索引编码问题，针对更一般的实例（不限于特定约束）提出了基于解码链构造的新算法，推导出广播速率下界，并具体刻画了某些类别实例的最优广播速率。


<details>
  <summary>Details</summary>
Motivation: 现有大多数已解决的索引编码实例都属于特殊类别，即具有特定边信息基数的接收者要么全部存在要么全部不存在。本文旨在研究更一般的实例，不施加这种约束，以扩展可塑性索引编码问题的理论边界。

Method: 设计了一种新颖算法，通过迭代添加可由边信息已在链中的接收者解码的消息来构建解码链。当解码链因缺少具有所需消息的接收者而无法继续时，算法会跳过消息（直接将其加入链）。证明最优广播速率的下界是跳过消息数量的函数，该结果适用于所有可能的接收者解码选择和算法的任何实现。

Result: 推导出广播速率的下界表达式，并基于此给出了特定类别实例的显式下界。具体刻画了：1）最多包含四个缺失接收者且具有任意边信息模式的实例的最优广播速率；2）边信息集合以特定方式嵌套的实例的最优广播速率。

Conclusion: 本文提出的解码链算法和分析框架为可塑性索引编码问题提供了新的理论工具，成功刻画了更一般类别实例的广播速率下界，并具体解决了具有特定缺失接收者模式和嵌套边信息结构的实例的最优广播速率问题。

Abstract: We characterise bounds on the optimal broadcast rate for a few classes of pliable-index-coding instances. Unlike the majority of currently solved instances, which belong to a special class where all receivers with a certain side-information cardinality are either present or absent, we consider more general instances without this constraint. We devise a novel algorithm that constructs a decoding chain by iteratively adding a message that can be decoded by a receiver whose side information is already in the chain. If the decoding chain cannot proceed due to the absence of a receiver with the required messages, we skip a message by adding it to the chain regardless. We prove that a lower bound on the optimal broadcast rate is a function of the number of skipped messages, across all possible decoding choices of the receivers and any realisation of the algorithm for each decoding choice. While this result is not computationally feasible in isolation, it serves as a basis for deriving explicit lower bounds on the broadcast rate for specific classes of pliable-index-coding instances. These lower bounds depend on the number of absent receivers or the pattern of their side-information sets. Specifically, we explicitly characterise the optimal broadcast rate for instances with up to and including four absent receivers with any side-information pattern, as well as instances where the side-information sets are nested in particular ways.

</details>


### [54] [Trajectory Optimization for Cellular-Connected UAV in Complex Environment with Partial CKM](https://arxiv.org/abs/2512.06452)
*Yuxuan Song,Haiquan Lu,Chiya Zhang,Beixiong Zheng,Yong Zeng*

Main category: cs.IT

TL;DR: 该论文提出了一种将无人机导航与信道知识图(CKM)补全相结合的新框架，通过克里金插值和图论优化方法，在导航过程中同时完成CKM更新。


<details>
  <summary>Details</summary>
Motivation: 蜂窝连接无人机在无线网络中日益重要，但空中-地面信道的高视距概率导致严重同频干扰。现有CKM方法存在信息不完整和老化问题，需要定期更新以应对复杂环境的动态变化。

Method: 提出将无人机导航与CKM补全结合的统一框架：1) 无人机飞行时测量无线电信息，使用克里金插值补全CKM；2) 基于网格离散化和球面近似，建立混合整数多目标优化问题；3) 将问题转化为图论中的最短路径问题(SPP)和旅行商问题(TSP)变体；4) 提出两种基于不同模型的导航策略。

Result: 仿真结果表明，所提导航策略能快速扩展问题的帕累托边界，并接近完全已知CKM的性能。两种策略在无人机导航和CKM补全之间存在权衡关系，为工程实践提供了可实施方法。

Conclusion: 该研究成功建立了无人机导航与CKM补全的协同框架，通过图论方法有效解决了多目标优化问题，为蜂窝连接无人机的可靠导航和动态环境适应提供了创新解决方案。

Abstract: Cellular-connected unmanned aerial vehicles (UAVs) are expected to play an increasingly important role in future wireless networks. To facilitate the reliable navigation for cellular-connected UAVs, channel knowledge map (CKM) is considered a promising approach capable of tackling the non-negligible co-channel interference resulting from the high line-of-sight (LoS) probability of air-ground (AG) channels. Nevertheless, due to measurement constraints and the aging of information, CKM is usually incomplete and needs to be regularly updated to capture the dynamic nature of complex environments. In this paper, we propose a novel trajectory design strategy in which UAV navigation and CKM completion are incorporated into a common framework, enabling mutual benefits for both tasks. Specifically, a cellular-connected UAV deployed in an urban environment measures the radio information during its flight and completes the CKM with Kriging interpolation. Based on the method of grid discretization and spherical approximation, a mixed-integer multi-objective optimization problem is formulated. The problem falls into the category of combinatorial mathematics and is essentially equivalent to determining an optimum sequence of grid points to traverse. Through proper mathematical manipulation, the problem is reformulated as variants of two classic models in graph theory, namely the shortest-path problem (SPP) and the traveling salesman problem (TSP). Two navigation strategies based on the two different models are proposed and thoroughly compared based on numerical results to provide implementable methods for engineering practice and reveal the trade-offs between UAV navigation and CKM completion. Simulation results reveal that the proposed navigation strategies can quickly expand the Pareto boundary of the problem and approach the performance of fully-known CKM.

</details>


### [55] [Algebra in Algorithmic Coding Theory](https://arxiv.org/abs/2512.06478)
*Madhu Sudan*

Main category: cs.IT

TL;DR: 关于纠错码及其算法的综述，强调代数在构造和算法设计中的作用


<details>
  <summary>Details</summary>
Motivation: 虽然代数在纠错码构造中的作用已被广泛认可，但在算法设计中的作用往往较少被理解，本文旨在缩小这种认知差距

Method: 综述纠错码的概念和历史，介绍使纠错码在信息传输中有效的算法，提供基于应用代数的基本和现代构造方法

Result: 系统性地展示了代数在纠错码构造和算法设计中的重要作用，提供了从基础到现代的完整技术框架

Conclusion: 代数不仅在纠错码构造中至关重要，在算法设计中也发挥着同等重要的作用，本文通过综述帮助读者更好地理解这一联系

Abstract: We survey the notion and history of error-correcting codes and the algorithms needed to make them effective in information transmission. We then give some basic as well as more modern constructions of, and algorithms for, error-correcting codes that depend on relatively simple elements of applied algebra. While the role of algebra in the constructions of codes has been widely acknowledged in texts and other writings, the role in the design of algorithms is often less widely understood, and this survey hopes to reduce this difference to some extent.

</details>


### [56] [Improved Interactive Protocol for Synchronizing From Deletions](https://arxiv.org/abs/2512.06606)
*Haolun,Ni,Lev Tauz,Ryan Gabrys,Lara Dolecek*

Main category: cs.IT

TL;DR: 提出一种改进的文件同步协议，通过整合多删除纠错码和更通用的分区技术，降低通信成本，适用于一个文件是另一个文件子序列且存在恒定删除率的情况。


<details>
  <summary>Details</summary>
Motivation: 数据同步在云存储、基因组学和分布式系统等多个领域都是基础问题。现有协议在处理一个文件是另一个文件子序列且存在恒定删除率的情况时，主要依赖单删除纠错码，通信效率有待提升。

Method: 在现有基线协议基础上，整合先进的多删除纠错码，采用更通用的分区技术，提出改进的通信协议。推导适用于广泛删除纠错码类别的期望传输比特数上界。

Result: 实验结果表明，新方法在通信成本上优于基线协议。提出的改进协议在删除错误场景下能够实现低冗余同步。

Conclusion: 通过整合多删除纠错码和更通用的分区技术，提出的改进协议有效降低了通信成本，为存在删除错误的文件同步场景提供了更高效的解决方案。

Abstract: Data synchronization is a fundamental problem with applications in diverse fields such as cloud storage, genomics, and distributed systems. This paper addresses the challenge of synchronizing two files, one of which is a subsequence of the other and related through a constant rate of deletions, using an improved communication protocol. Building upon prior work, we integrate advanced multi-deletion correction codes into an existing baseline protocol, which previously relied on single-deletion correction. Our proposed protocol reduces communication cost by leveraging more general partitioning techniques as well as multi-deletion error correction. We derive a generalized upper bound on the expected number of transmitted bits, applicable to a broad class of deletion correction codes. Experimental results demonstrate that our approach outperforms the baseline in communication cost. These findings establish the efficacy of the improved protocol in achieving low-redundancy synchronization in scenarios where deletion errors occur.

</details>


### [57] [Cell-free ISAC for Drone Detection Considering Coverage and Age of Sensing](https://arxiv.org/abs/2512.06998)
*Zinat Behdad,Ozan Alp Topal,Cicek Cavdar*

Main category: cs.IT

TL;DR: 提出基于无蜂窝大规模MIMO的集成感知通信框架，用于无人机检测，引入感知新鲜度(AoS)和感知覆盖两个关键指标，通过热点分组、AP聚类和感知导频分配优化网络配置。


<details>
  <summary>Details</summary>
Motivation: 未经授权的无人机对公共安全构成重大威胁，需要空中监视解决方案。现有通信网络基础设施需要同时支持无人机检测和通信服务。

Method: 采用无蜂窝大规模MIMO架构，利用分布式接入点的空间多样性和协调性检测空中被动目标。引入感知新鲜度(AoS)指标（包含传输延迟、感知处理和网络延迟），提出模糊度参数量化热点间信道相似性，开发网络配置策略包括热点分组、AP聚类和感知导频分配。

Result: 当共享相同时频资源的热点数量与感知导频数量匹配时，AoS和感知覆盖之间的权衡达到最佳，表明模糊度是限制感知性能的主要因素。

Conclusion: 提出的无蜂窝ISAC框架能够在现有通信网络基础设施中实现无人机检测，同时保持通信服务。通过优化的网络配置策略，可以有效平衡感知新鲜度和覆盖范围。

Abstract: The growing presence of unauthorized drones poses significant threats to public safety, underscoring the need for aerial surveillance solutions. This work proposes a cell-free integrated sensing and communication (ISAC) framework enabling drone detection within the existing communication network infrastructure, while maintaining communication services. The system exploits the spatial diversity and coordination of distributed access points (APs) in a cell-free massive MIMO architecture to detect aerial passive targets. To evaluate sensing performance, we introduce two key metrics: age of sensing (AoS), capturing the freshness of sensing information, and sensing coverage. The proposed AoS metric includes not only the transmission delays as in the existing models, but also the processing for sensing and networking delay, which are critical in dynamic environments like drone detection. We introduce an ambiguity parameter quantifying the similarity between the target-to-receiver channels for two hotspots and develop a novel network configuration strategy, including hotspot grouping, AP clustering, and sensing pilot assignment, leveraging simultaneous multi-point sensing to minimize AoS. Our results show that the best trade-off between AoS and sensing coverage is achieved when the number of hotspots sharing the same time/frequency resource matches the number of sensing pilots, indicating ambiguity as the primary factor limiting the sensing performance.

</details>


### [58] [Function-Correcting Codes for Insertion-Deletion Channel](https://arxiv.org/abs/2512.07243)
*Anamika Singh,Abhay Kumar Singh*

Main category: cs.IT

TL;DR: 提出函数校正码在插入删除信道中的新框架，建立三种等价表述，推导冗余度上下界，并针对特定函数类给出具体界限。


<details>
  <summary>Details</summary>
Motivation: 插入删除信道中的冗余度优化是长期开放问题，对DNA数据存储和文档交换有重要应用价值。受函数校正码框架启发，将其扩展到插入删除信道。

Method: 提出函数校正插入码、删除码和插入删除码三种等价框架，定义插入删除距离矩阵和不规则插入删除距离码，推导冗余度上下界和Gilbert-Varshamov、Plotkin类界限。

Result: 建立了函数校正插入删除码的冗余度理论界限，针对局部有界函数、VT综合征函数、游程数函数和最大游程长度函数等特定函数类给出了具体冗余度界限。

Conclusion: 为插入删除信道中的函数校正码建立了系统理论框架，提供了冗余度优化的理论工具，对DNA数据存储等应用有重要价值。

Abstract: In coding theory, handling errors that occur when symbols are inserted or deleted from a transmitted message is a long-standing challenge. Optimising redundancy for insertion and deletion channels remains a key open problem with significant importance for applications in DNA data storage and document exchange. Recently, a coding framework known as function-correcting codes has been proposed to address the challenge of minimising redundancy while preserving specific functions of the message. This framework has gained attention due to its potential applications in machine learning systems and long-term archival data storage. Motivated by the problem of redundancy optimisation for insertion and deletion channels, we propose a new framework called function-correcting codes for insdel channels. In this paper, we introduce the notions of function-correcting insertion codes, function-correcting deletion codes, and function-correcting insdel codes, and we show that these three formulations are equivalent. We then define insdel distance matrices and irregular insdel-distance codes, and derive lower and upper bounds on the optimal redundancy achievable by function-correcting codes for insdel channels. In addition, we establish Gilbert-Varshamov and Plotkin-like bounds on the length of irregular insdel-distance codes. Using the relation between optimal redundancy and the length of such codes, we obtain a simplified lower bound on optimal redundancy. Finally, we derive bounds on the optimal redundancy of function-correcting insdel codes for several classes of functions, including locally bounded functions, VT syndrome functions, the number-of-runs function, and the maximum-run-length function.

</details>


### [59] [Improved bounds and optimal constructions of pure quantum locally recoverable codes](https://arxiv.org/abs/2512.07256)
*Yang Li,Shitao Li,Gaojun Luo,San Ling*

Main category: cs.IT

TL;DR: 本文针对量子局部可恢复码(qLRCs)提出新的紧致界，基于Hermitian构造识别多种最优qLRC无限族，显著扩展了已知最优码的长度范围。


<details>
  <summary>Details</summary>
Motivation: 量子局部可恢复码在大规模量子数据存储和量子LDPC码中具有重要应用潜力，但现有qLRC的界不够紧致，最优构造研究不足。

Method: 聚焦于Hermitian构造的纯qLRCs，提出多个新的紧致界，并证明量子Hamming码、量子GRM码、量子Solomon-Stiffler码等经典量子纠错码可产生具有明确参数的纯qLRCs。

Result: 新提出的界比已知界更紧致，基于构造识别出许多相对于不同界的最优qLRC无限族，其码长远超已知最优qLRCs。

Conclusion: 通过提出更紧致的界和构造方法，显著推进了量子局部可恢复码的理论发展，为大规模量子数据存储应用提供了更优的编码方案。

Abstract: By incorporating the concept of locality into quantum information theory, quantum locally recoverable codes (qLRCs) have been proposed, motivated by their potential applications in large-scale quantum data storage and their relevance to quantum LDPC codes. Despite the progress in optimal quantum error-correcting codes (QECCs), optimal constructions of qLRCs remain largely unexplored, partly due to the fact that the existing bounds for qLRCs are not sufficiently tight. In this paper, we focus on pure qLRCs derived from the Hermitian construction. We provide several new bounds for pure qLRCs and demonstrate that they are tighter than previously known bounds. Moreover, we show that a variety of classical QECCs, including quantum Hamming codes, quantum GRM codes, and quantum Solomon-Stiffler codes, give rise to pure qLRCs with explicit parameters. Based on these constructions, we further identify many infinite families of optimal qLRCs with respect to different bounds, achieving code lengths much larger than those of known optimal qLRCs.

</details>


### [60] [Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals](https://arxiv.org/abs/2512.07309)
*Guosheng Wang,Shen Wang,Lei Yang*

Main category: cs.IT

TL;DR: RFRP是一种自监督预训练框架，通过结合定位模型和神经射频辐射场，利用大规模无标签RF数据进行表征学习，显著提升了跨场景室内定位的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的室内定位模型依赖场景特定的标注数据，在跨场景泛化方面面临重大挑战。需要一种能够利用大规模无标签RF数据进行有效表征学习的方法。

Method: 提出Radiance-Field Reinforced Pretraining (RFRP)框架，采用非对称自编码器架构，将大型定位模型与神经射频辐射场耦合。定位模型编码接收的RF频谱为位置相关表征，RF-NeRF解码这些表征以重建原始频谱。

Result: 在100个不同场景的7,327,321个位置收集RF数据，使用75个场景训练，25个场景评估。RFRP预训练的定位模型相比未预训练模型减少40%以上的定位误差，相比监督学习预训练模型减少21%误差。

Conclusion: RFRP框架通过自监督预训练有效学习位置相关表征，显著提升跨场景室内定位的泛化性能，为RF-based定位系统提供了更强大的基础模型。

Abstract: Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning.

</details>


### [61] [Linear codes over $\frac{\mathbb{F}_q[u]}{\langle u^2 \rangle}$ with mixed-alphabet defining sets and their Gray images: Constructions of projective few-weight, distance-optimal and minimal codes](https://arxiv.org/abs/2512.07343)
*Leijo Jose,Lavanya G.,Anuradha Sharma*

Main category: cs.IT

TL;DR: 在混合字母环上构造四族线性码，研究其参数、Lee重量分布、Gray像，得到新的少重量、近Griesmer、距离最优和极小码，并应用于秘密共享方案和局部性分析。


<details>
  <summary>Details</summary>
Motivation: 研究混合字母环上的线性码构造，旨在获得具有新参数的少重量码、距离最优码和极小码，并探索其在秘密共享方案和局部性编码中的应用。

Method: 在混合字母环R = F_q[u]/⟨u²⟩ × F_q上，利用F_q^m中具有单个极大元素的单纯复形构造四族线性码，通过定义集方法确定参数和Lee重量分布，研究其Gray像。

Result: 获得了三族具有新参数的少重量、近Griesmer、距离最优和极小码；构造了两族新的射影少重量码；得到了两族二进制距离最优射影码和一族维数最优射影码；构造了四元射影3-重量码，其非零Hamming重量和为码长的9/4倍，产生强行走正则图。

Conclusion: 成功构造了混合字母环上的多族线性码，获得了具有新参数的各种最优码，并展示了其在秘密共享方案和局部性编码中的应用价值，为编码理论和应用提供了新的工具。

Abstract: Let $\mathcal{R}=\frac{\mathbb{F}_q[u]}{\langle u^2 \rangle}\times \mathbb{F}_q$ be the mixed alphabet ring. In this paper, we construct four infinite families of linear codes over the ring $\frac{\mathbb{F}_q[u]}{\langle u^2 \rangle}$ whose defining sets are certain nonempty subsets of $\mathcal{R}^m$ associated with three simplicial complexes of $\mathbb{F}_q^m,$ each possessing a single maximal element. We explicitly determine the parameters and Lee weight distributions of these codes. We also study their Gray images and obtain three infinite families of few weight, near Griesmer, distance optimal and minimal codes over $\mathbb{F}_q$ with new parameters. We also provide two constructions of infinite families of projective few weight codes over $\mathbb{F}_q$ with new parameters, and observe that these codes are self orthogonal for $q=2$ or $3.$ Additionally, we obtain two infinite families of binary distance optimal projective codes and an infinite family of dimension optimal projective codes over $\mathbb{F}_q$ with new parameters. Apart from this, we construct an infinite family of quaternary projective $3$-weight codes whose non zero Hamming weights sum to $\frac{9}{4}$ times the code length, which give rise to strongly walk regular graphs. As an application of our newly constructed minimal codes over $\mathbb{F}_q$, we examine the minimal access structures of Massey's secret sharing schemes based on their duals and determine the number of dictatorial participants in these schemes. Finally, we investigate the locality properties of our newly constructed projective codes.

</details>


### [62] [Dualities of dihedral and generalised quaternion codes and applications to quantum codes](https://arxiv.org/abs/2512.07354)
*Miguel Sales-Cabrera,Xaro Soler-Escrivà,Víctor Sotomayor*

Main category: cs.IT

TL;DR: 该论文研究了有限域上二面体群D_n和广义四元数群Q_n的群码，通过Wedderburn-Artin分解完整描述了它们的Hermitian对偶码和Euclidean对偶码，并应用于构建量子纠错码。


<details>
  <summary>Details</summary>
Motivation: 研究有限域上群代数的结构，特别是二面体群和广义四元数群的群码，旨在为这些特殊群码的对偶结构提供完整的代数描述，并应用于量子纠错码的构造。

Method: 利用Wedderburn-Artin分解理论，对群代数F_q^2[D_n]和F_q[Q_n]进行分解，通过群代数的结构理论分析群码的对偶性质，特别是Hermitian对偶和Euclidean对偶。

Result: 1. 完整描述了F_q^2上D_n码的Hermitian对偶码；2. 确定了所有Hermitian自正交D_n码；3. 完整描述了F_q上Q_n码的Euclidean对偶码；4. 由于F_q^2[Q_n]与F_q^2[D_{2n}]同构，也描述了Q_n码的Hermitian对偶码。

Conclusion: 通过群代数的Wedderburn-Artin分解，系统性地描述了特殊群码的对偶结构，并将这些理论结果应用于量子纠错码的构造，重建了一些已知的最优量子码。

Abstract: Let $\mathbb{F}_q$ be a finite field of $q$ elements, for some prime power $q$, and let $G$ be a finite group. A (left) group code, or simply a $G$-code, is a (left) ideal of the group algebra $\mathbb{F}_q[G]$. In this paper, we provide a complete algebraic description for the hermitian dual code of any $D_n$-code over $\mathbb{F}_{q^2}$, where $D_n$ is a dihedral group of order $2n$ with $\gcd(q,n)=1$, through a suitable Wedderburn-Artin's decomposition of the group algebra $\mathbb{F}_{q^2}[D_n]$, and we determine all distinct hermitian self-orthogonal $D_n$-codes over $\mathbb{F}_{q^2}$. We also present a thorough representation of the euclidean dual code of any $Q_n$-code over $\mathbb{F}_q$, where $Q_n$ is a generalised quaternion group of order $4n$ with $\gcd(q,4n)=1$, via the Wedderburn-Artin's decomposition of the group algebra $\mathbb{F}_q[Q_n]$. In particular, since the semisimple group algebras $\mathbb{F}_{q^2}[Q_n]$ and $\mathbb{F}_{q^2}[D_{2n}]$ are isomorphic, then the hermitian dual code of any $Q_n$-code has also been fully described. As application of the hermitian dualities computed, we give a systematic construction, via the structure of the group algebra, to obtain quantum error-correcting codes, and in fact we rebuild some already known optimal quantum codes with this methodical approach.

</details>


### [63] [Orbit recovery under the rigid motions group](https://arxiv.org/abs/2512.07405)
*Amnon Balanov,Tamir Bendory,Dan Edidin*

Main category: cs.IT

TL;DR: 该论文研究了SE(n)刚性运动群下的轨道恢复问题，建立了样本复杂度界限，并提出了三维刚性运动轨道恢复的可证明计算流程。


<details>
  <summary>Details</summary>
Motivation: 轨道恢复问题在信号处理、计算机视觉和结构生物学中具有基础重要性。特别是在结构生物学中，单粒子冷冻电镜（cryo-EM）和冷冻电子断层扫描（cryo-ET）等技术需要从受未知旋转和平移影响的噪声观测中重建未知信号。

Method: 主要理论贡献是：如果d阶SO(n)矩唯一确定信号轨道，则SE(n)下的轨道恢复可以通过N≳σ^{2d+4}个样本实现。关键技术洞察是从(d+2)阶SE(n)自相关中显式恢复d阶SO(n)矩，从而将旋转设置的结果转移到刚性运动情况。此外，提出了三维刚性运动轨道恢复的可证明计算流程，从刚性运动自相关中提取SO(3)矩并重建3D大分子结构。

Result: 建立了轨道恢复问题的样本复杂度界限：当噪声方差σ²→∞时，需要N≳σ^{2d+4}个样本。该结果进一步推导出多目标检测模型的匹配样本复杂度界限。在计算方面，成功重建了3D大分子结构，且算法在任何噪声水平下都有效。

Conclusion: 该工作为刚性运动群下的轨道恢复问题提供了理论框架和实用算法，表明即使是非常小的大分子，只要有足够数据，原则上都可以通过结构生物学电子显微镜技术重建，这对冷冻电镜等领域具有重要应用价值。

Abstract: We study the orbit recovery problem under the rigid-motion group SE(n), where the objective is to reconstruct an unknown signal from multiple noisy observations subjected to unknown rotations and translations. This problem is fundamental in signal processing, computer vision, and structural biology.
  Our main theoretical contribution is bounding the sample complexity of this problem. We show that if the d-th order moment under the rotation group SO(n) uniquely determines the signal orbit, then orbit recovery under SE(n) is achievable with $N\gtrsim σ^{2d+4}$ samples as the noise variance $σ^2 \to \infty$. The key technical insight is that the d-th order SO(n) moments can be explicitly recovered from (d+2)-order SE(n) autocorrelations, enabling us to transfer known results from the rotation-only setting to the rigid-motion case. We further harness this result to derive a matching bound to the sample complexity of the multi-target detection model that serves as an abstract framework for electron-microscopy-based technologies in structural biology, such as single-particle cryo-electron microscopy (cryo-EM) and cryo-electron tomography (cryo-ET).
  Beyond theory, we present a provable computational pipeline for rigid-motion orbit recovery in three dimensions. Starting from rigid-motion autocorrelations, we extract the SO(3) moments and demonstrate successful reconstruction of a 3-D macromolecular structure. Importantly, this algorithmic approach is valid at any noise level, suggesting that even very small macromolecules, long believed to be inaccessible using structural biology electron-microscopy-based technologies, may, in principle, be reconstructed given sufficient data.

</details>


### [64] [Neural Compress-and-Forward for the Primitive Diamond Relay Channel](https://arxiv.org/abs/2512.07662)
*Ozan Aygün,Ezgi Ozyilkan,Elza Erkip*

Main category: cs.IT

TL;DR: 提出了一种用于双中继钻石信道的神经压缩转发方案，通过分布式学习量化器实现无协调协作，性能接近理论界限。


<details>
  <summary>Details</summary>
Motivation: 钻石信道是协作通信的经典模型，传统压缩转发（CF）在无协调中继场景下有效，但将神经CF扩展到多中继系统面临分布式压缩的挑战。

Method: 采用端到端学习框架，每个中继使用独立的一步学习量化器压缩观测信号，目的地联合解码源消息，利用输入相关性实现分布式压缩。

Result: 仿真结果显示，该方案在有限阶调制下训练，性能接近已知理论界限，证明神经CF可扩展到多中继系统。

Conclusion: 神经压缩转发可扩展到多中继系统，通过分布式学习量化器实现有效协作压缩，同时保持性能和可解释性。

Abstract: The diamond relay channel, where a source communicates with a destination via two parallel relays, is one of the canonical models for cooperative communications. We focus on the primitive variant, where each relay observes a noisy version of the source signal and forwards a compressed description over an orthogonal, noiseless, finite-rate link to the destination. Compress-and-forward (CF) is particularly effective in this setting, especially under oblivious relaying where relays lack access to the source codebook. While neural CF methods have been studied in single-relay channels, extending them to the two-relay case is non-trivial, as it requires fully distributed compression without any inter-relay coordination. We demonstrate that learning-based quantizers at the relays can harness input correlations by operating remote, yet in a collaborative fashion, enabling effective distributed compression in line with Berger-Tung-style coding. Each relay separately compresses its observation using a one-shot learned quantizer, and the destination jointly decodes the source message. Simulation results show that the proposed scheme, trained end-to-end with finite-order modulation, operates close to the known theoretical bounds. These results demonstrate that neural CF can scale to multi-relay systems while maintaining both performance and interpretability.

</details>


### [65] [Enhancing Channel Estimation for OTFS systems using Sparse Bayesian Learning with Adaptive Threshold](https://arxiv.org/abs/2512.07704)
*Tengfei Qi,Yifei Yang,Xiong Deng,Zhinan Sun,Ziqiang Gao,Xihua Zou,Wei Pan,Lianshan Yan*

Main category: cs.IT

TL;DR: 论文提出了一种用于OTFS系统信道估计的自适应贝叶斯阈值主动去噪机制，结合无逆稀疏贝叶斯学习，解决了低信噪比下的伪峰问题，同时保持低复杂度。


<details>
  <summary>Details</summary>
Motivation: OTFS调制在高速多普勒频移环境中优于OFDM，但DD域信道估计面临低信噪比下的伪峰问题，现有方法在抗噪性能和复杂度方面有待改进。

Method: 将DD域信道估计建模为稀疏信号恢复问题，在现有稀疏贝叶斯学习框架下，提出自适应贝叶斯阈值主动去噪机制，结合无逆稀疏贝叶斯学习算法。

Result: 仿真结果表明，该算法在抗噪性能和复杂度方面均优于现有信道估计算法，特别是在低信噪比场景下有效解决了伪峰问题。

Conclusion: 提出的自适应贝叶斯阈值主动去噪机制与无逆稀疏贝叶斯学习相结合，为OTFS系统提供了一种高效、低复杂度的信道估计解决方案。

Abstract: Orthogonal time frequency space (OTFS) modulation is a two-dimensional modulation scheme designed in the delay-Doppler (DD) domain, exhibiting superior performance over orthogonal frequency division multiplexing (OFDM) modulation in environments with high Doppler frequency shifts. We investigated the channel estimation in the DD domain of OTFS systems, modeling it as a sparse signal recovery problem. Subsequently, within the existing sparse Bayesian learning framework, we proposed an adaptive Bayesian threshold-based active denoising mechanism. Combined with inverse-free sparse Bayesian learning, this effectively addresses the pseudo-peak issue in low signal-to-noise ratio (SNR) scenarios while maintaining low complexity. The simulation results demonstrate that this algorithm outperforms existing channel estimation algorithms in terms of anti-noise performance and complexity.

</details>
