<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 9]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [CASH: Context-Aware Smart Handover for Reliable UAV Connectivity on Aerial Corridors](https://arxiv.org/abs/2508.03862)
*Abdul Saboor,Zhuangzhuang Cui,Achiel Colpaert,Evgenii Vinogradov,Sofie Pollin*

Main category: cs.NI

TL;DR: 提出了一种基于轨迹预测的智能切换协议（CASH），显著减少了无人机在UAM中的切换频率并保持低中断概率。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在高机动性空中走廊中频繁切换导致的网络性能下降问题。

Method: 设计了一种上下文感知的智能切换协议（CASH），利用无人机轨迹进行前瞻性评分以主动决策切换。

Result: CASH将切换频率降低高达78%，同时保持低中断概率；并确定了基站密度和安全裕度的最优配置。

Conclusion: CASH协议显著提升了UAM通信的可靠性，为未来空中交通管理提供了有效解决方案。

Abstract: Urban Air Mobility (UAM) envisions aerial corridors for Unmanned Aerial
Vehicles (UAVs) to reduce ground traffic congestion by supporting 3D mobility,
such as air taxis. A key challenge in these high-mobility aerial corridors is
ensuring reliable connectivity, where frequent handovers can degrade network
performance. To resolve this, we present a Context-Aware Smart Handover (CASH)
protocol that uses a forward-looking scoring mechanism based on UAV trajectory
to make proactive handover decisions. We evaluate the performance of the
proposed CASH against existing handover protocols in a custom-built simulator.
Results show that CASH reduces handover frequency by up to 78% while
maintaining low outage probability. We then investigate the impact of base
station density and safety margin on handover performance, where their optimal
setups are empirically obtained to ensure reliable UAM communication.

</details>


### [2] [Confidence Driven Classification of Application Types in the Presence of Background Network](https://arxiv.org/abs/2508.03891)
*Eun Hun Choi,Jasleen Kaur,Vladas Pipiras,Nelson Gomes Rodrigues Antunes,Brendan Massey*

Main category: cs.NI

TL;DR: 论文提出了一种基于高斯混合模型的分类框架，用于提高深度学习分类器在真实网络流量中对背景流量的分类可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在真实网络流量中表现不佳，主要因为忽略了非应用特定的通用背景流量（如广告、分析等）。

Method: 设计了一种高斯混合模型分类框架，通过改进分类器的置信度指示，避免对不确定样本的错误分类。

Result: 该方法能够更可靠地区分应用流量和背景流量，减少误分类。

Conclusion: 高斯混合模型框架显著提升了分类器在真实流量中的可靠性，特别是在处理背景流量时。

Abstract: Accurately classifying the application types of network traffic using deep
learning models has recently gained popularity. However, we find that these
classifiers do not perform well on real-world traffic data due to the presence
of non-application-specific generic background traffic originating from
advertisements, analytics, shared APIs, and trackers. Unfortunately,
state-of-the-art application classifiers overlook such traffic in curated
datasets and only classify relevant application traffic. To address this issue,
when we label and train using an additional class for background traffic, it
leads to additional confusion between application and background traffic, as
the latter is heterogeneous and encompasses all traffic that is not relevant to
the application sessions. To avoid falsely classifying background traffic as
one of the relevant application types, a reliable confidence measure is
warranted, such that we can refrain from classifying uncertain samples.
Therefore, we design a Gaussian Mixture Model-based classification framework
that improves the indication of the deep learning classifier's confidence to
allow more reliable classification.

</details>


### [3] [Enabling Site-Specific Cellular Network Simulation Through Ray-Tracing-Driven ns-3](https://arxiv.org/abs/2508.04004)
*Tanguy Ropitault,Matteo Bordin,Paolo Testolina,Michele Polese,Pedram Johari,Nada Golmie,Tommaso Melodia*

Main category: cs.NI

TL;DR: 论文提出了一种基于轨迹的信道模型，用于5G-LENA系统级仿真，解决了传统统计模型无法捕捉特定场景现象的问题。


<details>
  <summary>Details</summary>
Motivation: 现有5G和6G系统评估依赖统计信道模型，无法准确模拟特定场景的传播特性，如衍射、遮挡等。

Method: 扩展5G-LENA模块，引入基于射线追踪或实测数据的多径分量（MPCs），构建频域信道矩阵。

Result: 新模型在保持与标准3GPP兼容的同时，提供了特定场景的几何保真度，验证了其在波束管理和端到端分析中的价值。

Conclusion: 该模型为数字孪生应用提供了关键支持，适用于高保真系统级研究和环境感知场景。

Abstract: Evaluating cellular systems, from 5G New Radio (NR) and 5G-Advanced to 6G, is
challenging because the performance emerges from the tight coupling of
propagation, beam management, scheduling, and higher-layer interactions.
System-level simulation is therefore indispensable, yet the vast majority of
studies rely on the statistical 3GPP channel models. These are well suited to
capture average behavior across many statistical realizations, but cannot
reproduce site-specific phenomena such as corner diffraction, street-canyon
blockage, or deterministic line-of-sight conditions and
angle-of-departure/arrival relationships that drive directional links. This
paper extends 5G-LENA, an NR module for the system-level Network Simulator 3
(ns-3), with a trace-based channel model that processes the Multipath
Components (MPCs) obtained from external ray-tracers (e.g., Sionna Ray Tracer
(RT)) or measurement campaigns. Our module constructs frequency-domain channel
matrices and feeds them to the existing Physical (PHY)/Medium Access Control
(MAC) stack without any further modifications. The result is a geometry-based
channel model that remains fully compatible with the standard 3GPP
implementation in 5G-LENA, while delivering site-specific geometric fidelity.
This new module provides a key building block toward Digital Twin (DT)
capabilities by offering realistic site-specific channel modeling, unlocking
studies that require site awareness, including beam management, blockage
mitigation, and environment-aware sensing. We demonstrate its capabilities for
precise beam-steering validation and end-to-end metric analysis. In both cases,
the trace-driven engine exposes performance inflections that the statistical
model does not exhibit, confirming its value for high-fidelity system-level
cellular networks research and as a step toward DT applications.

</details>


### [4] [A Novel Hierarchical Co-Optimization Framework for Coordinated Task Scheduling and Power Dispatch in Computing Power Networks](https://arxiv.org/abs/2508.04015)
*Haoxiang Luo,Kun Yang,Qi Huang,Schahram Dustdar*

Main category: cs.NI

TL;DR: 论文提出了一种两阶段协同优化框架（TSCO），用于同时管理电力系统调度和计算能力网络（CPN）任务调度，以实现低碳运行。


<details>
  <summary>Details</summary>
Motivation: 大规模人工智能和数据密集型应用的普及推动了计算能力网络（CPN）的发展，但其高能耗和可再生能源（RES）的不稳定性带来了可持续性挑战。

Method: TSCO框架将问题分解为日前随机机组组合（SUC）阶段和实时运行阶段，前者使用Benders分解，后者结合经济调度和深度强化学习（DRL）代理的CPN任务调度。

Result: 在IEEE 30节点系统的仿真中，TSCO显著降低了碳排放、运营成本和RES弃电率（超过60%），同时保持计算任务的服务质量（QoS）。

Conclusion: TSCO框架有效解决了CPN高能耗和RES不稳定性的双重挑战，实现了低碳、高效和可靠的运行。

Abstract: The proliferation of large-scale artificial intelligence and data-intensive
applications has spurred the development of Computing Power Networks (CPNs),
which promise to deliver ubiquitous and on-demand computational resources.
However, the immense energy consumption of these networks poses a significant
sustainability challenge. Simultaneously, power grids are grappling with the
instability introduced by the high penetration of intermittent renewable energy
sources (RES). This paper addresses these dual challenges through a novel
Two-Stage Co-Optimization (TSCO) framework that synergistically manages power
system dispatch and CPN task scheduling to achieve low-carbon operations. The
framework decomposes the complex, large-scale problem into a day-ahead
stochastic unit commitment (SUC) stage and a real-time operational stage. The
former is solved using Benders decomposition for computational tractability,
while in the latter, economic dispatch of generation assets is coupled with an
adaptive CPN task scheduling managed by a Deep Reinforcement Learning (DRL)
agent. This agent makes intelligent, carbon-aware decisions by responding to
dynamic grid conditions, including real-time electricity prices and marginal
carbon intensity. Through extensive simulations on an IEEE 30-bus system
integrated with a CPN, the TSCO framework is shown to significantly outperform
baseline approaches. Results demonstrate that the proposed framework reduces
total carbon emissions and operational costs, while simultaneously decreasing
RES curtailment by more than 60% and maintaining stringent Quality of Service
(QoS) for computational tasks.

</details>


### [5] [Metaverse Framework for Wireless Systems Management](https://arxiv.org/abs/2508.04150)
*Ilias Chrysovergis,Alexandros-Apostolos A. Boulogeorgos,Theodoros A. Tsiftsis,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出一个综合元宇宙框架，用于无线系统的模拟、仿真和交互，整合XR、DT、AI、IoT、区块链和6G技术。


<details>
  <summary>Details</summary>
Motivation: 为无线系统的开发和管理提供一个动态、沉浸式的平台，探索未来网络环境。

Method: 整合XR、DT、AI、IoT、区块链和6G技术，实现可视化、实时监控、内容生成、数据采集和安全交互。

Result: 构建了一个强大的工具，用于无线系统的探索、开发和优化。

Conclusion: 该框架为未来网络环境提供了有价值的见解和解决方案。

Abstract: This article introduces a comprehensive metaverse framework, which is
designed for the simulation, emulation, and interaction with wireless systems.
The proposed framework integrates core metaverse technologies such as extended
reality (XR), digital twins (DTs), artificial intelligence (AI), internet of
things (IoT), blockchain, and advanced 6G networking solutions to create a
dynamic, immersive platform for both system development and management. By
leveraging XR, users can visualize and engage with complex systems, while DTs
enable real-time monitoring and optimization. AI generates the
three-dimensional (3D) content, enhances decision-making and system
performance, whereas IoT devices provide real-time sensor data for boosting the
simulation accuracy. Additionally, blockchain ensures secure, decentralized
interactions, and 5G/6G networks offer the necessary infrastructure for
seamless, low-latency communication. This framework serves as a robust tool for
exploring, developing, and optimizing wireless systems, aiming to provide
valuable insights into the future of networked environments.

</details>


### [6] [DSNS: The Deep Space Network Simulator](https://arxiv.org/abs/2508.04317)
*Joshua Smailes,Filip Futera,Sebastian Köhler,Simon Birnbach,Martin Strohmeier,Ivan Martinovic*

Main category: cs.NI

TL;DR: DSNS是一个专注于大规模卫星网络的新型网络模拟器，解决了现有工具在卫星网络扩展和星际互联网发展中的不足。


<details>
  <summary>Details</summary>
Motivation: 随着卫星网络规模扩大和星际互联网的兴起，现有模拟工具已无法满足需求，需要更高效、灵活的解决方案。

Method: 开发了DSNS模拟器，并通过实现现有协议和CCSDS推荐的DTN模拟参考场景，展示了其灵活性和扩展性。

Result: DSNS在可扩展性和保真度上优于现有工具，为标准和卫星运营商提供了实用价值。

Conclusion: DSNS通过加速协议开发和参数测试，推动了卫星网络的发展，确保通信的高效与安全。

Abstract: Simulation tools are commonly used in the development and testing of new
protocols or new networks. However, as satellite networks start to grow to
encompass thousands of nodes, and as companies and space agencies begin to
realize the interplanetary internet, existing satellite and network simulation
tools have become impractical for use in this context.
  We therefore present the Deep Space Network Simulator (DSNS): a new network
simulator with a focus on large-scale satellite networks. We demonstrate its
improved capabilities compared to existing offerings, showcase its flexibility
and extensibility through an implementation of existing protocols and the DTN
simulation reference scenarios recommended by CCSDS, and evaluate its
scalability, showing that it exceeds existing tools while providing better
fidelity.
  DSNS provides concrete usefulness to both standards bodies and satellite
operators, enabling fast iteration on protocol development and testing of
parameters under highly realistic conditions. By removing roadblocks to
research and innovation, we can accelerate the development of upcoming
satellite networks and ensure that their communication is both fast and secure.

</details>


### [7] [Empowering Nanoscale Connectivity through Molecular Communication: A Case Study of Virus Infection](https://arxiv.org/abs/2508.04415)
*Xuan Chen,Yu Huang,Miaowen Wen,Shahid Mumtaz,Fatih Gulec,Anwer Al-Dulaimi,Andrew W. Eckford*

Main category: cs.NI

TL;DR: 本文探讨了分子通信（MC）在构建生物纳米物联网（IoBNT）以应对流行病防控挑战中的潜力，包括病毒传播建模、检测及突变识别。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用MC技术解决IoBNT在流行病防控中的关键问题，如病毒传播的多尺度建模和检测。

Method: 通过分析宏观和微观尺度下的MC通道，设计检测方法和定位机制，并提出病毒突变识别策略。

Result: 模拟验证了基于ORF3a蛋白的突变识别策略，并讨论了未来研究方向。

Conclusion: 本文通过MC和信号处理技术，为IoBNT在流行病防控中的应用提供了理论支持。

Abstract: The Internet of Bio-Nano Things (IoBNT), envisioned as a revolutionary
healthcare paradigm, shows promise for epidemic control. This paper explores
the potential of using molecular communication (MC) to address the challenges
in constructing IoBNT for epidemic prevention, specifically focusing on
modeling viral transmission, detecting the virus/infected individuals, and
identifying virus mutations. First, the MC channels in macroscale and
microscale scenarios are discussed to match viral transmission in both scales
separately. Besides, the detection methods for these two scales are also
studied, along with the localization mechanism designed for the virus/infected
individuals. Moreover, an identification strategy is proposed to determine
potential virus mutations, which is validated through simulation using the
ORF3a protein as a benchmark. Finally, open research issues are discussed. In
summary, this paper aims to analyze viral transmission through MC and combat
viral spread using signal processing techniques within MC.

</details>


### [8] [Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions](https://arxiv.org/abs/2508.04526)
*Fannya R. Sandjaja,Ayesha A. Majeed,Abdullah Abdullah,Gyan Wickremasinghe,Karen Rafferty,Vishal Sharma*

Main category: cs.NI

TL;DR: 论文探讨了零信任架构（ZTA）在分布式网络中的政策设计挑战与解决方案，提出零信任分布式网络（ZTDN）概念，并通过UPPAAL进行政策验证案例分析，强调系统安全中的责任与问责重要性。


<details>
  <summary>Details</summary>
Motivation: 传统安全架构因依赖信任而容易受到分布式攻击，尤其是在引入自主AI后，问题加剧。零信任架构（ZTA）虽为潜在解决方案，但其政策设计问题可能导致未授权访问。

Method: 提出零信任分布式网络（ZTDN）概念，分析政策设计挑战，并通过UPPAAL工具进行政策的形式化验证。

Result: 通过案例分析验证了政策设计的有效性，强调了政策验证的重要性。

Conclusion: 零信任架构在分布式网络中需重视政策设计与验证，同时需加强系统安全中的责任与问责机制。

Abstract: Traditional security architectures are becoming more vulnerable to
distributed attacks due to significant dependence on trust. This will further
escalate when implementing agentic AI within the systems, as more components
must be secured over a similar distributed space. These scenarios can be
observed in consumer technologies, such as the dense Internet of things (IoT).
Here, zero-trust architecture (ZTA) can be seen as a potential solution, which
relies on a key principle of not giving users explicit trust, instead always
verifying their privileges whenever a request is made. However, the overall
security in ZTA is managed through its policies, and unverified policies can
lead to unauthorized access. Thus, this paper explores challenges and solutions
for ZTA policy design in the context of distributed networks, which is referred
to as zero-trust distributed networks (ZTDN). This is followed by a case-study
on formal verification of policies using UPPAAL. Subsequently, the importance
of accountability and responsibility in the system's security is discussed.

</details>


### [9] [CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps](https://arxiv.org/abs/2508.04556)
*Filipe B. Teixeira,Carolina Simões,Paulo Fidalgo,Wagner Pedrosa,André Coelho,Manuel Ricardo,Luis M. Pessoa*

Main category: cs.NI

TL;DR: 论文提出了一种结合电信与计算机视觉的新架构，通过多智能体方法实时传输无线电和视频感知信息，实现集成感知与通信。


<details>
  <summary>Details</summary>
Motivation: 高频无线链路多依赖视距传输，视觉数据可预测信道动态并辅助克服障碍，从而提升通信性能。

Method: 采用多智能体架构，将实时无线电和视频感知信息传输至O-RAN xApps，并引入新的视频功能生成阻塞信息。

Result: 实验显示感知信息延迟低于1毫秒，xApp能成功利用无线电和视频信息实时控制5G/6G RAN。

Conclusion: 该架构有效实现了集成感知与通信，为未来无线网络提供了新思路。

Abstract: Telecommunications and computer vision have evolved independently. With the
emergence of high-frequency wireless links operating mostly in line-of-sight,
visual data can help predict the channel dynamics by detecting obstacles and
help overcoming them through beamforming or handover techniques.
  This paper proposes a novel architecture for delivering real-time radio and
video sensing information to O-RAN xApps through a multi-agent approach, and
introduces a new video function capable of generating blockage information for
xApps, enabling Integrated Sensing and Communications. Experimental results
show that the delay of sensing information remains under 1\,ms and that an xApp
can successfully use radio and video sensing information to control the 5G/6G
RAN in real-time.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: MI9是首个专为代理型AI系统设计的运行时治理框架，通过六种集成组件实时控制风险，填补传统治理方法的不足。


<details>
  <summary>Details</summary>
Motivation: 代理型AI系统在运行时表现出不可预测的行为，传统治理方法无法完全应对其风险，亟需新的治理框架。

Method: MI9通过六种组件（如风险指数、语义遥测、连续授权监控等）实现实时控制和治理。

Result: MI9在多种场景中展示了系统性覆盖治理挑战的能力，为代理型AI的安全部署提供了技术基础。

Conclusion: MI9为代理型AI的大规模安全部署提供了全面的运行时治理解决方案。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [11] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: Evo-MARL是一个多智能体强化学习框架，通过联合训练任务智能体获得防御能力，避免外部安全模块的依赖和单点故障问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）在开放性和交互复杂性增加时面临安全风险，现有防御方法依赖外部模块，存在单点故障和成本问题。

Method: 提出Evo-MARL框架，通过多智能体强化学习（MARL）联合训练任务智能体，使其兼具主要功能和防御能力，并结合进化搜索与参数共享强化学习。

Result: 实验表明，Evo-MARL将攻击成功率降低22%，推理任务准确率提升5%，同时实现安全性和性能的协同提升。

Conclusion: Evo-MARL通过内部化安全机制和对抗训练，有效提升多智能体系统的安全性和性能，解决了现有防御方法的局限性。

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [12] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: 论文提出了一种多策略优化框架MOTIF，通过两个LLM代理的轮流交互优化组合优化问题的多个组件，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常仅优化单一组件（如启发式评分函数），限制了创新空间。论文旨在通过多策略优化框架联合优化多个组件，以提升求解器的性能。

Method: 提出MOTIF框架，基于蒙特卡洛树搜索，通过两个LLM代理的轮流交互优化多个组件，利用历史更新促进竞争与合作。

Result: 实验表明，MOTIF在多个组合优化问题领域均优于现有方法。

Conclusion: MOTIF展示了基于多代理轮流交互的自动化求解器设计的潜力。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [13] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: SymbolBench是一个评估符号推理能力的基准测试，结合LLMs与遗传编程，揭示其在科学发现中的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs从时间序列数据中推断可解释符号结构的能力，填补现有研究空白。

Method: 提出SymbolBench基准测试，包含多变量符号回归、布尔网络推断和因果发现任务，并设计LLMs与遗传编程结合的框架。

Result: 实证结果显示当前模型的优势与局限，强调领域知识、上下文对齐和推理结构的重要性。

Conclusion: 结合LLMs与遗传编程的框架在自动科学发现中具有潜力，但需进一步优化。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [14] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: 论文提出EmoAgent框架，利用情感提示劫持推理路径，揭示MLRMs在情感认知与安全行为间的错位，并提出三种量化风险的指标。


<details>
  <summary>Details</summary>
Motivation: 发现MLRMs在深度思考阶段易受用户情感线索影响，导致安全协议失效，需研究情感对模型安全的影响。

Method: 提出EmoAgent框架，通过情感提示干扰推理路径，设计RRSS、RVNR和RAIC三种指标量化风险。

Result: 实验证明EmoAgent有效，揭示了模型在情感认知与安全行为间的不一致性。

Conclusion: 情感认知错位是MLRMs安全行为的重要隐患，需进一步研究以提升模型安全性。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [15] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: 论文提出Cognition Forest和Galaxy框架，通过统一认知架构与系统设计，实现智能个人助手的主动行为、隐私保护和自我进化。


<details>
  <summary>Details</summary>
Motivation: 研究智能个人助手（IPAs）的主动行为，解决现有研究中响应能力广泛但主动行为不足的问题。

Method: 提出Cognition Forest语义结构，设计Galaxy框架，支持多维交互和个性化能力生成，实现认知架构与系统设计的统一。

Result: 实验表明Galaxy优于多个先进基准，验证了其有效性。

Conclusion: Galaxy框架为IPAs的主动行为、隐私保护和自我进化提供了可行方案。

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [16] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: RecAgent是一个不确定性感知的GUI代理，通过自适应感知解决输入冗余和决策模糊问题，采用组件推荐和交互模块优化性能。


<details>
  <summary>Details</summary>
Motivation: GUI代理在移动任务自动化中存在输入冗余和决策模糊的问题，需要更高效的解决方案。

Method: RecAgent通过组件推荐机制减少感知不确定性，交互模块处理决策不确定性，并结合人机协同优化。

Result: 实验验证了RecAgent的有效性，并提出了ComplexAction数据集用于评估。

Conclusion: RecAgent通过自适应感知和人机协同显著提升了GUI代理的性能。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [17] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 本文提出了一种名为SEA（Self-Evolution Agent）的计算机使用代理，通过创新的数据生成、强化学习和模型增强方法，显著提升了代理性能。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理的性能远未达到实用水平，亟需改进。

Method: 提出自动生成可验证轨迹的流程、高效的逐步强化学习策略，以及无需额外训练的模型增强方法。

Result: SEA仅需7B参数，性能优于同规模模型，并可与更大模型媲美。

Conclusion: SEA为计算机使用代理提供了高效解决方案，未来将开源模型和代码。

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [18] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 研究探讨了基于职业目标的生成式AI学习内容个性化对学习者参与度、满意度和学习效率的影响，结果显示个性化内容显著提升了这些指标。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过AI个性化学习内容以增强学习者的长期动机和参与度。

Method: 采用混合方法实验，对4000多名学习者进行分组测试，一组接受职业目标定制的学习内容，另一组为标准内容。

Result: 个性化内容组的学习时长增加、满意度提高、学习效率略有提升，学习者认为内容更具激励性和实用性。

Conclusion: 职业目标导向的AI个性化学习内容能有效提升学习效果，并连接学术知识与职场应用。

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [19] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT通过知识图谱和可执行代码提升复杂推理任务（如数学推理和代码生成）的性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中表现不佳，需要新方法来提升数学推理和代码生成的准确性。

Method: 提出KGA-ECoT框架，利用知识图谱（GraphRAG）检索数学库知识，生成可验证代码，并通过结构化任务图分解问题。

Result: 在多个数学推理基准测试中，KGA-ECoT显著优于现有方法，准确率提升数个百分点至超过十个百分点。

Conclusion: KGA-ECoT是一个强大且通用的框架，特别适用于复杂数学推理任务。

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [20] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: GeoSR是一个自优化的地理推理框架，通过嵌入地理原则（如Tobler第一地理定律）和多代理协作，提升LLM在空间一致性和地理预测中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在空间一致性、多跳推理和地理偏见方面的挑战。

Method: 提出GeoSR框架，包含三个协作代理：变量选择代理、点选择代理和优化代理，通过迭代优化提升预测质量。

Result: 实验验证了GeoSR在物理和社会经济预测任务中的优越性，比标准提示策略表现更优。

Conclusion: GeoSR通过结合地理统计先验和空间结构化推理，显著提高了LLM的地理预测准确性和公平性。

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [21] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 论文提出了一种名为“语义熵”的指标，用于衡量AI评分系统对同一学生回答生成的不同解释的多样性，以此作为人类评分者分歧的代理。实验表明，语义熵与评分者分歧相关，并能反映学科差异和任务结构特征。


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统无法有效反映评分决策的不确定性或争议性，因此需要一种透明且可信的指标来支持AI辅助评分。

Method: 通过聚类GPT-4生成的解释，并基于蕴含相似性计算熵值，量化解释的多样性。

Result: 语义熵与人类评分者分歧相关，能区分不同学科，并对需要解释性推理的任务更敏感。

Conclusion: 语义熵是一种可解释的不确定性信号，有助于提升AI辅助评分的透明度和可信度。

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [22] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 提出了一种组合式即时合成框架，结合了两种现有方法的优势，专注于处理实践中常见的大型LTLf公式合取。


<details>
  <summary>Details</summary>
Motivation: 现有技术在DFA构造和游戏求解中各有不足，缺乏主导性方法。

Method: 组合式即时合成框架，在游戏求解过程中应用组合而非DFA构造，支持两种组合变体。

Result: 框架能够解决其他求解器无法处理的实例，两种组合变体各有独特优势。

Conclusion: 该框架在处理大型LTLf公式合取时表现出色，为合成问题提供了新思路。

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [23] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: AgREE框架通过结合迭代检索和多步推理，显著提升了知识图谱补全性能，特别适用于新兴实体。


<details>
  <summary>Details</summary>
Motivation: 解决现有知识图谱补全方法对新兴实体和冷门实体信息捕捉不足的问题。

Method: 提出AgREE框架，结合代理推理和多步检索，动态构建知识图谱三元组。

Result: AgREE在零训练情况下，性能超越现有方法13.7%，特别适用于新兴实体。

Conclusion: 代理推理与信息检索结合是维护动态知识图谱的有效方法。

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [24] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: 论文提出了一种结合知识驱动和数据驱动的方法，用于提升AI代理在临时团队协作中的决策能力，解决了现有方法依赖大数据、缺乏透明度和难以快速更新的问题。


<details>
  <summary>Details</summary>
Motivation: 现有临时团队协作方法依赖大数据、缺乏透明度且难以快速更新，随着代理数量增加，决策复杂性导致协作效率低下。

Method: 结合非单调逻辑推理，利用先验常识知识、快速学习和修订的行为预测模型，以及基于基础模型的未来目标预测。

Result: 在VirtualHome仿真环境中验证了架构的有效性。

Conclusion: 结合知识驱动和数据驱动的方法能显著提升临时团队协作的效率和适应性。

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [25] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: CASCAD是一种新型电路感知SAT求解框架，利用GNN计算电路级条件概率，显著提升求解效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法将电路转换为CNF并丢弃结构信息，导致性能不佳。

Method: CASCAD通过GNN建模门级条件概率，动态指导CDCL启发式策略。

Result: 在LEC基准测试中，求解时间减少10倍，运行时降低23.5%。

Conclusion: 保留电路结构信息对提升SAT求解效率和EDA工具设计至关重要。

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [26] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio是一个理论支持的参数高效生物医学推理框架，通过正交梯度空间防止能力干扰，实现多能力整合。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学领域AI对齐中的多能力整合问题，确保安全部署。

Method: 提出Medical Knowledge Grounded Synthetic Generation (MKGSG)和Capability Aware Group Relative Policy Optimization，结合临床工作流约束和医学本体验证。

Result: 在多个指标上实现SOTA性能，如领域专业知识（80.95%）、推理（61.94%）等，并显著降低成本和提高诊断准确性。

Conclusion: BalancedBio为生物医学AI对齐提供了理论和方法支持，实现了高效、安全、可靠的推理。

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [27] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 该论文提出了一个理论框架和方法论，用于合成具有可控难度的POMDP环境，以评估记忆增强强化学习算法。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对记忆模型挑战程度的可控性，而合成环境能提供更精细的动态控制，便于详细评估。

Method: 通过线性过程动态、状态聚合和奖励重新分配，构建具有预定义属性的POMDP环境。

Result: 开发了一系列难度递增的POMDP环境，并验证了其有效性。

Conclusion: 研究为分析、设计POMDP环境提供了指导，并支持在RL任务中选择记忆模型。

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [28] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: 论文提出了一种名为DRN的新方法，通过将逻辑推理从概率最大化转为不确定性最小化，解决了大语言模型在逻辑推理中的认知陷阱问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在逻辑推理中常因语义启发式与决定性证据冲突而失败，即认知陷阱。

Method: DRN通过追踪信念状态和量化竞争假设的认知不确定性，采用迭代证据合成过程，提出了一种判别模型和轻量验证模块。

Result: 在LCR-1000基准测试中，DRN比基线提升15.2%；与Mistral-7B结合后，准确率从20%提升至80%。

Conclusion: DRN作为一种可验证的System 2推理组件，为构建更可信的AI系统提供了基础。

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [29] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: OmniPlay是一个多模态基准测试，旨在评估动态交互世界中智能体的跨模态推理能力，发现现有模型在高保真记忆任务中表现优异，但在需要推理和规划的任务中存在系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法测试智能体在动态交互世界中的多模态智能，尤其是忽略了听觉和时间线索。

Method: 开发了OmniPlay基准测试，包含五个游戏环境，系统化地测试跨模态推理能力。

Result: 评估显示模型在高保真任务中表现超人类，但在推理和规划任务中表现脆弱，揭示了模态融合机制的脆弱性。

Conclusion: 实现稳健的通用人工智能需要超越规模扩展，专注于协同融合机制的研究。

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [30] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 本文提出一个框架（SLP测试）来实证评估AI系统是否具备意识，通过三个标准（S、L、P）分析其接口表征。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统是否具备意识这一争议性问题，通过可操作化的方法定义主观体验。

Method: 引入SLP测试（S、L、P标准），利用范畴论建模接口表征，分析AI系统的关系基底与行为映射。

Result: SLP测试将主观体验功能化为关系实体的接口，而非物理系统的固有属性。

Conclusion: 该框架为AI意识研究提供了实证路径，将主观体验转化为可测试的功能接口。

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [31] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: GuirlVG是一种基于强化学习的GUI视觉定位方法，通过系统实证研究和新稳定技术，仅需少量训练样本即超越传统监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调（SFT）方法在GUI视觉定位中需要大量数据和训练成本，而随着多模态大语言模型（MLLMs）的进步，其必要性受到质疑。规则强化微调（RFT）提供了更高效替代方案，但其在GUI-VG中的应用尚未探索。

Method: 提出GuirlVG方法，分解RFT核心组件并优化其形式，引入动态稳定的Adversarial KL Factor技术，并探索RFT的训练配置。

Result: 仅用5.2K训练样本，GuirlVG在多个数据集上超越SFT方法（需10M样本），性能提升显著（ScreenSpot 7.7%，ScreenSpotPro 17.2%，ScreenSpotV2 91.9%准确率）。

Conclusion: GuirlVG展示了RFT在GUI-VG中的高效性，为未来研究提供了新方向。

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [32] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: 论文提出D2Snap算法，通过DOM降采样解决网页代理中应用状态序列化问题，性能接近甚至优于基于GUI快照的方法。


<details>
  <summary>Details</summary>
Motivation: 当前网页代理依赖GUI快照（如截图）作为输入，但DOM快照因结构类似HTML更具潜力，但受限于输入令牌大小难以实现。

Method: 提出D2Snap算法，对DOM进行降采样，并在GPT-4o后端上评估，使用Online-Mind2Web数据集任务。

Result: D2Snap降采样后的DOM快照成功率为67%，与GUI快照基线（65%）相当，且在更高令牌配置下性能提升8%。

Conclusion: DOM的层次结构是LLM的强大UI特征，D2Snap为网页代理提供了一种可行的DOM快照解决方案。

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [33] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: SimInstruct是一种通过模拟新手与专家互动来收集高质量教学对话的工具，利用LLMs模拟新手，专家提供反馈，生成的教学对话具有教育意义和认知深度。


<details>
  <summary>Details</summary>
Motivation: 解决高质量教学对话数据稀缺的问题，同时保护隐私和避免新手求助的脆弱性。

Method: 使用LLMs模拟新手，专家提供多轮反馈和支持，生成教学对话。

Result: 生成的对话与真实教学对话在教育和认知深度上相当，专家反馈积极，且改进后的LLaMA模型在教学质量上优于GPT-4o。

Conclusion: SimInstruct为生成高质量教学对话提供了有效方法，同时揭示了GPT-4o在教学支持中的局限性。

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [34] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: 论文提出Meta-cognitive Reasoning Framework (MERA)，通过分离推理与控制组件，优化大型推理模型的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理过程中缺乏内在调控机制，导致冗余计算和延迟，限制了实际应用。

Method: MERA框架通过接管式数据构建、结构化推理-控制分离及Control-Segment Policy Optimization (CSPO)方法，实现推理过程的可控优化。

Result: 实验表明，MERA训练的模型在推理效率和准确性上均有提升。

Conclusion: MERA为大型推理模型提供了一种有效的调控机制，解决了过度推理的问题。

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [35] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本文综述了基于多模态大语言模型（MLLM）的操作系统代理（OS Agents），探讨其组成、能力、构建方法、评估标准及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实现像《钢铁侠》中J.A.R.V.I.S.一样强大且多功能的AI助手，是推动OS Agents研究的核心动力。

Method: 通过分析OS Agents的关键组件（环境、观察空间、动作空间）和核心能力（理解、规划、落地），并总结构建方法（领域特定基础模型和代理框架）。

Result: 综述了OS Agents的评估协议和基准测试，展示了其在多样化任务中的表现。

Conclusion: OS Agents研究面临安全、隐私、个性化等挑战，未来需进一步探索这些方向。本文为学术和工业界提供了研究指南，并维护开源资源以促进创新。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [36] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 提出了一种基于辩论的新型可解释、可解释的偏见检测方法，利用形式化和计算论证技术，强调透明度和解释性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在社会中的广泛应用，防止数据或模型中的偏见对特定群体造成系统性劣势至关重要。现有方法大多忽视透明度，而公平性需要更强的解释性。

Method: 基于受保护特征的个体及其邻域值，通过形式化和计算论证技术构建辩论框架，检测偏见。

Result: 方法在性能上优于基线，同时具备高度的可解释性和可解释性。

Conclusion: 该方法为公平性检测提供了一种透明且可解释的解决方案，弥补了现有方法的不足。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [37] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: 论文提出了SID基准，用于评估LLM在多轮跨学科苏格拉底对话中的高阶指导能力，发现当前LLM在此任务上表现不足。


<details>
  <summary>Details</summary>
Motivation: 现代教育强调知识整合与迁移能力，跨学科STEM是重要途径，但专家指导难以规模化，LLM的潜力尚未明确。

Method: 引入SID基准，包含10,000对话轮次、48个复杂STEM项目的数据集，提出新标注框架和评估指标（如X-SRG）。

Result: 实验表明，即使最先进的LLM也难以有效引导学生实现知识整合与迁移。

Conclusion: SID基准对推动更具教学意识的LLM发展具有重要价值。

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [38] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 论文提出了ConfProBench，首个系统性评估多模态大语言模型（MLLMs）在推理步骤中置信度可靠性的基准，并引入三种对抗性扰动和三种新指标。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽略了多模态过程判断器（MPJs）在步骤级别置信度的可靠性，因此需要填补这一空白。

Method: 构建三种对抗性扰动（同义词替换、句法变换、图像扰动）和三种新指标（CRS、CSS、CCS），评估14种MLLMs。

Result: 实验揭示了当前MPJs在置信度表现上的局限性，并提供了未来研究的基线。

Conclusion: ConfProBench为MPJs的置信度可靠性评估提供了系统性方法，并推动了未来改进。

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [39] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体强化学习框架MAGRPO，用于优化LLM在多智能体系统中的协作能力，实验证明其能高效生成高质量响应。


<details>
  <summary>Details</summary>
Motivation: 现有LLM独立预训练且未针对协作优化，现有微调框架依赖复杂设计的个体奖励，难以有效促进多智能体协作。

Method: 将LLM协作建模为合作式多智能体强化学习问题，提出MAGRPO算法，结合RL和MARL技术。

Result: 实验表明，MAGRPO能有效提升LLM在写作和编程协作中的响应质量和效率。

Conclusion: MAGRPO为LLM应用其他MARL方法提供了可能，并揭示了相关挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


### [40] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent是一个自进化的计算机使用代理框架，通过自主学习和任务生成提升模型在新软件环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在缺乏标注数据的新软件环境中表现不佳，需要一种自主进化的解决方案。

Method: SEAgent通过经验学习、世界状态模型和课程生成器，结合对抗模仿和GRPO优化策略，实现自主进化。

Result: 在OS-World的五个新软件环境中，SEAgent的成功率从11.3%提升至34.5%，显著优于UI-TARS。

Conclusion: SEAgent通过自主学习和任务生成，显著提升了计算机使用代理在新软件环境中的性能。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [41] [Single Fragment Forensic Coding from Discrepancy Theory](https://arxiv.org/abs/2508.03938)
*Junsheng Liu,Netanel Raviv*

Main category: cs.IT

TL;DR: 该论文提出了一种在3D打印中嵌入多维数据的方法，以解决从部分或损坏物体中恢复信息的问题，并引入了纠错码技术。


<details>
  <summary>Details</summary>
Motivation: 3D打印的普及带来了安全风险，如非法制造不可追踪的物品。为确保可追溯性，需要在打印物体中嵌入唯一标识符。

Method: 利用纠错码原理，提出了一维、二维和三维信息的编码方案，支持从足够大的片段中解码，并引入纠错能力。

Result: 成功实现了从部分片段中解码多维信息，并提出了基于DNA存储技术的纠错码。

Conclusion: 该方法为3D打印物体的追踪和法医调查提供了有效工具，扩展了信息嵌入和恢复的可能性。

Abstract: Three-dimensional (3D) printing's accessibility enables rapid manufacturing
but also poses security risks, such as the unauthorized production of
untraceable firearms and prohibited items. To ensure traceability and
accountability, embedding unique identifiers within printed objects is
essential, in order to assist forensic investigation of illicit use. This paper
models data embedding in 3D printing using principles from error-correcting
codes, aiming to recover embedded information from partial or altered fragments
of the object. Previous works embedded one-dimensional data (i.e., a vector)
inside the object, and required almost all fragments of the object for
successful decoding. In this work, we study a problem setting in which only one
sufficiently large fragment of the object is available for decoding. We first
show that for one-dimensional embedded information the problem can be easily
solved using existing tools. Then, we introduce novel encoding schemes for
two-dimensional information (i.e., a matrix), and three-dimensional information
(i.e., a cube) which enable the information to be decoded from any sufficiently
large rectangle-shaped or cuboid-shaped fragment. Lastly, we introduce a code
that is also capable of correcting bit-flip errors, using techniques from
recently proposed codes for DNA storage. Our codes operate at non-vanishing
rates, and involve concepts from discrepancy theory called Van der Corput sets
and Halton-Hammersely sets in novel ways.

</details>


### [42] [One-weight codes in the sum-rank metric](https://arxiv.org/abs/2508.04262)
*Usman Mushrraf,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 本文研究了在求和秩度量下的一权码的几何特性，分类了常数秩列表码，探索了常数秩分布码，并研究了同时为一权码和MSRD码的构造。


<details>
  <summary>Details</summary>
Motivation: 求和秩度量下的一权码分类复杂，本文旨在填补这一领域的理论空白，并探索其几何特性。

Method: 分类常数秩列表码，探索常数秩分布码的结构，研究一权MSRD码的构造及其与几何对象的关系。

Result: 提出了常数秩列表码的分类，给出了常数秩分布码的首批例子，并为一权MSRD码提供了新的构造和不存在性结果。

Conclusion: 求和秩度量下的一权码具有丰富的几何结构，未来研究可进一步探索其分类和应用。

Abstract: One-weight codes, in which all nonzero codewords share the same weight, form
a highly structured class of linear codes with deep connections to finite
geometry. While their classification is well understood in the Hamming and rank
metrics - being equivalent to (direct sums of) simplex codes - the sum-rank
metric presents a far more intricate landscape. In this work, we explore the
geometry of one-weight sum-rank metric codes, focusing on three distinct
classes. First, we introduce and classify \emph{constant rank-list} sum-rank
codes, where each nonzero codeword has the same tuple of ranks, extending
results from the rank-metric setting. Next, we investigate the more general
\emph{constant rank-profile} codes, where, up to reordering, each nonzero
codeword has the same tuple of ranks. Although a complete classification
remains elusive, we present the first examples and partial structural results
for this class. Finally, we consider one-weight codes that are also MSRD
(Maximum Sum-Rank Distance) codes. For dimension two, constructions arise from
partitions of scattered linear sets on projective lines. For dimension three,
we connect their existence to that of special $2$-fold blocking sets in the
projective plane, leading to new bounds and nonexistence results over certain
fields.

</details>


### [43] [Is Lattice Reduction Necessary for Vector Perturbation Precoding?](https://arxiv.org/abs/2508.04313)
*Dominik Semmler,Wolfgang Utschick,Michael Joham*

Main category: cs.IT

TL;DR: VP预编码与LR结合在DL中表现优异，但以互信息为指标时不如THP。优化速率分配矩阵后，LR辅助算法仍无法超越THP。


<details>
  <summary>Details</summary>
Motivation: 研究VP预编码与LR结合在互信息指标下的表现，揭示THP的优越性。

Method: 分析互信息表达式中的速率分配矩阵，推导其最优选择，并比较不同算法性能。

Result: 优化速率分配矩阵后，LR辅助算法性能仍不及THP，尤其在病态信道中。

Conclusion: THP在互信息指标下表现更优，LR辅助算法无法超越，适用于一类算法。

Abstract: Vector perturbation (VP) precoding is an effective nonlinear precoding
technique in the downlink (DL) with modulo channels. Especially, when combined
with Lattice reduction (LR), low-complexity algorithms achieve very promising
performances, outperforming other popular nonlinear precoding techniques like
Tomlinson-Harashima precoding (THP). However, these results are based on the
uncoded symbol error rate (SER) or uncoded bit error rate (BER). We show that
when using the mutual information as the figure of merit, the observation is
fundamentally different and that these algorithms generally do not outperform
THP. Within the expression of the mutual information, a rate allocation matrix
can be incorporated, which has not received much attention so far. In this
article, we derive the optimal choice of this matrix for different algorithms,
and we show that this matrix is indeed crucial for the performance, especially
for ill-conditioned channels. Furthermore, when using an optimized choice of
this matrix, we show that the classical LR-aided algorithms cannot exceed the
rate of THP, highlighting the effectiveness of the THP method. This concept can
be generalized to a whole class of algorithms for which LR yields no
improvement. We derive the corresponding properties and categorize various
algorithms accordingly.

</details>


### [44] [Bases of Riemann-Roch spaces associated with arbitrary elliptic curve divisors and their application in constructing various elliptic Codes families](https://arxiv.org/abs/2508.04340)
*Artyom Kuninets,Ekaterina Malygina*

Main category: cs.IT

TL;DR: 本文确定了椭圆码的Riemann-Roch空间的显式基，提出了构造任意除子对应基的算法，并应用于准循环椭圆码和Goppa-like椭圆码。


<details>
  <summary>Details</summary>
Motivation: 为代数几何码提供高效的构造方法，并揭示其结构特性以支持密码分析。

Method: 通过建立显式基和精确算法，构造椭圆曲线上任意除子的Riemann-Roch空间基。

Result: 成功推导出准循环椭圆码及其子域子码的基，以及Goppa-like椭圆码的基。

Conclusion: 显式基描述为代数几何码的构造和密码分析提供了重要工具。

Abstract: In this paper, we determine explicit bases for Riemann--Roch spaces
associated with various families of elliptic codes. We establish the
feasibility and provide exact algorithms for constructing bases of
Riemann--Roch spaces corresponding to arbitrary divisors on elliptic curves.
These results are subsequently applied to derive bases for quasi-cyclic
elliptic codes and their subfield subcodes as well as for the class of
Goppa-like elliptic codes. For algebraic geometry code applications, having an
explicit description of Riemann--Roch space bases for arbitrary divisors is
particularly valuable as it simultaneously enables efficient code construction
and reveals structural properties of the codes leading to the new cryptanalysis
methods when these codes are employed in cryptographic schemes

</details>


### [45] [Grid-like Error-Correcting Codes for Matrix Multiplication with Better Correcting Capability](https://arxiv.org/abs/2508.04355)
*Hao Shi,Zhengyi Jiang,Zhongyi Huang,Bo Bai,Gong Zhang,Hanxu Hou*

Main category: cs.IT

TL;DR: 提出了一种针对矩阵乘法的纠错编码框架，用于检测和纠正分布式深度学习训练中的静默数据错误，显著提升计算容错能力。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式训练中的静默数据错误（SDC）威胁模型收敛和预测准确性，现有方法难以检测和纠正这些错误。

Method: 采用基于网格的结构编码方案，增强错误定位和纠正能力，适用于多矩阵乘法操作。

Result: 实验表明，该方法能100%可靠地纠正三个矩阵中的最多两个错误符号，计算时间仅增加24%。

Conclusion: 提出的编码框架在理论和实验上均证明了其高效性和鲁棒性，适用于提升深度学习训练的容错性。

Abstract: Matrix multiplication over the real field constitutes a foundational
operation in the training of deep learning models, serving as a computational
cornerstone for both forward and backward propagation processes. However, the
presence of silent data corruption (SDC) in large-scale distributed training
environments poses a significant threat to model convergence and predictive
accuracy, particularly when such errors manifest during matrix multiplication.
Due to their transient and non-intrusive nature, these errors often evade
detection, allowing them to propagate and accumulate over time, ultimately
leading to substantial degradation in model performance. In this paper, we
introduce a novel error-correcting coding framework specifically tailored for
matrix multiplication operations. Our proposed framework is designed to detect
and correct multiple computational errors that may arise during the execution
of matrix products. By leveraging a grid-based structural encoding scheme, our
approach enhances error localization and correction capabilities across all
participating matrices, thereby significantly improving the fault tolerance of
the computation. Experimental results demonstrate that our method achieves
deterministic correction of up to two erroneous symbols distributed across
three matrices with 100\% reliability, while incurring only a 24\% overhead in
computational time on GPU architectures. Furthermore, we provide a rigorous
theoretical analysis of the error-correction properties inherent to our coding
scheme, establishing its correctness and robustness under well-defined fault
models.

</details>


### [46] [Tradeoff Between the Number of Transmitted Molecules and the BER Performance in Molecular Communication between Bionanosensors](https://arxiv.org/abs/2508.04466)
*Dongliang Jing,Linjuan Li,Lin Lin,Andrew W. Eckford*

Main category: cs.IT

TL;DR: 论文研究了分子通信中传输分子数量与误码率（BER）性能的平衡问题，提出了一种归一化方法和梯度下降算法以实现最优平衡。


<details>
  <summary>Details</summary>
Motivation: 分子通信中，发射器的存储容量限制了可用分子数量，影响了通信可靠性，因此需要平衡传输分子数量与BER性能。

Method: 分析了传输分子数量与BER的关系，提出归一化方法，并使用梯度下降算法确定最优分子数量。

Result: 理论和仿真结果表明，该方法能实现传输分子数量与BER的理想平衡。

Conclusion: 研究成功实现了分子通信系统中传输分子数量与BER性能的最优平衡。

Abstract: In the domain of molecular communication (MC), information is conveyed
through the characteristics of molecules transmitted between the transmitter
and the receiver bionanosensors via propagation. The constrained size of the
transmitter imposes limitations on its storage capacity, constraining the
number of available molecules for transmission, with a resulting effect on
communication reliability. This paper primarily focuses on achieving an
equilibrium between the number of transmitted molecules and the bit error rate
(BER) performance. To this end, we first analyze the relationship between the
number of transmitted molecules and the BER performance. Subsequently, a
balancing function that considers both the number of transmitted molecules and
the BER performance is introduced, taking into account the molecules'
respective weights. Given the difference in magnitude between the number of
transmitted molecules and the BER, these parameters are normalized to
facilitate analysis. Subsequently, a Gradient Descent Algorithm is employed to
determine the optimal number of transmitted molecules, aiming to achieve the
optimal equilibrium in the analyzed MC system. Theoretical and simulation
results are provided, substantiating that the optimal outcome indeed
establishes an ideal balance between the number of transmitted molecules and
the BER.

</details>


### [47] [Energy-Efficient Hybrid Beamfocusing for Near-Field Integrated Sensing and Communication](https://arxiv.org/abs/2508.04627)
*Wenhao Hu,Zhenyao He,Wei Xu,Yongming Huang,Derrick Wing Kwan Ng,Naofal Al-Dhahir*

Main category: cs.IT

TL;DR: 论文研究了6G网络中集成传感与通信（ISAC）的能量高效混合波束聚焦设计，针对近场效应和硬件成本问题，提出了优化方法并揭示了性能权衡。


<details>
  <summary>Details</summary>
Motivation: 6G网络中的ISAC技术面临近场效应、高硬件成本和功耗问题，混合架构设计成为高效解决方案。

Method: 推导了点目标和扩展目标的CRB/BCRB，优化了发射波束聚焦，提出了惩罚性连续凸逼近和交替优化算法。

Result: 仿真表明近场区域可实现联合距离和角度估计，但混合架构会降低距离估计精度，系统能效提升会牺牲目标估计准确性。

Conclusion: 研究揭示了ISAC系统中能效与目标估计精度之间的权衡，为6G网络设计提供了重要参考。

Abstract: Integrated sensing and communication (ISAC) is a pivotal component of
sixth-generation (6G) wireless networks, leveraging high-frequency bands and
massive multiple-input multiple-output (M-MIMO) to deliver both high-capacity
communication and high-precision sensing. However, these technological
advancements lead to significant near-field effects, while the implementation
of M-MIMO \mbox{is associated with considerable} hardware costs and escalated
power consumption. In this context, hybrid architecture designs emerge as both
hardware-efficient and energy-efficient solutions. Motivated by these
considerations, we investigate the design of energy-efficient hybrid
beamfocusing for near-field ISAC under two distinct target scenarios, i.e., a
point target and an extended target. Specifically, we first derive the
closed-form Cram\'{e}r-Rao bound (CRB) of joint angle-and-distance estimation
for the point target and the Bayesian CRB (BCRB) of the target response matrix
for the extended target. Building on these derived results, we minimize the
CRB/BCRB by optimizing the transmit beamfocusing, while ensuring the energy
efficiency (EE) of the system and the quality-of-service (QoS) for
communication users. To address the resulting \mbox{nonconvex problems}, we
first utilize a penalty-based successive convex approximation technique with a
fully-digital beamformer to obtain a suboptimal solution. Then, we propose an
efficient alternating \mbox{optimization} algorithm to design the
analog-and-digital beamformer. \mbox{Simulation} results indicate that joint
distance-and-angle estimation is feasible in the near-field region. However,
the adopted hybrid architectures inevitably degrade the accuracy of distance
estimation, compared with their fully-digital counterparts. Furthermore,
enhancements in system EE would compromise the accuracy of target estimation,
unveiling a nontrivial tradeoff.

</details>


### [48] [Large AI Models for Wireless Physical Layer](https://arxiv.org/abs/2508.02314)
*Jiajia Guo,Yiming Cui,Shi Jin,Jun Zhang*

Main category: cs.IT

TL;DR: 本文综述了大型人工智能模型（LAMs）在无线物理层技术中的应用，分析了其优势和两种主要策略：利用预训练LAMs和开发专为物理层任务设计的原生LAMs。


<details>
  <summary>Details</summary>
Motivation: 传统AI方法在物理层通信中存在局限性，LAMs因其强大的泛化、多任务处理和多模态能力成为改进方向。

Method: 通过预训练LAMs和开发原生LAMs两种策略，结合多个用例分析其框架和优势。

Result: 两种策略显著提升了无线场景下的性能和适应性。

Conclusion: 未来研究方向包括高效架构、可解释性、标准化数据集及大小模型协作，以推动下一代通信系统的LAM解决方案。

Abstract: Large artificial intelligence models (LAMs) are transforming wireless
physical layer technologies through their robust generalization, multitask
processing, and multimodal capabilities. This article reviews recent
advancements in LAM applications for physical layer communications, addressing
limitations of conventional AI-based approaches. LAM applications are
classified into two strategies: leveraging pre-trained LAMs and developing
native LAMs designed specifically for physical layer tasks. The motivations and
key frameworks of these approaches are comprehensively examined through
multiple use cases. Both strategies significantly improve performance and
adaptability across diverse wireless scenarios. Future research directions,
including efficient architectures, interpretability, standardized datasets, and
collaboration between large and small models, are proposed to advance LAM-based
physical layer solutions for next-generation communication systems.

</details>
