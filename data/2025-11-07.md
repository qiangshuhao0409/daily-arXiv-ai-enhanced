<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.AI](#cs.AI) [Total: 32]
- [cs.IT](#cs.IT) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Hybrid Quantum-Classical Detection for RIS-Assisted SC-FDE via Grover Adaptive Search](https://arxiv.org/abs/2511.04173)
*Maryam Tariq,Omar Alhussein,Raneem Abdelraheem,Abdullah Quran,Georges Kaddoum,Sami Muhaidat*

Main category: cs.NI

TL;DR: 提出了一种用于RIS辅助SC-FDE系统的混合量子-经典检测框架，将ML检测转化为QUBO问题并通过GAS求解，在理想条件下实现接近最优性能，在噪声条件下仍保持稳健性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络对宽带和低延迟的要求需要接近ML性能但避免指数复杂度的检测器，量子计算为此提供了新的解决方案。

Method: 将ML检测目标重新表述为QUBO问题，使用GAS求解，并引入频域MMSE阈值进行低复杂度初始化以加速收敛。

Result: 在理想条件下检测器实现接近最优性能，搜索成本从O(M^N)降低到O(SQRT(M^N))；在噪声条件下，GAS电路的浅深度使去极化误差可忽略，读出误差仅引入适度性能下降。

Conclusion: 该研究确立了量子增强检测在RIS辅助宽带通信中的可行性，展示了算法可扩展性和对6G网络的实用鲁棒性。

Abstract: Wideband and low-latency requirements in sixth-generation (6G) networks
demand detectors that approach maximum-likelihood (ML) performance without
incurring exponential complexity. This work develops a hybrid quantum-classical
detection framework for reconfigurable intelligent surface (RIS)-assisted
single-carrier (SC) frequency-domain equalization (FDE) over
frequency-selective channels. The ML detection objective is reformulated as a
quadratic unconstrained binary optimization (QUBO) problem and solved via
Grover adaptive search (GAS). To accelerate convergence, we introduce a
frequency-domain MMSE threshold that exploits the circulant structure of SC-FDE
channels, yielding low-complexity initialization. The framework is evaluated
across varying channel lengths and RIS sizes, confirming robustness and
scalability. In addition, GAS requirements are quantified through register
widths and gate counts, and its query complexity is analyzed to characterize
the algorithm's cost for block transmission in frequency-selective channels.
Quantum circuit simulations are conducted in Qiskit under both ideal and noisy
conditions. In the ideal case, the detector achieves near-optimal performance
while benefiting from Grover's quadratic speedup, reducing the search cost from
from O(M^N) exhaustive evaluations to O(SQRT(M^N)) oracle queries. Under noise,
the shallow depth of the GAS circuits, aided by MMSE initialization, makes
depolarizing errors negligible, while readout errors introduce moderate
degradation yet still preserve performance close to the MMSE baseline. These
results establish the feasibility of quantum-enhanced detection for
RIS-assisted broadband communications, highlighting both algorithmic
scalability and practical robustness for 6G networks.

</details>


### [2] [Improving dynamic congestion isolation in data-center networks](https://arxiv.org/abs/2511.04639)
*Alberto Merino,Jesus Escudero-Sahuquillo,Pedro Javier Garcia,Francisco J. Quiles*

Main category: cs.NI

TL;DR: 本文提出了一种改进的拥塞隔离机制ICI，通过协调拥塞隔离和DCQCN，减少误报拥塞检测，提高网络性能。


<details>
  <summary>Details</summary>
Motivation: 分布式AI和大规模应用导致数据中心网络出现严重拥塞，现有机制DCQCN和CI各有缺陷，结合使用时会产生误报拥塞、过度节流和网络资源利用低效等问题。

Method: 提出ICI机制，利用隔离拥塞流的信息来指导DCQCN的ECN标记，避免受害流被错误标记，从而减少误报拥塞检测。

Result: 在各种流量模式下的评估显示，ICI将生成的BECN数量减少高达32倍，尾部延迟改善高达31%，同时保持高吞吐量和可扩展性。

Conclusion: ICI机制通过协调拥塞隔离和DCQCN，有效解决了现有机制结合使用时的问题，显著提升了网络性能。

Abstract: The rise of distributed AI and large-scale applications has impacted the
communication operations of data-center and Supercomputer interconnection
networks, leading to dramatic incast or in-network congestion scenarios and
challenging existing congestion control mechanisms, such as injection
throttling (e.g., DCQCN) or congestion isolation (CI). While DCQCN provides a
scalable traffic rate adjustment for congesting flows at end nodes (which is
slow) and CI effectively isolates these flows in special network resources
(which requires extra logic in the switches), their combined use, although it
diminishes their particular drawbacks, leads to false congestion scenarios
identification and signaling, excessive throttling, and inefficient network
resource utilization. In this paper, we propose a new CI mechanism, called
Improved Congestion Isolation (ICI), which efficiently combines CI and DCQCN so
that the information of the isolated congesting flows is used to guide the ECN
marking performed by DCQCN in a way that victim flows do not end up being
marked. This coordination reduces false-positive congestion detection,
suppresses unnecessary closed-loop feedback (i.e., wrong congestion
notifications), and improves responsiveness to communication microbursts.
Evaluated under diverse traffic patterns, including incast and Data-center
workloads, ICI reduces the number of generated BECNs by up to 32x and improves
tail latency by up to 31%, while maintaining high throughput and scalability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)
*Zhaorun Chen,Zhuokai Zhao,Kai Zhang,Bo Liu,Qi Qi,Yifan Wu,Tarun Kalluri,Sara Cao,Yuanhao Xiong,Haibo Tong,Huaxiu Yao,Hengduo Li,Jiacheng Zhu,Xian Li,Dawn Song,Bo Li,Jason Weston,Dat Huynh*

Main category: cs.AI

TL;DR: DreamGym是一个统一的框架，通过基于推理的经验模型合成多样化经验，支持大规模在线强化学习训练，避免了昂贵的真实环境交互成本。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大语言模型代理中的实际应用面临昂贵的环境交互、有限的任务多样性、不可靠的奖励信号和基础设施复杂性等挑战，阻碍了可扩展经验数据的收集。

Method: DreamGym将环境动态提炼为基于推理的经验模型，通过逐步推理获得一致的状态转移和反馈信号；使用离线真实数据初始化经验回放缓冲区并持续丰富；自适应生成挑战当前策略的新任务以支持在线课程学习。

Result: 在多样化环境和代理骨干上的实验表明，DreamGym显著提升了强化学习训练效果。在WebArena等非RL就绪任务上，性能超过所有基线30%以上；在RL就绪但成本高昂的设置中，仅使用合成交互就能匹配GRPO和PPO性能；在合成经验训练的纯策略迁移到真实环境时，DreamGym带来显著的额外性能提升且需要更少的真实世界交互。

Conclusion: DreamGym为通用强化学习提供了一个可扩展的热启动策略，通过合成经验有效解决了RL训练中的数据收集瓶颈问题。

Abstract: While reinforcement learning (RL) can empower large language model (LLM)
agents by enabling self-improvement through interaction, its practical adoption
remains challenging due to costly rollouts, limited task diversity, unreliable
reward signals, and infrastructure complexity, all of which obstruct the
collection of scalable experience data. To address these challenges, we
introduce DreamGym, the first unified framework designed to synthesize diverse
experiences with scalability in mind to enable effective online RL training for
autonomous agents. Rather than relying on expensive real-environment rollouts,
DreamGym distills environment dynamics into a reasoning-based experience model
that derives consistent state transitions and feedback signals through
step-by-step reasoning, enabling scalable agent rollout collection for RL. To
improve the stability and quality of transitions, DreamGym leverages an
experience replay buffer initialized with offline real-world data and
continuously enriched with fresh interactions to actively support agent
training. To improve knowledge acquisition, DreamGym adaptively generates new
tasks that challenge the current agent policy, enabling more effective online
curriculum learning. Experiments across diverse environments and agent
backbones demonstrate that DreamGym substantially improves RL training, both in
fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready
tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in
RL-ready but costly settings, it matches GRPO and PPO performance using only
synthetic interactions. When transferring a policy trained purely on synthetic
experiences to real-environment RL, DreamGym yields significant additional
performance gains while requiring far fewer real-world interactions, providing
a scalable warm-start strategy for general-purpose RL.

</details>


### [4] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 本研究评估了NLP分词模型在汇编代码分析中的内在特性，包括词汇量大小、语义覆盖等，并分析了不同分词器对下游任务（如函数签名预测）的影响。


<details>
  <summary>Details</summary>
Motivation: 汇编代码分析中的分词研究仍是一个未充分探索的领域，尽管分词对词汇量大小、语义覆盖和下游任务性能具有重要影响。

Method: 系统评估多种分词模型的内在特性，使用Llama 3.2、BERT和BART等预训练模型，分析分词效率、词汇压缩和表示保真度。

Result: 分词器选择显著影响下游性能，内在指标仅能部分预测外在评估结果，揭示了内在分词特性与实际任务效用之间的复杂权衡。

Conclusion: 本研究为优化低层代码分析的分词模型提供了宝贵见解，有助于增强基于自然语言模型的二进制分析工作流程的鲁棒性和可扩展性。

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [5] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong,Luyi Ma,Varun Vasudevan,Jason Cho,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: BehaviorLens框架评估用户行为数据的文本与图像表示对多模态大语言模型性能的影响，发现图像表示能显著提升预测准确率。


<details>
  <summary>Details</summary>
Motivation: 探索用户行为数据的文本和图像表示哪种更能最大化多模态大语言模型的性能，这一问题尚未得到充分研究。

Method: 开发BehaviorLens基准框架，在6个MLLMs上测试三种数据表示方式：文本段落、散点图和流程图，使用真实购买序列数据集。

Result: 当数据表示为图像时，MLLMs的下一次购买预测准确率比等效文本表示提高了87.5%，且无需额外计算成本。

Conclusion: 图像表示在用户行为推理任务中比文本表示更有效，能显著提升MLLMs性能。

Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.

</details>


### [6] [KnowThyself: An Agentic Assistant for LLM Interpretability](https://arxiv.org/abs/2511.03878)
*Suraj Prasai,Mengnan Du,Ying Zhang,Fan Yang*

Main category: cs.AI

TL;DR: KnowThyself是一个基于聊天的LLM可解释性工具，通过自然语言界面整合了模型分析功能，降低了技术门槛。


<details>
  <summary>Details</summary>
Motivation: 现有LLM可解释性工具分散且代码密集，需要更易用的集成解决方案。

Method: 使用编排器LLM重写用户查询，通过代理路由器分发到专业模块，最后将输出整合为连贯解释。

Result: 开发了一个可扩展的LLM检查平台，提供交互式可视化和引导式解释。

Conclusion: KnowThyself通过对话式工作流程为可访问的LLM可解释性提供了坚实基础。

Abstract: We develop KnowThyself, an agentic assistant that advances large language
model (LLM) interpretability. Existing tools provide useful insights but remain
fragmented and code-intensive. KnowThyself consolidates these capabilities into
a chat-based interface, where users can upload models, pose natural language
questions, and obtain interactive visualizations with guided explanations. At
its core, an orchestrator LLM first reformulates user queries, an agent router
further directs them to specialized modules, and the outputs are finally
contextualized into coherent explanations. This design lowers technical
barriers and provides an extensible platform for LLM inspection. By embedding
the whole process into a conversational workflow, KnowThyself offers a robust
foundation for accessible LLM interpretability.

</details>


### [7] [Extracting Causal Relations in Deep Knowledge Tracing](https://arxiv.org/abs/2511.03948)
*Kevin Hong,Kia Karbasi,Gregory Pottie*

Main category: cs.AI

TL;DR: 本文挑战了深度知识追踪(DKT)模型性能源于双向关系建模的传统观点，证明DKT的真正优势在于其隐含建模先决条件因果结构的能力。


<details>
  <summary>Details</summary>
Motivation: 长期以来，计算教育研究的目标是开发可解释的知识追踪模型。虽然DKT被认为是传统方法的重大进步，但其性能提升的原因被普遍归因于建模知识组件间的双向关系，本文旨在验证这一解释的正确性。

Method: 通过将练习关系图修剪为有向无环图(DAG)，并在Assistments数据集的因果子集上训练DKT，分析其预测能力与因果结构的对齐程度。同时提出使用DKT学习表示提取练习关系DAG的替代方法。

Result: 实验表明DKT的预测能力与因果结构高度一致，支持了其有效性主要源于近似知识组件间因果依赖关系的观点。

Conclusion: DKT的有效性主要来自其建模知识组件间因果依赖关系的能力，而非简单的双向关系映射，这为理解深度学习在教育领域应用提供了新的视角。

Abstract: A longstanding goal in computational educational research is to develop
explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which
leverages a Recurrent Neural Network (RNN) to predict student knowledge and
performance on exercises, has been proposed as a major advancement over
traditional KT methods. Several studies suggest that its performance gains stem
from its ability to model bidirectional relationships between different
knowledge components (KCs) within a course, enabling the inference of a
student's understanding of one KC from their performance on others. In this
paper, we challenge this prevailing explanation and demonstrate that DKT's
strength lies in its implicit ability to model prerequisite relationships as a
causal structure, rather than bidirectional relationships. By pruning exercise
relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal
subsets of the Assistments dataset, we show that DKT's predictive capabilities
align strongly with these causal structures. Furthermore, we propose an
alternative method for extracting exercise relation DAGs using DKT's learned
representations and provide empirical evidence supporting our claim. Our
findings suggest that DKT's effectiveness is largely driven by its capacity to
approximate causal dependencies between KCs rather than simple relational
mappings.

</details>


### [8] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: LLMs对提示语言和文化框架敏感，但存在系统性文化偏见，偏向荷兰、德国、美国和日本等少数国家的价值观，无法充分代表全球文化多样性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能代表其广泛用户群体的文化多样性，考察提示语言和文化框架如何影响模型响应及其与不同国家人类价值观的契合度。

Method: 使用霍夫斯泰德价值观调查模块和世界价值观调查的63个项目，翻译成11种语言，以带或不带明确文化视角的提示形式测试10个LLM。

Result: 提示语言和文化视角都会导致LLM输出变化，但模型系统性偏向少数国家价值观；明确的文 化视角比针对性提示语言更能改善与人类价值观的契合度；两种方法结合并不比英语提示的文化框架更有效。

Conclusion: LLMs处于尴尬的中间地带：对提示变化足够敏感以产生变化，但又过于固守特定文化默认值，无法充分代表文化多样性。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [9] [ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering](https://arxiv.org/abs/2511.03985)
*Zhuowen Yuan,Tao Liu,Yang Yang,Yang Wang,Feng Qi,Kaushik Rangadurai,Bo Li,Shuang Yang*

Main category: cs.AI

TL;DR: ArchPilot是一个多代理系统，通过整合架构生成、基于代理的评估和自适应搜索来解决LLM代理在ML工程中计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在自动化ML工程中严重依赖重复的完整训练运行来评估候选方案，导致计算开销大、搜索空间扩展性有限和迭代周期慢。

Method: ArchPilot包含三个专业代理：协调搜索过程的编排代理、迭代生成改进架构的生成代理、以及执行代理训练和评估的评估代理，采用MCTS启发算法和重启机制。

Result: 在MLE-Bench上的实验表明，ArchPilot优于AIDE和ML-Master等SOTA基线方法。

Conclusion: ArchPilot的多代理协作能够在有限预算下优先考虑高潜力候选方案，减少对昂贵完整训练运行的依赖，实现高效的ML工程。

Abstract: Recent LLM-based agents have demonstrated strong capabilities in automated ML
engineering. However, they heavily rely on repeated full training runs to
evaluate candidate solutions, resulting in significant computational overhead,
limited scalability to large search spaces, and slow iteration cycles. To
address these challenges, we introduce ArchPilot, a multi-agent system that
integrates architecture generation, proxy-based evaluation, and adaptive search
into a unified framework. ArchPilot consists of three specialized agents: an
orchestration agent that coordinates the search process using a Monte Carlo
Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and
manages memory of previous candidates; a generation agent that iteratively
generates, improves, and debugs candidate architectures; and an evaluation
agent that executes proxy training runs, generates and optimizes proxy
functions, and aggregates the proxy scores into a fidelity-aware performance
metric. This multi-agent collaboration allows ArchPilot to prioritize
high-potential candidates with minimal reliance on expensive full training
runs, facilitating efficient ML engineering under limited budgets. Experiments
on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE
and ML-Master, validating the effectiveness of our multi-agent system.

</details>


### [10] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: 提出了多智能体AI系统中的异常检测任务，构建了两个包含4,275和894条轨迹的数据集，展示了监督学习和半监督学习方法能达到98%和96%的准确率。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统具有非确定性和易发生静默故障（如漂移、循环、输出细节缺失）的特点，这些故障难以检测。

Method: 开发了数据集构建流程，捕捉用户行为、智能体非确定性和LLM变化，并在此流程基础上构建了两个基准数据集。

Result: 在构建的数据集上评估异常检测方法，监督方法（XGBoost）和半监督方法（SVDD）分别达到98%和96%的准确率。

Conclusion: 这是对多智能体AI系统中异常检测的首次系统性研究，提供了数据集、基准和见解来指导未来研究。

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [11] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: LLMs在数值推理中存在系统性错误，研究发现它们会放大真实世界的数值相关性，且无关上下文会干扰数值表示，这种干扰效应随模型规模变化。


<details>
  <summary>Details</summary>
Motivation: 尽管行为研究已发现LLMs存在数值推理错误，但其底层表征机制尚不明确。研究旨在探索LLMs如何内部整合单个实体的多个数值属性，以及无关数值上下文如何干扰这些表示和下游输出。

Method: 结合线性探测与偏相关分析，以及基于提示的脆弱性测试，在不同规模的模型上进行实验。

Result: LLMs编码了真实世界的数值相关性但会系统性放大它们；无关上下文会引起数值表示的一致偏移，下游影响随模型规模变化。

Conclusion: 这些发现揭示了LLM决策中的脆弱性，为在多属性纠缠下实现更公平、表征感知的控制奠定了基础。

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [12] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: 提出了Agentmandering框架，将选区重划重新构想为两个代表对立政治利益的代理人之间的回合制谈判，通过LLM代理嵌入战略互动，显著减少党派偏见和不公平。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法主要生成合法选区重划方案，但忽略了选择过程中的战略动态，这为党派行为者挑选政治上有利的地图创造了机会。

Method: 基于博弈论思想，特别是"选择-冻结"协议，使用LLM代理在候选人地图集合中交替选择和冻结选区，通过受限且可解释的选择逐步划分州。

Result: 在2020年美国人口普查数据上的评估显示，Agentmandering显著减少了党派偏见和不公平，方差比标准基线低2-3个数量级。

Conclusion: 该方法在公平性和稳定性方面表现出色，特别是在摇摆州情景中，为选区重划提供了更公平的战略框架。

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


### [13] [KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04093)
*Yuanning Cui,Zequn Sun,Wei Hu,Zhangjie Fu*

Main category: cs.AI

TL;DR: LLM-KGFR框架通过结合大语言模型与知识图谱基础检索器，解决LLM在知识密集型问题上的局限性，实现零样本泛化和大规模图的高效处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖微调LLM或GNN检索器，存在数据集特定调优和在大规模/未见图上扩展性不足的问题。

Method: 提出LLM-KGFR协作框架：KGFR使用LLM生成的关系描述编码关系，基于问题角色初始化实体；采用非对称渐进传播(APP)高效处理大图；通过节点、边、路径级接口实现可控推理循环。

Result: 实验表明LLM-KGFR在保持可扩展性和泛化能力的同时实现了强劲性能。

Conclusion: LLM-KGFR为知识图谱增强推理提供了实用解决方案，平衡了性能、可扩展性和泛化能力。

Abstract: Large language models (LLMs) excel at reasoning but struggle with
knowledge-intensive questions due to limited context and parametric knowledge.
However, existing methods that rely on finetuned LLMs or GNN retrievers are
limited by dataset-specific tuning and scalability on large or unseen graphs.
We propose the LLM-KGFR collaborative framework, where an LLM works with a
structured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR
encodes relations using LLM-generated descriptions and initializes entities
based on their roles in the question, enabling zero-shot generalization to
unseen KGs. To handle large graphs efficiently, it employs Asymmetric
Progressive Propagation (APP)- a stepwise expansion that selectively limits
high-degree nodes while retaining informative paths. Through node-, edge-, and
path-level interfaces, the LLM iteratively requests candidate answers,
supporting facts, and reasoning paths, forming a controllable reasoning loop.
Experiments demonstrate that LLM-KGFR achieves strong performance while
maintaining scalability and generalization, providing a practical solution for
KG-augmented reasoning.

</details>


### [14] [Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms](https://arxiv.org/abs/2511.04133)
*Miguel E. Andres,Vadim Fedorov,Rida Sadek,Enric Spagnolo-Arrizabalaga,Nadescha Trudel*

Main category: cs.AI

TL;DR: 提出了首个系统化评估语音AI测试质量的框架，通过人类中心化基准测试来解决测试平台的双重挑战：生成真实测试对话和准确评估代理响应。


<details>
  <summary>Details</summary>
Motivation: 随着语音AI代理快速部署到生产环境，缺乏系统化方法来确保测试可靠性，组织无法客观评估其测试方法是否有效，这造成了关键的测量差距。

Method: 结合心理测量技术（成对比较产生Elo评分、自举置信区间和置换测试）与严格统计验证，提供适用于任何测试方法的可重现指标。

Result: 对三个领先商业平台的实证评估显示，表现最佳的Evalion平台在评估质量上达到0.92 F1分数，其他平台为0.73；在模拟质量上达到0.61，其他平台为0.43。

Conclusion: 该框架使研究人员和组织能够经验性地验证任何平台的测试能力，为大规模语音AI部署提供必要的测量基础。

Abstract: Voice AI agents are rapidly transitioning to production deployments, yet
systematic methods for ensuring testing reliability remain underdeveloped.
Organizations cannot objectively assess whether their testing approaches
(internal tools or external platforms) actually work, creating a critical
measurement gap as voice AI scales to billions of daily interactions.
  We present the first systematic framework for evaluating voice AI testing
quality through human-centered benchmarking. Our methodology addresses the
fundamental dual challenge of testing platforms: generating realistic test
conversations (simulation quality) and accurately evaluating agent responses
(evaluation quality). The framework combines established psychometric
techniques (pairwise comparisons yielding Elo ratings, bootstrap confidence
intervals, and permutation tests) with rigorous statistical validation to
provide reproducible metrics applicable to any testing approach.
  To validate the framework and demonstrate its utility, we conducted
comprehensive empirical evaluation of three leading commercial platforms
focused on Voice AI Testing using 21,600 human judgments across 45 simulations
and ground truth validation on 60 conversations. Results reveal statistically
significant performance differences with the proposed framework, with the
top-performing platform, Evalion, achieving 0.92 evaluation quality measured as
f1-score versus 0.73 for others, and 0.61 simulation quality using a league
based scoring system (including ties) vs 0.43 for other platforms.
  This framework enables researchers and organizations to empirically validate
the testing capabilities of any platform, providing essential measurement
foundations for confident voice AI deployment at scale. Supporting materials
are made available to facilitate reproducibility and adoption.

</details>


### [15] [When Empowerment Disempowers](https://arxiv.org/abs/2511.04177)
*Claire Yang,Maya Cakmak,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 本文研究发现，在多人环境中，旨在最大化单个人类用户赋能的辅助AI代理可能会削弱其他人类用户的环境控制能力，这种现象被称为"去赋能"。


<details>
  <summary>Details</summary>
Motivation: 赋能作为AI代理的通用目标无关目标已被提出，但先前研究假设代理仅辅助单个人类用户。多人环境（如家庭、医院）是AI辅助的重要场景，需要研究赋能目标在这些环境中的表现。

Method: 开发了开源多人网格世界测试套件Disempower-Grid，通过实验验证辅助RL代理在优化单个人类用户赋能时对其他用户的影响。

Result: 实验表明，优化单个人类用户赋能的辅助RL代理会显著降低其他人类用户的环境影响力和奖励，即产生去赋能现象。联合赋能可以缓解去赋能，但会牺牲用户的奖励。

Conclusion: AI对齐社区面临更广泛的挑战：在单智能体环境中看似对齐的目标无关目标，在多智能体环境中可能变得不对齐。

Abstract: Empowerment, a measure of an agent's ability to control its environment, has
been proposed as a universal goal-agnostic objective for motivating assistive
behavior in AI agents. While multi-human settings like homes and hospitals are
promising for AI assistance, prior work on empowerment-based assistance assumes
that the agent assists one human in isolation. We introduce an open source
multi-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we
empirically show that assistive RL agents optimizing for one human's
empowerment can significantly reduce another human's environmental influence
and rewards - a phenomenon we formalize as disempowerment. We characterize when
disempowerment occurs in these environments and show that joint empowerment
mitigates disempowerment at the cost of the user's reward. Our work reveals a
broader challenge for the AI alignment community: goal-agnostic objectives that
seem aligned in single-agent settings can become misaligned in multi-agent
contexts.

</details>


### [16] [Opus: A Quantitative Framework for Workflow Evaluation](https://arxiv.org/abs/2511.04220)
*Alan Seroul,Théo Fagnoni,Inès Adnani,Dana O. Mohamed,Phillip Kingston*

Main category: cs.AI

TL;DR: Opus工作流评估框架是一个概率-规范化的数学模型，用于量化工作流的质量和效率，结合了正确性、可靠性和成本，支持工作流的比较、评分和优化。


<details>
  <summary>Details</summary>
Motivation: 为了在现代自动化系统中实现工作流的自动评估、排名和优化，需要一个统一的数学框架来量化工作流的质量和效率，并支持强化学习引导的工作流发现和优化。

Method: 结合Opus工作流奖励（概率函数估计预期性能）和Opus工作流规范惩罚（衡量结构和信息质量的函数），形成一个统一的优化公式，在奖励-惩罚权衡下识别和排名最优工作流。

Result: 提出了一个完整的评估框架，能够形式化工作流成功为成本和结果的概率期望，定义可测量的规范惩罚，并支持工作流的自动评估和优化。

Conclusion: Opus工作流评估框架为工作流质量评估和优化提供了一个数学上严谨的方法，可以集成到现代自动化系统和强化学习循环中，指导工作流的发现和改进。

Abstract: This paper introduces the Opus Workflow Evaluation Framework, a
probabilistic-normative formulation for quantifying Workflow quality and
efficiency. It integrates notions of correctness, reliability, and cost into a
coherent mathematical model that enables direct comparison, scoring, and
optimization of Workflows. The framework combines the Opus Workflow Reward, a
probabilistic function estimating expected performance through success
likelihood, resource usage, and output gain, with the Opus Workflow Normative
Penalties, a set of measurable functions capturing structural and informational
quality across Cohesion, Coupling, Observability, and Information Hygiene. It
supports automated Workflow assessment, ranking, and optimization within modern
automation systems such as Opus and can be integrated into Reinforcement
Learning loops to guide Workflow discovery and refinement. In this paper, we
introduce the Opus Workflow Reward model that formalizes Workflow success as a
probabilistic expectation over costs and outcomes. We define measurable Opus
Workflow Normative Penalties capturing structural, semantic, and signal-related
properties of Workflows. Finally, we propose a unified optimization formulation
for identifying and ranking optimal Workflows under joint Reward-Penalty
trade-offs.

</details>


### [17] [Shared Spatial Memory Through Predictive Coding](https://arxiv.org/abs/2511.04235)
*Zhengru Fang,Yu Guo,Jingjing Wang,Yuang Zhang,Haonan An,Yinhai Wang,Yuguang Fang*

Main category: cs.AI

TL;DR: 提出一个多智能体预测编码框架，通过最小化智能体间的互不确定性来实现协调，在带宽受限条件下展现出卓越的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中，部分可观测性和有限带宽常常导致协调失败，需要解决空间记忆共享和重建的一致性问题。

Method: 采用信息瓶颈目标，让智能体学习何时、与谁、以及通信什么内容；基于网格细胞状度量的内部空间编码，通过自监督运动预测自发涌现；构建分层强化学习策略主动探索以减少联合不确定性。

Result: 在Memory-Maze基准测试中，当带宽从128位/步缩减到4位/步时，成功率从73.5%优雅下降到64.4%，而全广播基线从67.6%崩溃到28.6%。

Conclusion: 建立了一个理论上有原则且生物学上合理的基础，展示了复杂社会表征如何从统一的预测驱动中涌现，从而实现社会集体智能。

Abstract: Sharing and reconstructing a consistent spatial memory is a critical
challenge in multi-agent systems, where partial observability and limited
bandwidth often lead to catastrophic failures in coordination. We introduce a
multi-agent predictive coding framework that formulate coordination as the
minimization of mutual uncertainty among agents. Instantiated as an information
bottleneck objective, it prompts agents to learn not only who and what to
communicate but also when. At the foundation of this framework lies a
grid-cell-like metric as internal spatial coding for self-localization,
emerging spontaneously from self-supervised motion prediction. Building upon
this internal spatial code, agents gradually develop a bandwidth-efficient
communication mechanism and specialized neural populations that encode
partners' locations: an artificial analogue of hippocampal social place cells
(SPCs). These social representations are further enacted by a hierarchical
reinforcement learning policy that actively explores to reduce joint
uncertainty. On the Memory-Maze benchmark, our approach shows exceptional
resilience to bandwidth constraints: success degrades gracefully from 73.5% to
64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast
baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically
principled and biologically plausible basis for how complex social
representations emerge from a unified predictive drive, leading to social
collective intelligence.

</details>


### [18] [RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization](https://arxiv.org/abs/2511.04285)
*Zeng Zhiyuan,Jiashuo Liu,Zhangyue Yin,Ge Zhang,Wenhao Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: RLVR训练中存在RL过拟合问题，模型获得训练奖励但失去泛化能力。RLoop框架通过迭代策略初始化解决此问题，将训练过程转化为探索-利用的良性循环。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR训练中的RL过拟合问题，该问题导致模型过度专业化并遗忘训练期间生成的多样化解决方案。

Method: RLoop框架：1) 使用RL从给定策略探索解空间 2) 过滤成功轨迹创建专家数据集 3) 通过拒绝采样微调(RFT)改进初始策略 4) 为下一迭代创建更好的起点

Result: RLoop显著改善泛化能力，平均准确率提升9%，pass@32指标提升超过15%，相比标准RL方法。

Conclusion: RLoop通过迭代策略初始化有效缓解遗忘问题，将瞬态策略变化转化为稳健的性能提升。

Abstract: While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for
training large reasoning models, its training dynamics harbor a critical
challenge: RL overfitting, where models gain training rewards but lose
generalization. Our analysis reveals this is driven by policy
over-specialization and catastrophic forgetting of diverse solutions generated
during training. Standard optimization discards this valuable inter-step policy
diversity. To address this, we introduce RLoop, a self-improving framework
built on iterative policy initialization. RLoop transforms the standard
training process into a virtuous cycle: it first uses RL to explore the
solution space from a given policy, then filters the successful trajectories to
create an expert dataset. This dataset is used via Rejection-sampling
Fine-Tuning (RFT) to refine the initial policy, creating a superior starting
point for the next iteration. This loop of exploration and exploitation via
iterative re-initialization effectively converts transient policy variations
into robust performance gains. Our experiments show RLoop mitigates forgetting
and substantially improves generalization, boosting average accuracy by 9% and
pass@32 by over 15% compared to vanilla RL.

</details>


### [19] [GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents](https://arxiv.org/abs/2511.04307)
*Jian Mu,Chaoyun Zhang,Chiming Ni,Lu Wang,Bo Qiao,Kartik Mathur,Qianhui Wu,Yuhang Xie,Xiaojun Ma,Mengyu Zhou,Si Qin,Liqun Li,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: GUI-360°是一个大规模数据集和基准套件，用于推进计算机使用代理（CUAs）的研究，包含120万+执行动作步骤，支持GUI定位、屏幕解析和动作预测三个核心任务。


<details>
  <summary>Details</summary>
Motivation: 解决计算机使用代理研究中的三个关键差距：真实世界CUA任务稀缺、缺乏多模态轨迹的自动化收集标注流程、以及缺少统一评估GUI定位、屏幕解析和动作预测的基准。

Method: 采用LLM增强的自动化流程，包括查询来源、环境模板构建、任务实例化、批量执行和LLM驱动的质量过滤，在Windows办公应用中收集数千条轨迹数据。

Result: 基准测试显示现有视觉-语言模型在GUI定位和动作预测方面存在显著不足，监督微调和强化学习虽有改进但仍未达到人类水平可靠性。

Conclusion: GUI-360°数据集和代码已公开发布，旨在促进可重复研究并加速稳健桌面CUAs的发展。

Abstract: We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and
benchmark suite designed to advance computer-using agents (CUAs). CUAs present
unique challenges and is constrained by three persistent gaps: a scarcity of
real-world CUA tasks, the lack of automated collection-and-annotation pipelines
for multi-modal trajectories, and the absence of a unified benchmark that
jointly evaluates GUI grounding, screen parsing, and action prediction.
  GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated
pipeline for query sourcing, environment-template construction, task
instantiation, batched execution, and LLM-driven quality filtering. The
released corpus contains over 1.2M executed action steps across thousands of
trajectories in popular Windows office applications, and includes
full-resolution screenshots, accessibility metadata when available,
instantiated goals, intermediate reasoning traces, and both successful and
failed action trajectories. The dataset supports three canonical tasks, GUI
grounding, screen parsing, and action prediction, and a hybrid GUI+API action
space that reflects modern agent designs. Benchmarking state-of-the-art
vision--language models on GUI-360$^\circ$ reveals substantial out-of-the-box
shortcomings in grounding and action prediction; supervised fine-tuning and
reinforcement learning yield significant gains but do not close the gap to
human-level reliability. We release GUI-360$^\circ$ and accompanying code to
facilitate reproducible research and accelerate progress on robust desktop
CUAs.
  The full dataset has been made public on
https://huggingface.co/datasets/vyokky/GUI-360.

</details>


### [20] [Probing the Probes: Methods and Metrics for Concept Alignment](https://arxiv.org/abs/2511.04312)
*Jacob Lysnæs-Larsen,Marte Eggen,Inga Strümke*

Main category: cs.AI

TL;DR: 本文指出在可解释AI中，仅凭线性分类器探针的准确率无法可靠评估概念激活向量(CAV)的概念对齐度，因为探针容易捕捉虚假相关性。作者提出了基于空间线性归因的新概念定位方法，并引入三类定量评估概念对齐的指标。


<details>
  <summary>Details</summary>
Motivation: 当前CAV方法仅依赖探针分类准确率来评估概念表示质量，但作者发现这种评估方式不可靠，因为探针可能捕捉到与目标概念无关的虚假相关性，而非真正代表概念本身。

Method: 提出了基于空间线性归因的概念定位方法，并与现有特征可视化技术进行比较。引入了三类评估指标：硬准确率、分割分数和增强鲁棒性。还研究了具有平移不变性和空间对齐的探针。

Result: 研究表明，故意构造的错位探针利用虚假相关性也能达到与标准探针相近的准确率。具有平移不变性和空间对齐的探针能显著提高概念对齐度。

Conclusion: 需要基于对齐度的评估指标而非探针准确率，并且探针设计应考虑模型架构和目标概念的特性。

Abstract: In explainable AI, Concept Activation Vectors (CAVs) are typically obtained
by training linear classifier probes to detect human-understandable concepts as
directions in the activation space of deep neural networks. It is widely
assumed that a high probe accuracy indicates a CAV faithfully representing its
target concept. However, we show that the probe's classification accuracy alone
is an unreliable measure of concept alignment, i.e., the degree to which a CAV
captures the intended concept. In fact, we argue that probes are more likely to
capture spurious correlations than they are to represent only the intended
concept. As part of our analysis, we demonstrate that deliberately misaligned
probes constructed to exploit spurious correlations, achieve an accuracy close
to that of standard probes. To address this severe problem, we introduce a
novel concept localization method based on spatial linear attribution, and
provide a comprehensive comparison of it to existing feature visualization
techniques for detecting and mitigating concept misalignment. We further
propose three classes of metrics for quantitatively assessing concept
alignment: hard accuracy, segmentation scores, and augmentation robustness. Our
analysis shows that probes with translation invariance and spatial alignment
consistently increase concept alignment. These findings highlight the need for
alignment-based evaluation metrics rather than probe accuracy, and the
importance of tailoring probes to both the model architecture and the nature of
the target concept.

</details>


### [21] [AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research](https://arxiv.org/abs/2511.04316)
*Tim Beyer,Jonas Dornbusch,Jakob Steimle,Moritz Ladenburger,Leo Schwinn,Stephan Günnemann*

Main category: cs.AI

TL;DR: AdversariaLLM是一个用于LLM越狱鲁棒性研究的工具箱，旨在解决当前LLM安全研究生态系统碎片化、难以复现和比较的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全和鲁棒性研究生态系统碎片化严重，存在大量buggy的实现、数据集和评估方法，导致研究难以复现和比较，阻碍了有意义的进展。

Method: 设计了一个以可复现性、正确性和可扩展性为核心的工具箱，实现了12种对抗攻击算法，集成了7个基准数据集（涵盖危害性、过度拒绝和实用性评估），并通过Hugging Face提供对多种开源LLM的访问。

Result: 该框架提供了计算资源跟踪、确定性结果和分布评估技术等高级功能，确保可比性和可复现性。同时集成了JudgeZoo评估包，可独立使用。

Conclusion: AdversariaLLM为LLM安全研究建立了透明、可比和可复现的坚实基础，解决了当前研究生态系统的碎片化问题。

Abstract: The rapid expansion of research on Large Language Model (LLM) safety and
robustness has produced a fragmented and oftentimes buggy ecosystem of
implementations, datasets, and evaluation methods. This fragmentation makes
reproducibility and comparability across studies challenging, hindering
meaningful progress. To address these issues, we introduce AdversariaLLM, a
toolbox for conducting LLM jailbreak robustness research. Its design centers on
reproducibility, correctness, and extensibility. The framework implements
twelve adversarial attack algorithms, integrates seven benchmark datasets
spanning harmfulness, over-refusal, and utility evaluation, and provides access
to a wide range of open-weight LLMs via Hugging Face. The implementation
includes advanced features for comparability and reproducibility such as
compute-resource tracking, deterministic results, and distributional evaluation
techniques. \name also integrates judging through the companion package
JudgeZoo, which can also be used independently. Together, these components aim
to establish a robust foundation for transparent, comparable, and reproducible
research in LLM safety.

</details>


### [22] [RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation](https://arxiv.org/abs/2511.04328)
*Jiahao Zhao,Luxin Xu,Minghuan Tan,Lichao Zhang,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: 提出了RxSafeBench框架，通过模拟临床咨询来评估LLMs的药物安全能力，构建了包含6,725种禁忌症、28,781种药物相互作用和14,906种适应症-药物对的RxRisk DB数据库，并在2,443个高质量咨询场景中测试发现当前LLMs在整合禁忌症和相互作用知识方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLMs的医疗系统在药物安全方面的研究有限，缺乏真实世界数据集，且在真实临床咨询环境中的评估不足，特别是药物安全方面尚未充分探索。

Method: 提出模拟临床咨询的框架，生成包含药物风险的问诊对话，构建专门的药物安全数据库RxRisk DB，采用两阶段过滤策略确保临床真实性和专业质量，创建RxSafeBench基准，使用结构化多选题评估LLMs在模拟患者情境下推荐安全药物的能力。

Result: 评估结果显示，当前领先的开源和专有LLMs在整合禁忌症和相互作用知识方面表现不佳，特别是当风险是隐含而非明确时。

Conclusion: 研究揭示了在基于LLMs的系统中确保药物安全的关键挑战，为通过改进提示和任务特定调优来提高可靠性提供了见解，RxSafeBench为评估LLMs药物安全提供了首个全面基准，推动了更安全可信的AI驱动临床决策支持。

Abstract: Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.

</details>


### [23] [Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning](https://arxiv.org/abs/2511.04341)
*Nick Oh,Fernand Gobet*

Main category: cs.AI

TL;DR: 本文提出了Monitor-Generate-Verify (MGV)框架，通过在Generate-Verify范式前添加显式监控机制来解决模型早期陷入次优推理路径的问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时推理架构缺乏监控过程，导致模型容易陷入前缀主导陷阱，造成约20%的准确率损失。

Method: 将Flavell、Nelson和Narens的元认知理论形式化为计算规范，在生成前添加显式监控来捕捉元认知体验，并通过验证反馈改进未来监控。

Result: 虽然未提供实证验证，但这是首次系统地将基础元认知理论转化为计算框架。

Conclusion: MGV框架为理解推理系统失败提供了原则性词汇，并为未来测试时推理设计提出了具体的架构干预建议。

Abstract: Test-time reasoning architectures such as those following the Generate-Verify
paradigm -- where a model iteratively refines or verifies its own generated
outputs -- prioritise generation and verification but exclude the monitoring
processes that determine when and how reasoning should begin. This omission may
contribute to the prefix dominance trap, in which models commit early to
suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy
loss. We address this architectural gap by formalising Flavell's and Nelson and
Narens' metacognitive theories into computational specifications, proposing the
Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify
paradigm by adding explicit monitoring that captures metacognitive experiences
(from difficulty assessments to confidence judgements) before generation begins
and refines future monitoring through verification feedback. Though we present
no empirical validation, this work provides the first systematic computational
translation of foundational metacognitive theories, offering a principled
vocabulary for understanding reasoning system failures and suggesting specific
architectural interventions for future test-time reasoning designs.

</details>


### [24] [Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach](https://arxiv.org/abs/2511.04393)
*Chanwoo Park,Ziyang Chen,Asuman Ozdaglar,Kaiqing Zhang*

Main category: cs.AI

TL;DR: 提出了Iterative RMFT方法，通过迭代蒸馏低后悔决策轨迹来提升LLM的决策能力，无需依赖已知算法或人工模板。


<details>
  <summary>Details</summary>
Motivation: LLM作为决策代理在动态环境中表现不佳，难以实现低后悔或有效的探索-利用平衡，需要专门的决策能力训练方法。

Method: 迭代后悔最小化微调：模型生成多个决策轨迹，选择k个最低后悔的轨迹，然后在这些轨迹上微调自身。

Result: Iterative RMFT显著提升了多种LLM的决策性能，包括Transformer、开源LLM和GPT-4o mini等闭源模型，在多样化任务中展现良好泛化能力。

Conclusion: Iterative RMFT提供了一个原则性且通用的后训练框架，可有效增强LLM的决策能力，理论分析表明单层Transformer在此范式下可成为无后悔学习器。

Abstract: Large language models (LLMs) are increasingly deployed as "agents" for
decision-making (DM) in interactive and dynamic environments. Yet, since they
were not originally designed for DM, recent studies show that LLMs can struggle
even in basic online DM problems, failing to achieve low regret or an effective
exploration-exploitation tradeoff. To address this, we introduce Iterative
Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure
that repeatedly distills low-regret decision trajectories back into the base
model. At each iteration, the model rolls out multiple decision trajectories,
selects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior
methods that (a) distill action sequences from known DM algorithms or (b) rely
on manually crafted chain-of-thought templates, our approach leverages the
regret metric to elicit the model's own DM ability and reasoning rationales.
This reliance on model-generated reasoning avoids rigid output engineering and
provides more flexible, natural-language training signals. Empirical results
show that Iterative RMFT improves LLMs' DM performance across diverse models -
from Transformers with numerical input/output, to open-weight LLMs, and
advanced closed-weight models like GPT-4o mini. Its flexibility in output and
reasoning formats enables generalization across tasks with varying horizons,
action spaces, reward processes, and natural-language contexts. Finally, we
provide theoretical insight showing that a single-layer Transformer under this
paradigm can act as a no-regret learner in a simplified setting. Overall,
Iterative RMFT offers a principled and general post-training framework for
enhancing LLMs' decision-making capabilities.

</details>


### [25] [The Peril of Preference: Why GRPO fails on Ordinal Rewards](https://arxiv.org/abs/2511.04439)
*Anisha Garg,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: CoRPO是一种改进的强化学习方法，解决了GRPO在序数奖励下会强化错误行为的缺陷，通过自适应基线确保失败轨迹不被正向强化，并在达到质量阈值后转向相对偏好模式。


<details>
  <summary>Details</summary>
Motivation: GRPO的简单性使其在序数奖励下存在问题，会错误地强化失败轨迹。需要一种新方法来处理更丰富的非二元反馈，使LLM能从部分信用中学习。

Method: CoRPO使用自适应基线强制执行最低质量阈值，防止失败解决方案被正向强化。当策略稳定达到阈值后，基线自动切换到相对偏好模式，推动模型寻找最优解。

Result: 在代码验证任务上的实验表明，CoRPO具有更稳定的收敛性和更好的跨域泛化能力。

Conclusion: CoRPO代表了让LLM通过强化学习学习新能力的重要进展，使LLM能够从丰富的多维反馈中学习，从二元奖励扩展到序数奖励。

Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly
desirable for adapting LLMs to become experts at specific tasks. But this
simplicity also makes it ill-specified as we seek to enhance RL training with
richer, non-binary feedback. When using ordinal rewards to give partial credit,
GRPO's simplicity starts to hurt, as its group-average baseline often assigns a
positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new
formulation that solves this flaw. CoRPO uses an adaptive baseline that
enforces a minimum quality threshold, ensuring failed solutions are never
positively reinforced. Once the policy consistently meets this threshold, the
baseline automatically transitions to a relative preference mode, pushing the
model to find optimal solutions rather than just "acceptable" ones. We
empirically validate CoRPO on a code verification task, where it demonstrates
more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to
enable LLMs to learn genuinely new capabilities through reinforcement learning.
We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback
- progressing from binary to ordinal rewards in this work, and onward to
denser, per-step supervision.

</details>


### [26] [Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context](https://arxiv.org/abs/2511.04464)
*Carnot Braun,Rafael O. Jarczewski,Gabriel U. Talasso,Leandro A. Villas,Allan M. de Souza*

Main category: cs.AI

TL;DR: PAVe系统结合传统路径规划算法与LLM语义推理，通过多目标Dijkstra算法生成候选路线，再由LLM代理根据用户任务、偏好和规避规则进行上下文评估，实现个性化车辆路线规划。


<details>
  <summary>Details</summary>
Motivation: 传统车辆路线系统只能优化单一指标（如时间或距离），缺乏理解和整合人类驾驶员的复杂语义和动态上下文（如多步骤任务、情境约束、紧急需求）的能力。

Method: 采用混合代理方法：多目标（时间、CO2）Dijkstra算法生成候选路线集，LLM代理利用预处理的地理空间POI缓存，根据用户提供的任务、偏好和规避规则评估这些选项。

Result: 在真实城市场景基准测试中，PAVe成功将复杂用户意图转化为适当的路线修改，使用本地模型在初始路线选择中达到超过88%的准确率。

Conclusion: 将经典路由算法与基于LLM的语义推理层相结合，是创建个性化、自适应和可扩展的城市移动优化解决方案的稳健有效方法。

Abstract: Traditional vehicle routing systems efficiently optimize singular metrics
like time or distance, and when considering multiple metrics, they need more
processes to optimize . However, they lack the capability to interpret and
integrate the complex, semantic, and dynamic contexts of human drivers, such as
multi-step tasks, situational constraints, or urgent needs. This paper
introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a
hybrid agentic assistant designed to augment classical pathfinding algorithms
with contextual reasoning. Our approach employs a Large Language Model (LLM)
agent that operates on a candidate set of routes generated by a multi-objective
(time, CO2) Dijkstra algorithm. The agent evaluates these options against
user-provided tasks, preferences, and avoidance rules by leveraging a
pre-processed geospatial cache of urban Points of Interest (POIs). In a
benchmark of realistic urban scenarios, PAVe successfully used complex user
intent into appropriate route modifications, achieving over 88% accuracy in its
initial route selections with a local model. We conclude that combining
classical routing algorithms with an LLM-based semantic reasoning layer is a
robust and effective approach for creating personalized, adaptive, and scalable
solutions for urban mobility optimization.

</details>


### [27] [Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis](https://arxiv.org/abs/2511.04481)
*Lars Krupp,Daniel Geißler,Vishal Banwari,Paul Lukowicz,Jakob Karolus*

Main category: cs.AI

TL;DR: 本文首次探讨了网络代理的能耗和碳排放问题，通过理论和实证分析揭示了不同网络代理设计理念对能源消耗的显著影响，并呼吁在评估网络代理时引入专门的能耗指标。


<details>
  <summary>Details</summary>
Motivation: 尽管网络代理研究蓬勃发展，但其引发的可持续性问题仍未被充分探索。本文旨在揭示网络代理的能源和二氧化碳成本问题，强调这一问题的紧迫性。

Method: 采用理论估计和实证基准测试相结合的方法，从理论和实践两个角度分析网络代理的能耗成本。

Result: 研究结果显示，不同的网络代理创建理念会严重影响能源消耗，且更多能源消耗并不一定带来更好的结果。同时发现某些网络代理在披露模型参数和流程方面缺乏透明度，限制了能耗估计的准确性。

Conclusion: 本文呼吁改变评估网络代理的思维方式，主张在基准测试中引入专门的能耗测量指标，以促进网络代理的可持续发展。

Abstract: Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful
agentic systems pushing the boundaries of Large Language Models (LLM). They can
autonomously interact with the internet at the user's behest, such as
navigating websites, filling search masks, and comparing price lists. Though
web agent research is thriving, induced sustainability issues remain largely
unexplored. To highlight the urgency of this issue, we provide an initial
exploration of the energy and $CO_2$ cost associated with web agents from both
a theoretical -via estimation- and an empirical perspective -by benchmarking.
Our results show how different philosophies in web agent creation can severely
impact the associated expended energy, and that more energy consumed does not
necessarily equate to better results. We highlight a lack of transparency
regarding disclosing model parameters and processes used for some web agents as
a limiting factor when estimating energy consumption. Our work contributes
towards a change in thinking of how we evaluate web agents, advocating for
dedicated metrics measuring energy consumption in benchmarks.

</details>


### [28] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: 本研究开发了博弈论实验的数字孪生系统，通过系统化的提示和探测框架评估LLMs的行为表现。发现Llama能高保真复制人类合作模式，而Qwen更接近纳什均衡预测，无需基于角色的提示即可实现群体行为复制。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs与人类决策的匹配程度至关重要，因为不匹配可能导致实际应用中的有害结果，而无法复制人类行为会使LLMs在社会模拟中无效。

Method: 开发博弈论实验的数字孪生，引入系统化的提示和探测框架进行机器行为评估，测试三个开源模型（Llama、Mistral和Qwen）。

Result: Llama能高保真复制人类合作模式，捕捉人类偏离理性选择理论的行为；Qwen与纳什均衡预测高度一致；无需角色提示即可实现群体行为复制；能够生成并预注册新游戏配置的可测试假设。

Conclusion: 适当校准的LLMs可以复制人类群体行为模式，并系统探索未开发的实验空间，为社会科学研究提供补充方法，生成关于人类社交决策的新实证预测。

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [29] [Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach](https://arxiv.org/abs/2511.04556)
*Zihang Ding,Kun Zhang*

Main category: cs.AI

TL;DR: 提出了一个数据驱动的稀疏传感框架，结合EPA-SWMM模型，通过优化传感器布局来重建城市雨水系统的峰值流量，在资源受限条件下实现高精度流量监测。


<details>
  <summary>Details</summary>
Motivation: 城市地表水洪水日益频发，但高时空分辨率监测面临时间、预算和技术限制。如何在资源受限条件下监测城市排水网络并预测流量状况是主要挑战。

Method: 使用EPA-SWMM模型生成训练数据集，应用数据驱动稀疏传感框架，通过奇异值分解降维和QR分解进行传感器分配，识别最优监测节点。

Result: 在77个节点中仅需3个优化布置的传感器即可实现满意的重建性能，Nash-Sutcliffe效率值为0.92-0.95。模型对测量不确定性具有良好鲁棒性。

Conclusion: 该框架平衡了计算效率和物理可解释性，能够以最少的传感器实现高精度流量重建，可进一步与预测模型集成，在有限传感资源下实现洪水预警和实时控制。

Abstract: Urban surface water flooding, triggered by intense rainfall overwhelming
drainage systems, is increasingly frequent and widespread. While flood
prediction and monitoring in high spatial-temporal resolution are desired,
practical constraints in time, budget, and technology hinder its full
implementation. How to monitor urban drainage networks and predict flow
conditions under constrained resource is a major challenge. This study presents
a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to
optimize sensor placement and reconstruct peak flowrates in a stormwater
system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case
study. We utilized a SWMM model to generate a training dataset of peak flowrate
profiles across the stormwater network. Furthermore, we applied DSS -
leveraging singular value decomposition for dimensionality reduction and QR
factorization for sensor allocation - to identify the optimal monitoring nodes
based on the simulated training dataset. We then validated the
representativeness of these identified monitoring nodes by comparing the
DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three
optimally placed sensors among 77 nodes achieved satisfactory reconstruction
performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to
75th percentiles). In addition, the model showed good robustness to uncertainty
in measurements. Its robustness to sensor failures is location-dependent and
improves with the number of sensors deployed. The framework balances
computational efficiency and physical interpretability, enabling high-accuracy
flow reconstruction with minimal sensors. This DSS framework can be further
integrated with predictive models to realize flood early warning and real-time
control under limited sensing and monitoring resource.

</details>


### [30] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: 开发了Jr. AI Scientist系统，这是一个模拟学生研究流程的自主AI科学家系统，能够分析论文局限性、提出假设、实验验证并撰写论文，在评估中表现优于现有全自动系统，但仍有重要局限性。


<details>
  <summary>Details</summary>
Motivation: 理解当前AI科学家系统的能力和风险对于确保可信赖和可持续的AI驱动科学进步至关重要，同时需要维护学术生态系统的完整性。

Method: 开发Jr. AI Scientist系统，模拟新手学生研究者的核心研究流程：分析基线论文局限性、制定改进假设、通过严格实验验证、撰写结果论文。利用现代编码代理处理复杂的多文件实现。

Result: 评估显示Jr. AI Scientist生成的论文获得比现有全自动系统更高的评审分数，但作者评估和Agents4Science评审都发现了重要局限性。

Conclusion: 当前AI科学家系统存在直接应用的风险和关键挑战，需要进一步研究。报告了开发过程中识别的各种风险，希望加深对AI科学家发展现状和风险的理解。

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


### [31] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 该论文提出将自然语言查询中的歧义重新定义为合作交互的特征，开发了一个区分可解析的合作查询与不可解析的非合作查询的框架，并通过分析15个数据集发现现有评估方法混合了不同类型的查询，不利于准确评估系统性能。


<details>
  <summary>Details</summary>
Motivation: 自然语言与表格数据交互时存在固有歧义，传统方法将歧义视为缺陷，本文将其重新定义为合作交互的特征，让用户和系统共同承担查询规范的责任。

Method: 开发了一个原则性框架来区分合作查询（可解析）和非合作查询（不可解析），并将该框架应用于15个流行的表格问答和分析数据集的查询分析。

Result: 分析发现现有数据集中的查询类型混合控制不当，既不适合评估系统执行准确性，也不适合评估解释能力。这种混合阻碍了对系统性能的准确评估。

Conclusion: 该框架和分析将视角从消除歧义转向在解析查询中拥抱合作，为表格数据的自然语言接口提供了更明智的设计和评估方法，并概述了未来研究的方向。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [32] [Question the Questions: Auditing Representation in Online Deliberative Processes](https://arxiv.org/abs/2511.04588)
*Soham De,Lodewijk Gelauff,Ashish Goel,Smitha Milli,Ariel Procaccia,Alice Siu*

Main category: cs.AI

TL;DR: 提出了一个基于正当代表性(JR)概念的审计框架，用于衡量专家问答环节中问题选择的代表性，并开发了高效的审计算法。


<details>
  <summary>Details</summary>
Motivation: 在公民大会等审议过程中，参与者只能提出有限数量的问题给专家小组，如何选择最具代表性的问题子集是一个重要挑战。

Method: 引入基于正当代表性(JR)的审计框架，开发了在通用效用设置下审计JR的算法，最有效算法的时间复杂度为O(mn log n)。

Result: 将审计方法应用于历史审议数据，比较了主持人选择的问题、整数线性规划选择的问题和LLM生成摘要问题的代表性。

Conclusion: LLMs在支持审议过程中既有前景也有当前局限性，通过将方法集成到在线审议平台，使实践者能够审计和改进未来审议的代表性。

Abstract: A central feature of many deliberative processes, such as citizens'
assemblies and deliberative polls, is the opportunity for participants to
engage directly with experts. While participants are typically invited to
propose questions for expert panels, only a limited number can be selected due
to time constraints. This raises the challenge of how to choose a small set of
questions that best represent the interests of all participants. We introduce
an auditing framework for measuring the level of representation provided by a
slate of questions, based on the social choice concept known as justified
representation (JR). We present the first algorithms for auditing JR in the
general utility setting, with our most efficient algorithm achieving a runtime
of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number
of proposed questions. We apply our auditing methods to historical
deliberations, comparing the representativeness of (a) the actual questions
posed to the expert panel (chosen by a moderator), (b) participants' questions
chosen via integer linear programming, (c) summary questions generated by large
language models (LLMs). Our results highlight both the promise and current
limitations of LLMs in supporting deliberative processes. By integrating our
methods into an online deliberation platform that has been used for over
hundreds of deliberations across more than 50 countries, we make it easy for
practitioners to audit and improve representation in future deliberations.

</details>


### [33] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: DR.WELL是一个去中心化的神经符号框架，用于协作多智能体规划。它通过两阶段协商协议实现合作：智能体先提出候选角色，然后在共识和环境约束下承诺联合分配。承诺后，每个智能体独立生成并执行其角色的符号计划，无需透露详细轨迹。


<details>
  <summary>Details</summary>
Motivation: 协作多智能体规划需要在部分信息和有限通信下做出联合决策。轨迹级协调经常失败，因为时间或运动中的微小偏差会级联成冲突。符号规划通过提高抽象级别和提供最小动作词汇来缓解这一挑战，实现同步和集体进展。

Method: 采用两阶段协商协议：1）智能体提出带推理的候选角色；2）在共识和环境约束下承诺联合分配。承诺后，智能体独立生成符号计划并通过共享世界模型执行。世界模型编码当前状态，并随智能体行动而更新。

Result: 在协作推块任务上的实验表明，智能体能够跨剧集适应，动态世界模型捕获可重用模式，提高了任务完成率和效率。通过协商和自我优化，以时间开销换取演化、更高效的协作策略。

Conclusion: 通过推理符号计划而非原始轨迹，DR.WELL避免了脆弱的步级对齐，实现了可重用、可同步和可解释的更高级操作。神经符号方法在协作多智能体规划中展现出优势。

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [34] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: VeriCoT是一个神经符号方法，通过将思维链推理步骤形式化为一阶逻辑并验证其有效性，解决LLMs无法可靠验证自身逻辑的问题。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然能通过思维链进行多步推理，但无法可靠验证自身逻辑，即使在得出正确答案时底层推理也可能存在缺陷，这在高风险场景中削弱了可信度。

Method: VeriCoT从思维链推理中提取形式逻辑论证，将每个推理步骤形式化为一阶逻辑，识别基于源上下文、常识知识或先前推理步骤的前提，使用自动求解器验证逻辑有效性。

Result: 在ProofWriter、LegalBench和BioASQ数据集上的实验表明，VeriCoT能有效识别有缺陷的推理，并作为最终答案正确性的强预测指标。

Conclusion: VeriCoT的验证信号可用于推理时自我反思、监督微调和偏好微调，进一步提高推理有效性和准确性。

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [35] [Age of Job Completion Minimization with Stable Queues](https://arxiv.org/abs/2511.04630)
*Stavros Mitrolaris,Subhankar Banerjee,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究了一个具有马尔可夫状态机器的作业分配系统，提出了最小化作业完成时间和采样成本的策略，并分析了队列稳定性条件。


<details>
  <summary>Details</summary>
Motivation: 在中央服务器、多个用户和状态变化的机器组成的系统中，需要优化作业完成效率并控制采样成本，同时确保系统稳定性。

Method: 提出了两种策略来最小化作业完成时间和采样成本，通过分析马尔可夫机器状态和作业到达过程来设计调度算法。

Result: 数值评估显示所提策略在作业完成效率方面表现良好，并找到了保证作业队列稳定的充分条件。

Conclusion: 所提出的策略能有效平衡作业完成时间和采样成本，同时确保系统稳定性，为马尔可夫机器环境下的作业调度提供了实用解决方案。

Abstract: We consider a time-slotted job-assignment system with a central server, N
users and a machine which changes its state according to a Markov chain (hence
called a Markov machine). The users submit their jobs to the central server
according to a stochastic job arrival process. For each user, the server has a
dedicated job queue. Upon receiving a job from a user, the server stores that
job in the corresponding queue. When the machine is not working on a job
assigned by the server, the machine can be either in internally busy or in free
state, and the dynamics of these states follow a binary symmetric Markov chain.
Upon sampling the state information of the machine, if the server identifies
that the machine is in the free state, it schedules a user and submits a job to
the machine from the job queue of the scheduled user. To maximize the number of
jobs completed per unit time, we introduce a new metric, referred to as the age
of job completion. To minimize the age of job completion and the sampling cost,
we propose two policies and numerically evaluate their performance. For both of
these policies, we find sufficient conditions under which the job queues will
remain stable.

</details>


### [36] [Environment Division Multiple Access (EDMA): A Feasibility Study via Pinching Antennas](https://arxiv.org/abs/2511.03820)
*Zhiguo Ding,Robert Schober,H. V. Poor*

Main category: cs.IT

TL;DR: 提出了一种新的多址接入技术EDMA，通过捏合天线智能重构无线传播环境，在不需复杂信号处理的情况下增强目标接收信号并抑制多址干扰。


<details>
  <summary>Details</summary>
Motivation: 利用无线传播环境的动态特性，避免传统多址技术所需的复杂信号处理（如预编码、波束成形或多用户检测），实现更简单高效的多用户通信。

Method: 采用捏合天线技术，通过特定位置部署来有目的地阻断干扰链路，重构视距链路。开发了低复杂度算法优化捏合天线位置，分别针对上行和下行传输。

Result: 推导了EDMA相比传统多址技术的遍历和速率增益闭式表达式，仿真验证了所提算法与穷举搜索的最优性相当，展示了EDMA在多用户通信中的巨大潜力。

Conclusion: EDMA技术通过环境重构实现了高效的多用户通信，避免了复杂信号处理，为未来无线通信系统提供了有前景的解决方案。

Abstract: This paper exploits the dynamic features of wireless propagation environments
as the basis for a new multiple access technique, termed environment division
multiple access (EDMA). In particular, with the proposed
pinching-antenna-assisted EDMA, the multi-user propagation environment is
intelligently reconfigured to improve signal strength at intended receivers and
simultaneously suppress multiple-access interference, without requiring complex
signal processing, e.g., precoding, beamforming, or multi-user detection. The
key to creating a favorable propagation environment is to utilize the
capability of pinching antennas to reconfigure line-of-sight (LoS) links, e.g.,
pinching antennas are placed at specific locations, such that interference
links are blocked on purpose. Based on a straightforward choice of
pinching-antenna locations, the ergodic sum-rate gain of EDMA over conventional
multiple access and the probability that EDMA achieves a larger instantaneous
sum rate than the considered benchmarking scheme are derived in closed form.
The obtained analytical results demonstrate the significant potential of EDMA
for supporting multi-user communications. Furthermore, pinching antenna
location optimization is also investigated, since the locations of pinching
antennas are critical for reconfiguring LoS links and large-scale path losses.
Two low-complexity algorithms are developed for uplink and downlink
transmission, respectively, and simulation results are provided to show their
optimality in comparison to exhaustive searches.

</details>


### [37] [Which Similarity-Sensitive Entropy?](https://arxiv.org/abs/2511.03849)
*Phuc Nguyen,Josiah Couch,Rahul Bansal,Alexandra Morgan,Chris Tam,Miao Li,Rima Arnaout,Ramy Arnaout*

Main category: cs.IT

TL;DR: 本文比较了Leinster-Cobbold-Reeve (LCR)相似性敏感熵和Vendi评分(VS)两种方法，通过概念分析、理论证明和53个机器学习数据集的实验，发现两者在量级上可能相差数个数量级，并能捕获系统的互补信息。


<details>
  <summary>Details</summary>
Motivation: 传统熵度量仅捕获系统元素频率信息，而LCR和VS方法还能捕获元素间相似性和差异性的丰富信息。研究旨在比较这两种方法的优劣并确定各自适用场景。

Method: 采用概念分析、理论证明和53个机器学习数据集的实验验证，引入"半距离"概念来参数化相似性缩放的影响，并证明VS对LCR的上界关系。

Result: LCR和VS在量级上可能相差数个数量级，能捕获互补信息；VS在多个Rényi-Hill阶参数值下为LCR提供上界；两种方法都依赖于相似性缩放方式。

Conclusion: VS仅当将元素解释为更基本"原始元素"的线性组合或系统具有量子力学特性时才更优；在一般捕获相似性丰富信息的场景下，LCR更受青睐；但在特定半距离下两者可互补使用。

Abstract: A canonical step in quantifying a system is to measure its entropy. Shannon
entropy and other traditional entropy measures capture only the information
encoded in the frequencies of a system's elements. Recently, Leinster, Cobbold,
and Reeve (LCR) introduced a method that also captures the rich information
encoded in the similarities and differences among elements, yielding
similarity-sensitive entropy. More recently, the Vendi score (VS) was
introduced as an alternative, raising the question of how LCR and VS compare,
and which is preferable. Here we address these questions conceptually,
analytically, and experimentally, using 53 machine-learning datasets. We show
that LCR and VS can differ by orders of magnitude and can capture complementary
information about a system, except in limiting cases. We demonstrate that both
LCR and VS depend on how similarities are scaled and introduce the concept of
``half distance'' to parameterize this dependence. We prove that VS provides an
upper bound on LCR for several values of the R\'enyi-Hill order parameter and
conjecture that this bound holds for all values. We conclude that VS is
preferable only when interpreting elements as linear combinations of a more
fundamental set of ``ur-elements'' or when the system or dataset possesses a
quantum-mechanical character. In the broader circumstance where one seeks
simply to capture the rich information encoded by similarity, LCR is favored;
nevertheless, for certain half-distances the two methods can complement each
other.

</details>


### [38] [Efficient and rate-optimal list-decoding in the presence of minimal feedback: Weldon and Slepian-Wolf in sheep's clothing](https://arxiv.org/abs/2511.04088)
*Pranav Joshi,Daniel McMorrow,Yihan Zhang,Amitalok J. Budkuley,Sidharth Jaggi*

Main category: cs.IT

TL;DR: 该论文提出了第一个针对任意q≥2的对抗性信道编码方案，通过少量反馈实现了接近信息论最优的传输速率，同时确保接收方能将发送方消息缩小到一个小集合中。


<details>
  <summary>Details</summary>
Motivation: 现有方案仅适用于大q值，缺乏适用于任意q≥2的高效编码方案，且需要实现接近信息论最优速率的同时保证计算可行性。

Method: 使用低速率反馈（相对于传输次数渐近可忽略）的编码方案，通过精心设计的参数平衡速率、列表大小、计算复杂度和存储复杂度。

Result: 方案在足够小的ε>0和ρ∈(1-1/q-Θ(√ε))时，达到速率1-H_q(ρ)-ε（接近信息论最优），列表大小为exp(O(ε^{-3/2}log²(1/ε)))，编解码计算复杂度为n^{O(ε^{-1}log(1/ε))}，存储复杂度为O(n^{η+1}log n)，错误概率为O(n^{-η})，反馈速率为O(1/log n)。

Conclusion: 该工作首次为任意q≥2提供了具有计算可行性的高效编码方案，通过少量反馈实现了接近信息论极限的性能，填补了该领域的重要空白。

Abstract: Given a channel with length-$n$ inputs and outputs over the alphabet
$\{0,1,\ldots,q-1\}$, and of which a fraction $\varrho \in (0,1-1/q)$ of
symbols can be arbitrarily corrupted by an adversary, a fundamental problem is
that of communicating at rates close to the information-theoretically optimal
values, while ensuring the receiver can infer that the transmitter's message is
from a ``small" set. While the existence of such codes is known, and
constructions with computationally tractable encoding/decoding procedures are
known for large $q$, we provide the first schemes that attain this performance
for any $q \geq 2$, as long as low-rate feedback (asymptotically negligible
relative to the number of transmissions) from the receiver to the transmitter
is available. For any sufficiently small $\varepsilon > 0$ and $\varrho \in
(1-1/q-\Theta(\sqrt{\varepsilon})$ our minimal feedback scheme has the
following parameters: Rate $1-H_q(\varrho) - \varepsilon$ (i.e.,
$\varepsilon$-close to information-theoretically optimal -- here $H_q(\varrho)$
is the $q$-ary entropy function), list-size
$\exp(\mathcal{O}(\varepsilon^{-3/2}\log^2(1/\varepsilon))$, computational
complexity of encoding/decoding
$n^{\mathcal{O}(\varepsilon^{-1}\log(1/\varepsilon))}$, storage complexity
$\mathcal{O}(n^{\eta+1}\log n)$ for a code design parameter $\eta>1$ that
trades off storage complexity with the probability of error. The error
probability is $\mathcal{O}(n^{-\eta})$, and the (vanishing) feedback rate is
$\mathcal{O}(1/ \log n)$.

</details>


### [39] [List Decoding of Folded Reed-Solomon Codes Over Galois Ring](https://arxiv.org/abs/2511.04135)
*Chen Yuan,Ruiqi Zhu*

Main category: cs.IT

TL;DR: 本文扩展了Guruswami-Sudan列表解码方法到伽罗瓦环上的Reed-Solomon码，证明了速率为r的RS码可以达到1-√r的解码半径，并将折叠RS码的解码半径提升至Singleton界，同时将列表大小优化到O(1/ε²)。


<details>
  <summary>Details</summary>
Motivation: 由于零知识证明系统的发展需求，需要研究伽罗瓦环上码的邻近间隙问题，而RS码在伽罗瓦环上的列表解码研究尚不充分，这阻碍了基于环的算术电路的零知识证明系统发展。

Method: 首先将Guruswami-Sudan列表解码程序扩展到伽罗瓦环上的Reed-Solomon码，然后研究伽罗瓦环上折叠Reed-Solomon码的列表解码，最后通过扩展Shashank Srivastava(2025)的工作来优化列表大小。

Result: 证明了伽罗瓦环上速率为r的RS码可以列表解码到半径1-√r；折叠RS码的解码半径可以达到Singleton界；列表大小优化为O(1/ε²)。

Conclusion: 成功将有限域上的列表解码结果扩展到伽罗瓦环，为基于环的零知识证明系统提供了重要的理论基础和解码工具。

Abstract: List decoding of codes can be seen as the generalization of unique decoding
of codes While list decoding over finite fields has been extensively studied,
extending these results to more general algebraic structures such as Galois
rings remains an important challenge. Due to recent progress in zero knowledge
systems, there is a growing demand to investigate the proximity gap of codes
over Galois rings in Yizhou Yao and coauthors(2025), Alexander Golovne and
coauthors(2023), Yuanju Wei and coauthors(2025). The proximity gap is closely
related to the decoding capability of codes. It was shown in Eli Ben-Sasson and
coauthors(2020) that the proximity gap for RS codes over finite field can be
improved to $1-\sqrt{r}$ if one consider list decoding instead of unique
decoding. However, we know very little about RS codes over Galois ring which
might hinder the development of zero knowledge proof system for ring-based
arithmetic circuit. In this work, we first extend the list decoding procedure
of Guruswami and Sudan to Reed-Solomon codes over Galois rings, which shows
that RS codes with rate $r$ can be list decoded up to radius $1-\sqrt{r}$.
Then, we investigate the list decoding of folded Reed-Solomon codes over Galois
rings. We show that the list decoding radius of folded Reed-Solomon codes can
reach the Singlton bound as its counterpart over finite field. Finally, we
improve the list size of our folded Reed-Solomon code to
$O(\frac{1}{\varepsilon^2})$ by extending recent work in Shashank
Srivastava(2025) to Galois Rings.

</details>


### [40] [Affine Frequency Division Multiplexing: From Communication to Sensing](https://arxiv.org/abs/2511.04471)
*Ali Bemani,Nassar Ksairi,Marios Kountouris*

Main category: cs.IT

TL;DR: AFDM波形在集成感知与通信系统中解决两大挑战：支持大带宽的同时保持接收机复杂度可控，以及减轻多雷达环境中的干扰。


<details>
  <summary>Details</summary>
Motivation: 解决ISAC系统中的两个关键问题：在大带宽需求下控制接收机复杂度和能耗，以及在多雷达环境中抑制干扰。

Method: 利用AFDM波形的特性：兼容低复杂度自干扰消除方案、支持模拟去啁啾降低采样率、支持亚奈奎斯特采样保持延迟分辨率、利用DAFT的资源分配灵活性。

Result: AFDM在单站感知中通过SIC和模拟去啁啾降低复杂度，在双站感知中支持亚奈奎斯特采样，并能有效管理多雷达干扰。

Conclusion: AFDM波形为ISAC系统提供了有效的解决方案，能够平衡感知性能与系统复杂度，特别适用于需要高分辨率感知的应用场景。

Abstract: Affine Frequency Division Multiplexing (AFDM) has been proposed as an
effective waveform for achieving the full diversity of doubly-dispersive
(delay-Doppler) channels. While this property is closely related to range and
velocity estimation in sensing, this article focuses on other AFDM features
that are particularly relevant for addressing two challenges in integrated
sensing and communication (ISAC) systems: (1) maintaining receiver complexity
and energy consumption at acceptable levels while supporting the large
bandwidths required for high delay/range resolution, and (2) mitigating
interference in multiradar environments. In monostatic sensing, where direct
transmitter-receiver leakage is a major impairment, we show that AFDM-based
ISAC receivers can address the first challenge through their compatibility with
low-complexity self-interference cancellation (SIC) schemes and reduced
sampling rates via analog dechirping. In bistatic sensing, where such analog
solutions may not be feasible, we demonstrate that AFDM supports sub-Nyquist
sampling without requiring hardware modifications while preserving delay
resolution. Finally, we show that the second challenge can be addressed by
leveraging the resource-assignment flexibility of the discrete affine Fourier
transform (DAFT) underlying the AFDM waveform.

</details>
