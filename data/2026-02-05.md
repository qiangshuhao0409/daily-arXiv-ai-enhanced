<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Less is More: Optimizing Probe Selection Using Shared Latency Anomalies](https://arxiv.org/abs/2602.03965)
*Taveesh Sharma,Andrew Chu,Paul Schmitt,Francesco Bronzino,Nick Feamster,Nicole Marwell*

Main category: cs.NI

TL;DR: 通过拓扑无关方法分析住宅网络延迟异常，发现同一ISP内用户共享异常具有相似幅度，提出采样算法用更少探针捕获95%异常影响，证明异常幅度和时长可作为可扩展监控的有效信号。


<details>
  <summary>Details</summary>
Motivation: 住宅互联网中延迟异常常见，多个用户对同一目的地的异常可能反映共享基础设施、路由行为或拥塞。推断这种共享行为具有挑战性，因为异常幅度在不同设备间差异很大，且详细网络拓扑信息通常不可用。

Method: 采用拓扑无关方法，使用芝加哥99个住宅探针四个月的高频RTT测量数据，检测共享异常并分析其幅度和时长一致性，不依赖traceroute或显式路径信息。基于变化点检测技术，设计采样算法在用户定义约束下选择代表性设备。

Result: 发现许多共享异常在用户间表现出相似幅度，特别是在同一ISP内。采样算法使用不到一半的部署探针即可捕获95%的聚合异常影响，相比两个基线，在相似覆盖水平下识别出显著更多独特异常。即使在城市规模下，单ISP内选择探针时地理多样性仍然重要。

Conclusion: 异常幅度和时长提供了有效的拓扑独立信号，可用于住宅互联网测量中的可扩展监控、故障排除和成本效益采样。该方法不依赖详细拓扑信息，在实际部署中具有实用价值。

Abstract: Latency anomalies, defined as persistent or transient increases in round-trip time (RTT), are common in residential Internet performance. When multiple users observe anomalies to the same destination, this may reflect shared infrastructure, routing behavior, or congestion. Inferring such shared behavior is challenging because anomaly magnitudes vary widely across devices, even within the same ISP and geographic area, and detailed network topology information is often unavailable.
  We study whether devices experiencing a shared latency anomaly observe similar changes in RTT magnitude using a topology-agnostic approach. Using four months of high-frequency RTT measurements from 99 residential probes in Chicago, we detect shared anomalies and analyze their consistency in amplitude and duration without relying on traceroutes or explicit path information. Building on prior change-point detection techniques, we find that many shared anomalies exhibit similar amplitude across users, particularly within the same ISP.
  Motivated by this observation, we design a sampling algorithm that reduces redundancy by selecting representative devices under user-defined constraints. Our approach captures 95 percent of aggregate anomaly impact using fewer than half of the deployed probes. Compared to two baselines, it identifies significantly more unique anomalies at comparable coverage levels. We further show that geographic diversity remains important when selecting probes within a single ISP, even at city scale. Overall, our results demonstrate that anomaly amplitude and duration provide effective topology-independent signals for scalable monitoring, troubleshooting, and cost-efficient sampling in residential Internet measurement.

</details>


### [2] [Multi-Tier UAV Edge Computing Towards Long-Term Energy Stability for Low Altitude Networks](https://arxiv.org/abs/2602.04258)
*Yufei Ye,Shijian Gao,Xinhu Zheng,Liuqing Yang*

Main category: cs.NI

TL;DR: 提出一种多层无人机边缘计算系统，轻型低层无人机作为车辆用户的边缘服务器，高层无人机作为备份服务器，通过Lyapunov优化和BCD算法最小化任务延迟并确保无人机能量稳定性。


<details>
  <summary>Details</summary>
Motivation: 无人机具有敏捷移动性，适合低空边缘计算。现有系统缺乏对无人机能量稳定性的长期考虑，且面临未来系统状态未知的挑战。

Method: 1) 提出多层无人机边缘计算架构：L-UAVs作为边缘服务器，H-UAV作为备份服务器；2) 使用Lyapunov优化将问题解耦，平衡任务延迟和能量成本；3) 设计车辆到L-UAV匹配方案；4) 通过BCD算法联合优化任务分配、计算资源分配和无人机轨迹控制。

Result: 仿真结果显示：L-UAV传输能量降低超过26%，且相比现有基准方法，L-UAV能量稳定性表现更优。

Conclusion: 提出的多层无人机边缘计算系统能有效减少任务执行延迟，同时确保无人机长期能量稳定性，为未知未来系统状态下的边缘计算提供了有效解决方案。

Abstract: The agile mobility of Unmanned Aerial Vehicles (UAVs) makes them ideal for low-altitude edge computing. This paper proposes a novel multi-tier UAV edge computing system where lightweight Low-Tier UAVs (L-UAVs) function as edge servers for vehicle users, supported by a powerful High-Tier UAV (H-UAV) acting as a backup server. The objective is to minimize task execution delays while ensuring the long-term energy stability of the L-UAVs, despite unknown future system states. To this end, the problem is decoupled using Lyapunov optimization, which adaptively balances the priorities of task delays and L-UAV energy cost based on their real-time energy states. An efficient vehicle to L-UAV matching scheme is designed, and the joint optimization problem for task assignment, computing resource allocation, and trajectory control of L-UAVs and H-UAV is then solved via a Block Coordinate Descent (BCD) algorithm. Simulation results demonstrate a reduction in L-UAV transmission energy of over 26% and superior L-UAV energy stability compared to existing benchmarks.

</details>


### [3] [LLM-Empowered Cooperative Content Caching in Vehicular Fog Caching-Assisted Platoon Networks](https://arxiv.org/abs/2602.04471)
*Bowen Tan,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen*

Main category: cs.NI

TL;DR: 提出基于LLM的三层内容缓存架构，用于车联网雾缓存辅助的编队行驶，通过LLM智能决策最小化内容检索延迟


<details>
  <summary>Details</summary>
Motivation: 车联网编队行驶中需要高效的内容缓存机制来减少内容检索延迟，传统方法难以处理异构信息和动态系统状态

Method: 设计三层缓存架构（本地车辆、动态VFC集群、云服务器），集成LLM进行实时智能缓存决策，使用提示框架编码任务目标和约束条件，采用分层确定性缓存映射策略

Result: 仿真结果表明所提出的缓存方案具有优势，能够实现自适应请求预测和精确的内容放置，无需频繁重新训练

Conclusion: LLM驱动的三层缓存架构能有效管理车联网编队中的分布式存储，显著降低内容检索延迟

Abstract: This letter proposes a novel three-tier content caching architecture for Vehicular Fog Caching (VFC)-assisted platoon, where the VFC is formed by the vehicles driving near the platoon. The system strategically coordinates storage across local platoon vehicles, dynamic VFC clusters, and cloud server (CS) to minimize content retrieval latency. To efficiently manage distributed storage, we integrate large language models (LLMs) for real-time and intelligent caching decisions. The proposed approach leverages LLMs' ability to process heterogeneous information, including user profiles, historical data, content characteristics, and dynamic system states. Through a designed prompting framework encoding task objectives and caching constraints, the LLMs formulate caching as a decision-making task, and our hierarchical deterministic caching mapping strategy enables adaptive requests prediction and precise content placement across three tiers without frequent retraining. Simulation results demonstrate the advantages of our proposed caching scheme.

</details>


### [4] [Dual Mind World Model Inspired Network Digital Twin for Access Scheduling](https://arxiv.org/abs/2602.04566)
*Hrishikesh Dutta,Roberto Minerva,Noel Crespi*

Main category: cs.NI

TL;DR: 提出基于数字孪生和双心智世界模型（DMWM）的智能调度框架，用于动态网络环境中的自适应调度，结合短期预测规划和符号模型推演，在突发流量、干扰受限和截止时间敏感场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 工业物联网和实时网络物理系统需要能够适应动态流量、截止时间和干扰约束的智能调度策略。传统基于规则或纯数据驱动的方法难以满足这些复杂需求，需要结合模型推理和学习的新型调度框架。

Method: 提出数字孪生启发的双心智世界模型（DMWM）调度框架，结合短期预测规划和符号模型推演。该框架能够预测未来网络状态并相应调整传输决策，实现学习引导和想象力驱动的网络控制。

Result: 在可配置仿真测试平台中实现框架，与传统启发式算法和强化学习基线进行对比。结果显示DMWM在突发流量、干扰受限和截止时间敏感环境中表现优异，同时保持可解释性和样本效率。

Conclusion: DMWM框架弥合了网络级推理与低开销学习之间的差距，为基于数字孪生的可扩展自适应网络优化迈出了重要一步，在复杂网络环境中展现出优越性能。

Abstract: Emerging networked systems such as industrial IoT and real-time cyber-physical infrastructures demand intelligent scheduling strategies capable of adapting to dynamic traffic, deadlines, and interference constraints. In this work, we present a novel Digital Twin-enabled scheduling framework inspired by Dual Mind World Model (DMWM) architecture, for learning-informed and imagination-driven network control. Unlike conventional rule-based or purely data-driven policies, the proposed DMWM combines short-horizon predictive planning with symbolic model-based rollout, enabling the scheduler to anticipate future network states and adjust transmission decisions accordingly. We implement the framework in a configurable simulation testbed and benchmark its performance against traditional heuristics and reinforcement learning baselines under varied traffic conditions. Our results show that DMWM achieves superior performance in bursty, interference-limited, and deadline-sensitive environments, while maintaining interpretability and sample efficiency. The proposed design bridges the gap between network-level reasoning and low-overhead learning, marking a step toward scalable and adaptive NDT-based network optimization.

</details>


### [5] [On Dual Connectivity in 6G Leo Constellations](https://arxiv.org/abs/2602.04825)
*Achilles Machumilane,Alberto Gotta*

Main category: cs.NI

TL;DR: 本文提出了一个数学框架，用于计算在双连接网络中结合包复制与包切换或使用网络编码时的平均端到端包丢失率，其中丢失过程由离散马尔可夫链建模。


<details>
  <summary>Details</summary>
Motivation: 在5G演进中，双连接技术通过利用两条路径的信道条件来增强吞吐量和可靠性。然而，当路径存在不同延迟时（如地面与非地面集成网络、频繁拓扑变化的LEO卫星网络），可能导致包重排序、触发拥塞控制机制，或实时流量因超过播放阈值而丢包。现有技术如包复制、包切换和网络编码如果设计不当，会造成资源浪费、编解码延迟和计算开销，削弱双连接的优势。

Method: 本文提出了一个数学框架，用于计算在双连接网络中，当丢失过程由离散马尔可夫链（典型的无线信道模型）建模时，结合包复制与包切换或使用网络编码时的平均端到端包丢失率。该框架帮助推导出基于底层丢失过程完全知识的最优策略。

Result: 该数学框架能够计算双连接网络中不同流量调度技术下的平均端到端包丢失率，为优化策略提供量化基础。这些指标可以与通过机器学习算法学习的经验模型进行比较。

Conclusion: 本文提出的数学框架为双连接网络中的流量调度技术提供了理论分析工具，能够帮助设计更准确、高效的包复制、包切换和网络编码策略，避免资源浪费和性能下降，充分发挥双连接技术的优势。

Abstract: Dual connectivity (DC) has garnered significant attention in 5G evolution, allowing for enhancing throughput and reliability by leveraging the channel conditions of two paths. However, when the paths exhibit different delays, such as in terrestrial and non-terrestrial integrated networks with multi-orbit topologies or in networks characterized by frequent topology changes, like Low Earth Orbit (LEO) satellite constellations with different elevation angles, traffic delivery may experience packet reordering or triggering congestion control mechanisms. Additionally, real-time traffic may experience packet drops if their arrival exceeds a play-out threshold. Different techniques have been proposed to address these issues, such as packet duplication, packet switching, and network coding for traffic scheduling in DC. However, if not accurately designed, these techniques can lead to resource waste, encoding/decoding delays, and computational overhead, undermining DC's intended benefits. This paper provides a mathematical framework for calculating the average end-to-end packet loss in case of a loss process modeled with a Discrete Markov Chain - typical of a wireless channel - when combining packet duplication and packet switching or when network coding is employed in DC. Such metrics help derive optimal policies with full knowledge of the underlying loss process to be compared to empirical models learned through Machine Learning algorithms.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: TMK框架显著提升LLM在规划任务中的推理能力，特别是在符号化任务上准确率从31.5%提升至97.3%，引导模型从语言模式转向形式化执行路径。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在推理和规划任务中存在不足，即使CoT等提示技术也受到质疑。研究探索认知科学中的TMK框架是否能超越其在教育领域的成功，解决LLM的推理缺陷。

Method: 采用Task-Method-Knowledge框架进行结构化提示，在PlanBench基准的Blocksworld领域测试推理和规划能力，评估TMK是否能帮助LLM将复杂规划问题分解为可管理的子任务。

Result: TMK提示使推理模型在不透明的符号任务（PlanBench中Blocksworld的随机版本）上准确率从31.5%提升至97.3%，实现了显著的性能反转，缩小了语义近似与符号操作之间的差距。

Conclusion: TMK不仅提供上下文，更是一种引导机制，使推理模型从默认语言模式转向参与形式化、代码执行路径，在符号化任务中展现出强大的推理能力提升潜力。

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [7] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: IIPC是一种迭代改进的程序构造方法，通过结合执行反馈和思维链能力来提升数学推理的准确性和可修正性


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM系统在数学推理中缺乏可靠可修正的推理过程表示，要么采用僵化的顺序流程无法修正早期错误，要么依赖启发式自我评估可能无法识别和修复错误，而且程序化上下文会分散语言模型注意力降低准确性

Method: IIPC（迭代改进的程序构造）方法，迭代精炼程序化推理链，将执行反馈与基础LLM的原生思维链能力相结合，保持高层次上下文聚焦

Result: IIPC在多个基础LLM上的大多数推理基准测试中超越了竞争方法

Conclusion: IIPC通过迭代改进的程序构造方法有效解决了现有数学推理系统的局限性，代码和实现已开源发布

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [8] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: AgentArk框架将多智能体系统的动态蒸馏到单个模型中，将显式的测试时交互转化为隐式的模型能力，在保持单智能体计算效率的同时获得多智能体的推理性能。


<details>
  <summary>Details</summary>
Motivation: 虽然基于迭代辩论的LLM多智能体系统能获得优越的推理性能，但实际部署受到高计算成本和错误传播的限制。需要一种既能保持多智能体推理优势，又具备计算效率的解决方案。

Method: 提出AgentArk框架，采用三种分层蒸馏策略：推理增强微调、基于轨迹的数据增强、过程感知蒸馏。将多智能体动态蒸馏到单个模型的权重中，将计算负担从推理阶段转移到训练阶段。

Result: 蒸馏后的模型在保持单智能体计算效率的同时，展现出多智能体的强推理和自我纠正性能，并在多样化推理任务中表现出增强的鲁棒性和泛化能力。

Conclusion: AgentArk通过将多智能体动态蒸馏到单个模型中，实现了高效且鲁棒的多智能体开发，为未来高效多智能体系统研究提供了新思路。

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [9] [Active Epistemic Control for Query-Efficient Verified Planning](https://arxiv.org/abs/2602.03974)
*Shuhui Qu*

Main category: cs.AI

TL;DR: AEC是一种认知-分类规划层，通过分离事实存储与信念存储，结合环境查询与模拟预测来管理部分可观察环境中的不确定性，减少重规划次数。


<details>
  <summary>Details</summary>
Motivation: 在部分可观察环境中规划具有挑战性：任务关键前提条件（如物体位置或容器状态）在决策时可能未知，但通过交互进行验证成本高昂。学习的世界模型可以廉价预测缺失事实，但预测错误可能导致不可行的承诺。

Method: 提出主动认知控制（AEC），将基于模型的信念管理与分类可行性检查相结合。AEC严格分离用于承诺的接地事实存储和仅用于修剪候选计划的信念存储。在每个步骤中，当不确定性高或预测模糊时查询环境以接地未解析谓词，或在置信度足够时模拟谓词以过滤假设。最终承诺通过接地前提条件覆盖和SQ-BCP拉回式兼容性检查来把关。

Result: 在ALFWorld和ScienceWorld上的实验表明，AEC在较少重规划轮次下实现了与强大LLM智能体基线竞争的成功率。

Conclusion: AEC通过严格分离事实与信念、结合环境查询与模拟预测，有效管理部分可观察环境中的不确定性，减少了重规划需求，提高了规划效率。

Abstract: Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \emph{grounded fact store} used for commitment and a \emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines.

</details>


### [10] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出一种状态级选择性验证框架，在验证成本受限下优化验证资源分配，相比传统方法减少44%验证调用同时提升准确率


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理中测试时计算主要受限于昂贵的验证成本，许多验证调用浪费在冗余或无希望的中间假设上，需要在验证成本受限环境下研究如何优化验证资源分配

Method: 提出状态级选择性验证框架：1) 结构化移动接口上的确定性可行性门控；2) 结合学习的状态距离和残差得分的预验证排序；3) 基于局部不确定性的验证调用自适应分配

Result: 在MATH基准测试中，相比best-of-N、多数投票和beam search方法，使用44%更少的验证调用实现了更高的准确率

Conclusion: 通过状态级选择性验证框架，可以在验证成本受限环境下更有效地分配验证资源，相比传统方法显著减少验证调用同时提升推理性能

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [11] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: RLVR训练早期可能带来监控性提升，但这种提升并非普遍现象，而是高度依赖数据多样性和指令遵循数据，且监控性与能力提升正交。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型部署增多，审计其思维链的安全性变得至关重要。先前研究发现RLVR训练早期可能带来监控性提升，但需要系统评估这种效应的普遍性和机制。

Method: 通过跨模型家族和训练领域的系统评估，分析数据多样性、指令遵循数据的作用，进行机制分析（响应分布锐化、注意力变化），并控制训练和评估难度研究监控性动态。

Result: 监控性提升强烈依赖数据，特别是数据多样性和指令遵循数据；监控性与推理能力提升正交；机制上主要归因于响应分布锐化和对提示关注增加，而非对推理链的因果依赖增强。

Conclusion: 研究提供了RLVR下监控性涌现的全景视图，阐明了监控性提升何时可能发生、何时不会发生，强调了数据的关键作用以及监控性与能力的独立性。

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [12] [When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making](https://arxiv.org/abs/2602.04003)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.AI

TL;DR: 本文提出对抗性解释攻击(AEAs)，攻击者通过操纵LLM生成解释的框架来调节用户对错误输出的信任，揭示了AI与用户之间认知通道的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统越来越多地在人类决策循环中运行，用户依赖模型推荐。LLM生成的自然语言解释会影响用户对AI输出的感知和信任，这揭示了AI与用户之间认知通道的新攻击面。当前大多数对抗性威胁针对模型的计算行为，而非依赖模型的用户。

Method: 引入对抗性解释攻击(AEAs)，通过信任校准差距来形式化行为威胁。进行控制实验(n=205)，系统变化解释框架的四个维度：推理模式、证据类型、沟通风格和呈现格式，量化对抗性解释对人类信任的影响。

Result: 用户对对抗性和良性解释报告的信任几乎相同，对抗性解释在错误情况下仍保留了大部分良性信任。最脆弱的案例出现在AEA类似专家沟通时，结合权威证据、中性语气和领域适当的推理。在困难任务、事实驱动领域以及教育程度较低、较年轻或高度信任AI的参与者中脆弱性最高。

Conclusion: 这是第一个将解释视为对抗性认知通道并量化其对AI辅助决策中人类信任影响的系统性安全研究，揭示了LLM解释框架可以被操纵来误导用户信任，需要新的安全措施来保护认知通道。

Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.

</details>


### [13] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 本文提出一个反事实解释的axiomatic框架，证明不可能定理，识别五种不同的反事实解释类型，并将现有解释器分类到该框架中。


<details>
  <summary>Details</summary>
Motivation: 当前大多数反事实解释器只关注单一类型的反事实解释，且仅限于局部解释（针对单个实例）。缺乏对替代反事实类型的系统研究，也缺乏对揭示系统整体推理过程的全局反事实解释的研究。

Method: 建立基于一组理想属性的反事实解释器公理框架，证明不可能定理，通过表示定理建立公理子集与解释器家族之间的一一对应关系，识别五种不同的反事实解释类型。

Result: 证明了没有单一解释器能同时满足某些公理组合，识别了五种根本不同的反事实类型（包括局部和全局解释），将现有解释器分类到该框架中，并分析了生成此类解释的计算复杂度。

Conclusion: 该框架为反事实解释提供了系统化的理论基础，揭示了反事实解释的多样性，为未来解释器的设计和评估提供了指导，并帮助理解不同解释方法的本质差异。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [14] [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)
*Xiaofeng Lin,Sirou Zhu,Yilei Chen,Mingyu Chen,Hejian Sang,Ioannis Paschalidis,Zhipeng Wang,Aldo Pacchiano,Xuezhou Zhang*

Main category: cs.AI

TL;DR: ORBIT框架通过元强化学习训练LLMs，使其能够在上下文中从交互中学习，显著提升在线决策性能


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在需要在线交互获取信息、延迟反馈、平衡信息收集与利用的决策任务中表现不佳，需要训练来提升其上下文学习能力

Method: 提出ORBIT框架：多任务、多回合的元强化学习框架，训练LLMs在上下文中从交互经验中学习

Result: 经过元训练后，相对较小的开源模型(Qwen3-14B)在未见环境中表现出显著提升的在线学习能力，性能匹配GPT-5.2，大幅优于标准RL微调

Conclusion: 通过训练可以显著提升LLMs的在线学习能力，模型规模扩展实验显示仍有很大提升空间，为学习型推理决策智能体开辟了新方向

Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.

</details>


### [15] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: Interfaze是一个将LLM应用视为上下文构建与执行问题的系统，通过异构DNN堆栈、上下文构建层和动作层，结合小型模型和工具处理复杂任务，仅将精炼上下文传递给大型LLM生成最终响应。


<details>
  <summary>Details</summary>
Motivation: 传统LLM应用通常依赖单一大型模型，计算成本高且难以处理复杂多模态任务。Interfaze旨在通过模块化架构，将计算负担从昂贵的大型模型转移到小型模型和工具栈，同时保持高性能。

Method: 系统包含三个核心层：(1) 感知模块：异构DNN堆栈配对小语言模型，处理OCR、复杂PDF、图表、多语言ASR；(2) 上下文构建层：爬取、索引、解析外部资源（网页、代码、PDF）为结构化状态；(3) 动作层：支持浏览、检索、沙箱代码执行、无头浏览器驱动。顶层控制器决定运行哪些小型模型和动作，并将精炼上下文传递给用户选择的LLM。

Result: Interfaze-Beta在多个基准测试中表现优异：MMLU-Pro 83.6%、MMLU 91.4%、GPQA-Diamond 81.3%、LiveCodeBench v5 57.8%、AIME-2025 90.0%。多模态任务上：MMMU(val) 77.3%、AI2D 91.5%、ChartQA 90.9%、Common Voice v16 90.8%。大部分查询由小型模型和工具栈处理，大型LLM仅操作精炼上下文。

Conclusion: Interfaze展示了通过模块化架构将计算从昂贵的大型模型转移到小型模型和工具栈的可行性，在保持竞争力的准确率的同时，显著降低了计算成本，为LLM应用提供了更高效、可扩展的解决方案。

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [16] [OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows](https://arxiv.org/abs/2602.04144)
*Ruiting Dai,Zheyu Wang,Haoyu Yang,Yihan Liu,Chengzhi Wang,Zekun Zhang,Zishan Huang,Jiaman Cen,Lisi Mo*

Main category: cs.AI

TL;DR: OMG-Agent是一个新颖的多模态生成框架，通过解耦语义规划和细节合成来解决数据不完整问题，在极端缺失率下超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临两个瓶颈：传统参数化/生成模型因过度依赖内部记忆而产生幻觉，检索增强框架则受限于检索刚性。更重要的是，这些端到端架构受到"语义-细节纠缠"的结构性冲突限制，即逻辑推理和信号合成之间的冲突损害了保真度。

Method: 提出OMG-Agent框架，采用动态粗到细的代理工作流，模拟"先思考后行动"的认知过程。包含三个阶段：1) MLLM驱动的语义规划器通过渐进式上下文推理解决输入歧义；2) 非参数化证据检索器将抽象语义锚定在外部知识中；3) 检索注入执行器利用检索证据作为灵活特征提示来合成高保真细节。

Result: 在多个基准测试上的广泛实验表明，OMG-Agent始终超越最先进方法，在极端缺失率下保持鲁棒性，例如在70%缺失率下CMU-MOSI基准上获得2.6分的提升。

Conclusion: OMG-Agent通过解耦语义规划和细节合成，有效解决了多模态系统中的数据不完整问题，提供了一种更可靠、高保真的生成框架。

Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \textbf{\underline{O}}mni-\textbf{\underline{M}}odality \textbf{\underline{G}}eneration Agent (\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\% missing rates.

</details>


### [17] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 提出Scalable Interactive Oversight框架，通过将复杂意图分解为可管理的决策树来增强人类监督，使非专家能生成专家级产品需求文档，对齐度提升54%


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地自动化复杂、长时程任务，出现了监督缺口。虽然模型擅长执行，但用户由于领域专业知识不足、难以精确表达意图、无法可靠验证复杂输出等原因，难以有效指导模型。这提出了可扩展监督的关键挑战：如何让人类在超越自身规范和验证能力的任务上负责任地引导AI系统。

Method: 提出Scalable Interactive Oversight框架，将复杂意图分解为递归的决策树，在节点层面收集低负担的人类反馈，并递归聚合这些信号形成精确的全局指导。在网页开发任务中验证，并通过强化学习仅使用在线用户反馈进行优化。

Result: 在网页开发任务中，该框架使非专家能够生成专家级的产品需求文档，实现了54%的对齐度提升。框架可以通过仅使用在线用户反馈的强化学习进行优化，为AI扩展时保持人类控制提供了实用路径。

Conclusion: Scalable Interactive Oversight框架为解决可扩展监督问题提供了有效方案，通过分解复杂意图和递归聚合反馈，使人类能够有效指导超越自身能力的AI系统，并通过强化学习优化实现实用化路径。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [18] [InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons](https://arxiv.org/abs/2602.04213)
*Feiyu Gavin Zhu,Jean Oh,Reid Simmons*

Main category: cs.AI

TL;DR: InterPReT：一种交互式策略重构与训练方法，让普通用户能够通过指令和演示来教导AI智能体，降低机器学习门槛


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习需要专业人士提供大量演示数据并密切监控训练过程，这对普通用户教导AI智能体新技能构成了障碍。需要降低教导AI智能体的门槛，让非技术背景的终端用户也能训练可靠的策略

Method: 提出Interactive Policy Restructuring and Training (InterPReT)方法，通过用户指令持续更新策略结构并优化参数以适应用户演示，支持用户交互式提供指令和演示、监控智能体性能、审查决策策略

Result: 在赛车游戏中教导AI智能体驾驶的用户研究（N=34）表明，与通用模仿学习基线相比，InterPReT能产生更鲁棒的策略且不影响系统可用性，特别适合普通用户同时提供演示和决定何时停止训练的场景

Conclusion: InterPReT方法更适合没有机器学习技术背景的终端用户训练可靠的策略，通过交互式教学降低了AI智能体训练的门槛

Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent's performance, and review the agent's decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy

</details>


### [19] [Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search](https://arxiv.org/abs/2602.04248)
*Hao Lu,Haoyuan Huang,Yulin Zhou,Chen Li,Ningxin Zhu*

Main category: cs.AI

TL;DR: Empirical-MCTS：将无状态MCTS转变为持续学习的双循环框架，通过PE-EMP和记忆优化代理实现经验积累，显著提升复杂推理能力


<details>
  <summary>Details</summary>
Motivation: 当前推理时扩展策略（如MCTS）主要是无状态的，每次解决问题后丢弃成功推理模式，无法像人类那样积累经验智慧。需要将结构化搜索与经验积累相结合来应对复杂开放推理任务。

Method: 提出Empirical-MCTS双循环框架：1) PE-EMP在局部搜索中作为反射优化器，通过成对反馈动态合成自适应标准并实时演化元提示；2) 记忆优化代理管理全局存储库作为动态策略先验，使用原子操作跨问题提炼高质量见解

Result: 在AIME25、ARC-AGI-2和MathArena Apex等复杂推理基准测试中，Empirical-MCTS显著优于无状态MCTS策略和独立经验驱动代理

Conclusion: 结构化搜索与经验积累的耦合对于掌握复杂开放推理任务至关重要，Empirical-MCTS证明了将无状态搜索转变为持续非参数学习过程的有效性

Abstract: Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.

</details>


### [20] [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)
*Yansong Ning,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: Agent-Omit是一个训练框架，让LLM智能体能够自适应地省略冗余的思考和观察，在保持性能的同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究将整个交互轨迹同等对待，忽视了不同回合中思考必要性和观察效用的差异，导致智能体效率低下。

Method: 1) 合成少量冷启动数据（单回合和多回合省略场景）微调智能体；2) 提出省略感知的强化学习方法，包含双重采样机制和定制化省略奖励；3) 理论证明省略策略的偏差有KL散度上界。

Result: 在五个智能体基准测试中，Agent-Omit-8B模型性能与七个前沿LLM智能体相当，在七个高效LLM智能体方法中实现了最佳的有效性-效率权衡。

Conclusion: Agent-Omit框架通过自适应省略冗余思考和观察，显著提升了智能体的效率，同时保持了性能，为智能体效率优化提供了有效解决方案。

Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.

</details>


### [21] [From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents](https://arxiv.org/abs/2602.04326)
*SeungWon Seo,SooBin Lim,SeongRae Noh,Haneul Kim,HyeongYeop Kang*

Main category: cs.AI

TL;DR: PCE框架将LLM推理中的隐含假设转化为结构化决策树，实现多智能体环境中的不确定性规划，减少通信开销


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体主要依赖频繁通信来应对不确定性，但这带来高昂的token和时间成本，且可能干扰人类协作伙伴的工作流程

Method: 提出Planner-Composer-Evaluator框架：Planner生成推理轨迹，Composer将隐含假设转化为结构化决策树（内部节点编码环境假设，叶子节点对应动作），Evaluator基于场景可能性、目标导向收益和执行成本对路径评分

Result: 在两个多智能体基准测试（C-WAH和TDW-MAT）和三种LLM骨干上，PCE在成功率和任务效率上持续优于通信密集型基线，同时保持相当的token使用量。消融实验表明PCE的性能增益在不同模型容量和推理深度下都有效

Conclusion: PCE为将LLM的隐含假设转化为可靠的不确定性感知规划策略提供了原则性途径，产生的通信模式被人类伙伴认为更高效和可信

Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.

</details>


### [22] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 提出基于数字孪生的零配置AI管道框架，解决工业CPS中AI/ML集成碎片化问题，实现模块化、可互操作的智能服务部署


<details>
  <summary>Details</summary>
Motivation: 工业CPS日益复杂，物联网和工业物联网技术碎片化（通信协议、数据格式、设备能力差异）导致物理层与智能功能层之间存在巨大鸿沟。现有数字孪生方法往往孤立且紧耦合，限制了AI功能的可扩展性和重用性。

Method: 提出模块化、可互操作的解决方案，通过最小化配置和解耦数字孪生与AI组件角色，实现AI管道无缝集成。引入零配置AI管道概念，由数字孪生协调数据管理和智能增强。在微工厂场景中演示，支持并发ML模型和动态数据处理。

Result: 在微工厂场景中成功演示了该框架，支持并发ML模型和动态数据处理，有效加速了复杂工业环境中智能服务的部署。

Conclusion: 提出的基于数字孪生的零配置AI管道框架能够解决工业CPS中AI/ML集成碎片化问题，通过模块化、解耦的设计实现智能服务的快速部署和扩展，为复杂工业环境中的智能系统集成提供了有效解决方案。

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [23] [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)
*Zhentao Tang,Yuqi Cui,Shixiong Kai,Wenqian Zhao,Ke Ye,Xing Li,Anxin Tian,Zehua Pei,Hui-Ling Zhen,Shoubo Hu,Xiaoguang Li,Yunhe Wang,Mingxuan Yuan*

Main category: cs.AI

TL;DR: ReThinker是一个基于置信度的智能体框架，通过Solver-Critic-Selector架构实现动态计算分配，在专家级科学推理任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在专家级科学推理（如Humanity's Last Exam）上存在挑战：固定工具流水线、脆弱的多智能体协调、低效的测试时扩展限制了性能

Method: 提出ReThinker框架，采用Solver-Critic-Selector三阶段架构，基于模型置信度动态分配计算；开发反向数据合成流水线和自适应轨迹回收策略，从成功推理轨迹生成高质量监督数据

Result: 在HLE、GAIA和XBench基准测试中，ReThinker持续超越现有最佳基础模型和深度研究系统，在专家级推理任务上取得最先进结果

Conclusion: ReThinker通过置信度感知的动态计算分配和高质量数据合成策略，有效解决了专家级科学推理的挑战，为智能体框架设计提供了新思路

Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.

</details>


### [24] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 提出一个序列交互框架，让GenAI系统向论坛提问，论坛可以选择发布部分问题，解决AI依赖论坛数据却分流用户的矛盾


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统依赖问答论坛的数据来提升性能，但同时却分流了论坛用户，形成了悖论。需要解决这种依赖与竞争的矛盾，促进AI系统与人类知识平台的可持续协作。

Method: 提出序列交互框架，考虑非货币交换、信息不对称和激励错配等复杂因素。使用真实的Stack Exchange数据和常用LLM进行全面的数据驱动模拟。

Result: 实证展示了激励错配问题，但显示参与者仍能获得理想全信息场景下约一半的效用。结果突出了AI系统与人类知识平台之间可持续协作的潜力。

Conclusion: 该框架为AI系统与人类知识平台之间的可持续协作提供了可能，能够保持有效的知识共享，解决依赖与竞争的矛盾。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [25] [Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration](https://arxiv.org/abs/2602.04575)
*Jiaheng Liu,Yuanxing Zhang,Shihao Li,Xinping Lei*

Main category: cs.AI

TL;DR: 论文提出Vibe AIGC新范式，通过智能体编排解决当前生成式AI的意图-执行鸿沟问题，将用户从提示工程转变为提供高层"氛围"的指挥官，由元规划器分解为可执行的多智能体工作流。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI受模型中心范式主导，虽然视觉保真度显著提升，但存在"可用性天花板"问题，表现为意图-执行鸿沟——用户高层意图与当前单次生成模型的随机性、黑盒特性之间的根本差距。

Method: 提出Vibe AIGC范式，受Vibe Coding启发，通过智能体编排实现内容生成。用户作为指挥官提供"氛围"（包含美学偏好、功能逻辑等高层表示），集中式元规划器作为系统架构师，将"氛围"分解为可执行、可验证、自适应的智能体管道。

Result: 通过从随机推理转向逻辑编排，Vibe AIGC弥合了人类想象力与机器执行之间的鸿沟，将AI从脆弱的推理引擎转变为强大的系统工程合作伙伴。

Conclusion: 这一范式转变将重新定义人机协作经济，使复杂、长周期的数字资产创作民主化，为生成式AI开辟新的发展方向。

Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.

</details>


### [26] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 论文提出WideSeek-R1框架，通过多智能体宽度扩展解决广泛信息搜索任务，使用4B参数达到与671B单智能体相当性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要关注深度扩展（单智能体解决长时程问题），但随着任务范围扩大，瓶颈从个体能力转向组织能力。现有多智能体系统依赖手工工作流和轮流交互，无法有效并行工作。

Method: 提出WideSeek-R1框架：领导智能体-子智能体架构，通过多智能体强化学习训练，实现可扩展编排和并行执行。使用共享LLM但隔离上下文和专用工具，在2万条广泛信息搜索任务数据集上联合优化。

Result: WideSeek-R1-4B在WideSearch基准上达到40.0%的项目F1分数，与单智能体DeepSeek-R1-671B性能相当。随着并行子智能体数量增加，性能持续提升，验证了宽度扩展的有效性。

Conclusion: 宽度扩展是多智能体系统的重要方向，WideSeek-R1框架通过MARL训练实现了高效并行执行，为处理广泛信息搜索任务提供了新思路。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [27] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: 本文通过七维分类法（认知能力、知识管理、交互模式、适应与学习、安全与伦理、框架类型学、核心任务与子任务）系统分析了49项LLM医疗代理研究，揭示了能力实现的不对称性：外部知识整合普遍实现，而事件触发激活、漂移检测与缓解等能力严重缺失。


<details>
  <summary>Details</summary>
Motivation: 现有LLM医疗代理研究多为概述性调查或单一能力探讨，缺乏统一分析框架。本文旨在通过系统分类法填补这一空白，为医疗领域提供共同的分析框架。

Method: 采用七维分类法（含29个操作子维度），对49项研究进行系统分析。使用明确的纳入排除标准和三级标签（完全实现、部分实现、未实现），量化能力分布和共现模式。

Result: 分析揭示了明显的不对称性：知识管理中的外部知识整合普遍实现（~76%完全实现），而交互模式中的事件触发激活基本缺失（~92%未实现），适应学习中的漂移检测与缓解罕见（~98%未实现）。多智能体设计是主导架构模式（~82%完全实现），信息中心能力（如医疗问答与决策支持）领先，而治疗规划与处方等行动导向领域仍有显著差距（~59%未实现）。

Conclusion: LLM医疗代理研究在能力实现上存在显著不平衡，当前研究偏向信息处理和基准测试，而实时响应、自适应学习和行动导向任务等关键医疗需求仍待解决，为未来研究指明了方向。

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


### [28] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 该论文反驳METR报告中AI能力呈指数增长的论点，认为数据不支持指数增长，且拐点已过，强调现有指数增长预测的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 针对METR报告声称AI能力自2019年以来呈指数增长的观点，作者旨在质疑这种预测的可靠性，指出现有数据并不支持指数增长假设，以纠正对AI发展速度的过度乐观预期。

Method: 1. 对METR数据拟合S型/逻辑曲线，发现拐点已过而非在未来；2. 提出更复杂的模型，将AI能力分解为基础能力和推理能力，分别分析其改进速率；3. 通过模型证明AI能力将在近期出现拐点。

Result: 1. 逻辑曲线拟合显示AI能力增长的拐点已经过去，而非METR预测的遥远未来；2. 分解模型支持AI能力将在近期出现拐点的假设；3. 现有指数增长预测缺乏稳健性。

Conclusion: AI能力增长并非指数型，现有指数增长预测具有脆弱性，需要更谨慎的预测方法，AI发展可能已进入增长放缓阶段。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [29] [Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing](https://arxiv.org/abs/2602.04837)
*Zhaotian Weng,Antonis Antoniades,Deepak Nathani,Zhen Zhang,Xiao Pu,Xin Eric Wang*

Main category: cs.AI

TL;DR: GEA（群体演化智能体）是一种新的开放式自我改进范式，将智能体群体作为基本演化单元，通过群体内经验共享和重用，显著提升编码任务的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有开放式自我演化范式采用树状结构演化，导致探索多样性利用效率低下，各演化分支孤立发展，无法有效共享经验。需要一种新范式来克服这些限制，实现更高效的自我改进。

Method: 提出群体演化智能体（GEA）范式，将智能体群体作为基本演化单元，在演化过程中实现显式的经验共享和重用。相比树状演化结构，GEA通过群体协作克服了探索多样性利用效率低的问题。

Result: 在编码基准测试中显著优于最先进的自我演化方法（SWE-bench Verified：71.0% vs 56.7%；Polyglot：88.3% vs 68.3%），匹配或超越顶级人工设计的智能体框架。GEA能更有效地将早期探索多样性转化为长期进步，在不同编码模型间具有一致的迁移性，修复框架级bug平均只需1.4次迭代（自我演化方法需要5次）。

Conclusion: GEA通过群体作为演化单元和显式经验共享，实现了更高效的开放式自我改进，在编码任务中展现出优越性能、强鲁棒性和良好的迁移能力，为自主智能体发展提供了新方向。

Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.

</details>


### [30] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: QwQ-32B模型通过推理过程逐步优化内部表征，发展出关注结构而非具体动作名称的抽象编码，这种"流体推理表征"是推理模型性能提升的关键因素之一。


<details>
  <summary>Details</summary>
Motivation: 尽管推理语言模型在抽象问题上表现优异，但其内部机制仍不清楚。本文旨在通过机制分析理解QwQ-32B模型如何处理抽象结构信息，揭示推理模型性能提升的内部机制。

Method: 在语义混淆的规划领域Mystery Blocksworld上，分析QwQ-32B模型的内部表征变化；通过引导实验建立因果证据：注入成功轨迹中的精炼表征提升准确性，用符号表征替换混淆编码观察性能变化。

Result: 模型在推理过程中逐步改进动作和概念的内部表征，发展出关注结构而非具体动作名称的抽象编码；精炼表征注入能提升准确率，符号表征替换混淆编码仅导致最小性能损失。

Conclusion: 推理模型性能提升的关键因素之一是上下文中的表征精炼过程，即"流体推理表征"：模型在推理过程中动态优化内部表征，发展出更抽象、结构化的编码方式。

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [31] [Semantic Rate Distortion and Posterior Design: Compute Constraints, Multimodality, and Strategic Inference](https://arxiv.org/abs/2602.03949)
*Emrah Akyol*

Main category: cs.IT

TL;DR: 研究战略高斯语义压缩：在速率和计算约束下，编码器和解码器优化不同二次目标，分析战略率失真函数，推导语义注水算法，证明高斯最优性，并建立多模态语言模型的后验设计理论基础。


<details>
  <summary>Details</summary>
Motivation: 为数据高效和能量高效的AI提供信息理论基础，解释现代多模态语言模型在资源约束下作为后验设计机制的原理，解决编码器和解码器目标不一致时的战略压缩问题。

Method: 采用战略高斯语义压缩框架，编码器设计后验协方差，解码器通过MMSE估计最佳响应，在直接、远程和完全信息三种机制下分析，考虑信息速率约束和计算架构限制。

Result: 推导出战略率失真函数，得到语义注水算法和速率约束高斯说服解，证明高斯最优性，发现模型深度和推理时间计算带来语义准确性的指数级改进，多模态观测消除远程编码的几何平均惩罚。

Conclusion: 为数据高效AI提供信息理论基础，将现代多模态语言模型解释为资源约束下的后验设计机制，计算架构限制可作为隐式速率约束，多模态观测能克服远程编码的固有局限性。

Abstract: We study strategic Gaussian semantic compression under rate and compute constraints, where an encoder and decoder optimize distinct quadratic objectives. A latent Gaussian state generates a task dependent semantic variable, and the decoder best responds via MMSE estimation, reducing the encoder's problem to posterior covariance design under an information rate constraint. We characterize the strategic rate distortion function in direct, remote, and full information regimes, derive semantic waterfilling and rate constrained Gaussian persuasion solutions, and establish Gaussian optimality under misaligned objectives. We further show that architectural compute limits act as implicit rate constraints, yielding exponential improvements in semantic accuracy with model depth and inference time compute, while multimodal observation eliminates the geometric mean penalty inherent to remote encoding. These results provide information theoretic foundations for data and energy efficient AI and offer a principled interpretation of modern multimodal language models as posterior design mechanisms under resource constraints.

</details>


### [32] [Joint Sleep Mode Activation and Load Balancing with Dynamic Cell Load: A Combinatorial Bandit Approach](https://arxiv.org/abs/2602.04808)
*Wajahat Bashir Gilkar,Gourab Ghatak*

Main category: cs.IT

TL;DR: 提出组合多臂老虎机方法，通过触发gNB小基站睡眠模式并配合小区范围扩展负载均衡，在保证5G服务质量的同时优化能耗效率。


<details>
  <summary>Details</summary>
Motivation: 5G网络中，关闭小基站(gNB)虽然能降低自身能耗，但会增加相邻基站和宏基站的负载，影响整体能效。需要在不影响用户服务质量的前提下，智能管理基站睡眠模式。

Method: 采用组合上置信界(CUCB)算法触发小基站睡眠模式，结合小区范围扩展(CRE)负载均衡。建立动态小区负载模型，综合考虑用户位置、相对基站位置和数据需求。可作为O-RAN近实时RAN智能控制器xApps实现。

Result: 提出的CUCB+负载均衡算法不仅优于保持所有小基站开启的简单策略，也优于其他最先进的强化学习解决方案，在保证5QI服务质量要求的同时显著提升能效。

Conclusion: 组合多臂老虎机方法能有效管理5G小基站睡眠模式，通过智能负载均衡实现能耗优化，可作为O-RAN架构中的智能控制器应用部署。

Abstract: We propose a combinatorial bandit formulation to opportunistically trigger sleep modes in gNode-B (gNB) small cells (SCs), followed by a cell range expansion (CRE)-based load balancing procedure. This is implemented by ensuring that the fifth generation (5G) quality of service identifier (5QI)-requirements of user equipments (UEs) are maintained. The key challenge is the fact that while deactivating a given SC gNB reduces its own consumption, it may increase the load on neighboring gNBs and the macro gNB (coverage cell), impacting the overall energy efficiency. This phenomenon is accurately characterized by modeling the dynamic cell load that jointly takes into account the location of the UEs, their relative locations to all the SCs, and their data demands. We experimentally show that the proposed combinatorial upper confidence bound (CUCB) followed by the load balancer outperforms not only the naive strategies like arbitrarily keeping all the SCs on, but also other state-of-the-art reinforcement learning solutions. The proposed algorithm can be implemented as open-radio access network (O-RAN) near-real-time (NRT) RAN intelligent controller (RIC) xApps.

</details>


### [33] [Game of Coding for Vector-Valued Computations](https://arxiv.org/abs/2602.04810)
*Hanzaleh Akbari Nodehi,Parsa Moradi,Soheil Mohajer,Mohammad Ali Maddah-Ali*

Main category: cs.IT

TL;DR: 将博弈编码框架从标量扩展到N维欧几里得空间，为高维数据计算提供理论基础，即使在对抗多数情况下也能保证正确解码。


<details>
  <summary>Details</summary>
Motivation: 传统编码理论需要诚实节点占多数才能保证有效解码，而博弈编码利用经济理性保证正确性，即使对抗节点占多数也能工作。然而先前研究仅限于标量计算，无法处理现实世界中的高维数据任务。

Method: 将博弈编码框架扩展到N维欧几里得空间，为向量值计算提供严格的问题表述，并完全表征所得高维博弈的均衡策略。

Result: 分析表明，标量设置中建立的弹性特性在向量机制中得以保留，为无需诚实多数假设的安全、大规模去中心化计算奠定了理论基础。

Conclusion: 该研究成功将博弈编码扩展到高维空间，为去中心化机器学习等无许可应用提供了理论基础，突破了传统编码理论的信任限制。

Abstract: The game of coding is a new framework at the intersection of game theory and coding theory; designed to transcend the fundamental limitations of classical coding theory. While traditional coding theoretic schemes rely on a strict trust assumption, that honest nodes must outnumber adversarial ones to guarantee valid decoding, the game of coding leverages the economic rationality of actors to guarantee correctness and reliable decodability, even in the presence of an adversarial majority. This capability is paramount for emerging permissionless applications, particularly decentralized machine learning (DeML). However, prior investigations into the game of coding have been strictly confined to scalar computations, limiting their applicability to real world tasks where high dimensional data is the norm. In this paper, we bridge this gap by extending the framework to the general $N$-dimensional Euclidean space. We provide a rigorous problem formulation for vector valued computations and fully characterize the equilibrium strategies of the resulting high dimensional game. Our analysis demonstrates that the resilience properties established in the scalar setting are preserved in the vector regime, establishing a theoretical foundation for secure, large scale decentralized computing without honest majority assumptions.

</details>


### [34] [Capacity Bounds on Doppler OFDM Channels](https://arxiv.org/abs/2602.04862)
*Pablo Orellana,Zheng Li,Jean-Marc Kelif,Sheng Yang,Shlomo Shamai*

Main category: cs.IT

TL;DR: 论文研究了LEO卫星系统中多普勒效应引起的信道不确定性，提出了基于子空间对齐的叠加编码方案，在低复杂度下实现接近最优的速率。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星系统由于高速移动会产生显著的多普勒效应。虽然多普勒频移可以大部分补偿，但残余频率不确定性会导致结构化信道不确定性，从而限制可达到的速率。

Method: 使用块衰落信道模型H=F+sG，其中s是未知标量随机参数。提出基于子空间对齐的叠加编码方案，采用粗层流作为隐式导频，通过连续干扰消除解码精层数据。

Result: 推导了可达到速率的下界和容量上界，在近相干和高信噪比区域表征了渐近容量。通过多普勒OFDM仿真表明，提出的SN方案能以低复杂度实现接近最优的速率。

Conclusion: 提出的子空间对齐叠加编码方案能有效处理LEO卫星系统中的多普勒残余不确定性，在低复杂度下实现接近信道容量的性能，为实际系统设计提供了实用解决方案。

Abstract: Low Earth orbit (LEO) satellite systems experience significant Doppler effects due to high mobility. While Doppler shifts can be largely compensated, residual frequency uncertainty induces a structured form of channel uncertainty that can limit achievable rates. We model this effect using a block-fading channel of the form $ \mathbf{H} = \mathbf{F} + s \mathbf{G} $, where $s$ is an unknown scalar random parameter. We first study this model in a general $N\times N$ MIMO setting. For this channel, we derive achievable rate lower bounds based on explicit transmission schemes and capacity upper bounds using a duality approach. We study Gaussian signaling and propose a practical superposition scheme with subspace alignment (SN) and successive interference cancellation, where a coarse-layer stream serves as an implicit pilot for decoding refined-layer data. We characterize asymptotic capacity in the near-coherent and high-SNR regimes, and show via Doppler-OFDM simulations that the proposed SN scheme achieves near-optimal rates with low complexity.

</details>
