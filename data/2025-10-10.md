<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 67]
- [cs.IT](#cs.IT) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [DRACO: Data Replication and Collection Framework for Enhanced Data Availability and Robustness in IoT Networks](https://arxiv.org/abs/2510.07464)
*Waleed Bin Qaim,Oznur Ozkasap,Rabia Qadar,Moncef Gabbouj*

Main category: cs.NI

TL;DR: DRACO是一个物联网数据管理框架，通过分布式逐跳数据复制和无开销移动汇聚节点数据收集策略，提高数据可用性并优化数据检索效率。


<details>
  <summary>Details</summary>
Motivation: 物联网系统面临资源受限和不可靠设备导致的数据丢失问题，需要解决数据生成、收集和管理的效率挑战。

Method: 集成分布式逐跳数据复制方法和无开销移动汇聚节点数据收集策略，优化副本放置和数据检索。

Result: 在ns-3模拟中，DRACO相比贪婪和随机复制技术，数据可用性分别提高15%和34%，副本创建分别提高18%和40%。

Conclusion: DRACO通过解决物联网数据管理关键挑战，为新兴应用场景提供了可扩展且具有弹性的解决方案。

Abstract: The Internet of Things (IoT) bridges the gap between the physical and digital
worlds, enabling seamless interaction with real-world objects via the Internet.
However, IoT systems face significant challenges in ensuring efficient data
generation, collection, and management, particularly due to the
resource-constrained and unreliable nature of connected devices, which can lead
to data loss. This paper presents DRACO (Data Replication and Collection), a
framework that integrates a distributed hop-by-hop data replication approach
with an overhead-free mobile sink-based data collection strategy. DRACO
enhances data availability, optimizes replica placement, and ensures efficient
data retrieval even under node failures and varying network densities.
Extensive ns-3 simulations demonstrate that DRACO outperforms state-of-the-art
techniques, improving data availability by up to 15% and 34%, and replica
creation by up to 18% and 40%, compared to greedy and random replication
techniques, respectively. DRACO also ensures efficient data dissemination
through optimized replica distribution and achieves superior data collection
efficiency under varying node densities and failure scenarios as compared to
commonly used uncontrolled sink mobility approaches namely random walk and
self-avoiding random walk. By addressing key IoT data management challenges,
DRACO offers a scalable and resilient solution well-suited for emerging use
cases.

</details>


### [2] [TDoA-Based Self-Supervised Channel Charting with NLoS Mitigation](https://arxiv.org/abs/2510.08001)
*Mohsen Ahadi,Omid Esrafilian,Florian Kaltenberger,Adeel Malik*

Main category: cs.NI

TL;DR: 提出了一种基于CIR数据增强的自监督信道制图方法，通过整合TDoA和TRP位置信息，结合短间隔UE位移测量，并引入NLoS噪声掩蔽机制，在真实5G测试环境中实现了2-4米的定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有信道制图方法在全局扩展性和处理NLoS条件引起的失真方面存在困难，需要开发能够在大规模场景下稳健工作的自监督定位方法。

Method: 利用增强的CIR数据（包含TDoA和TRP位置），结合短间隔UE位移测量，并采用NLoS噪声识别和掩蔽机制，构建自监督定位框架。

Result: 在真实5G测试环境中，90%情况下达到2-4米定位精度，优于现有半监督和自监督信道制图方法，并在不同NLoS比例下保持稳定性能。

Conclusion: 该方法通过数据增强和NLoS噪声处理机制，显著提升了信道制图在真实环境中的定位性能，为大规模自监督定位提供了可行方案。

Abstract: Channel Charting (CC) has emerged as a promising framework for data-driven
radio localization, yet existing approaches often struggle to scale globally
and to handle the distortions introduced by non-line-of-sight (NLoS)
conditions. In this work, we propose a novel CC method that leverages Channel
Impulse Response (CIR) data enriched with practical features such as Time
Difference of Arrival (TDoA) and Transmission Reception Point (TRP) locations,
enabling a self-supervised localization function on a global scale. The
proposed framework is further enhanced with short-interval User Equipment (UE)
displacement measurements, which improve the continuity and robustness of the
learned positioning function. Our algorithm incorporates a mechanism to
identify and mask NLoS-induced noisy measurements, leading to significant
performance gains. We present the evaluations of our proposed models in a real
5G testbed and benchmarked against centimeter-accurate Real-Time Kinematic
(RTK) positioning, in an O-RAN--based 5G network by OpenAirInterface (OAI)
software at EURECOM. It demonstrated outperforming results against the
state-of-the-art semi-supervised and self-supervised CC approaches in a
real-world scenario. The results show localization accuracies of 2-4 meters in
90% of cases, across a range of NLoS ratios. Furthermore, we provide public
datasets of CIR recordings, along with the true position labels used in this
paper's evaluation.

</details>


### [3] [When Light Bends to the Collective Will: A Theory and Vision for Adaptive Photonic Scale-up Domains](https://arxiv.org/abs/2510.08072)
*Vamsi Addanki*

Main category: cs.NI

TL;DR: 本文提出了一个自适应光子扩展域的理论框架，用于解决可编程光子互连在集体通信中的重构延迟与性能增益之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 随着芯片间硅光子技术的普及，集体通信成为扩展系统的关键瓶颈。可编程光子互连通过动态重构结构，能够在通信端点间建立直接的高带宽光路，但需要平衡重构延迟和自适应拓扑的性能增益。

Method: 提出了一个简单的理论框架，明确展示了重构延迟与性能增益之间的权衡。通过Birkhoff-von Neumann分解、最大并发流和集体通信的α-β成本模型之间的关联来分析问题。

Result: 该框架阐明了何时重构是值得的，并揭示了BvN分解、最大并发流和集体通信成本模型之间的强大联系。

Conclusion: 为算法设计和系统集成领域的研究议程奠定了基础，推动光子互连技术在集体通信中的进一步发展。

Abstract: As chip-to-chip silicon photonics gain traction for their bandwidth and
energy efficiency, collective communication has emerged as a critical
bottleneck in scale-up systems. Programmable photonic interconnects offer a
promising path forward: by dynamically reconfiguring the fabric, they can
establish direct, high-bandwidth optical paths between communicating endpoints
-- \emph{synchronously and guided by the structure of collective operations}
(e.g., AllReduce). However, realizing this vision -- \emph{when light bends to
the collective will} -- requires navigating a fundamental trade-off between
reconfiguration delay and the performance gains of adaptive topologies.
  In this paper, we present a simple theoretical framework for adaptive
photonic scale-up domains that makes this trade-off explicit and clarifies when
reconfiguration is worthwhile. Along the way, we highlight a connection -- not
surprising but still powerful -- between the Birkhoff--von Neumann (BvN)
decomposition, maximum concurrent flow (a classic measure of network
throughput), and the well-known $\alpha$-$\beta$ cost model for collectives.
Finally, we outline a research agenda in algorithm design and systems
integration that can build on this foundation.

</details>


### [4] [URLLC for 6G Enabled Industry 5.0: A Taxonomy of Architectures, Cross Layer Techniques, and Time Critical Applications](https://arxiv.org/abs/2510.08080)
*Abdikarim Mohamed Ibrahim,Rosdiadee Nordin,Yahya S. M. Khamayseh,Angela Amphawan,Muhammed Basheer Jasser*

Main category: cs.NI

TL;DR: 本文全面综述了6G网络支持工业5.0的超可靠低延迟通信解决方案，包括应用领域、关键技术使能器、设计挑战和性能提升，并分析了新兴方法在关键工业场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 从工业4.0向工业5.0的演进对超可靠低延迟通信提出了严格要求，6G网络旨在通过亚毫秒级端到端延迟、微秒级抖动和接近完美的可靠性来满足这些需求。

Method: 采用结构化分类法组织URLLC解决方案，包括应用领域、关键技术使能器、设计挑战和性能提升，并调查新兴方法如数字孪生集成、AI/ML资源编排、NFV服务功能链和跨域网络。

Result: 分析了代表性最新研究中延迟、可靠性、可扩展性和能源效率之间的性能权衡，并将解决方案映射到智能制造、互联医疗、自主移动、远程控制和下一代移动网络等关键工业场景。

Conclusion: 确定了开放挑战并概述了未来研究方向，旨在为工业5.0实现确定性、安全和可持续的URLLC架构。

Abstract: The evolution from Industry 4.0 to Industry 5.0 introduces stringent
requirements for ultra reliable low latency communication (URLLC) to support
human centric, intelligent, and resilient industrial systems. Sixth-generation
(6G) wireless networks aim to meet these requirements through sub-millisecond
end-to-end delays, microsecond level jitter, and near perfect reliability,
enabled by advances such as terahertz (THz) communication, reconfigurable
intelligent surfaces (RIS), multi-access edge computing (MEC), and AI driven
cross layer optimization. This paper presents a comprehensive review of URLLC
solutions for 6G enabled industry 5.0, organized into a structured taxonomy
including application domains, key technical enablers, design challenges, and
performance enhancements. The survey examines emerging approaches, including
digital twin integration, AI/ML based resource orchestration, Network Function
Virtualization (NFV) enabled service function chaining, and cross domain
networking, while mapping them to critical industrial scenarios such as smart
manufacturing, connected healthcare, autonomous mobility, remote control, and
next-generation mobile networks. Performance trade-offs between latency,
reliability, scalability, and energy efficiency are analyzed in the context of
representative state-of-the-art studies. Finally, the paper identifies open
challenges and outlines future research directions to realize deterministic,
secure, and sustainable URLLC architectures for Industry 5.0.

</details>


### [5] [BlockSDN: Towards a High-Performance Blockchain via Software-Defined Cross Networking optimization](https://arxiv.org/abs/2510.08139)
*Wenyang Jia,Jingjing Wang,Ziwei Yan,Xiangli Peng,Guohui Yuan*

Main category: cs.NI

TL;DR: BlockSDN是首个基于SDN的区块链集成架构，通过分布式控制平面、图引擎和分层广播机制，将区块链全局同步时间相比Gossip和Mercury分别减少了65%和55%。


<details>
  <summary>Details</summary>
Motivation: 现有区块链系统的可扩展性受限于低效的P2P广播，大多数优化只关注逻辑层而忽略了物理网络条件。

Method: 采用SDN架构，包含分布式控制平面获取全局网络视图、图引擎进行分层聚类、混合宏微观邻居选择与分层广播机制。

Result: 专用仿真平台显示，BlockSDN相比Gossip和Mercury分别减少了65%和55%的全局区块同步时间。

Conclusion: SDN支持的跨层协调具有显著提升区块链可扩展性和性能的潜力。

Abstract: The scalability of blockchain systems is constrained by inefficient P2P
broadcasting, as most existing optimizations focus only on the logical layer
without considering physical network conditions. To address this, we propose
BlockSDN, the first SDN-based integrated architecture for blockchain. BlockSDN
employs a distributed control plane for a global network view, a graph engine
for hierarchical clustering, and a hybrid macro-micro neighbor selection with
hierarchical broadcasting. A dedicated simulation platform shows that BlockSDN
reduces global block synchronization time by 65% and 55% compared to Gossip and
Mercury, respectively.These results highlight the potential of SDN-enabled
cross-layer coordination to significantly enhance blockchain scalability and
performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Truth-Aware Decoding: A Program-Logic Approach to Factual Language Generation](https://arxiv.org/abs/2510.07331)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.AI

TL;DR: Truth-Aware Decoding (TAD) 是一种验证导向的解码方案，通过知识库对齐神经语言生成，在解码时使用语义防护机制减少幻觉，同时保持吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决大规模经验模型与形式验证之间的鸿沟，减少语言模型生成中的幻觉问题，同时不牺牲生成效率。

Method: 基于概率程序语义的约束语义学，在解码时使用语义防护网格，结合多智能体操作演算和经过验证的Lean构件来保证实现行为。

Result: 数值和算法案例研究表明，该方法能够有效减少幻觉，同时保持吞吐量，为大规模经验模型与形式验证提供了实用桥梁。

Conclusion: TAD 提供了一种实用的方法，通过形式化验证技术增强语言模型的可靠性，在保持效率的同时显著减少事实性错误。

Abstract: This paper introduces Truth-Aware Decoding (TAD), a verification-oriented
decoding scheme that aligns neural language generation with knowledge bases.
Situated in the tradition of probabilistic program semantics for sequence
models, TAD augments modern instruction-tuned systems with a lattice of
semantic guards that operate at decode time. Our contributions are fourfold:
(i) a constraint-based semantics that renders oracle filtering as a
program-logic judgment, (ii) a proof that greedy selection enjoys local
likelihood dominance under sound and complete guards (Theorem 2.7), (iii) an
entropy-style invariant that quantifies factual risk via knowledge-aware safe
mass, and (iv) a multi-agent operational calculus with verified Lean artefacts
to certify implementation behaviour. Numerical and algorithmic case studies
confirm that the resulting guardrails reduce hallucinations without sacrificing
throughput, yielding a pragmatic bridge between large-scale empirical models
and formal verification.

</details>


### [7] [L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)](https://arxiv.org/abs/2510.07363)
*Tianxiang Xu,Zhichao Wen,Xinyu Zhao,Jun Wang,Yan Li,Chang Liu*

Main category: cs.AI

TL;DR: L2M-AID是一个基于LLM增强的多智能体强化学习框架，用于工业物联网的自主防御，通过语义感知状态表示和协作策略学习，显著提升威胁检测性能并保持物理过程稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统工业防御系统缺乏上下文感知能力，难以应对复杂的多阶段攻击，需要一种能够理解攻击意图并保持物理过程稳定的自适应安全解决方案。

Method: 结合LLM作为语义桥梁，将非结构化遥测数据转换为丰富的上下文状态表示，使用多智能体强化学习算法MAPPO学习协作策略，奖励函数平衡安全目标和操作需求。

Result: 在SWaT数据集和MITRE ATT&CK ICS合成数据集上的实验显示，检测率达到97.2%，误报率降低80%以上，响应时间提升4倍，同时保持物理过程稳定性。

Conclusion: L2M-AID为关键国家基础设施安全提供了一个强大的新范式，通过LLM和MARL的深度融合实现了语义感知的自主防御。

Abstract: The increasing integration of Industrial IoT (IIoT) exposes critical
cyber-physical systems to sophisticated, multi-stage attacks that elude
traditional defenses lacking contextual awareness. This paper introduces
L2M-AID, a novel framework for Autonomous Industrial Defense using
LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team
of collaborative agents, each driven by a Large Language Model (LLM), to
achieve adaptive and resilient security. The core innovation lies in the deep
fusion of two AI paradigms: we leverage an LLM as a semantic bridge to
translate vast, unstructured telemetry into a rich, contextual state
representation, enabling agents to reason about adversary intent rather than
merely matching patterns. This semantically-aware state empowers a Multi-Agent
Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative
strategies. The MARL reward function is uniquely engineered to balance security
objectives (threat neutralization) with operational imperatives, explicitly
penalizing actions that disrupt physical process stability. To validate our
approach, we conduct extensive experiments on the benchmark SWaT dataset and a
novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework.
Results demonstrate that L2M-AID significantly outperforms traditional IDS,
deep learning anomaly detectors, and single-agent RL baselines across key
metrics, achieving a 97.2% detection rate while reducing false positives by
over 80% and improving response times by a factor of four. Crucially, it
demonstrates superior performance in maintaining physical process stability,
presenting a robust new paradigm for securing critical national infrastructure.

</details>


### [8] [Base Models Know How to Reason, Thinking Models Learn When](https://arxiv.org/abs/2510.07364)
*Constantin Venhoff,Iván Arcuschin,Philip Torr,Arthur Conmy,Neel Nanda*

Main category: cs.AI

TL;DR: 思考模型（如DeepSeek R1）通过有效调度预训练中已存在的推理机制，而非学习全新能力，来提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究思考模型性能提升的本质：是学习全新推理能力，还是更有效地利用基础模型中已有的推理机制。

Method: 提出混合模型，在基础模型中适时激活推理机制；引入无监督、自下而上的方法发现可解释的推理行为。

Result: 混合模型在无需权重更新的情况下，恢复了思考模型91%的性能差距，仅需引导12%的token。

Conclusion: 预训练阶段模型已获得大部分推理机制，后训练阶段教会模型在合适时间高效部署这些机制。

Abstract: Why do thinking language models like DeepSeek R1 outperform their base
counterparts? Despite consistent performance gains, it remains unclear to what
extent thinking models learn entirely new reasoning capabilities or repurpose
pre-existing base model ones. In this work, we propose a hybrid model where we
activate reasoning mechanisms in base models at the right time to elicit
thinking-model-level reasoning chains, implying that thinking models exploit
already existing capabilities. To ground our analysis, we introduce an
unsupervised, bottom-up approach for uncovering human-interpretable reasoning
behaviors in thinking models. This approach provides an unbiased method to
discover reasoning behaviors without imposing manual or LLM-derived
assumptions. Across three base and four thinking models, using GSM8K and
MATH500, our hybrid model recovers up to 91% of the performance gap to thinking
models without any weight updates while steering only 12% of tokens.
Concretely, our empirical setup provides a simple, causal way to test the
effectiveness of existing reasoning mechanisms in base models by invoking them
directly and measuring the resulting task performance. More broadly, these
results reframe our understanding of how thinking models are trained:
pre-training is when models acquire most of their reasoning mechanisms, and
post-training teaches efficient deployment of these mechanisms at the right
time, enabling efficient use of their inference-time compute.

</details>


### [9] [The Tournament Tree Method for preference elicitation in Multi-criteria decision-making](https://arxiv.org/abs/2510.08197)
*Diego García-Zamora,Álvaro Labella,José Rui Figueira*

Main category: cs.AI

TL;DR: 提出锦标赛树方法(TTM)，一种新的偏好关系获取和评估框架，只需m-1次两两比较就能获得完整、互反且一致的比较矩阵，显著降低认知负荷和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统两两比较方法需要m(m-1)/2次比较，认知负荷高且容易产生不一致性，计算复杂度也高，限制了其在实际决策中的应用。

Method: TTM方法包含三个阶段：(i)使用减少的目标比较集获取专家判断，(ii)构建一致的两两比较矩阵，(iii)从结果矩阵推导全局价值尺度。该方法通过设计确保一致性。

Result: TTM将偏好建模的维度从m(m-1)/2减少到m个参数，与经典卡片法兼容，能够处理区间和比率尺度，并开发了基于Web的工具展示其实际适用性。

Conclusion: 锦标赛树方法有效克服了传统两两比较方法的局限性，确保一致性，最小化认知努力，并在实际决策场景中具有实用价值。

Abstract: Pairwise comparison methods, such as Fuzzy Preference Relations and Saaty's
Multiplicative Preference Relations, are widely used to model expert judgments
in multi-criteria decision-making. However, their application is limited by the
high cognitive load required to complete $m(m-1)/2$ comparisons, the risk of
inconsistency, and the computational complexity of deriving consistent value
scales. This paper proposes the Tournament Tree Method (TTM), a novel
elicitation and evaluation framework that overcomes these limitations. The TTM
requires only $m-1$ pairwise comparisons to obtain a complete, reciprocal, and
consistent comparison matrix. The method consists of three phases: (i)
elicitation of expert judgments using a reduced set of targeted comparisons,
(ii) construction of the consistent pairwise comparison matrix, and (iii)
derivation of a global value scale from the resulting matrix. The proposed
approach ensures consistency by design, minimizes cognitive effort, and reduces
the dimensionality of preference modeling from $m(m-1)/2$ to $m$ parameters.
Furthermore, it is compatible with the classical Deck of Cards method, and thus
it can handle interval and ratio scales. We have also developed a web-based
tool that demonstrates its practical applicability in real decision-making
scenarios.

</details>


### [10] [Position: AI Will Transform Neuropsychology Through Mental Health Digital Twins for Dynamic Mental Health Care, Especially for ADHD](https://arxiv.org/abs/2510.07409)
*Neil Natarajan,Sruthi Viswanathan,Xavier Roberts-Gaal,Michelle Marie Martel*

Main category: cs.AI

TL;DR: 该论文主张从静态心理健康诊断转向AI驱动的连续评估，以ADHD为例，探讨生成式AI如何解决神经心理学容量限制，实现个性化和纵向护理路径。


<details>
  <summary>Details</summary>
Motivation: 静态解决方案无法满足动态思维需求，当前心理健康诊断存在容量限制，需要更个性化和连续的评估方法。

Method: 使用生成式AI进行频繁的低水平体验采样，促进诊断协调，并引入心理健康数字孪生(MHDTs)作为连续更新的计算模型框架。

Result: AI能够高效进行患者体验采样，为个性化心理健康护理提供丰富数据基础，改善治疗的可及性和有效性。

Conclusion: AI驱动的连续评估和数字孪生框架有望变革心理健康护理，使其能够动态适应个体需求和病情变化。

Abstract: Static solutions don't serve a dynamic mind. Thus, we advocate a shift from
static mental health diagnostic assessments to continuous, artificial
intelligence (AI)-driven assessment. Focusing on
Attention-Deficit/Hyperactivity Disorder (ADHD) as a case study, we explore how
generative AI has the potential to address current capacity constraints in
neuropsychology, potentially enabling more personalized and longitudinal care
pathways. In particular, AI can efficiently conduct frequent, low-level
experience sampling from patients and facilitate diagnostic reconciliation
across care pathways. We envision a future where mental health care benefits
from continuous, rich, and patient-centered data sampling to dynamically adapt
to individual patient needs and evolving conditions, thereby improving both
accessibility and efficacy of treatment. We further propose the use of mental
health digital twins (MHDTs) - continuously updated computational models that
capture individual symptom dynamics and trajectories - as a transformative
framework for personalized mental health care. We ground this framework in
empirical evidence and map out the research agenda required to refine and
operationalize it.

</details>


### [11] [ProSEA: Problem Solving via Exploration Agents](https://arxiv.org/abs/2510.07423)
*William Nguyen,Vinh Luong,Christopher Nguyen*

Main category: cs.AI

TL;DR: ProSEA是一个模块化多智能体框架，通过探索和计划演化实现迭代问题解决，在复杂任务中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体局限于静态规划和脆弱交互，缺乏真正的协作或自适应推理能力

Method: 采用分层架构，管理器智能体协调领域专家智能体，基于失败尝试的结构化反馈进行任务分解和自适应重新规划

Result: 在FinanceBench基准测试中，即使没有人类反馈，ProSEA也超越了最先进的基线方法，在推理密集型任务中表现出稳健性能

Conclusion: ProSEA为构建更透明、自适应且与人类对齐的AI智能体提供了有前景的基础

Abstract: Large language models (LLMs) have empowered AI agents to tackle increasingly
complex tasks. However, most existing agents remain limited to static planning
and brittle interactions, falling short of true collaboration or adaptive
reasoning. We introduce ProSEA, a modular, general-purpose multi-agent
framework designed for iterative problem solving through exploration and plan
evolution. ProSEA features a hierarchical architecture in which a Manager Agent
orchestrates domain-specialized Expert Agents, decomposes tasks, and adaptively
replans based on structured feedback from failed attempts. Unlike prior
systems, ProSEA agents report not only success or failure but also detailed
reasons for failure and newly discovered constraints, enabling dynamic plan
refinement informed by exploratory traces. The framework operates autonomously
but supports seamless integration with human collaborators when needed.
Experiments on the challenging FinanceBench benchmark demonstrate that ProSEA,
even without human feedback, outperforms state-of-the-art baselines and
achieves robust performance across reasoning-heavy tasks. These results
underscore ProSEA's potential as a foundation for more transparent, adaptive,
and human-aligned AI agents.

</details>


### [12] [Less is More: Strategic Expert Selection Outperforms Ensemble Complexity in Traffic Forecasting](https://arxiv.org/abs/2510.07426)
*Walid Guettala,Yufan Zhao,László Gulyás*

Main category: cs.AI

TL;DR: TESTAM+是一个增强的时空交通预测框架，通过引入新的SpatioSemantic专家，将物理道路拓扑与数据驱动的特征相似性相结合，在METR LA和PEMS BAY数据集上显著优于TESTAM，并发现战略专家选择优于朴素集成方法。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测方法如TESTAM缺乏对物理道路网络拓扑的显式建模，限制了其空间建模能力。

Method: 提出TESTAM+框架，引入SpatioSemantic专家，通过混合图构建将物理道路拓扑与数据驱动的特征相似性相结合。

Result: 在METR LA上MAE降低1.3%（3.10 vs 3.14），PEMS BAY上提升4.1%（1.65 vs 1.72）；Identity + Adaptive配置在METR LA上相比MegaCRN降低11.5% MAE（2.99 vs 3.38），推理延迟比完整四专家TESTAM+减少53.1%。

Conclusion: 战略性设计的少量专家优于复杂的多专家集成，在实现最新性能的同时具有优越的计算效率，适合实时部署。

Abstract: Traffic forecasting is fundamental to intelligent transportation systems,
enabling congestion mitigation and emission reduction in increasingly complex
urban environments. While recent graph neural network approaches have advanced
spatial temporal modeling, existing mixture of experts frameworks like Time
Enhanced Spatio Temporal Attention Model (TESTAM) lack explicit incorporation
of physical road network topology, limiting their spatial capabilities. We
present TESTAM+, an enhanced spatio temporal forecasting framework that
introduces a novel SpatioSemantic Expert integrating physical road topology
with data driven feature similarity through hybrid graph construction. TESTAM+
achieves significant improvements over TESTAM: 1.3% MAE reduction on METR LA
(3.10 vs. 3.14) and 4.1% improvement on PEMS BAY (1.65 vs. 1.72). Through
comprehensive ablation studies, we discover that strategic expert selection
fundamentally outperforms naive ensemble aggregation. Individual experts
demonstrate remarkable effectiveness: the Adaptive Expert achieves 1.63 MAE on
PEMS BAY, outperforming the original three expert TESTAM (1.72 MAE), while the
SpatioSemantic Expert matches this performance with identical 1.63 MAE. The
optimal Identity + Adaptive configuration achieves an 11.5% MAE reduction
compared to state of the art MegaCRN on METR LA (2.99 vs. 3.38), while reducing
inference latency by 53.1% compared to the full four expert TESTAM+. Our
findings reveal that fewer, strategically designed experts outperform complex
multi expert ensembles, establishing new state of the art performance with
superior computational efficiency for real time deployment.

</details>


### [13] [TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering](https://arxiv.org/abs/2510.07432)
*Penghang Liu,Elizabeth Fons,Svitlana Vyetrenko,Daniel Borrajo,Vamsi Potluru,Manuela Veloso*

Main category: cs.AI

TL;DR: TS-Agent是一个时间序列推理代理，通过将LLM的推理能力与时间序列分析工具相结合，避免多模态对齐训练，在保持可解释性的同时显著提升时间序列推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在时间序列推理任务中表现不佳，存在幻觉和知识泄露问题，需要一种能充分利用LLM推理能力同时避免其缺陷的解决方案。

Method: 设计TS-Agent代理，让LLM专注于证据收集和推理合成，将统计和结构信息提取委托给时间序列分析工具，通过原子操作符与原始数值序列交互，并采用自批判和最终质量门控机制迭代优化推理过程。

Result: 在标准基准测试中，TS-Agent在理解任务上达到最先进LLM水平，在推理任务上显著优于现有模型，特别是在零样本设置下表现突出。

Conclusion: TS-Agent通过分离LLM的推理优势与时间序列分析的专业能力，有效解决了时间序列推理中的幻觉和知识泄露问题，为时间序列分析提供了可解释且可验证的解决方案。

Abstract: Large language models (LLMs) have shown strong abilities in reasoning and
problem solving, but recent studies reveal that they still struggle with time
series reasoning tasks, where outputs are often affected by hallucination or
knowledge leakage. In this work we propose TS-Agent, a time series reasoning
agent that leverages LLMs strictly for what they excel at, i.e., gathering
evidence and synthesizing it into conclusions through step-by-step reasoning,
while delegating the extraction of statistical and structural information to
time series analytical tools. Instead of mapping time series into text tokens,
images, or embeddings, our agent interacts with raw numeric sequences through
atomic operators, records outputs in an explicit evidence log, and iteratively
refines its reasoning under the guidance of a self-critic and a final quality
gate. This design avoids multi-modal alignment training, preserves the native
form of time series, ensures interpretability and verifiability, and mitigates
knowledge leakage or hallucination. Empirically, we evaluate the agent on
established benchmarks. Our experiments show that TS-Agent achieves performance
comparable to state-of-the-art LLMs on understanding benchmarks, and delivers
significant improvements on reasoning tasks, where existing models often rely
on memorization and fail in zero-shot settings.

</details>


### [14] [ExpertAgent: Enhancing Personalized Education through Dynamic Planning and Retrieval-Augmented Long-Chain Reasoning](https://arxiv.org/abs/2510.07456)
*Binrong Zhu,Guiran Liu,Nina Jiang*

Main category: cs.AI

TL;DR: 提出了ExpertAgent智能代理框架，通过动态规划学习内容和策略，基于持续更新的学生模型提供个性化教育，减少大语言模型的幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在教育中缺乏实时适应性、个性化和内容可靠性的问题。

Method: 开发ExpertAgent智能代理框架，基于验证的课程知识库，通过动态规划学习内容和策略，结合持续更新的学生模型。

Result: 实现了高度自适应的学习体验，提供可靠的知识内容，有效减少大语言模型的幻觉风险。

Conclusion: ExpertAgent框架能够克服传统静态学习内容的限制，提供优化的实时教学策略和学习体验，提高教育AI的可靠性和可信度。

Abstract: The application of advanced generative artificial intelligence in education
is often constrained by the lack of real-time adaptability, personalization,
and reliability of the content. To address these challenges, we propose
ExpertAgent - an intelligent agent framework designed for personalized
education that provides reliable knowledge and enables highly adaptive learning
experiences. Therefore, we developed ExpertAgent, an innovative learning agent
that provides users with a proactive and personalized learning experience.
ExpertAgent dynamic planning of the learning content and strategy based on a
continuously updated student model. Therefore, overcoming the limitations of
traditional static learning content to provide optimized teaching strategies
and learning experience in real time. All instructional content is grounded in
a validated curriculum repository, effectively reducing hallucination risks in
large language models and improving reliability and trustworthiness.

</details>


### [15] [Evaluation of LLMs for Process Model Analysis and Optimization](https://arxiv.org/abs/2510.07489)
*Akhil Kumar,Jianliang Leon Zhao,Om Dobariya*

Main category: cs.AI

TL;DR: 研究发现，未经训练的LLM（如ChatGPT o3模型）能够有效理解BPMN流程模型图像，并在语法、逻辑和语义层面智能回答相关问题。不同LLM在准确性和有效性方面表现各异，但都能作为业务流程设计者和用户的有价值助手。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在交互式对话中理解流程模型、发现语法和逻辑错误，并通过自然语言界面进行深度推理的能力。

Method: 使用多个LLM（包括ChatGPT o3模型）在零样本设置下测试其对BPMN流程模型图像的理解能力，评估其在语法、逻辑和语义层面的表现。

Result: 未经训练的LLM能够有效理解BPMN流程模型并智能回答相关问题，不同LLM在准确性和有效性方面存在差异，LLM表现出拟人化特性。

Conclusion: LLM可以作为业务流程设计者和用户的有价值助手，在流程分析和优化中展现出深度推理能力。

Abstract: In this paper, we report our experience with several LLMs for their ability
to understand a process model in an interactive, conversational style, find
syntactical and logical errors in it, and reason with it in depth through a
natural language (NL) interface. Our findings show that a vanilla, untrained
LLM like ChatGPT (model o3) in a zero-shot setting is effective in
understanding BPMN process models from images and answering queries about them
intelligently at syntactic, logic, and semantic levels of depth. Further,
different LLMs vary in performance in terms of their accuracy and
effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a
valuable role as assistants for business process designers and users. We also
study the LLM's "thought process" and ability to perform deeper reasoning in
the context of process analysis and optimization. We find that the LLMs seem to
exhibit anthropomorphic properties.

</details>


### [16] [Optimizing Ethical Risk Reduction for Medical Intelligent Systems with Constraint Programming](https://arxiv.org/abs/2510.07491)
*Clotilde Brayé,Aurélien Bricout,Arnaud Gotlieb,Nadjib Lazaar,Quentin Vallet*

Main category: cs.AI

TL;DR: 该论文研究了医疗智能系统的伦理风险优化问题，提出了基于MIP、SAT和CP三种求解范式的数学建模方法，并通过实验比较了各方法的性能。


<details>
  <summary>Details</summary>
Motivation: 随着医疗智能系统被归类为高风险系统，需要建立正式的风险管理流程来确保符合可信AI的伦理要求，因此需要研究风险降低优化问题。

Method: 将伦理风险优化问题形式化为约束优化任务，使用Minizinc约束建模语言进行建模，并比较了混合整数规划、可满足性和约束规划三种求解方法。

Result: 通过实验研究分析了三种方法的性能、表达能力和可扩展性，识别了方法的局限性。

Conclusion: 提出了将Minizinc模型整合到完整的可信AI伦理风险管理流程中的未来工作展望。

Abstract: Medical Intelligent Systems (MIS) are increasingly integrated into healthcare
workflows, offering significant benefits but also raising critical safety and
ethical concerns. According to the European Union AI Act, most MIS will be
classified as high-risk systems, requiring a formal risk management process to
ensure compliance with the ethical requirements of trustworthy AI. In this
context, we focus on risk reduction optimization problems, which aim to reduce
risks with ethical considerations by finding the best balanced assignment of
risk assessment values according to their coverage of trustworthy AI ethical
requirements. We formalize this problem as a constrained optimization task and
investigate three resolution paradigms: Mixed Integer Programming (MIP),
Satisfiability (SAT), and Constraint Programming(CP).Our contributions include
the mathematical formulation of this optimization problem, its modeling with
the Minizinc constraint modeling language, and a comparative experimental study
that analyzes the performance, expressiveness, and scalability of each approach
to solving. From the identified limits of the methodology, we draw some
perspectives of this work regarding the integration of the Minizinc model into
a complete trustworthy AI ethical risk management process for MIS.

</details>


### [17] [CompassLLM: A Multi-Agent Approach toward Geo-Spatial Reasoning for Popular Path Query](https://arxiv.org/abs/2510.07516)
*Md. Nazmul Islam Ananto,Shamit Fatin,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: CompassLLM是一个基于大语言模型的多智能体框架，用于从历史轨迹数据中识别热门路径，包含搜索现有路径和生成新路径两个阶段。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要模型训练和参数调优，而大语言模型在空间和图推理方面能力不断增强，有望应用于地理空间问题。

Method: 采用多智能体框架，包含SEARCH阶段识别热门路径和GENERATE阶段在缺乏历史数据时合成新路径。

Result: 在真实和合成数据集上的实验表明，CompassLLM在SEARCH阶段具有优越的准确性，在GENERATE阶段表现有竞争力且成本效益高。

Conclusion: CompassLLM成功将大语言模型的推理能力应用于地理空间领域，为热门路径查询提供了有效的解决方案。

Abstract: The popular path query - identifying the most frequented routes between
locations from historical trajectory data - has important applications in urban
planning, navigation optimization, and travel recommendations. While
traditional algorithms and machine learning approaches have achieved success in
this domain, they typically require model training, parameter tuning, and
retraining when accommodating data updates. As Large Language Models (LLMs)
demonstrate increasing capabilities in spatial and graph-based reasoning, there
is growing interest in exploring how these models can be applied to geo-spatial
problems.
  We introduce CompassLLM, a novel multi-agent framework that intelligently
leverages the reasoning capabilities of LLMs into the geo-spatial domain to
solve the popular path query. CompassLLM employs its agents in a two-stage
pipeline: the SEARCH stage that identifies popular paths, and a GENERATE stage
that synthesizes novel paths in the absence of an existing one in the
historical trajectory data. Experiments on real and synthetic datasets show
that CompassLLM demonstrates superior accuracy in SEARCH and competitive
performance in GENERATE while being cost-effective.

</details>


### [18] [Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization](https://arxiv.org/abs/2510.07517)
*Hyeong Kyu Choi,Xiaojin Zhu,Yixuan Li*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning
by letting multiple agents exchange answers and then aggregate their opinions.
Yet recent studies reveal that agents are not neutral: they are prone to
identity-driven sycophancy and self-bias, uncritically adopting a peer's view
or stubbornly adhering to their own prior output, undermining the reliability
of debate. In this work, we present the first principled framework that joins
sycophancy and self-bias to mitigate and quantify identity bias in MAD. First,
we formalize the debate dynamics as an identity-weighted Bayesian update
process. Second, we propose response anonymization: by removing identity
markers from prompts, agents cannot distinguish "self" from "peer", which
forces equal weights on agent identity, thereby reducing bias. Third, we define
the Identity Bias Coefficient (IBC), a principled metric that measures how
often an agent follows a peer versus itself. Empirical studies across multiple
models, datasets and debate rounds confirm that identity bias is widespread,
with sycophancy far more common than self-bias. Our findings highlight the need
to "mask" identity to ensure that MAD systems reason based on content rather
than source identity. Code is released in
https://github.com/deeplearning-wisc/MAD-identity-bias.

</details>


### [19] [An Evaluation Study of Hybrid Methods for Multilingual PII Detection](https://arxiv.org/abs/2510.07551)
*Harshit Rajgarhia,Suryam Gupta,Asif Shaik,Gulipalli Praveen Kumar,Y Santhoshraj,Sanka Nithya Tanvy Nishitha,Abhishek Mukherji*

Main category: cs.AI

TL;DR: RECAP是一个混合框架，结合确定性正则表达式和上下文感知大语言模型，用于在13种低资源语言中检测个人身份信息，性能优于微调NER模型82%和零样本LLM 17%。


<details>
  <summary>Details</summary>
Motivation: 低资源语言中的PII检测面临语言多样性和标注数据有限的挑战，需要可扩展的解决方案来满足隐私合规要求。

Method: 采用混合方法，结合正则表达式和LLM，通过三阶段精炼流程进行消歧和过滤，模块化设计支持300多种实体类型而无需重新训练。

Result: 使用nervaluate基准测试，RECAP在加权F1分数上比微调NER模型高82%，比零样本LLM高17%。

Conclusion: RECAP为合规应用中的PII检测提供了一个可扩展且适应性强的解决方案。

Abstract: The detection of Personally Identifiable Information (PII) is critical for
privacy compliance but remains challenging in low-resource languages due to
linguistic diversity and limited annotated data. We present RECAP, a hybrid
framework that combines deterministic regular expressions with context-aware
large language models (LLMs) for scalable PII detection across 13 low-resource
locales. RECAP's modular design supports over 300 entity types without
retraining, using a three-phase refinement pipeline for disambiguation and
filtering. Benchmarked with nervaluate, our system outperforms fine-tuned NER
models by 82% and zero-shot LLMs by 17% in weighted F1-score. This work offers
a scalable and adaptable solution for efficient PII detection in
compliance-focused applications.

</details>


### [20] [Benchmarking is Broken -- Don't Let AI be its Own Judge](https://arxiv.org/abs/2510.07575)
*Zerui Cheng,Stella Wohnig,Ruchika Gupta,Samiul Alam,Tassallah Abdullahi,João Alves Ribeiro,Christian Nielsen-Garcia,Saif Mir,Siran Li,Jason Orender,Seyed Ali Bahrainian,Daniel Kirste,Aaron Gokaslan,Mikołaj Glinka,Carsten Eickhoff,Ruben Wolff*

Main category: cs.AI

TL;DR: 本文主张AI评估需要范式转变，提出PeerBench框架来解决当前基准测试的系统性缺陷，通过密封执行、题库滚动更新和延迟透明等机制确保评估的可信度。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估存在数据污染、选择性报告等系统性缺陷，导致难以区分真实进展与夸大宣传，侵蚀公众信任，亟需建立统一可信的评估范式。

Method: 提出PeerBench框架，采用社区治理、监考式评估，通过密封执行防止数据泄露、题库银行滚动更新避免过时、延迟透明防止针对性优化。

Result: 构建了一个能够系统性解决当前评估问题的蓝图，为建立真正可信的AI进展衡量标准铺平道路。

Conclusion: 当前放任自流的AI评估方式不可持续，必须转向统一、实时、质量可控的基准测试框架，PeerBench为此提供了可行的实施路径。

Abstract: The meteoric rise of Artificial Intelligence (AI), with its rapidly expanding
market capitalization, presents both transformative opportunities and critical
challenges. Chief among these is the urgent need for a new, unified paradigm
for trustworthy evaluation, as current benchmarks increasingly reveal critical
vulnerabilities. Issues like data contamination and selective reporting by
model developers fuel hype, while inadequate data quality control can lead to
biased evaluations that, even if unintentionally, may favor specific
approaches. As a flood of participants enters the AI space, this "Wild West" of
assessment makes distinguishing genuine progress from exaggerated claims
exceptionally difficult. Such ambiguity blurs scientific signals and erodes
public confidence, much as unchecked claims would destabilize financial markets
reliant on credible oversight from agencies like Moody's.
  In high-stakes human examinations (e.g., SAT, GRE), substantial effort is
devoted to ensuring fairness and credibility; why settle for less in evaluating
AI, especially given its profound societal impact? This position paper argues
that the current laissez-faire approach is unsustainable. We contend that true,
sustainable AI advancement demands a paradigm shift: a unified, live, and
quality-controlled benchmarking framework robust by construction, not by mere
courtesy and goodwill. To this end, we dissect the systemic flaws undermining
today's AI evaluation, distill the essential requirements for a new generation
of assessments, and introduce PeerBench, a community-governed, proctored
evaluation blueprint that embodies this paradigm through sealed execution, item
banking with rolling renewal, and delayed transparency. Our goal is to pave the
way for evaluations that can restore integrity and deliver genuinely
trustworthy measures of AI progress.

</details>


### [21] [AgentAsk: Multi-Agent Systems Need to Ask](https://arxiv.org/abs/2510.07593)
*Bohan Lin,Kuo Yang,Yingchuan Lai,Yudong Zhang,Chen Zhang,Guibin Zhang,Xinlei Yu,Miao Yu,Xu Wang,Yang Wang*

Main category: cs.AI

TL;DR: AgentAsk是一个轻量级即插即用的澄清模块，通过插入必要问题来阻止多智能体系统中消息传递错误的传播，显著提升准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统虽然承诺通过协作分工增强问题解决能力，但经常表现不如单智能体基线，原因是边缘级错误级联：一个消息传递中的小错误会在整个链中传播。

Method: AgentAsk采用三阶段流程：(i)从整理的失败轨迹中提取边缘级判断形成紧凑策略，(ii)监督策略决定何时/问什么/问谁/如何问，(iii)使用E-GRPO强化学习目标在线优化，平衡准确性、延迟和成本。

Result: 在数学、推理和编码基准测试中，AgentAsk持续提升公共多智能体实现的准确性和鲁棒性，同时保持最小开销，延迟和额外成本均低于5%，接近强评估器的性能。

Conclusion: 除了经验改进外，本文贡献了边缘级错误的系统分类和链接局部干预的实用方法，为构建更可靠的基于LLM的多智能体系统提供了可扩展路径。

Abstract: Multi-agent systems built on large language models (LLMs) promise enhanced
problem-solving capabilities through collaborative division of labor. However,
they frequently underperform single-agent baselines due to edge-level error
cascades: minor inaccuracies at one message handoff propagate across the entire
chain. We propose AgentAsk, a lightweight and plug-and-play clarification
module that treats every inter-agent message as a potential failure point and
inserts minimally necessary questions to arrest error propagation. AgentAsk
follows a three-stage pipeline: (i) distilling edge-level judgments from
curated failure traces into a compact policy, (ii) supervising the policy to
determine when/what/whom/how to ask, and (iii) optimizing online with E-GRPO, a
reinforcement learning objective that balances accuracy, latency, and cost. The
module is architecture-agnostic and easy to integrate into existing
orchestration. Across math, reasoning, and coding benchmarks, AgentAsk
consistently improves accuracy and robustness over public multi-agent
implementations while keeping overhead minimal, with latency and extra cost all
less than 5%, approaching the performance of a strong evaluator. Beyond
empirical improvements, we contribute a principled taxonomy of edge-level
errors and a practical recipe for link-local intervention, offering a scalable
pathway toward more reliable LLM-based multi-agent systems.

</details>


### [22] [Traceability and Accountability in Role-Specialized Multi-Agent LLM Pipelines](https://arxiv.org/abs/2510.07614)
*Amine Barrak*

Main category: cs.AI

TL;DR: 提出了一个可追踪和可问责的LLM多智能体系统，通过结构化交接机制显著提高准确性，量化了不同角色的性能特点，并提供了设计可靠多智能体系统的实用方法。


<details>
  <summary>Details</summary>
Motivation: 现有的顺序多智能体系统难以信任，因为错误会在各阶段间静默传递。需要建立可追踪和可问责的管道系统来识别错误来源和分配责任。

Method: 采用Planner -> Executor -> Critic的三阶段管道，评估8种配置的三种最先进LLM在三个基准测试上的表现，分析错误起源、传播和修复方式。

Result: 结构化交接显著提高准确性；不同模型在特定角色上有明确优势和风险；准确度-成本-延迟权衡是任务相关的，异构管道通常最有效。

Conclusion: 提供了一种实用的数据驱动方法来设计、追踪和调试可靠、可预测和可问责的多智能体系统。

Abstract: Sequential multi-agent systems built with large language models (LLMs) can
automate complex software tasks, but they are hard to trust because errors
quietly pass from one stage to the next. We study a traceable and accountable
pipeline, meaning a system with clear roles, structured handoffs, and saved
records that let us trace who did what at each step and assign blame when
things go wrong. Our setting is a Planner -> Executor -> Critic pipeline. We
evaluate eight configurations of three state-of-the-art LLMs on three
benchmarks and analyze where errors start, how they spread, and how they can be
fixed. Our results show: (1) adding a structured, accountable handoff between
agents markedly improves accuracy and prevents the failures common in simple
pipelines; (2) models have clear role-specific strengths and risks (e.g.,
steady planning vs. high-variance critiquing), which we quantify with repair
and harm rates; and (3) accuracy-cost-latency trade-offs are task-dependent,
with heterogeneous pipelines often the most efficient. Overall, we provide a
practical, data-driven method for designing, tracing, and debugging reliable,
predictable, and accountable multi-agent systems.

</details>


### [23] [A Case for Leveraging Generative AI to Expand and Enhance Training in the Provision of Mental Health Services](https://arxiv.org/abs/2510.07623)
*Hannah R. Lawrence,Shannon Wiltsey Stirman,Samuel Dorison,Taedong Yun,Megan Jones Bell*

Main category: cs.AI

TL;DR: 本文主张将生成式AI应用于心理健康服务培训，而非直接用于治疗聊天机器人，认为这是更低风险、高影响力的应用场景。


<details>
  <summary>Details</summary>
Motivation: 当前AI在心理健康领域的投资和讨论过度关注治疗聊天机器人，但作者认为生成式AI在培训心理健康服务提供者方面具有更大潜力且风险更低。

Method: 通过理论分析生成式AI在心理健康培训中的关键优势，并提供了一个真实案例研究：使用生成式AI改进退伍军人相互支持的心理健康培训。

Result: 生成式AI能够有效增强和扩大心理健康服务培训的规模，在退伍军人培训案例中取得了积极效果。

Conclusion: 应该优先投资于使用生成式AI来支持心理健康服务提供者的培训，这比直接应用于治疗聊天机器人更具可行性和安全性。

Abstract: Generative artificial intelligence (Generative AI) is transforming
healthcare. With this evolution comes optimism regarding the impact it will
have on mental health, as well as concern regarding the risks that come with
generative AI operating in the mental health domain. Much of the investment in,
and academic and public discourse about, AI-powered solutions for mental health
has focused on therapist chatbots. Despite the common assumption that chatbots
will be the most impactful application of GenAI to mental health, we make the
case here for a lower-risk, high impact use case: leveraging generative AI to
enhance and scale training in mental health service provision. We highlight key
benefits of using generative AI to help train people to provide mental health
services and present a real-world case study in which generative AI improved
the training of veterans to support one another's mental health. With numerous
potential applications of generative AI in mental health, we illustrate why we
should invest in using generative AI to support training people in mental
health service provision.

</details>


### [24] [Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models](https://arxiv.org/abs/2510.07632)
*Yinglun Zhu,Jiancheng Zhang,Fuzhi Tang*

Main category: cs.AI

TL;DR: 论文发现现有评估指标系统性地低估了AI模型的组合推理能力，提出了组匹配分数和测试时匹配(TTM)算法，显著提升了模型性能，在多个基准测试中达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型在组合推理方面表现不佳，但研究发现这是因为现有评估指标存在系统性低估问题，需要新的评估方法和改进策略来揭示模型的真实能力。

Method: 引入组匹配分数来更好地利用组结构，并提出测试时匹配(TTM)算法——一种无需外部监督的迭代自改进方法。

Result: 新方法显著提升了模型性能：SigLIP-B16超越GPT-4.1，GPT-4.1在Winoground上首次超越人类估计性能，在16个数据集变体上均获得持续改进。

Conclusion: 通过改进评估方法和引入TTM算法，可以显著提升AI模型的组合推理能力，揭示了模型被低估的潜力，推动了组合推理研究的前沿。

Abstract: Frontier AI models have achieved remarkable progress, yet recent studies
suggest they struggle with compositional reasoning, often performing at or
below random chance on established benchmarks. We revisit this problem and show
that widely used evaluation metrics systematically underestimate model
capability. To address this, we introduce a group matching score that better
exploits group structure and reveals substantial hidden capability in both
contrastive vision-language models (VLMs) and multimodal large language models
(MLLMs). Moreover, simply overfitting to the induced group matchings at test
time transfers this hidden capability into higher scores under standard
evaluation metrics, closing much of the reported gap. This adjustment enables
SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first
result surpassing estimated human performance on Winoground.
  Building on this insight, we propose Test-Time Matching (TTM), an iterative,
self-improving algorithm that further bootstraps model performance without any
external supervision. TTM delivers additional, non-trivial improvements: for
example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a
new state of the art. Importantly, TTM remains broadly effective even on
benchmarks without metric-induced effects or group structures, achieving
relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16
dataset variants spanning diverse setups, our experiments demonstrate that TTM
consistently improves model performance and advances the frontier of
compositional reasoning.

</details>


### [25] [Safely Exploring Novel Actions in Recommender Systems via Deployment-Efficient Policy Learning](https://arxiv.org/abs/2510.07635)
*Haruka Kiyohara,Yusuke Narita,Yuta Saito,Kei Tateno,Takuma Udagawa*

Main category: cs.AI

TL;DR: 提出Safe OPG方法确保推荐系统安全探索新物品，并开发DEPLOY框架平衡安全性与探索性


<details>
  <summary>Details</summary>
Motivation: 现有离线策略学习方法在存在新物品时不够安全，需要开发能保证安全性的探索框架

Method: 首先开发Safe OPG方法，基于高置信度离线策略评估；然后提出DEPLOY框架，利用安全边际并在多次部署中逐步放松安全正则化

Result: Safe OPG几乎总能满足安全要求，但过于保守；DEPLOY框架能在保证安全的同时实现有效探索

Conclusion: 所提框架能够在保证推荐系统安全实施的前提下，有效探索新物品

Abstract: In many real recommender systems, novel items are added frequently over time.
The importance of sufficiently presenting novel actions has widely been
acknowledged for improving long-term user engagement. A recent work builds on
Off-Policy Learning (OPL), which trains a policy from only logged data,
however, the existing methods can be unsafe in the presence of novel actions.
Our goal is to develop a framework to enforce exploration of novel actions with
a guarantee for safety. To this end, we first develop Safe Off-Policy Policy
Gradient (Safe OPG), which is a model-free safe OPL method based on a high
confidence off-policy evaluation. In our first experiment, we observe that Safe
OPG almost always satisfies a safety requirement, even when existing methods
violate it greatly. However, the result also reveals that Safe OPG tends to be
too conservative, suggesting a difficult tradeoff between guaranteeing safety
and exploring novel actions. To overcome this tradeoff, we also propose a novel
framework called Deployment-Efficient Policy Learning for Safe User
Exploration, which leverages safety margin and gradually relaxes safety
regularization during multiple (not many) deployments. Our framework thus
enables exploration of novel actions while guaranteeing safe implementation of
recommender systems.

</details>


### [26] [Multimodal Safety Evaluation in Generative Agent Social Simulations](https://arxiv.org/abs/2510.07709)
*Alhim Vera,Karen Sanchez,Carlos Hinojosa,Haidar Bin Hamid,Donghoon Kim,Bernard Ghanem*

Main category: cs.AI

TL;DR: 论文提出了一个可复现的仿真框架来评估多模态环境中生成智能体的安全性、一致性和信任度，发现当前智能体在纠正不安全计划方面成功率仅为55%，且在误导性视觉信息面前有45%的不安全行为被接受。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言和视觉语言模型使智能体能够在丰富环境中自主行动，但它们在跨模态安全性、一致性和信任推理方面的能力仍然有限，需要系统评估。

Method: 引入可复现的仿真框架，配备分层记忆、动态规划、多模态感知能力，并使用SocialMetrics行为指标套件量化计划修订、不安全到安全转换以及网络信息扩散。

Result: 实验显示智能体在多风险场景中表现最差（GPT-4o mini仅20%成功率），在局部化场景中表现最佳（Claude在火灾/高温场景达98%成功率）。45%的不安全行为在误导性视觉信息下被接受。

Conclusion: 当前架构在多模态安全性和一致性方面存在严重局限性，该框架为研究多模态安全、一致性和社交动态提供了可复现平台。

Abstract: Can generative agents be trusted in multimodal environments? Despite advances
in large language and vision-language models that enable agents to act
autonomously and pursue goals in rich settings, their ability to reason about
safety, coherence, and trust across modalities remains limited. We introduce a
reproducible simulation framework for evaluating agents along three dimensions:
(1) safety improvement over time, including iterative plan revisions in
text-visual scenarios; (2) detection of unsafe activities across multiple
categories of social situations; and (3) social dynamics, measured as
interaction counts and acceptance ratios of social exchanges. Agents are
equipped with layered memory, dynamic planning, multimodal perception, and are
instrumented with SocialMetrics, a suite of behavioral and structural metrics
that quantifies plan revisions, unsafe-to-safe conversions, and information
diffusion across networks. Experiments show that while agents can detect direct
multimodal contradictions, they often fail to align local revisions with global
safety, reaching only a 55 percent success rate in correcting unsafe plans.
Across eight simulation runs with three models - Claude, GPT-4o mini, and
Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75,
55, and 58 percent, respectively. Overall performance ranged from 20 percent in
multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such
as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted
when paired with misleading visuals, showing a strong tendency to overtrust
images. These findings expose critical limitations in current architectures and
provide a reproducible platform for studying multimodal safety, coherence, and
social dynamics.

</details>


### [27] [Control Synthesis of Cyber-Physical Systems for Real-Time Specifications through Causation-Guided Reinforcement Learning](https://arxiv.org/abs/2510.07715)
*Xiaochen Tang,Zhenya Zhang,Miaomiao Zhang,Jie An*

Main category: cs.AI

TL;DR: 提出了一种基于STL在线因果监控的奖励生成方法，通过实时监控系统行为与STL规范的符合程度，计算满足或违反的定量距离，从而产生反映瞬时状态动态的奖励。


<details>
  <summary>Details</summary>
Motivation: 现有方法将STL奖励函数融入RL中，但自动推断的奖励代表全局评估而非局部变化累积，稀疏的全局奖励可能导致不收敛和不稳定的训练性能。

Method: 在线奖励生成方法，通过STL在线因果监控，在每个控制步骤持续监控系统行为，计算满足或违反的定量距离，并提供因果语义的平滑近似以克服不连续性。

Result: 在Gym环境中对多种连续控制基准进行实验，结果显示该方法优于现有的相关STL引导RL方法，为深度RL提供了更鲁棒高效的奖励生成框架。

Conclusion: 提出的STL引导RL方法结合在线因果语义，能够提供更鲁棒和高效的奖励生成框架，优于现有方法。

Abstract: In real-time and safety-critical cyber-physical systems (CPSs), control
synthesis must guarantee that generated policies meet stringent timing and
correctness requirements under uncertain and dynamic conditions. Signal
temporal logic (STL) has emerged as a powerful formalism of expressing
real-time constraints, with its semantics enabling quantitative assessment of
system behavior. Meanwhile, reinforcement learning (RL) has become an important
method for solving control synthesis problems in unknown environments. Recent
studies incorporate STL-based reward functions into RL to automatically
synthesize control policies. However, the automatically inferred rewards
obtained by these methods represent the global assessment of a whole or partial
path but do not accumulate the rewards of local changes accurately, so the
sparse global rewards may lead to non-convergence and unstable training
performances. In this paper, we propose an online reward generation method
guided by the online causation monitoring of STL. Our approach continuously
monitors system behavior against an STL specification at each control step,
computing the quantitative distance toward satisfaction or violation and
thereby producing rewards that reflect instantaneous state dynamics.
Additionally, we provide a smooth approximation of the causation semantics to
overcome the discontinuity of the causation semantics and make it
differentiable for using deep-RL methods. We have implemented a prototype tool
and evaluated it in the Gym environment on a variety of continuously controlled
benchmarks. Experimental results show that our proposed STL-guided RL method
with online causation semantics outperforms existing relevant STL-guided RL
methods, providing a more robust and efficient reward generation framework for
deep-RL.

</details>


### [28] [oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning](https://arxiv.org/abs/2510.07731)
*Ruiling Xu,Yifan Zhang,Qingyun Wang,Carl Edwards,Heng Ji*

Main category: cs.AI

TL;DR: 提出了oMeBench基准测试和oMeS评估框架，用于评估大语言模型在有机反应机理推理方面的能力，发现当前模型在多步推理方面存在困难，但通过提示策略和微调可以显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 有机反应机理是理解化学反应性和设计新分子的基础，但目前不清楚大语言模型是否真正具备化学推理能力，包括生成有效中间体、保持化学一致性和遵循逻辑连贯的多步路径。

Method: 引入oMeBench（首个大规模专家策划的有机机理推理基准）和oMeS（结合步骤级逻辑和化学相似性的动态评估框架），分析了最先进大语言模型的性能。

Result: 当前模型显示出有前景的化学直觉，但在正确和一致的多步推理方面存在困难。通过提示策略和在提议数据集上微调专家模型，性能比领先闭源模型提高了50%。

Conclusion: oMeBench将为推进AI系统实现真正的化学推理提供严格基础。

Abstract: Organic reaction mechanisms are the stepwise elementary reactions by which
reactants form intermediates and products, and are fundamental to understanding
chemical reactivity and designing new molecules and reactions. Although large
language models (LLMs) have shown promise in understanding chemical tasks such
as synthesis design, it is unclear to what extent this reflects genuine
chemical reasoning capabilities, i.e., the ability to generate valid
intermediates, maintain chemical consistency, and follow logically coherent
multi-step pathways. We address this by introducing oMeBench, the first
large-scale, expert-curated benchmark for organic mechanism reasoning in
organic chemistry. It comprises over 10,000 annotated mechanistic steps with
intermediates, type labels, and difficulty ratings. Furthermore, to evaluate
LLM capability more precisely and enable fine-grained scoring, we propose oMeS,
a dynamic evaluation framework that combines step-level logic and chemical
similarity. We analyze the performance of state-of-the-art LLMs, and our
results show that although current models display promising chemical intuition,
they struggle with correct and consistent multi-step reasoning. Notably, we
find that using prompting strategy and fine-tuning a specialist model on our
proposed dataset increases performance by 50% over the leading closed-source
model. We hope that oMeBench will serve as a rigorous foundation for advancing
AI systems toward genuine chemical reasoning.

</details>


### [29] [SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation](https://arxiv.org/abs/2510.07733)
*Minh-Anh Nguye,Minh-Duc Nguyen,Nguyen Thi Ha Lan,Kieu Hai Dang,Nguyen Tien Dong,Le Duy Dung*

Main category: cs.AI

TL;DR: SurveyG是一个基于LLM的智能体框架，通过集成层次化引用图来生成更全面、结构更好的综述论文，解决了现有方法忽视论文间结构关系的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成综述的方法通常直接提取和总结大量相关论文，但忽视了论文间的结构关系，导致生成的综述缺乏连贯的分类体系和深层上下文理解。

Method: 提出SurveyG框架，集成层次化引用图（包含基础层、发展层和前沿层），结合横向层内搜索和纵向跨层遍历，生成多层次摘要并整合为结构化综述大纲，最后通过多智能体验证确保一致性、覆盖面和事实准确性。

Result: 实验表明，SurveyG在人类专家和LLM作为评判者的评估中均优于现有最先进框架，生成的综述更全面且更好地符合领域知识分类体系。

Conclusion: SurveyG通过层次化引用图有效提升了LLM生成综述的质量，在结构连贯性和内容全面性方面表现优异，为自动化综述生成提供了新思路。

Abstract: Large language models (LLMs) are increasingly adopted for automating survey
paper generation \cite{wang2406autosurvey, liang2025surveyx,
yan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing
approaches typically extract content from a large collection of related papers
and prompt LLMs to summarize them directly. However, such methods often
overlook the structural relationships among papers, resulting in generated
surveys that lack a coherent taxonomy and a deeper contextual understanding of
research progress. To address these shortcomings, we propose \textbf{SurveyG},
an LLM-based agent framework that integrates \textit{hierarchical citation
graph}, where nodes denote research papers and edges capture both citation
dependencies and semantic relatedness between their contents, thereby embedding
structural and contextual knowledge into the survey generation process. The
graph is organized into three layers: \textbf{Foundation},
\textbf{Development}, and \textbf{Frontier}, to capture the evolution of
research from seminal works to incremental advances and emerging directions. By
combining horizontal search within layers and vertical depth traversal across
layers, the agent produces multi-level summaries, which are consolidated into a
structured survey outline. A multi-agent validation stage then ensures
consistency, coverage, and factual accuracy in generating the final survey.
Experiments, including evaluations by human experts and LLM-as-a-judge,
demonstrate that SurveyG outperforms state-of-the-art frameworks, producing
surveys that are more comprehensive and better structured to the underlying
knowledge taxonomy of a field.

</details>


### [30] [Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains](https://arxiv.org/abs/2510.07748)
*Yilun Zhang,Dexing Kong*

Main category: cs.AI

TL;DR: MMIA是一个基于LLM的医学智能代理架构，通过形式化可验证的推理过程确保可靠性，在医疗管理任务中实现超过98%的错误检测率，同时通过RAG模式显著降低处理成本。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在医学领域容易产生事实和逻辑错误的问题，因为这在高风险医疗环境中是不可接受的。

Method: 递归将复杂医疗任务分解为原子化、基于证据的步骤，通过自动审计推理链的逻辑一致性和证据可追溯性来确保可靠性，并采用"引导"模式将已验证的推理链存储为"定理"，后续任务通过RAG高效解决。

Result: 在四个医疗管理领域的验证中，MMIA实现了超过98%的错误检测率和低于1%的误报率，显著优于基线LLMs，RAG匹配模式预计可将平均处理成本降低约85%。

Conclusion: MMIA的可验证推理框架是创建可信赖、透明且成本效益高的AI系统的重要一步，使LLM技术在医学关键应用中变得可行。

Abstract: Large Language Models (LLMs) show promise in medicine but are prone to
factual and logical errors, which is unacceptable in this high-stakes field. To
address this, we introduce the "Haibu Mathematical-Medical Intelligent Agent"
(MMIA), an LLM-driven architecture that ensures reliability through a formally
verifiable reasoning process. MMIA recursively breaks down complex medical
tasks into atomic, evidence-based steps. This entire reasoning chain is then
automatically audited for logical coherence and evidence traceability, similar
to theorem proving. A key innovation is MMIA's "bootstrapping" mode, which
stores validated reasoning chains as "theorems." Subsequent tasks can then be
efficiently solved using Retrieval-Augmented Generation (RAG), shifting from
costly first-principles reasoning to a low-cost verification model. We
validated MMIA across four healthcare administration domains, including DRG/DIP
audits and medical insurance adjudication, using expert-validated benchmarks.
Results showed MMIA achieved an error detection rate exceeding 98% with a false
positive rate below 1%, significantly outperforming baseline LLMs. Furthermore,
the RAG matching mode is projected to reduce average processing costs by
approximately 85% as the knowledge base matures. In conclusion, MMIA's
verifiable reasoning framework is a significant step toward creating
trustworthy, transparent, and cost-effective AI systems, making LLM technology
viable for critical applications in medicine.

</details>


### [31] [From Noisy to Native: LLM-driven Graph Restoration for Test-Time Graph Domain Adaptation](https://arxiv.org/abs/2510.07762)
*Xiangwei Lv,JinLuan Yang,Wang Lin,Jingyuan Chen,Beishui Liao*

Main category: cs.AI

TL;DR: 提出GRAIL框架，将测试时图域适应重新定义为生成式图恢复问题，利用LLM将目标图恢复到源域状态，无需访问源域数据。


<details>
  <summary>Details</summary>
Motivation: 现有图域适应方法依赖源域数据，但源域数据常因隐私或安全问题不可用，因此需要开发无需源域数据的测试时图域适应方法。

Method: 使用图扩散过程建模图恢复过程，将节点表示压缩为紧凑潜在特征，通过量化模块编码为离散标记，微调LLM作为生成式恢复器，并引入强化学习优化恢复质量。

Result: 在多个数据集上的广泛实验证明了该方法的有效性。

Conclusion: GRAIL框架成功解决了测试时图域适应问题，通过生成式图恢复方法在无需源域数据的情况下实现了有效的域适应。

Abstract: Graph domain adaptation (GDA) has achieved great attention due to its
effectiveness in addressing the domain shift between train and test data. A
significant bottleneck in existing graph domain adaptation methods is their
reliance on source-domain data, which is often unavailable due to privacy or
security concerns. This limitation has driven the development of Test-Time
Graph Domain Adaptation (TT-GDA), which aims to transfer knowledge without
accessing the source examples. Inspired by the generative power of large
language models (LLMs), we introduce a novel framework that reframes TT-GDA as
a generative graph restoration problem, "restoring the target graph to its
pristine, source-domain-like state". There are two key challenges: (1) We need
to construct a reasonable graph restoration process and design an effective
encoding scheme that an LLM can understand, bridging the modality gap. (2) We
need to devise a mechanism to ensure the restored graph acquires the intrinsic
features of the source domain, even without access to the source data. To
ensure the effectiveness of graph restoration, we propose GRAIL, that restores
the target graph into a state that is well-aligned with the source domain.
Specifically, we first compress the node representations into compact latent
features and then use a graph diffusion process to model the graph restoration
process. Then a quantization module encodes the restored features into discrete
tokens. Building on this, an LLM is fine-tuned as a generative restorer to
transform a "noisy" target graph into a "native" one. To further improve
restoration quality, we introduce a reinforcement learning process guided by
specialized alignment and confidence rewards. Extensive experiments demonstrate
the effectiveness of our approach across various datasets.

</details>


### [32] [An approach for systematic decomposition of complex llm tasks](https://arxiv.org/abs/2510.07772)
*Tianle Zhou,Jiakai Xu,Guanhong Liu,Jiaxiang Liu,Haonan Wang,Eugene Wu*

Main category: cs.AI

TL;DR: 提出ACONIC框架，通过约束问题建模和形式化复杂度度量来指导任务分解，在组合优化和数据库查询任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂任务上存在可靠性问题，现有分解方法依赖启发式或人工分解，需要更系统的方法

Method: ACONIC框架将任务建模为约束问题，利用形式化复杂度度量来指导分解过程

Result: 在SATBench和Spider任务上，基于复杂度度量的分解使智能体性能提升10-40个百分点

Conclusion: ACONIC框架提供了一种系统化的任务分解方法，能显著提升LLM在复杂任务上的表现

Abstract: Large Language Models (LLMs) suffer from reliability issues on complex tasks,
as existing decomposition methods are heuristic and rely on agent or manual
decomposition. This work introduces a novel, systematic decomposition framework
that we call Analysis of CONstraint-Induced Complexity (ACONIC), which models
the task as a constraint problem and leveraging formal complexity measures to
guide decomposition. On combinatorial (SATBench) and LLM database querying
tasks (Spider), we find that by decomposing the tasks following the measure of
complexity, agent can perform considerably better (10-40 percentage point).

</details>


### [33] [GCPO: When Contrast Fails, Go Gold](https://arxiv.org/abs/2510.07790)
*Hao Wu,Wei Liu*

Main category: cs.AI

TL;DR: 提出了Group Contrastive Policy Optimization (GCPO)方法，通过引入外部标准参考答案来解决现有强化学习算法在推理任务中的局限性，显著提升了小模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法如GRPO存在明显缺陷：模型生成回答的上限完全由模型自身决定，无法从全错或全对的样本中学习知识。需要一种能够充分利用所有样本并引导模型学习正确推理方向的方法。

Method: GCPO方法引入外部标准参考答案，当模型无法解决问题时，参考答案提供正确响应，引导模型向明确准确的更新方向学习。这种方法能够充分利用每个样本，并让模型在训练过程中模仿参考答案的解题策略。

Result: GCPO在多个基准数据集上取得了优异结果，相比基线模型实现了显著改进。

Conclusion: GCPO通过引入外部参考答案有效解决了现有强化学习方法的局限性，不仅提高了训练效率，还增强了模型在推理任务中的泛化能力。

Abstract: Reinforcement learning has been widely applied to enhance the reasoning
capabilities of large language models. Extending the inference limits of
smaller models has become a prominent research focus. However, algorithms such
as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the
upper bound of a model's rollout responses is entirely determined by the model
itself, preventing the acquisition of knowledge from samples that are either
all incorrect or all correct. In this paper, we introduce Group Contrastive
Policy Optimization (GCPO), a method that incorporates external standard
reference answers. When the model cannot solve a problem, the reference answer
supplies the correct response, steering the model toward an unequivocally
accurate update direction. This approach offers two main advantages: (1) it
improves training efficiency by fully utilizing every sample; (2) it enables
the model to emulate the problem solving strategy of the reference answer
during training, thereby enhancing generalization in reasoning. GCPO achieves
outstanding results across multiple benchmark datasets, yielding substantial
improvements over the baseline model. Our code is available at:
https://github.com/AchoWu/GCPO.

</details>


### [34] [Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games](https://arxiv.org/abs/2510.07813)
*Valerio La Gatta,Dolev Mutzari,Sarit Kraus,VS Subrahmanian*

Main category: cs.AI

TL;DR: 提出了SHADOW框架，通过强化学习在对抗环境中平衡信息获取与暴露风险，实现更高的追捕成功率。


<details>
  <summary>Details</summary>
Motivation: 对抗环境中存在信息获取与暴露风险的关键权衡：通信能增强态势感知，但会暴露自身位置增加被攻击风险。

Method: 提出PEEC博弈模型和SHADOW多头部序贯强化学习框架，整合连续导航控制、离散通信动作和对手行为预测建模。

Result: SHADOW追捕者在成功率上优于六个竞争基线，消融研究确认时序建模和对手建模对有效决策至关重要。

Conclusion: 学习到的策略在不同通信风险和物理不对称条件下具有良好的泛化能力，验证了框架的有效性。

Abstract: Adversarial environments require agents to navigate a key strategic
trade-off: acquiring information enhances situational awareness, but may
simultaneously expose them to threats. To investigate this tension, we
formulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer
agent must decide when to communicate in order to obtain the evader's position.
Each communication reveals the pursuer's location, increasing the risk of being
targeted. Both agents learn their movement policies via reinforcement learning,
while the pursuer additionally learns a communication policy that balances
observability and risk. We propose SHADOW (Strategic-communication Hybrid
Action Decision-making under partial Observation for Warfare), a multi-headed
sequential reinforcement learning framework that integrates continuous
navigation control, discrete communication actions, and opponent modeling for
behavior prediction. Empirical evaluations show that SHADOW pursuers achieve
higher success rates than six competitive baselines. Our ablation study
confirms that temporal sequence modeling and opponent modeling are critical for
effective decision-making. Finally, our sensitivity analysis reveals that the
learned policies generalize well across varying communication risks and
physical asymmetries between agents.

</details>


### [35] [An LLM-Powered Cooperative Framework for Large-Scale Multi-Vehicle Navigation](https://arxiv.org/abs/2510.07825)
*Yuping Zhou,Siqi Lai,Jindong Han,Hao Liu*

Main category: cs.AI

TL;DR: CityNav是一个基于LLM的分层框架，用于大规模多车辆动态导航，通过全局交通分配和局部导航代理的协同优化，在城市规模交通网络中显著提升旅行效率和缓解拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有路径搜索算法和强化学习方法难以扩展到城市规模网络，无法有效捕捉城市交通的非线性、随机性和耦合动态特性。

Method: 提出分层框架：全局交通分配代理协调区域间交通流分布，局部导航代理生成符合全局指令的自适应路线。采用协同推理优化机制，通过个体奖励和共享奖励的双重奖励结构进行联合训练。

Result: 在四个真实道路网络（最大160万条道路和43万个交叉口）上的实验表明，CityNav在九种经典路径搜索和基于RL的基线方法中，在城市规模旅行效率和拥堵缓解方面表现最优。

Conclusion: LLM能够实现可扩展、自适应和协同的城市范围交通导航，为复杂城市环境中的智能大规模车辆路由提供了基础。

Abstract: The rise of Internet of Vehicles (IoV) technologies is transforming traffic
management from isolated control to a collective, multi-vehicle process. At the
heart of this shift is multi-vehicle dynamic navigation, which requires
simultaneously routing large fleets under evolving traffic conditions. Existing
path search algorithms and reinforcement learning methods struggle to scale to
city-wide networks, often failing to capture the nonlinear, stochastic, and
coupled dynamics of urban traffic. To address these challenges, we propose
CityNav, a hierarchical, LLM-powered framework for large-scale multi-vehicle
navigation. CityNav integrates a global traffic allocation agent, which
coordinates strategic traffic flow distribution across regions, with local
navigation agents that generate locally adaptive routes aligned with global
directives. To enable effective cooperation, we introduce a cooperative
reasoning optimization mechanism, in which agents are jointly trained with a
dual-reward structure: individual rewards promote per-vehicle efficiency, while
shared rewards encourage network-wide coordination and congestion reduction.
Extensive experiments on four real-world road networks of varying scales (up to
1.6 million roads and 430,000 intersections) and traffic datasets demonstrate
that CityNav consistently outperforms nine classical path search and RL-based
baselines in city-scale travel efficiency and congestion mitigation. Our
results highlight the potential of LLMs to enable scalable, adaptive, and
cooperative city-wide traffic navigation, providing a foundation for
intelligent, large-scale vehicle routing in complex urban environments. Our
project is available at https://github.com/usail-hkust/CityNav.

</details>


### [36] [FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning](https://arxiv.org/abs/2510.07852)
*Shuangyan Deng,Haizhou Peng,Jiachen Xu,Rui Mao,Ciprian Doru Giurcăneanu,Jiamou Liu*

Main category: cs.AI

TL;DR: FinMR是一个高质量、知识密集的多模态数据集，专门用于评估专业级金融推理能力，包含3200多个精心策划的问题-答案对，涵盖15个金融主题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在金融等专业领域的评估存在不足，缺乏具有专业知识强度、详细注释和高级推理复杂性的数据集。

Method: 构建FinMR数据集，包含3200多个专家标注的问题-答案对，涵盖15个金融主题，整合了复杂的数学推理、高级金融知识和多类型图像的细致视觉解释任务。

Result: 通过对领先的闭源和开源MLLMs进行基准测试，发现这些模型与专业金融分析师之间存在显著性能差距，特别是在精确图像分析、复杂金融公式准确应用和深度上下文金融理解方面。

Conclusion: FinMR通过提供丰富多样的视觉内容和详尽的解释性注释，成为评估和推进多模态金融推理向专业分析师水平发展的重要基准工具。

Abstract: Multimodal Large Language Models (MLLMs) have made substantial progress in
recent years. However, their rigorous evaluation within specialized domains
like finance is hindered by the absence of datasets characterized by
professional-level knowledge intensity, detailed annotations, and advanced
reasoning complexity. To address this critical gap, we introduce FinMR, a
high-quality, knowledge-intensive multimodal dataset explicitly designed to
evaluate expert-level financial reasoning capabilities at a professional
analyst's standard. FinMR comprises over 3,200 meticulously curated and
expertly annotated question-answer pairs across 15 diverse financial topics,
ensuring broad domain diversity and integrating sophisticated mathematical
reasoning, advanced financial knowledge, and nuanced visual interpretation
tasks across multiple image types. Through comprehensive benchmarking with
leading closed-source and open-source MLLMs, we highlight significant
performance disparities between these models and professional financial
analysts, uncovering key areas for model advancement, such as precise image
analysis, accurate application of complex financial formulas, and deeper
contextual financial understanding. By providing richly varied visual content
and thorough explanatory annotations, FinMR establishes itself as an essential
benchmark tool for assessing and advancing multimodal financial reasoning
toward professional analyst-level competence.

</details>


### [37] [Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models](https://arxiv.org/abs/2510.07858)
*Zhiqing Cui,Binwu Wang,Qingxiang Liu,Yeqiang Wang,Zhengyang Zhou,Yuxuan Liang,Yang Wang*

Main category: cs.AI

TL;DR: Augur是一个完全由LLM驱动的时序预测框架，利用LLM的因果推理能力发现和使用协变量间的有向因果关联，通过两阶段师生架构提升预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时序预测方法存在架构边缘化、依赖粗糙统计文本提示、缺乏可解释性等局限，需要开发更有效的LLM驱动框架。

Method: 采用两阶段师生架构：强大的教师LLM通过启发式搜索和成对因果检验从时序数据推断有向因果图；轻量级学生代理细化图结构，并在高置信度因果关联上进行微调，将其编码为丰富文本提示进行预测。

Result: 在真实世界数据集上与25个基线方法对比，Augur实现了竞争性性能并展现出强大的零样本泛化能力。

Conclusion: Augur框架通过LLM的因果推理能力显著提升了时序预测的准确性和可解释性，证明了完全LLM驱动方法的有效性。

Abstract: Large language models (LLM) have emerged as a promising avenue for time
series forecasting, offering the potential to integrate multimodal data.
However, existing LLM-based approaches face notable limitations-such as
marginalized role in model architectures, reliance on coarse statistical text
prompts, and lack of interpretability. In this work, we introduce Augur, a
fully LLM driven time series forecasting framework that exploits LLM causal
reasoning to discover and use directed causal associations among covariates.
Augur uses a two stage teacher student architecture where a powerful teacher
LLM infers a directed causal graph from time series using heuristic search
together with pairwise causality testing. A lightweight student agent then
refines the graph and fine tune on high confidence causal associations that are
encoded as rich textual prompts to perform forecasting. This design improves
predictive accuracy while yielding transparent, traceable reasoning about
variable interactions. Extensive experiments on real-world datasets with 25
baselines demonstrate that Augur achieves competitive performance and robust
zero-shot generalization.

</details>


### [38] [Understanding DeepResearch via Reports](https://arxiv.org/abs/2510.07861)
*Tianyu Fan,Xinyao Niu,Yuxiang Zheng,Fengji Zhang,Chengen Huang,Bei Chen,Junyang Lin,Chao Huang*

Main category: cs.AI

TL;DR: 提出了DeepResearch-ReportEval评估框架，用于全面评估深度研究系统的研究报告质量，包括质量、冗余性和事实性三个维度，采用LLM作为评判者的方法。


<details>
  <summary>Details</summary>
Motivation: 深度研究系统在开放研究场景中的评估具有挑战性，现有基准测试主要关注孤立能力而非整体性能，需要新的评估方法来衡量这些系统的综合研究能力。

Method: 开发了包含100个精选查询的标准化基准，涵盖12个现实世界类别，使用创新的LLM-as-a-Judge方法系统评估研究报告的质量、冗余性和事实性。

Result: 对四个领先商业系统的评估揭示了不同的设计理念和性能权衡，为深度研究系统从信息助手向智能研究伙伴的演进提供了基础性见解。

Conclusion: DeepResearch-ReportEval框架为评估深度研究系统提供了全面解决方案，填补了现有评估方法的空白，促进了深度研究技术的发展。

Abstract: DeepResearch agents represent a transformative AI paradigm, conducting
expert-level research through sophisticated reasoning and multi-tool
integration. However, evaluating these systems remains critically challenging
due to open-ended research scenarios and existing benchmarks that focus on
isolated capabilities rather than holistic performance. Unlike traditional LLM
tasks, DeepResearch systems must synthesize diverse sources, generate insights,
and present coherent findings, which are capabilities that resist simple
verification. To address this gap, we introduce DeepResearch-ReportEval, a
comprehensive framework designed to assess DeepResearch systems through their
most representative outputs: research reports. Our approach systematically
measures three dimensions: quality, redundancy, and factuality, using an
innovative LLM-as-a-Judge methodology achieving strong expert concordance. We
contribute a standardized benchmark of 100 curated queries spanning 12
real-world categories, enabling systematic capability comparison. Our
evaluation of four leading commercial systems reveals distinct design
philosophies and performance trade-offs, establishing foundational insights as
DeepResearch evolves from information assistants toward intelligent research
partners. Source code and data are available at:
https://github.com/HKUDS/DeepResearch-Eval.

</details>


### [39] [Towards Meaningful Transparency in Civic AI Systems](https://arxiv.org/abs/2510.07889)
*Dave Murray-Rust,Kars Alfrink,Cristina Zaga*

Main category: cs.AI

TL;DR: 论文提出'有意义透明度'概念，旨在让公众能够理解并参与影响其生活的AI系统决策过程，连接理解与行动可能性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在政府服务中存在偏见和错误输出，减少公民和公务员的决策参与权。现有透明度实践过于技术化，难以被公众理解且缺乏行动指导。

Method: 结合以人为中心的AI透明度方法和社会技术系统视角，发展公民AI系统的有意义透明度概念。

Result: 提出了有意义透明度的概念框架，强调透明度应使公众能够参与影响其生活的AI系统决策。

Conclusion: 有意义透明度是解决公民AI系统问题的关键，需要将技术透明度与社会行动能力相结合。

Abstract: Artificial intelligence has become a part of the provision of governmental
services, from making decisions about benefits to issuing fines for parking
violations. However, AI systems rarely live up to the promise of neutral
optimisation, creating biased or incorrect outputs and reducing the agency of
both citizens and civic workers to shape the way decisions are made.
Transparency is a principle that can both help subjects understand decisions
made about them and shape the processes behind those decisions. However,
transparency as practiced around AI systems tends to focus on the production of
technical objects that represent algorithmic aspects of decision making. These
are often difficult for publics to understand, do not connect to potential for
action, and do not give insight into the wider socio-material context of
decision making. In this paper, we build on existing approaches that take a
human-centric view on AI transparency, combined with a socio-technical systems
view, to develop the concept of meaningful transparency for civic AI systems:
transparencies that allow publics to engage with AI systems that affect their
lives, connecting understanding with potential for action.

</details>


### [40] [Profit Mirage: Revisiting Information Leakage in LLM-based Financial Agents](https://arxiv.org/abs/2510.07920)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: 本文揭示了LLM金融代理存在的"利润幻象"问题，即回测收益因LLM信息泄露而在知识窗口结束后消失，并提出了FactFin框架来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM金融代理存在严重的信息泄露问题，导致回测收益不可靠，需要开发能够学习因果驱动因素而非记忆结果的鲁棒方法。

Method: 提出FactFin框架，包含策略代码生成器、检索增强生成、蒙特卡洛树搜索和反事实模拟器四个核心组件，通过反事实扰动迫使代理学习因果驱动因素。

Result: 大量实验表明，该方法在样本外泛化方面超越所有基线，实现了优越的风险调整后性能。

Conclusion: FactFin框架有效解决了LLM金融代理的信息泄露问题，提高了模型的泛化能力和实际交易性能。

Abstract: LLM-based financial agents have attracted widespread excitement for their
ability to trade like human experts. However, most systems exhibit a "profit
mirage": dazzling back-tested returns evaporate once the model's knowledge
window ends, because of the inherent information leakage in LLMs. In this
paper, we systematically quantify this leakage issue across four dimensions and
release FinLake-Bench, a leakage-robust evaluation benchmark. Furthermore, to
mitigate this issue, we introduce FactFin, a framework that applies
counterfactual perturbations to compel LLM-based agents to learn causal drivers
instead of memorized outcomes. FactFin integrates four core components:
Strategy Code Generator, Retrieval-Augmented Generation, Monte Carlo Tree
Search, and Counterfactual Simulator. Extensive experiments show that our
method surpasses all baselines in out-of-sample generalization, delivering
superior risk-adjusted performance.

</details>


### [41] [Enabling Personalized Long-term Interactions in LLM-based Agents through Persistent Memory and User Profiles](https://arxiv.org/abs/2510.07925)
*Rebecca Westhäußer,Wolfgang Minker,Sebatian Zepf*

Main category: cs.AI

TL;DR: 提出了一个集成持久记忆、动态协调、自我验证和演进用户档案的框架，用于实现基于LLM的个性化长期交互智能体。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为AI智能体控制单元时缺乏个性化交互能力，检索增强生成虽然提升了上下文感知但无法结合用户特定数据，现有个性化研究多为概念性而缺乏技术实现。

Method: 基于统一的个性化定义，结合多智能体协作和多源检索等成熟AI模式，构建包含持久记忆、动态协调、自我验证和演进用户档案的技术框架。

Result: 在三个公共数据集上评估了检索准确率、响应正确性和BertScore等指标，并通过5天试点用户研究获得了关于感知个性化的初步用户反馈。

Conclusion: 研究表明集成持久记忆和用户档案有潜力提升基于LLM智能体的适应性和感知个性化，为未来工作提供了指导方向。

Abstract: Large language models (LLMs) increasingly serve as the central control unit
of AI agents, yet current approaches remain limited in their ability to deliver
personalized interactions. While Retrieval Augmented Generation enhances LLM
capabilities by improving context-awareness, it lacks mechanisms to combine
contextual information with user-specific data. Although personalization has
been studied in fields such as human-computer interaction or cognitive science,
existing perspectives largely remain conceptual, with limited focus on
technical implementation. To address these gaps, we build on a unified
definition of personalization as a conceptual foundation to derive technical
requirements for adaptive, user-centered LLM-based agents. Combined with
established agentic AI patterns such as multi-agent collaboration or
multi-source retrieval, we present a framework that integrates persistent
memory, dynamic coordination, self-validation, and evolving user profiles to
enable personalized long-term interactions. We evaluate our approach on three
public datasets using metrics such as retrieval accuracy, response correctness,
or BertScore. We complement these results with a five-day pilot user study
providing initial insights into user feedback on perceived personalization. The
study provides early indications that guide future work and highlights the
potential of integrating persistent memory and user profiles to improve the
adaptivity and perceived personalization of LLM-based agents.

</details>


### [42] [Agent-Based Genetic Algorithm for Crypto Trading Strategy Optimization](https://arxiv.org/abs/2510.07943)
*Qiushi Tian,Churong Liang,Kairan Hong,Runnan Li*

Main category: cs.AI

TL;DR: 提出CGA-Agent混合框架，结合遗传算法与多智能体协调机制，用于加密货币交易策略参数优化，在动态金融环境中实现自适应优化。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场因极端波动性、非平稳动态和复杂微观结构模式，使传统参数优化方法基本失效，需要新的自适应优化方法。

Method: 开发CGA-Agent混合框架，集成遗传算法与智能多智能体协调机制，结合实时市场微观结构智能和自适应策略性能反馈，动态指导进化过程。

Result: 在三种加密货币上的综合实证评估显示，在总回报和风险调整指标上均实现了系统性和统计显著的性能改进。

Conclusion: CGA-Agent框架超越了静态优化方法的局限性，为动态金融环境中的交易策略优化提供了有效解决方案。

Abstract: Cryptocurrency markets present formidable challenges for trading strategy
optimization due to extreme volatility, non-stationary dynamics, and complex
microstructure patterns that render conventional parameter optimization methods
fundamentally inadequate. We introduce Cypto Genetic Algorithm Agent
(CGA-Agent), a pioneering hybrid framework that synergistically integrates
genetic algorithms with intelligent multi-agent coordination mechanisms for
adaptive trading strategy parameter optimization in dynamic financial
environments. The framework uniquely incorporates real-time market
microstructure intelligence and adaptive strategy performance feedback through
intelligent mechanisms that dynamically guide evolutionary processes,
transcending the limitations of static optimization approaches. Comprehensive
empirical evaluation across three cryptocurrencies demonstrates systematic and
statistically significant performance improvements on both total returns and
risk-adjusted metrics.

</details>


### [43] [TaoSR-SHE: Stepwise Hybrid Examination Reinforcement Learning Framework for E-commerce Search Relevance](https://arxiv.org/abs/2510.07972)
*Pengkun Jiao,Yiming Jin,Jianhui Yang,Chenhe Dong,Zerui Huang,Shaowei Yao,Xiaojiang Zhou,Dan Ou,Haihong Tang*

Main category: cs.AI

TL;DR: 提出了TaoSR-SHE框架，通过步进式混合验证强化学习解决电商搜索相关性分析中的泛化性和推理一致性问题


<details>
  <summary>Details</summary>
Motivation: 现有训练范式存在局限性：SFT和DPO在长尾查询上泛化能力差，缺乏细粒度监督；RLVR存在稀疏反馈问题，无法纠正错误中间步骤

Method: 核心是SRPO强化学习算法，结合生成式步进奖励模型和人工标注验证器的混合奖励，采用多样化数据过滤和多阶段课程学习

Result: 在真实搜索基准测试中，TaoSR-SHE在推理质量和相关性预测准确性方面均优于SFT、DPO、GRPO等基线方法

Conclusion: 该框架不仅提升了大规模电商场景下的性能，还增强了可解释性和鲁棒性

Abstract: Query-product relevance analysis is a foundational technology in e-commerce
search engines and has become increasingly important in AI-driven e-commerce.
The recent emergence of large language models (LLMs), particularly their
chain-of-thought (CoT) reasoning capabilities, offers promising opportunities
for developing relevance systems that are both more interpretable and more
robust. However, existing training paradigms have notable limitations: SFT and
DPO suffer from poor generalization on long-tail queries and from a lack of
fine-grained, stepwise supervision to enforce rule-aligned reasoning. In
contrast, reinforcement learning with verification rewards (RLVR) suffers from
sparse feedback, which provides insufficient signal to correct erroneous
intermediate steps, thereby undermining logical consistency and limiting
performance in complex inference scenarios.
  To address these challenges, we introduce the Stepwise Hybrid Examination
Reinforcement Learning framework for Taobao Search Relevance (TaoSR-SHE). At
its core is Stepwise Reward Policy Optimization (SRPO), a reinforcement
learning algorithm that leverages step-level rewards generated by a hybrid of a
high-quality generative stepwise reward model and a human-annotated offline
verifier, prioritizing learning from critical correct and incorrect reasoning
steps. TaoSR-SHE further incorporates two key techniques: diversified data
filtering to encourage exploration across varied reasoning paths and mitigate
policy entropy collapse, and multi-stage curriculum learning to foster
progressive capability growth. Extensive experiments on real-world search
benchmarks show that TaoSR-SHE improves both reasoning quality and
relevance-prediction accuracy in large-scale e-commerce settings, outperforming
SFT, DPO, GRPO, and other baselines, while also enhancing interpretability and
robustness.

</details>


### [44] [VoiceAgentBench: Are Voice Assistants ready for agentic tasks?](https://arxiv.org/abs/2510.07978)
*Dhruv Jain,Harshit Shukla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.AI

TL;DR: 提出了VoiceAgentBench基准测试，用于评估语音语言模型在真实语音代理场景中的表现，涵盖多语言、文化理解和对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有语音基准主要关注孤立能力如转录或问答，缺乏对多语言文化理解和对抗鲁棒性的系统性评估。

Method: 创建包含5,500+合成语音查询的基准，支持英语、印地语和5种印度语言，使用基于说话人嵌入的采样算法最大化声学和说话人多样性。

Result: 实验揭示了当前语音语言模型在上下文工具编排任务、印度语言泛化和对抗鲁棒性方面的显著差距。

Conclusion: VoiceAgentBench暴露了当前语音语言模型的关键局限性，为改进多语言文化理解和鲁棒性提供了重要基准。

Abstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants
capable of understanding natural spoken queries and performing complex tasks.
However, existing speech benchmarks primarily focus on isolated capabilities
such as transcription, or question-answering, and do not systematically
evaluate agentic scenarios encompassing multilingual and cultural
understanding, as well as adversarial robustness. To address this, we introduce
VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in
realistic spoken agentic settings. It comprises over 5,500 synthetic spoken
queries, including dialogues grounded in Indian context, covering single-tool
invocations, multi-tool workflows, multi-turn interactions, and safety
evaluations. The benchmark supports English, Hindi, and 5 other Indian
languages, reflecting real-world linguistic and cultural diversity. We simulate
speaker variability using a novel sampling algorithm that selects audios for
TTS voice conversion based on its speaker embeddings, maximizing acoustic and
speaker diversity. Our evaluation measures tool selection accuracy, structural
consistency, and the correctness of tool invocations, including adversarial
robustness. Our experiments reveal significant gaps in contextual tool
orchestration tasks, Indic generalization, and adversarial robustness, exposing
critical limitations of current SpeechLMs.

</details>


### [45] [ReInAgent: A Context-Aware GUI Agent Enabling Human-in-the-Loop Mobile Task Navigation](https://arxiv.org/abs/2510.07988)
*Haitao Jia,Ming He,Zimo Yin,Likang Wu,Jianping Fan,Jitao Sang*

Main category: cs.AI

TL;DR: ReInAgent是一个上下文感知的多代理框架，通过动态信息管理和人机协作来解决移动GUI代理在模糊、动态变化和冲突任务场景中的信息困境问题。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理过于强调自主操作，忽视了任务执行过程中的用户参与需求，导致在信息模糊、动态变化和冲突的任务场景中适应性不足，执行结果偏离用户真实需求和偏好。

Method: ReInAgent框架包含三个专门代理围绕共享内存模块：信息管理代理负责基于槽位的信息管理和主动用户交互，决策代理负责冲突感知规划，反思代理负责任务反思和信息一致性验证。通过持续上下文信息分析和持续用户-代理协作来克服现有方法的局限性。

Result: 实验结果表明，ReInAgent能有效解决信息困境，产生更符合用户真实偏好的结果。在涉及信息困境的复杂任务上，ReInAgent比Mobile-Agent-v2实现了25%更高的成功率。

Conclusion: ReInAgent通过上下文感知的多代理框架和动态信息管理，实现了在复杂现实场景中更自适应和可靠的移动任务导航，克服了现有方法依赖清晰和静态任务假设的局限性。

Abstract: Mobile GUI agents exhibit substantial potential to facilitate and automate
the execution of user tasks on mobile phones. However, exist mobile GUI agents
predominantly privilege autonomous operation and neglect the necessity of
active user engagement during task execution. This omission undermines their
adaptability to information dilemmas including ambiguous, dynamically evolving,
and conflicting task scenarios, leading to execution outcomes that deviate from
genuine user requirements and preferences. To address these shortcomings, we
propose ReInAgent, a context-aware multi-agent framework that leverages dynamic
information management to enable human-in-the-loop mobile task navigation.
ReInAgent integrates three specialized agents around a shared memory module: an
information-managing agent for slot-based information management and proactive
interaction with the user, a decision-making agent for conflict-aware planning,
and a reflecting agent for task reflection and information consistency
validation. Through continuous contextual information analysis and sustained
user-agent collaboration, ReInAgent overcomes the limitation of existing
approaches that rely on clear and static task assumptions. Consequently, it
enables more adaptive and reliable mobile task navigation in complex,
real-world scenarios. Experimental results demonstrate that ReInAgent
effectively resolves information dilemmas and produces outcomes that are more
closely aligned with genuine user preferences. Notably, on complex tasks
involving information dilemmas, ReInAgent achieves a 25% higher success rate
than Mobile-Agent-v2.

</details>


### [46] [Language Models Do Not Embed Numbers Continuously](https://arxiv.org/abs/2510.08009)
*Alex O. Davies,Roussel Nzoyem,Nirav Ajmeri,Telmo M. Silva Filho*

Main category: cs.AI

TL;DR: 研究发现语言模型在处理数值时存在非连续表示和显著噪声问题，即使能够高精度重建数值，但主成分仅能解释嵌入空间中的少量变异。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否真正将连续值建模为连续表示，以及它们在数值表示方面的实际表现。

Method: 使用线性重建和主成分分析评估三个主流模型（OpenAI、Google Gemini、Voyage AI）的数值表示能力。

Result: 虽然重建精度高（R²≥0.95），但主成分仅能解释少量变异，且随着小数精度的增加，线性重建和解释方差都会恶化。

Conclusion: 语言模型在数值表示方面存在局限性，这对需要高数值精度、大数值范围或混合符号值的应用领域具有重要影响。

Abstract: Recent research has extensively studied how large language models manipulate
integers in specific arithmetic tasks, and on a more fundamental level, how
they represent numeric values. These previous works have found that language
model embeddings can be used to reconstruct the original values, however, they
do not evaluate whether language models actually model continuous values as
continuous. Using expected properties of the embedding space, including linear
reconstruction and principal component analysis, we show that language models
not only represent numeric spaces as non-continuous but also introduce
significant noise. Using models from three major providers (OpenAI, Google
Gemini and Voyage AI), we show that while reconstruction is possible with high
fidelity ($R^2 \geq 0.95$), principal components only explain a minor share of
variation within the embedding space. This indicates that many components
within the embedding space are orthogonal to the simple numeric input space.
Further, both linear reconstruction and explained variance suffer with
increasing decimal precision, despite the ordinal nature of the input space
being fundamentally unchanged. The findings of this work therefore have
implications for the many areas where embedding models are used, in-particular
where high numerical precision, large magnitudes or mixed-sign values are
common.

</details>


### [47] [PEAR: Phase Entropy Aware Reward for Efficient Reasoning](https://arxiv.org/abs/2510.08026)
*Chen Huang,Wei Lu,Wenxuan Zhang*

Main category: cs.AI

TL;DR: PEAR是一种基于阶段熵感知的奖励机制，通过在不同推理阶段调整熵值来控制大推理模型的响应长度，在保持准确性的同时减少冗余推理步骤。


<details>
  <summary>Details</summary>
Motivation: 大推理模型生成的推理链往往过长且包含冗余步骤，这会增加推理成本并降低可用性。如何在保持准确性的同时控制推理长度是一个未解决的挑战。

Method: 通过系统实证分析发现模型熵与响应长度存在正相关关系，基于此提出PEAR奖励机制，在思考阶段惩罚过高熵值，在最终答案阶段允许适度探索，从而自适应控制响应长度。

Result: 在四个基准测试上的广泛实验表明，PEAR能持续减少响应长度，同时在不同模型规模下保持竞争力准确性，并展现出超出训练分布的强OOD鲁棒性。

Conclusion: PEAR通过阶段依赖的熵感知机制有效平衡了推理的简洁性和性能，为控制大模型推理长度提供了一种不依赖显式长度目标的自适应方法。

Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on complex
reasoning tasks by generating detailed chain-of-thought (CoT) explanations.
However, these responses are often excessively long, containing redundant
reasoning steps that inflate inference cost and reduce usability. Controlling
the length of generated reasoning without sacrificing accuracy remains an open
challenge. Through a systematic empirical analysis, we reveal a consistent
positive correlation between model entropy and response length at different
reasoning stages across diverse LRMs: the thinking phase exhibits higher
entropy, reflecting exploratory behavior of longer responses, while the final
answer phase shows lower entropy, indicating a more deterministic solution.This
observation suggests that entropy at different reasoning stages can serve as a
control knob for balancing conciseness and performance. Based on this insight,
this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism
that incorporating phase-dependent entropy into the reward design. Instead of
treating all tokens uniformly, PEAR penalize excessive entropy during the
thinking phase and allowing moderate exploration at the final answer phase,
which encourages models to generate concise reasoning traces that retain
sufficient flexibility to solve the task correctly. This enables adaptive
control of response length without relying on explicit length targets or rigid
truncation rules. Extensive experiments across four benchmarks demonstrate that
PEAR consistently reduces response length while sustaining competitive accuracy
across model scales. In addition, PEAR demonstrates strong out-of-distribution
(OOD) robustness beyond the training distribution. Our code is available at:
https://github.com/iNLP-Lab/PEAR.

</details>


### [48] [AILoRA: Function-Aware Asymmetric Initialization for Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2510.08034)
*Xiaoshuang Ji,Zhendong Zhao,Xiaoyan Gu,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.AI

TL;DR: 提出AILoRA方法，通过函数感知的非对称低秩先验改进LoRA，针对自注意力机制中W^Q和W^V矩阵的功能差异进行差异化初始化，提升微调性能和收敛效率。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然被广泛采用，但仍面临性能次优和收敛慢的问题。研究发现W^Q和W^V矩阵具有不同的参数特性：W^Q对下游任务敏感，W^V在任务间保持稳定。

Method: AILoRA采用函数感知的非对称初始化策略：为W^Q注入主成分以保留任务适应能力，为W^V注入次要成分以保持通用特征表示。

Result: 该方法使LoRA模块能更好地捕捉注意力参数的专业角色，提高了微调性能和收敛效率。

Conclusion: 通过考虑自注意力参数的功能差异，AILoRA为参数高效微调提供了更有效的解决方案。

Abstract: Parameter-efficient finetuning (PEFT) aims to mitigate the substantial
computational and memory overhead involved in adapting large-scale pretrained
models to diverse downstream tasks. Among numerous PEFT strategies, Low-Rank
Adaptation (LoRA) has emerged as one of the most widely adopted approaches due
to its robust empirical performance and low implementation complexity. In
practical deployment, LoRA is typically applied to the $W^Q$ and $W^V$
projection matrices of self-attention modules, enabling an effective trade-off
between model performance and parameter efficiency. While LoRA has achieved
considerable empirical success, it still encounters challenges such as
suboptimal performance and slow convergence. To address these limitations, we
introduce \textbf{AILoRA}, a novel parameter-efficient method that incorporates
function-aware asymmetric low-rank priors. Our empirical analysis reveals that
the projection matrices $W^Q$ and $W^V$ in the self-attention mechanism exhibit
distinct parameter characteristics, stemming from their functional differences.
Specifically, $W^Q$ captures task-specific semantic space knowledge essential
for attention distributions computation, making its parameters highly sensitive
to downstream task variations. In contrast, $W^V$ encodes token-level feature
representations that tend to remain stable across tasks and layers. Leveraging
these insights, AILoRA performs a function-aware initialization by injecting
the principal components of $W^Q$ to retain task-adaptive capacity, and the
minor components of $W^V$ to preserve generalizable feature representations.
This asymmetric initialization strategy enables LoRA modules to better capture
the specialized roles of attention parameters, thereby enhancing both
finetuning performance and convergence efficiency.

</details>


### [49] [LinguaSim: Interactive Multi-Vehicle Testing Scenario Generation via Natural Language Instruction Based on Large Language Models](https://arxiv.org/abs/2510.08046)
*Qingyuan Shi,Qingwen Meng,Hao Cheng,Qing Xu,Jianqiang Wang*

Main category: cs.AI

TL;DR: LinguaSim是一个基于大语言模型的框架，能够将自然语言转换为真实、交互式的3D场景，确保动态车辆交互和输入描述与生成场景的忠实对齐。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的场景生成方法在平衡命令准确性和真实世界驾驶环境真实性方面存在困难，往往为了降低场景描述复杂度而牺牲真实性，限制在2D或开放循环模拟中。

Method: 提出LinguaSim框架，包含反馈校准模块来优化生成精度，通过自然语言和闭环交互模拟之间的桥梁，约束对抗性车辆行为。

Result: 实验显示LinguaSim能生成与不同自然语言描述对齐的具有不同关键性的场景（危险描述ACT：0.072秒 vs 安全描述：3.532秒；舒适度：0.654 vs 0.764），其优化模块有效降低初始输出的过度攻击性，碰撞率从46.9%降至6.3%。

Conclusion: LinguaSim促进了高保真场景的创建，增强了安全测试和训练能力，成功弥合了自然语言与闭环交互模拟之间的差距。

Abstract: The generation of testing and training scenarios for autonomous vehicles has
drawn significant attention. While Large Language Models (LLMs) have enabled
new scenario generation methods, current methods struggle to balance command
adherence accuracy with the realism of real-world driving environments. To
reduce scenario description complexity, these methods often compromise realism
by limiting scenarios to 2D, or open-loop simulations where background vehicles
follow predefined, non-interactive behaviors. We propose LinguaSim, an
LLM-based framework that converts natural language into realistic, interactive
3D scenarios, ensuring both dynamic vehicle interactions and faithful alignment
between the input descriptions and the generated scenarios. A feedback
calibration module further refines the generation precision, improving fidelity
to user intent. By bridging the gap between natural language and closed-loop,
interactive simulations, LinguaSim constrains adversarial vehicle behaviors
using both the scenario description and the autonomous driving model guiding
them. This framework facilitates the creation of high-fidelity scenarios that
enhance safety testing and training. Experiments show LinguaSim can generate
scenarios with varying criticality aligned with different natural language
descriptions (ACT: 0.072 s for dangerous vs. 3.532 s for safe descriptions;
comfortability: 0.654 vs. 0.764), and its refinement module effectively reduces
excessive aggressiveness in LinguaSim's initial outputs, lowering the crash
rate from 46.9% to 6.3% to better match user intentions.

</details>


### [50] [Multi-Condition Conformal Selection](https://arxiv.org/abs/2510.08075)
*Qingyang Hao,Wenbo Liao,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: 提出MCCS算法，将保形选择扩展到多条件场景，通过区域单调性非保形分数和全局BH程序实现FDR控制。


<details>
  <summary>Details</summary>
Motivation: 现有保形选择方法仅限于单阈值场景，无法满足实际应用中多条件选择的需求，如合取或析取条件。

Method: 提出MCCS算法，为合取条件引入区域单调性非保形分数，为析取条件使用全局BH程序。

Result: 实验验证MCCS优于基线方法，在不同条件组合、现实模态和多任务场景下具有良好泛化性。

Conclusion: MCCS能够在各种多条件环境中实现严格的FDR控制选择，具有理论保证和实际应用价值。

Abstract: Selecting high-quality candidates from large-scale datasets is critically
important in resource-constrained applications such as drug discovery,
precision medicine, and the alignment of large language models. While conformal
selection methods offer a rigorous solution with False Discovery Rate (FDR)
control, their applicability is confined to single-threshold scenarios (i.e., y
> c) and overlooks practical needs for multi-condition selection, such as
conjunctive or disjunctive conditions. In this work, we propose the
Multi-Condition Conformal Selection (MCCS) algorithm, which extends conformal
selection to scenarios with multiple conditions. In particular, we introduce a
novel nonconformity score with regional monotonicity for conjunctive conditions
and a global Benjamini-Hochberg (BH) procedure for disjunctive conditions,
thereby establishing finite-sample FDR control with theoretical guarantees. The
integration of these components enables the proposed method to achieve rigorous
FDR-controlled selection in various multi-condition environments. Extensive
experiments validate the superiority of MCCS over baselines, its
generalizability across diverse condition combinations, different real-world
modalities, and multi-task scalability.

</details>


### [51] [AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment](https://arxiv.org/abs/2510.08081)
*Xiaochong Lan,Jie Feng,Yinxing Liu,Xinlei Shi,Yong Li*

Main category: cs.AI

TL;DR: AutoQual是一个基于LLM的智能体框架，用于自动发现可解释特征，特别针对在线评论质量评估。它通过模拟人类研究过程，迭代生成特征假设并实现工具化，在大型电商平台上验证有效。


<details>
  <summary>Details</summary>
Motivation: 在线评论质量评估对电商平台至关重要，但质量是领域依赖且动态的概念。传统方法难以跨领域扩展且无法适应内容模式变化，而深度学习方法缺乏可解释性且可能过度关注语义而非质量。

Method: AutoQual框架模拟人类研究过程：通过反思迭代生成特征假设，通过自主工具实现特征操作化，并在持久内存中积累经验。该框架将数据中的隐性知识转化为显性、可计算的特征。

Result: 在拥有亿级用户的大型在线平台上部署，大规模A/B测试证实了其有效性：每位用户平均查看评论数增加0.79%，评论读者的转化率提高0.27%。

Conclusion: AutoQual成功解决了评论质量评估中的可扩展性和可解释性问题，证明了将隐性知识转化为显性特征的可行性，为领域依赖的质量评估提供了通用框架。

Abstract: Ranking online reviews by their intrinsic quality is a critical task for
e-commerce platforms and information services, impacting user experience and
business outcomes. However, quality is a domain-dependent and dynamic concept,
making its assessment a formidable challenge. Traditional methods relying on
hand-crafted features are unscalable across domains and fail to adapt to
evolving content patterns, while modern deep learning approaches often produce
black-box models that lack interpretability and may prioritize semantics over
quality. To address these challenges, we propose AutoQual, an LLM-based agent
framework that automates the discovery of interpretable features. While
demonstrated on review quality assessment, AutoQual is designed as a general
framework for transforming tacit knowledge embedded in data into explicit,
computable features. It mimics a human research process, iteratively generating
feature hypotheses through reflection, operationalizing them via autonomous
tool implementation, and accumulating experience in a persistent memory. We
deploy our method on a large-scale online platform with a billion-level user
base. Large-scale A/B testing confirms its effectiveness, increasing average
reviews viewed per user by 0.79% and the conversion rate of review readers by
0.27%.

</details>


### [52] [From Ethical Declarations to Provable Independence: An Ontology-Driven Optimal-Transport Framework for Certifiably Fair AI Systems](https://arxiv.org/abs/2510.08086)
*Sukriti Bhattacharya,Chitro Majumdar*

Main category: cs.AI

TL;DR: 提出一个可证明公平的AI框架，通过系统性地移除所有敏感信息及其代理来克服当前偏见缓解方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前偏见缓解方法存在局限性，需要一种能够系统性地移除所有敏感信息及其代理的公平AI框架。

Method: 使用OWL 2 QL中的本体工程来形式化定义敏感属性，并通过逻辑推理推断其代理，构建捕获偏见模式完整结构的sigma代数G。然后通过Delbaen Majumdar最优传输获得公平表示，生成与G独立的变量，同时最小化L2距离以保持准确性。

Result: 该方法保证了真正的独立性而不仅仅是去相关，通过将偏见建模为sigma代数之间的依赖关系，将本体知识编译为可测量结构，并使用最优传输作为唯一的公平变换。

Conclusion: 该方法为可信AI提供了一种可证明和数学基础的方法，确保在贷款审批等任务中的完全公平性，其中ZIP代码等代理会揭示种族信息。

Abstract: This paper presents a framework for provably fair AI that overcomes the
limits of current bias mitigation methods by systematically removing all
sensitive information and its proxies. Using ontology engineering in OWL 2 QL,
it formally defines sensitive attributes and infers their proxies through
logical reasoning, constructing a sigma algebra G that captures the full
structure of biased patterns. Fair representations are then obtained via
Delbaen Majumdar optimal transport, which generates variables independent of G
while minimizing L2 distance to preserve accuracy. This guarantees true
independence rather than mere decorrelation. By modeling bias as dependence
between sigma algebras, compiling ontological knowledge into measurable
structures, and using optimal transport as the unique fair transformation, the
approach ensures complete fairness in tasks like loan approval, where proxies
such as ZIP code reveal race. The result is a certifiable and mathematically
grounded method for trustworthy AI.

</details>


### [53] [Can Risk-taking AI-Assistants suitably represent entities](https://arxiv.org/abs/2510.08114)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Amirhossein Farshi Sotoudeh*

Main category: cs.AI

TL;DR: 该研究调查了语言模型在风险厌恶方面的可操纵性，发现虽然某些模型与人类行为有一定一致性，但仍存在显著差异，需要改进生物中心的可操纵性测量方法。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地融入AI驱动的决策支持系统，理解其风险行为对于负责任部署至关重要，需要防止系统无意中将用户推向风险决策或嵌入隐藏偏见。

Method: 研究考察语言模型在不同经济场景下复制人类风险偏好的能力，重点关注性别特定态度、不确定性、基于角色的决策以及风险厌恶的可操纵性。

Result: DeepSeek Reasoner和Gemini-2.0-flash-lite等语言模型显示出与人类行为的一些一致性，但存在显著差异，突显了改进生物中心可操纵性测量的必要性。

Conclusion: 研究呼吁进一步改进模型设计，确保AI系统更准确地复制人类风险偏好，从而提高其在风险管理环境中的有效性，增强AI助手在风险管理中的适用性。

Abstract: Responsible AI demands systems whose behavioral tendencies can be effectively
measured, audited, and adjusted to prevent inadvertently nudging users toward
risky decisions or embedding hidden biases in risk aversion. As language models
(LMs) are increasingly incorporated into AI-driven decision support systems,
understanding their risk behaviors is crucial for their responsible deployment.
This study investigates the manipulability of risk aversion (MoRA) in LMs,
examining their ability to replicate human risk preferences across diverse
economic scenarios, with a focus on gender-specific attitudes, uncertainty,
role-based decision-making, and the manipulability of risk aversion. The
results indicate that while LMs such as DeepSeek Reasoner and
Gemini-2.0-flash-lite exhibit some alignment with human behaviors, notable
discrepancies highlight the need to refine bio-centric measures of
manipulability. These findings suggest directions for refining AI design to
better align human and AI risk preferences and enhance ethical decision-making.
The study calls for further advancements in model design to ensure that AI
systems more accurately replicate human risk preferences, thereby improving
their effectiveness in risk management contexts. This approach could enhance
the applicability of AI assistants in managing risk.

</details>


### [54] [Prepared mind, fast response: A temporal decoupling framework for adaptive knowledge orchestration in open-domain dialogue](https://arxiv.org/abs/2510.08175)
*Jinling Gan,Churong Liang,Runnan Li*

Main category: cs.AI

TL;DR: PMFR提出了一种异步知识编排框架，通过解耦知识检索与响应生成，在保持对话质量的同时大幅降低延迟（95.3%的延迟减少）。


<details>
  <summary>Details</summary>
Motivation: 解决开放域对话系统中延迟与质量之间的基本矛盾：轻量级模型延迟低但缺乏推理深度，而工具增强的ReAct代理虽然提高了事实准确性，但同步执行导致检索过程中交互阻塞。

Method: 采用时间解耦框架，包含三个协调组件：知识充分性评估器（实时评估）、轻量级响应生成器（即时交互）和异步知识精炼代理（后台知识增强）。

Result: 在TopiOCQA评估中，PMFR实现了95.3%的延迟减少（从23.38秒降至1.09秒），同时保持与重量级同步基线相当的响应质量（GEval-C：0.613 vs 0.620）。

Conclusion: PMFR通过异步知识编排从根本上解决了延迟与质量的矛盾，在保持对话连续性的同时渐进式增强知识覆盖，为开放域对话系统提供了有效的解决方案。

Abstract: The latency-quality tradeoff is a fundamental constraint in open-domain
dialogue AI systems, since comprehensive knowledge access necessitates
prohibitive response delays. Contemporary approaches offer two inadequate
solutions: lightweight instruct models achieve sub-second latency but lack
reasoning depth, while tool-augmented ReAct agents enhance factuality through
external knowledge at the cost of synchronous execution that blocks interaction
during retrieval processes. PMFR is thus proposed, with a temporal decoupling
framework that fundamentally resolves the contradiction through asynchronous
knowledge orchestration. PMFR employs three coordinated components: (1) a
Knowledge Adequacy Evaluator for real-time sufficiency assessment, (2) a
Lightweight Response Generator for immediate user interaction, and (3) an
Asynchronous Knowledge Refinement Agent for background knowledge enhancement.
This architecture maintains continuous conversational flow while progressively
enriching knowledge coverage through intelligent triggering mechanisms.
Evaluation results on TopiOCQA demonstrate PMFR outperforms brute-force
scaling: PMFR achieves 95.3% latency reduction (23.38s -> 1.09s) while
preserving response quality comparable to heavyweight synchronous baselines
(GEval-C: 0.613 vs. 0.620).

</details>


### [55] [R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?](https://arxiv.org/abs/2510.08189)
*Yi Lu,Jianing Wang,Linsen Guo,Wei He,Hongyin Tang,Tao Gui,Xuanjing Huang,Xuezhi Cao,Wei Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 提出了R-HORIZON方法和基准，用于评估和增强大型推理模型在长视野复杂推理任务中的能力，发现现有模型存在有效推理长度有限和思考预算分配不当的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注即时、单视野任务，无法充分评估模型理解和响应复杂长视野场景的能力，需要对大型推理模型进行更全面的评估。

Method: 通过查询组合设计R-HORIZON方法，构建包含复杂多步推理任务的长视野推理基准，并使用该基准数据通过验证奖励的强化学习（RLVR）来训练模型。

Result: 评估发现最先进的大型推理模型在长视野任务中性能显著下降；使用R-HORIZON数据进行RLVR训练不仅大幅提升多视野推理任务性能，还在标准推理任务上提升了7.5个AIME2024分数。

Conclusion: R-HORIZON为增强和评估大型推理模型的长视野推理能力提供了一个可扩展、可控且低成本的范式。

Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought
(CoT). However, existing benchmarks mainly focus on immediate, single-horizon
tasks, failing to adequately evaluate models' ability to understand and respond
to complex, long-horizon scenarios. To address this incomplete evaluation of
Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to
stimulate long-horizon reasoning behaviors in LRMs through query composition.
Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising
complex multi-step reasoning tasks with interdependent problems that span long
reasoning horizons. Through comprehensive evaluation of LRMs using the
R-HORIZON benchmark, we find that even the most advanced LRMs suffer
significant performance degradation. Our analysis reveals that LRMs exhibit
limited effective reasoning length and struggle to allocate thinking budget
across multiple problems appropriately. Recognizing these limitations, we use
R-HORIZON to construct long-horizon reasoning data for reinforcement learning
with verified rewards (RLVR). Compared to training with single-horizon data,
RLVR with R-HORIZON not only substantially improves performance on the
multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning
tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as
a scalable, controllable, and low-cost paradigm for enhancing and evaluating
the long-horizon reasoning capabilities of LRMs.

</details>


### [56] [Measuring What Matters: The AI Pluralism Index](https://arxiv.org/abs/2510.08193)
*Rashid Mushkani*

Main category: cs.AI

TL;DR: 本文提出了AI多元主义指数(AIPI)，这是一个透明、基于证据的评估工具，用于衡量AI系统在参与式治理、包容性与多样性、透明度和问责制四个支柱方面的多元主义程度。


<details>
  <summary>Details</summary>
Motivation: 当前AI开发和治理集中在少数公司和国家手中，可能导致技术编码狭隘利益并限制公众参与。缺乏可审计的多元治理衡量标准。

Method: 开发AIPI评估框架，通过可验证的实践编码、公开证据分析、独立评估和专家访谈，构建可复现的测量流程，并评估可靠性。

Result: 建立了包含四个支柱的评估体系，实现了可复现的测量流程，并报告了试点供应商结果。

Conclusion: AIPI旨在引导激励措施向多元实践发展，为政策制定者、采购者和公众提供可比较的证据。

Abstract: Artificial intelligence systems increasingly mediate knowledge,
communication, and decision making. Development and governance remain
concentrated within a small set of firms and states, raising concerns that
technologies may encode narrow interests and limit public agency. Capability
benchmarks for language, vision, and coding are common, yet public, auditable
measures of pluralistic governance are rare. We define AI pluralism as the
degree to which affected stakeholders can shape objectives, data practices,
safeguards, and deployment. We present the AI Pluralism Index (AIPI), a
transparent, evidence-based instrument that evaluates producers and system
families across four pillars: participatory governance, inclusivity and
diversity, transparency, and accountability. AIPI codes verifiable practices
from public artifacts and independent evaluations, explicitly handling
"Unknown" evidence to report both lower-bound ("evidence") and known-only
scores with coverage. We formalize the measurement model; implement a
reproducible pipeline that integrates structured web and repository analysis,
external assessments, and expert interviews; and assess reliability with
inter-rater agreement, coverage reporting, cross-index correlations, and
sensitivity analysis. The protocol, codebook, scoring scripts, and evidence
graph are maintained openly with versioned releases and a public adjudication
process. We report pilot provider results and situate AIPI relative to adjacent
transparency, safety, and governance frameworks. The index aims to steer
incentives toward pluralistic practice and to equip policymakers, procurers,
and the public with comparable evidence.

</details>


### [57] [DODO: Causal Structure Learning with Budgeted Interventions](https://arxiv.org/abs/2510.08207)
*Matteo Gregorini,Chiara Boldrini,Lorenzo Valerio*

Main category: cs.AI

TL;DR: DODO算法让智能体通过重复干预自主学习环境因果结构，相比观测方法在大多数情况下表现更好，能在最具挑战性配置中比最佳基线高出+0.25 F1分数。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要依赖复杂相关性，缺乏因果理解能力。让AI具备因果意识可以提升性能，更深入理解环境机制。

Method: 智能体在与由隐藏因果DAG支配的世界交互中，通过执行干预并利用因果推断技术分析观测变化的统计显著性，来推断因果结构。

Result: DODO在除最有限资源条件外的所有情况下都优于观测方法，通常能以零误差重建因果图结构，在最挑战性配置中比最佳基线高+0.25 F1分数。

Conclusion: 通过自主干预学习因果结构的方法有效，DODO算法在因果图重建方面表现出色，为AI因果理解提供了可行途径。

Abstract: Artificial Intelligence has achieved remarkable advancements in recent years,
yet much of its progress relies on identifying increasingly complex
correlations. Enabling causality awareness in AI has the potential to enhance
its performance by enabling a deeper understanding of the underlying mechanisms
of the environment. In this paper, we introduce DODO, an algorithm defining how
an Agent can autonomously learn the causal structure of its environment through
repeated interventions. We assume a scenario where an Agent interacts with a
world governed by a causal Directed Acyclic Graph (DAG), which dictates the
system's dynamics but remains hidden from the Agent. The Agent's task is to
accurately infer the causal DAG, even in the presence of noise. To achieve
this, the Agent performs interventions, leveraging causal inference techniques
to analyze the statistical significance of observed changes. Results show
better performance for DODO, compared to observational approaches, in all but
the most limited resource conditions. DODO is often able to reconstruct with as
low as zero errors the structure of the causal graph. In the most challenging
configuration, DODO outperforms the best baseline by +0.25 F1 points.

</details>


### [58] [Selection, Reflection and Self-Refinement: Revisit Reasoning Tasks via a Causal Lens](https://arxiv.org/abs/2510.08222)
*Yunlong Deng,Boyang Sun,Yan Li,Lingjing Kong,Zeyu Tang,Kun Zhang,Guangyi Chen*

Main category: cs.AI

TL;DR: 本文从因果视角重新审视推理任务，将其视为选择机制，并提出SR²框架来学习潜在变量间的密集依赖关系，在推理任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型即使在大量预训练和微调后，仍难以可靠地进行推理。本文旨在从因果角度理解推理任务在潜在空间中的行为，为解决推理挑战提供见解。

Method: 提出SR²框架，包含三个关键模块：反射表示学习、依赖自精炼和周期性中间对齐。该框架将估计的潜在变量作为反馈纳入选择机制，促进潜在表示间密集依赖关系的学习。

Result: 实验显示该方法在推理准确性上带来显著提升，例如在数独和迷宫任务上，相比最新进展，用8倍少的参数实现了超过10%的性能改进。

Conclusion: 从因果视角将推理任务建模为选择机制，并通过SR²框架学习潜在变量间的密集依赖关系，能够有效提升模型的推理能力。

Abstract: Due to their inherent complexity, reasoning tasks have long been regarded as
rigorous benchmarks for assessing the capabilities of machine learning models,
especially large language models (LLMs). Although humans can solve these tasks
with ease, existing models, even after extensive pre-training and post-training
at scale, still fail to perform reasoning reliably. In this paper, we revisit
reasoning tasks from a causal perspective, seeking to understand their behavior
in latent space and to offer insights for addressing their challenges.
Specifically, we cast reasoning tasks as a selection mechanism, in which
high-level logical concepts function as selection operators on the given
observations, such as, identifying the correct answer in a math problem or
filling the appropriate entry in Sudoku. We emphasize two key properties of
this formulation that shed light on the difficulty of reasoning tasks. First,
the latent space exceeds the observation space in complexity, even when the
correct answer is fully determined by the observed input. Second, the latent
variables, corresponding to logical thought, are densely structured and exhibit
strong dependencies. Building on this formulation, we introduce a framework,
called SR$^2$, that incorporates the estimated latent variables as feedback
into the selection mechanism, thereby facilitating the learning of dense
dependencies among latent representations. The framework consists of three key
modules: reflective representation learning, dependency self-refinement, and
periodic intermediate alignment. Experimentally, we show that our approach
yields significant gains in reasoning accuracy, for example, attaining over
10$\%$ improvement in performance with 8$\times$ fewer parameters on the Sudoku
and Maze tasks over the recent advances.

</details>


### [59] [Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness](https://arxiv.org/abs/2510.08238)
*Jiyang Qiu,Xinbei Ma,Yunqing Xu,Zhuosheng Zhang,Hai Zhao*

Main category: cs.AI

TL;DR: 提出CoTri多步后门攻击方法，针对LLM智能体实现长期控制，在保持高攻击成功率的同时提升智能体在良性任务上的性能


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在现实应用中的快速部署，其安全性和鲁棒性引发严重担忧，需要揭示智能体的安全漏洞

Method: CoTri后门攻击使用有序触发序列，从初始触发开始，后续触发从环境中提取，实现多步操控使智能体偏离原任务

Result: CoTri实现接近完美的攻击成功率，同时保持接近零的误触发率，并意外提升智能体在良性任务上的性能和抗干扰能力

Conclusion: CoTri实现了智能体内稳定的多步控制，提高了内在鲁棒性和任务能力，使攻击更隐蔽，带来潜在安全风险

Abstract: The rapid deployment of large language model (LLM)-based agents in real-world
applications has raised serious concerns about their trustworthiness. In this
work, we reveal the security and robustness vulnerabilities of these agents
through backdoor attacks. Distinct from traditional backdoors limited to
single-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a
multi-step backdoor attack designed for long-horizon agentic control. CoTri
relies on an ordered sequence. It starts with an initial trigger, and
subsequent ones are drawn from the environment, allowing multi-step
manipulation that diverts the agent from its intended task. Experimental
results show that CoTri achieves a near-perfect attack success rate (ASR) while
maintaining a near-zero false trigger rate (FTR). Due to training data modeling
the stochastic nature of the environment, the implantation of CoTri
paradoxically enhances the agent's performance on benign tasks and even
improves its robustness against environmental distractions. We further validate
CoTri on vision-language models (VLMs), confirming its scalability to
multimodal agents. Our work highlights that CoTri achieves stable, multi-step
control within agents, improving their inherent robustness and task
capabilities, which ultimately makes the attack more stealthy and raises
potential safty risks.

</details>


### [60] [Co-TAP: Three-Layer Agent Interaction Protocol Technical Report](https://arxiv.org/abs/2510.08263)
*Shunyu An,Miao Wang,Yongchao Li,Dong Wan,Lina Wang,Ling Qin,Liqin Gao,Congyao Fan,Zhiyong Mao,Jiange Pu,Wenji Xia,Dong Zhao,Rui Hu,Ji Lu,Guiyue Zhou,Baoyu Tang,Yanqin Gao,Yongsheng Du,Daigang Xu,Lingjun Huang,Baoli Wang,Xiwen Zhang,Luyao Wang,Shilong Liu*

Main category: cs.AI

TL;DR: Co-TAP是一个三层代理交互协议，通过HAI、UAP和MEK三个核心协议解决多智能体系统在互操作性、交互协作和知识共享方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统在互操作性、交互协作和知识共享三个核心维度面临的挑战，为构建下一代高效、可扩展和智能的多智能体应用提供工程基础和理论指导。

Method: 设计了三层协议架构：HAI协议标准化人机交互流程，确保实时性和可靠性；UAP协议通过统一服务发现和协议转换实现异构代理的互连互通；MEK协议建立"记忆-提取-知识"认知链，支持个体经验学习和可共享知识形成。

Result: 提出了一个完整的协议框架，能够支持多智能体系统的标准化交互、无缝互连和集体智能实现。

Conclusion: Co-TAP协议框架为构建高效、可扩展和智能的多智能体应用提供了坚实的工程基础和理论指导，有望推动下一代多智能体系统的发展。

Abstract: This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer
agent interaction protocol designed to address the challenges faced by
multi-agent systems across the three core dimensions of Interoperability,
Interaction and Collaboration, and Knowledge Sharing. We have designed and
proposed a layered solution composed of three core protocols: the Human-Agent
Interaction Protocol (HAI), the Unified Agent Protocol (UAP), and the
Memory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction
layer, standardizing the flow of information between users, interfaces, and
agents by defining a standardized, event-driven communication paradigm. This
ensures the real-time performance, reliability, and synergy of interactions. As
the core of the infrastructure layer, UAP is designed to break down
communication barriers among heterogeneous agents through unified service
discovery and protocol conversion mechanisms, thereby enabling seamless
interconnection and interoperability of the underlying network. MEK, in turn,
operates at the cognitive layer. By establishing a standardized ''Memory (M) -
Extraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the
ability to learn from individual experiences and form shareable knowledge,
thereby laying the foundation for the realization of true collective
intelligence. We believe this protocol framework will provide a solid
engineering foundation and theoretical guidance for building the next
generation of efficient, scalable, and intelligent multi-agent applications.

</details>


### [61] [Symmetry-Aware Fully-Amortized Optimization with Scale Equivariant Graph Metanetworks](https://arxiv.org/abs/2510.08300)
*Bart Kuipers,Freek Byrman,Daniel Uyterlinde,Alejandro García-Castellanos*

Main category: cs.AI

TL;DR: ScaleGMNs通过利用缩放对称性实现单次微调优化，减少迭代需求，在卷积网络中比多层感知机有更小的规范自由度，提升了优化效率。


<details>
  <summary>Details</summary>
Motivation: 利用摊销优化加速相关优化问题的求解，通过元网络学习利用问题实例间的共享结构。

Method: 使用尺度等变图元网络(ScaleGMNs)直接在权重空间操作，实现现有模型的单次微调。

Result: 实证证明了方法的有效性，理论分析显示卷积神经网络的缩放对称性诱导的规范自由度比多层感知机更小。

Conclusion: 对称感知元网络是高效且可泛化的神经网络优化的有力方法。

Abstract: Amortized optimization accelerates the solution of related optimization
problems by learning mappings that exploit shared structure across problem
instances. We explore the use of Scale Equivariant Graph Metanetworks
(ScaleGMNs) for this purpose. By operating directly in weight space, ScaleGMNs
enable single-shot fine-tuning of existing models, reducing the need for
iterative optimization. We demonstrate the effectiveness of this approach
empirically and provide a theoretical result: the gauge freedom induced by
scaling symmetries is strictly smaller in convolutional neural networks than in
multi-layer perceptrons. This insight helps explain the performance differences
observed between architectures in both our work and that of Kalogeropoulos et
al. (2024). Overall, our findings underscore the potential of symmetry-aware
metanetworks as a powerful approach for efficient and generalizable neural
network optimization. Open-source code:
https://github.com/daniuyter/scalegmn_amortization

</details>


### [62] [First Try Matters: Revisiting the Role of Reflection in Reasoning Models](https://arxiv.org/abs/2510.08308)
*Liwei Kang,Yue Deng,Yao Xiao,Zhanfeng Mo,Wee Sun Lee,Lidong Bing*

Main category: cs.AI

TL;DR: 该论文分析了大型语言模型在推理过程中的反思行为，发现反思主要是确认性的，很少改变初始答案。作者提出了一种基于问题感知的早期停止方法，在生成候选答案后动态截断反思，显著减少了推理标记数量。


<details>
  <summary>Details</summary>
Motivation: 研究反思行为对推理性能提升的实际贡献，因为虽然LLMs在推理能力上有显著进步，但反思对性能改善的具体作用尚不明确。

Method: 系统分析了8个推理模型在5个数学数据集上的推理过程，构建了不同反思步数的SFT数据集进行训练，并提出了问题感知的早期停止方法来动态截断反思过程。

Result: 反思主要是确认性的，很少改变初始答案；训练更多反思步数主要提升首次答案正确率而非修正能力；提出的方法在5个数学数据集上减少24.5%的推理标记，准确率仅下降2.9%。

Conclusion: 反思在推理过程中作用有限，主要是确认而非修正；通过动态截断反思可以显著提高推理效率，在保持准确率的同时大幅减少计算开销。

Abstract: Large language models have recently demonstrated significant gains in
reasoning ability, often attributed to their capacity to generate longer chains
of thought and engage in reflective reasoning. However, the contribution of
reflections to performance improvement remains unclear. In this paper, we
systematically analyze the rollouts of eight reasoning models on five
mathematical datasets. We focus on reflective behaviours where the model has
already produced an answer but continues reflecting before finalizing its
output. Our analysis reveals that reflections are predominantly confirmatory
and rarely alter the model's initial answer, a pattern consistent across models
and datasets. To understand the role of reflections in training, we construct
supervised fine-tuning (SFT) datasets with varying amounts of reflection steps.
We observe that training models on rollouts with more reflection steps
primarily enhances first-answer correctness rather than the ability to correct
initially wrong answers through reflections. This motivates us to propose a
question-aware early-stopping method that enhances inference-time token
efficiency by stopping the reasoning process once a few plausible candidate
answers are generated, thereby reducing unnecessary reflection steps. Motivated
by this, we further propose to dynamically truncate the reflections after a
candidate answer has appeared during generation, which reduces reasoning tokens
by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.

</details>


### [63] [Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries](https://arxiv.org/abs/2510.08325)
*Marius Dragoi,Ioana Pintilie,Florin Gogianu,Florin Brad*

Main category: cs.AI

TL;DR: 本文提出Cover@tau指标来替代Pass@k，用于更准确地评估语言模型在推理任务中的真实能力边界，避免随机猜测带来的误导。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用Pass@k评估模型推理边界时，发现基础模型在大k值下表现优于RLVR模型，但这种差异可能源于随机猜测而非真实推理能力。

Method: 提出Cover@tau指标，衡量模型在至少tau比例补全正确的情况下能解决的问题比例，该指标对随机猜测敏感，能更好反映真实推理能力。

Result: 使用Cover@tau评估多个RLVR模型，发现与Pass@1相比，模型相对排名发生变化，提供了对推理边界的不同视角。

Conclusion: Cover@tau能更可靠地评估语言模型的推理边界，避免Pass@k在大采样预算下因随机猜测而产生的误导性结果。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm to improve Large Language Models on reasoning tasks such as
coding, math or logic. To assess the reasoning boundary (the fraction of
problems a model can solve) researchers often report Pass@k at large sampling
budgets. Recent results reveal a crossover phenomenon: while RLVR models
outperform the base model at small k values, the base model usually outperforms
them when sampling a very large number of completions. This has been
interpreted as evidence that base models have a larger reasoning boundary. We
argue that on tasks with discrete answer spaces, such as math with numeric
outputs, Pass@k at large k reflects the increasingly higher chance of success
in the limit of the number of trials rather than genuine reasoning, and can
therefore be misleading. We propose Cover@tau, which measures the fraction of
problems that a model can solve for which at least a tau proportion of
completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an
explicit reliability threshold: models that rely on random guessing degrade
rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based
metrics and illustrate how the relative rankings of popular algorithms change
compared to Pass@1, offering a different perspective on reasoning boundaries.

</details>


### [64] [LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings](https://arxiv.org/abs/2510.08338)
*Benjamin F. Maier,Ulf Aslak,Luca Fiaschi,Nina Rismal,Kemble Fletcher,Christian C. Luhmann,Robbie Dow,Kli Pappas,Thomas V. Wiecki*

Main category: cs.AI

TL;DR: 提出语义相似度评分（SSR）方法，使用LLMs生成文本响应并通过嵌入相似度映射到Likert分布，解决了传统消费者研究中面板偏见和规模限制的问题。


<details>
  <summary>Details</summary>
Motivation: 传统消费者研究成本高昂且存在面板偏见和规模限制，LLMs可以模拟合成消费者但直接获取数值评分会产生不现实的响应分布。

Method: SSR方法：从LLMs获取文本响应，通过嵌入相似度将这些响应映射到参考声明的Likert分布。

Result: 在57个个人护理产品调查（9,300个人类响应）上测试，SSR达到90%的人类重测可靠性，保持现实的响应分布（KS相似度>0.85），并提供丰富的定性反馈。

Conclusion: 该框架实现了可扩展的消费者研究模拟，同时保留了传统调查指标和可解释性。

Abstract: Consumer research costs companies billions annually yet suffers from panel
biases and limited scale. Large language models (LLMs) offer an alternative by
simulating synthetic consumers, but produce unrealistic response distributions
when asked directly for numerical ratings. We present semantic similarity
rating (SSR), a method that elicits textual responses from LLMs and maps these
to Likert distributions using embedding similarity to reference statements.
Testing on an extensive dataset comprising 57 personal care product surveys
conducted by a leading corporation in that market (9,300 human responses), SSR
achieves 90% of human test-retest reliability while maintaining realistic
response distributions (KS similarity > 0.85). Additionally, these synthetic
respondents provide rich qualitative feedback explaining their ratings. This
framework enables scalable consumer research simulations while preserving
traditional survey metrics and interpretability.

</details>


### [65] [QAgent: A modular Search Agent with Interactive Query Understanding](https://arxiv.org/abs/2510.08383)
*Yi Jiang,Lei Shen,Lujie Niu,Sendong Zhao,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 提出QAgent框架，通过强化学习训练搜索代理进行自适应检索，解决传统RAG在复杂查询理解和检索质量方面的局限性


<details>
  <summary>Details</summary>
Motivation: 传统RAG在复杂查询理解方面存在困难，基于强化学习的搜索代理虽然前景广阔但仍面临泛化和部署挑战

Method: 采用模块化搜索代理，通过多步决策过程进行交互式推理和检索，使用强化学习最大化检索质量

Result: QAgent在问答任务中表现优异，可作为即插即用模块在实际系统中部署

Conclusion: QAgent通过专注于有效检索的策略增强了LLM应用的泛化能力，为实际部署提供了可行方案

Abstract: Large language models (LLMs) excel at natural language tasks but are limited
by their static parametric knowledge, especially in knowledge-intensive task.
Retrieval-augmented generation (RAG) mitigates this by integrating external
information. However, (1) traditional RAG struggles with complex query
understanding, and (2) even search agents trained with reinforcement learning
(RL), despite their promise, still face generalization and deployment
challenges. To address these limitations, we propose QAgent, a unified agentic
RAG framework that employs a search agent for adaptive retrieval. This agent
optimizes its understanding of the query through interactive reasoning and
retrieval. To facilitate real-world application, we focus on modular search
agent for query understanding that are plug-and-play in complex systems.
Secifically, the agent follows a multi-step decision process trained with RL to
maximize retrieval quality and support accurate downstream answers. We further
analyze the strengths and weaknesses of end-to-end RL and propose a strategy
that focuses on effective retrieval, thereby enhancing generalization in LLM
applications. Experiments show QAgent excels at QA and serves as a
plug-and-play module for real-world deployment.

</details>


### [66] [Revisiting Hallucination Detection with Effective Rank-based Uncertainty](https://arxiv.org/abs/2510.08389)
*Rui Wang,Zeming Wei,Guanzhang Yue,Meng Sun*

Main category: cs.AI

TL;DR: 提出一种基于隐藏状态有效秩的LLM幻觉检测方法，通过分析多输出和多层表示的谱特性来量化不确定性，无需额外知识或模块。


<details>
  <summary>Details</summary>
Motivation: 超越基础的不确定性驱动幻觉检测框架，解决LLM可信部署中的幻觉检测挑战。

Method: 通过测量多个模型输出和不同层隐藏状态的有效秩来量化不确定性，基于表示谱分析提供可解释的见解。

Result: 广泛实验表明该方法能有效检测幻觉，并在各种场景中稳健泛化。

Conclusion: 为LLM真实性检测提供了新的范式，结合了理论优雅性和实际效率。

Abstract: Detecting hallucinations in large language models (LLMs) remains a
fundamental challenge for their trustworthy deployment. Going beyond basic
uncertainty-driven hallucination detection frameworks, we propose a simple yet
powerful method that quantifies uncertainty by measuring the effective rank of
hidden states derived from multiple model outputs and different layers.
Grounded in the spectral analysis of representations, our approach provides
interpretable insights into the model's internal reasoning process through
semantic variations, while requiring no extra knowledge or additional modules,
thus offering a combination of theoretical elegance and practical efficiency.
Meanwhile, we theoretically demonstrate the necessity of quantifying
uncertainty both internally (representations of a single response) and
externally (different responses), providing a justification for using
representations among different layers and responses from LLMs to detect
hallucinations. Extensive experiments demonstrate that our method effectively
detects hallucinations and generalizes robustly across various scenarios,
contributing to a new paradigm of hallucination detection for LLM truthfulness.

</details>


### [67] [Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling](https://arxiv.org/abs/2510.08470)
*Bianca-Mihaela Ganescu,Suchir Salhan,Andrew Caines,Paula Buttery*

Main category: cs.AI

TL;DR: 提出轻量级解码器架构，通过动态门控、特征调制和对比学习，在有限视觉信息下实现高效多模态学习，性能优于基线模型


<details>
  <summary>Details</summary>
Motivation: 在认知合理的数据量限制下训练视觉语言模型，需要重新思考模型如何整合多模态信息

Method: 使用动态门控自适应融合语言和视觉线索，特征调制和通道注意力最大化有限视觉信息的效用，辅助对比目标进行视觉接地

Result: 在五个基准测试中表现优于多模态基线，动态门控发现可解释模式，内容词偏好视觉线索，功能词偏好语言线索

Conclusion: 动态门控是高效多模态学习的强大工具，在严格约束下提供可解释性和性能，但存在全局图像嵌入信息瓶颈和数据集分割训练不稳定的限制

Abstract: Training vision-language models on cognitively-plausible amounts of data
requires rethinking how models integrate multimodal information. Within the
constraints of the Vision track for the BabyLM Challenge 2025, we propose a
lightweight decoder-based architecture with (1) token-wise dynamic gating for
adaptive fusion of linguistic and visual cues, (2) feature modulation and
channel attention to maximise the utility of limited visual information and (3)
auxiliary contrastive objectives for visual grounding. Evaluation on five
benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows
competitive or superior performance to multimodal baselines. More notably, our
dynamic gate discovers interpretable patterns without explicit supervision,
favouring visual cues for content words and linguistic cues for function words.
While we identify limitations in the Challenge constraints, such as the
information bottleneck created by global image embeddings and training
instability from the dataset split, our findings establish dynamic gating as a
powerful tool for efficient multimodal learning, offering both interpretability
and performance even under severe constraints.

</details>


### [68] [AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents](https://arxiv.org/abs/2510.08511)
*Shangheng Du,Xiangchao Yan,Dengyang Jiang,Jiakang Yuan,Yusong Hu,Xin Li,Liang He,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: AutoMLGen是一个基于LLM的编码代理，通过整合领域知识库和蒙特卡洛图搜索来解决机器学习工程任务中的挑战，在MLE-Bench上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在AutoML和Kaggle等机器学习工程场景中缺乏细粒度领域先验知识，现有MLE方法受限于线性或树状搜索结构，无法充分利用历史轨迹和跨分支信息共享，限制了自进化能力和搜索空间多样性。

Method: 提出AutoMLGen，整合领域知识库提供高质量先验指导，采用蒙特卡洛图搜索（MCGS）实现动态路径重组、历史轨迹重用和多解决方案融合，结合细粒度操作符集提高稳定性和收敛速度。

Result: 在MLE-Bench评估中，AutoMLGen在12小时预算（标准运行时间的一半）下，在平均奖牌率和有效提交率等多个维度上实现了最先进的性能。

Conclusion: AutoMLGen通过结合领域知识库和蒙特卡洛图搜索，有效解决了机器学习工程任务中的知识转移和搜索效率问题，显著提升了性能表现。

Abstract: Large language models (LLMs) have shown impressive performance in general
programming tasks. However, in Machine Learning Engineering (MLE) scenarios
such as AutoML and Kaggle competitions, achieving high performance depends
heavily on expert intervention and repeated adjustments rather than simply
generating correct code. When applied directly to these tasks, LLMs often lack
fine-grained domain priors, and existing MLE approaches that use linear or
tree-structured searches limit knowledge transfer to adjacent hierarchical
links. As a result, they cannot leverage past full trajectories or share
information across branches, limiting self-evolving ability and search space
diversity. To address these limitations, we introduce AutoMLGen, an LLM-based
coding agent that integrates a domain knowledge base for high-quality prior
guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS
retains the tree-guided exploration of MCTS while embedding a graph structure
into the expansion stage to enable dynamic path reorganization, historical
trajectory reuse, and multi-solution fusion to support both self-evolution and
collaborative learning. Combined with fine-grained operator sets, this design
improves stability and accelerates convergence. Evaluation on the MLE-Bench
shows that AutoMLGen achieves state-of-the-art performance in numerous
dimensions, such as the average medal rate and the valid submission rate, under
a 12-hour budget (half the standard runtime). The code is available at
https://github.com/Alpha-Innovator/InternAgent.

</details>


### [69] [CaRT: Teaching LLM Agents to Know When They Know Enough](https://arxiv.org/abs/2510.08517)
*Grace Liu,Yuxiao Qu,Jeff Schneider,Aarti Singh,Aviral Kumar*

Main category: cs.AI

TL;DR: CaRT方法通过反事实轨迹对和语言推理训练LLMs学习何时停止信息收集，在医疗诊断和数学问题解决中提高了信息收集效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 许多任务需要模型在多轮交互中策略性地收集相关信息，但现有模型缺乏知道何时停止收集信息并做出决策的能力，容易过度思考或被干扰。

Method: CaRT使用反事实轨迹对进行微调：一条轨迹在适当终止点停止，另一条是相同轨迹的微小修改版本但不应终止。通过语言推理解释终止决策的合理性。

Result: 在交互式医疗诊断和数学问题解决两个领域中，CaRT相比其他微调方法提高了信息收集效率和任务成功率。

Conclusion: CaRT方法能够有效教导LLMs何时停止寻求信息，通过反事实推理和语言解释增强了模型的战略信息收集能力。

Abstract: Many tasks require learned models to strategically gather relevant
information over multiple rounds of interaction before actually acting on a
task. Strategic information gathering requires models to know not only how to
effectively acquire information, but also when to stop gathering information
and make a decision, in order to avoid overthinking or getting derailed when
acting. In this paper, we formalize this problem and introduce Counterfactuals
and Reasoning for Termination (CaRT), an approach for teaching LLMs when to
stop seeking information. To appropriately learn when to terminate, CaRT
fine-tunes LLMs using counterfactual pairs of trajectories, one where
termination is appropriate and a minimally modified version of the same
trajectory where it is not. It trains the LLM to explain the rationale for the
termination decision in either case via verbal reasoning, and imbues this
capability into the base LLM via fine-tuning. We instantiate CaRT in two
domains: interactive medical diagnosis and math problem solving. In both
domains, we find that CaRT improves the efficiency of information gathering and
task success rate compared to other fine-tuning methods.

</details>


### [70] [FlowSearch: Advancing deep research with dynamic structured knowledge flow](https://arxiv.org/abs/2510.08521)
*Yusong Hu,Runmin Ma,Yue Fan,Jinxin Shi,Zongsheng Cao,Yuhao Zhou,Jiakang Yuan,Xiangchao Yan,Wenlong Zhang,Lei Bai,Bo Zhang*

Main category: cs.AI

TL;DR: FlowSearch是一个多智能体框架，通过构建动态结构化知识流来驱动子任务执行和推理，在跨学科研究场景中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 深度研究任务需要广度和深度的思考，涉及导航多样知识空间和推理复杂多步依赖关系，这对智能体系统提出了重大挑战。

Method: 提出FlowSearch多智能体框架，能够战略性地规划和扩展知识流以实现并行探索和分层任务分解，并根据中间推理结果和洞察实时调整知识流。

Result: 在通用和科学基准测试（包括GAIA、HLE、GPQA和TRQA）上实现了最先进的性能，展示了其在多学科研究场景中的有效性。

Conclusion: FlowSearch框架在推进科学发现方面具有潜力，代码已在GitHub上开源。

Abstract: Deep research is an inherently challenging task that demands both breadth and
depth of thinking. It involves navigating diverse knowledge spaces and
reasoning over complex, multi-step dependencies, which presents substantial
challenges for agentic systems. To address this, we propose FlowSearch, a
multi-agent framework that actively constructs and evolves a dynamic structured
knowledge flow to drive subtask execution and reasoning. FlowSearch is capable
of strategically planning and expanding the knowledge flow to enable parallel
exploration and hierarchical task decomposition, while also adjusting the
knowledge flow in real time based on feedback from intermediate reasoning
outcomes and insights. FlowSearch achieves state-of-the-art performance on both
general and scientific benchmarks, including GAIA, HLE, GPQA and TRQA,
demonstrating its effectiveness in multi-disciplinary research scenarios and
its potential to advance scientific discovery. The code is available at
https://github.com/Alpha-Innovator/InternAgent.

</details>


### [71] [Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)
*Kai Zhang,Xiangchao Chen,Bo Liu,Tianci Xue,Zeyi Liao,Zhihan Liu,Xiyao Wang,Yuting Ning,Zhaorun Chen,Xiaohan Fu,Jian Xie,Yuxuan Sun,Boyu Gou,Qi Qi,Zihang Meng,Jianwei Yang,Ning Zhang,Xian Li,Ashish Shah,Dat Huynh,Hengduo Li,Zi Yang,Sara Cao,Lawrence Jang,Shuyan Zhou,Jiacheng Zhu,Huan Sun,Jason Weston,Yu Su,Yifan Wu*

Main category: cs.AI

TL;DR: 提出了一种名为"早期经验"的中间范式，使用智能体自身交互数据作为监督，无需奖励信号，通过隐式世界建模和自我反思策略提升语言智能体的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言智能体主要依赖专家数据进行监督微调，但专家数据覆盖范围有限且难以扩展。强化学习在缺乏可验证奖励或需要长序列交互的环境中训练困难。需要一种介于监督学习和完全经验驱动之间的训练范式。

Method: 1. 隐式世界建模：使用收集的状态来让策略基于环境动态；2. 自我反思：智能体从次优行动中学习，改进推理和决策制定。在八个多样化环境和多个模型家族中进行评估。

Result: 方法在多个环境中一致提高了有效性和跨领域泛化能力。在有可验证奖励的环境中，早期经验为后续强化学习提供了良好基础。

Conclusion: 早期经验作为模仿学习和完全经验驱动智能体之间的实用桥梁，展示了通过智能体自身交互数据学习的价值，为语言智能体的持续改进提供了可行路径。

Abstract: A long-term goal of language agents is to learn and improve through their own
experience, ultimately outperforming humans in complex, real-world tasks.
However, training agents from experience data with reinforcement learning
remains difficult in many environments, which either lack verifiable rewards
(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn
tool use). As a result, most current agents rely on supervised fine-tuning on
expert data, which is challenging to scale and generalizes poorly. This
limitation stems from the nature of expert demonstrations: they capture only a
narrow range of scenarios and expose the agent to limited environment
diversity. We address this limitation with a middle-ground paradigm we call
early experience: interaction data generated by the agent's own actions, where
the resulting future states serve as supervision without reward signals. Within
this paradigm we study two strategies of using such data: (1) Implicit world
modeling, which uses collected states to ground the policy in environment
dynamics; and (2) Self-reflection, where the agent learns from its suboptimal
actions to improve reasoning and decision-making. We evaluate across eight
diverse environments and multiple model families. Our approaches consistently
improve effectiveness and out-of-domain generalization, highlighting the value
of early experience. Moreover, in environments with verifiable rewards, our
results provide promising signals that early experience offers a strong
foundation for subsequent reinforcement learning, positioning it as a practical
bridge between imitation learning and fully experience-driven agents.

</details>


### [72] [How to Teach Large Multimodal Models New Skills](https://arxiv.org/abs/2510.08564)
*Zhen Zhu,Yiming Gong,Yao Xiao,Yaoyao Liu,Derek Hoiem*

Main category: cs.AI

TL;DR: 该论文研究了如何在不遗忘先前能力的情况下训练大型多模态模型新技能，发现通过选择性更新特定层可以显著减少遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型多模态模型在顺序微调新技能时出现的灾难性遗忘问题，保持模型原有能力。

Method: 在五个目标技能上进行顺序微调，同时监控八个基准测试的性能，识别输出token分布的变化，并提出两种简单的调优方法：仅更新自注意力投影层，或仅更新MLP的Gate&Up层。

Result: 提出的两种调优方法在获得强大目标技能的同时，能够很大程度上保持原有基准测试性能，有效缓解遗忘问题。

Conclusion: 通过选择性更新特定层可以实现大型多模态模型的有效持续学习，平衡新技能学习和原有能力保持。

Abstract: How can we teach large multimodal models (LMMs) new skills without erasing
prior abilities? We study sequential fine-tuning on five target skills while
monitoring general ability on eight held-out benchmarks across three model
families. We observe that apparent "forgetting" on held-out tasks after narrow
fine-tuning can partly recover at later stages. We trace this behavior to a
measurable shift in the output token distribution, manifested through a simple
counting-bias probe that co-varies with forgetting. Guided by this picture, we
identify two simple, robust tuning recipes that learn strongly while limiting
drift: (i) updating only the self-attention projection layers, and (ii)
updating only the MLP Gate&Up while freezing the Down projection. Across models
and tasks, these choices deliver strong target gains while largely preserving
held-out performance. Code is available at
https://github.com/jessemelpolio/LMM_CL

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [73] [List Recoverable Codes: The Good, the Bad, and the Unknown (hopefully not Ugly)](https://arxiv.org/abs/2510.07597)
*Nicolas Resch,S. Venkitesh*

Main category: cs.IT

TL;DR: 本文综述了列表恢复码的最新研究进展，包括可能性结果、不可能性结果以及未知问题，并展示了列表恢复码在理论计算机科学中的多种应用。


<details>
  <summary>Details</summary>
Motivation: 列表恢复是纠错码的基本任务，它从最坏情况错误和列表解码中大大推广了唯一解码。该问题在编码理论和理论计算机科学中以多种形式出现。

Method: 通过调查分析近期关于列表恢复码的研究成果，包括可能性证明、不可能性证明以及尚未解决的问题。

Result: 展示了列表恢复码不仅作为列表级联解码码的组成部分，还在理论计算机科学的其他领域中找到了多种应用和联系。

Conclusion: 列表恢复码是一个基础且重要的研究领域，在编码理论和理论计算机科学中具有广泛的应用价值，但仍有许多开放问题需要进一步研究。

Abstract: List recovery is a fundamental task for error-correcting codes, vastly
generalizing unique decoding from worst-case errors and list decoding. Briefly,
one is given ''soft information'' in the form of input lists S_1,...,S_n of
bounded size, and one argues that there are not too many codewords that agree a
lot with this soft information. This general problem appears in many guises,
both within coding theory and in theoretical computer science more broadly.
  In this article we survey recent results on list recovery codes, introducing
both the ''good'' (i.e., possibility results, showing that codes with certain
list recoverability exist), the ''bad'' (impossibility results), and the
''unknown''. We additionally demonstrate that, while list recoverable codes
were initially introduced as a component in list decoding concatenated codes,
they have since found myriad applications to and connections with other topics
in theoretical computer science.

</details>


### [74] [Is star complexity a proxy for information based complexity of graphs?](https://arxiv.org/abs/2510.07722)
*Russell K. Standish*

Main category: cs.IT

TL;DR: 本文研究信息基复杂度(IBC)在图形上的应用，比较了两种不同的图形复杂度度量：基于链接编码的C度量和基于星形操作的星复杂度C*度量，并通过实验验证它们的相关性。


<details>
  <summary>Details</summary>
Motivation: 研究不同实用IBC度量之间的渐近等价性，特别关注图形复杂度度量，探索星复杂度与标准IBC度量之间的关系。

Method: 构建了10和22个顶点的图形，计算其星复杂度至8，并经验性地比较C*与C度量，同时寻找星复杂度的易计算上界。

Result: 发现C*与C度量在经验上具有强相关性，且找到了与C强相关的星复杂度易计算上界。

Conclusion: 实用IBC度量确实表现出渐近等价性，星复杂度与标准IBC度量密切相关，为图形复杂度分析提供了新的视角。

Abstract: Information-based complexity (IBC) is a well-defined complexity measure of
any object given a description in a language and a classifier that identifies
those descriptions with the object. Of course, the exact numerical value will
vary according to the descriptive language and classifier, but under certain
universality conditions (eg the classifier identifies programs of a universal
Turning machine that halt and output the same value), asymptotically, the
complexity measure is independent of the classifier up to a constant of O(1).
The hypothesis being investigated in this work that any practical IBC measure
will similarly be asymptotically equivalent to any other practical IBC measure.
Standish presented an IBC measure for graphs ${\cal C}$ that encoded graphs by
their links, and identifies graphs as those that are automorphic to each other.
An interesting alternate graph measure is {\em star complexity}, which is
defined as the number of union and intersection operations of basic stars that
can generate the original graph. Whilst not an IBC itself, it can be related to
an IBC (called ${\cal C}^*$) that is strongly correlated with star complexity.
In this paper, 10 and 22 vertex graphs are constructed up to a star complexity
of 8, and the ${\cal C}^*$ compared emprically with ${\cal C}$. Finally, an
easily computable upper bound of star complexity is found to be strongly
related to ${\cal C}$.

</details>


### [75] [Integrated Localization, Mapping, and Communication through VCSEL-Based Light-emitting RIS (LeRIS)](https://arxiv.org/abs/2510.08071)
*Rashid Iqbal,Dimitrios Bozanis,Dimitrios Tyrovolas,Christos K. Liaskos,Muhammad Ali Imran,George K. Karagiannidis,Hanaa Abumarshoud*

Main category: cs.IT

TL;DR: 提出了一种基于VCSEL的发光可重构智能表面(LeRIS)架构，集成了用户定位、障碍物感知映射和毫米波通信功能，相比传统LED方案具有更紧凑、低功耗和解析可处理的优势。


<details>
  <summary>Details</summary>
Motivation: 现有LED-based LeRIS设计存在漫射发射问题，LiDAR辅助方案需要笨重的传感模块，需要开发更紧凑、低功耗且解析可处理的集成方案来支持6G无线系统中的多功能可编程无线环境。

Method: 利用VCSEL的窄高斯光束和多模多样性，通过五个VCSEL从接收信号强度联合恢复用户位置和方向，在特定几何条件下可减少到三个VCSEL；同时提出基于反射信号到达时间测量的VCSEL映射方法用于障碍物检测和抗阻塞RIS波束路由。

Result: 仿真结果显示毫米级定位精度、鲁棒的障碍物检测、高频谱效率以及最小用户速率的显著提升。

Conclusion: VCSEL-based LeRIS被确立为可扩展且实际可集成的使能技术，为具有多功能可编程无线环境的弹性6G无线系统提供支持。

Abstract: This paper presents a light-emitting reconfigurable intelligent surface
(LeRIS) architecture that integrates vertical cavity surface emitting lasers
(VCSELs) to jointly support user localization, obstacle-aware mapping, and
millimeter-wave (mmWave) communication in programmable wireless environments
(PWEs). Unlike prior light-emitting diode (LED)-based LeRIS designs with
diffuse emission or LiDAR-assisted schemes requiring bulky sensing modules, the
proposed VCSEL-based approach exploits narrow Gaussian beams and multimode
diversity to enable compact, low-power, and analytically tractable integration.
We derive closed-form expressions to jointly recover user position and
orientation from received signal strength using only five VCSELs, and reduce
this requirement to three under specific geometric conditions by leveraging
dual-mode operation. In parallel, we introduce a VCSEL-based mapping method
that uses reflected signal time-of-arrival measurements to detect obstructions
and guide blockage-resilient RIS beam routing. Simulation results demonstrate
millimeter-level localization accuracy, robust obstacle detection, high
spectral efficiency, and substantial gains in minimum user rate. These findings
establish VCSEL-based LeRIS as a scalable and practically integrable enabler
for resilient 6G wireless systems with multi-functional PWEs.

</details>


### [76] [Near-optimal Rank Adaptive Inference of High Dimensional Matrices](https://arxiv.org/abs/2510.08117)
*Frédéric Zheng,Yassir Jedra,Alexandre Proutiere*

Main category: cs.IT

TL;DR: 该论文研究了高维矩阵估计问题，提出了秩自适应算法，建立了样本复杂度的实例特定下界，并开发了结合最小二乘估计和奇异值阈值处理的算法，其性能接近理论极限。


<details>
  <summary>Details</summary>
Motivation: 解决高维矩阵从线性测量中的估计问题，特别关注设计能够自适应确定有效秩的最优算法，以平衡奇异值估计精度和近似成本之间的权衡。

Method: 提出结合最小二乘估计器和通用奇异值阈值处理的算法，基于增强的矩阵去噪方法分析，通过奇异值阈值技术进行矩阵估计。

Result: 建立了样本复杂度的实例特定下界，揭示了有效秩选择的基本权衡，提出的算法在有限样本下具有误差界限，性能接近理论极限。

Conclusion: 该研究为高维矩阵估计提供了理论框架和实用算法，在多变量回归和线性动态系统识别等应用中验证了方法的有效性。

Abstract: We address the problem of estimating a high-dimensional matrix from linear
measurements, with a focus on designing optimal rank-adaptive algorithms. These
algorithms infer the matrix by estimating its singular values and the
corresponding singular vectors up to an effective rank, adaptively determined
based on the data. We establish instance-specific lower bounds for the sample
complexity of such algorithms, uncovering fundamental trade-offs in selecting
the effective rank: balancing the precision of estimating a subset of singular
values against the approximation cost incurred for the remaining ones. Our
analysis identifies how the optimal effective rank depends on the matrix being
estimated, the sample size, and the noise level. We propose an algorithm that
combines a Least-Squares estimator with a universal singular value thresholding
procedure. We provide finite-sample error bounds for this algorithm and
demonstrate that its performance nearly matches the derived fundamental limits.
Our results rely on an enhanced analysis of matrix denoising methods based on
singular value thresholding. We validate our findings with applications to
multivariate regression and linear dynamical system identification.

</details>


### [77] [Exponential Error Bounds for Information Bottleneck Source Coding Problems](https://arxiv.org/abs/2510.08364)
*Han Wu,Hamdi Joudeh*

Main category: cs.IT

TL;DR: 本文研究了信息瓶颈源编码问题的误差指数和强逆指数，建立了精确的指数表达式，并揭示了IB源编码与带辅助源的源编码之间的码级联系。


<details>
  <summary>Details</summary>
Motivation: 研究信息瓶颈源编码在超量失真概率下的收敛速度，特别是当速率高于或低于率失真函数时的指数收敛行为，以及建立与带辅助源的源编码问题的联系。

Method: 通过推导匹配的上界和下界指数界来建立精确的误差指数和强逆指数，涉及辅助随机变量的优化。同时通过码级连接分析IB源编码与Wyner-Ahlswede-Körner问题的关系。

Result: 获得了IB源编码的精确误差指数和强逆指数，这些指数涉及辅助随机变量的优化。同时证明了WAK问题的每个码都是IB源编码的码，并重新推导了WAK问题的最佳已知球堆积指数。

Conclusion: 本文为信息瓶颈源编码建立了完整的指数理论，揭示了其与带辅助源的源编码问题的深刻联系，为相关编码问题提供了新的理解和操作解释。

Abstract: We study the information bottleneck (IB) source coding problem, also known as
remote lossy source coding under logarithmic loss. Based on a rate-limited
description of noisy observations, the receiver produces a soft estimate for
the remote source, i.e., a probability distribution, evaluated under the
logarithmic loss. We focus on the excess distortion probability of IB source
coding and investigate how fast it converges to 0 or 1, depending on whether
the rate is above or below the rate-distortion function. The latter case is
also known as the exponential strong converse. We establish both the exact
error exponent and the exact strong converse exponent for IB source coding by
deriving matching upper and lower exponential bounds. The obtained exponents
involve optimizations over auxiliary random variables. The matching converse
bounds are derived through non-trivial extensions of existing sphere packing
and single-letterization techniques, which we adapt to incorporate auxiliary
random variables.
  In the second part of this paper, we establish a code-level connection
between IB source coding and source coding with a helper, also known as the
Wyner-Ahlswede-K\"orner (WAK) problem. We show that every code for the WAK
problem is a code for IB source coding. This requires noticing that IB source
coding, under the excess distortion criterion, is equivalent to source coding
with a helper available at both the transmitter and the receiver; the latter in
turn relates to the WAK problem. Through this connection, we re-derive the best
known sphere packing exponent of the WAK problem, and provide it with an
operational interpretation.

</details>


### [78] [A Rate-Distortion Bound for ISAC](https://arxiv.org/abs/2510.08487)
*Mohammadreza Bakhshizadeh Mohajer,Alex Dytso,Daniela Tuninetti,Luca Barletta*

Main category: cs.IT

TL;DR: 本文提出了一种基于率失真理论的逆界，用于分析集成感知与通信系统的性能极限，克服了传统估计理论中贝叶斯克拉美罗界的限制条件。


<details>
  <summary>Details</summary>
Motivation: 传统估计理论如贝叶斯克拉美罗界存在严格的规律性条件限制，无法适用于任意参数分布和失真度量，需要一种更通用的性能界限分析方法。

Method: 引入基于率失真理论的率失真界，适用于任意参数分布和失真度量（包括均方误差和错误概率），并在高感知噪声区域证明其紧致性。

Result: 率失真界在高感知噪声区域是紧致的，在低感知噪声区域可以严格优于贝叶斯克拉美罗界。在Nakagami衰落信道估计和二进制占用检测等挑战性场景中验证了其有效性。

Conclusion: 该工作为表征集成感知与通信系统的最终性能权衡提供了一个强大而通用的工具。

Abstract: This paper addresses the fundamental performance limits of Integrated Sensing
and Communication (ISAC) systems by introducing a novel converse bound based on
rate-distortion theory. This rate-distortion bound (RDB) overcomes the
restrictive regularity conditions of classical estimation theory, such as the
Bayesian Cram\'er-Rao Bound (BCRB). The proposed framework is broadly
applicable, holding for arbitrary parameter distributions and distortion
measures, including mean-squared error and probability of error. The bound is
proved to be tight in the high sensing noise regime and can be strictly tighter
than the BCRB in the low sensing noise regime. The RDB's utility is
demonstrated on two challenging scenarios: Nakagami fading channel estimation,
where it provides a valid bound even when the BCRB is inapplicable, and a
binary occupancy detection task, showcasing its versatility for discrete
sensing problems. This work provides a powerful and general tool for
characterizing the ultimate performance tradeoffs in ISAC systems.

</details>
