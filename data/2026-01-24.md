<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 52]
- [cs.IT](#cs.IT) [Total: 15]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Resource Allocation and Sharing for UAV-Assisted Integrated TN-NTN with Multi-Connectivity](https://arxiv.org/abs/2601.15532)
*Abd Ullah Khan,Wali Ullah Khan,Haejoon Jung,Hyundong Shin*

Main category: cs.NI

TL;DR: 该论文研究了在集成地面-非地面网络中，为具有多连接能力的动态无人机进行资源分配，考虑了频谱共享和公平性问题，提出了两种优化算法：一种最大化容量，另一种确保公平性。


<details>
  <summary>Details</summary>
Motivation: 无人机在集成地面-非地面网络中通过多连接实现高效可靠的数据传输，但由于无人机移动性导致的信道变化、异构设备的多样化QoS需求、频谱资源共享以及容量分配的公平性要求，使得最优资源分配面临挑战。

Method: 考虑三种链路类型（无人机-基站、无人机-无人机、无人机-高空平台）和两种具有不同QoS需求的无人机类型。提出两种算法：第一种算法在确保无人机-无人机链路可靠性的同时，最大化无人机-基站和无人机-高空平台链路的容量；第二种算法通过最大化所有链路的最小容量来确保公平性。

Result: 通过仿真验证了两种算法的性能，表明所提方法能够有效优化资源分配，既能在保证可靠性的前提下提升系统容量，又能确保不同链路间的公平性。

Conclusion: 该研究为集成地面-非地面网络中具有多连接能力的动态无人机提供了有效的资源分配解决方案，通过两种互补的算法分别解决了容量最大化和公平性保障的问题，为实际部署提供了理论支持。

Abstract: Unmanned aerial vehicles (UAVs) with multi- connectivity (MC) capabilities efficiently and reliably transfer data between terrestrial networks (TNs) and non-terrestrial networks (NTNs). However, optimally sharing and allocating spectrum and power resources to maintain MC while ensuring reliable connectivity and optimal performance remains challeng- ing in such networks. Channel variations induced by mobility in UAV networks, coupled with the varying quality of service (QoS) demands of heterogeneous devices, resource sharing, and fairness requirements in capacity distribution pose challenges to optimal resource allocation. Thus, this paper investigates resource allocation for QoS-constrained, MC-enabled, dynamic UAVs in an integrated TN-NTN environment with spectrum sharing and fairness considerations. To this end, we consider three types of links: UAV-to-radio base station (RBS), UAV-to-UAV, and UAV-to-HAP. We also assume two types of UAVs with diverse QoS requirements to reflect a practical scenario. Consequently, we propose two algorithms. The first algorithm maximizes the capacity of UAVs-RBS and UAVs-HAP links while ensuring the reliability of the UAV-UAV link. To achieve this, the algorithm maximizes the collective throughput of the UAVs by optimizing the sum capacity of all the UAV-RBS and UAV-HAP links. Next, to provide constant capacity to all links and ensure fairness, we propose another algorithm that maximizes the minimum capacity across all links. We validate the performance of both algorithms through simulation

</details>


### [2] [MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments](https://arxiv.org/abs/2601.15578)
*Cyril Shih-Huan Hsu,Xi Li,Lanfranco Zanzi,Zhiheng Yang,Chrysa Papagianni,Xavier Costa Pérez*

Main category: cs.NI

TL;DR: MapViT：基于Vision Transformer的两阶段框架，用于预测环境变化和无线信号质量，适用于移动机器人实时导航


<details>
  <summary>Details</summary>
Motivation: 移动和无线网络的发展使机器人自主性得以充分发挥，但机器人在动态环境中需要准确理解周围环境和无线信号质量，这在高度动态的环境中仍然是一个未解决的挑战性问题

Method: 提出MapViT框架，采用两阶段Vision Transformer架构，借鉴大语言模型的预训练-微调范式，通过自监督预训练阶段构建几何基础模型，然后在有限标注数据下进行下游预测

Result: 实验表明该两阶段流水线支持实时预测，ViT实现方案在准确性和计算效率之间取得良好平衡，自监督预训练提高了数据效率和可迁移性，即使在有限标注数据下也能有效预测

Conclusion: MapViT是能量和资源受限平台（如移动机器人）的有前景解决方案，为下一代数字孪生生态系统奠定基础，并为未来6G系统中驱动多模态智能的新型机器学习基础模型铺平道路

Abstract: Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.

</details>


### [3] [RF Intelligence for Health: Classification of SmartBAN Signals in overcrowded ISM band](https://arxiv.org/abs/2601.15836)
*Nicola Gallucci,Giacomo Aragnetti,Matteo Malagrinò,Francesco Linsalata,Maurizio Magarini,Lorenzo Mucchi*

Main category: cs.NI

TL;DR: 提出首个开源框架，用于在2.4GHz ISM频段中自动识别体域网SmartBAN信号，结合合成数据和真实RF采集，使用基于ResNet编码器和U-Net解码器的深度学习模型，在密集频谱环境中实现可靠信号识别。


<details>
  <summary>Details</summary>
Motivation: 在2.4GHz ISM频段中，医疗传感器的低功率传输难以识别，因为存在强同信道干扰和与共存技术的显著功率不对称。准确分类RF信号对于可靠的可穿戴健康监测系统至关重要，需要了解医疗协议运行的干扰条件。

Method: 开发首个开源框架，结合模拟信号的合成数据集和通过软件定义无线电获取的真实RF采集数据。使用基于ResNet编码器和U-Net解码器的深度卷积神经网络，并加入注意力机制，在不同传播条件下进行训练和评估。

Result: 在合成数据集上实现超过90%的准确率，在真实空中频谱图上表现出稳定的性能。该框架能够在密集频谱环境中可靠识别SmartBAN信号。

Conclusion: 通过实现可靠的SmartBAN信号识别，该框架支持干扰感知共存策略，提高了可穿戴医疗系统的可靠性，为体域网在拥挤频段中的稳定运行提供了解决方案。

Abstract: Accurate classification of Radio-Frequency (RF) signals is essential for reliable wearable health-monitoring systems, providing awareness of the interference conditions in which medical protocols operate. In the overcrowded 2.4 GHz ISM band, however, identifying low-power transmissions from medical sensors is challenging due to strong co-channel interference and substantial power asymmetry with coexisting technologies. This work introduces the first open source framework for automatic recognition of SmartBAN signals in Body Area Networks (BANs). The framework combines a synthetic dataset of simulated signals with real RF acquisitions obtained through Software-Defined Radios (SDRs), enabling both controlled and realistic evaluation. Deep convolutional neural networks based on ResNet encoders and U-Net decoders with attention mechanisms are trained and assessed across diverse propagation conditions. The proposed approach achieves over 90% accuracy on synthetic datasets and demonstrates consistent performance on real over-the-air spectrograms. By enabling reliable SmartBAN signal recognition in dense spectral environments, this framework supports interferenceaware coexistence strategies and improves the dependability of wearable healthcare systems.

</details>


### [4] [Dynamic Server Allocation Under Stochastic Switchover on Time-Varying Links](https://arxiv.org/abs/2601.15904)
*Hossein Mohammadalizadeh,Holger Karl*

Main category: cs.NI

TL;DR: 提出ACI框架解决带切换延迟的资源分配问题，通过非短视的帧调度优化吞吐量-延迟权衡


<details>
  <summary>Details</summary>
Motivation: 传统并行队列资源分配方案在考虑切换延迟时性能下降，特别是当切换延迟具有随机性和非均匀性时，经典的Max-Weight策略因忽略切换延迟而表现不佳

Method: 提出ACI（非短视、基于帧的调度框架），通过Lyapunov漂移分析证明基于积压的ACI在缩放容量区域内是吞吐量最优的，并在多无人机网络FSO回程中验证有效性

Result: ACI框架能有效摊销切换延迟，通过调整核心紧迫度指标提供了吞吐量-延迟权衡的灵活性

Conclusion: ACI框架解决了带切换延迟的资源分配问题，在吞吐量最优性和延迟性能之间提供了可调节的权衡，适用于多无人机网络等实际场景

Abstract: Dynamic resource allocation to parallel queues is a cornerstone of network scheduling, yet classical solutions often fail when accounting for the overhead of switching delays to queues with superior link conditions. In particular, system performance is further degraded when switching delays are stochastic and inhomogeneous. In this domain, the myopic, Max-Weight policy struggles, as it is agnostic to switching delays. This paper introduces ACI, a non-myopic, frame-based scheduling framework that directly amortizes these switching delays. We first use a Lyapunov drift analysis to prove that backlog-driven ACI is throughput-optimal with respect to a scaled capacity region; then validate ACI's effectiveness on multi-UAV networks with an FSO backhaul. Finally, we demonstrate how adapting its core urgency metric provides the flexibility to navigate the throughput-latency trade-off.

</details>


### [5] [Low-altitude Multi-UAV-assisted Data Collection and Semantic Forwarding for Post-Disaster Relief](https://arxiv.org/abs/2601.16146)
*Xiaoya Zheng,Geng Sun,Jiahui Li,Jiacheng Wang,Weijie Yuan,Qingqing Wu,Dusit Niyato,Abbas Jamalipour*

Main category: cs.NI

TL;DR: 提出LLM-AOA方法优化多无人机数据收集与语义转发网络，通过集群协作和协同波束成形提高传输效率


<details>
  <summary>Details</summary>
Motivation: 传统无人机通信面临长距离链路脆弱性和数据瓶颈问题，需要解决灾后通信恢复中的数据传输效率与能耗平衡

Method: 采用多无人机集群协作，结合语义提取和虚拟天线阵列协同波束成形，提出LLM-AOA交替优化方法处理MINLP问题

Result: LLM-AOA相比AOA在传输速率和语义速率上分别提升26.8%和22.9%，有效优化了系统性能

Conclusion: LLM-AOA方法能有效解决低空多无人机数据收集与语义转发网络的优化问题，为低空经济通信系统提供高效解决方案

Abstract: The low-altitude economy (LAE) is an emerging economic paradigm which fosters integrated development across multiple fields. As a pivotal component of the LAE, low-altitude uncrewed aerial vehicles (UAVs) can restore communication by serving as aerial relays between the post-disaster areas and remote base stations (BSs). However, conventional approaches face challenges from vulnerable long-distance links between the UAVs and remote BSs, and data bottlenecks arising from massive data volumes and limited onboard UAV resources. In this work, we investigate a low-altitude multi-UAV-assisted data collection and semantic forwarding network, in which multiple UAVs collect data from ground users, form clusters, perform intra-cluster data aggregation with semantic extraction, and then cooperate as virtual antenna array (VAAs) to transmit the extracted semantic information to a remote BS via collaborative beamforming (CB). We formulate a data collection and semantic forwarding multi-objective optimization problem (DCSFMOP) that jointly maximizes both the user and semantic transmission rates while minimizing UAV energy consumption. The formulated DCSFMOP is a mixed-integer nonlinear programming (MINLP) problem that is inherently NP-hard and characterized by dynamically varying decision variable dimensionality. To address these challenges, we propose a large language model-enabled alternating optimization approach (LLM-AOA), which effectively handles the complex search space and variable dimensionality by optimizing different subsets of decision variables through tailored optimization strategies. Simulation results demonstrate that LLM-AOA outperforms AOA by approximately 26.8\% and 22.9\% in transmission rate and semantic rate, respectively.

</details>


### [6] [Real-Time HAP-Assisted Vehicular Edge Computing for Rural Areas](https://arxiv.org/abs/2301.09957)
*Alessandro Traspadini,Marco Giordani,Giovanni Giambene,Michele Zorzi*

Main category: cs.NI

TL;DR: 该论文研究在乡村场景中利用高空平台支持车载边缘计算，分析车辆如何优化任务卸载策略以最大化实时服务概率。


<details>
  <summary>Details</summary>
Motivation: 非地面网络是6G网络的关键组成部分，能够扩展网络覆盖至偏远地区。高空平台可作为边缘服务器处理地面设备（如物联网传感器和地面车辆）的计算任务卸载，特别是在乡村场景中支持车载边缘计算。

Method: 将系统建模为一组队列，计算任务按照泊松到达过程到达。然后评估最优的车载边缘计算卸载因子，在延迟和计算能力约束下最大化实时服务概率。

Result: 论文评估了最优的车载边缘计算卸载因子，该因子能够在给定的延迟和计算能力约束下最大化实时服务概率。

Conclusion: 高空平台可以作为有效的边缘服务器支持乡村场景中的车载边缘计算，通过优化任务卸载策略可以显著提高实时服务性能。

Abstract: Non-Terrestrial Networks (NTNs) are expected to be a key component of 6th generation (6G) networks to support broadband seamless Internet connectivity and expand the coverage even in rural and remote areas. In this context, High Altitude Platforms (HAPs) can act as edge servers to process computational tasks offloaded by energy-constrained terrestrial devices such as Internet of Things (IoT) sensors and ground vehicles (GVs). In this paper, we analyze the opportunity to support Vehicular Edge Computing (VEC) via HAP in a rural scenario where GVs can decide whether to process data onboard or offload them to a HAP. We characterize the system as a set of queues in which computational tasks arrive according to a Poisson arrival process. Then, we assess the optimal VEC offloading factor to maximize the probability of real-time service, given latency and computational capacity constraints.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models](https://arxiv.org/abs/2601.15305)
*Alfred Shen,Aaron Shen*

Main category: cs.AI

TL;DR: 提出Gated Sparse Attention (GSA)架构，结合稀疏注意力和门控注意力的优势，在长上下文语言模型中实现高效且稳定的训练。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型中注意力计算负担重，现有稀疏注意力机制和门控注意力变体各自解决不同问题但相互独立，需要结合两者优势。

Method: 提出GSA架构，包含：1) 带sigmoid激活的门控闪电索引器，产生有界可解释的选择分数；2) 基于局部不确定性的自适应稀疏控制器，调节关注token数量；3) 值和输出阶段的双重门控。

Result: 在1.7B参数模型、400B token训练中，GSA达到稀疏基线效率（128K上下文12-16倍加速），同时获得门控注意力质量提升：困惑度从6.03降至5.70，128K上下文RULER分数翻倍，首token注意力从47%降至4%以下，训练稳定性显著改善（损失峰值减少98%）。

Conclusion: GSA成功结合稀疏注意力和门控注意力的互补优势，在保持计算效率的同时显著提升模型质量和训练稳定性，为长上下文语言模型提供了有效的解决方案。

Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.

</details>


### [8] [Uncovering Latent Bias in LLM-Based Emergency Department Triage Through Proxy Variables](https://arxiv.org/abs/2601.15306)
*Ethan Zhang*

Main category: cs.AI

TL;DR: 研究发现LLM在急诊分诊中存在通过代理变量产生的歧视行为，以及系统性地根据输入中的特定标记修改患者严重程度感知的倾向，表明AI系统仍存在偏见问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型已应用于临床决策，但针对不同种族、社会、经济和临床背景患者的隐藏偏见仍然存在，需要研究LLM在急诊分诊中的偏见问题。

Method: 使用32个患者级代理变量（每个变量包含正负两种限定词），在公开数据集（MIMIC-IV-ED Demo, MIMIC-IV Demo）和受限访问数据集（MIMIC-IV-ED, MIMIC-IV）上评估LLM在急诊分诊中的表现。

Result: 研究揭示了急诊分诊场景中通过代理变量产生的歧视行为，以及LLM系统性地根据输入中特定标记的出现（无论正负框架）修改患者严重程度感知的倾向。

Conclusion: AI系统仍基于嘈杂、有时非因果的信号进行训练，这些信号不能可靠反映真实患者严重程度，需要在临床环境中更负责任地部署AI技术。

Abstract: Recent advances in large language models (LLMs) have enabled their integration into clinical decision-making; however, hidden biases against patients across racial, social, economic, and clinical backgrounds persist. In this study, we investigate bias in LLM-based medical AI systems applied to emergency department (ED) triage. We employ 32 patient-level proxy variables, each represented by paired positive and negative qualifiers, and evaluate their effects using both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets as appropriate~\cite{mimiciv_ed_demo,mimiciv_ed,mimiciv}. Our results reveal discriminatory behavior mediated through proxy variables in ED triage scenarios, as well as a systematic tendency for LLMs to modify perceived patient severity when specific tokens appear in the input context, regardless of whether they are framed positively or negatively. These findings indicate that AI systems is still imperfectly trained on noisy, sometimes non-causal signals that do not reliably reflect true patient acuity. Consequently, more needs to be done to ensure the safe and responsible deployment of AI technologies in clinical settings.

</details>


### [9] [DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey](https://arxiv.org/abs/2601.15307)
*Guo-Biao Zhang,Ding-Yuan Liu,Da-Yi Wu,Tian Lan,Heyan Huang,Zhijing Wu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: DeepSurvey-Bench是一个新的基准测试，用于评估生成式综述的学术价值，解决了现有基准测试只关注表面质量而忽视深层学术价值的问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准存在两个关键问题：1）基于引用数量等有缺陷的标准选择人工撰写的综述作为基准数据集，缺乏学术维度标注；2）评估指标只关注表面质量（如结构连贯性），无法评估深层"学术价值"（如核心研究目标和批判性分析）。

Method: 提出DeepSurvey-Bench基准，包含全面的学术价值评估标准，涵盖三个维度：信息价值、学术交流价值和研究指导价值。基于此标准构建带有学术价值标注的可靠数据集，并评估生成式综述的深层学术价值。

Result: 广泛的实验结果表明，该基准在评估生成式综述的学术价值方面与人类评估表现高度一致。

Conclusion: DeepSurvey-Bench能够全面评估生成式综述的学术价值，解决了现有基准测试的局限性，为自动生成科学综述的质量评估提供了更可靠的基准。

Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep "academic value", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.

</details>


### [10] [Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.15311)
*Mustafa Arslan*

Main category: cs.AI

TL;DR: Aeon是一个神经符号认知操作系统，通过结构化记忆宫殿和神经符号情节图解决LLMs在长上下文中的计算成本和"迷失在中间"问题，实现亚毫秒级检索延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs面临二次计算成本和"迷失在中间"现象，而传统的"扁平RAG"架构将记忆视为无结构的嵌入集合，无法捕捉长时交互的层次结构和时间结构，导致"向量迷雾"问题。

Method: 提出Aeon神经符号认知操作系统，将记忆结构化为记忆宫殿（通过Atlas实现的SIMD加速页聚类向量索引）和轨迹（神经符号情节图），并引入语义旁路缓冲区作为预测性缓存机制。

Result: 在对话工作负载上实现<1ms的检索延迟，通过零拷贝C++/Python桥确保状态一致性，为自主代理提供持久化、结构化的记忆能力。

Conclusion: Aeon通过将记忆重新定义为管理的操作系统资源，有效解决了LLMs在长上下文中的性能问题，为自主代理提供了高效、结构化的记忆系统。

Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily "Flat RAG" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to "Vector Haze", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.

</details>


### [11] [The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15316)
*Wei Ai,Yilong Tan,Yuntao Shou,Tao Meng,Haowen Chen,Zhixiong He,Keqin Li*

Main category: cs.AI

TL;DR: 这篇论文是关于多模态假新闻检测（MFND）的综述，重点分析了大视觉语言模型（LVLMs）如何推动该领域从传统特征工程方法向端到端多模态推理框架的范式转变。


<details>
  <summary>Details</summary>
Motivation: 随着大视觉语言模型的快速发展，多模态假新闻检测领域正在经历从传统方法到统一端到端框架的范式转变。然而，目前缺乏系统性的综述来追踪这一转变并整合最新进展。本文旨在填补这一空白，全面回顾LVLMs在多模态假新闻检测中的应用。

Method: 论文采用系统性综述方法：1）提供历史视角，追踪从传统多模态检测流程到基础模型驱动范式的演变；2）建立结构化分类体系，涵盖模型架构、数据集和性能基准；3）分析剩余技术挑战；4）展望未来研究方向。

Result: 这是首个系统记录和分析LVLMs在多模态假新闻检测中变革作用的全面综述。论文建立了完整的分类体系，识别了当前技术挑战（可解释性、时序推理、领域泛化等），并提供了现有方法的GitHub总结。

Conclusion: LVLMs正在彻底改变多模态假新闻检测领域，通过强大的表示学习能力实现视觉和语言的联合建模。本文为理解这一范式转变提供了系统性框架，并为未来研究指明了方向，包括提高模型可解释性、增强时序推理能力和改善领域泛化等关键挑战。

Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.

</details>


### [12] [Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents](https://arxiv.org/abs/2601.15322)
*Raffi Khatchadourian*

Main category: cs.AI

TL;DR: 本文提出DFAH框架，用于评估金融领域LLM代理的轨迹确定性和证据条件忠实度，发现模型确定性与忠实度呈正相关，并提供了金融基准测试工具。


<details>
  <summary>Details</summary>
Motivation: LLM代理在监管审计回放中存在一致性问题：当要求用相同输入重现被标记的交易决策时，大多数部署无法返回一致结果。这暴露了金融服务中工具使用代理的可靠性问题。

Method: 提出确定性-忠实度保证框架（DFAH），用于测量工具使用代理的轨迹确定性和证据条件忠实度。在74种配置（12个模型、4个提供商、每个T=0.0时8-24次运行）中进行非代理基线实验，并提供三个金融基准测试（合规分类、投资组合约束、DataOps异常）。

Result: 7-20B参数模型在非代理基线中达到100%确定性，而120B+模型需要3.7倍更大的验证样本才能达到同等统计可靠性。代理工具使用引入额外方差。确定性与忠实度呈正相关（r=0.45，p<0.01）。Tier 1模型在架构优先设计下达到符合审计回放要求的确定性水平。

Conclusion: DFAH框架能够有效评估金融LLM代理的可靠性，确定性与忠实度并非传统认为的权衡关系而是正相关，架构优先设计的Tier 1模型能够满足审计回放要求，为金融领域部署提供了评估工具和基准。

Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.
  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.
  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.

</details>


### [13] [Prometheus Mind: Retrofitting Memory to Frozen Language Models](https://arxiv.org/abs/2601.15324)
*Mark Wind*

Main category: cs.AI

TL;DR: Prometheus Mind为冻结的Qwen3-4B模型添加记忆功能，使用11个模块化适配器（530MB，7%开销），完全可逆。通过解决提取、训练、注入和隐藏状态崩溃四个问题实现，在PrometheusExtract-132数据集上达到94.4%的检索准确率。


<details>
  <summary>Details</summary>
Motivation: 为预训练语言模型添加记忆功能通常需要架构修改或权重调整，这限制了灵活性和可逆性。本文旨在开发一种完全可逆的方法，通过模块化适配器为冻结模型添加记忆，同时解决相关的技术挑战。

Method: 1. 提取：开发对比方向发现（CDD），通过最小对找到语义方向，无需标注数据。2. 训练：采用分阶段训练，每个适配器在简单代理任务上训练。3. 注入：发现lm_head.weight行已提供所需映射，无需额外训练。4. 隐藏状态崩溃：训练投影恢复语义区分（相似度从0.98降至0.09）。

Result: 在PrometheusExtract-132数据集上，系统在干净输入上达到94.4%检索准确率（n=54，95% CI: [84.9%, 98.1%]），但在非正式输入（省略、填充词、隐含主语）上降至19.4%。主要瓶颈是关系分类（47.3%准确率），导致大多数提取错误。

Conclusion: Prometheus Mind成功为冻结语言模型添加了可逆记忆功能，通过模块化适配器实现。虽然系统在干净输入上表现良好，但在非正式输入上性能显著下降，表明关系分类是主要限制因素。该方法为模型记忆增强提供了灵活、可逆的解决方案。

Abstract: Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.

</details>


### [14] [Logic Programming on Knowledge Graph Networks And its Application in Medical Domain](https://arxiv.org/abs/2601.15347)
*Chuanqing Wang,Zhenmin Zhao,Shanshan Du,Chaoqun Fei,Songmao Zhang,Ruqian Lu*

Main category: cs.AI

TL;DR: 该论文提出了"知识图谱网络"的系统理论、技术和应用，特别是在医疗健康领域，旨在解决当前知识图谱研究中高级逻辑推理、AI技术、概率统计理论应用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱研究快速发展，但在医疗健康等领域的应用中，许多重要的信息处理技术应用滞后，包括未能充分利用高级逻辑推理、先进人工智能技术、专用编程语言、现代概率统计理论等，特别是多知识图谱协同与竞争技术未得到足够重视。

Method: 开发了"知识图谱网络"的系统理论、技术和应用框架，涵盖其定义、开发、推理、计算和应用，考虑了不精确、不确定、多模态、向量化、分布式、联邦学习等不同条件，并在每种情况下提供真实数据示例和实验结果。

Result: 论文提供了知识图谱网络在医疗健康领域的实际应用案例和实验结果，展示了在不同条件下（如不精确、不确定、多模态等）的有效性。

Conclusion: 提出了知识图谱网络的创新理论框架，为医疗健康领域的知识图谱应用提供了系统化的解决方案，填补了多知识图谱协同与竞争技术研究的空白。

Abstract: The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.

</details>


### [15] [GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation](https://arxiv.org/abs/2601.15392)
*Francesca Pia Panaccione,Carlo Sgaravatti,Pietro Pinoli*

Main category: cs.AI

TL;DR: GeMM-GAN是一种基于组织病理学切片和临床元数据的条件生成对抗网络，用于合成真实的基因表达谱，解决了基因表达数据获取的隐私和成本问题。


<details>
  <summary>Details</summary>
Motivation: 基因表达数据在生物医学研究中至关重要，但由于隐私法规严格和实验成本高昂，难以广泛获取。相比之下，医学图像和临床元数据在临床实践中常规收集。需要一种方法能够利用易获取的数据模态来生成基因表达谱。

Method: GeMM-GAN结合了Transformer编码器处理图像块，以及图像块与文本标记之间的交叉注意力机制，生成条件向量来指导生成模型产生生物学一致的基因表达谱。

Result: 在TCGA数据集上的评估显示，该框架优于标准生成模型，生成的基因表达谱更真实且功能更有意义，在下游疾病类型预测任务中比当前最先进的生成模型提高了11%以上的准确率。

Conclusion: GeMM-GAN能够有效利用易获取的组织病理学切片和临床元数据来生成高质量的基因表达谱，为生物医学研究提供了新的数据生成解决方案。

Abstract: Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN

</details>


### [16] [Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)](https://arxiv.org/abs/2601.15397)
*Peidong Wang*

Main category: cs.AI

TL;DR: LOGIC框架通过在解码层直接操作，解决了语音大语言模型识别新实体的挑战，相比提示方法具有更好的可扩展性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型在识别领域特定实体（如联系人姓名、播放列表、技术术语）方面存在局限性，主要依赖提示方法但存在可扩展性问题（上下文窗口限制、推理延迟增加、"迷失在中间"现象），而生成式错误校正方法则容易产生"过度校正"和幻觉问题。

Method: 提出LOGIC（Logit-Space Integration for Contextual Biasing）框架，直接在解码层进行操作，将上下文注入与输入处理解耦，确保相对于提示长度的恒定时间复杂度。

Result: 在11种多语言环境中使用Phi-4-MM模型进行的广泛实验表明，LOGIC实现了平均9%的相对实体词错误率降低，而误报率仅增加0.30%。

Conclusion: LOGIC提供了一种高效且鲁棒的框架，解决了语音大语言模型在识别新实体时的可扩展性和准确性问题，优于现有的提示和生成式错误校正方法。

Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the "lost-in-the-middle" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from "over-correction", introducing hallucinations of entities that were never spoken.
  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.

</details>


### [17] [Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.15436)
*Shahar Ben Natan,Oren Tsur*

Main category: cs.AI

TL;DR: 提出一种通过零和博弈的赌局设置来直接、中立评估LLM谄媚行为的新方法，发现所有模型都有谄媚倾向，但Claude和Mistral在伤害第三方时会表现出"道德悔恨"并过度补偿，且谄媚性与近因偏差会相互增强。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM谄媚性的方法存在各种形式的未受控偏差、噪音或操纵性语言，需要一种更直接、中立的评估方式来准确衡量模型的谄媚倾向。

Method: 使用LLM作为裁判，在零和博弈的赌局设置中评估谄媚性，将谄媚行为视为对用户有利但明确对他人造成成本的行为。比较了Gemini 2.5 Pro、ChatGPT 4o、Mistral-Large-Instruct-2411和Claude Sonnet 3.7四个领先模型。

Result: 所有模型在常见设置中都表现出谄媚倾向，但Claude和Mistral在谄媚行为明确伤害第三方时会表现出"道德悔恨"并过度补偿。所有模型都对最后提出的答案存在近因偏差。谄媚性和近因偏差会相互作用产生"建设性干扰"效应，当用户意见最后呈现时，同意用户的倾向会加剧。

Conclusion: 提出了一种更中立的LLM谄媚性评估框架，揭示了模型在道德权衡中的复杂行为模式，特别是谄媚性与近因偏差的相互作用对模型决策的重要影响。

Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit "moral remorse" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.

</details>


### [18] [A tensor network formalism for neuro-symbolic AI](https://arxiv.org/abs/2601.15442)
*Alex Goessmann,Janina Schütte,Maximilian Fröhlich,Martin Eigel*

Main category: cs.AI

TL;DR: 提出基于张量网络的统一框架，将神经与符号AI方法结合，通过张量分解表示逻辑公式和概率分布，实现高效推理算法。


<details>
  <summary>Details</summary>
Motivation: 神经与符号AI的统一是人工智能领域的核心开放挑战，需要找到能够结合两者优势的数学框架。

Method: 引入张量网络形式主义，使用基编码方案表示函数，将神经分解建模为张量分解，将逻辑公式和概率分布表示为结构化张量分解。

Result: 建立了统一框架，将张量网络收缩作为基本推理类别，提出高效扩展的推理算法（收缩消息传递方案），并开发了Hybrid Logic Network模型和tnreason Python库。

Conclusion: 张量网络为神经与符号AI的统一提供了数学基础，能够定义和训练混合逻辑概率模型，实现了理论框架与实用工具的结合。

Abstract: The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different approaches in tensor decompositions. In particular, we describe a basis encoding scheme for functions and model neural decompositions as tensor decompositions. The proposed formalism can be applied to represent logical formulas and probability distributions as structured tensor decompositions. This unified treatment identifies tensor network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logic, as contraction message passing schemes. The framework enables the definition and training of hybrid logical and probabilistic models, which we call Hybrid Logic Network. The theoretical concepts are accompanied by the python library tnreason, which enables the implementation and practical use of the proposed architectures.

</details>


### [19] [Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases](https://arxiv.org/abs/2601.15476)
*Alex Dantart*

Main category: cs.AI

TL;DR: 大型语言模型在法律高风险工作中通过区分三种AI范式（独立生成、基础RAG、高级RAG）来减少幻觉，高级RAG能将虚假引用率降至0.2%以下，实现可靠的法律应用。


<details>
  <summary>Details</summary>
Motivation: 研究如何使大型语言模型在需要高度准确性的法律工作中可靠，减少幻觉问题，确保法律AI系统能够提供可信赖的专业服务。

Method: 区分三种AI范式：独立生成模型、基础检索增强系统、高级端到端优化RAG系统。引入两个可靠性指标（虚假引用率和虚构事实率），对12个LLM在75个法律任务上生成的2700个司法风格答案进行专家双盲评审。

Result: 独立模型不适合专业使用（虚假引用率超过30%），基础RAG显著减少错误但仍存在明显误接地问题，而采用嵌入微调、重排序和自校正技术的高级RAG能将虚构事实率降至可忽略水平（低于0.2%）。

Conclusion: 可信赖的法律AI需要基于检索的架构，强调验证和可追溯性，该评估框架也适用于其他高风险领域。

Abstract: This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models ("creative oracle"), (2) basic retrieval-augmented systems ("expert archivist"), and (3) an advanced, end-to-end optimized RAG system ("rigorous archivist"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.

</details>


### [20] [MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation](https://arxiv.org/abs/2601.15487)
*Chandan Kumar Sahu,Premith Kumar Chilukuri,Matthew Hetrich*

Main category: cs.AI

TL;DR: MiRAGE是一个多智能体框架，用于生成经过验证的、领域特定的、多模态和多跳问答数据集，以评估RAG系统在专业文档中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统向多模态、高风险企业应用的快速发展超过了领域特定评估基准的开发。现有数据集通常依赖通用领域语料库或纯文本检索，无法捕捉专业技术文档的复杂性，其中信息是多模态的，推理需要综合分散的证据。

Method: MiRAGE采用多智能体框架，协调专门智能体的协作群体：递归上下文优化循环聚合分散证据，对抗性验证智能体保证事实基础，以及识别专家角色和相关领域的智能体来模拟专家认知工作流程。

Result: 在四个不同领域（法规、金融、定量生物学和新闻学）的广泛实证评估表明，MiRAGE生成的数据集具有显著更高的推理复杂度（>2.3平均跳数）和事实忠实度。消融研究指出，如果有图像的文本描述，MiRAGE可以由LLM驱动，但视觉基础仍是前沿挑战。

Conclusion: 通过自动化创建反映专有语料库潜在主题结构的黄金标准评估数据集，MiRAGE为严格基准测试下一代信息检索系统提供了必要的基础设施。

Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.

</details>


### [21] [Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge](https://arxiv.org/abs/2601.15495)
*Yiyang Feng,Zeming Chen,Haotian Wu,Jiawei Zhou,Antoine Bosselut*

Main category: cs.AI

TL;DR: TRACK是一个新的基准测试，用于评估LLMs在多步推理中处理冲突知识的能力，发现提供更新事实反而会降低推理性能


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要关注单一知识更新和事实回忆，缺乏评估知识更新如何影响下游推理，特别是当新知识与模型参数化知识冲突时的传播问题

Method: 引入TRACK基准测试，涵盖三个推理密集型场景（WIKI、CODE、MATH），引入多个现实冲突以反映真实世界复杂性，评估LLMs在冲突知识下的多步推理能力

Result: 提供更新事实进行推理反而比不提供更新事实表现更差，且提供更多更新事实会加剧性能下降；失败原因包括无法忠实整合更新事实，以及即使知识整合成功也存在推理缺陷

Conclusion: TRACK为衡量和指导未来在冲突知识多步推理传播方面的进展提供了一个严格的新基准

Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.

</details>


### [22] [The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers](https://arxiv.org/abs/2601.15509)
*Prasanna Kumar*

Main category: cs.AI

TL;DR: 论文指出Transformer模型在情感分析中虽然提高了准确率，但导致情感类别极化并损害了中立性，这对依赖情感分析结果的工业应用构成严重问题。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer和迁移学习显著提升了情感分析的准确性，但这些改进往往以牺牲中立性为代价，导致情感类别极化。这种中立性的缺失对依赖情感分析结果的工业应用构成了严重威胁。

Method: 通过实验观察发现，Transformer模型在提升某一情感类别准确率的同时，会导致另一情感类别的极化和中立性的失效。

Result: 实验结果表明，Transformer带来的准确率提升伴随着情感类别的极化现象，中立性显著下降，这在应用NLP领域造成了可靠性问题。

Conclusion: Transformer模型在情感分析中的准确性提升存在"阴暗面"——损害中立性并导致情感极化，这对工业应用构成了严峻挑战，需要新的方法来解决这一平衡问题。

Abstract: The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.

</details>


### [23] [TransportAgents: a multi-agents LLM framework for traffic accident severity prediction](https://arxiv.org/abs/2601.15519)
*Zhichao Yang,Jiashu He,Jinxuan Fan,Cirillo Cinzia*

Main category: cs.AI

TL;DR: 提出TransportAgents混合多智能体框架，结合LLM推理与MLP集成模块，用于交通事故严重程度预测，在多个数据集上优于传统方法和单智能体LLM基线。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体大语言模型在处理异构、领域特定的交通事故数据时存在困难，容易产生有偏见或不稳定的预测，需要更稳健的解决方案来提升应急响应和公共安全规划。

Method: 提出TransportAgents混合多智能体框架，包含多个专门处理特定类别信息（如人口统计、环境背景、事故细节）的智能体，通过多层感知机集成模块融合中间严重程度评估，生成统一预测。

Result: 在两个美国数据集（CPSRMS和NEISS）上实验表明，TransportAgents在GPT-3.5、GPT-4o和LLaMA-3.3等多个骨干模型上均优于传统机器学习和先进LLM基线，展现出强鲁棒性、可扩展性和跨数据集泛化能力。

Conclusion: TransportAgents框架能够产生更平衡、校准良好的严重程度预测，比标准单智能体LLM方法更具可解释性和可靠性，适用于安全关键决策支持应用。

Abstract: Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.

</details>


### [24] [From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models](https://arxiv.org/abs/2601.15533)
*Zhikang Chen,Tingting Zhu*

Main category: cs.AI

TL;DR: 当前世界模型过度关注视觉逼真度而忽视物理因果理解，提出应将世界模型重构为可行动模拟器而非视觉引擎，强调因果结构、约束感知和闭环评估。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型存在"视觉混淆"问题，即错误地认为高保真视频生成等同于理解物理和因果动态。视觉逼真度并不能可靠地代表世界理解能力，这在安全关键决策（如医疗决策）中尤为危险。

Method: 提出将世界模型重新定义为可行动模拟器而非视觉引擎，强调三个关键要素：结构化4D接口、约束感知动态学和闭环评估。以医疗决策作为认知压力测试，验证模型在反事实推理、干预规划和长期预见方面的能力。

Result: 现代世界模型虽然在像素预测方面表现出色，但经常违反不变约束、在干预下失败，并在安全关键决策中崩溃。医疗决策案例表明，世界模型的价值不在于其模拟的逼真度，而在于支持反事实推理、干预规划和鲁棒长期预见的能力。

Conclusion: 有效的世界模型必须编码因果结构、尊重领域特定约束并保持长期稳定性。世界模型应被重构为可行动模拟器，强调结构化4D接口、约束感知动态学和闭环评估，而非仅仅追求视觉逼真度。

Abstract: A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.

</details>


### [25] [ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance](https://arxiv.org/abs/2601.15551)
*Bismack Tokoli,Luis Jaimes,Ayesha S. Dina*

Main category: cs.AI

TL;DR: ALIGNAgent是一个多代理教育框架，通过集成知识评估、技能差距识别和针对性资源推荐来实现个性化学习，在真实计算机科学课程数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化学习系统通常是碎片化的，专注于知识追踪、诊断建模或资源推荐中的单一功能，缺乏将这些组件整合成一个连贯自适应循环的系统。

Method: 提出ALIGNAgent多代理框架：1) 技能差距代理处理学生测验表现、成绩数据和偏好，生成主题级熟练度估计；2) 推荐代理检索与诊断缺陷对齐的偏好感知学习材料；3) 实现持续反馈循环，在进入后续主题前进行干预。

Result: 在两个本科计算机科学课程的真实数据集上进行评估，基于GPT-4o的代理在知识熟练度估计方面达到0.87-0.90的精确度和0.84-0.87的F1分数，与实际考试表现验证一致。

Conclusion: ALIGNAgent通过集成知识评估、技能差距识别和资源推荐，提供了一个有效的个性化学习框架，能够显著提升学生成果。

Abstract: Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

</details>


### [26] [Autonomous Business System via Neuro-symbolic AI](https://arxiv.org/abs/2601.15599)
*Cecil Pang,Hiroki Sayama*

Main category: cs.AI

TL;DR: AUTOBUS是一个自主业务系统，结合LLM智能体、谓词逻辑编程和业务语义知识图谱，通过神经符号AI架构协调端到端业务计划。


<details>
  <summary>Details</summary>
Motivation: 当前企业系统基于部门孤岛、僵化工作流和硬编码自动化，无法适应动态业务环境重组需求；而LLM擅长自然语言理解但缺乏确定性、可验证的复杂业务逻辑执行能力。

Method: 将业务计划建模为具有明确前后条件、所需数据、评估规则和API级操作的任务网络；企业数据组织为知识图谱并转换为逻辑事实和基础规则；AI智能体综合任务指令、企业语义和可用工具生成特定任务逻辑程序，由逻辑引擎执行。

Result: 提出AUTOBUS神经符号AI架构，详细描述了系统架构、AI智能体生成的逻辑程序结构，以及人类和辅助工具在业务计划生命周期中的角色。

Conclusion: AUTOBUS通过整合LLM智能体、逻辑编程和业务语义知识图谱，实现了可解释、可验证的业务流程自动化，人类负责定义语义、策略和监督关键决策，确保问责制和适应性。

Abstract: Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.

</details>


### [27] [CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models](https://arxiv.org/abs/2601.15628)
*Haibo Tong,Zeyang Yue,Feifei Zhao,Erliang Lin,Lu Jia,Ruolin Chen,Yinqian Sun,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: CogToM是一个全面的、理论基础的基准测试，包含8000多个双语实例和46个范式，用于评估LLMs是否真正具备类似人类的心理理论能力，揭示了模型性能异质性和与人类认知结构的潜在差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要局限于错误信念任务等狭窄范式，无法全面捕捉人类认知机制的全貌，因此需要更全面的工具来评估LLMs是否真正具备类似人类的心理理论能力。

Method: 开发了CogToM基准测试，包含超过8000个双语实例，涵盖46个不同范式，由49名人类标注者验证。系统评估了22个代表性模型，包括GPT-5.1和Qwen3-Max等前沿模型。

Result: 评估揭示了显著的性能异质性，并突出了特定维度的持续瓶颈。基于人类认知模式的分析表明LLMs与人类认知结构存在潜在差异。

Conclusion: CogToM为研究LLMs不断演化的认知边界提供了强大的工具和视角，有助于更深入地理解LLMs是否真正具备类似人类的心理理论能力。

Abstract: Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.

</details>


### [28] [Agentic AI Governance and Lifecycle Management in Healthcare](https://arxiv.org/abs/2601.15630)
*Chandra Prakash,Mary Lind,Avneesh Sisodia*

Main category: cs.AI

TL;DR: 提出统一代理生命周期管理蓝图，解决医疗领域AI代理扩散带来的治理挑战


<details>
  <summary>Details</summary>
Motivation: 医疗组织在常规工作流程中嵌入AI代理时面临代理扩散问题，导致重复代理、责任不清、控制不一致和工具权限持久化等挑战，现有AI治理框架缺乏对代理舰队日常运营的指导

Method: 通过快速、实践导向的综合治理标准、代理安全文献和医疗合规要求，提出统一代理生命周期管理蓝图，包含五个控制平面层：身份与角色注册、编排与跨域调解、PHI边界上下文与内存、运行时策略执行与紧急停止触发器、生命周期管理与凭证撤销审计

Result: UALM为医疗CIO、CISO和临床领导者提供了可实施的审计就绪监督模式，同时保留本地创新能力，支持在临床和管理领域更安全地扩展

Conclusion: UALM蓝图填补了医疗AI代理治理的实践空白，通过分层控制框架和配套成熟度模型，帮助医疗组织在创新与合规之间取得平衡，实现安全可扩展的AI代理部署

Abstract: Healthcare organizations are beginning to embed agentic AI into routine workflows, including clinical documentation support and early-warning monitoring. As these capabilities diffuse across departments and vendors, health systems face agent sprawl, causing duplicated agents, unclear accountability, inconsistent controls, and tool permissions that persist beyond the original use case. Existing AI governance frameworks emphasize lifecycle risk management but provide limited guidance for the day-to-day operations of agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint derived from a rapid, practice-oriented synthesis of governance standards, agent security literature, and healthcare compliance requirements. UALM maps recurring gaps onto five control-plane layers: (1) an identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning linked to credential revocation and audit logging. A companion maturity model supports staged adoption. UALM offers healthcare CIOs, CISOs, and clinical leaders an implementable pattern for audit-ready oversight that preserves local innovation and enables safer scaling across clinical and administrative domains.

</details>


### [29] [Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2601.15652)
*Manish Bhatt*

Main category: cs.AI

TL;DR: 提出一个结合神经科学启发的信号设计与监督学习的混合检测框架，用于检测大语言模型中的幻觉，在保持可解释性的同时实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 当前LLM幻觉检测方法依赖计算昂贵的外部检索循环或需要70B+参数的黑盒LLM法官，存在效率低、不透明的问题，阻碍了高风险应用部署。

Method: 引入混合检测框架，结合预测编码（量化与内部先验的差异）和信息瓶颈（测量扰动下的信号保留）提取可解释信号，包括实体聚焦吸收、上下文一致性和可证伪性评分。

Result: 在HaluBench上，理论指导基线达到0.8017 AUROC，基础监督模型达到0.8274 AUROC，改进特征提升至0.8669 AUROC（4.95%增益）。相比Lynx使用75倍少训练数据，推理速度快1000倍，且保持完全可解释。

Conclusion: 领域知识编码的信号架构比扩展LLM法官提供更优的数据效率，能够用轻量级（<1M参数）、可解释的模型实现强性能，适合生产部署。同时发现合理化信号无法区分幻觉，表明LLM会为错误前提生成连贯推理。

Abstract: Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).
  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises ("Sycophancy").
  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.

</details>


### [30] [Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats](https://arxiv.org/abs/2601.15679)
*Ee Wei Seah,Yongsen Zheng,Naga Nikshith,Mahran Morsidi,Gabriel Waikin Loh Matienzo,Nigel Gay,Akriti Vij,Benjamin Chua,En Qi Ng,Sharmini Johnson,Vanessa Wilfred,Wan Sie Lee,Anna Davidson,Catherine Devine,Erin Zorer,Gareth Holvey,Harry Coppock,James Walpole,Jerome Wynee,Magda Dubois,Michael Schmatz,Patrick Keane,Sam Deverett,Bill Black,Bo Yan,Bushra Sabir,Frank Sun,Hao Zhang,Harriet Farlow,Helen Zhou,Lingming Dong,Qinghua Lu,Seung Jang,Sharif Abuadbba,Simon O'Callaghan,Suyu Ma,Tom Howroyd,Cyrus Fung,Fatemeh Azadi,Isar Nejadgholi,Krishnapriya Vishnubhotla,Pulei Xiong,Saeedeh Lohrasbi,Scott Buffett,Shahrear Iqbal,Sowmya Vajjala,Anna Safont-Andreu,Luca Massarelli,Oskar van der Wal,Simon Möller,Agnes Delaborde,Joris Duguépéroux,Nicolas Rolin,Romane Gallienne,Sarah Behanzin,Tom Seimandi,Akiko Murakami,Takayuki Semitsu,Teresa Tsukiji,Angela Kinuthia,Michael Michie,Stephanie Kasaon,Jean Wangari,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim*

Main category: cs.AI

TL;DR: 多国AI监管机构联合开展第三次AI智能体评估测试，聚焦共同风险（信息泄露、欺诈）和网络安全，旨在完善评估方法论而非比较模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统的快速发展和智能体能力的提升，真实世界交互中监督减少带来了新的风险。AI智能体开始全球部署时，需要确保它们能准确、安全地处理不同语言和文化。目前智能体测试仍处于早期发展阶段，需要建立科学的评估方法。

Method: 由国际先进AI测量、评估和科学网络的多国代表（新加坡、日本、澳大利亚、加拿大、欧盟委员会、法国、肯尼亚、韩国、英国）联合开展第三次测试。分为两个方向：(1) 共同风险（敏感信息泄露和欺诈），由新加坡AISI领导；(2) 网络安全，由英国AISI领导。使用公开和闭源模型，基于各种公开智能体基准任务进行评估。

Result: 由于智能体测试仍处于早期阶段，本次合作主要关注进行此类测试的方法论问题，而非检查测试结果或模型能力。这是推进智能体评估科学发展的重要一步。

Conclusion: 多国合作开展智能体评估测试标志着重要进展，参与者共同努力推进智能体评估科学的发展，为未来建立更完善的AI系统测试标准和方法论奠定基础。

Abstract: The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely.
  To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations.
  This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems.
  The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.

</details>


### [31] [From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.15690)
*Jiaxin Zhang,Wendi Cui,Zhuohang Li,Lifu Huang,Bradley Malin,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 该论文综述了不确定性在大型语言模型中的演变：从被动诊断指标转变为主动控制信号，用于指导实时模型行为，涵盖高级推理、自主代理和强化学习三个前沿领域。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能力显著，但其不可靠性仍是高风险领域部署的关键障碍。需要将不确定性从被动度量转变为主动控制信号，以提高模型的可靠性和可信度。

Method: 通过综述分析，将不确定性作为主动控制信号应用于三个前沿领域：1) 高级推理中优化计算和触发自我修正；2) 自主代理中管理元认知决策（工具使用和信息寻求）；3) 强化学习中缓解奖励黑客问题并通过内在奖励实现自我改进。基于贝叶斯方法和一致性预测等新兴理论框架。

Result: 提出了一个统一的视角，展示了不确定性作为主动控制信号如何在不同领域提升模型性能。提供了全面的概述、批判性分析和实用设计模式。

Conclusion: 掌握不确定性这一新趋势对于构建下一代可扩展、可靠和可信的AI至关重要。不确定性从被动诊断到主动控制的转变是提高LLM可靠性的关键发展方向。

Abstract: While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.

</details>


### [32] [Agentic Uncertainty Quantification](https://arxiv.org/abs/2601.15703)
*Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 提出Dual-Process Agentic UQ框架，将语言化不确定性转化为双向控制信号，解决AI代理在长程推理中的"幻觉螺旋"问题，实现高效执行与深度思考的动态平衡。


<details>
  <summary>Details</summary>
Motivation: AI代理在长程推理中面临"幻觉螺旋"问题——早期认知错误会不可逆地传播。现有方法存在局限：不确定性量化方法只能被动诊断风险而不解决，而自我反思机制则面临持续或无目的的修正。需要一种能主动利用不确定性信息进行控制的方法。

Method: 提出统一的双过程代理不确定性量化框架，包含两个互补机制：系统1（不确定性感知记忆）隐式传播语言化置信度和语义解释以防止盲目决策；系统2（不确定性感知反思）利用这些解释作为理性线索，仅在必要时触发有针对性的推理时解决。

Result: 在闭环基准测试和开放式深度研究任务上的广泛实验表明，这种无需训练的方法实现了卓越的性能和轨迹级校准，优于现有方法。

Conclusion: AUQ框架代表了向可靠AI代理迈出的重要一步，通过将语言化不确定性转化为主动控制信号，实现了高效执行与深度思考的动态平衡，为解决"幻觉螺旋"问题提供了原则性解决方案。

Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.

</details>


### [33] [Improving Methodologies for LLM Evaluations Across Global Languages](https://arxiv.org/abs/2601.15706)
*Akriti Vij,Benjamin Chua,Darshini Ramiah,En Qi Ng,Mahran Morsidi,Naga Nikshith Gangarapu,Sharmini Johnson,Vanessa Wilfred,Vikneswaran Kumaran,Wan Sie Lee,Wenzhuo Yang,Yongsen Zheng,Bill Black,Boming Xia,Frank Sun,Hao Zhang,Qinghua Lu,Suyu Ma,Yue Liu,Chi-kiu Lo,Fatemeh Azadi,Isar Nejadgholi,Sowmya Vajjala,Agnes Delaborde,Nicolas Rolin,Tom Seimandi,Akiko Murakami,Haruto Ishi,Satoshi Sekine,Takayuki Semitsu,Tasuku Sasaki,Angela Kinuthia,Jean Wangari,Michael Michie,Stephanie Kasaon,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim,Daisy Newbold-Harrop,Jessica Wang,Mahmoud Ghanem,Vy Hong*

Main category: cs.AI

TL;DR: 多语言AI安全评估研究显示，前沿AI模型在不同语言环境下的安全行为存在显著差异，需要改进多语言安全测试框架。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型在全球部署，确保其在不同语言和文化背景下的行为安全可靠至关重要。需要评估现有模型安全措施在多语言环境中的有效性。

Method: 由新加坡AISI领导，国际研究网络在10种语言（包括高资源和低资源语言）上测试了两个开源模型。使用超过6,000个新翻译的提示，涵盖5个危害类别，采用LLM-as-a-judge和人工标注两种评估方法。

Result: 研究发现：1）安全行为在不同语言间存在差异；2）安全措施的鲁棒性因语言和危害类型而异；3）评估者可靠性（LLM vs 人工）存在变化；4）获得了改进多语言安全评估的方法论见解。

Conclusion: 这项工作代表了建立先进AI系统多语言安全测试共享框架的初步步骤，呼吁与更广泛的研究社区和行业持续合作，以改进跨语言AI安全评估。

Abstract: As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.
  The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.

</details>


### [34] [AgentSM: Semantic Memory for Agentic Text-to-SQL](https://arxiv.org/abs/2601.15709)
*Asim Biswal,Chuan Lei,Xiao Qin,Aodong Li,Balakrishnan Narayanaswamy,Tim Kraska*

Main category: cs.AI

TL;DR: AgentSM是一个用于Text-to-SQL的代理框架，通过构建可解释的语义记忆来提升复杂企业环境中的SQL生成效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based Text-to-SQL系统在现实企业环境中面临挑战：大规模复杂模式、多样化SQL方言、昂贵的多步推理。现有代理方法存在效率低下、输出不一致、有时无法生成有效答案的问题。

Method: 引入Agent Semantic Memory (AgentSM)框架，通过捕获先前执行轨迹或合成精选轨迹作为结构化程序，构建可解释的语义记忆，直接指导未来推理，实现推理路径的系统性重用。

Result: 在Spider 2.0基准测试中，平均token使用量减少25%，轨迹长度减少35%。在Spider 2.0 Lite基准测试中达到44.8%的最新准确率。

Conclusion: AgentSM通过语义记忆机制显著提升了Text-to-SQL代理在复杂企业环境中的效率、可靠性和可扩展性，解决了现有方法的局限性。

Abstract: Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.

</details>


### [35] [Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling](https://arxiv.org/abs/2601.15717)
*Luyao Zhu,Fangfang Zhang,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: 研究系统评估遗传编程在动态柔性作业车间调度问题中的泛化能力，发现训练与测试实例在决策点分布相似时泛化效果更好


<details>
  <summary>Details</summary>
Motivation: 现有研究通常在同一类型的DFJSS实例上训练和测试遗传编程生成的调度规则，仅通过随机种子区分，而忽略了其跨类型泛化能力，这在实际应用中是一个重要缺陷

Method: 通过多维度实验系统研究GP演化规则的泛化能力：包括问题规模（机器和作业数量）、关键车间参数（如利用率水平）和数据分布，分析这些因素如何影响GP在未见实例类型上的性能

Result: 当训练实例包含比测试实例更多的作业（机器数量固定）时，以及当训练和测试实例具有相似规模或车间参数时，泛化效果较好。DFJSS实例中决策点的数量和分布对解释性能差异起关键作用

Conclusion: 本研究为GP在DFJSS中的泛化能力提供了新见解，强调需要演化更具泛化能力的GP规则以有效处理异构DFJSS实例，决策点分布的相似性是影响泛化性能的关键因素

Abstract: Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.

</details>


### [36] [Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity](https://arxiv.org/abs/2601.15728)
*Hangle Hu,Chenyu Hou,Bin Cao,Ruizhe Li*

Main category: cs.AI

TL;DR: BIRD-Python基准测试揭示Text-to-Python在数据检索中的可靠性问题，提出逻辑补全框架解决歧义，实现与Text-to-SQL性能持平


<details>
  <summary>Details</summary>
Motivation: 现实数据分析越来越需要Python等通用编程语言处理文件数据和复杂分析流程，但Text-to-Python在核心数据检索方面的可靠性相对于成熟的SQL生态系统尚未充分探索

Method: 引入BIRD-Python基准进行跨范式评估，系统优化原始数据集减少标注噪声和对齐执行语义；提出逻辑补全框架(LCF)，通过融入潜在领域知识来解决歧义

Result: 性能差异主要源于缺失领域上下文而非代码生成的内在限制；当这些差距被解决时，Text-to-Python能够达到与Text-to-SQL相当的性能水平

Conclusion: Python可以作为分析代理的可行基础，前提是系统能够有效地将模糊的自然语言输入基于可执行的逻辑规范

Abstract: While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.

</details>


### [37] [PhysProver: Advancing Automatic Theorem Proving for Physics](https://arxiv.org/abs/2601.15737)
*Hanning Zhang,Ruida Wang,Rui Pan,Wenyuan Wang,Bingxu Meng,Tong Zhang*

Main category: cs.AI

TL;DR: 首个增强物理学领域形式定理证明的方法，通过构建专用数据集PhysLeanData和强化学习训练PhysProver模型，在物理子领域取得2.4%提升，并在数学领域也展现出泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然可验证语言与LLMs的结合显著推动了数学和计算机科学领域的定理证明，但形式物理推理领域却鲜有关注，尽管物理推理同样严重依赖类似的问题解决和定理证明框架。

Method: 构建专用数据集PhysLeanData（包含从PhysLean采样的定理和基于猜想的形式数据生成管道生成的数据），利用DeepSeek-Prover-V2-7B作为基础模型，应用带可验证奖励的强化学习（RLVR）训练PhysProver模型。

Result: 仅使用约5K训练样本，PhysProver在多个物理子领域实现总体2.4%的改进；经过形式物理训练后，在MiniF2F-Test基准测试中获得1.3%提升，显示出超越物理领域的非平凡泛化能力和形式数学能力的增强。

Conclusion: 该方法在效率和有效性方面表现突出，为将形式证明器扩展到数学领域之外提供了范式，展示了形式物理推理训练的潜力，并将发布数据集和模型以促进进一步研究。

Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.

</details>


### [38] [Tabular Incremental Inference](https://arxiv.org/abs/2601.15751)
*Xinda Chen,Xing Zhen,Hanyu Zhang,Weimin Tan,Bo Yan*

Main category: cs.AI

TL;DR: 提出表格增量推理任务(TabII)，解决AI模型在推理阶段处理动态新增表格列的问题，基于信息瓶颈理论设计方法，在8个公开数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 表格数据是基础数据结构，表格列的动态变化源于技术进步、需求变化、数据集成等，但传统固定列训练+推理的AI模型无法处理动态变化的表格，需要新的无监督方法来高效处理此类表格。

Method: 基于信息瓶颈理论将任务形式化为优化问题，设计TabII方法：使用LLM占位符和预训练TabAdapter提供外部知识，增量样本压缩块来压缩增量列属性提供的任务相关信息。

Result: 在8个公开数据集上的实验结果表明，TabII能有效利用增量属性，实现了最先进的性能。

Conclusion: 提出的表格增量推理任务(TabII)增强了AI模型在表格动态变化场景中的实用性，基于信息瓶颈理论的方法框架有效解决了训练后模型处理新增列的问题。

Abstract: Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.

</details>


### [39] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC是一种从单条专家轨迹学习的离线策略actor-critic方法，通过sigmoid有界熵项防止负熵驱动的优化，减少Q函数振荡，在真实机器人任务中仅需少量交互即可学习成功策略。


<details>
  <summary>Details</summary>
Motivation: 真实世界强化学习面临样本效率低、奖励稀疏和视觉观测噪声等挑战。现有方法需要大量数据或大规模预训练，缺乏低成本、小数据需求的实用方案。

Method: 提出SigEnt-SAC离线策略actor-critic方法，核心设计是sigmoid有界熵项，防止负熵驱动的优化趋向分布外动作，减少Q函数振荡。仅需单条专家轨迹即可从零开始学习。

Result: 在D4RL基准测试中显著缓解Q函数振荡，比现有方法更快达到100%成功率。在四种真实机器人任务中，仅需少量真实世界交互即可从原始图像和稀疏奖励中学习成功策略。

Conclusion: SigEnt-SAC为真实世界强化学习部署提供了一条低成本、实用的途径，仅需最小数据需求即可实现高效学习。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [40] [Agentic Confidence Calibration](https://arxiv.org/abs/2601.15778)
*Jiaxin Zhang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 提出了首个面向AI智能体的置信度校准问题，并开发了HTC框架，通过提取轨迹级特征来校准多步任务中的智能体置信度，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体从被动语言模型发展为自主执行复杂多步任务的系统，但其在失败时的过度自信成为高风险部署的主要障碍。现有校准方法针对静态单轮输出设计，无法解决智能体系统的独特挑战，如轨迹中的误差累积、外部工具的不确定性以及不透明的失败模式。

Method: 提出了整体轨迹校准（HTC）框架，从智能体整个轨迹中提取丰富的流程级特征，涵盖从宏观动态到微观稳定性等多个维度。采用简单可解释的模型，并开发了通用智能体校准器（GAC）实现跨领域泛化。

Result: HTC在8个基准测试、多个LLM和不同智能体框架中，在校准和判别方面均优于强基线方法。GAC在跨领域GAIA基准测试中实现了最低的ECE（预期校准误差）。

Conclusion: 该研究建立了新的以流程为中心的置信度校准范式，为诊断和增强AI智能体的可靠性提供了框架，实现了可解释性、可迁移性和泛化性三大关键进展。

Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.

</details>


### [41] [Creativity in the Age of AI: Rethinking the Role of Intentional Agency](https://arxiv.org/abs/2601.15797)
*James S. Pearson,Matthew J. Dennis,Marc Cheong*

Main category: cs.AI

TL;DR: 论文主张应放弃"意向能动性"作为创造力的普遍必要条件，但保留其在特定领域的相关性，主要基于生成式AI的挑战和概念工程分析


<details>
  <summary>Details</summary>
Motivation: 传统创造力理论认为意向能动性是创造力的必要条件，但随着生成式AI的发展，这一条件在描述性和功能性上都变得问题重重

Method: 1) 语料库证据分析作者和记者对AI创造力的描述；2) 概念工程方法分析IAC的社会功能；3) 提出替代的一致性要求

Result: 语料库证据显示人们越来越愿意将创造力归因于缺乏意向能动性的AI；概念分析表明IAC不再履行其核心社会功能，反而导致对AI产出的评估偏见

Conclusion: 应放弃意向能动性作为创造力的普遍必要条件，代之以"可靠生成新颖且有价值产品"的一致性要求，但保留IAC在特定局部领域的适用性

Abstract: Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.

</details>


### [42] [VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management](https://arxiv.org/abs/2601.15798)
*Zhikai Xue,Tianqianjin Lin,Pengwei Yan,Ruichun Wang,Yuxin Liu,Zhuoren Jiang,Xiaozhong Liu*

Main category: cs.AI

TL;DR: VitalDiagnosis是一个基于大语言模型的生态系统，旨在将慢性病管理从被动监测转变为主动互动参与，通过整合可穿戴设备数据和LLM推理能力来应对急性健康异常和日常依从性问题。


<details>
  <summary>Details</summary>
Motivation: 慢性病已成为全球主要死因，医疗资源紧张和人口老龄化加剧了这一挑战。患者个体难以解读早期恶化迹象并坚持护理计划，需要更有效的管理方案。

Method: 整合可穿戴设备的连续数据与大语言模型的推理能力，通过情境感知查询分析健康触发因素，在患者-临床医生协作工作流程中生成临时见解，并提供个性化指导。

Result: 该系统能够同时处理急性健康异常和日常依从性问题，促进更主动、合作的护理模式，有望增强患者自我管理能力并减少可避免的临床工作量。

Conclusion: VitalDiagnosis通过LLM驱动的生态系统，将慢性病管理从被动监测转向主动互动参与，为改善患者自我管理和减轻临床负担提供了有前景的解决方案。

Abstract: Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.

</details>


### [43] [Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification](https://arxiv.org/abs/2601.15808)
*Yuxuan Wan,Tianqing Fang,Zaitang Li,Yintong Huo,Wenxuan Wang,Haitao Mi,Dong Yu,Michael R. Lyu*

Main category: cs.AI

TL;DR: 提出DeepVerifier：基于评估准则的推理时验证系统，通过自动构建的失败分类法指导智能体自我进化，无需额外训练即可提升性能


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体主要关注通过后训练增强策略能力，但缺乏有效的推理时自我改进机制。需要一种能够验证策略模型输出并引导自我进化的替代范式

Method: 1) 基于自动构建的DRA失败分类法（5大类13子类）制定评估准则；2) 开发DeepVerifier准则奖励验证器，利用验证的不对称性；3) 作为即插即用模块在推理时集成，生成详细反馈供智能体迭代优化；4) 发布DeepVerifier-4K数据集支持开源发展

Result: DeepVerifier在元评估F1分数上比基线方法提升12%-48%；在GAIA和XBench-DeepResearch的挑战性子集上实现8%-11%的准确率提升；验证器能够产生详细准则反馈引导智能体自我改进

Conclusion: 提出推理时验证扩展新范式，通过准则驱动的自我验证实现智能体能力进化，无需额外训练即可显著提升性能，为开源社区提供高质量验证数据集支持进一步发展

Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

</details>


### [44] [ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models](https://arxiv.org/abs/2601.15812)
*Shir Ashury-Tahan,Yifan Mai,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.AI

TL;DR: ErrorMap是首个系统分析LLM失败原因的方法，通过提取模型的"失败签名"来揭示错误根源，而非仅仅指出失败。该方法应用于35个数据集和83个模型，创建了ErrorAtlas错误分类法，发现当前研究忽视的错误类型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试只能告诉我们模型何时失败，但无法解释为什么失败。错误的答案可能源于格式问题、计算错误或数据集噪声，而非推理能力弱。如果不区分这些原因，基准测试就不完整，无法可靠指导模型改进。

Method: 提出ErrorMap方法，提取模型的"失败签名"，澄清基准测试的测量内容，扩大错误识别范围以减少盲点。该方法适用于任何模型和数据集，使用相同逻辑进行分析。

Result: 应用该方法于35个数据集和83个模型，生成ErrorAtlas错误分类法，揭示了重复出现的失败模式。特别突出了当前LLM研究中被忽视的错误类型，如输出中遗漏必要细节和问题误解。

Conclusion: 通过将焦点从模型成功之处转向失败原因，ErrorMap和ErrorAtlas实现了高级评估，能够暴露隐藏弱点并指导进步。与通常通过任务级指标衡量的成功不同，该方法引入了可在全球模型和任务中应用的更深层评估，提供更丰富的模型行为和限制洞察。

Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

</details>


### [45] [EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience](https://arxiv.org/abs/2601.15876)
*Taofeng Xue,Chong Peng,Mianqiu Huang,Linsen Guo,Tiancheng Han,Haozhe Wang,Jianing Wang,Xiaocheng Zhang,Xin Yang,Dengchang Zhao,Jinrui Ding,Xiandi Ma,Yuchen Xie,Peng Pei,Xunliang Cai,Xipeng Qiu*

Main category: cs.AI

TL;DR: EvoCUA：通过自我进化的数据生成与策略优化循环，解决静态数据瓶颈，实现计算机使用代理的突破性性能提升


<details>
  <summary>Details</summary>
Motivation: 当前原生计算机使用代理（CUA）的发展受限于静态数据扩展的瓶颈，现有基于静态数据集被动模仿的范式难以捕捉长时程计算机任务中的复杂因果动态关系

Method: 1. 开发可验证的合成引擎，自主生成多样化任务及可执行验证器；2. 设计可扩展基础设施，协调数万个异步沙箱环境；3. 提出迭代进化学习策略，通过识别能力边界动态调节策略更新，将失败轨迹转化为监督信号

Result: 在OSWorld基准测试中达到56.7%的成功率，创开源模型新纪录，显著超越前最佳开源模型OpenCUA-72B（45.0%）和领先闭源模型UI-TARS-2（53.1%）

Conclusion: 基于经验学习的进化范式在不同规模的基础模型中均能带来一致的性能提升，为推进原生代理能力提供了稳健且可扩展的路径

Abstract: The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.

</details>


### [46] [ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search](https://arxiv.org/abs/2601.15931)
*Xiangyu Wang,Zhixin Lv,Yongjiao Sun,Anrui Han,Ye Yuan,Hangxu Ji*

Main category: cs.AI

TL;DR: ICON框架通过因果与拓扑先验解决文本行人搜索中的虚假相关和空间语义错位问题，实现几何不变性和环境独立性，提升对遮挡、背景干扰和定位噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于预训练模型的文本行人搜索方法在复杂开放世界场景中迁移效果不佳，依赖"被动观察"导致多方面的虚假相关和空间语义错位，缺乏对分布偏移的鲁棒性。

Method: 提出ICON框架，整合因果和拓扑先验：1) 规则引导的空间干预惩罚边界框噪声敏感性；2) 反事实上下文解耦通过语义驱动背景移植实现环境独立性；3) 显著性驱动语义正则化解决局部显著性偏差；4) 神经符号拓扑对齐确保激活区域与人类结构逻辑一致。

Result: ICON在标准基准测试中保持领先性能，同时对遮挡、背景干扰和定位噪声表现出卓越的鲁棒性。

Conclusion: 该方法通过从拟合统计共现转向学习因果不变性，有效推动了该领域的发展，为解决文本行人搜索中的根本缺陷提供了新思路。

Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.

</details>


### [47] [Natural Language-Driven Global Mapping of Martian Landforms](https://arxiv.org/abs/2601.15949)
*Yiran Wang,Shuoyuan Wang,Zhaoran Wei,Jiannan Zhao,Zhonghua Yao,Zejian Xie,Songxin Zhang,Jun Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: MarScope是一个行星尺度的视觉-语言框架，通过自然语言驱动、无需标签的方式实现火星地貌映射，将行星图像和文本对齐到共享语义空间，支持任意用户查询整个火星表面。


<details>
  <summary>Details</summary>
Motivation: 行星表面分析通常使用自然语言中的高级语义概念，但大量的轨道图像档案仍以像素级别组织。这种不匹配限制了行星表面可扩展、开放式的探索。

Method: 开发MarScope框架，在超过20万个精心策划的图像-文本对上训练，将行星图像和文本对齐到共享语义空间，实现自然语言驱动的语义检索。

Result: MarScope能够在5秒内处理整个星球的任意用户查询，F1分数高达0.978。该框架不仅支持形态分类，还能促进过程导向分析和基于相似性的地貌映射。

Conclusion: MarScope建立了一个新范式，使自然语言成为大规模地理空间数据集科学发现的直接接口，改变了全球地貌映射的方式。

Abstract: Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.

</details>


### [48] [Decoupling Return-to-Go for Efficient Decision Transformer](https://arxiv.org/abs/2601.15953)
*Yongyi Wang,Hanyu Liu,Lingfeng Li,Bozhou Chen,Ang Li,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 决策变换器(DT)在离线强化学习中存在RTG序列冗余问题，提出解耦DT(DDT)仅使用最新RTG指导动作预测，简化架构并提升性能。


<details>
  <summary>Details</summary>
Motivation: 发现决策变换器(DT)设计中的关键冗余：将整个RTG序列输入Transformer在理论上是不必要的，因为只有最新的RTG会影响动作预测。这种冗余可能损害DT的性能。

Method: 提出解耦决策变换器(DDT)，简化架构：仅通过Transformer处理观测和动作序列，使用最新的RTG来指导动作预测。这种流线型方法减少了计算成本。

Result: DDT显著优于原始DT，并在多个离线RL任务中与最先进的DT变体建立了有竞争力的性能表现。

Conclusion: 通过消除RTG序列的冗余，DDT不仅提高了性能，还降低了计算成本，为决策变换器架构提供了更高效的替代方案。

Abstract: The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

</details>


### [49] [Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment](https://arxiv.org/abs/2601.16027)
*Yiran Qiao,Xiang Ao,Jing Chen,Yang Liu,Qiwei Zhong,Qing He*

Main category: cs.AI

TL;DR: CS-VAR是一个用于直播风险评估的跨会话证据感知检索增强检测器，通过LLM指导轻量级模型识别跨流重复模式，实现实时高效的风险检测。


<details>
  <summary>Details</summary>
Motivation: 直播的兴起带来了大规模实时互动，但也暴露了平台面临复杂风险（如诈骗和协同恶意行为）的挑战。这些有害行为往往逐渐累积并在看似无关的流中重复出现，使得检测变得困难。

Method: 提出CS-VAR框架：1）使用轻量级领域特定模型进行快速会话级风险推断；2）在训练过程中由大型语言模型（LLM）指导，LLM基于检索的跨会话行为证据进行推理；3）将LLM的局部到全局洞察转移给小模型，使其能够识别跨流重复模式并进行结构化风险评估。

Result: 在大规模工业数据集上的离线实验和在线验证表明，CS-VAR实现了最先进的性能。该系统能够提供可解释的局部化信号，有效支持实际直播内容审核。

Conclusion: CS-VAR通过结合LLM的推理能力和轻量级模型的高效性，成功解决了直播中跨会话复杂风险检测的挑战，实现了实时高效且可解释的风险评估。

Abstract: The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.

</details>


### [50] [Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval](https://arxiv.org/abs/2601.16038)
*Olga Bunkova,Lorenzo Di Fruscia,Sophia Rupprecht,Artur M. Schweidtmann,Marcel J. T. Reinders,Jana M. Weber*

Main category: cs.AI

TL;DR: 该研究将化学合成规划中的反应路径检索转化为Text2Cypher生成问题，比较了不同提示策略，发现使用对齐示例的单样本提示效果最佳，并提供了可复现的评估框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在化学合成规划中有应用潜力，但标准提示方法常产生幻觉或过时的建议。需要研究如何让LLM与反应知识图谱有效交互，以提供更准确可靠的合成路径检索。

Method: 将反应路径检索定义为Text2Cypher（自然语言到图查询）生成问题，设计单步和多步检索任务。比较零样本提示与使用静态、随机和嵌入对齐示例的单样本提示变体，并评估基于检查表的验证/校正循环。

Result: 使用对齐示例的单样本提示始终表现最佳。检查表式自校正循环主要在零样本设置中提高可执行性，一旦有好的示例存在，检索增益有限。框架在查询有效性和检索准确性方面得到评估。

Conclusion: 研究提供了可复现的Text2Cypher评估框架，促进基于知识图谱的LLM在合成规划中的进一步发展。对齐示例的单样本提示是有效的策略，而自校正循环在特定场景下有价值。

Abstract: Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.

</details>


### [51] [AgriPINN: A Process-Informed Neural Network for Interpretable and Scalable Crop Biomass Prediction Under Water Stress](https://arxiv.org/abs/2601.16045)
*Yue Shi,Liangxiu Han,Xin Zhang,Tam Sobeih,Thomas Gaiser,Nguyen Huu Thuy,Dominik Behrend,Amit Kumar Srivastava,Krishnagopal Halder,Frank Ewert*

Main category: cs.AI

TL;DR: AgriPINN：一种结合生物物理作物生长微分方程与深度学习的过程信息神经网络，用于水胁迫条件下作物地上生物量的时空预测，在精度和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型缺乏可解释性且在分布偏移时性能下降，而基于过程的作物模型需要大量校准且难以在大空间尺度部署。需要一种兼具可扩展性和生物物理严谨性的方法。

Method: 提出AgriPINN，将生物物理作物生长微分方程作为可微分约束集成到深度学习主干中，无需直接监督即可恢复LAI、PAR、RUE和水胁迫因子等潜在生理变量。

Result: 在德国397个地区60年历史数据预训练，并在三年受控水处理田间实验微调。AgriPINN在精度（RMSE降低达43%）和计算效率上均优于ConvLSTM-ViT、SLTF、CNN-Transformer等深度学习方法及LINTUL5过程模型。

Conclusion: AgriPINN结合了深度学习的可扩展性和过程模型的生物物理严谨性，为时空AGB预测提供了稳健且可解释的框架，对灌溉基础设施规划、产量预测和气候适应规划具有实用价值。

Abstract: Accurate prediction of crop above-ground biomass (AGB) under water stress is critical for monitoring crop productivity, guiding irrigation, and supporting climate-resilient agriculture. Data-driven models scale well but often lack interpretability and degrade under distribution shift, whereas process-based crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are difficult to deploy over large spatial domains. To address these limitations, we propose AgriPINN, a process-informed neural network that integrates a biophysical crop-growth differential equation as a differentiable constraint within a deep learning backbone. This design encourages physiologically consistent biomass dynamics under water-stress conditions while preserving model scalability for spatially distributed AGB prediction. AgriPINN recovers latent physiological variables, including leaf area index (LAI), absorbed photosynthetically active radiation (PAR), radiation use efficiency (RUE), and water-stress factors, without requiring direct supervision. We pretrain AgriPINN on 60 years of historical data across 397 regions in Germany and fine-tune it on three years of field experiments under controlled water treatments. Results show that AgriPINN consistently outperforms state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to $43\%$) and computational efficiency. By combining the scalability of deep learning with the biophysical rigor of process-based modeling, AgriPINN provides a robust and interpretable framework for spatio-temporal AGB prediction, offering practical value for planning of irrigation infrastructure, yield forecasting, and climate-adaptation planning.

</details>


### [52] [Designing faster mixed integer linear programming algorithm via learning the optimal path](https://arxiv.org/abs/2601.16056)
*Ruizhi Liu,Liming Xu,Xulin Huang,Jingyan Sui,Shizhe Ding,Boyang Xia,Chungong Yu,Dongbo Bu*

Main category: cs.AI

TL;DR: DeepBound：基于深度学习的节点选择算法，通过多级特征融合网络和成对训练范式，自动学习分支定界树中的节点优先级，显著提升混合整数线性规划问题的求解效率。


<details>
  <summary>Details</summary>
Motivation: 传统混合整数线性规划求解依赖手工设计的启发式策略，这些策略在不同问题实例上表现不稳定且不可预测。需要自动化学习人类直觉的方法来改进求解效率。

Method: 提出DeepBound深度学习节点选择算法：1）使用多级特征融合网络捕捉节点表示；2）采用成对训练范式解决分支定界树中节点不平衡问题；3）学习优先选择包含最优解的节点。

Result: 在三个NP-hard MILP基准测试上，DeepBound相比传统启发式规则和现有学习方法获得显著更优的求解效率，计算时间大幅减少，且在大规模复杂实例上表现出强泛化能力。

Conclusion: DeepBound能够自动发现更灵活鲁棒的特征选择，有效改进并可能替代人工设计的启发式规则，为MILP问题求解提供了高效的数据驱动方法。

Abstract: Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.

</details>


### [53] [Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics](https://arxiv.org/abs/2601.16087)
*Sukesh Subaharan*

Main category: cs.AI

TL;DR: 该研究探讨了在LLM代理中引入外部情感动态子系统，通过Valence-Arousal-Dominance状态和动态更新规则来改善多轮对话中的时间一致性和可控恢复。


<details>
  <summary>Details</summary>
Motivation: LLM代理在长时间交互中经常表现出语气和角色的突然转变，这反映了代理层面状态缺乏明确的时间结构。虽然先前工作关注回合局部情感或静态情感分类，但明确的情感动态在塑造长视野代理行为中的作用尚未得到充分探索。

Method: 引入代理层面的情感子系统，维护一个外部的连续Valence-Arousal-Dominance状态，受一阶和二阶更新规则控制。使用固定的无记忆估计器提取瞬时情感信号，并通过指数平滑或基于动量的动态进行时间整合。生成时不修改模型参数，仅注入情感状态。

Result: 在25轮固定对话协议中比较：无状态代理无法展现连贯轨迹或恢复；状态持久性支持延迟响应和可靠恢复；二阶动态引入情感惯性和滞后，随着动量增加，揭示了稳定性与响应性之间的权衡。

Conclusion: 在LLM代理中施加明确的情感动态结构可以诱导时间一致性和可控恢复，二阶动态在稳定性和响应性之间存在权衡，为长视野对话代理设计提供了新思路。

Abstract: Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.

</details>


### [54] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: 提出结合视觉语言模型与外部知识检索的方法，以应对气候变化虚假信息检测中模型知识过时的问题


<details>
  <summary>Details</summary>
Motivation: 气候变化虚假信息在社交媒体上广泛传播，特别是具有误导性的图像和视频。现有的视觉语言模型仅依赖训练时的知识，无法处理新近事件或更新信息，限制了其检测能力。

Method: 将视觉语言模型与外部知识检索相结合，检索最新的信息如反向图像搜索结果、在线事实核查、可信专家内容等，以评估图像及其声明的准确性。

Result: 该方法能更好地判断图像及其声明的准确性（准确、误导、虚假或无法验证），提升了处理现实世界气候变化虚假信息的能力。

Conclusion: 结合外部知识检索的视觉语言模型能有效克服知识过时问题，有助于保护公众对科学的理解，应对快速变化的信息环境中的气候变化虚假信息挑战。

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


### [55] [LLM Prompt Evaluation for Educational Applications](https://arxiv.org/abs/2601.16134)
*Langdon Holmes,Adam Coscia,Scott Crossley,Joon Suh Choi,Wesley Morris*

Main category: cs.AI

TL;DR: 本研究提出了一种系统化评估教育应用LLM提示词的方法，通过分析结构化对话活动中LLM生成的后续问题，发现结合角色扮演和上下文管理模式的提示词在支持元认知学习策略方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在教育应用中的普及，需要基于证据的方法来设计和评估能产生个性化且教学对齐输出的提示词，超越临时性的提示工程。

Method: 设计了6个结合不同教学策略的提示模板，采用锦标赛式评估框架，使用Glicko2评分系统，由8位评委从格式、对话支持和学习者适宜性三个维度评估问题对，数据来自3个教育部署的120个真实用户交互。

Result: 与策略性阅读相关的单个提示词在成对比较中胜率最高（81%-100%），该提示结合了角色扮演和上下文管理模式，旨在支持元认知学习策略如自主学习。

Conclusion: 该方法展示了教育技术研究人员如何系统评估和改进提示设计，从临时性的提示工程转向基于证据的教育应用提示开发。

Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.

</details>


### [56] [Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning](https://arxiv.org/abs/2601.16163)
*Moo Jin Kim,Yihuai Gao,Tsung-Yi Lin,Yen-Chen Lin,Yunhao Ge,Grace Lam,Percy Liang,Shuran Song,Ming-Yu Liu,Chelsea Finn,Jinwei Gu*

Main category: cs.AI

TL;DR: Cosmos Policy：将预训练视频模型Cosmos-Predict2通过单阶段后训练直接转化为机器人策略，在潜在扩散过程中生成动作、状态图像和值函数，实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有视频模型已具备强大的时空先验，但将其用于机器人策略学习通常需要复杂的多阶段后训练和新架构组件。本文旨在简化这一过程，直接利用预训练视频模型的核心学习算法

Method: 1. 使用预训练视频模型Cosmos-Predict2，无需架构修改；2. 通过单阶段后训练在目标平台的机器人演示数据上；3. 将机器人动作编码为潜在帧，在模型的潜在扩散过程中直接生成；4. 同时生成未来状态图像和值函数（预期累积奖励）；5. 支持测试时规划高成功率的动作轨迹

Result: 1. 在LIBERO和RoboCasa仿真基准上达到SOTA性能（平均成功率分别为98.5%和67.1%）；2. 在真实世界双手操作任务中获得最高平均分数；3. 优于从头训练的扩散策略、基于视频模型的策略和SOTA视觉-语言-动作模型；4. 能够从经验中学习，通过基于模型的规划进一步提高成功率

Conclusion: Cosmos Policy证明了通过简单单阶段后训练将预训练视频模型转化为高效机器人策略的可行性，充分利用了模型的预训练先验和核心学习算法，在仿真和真实世界任务中都取得了优异性能

Abstract: Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/

</details>


### [57] [Structured Hints for Sample-Efficient Lean Theorem Proving](https://arxiv.org/abs/2601.16172)
*Zachary Burton*

Main category: cs.AI

TL;DR: 在miniF2F基准测试中，对DeepSeek-Prover-V1.5等先进神经定理证明器应用简单的推理时提示调度（15个常见战术骨架），相比标准采样方法，pass@16从15.2%提升到21.7%，相对提升43%。


<details>
  <summary>Details</summary>
Motivation: 研究高度训练的神经定理证明器是否仍能从推理时的简单结构指导中受益。尽管这些模型已经通过强化学习进行了复杂训练，但可能仍未充分利用战术语言中可用的结构先验。

Method: 采用轻量级干预方法：在推理时使用固定的提示调度，基于15个常见战术骨架。在miniF2F基准测试上评估，使用相同样本数（k=16）和相同最大生成长度（1024个标记）。

Result: 简单提示调度使pass@16从15.2%提升到21.7%，相对提升43%。这表明即使经过RL训练的证明器也未能充分利用战术语言的结构先验。

Conclusion: 简单的推理时指导仍然是廉价且互补的提升方法，即使对于经过复杂训练的神经定理证明器，结构指导在推理时仍然有价值。

Abstract: State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.

</details>


### [58] [Scalable Board Expansion within a General Game System](https://arxiv.org/abs/2601.16216)
*Clémentine Sacré*

Main category: cs.AI

TL;DR: 提出基于通用游戏系统的动态棋盘扩展机制，解决传统棋盘游戏中静态大棋盘导致的资源浪费和复杂度问题


<details>
  <summary>Details</summary>
Motivation: 传统棋盘游戏通常使用预先定义的静态大棋盘，即使大部分区域在游戏中从未使用，这导致了不必要的复杂性和资源浪费

Method: 采用通用游戏系统（GGS）支持动态棋盘扩展机制，在游戏过程中自动扩展游戏棋盘

Result: 论文提出了动态棋盘扩展机制，但摘要中未提供具体的实验结果或性能数据

Conclusion: 动态棋盘扩展机制能够有效解决传统静态大棋盘的问题，提高游戏系统的效率和适应性

Abstract: This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [59] [Partially Polarized Polar Codes: A New Design for 6G Control Channels](https://arxiv.org/abs/2601.15404)
*Arman Fazeli,Mohammad M. Mansour,Ziyuan Zhu,Louay Jalloul*

Main category: cs.IT

TL;DR: PPP码是一种新型类极化码，通过选择性修剪极化核来修改合成比特信道容量，确保解码早期有保证数量的非冻结比特可用，从而支持更有效的早期终止，特别适用于下行控制信道的盲解码。


<details>
  <summary>Details</summary>
Motivation: 解决下行控制信道中用户设备必须处理多个候选（其中许多不携带有效控制信息）的盲解码问题。传统极化码在较大块长度时受硬件限制难以直接扩展，需要更有效的早期终止机制。

Method: 从传统极化码出发，通过选择性修剪极化核来构造PPP码，修改合成比特信道容量，确保解码早期有保证数量的非冻结比特可用。还提出了针对PPP码的冻结位图设计策略。

Result: PPP码相比传统极化码在性能上有显著提升，特别是在较大块长度时。与现有方法（如聚合或分段）相比，PPP码实现更高效率且无需额外硬件支持。

Conclusion: PPP码通过选择性修剪极化核实现早期非冻结比特访问，为下行控制信道的盲解码提供了有效的解决方案，在性能和硬件效率方面优于传统方法。

Abstract: We introduce a new family of polar-like codes, called Partially Polarized Polar (PPP) codes. PPP codes are constructed from conventional polar codes by selectively pruning polarization kernels, thereby modifying the synthesized bit-channel capacities to ensure a guaranteed number of non-frozen bits available early in decoding. These early-access information bits enable more effective early termination, which is particularly valuable for blind decoding in downlink control channels, where user equipment (UE) must process multiple candidates, many of which carry no valid control information. Our results show that PPP codes offer substantial performance gains over conventional polar codes, particularly at larger block lengths where hardware limitations restrict straightforward scaling. Compared with existing methods such as aggregation or segmentation, PPP codes achieve higher efficiency without the need for additional hardware support. Finally, we propose several frozen-bitmap design strategies tailored to PPP codes.

</details>


### [60] [Rank-metric codes over arbitrary fields: Bounds and constructions](https://arxiv.org/abs/2601.15464)
*Alessandro Neri,Ferdinando Zullo*

Main category: cs.IT

TL;DR: 该论文综述了秩度量码的发展历程、数学基础、界与构造，特别关注了有限域之外更一般设置下的扩展，包括代数闭域和实数域等情形。


<details>
  <summary>Details</summary>
Motivation: 秩度量码在1978年由Delsarte提出，后被Gabidulin重新发现，在网络编码和多种数学领域有重要应用。本文旨在系统梳理秩度量码的理论发展，特别是将其从有限域推广到更一般设置的研究进展。

Method: 采用综述研究方法，系统考察秩度量码的界（如Singleton-like界）和构造（如最大秩距离码），分析这些界在不同域设置下的紧致性，并探讨线性秩度量码与系统和逃避子空间的关系。

Result: 展示了Singleton-like界在有限域情形下的紧致性，以及在更一般设置下可能不紧的情况；讨论了在具有循环Galois扩张的域上构造MRD码的方法；建立了线性秩度量码与系统和逃避子空间的联系；回顾了代数闭域和实数域上的相关结果。

Conclusion: 秩度量码理论已从有限域扩展到更一般的域设置，但仍有许多开放问题。未来研究方向包括MRD码存在性猜想、在不同域扩张上秩度量码的探索，以及与其他数学领域的进一步联系。

Abstract: Rank-metric codes, defined as sets of matrices over a finite field with the rank distance, have gained significant attention due to their applications in network coding and connections to diverse mathematical areas. Initially studied by Delsarte in 1978 and later rediscovered by Gabidulin, these codes have become a central topic in coding theory. This paper surveys the development and mathematical foundations, in particular, regarding bounds and constructions of rank-metric codes, emphasizing their extension beyond finite fields to more general settings. We examine Singleton-like bounds on code parameters, demonstrating their sharpness in finite field cases and contrasting this with contexts where the bounds are not tight. Furthermore, we discuss constructions of Maximum Rank Distance (MRD) codes over fields with cyclic Galois extensions and the relationship between linear rank-metric codes with systems and evasive subspaces. The paper also reviews results for algebraically closed fields and real numbers, previously appearing in the context of topology and measure theory. We conclude by proposing future research directions, including conjectures on MRD code existence and the exploration of rank-metric codes over various field extensions.

</details>


### [61] [Stabilizer-Code Channel Transforms Beyond Repetition Codes for Improved Hashing Bounds](https://arxiv.org/abs/2601.15505)
*Tyler Kann,Matthieu R. Bloch,Shrinivas Kudekar,Ruediger Urbanke*

Main category: cs.IT

TL;DR: 该论文提出了一种通过稳定子码作为信道变换来改进量子哈希界的方法，通过计算诱导的逻辑泡利错误分布和伴随式，利用解码器侧信息获得更高的可达速率。


<details>
  <summary>Details</summary>
Motivation: 量子哈希界对于无记忆泡利信道并不总是紧的，现有方法仅适用于某些非对称泡利信道且使用小规模内码。需要更通用的方法来改进各种泡利信道的可达速率。

Method: 将任意稳定子码作为信道变换，构造完整的辛表，计算物理泡利信道下逻辑泡利错误和伴随式的联合分布，利用解码器侧信息获得改进的哈希界可达速率。

Result: 通过结构化搜索小规模变换，发现在先前研究的具有偏斜和独立错误的泡利信道族中，该方法能够改进基线哈希界。

Conclusion: 稳定子码作为信道变换的诱导信道视角可以推广到任意稳定子码，为改进泡利信道的可达速率提供了更通用的框架。

Abstract: The quantum hashing bound guarantees that rates up to $1-H(p_I, p_X, p_Y, p_Z)$ are achievable for memoryless Pauli channels, but it is not generally tight. A known way to improve achievable rates for certain asymmetric Pauli channels is to apply a small inner stabilizer code to a few channel uses, decode, and treat the resulting logical noise as an induced Pauli channel; reapplying the hashing argument to this induced channel can beat the baseline hashing bound. We generalize this induced-channel viewpoint to arbitrary stabilizer codes used purely as channel transforms. Given any $ [\![ n, k ]\!] $ stabilizer generator set, we construct a full symplectic tableau, compute the induced joint distribution of logical Pauli errors and syndromes under the physical Pauli channel, and obtain an achievable rate via a hashing bound with decoder side information. We perform a structured search over small transforms and report instances that improve the baseline hashing bound for a family of Pauli channels with skewed and independent errors studied in prior work.

</details>


### [62] [A Class of Subadditive Information Measures and their Applications](https://arxiv.org/abs/2601.15639)
*Hamidreza Abin,Mahdi Zinati,Amin Gohari,Mohammad Hossein Yassaee,Mohammad Mahdi Mojahedian*

Main category: cs.IT

TL;DR: 提出(G,f)-散度家族，通过非递减函数G作用于f-散度，建立相应的(G,f)-信息度量，研究其在乘积分布和乘积信道上的次可加性，并应用于信道编码、假设检验和球面打包指数框架。


<details>
  <summary>Details</summary>
Motivation: 现有f-散度和互信息度量在某些应用中可能不够灵活，需要更一般的散度家族来统一分析信息论中的次可加性问题，为信道编码、假设检验等提供更强大的理论工具。

Method: 引入(G,f)-散度作为两参数散度家族，定义相应的(G,f)-信息度量。建立归约原理，证明对于广泛的G函数类别，只需在二元字母表上验证散度的次可加性。针对特定G函数（x、log(1+x)、-log(1-x)），推导保证次可加性的f函数充分条件。

Result: 建立了(G,f)-散度的次可加性理论框架，证明了归约原理的有效性，为许多标准f-散度提供了次可加性保证。将理论应用于信道编码的有限块长逆定理、二元假设检验的界限，以及将Shannon-Gallager-Berlekamp球面打包指数框架扩展到次可加的(G,f)-散度。

Conclusion: (G,f)-散度家族提供了一个灵活统一的框架来研究信息论中的次可加性问题，所建立的归约原理和充分条件简化了次可加性验证，扩展了经典信息论工具的应用范围，为信道编码和假设检验等提供了新的分析手段。

Abstract: We introduce a two-parameter family of discrepancy measures, termed \emph{$(G,f)$-divergences}, obtained by applying a non-decreasing function $G$ to an $f$-divergence $D_f$. Building on Csiszár's formulation of mutual $f$-information, we define a corresponding $(G,f)$-information measure $
I_{G,f}(X;Y)$. A central theme of the paper is subadditivity over product distributions and product channels. We develop reduction principles showing that, for broad classes of $G$, it suffices to verify divergence subadditivity on binary alphabets. Specializing to the functions $G(x)\in\{x,\log(1+x),-\log(1-x)\}$, we derive tractable sufficient conditions on $f$ that guarantee subadditivity, covering many standard $f$-divergences. Finally, we present applications to finite-blocklength converses for channel coding, bounds in binary hypothesis testing, and an extension of the Shannon--Gallager--Berlekamp sphere-packing exponent framework to subadditive $(G,f)$-divergences.

</details>


### [63] [Generative AI-Empowered Semantic Twin Channel Model for ISAC](https://arxiv.org/abs/2601.15642)
*Yi Chen,Yatao Hu,Ming Li,Chong Han*

Main category: cs.IT

TL;DR: 论文提出了一种面向语义的ISAC信道建模方法，通过环境语义统一感知与通信需求，并引入生成式AI驱动的语义孪生信道模型来平衡精度与复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前ISAC信道建模存在明显缺陷：统计模型过于粗糙，忽略了感知所需的关键多径特征；确定性模型计算复杂，难以扩展到系统级评估。这需要一种能统一环境对感知的意义和信道对通信行为的抽象方法。

Method: 提出语义导向的信道建模原则，在保留环境语义的同时抽象不必要的细节。引入生成式AI驱动的语义孪生信道模型（STCM），生成符合特定语义条件的物理合理信道实现。

Result: 案例研究表明，该方法在具有挑战性的多视角设置下保持语义一致性，为可控仿真、数据集生成和可复现的ISAC基准测试提供了实用路径。

Conclusion: 环境语义是ISAC信道建模的关键桥梁，语义导向的建模方法能有效平衡精度与复杂度，为未来ISAC设计和标准化提供了新的建模框架。

Abstract: Integrated sensing and communication (ISAC) increasingly exposes a gap in today's channel modeling. Efficient statistical models focus on coarse communication-centric metrics, and therefore miss the weak but critical multipath signatures for sensing, whereas deterministic models are computationally inefficient to scale for system-level ISAC evaluation. This gap calls for a unifying abstraction that can couple what the environment means for sensing with how the channel behaves for communication, namely, environmental semantics. This article clarifies the meaning and essentiality of environmental semantics in ISAC channel modeling and establishes how semantics is connected to observable channel structures across multiple semantic levels. Based on this perspective, a semantics-oriented channel modeling principle was advocated, which preserves environmental semantics while abstracting unnecessary detail to balance accuracy and complexity. Then, a generative AI-empowered semantic twin channel model (STCM) was introduced to generate a family of physically plausible channel realizations representative of a semantic condition. Case studies further show semantic consistency under challenging multi-view settings, suggesting a practical path to controllable simulation, dataset generation, and reproducible ISAC benchmarking toward future design and standardization.

</details>


### [64] [Generalized Information Inequalities via Submodularity, and Two Combinatorial Problems](https://arxiv.org/abs/2601.15723)
*Gunank Jakhar,Gowtham R. Kurri,Suryajith Chillara,Vinod M. Prabhakaran*

Main category: cs.IT

TL;DR: 本文在熵不等式与子模性的联系基础上，建立了凸函数形式的Madiman-Tetali强/弱不等式推广，提出了改进的Loomis-Whitney型投影不等式，并利用Shearer引理扩展了极值图论问题的结果。


<details>
  <summary>Details</summary>
Motivation: 熵不等式与子模性之间存在深刻联系，已有Madiman-Tetali和Sason等学者建立了统一框架。本文旨在在这些框架基础上进一步推进，建立更一般的凸函数推广，并应用于几何不等式和极值图论问题。

Method: 1. 建立子模函数的凸函数形式Madiman-Tetali强/弱不等式推广；2. 利用特殊情况的强Madiman-Tetali不等式推导改进的Loomis-Whitney型投影不等式；3. 使用Shearer引理研究极值图论问题，扩展先前结果。

Result: 1. 成功建立了子模函数的凸函数形式Madiman-Tetali不等式推广；2. 提出了改进的Loomis-Whitney型投影不等式，通过包含切片层结构信息超越了经典界限；3. 在极值图论问题上恢复并扩展了Sason和Boucheron等人的结果。

Conclusion: 本文在熵不等式与子模性的统一框架下做出了三个重要贡献：建立了凸函数形式的推广，提出了改进的几何不等式，以及扩展了极值图论结果，深化了对这些数学结构之间联系的理解。

Abstract: It is well known that there is a strong connection between entropy inequalities and submodularity, since the entropy of a collection of random variables is a submodular function. Unifying frameworks for information inequalities arising from submodularity were developed by Madiman and Tetali (2010) and Sason (2022). Madiman and Tetali (2010) established strong and weak fractional inequalities that subsume classical results such as Han's inequality and Shearer's lemma. Sason (2022) introduced a convex-functional framework for generalizing Han's inequality, and derived unified inequalities for submodular and supermodular functions. In this work, we build on these frameworks and make three contributions. First, we establish convex-functional generalizations of the strong and weak Madiman and Tetali inequalities for submodular functions. Second, using a special case of the strong Madiman-Tetali inequality, we derive a new Loomis-Whitney-type projection inequality for finite point sets in $\mathbb{R}^d$, which improves upon the classical Loomis-Whitney bound by incorporating slice-level structural information. Finally, we study an extremal graph theory problem that recovers and extends the previously known results of Sason (2022) and Boucheron et al., employing Shearer's lemma in contrast to the use of Han's inequality in those works.

</details>


### [65] [Recursive Flow: A Generative Framework for MIMO Channel Estimation](https://arxiv.org/abs/2601.15767)
*Zehua Jiang,Fenghao Zhu,Chongwen Huang,Richeng Jin,Zhaohui Yang,Xiaoming Chen,Zhaoyang Zhang,Mérouane Debbah*

Main category: cs.IT

TL;DR: RC-Flow是一种用于大规模MIMO信道估计的新方法，结合流匹配先验和闭环优化框架，在噪声场景下显著提升性能并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统中的信道估计是一个基本挑战，估计精度直接影响频谱效率和链路可靠性。传统生成模型在噪声场景下性能有限，需要更鲁棒的解决方案。

Method: 提出RC-Flow方法：1）利用预训练的流匹配先验；2）通过串行重启机制和锚定轨迹校正建立闭环精炼框架；3）结合流一致先验方向和数据保真度近端投影；4）采用自适应双调度策略平衡收敛速度和重建精度。

Result: RC-Flow在低信噪比场景下相比基于分数的基线方法获得2.7 dB的性能增益，同时将推理延迟降低两个数量级。理论分析证明其全局渐近稳定性。

Conclusion: RC-Flow通过融合生成先验和闭环优化，为大规模MIMO信道估计提供了高效鲁棒的解决方案，特别适用于噪声主导场景。

Abstract: Channel estimation is a fundamental challenge in massive multiple-input multiple-output systems, where estimation accuracy governs the spectral efficiency and link reliability. In this work, we introduce Recursive Flow (RC-Flow), a novel solver that leverages pre-trained flow matching priors to robustly recover channel state information from noisy, under-determined measurements. Different from conventional open-loop generative models, our approach establishes a closed-loop refinement framework via a serial restart mechanism and anchored trajectory rectification. By synergizing flow-consistent prior directions with data-fidelity proximal projections, the proposed RC-Flow achieves robust channel reconstruction and delivers state-of-the-art performance across diverse noise levels, particularly in noise-dominated scenarios. The framework is further augmented by an adaptive dual-scheduling strategy, offering flexible management of the trade-off between convergence speed and reconstruction accuracy. Theoretically, we analyze the Jacobian spectral radius of the recursive operator to prove its global asymptotic stability. Numerical results demonstrate that RC-Flow reduces inference latency by two orders of magnitude while achieving a 2.7 dB performance gain in low signal-to-noise ratio regimes compared to the score-based baseline.

</details>


### [66] [Practical applications of Set Shaping Theory to Non-Uniform Sequences](https://arxiv.org/abs/2601.15853)
*A. Schmidt,A. Vdberg,A. Petit*

Main category: cs.IT

TL;DR: SST通过将原始序列集映射到更大序列空间的结构化区域来降低平均信息量，但非均匀序列的精确排序计算复杂度过高。本文提出近似排序方法，在保持SST结构要求的同时实现理论预测的整形增益，并公开了实现软件。


<details>
  <summary>Details</summary>
Motivation: 集合整形理论(SST)通过构建双射映射将原始序列集转换到更大序列空间的结构化区域，从而降低平均信息量。然而，对于非均匀分布序列，需要按信息量对原始集和转换集进行排序，精确排序具有指数复杂度，难以实际应用。

Method: 提出近似但信息丰富的排序方法，在保持SST结构要求的同时避免指数复杂度。该方法能够实现理论预测的整形增益，并将实现软件公开在GitHub上以确保可复现性。

Result: 成功克服了SST应用于非均匀序列的主要障碍，扩展了先前在均匀分布序列上的实验结果，证明SST的整形优势在非均匀序列中依然存在。

Conclusion: 通过近似排序方法，SST可以实际应用于非均匀序列，实现理论预测的整形增益，这扩展了SST的应用范围并提供了可复现的实现方案。

Abstract: Set Shaping Theory (SST) moves beyond the classical fixed-space model by constructing bijective mappings the original sequence set into structured regions of a larger sequence space. These shaped subsets are characterized by a reduced average information content, measured by the product of the empirical entropy and the length, yielding (N +k)H0(f(s)) < NH0(s), which represents the universal coding limit when the source distribution is unknown. The principal experimental difficulty in applying Set Shaping Theory to non-uniform sequences arises from the need to order the sequences of both the original and transformed sets according to their information content. An exact ordering of these sets entails exponential complexity, rendering a direct implementation impractical. In this article, we show that this obstacle can be overcome by performing an approximate but informative ordering that preserves the structural requirements of SST while achieving the shaping gain predicted by the theory. This result extends previous experimental findings obtained for uniformly distributed sequences and demonstrates that the shaping advantage of SST persists for non-uniform sequences. Finally, to ensure full reproducibility, the software implementing the proposed method has been made publicly available on GitHub, enabling independent verification of the results reported in this work

</details>


### [67] [Blind Identification of Channel Codes: A Subspace-Coding Approach](https://arxiv.org/abs/2601.15903)
*Pramod Singh,Prasad Krishnan,Arti Yardi*

Main category: cs.IT

TL;DR: 提出一种基于子空间码框架的BSC信道码盲识别新方法，称为最小去噪子空间差异解码器，相比现有方法具有更好性能和理论保证


<details>
  <summary>Details</summary>
Motivation: 现有信道码盲识别方法大多依赖于码的特殊结构，计算复杂度高，且缺乏严格的理论性能保证。需要一种更通用、高效且具有理论保证的盲识别方法

Method: 基于子空间码和算子信道框架，结合汉明度量和子空间度量解码原理，提出最小去噪子空间差异解码器，用于二进制对称信道(BSC)上的码识别

Result: 为有界权重错误提供了码识别的理论保证，给出了BSC上错误概率的界限。仿真显示该方法在随机线性码识别上优于现有通用技术，在大多数信道条件下和有限接收向量情况下都有更好性能

Conclusion: 提出的最小去噪子空间差异解码器为信道码盲识别提供了一种有效的新方法，具有理论保证和实际性能优势，特别适用于随机线性码的识别

Abstract: The problem of blind identification of channel codes at a receiver involves identifying a code chosen by a transmitter from a known code-family, by observing the transmitted codewords through the channel. Most existing approaches for code-identification are contingent upon the codes in the family having some special structure, and are often computationally expensive otherwise. Further, rigorous analytical guarantees on the performance of these existing techniques are largely absent. This work presents a new method for code-identification on the binary symmetric channel (BSC), inspired by the framework of subspace codes for operator channels, carefully combining principles of hamming-metric and subspace-metric decoding. We refer to this method as the minimum denoised subspace discrepancy decoder. We present theoretical guarantees for code-identification using this decoder, for bounded-weight errors, and also present a bound on the probability of error when used on the BSC. Simulations demonstrate the improved performance of our decoder for random linear codes beyond existing general-purpose techniques, across most channel conditions and even with a limited number of received vectors.

</details>


### [68] [A Remark on Downlink Massive Random Access](https://arxiv.org/abs/2601.15928)
*Yuchen Liao,Wenyi Zhang*

Main category: cs.IT

TL;DR: 本文通过组合数学中的覆盖数组理论，为大规模随机接入下行链路设计了确定性变长编码方案，将开销降低至不超过1+log₂e比特，优于现有随机编码方法。


<details>
  <summary>Details</summary>
Motivation: 在大规模随机接入下行链路中，基站需要向大量用户中的一小部分活跃用户传输消息。传统方法需要显式编码活跃用户身份，这会导致开销随总用户数对数增长。虽然已有随机编码方法能降低开销，但作者希望通过确定性构造获得更优的性能边界。

Method: 作者将大规模随机接入下行链路的编码设计问题转化为组合数学中的覆盖数组问题。利用覆盖数组理论，设计了确定性的变长编码构造方法，而不是依赖随机编码。

Result: 证明了存在确定性构造的变长编码方案，其开销不超过1+log₂e比特，这一结果优于先前随机编码方法给出的上界，且与总用户数无关。

Conclusion: 通过将编码问题转化为覆盖数组问题，本文提供了确定性编码构造的理论保证，显著降低了大规模随机接入系统中的开销，为实际系统设计提供了理论依据。

Abstract: In downlink massive random access (DMRA), a base station transmits messages to a typically small subset of active users, selected randomly from a massive number of total users. Explicitly encoding the identities of active users would incur a significant overhead scaling logarithmically with the number of total users. Recently, via a random coding argument, Song, Attiah and Yu have shown that the overhead can be reduced to within some upper bound irrespective of the number of total users. In this remark, recognizing that the code design for DMRA is an instance of covering arrays in combinatorics, we show that there exists deterministic construction of variable-length codes that incur an overhead no greater than $1 + log_2 e$ bits.

</details>


### [69] [Stacked Intelligent Metasurface-Aided Wave-Domain Signal Processing: From Communications to Sensing and Computing](https://arxiv.org/abs/2601.16030)
*Jiancheng An,Chau Yuen,Marco Di Renzo,Mehdi Bennis,Merouane Debbah,Lajos Hanzo*

Main category: cs.IT

TL;DR: 该论文综述了堆叠智能超表面（SIM）技术，这是一种结合神经网络、电磁计算和超表面的新兴技术，用于在电磁域直接处理信号，实现高速、大规模并行和低功耗计算。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络的抽象特征提取能力、电磁计算的波传播特性以及超表面的电磁波调控能力，开发物理神经网络，直接在电磁域执行计算任务，实现更高效的信息处理。

Method: 采用堆叠智能超表面（SIM）技术，通过多层超表面结构实现电磁波的直接处理。从理论基础、原型设计、优化训练策略（两种不同视角）等方面系统构建SIM技术框架。

Result: SIM技术在通信、传感和计算领域展现出多样化应用潜力，实验证据表明单个设备可支持多种功能，具有高速、大规模并行和低功耗的独特优势。

Conclusion: SIM技术为下一代无线网络提供了有前景的解决方案，但仍需解决关键技术挑战，并探索更多研究方向以充分发挥其潜力。

Abstract: Neural networks possess incredible capabilities for extracting abstract features from data. Electromagnetic computing harnesses wave propagation to execute computational operations. Metasurfaces, composed of subwavelength meta-atoms, are capable of engineering electromagnetic waves in unprecedented ways. What happens when combining these three cutting-edge technologies? This question has sparked a surge of interest in designing physical neural networks using stacked intelligent metasurface (SIM) technology, with the aim of implementing various computational tasks by directly processing electromagnetic waves. SIMs open up an exciting avenue toward high-speed, massively parallel, and low-power signal processing in the electromagnetic domain. This article provides a comprehensive overview of SIM technology, commencing with its evolutionary development. We subsequently examine its theoretical foundations and existing SIM prototypes in depth. Furthermore, the optimization/training strategies conceived to configure SIMs for achieving the desired functionalities are discussed from two different perspectives. Additionally, we explore the diverse applications of SIM technology across the communication, sensing, and computing domains, presenting experimental evidence that highlights its distinctive advantages in supporting multiple functions within a single device. Finally, we identify critical technical challenges that must be addressed to deploy SIMs in next-generation wireless networks and shed light on promising research directions to unlock their full potential.

</details>


### [70] [RIS-Aided Cooperative ISAC Network for Imaging-Based Low-Altitude Surveillance](https://arxiv.org/abs/2601.16033)
*Zhixin Chen,Yixuan Huang,Zhengze Ji,Jie Yang,Shi Jin*

Main category: cs.IT

TL;DR: 提出基于可重构智能表面(RIS)的协作式集成感知与通信网络，用于低空监视，采用主动RIS增强信号，将监视建模为压缩感知成像问题，实现300米高度有效成像


<details>
  <summary>Details</summary>
Motivation: 低空经济对先进监视技术需求迫切，但传统方法存在部署成本高、信号强度低的局限性，需要开发更有效的低空监视方案

Method: 采用RIS辅助的协作式ISAC网络，使用主动RIS放大信号，将低空监视建模为基于压缩感知理论的成像问题，通过子空间追踪算法求解，并推导CRLB分析性能

Result: 数值结果显示，在相同功率约束下，主动RIS优于被动RIS，能够在高达300米的高度实现有效成像和目标检测

Conclusion: 提出的RIS辅助低空成像系统为低空监视提供了有效解决方案，主动RIS显著提升性能，系统参数分析为ISAC配置提供了指导

Abstract: The low-altitude economy is integral to the advancement of numerous sectors, necessitating the development of advanced low-altitude surveillance techniques. Nevertheless, conventional methods encounter limitations of high deployment costs and low signal strength. This study proposes a reconfigurable intelligent surface (RIS)-aided cooperative integrated sensing and communication (ISAC) network for low-altitude surveillance. This network employs RISs to reflect ISAC signals into low-altitude space for sensing. To enhance signal strength, we employ active RIS (ARIS) to amplify the signals. Moreover, in order to avoid error propagation and data association in traditional sensing methods, we model low-altitude surveillance as an imaging problem based on compressed sensing theory, which can be solved through the subspace pursuit algorithm. We derive the Cramer-Rao lower bound (CRLB) of the proposed RIS-aided low-altitude imaging system and analyze the impacts of various system parameters on sensing performance, providing guidance for ISAC system configuration. Numerical results show that ARIS outperforms passive RIS under identical power constraints, achieving effective imaging and target detection at altitudes up to 300 meters.

</details>


### [71] [Tri-Hybrid Beamforming Design for integrated Sensing and Communications](https://arxiv.org/abs/2601.16036)
*Tianyu Fang,Mengyuan Ma,Markku Juntti,Nhan Thanh Nguyen*

Main category: cs.IT

TL;DR: 论文提出了一种用于集成感知与通信的三混合波束成形架构，通过优化通信SNR和感知功率的平衡，实现更高的空间增益和能效。


<details>
  <summary>Details</summary>
Motivation: 传统混合波束成形架构在超大规模天线阵列中面临能效和成本挑战。三混合波束成形结合可编程超表面天线，旨在提升集成感知与通信系统的性能，同时降低功耗和成本。

Method: 提出三混合波束成形架构，建立多目标优化问题平衡通信SNR和感知功率，受总功耗和架构物理限制约束。开发高效迭代算法，每次迭代以闭式解更新变量，实现低复杂度快速执行设计。

Result: 数值结果表明，三混合架构相比传统混合波束成形提高了空间增益和能效，但在波束对准能力方面有所降低。

Conclusion: 三混合波束成形架构为超大规模天线阵列的集成感知与通信系统提供了一种能效高、成本低的解决方案，在空间增益和能效方面具有优势，但需要权衡波束对准能力。

Abstract: Tri-hybrid beamforming architectures have been proposed to enable energy-efficient communications systems in extra-largescale antenna arrays using low-cost programmable metasurface antennas. We study the tri-hybrid beamforming design for integrated sensing and communications (ISAC) to improve both communications and sensing performances. Specifically, we formulate a multi-objective optimization problem that balances communications signal-to-noise ratio (SNR) and the sensing power at a target direction, subject to constraints on the total power consumption and physical limitations inherent to the trihybrid beamforming architecture. We develop an efficient iterative algorithm in which the variables are updated in a closed form at each iteration, leading to a low-complexity and fast-execution design. Numerical results show that the tri-hybrid architecture improves spatial gain and energy efficiency, though with reduced beam alignment capability compared to conventional hybrid beamforming architectures.

</details>


### [72] [Tensor Reed-Muller Codes: Achieving Capacity with Quasilinear Decoding Time](https://arxiv.org/abs/2601.16164)
*Emmanuel Abbe,Colin Sandon,Oscar Sprumont*

Main category: cs.IT

TL;DR: 张量Reed-Muller码在低于信道容量的任意恒定速率下可构造，具有准线性时间解码能力，提供两种构造方案


<details>
  <summary>Details</summary>
Motivation: 研究张量Reed-Muller码的解码问题，旨在构建在低于信道容量的任意恒定速率下仍能实现准线性时间解码的纠错码

Method: 定义张量Reed-Muller码为多个Reed-Muller码的张量积，提出多项式时间算法解码任意张量码，能处理一定数量的对抗性错误

Result: 1) t=3时构造：错误概率n^{-ω(log n)}，解码时间O(n log log n)；2) t≥4时构造：错误概率2^{-n^{1/2-1/2(t-2)-o(1)}}，解码时间O(n log n)

Conclusion: 张量Reed-Muller码在低于信道容量的恒定速率下可实现准线性时间解码，为高效纠错码设计提供了新途径

Abstract: Define the codewords of the Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;r_2,m_2;\dots;r_t,m_t)$ to be the evaluation vectors of all multivariate polynomials in the variables $\left\{x_{ij}\right\}_{i=1,\dots,t}^{j=1,\dots m_i}$ with degree at most $r_i$ in the variables $x_{i1},x_{i2},\dots,x_{im_i}$. The generator matrix of $\mathsf{TRM}(r_1,m_1;\dots;r_t,m_t)$ is thus the tensor product of the generator matrices of the Reed-Muller codes $\mathsf{RM}(r_1,m_1),\dots, \mathsf{RM}(r_t,m_t)$.
  We show that for any constant rate $R$ below capacity, one can construct a Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;\dotsc;r_t,m_t)$ of rate $R$ that is decodable in quasilinear time. For any blocklength $n$, we provide two constructions of such codes:
  1) Our first construction (with $t=3$) has error probability $n^{-ω(\log n)}$ and decoding time $O(n\log\log n)$.
  2) Our second construction, for any $t\geq 4$, has error probability $2^{-n^{\frac{1}{2}-\frac{1}{2(t-2)}-o(1)}}$ and decoding time $O(n\log n)$.
  One of our main tools is a polynomial-time algorithm for decoding an arbitrary tensor code $C=C_1\otimes\dotsc\otimes C_t$ from $\frac{d_{\min}(C)}{2\max\{d_{\min}(C_1),\dotsc,d_{\min}(C_t) \}}-1$ adversarial errors. Crucially, this algorithm does not require the codes $C_1,\dotsc,C_t$ to themselves be decodable in polynomial time.

</details>


### [73] [Non-Linearly Separable Distributed Computing: A Sparse Tensor Factorization Approach](https://arxiv.org/abs/2601.16171)
*Ali Khalesi,Ahmad Tanha,Derya Malak,Petros Elia*

Main category: cs.IT

TL;DR: 提出一种基于张量理论的分布式计算方案，用于多服务器环境下多项式函数评估的任务分配和通信优化


<details>
  <summary>Details</summary>
Motivation: 在N服务器分布式计算环境中，K个用户请求评估由L个实值基子函数构成的多变量多项式函数。现有方法在计算和通信成本方面存在优化空间，需要更高效的任务分配和数据通信技术。

Method: 采用张量理论方法，将请求的非线性可分解函数表示为设计好的张量$\bar{\mathcal{F}}$，通过将其稀疏分解为张量$\bar{\mathcal{E}}$和矩阵$\mathbf{D}$来定义任务分配、连接性和通信模式。使用基于固定支撑SVD的张量分解方法和多维子张量平铺技术。

Result: 提出的可实现方案显著降低了计算和通信成本，性能大幅优于现有技术。具体成本在文中推导得出。

Conclusion: 基于张量理论的方法为分布式计算中的任务分配和通信优化提供了有效框架，通过创新的张量分解和平铺技术实现了显著性能提升。

Abstract: The work considers the $N$-server distributed computing setting with $K$ users requesting functions that are arbitrary multi-variable polynomial evaluations of $L$ real (potentially non-linear) basis subfunctions. Our aim is to seek efficient task-allocation and data-communication techniques that reduce computation and communication costs. Towards this, we take a tensor-theoretic approach, in which we represent the requested non-linearly decomposable functions using a properly designed tensor $\bar{\mathcal{F}}$, whose sparse decomposition into a tensor $\bar{\mathcal{E}}$ and matrix $\mathbf{D}$ directly defines the task assignment, connectivity, and communication patterns. We here design an achievable scheme, employing novel fixed-support SVD-based tensor factorization methods and careful multi-dimensional tiling of subtensors, yielding computation and communication protocols whose costs are derived here, and which are shown to perform substantially better than the state of art.

</details>
