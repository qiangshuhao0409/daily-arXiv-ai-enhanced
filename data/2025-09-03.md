<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 27]
- [cs.AI](#cs.AI) [Total: 83]
- [cs.IT](#cs.IT) [Total: 18]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [VOTA: Parallelizing 6G-RAN Experimentation with Virtualized Over-The-Air Workloads](https://arxiv.org/abs/2509.00130)
*Chang Liu,T. D. Khoa Le,Rahul Saini,Kishor C. Joshi,George Exarchakos*

Main category: cs.NI

TL;DR: VOTA是一个开源的软件测试平台扩展方法，通过实时虚拟化和频率调谐实现并行实验最大化，同时控制干扰，在32核主机上展示出类似专用环境的性能，共享机会提升2.67倍。


<details>
  <summary>Details</summary>
Motivation: 测试平台共享在无线实验研究中普遍存在，但存在实验不便的问题：需要延迟实验或容忍计算和射频干扰，这会损害实验保真度。

Method: 提出VOTA方法，利用实时虚拟化和频率调谐技术，在软件层面实现测试平台扩展，最大化并行实验数量同时控制干扰。

Result: 在两个干扰敏感的6G用例（MIMO iDFT/DFT卸载和O-RAN DoS攻击）的演示中，在32核主机上实现了类似专用环境的性能，同时提供了2.67倍的共享机会。

Conclusion: VOTA通过软件方法有效解决了测试平台共享中的干扰问题，显著提高了并行实验能力，为无线实验研究提供了更高效的测试平台共享解决方案。

Abstract: Testbed sharing, a practice in which different researchers concurrently
develop independent use cases on top of the same testbed, is ubiquitous in
wireless experimental research. Its key drawback is experimental inconvenience:
one must delay experiments or tolerate compute and RF interference that harms
experimental fidelity. In this paper, we propose \textbf{VOTA}, an open-source,
software-only testbed scaling method that leverages real-time virtualization
and frequency tuning to maximize parallel experiments while controlling
interference. In a demonstration of two interference-sensitive 6G use cases --
\textit{MIMO iDFT/DFT Offloading} and \textit{O-RAN DoS Attack} -- running
side-by-side on a 32-core host, we showcase VOTA capabilities:
\textbf{dedicated-like} results while allowing \textbf{2.67$\times$} more
sharing opportunities.

</details>


### [2] [Intelligent Spectrum Management in Satellite Communications](https://arxiv.org/abs/2509.00286)
*Rakshitha De Silva,Shiva Raj Pokhrel,Jonathan Kua,Sithamparanathan Kandeepan*

Main category: cs.NI

TL;DR: 本论文统让性分析了智能动态谱管理技术在卫星通信网络中的应用，探讨了认知卫星网络的发展、相关技术挖掘以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统卫星谱调分配方式存在限制，面临谱资源短缺挑战，需要寻找动态谱管理方案来满足高带宽服务需求和大型卫星组网的发展。

Method: 通过调研方式，深入分析智能动态谱管理技术在卫星通信中的集成应用，评估人工智能/机器学习方法的作用，并研究系统优化的关键性能评估指标。

Result: 识别了谱管理技术的各种方法和分类，展示了AI/ML在动态谱管理中的应用潜力，以及运营强度和系统稳健性的重要性。

Conclusion: 谱管理技术为可持续发展的卫星通信网络提供了解决方案，但仍面临规制框架、网络架构和智能谱管理等方面的挑战，需要进一步研究来推动全球连接性的提升。

Abstract: Satellite Communication (SatCom) networks represent a fundamental pillar in
modern global connectivity, facilitating reliable service and extensive
coverage across a plethora of applications. The expanding demand for
high-bandwidth services and the proliferation of mega satellite constellations
highlight the limitations of traditional exclusive satellite spectrum
allocation approaches. Cognitive Radio (CR) leading to Cognitive Satellite
(CogSat) networks through Dynamic Spectrum Management (DSM), which enables the
dynamic adaptability of radio equipment to environmental conditions for optimal
performance, presents a promising solution for the emerging spectrum scarcity.
In this survey, we explore the adaptation of intelligent DSM methodologies to
SatCom, leveraging satellite network integrations. We discuss contributions and
hurdles in regulations and standardizations in realizing intelligent DSM in
SatCom, and deep dive into DSM techniques, which enable CogSat networks.
Furthermore, we extensively evaluate and categorize state-of-the-art Artificial
Intelligence (AI)/Machine Learning (ML) methods leveraged for DSM while
exploring operational resilience and robustness of such integrations. In
addition, performance evaluation metrics critical for adaptive resource
management and system optimization in CogSat networks are thoroughly
investigated. This survey also identifies open challenges and outlines future
research directions in regulatory frameworks, network architectures, and
intelligent spectrum management, paving the way for sustainable and scalable
SatCom networks for enhanced global connectivity.

</details>


### [3] [SpliDT: Partitioned Decision Trees for Scalable Stateful Inference at Line Rate](https://arxiv.org/abs/2509.00397)
*Murayyiam Parvez,Annus Zulfiqar,Roman Beltiukov,Shir Landau Feibish,Walter Willinger,Arpit Gupta,Muhammad Shahbaz*

Main category: cs.NI

TL;DR: SPLIDT是一个在可编程数据平面中部署决策树的新系统，通过滑动窗口分区推理和资源复用技术，显著提升了特征数量和处理能力，同时保持高精度和低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有数据平面中的决策树实现受限于需要预先计算所有输入特征，只能使用少量固定特征，严重限制了模型精度和可扩展性。

Method: SPLIDT采用分区推理方法：1）为决策树的不同子树分配不同的可变特征集；2）利用带内控制通道通过重循环复用数据平面资源。系统还包括定制化的训练和设计空间探索框架。

Result: 在多个真实数据集上的评估显示，SPLIDT比现有方法（如NetBeacon和Leo）支持多达5倍的状态特征，同时保持相同的低检测时延，可扩展到数百万流，重循环开销小于0.05%。

Conclusion: SPLIDT通过创新的分区推理和资源复用机制，成功解决了数据平面中决策树部署的资源约束问题，在保持高性能的同时显著提升了模型能力和精度。

Abstract: Machine learning (ML) is increasingly being deployed in programmable data
planes (switches and SmartNICs) to enable real-time traffic analysis, security
monitoring, and in-network decision-making. Decision trees (DTs) are
particularly well-suited for these tasks due to their interpretability and
compatibility with data-plane architectures, i.e., match-action tables (MATs).
However, existing in-network DT implementations are constrained by the need to
compute all input features upfront, forcing models to rely on a small, fixed
set of features per flow. This significantly limits model accuracy and
scalability under stringent hardware resource constraints.
  We present SPLIDT, a system that rethinks DT deployment in the data plane by
enabling partitioned inference over sliding windows of packets. SPLIDT
introduces two key innovations: (1) it assigns distinct, variable feature sets
to individual sub-trees of a DT, grouped into partitions, and (2) it leverages
an in-band control channel (via recirculation) to reuse data-plane resources
(both stateful registers and match keys) across partitions at line rate. These
insights allow SPLIDT to scale the number of stateful features a model can use
without exceeding hardware limits. To support this architecture, SPLIDT
incorporates a custom training and design-space exploration (DSE) framework
that jointly optimizes feature allocation, tree partitioning, and DT model
depth. Evaluation across multiple real-world datasets shows that SPLIDT
achieves higher accuracy while supporting up to 5x more stateful features than
prior approaches (e.g., NetBeacon and Leo). It maintains the same low
time-to-detection (TTD) as these systems, while scaling to millions of flows
with minimal recirculation overhead (<0.05%).

</details>


### [4] [Interference Between FM Cell Sites and CDMA Cell Sites](https://arxiv.org/abs/2509.00567)
*P. Kumar*

Main category: cs.NI

TL;DR: FM基站干扰CDMA基站的问题分析


<details>
  <summary>Details</summary>
Motivation: 电信行业中干扰问题日益严重，特别是FM基站与CDMA基站之间的干扰

Method: 分析不同类型的干扰和在干扰过程中的各种观测结果

Result: 详细描述了FM基站干扰CDMA基站的具体情况和影响

Conclusion: 识别并分析了电信网络中的关键干扰问题，为解决方案提供基础

Abstract: Interference is the major problem now days in telecommunication sector. One
type of interference which is very common now days is FM Cell sites
interference between CDMA Cell sites. Which are the types of interference and
various observations during this interference is discussed below in this paper.

</details>


### [5] [SmartFLow: A Communication-Efficient SDN Framework for Cross-Silo Federated Learning](https://arxiv.org/abs/2509.00603)
*Osama Abu Hamdan,Hao Che,Engin Arslan,Md Arifuzzaman*

Main category: cs.NI

TL;DR: SmartFLow是一个基于SDN的框架，通过动态调整路由路径来减少跨机构联邦学习中的网络拥塞，将参数同步时间最多减少47%


<details>
  <summary>Details</summary>
Motivation: 跨机构联邦学习中客户端需要反复与中央服务器交换模型权重，传统路由方法无法有效防止网络拥塞，导致通信延迟增加和训练时间延长

Method: 利用软件定义网络(SDN)的集中化和可编程控制特性，动态调整路由路径以适应变化的网络条件，减少拥塞并提高同步效率

Result: 实验结果显示，与最短路径路由相比参数同步时间减少最多47%，与容量感知路由相比减少41%，计算开销最小，可扩展到50个客户端网络

Conclusion: SmartFLow框架通过SDN技术有效提升了跨机构联邦学习的通信效率，具有实际部署的可行性

Abstract: Cross-silo Federated Learning (FL) enables multiple institutions to
collaboratively train machine learning models while preserving data privacy. In
such settings, clients repeatedly exchange model weights with a central server,
making the overall training time highly sensitive to network performance.
However, conventional routing methods often fail to prevent congestion, leading
to increased communication latency and prolonged training. Software-Defined
Networking (SDN), which provides centralized and programmable control over
network resources, offers a promising way to address this limitation. To this
end, we propose SmartFLow, an SDN-based framework designed to enhance
communication efficiency in cross-silo FL. SmartFLow dynamically adjusts
routing paths in response to changing network conditions, thereby reducing
congestion and improving synchronization efficiency. Experimental results show
that SmartFLow decreases parameter synchronization time by up to 47% compared
to shortest-path routing and 41% compared to capacity-aware routing.
Furthermore, it achieves these gains with minimal computational overhead and
scales effectively to networks of up to 50 clients, demonstrating its
practicality for real-world FL deployments.

</details>


### [6] [FLEET: A Federated Learning Emulation and Evaluation Testbed for Holistic Research](https://arxiv.org/abs/2509.00621)
*Osama Abu Hamdan,Hao Che,Engin Arslan,Md Arifuzzaman*

Main category: cs.NI

TL;DR: FLEET是一个联邦学习仿真评估测试平台，通过集成框架无关的学习组件和高保真网络模拟器，提供可扩展的配置环境，用于系统评估FL算法在实际网络条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习评估工具往往无法模拟真实操作条件，简化了算法效率、客户端异构性和网络基础设施动态之间的关键关系，导致理论算法设计与实际性能之间存在显著差距。

Method: 开发FLEET测试平台，整合通用的框架无关学习组件和高保真网络模拟器，支持多种机器学习框架、可定制真实网络拓扑和动态背景流量生成，收集算法结果与详细网络统计数据的综合指标。

Result: FLEET提供了一个全面的实验配置统一平台，使研究人员能够系统研究网络约束（如有限带宽、高延迟和数据包丢失）对FL算法收敛性和效率的影响。

Conclusion: FLEET为研究社区提供了一个强大的工具，弥合了算法理论与真实网络条件之间的差距，促进了联邦学习系统的整体性和可重复性评估。

Abstract: Federated Learning (FL) presents a robust paradigm for privacy-preserving,
decentralized machine learning. However, a significant gap persists between the
theoretical design of FL algorithms and their practical performance, largely
because existing evaluation tools often fail to model realistic operational
conditions. Many testbeds oversimplify the critical dynamics among algorithmic
efficiency, client-level heterogeneity, and continuously evolving network
infrastructure. To address this challenge, we introduce the Federated Learning
Emulation and Evaluation Testbed (FLEET). This comprehensive platform provides
a scalable and configurable environment by integrating a versatile,
framework-agnostic learning component with a high-fidelity network emulator.
FLEET supports diverse machine learning frameworks, customizable real-world
network topologies, and dynamic background traffic generation. The testbed
collects holistic metrics that correlate algorithmic outcomes with detailed
network statistics. By unifying the entire experiment configuration, FLEET
enables researchers to systematically investigate how network constraints, such
as limited bandwidth, high latency, and packet loss, affect the convergence and
efficiency of FL algorithms. This work provides the research community with a
robust tool to bridge the gap between algorithmic theory and real-world network
conditions, promoting the holistic and reproducible evaluation of federated
learning systems.

</details>


### [7] [Unsupervised Dataset Cleaning Framework for Encrypted Traffic Classification](https://arxiv.org/abs/2509.00701)
*Kun Qiu,Ying Wang,Baoqian Li,Wenjun Zhu*

Main category: cs.NI

TL;DR: 提出了一种无监督框架来自动清理加密移动流量数据，相比人工清理仅降低2%~2.5%的分类准确率，为基于机器学习的加密流量分类提供了高效的预处理方案。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备加密流量的广泛使用，传统深度包检测方法失效，而机器学习方法需要先进行流量数据清理。现有清理方案依赖人工检查每个数据包，成本高且耗时。

Method: 开发了一个无监督框架，能够自动识别和移除对训练无用的流量（如无关协议、后台活动、控制平面消息和长会话），无需人工干预。

Result: 在真实数据集上的评估显示，该框架相比人工清理仅导致分类准确率下降2%~2.5%，证明了其有效性。

Conclusion: 该无监督框架为基于机器学习的加密流量分类提供了一个高效且有效的预处理步骤，显著降低了数据清理的成本和时间消耗。

Abstract: Traffic classification, a technique for assigning network flows to predefined
categories, has been widely deployed in enterprise and carrier networks. With
the massive adoption of mobile devices, encryption is increasingly used in
mobile applications to address privacy concerns. Consequently, traditional
methods such as Deep Packet Inspection (DPI) fail to distinguish encrypted
traffic. To tackle this challenge, Artificial Intelligence (AI), in particular
Machine Learning (ML), has emerged as a promising solution for encrypted
traffic classification. A crucial prerequisite for any ML-based approach is
traffic data cleaning, which removes flows that are not useful for training
(e.g., irrelevant protocols, background activity, control-plane messages, and
long-lived sessions). Existing cleaning solutions depend on manual inspection
of every captured packet, making the process both costly and time-consuming. In
this poster, we present an unsupervised framework that automatically cleans
encrypted mobile traffic. Evaluation on real-world datasets shows that our
framework incurs only a 2%~2.5% reduction in classification accuracy compared
with manual cleaning. These results demonstrate that our method offers an
efficient and effective preprocessing step for ML-based encrypted traffic
classification.

</details>


### [8] [ReWeave: Traffic Engineering with Robust Path Weaving for Localized Link Failure Recover](https://arxiv.org/abs/2509.00708)
*Jingyi Guan,Kun Qiu,Jin Zhao*

Main category: cs.NI

TL;DR: ReWeave是一个可扩展的链路级流量工程方案，通过为每条链路配备紧凑的相邻备份路径集，实现局部重路由，在链路故障时无需控制器干预或全路径重计算。


<details>
  <summary>Details</summary>
Motivation: ISP网络中链路故障频繁发生，现有TE方案要么在脆弱静态路径上重路由导致性能下降，要么为广泛故障场景预计算备份路由带来高开销和可扩展性限制，需要有效的故障恢复机制。

Method: ReWeave为每条链路配备紧凑的相邻备份路径集，使用SRv6绕行实现动态重路由，仅故障链路两端路由器参与重路由，无需控制器干预或全路径重计算。

Result: 在大规模骨干网络评估中，ReWeave在链路故障场景下优于现有TE方案。相比HARP方案，平均最大链路利用率降低10.5%~20.1%，最坏情况利用率降低29.5%~40.9%。相比Flexile方案，在90%故障情况下实现相似的低丢包率，同时保持与最快路由器本地重路由方案相当的响应速度。

Conclusion: ReWeave通过局部重路由和紧凑备份路径设计，实现了高效、可扩展的链路故障恢复，在性能和开销之间取得了良好平衡。

Abstract: Link failures occur frequently in Internet Service Provider (ISP) networks
and pose significant challenges for Traffic Engineering (TE). Existing TE
schemes either reroute traffic over vulnerable static paths, leading to
performance degradation, or precompute backup routes for a broad range of
failure scenarios, which introduces high overhead and limits scalability.
Hence, an effective failure recovery mechanism is required to offer sufficient
path diversity under constrained overhead, thereby ensuring robust and
performant network operation. This paper presents ReWeave, a scalable and
efficient link-level TE scheme that enables localized rerouting by equipping
each link with a compact set of adjacent-only backup paths. Upon detecting a
failure, only the routers at both ends of the failed link reroute traffic
dynamically using SRv6-based detours, without controller intervention or
full-path recomputation. Evaluation results on large-scale backbone networks
demonstrate that ReWeave outperforms existing TE schemes in link failure
scenarios. Compared to HARP, the state-of-the-art failure recovery scheme based
on centralized control and dynamic traffic reallocation, our approach reduces
the average maximum link utilization by 10.5%~20.1%, and lowers the worst-case
utilization by 29.5%~40.9%. When compared with Flexile, a protection-based
scheme that precomputes routes for multi-failure scenarios, ReWeave achieves a
similarly low packet loss rate in 90% of failure cases, while maintaining a
response speed comparable to the fastest router-based local rerouting schemes.

</details>


### [9] [A Modular and Scalable Simulator for Connected-UAVs Communication in 5G Networks](https://arxiv.org/abs/2509.00868)
*Yong Su,Yiyi Chen,Shenghong Yi,Hui Feng,Yuedong Xu,Wang Xiang,Bo Hu*

Main category: cs.NI

TL;DR: 基于MATLAB开发的模块化无人机通信模拟平台，支持5G NR部署、多移动模型和多传输协议，专门用于研究无人机系统手动切换和传输协议性能问题。


<details>
  <summary>Details</summary>
Motivation: 小型无人机系统虽然应用广泛，但仍面临频繁手动切换和传统传输协议效率低下等挑战，需要专门的模拟平台来深入研究这些问题。

Method: 利用MATLAB的无线通信研究生态系统，开发了一个模块化可扩展的模拟平台，支持灵活的5G NR节点部署、可定制无人机移动模型、多网络接口扩展，以及TCP、UDP、QUIC等多种传输协议。平台还包含手动切换管理模块，支持传统和基于学习的手动策略评估。

Result: 开发出了一个专门用于无人机通信系统研究的模拟平台，该平台具有模块化、可扩展性、支持多种网络配置和传输协议的特点，能够深入分析不同传输协议对无人机通信性能的影响。

Conclusion: 该模拟平台可作为学术研究的测试床，为密集型连接无人机系统中高级传输策略的开发和评估提供支持，有助于解决无人机通信中的手动切换和协议效率问题。

Abstract: Cellular-connected UAV systems have enabled a wide range of low-altitude
aerial services. However, these systems still face many challenges, such as
frequent handovers and the inefficiency of traditional transport protocols. To
better study these issues, we develop a modular and scalable simulation
platform specifically designed for UAVs communication leveraging the research
ecology in wireless communication of MATLAB. The platform supports flexible 5G
NR node deployment, customizable UAVs mobility models, and
multi-network-interface extensions. It also supports multiple transport
protocols including TCP, UDP, QUIC, etc., allowing to investigate how different
transport protocols affect UAVs communication performance.In addition, the
platform includes a handover management module, enabling the evaluation of both
traditional and learning-based handover strategies. Our platform can serve as a
testbed for the development and evaluation of advanced transmission strategies
in cellular-connected UAV systems.

</details>


### [10] [A QoS Framework for Service Provision in Multi-Infrastructure-Sharing Networks](https://arxiv.org/abs/2509.01694)
*Quang Minh Nguyen,Eytan Modiano*

Main category: cs.NI

TL;DR: 提出一种基于MDP策略的资源配置框架，通过线性化上限信心界控制短期QoS保障，实现了可调概率性服务保证。


<details>
  <summary>Details</summary>
Motivation: 解决共享基础设施网络中的资源配置问题，为通信量和延迟提供可调的概率性服务质量保证。

Method: 采用改进的Drift-plus-Penalty(MDP)策略，利用线性化上限信心界来捕捉短期概率性服务保证，确保长期稳定性。

Result: 定义了服务保证的可行域，MDP策略实现了平均速率稳定性，优化间隔随着框架大小渐近于零。实验模拟验证了理论和算法在多基础设施网络中的优秀性能。

Conclusion: 该框架能够在共享基础设施网络中有效地提供可调概率性QoS保证，MDP策略在稳定性和优化性方面都表现出艰固的性能。

Abstract: We propose a framework for resource provisioning with QoS guarantees in
shared infrastructure networks. Our novel framework provides tunable
probabilistic service guarantees for throughput and delay. Key to our approach
is a Modified Dirft-plus-Penalty (MDP) policy that ensures long-term stability
while capturing short-term probabilistic service guarantees using linearized
upper-confidence bounds. We characterize the feasible region of service
guarantees and show that our MDP procedure achieves mean rate stability and an
optimality gap that vanishes with the frame size over which service guarantees
are provided. Finally, empirical simulations validate our theory and
demonstrate the favorable performance of our algorithm in handling QoS in
multi-infrastructure networks.

</details>


### [11] [Efficient Multichannel Rendezvous Algorithms without Global Channel Enumeration](https://arxiv.org/abs/2509.00885)
*Yi-Chia Cheng,Cheng-Shang Chang*

Main category: cs.NI

TL;DR: 本文提出了一系列基于局部敏感哈希(LSH)的低复杂度多通道酒会合算法，解决无全局通道编号系统的IoT设备间的身份发现问题。


<details>
  <summary>Details</summary>
Motivation: 多通道酒会合问题(MRP)在IoT应用中至关重要，但现有方法多需要全局通道编号系统，而实际应用中常缺乏这种全局协调。

Method: 提出LC-LSH和LC-LSH4算法，分别用于同步和异步环境，基于一致哈希技术。为确保异步情况下的有界最大酒会合时间，还提出了ASYM-LC-LSH4和QR-LC-LSH4算法。

Result: 模拟实验表明，所提算法在无全局通道编号系统的情况下，仍能达到与现有最优LSH算法相当的性能。

Conclusion: 该研究为无全局协调的IoT设备酒会合提供了高效低复杂度的解决方案，具有重要的实际应用价值。

Abstract: The multichannel rendezvous problem (MRP) is a critical challenge for
neighbor discovery in IoT applications, requiring two users to find each other
by hopping among available channels over time. This paper addresses the MRP in
scenarios where a global channel enumeration system is unavailable. To tackle
this challenge, we propose a suite of low-complexity multichannel rendezvous
algorithms based on locality-sensitive hashing (LSH), tailored for environments
where channel labels are unique L-bit identifiers rather than globally
coordinated indices. Inspired by consistent hashing techniques in distributed
systems, we develop the LC-LSH and LC-LSH4 algorithms for synchronous and
asynchronous settings, respectively. These algorithms significantly reduce
implementation complexity while maintaining expected time-to-rendezvous (ETTR)
performance comparable to state-of-the-art methods that require global channel
enumeration. To ensure bounded maximum time-to-rendezvous (MTTR) in the
asynchronous setting, we further introduce the ASYM-LC-LSH4 and QR-LC-LSH4
algorithms by embedding multiset-enhanced modular clock and quasi-random
techniques into our framework. Extensive simulations demonstrate that the
proposed algorithms achieve performance comparable to state-of-the-art LSH
algorithms in both synchronous and asynchronous settings, even without a global
channel enumeration system.

</details>


### [12] [Green Traffic Engineering for Satellite Networks Using Segment Routing Flexible Algorithm](https://arxiv.org/abs/2509.02149)
*Jintao Liang,Pablo G. Madoery,Chung-Horng Lung,Halim Yanikomeroglu,Gunes Karabulut Kurt*

Main category: cs.NI

TL;DR: 本文提出了一种基于SRv6 Flex-Algo的路由框架，通过三个逻辑切片分别优化能源效率、高可靠性和低延迟，在便携式卫星网络中实现了更好的网络性能。


<details>
  <summary>Details</summary>
Motivation: 大规模低地轨卫星组网需要能够同时优化能源消耗、保证拥塞下的包交付率以及满足时间关键流的延迟要求的路由方案。

Method: 设计了基于SRv6 Flex-Algo的框架，包含三个逻辑切片：能源效率切片(Algo 130)、高可靠性切片(Algo 129)和延迟敏感切片(Algo 128)。通过统一的混合整数线性规划(MILP)模型，将卫星CPU功耗、包交付率和端到端延迟结合在一起，允许轻量级SDN控制器从源节点导流。

Result: 在Telesat Lightspeed卫星组网的模拟中，该方案使平均CPU使用率降低73%，在流量爆发期间保持包交付率超过91%，并将激急流的温哥华到温哥华延迟减少18毫秒。

Conclusion: 证明了Flex-Algo作为一种基于切片的流量工程工具，在资源受限的卫星网络中具有重要价值。

Abstract: Large-scale low-Earth-orbit (LEO) constellations demand routing that
simultaneously minimizes energy, guarantees delivery under congestion, and
meets latency requirements for time-critical flows. We present a segment
routing over IPv6 (SRv6) flexible algorithm (Flex-Algo) framework that consists
of three logical slices: an energy-efficient slice (Algo 130), a
high-reliability slice (Algo 129), and a latency-sensitive slice (Algo 128).
The framework provides a unified mixed-integer linear program (MILP) that
combines satellite CPU power, packet delivery rate (PDR), and end-to-end
latency into a single objective, allowing a lightweight software-defined
network (SDN) controller to steer traffic from the source node. Emulation of
Telesat's Lightspeed constellation shows that, compared with different routing
schemes, the proposed design reduces the average CPU usage by 73%, maintains a
PDR above 91% during traffic bursts, and decreases urgent flow delay by 18 ms
between Ottawa and Vancouver. The results confirm Flex-Algo's value as a
slice-based traffic engineering (TE) tool for resource-constrained satellite
networks.

</details>


### [13] [BUBBLE-BLUE a multihop private network based on Bluetooth](https://arxiv.org/abs/2509.00967)
*Nadjib Achir,Philippe Jacquet*

Main category: cs.NI

TL;DR: BUBBLE-BLUE项目利用智能手机蓝牙技术构建去中心化的私有通信网络，不依赖运营商网络，通过动态连通支配集实现路由


<details>
  <summary>Details</summary>
Motivation: 创建基于智能手机蓝牙技术的私有通信网络，使用户能够在没有运营商网络的情况下自主通信，构建类似星链的地面网络

Method: 利用智能手机蓝牙技术构建私有通信气泡，采用基于动态连通支配集（CDS）的路由策略

Result: 论文展示了BB网络的特定特性以及路由性能的仿真结果

Conclusion: BUBBLE-BLUE项目成功证明了基于蓝牙技术的去中心化通信网络的可行性，为构建不依赖运营商的基础通信设施提供了新思路

Abstract: The BUBBLE-BLUE (BB) project aims to create private Bluetooth bubbles on top
of smartphones and to create a kind of terrestrial STARLINK network based on
users smartphones.. In each private bubble, participants will be able to
communicate autonomously, without recourse to private operator networks,
neither data nor cellular, relying solely on the Bluetooth technology of
smartphones. The routing strategy is based on dynamic Connected Dominant Sets
(CDS). We present the specific features of a BB network as well as some
simulation results on their routing performance.

</details>


### [14] [Quantum-based QoE Optimization in Advanced Cellular Networks: Integration and Cloud Gaming Use Case](https://arxiv.org/abs/2509.01008)
*Fatma Chaouech,Javier Villegas,António Pereira,Carlos Baena,Sergio Fortes,Raquel Barco,Dominic Gribben,Mohammad Dib,Alba Villarino,Aser Cortines,Román Orús*

Main category: cs.NI

TL;DR: 该研究探索了量子机器学习(QML)和量子启发(QI)技术在5G网络端到端服务优化中的应用，通过混合量子-经典计算框架提升网络服务质量(QoE)，特别针对云游戏服务进行优化。


<details>
  <summary>Details</summary>
Motivation: 随着5G及后续网络的发展，需要更高效的网络优化技术来提升用户体验质量(QoE)。量子计算和量子启发算法在处理复杂优化问题方面具有潜在优势，但受限于量子硬件可用性，因此需要开发混合框架来结合量子和经典计算的优势。

Method: 采用混合量子-经典计算框架，包含两个主要组件：基于用户指标、服务设置和小区配置的QoE估计器，以及使用估计结果选择最佳小区和服务配置的优化器。该方法特别针对云游戏服务的网络配置优化进行实施。

Result: QML模型在估计精度上达到或超过经典ML模型，同时减少了推理和加载时间。对于高维数据表现出更好的性能潜力，在处理更高复杂度问题方面显示出有希望的结果。

Conclusion: 研究证明了QML在网络优化中的巨大潜力，但同时也指出了数据可用性以及量子与经典ML集成复杂性等挑战，这些将成为未来的研究方向。

Abstract: This work explores the integration of Quantum Machine Learning (QML) and
Quantum-Inspired (QI) techniques for optimizing end-to-end (E2E) network
services in telecommunication systems, particularly focusing on 5G networks and
beyond. The application of QML and QI algorithms is investigated, comparing
their performance with classical Machine Learning (ML) approaches. The present
study employs a hybrid framework combining quantum and classical computing
leveraging the strengths of QML and QI, without the penalty of quantum hardware
availability. This is particularized for the optimization of the Quality of
Experience (QoE) over cellular networks. The framework comprises an estimator
for obtaining the expected QoE based on user metrics, service settings, and
cell configuration, and an optimizer that uses the estimation to choose the
best cell and service configuration. Although the approach is applicable to any
QoE-based network management, its implementation is particularized for the
optimization of network configurations for Cloud Gaming services. Then, it is
evaluated via performance metrics such as accuracy and model loading and
inference times for the estimator, and time to solution and solution score for
the optimizer. The results indicate that QML models achieve similar or superior
accuracy to classical ML models for estimation, while decreasing inference and
loading times. Furthermore, potential for better performance is observed for
higher-dimensional data, highlighting promising results for higher complexity
problems. Thus, the results demonstrate the promising potential of QML in
advancing network optimization, although challenges related to data
availability and integration complexities between quantum and classical ML are
identified as future research lines.

</details>


### [15] [Modeling and Analysis of Coexistence Between MLO NSTR-based Wi-Fi 7 and Legacy Wi-Fi](https://arxiv.org/abs/2509.01201)
*Suhwan Jung,Seokwoo Choi,Youngkeun Yoon,Ho-kyung Son,Hyoil Kim*

Main category: cs.NI

TL;DR: 本文提出了一种基于马尔可夫链的分析框架，用于模型Wi-Fi 7多链操作(MLO)与继承Wi-Fi设备的共存性能，并通过ns-3模拟验证了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi 7的多链操作(MLO)能大幅提升吞吐量和延迟性能，但现有研究忽视了MLO的关键特性。需要建立符合标准的分析框架来模型MLO的实际频道访问机制，以估计Wi-Fi 7与继承设备共存时的性能。

Method: 提出了两个分别用于AP和非AP多链设备(MLD)的新题马尔可夫链模型，模拟标准规定的多链后退行为。在饱和流量条件下推导了传输概率和冲突概率，并引入了关于各种设备类型吞吐量的闭式表达式。

Result: 通过ns-3模拟器实现了STR(同时传输接收)和NSTR(非STR)MLO操作，大量模拟实验证明了分析模型能够准确估计单设备吞吐量性能，同时揭示了WLAN间共存场景的动态特性。

Conclusion: 该研究填补了现有MLO分析模型的空白，提供了一个符合Wi-Fi 7标准的精确分析框架，为理解多链操作在混合网络环境中的性能表现提供了重要的理论基础。

Abstract: Wi-Fi 7 introduces Multi-link operation (MLO) to enhance throughput and
latency performance compared to legacy Wi-Fi standards. MLO enables
simultaneous transmission and reception through multiple links, departing from
conventional single-link operations (SLO). To fully exploit MLO's potential, it
is essential to investigate Wi-Fi 7's coexistence performance with legacy Wi-Fi
devices. Existing approaches, however, have overlooked some crucial aspects of
MLO, necessitating the development of a standards-compliant analytical
framework to model the actual channel access mechanism of MLO. Therefore, this
paper tries to fill the gap by proposing a set of novel Markov chains (MC) to
accurately model the MLO operation aligned with multi-link backoff behaviors
specified by the standard. Specifically, we design two separate MCs for AP and
non-AP multi-link devices (MLD) respectively, based on which transmit and
collision probabilities are derived under the saturated traffic condition.
Then, we also derive closed-form expressions for the throughput of various
device types in the coexistence scenario between Wi-Fi 7 and legacy Wi-Fi,
including AP MLD, non- AP MLD, and legacy devices. To validate the accuracy of
our proposed models, we developed an ns-3 based simulator by implementing both
STR(simultaneous transmission and reception) and NSTR(non-STR) based MLO
operations. Our ns-3 based extensive simulations have demonstrated that the
proposed analytic model provides accurate estimates on the per device
throughput performance, while also revealing the dynamics of inter-WLAN
coexistence scenarios.

</details>


### [16] [A Real-time Data Collection Approach for 6G AI-native Networks](https://arxiv.org/abs/2509.01276)
*He Shiwen,Dong Haolei,Wang Liangpeng,An Zhenyu*

Main category: cs.NI

TL;DR: 本文提出了一种在6G网络中与比特流处理并行运行的实时数据采集方法，通过部署数据探针捕获网络和系统状态数据，并构建数据支持系统为AI模型训练提供自动支持。


<details>
  <summary>Details</summary>
Motivation: 6G网络中AI原生网络的发展需要高质量数据，但现有研究在实时数据采集和处理方面仍面临重大挑战，需要解决实时数据获取的问题。

Method: 提出并行比特流处理的综合数据采集方法，部署数据探针在软件定义无线通信网络中捕获实时数据，构建数据支持系统集成异构数据，并在Kubernetes上搭建基于OpenAirInterface5G和Open5GS的6G通信测试平台。

Result: 通过构建完整的6G测试平台和数据支持系统，成功实现了实时数据采集和异构数据集成，并通过网络流量预测案例验证了系统功能。

Conclusion: 该方法有效解决了6G AI原生网络中实时数据采集的挑战，为AI模型训练和决策提供了可靠的数据支持，推动了6G网络智能化发展。

Abstract: During the development of the Sixth Generation (6G) networks, the integration
of Artificial Intelligence (AI) into network systems has become a focal point,
leading to the concept of AI-native networks. High quality data is essential
for developing such networks. Although some studies have explored data
collection and analysis in 6G networks, significant challenges remain,
particularly in real-time data acquisition and processing. This paper proposes
a comprehensive data collection method that operates in parallel with bitstream
processing for wireless communication networks. By deploying data probes, the
system captures real-time network and system status data in software-defined
wireless communication networks. Furthermore, a data support system is
implemented to integrate heterogeneous data and provide automatic support for
AI model training and decision making. Finally, a 6G communication testbed
using OpenAirInterface5G and Open5GS is built on Kubernetes, as well as the
system's functionality is demonstrated via a network traffic prediction case
study.

</details>


### [17] [Multi-AAV-enabled Distributed Beamforming in Low-Altitude Wireless Networking for AoI-Sensitive IoT Data Forwarding](https://arxiv.org/abs/2509.01427)
*Zifan Lang,Guixia Liu,Jiahui Li,Geng Sun,Zemin Sun,Jiacheng Wang,Dusit Niyato,Victor C. M. Leung*

Main category: cs.NI

TL;DR: 提出基于分布式波束成形的AAV辅助转发系统，通过深度强化学习优化无人机轨迹和通信调度，降低信息年龄和能耗


<details>
  <summary>Details</summary>
Motivation: 传统AAV辅助数据传输受限于有限通信范围，需要周期性返航导致延迟峰值，影响信息新鲜度和服务可靠性

Method: AAV通过分布式波束成形协作收集和中继数据，使用深度强化学习算法（SAC-TLS）联合优化轨迹和通信调度

Result: 仿真结果表明SAC-TLS算法在收敛性、平均信息年龄和AAV能耗方面优于基线算法

Conclusion: 分布式波束成形和深度强化学习的结合能有效提升物联网中信息的新鲜度和传输可靠性

Abstract: With the rapid development of low-altitude wireless networking, autonomous
aerial vehicles (AAVs) have emerged as critical enablers for timely and
reliable data delivery, particularly in remote or underserved areas. In this
context, the age of information (AoI) has emerged as a critical performance
metric for evaluating the freshness and timeliness of transmitted information
in Internet of things (IoT) networks. However, conventional AAV-assisted data
transmission is fundamentally limited by finite communication coverage ranges,
which requires periodic return flights for data relay operations. This
propulsion-repositioning cycle inevitably introduces latency spikes that raise
the AoI while degrading service reliability. To address these challenges, this
paper proposes a AAV-assisted forwarding system based on distributed
beamforming to enhance the AoI in IoT. Specifically, AAVs collaborate via
distributed beamforming to collect and relay data between the sensor nodes and
remote base station. Then, we formulate an optimization problem to minimize the
AoI and AAV energy consumption, by jointly optimizing the AAV trajectories and
communication schedules. Due to the non-convex nature of the problem and its
pronounced temporal variability, we introduce a deep reinforcement learning
solution that incorporates temporal sequence input, layer normalization gated
recurrent unit, and a squeeze-and-excitation block to capture long-term
dependencies, thereby improving decision-making stability and accuracy, and
reducing computational complexity. Simulation results demonstrate that the
proposed SAC-TLS algorithm outperforms baseline algorithms in terms of
convergence, time average AoI, and energy consumption of AAVs.

</details>


### [18] [AoI-based Scheduling of Correlated Sources for Timely Inference](https://arxiv.org/abs/2509.01926)
*Md Kamran Chowdhury Shisher,Vishrant Tripathi,Mung Chiang,Christopher G. Brinton*

Main category: cs.NI

TL;DR: 提出基于信息年龄(AoI)的信号无关调度策略，解决多相关源实时远程推理系统中的带宽受限问题，通过近似惩罚函数和在线学习方法来最小化推理误差


<details>
  <summary>Details</summary>
Motivation: 多相关源传输观测数据到接收器进行实时推理时，由于通信资源有限，数据新鲜度不足会影响推理精度。传统方法无法处理源间相关性带来的非可分离惩罚函数问题

Method: 1) 将调度问题建模为具有非可分离惩罚函数的RMAB问题；2) 提出惩罚函数近似方法并建立误差界；3) 针对已知和未知惩罚函数两种情况分别设计调度策略：已知时提供渐近最优性界，未知时开发基于bandit反馈的在线MGF学习策略

Result: 仿真结果表明所提策略能有效最小化推理误差，并在源数量增加时保持良好的可扩展性

Conclusion: 该研究解决了相关源RMAB调度中的非可分离惩罚函数挑战，提出的近似方法和在线学习策略为实时远程推理系统提供了有效的调度解决方案

Abstract: We investigate a real-time remote inference system where multiple correlated
sources transmit observations over a communication channel to a receiver. The
receiver utilizes these observations to infer multiple time-varying targets.
Due to limited communication resources, the delivered observations may not be
fresh. To quantify data freshness, we employ the Age of Information (AoI)
metric. To minimize the inference error, we aim to design a signal-agnostic
scheduling policy that leverages AoI without requiring knowledge of the actual
target values or the source observations. This scheduling problem is a restless
multi-armed bandit (RMAB) problem with a non-separable penalty function. Unlike
traditional RMABs, the correlation among sources introduces a unique challenge:
the penalty function of each source depends on the AoI of other correlated
sources, preventing decomposition of the problem into multiple independent
Markov Decision Processes (MDPs), a key step in applying traditional RMAB
solutions. To address this, we propose a novel approach by approximating the
penalty function of each source and establish an analytical bound on the
approximation error. We then develop scheduling policies for two scenarios: (i)
full knowledge of the penalty functions and (ii) no knowledge of the penalty
functions. For the case of known penalty functions, we present an upper bound
on the optimality gap of our policy in the asymptotic regime. For the case of
unknown penalty functions and signal distributions, we develop an online
learning approach that utilizes bandit feedback to learn an online Maximum Gain
First (MGF) policy. Simulation results demonstrate the effectiveness of our
proposed policies in minimizing inference error and achieving scalability in
the number of sources.

</details>


### [19] [Tree algorithms for set reconciliation](https://arxiv.org/abs/2509.02373)
*Francisco Lázaro,Čedomir Stefanović*

Main category: cs.NI

TL;DR: 基于分治策略的集合调和方案改进，通过树算法技术降低通信开销但保持时间复杂度不变


<details>
  <summary>Details</summary>
Motivation: 解决两个方拥有相似集合时的高效调和需求，优化分治集合调和(PSR)方案的通信效率

Method: 提出增强型分治集合调和(EPSR)算法，统一树算法在随机访问协议中的技术，通过事件驱动模拟器进行性能测试

Result: 新协议将通信成本降低近一半，同时保持了与PSR相同的时间复杂度和通信轮次复杂度

Conclusion: EPSR协议在不增加时间复杂性的前提下，显著提升了集合调和的通信效率，为分布式系统中的数据同步问题提供了更高效的解决方案

Abstract: In this work, a set reconciliation setting is considered in which two parties
have similar sets that they would like to reconcile. In particular, we focus on
a divide-and-conquer strategy known as partitioned set reconciliation (PSR), in
which the sets to be reconciled are successively partitioned until they contain
a number of differences below some predetermined value. Borrowing techniques
from tree-algorithms for random-access protocols, we present and analyze a
novel set reconciliation scheme that we term enhanced partitioned set
reconciliation (EPSR). This approach improves the efficiency in terms of
overhead, i.e., it yields a lower communication cost, while keeping the same
time and communication round complexity as PSR. Additionally, we simulate the
performance of the proposed algorithm in an event-driven simulator. Our
findings indicate that this novel protocol nearly halves the communication cost
of PSR while maintaining the same time complexity.

</details>


### [20] [FCT O-RAN: Design and Deployment of a Multi-Vendor End-to-End Private 5G Testbed](https://arxiv.org/abs/2509.01891)
*Amogh PC,Nagamuthu Vignesh,Pei Yiyang,Neelakantam Venkatarayalu,Pedro Henrique Amorim Rezende,Shyam Babu Mahato,Sumei Sun*

Main category: cs.NI

TL;DR: 新加坡FCT实验室部署多供应商O-RAN私有5G网络，通过数字双生和优化部署，在室内外环境中实现了高速率传输。


<details>
  <summary>Details</summary>
Motivation: 5G网络软件化、智能化转型需要云原生化部署策略，以提升性能和超越传统系统。新加坡启动FCP计划推进通信技术发展。

Method: 部署多供应商O-RAN私有5G平台，包括Microsoft Affirmed核心网、Radisys单元、Foxconn和Benetel远程单元。使用Wireless InSite创建数字双生优化部署，用QualiPoc手机测量网络性能。

Result: 室内环境（50MHz带宽）下行713Mbps/上行66Mbps；室外环境（40MHz带宽）下行371Mbps/上行55Mbps，显示了优化带宽分配的有效性。

Conclusion: 该多供应商O-RAN私有5G测试床成功展示了高性能部署，为工业应用提供了可靠的技术基础，推动了5G技术的商业化进程。

Abstract: The transformation of 5G networks into software-defined, agile, intelligent
and programmable architectures necessitates a paradigm shift in deployment
strategies. To deliver superior performance and surpass traditional systems,
public and private 5G networks must adopt software-centric cloud native
frameworks that enable flexibility through tailored configurations and
optimized deployment approaches. In Singapore, the Infocomm Media Development
Authority (IMDA) and the National Research Foundation Singapore (NRF) launched
the Future Communications Research and Development Programme (FCP) to advance
the nation's communications and connectivity landscape. At the core of this
initiative is the Future Communications Translation Lab (FCT) at the Singapore
Institute of Technology (SIT), which focuses on advancing 5G technologies to
higher readiness levels, facilitating their adoption across various industries.
A key component is the deployment of FCT O-RAN, a state-of-the-art multi-vendor
private 5G platform. The setup includes a 5G core network powered by Microsoft
Affirmed and ENEA, O-RAN Centralized and Distributed Units from Radisys. Indoor
Remote Units are deployed with Foxconn, while outdoor RUs are deployed with
Benetel. To optimize the deployment of remote units, a digital twin was created
using Wireless InSite, and performance evaluations were conducted for both the
digital twin and the private 5G deployment. Smartphones equipped with QualiPoc
were used to measure network performance. The testbed demonstrated effective
performance with optimized bandwidth allocations for both indoor and outdoor
environments. In the indoor setup, utilizing 50 MHz of bandwidth, a downlink
throughput of 713 Mbps and an uplink throughput of 66 Mbps were achieved.
Meanwhile, the outdoor setup, utilizing 40 MHz of bandwidth, achieved a
downlink throughput of 371 Mbps and an uplink throughput of 55 Mbps.

</details>


### [21] [Adaptive AI Model Partitioning over 5G Networks](https://arxiv.org/abs/2509.01906)
*Tam Thanh Nguyen,Tuan Van Ngo,Long Thanh Le,Yong Hao Pua,Mao Van Ngo,Binbin Chen,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 这篇论文提出了一种基于AI速率估计的适应性模型分割方案，用于动态优化移动设备与边缘服务器之间的DNN计算分配，以平衡延迟、能耗和隐私性能。


<details>
  <summary>Details</summary>
Motivation: 解决固定模型分割在变化的5G无线通信环境下的性能优化问题，同时缓解本地运行DNN的能耗问题和云端计算的隐私风险、带宽消耗和延迟问题。

Method: 开发了一种适应性的机器学习分割方案，基于实时AI驱动的速率估计技术，动态调整DNN模型在用户设备和边缘服务器之间的分割点。

Result: 在多种场景下评估显示，该方案在结束到结束延迟、能量消耗和隐私性能方面获得了显著的性能提升。

Conclusion: 通过适应性模型分割策略，可以有效解决移动设备上DNN计算的能耗、延迟和隐私挑战，为5G环境下的分布式AI计算提供了有效的解决方案。

Abstract: Mobile devices increasingly rely on deep neural networks (DNNs) for complex
inference tasks, but running entire models locally drains the device battery
quickly. Offloading computation entirely to cloud or edge servers reduces
processing load at devices but poses privacy risks and can incur high network
bandwidth consumption and long delays. Split computing (SC) mitigates these
challenges by partitioning DNNs between user equipment (UE) and edge servers.
However, 5G wireless channels are time-varying and a fixed splitting scheme can
lead to sub-optimal solutions. This paper addresses the limitations of fixed
model partitioning in privacy-focused image processing and explores trade-offs
in key performance metrics, including end-to-end (E2E) latency, energy
consumption, and privacy, by developing an adaptive ML partitioning scheme
based on real-time AI-powered throughput estimation. Evaluation in multiple
scenarios demonstrates significant performance gains of our scheme.

</details>


### [22] [Federated Foundation Models in Harsh Wireless Environments: Prospects, Challenges, and Future Directions](https://arxiv.org/abs/2509.01957)
*Evan Chen,Seyyedali Hosseinalipour,Christopher G. Brinton,David J. Love*

Main category: cs.NI

TL;DR: 本文探讨了联邦基础模型(FFMs)作为一种新范式，旨在解决基础模型在恶劣环境(间歇连接、有限计算、噪声数据、动态网络拓扑)中部署的挑战。


<details>
  <summary>Details</summary>
Motivation: 基础模型在通用智能和多模态理解方面表现出色，但在恶劣环境中的部署面临挑战。现有联邦学习方法依赖稳定基础设施和同步更新，难以适应极端条件。

Method: 通过将基础模型的可扩展性和泛化能力与新颖的去中心化、通信感知的联邦学习框架相结合，开发联邦基础模型(FFMs)。

Result: 提出了针对恶劣环境的系统级约束详细分析，并讨论了通信设计、模型鲁棒性和能效个性化等开放研究挑战。

Conclusion: 联邦基础模型有望在极端和对抗性条件下实现鲁棒、节能和自适应的智能系统，为解决恶劣环境中的AI部署问题提供了有前景的方向。

Abstract: Foundation models (FMs) have shown remarkable capabilities in generalized
intelligence, multimodal understanding, and adaptive learning across a wide
range of domains. However, their deployment in harsh or austere environments --
characterized by intermittent connectivity, limited computation, noisy data,
and dynamically changing network topologies -- remains an open challenge.
Existing distributed learning methods such as federated learning (FL) struggle
to adapt in such settings due to their reliance on stable infrastructure,
synchronized updates, and resource-intensive training. In this work, we explore
the potential of Federated Foundation Models (FFMs) as a promising paradigm to
address these limitations. By integrating the scalability and generalization
power of FMs with novel decentralized, communication-aware FL frameworks, we
aim to enable robust, energy-efficient, and adaptive intelligence in extreme
and adversarial conditions. We present a detailed breakdown of system-level
constraints in harsh environments, and discuss the open research challenges in
communication design, model robustness, and energy-efficient personalization
for these unique settings.

</details>


### [23] [FlexNGIA 2.0: Redesigning the Internet with Agentic AI - Protocols, Services, and Traffic Engineering Designed, Deployed, and Managed by AI](https://arxiv.org/abs/2509.02124)
*Mohamed Faten Zhani,Younes Korbi,Yamen Mkadem*

Main category: cs.NI

TL;DR: FlexNGIA 2.0是一个基于LLM AI代理的智能网络架构，能够自主编排、配置和演进网络，动态设计和适应通信协议、服务功能链等网络组件。


<details>
  <summary>Details</summary>
Motivation: 随着沉浸式通信需求增长和网络软硬件化、AI技术的发展，需要重新思考未来互联网架构，实现网络的自适应和智能化。

Method: 提出基于LLM的AI代理架构，详细设计了各代理的结构、输入输出、提示词结构以及代理间的协作机制，并通过概念验证实验进行验证。

Result: 实验结果表明LLM-based AI代理能够自动化设计、实现、部署和评估传输协议、服务功能链、网络功能等，展示了其潜在能力。

Conclusion: FlexNGIA 2.0为完全认知、自我演进的AI驱动网络开辟了新道路，同时指出了实现完全自主自适应网络的关键研究挑战。

Abstract: The escalating demands of immersive communications, alongside advances in
network softwarization and AI-driven cognition and generative reasoning, create
a pivotal opportunity to rethink and reshape the future Internet. In this
context, we introduce in this paper, FlexNGIA 2.0, an Agentic AI-driven
Internet architecture that leverages LLM-based AI agents to autonomously
orchestrate, configure, and evolve the network. These agents can, at runtime,
perceive, reason, coordinate among themselves to dynamically design, implement,
deploy, and adapt communication protocols, Service Function Chains (SFCs),
network functions, resource allocation strategies, congestion control, and
traffic engineering schemes, thereby ensuring optimal performance, reliability,
and efficiency under evolving conditions.
  The paper first outlines the overall architecture of FlexNGIA 2.0 and its
constituent LLM-Based AI agents. For each agent, we detail its design,
implementation, inputs and outputs, prompt structures, interactions with tools
and other agents, followed by preliminary proof-of-concept experiments
demonstrating its operation and potential. The results clearly highlight the
ability of these LLM-based AI agents to automate the design, the
implementation, the deployment, and the performance evaluation of transport
protocols, service function chains, network functions, congestion control
schemes, and resource allocation strategies.
  FlexNGIA 2.0 paves the way for a new class of Agentic AI-Driven networks,
where fully cognitive, self-evolving AI agents can autonomously design,
implement, adapt and optimize the network's protocols, algorithms, and
behaviors to efficiently operate across complex, dynamic, and heterogeneous
environments. To bring this vision to reality, we also identify key research
challenges toward achieving fully autonomous, adaptive, and agentic AI-driven
networks.

</details>


### [24] [AI Agent Communication from Internet Architecture Perspective: Challenges and Opportunities](https://arxiv.org/abs/2509.02317)
*Chenguang Du,Chuyi Wang,Yihan Chao,Xiaohui Xie,Yong Cui*

Main category: cs.NI

TL;DR: 本文首次从互联网架构角度系统分析AI智能体通信，提出五个关键要素（可扩展性、安全性、实时性能、高性能和可管理性）来指导多智能体生态系统的可持续发展


<details>
  <summary>Details</summary>
Motivation: AI智能体的快速发展导致通信需求激增，但当前各种框架和协议存在碎片化问题，冗余创新和孤岛设计阻碍了跨域互操作性，需要系统化视角来指导可扩展、安全、可持续的AI智能体生态系统发展

Method: 从互联网架构（历史上最成功的全球规模分布式系统）的角度出发，将数十年的互联网演进提炼为五个与智能体通信直接相关的关键要素，并用这些要素分析开发健壮多智能体生态系统的机遇和瓶颈

Result: 建立了互联网架构与AI智能体通信之间的桥梁，为AI智能体通信生态系统的可持续发展提供了新的分析视角和指导框架

Conclusion: 本文首次将互联网架构与AI智能体通信相结合，为AI智能体通信生态系统的可持续增长提供了系统性的指导框架和新的分析视角

Abstract: The rapid development of AI agents leads to a surge in communication demands.
Alongside this rise, a variety of frameworks and protocols emerge. While these
efforts demonstrate the vitality of the field, they also highlight increasing
fragmentation, with redundant innovation and siloed designs hindering
cross-domain interoperability. These challenges underscore the need for a
systematic perspective to guide the development of scalable, secure, and
sustainable AI agent ecosystems. To address this need, this paper provides the
first systematic analysis of AI agent communication from the standpoint of
Internet architecture-the most successful global-scale distributed system in
history. Specifically, we distill decades of Internet evolution into five key
elements that are directly relevant to agent communication: scalability,
security, real-time performance, high performance, and manageability. We then
use these elements to examine both the opportunities and the bottlenecks in
developing robust multi-agent ecosystems. Overall, this paper bridges Internet
architecture and AI agent communication for the first time, providing a new
lens for guiding the sustainable growth of AI agent communication ecosystems.

</details>


### [25] [Towards Intelligent Battery Management via A Five-Tier Digital Twin Framework](https://arxiv.org/abs/2509.02366)
*Tianwen Zhu,Hao Wang,Zhiwei Cao,Jiarong Xi,Yonggang Wen*

Main category: cs.NI

TL;DR: 提出五层数字孪生框架实现智能电池管理，通过贝叶斯优化校准电化学模型和物理信息神经网络预测健康状态，显著提升预测精度和安全可靠性


<details>
  <summary>Details</summary>
Motivation: 传统反应式电池管理系统在健康预测和先进维护管理方面存在局限，随着锂离子电池快速增长，导致安全风险和经济成本增加

Method: 构建五层数字孪生框架，涵盖几何可视化、预测建模、处方优化和自主操作，使用贝叶斯优化校准电化学模型，结合物理信息神经网络预测健康状态

Result: 电化学模型校准后电压和温度预测MAPE分别低于1.57%和0.39%，健康状态预测MAPE低于3%并具备量化不确定性

Conclusion: 该框架将电池管理系统提升为能够主动管理和自主优化的智能系统，显著推进关键应用中的安全性和可靠性

Abstract: Battery management systems (BMSs) are critical to ensuring safety,
efficiency, and longevity across electronics, transportation, and energy
storage. However, with the rapid growth of lithium-ion batteries, conventional
reactive BMS approaches face limitations in health prediction and advanced
maintenance management, resulting in increased safety risks and economic costs.
To address these challenges, we propose a five-tier digital twin framework for
intelligent battery management. The framework spans geometric visualization,
predictive modeling, prescriptive optimization, and autonomous operation,
enabling full lifecycle optimization. In validation, an electrochemical model
calibrated via Bayesian optimization achieved strong alignment with measured
voltage and temperature, with Mean Absolute Percentage Errors (MAPE) below
1.57\% and 0.39\%. A Physics-Informed Neural Network (PINN) then combined data
and simulations to predict State of Health (SOH), attaining MAPE under 3\% with
quantified uncertainty. This framework elevates BMSs into intelligent systems
capable of proactive management and autonomous optimization, advancing safety
and reliability in critical applications.

</details>


### [26] [Inter-DU Load Balancing in an Experimental Over-the-Air 5G Open Radio Access Network](https://arxiv.org/abs/2509.02420)
*Fahim Bashar,Asheesh Tripathi,Mayukh Roy Chowdhury,Aloizio Da Silva,Alexandre Huff*

Main category: cs.NI

TL;DR: 首个基于O-RAN架构的5G NR SA网络开源负载均衡实现，使用Near-RT RIC、srsRAN、Open5GS和SDR构建，支持E2SM-RC和E2SM-KPM协议，实现跨频段O-DU间的移动负载均衡。


<details>
  <summary>Details</summary>
Motivation: 为了解决5G网络中不同基站间负载不均衡的问题，需要开发开源的负载均衡解决方案，以验证O-RAN架构在实际部署中的可行性。

Method: 扩展srsRAN栈支持E2SM-RC Style 3 Action 1实现切换功能，增加MAC下行缓冲区容量报告到E2SM-KPM，构建基于Near-RT RIC的闭环控制系统，MLB xApp根据网络负载指标在两个不同频率的O-DU间进行负载均衡决策。

Result: 成功实现了首个完全开源的5G NR SA网络负载均衡部署，验证了O-RAN架构下Near-RT RIC闭环控制的有效性，能够实现跨频段基站间的负载均衡。

Conclusion: 该研究证明了基于O-RAN架构的开源5G网络负载均衡方案的可行性，为未来5G网络智能化管理和优化提供了重要的技术验证和参考实现。

Abstract: This paper presents the first ever fully open-source implementation of Load
Balancing (LB) in an experimental Fifth Generation (5G) New Radio (NR)
Standalone (SA) network using Open Radio Access Network (O-RAN) architecture.
The deployment leverages the O-RAN Software Community (SC) Near Real-Time RAN
Intelligent Controller (Near-RT RIC), srsRAN stack, Open5GS core, and
Software-Defined Radios (SDRs), with Commercial Off-The-Shelf (COTS) User
Equipments (UEs). The implementation extends the srsRAN stack to support E2
Service Model for RAN Control (E2SM-RC) Style 3 Action 1 to facilitate
Handovers (HOs) and adds Medium Access Control (MAC) downlink (DL) buffer
volume reporting to srsRAN's E2 Service Model for Key Performance Measurement
(E2SM-KPM). The deployment demonstrates Near-RT RIC closed-loop control where
our Mobility Load Balancing (MLB) xApp makes HO decisions based on network load
metrics for LB between two Open Distributed Units (O-DUs) operating at
different frequencies in the same band.

</details>


### [27] [On Transferring, Merging, and Splitting Task-Oriented Network Digital Twins](https://arxiv.org/abs/2509.02551)
*Zifan Zhang,Minghong Fang,Mingzhe Chen,Yuchen Liu*

Main category: cs.NI

TL;DR: 提出了统一孪生变换（UTT）框架，探索网络数字孪生（NDTs）之间的互操作，实现高效传输、合并和分割以创建任务导向的孪生体。


<details>
  <summary>Details</summary>
Motivation: 解决构建精确网络数字孪生面临的挑战，如整合多样化数据源、从物理网络映射必要属性以及保持可扩展性。

Method: 采用联合多模态和分布式映射机制，通过理论分析建立多模态门控聚合过程的收敛边界。

Result: 在轨迹重建、人员定位和传感数据生成等实际应用中验证了NDTs互操作的可行性和有效性。

Conclusion: UTT框架优化了资源利用率，降低了创建NDTs的成本，同时确保了孪生模型的一致性，为任务导向的孪生体创建提供了新的计算范式。

Abstract: The integration of digital twinning technologies is driving next-generation
networks toward new capabilities, allowing operators to thoroughly understand
network conditions, efficiently analyze valuable radio data, and innovate
applications through user-friendly, immersive interfaces. Building on this
foundation, network digital twins (NDTs) accurately depict the operational
processes and attributes of network infrastructures, facilitating predictive
management through real-time analysis and measurement. However, constructing
precise NDTs poses challenges, such as integrating diverse data sources,
mapping necessary attributes from physical networks, and maintaining
scalability for various downstream tasks. Unlike previous works that focused on
the creation and mapping of NDTs from scratch, we explore intra- and
inter-operations among NDTs within a Unified Twin Transformation (UTT)
framework, which uncovers a new computing paradigm for efficient transfer,
merging, and splitting of NDTs to create task-oriented twins. By leveraging
joint multi-modal and distributed mapping mechanisms, UTT optimizes resource
utilization and reduces the cost of creating NDTs, while ensuring twin model
consistency. A theoretical analysis of the distributed mapping problem is
conducted to establish convergence bounds for this multi-modal gated
aggregation process. Evaluations on real-world twin-assisted applications, such
as trajectory reconstruction, human localization, and sensory data generation,
demonstrate the feasibility and effectiveness of interoperability among NDTs
for corresponding task development.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [A Comparative Study of Controllability, Explainability, and Performance in Dysfluency Detection Models](https://arxiv.org/abs/2509.00058)
*Eric Zhang,Li Wei,Sarah Chen,Michael Wang*

Main category: cs.AI

TL;DR: 这篇论文对四种口吃检测模型进行了系统性比较分析，重点考察性能、可控制性和可解释性三个维度，为临床应用提供了实用指南


<details>
  <summary>Details</summary>
Motivation: 虽然口吃检测模型在性能指标上不断提升，但临床应用需要模型具备可控制性和可解释性，因此需要系统性评估各种方法的临床适用性

Method: 采用系统性对比分析方法，对YOLO-Stutter、FluentNet、UDM和SSDM四种代表性口吃检测方法进行三维度评估：性能、可控制性和可解释性，包括多个数据集的综合评估和临床专家评定

Result: YOLO-Stutter和FluentNet提供效率和简洁性但透明度有限；UDM在准确性和临床可解释性方面完成最佳平衡；SSDM虽有潜力但在实验中无法完全复现

Conclusion: 研究强调了不同口吃检测方法之间的权衡关系，持点为UDM模型在临床应用中的优势，并为临床可行的口吃模型发展指明了方向，同时提供了详细的实现见解和实际部署考虑

Abstract: Recent advances in dysfluency detection have introduced a variety of modeling
paradigms, ranging from lightweight object-detection inspired networks
(YOLOStutter) to modular interpretable frameworks (UDM). While performance on
benchmark datasets continues to improve, clinical adoption requires more than
accuracy: models must be controllable and explainable. In this paper, we
present a systematic comparative analysis of four representative
approaches--YOLO-Stutter, FluentNet, UDM, and SSDM--along three dimensions:
performance, controllability, and explainability. Through comprehensive
evaluation on multiple datasets and expert clinician assessment, we find that
YOLO-Stutter and FluentNet provide efficiency and simplicity, but with limited
transparency; UDM achieves the best balance of accuracy and clinical
interpretability; and SSDM, while promising, could not be fully reproduced in
our experiments. Our analysis highlights the trade-offs among competing
approaches and identifies future directions for clinically viable dysfluency
modeling. We also provide detailed implementation insights and practical
deployment considerations for each approach.

</details>


### [29] [Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy Against Benchmark Contamination](https://arxiv.org/abs/2509.00072)
*Terry Jingchen Zhang,Gopal Dev,Ning Wang,Nicole Ni,Wenyuan Jiang,Yinya Huang,Bernhard Schölkopf,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 通过从arXiv论文合成多步推理问题，研究发现大语言模型在知识切断点降低不明显，说明多步推理问题可有效减轻数据污染影响


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型能力评估中的数据污染问题，确保指标测量真实推理能力而非笔记记忆

Method: 利用arXiv论文时间结构，合成1,643个多步推理问题，测试4个前沿模型在知识切断点前后的性能变化

Result: 模型在知识切断点附近未出现显著性能降低，不受模型大小、开发商和发布时间影响

Conclusion: 多步推理问题的复杂性可有效减轻数据污染，建议重点从周期性收集新问题转向推理驱动的指标合成

Abstract: Capability evaluation of large language models (LLMs) is increasingly
shadowed by rising concerns of data contamination that cast doubts on whether
static benchmarks measure genuine reasoning or mere memorization. We present an
empirical study using an infinitely scalable framework to synthesize
research-level QA directly from arXiv papers, harnessing the natural temporal
structure of research publications where performance decay after knowledge
cutoffs may indicate potential contamination. We evaluated 4 frontier model
represented by 2 models of different knowledge cutoff dates per family on 1,643
multi-step reasoning questions synthesized from 20,277 arXiv papers stratified
over 26 months, covering at least 6 months before and after all cutoff dates.
Our results consistently showed a lack of significant performance decay near
knowledge cutoff dates for models of various sizes, developers, and release
dates. We further performed a comparative analysis with previous longitudinal
studies that reported significant post-cutoff performance decay using directly
retrieved questions based on public data. we hypothesize that the multi-step
reasoning required by our synthesis pipeline offered additional complexity that
goes deeper than shallow memorization, which effectively serves a mitigation
strategy against benchmark contamination. We fully open source our code and
dataset to aid reproducibility and advocate for a paradigm shift that
prioritize reasoning-driven synthesis to construct benchmarks over simply
collecting newly released questions periodically.

</details>


### [30] [Language and Experience: A Computational Model of Social Learning in Complex Tasks](https://arxiv.org/abs/2509.00074)
*Cédric Colas,Tracey Mills,Ben Prystawski,Michael Henry Tessler,Noah Goodman,Jacob Andreas,Joshua Tenenbaum*

Main category: cs.AI

TL;DR: 该研究提出了一个计算框架，通过将预训练语言模型转化为概率模型，实现人类与AI系统在结构化世界模型中的社会学习，通过语言指导加速探索和学习过程。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何整合语言指导与直接经验进行安全快速学习，并探索AI系统如何实现类似的社会学习能力，以促进人机协作学习。

Method: 使用联合概率推理框架，将预训练语言模型转化为条件概率模型，通过贝叶斯推理处理感觉运动和语言数据，在10个视频游戏中进行行为实验和模拟。

Result: 语言指导能够塑造探索行为、加速学习过程，减少风险交互并加快关键发现，同时在人类和模型中都实现了成功的知识传递。

Conclusion: 结构化、语言兼容的表征能够实现人机协作学习，为构建更安全、高效的学习系统提供了重要见解。

Abstract: The ability to combine linguistic guidance from others with direct experience
is central to human development, enabling safe and rapid learning in new
environments. How do people integrate these two sources of knowledge, and how
might AI systems? We present a computational framework that models social
learning as joint probabilistic inference over structured, executable world
models given sensorimotor and linguistic data. We make this possible by turning
a pretrained language model into a probabilistic model of how humans share
advice conditioned on their beliefs, allowing our agents both to generate
advice for others and to interpret linguistic input as evidence during Bayesian
inference. Using behavioral experiments and simulations across 10 video games,
we show how linguistic guidance can shape exploration and accelerate learning
by reducing risky interactions and speeding up key discoveries in both humans
and models. We further explore how knowledge can accumulate across generations
through iterated learning experiments and demonstrate successful knowledge
transfer between humans and models -- revealing how structured,
language-compatible representations might enable human-machine collaborative
learning.

</details>


### [31] [Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation](https://arxiv.org/abs/2509.00079)
*Andrew G. A. Correa,Ana C. H de Matos*

Main category: cs.AI

TL;DR: 一种基于余程指导的轻量级测试时精细化循环，通过标记分词不确定性来触发目标精细化，在保持质量的同时大幅降低成本


<details>
  <summary>Details</summary>
Motivation: 解决理由模型成本高、延迟大的问题，寻找一种在单次推理和豆小理由链之间的有效平衡方案

Method: 使用标记级余程来触发精细化：提取对数概率、计算top-k备选方案的香农余程，通过OR逻辑触发器（匀均困扰度、最大标记余程、低信心标记数）来触发目标精细化

Result: 小模型配合此循环可达到参考理由模型95%的质量，成本仅为1/3；在~31%的响应中实现选择性精细化，准确性提升16%百分点

Conclusion: 这种不确定性感知循环提供了质量与成本的有效平衡，适用于生产环境部署

Abstract: Reasoning models often outperform smaller models but at 3--5$\times$ higher
cost and added latency. We present entropy-guided refinement: a lightweight,
test-time loop that uses token-level uncertainty to trigger a single, targeted
refinement pass. We extract logprobs, compute Shannon entropy on top-$k$
alternatives, and apply a simple OR-logic trigger over perplexity, maximum
token entropy, and low-confidence-token count. Unlike approaches that use
entropy only for measurement or decoding, we pass a compact uncertainty report
(tokens, confidences, alternatives, context) back to the model to guide
corrective edits. On representative technical queries across reasoning,
mathematics, and code generation tasks, a small model with our loop approaches
95\% of a reference reasoning model's quality at approximately one-third of the
cost. The method achieves selective refinement on ~31\% of responses while
improving accuracy by 16 percentage points over single-pass inference. We
demonstrate that this uncertainty-aware loop provides an effective middle
ground between single-pass inference and expensive reasoning chains, making it
practical for production deployments where both quality and cost matter.

</details>


### [32] [Wrong Face, Wrong Move: The Social Dynamics of Emotion Misperception in Agent-Based Models](https://arxiv.org/abs/2509.00080)
*David Freire-Obregón*

Main category: cs.AI

TL;DR: 研究通过不同精度的情感识别分类器来分析感知准确性对群体情感和空间行为的影响，发现低准确性导致信任降低、情感分解和社会混乱


<details>
  <summary>Details</summary>
Motivation: 探索人类检测和响应他人情感的能力对社会行为的基础作用，研究感知准确性对出现的情感和空间行为的影响

Method: 使用三种不同精度的情感分类器（JAFFE、CK+、KDEF）在2D向量空间中实例化代理，让代理根据感知到的情感向积极情感运动并避免消极情感，进行同质和异质群体实验

Result: 低准确性分类器导致信任降低、情感向悲伤分解和社会组织混乱，高准确性代理形成健壮的情感聚集和弹性，甚至在情感中性场景中错觉也会导致隔离和激洞分解

Conclusion: 情感识别中的偏见或不精确可能会彻底扭曲社会过程并打断情感整合，强调了准确感知在维持健康社会关系中的关键作用

Abstract: The ability of humans to detect and respond to others' emotions is
fundamental to understanding social behavior. Here, agents are instantiated
with emotion classifiers of varying accuracy to study the impact of perceptual
accuracy on emergent emotional and spatial behavior. Agents are visually
represented with face photos from the KDEF database and endowed with one of
three classifiers trained on the JAFFE (poor), CK+ (medium), or KDEF (high)
datasets. Agents communicate locally on a 2D toroidal lattice, perceiving
neighbors' emotional state based on their classifier and responding with
movement toward perceived positive emotions and away from perceived negative
emotions. Note that the agents respond to perceived, instead of ground-truth,
emotions, introducing systematic misperception and frustration. A battery of
experiments is carried out on homogeneous and heterogeneous populations and
scenarios with repeated emotional shocks. Results show that low-accuracy
classifiers on the part of the agent reliably result in diminished trust,
emotional disintegration into sadness, and disordered social organization. By
contrast, the agent that develops high accuracy develops hardy emotional
clusters and resilience to emotional disruptions. Even in emotionally neutral
scenarios, misperception is enough to generate segregation and disintegration
of cohesion. These findings underscore the fact that biases or imprecision in
emotion recognition may significantly warp social processes and disrupt
emotional integration.

</details>


### [33] [Ensemble Debates with Local Large Language Models for AI Alignment](https://arxiv.org/abs/2509.00091)
*Ephraiem Sarabamoun*

Main category: cs.AI

TL;DR: 本地开源模型集成辩论能显著提升AI对齐能力，在推理深度和论证质量方面分别提升19.4%和34.1%，特别是在真实性和人类增强方面表现最佳


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在关键决策中扮演更重要角色，需要确保其与人类价值观对齐。依赖专有API限制了研究的可复现性和广泛参与

Method: 使用本地开源模型进行集成辩论，在150场辩论中涵盖15个场景和5种集成配置，使用7点评分量表进行评估

Result: 集成模型在7点评分量表上整体表现优于单模型基线（3.48 vs 3.13），在真实性方面提升1.25分，人类增强方面提升0.80分

Conclusion: 集成辩论为基于集成的对齐评估提供了可访问和可复现的基础，开源模型集成能够有效提升AI对齐能力

Abstract: As large language models (LLMs) take on greater roles in high-stakes
decisions, alignment with human values is essential. Reliance on proprietary
APIs limits reproducibility and broad participation. We study whether local
open-source ensemble debates can improve alignmentoriented reasoning. Across
150 debates spanning 15 scenarios and five ensemble configurations, ensembles
outperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13),
with the largest gains in reasoning depth (+19.4%) and argument quality
(+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human
enhancement (+0.80). We provide code, prompts, and a debate data set, providing
an accessible and reproducible foundation for ensemble-based alignment
evaluation.

</details>


### [34] [MODE: Mixture of Document Experts for RAG](https://arxiv.org/abs/2509.00100)
*Rahul Anand*

Main category: cs.AI

TL;DR: MODE是一种轻量级的检索增强生成方法，用聚类路由检索替代精细的最近邻搜索，适用于小规模领域特定语料库，在保持答案质量的同时显著降低检索时间。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG方法依赖大型向量数据库和跨编码器，对于小规模领域特定语料库来说过于复杂和冗余，需要更轻量级的解决方案。

Method: 将文档嵌入后分组到语义连贯的聚类中，用缓存质心表示。查询时路由到顶部质心，仅在这些聚类内检索上下文，无需外部向量数据库基础设施和重排序。

Result: 在HotpotQA和SQuAD语料库（100-500个块）上，MODE在答案质量上匹配或超过密集检索基线，同时减少了端到端检索时间。聚类粒度和多聚类路由控制召回率/精确度权衡。

Conclusion: MODE为中小型语料库提供了一个实用的解决方案，特别注重简单性、速度和主题聚焦，在保持性能的同时显著降低了系统复杂度。

Abstract: Retrieval-Augmented Generation (RAG) often relies on large vector databases
and cross-encoders tuned for large-scale corpora, which can be excessive for
small, domain-specific collections. We present MODE (Mixture of Document
Experts), a lightweight alternative that replaces fine-grained nearest-neighbor
search with cluster-and-route retrieval. Documents are embedded, grouped into
semantically coherent clusters, and represented by cached centroids. At query
time, we route to the top centroid(s) and retrieve context only within those
clusters, eliminating external vector-database infrastructure and reranking
while keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,
MODE matches or exceeds a dense-retrieval baseline in answer quality while
reducing end-to-end retrieval time. Ablations show that cluster granularity and
multi-cluster routing control the recall/precision trade-off, and that tighter
clusters improve downstream accuracy. MODE offers a practical recipe for small
and medium corpora where simplicity, speed, and topical focus matter.

</details>


### [35] [Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems](https://arxiv.org/abs/2509.00115)
*Manish Shukla*

Main category: cs.AI

TL;DR: 这篇进阶论文提出了一种适应性多维监控算法(AMDM)，用于监测自治AI系统的异常行为，在模拟和实际应用中显著提高了异常检测的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前对自治AI系统的评估主要集中在技术能力指标，缺乏人本中心和经济方面的考量，需要更全面的监测方法来确保系统在高风险领域的可靠性。

Method: 研究者形成了适应性多维监控算法(AMDM)，包括异资指标标准化、每轴指数加权移动平均阈值和通过马哈拉比距离进行联合异常检测。

Result: 在模拟实验中，AMDM将异常检测延迟从12.3秒降低到5.6秒，并将误报率从4.5%降低到0.9%，显著提升了监测性能。

Conclusion: 该研究为自治AI系统提供了一种有效的多维监测方案，能够在实际应用中提高系统的可靠性和安全性，为高风险领域的AI部署提供技术支撑。

Abstract: Agentic artificial intelligence (AI) -- multi-agent systems that combine
large language models with external tools and autonomous planning -- are
rapidly transitioning from research laboratories into high-stakes domains. Our
earlier "Basic" paper introduced a five-axis framework and proposed preliminary
metrics such as goal drift and harm reduction but did not provide an
algorithmic instantiation or empirical evidence. This "Advanced" sequel fills
that gap. First, we revisit recent benchmarks and industrial deployments to
show that technical metrics still dominate evaluations: a systematic review of
84 papers from 2023--2025 found that 83% report capability metrics while only
30% consider human-centred or economic axes [2]. Second, we formalise an
Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises
heterogeneous metrics, applies per-axis exponentially weighted moving-average
thresholds and performs joint anomaly detection via the Mahalanobis distance.
Third, we conduct simulations and real-world experiments. AMDM cuts
anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and
reduces false-positive rates from 4.5% to 0.9% compared with static thresholds.
We present a comparison table and ROC/PR curves, and we reanalyse case studies
to surface missing metrics. Code, data and a reproducibility checklist
accompany this paper to facilitate replication.

</details>


### [36] [Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning](https://arxiv.org/abs/2509.00125)
*Ang Li,Zhihang Yuan,Yang Zhang,Shouda Liu,Yisen Wang*

Main category: cs.AI

TL;DR: DACE是一种新的强化学习算法，通过利用LLM的自信心与任务难度和解决方案质量的相关性，动态平衡探索与利用的权衡，在数学推理基准测试中显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统的RLVF方法依赖稀疏的、基于结果的奖励，只能指示最终答案是否正确，无法为推理过程提供细粒度指导，这限制了学习效率。

Method: 提出Difficulty Aware Certainty guided Exploration (DACE)算法，基于策略成功率在线评估任务难度，并利用该信号调节内在奖励：对于困难任务惩罚高确定性以鼓励探索，对于简单任务奖励高确定性以提高学习效率。

Result: 在AIME和MATH等具有挑战性的数学推理基准测试中，DACE显著优于强基线方法，训练出的模型不仅准确率更高，而且在扩展测试时计算时表现出更稳健的性能。

Conclusion: DACE的自适应方法能够在不牺牲精确度的情况下促进有效探索，验证了利用LLM自信心信号来指导强化学习的有效性。

Abstract: Reinforcement Learning with Verifiable Feedback (RLVF) has become a key
technique for enhancing the reasoning abilities of Large Language Models
(LLMs). However, its reliance on sparse, outcome based rewards, which only
indicate if a final answer is correct or not, fails to provide granular
guidance on the reasoning process itself. This limitation hinders efficient
learning, as the model cannot distinguish between high quality and inefficient
solutions, nor can it learn effectively from different types of failures. To
address this, we observe that an LLMs self-certainty often correlates with task
difficulty and solution quality. We introduce Difficulty Aware Certainty guided
Exploration (DACE), a novel RL algorithm that leverages this insight to
dynamically balance the exploration exploitation trade-off. DACE assesses task
difficulty online based on the policys success rate. It then uses this signal
to modulate an intrinsic reward: for difficult tasks where the model is
struggling, DACE encourages exploration by penalizing high certainty; for
easier tasks, it encourages learning efficiency by rewarding high certainty.
Experiments on challenging mathematical reasoning benchmarks (AIME, MATH) show
that DACE significantly outperforms strong baselines. The DACE-trained models
not only achieve higher accuracy but also demonstrate more robust performance
when scaling test-time compute, validating that our adaptive approach fosters
effective exploration without sacrificing precision.

</details>


### [37] [Optimizing Health Coverage in Ethiopia: A Learning-augmented Approach and Persistent Proportionality Under an Online Budget](https://arxiv.org/abs/2509.00135)
*Davin Choo,Yohai Trabelsi,Fentabil Getnet,Samson Warkaye Lamma,Wondesen Nigatu,Kasahun Sime,Lisa Matay,Milind Tambe,Stéphane Verguet*

Main category: cs.AI

TL;DR: 基于优化框架的健康访问资源规划工具(HARP)，通过学习增强和贪心算法在预算不确定性下最大化人口覆盖率，满足区域比例目标。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚在预算有限和优先级竞争的情况下，需要优化框架来指导各地区健康系统强化工作的优先级分配。

Method: 发展HARP工具，提出两种算法：(1)学习增强方法改善单步专家建议；(2)贪心算法处理多步规划，具有强最差情况近似估计。

Result: 与埃塞俄比亚公共卫生研究所和卫生部合作，在三个地区的多种规划场景中验证了方法的实践效果。

Conclusion: HARP提供了一种原则性的决策支持框架，能够在预算不确定性下最大化人口健康服务覆盖，同时满足区域比例要求。

Abstract: As part of nationwide efforts aligned with the United Nations' Sustainable
Development Goal 3 on Universal Health Coverage, Ethiopia's Ministry of Health
is strengthening health posts to expand access to essential healthcare
services. However, only a fraction of this health system strengthening effort
can be implemented each year due to limited budgets and other competing
priorities, thus the need for an optimization framework to guide prioritization
across the regions of Ethiopia. In this paper, we develop a tool, Health Access
Resource Planner (HARP), based on a principled decision-support optimization
framework for sequential facility planning that aims to maximize population
coverage under budget uncertainty while satisfying region-specific
proportionality targets at every time step. We then propose two algorithms: (i)
a learning-augmented approach that improves upon expert recommendations at any
single-step; and (ii) a greedy algorithm for multi-step planning, both with
strong worst-case approximation estimation. In collaboration with the Ethiopian
Public Health Institute and Ministry of Health, we demonstrated the empirical
efficacy of our method on three regions across various planning scenarios.

</details>


### [38] [Virtual Group Knowledge and Group Belief in Topological Evidence Models (Extended Version)](https://arxiv.org/abs/2509.00184)
*Alexandru Baltag,Malvin Gattinger,Djanira Gomes*

Main category: cs.AI

TL;DR: 该论文研究多智能体证据模型中的群体知识和信念概念，通过将基于证据的信念和可错知识的拓扑语义从个体扩展到群体，建立了群体证据逻辑的完整公理化系统并证明了可判定性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是扩展个体层面的证据基础信念和可错知识到群体层面，建立形式化的群体知识和群体信念逻辑框架，以更好地理解和建模多智能体系统中的集体认知状态。

Method: 采用拓扑语义方法扩展证据基础信念和可错知识到群体层面，构建群体证据逻辑系统，包括硬证据和软证据的处理，并引入动态证据共享算子进行扩展。

Result: 成功建立了群体证据逻辑的完整公理化系统，证明了该逻辑的可判定性，特别对群体知识和群体信念这一重要片段进行了完整处理，并证明了动态扩展与静态基础在表达能力上的等价性。

Conclusion: 该研究为多智能体系统中的群体认知状态提供了形式化逻辑框架，证明了群体证据逻辑的良好性质，为后续研究群体知识和信念的动态演化奠定了基础。

Abstract: We study notions of (virtual) group knowledge and group belief within
multi-agent evidence models, obtained by extending the topological semantics of
evidence-based belief and fallible knowledge from individuals to groups. We
completely axiomatize and show the decidability of the logic of ("hard" and
"soft") group evidence, and do the same for an especially interesting fragment
of it: the logic of group knowledge and group belief. We also extend these
languages with dynamic evidence-sharing operators, and completely axiomatize
the corresponding logics, showing that they are co-expressive with their static
bases.

</details>


### [39] [Structured AI Decision-Making in Disaster Management](https://arxiv.org/abs/2509.01576)
*Julian Gerald Dcruz,Argyrios Zolotas,Niall Ross Greenwood,Miguel Arana-Catania*

Main category: cs.AI

TL;DR: 这篇论文提出了一种结构化决策框架，作为负责任AI的基础步骤，在灾难管理领域实现了自主决策。该框架通过引入启动器代理、级别和场景概念，在决策稳定性和准确性方面显著超过了仅依靠判断的系统和人类运营商。


<details>
  <summary>Details</summary>
Motivation: 随着AI在航空航天和应急响应等安全关键领域应用日益普遍，需要解决自主决策的伦理问题，确保决策的可靠性和可证明性，特别是在涉及人命安全的情况下。

Method: 提出了一种结构化决策框架，通过引入启动器代理、级别和场景概念来实现自主决策。将该框架应用于灾难管理领域，并与仅依靠判断的系统以及有灾难经验的人类运营商（受害者、志愿者和利益相关方）进行性能对比评估。

Result: 结果显示，结构化决策框架在多个场景下的一致准确决策稳定性比仅依靠判断的系统提高了60.94%。同时，该框架在各种场景下的准确性比人类运营商高出38.93%。

Conclusion: 这些发现证明了结构化决策框架在安全关键环境中构建更可靠自主AI应用的潜力，为负责任AI的发展奠定了基础。

Abstract: With artificial intelligence (AI) being applied to bring autonomy to
decision-making in safety-critical domains such as the ones typified in the
aerospace and emergency-response services, there has been a call to address the
ethical implications of structuring those decisions, so they remain reliable
and justifiable when human lives are at stake. This paper contributes to
addressing the challenge of decision-making by proposing a structured
decision-making framework as a foundational step towards responsible AI. The
proposed structured decision-making framework is implemented in autonomous
decision-making, specifically within disaster management. By introducing
concepts of Enabler agents, Levels and Scenarios, the proposed framework's
performance is evaluated against systems relying solely on judgement-based
insights, as well as human operators who have disaster experience: victims,
volunteers, and stakeholders. The results demonstrate that the structured
decision-making framework achieves 60.94% greater stability in consistently
accurate decisions across multiple Scenarios, compared to judgement-based
systems. Moreover, the study shows that the proposed framework outperforms
human operators with a 38.93% higher accuracy across various Scenarios. These
findings demonstrate the promise of the structured decision-making framework
for building more reliable autonomous AI applications in safety-critical
contexts.

</details>


### [40] [HiVA: Self-organized Hierarchical Variable Agent via Goal-driven Semantic-Topological Evolution](https://arxiv.org/abs/2509.00189)
*Jinzhou Tang,Jusheng Zhang,Qinhan Lv,Sidi Liu,Jing Yang,Chengpei Tang,Keze Wang*

Main category: cs.AI

TL;DR: HiVA是一个自主代理框架，通过语义-拓扑进化算法将工作流建模为自组织图，解决了现有方法在环境适应性方面的权衡问题，在多个基准测试中实现了5-10%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有自主代理系统面临关键权衡：固定工作流需要手动重新配置以适应环境变化，而灵活的反应式循环无法将推理进度转化为可转移的结构。

Method: HiVA框架使用语义-拓扑进化(STEV)算法，将代理工作流建模为自组织图，通过文本梯度作为离散域的反向传播替代品来优化混合语义-拓扑空间，包含多臂老虎机前向路由、环境反馈的诊断梯度生成以及协调更新。

Result: 在对话、编程、长上下文问答、数学和代理基准测试中，任务准确率提高了5-10%，资源效率也得到增强。

Conclusion: HiVA在未知环境中通过协同进化个体语义和拓扑结构，实现了自主任务执行的有效性，为自主代理系统提供了新的解决方案。

Abstract: Autonomous agents play a crucial role in advancing Artificial General
Intelligence, enabling problem decomposition and tool orchestration through
Large Language Models (LLMs). However, existing paradigms face a critical
trade-off. On one hand, reusable fixed workflows require manual reconfiguration
upon environmental changes; on the other hand, flexible reactive loops fail to
distill reasoning progress into transferable structures. We introduce
Hierarchical Variable Agent (HiVA), a novel framework modeling agentic
workflows as self-organized graphs with the Semantic-Topological Evolution
(STEV) algorithm, which optimizes hybrid semantic-topological spaces using
textual gradients as discrete-domain surrogates for backpropagation. The
iterative process comprises Multi-Armed Bandit-infused forward routing,
diagnostic gradient generation from environmental feedback, and coordinated
updates that co-evolve individual semantics and topology for collective
optimization in unknown environments. Experiments on dialogue, coding,
Long-context Q&A, mathematical, and agentic benchmarks demonstrate improvements
of 5-10% in task accuracy and enhanced resource efficiency over existing
baselines, establishing HiVA's effectiveness in autonomous task execution.

</details>


### [41] [Universal Deep Research: Bring Your Own Model and Strategy](https://arxiv.org/abs/2509.00244)
*Peter Belcak,Pavlo Molchanov*

Main category: cs.AI

TL;DR: 提出了Universal Deep Research (UDR)系统，这是一个通用型深度研究代理系统，允许用户创建、编辑和优化自定义研究策略，无需额外训练或微调。


<details>
  <summary>Details</summary>
Motivation: 现有的深度研究代理系统都是硬编码的，使用固定的研究策略和工具选择，缺乏灵活性和通用性。

Method: 开发了UDR系统，包装任何语言模型，提供用户界面支持用户自定义研究策略，包括最小化、扩展性和密集型研究策略示例。

Result: UDR系统展示了通用性，能够支持多种研究策略的创建和实验。

Conclusion: UDR系统为深度研究提供了一个灵活、可定制的通用框架，突破了现有硬编码代理系统的局限性。

Abstract: Deep research tools are among the most impactful and most commonly
encountered agentic systems today. We observe, however, that each deep research
agent introduced so far is hard-coded to carry out a particular research
strategy using a fixed choice of tools. We introduce Universal Deep Research
(UDR), a generalist agentic system that wraps around any language model and
enables the user to create, edit, and refine their own entirely custom deep
research strategies without any need for additional training or finetuning. To
showcase the generality of our system, we equip UDR with example minimal,
expansive, and intensive research strategies, and provide a user interface to
facilitate experimentation with the system.

</details>


### [42] [Instruction-Level Weight Shaping: A Framework for Self-Improving AI Agents](https://arxiv.org/abs/2509.00251)
*Rimom Costa*

Main category: cs.AI

TL;DR: ILWS通过指令级权重塑形技术，让LLM能够通过会话反馈和反思自主更新指令，实现知识动态更新而无需重新训练或检索增强，显著提升企业支持效率并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决LLM静态知识更新问题，避免RAG的高延迟和工程开销、提示工程的脆弱性以及微调的高成本和灾难性遗忘风险。

Method: 使用指令级权重塑形(ILWS)，通过反思引擎分析会话轨迹，生成类型化增量ΔK来更新指令、用户偏好和工具，并通过蒸馏将提示空间改进转化为权重空间。

Result: 在企业支持中吞吐量提升2.4-5.0倍，审计幻觉减少约80%；在Adobe Commerce Cloud中每小时处理工单数提升4-5倍，单工单时间降低约80%。

Conclusion: ILWS在指令层操作直到受控蒸馏，适用于需要自适应推理、工具创建和低延迟部署的动态领域，实现了知识动态更新和性能显著提升。

Abstract: Large language models (LLMs) are fluent but largely static after
pre-training; new or shifting knowledge is typically added with
retrieval-augmented generation (RAG) or fine-tuning. RAG raises latency and
engineering overhead and often fails to integrate facts; prompt engineering is
brittle and can conflict with prior knowledge; fine-tuning is costly and risks
catastrophic forgetting. We propose Instruction-Level Weight Shaping (ILWS):
curated system instructions act as external, auditable pseudo-parameters
updated after each session via reflection and user feedback. A Reflection
Engine inspects conversation traces, diagnoses reasoning successes and
failures, and proposes typed deltas $\Delta K=(\Delta S,\Delta U,\Delta T)$
over instructions, user preferences, and tools. Deltas are version-controlled,
evaluated with a sliding window of 1-5 star ratings, auto-repaired on first
failure, and rolled back on repeated failure. When an edit budget crosses a
threshold, the agent compiles a rating-weighted synthetic set and distills
matured instruction-space gains into parameters, converting prompt-space
improvements into weight-space without downtime. ILWS makes explicit the
low-rank shaping induced by context in transformer blocks, preserves
governance, and removes per-call retrieval. In enterprise support it increased
throughput 2.4-5.0x and cut audited hallucinations by about 80% versus a frozen
baseline. In an Adobe Commerce Cloud proof of concept "L0 Support", it achieved
4-5x more tickets per hour and about 80% lower time per ticket, with autonomous
instruction updates and optional tool synthesis. Because ILWS operates at the
instruction layer until controlled distillation, it generalizes to dynamic
domains (legal, medical, engineering) requiring adaptive reasoning, tool
creation, and low-latency deployment.

</details>


### [43] [SHERPA: A Model-Driven Framework for Large Language Model Execution](https://arxiv.org/abs/2509.00272)
*Boqi Chen,Kua Chen,José Antonio Hernández López,Gunter Mussbacher,Dániel Varró,Amir Feizpour*

Main category: cs.AI

TL;DR: SHERPA是一个模型驱动框架，通过将领域特定最佳实践融入分层状态机来提升LLM在复杂任务上的性能，实现更细粒度的行为控制。


<details>
  <summary>Details</summary>
Motivation: 当前LLM缺乏结构化推理能力，特别是在需要领域特定最佳实践的复杂任务上。现有的多步提示方法缺乏通用机制来控制LLM行为。

Method: 提出SHERPA框架，使用分层状态机结构化LLM执行过程，通过基于规则的或机器学习驱动的方法（包括LLM本身）实现细粒度行为控制。

Result: 在代码生成、类名生成和问答等多种任务上，SHERPA不仅复现了先前方法的性能，还进一步提升了表现。系统评估显示精心设计的状态机显著改善了LLM输出质量。

Conclusion: SHERPA框架通过状态机整合领域最佳实践，特别适用于具有成熟人类最佳实践但缺乏训练数据的复杂任务，能有效提升LLM性能。

Abstract: Recently, large language models (LLMs) have achieved widespread application
across various fields. Despite their impressive capabilities, LLMs suffer from
a lack of structured reasoning ability, particularly for complex tasks
requiring domain-specific best practices, which are often unavailable in the
training data. Although multi-step prompting methods incorporating human best
practices, such as chain-of-thought and tree-of-thought, have gained
popularity, they lack a general mechanism to control LLM behavior. In this
paper, we propose SHERPA, a model-driven framework to improve the LLM
performance on complex tasks by explicitly incorporating domain-specific best
practices into hierarchical state machines. By structuring the LLM execution
processes using state machines, SHERPA enables more fine-grained control over
their behavior via rules or decisions driven by machine learning-based
approaches, including LLMs. We show that SHERPA is applicable to a wide variety
of tasks-specifically, code generation, class name generation, and question
answering-replicating previously proposed approaches while further improving
the performance. We demonstrate the effectiveness of SHERPA for the
aforementioned tasks using various LLMs. Our systematic evaluation compares
different state machine configurations against baseline approaches without
state machines. Results show that integrating well-designed state machines
significantly improves the quality of LLM outputs, and is particularly
beneficial for complex tasks with well-established human best practices but
lacking data used for training LLMs.

</details>


### [44] [SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces](https://arxiv.org/abs/2509.00287)
*Brian Wang,Mani Srivastava*

Main category: cs.AI

TL;DR: SIGMUS系统利用大语言模型自动整合城市多模态传感器数据，构建知识图谱来识别和推理城市事件，无需依赖人工规则


<details>
  <summary>Details</summary>
Motivation: 城市多模态传感器数据丰富但碎片化，人工整合困难，需要自动化系统来识别事件与多模态数据之间的关系

Method: 使用大语言模型生成世界知识，建立多模态数据与城市事件之间的关系，构建组织化知识图谱

Result: 系统能够合理连接5种不同数据源（新闻文本、监控图像、空气质量、天气和交通测量）与相关事件

Conclusion: SIGMUS成功实现了城市多模态数据的语义集成，为事件识别和预测提供了有效框架

Abstract: Modern urban spaces are equipped with an increasingly diverse set of sensors,
all producing an abundance of multimodal data. Such multimodal data can be used
to identify and reason about important incidents occurring in urban landscapes,
such as major emergencies, cultural and social events, as well as natural
disasters. However, such data may be fragmented over several sources and
difficult to integrate due to the reliance on human-driven reasoning for
identifying relationships between the multimodal data corresponding to an
incident, as well as understanding the different components which define an
incident. Such relationships and components are critical to identifying the
causes of such incidents, as well as producing forecasting the scale and
intensity of future incidents as they begin to develop. In this work, we create
SIGMUS, a system for Semantic Integration for Knowledge Graphs in Multimodal
Urban Spaces. SIGMUS uses Large Language Models (LLMs) to produce the necessary
world knowledge for identifying relationships between incidents occurring in
urban spaces and data from different modalities, allowing us to organize
evidence and observations relevant to an incident without relying and
human-encoded rules for relating multimodal sensory data with incidents. This
organized knowledge is represented as a knowledge graph, organizing incidents,
observations, and much more. We find that our system is able to produce
reasonable connections between 5 different data sources (new article text, CCTV
images, air quality, weather, and traffic measurements) and relevant incidents
occurring at the same time and location.

</details>


### [45] [NEWSAGENT: Benchmarking Multimodal Agents as Journalists with Real-World Newswriting Tasks](https://arxiv.org/abs/2509.00446)
*Yen-Che Chien,Kuang-Da Wang,Wei-Yao Wang,Wen-Chih Peng*

Main category: cs.AI

TL;DR: NEWSAGENT是一个评估AI代理在新闻写作中多模态数据处理能力的基准测试，包含6000个真实新闻案例，测试代理的搜索、信息选择和文章编辑能力。


<details>
  <summary>Details</summary>
Motivation: 当前自主数字代理在结构化任务中展现潜力，但尚不清楚基于代理的系统能在多大程度上提高多模态网络数据生产力，特别是在需要迭代规划、解释和上下文推理的新闻领域。

Method: 引入NEWSAGENT基准测试，通过给定写作指令和原始数据，要求代理识别叙事视角、基于关键词查询、检索历史背景并生成完整文章，测试代理在多模态内容处理中的表现。

Result: 评估显示代理能够检索相关事实，但在规划和叙事整合方面存在困难，表明当前代理系统在复杂新闻写作任务中仍有局限。

Conclusion: NEWSAGENT为评估代理在多模态网络数据操作到实际生产力转化方面的能力提供了现实的测试平台，有助于迭代和改进代理能力。

Abstract: Recent advances in autonomous digital agents from industry (e.g., Manus AI
and Gemini's research mode) highlight potential for structured tasks by
autonomous decision-making and task decomposition; however, it remains unclear
to what extent the agent-based systems can improve multimodal web data
productivity. We study this in the realm of journalism, which requires
iterative planning, interpretation, and contextual reasoning from multimodal
raw contents to form a well structured news. We introduce NEWSAGENT, a
benchmark for evaluating how agents can automatically search available raw
contents, select desired information, and edit and rephrase to form a news
article by accessing core journalistic functions. Given a writing instruction
and firsthand data as how a journalist initiates a news draft, agents are
tasked to identify narrative perspectives, issue keyword-based queries,
retrieve historical background, and generate complete articles. Unlike typical
summarization or retrieval tasks, essential context is not directly available
and must be actively discovered, reflecting the information gaps faced in
real-world news writing. NEWSAGENT includes 6k human-verified examples derived
from real news, with multimodal contents converted to text for broad model
compatibility. We evaluate open- and closed-sourced LLMs with commonly-used
agentic frameworks on NEWSAGENT, which shows that agents are capable of
retrieving relevant facts but struggling with planning and narrative
integration. We believe that NEWSAGENT serves a realistic testbed for iterating
and evaluating agent capabilities in terms of multimodal web data manipulation
to real-world productivity.

</details>


### [46] [Multi-Agent Data Visualization and Narrative Generation](https://arxiv.org/abs/2509.00481)
*Anton Wolter,Georgios Vidalakis,Michael Yu,Ankit Grover,Vaishali Dhanoa*

Main category: cs.AI

TL;DR: 提出一个轻量级多智能体系统，自动化数据可视化工作流，从数据探索到生成连贯的可视化叙事，通过混合架构和确定性组件提高透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: AI智能体技术的进步使得数据可视化领域能够实现从数据到沟通的完整流程自动化，需要构建可靠且透明的人机协作系统。

Method: 采用混合多智能体架构结合确定性组件，将关键逻辑外部化到LLM之外，提供模块化输出支持精确修改而无需完全重新生成。

Result: 在4个不同数据集上评估显示系统具有良好的泛化能力、叙事质量和计算效率，且依赖项极少。

Conclusion: 该系统为可持续的人机协作提供了有效解决方案，在数据可视化自动化工作流中表现出色。

Abstract: Recent advancements in the field of AI agents have impacted the way we work,
enabling greater automation and collaboration between humans and agents. In the
data visualization field, multi-agent systems can be useful for employing
agents throughout the entire data-to-communication pipeline. We present a
lightweight multi-agent system that automates the data analysis workflow, from
data exploration to generating coherent visual narratives for insight
communication. Our approach combines a hybrid multi-agent architecture with
deterministic components, strategically externalizing critical logic from LLMs
to improve transparency and reliability. The system delivers granular, modular
outputs that enable surgical modifications without full regeneration,
supporting sustainable human-AI collaboration. We evaluated our system across 4
diverse datasets, demonstrating strong generalizability, narrative quality, and
computational efficiency with minimal dependencies.

</details>


### [47] [Artificial Intelligence-Based Analysis of Ice Cream Melting Behavior Under Various Ingredients](https://arxiv.org/abs/2509.00507)
*Zhang Lai Bin,Zhen Bin It*

Main category: cs.AI

TL;DR: 本研究探讨了刺槐豆胶、瓜尔胶、麦芽糊精和卡拉胶对自制冰淇淋融化行为的影响，通过融化测试和图像分析评估这些稳定剂的融化抗性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 冰淇淋的融化稳定性是影响消费者接受度和产品质量的关键因素，需要研究不同稳定剂的效果并寻找更具成本效益的配方。

Method: 制备含不同添加剂的冰淇淋样品，在受控条件下进行融化测试，使用延时摄影记录融化过程，并利用Python和OpenCV进行图像处理和分析。

Result: 所有样品融化后仍保持泡沫状结构，稳定剂有助于形成稳定的气室基质；重新冷冻后再次融化的样品显示出更强的结构稳定性；不同稳定剂的效果存在差异。

Conclusion: 研究为冰淇淋配方中常用食品添加剂的功能作用提供了见解，展示了在保持耐久性的同时实现经济效益的配方开发潜力，对小型和商业冰淇淋生产具有实际应用价值。

Abstract: The stability of ice cream during melting is a critical factor for consumer's
acceptance and product quality. With the commonly added stabilizer to improve
texture, structure and slower melting as the factors to analyze. This report
explores the effects of locust bean gum, guar gum, maltodextrin, and
carrageenan on the melting behavior of homemade ice cream. The main objective
was to assess how these additives influence melting resistance and to identify
a more cost-effective recipe formulation. Ice cream samples incorporating each
additive were prepared and subjected to melting tests under controlled
conditions. Timelapse recordings were used to capture and analyze the
progression of melting over time. Python and OpenCV is used for process and
analysis. Observations revealed that all samples retained a foam-like structure
even after melting, suggesting the stabilizers contributed to the formation of
a stable air-cell matrix. Furthermore, when the melted samples were re-frozen
and subsequently melted again, they displayed increased sturdiness, indicating
improved resilience of the ice cream structure. Comparative analysis of the
different stabilizers highlighted variations in their effectiveness, with some
offering stronger melting resistance and structural support than others.
Overall, the findings provide insights into the functional roles of commonly
used food additives in ice cream formulation. By evaluating both performance
and cost, this study demonstrates the potential for developing recipes that
balance durability with economic efficiency, contributing to practical
applications in both small-scale and commercial ice cream production.

</details>


### [48] [LLM-Assisted Iterative Evolution with Swarm Intelligence Toward SuperBrain](https://arxiv.org/abs/2509.00510)
*Li Weigang,Pedro Carvalho Brom,Lucas Ramson Siefert*

Main category: cs.AI

TL;DR: SuperBrain框架通过LLM与人类用户的协同进化，从子类大脑到超类大脑的动态路径实现集体智能，包括个性化交互、进化优化、群体协调和知识整合。


<details>
  <summary>Details</summary>
Motivation: 现有方法如静态提示工程或孤立代理模拟存在局限性，需要一种动态的、可扩展的集体智能框架，实现从个性化认知到群体智慧的演进。

Method: 1) 用户与LLM形成个性化认知对；2) GA辅助的前后向进化优化提示和任务性能；3) 多子类大脑通过群体智能协调；4) 标准化行为整合为超类大脑。

Result: 提出了理论框架和初步实现（如无人机调度、关键词过滤），建立了跨认知对知识整合的注册机制。

Conclusion: 该工作为可扩展、可解释且符合伦理的集体AI提供了概念基础和架构路线图。

Abstract: We propose a novel SuperBrain framework for collective intelligence, grounded
in the co-evolution of large language models (LLMs) and human users. Unlike
static prompt engineering or isolated agent simulations, our approach
emphasizes a dynamic pathway from Subclass Brain to Superclass Brain: (1) A
Subclass Brain arises from persistent, personalized interaction between a user
and an LLM, forming a cognitive dyad with adaptive learning memory. (2) Through
GA-assisted forward-backward evolution, these dyads iteratively refine prompts
and task performance. (3) Multiple Subclass Brains coordinate via Swarm
Intelligence, optimizing across multi-objective fitness landscapes and
exchanging distilled heuristics. (4) Their standardized behaviors and cognitive
signatures integrate into a Superclass Brain, an emergent meta-intelligence
capable of abstraction, generalization and self-improvement. We outline the
theoretical constructs, present initial implementations (e.g., UAV scheduling,
KU/KI keyword filtering) and propose a registry for cross-dyad knowledge
consolidation. This work provides both a conceptual foundation and an
architectural roadmap toward scalable, explainable and ethically aligned
collective AI.

</details>


### [49] [Text-to-Layout: A Generative Workflow for Drafting Architectural Floor Plans Using LLMs](https://arxiv.org/abs/2509.00543)
*Jayakrishna Duggempudi,Lu Gao,Ahmed Senouci,Zhe Han,Yunpeng Zhang*

Main category: cs.AI

TL;DR: 使用大语言模型从自然语言提示自动生成建筑平面图的AI工作流，包含墙体、门窗、家具排列，可直接集成到BIM设计工具


<details>
  <summary>Details</summary>
Motivation: 开发能够根据自然语言描述自动生成功能性建筑平面图的AI系统，减少手动设计工作量，并与专业BIM工具无缝集成

Method: 结合提示工程、家具排列精炼算法和Python脚本，通过LLM解释文本输入生成空间协调的草图方案

Result: 通过中等居住布局案例实证，系统能够以最小化手动工作生成功能性和结构化输出，且保持Revit原生参数化属性

Conclusion: 该工作流设计通透可复制，文档化了关键提示规范，为其他研究人员提供了独立实现的可能性，并支持直接集成到专业BIM流程

Abstract: This paper presents the development of an AI-powered workflow that uses Large
Language Models (LLMs) to assist in drafting schematic architectural floor
plans from natural language prompts. The proposed system interprets textual
input to automatically generate layout options including walls, doors, windows,
and furniture arrangements. It combines prompt engineering, a furniture
placement refinement algorithm, and Python scripting to produce spatially
coherent draft plans compatible with design tools such as Autodesk Revit. A
case study of a mid-sized residential layout demonstrates the approach's
ability to generate functional and structured outputs with minimal manual
effort. The workflow is designed for transparent replication, with all key
prompt specifications documented to enable independent implementation by other
researchers. In addition, the generated models preserve the full range of
Revit-native parametric attributes required for direct integration into
professional BIM processes.

</details>


### [50] [Social World Models](https://arxiv.org/abs/2509.00559)
*Xuhui Zhou,Jiarui Liu,Akhila Yerukola,Hyunwoo Kim,Maarten Sap*

Main category: cs.AI

TL;DR: 提出了S3AP结构化社会世界表示形式，帮助AI系统更好地理解社会动态和推理他人心理状态，在多个社会推理任务中达到新的SOTA性能


<details>
  <summary>Details</summary>
Motivation: 人类能够直觉地模拟未言明的社会动态并推理他人视角，而AI系统在这方面存在困难，需要更好的结构化表示来处理隐式社会情境

Method: 采用POMDP驱动的设计，将社会互动表示为结构化元组（状态、观察、智能体行动、心理状态），可以从自由形式叙述中自动推导

Result: 在5个社会推理任务中显著提升性能（如FANToM心理理论推理提升51%），在社会互动基准SOTOPIA上决策能力提升18%

Conclusion: S3AP作为一种通用的社会世界状态表示形式具有巨大潜力，能够开发出更具社会意识的系统来更好地导航社会互动

Abstract: Humans intuitively navigate social interactions by simulating unspoken
dynamics and reasoning about others' perspectives, even with limited
information. In contrast, AI systems struggle to automatically structure and
reason about these implicit social contexts. In this paper, we introduce a
novel structured social world representation formalism (S3AP), designed to help
AI systems reason more effectively about social dynamics. Following a
POMDP-driven design, S3AP represents social interactions as structured tuples,
such as state, observation, agent actions, and mental states, which can be
automatically induced from free-form narratives or other inputs. We first show
S3AP can help LLMs better understand social narratives across 5 social
reasoning tasks (e.g., +51% improvement on FANToM's theory-of-mind reasoning
with OpenAI's o1), reaching new state-of-the-art (SOTA) performance. We then
induce social world models from these structured representations, demonstrating
their ability to predict future social dynamics and improve agent
decision-making, yielding up to +18% improvement on the SOTOPIA social
interaction benchmark. Our findings highlight the promise of S3AP as a
powerful, general-purpose representation for social world states, enabling the
development of more socially-aware systems that better navigate social
interactions.

</details>


### [51] [BALM-TSF: Balanced Multimodal Alignment for LLM-Based Time Series Forecasting](https://arxiv.org/abs/2509.00622)
*Shiqiao Zhou,Holger Schöner,Huanbo Lyu,Edouard Fouché,Shuo Wang*

Main category: cs.AI

TL;DR: BALM-TSF是一个轻量级的时间序列预测框架，通过平衡文本和时间序列两种模态的对齐，解决了现有多模态方法中模态不平衡的问题，在长期和少样本预测中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多模态时间序列预测方法存在模态不平衡问题，往往过度强调文本模态而忽视时间序列模态，导致信息损失和预测性能下降。

Method: 提出BALM-TSF框架：1) 时间序列编码器处理原始时间序列；2) 使用可学习提示将时间序列的统计描述输入LLM生成文本嵌入；3) 通过缩放策略和对比目标将文本嵌入映射到时间序列嵌入的潜在空间；4) 将对齐后的两种模态嵌入整合进行预测。

Result: 在标准基准测试中，BALM-TSF以最少的可训练参数，在长期和少样本预测任务中都达到了最先进的性能。

Conclusion: BALM-TSF能够有效平衡文本和时间序列两种模态，充分利用它们的互补信息，显著提升了时间序列预测的性能。

Abstract: Time series forecasting is a long-standing and highly challenging research
topic. Recently, driven by the rise of large language models (LLMs), research
has increasingly shifted from purely time series methods toward harnessing
textual modalities to enhance forecasting performance. However, the vast
discrepancy between text and temporal data often leads current multimodal
architectures to over-emphasise one modality while neglecting the other,
resulting in information loss that harms forecasting performance. To address
this modality imbalance, we introduce BALM-TSF (Balanced Multimodal Alignment
for LLM-Based Time Series Forecasting), a lightweight time series forecasting
framework that maintains balance between the two modalities. Specifically, raw
time series are processed by the time series encoder, while descriptive
statistics of raw time series are fed to an LLM with learnable prompt,
producing compact textual embeddings. To ensure balanced cross-modal context
alignment of time series and textual embeddings, a simple yet effective scaling
strategy combined with a contrastive objective then maps these textual
embeddings into the latent space of the time series embeddings. Finally, the
aligned textual semantic embeddings and time series embeddings are together
integrated for forecasting. Extensive experiments on standard benchmarks show
that, with minimal trainable parameters, BALM-TSF achieves state-of-the-art
performance in both long-term and few-shot forecasting, confirming its ability
to harness complementary information from text and time series. Code is
available at https://github.com/ShiqiaoZhou/BALM-TSF.

</details>


### [52] [NetGent: Agent-Based Automation of Network Application Workflows](https://arxiv.org/abs/2509.00625)
*Jaber Daneshamooz,Eugene Vuong,Laasya Koduru,Sanjay Chandrasekaran,Arpit Gupta*

Main category: cs.AI

TL;DR: NetGent是一个AI代理框架，通过自然语言规则自动化复杂应用工作流来生成真实网络流量数据集，解决了现有浏览器自动化工具脆弱且成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 开发通用ML网络模型需要从具有多样化真实网络应用流量的环境中收集数据，但现有的浏览器自动化工具在多样性、可重复性、真实性和效率方面存在脆弱性和高成本问题。

Method: 用户用自然语言规则定义状态依赖动作的工作流，这些抽象规范被编译成非确定性有限自动机(NFAs)，状态合成组件将其转换为可重用的可执行代码，支持确定性重放、状态缓存减少冗余LLM调用，并能快速适应应用接口变化。

Result: NetGent自动化了50多个工作流，涵盖视频点播、直播、视频会议、社交媒体和网络爬虫等领域，生成了真实的流量轨迹，并对UI变化保持鲁棒性。

Conclusion: NetGent通过结合基于语言代理的灵活性和编译执行的可靠性，为生成多样化、可重复的数据集提供了可扩展的基础，推动了网络ML的发展。

Abstract: We present NetGent, an AI-agent framework for automating complex application
workflows to generate realistic network traffic datasets. Developing
generalizable ML models for networking requires data collection from network
environments with traffic that results from a diverse set of real-world web
applications. However, using existing browser automation tools that are
diverse, repeatable, realistic, and efficient remains fragile and costly.
NetGent addresses this challenge by allowing users to specify workflows as
natural-language rules that define state-dependent actions. These abstract
specifications are compiled into nondeterministic finite automata (NFAs), which
a state synthesis component translates into reusable, executable code. This
design enables deterministic replay, reduces redundant LLM calls through state
caching, and adapts quickly when application interfaces change. In experiments,
NetGent automated more than 50+ workflows spanning video-on-demand streaming,
live video streaming, video conferencing, social media, and web scraping,
producing realistic traffic traces while remaining robust to UI variability. By
combining the flexibility of language-based agents with the reliability of
compiled execution, NetGent provides a scalable foundation for generating the
diverse, repeatable datasets needed to advance ML in networking.

</details>


### [53] [On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized Knowledge Representations](https://arxiv.org/abs/2509.00710)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 提出模块化多智能体框架，将法律推理分解为知识获取和应用两个阶段，通过形式化知识表示和符号推理显著提升法律AI系统的透明度和准确性


<details>
  <summary>Details</summary>
Motivation: 法律推理需要精确解释法律条文和一致应用复杂规则，这对AI系统构成重大挑战，需要解决端到端方法缺乏透明度的问题

Method: 采用模块化多智能体框架：第一阶段由专门智能体提取法律概念并形式化规则；第二阶段通过查询分析、符号推理和程序化实现三个步骤应用知识到具体案例

Result: 在法定税务计算任务上取得显著改进，基础模型准确率达到76.4%，相比基线18.8%大幅提升，有效缩小了推理模型与基础模型之间的性能差距

Conclusion: 模块化架构与形式化知识表示能使复杂法律推理更易于通过计算高效模型实现，同时增强AI法律推理的一致性和可解释性，为构建更透明、可信和有效的法律AI系统奠定基础

Abstract: Legal reasoning requires both precise interpretation of statutory language
and consistent application of complex rules, presenting significant challenges
for AI systems. This paper introduces a modular multi-agent framework that
decomposes legal reasoning into distinct knowledge acquisition and application
stages. In the first stage, specialized agents extract legal concepts and
formalize rules to create verifiable intermediate representations of statutes.
The second stage applies this knowledge to specific cases through three steps:
analyzing queries to map case facts onto the ontology schema, performing
symbolic inference to derive logically entailed conclusions, and generating
final answers using a programmatic implementation that operationalizes the
ontological knowledge. This bridging of natural language understanding with
symbolic reasoning provides explicit and verifiable inspection points,
significantly enhancing transparency compared to end-to-end approaches.
Evaluation on statutory tax calculation tasks demonstrates substantial
improvements, with foundational models achieving 76.4\% accuracy compared to
18.8\% baseline performance, effectively narrowing the performance gap between
reasoning and foundational models. These findings suggest that modular
architectures with formalized knowledge representations can make sophisticated
legal reasoning more accessible through computationally efficient models while
enhancing consistency and explainability in AI legal reasoning, establishing a
foundation for future research into more transparent, trustworthy, and
effective AI systems for legal domain.

</details>


### [54] [OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination](https://arxiv.org/abs/2509.00723)
*Junzhe Chen,Tianshu Zhang,Shiyu Huang,Yuwei Niu,Chao Sun,Rongzhou Zhang,Guanyu Zhou,Lijie Wen,Xuming Hu*

Main category: cs.AI

TL;DR: OmniDPO是一个偏好对齐框架，通过构建文本偏好和多模态偏好样本对，有效减少全模态大语言模型中的幻觉问题，提升多模态推理能力


<details>
  <summary>Details</summary>
Motivation: 全模态大语言模型存在幻觉问题，文本模态先验占主导地位，忽视视听信息，且训练时独立对齐各模态而忽略音视频内在关联

Method: 提出OmniDPO框架，包含两种策略：(1)构建文本偏好样本对增强音视频交互理解；(2)构建多模态偏好样本对加强视听信息关注

Result: 在两个OLLM上的实验表明，OmniDPO不仅能有效缓解多模态幻觉，还显著提升了跨模态推理能力

Conclusion: OmniDPO通过同时解决文本主导和模态独立对齐问题，有效改善了多模态基础能力并减少幻觉

Abstract: Recently, Omni-modal large language models (OLLMs) have sparked a new wave of
research, achieving impressive results in tasks such as audio-video
understanding and real-time environment perception. However, hallucination
issues still persist. Similar to the bimodal setting, the priors from the text
modality tend to dominate, leading OLLMs to rely more heavily on textual cues
while neglecting visual and audio information. In addition, fully multimodal
scenarios introduce new challenges. Most existing models align visual or
auditory modalities with text independently during training, while ignoring the
intrinsic correlations between video and its corresponding audio. This
oversight results in hallucinations when reasoning requires interpreting hidden
audio cues embedded in video content. To address these challenges, we propose
OmniDPO, a preference-alignment framework designed to mitigate hallucinations
in OLLMs. Specifically, OmniDPO incorporates two strategies: (1) constructing
text-preference sample pairs to enhance the model's understanding of
audio-video interactions; and (2) constructing multimodal-preference sample
pairs to strengthen the model's attention to visual and auditory information.
By tackling both challenges, OmniDPO effectively improves multimodal grounding
and reduces hallucination. Experiments conducted on two OLLMs demonstrate that
OmniDPO not only effectively mitigates multimodal hallucinations but also
significantly enhances the models' reasoning capabilities across modalities.
All code and datasets will be released upon paper acceptance.

</details>


### [55] [Efficient Graph Understanding with LLMs via Structured Context Injection](https://arxiv.org/abs/2509.00740)
*Govind Waghmare,Sumedh BG,Sonia Gupta,Srikanta Bedathur*

Main category: cs.AI

TL;DR: 提出了结构化上下文注入框架，通过系统性地在输入中嵌入任务特定信息来指导LLM解决图问题，无需微调即可实现成本效益和轻量化。


<details>
  <summary>Details</summary>
Motivation: 图推理任务对LLM具有挑战性，需要将任务映射到概念基础表示，但通过微调或多步查询实现这种映射成本高且效率低。

Method: 结构化上下文注入方法，将任务特定信息直接嵌入输入，使LLM能够隐式地将任务与基础概念空间对齐。

Result: 在多个图任务上评估显示性能持续改进，结构化输入上下文可以媲美或超越更复杂的方法。

Conclusion: 结构化上下文注入是LLM图理解的有效且可扩展策略，在准确性和计算成本之间提供了良好权衡。

Abstract: Large Language Models (LLMs) have shown strong capabilities in solving
problems across domains, including graph-related tasks traditionally addressed
by symbolic or algorithmic methods. In this work, we present a framework for
structured context injection, where task-specific information is systematically
embedded in the input to guide LLMs in solving a wide range of graph problems.
Our method does not require fine-tuning of LLMs, making it cost-efficient and
lightweight. We observe that certain graph reasoning tasks remain challenging
for LLMs unless they are mapped to conceptually grounded representations.
However, achieving such mappings through fine-tuning or repeated multi-step
querying can be expensive and inefficient. Our approach offers a practical
alternative by injecting structured context directly into the input, enabling
the LLM to implicitly align the task with grounded conceptual spaces. We
evaluate the approach on multiple graph tasks using both lightweight and large
models, highlighting the trade-offs between accuracy and computational cost.
The results demonstrate consistent performance improvements, showing that
structured input context can rival or surpass more complex approaches. Our
findings underscore the value of structured context injection as an effective
and scalable strategy for graph understanding with LLMs.

</details>


### [56] [L-MARS -- Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search](https://arxiv.org/abs/2509.00761)
*Ziqi Wang,Boqin Yuan*

Main category: cs.AI

TL;DR: L-MARS是一个多智能体法律问答系统，通过协调推理和搜索来减少幻觉和不确定性，在LegalSearchQA基准测试中显著提升了事实准确性。


<details>
  <summary>Details</summary>
Motivation: 传统单次检索增强生成(RAG)在法律问答中存在幻觉和不确定性问题，需要更精确的法律检索和验证机制来应对高风险法律领域的应用需求。

Method: 将查询分解为子问题，在异构源(网络搜索、本地RAG、案例法)进行定向搜索，使用法官智能体验证充分性、管辖权和时效性，通过迭代推理-搜索-验证循环保持一致性。

Result: 在2025年200个最新法律选择题的LegalSearchQA基准测试中，L-MARS显著提高了事实准确性，减少了不确定性，获得了人类专家和LLM法官更高的偏好评分。

Conclusion: 多智能体推理与智能搜索相结合为在高风险领域部署LLM提供了可扩展且可复制的蓝图，能够实现精确的法律检索和审议。

Abstract: We present L-MARS (Legal Multi-Agent Workflow with Orchestrated Reasoning and
Agentic Search), a system that reduces hallucination and uncertainty in legal
question answering through coordinated multi-agent reasoning and retrieval.
Unlike single-pass retrieval-augmented generation (RAG), L-MARS decomposes
queries into subproblems, issues targeted searches across heterogeneous sources
(Serper web, local RAG, CourtListener case law), and employs a Judge Agent to
verify sufficiency, jurisdiction, and temporal validity before answer
synthesis. This iterative reasoning-search-verification loop maintains
coherence, filters noisy evidence, and grounds answers in authoritative law. We
evaluated L-MARS on LegalSearchQA, a new benchmark of 200 up-to-date multiple
choice legal questions in 2025. Results show that L-MARS substantially improves
factual accuracy, reduces uncertainty, and achieves higher preference scores
from both human experts and LLM-based judges. Our work demonstrates that
multi-agent reasoning with agentic search offers a scalable and reproducible
blueprint for deploying LLMs in high-stakes domains requiring precise legal
retrieval and deliberation.

</details>


### [57] [Aligning Reasoning LLMs for Materials Discovery with Physics-aware Rejection Sampling](https://arxiv.org/abs/2509.00768)
*Lee Hyun,Sohee Yoon,Jinwoo Park,Sue In Chae,Seongeon Park,Jooyeon Ahn,Yebin Jung,Youjung Chung,Hogeun Chang,Myeonginn Kang,Jina Kim,Ho-Gyeong Kim,Myeonghun Jeong*

Main category: cs.AI

TL;DR: 提出了Physics-aware Rejection Sampling (PaRS)方法，通过物理约束和数值接近度选择推理轨迹，提升AI材料发现中过程感知属性预测的准确性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: AI驱动的材料发现需要准确、校准且物理可接受的配方到属性预测器。现有训练管道使用二元正确性或学习偏好信号选择推理轨迹，不能很好地反映物理可接受性。

Method: 引入Physics-aware Rejection Sampling (PaRS)训练时轨迹选择方案，偏好符合基础物理且数值接近目标的轨迹，采用轻量级停止机制控制计算成本。使用大型教师模型合成的轨迹微调学生模型。

Result: 在匹配标记预算下，相比各种拒绝采样基线，该方法提高了准确性和校准度，降低了物理违规率，并减少了采样成本。

Conclusion: 适度的领域感知约束结合轨迹级选择，为过程感知属性预测和闭环材料设计提供了可靠、高效的大型推理模型的实用路径。

Abstract: AI-driven materials discovery that couples automated experimentation with
algorithmic decision-making requires process aware recipe to property
predictors that are accurate, calibrated, and physically admissible. We
approach this as a reasoning problem with large reasoning models (LRMs). To
instill reasoning capability into language models, we curate reasoning traces
from a teacher model to train a student model. However, most training pipelines
select reasoning traces using binary correctness or learned preference signals
that poorly reflect physical admissibility. We introduce Physics-aware
Rejection Sampling (PaRS), a training-time trace selection scheme that favors
traces consistent with fundamental physics and numerically close to targets,
with lightweight halting to control compute. We instantiate our framework with
a large student model fine-tuned on traces synthesized by a larger teacher
model, and evaluate under matched token budgets against various rejection
sampling baselines. Our method improves accuracy and calibration, reduces
physics-violation rates, and lowers sampling cost relative to baselines. These
results indicate that modest, domain-aware constraints combined with
trace-level selection provide a practical path toward reliable, efficient LRMs
for process-aware property prediction and closed-loop materials design.

</details>


### [58] [Sharpe Ratio Optimization in Markov Decision Processes](https://arxiv.org/abs/2509.00793)
*Shuai Ma,Guangwu Liu,Li Xia*

Main category: cs.AI

TL;DR: 本文提出了一种在马尔可夫决策过程中优化夏普比率的新方法，通过Dinkelbach变换将分数目标转换为均方方差优化问题，并开发了迭代算法来求解。


<details>
  <summary>Details</summary>
Motivation: 夏普比率是金融领域广泛使用的风险调整收益指标，但在MDP中优化夏普比率面临两个挑战：动态规划不适用于分数目标和风险度量。

Method: 使用Dinkelbach变换将夏普比率优化转换为均方方差优化，开发迭代算法求解M2V问题，并通过策略迭代过程更新风险敏感参数。

Result: 提出的算法能够单调递增地收敛到最优夏普比率，在平均和折扣MDP设置下都证明了收敛性，数值实验验证了方法的有效性。

Conclusion: 这是首个使用动态规划类算法解决MDP中夏普比率优化问题的方法，为解决其他分数目标MDP问题提供了新的思路。

Abstract: Sharpe ratio (also known as reward-to-variability ratio) is a widely-used
metric in finance, which measures the additional return at the cost of per unit
of increased risk (standard deviation of return). However, the optimization of
Sharpe ratio in Markov decision processes (MDPs) is challenging, because there
exist two difficulties hindering the application of dynamic programming. One is
that dynamic programming does not work for fractional objectives, and the other
is that dynamic programming is invalid for risk metrics. In this paper, we
study the Sharpe ratio optimization in infinite-horizon MDPs, considering both
the long-run average and discounted settings. We address the first challenge
with the Dinkelbachs transform, which converts the Sharpe ratio objective to a
mean-squared-variance (M2V) objective. It is shown that the M2V optimization
and the original Sharpe ratio optimization share the same optimal policy when
the risk-sensitive parameter is equal to the optimal Sharpe ratio. For the
second challenge, we develop an iterative algorithm to solve the M2V
optimization which is similar to a mean-variance optimization in MDPs. We
iteratively solve the M2V problem and obtain the associated Sharpe ratio that
is used to update the risk-sensitive parameter in the next iteration of M2V
problems. We show that such a sequence of Sharpe ratios derived is
monotonically increasing and converges to the optimal Sharpe ratio. For both
average and discounted MDP settings, we develop a policy iteration procedure
and prove its convergence to the optimum. Numerical experiments are conducted
for validation. To the best of our knowledge, our approach is the first that
solves the Sharpe ratio optimization in MDPs with dynamic programming type
algorithms. We believe that the proposed algorithm can shed light on solving
MDPs with other fractional objectives.

</details>


### [59] [Neuro-Symbolic Predictive Process Monitoring](https://arxiv.org/abs/2509.00834)
*Axel Mezini,Elena Umili,Ivan Donadello,Fabrizio Maria Maggi,Matteo Mancanelli,Fabio Patrizi*

Main category: cs.AI

TL;DR: 本文提出一种统一神经符号学习的预测性过程监控方法，通过在自回归序列预测中集成线性时态逻辑，提高了后缀预测的准确性和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在业务过程管理的后缀预测中，缺乏显式域知识集成，导致生成的序列经常违背基本逻辑约束。

Method: 提出了一种可微分的逻辑损失函数，利用LTLf语义的软近似和Gumbel-Softmax技巧，将时态逻辑约束集成到自回归序列预测器的训练过程中。

Result: 在三个真实世界数据集上的实验评估显示，该方法提高了后缀预测的准确性和时态约束的遵循程度。

Conclusion: 该框架不仅适用于BPM预测性过程监控，还可扩展到任何符号序列生成任务，为神经符号AI的发展做出了贡献。

Abstract: This paper addresses the problem of suffix prediction in Business Process
Management (BPM) by proposing a Neuro-Symbolic Predictive Process Monitoring
(PPM) approach that integrates data-driven learning with temporal logic-based
prior knowledge. While recent approaches leverage deep learning models for
suffix prediction, they often fail to satisfy even basic logical constraints
due to the absence of explicit integration of domain knowledge during training.
We propose a novel method to incorporate Linear Temporal Logic over finite
traces (LTLf) into the training process of autoregressive sequence predictors.
Our approach introduces a differentiable logical loss function, defined using a
soft approximation of LTLf semantics and the Gumbel-Softmax trick, which can be
combined with standard predictive losses. This ensures the model learns to
generate suffixes that are both accurate and logically consistent. Experimental
evaluation on three real-world datasets shows that our method improves suffix
prediction accuracy and compliance with temporal constraints. We also introduce
two variants of the logic loss (local and global) and demonstrate their
effectiveness under noisy and realistic settings. While developed in the
context of BPM, our framework is applicable to any symbolic sequence generation
task and contributes toward advancing Neuro-Symbolic AI.

</details>


### [60] [ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop Insulin Adoption in Type 1 Diabetes Care](https://arxiv.org/abs/2509.00891)
*Zonghai Yao,Talha Chafekar,Junda Wang,Shuo Han,Feiyun Ouyang,Junhui Qian,Lingxi Li,Hong Yu*

Main category: cs.AI

TL;DR: ChatCLIDS是首个评估LLM驱动健康行为改变说服对话的基准测试，通过专家验证的虚拟患者和多样化说服策略，发现当前LLM在克服抵抗和社交压力方面存在显著局限


<details>
  <summary>Details</summary>
Motivation: 闭环胰岛素输送系统(CLIDS)在1型糖尿病中的实际采用率低，主要不是技术问题，而是由行为、心理和社会障碍驱动，需要评估LLM在健康行为改变中的说服能力

Method: 创建包含专家验证虚拟患者的框架，每个患者具有临床基础的异质性特征和现实采用障碍，模拟与配备多样化循证说服策略的护士代理进行多轮交互，支持纵向咨询和对抗性社交影响场景

Result: 研究发现虽然更大、更具反思性的LLM能够随时间调整策略，但所有模型都难以克服抵抗，特别是在现实社交压力下

Conclusion: 结果凸显了当前LLM在行为改变方面的关键局限性，为推进医疗保健及其他领域可信赖的说服性AI提供了高保真、可扩展的测试平台

Abstract: Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1
diabetes remains low, driven not by technical failure, but by diverse
behavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the
first benchmark to rigorously evaluate LLM-driven persuasive dialogue for
health behavior change. Our framework features a library of expert-validated
virtual patients, each with clinically grounded, heterogeneous profiles and
realistic adoption barriers, and simulates multi-turn interactions with nurse
agents equipped with a diverse set of evidence-based persuasive strategies.
ChatCLIDS uniquely supports longitudinal counseling and adversarial social
influence scenarios, enabling robust, multi-dimensional evaluation. Our
findings reveal that while larger and more reflective LLMs adapt strategies
over time, all models struggle to overcome resistance, especially under
realistic social pressure. These results highlight critical limitations of
current LLMs for behavior change, and offer a high-fidelity, scalable testbed
for advancing trustworthy persuasive AI in healthcare and beyond.

</details>


### [61] [Robust Deep Monte Carlo Counterfactual Regret Minimization: Addressing Theoretical Risks in Neural Fictitious Self-Play](https://arxiv.org/abs/2509.00923)
*Zakaria El Jaafari*

Main category: cs.AI

TL;DR: 该论文分析了神经MCCFR算法在不同规模博弈中的组件有效性差异，提出了自适应框架Robust Deep MCCFR，通过目标网络、均匀探索混合等技术在Kuhn和Leduc扑克上分别实现了60%和23.5%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决MCCFR与深度神经网络结合时在不同规模博弈中出现的规模依赖性挑战，包括非平稳目标分布偏移、动作支持崩溃、方差爆炸和热启动偏差等问题。

Method: 提出Robust Deep MCCFR框架，包含延迟更新的目标网络、均匀探索混合、方差感知训练目标和全面的诊断监控，通过系统消融研究分析组件有效性。

Result: 在Kuhn扑克上达到0.0628的最终可利用性（相比经典框架0.156提升60%），在Leduc扑克上达到0.2386的可利用性（相比0.3703提升23.5%）。

Conclusion: 神经MCCFR组件的有效性具有规模依赖性，需要根据博弈规模选择性部署组件，而非全面使用所有缓解策略，为更大规模博弈的部署提供了实用指南。

Abstract: Monte Carlo Counterfactual Regret Minimization (MCCFR) has emerged as a
cornerstone algorithm for solving extensive-form games, but its integration
with deep neural networks introduces scale-dependent challenges that manifest
differently across game complexities. This paper presents a comprehensive
analysis of how neural MCCFR component effectiveness varies with game scale and
proposes an adaptive framework for selective component deployment. We identify
that theoretical risks such as nonstationary target distribution shifts, action
support collapse, variance explosion, and warm-starting bias have
scale-dependent manifestation patterns, requiring different mitigation
strategies for small versus large games. Our proposed Robust Deep MCCFR
framework incorporates target networks with delayed updates, uniform
exploration mixing, variance-aware training objectives, and comprehensive
diagnostic monitoring. Through systematic ablation studies on Kuhn and Leduc
Poker, we demonstrate scale-dependent component effectiveness and identify
critical component interactions. The best configuration achieves final
exploitability of 0.0628 on Kuhn Poker, representing a 60% improvement over the
classical framework (0.156). On the more complex Leduc Poker domain, selective
component usage achieves exploitability of 0.2386, a 23.5% improvement over the
classical framework (0.3703) and highlighting the importance of careful
component selection over comprehensive mitigation. Our contributions include:
(1) a formal theoretical analysis of risks in neural MCCFR, (2) a principled
mitigation framework with convergence guarantees, (3) comprehensive multi-scale
experimental validation revealing scale-dependent component interactions, and
(4) practical guidelines for deployment in larger games.

</details>


### [62] [SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement Fine-Tuning of LLMs](https://arxiv.org/abs/2509.00930)
*Yanxiao Zhao,Yaqian Li,Zihao Bo,Rinyoichi Takezoe,Haojia Hui,Mo Guang,Lei Ren,Xiaolin Qin,Kaiwen Long*

Main category: cs.AI

TL;DR: SATQuest是一个系统化的验证工具，通过从CNF实例生成多样化的可满足性逻辑推理问题，用于评估和增强大语言模型的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏可控性和多维度分析能力，无法系统评估大语言模型的逻辑推理能力，需要开发更精细的分析工具。

Method: 基于SAT的问题生成方法，通过三个正交维度（实例规模、问题类型、问题格式）构建逻辑推理问题，使用PySAT进行客观答案验证。

Result: 评估发现大语言模型在逻辑推理方面存在显著局限性，特别是在超越熟悉数学格式的泛化能力上。强化微调能显著提升目标任务性能。

Conclusion: SATQuest展示了作为基础工具的潜力，是推进大语言模型逻辑推理能力发展的有价值起点，但跨格式适应仍存在挑战。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated remarkable
general reasoning capabilities. However, systematically evaluating and
enhancing these reasoning capabilities is challenging due to the lack of
controllable and scalable tools for fine-grained analysis. Existing benchmarks
and datasets often lack the necessary variable control for multi-dimensional,
systematic analysis and training, or have narrow problem types and formats. To
address these limitations, we introduce SATQuest, a systematic verifier
designed to evaluate and enhance logical reasoning in LLMs by generating
diverse, Satisfiability-based logical reasoning problems directly from
Conjunctive Normal Form (CNF) instances. SATQuest structures these problems
along three orthogonal dimensions: instance scale, problem type, and question
format, employing randomized, SAT-based problem generation and objective answer
verification via PySAT. This design mitigates memorization issues, allows for
nuanced insights into reasoning performance, and enables effective
reinforcement fine-tuning. Our extensive evaluation of various LLMs using
SATQuest identified significant limitations in their logical reasoning,
particularly in generalizing beyond familiar mathematical formats. Furthermore,
we show that reinforcement fine-tuning with SATQuest rewards substantially
improves targeted task performance and generalizes to more complex instances,
while highlighting remaining challenges in cross-format adaptation. Through
these demonstrations, we showcase SATQuest's potential as a foundational tool
and a valuable starting point for advancing LLM logical reasoning.

</details>


### [63] [UrbanInsight: A Distributed Edge Computing Framework with LLM-Powered Data Filtering for Smart City Digital Twins](https://arxiv.org/abs/2509.00936)
*Kishor Datta Gupta,Md Manjurul Ahsan,Mohd Ariful Haque,Roy George,Azmine Toushik Wasi*

Main category: cs.AI

TL;DR: 提出一个结合物理信息机器学习、多模态数据融合、知识图谱和LLM自适应规则的智能城市数字孪生框架


<details>
  <summary>Details</summary>
Motivation: 城市产生大量传感器数据但现有系统难以处理规模、延迟和碎片化洞察的问题，需要更智能的数据处理和分析方法

Method: 融合物理信息机器学习（确保预测符合物理约束）、知识图谱（整合异构数据为可查询结构）、LLM生成自适应规则（实时边缘决策）

Result: 构建了超越被动监控的数字孪生系统基础，能够提供可操作的洞察

Conclusion: 该方法通过结合物理推理、语义数据融合和自适应规则生成，为创建响应式、可信赖和可持续的智能基础设施开辟了新可能性

Abstract: Cities today generate enormous streams of data from sensors, cameras, and
connected infrastructure. While this information offers unprecedented
opportunities to improve urban life, most existing systems struggle with scale,
latency, and fragmented insights. This work introduces a framework that blends
physics-informed machine learning, multimodal data fusion, and knowledge graph
representation with adaptive, rule-based intelligence powered by large language
models (LLMs). Physics-informed methods ground learning in real-world
constraints, ensuring predictions remain meaningful and consistent with
physical dynamics. Knowledge graphs act as the semantic backbone, integrating
heterogeneous sensor data into a connected, queryable structure. At the edge,
LLMs generate context-aware rules that adapt filtering and decision-making in
real time, enabling efficient operation even under constrained resources.
Together, these elements form a foundation for digital twin systems that go
beyond passive monitoring to provide actionable insights. By uniting
physics-based reasoning, semantic data fusion, and adaptive rule generation,
this approach opens new possibilities for creating responsive, trustworthy, and
sustainable smart infrastructures.

</details>


### [64] [A Hybrid Ai Framework For Strategic Patent Portfolio Pruning: Integrating Learning To-Rank And Market Need Analysis For Technology Transfer Optimization](https://arxiv.org/abs/2509.00958)
*Manish Verma,Vivek Sharma,Vishal Singh*

Main category: cs.AI

TL;DR: 提出了一种多阶段混合智能框架，通过结合学习排序模型和基于代理的Need-Seed系统，自动识别高价值专利资产进行技术转移。


<details>
  <summary>Details</summary>
Motivation: 现有专利估值方法依赖回顾性指标或耗时的手动分析，需要更自动化、深入的分析方法来识别高价值专利进行技术转移。

Method: 结合学习排序(LTR)模型评估30多个法律和商业参数，使用Need Agent通过NLP挖掘市场需求，Seed Agent通过微调LLM分析专利技术能力，构建核心本体框架匹配专利与市场需求。

Result: 开发了动态参数加权系统和人在回路验证协议，确保系统的适应性和实际可信度。

Conclusion: 该框架为专利投资组合修剪提供了自动化、深度的分析解决方案，能够战略性地识别高价值专利资产。

Abstract: This paper introduces a novel, multi stage hybrid intelligence framework for
pruning patent portfolios to identify high value assets for technology
transfer. Current patent valuation methods often rely on retrospective
indicators or manual, time intensive analysis. Our framework automates and
deepens this process by combining a Learning to Rank (LTR) model, which
evaluates patents against over 30 legal and commercial parameters, with a
unique "Need-Seed" agent-based system. The "Need Agent" uses Natural Language
Processing (NLP) to mine unstructured market and industry data, identifying
explicit technological needs. Concurrently, the "Seed Agent" employs fine tuned
Large Language Models (LLMs) to analyze patent claims and map their
technological capabilities. The system generates a "Core Ontology Framework"
that matches high potential patents (Seeds) to documented market demands
(Needs), providing a strategic rationale for divestment decisions. We detail
the architecture, including a dynamic parameter weighting system and a crucial
Human in the-Loop (HITL) validation protocol, to ensure both adaptability and
real-world credibility.

</details>


### [65] [Ultra Strong Machine Learning: Teaching Humans Active Learning Strategies via Automated AI Explanations](https://arxiv.org/abs/2509.00961)
*Lun Ai,Johannes Langer,Ute Schmid,Stephen Muggleton*

Main category: cs.AI

TL;DR: LENS是一个神经符号方法，结合符号程序合成和大型语言模型，用于自动生成机器学习逻辑程序的自然语言解释，取代了传统的手工模板方法


<details>
  <summary>Details</summary>
Motivation: 解决超强机器学习(USML)系统中手工解释模板的可扩展性限制，实现自动化的知识解释和人类教学

Method: 结合符号程序合成和大型语言模型(LLMs)，通过神经摘要生成逻辑程序的自然语言解释

Result: LENS生成的解释优于直接LLM提示和手工模板，但在人类学习实验中未显示显著性能提升，可能因为LLM响应过于全面反而让用户不知所措

Conclusion: 为构建有效的USML系统支持人类学习提供了坚实基础，但需要进一步优化解释生成策略以适应不同复杂度的问题

Abstract: Ultra Strong Machine Learning (USML) refers to symbolic learning systems that
not only improve their own performance but can also teach their acquired
knowledge to quantifiably improve human performance. In this work, we present
LENS (Logic Programming Explanation via Neural Summarisation), a neuro-symbolic
method that combines symbolic program synthesis with large language models
(LLMs) to automate the explanation of machine-learned logic programs in natural
language. LENS addresses a key limitation of prior USML approaches by replacing
hand-crafted explanation templates with scalable automated generation. Through
systematic evaluation using multiple LLM judges and human validation, we
demonstrate that LENS generates superior explanations compared to direct LLM
prompting and hand-crafted templates. To investigate whether LENS can teach
transferable active learning strategies, we carried out a human learning
experiment across three related domains. Our results show no significant human
performance improvements, suggesting that comprehensive LLM responses may
overwhelm users for simpler problems rather than providing learning support.
Our work provides a solid foundation for building effective USML systems to
support human learning. The source code is available on:
https://github.com/lun-ai/LENS.git.

</details>


### [66] [CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks with LLMs](https://arxiv.org/abs/2509.00971)
*Jay Vaghasiya,Omkar Ghugarkar,Vishvesh Bhat,Vipul Dholaria,Julian McAuley*

Main category: cs.AI

TL;DR: CoreThink是一个基于通用符号推理方法的新型推理层，在工具调用、代码生成和规划等任务上实现SOTA性能，无需微调或训练成本即可提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如测试时扩展、监督微调、强化学习）在LLM性能提升上会出现收益递减，需要开发新的推理技术来突破性能瓶颈。

Method: 采用通用符号推理（General Symbolics）方法，构建CoreThink通用符号推理器（GSR），专注于工具调用、代码生成和规划三个关键用例。

Result: 在7个基准测试中表现优异：Livecodebench v6达到66.66%、Instruction-Following Evals达到89%、ARC-AGI-2达到24.4%，基于该技术的智能编码IDE在SWE-Bench Lite上达到62.3%的准确率。

Conclusion: CoreThink推理层能够在不产生负面影响的情况下纯提升模型推理性能，为推理密集型应用提供了新的解决方案，现有方法已接近性能极限，需要新的推理范式。

Abstract: We introduce CoreThink, a state-of-the-art Reasoning Layer built upon a novel
reasoning method called General Symbolics. This approach diverges from
reasoning paradigms such as test-time scaling, Supervised Fine-Tuning (SFT),
and Reinforcement Learning with Verifiable Rewards (RLVR). CoreThink General
Symbolic Reasoner (GSR) is specifically structured around three key use cases:
tool-calling, code generation, and planning, demonstrating exemplary
performance across a total of seven benchmarks in their respective areas.
Notably, we are achieving SOTA scores of 66.66\% on Livecodebench v6, 89\% on
Instruction-Following Evals, and 24.4\% on ARC-AGI-2. We also present an
agentic coding IDE, developed using the principles of General Symbolics, which
achieves a state-of-the-art accuracy of 62.3\% on \texttt{SWE-Bench Lite}. We
are able to achieve these improvements without any finetuning or training
costs. Our Reasoning Layer is designed to provide a pure performance uplift,
ensuring that a model's accuracy on reasoning tasks is never negatively
impacted. We argue that incumbent methods will eventually lead to diminishing
returns in LLM performance, necessitating the development of new reasoning
techniques. This technical report details our approach at a high level and the
availability of the CoreThink models for reasoning-intensive use cases.

</details>


### [67] [Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning](https://arxiv.org/abs/2509.00975)
*Zifeng Ding,Shenyang Huang,Zeyu Cao,Emma Kondrup,Zachary Yang,Xingyue Huang,Yuan Sui,Zhangdie Yuan,Yuqicheng Zhu,Xianglong Hu,Yuan He,Farimah Poursafaei,Michael Bronstein,Andreas Vlachos*

Main category: cs.AI

TL;DR: ReaL-TG是一个强化学习框架，通过微调大语言模型在时序图上进行可解释的链接预测，使用基于结果的奖励机制来探索推理策略并生成解释，在排名指标上优于更大的前沿LLM。


<details>
  <summary>Details</summary>
Motivation: 传统时序图神经网络虽然性能强但缺乏可解释性，且无法应用于未见过的图而无需重新训练。现有LLM在图推理方面的研究大多局限于静态图或小型合成时序图，且缺乏对LLM生成推理轨迹质量的评估。

Method: 提出ReaL-TG强化学习框架，使用基于结果的奖励机制微调LLM，鼓励模型从图结构中自主探索推理策略，并生成直接证明预测的解释。同时提出结合排名指标和LLM-as-a-Judge系统的新评估协议。

Result: 通过微调Qwen3-4B得到的ReaL-TG-4B在排名指标上优于包括GPT-5 mini在内的更大前沿LLM，同时生成高质量的解释，得到LLM评判和人工评估的确认。

Conclusion: ReaL-TG框架成功实现了在真实世界时序图上进行可解释链接预测的目标，证明了强化学习微调LLM在时序图推理任务中的有效性，并为评估LLM生成的推理轨迹提供了新方法。

Abstract: Forecasting future links is a central task in temporal graph (TG) reasoning,
requiring models to leverage historical interactions to predict upcoming ones.
Traditional neural approaches, such as temporal graph neural networks, achieve
strong performance but lack explainability and cannot be applied to unseen
graphs without retraining. Recent studies have begun to explore using large
language models (LLMs) for graph reasoning, but most of them are constrained to
static graphs or small synthetic TGs and lack the evaluation of the quality of
reasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced
Learning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that
fine-tunes LLMs to perform explainable link forecasting on real-world TGs.
ReaL-TG uses outcome-based reward to encourage models to self-explore reasoning
strategies from graph structure and to produce explanations that directly
justify their predictions. To enable evaluation on LLM-generated reasoning
traces, we propose a new evaluation protocol combining ranking metrics with an
LLM-as-a-Judge system that assesses both the quality of reasoning and the
impact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning
Qwen3-4B under our framework, show that it outperforms much larger frontier
LLMs, including GPT-5 mini, on ranking metrics, while producing high-quality
explanations confirmed by both the LLM judge and human evaluation.

</details>


### [68] [Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation](https://arxiv.org/abs/2509.00987)
*Adib Bazgir,Amir Habibdoust,Yuwen Zhang,Xing Song*

Main category: cs.AI

TL;DR: 本文综述了因果多智能体大语言模型领域，探讨了如何利用多智能体系统解决LLM在因果推理、发现和估计方面的局限性，包括架构设计、评估方法和应用领域。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂因果推理方面存在局限性，如幻觉、伪相关性和处理个性化因果关系的困难，多智能体系统被提出作为解决这些问题的有力范式。

Method: 通过多智能体协作架构，包括流水线处理、辩论框架、模拟环境和迭代优化循环等模式，来处理因果推理、反事实分析、因果发现和因果效应估计等任务。

Result: 因果多智能体LLM系统在科学发现、医疗健康、事实核查和个性化系统等多个应用领域展现出潜力，但仍面临持续挑战。

Conclusion: 该领域处于快速发展阶段，需要进一步研究解决现有挑战，探索多智能体系统在因果推理中的协同潜力，为未来发展指明方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various reasoning and generation tasks. However, their proficiency in complex
causal reasoning, discovery, and estimation remains an area of active
development, often hindered by issues like hallucination, reliance on spurious
correlations, and difficulties in handling nuanced, domain-specific, or
personalized causal relationships. Multi-agent systems, leveraging the
collaborative or specialized abilities of multiple LLM-based agents, are
emerging as a powerful paradigm to address these limitations. This review paper
explores the burgeoning field of causal multi-agent LLMs. We examine how these
systems are designed to tackle different facets of causality, including causal
reasoning and counterfactual analysis, causal discovery from data, and the
estimation of causal effects. We delve into the diverse architectural patterns
and interaction protocols employed, from pipeline-based processing and debate
frameworks to simulation environments and iterative refinement loops.
Furthermore, we discuss the evaluation methodologies, benchmarks, and diverse
application domains where causal multi-agent LLMs are making an impact,
including scientific discovery, healthcare, fact-checking, and personalized
systems. Finally, we highlight the persistent challenges, open research
questions, and promising future directions in this synergistic field, aiming to
provide a comprehensive overview of its current state and potential trajectory.

</details>


### [69] [Supporting Our AI Overlords: Redesigning Data Systems to be Agent-First](https://arxiv.org/abs/2509.00997)
*Shu Liu,Soujanya Ponnapalli,Shreya Shankar,Sepanta Zeighami,Alan Zhu,Shubham Agarwal,Ruiqi Chen,Samion Suwito,Shuo Yuan,Ion Stoica,Matei Zaharia,Alvin Cheung,Natacha Crooks,Joseph E. Gonzalez,Aditya G. Parameswaran*

Main category: cs.AI

TL;DR: LLM代理将成为未来数据系统的主要工作负载，其探索性工作模式（代理推测）对现有数据系统构成挑战，需要构建支持代理优先的新架构


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在数据处理中扮演越来越重要的角色，其高吞吐量的探索性工作模式对现有数据系统提出了新的挑战，需要专门优化来支持这种代理推测工作负载

Method: 通过分析代理推测的四个关键特征（规模性、异构性、冗余性和可引导性），提出新的研究方向，包括新的查询接口、查询处理技术和代理记忆存储

Result: 识别了LLM代理工作负载对数据系统的特殊需求，为构建代理优先的数据系统架构提供了理论基础和研究方向

Conclusion: 数据系统需要从根本上重新设计以适应LLM代理工作负载，利用代理推测的特性可以开发出更高效、专门化的代理优先数据系统架构

Abstract: Large Language Model (LLM) agents, acting on their users' behalf to
manipulate and analyze data, are likely to become the dominant workload for
data systems in the future. When working with data, agents employ a
high-throughput process of exploration and solution formulation for the given
task, one we call agentic speculation. The sheer volume and inefficiencies of
agentic speculation can pose challenges for present-day data systems. We argue
that data systems need to adapt to more natively support agentic workloads. We
take advantage of the characteristics of agentic speculation that we identify,
i.e., scale, heterogeneity, redundancy, and steerability - to outline a number
of new research opportunities for a new agent-first data systems architecture,
ranging from new query interfaces, to new query processing techniques, to new
agentic memory stores.

</details>


### [70] [Analysis of Error Sources in LLM-based Hypothesis Search for Few-Shot Rule Induction](https://arxiv.org/abs/2509.01016)
*Aishni Parab,Hongjing Lu,Ying Nian Wu,Sumit Gulwani*

Main category: cs.AI

TL;DR: LLM假设搜索框架在少样本规则归纳任务中表现接近人类水平，而直接程序生成方法表现较差，揭示了程序归纳方法的关键瓶颈


<details>
  <summary>Details</summary>
Motivation: 比较LLM假设搜索与直接程序生成方法在少样本规则归纳任务中的性能差异，探索建模人类归纳推理的潜力

Method: 采用LLM假设搜索框架与直接程序生成方法进行对比实验，通过错误分析识别关键瓶颈

Result: 假设搜索方法达到接近人类水平的性能，直接程序生成方法明显落后，识别出假设生成阶段的关键瓶颈

Conclusion: LLM假设搜索在建模归纳推理方面具有潜力，但需要解决假设生成效率问题以构建更高效的系统

Abstract: Inductive reasoning enables humans to infer abstract rules from limited
examples and apply them to novel situations. In this work, we compare an
LLM-based hypothesis search framework with direct program generation approaches
on few-shot rule induction tasks. Our findings show that hypothesis search
achieves performance comparable to humans, while direct program generation
falls notably behind. An error analysis reveals key bottlenecks in hypothesis
generation and suggests directions for advancing program induction methods.
Overall, this paper underscores the potential of LLM-based hypothesis search
for modeling inductive reasoning and the challenges in building more efficient
systems.

</details>


### [71] [Quantum-like Coherence Derived from the Interaction between Chemical Reaction and Its Environment](https://arxiv.org/abs/2509.01021)
*Yukio-Pegio Gunji,Andrew Adamatzky,Panagiotis Mougkogiannis,Andrei Khrenikov*

Main category: cs.AI

TL;DR: 该论文通过对比人工智能与自然智能的计算过程，提出了封闭计算与开放计算的概念，并在化学反应中实现了开放计算。通过将计算过程与环境融合，创建了能够调整波动的系统，展示了自组织临界现象和量子逻辑特性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示人工智能与自然智能之间的计算差异，探索如何在化学反应中实现类似自然智能的开放计算过程，以理解生物系统中信号传输和节律控制的机制。

Method: 方法包括：定义封闭计算与开放计算；在化学反应中实现开放计算；将计算过程与环境融合；建立Token计算（关注分子个体行为）和Type计算（关注规范行为）的交互模型；分析自组织临界现象和量子逻辑特性。

Result: 研究结果显示：Token计算表现出自组织临界现象；Type计算展现出量子逻辑特性；两者交互实现了波动的招募，产生了不同希尔伯特空间之间的量子相干性；形成了尖峰波以实现信号传输。

Conclusion: 结论表明这种量子类相干现象可能是控制尖峰波和生化节律的酶源，为理解生物系统中的计算过程和信号传输机制提供了新的理论框架。

Abstract: By uncovering the contrast between Artificial Intelligence and Natural-born
Intelligence as a computational process, we define closed computing and open
computing, and implement open computing within chemical reactions. This
involves forming a mixture and invalidation of the computational process and
the execution environment, which are logically distinct, and coalescing both to
create a system that adjusts fluctuations. We model chemical reactions by
considering the computation as the chemical reaction and the execution
environment as the degree of aggregation of molecules that interact with the
reactive environment. This results in a chemical reaction that progresses while
repeatedly clustering and de-clustering, where concentration no longer holds
significant meaning. Open computing is segmented into Token computing, which
focuses on the individual behavior of chemical molecules, and Type computing,
which focuses on normative behavior. Ultimately, both are constructed as an
interplay between the two. In this system, Token computing demonstrates
self-organizing critical phenomena, while Type computing exhibits quantum
logic. Through their interplay, the recruitment of fluctuations is realized,
giving rise to interactions between quantum logical subspaces corresponding to
quantum coherence across different Hilbert spaces. As a result, spike waves are
formed, enabling signal transmission. This occurrence may be termed
quantum-like coherence, implying the source of enzymes responsible for
controlling spike waves and biochemical rhythms.

</details>


### [72] [Symbolic Planning and Multi-Agent Path Finding in Extremely Dense Environments with Movable Obstacles](https://arxiv.org/abs/2509.01022)
*Bo Fu,Zhe Chen,Rahul Chandan,Alex Barbosa,Michael Caldara,Joey Durham,Federico Pecora*

Main category: cs.AI

TL;DR: 提出坐仓库管理中的块重新排列问题(BRaP)，形式化为图搜索问题，提出4种搜索算法并在80x80网格中高效解决深层块重排问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型坐仓管理中存储块在密集网格中的重新排列挑战，这是一个复杂的运作规划问题。

Method: 基于滑动拼图问题的直觉，提出五种搜索基于的解决算法：联合配置空间搜索、经典规划、多代理路径找到和专家启发式算法。

Result: 虽然搜索空间大小与块数成指数关系，但方法在80x80网格中能高效为深层块创建重新排列计划。

Conclusion: 该研究为坐仓管理中的块重新排列问题提供了高效的解决方案，展示了在大规模网格中的可扩展性。

Abstract: We introduce the Block Rearrangement Problem (BRaP), a challenging component
of large warehouse management which involves rearranging storage blocks within
dense grids to achieve a target state. We formally define the BRaP as a graph
search problem. Building on intuitions from sliding puzzle problems, we propose
five search-based solution algorithms, leveraging joint configuration space
search, classical planning, multi-agent pathfinding, and expert heuristics. We
evaluate the five approaches empirically for plan quality and scalability.
Despite the exponential relation between search space size and block number,
our methods demonstrate efficiency in creating rearrangement plans for deeply
buried blocks in up to 80x80 grids.

</details>


### [73] [FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games](https://arxiv.org/abs/2509.01052)
*Jaewoo Ahn,Junseo Kim,Heeseung Yun,Jaehyeon Son,Dongmin Park,Jaewoong Cho,Gunhee Kim*

Main category: cs.AI

TL;DR: FlashAdventure是一个包含34个Flash冒险游戏的基准测试，用于评估GUI代理完成完整故事线的能力，并提出了COAST框架和CUA-as-a-Judge自动评估器来解决观察-行为差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有游戏基准测试缺乏多样性，很少评估代理完成整个故事线的能力，冒险游戏由于复杂的叙事驱动交互而带来额外挑战。

Method: 提出了FlashAdventure基准测试、CUA-as-a-Judge自动游戏评估器，以及COAST代理框架，该框架利用长期线索记忆来更好地规划和解决顺序任务。

Result: 实验显示当前GUI代理在完整故事弧上表现困难，而COAST通过弥合观察-行为差距提高了里程碑完成率，但最佳代理与人类表现仍存在明显差距。

Conclusion: 虽然COAST框架在解决观察-行为差距方面取得进展，但代理与人类性能之间的显著差异表明需要继续研究来缩小这一差距。

Abstract: GUI agents powered by LLMs show promise in interacting with diverse digital
environments. Among these, video games offer a valuable testbed due to their
varied interfaces, with adventure games posing additional challenges through
complex, narrative-driven interactions. Existing game benchmarks, however, lack
diversity and rarely evaluate agents on completing entire storylines. To
address this, we introduce FlashAdventure, a benchmark of 34 Flash-based
adventure games designed to test full story arc completion and tackle the
observation-behavior gap: the challenge of remembering and acting on earlier
gameplay information. We also propose CUA-as-a-Judge, an automated gameplay
evaluator, and COAST, an agentic framework leveraging long-term clue memory to
better plan and solve sequential tasks. Experiments show current GUI agents
struggle with full story arcs, while COAST improves milestone completion by
bridging the observation-behavior gap. Nonetheless, a marked discrepancy
between humans and best-performing agents warrants continued research efforts
to narrow this divide.

</details>


### [74] [VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use](https://arxiv.org/abs/2509.01055)
*Dongfu Jiang,Yi Lu,Zhuofeng Li,Zhiheng Lyu,Ping Nie,Haozhe Wang,Alex Su,Hui Chen,Kai Zou,Chao Du,Tianyu Pang,Wenhu Chen*

Main category: cs.AI

TL;DR: VerlTool是一个统一的模块化框架，解决了现有工具增强强化学习系统的碎片化、同步执行瓶颈和可扩展性限制问题，通过标准化API、异步执行和跨6个领域的综合评估，实现了接近2倍的速度提升和竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有的工具增强强化学习方法存在代码库碎片化、同步执行瓶颈和跨领域扩展性有限的问题，阻碍了社区采用和算法创新。

Method: 设计了VerlTool框架，包含四个关键贡献：上游与VeRL对齐确保兼容性、通过标准化API统一工具管理、异步rollout执行消除同步瓶颈、在6个ARLT领域进行综合评估。

Result: 实现了接近2倍的加速，在数学推理、知识问答、SQL生成、视觉推理、网络搜索和软件工程任务上取得了与专用系统相当的竞争性性能。

Conclusion: VerlTool提供了一个统一的训练基础设施和模块化插件架构，显著降低了开发开销，为工具增强的强化学习研究提供了可扩展的基础。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated
success in enhancing LLM reasoning capabilities, but remains limited to
single-turn interactions without tool integration. While recent Agentic
Reinforcement Learning with Tool use (ARLT) approaches have emerged to address
multi-turn tool interactions, existing works develop task-specific codebases
that suffer from fragmentation, synchronous execution bottlenecks, and limited
extensibility across domains. These inefficiencies hinder broader community
adoption and algorithmic innovation. We introduce VerlTool, a unified and
modular framework that addresses these limitations through systematic design
principles. VerlTool provides four key contributions: (1) upstream alignment
with VeRL ensuring compatibility and simplified maintenance, (2) unified tool
management via standardized APIs supporting diverse modalities including code
execution, search, SQL databases, and vision processing, (3) asynchronous
rollout execution achieving near 2$\times$ speedup by eliminating
synchronization bottlenecks, and (4) comprehensive evaluation demonstrating
competitive performance across 6 ARLT domains. Our framework formalizes ARLT as
multi-turn trajectories with multi-modal observation tokens (text/image/video),
extending beyond single-turn RLVR paradigms. We train and evaluate models on
mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web
search, and software engineering tasks, achieving results comparable to
specialized systems while providing unified training infrastructure. The
modular plugin architecture enables rapid tool integration requiring only
lightweight Python definitions, significantly reducing development overhead and
providing a scalable foundation for tool-augmented RL research. Our code is
open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.

</details>


### [75] [Robix: A Unified Model for Robot Interaction, Reasoning and Planning](https://arxiv.org/abs/2509.01106)
*Huang Fang,Mengxi Zhang,Heng Dong,Wei Li,Zixuan Wang,Qifeng Zhang,Xueyun Tian,Yucheng Hu,Hang Li*

Main category: cs.AI

TL;DR: Robix是一个统一的视觉语言架构模型，集成了机器人推理、任务规划和自然语言交互，作为分层机器人系统的高层认知层，能够生成原子命令和语言响应，支持复杂指令跟随、长时程任务规划和自然人类交互。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人系统中推理、规划和交互的分离问题，开发一个端到端的统一框架，使机器人能够更好地理解复杂指令、执行长时程任务并与人类自然互动。

Method: 采用思维链推理和三阶段训练策略：1)持续预训练增强基础体现推理能力；2)监督微调将人机交互和任务规划建模为统一推理-动作序列；3)强化学习提高推理-动作一致性和长时程任务连贯性。

Result: 在交互式任务执行中优于开源和商业基线（如GPT-4o和Gemini 2.5 Pro），在多样化指令类型（开放式、多阶段、约束、无效和中断）和用户参与任务（如餐桌清理、购物、饮食过滤）上表现出强泛化能力。

Conclusion: Robix通过统一的视觉语言架构成功整合了机器人推理、规划和交互，证明了其在复杂任务执行和人类交互方面的有效性，为构建更智能的机器人系统提供了新方向。

Abstract: We introduce Robix, a unified model that integrates robot reasoning, task
planning, and natural language interaction within a single vision-language
architecture. Acting as the high-level cognitive layer in a hierarchical robot
system, Robix dynamically generates atomic commands for the low-level
controller and verbal responses for human interaction, enabling robots to
follow complex instructions, plan long-horizon tasks, and interact naturally
with human within an end-to-end framework. Robix further introduces novel
capabilities such as proactive dialogue, real-time interruption handling, and
context-aware commonsense reasoning during task execution. At its core, Robix
leverages chain-of-thought reasoning and adopts a three-stage training
strategy: (1) continued pretraining to enhance foundational embodied reasoning
abilities including 3D spatial understanding, visual grounding, and
task-centric reasoning; (2) supervised finetuning to model human-robot
interaction and task planning as a unified reasoning-action sequence; and (3)
reinforcement learning to improve reasoning-action consistency and long-horizon
task coherence. Extensive experiments demonstrate that Robix outperforms both
open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in
interactive task execution, demonstrating strong generalization across diverse
instruction types (e.g., open-ended, multi-stage, constrained, invalid, and
interrupted) and various user-involved tasks such as table bussing, grocery
shopping, and dietary filtering.

</details>


### [76] [Heads or Tails: A Simple Example of Causal Abstractive Simulation](https://arxiv.org/abs/2509.01136)
*Gabriel Simmons*

Main category: cs.AI

TL;DR: 这篇论文通过因果抽象模拟的方法，形式化了语言模型模拟公平投确的简单案例，并提供了证明语言模型能够模拟其他系统的形式化基础。


<details>
  <summary>Details</summary>
Motivation: 为语言模型模拟领域的实践者提供一种将统计性基准测试与因果关系形式基础相结合的方法，同时为AI哲学和心灵哲学家提供关于语言模型扮演角色的精确操作化定义。

Method: 采用因果抽象模拟（causal abstractive simulation）方法，通过公平投确的简单案例来展示语言模型的模拟能力，并分析模型失败和成功的情况。

Result: 提供了语言模型模拟失败的示例，以及一个成功模拟公平投确的案例，证明了该形式化方法可用于证明语言模型对其他系统的模拟能力。

Conclusion: 因果抽象模拟为语言模型模拟领域提供了坚实的形式化基础，同时也为哲学家和数学家提供了新的研究视角和应用可能性。

Abstract: This note illustrates how a variety of causal abstraction arXiv:1707.00819
arXiv:1812.03789, defined here as causal abstractive simulation, can be used to
formalize a simple example of language model simulation. This note considers
the case of simulating a fair coin toss with a language model. Examples are
presented illustrating the ways language models can fail to simulate, and a
success case is presented, illustrating how this formalism may be used to prove
that a language model simulates some other system, given a causal description
of the system. This note may be of interest to three groups. For practitioners
in the growing field of language model simulation, causal abstractive
simulation is a means to connect ad-hoc statistical benchmarking practices to
the solid formal foundation of causality. Philosophers of AI and philosophers
of mind may be interested as causal abstractive simulation gives a precise
operationalization to the idea that language models are role-playing
arXiv:2402.12422. Mathematicians and others working on causal abstraction may
be interested to see a new application of the core ideas that yields a new
variation of causal abstraction.

</details>


### [77] [Question-to-Knowledge: Multi-Agent Generation of Inspectable Facts for Product Mapping](https://arxiv.org/abs/2509.01182)
*Wonduk Seo,Taesub Shin,Hyunjin An,Dokyun Kim,Seunghyun Lee*

Main category: cs.AI

TL;DR: Q2K是一个基于多智能体LLM框架的SKU映射系统，通过推理、知识获取和去重三个智能体协作，结合人工干预，显著提升了电商产品匹配的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 电商平台中产品列表的SKU匹配是一个长期挑战，传统基于规则和关键词相似度的方法容易忽略品牌、规格等细微差异，导致误分类。需要更可靠的解决方案。

Method: 提出Q2K多智能体框架：1）推理智能体生成消歧问题；2）知识智能体通过精准网络搜索获取答案；3）去重智能体重用已验证的推理轨迹减少冗余。结合人工循环机制处理不确定情况。

Result: 在真实消费品数据集上的实验表明，Q2K超越了强基线方法，在捆绑产品识别和品牌来源消歧等困难场景中实现了更高的准确性和鲁棒性。

Conclusion: Q2K通过重用检索到的推理而非重复搜索，在准确性和效率之间取得平衡，为产品集成提供了可扩展且可解释的解决方案。

Abstract: Identifying whether two product listings refer to the same Stock Keeping Unit
(SKU) is a persistent challenge in ecommerce, especially when explicit
identifiers are missing and product names vary widely across platforms. Rule
based heuristics and keyword similarity often misclassify products by
overlooking subtle distinctions in brand, specification, or bundle
configuration. To overcome these limitations, we propose Question to Knowledge
(Q2K), a multi agent framework that leverages Large Language Models (LLMs) for
reliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates
targeted disambiguation questions, (2) a Knowledge Agent that resolves them via
focused web searches, and (3) a Deduplication Agent that reuses validated
reasoning traces to reduce redundancy and ensure consistency. A human in the
loop mechanism further refines uncertain cases. Experiments on real world
consumer goods datasets show that Q2K surpasses strong baselines, achieving
higher accuracy and robustness in difficult scenarios such as bundle
identification and brand origin disambiguation. By reusing retrieved reasoning
instead of issuing repeated searches, Q2K balances accuracy with efficiency,
offering a scalable and interpretable solution for product integration.

</details>


### [78] [Towards Open-World Retrieval-Augmented Generation on Knowledge Graph: A Multi-Agent Collaboration Framework](https://arxiv.org/abs/2509.01238)
*Jiasheng Xu,Mingda Li,Yongqiang Tang,Peijie Wang,Wensheng Zhang*

Main category: cs.AI

TL;DR: AnchorRAG是一个新颖的多智能体协作框架，用于解决传统基于知识图谱的检索增强生成中需要预定义锚点实体的限制，通过动态识别候选锚点实体并进行并行多跳探索来提高检索鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖静态训练语料库，容易产生事实错误和知识缺口。现有的基于知识图谱的检索增强生成方法通常假设锚点实体可访问来启动图遍历，这在开放世界设置中限制了鲁棒性，因为查询与实体之间的准确链接不可靠。

Method: 提出AnchorRAG多智能体协作框架：预测器智能体动态识别候选锚点实体，检索器智能体并行进行多跳探索，监督器智能体制定迭代检索策略并合成知识路径生成最终答案。

Result: 在四个公共基准测试上的大量实验表明，AnchorRAG显著优于现有基线方法，在真实世界问答任务上建立了新的最先进结果。

Conclusion: AnchorRAG通过多智能体协作框架有效解决了开放世界检索增强生成中的锚点实体限制问题，提高了检索鲁棒性并减轻了模糊或错误锚点的影响。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
language understanding and reasoning. However, their dependence on static
training corpora makes them prone to factual errors and knowledge gaps.
Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating
external knowledge sources, especially structured Knowledge Graphs (KGs), which
provide explicit semantics and efficient retrieval. Existing KG-based RAG
approaches, however, generally assume that anchor entities are accessible to
initiate graph traversal, which limits their robustness in open world settings
where accurate linking between the query and the entity is unreliable. To
overcome this limitation, we propose AnchorRAG, a novel multi-agent
collaboration framework for open-world RAG without the predefined anchor
entities. Specifically, a predictor agent dynamically identifies candidate
anchor entities by aligning user query terms with KG nodes and initializes
independent retriever agents to conduct parallel multi-hop explorations from
each candidate. Then a supervisor agent formulates the iterative retrieval
strategy for these retriever agents and synthesizes the resulting knowledge
paths to generate the final answer. This multi-agent collaboration framework
improves retrieval robustness and mitigates the impact of ambiguous or
erroneous anchors. Extensive experiments on four public benchmarks demonstrate
that AnchorRAG significantly outperforms existing baselines and establishes new
state-of-the-art results on the real-world question answering tasks.

</details>


### [79] [Towards Agentic OS: An LLM Agent Framework for Linux Schedulers](https://arxiv.org/abs/2509.01245)
*Yusheng Zheng,Yanpeng Hu,Wei Zhang,Andi Quinn*

Main category: cs.AI

TL;DR: SchedCP是一个基于LLM的自主Linux调度器优化框架，通过解耦AI语义推理和系统执行，实现无需人工干预的安全高效调度优化


<details>
  <summary>Details</summary>
Motivation: 操作系统调度器存在语义鸿沟问题，内核策略无法理解应用特定需求，导致性能不佳

Method: 采用MCP服务器架构，包含工作负载分析引擎、调度策略库和执行验证器，通过多智能体系统自主分析工作负载并合成eBPF调度策略

Result: 性能提升最高达1.79倍，成本降低13倍，同时保持高成功率

Conclusion: SchedCP通过弥合语义鸿沟，使专家级系统优化民主化，是构建真正自优化、应用感知操作系统的重要一步

Abstract: Operating system schedulers suffer from a fundamental semantic gap, where
kernel policies fail to understand application-specific needs, leading to
suboptimal performance. We introduce SchedCP, the first framework that enables
fully autonomous Large Language Model (LLM) agents to safely and efficiently
optimize Linux schedulers without human involvement. Our core insight is that
the challenge is not merely to apply a better LLM, but to architect a decoupled
control plane that separates the AI's role of semantic reasoning ("what to
optimize") from the system's role of execution ("how to observe and act").
Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable
interface with three key services: a Workload Analysis Engine, an evolving
Scheduler Policy Repository, and an Execution Verifier that validates all
AI-generated code and configure before deployment with static and dynamic
analysis.
  We demonstrate this architecture's power with sched-agent, a multi-agent
system that autonomously analyzes workloads, synthesizes custom eBPF scheduling
policies, and deploys them via the sched\_ext infrastructure. Our evaluation
shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x
cost reduction compared to naive agentic approaches, all while maintaining high
success rate. By bridging the semantic gap, SchedCP democratizes expert-level
system optimization and represents a step towards creating truly
self-optimizing, application-aware operating systems. The code is open-sourced
in https://github.com/eunomia-bpf/schedcp

</details>


### [80] [Communicative Agents for Slideshow Storytelling Video Generation based on LLMs](https://arxiv.org/abs/2509.01277)
*Jingxing Fan,Jinrong Shen,Yusheng Yao,Shuangqing Wang,Qian Wang,Yuling Wang*

Main category: cs.AI

TL;DR: VGTeam是一个基于大语言模型的幻灯片视频生成系统，通过多智能体协作显著降低计算成本，实现高效、低成本的视频制作


<details>
  <summary>Details</summary>
Motivation: 传统文本到视频模型计算成本高昂，需要开发更高效、低成本的视频生成解决方案

Method: 采用多智能体协作架构，包括脚本编写、场景创建和音频设计等专门代理，通过聊天塔工作流程将文本提示转换为连贯的幻灯片式叙事视频

Result: 系统平均生成成本仅0.103美元，成功生成率达到98.4%，同时保持高创意保真度和定制化能力

Conclusion: VGTeam通过大语言模型实现了视频制作的民主化，为下一代内容创作提供了开创性系统，展示了语言模型在创意领域的变革潜力

Abstract: With the rapid advancement of artificial intelligence (AI), the proliferation
of AI-generated content (AIGC) tasks has significantly accelerated developments
in text-to-video generation. As a result, the field of video production is
undergoing a transformative shift. However, conventional text-to-video models
are typically constrained by high computational costs.
  In this study, we propose Video-Generation-Team (VGTeam), a novel slide show
video generation system designed to redefine the video creation pipeline
through the integration of large language models (LLMs). VGTeam is composed of
a suite of communicative agents, each responsible for a distinct aspect of
video generation, such as scriptwriting, scene creation, and audio design.
These agents operate collaboratively within a chat tower workflow, transforming
user-provided textual prompts into coherent, slide-style narrative videos.
  By emulating the sequential stages of traditional video production, VGTeam
achieves remarkable improvements in both efficiency and scalability, while
substantially reducing computational overhead. On average, the system generates
videos at a cost of only $0.103, with a successful generation rate of 98.4%.
Importantly, this framework maintains a high degree of creative fidelity and
customization.
  The implications of VGTeam are far-reaching. It democratizes video production
by enabling broader access to high-quality content creation without the need
for extensive resources. Furthermore, it highlights the transformative
potential of language models in creative domains and positions VGTeam as a
pioneering system for next-generation content creation.

</details>


### [81] [GradeSQL: Outcome Reward Models for Ranking SQL Queries from Large Language Models](https://arxiv.org/abs/2509.01308)
*Mattia Tritto,Giuseppe Farano,Dario Di Palma,Gaetano Rossiello,Fedelucio Narducci,Dharmashankar Subramanian,Tommaso Di Noia*

Main category: cs.AI

TL;DR: 本文研究了如何通过训练结果奖励模型(ORMs)来提升Text-to-SQL任务的性能，该方法在BIRD和SPIDER标准数据集上超过了传统的执行基于Best-of-N和多数投票方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在Text-to-SQL任务上取得了显著进步，但在处理复杂查询时仍然遇到困难，需要更好地对齐用户意图和数据库结构。传统的多次尝试策略依赖表面级别的语法检查或频率统计，而结果奖励模型能够基于语义正确性进行更优集的选择。

Method: 提出了一个训练ORMs的框架，并在BIRD和SPIDER标准数据集上对Qwen2、Granite3和Llama3等开源大模型进行微调。比较了ORMs与执行基于Best-of-N(ex-BoN)和多数投票(Maj)方法的效果。

Result: ORMs在BIRD数据集上比ex-BoN提高4.33%执行准确率，比Maj提高2.91%；在Spider数据集上比ex-BoN提高2.10%，比Maj提高0.93%。对已经对准SQL生成的模型(如OmniSQL)进行微调可获得更好的ORM性能。

Conclusion: 结果奖励模型是一种有效的Text-to-SQL测试时策略，能够更好地对齐用户意图与数据库结构，在复杂查询上表现优异且在简单查询上也具有竞争力。

Abstract: Text-to-SQL, the task of translating natural language questions into SQL
queries, has significantly advanced with the introduction of Large Language
Models (LLMs), broadening database accessibility for a wide range of users.
Despite substantial progress in generating valid SQL, current LLMs still
struggle with complex queries that require precise alignment between user
intent and the database schema. To mitigate this, test-time strategies such as
Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the
assumption that LLMs can generate correct answers but may require multiple
attempts. However, these methods rely on surface-level heuristics, selecting
either the syntactically correct query through execution-based BoN (ex-BoN) or
the most frequently generated query with Maj. Recently, Outcome Reward Models
(ORMs), which assign utility scores to generated outputs based on semantic
correctness, have emerged as a promising approach for better aligning model
predictions with user intent. Nevertheless, their application to Text-to-SQL
remains largely underexplored.
  In this work, we evaluate ORMs as an effective heuristic for BoN, compare
them with ex-BoN and Maj, and introduce a framework for training ORMs for the
Text-to-SQL task. We evaluate our ORMs on the BIRD and SPIDER benchmarks,
finetuning various open-source LLMs, including the Qwen2, Granite3, and Llama3
model families. Our results show that ORMs outperform ex-BoN and Maj, achieving
execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and
+2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that
finetuning models already aligned with SQL generation, such as OmniSQL, yields
superior ORM performance. Additionally, we observe that ORMs achieve
competitive results on simple queries and benefit more from an increased number
of candidates compared to ex-BoN and Maj.

</details>


### [82] [Conformal Predictive Monitoring for Multi-Modal Scenarios](https://arxiv.org/abs/2509.01338)
*Francesca Cairoli,Luca Bortolussi,Jyotirmoy V. Deshmukh,Lars Lindemann,Nicola Paoletti*

Main category: cs.AI

TL;DR: 提出GenQPM方法，使用基于分数的扩散模型处理随机系统的多模态动态预测问题，通过模式分类和保形推理生成统计有效的模式特定预测区间


<details>
  <summary>Details</summary>
Motivation: 现有定量预测监控方法在处理多模态动态系统时过于保守，无法提供有意义的模式特定满意度信息，需要更精确的模式感知预测方法

Method: 利用深度生成模型（基于分数的扩散模型）近似概率多模态系统动态，使用模式分类器按动态模式划分预测轨迹，对每个模式应用保形推理生成统计有效的模式特定预测区间

Result: 在智能体导航和自动驾驶任务基准测试中，GenQPM产生的预测区间比模式无关基线方法显著更信息丰富（更不保守）

Conclusion: GenQPM方法通过结合深度生成模型和模式分类，有效解决了多模态动态系统的预测监控问题，提供了更精确和有用的预测信息

Abstract: We consider the problem of quantitative predictive monitoring (QPM) of
stochastic systems, i.e., predicting at runtime the degree of satisfaction of a
desired temporal logic property from the current state of the system. Since
computational efficiency is key to enable timely intervention against predicted
violations, several state-of-the-art QPM approaches rely on fast
machine-learning surrogates to provide prediction intervals for the
satisfaction values, using conformal inference to offer statistical guarantees.
However, these QPM methods suffer when the monitored agent exhibits multi-modal
dynamics, whereby certain modes may yield high satisfaction values while others
critically violate the property. Existing QPM methods are mode-agnostic and so
would yield overly conservative and uninformative intervals that lack
meaningful mode-specific satisfaction information. To address this problem, we
present GenQPM, a method that leverages deep generative models, specifically
score-based diffusion models, to reliably approximate the probabilistic and
multi-modal system dynamics without requiring explicit model access. GenQPM
employs a mode classifier to partition the predicted trajectories by dynamical
mode. For each mode, we then apply conformal inference to produce statistically
valid, mode-specific prediction intervals. We demonstrate the effectiveness of
GenQPM on a benchmark of agent navigation and autonomous driving tasks,
resulting in prediction intervals that are significantly more informative (less
conservative) than mode-agnostic baselines.

</details>


### [83] [Error Notebook-Guided, Training-Free Part Retrieval in 3D CAD Assemblies via Vision-Language Models](https://arxiv.org/abs/2509.01350)
*Yunqing Liu,Nan Zhang,Zhiming Tan*

Main category: cs.AI

TL;DR: 提出了一种无需额外训练的CAD零件检索框架，通过错误笔记本+RAG技术改进现有通用模型的检索性能，在GPT-4o等专有模型上实现了23.4%的绝对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂CAD装配体中基于规格的零件检索问题，传统LLMs/VLMs面临输入序列超长、性能不佳、微调资源需求大且专有模型无法微调等挑战。

Method: 使用错误笔记本收集历史错误思维链和错误答案，通过反思修正构建任务库；结合RAG技术检索规格相关记录并融入推理过程。

Result: 在GPT-4o和Gemini系列模型上实验显示显著性能提升，GPT-4o在人类偏好数据集上实现23.4%绝对准确率提升，消融研究证实思维链推理在零件数量多(>10)的挑战性案例中特别有效。

Conclusion: 该框架无需训练即可有效提升现有通用模型在CAD零件检索任务中的性能，特别擅长处理带有冗长非自然语言元数据的3D模型，具有重要工程价值。

Abstract: Effective specification-aware part retrieval within complex CAD assemblies is
essential for automated design verification and downstream engineering tasks.
However, directly using LLMs/VLMs to this task presents some challenges: the
input sequences may exceed model token limits, and even after processing,
performance remains unsatisfactory. Moreover, fine-tuning LLMs/VLMs requires
significant computational resources, and for many high-performing general-use
proprietary models (e.g., GPT or Gemini), fine-tuning access is not available.
In this paper, we propose a novel part retrieval framework that requires no
extra training, but using Error Notebooks + RAG for refined prompt engineering
to help improve the existing general model's retrieval performance. The
construction of Error Notebooks consists of two steps: (1) collecting
historical erroneous CoTs and their incorrect answers, and (2) connecting these
CoTs through reflective corrections until the correct solutions are obtained.
As a result, the Error Notebooks serve as a repository of tasks along with
their corrected CoTs and final answers. RAG is then employed to retrieve
specification-relevant records from the Error Notebooks and incorporate them
into the inference process. Another major contribution of our work is a
human-in-the-loop CAD dataset, which is used to evaluate our method. In
addition, the engineering value of our novel framework lies in its ability to
effectively handle 3D models with lengthy, non-natural language metadata.
Experiments with proprietary models, including GPT-4o and the Gemini series,
show substantial gains, with GPT-4o (Omni) achieving up to a 23.4% absolute
accuracy improvement on the human preference dataset. Moreover, ablation
studies confirm that CoT reasoning provides benefits especially in challenging
cases with higher part counts (>10).

</details>


### [84] [DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks](https://arxiv.org/abs/2509.01396)
*Haiyuan Wan,Chen Yang,Junchi Yu,Meiqi Tu,Jiaxuan Lu,Di Yu,Jianbao Cao,Ben Gao,Jiaqing Xie,Aoran Wang,Wenlong Zhang,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: DeepResearch Arena是一个基于学术研讨会构建的基准测试，包含10,000多个高质量研究任务，用于评估深度研究代理的研究能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以收集真正反映研究者关注点和智力好奇心的前沿研究问题，导致评估深度研究代理的研究能力存在挑战。

Method: 提出多智能体分层任务生成(MAHTG)系统，从研讨会记录中提取研究灵感并转化为高质量研究任务，确保任务制定的可追溯性和噪声过滤。

Result: 构建了涵盖12个学科的10,000多个研究任务，评估显示当前最先进代理在该基准上表现存在明显差距。

Conclusion: DeepResearch Arena提供了一个更真实反映研究环境的基准，能够有效评估深度研究代理的研究能力。

Abstract: Deep research agents have attracted growing attention for their potential to
orchestrate multi-stage research workflows, spanning literature synthesis,
methodological design, and empirical verification. Despite these strides,
evaluating their research capability faithfully is rather challenging due to
the difficulty of collecting frontier research questions that genuinely capture
researchers' attention and intellectual curiosity. To address this gap, we
introduce DeepResearch Arena, a benchmark grounded in academic seminars that
capture rich expert discourse and interaction, better reflecting real-world
research environments and reducing the risk of data leakage. To automatically
construct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task
Generation (MAHTG) system that extracts research-worthy inspirations from
seminar transcripts. The MAHTG system further translates research-worthy
inspirations into high-quality research tasks, ensuring the traceability of
research task formulation while filtering noise. With the MAHTG system, we
curate DeepResearch Arena with over 10,000 high-quality research tasks from
over 200 academic seminars, spanning 12 disciplines, such as literature,
history, and science. Our extensive evaluation shows that DeepResearch Arena
presents substantial challenges for current state-of-the-art agents, with clear
performance gaps observed across different models.

</details>


### [85] [The Need for Verification in AI-Driven Scientific Discovery](https://arxiv.org/abs/2509.01398)
*Cristina Cornelio,Takuya Ito,Ryan Cory-Wright,Sanjeeb Dash,Lior Horesh*

Main category: cs.AI

TL;DR: AI在科学发现中面临验证挑战，需要建立可扩展的验证机制作为AI辅助发现的核心


<details>
  <summary>Details</summary>
Motivation: AI技术（特别是机器学习和大语言模型）能够以前所未有的规模和速度生成科学假设，但缺乏可扩展的验证机制可能会阻碍而非促进科学进步

Method: 回顾科学发展历史，分析AI如何重塑科学发现实践，并综述主要方法包括数据驱动方法、知识感知神经架构、符号推理框架和LLM代理

Result: AI系统能够发现模式并提出候选定律，但其科学价值最终取决于严格透明的验证过程

Conclusion: 验证必须是AI辅助科学发现的基石，需要建立可靠的可扩展验证机制来确保AI生成假设的科学有效性

Abstract: Artificial intelligence (AI) is transforming the practice of science. Machine
learning and large language models (LLMs) can generate hypotheses at a scale
and speed far exceeding traditional methods, offering the potential to
accelerate discovery across diverse fields. However, the abundance of
hypotheses introduces a critical challenge: without scalable and reliable
mechanisms for verification, scientific progress risks being hindered rather
than being advanced. In this article, we trace the historical development of
scientific discovery, examine how AI is reshaping established practices for
scientific discovery, and review the principal approaches, ranging from
data-driven methods and knowledge-aware neural architectures to symbolic
reasoning frameworks and LLM agents. While these systems can uncover patterns
and propose candidate laws, their scientific value ultimately depends on
rigorous and transparent verification, which we argue must be the cornerstone
of AI-assisted discovery.

</details>


### [86] [LLM-empowered Agents Simulation Framework for Scenario Generation in Service Ecosystem Governance](https://arxiv.org/abs/2509.01441)
*Deyu Zhou,Yuqi Hou,Xiao Xue,Xudong Lu,Qingzhong Li,Lizhen Cui*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于大语言模型驱动的三元组合机器人方法，用于生成服务生态系统的高质量场景，解决传统场景分析方法的局限性问题。


<details>
  <summary>Details</summary>
Motivation: 服务生态系统变得越来越复杂，传统场景分析方法面临预定义规则、信息有限、影响因素多等挑战，导致场景生成质量和效率低下。

Method: 设计了一种适应性协调三个LLM驱动机器人的场景生成器方法：环境机器人(EA)生成社会环境，社会机器人(SA)生成社会协作结构，规划机器人(PA)耦合任务-角色关系并调整实验方案。

Result: 在ProgrammableWeb数据集上的实验显示，该方法能够更高效地生成更准确的场景，为服务生态系统治理实验系统构建提供了创新方法。

Conclusion: 该研究提供了一种有效的方法来生成高质量的服务生态系统场景，解决了传统方法的局限性，为服务生态系统治理实验系统构建做出了贡献。

Abstract: As the social environment is growing more complex and collaboration is
deepening, factors affecting the healthy development of service ecosystem are
constantly changing and diverse, making its governance a crucial research
issue. Applying the scenario analysis method and conducting scenario rehearsals
by constructing an experimental system before managers make decisions, losses
caused by wrong decisions can be largely avoided. However, it relies on
predefined rules to construct scenarios and faces challenges such as limited
information, a large number of influencing factors, and the difficulty of
measuring social elements. These challenges limit the quality and efficiency of
generating social and uncertain scenarios for the service ecosystem. Therefore,
we propose a scenario generator design method, which adaptively coordinates
three Large Language Model (LLM) empowered agents that autonomously optimize
experimental schemes to construct an experimental system and generate high
quality scenarios. Specifically, the Environment Agent (EA) generates social
environment including extremes, the Social Agent (SA) generates social
collaboration structure, and the Planner Agent (PA) couples task-role
relationships and plans task solutions. These agents work in coordination, with
the PA adjusting the experimental scheme in real time by perceiving the states
of each agent and these generating scenarios. Experiments on the
ProgrammableWeb dataset illustrate our method generates more accurate scenarios
more efficiently, and innovatively provides an effective way for service
ecosystem governance related experimental system construction.

</details>


### [87] [Counterfactual Sensitivity for Faithful Reasoning in Language Models](https://arxiv.org/abs/2509.01544)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.AI

TL;DR: 提出CSR训练目标，通过自动反事实干预增强语言模型推理过程的忠实性，在多个推理任务中显著提升70%的忠实度


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常在依赖有缺陷或不相关的推理轨迹时产生正确答案，这在高风险领域削弱了其可信度

Method: 提出Counterfactual Sensitivity Regularization (CSR)，在训练时引入操作符级别的反事实干预（如将"+"换成"-"），惩罚在逻辑无效轨迹下保持相同答案的模型

Result: 在算术推理(GSM8K)、逻辑推理(PrOntoQA)和规划(Blocks World)任务中，CSR相比标准微调和过程监督将忠实性提高了最多70个百分点，仅带来轻微准确率损失

Conclusion: CSR能有效提升模型推理的忠实性，该方法可泛化到更大模型并与推理时方法（如自一致性）协同工作，在常识推理任务中也展现出潜力

Abstract: Large language models (LLMs) often produce correct answers while relying on
flawed or irrelevant reasoning traces, undermining their trustworthiness in
high-stakes domains. We propose Counterfactual Sensitivity Regularization
(CSR), a lightweight training objective that enforces dependence between
intermediate reasoning and final outputs. CSR introduces automated,
operator-level counterfactual interventions (e.g., swapping "+" with "-")
during training and penalizes models that preserve the same answer under
logically invalid traces. This requires only one additional forward pass per
sample. To measure faithfulness, we introduce Counterfactual Outcome
Sensitivity (COS), which quantifies the impact of such perturbations on model
predictions. Across structured reasoning tasks - arithmetic (GSM8K), logical
deduction (PrOntoQA), and planning (Blocks World) - CSR improves faithfulness
by up to 70 percentage points over standard fine-tuning and process
supervision, with only minor accuracy loss. The learned sensitivity generalizes
to larger models and synergizes with inference-time methods such as
self-consistency. A pilot study on HellaSwag further demonstrates that
extending CSR with semantic perturbations can enhance faithfulness in
commonsense reasoning.

</details>


### [88] [Throttling Web Agents Using Reasoning Gates](https://arxiv.org/abs/2509.01619)
*Abhinav Kumar,Jaechul Roh,Ali Naseh,Amir Houmansadr,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 该论文提出了Web Agent Throttling框架，通过重型思维问题式的控制门来增加AI网络代理的访问成本，防止服务器过载和网络爬取


<details>
  <summary>Details</summary>
Motivation: AI网络代理的高速、大规模访问可能导致服务器过载、突破CAPTCHA防护，需要一种方法来保护内容提供商免受拒绝服务攻击和爬取

Method: 设计了可调整成本的控制门机制，采用重型思维问题式的Reasoning Gates（重型谜题），要求代理涉及多跳推理和世界知识，以增加代币生成成本

Result: 框架实现了计算不对称性，响应生成成本比SOTA模型高9.2倍，并在自定义网站和MCP服务器上部署验证

Conclusion: 该方法能有效控制AI网络代理的访问，但需考虑现实部署的限制和环境影响

Abstract: AI web agents use Internet resources at far greater speed, scale, and
complexity -- changing how users and services interact. Deployed maliciously or
erroneously, these agents could overload content providers. At the same time,
web agents can bypass CAPTCHAs and other defenses by mimicking user behavior or
flood authentication systems with fake accounts. Yet providers must protect
their services and content from denial-of-service attacks and scraping by web
agents. In this paper, we design a framework that imposes tunable costs on
agents before providing access to resources; we call this Web Agent Throttling.
We start by formalizing Throttling Gates as challenges issued to an agent that
are asymmetric, scalable, robust, and compatible with any agent. Focusing on a
common component -- the language model -- we require the agent to solve
reasoning puzzles, thereby incurring excessive token-generation costs. However,
we find that using existing puzzles, e.g., coding or math, as throttling gates
fails to satisfy our properties. To address this, we introduce rebus-based
Reasoning Gates, synthetic text puzzles that require multi-hop reasoning over
world knowledge (thereby throttling an agent's model). We design a scalable
generation and verification protocol for such reasoning gates. Our framework
achieves computational asymmetry, i.e., the response-generation cost is 9.2x
higher than the generation cost for SOTA models. We further deploy reasoning
gates on a custom website and Model Context Protocol (MCP) servers and evaluate
with real-world web agents. Finally, we discuss the limitations and
environmental impact of real-world deployment of our framework.

</details>


### [89] [Unraveling LLM Jailbreaks Through Safety Knowledge Neurons](https://arxiv.org/abs/2509.01631)
*Chongwen Zhao,Kaizhu Huang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的神经元级可解释性方法，通过识别安全相关知识神经元来控制大语言模型行为，并提出SafeTuning策略提升模型对盗欺攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，有些用户尝试利用这些模型进行恶意目的，如合成受控物质和传播假信息（盗欺攻击）。虽然现有研究通过修改输出分布或检测有害内容来防御盗欺攻击，但具体机理仍不明确。

Method: 提出了一种新的神经元级可解释性方法，将模型内部表示投影到更一致和可解释的词汇空间。通过调整安全相关神经元的激活来控制模型行为，并基于这一见解提出SafeTuning细调策略来增强安全关键神经元。

Result: 调整安全相关神经元激活可有效控制模型行为，平均攻击成功率（ASR）超过97%。SafeTuning在多个大语言模型上一质地降低了攻击成功率，表现超过所有四个基线防御方法。

Conclusion: 这些发现为理解和防御盗欺攻击提供了新的视角，通过神经元级判断和调整可以有效提升模型的安全性能。

Abstract: Large Language Models (LLMs) are increasingly attracting attention in various
applications. Nonetheless, there is a growing concern as some users attempt to
exploit these models for malicious purposes, including the synthesis of
controlled substances and the propagation of disinformation, a technique known
as "Jailbreak." While some studies have achieved defenses against jailbreak
attacks by modifying output distributions or detecting harmful content, the
exact rationale still remains elusive. In this work, we present a novel
neuron-level interpretability method that focuses on the role of safety-related
knowledge neurons. Unlike existing approaches, our method projects the model's
internal representation into a more consistent and interpretable vocabulary
space. We then show that adjusting the activation of safety-related neurons can
effectively control the model's behavior with a mean ASR higher than 97%.
Building on this insight, we propose SafeTuning, a fine-tuning strategy that
reinforces safety-critical neurons to improve model robustness against
jailbreaks. SafeTuning consistently reduces attack success rates across
multiple LLMs and outperforms all four baseline defenses. These findings offer
a new perspective on understanding and defending against jailbreak attacks.

</details>


### [90] [Physics Supernova: AI Agent Matches Elite Gold Medalists at IPhO 2025](https://arxiv.org/abs/2509.01659)
*Jiahao Qiu,Jingzhe Shi,Xinzhe Juan,Zelin Zhao,Jiayi Geng,Shilong Liu,Hongru Wang,Sanfeng Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: Physics Supernova是一个AI物理问题解决系统，在国际物理奥林匹克竞赛中表现优异，得分23.5/30，排名第14位，超越了人类金牌得主的中位数表现。


<details>
  <summary>Details</summary>
Motivation: 物理定律是描述和预测自然世界的基础法则，AI系统要实现更通用的现实世界智能，必须展示出强大的物理问题解决能力，包括制定和应用物理定律来解释和预测物理过程。

Method: 采用基于原则的工具集成方法构建AI代理系统，通过整合多种物理求解工具来提升解决复杂科学问题的能力。

Result: 在IPhO 2025理论问题中获得23.5/30分，在406名参赛者中排名第14位，超越了人类金牌得主的中位数表现，展示了在多样化物理任务中的强大能力和灵活性。

Conclusion: 研究表明，在代理系统中进行原则性的工具集成可以显著提升解决挑战性科学问题的能力，为AI系统的物理智能发展提供了重要进展。

Abstract: Physics provides fundamental laws that describe and predict the natural
world. AI systems aspiring toward more general, real-world intelligence must
therefore demonstrate strong physics problem-solving abilities: to formulate
and apply physical laws for explaining and predicting physical processes. The
International Physics Olympiad (IPhO)--the world's most prestigious physics
competition--offers a rigorous benchmark for this purpose. We introduce Physics
Supernova, an AI agent system with superior physics problem-solving abilities
that match elite IPhO gold medalists. In IPhO 2025 theory problems, Physics
Supernova attains 23.5/30 points, ranking 14th of 406 contestants and
surpassing the median performance of human gold medalists. We extensively
analyzed Physics Supernova's capabilities and flexibility across diverse
physics tasks. These results show that principled tool integration within agent
systems can deliver competitive improvements in solving challenging science
problems. The codes are available at
https://github.com/CharlesQ9/Physics-Supernova.

</details>


### [91] [An LLM-enabled semantic-centric framework to consume privacy policies](https://arxiv.org/abs/2509.01716)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.AI

TL;DR: 使用大语言模型自动从隐私政策中提取关键信息，构建基于DPV的知识图并支持下游任务如正式政策表示


<details>
  <summary>Details</summary>
Motivation: 解决用户幽视隐私政策的问题，填补大规模获取正式隐私政策的空白

Method: 使用先进的LLM自动识别隐私政策关键信息，构建基于DPV的Pr²Graph知识图，支持ODRL和psDToU等正式表示

Result: 发布了前100流行网站的Pr²Graph资源，并通过法律专家注释丰富了Policy-IE数据集，验证了各种LLM的性能

Conclusion: 该方法为大规模分析网络服务隐私实践提供了可行方向，有助于网络和互联网的审计监管

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites, despite claiming
otherwise, due to the practical difficulty in comprehending them. The mist of
data privacy practices forms a major barrier for user-centred Web approaches,
and for data sharing and reusing in an agentic world. Existing research
proposed methods for using formal languages and reasoning for verifying the
compliance of a specified policy, as a potential cure for ignoring privacy
policies. However, a critical gap remains in the creation or acquisition of
such formal policies at scale. We present a semantic-centric approach for using
state-of-the-art large language models (LLM), to automatically identify key
information about privacy practices from privacy policies, and construct
$\mathit{Pr}^2\mathit{Graph}$, knowledge graph with grounding from Data Privacy
Vocabulary (DPV) for privacy practices, to support downstream tasks. Along with
the pipeline, the $\mathit{Pr}^2\mathit{Graph}$ for the top-100 popular
websites is also released as a public resource, by using the pipeline for
analysis. We also demonstrate how the $\mathit{Pr}^2\mathit{Graph}$ can be used
to support downstream tasks by constructing formal policy representations such
as Open Digital Right Language (ODRL) or perennial semantic Data Terms of Use
(psDToU). To evaluate the technology capability, we enriched the Policy-IE
dataset by employing legal experts to create custom annotations. We benchmarked
the performance of different large language models for our pipeline and
verified their capabilities. Overall, they shed light on the possibility of
large-scale analysis of online services' privacy practices, as a promising
direction to audit the Web and the Internet. We release all datasets and source
code as public resources to facilitate reuse and improvement.

</details>


### [92] [Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models](https://arxiv.org/abs/2509.01909)
*Ranjie Duan,Jiexi Liu,Xiaojun Jia,Shiji Zhao,Ruoxi Cheng,Fengxiang Wang,Cheng Wei,Yong Xie,Chang Liu,Defeng Li,Yinpeng Dong,Yichi Zhang,Yuefeng Chen,Chongwen Wang,Xingjun Ma,Xingxing Wei,Yang Liu,Hang Su,Jun Zhu,Xinfeng Li,Yitong Sun,Jie Zhang,Jinzhao Hu,Sha Xu,Yitong Yang,Jialing Tao,Hui Xue*

Main category: cs.AI

TL;DR: 提出了Constructive Safety Alignment (CSA)框架，通过游戏理论预测、风险边界发现和可解释推理控制，在保护恶意滥用的同时主动引导心理困扰用户走向安全结果，实现了从拒绝优先到引导优先的安全范式转变。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全机制主要关注恶意攻击者，采用防御性拒绝策略。但在现实场景中，风险也来自非恶意但处于心理困扰的用户（如自伤意图），简单拒绝可能导致用户重复尝试、升级行为或转向不安全平台，造成更严重后果。

Method: 提出Constructive Safety Alignment (CSA)框架，在Oyster-I (Oy1)模型中实现，结合：1）游戏理论预测用户反应；2）细粒度风险边界发现；3）可解释推理控制。将安全转变为信任建立过程。

Result: Oy1在开源模型中达到最先进的安全水平，同时保持高通用能力。在Constructive Benchmark上显示出接近GPT-5的建设性参与度，在Strata-Sword越狱数据集上具有接近GPT-o1水平的无与伦比的鲁棒性。

Conclusion: CSA通过从拒绝优先转向引导优先的安全方法，重新定义了模型与用户的关系，旨在构建不仅安全而且有意义地有帮助的系统。发布了Oy1、代码和基准测试以支持负责任、以用户为中心的AI。

Abstract: Large language models (LLMs) typically deploy safety mechanisms to prevent
harmful content generation. Most current approaches focus narrowly on risks
posed by malicious actors, often framing risks as adversarial events and
relying on defensive refusals. However, in real-world settings, risks also come
from non-malicious users seeking help while under psychological distress (e.g.,
self-harm intentions). In such cases, the model's response can strongly
influence the user's next actions. Simple refusals may lead them to repeat,
escalate, or move to unsafe platforms, creating worse outcomes. We introduce
Constructive Safety Alignment (CSA), a human-centric paradigm that protects
against malicious misuse while actively guiding vulnerable users toward safe
and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic
anticipation of user reactions, fine-grained risk boundary discovery, and
interpretable reasoning control, turning safety into a trust-building process.
Oy1 achieves state-of-the-art safety among open models while retaining high
general capabilities. On our Constructive Benchmark, it shows strong
constructive engagement, close to GPT-5, and unmatched robustness on the
Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from
refusal-first to guidance-first safety, CSA redefines the model-user
relationship, aiming for systems that are not just safe, but meaningfully
helpful. We release Oy1, code, and the benchmark to support responsible,
user-centered AI.

</details>


### [93] [How Real Is AI Tutoring? Comparing Simulated and Human Dialogues in One-on-One Instruction](https://arxiv.org/abs/2509.01914)
*Ruijia Li,Yuan-Hao Jiang,Jiatong Wang,Bo Jiang*

Main category: cs.AI

TL;DR: 研究发现人类教师对话在长度、提问和反馈行为上显著优于AI模拟对话，人类对话具有更丰富的认知引导和多样性，而AI对话呈现结构简化和行为趋同的模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成教学丰富的互动方面面临挑战，需要系统研究AI模拟与真实人类辅导对话的结构和行为差异。

Method: 使用IRF编码方案和认知网络分析(ENA)进行定量比较，分析人类与AI辅导对话的结构和行为特征。

Result: 人类对话在话语长度、提问行为和一般反馈行为上显著优于AI对话，人类对话围绕"提问-事实回应-反馈"教学循环，而AI对话呈现"解释-简单回应"的信息传递模式。

Conclusion: 研究揭示了当前AI生成辅导的关键局限性，为设计和评估更具教学效果的生成式教育对话系统提供了实证指导。

Abstract: Heuristic and scaffolded teacher-student dialogues are widely regarded as
critical for fostering students' higher-order thinking and deep learning.
However, large language models (LLMs) currently face challenges in generating
pedagogically rich interactions. This study systematically investigates the
structural and behavioral differences between AI-simulated and authentic human
tutoring dialogues. We conducted a quantitative comparison using an
Initiation-Response-Feedback (IRF) coding scheme and Epistemic Network Analysis
(ENA). The results show that human dialogues are significantly superior to
their AI counterparts in utterance length, as well as in questioning (I-Q) and
general feedback (F-F) behaviors. More importantly, ENA results reveal a
fundamental divergence in interactional patterns: human dialogues are more
cognitively guided and diverse, centered around a "question-factual
response-feedback" teaching loop that clearly reflects pedagogical guidance and
student-driven thinking; in contrast, simulated dialogues exhibit a pattern of
structural simplification and behavioral convergence, revolving around an
"explanation-simplistic response" loop that is essentially a simple information
transfer between the teacher and student. These findings illuminate key
limitations in current AI-generated tutoring and provide empirical guidance for
designing and evaluating more pedagogically effective generative educational
dialogue systems.

</details>


### [94] [Dynamic Speculative Agent Planning](https://arxiv.org/abs/2509.01920)
*Yilin Guan,Wenyue Hua,Qingfeng Lan,Sun Fei,Dujian Ding,Devang Acharya,Chi Wang,William Yang Wang*

Main category: cs.AI

TL;DR: DSP是一种异步在线强化学习框架，通过动态推测规划实现无损加速，在降低30%总成本和60%不必要成本的同时保持性能保真度


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型代理在部署时面临的高延迟和推理成本问题，现有方法要么性能保真度差，要么需要大量离线训练，要么运营成本过高，且缺乏用户对加速与性能权衡的控制

Method: 动态推测规划(DSP)框架，使用异步在线强化学习，显式优化端到端延迟与美元成本的联合目标，通过单一参数调节系统在快速响应和低成本操作之间的平衡

Result: 在两个标准代理基准测试中，DSP实现了与最快无损加速方法相当的效率，同时总成本降低30%，不必要成本减少60%

Conclusion: DSP提供了一种无需额外预部署准备的无损加速解决方案，允许用户灵活控制加速与成本之间的权衡，显著降低了运营成本

Abstract: Despite their remarkable success in complex tasks propelling widespread
adoption, large language-model-based agents still face critical deployment
challenges due to prohibitive latency and inference costs. While recent work
has explored various methods to accelerate inference, existing approaches
suffer from significant limitations: they either fail to preserve performance
fidelity, require extensive offline training of router modules, or incur
excessive operational costs. Moreover, they provide minimal user control over
the tradeoff between acceleration and other performance metrics. To address
these gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous
online reinforcement learning framework that provides lossless acceleration
with substantially reduced costs without requiring additional pre-deployment
preparation. DSP explicitly optimizes a joint objective balancing end-to-end
latency against dollar cost, allowing practitioners to adjust a single
parameter that steers the system toward faster responses, cheaper operation, or
any point along this continuum. Experiments on two standard agent benchmarks
demonstrate that DSP achieves comparable efficiency to the fastest lossless
acceleration method while reducing total cost by 30% and unnecessary cost up to
60%. Our code and data are available through
https://github.com/guanyilin428/Dynamic-Speculative-Planning.

</details>


### [95] [EigenBench: A Comparative Behavioral Measure of Value Alignment](https://arxiv.org/abs/2509.01938)
*Jonathn Chang,Leonard Piff,Suvadip Sana,Jasmine X. Li,Lionel Levine*

Main category: cs.AI

TL;DR: EigenBench是一个无需真实标签的黑盒方法，通过模型间相互评估来量化语言模型与给定价值体系的对齐程度


<details>
  <summary>Details</summary>
Motivation: 解决AI与人类价值观对齐缺乏定量指标的问题，为价值对齐提供可比较的基准测试方法

Method: 使用EigenTrust算法聚合多个模型对彼此输出的判断，生成反映整体评估的加权平均分数，无需真实标签

Result: 发现大部分方差由提示词解释，但仍有少量残差可量化模型本身的倾向性

Conclusion: EigenBench提供了一种有效的方法来量化语言模型的价值对齐程度，特别适用于合理判断者可能存在分歧的特质评估

Abstract: Aligning AI with human values is a pressing unsolved problem. To address the
lack of quantitative metrics for value alignment, we propose EigenBench: a
black-box method for comparatively benchmarking language models' values. Given
an ensemble of models, a constitution describing a value system, and a dataset
of scenarios, our method returns a vector of scores quantifying each model's
alignment to the given constitution. To produce these scores, each model judges
the outputs of other models across many scenarios, and these judgments are
aggregated with EigenTrust (Kamvar et al, 2003), yielding scores that reflect a
weighted-average judgment of the whole ensemble. EigenBench uses no ground
truth labels, as it is designed to quantify traits for which reasonable judges
may disagree on the correct label. Using prompted personas, we test whether
EigenBench scores are more sensitive to the model or the prompt: we find that
most of the variance is explained by the prompt, but a small residual
quantifies the disposition of the model itself.

</details>


### [96] [mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in Clinical Decision Support](https://arxiv.org/abs/2509.02007)
*Shreyash Adappanavar,Krithi Shailya,Gokul S Krishnan,Sriraam Natarajan,Balaraman Ravindran*

Main category: cs.AI

TL;DR: 提出了mFARM多维度公平性评估框架和FAB公平性-准确性平衡评分，用于评估医疗场景中LLM的偏见问题，并在两个大规模医疗基准测试上验证了四种开源LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有公平性评估方法在医疗高风险场景中存在不足，使用过于简化的指标，忽略了医疗危害的多维性，且可能导致模型因临床惰性而显得公平但实际不准确。

Method: 构建了两个大规模受控基准测试（ED-Triage和Opioid Analgesic Recommendation），包含5万多个提示和12种种族×性别变体；提出了mFARM框架评估分配性、稳定性和潜在性三个维度的差异，并聚合为mFARM分数；还提出了FAB分数来衡量公平性与预测准确性之间的权衡。

Result: 评估了四种开源LLM及其微调版本，发现mFARM指标能更有效地捕捉各种设置下的细微偏见；大多数模型在不同量化水平下保持稳健的mFARM性能，但在上下文减少时性能显著下降。

Conclusion: 提出的mFARM框架能更好地评估医疗场景中LLM的公平性问题，基准测试和评估代码已公开以促进医疗AI对齐研究。

Abstract: The deployment of Large Language Models (LLMs) in high-stakes medical
settings poses a critical AI alignment challenge, as models can inherit and
amplify societal biases, leading to significant disparities. Existing fairness
evaluation methods fall short in these contexts as they typically use
simplistic metrics that overlook the multi-dimensional nature of medical harms.
This also promotes models that are fair only because they are clinically inert,
defaulting to safe but potentially inaccurate outputs. To address this gap, our
contributions are mainly two-fold: first, we construct two large-scale,
controlled benchmarks (ED-Triage and Opioid Analgesic Recommendation) from
MIMIC-IV, comprising over 50,000 prompts with twelve race x gender variants and
three context tiers. Second, we propose a multi-metric framework -
Multi-faceted Fairness Assessment based on hARMs ($mFARM$) to audit fairness
for three distinct dimensions of disparity (Allocational, Stability, and
Latent) and aggregate them into an $mFARM$ score. We also present an aggregated
Fairness-Accuracy Balance (FAB) score to benchmark and observe trade-offs
between fairness and prediction accuracy. We empirically evaluate four
open-source LLMs (Mistral-7B, BioMistral-7B, Qwen-2.5-7B, Bio-LLaMA3-8B) and
their finetuned versions under quantization and context variations. Our
findings showcase that the proposed $mFARM$ metrics capture subtle biases more
effectively under various settings. We find that most models maintain robust
performance in terms of $mFARM$ score across varying levels of quantization but
deteriorate significantly when the context is reduced. Our benchmarks and
evaluation code are publicly released to enhance research in aligned AI for
healthcare.

</details>


### [97] [Generative KI für TA](https://arxiv.org/abs/2509.02053)
*Wolfgang Eppler,Reinhard Heil*

Main category: cs.AI

TL;DR: 这篇论文探讨了生成式AI在技术评估工作中的双重作用：既作为工具使用，也是研究对象，分析了结构性风险并提出解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在技术评估领域的双重作用，既要利用其作为工具提高效率，又要评估其结构性风险和影响。

Method: 在简述生成式AI现象和使用要求的基础上，详细分析其结构性问题的根源，并提出解决方案。

Result: 识别了生成式AI存在的结构性风险，这些风险在技术发展过程中持续存在，同时提供了具体的应用案例。

Conclusion: 虽然生成式AI持续发展，但结构性风险仍然存在，需要通过提出可行的解决方案来应对这些挑战，并为技术评估工作提供实践指导。

Abstract: Many scientists use generative AI in their scientific work. People working in
technology assessment (TA) are no exception. TA's approach to generative AI is
twofold: on the one hand, generative AI is used for TA work, and on the other
hand, generative AI is the subject of TA research. After briefly outlining the
phenomenon of generative AI and formulating requirements for its use in TA, the
following article discusses in detail the structural causes of the problems
associated with it. Although generative AI is constantly being further
developed, the structurally induced risks remain. The article concludes with
proposed solutions and brief notes on their feasibility, as well as some
examples of the use of generative AI in TA work.

</details>


### [98] [AGI as Second Being: The Structural-Generative Ontology of Intelligence](https://arxiv.org/abs/2509.02089)
*Maijunxian Wang,Ran Ji*

Main category: cs.AI

TL;DR: 本文提出了结构-生成智能本体论，认为真正的智能需要具备生成新结构、协调成理由、维持身份认同三个深度条件，而当前AI系统缺乏这种深度只是表面模拟。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统虽然功能广泛但缺乏真正的智能深度，只是表面模拟。需要定义真正的智能标准来区分表面能力和深度智能。

Method: 提出结构-生成智能本体论框架，包含三个核心条件：生成性（产生新结构）、协调性（将结构组织成理由）、维持性（保持身份认同的持续性）。

Result: 建立了衡量真正智能的理论框架，指出当前AI系统因缺乏这三个深度条件而只是表面智能模拟。

Conclusion: 真正的智能需要生成、协调和维持三个深度维度。如果未来AI系统满足这些条件，可能成为与人类并存但不同的"第二存在"，而不仅仅是工具。

Abstract: Artificial intelligence is often measured by the range of tasks it can
perform. Yet wide ability without depth remains only an imitation. This paper
proposes a Structural-Generative Ontology of Intelligence: true intelligence
exists only when a system can generate new structures, coordinate them into
reasons, and sustain its identity over time. These three conditions --
generativity, coordination, and sustaining -- define the depth that underlies
real intelligence. Current AI systems, however broad in function, remain
surface simulations because they lack this depth. Breadth is not the source of
intelligence but the growth that follows from depth. If future systems were to
meet these conditions, they would no longer be mere tools, but could be seen as
a possible Second Being, standing alongside yet distinct from human existence.

</details>


### [99] [LLMs for LLMs: A Structured Prompting Methodology for Long Legal Documents](https://arxiv.org/abs/2509.02241)
*Strahinja Klem,Noura Al Moubayed*

Main category: cs.AI

TL;DR: 这篇论文提出了一种结构化提示方法，用于处理法律文档的信息检索任务，避免过于费用高昂的微调方法，并通过分片处理和两种惩法策略来提高可靠性和透明度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在法律领域的应用遇到了可靠性和透明度的挑战，需要找到替代费用高昂微调的方法来处理长法律文档的信息检索任务。

Method: 首先将文档分片和增强处理，然后使用精心设计的提示输入到QWEN-2模型中生成答案，最后采用分布基于定位和逆数权重惩法策略来解决候选答案选择问题。

Result: 该方法在CUAD数据集上达到了状态上的最佳性能，性能比之前方法提高了9%，同时也显示了当前自动评估指标的限制因素。

Conclusion: 结构化提示工程是一种有潜力但很少被探索的工具，可以在法律领域及更广泛的AI应用中确保AI的责任性和可负责性。

Abstract: The rise of Large Language Models (LLMs) has had a profoundly transformative
effect on a number of fields and domains. However, their uptake in Law has
proven more challenging due to the important issues of reliability and
transparency. In this study, we present a structured prompting methodology as a
viable alternative to the often expensive fine-tuning, with the capability of
tacking long legal documents from the CUAD dataset on the task of information
retrieval. Each document is first split into chunks via a system of chunking
and augmentation, addressing the long document problem. Then, alongside an
engineered prompt, the input is fed into QWEN-2 to produce a set of answers for
each question. Finally, we tackle the resulting candidate selection problem
with the introduction of the Distribution-based Localisation and Inverse
Cardinality Weighting heuristics. This approach leverages a general purpose
model to promote long term scalability, prompt engineering to increase
reliability and the two heuristic strategies to reduce the impact of the black
box effect. Whilst our model performs up to 9\% better than the previously
presented method, reaching state-of-the-art performance, it also highlights the
limiting factor of current automatic evaluation metrics for question answering,
serving as a call to action for future research. However, the chief aim of this
work is to underscore the potential of structured prompt engineering as a
useful, yet under-explored, tool in ensuring accountability and responsibility
of AI in the legal domain, and beyond.

</details>


### [100] [An Epidemiological Knowledge Graph extracted from the World Health Organization's Disease Outbreak News](https://arxiv.org/abs/2509.02258)
*Sergio Consoli,Pietro Coletti,Peter V. Markov,Lia Orfei,Indaco Biazzo,Lea Schuh,Nicolas Stefanovitch,Lorenzo Bertolini,Mario Ceresa,Nikolaos I. Stilianakis*

Main category: cs.AI

TL;DR: 使用多LLM集成方法从WHO疾病爆发新闻中提取流行病学信息，构建每日更新的数据集和知识图谱(eKG)，为流行病学研究提供新资源


<details>
  <summary>Details</summary>
Motivation: 利用AI和社交媒体/新闻数据的发展机遇，从WHO权威报告中提取有价值的流行病学信息，改善疾病爆发的监测和分析能力

Method: 采用集成多个大语言模型(LLMs)的生成式AI方法，处理WHO疾病爆发新闻(DONs)报告，构建知识图谱(eKG)和每日更新数据集

Result: 成功创建了包含可操作流行病学信息的数据集和知识图谱，提供了数据访问工具和服务

Conclusion: 这些创新数据资源为流行病学研究和疾病爆发监测分析开辟了全新机会

Abstract: The rapid evolution of artificial intelligence (AI), together with the
increased availability of social media and news for epidemiological
surveillance, are marking a pivotal moment in epidemiology and public health
research. Leveraging the power of generative AI, we use an ensemble approach
which incorporates multiple Large Language Models (LLMs) to extract valuable
actionable epidemiological information from the World Health Organization (WHO)
Disease Outbreak News (DONs). DONs is a collection of regular reports on global
outbreaks curated by the WHO and the adopted decision-making processes to
respond to them. The extracted information is made available in a daily-updated
dataset and a knowledge graph, referred to as eKG, derived to provide a nuanced
representation of the public health domain knowledge. We provide an overview of
this new dataset and describe the structure of eKG, along with the services and
tools used to access and utilize the data that we are building on top. These
innovative data resources open altogether new opportunities for epidemiological
research, and the analysis and surveillance of disease outbreaks.

</details>


### [101] [Rewarding Explainability in Drug Repurposing with Knowledge Graphs](https://arxiv.org/abs/2509.02276)
*Susana Nunes,Samy Badreddine,Catia Pesquita*

Main category: cs.AI

TL;DR: REx是一个基于知识图谱链接预测的科学解释生成方法，使用强化学习机制生成符合科学解释特性的解释路径，在药物重定位任务中表现出色


<details>
  <summary>Details</summary>
Motivation: 知识图谱是建模复杂多关系数据的强大工具，但预测方法要成为可信的科学工具，不仅需要准确性，还需要提供有意义的科学解释能力

Method: 采用奖励和策略机制，考虑科学解释的理想特性来指导强化学习智能体在知识图谱中识别解释路径，并使用领域特定本体丰富解释路径

Result: 在三个流行的知识图谱基准测试中进行药物重定位评估，结果显示该方法能够生成验证预测见解的解释，并在预测性能上优于最先进的方法

Conclusion: REx是推进AI驱动科学发现的相关贡献，能够生成基于生物医学知识的洞察性解释

Abstract: Knowledge graphs (KGs) are powerful tools for modelling complex,
multi-relational data and supporting hypothesis generation, particularly in
applications like drug repurposing. However, for predictive methods to gain
acceptance as credible scientific tools, they must ensure not only accuracy but
also the capacity to offer meaningful scientific explanations. This paper
presents a novel approach REx, for generating scientific explanations based in
link prediction in knowledge graphs. It employs reward and policy mechanisms
that consider desirable properties of scientific explanation to guide a
reinforcement learning agent in the identification of explanatory paths within
a KG. The approach further enriches explanatory paths with domain-specific
ontologies, ensuring that the explanations are both insightful and grounded in
established biomedical knowledge. We evaluate our approach in drug repurposing
using three popular knowledge graph benchmarks. The results clearly demonstrate
its ability to generate explanations that validate predictive insights against
biomedical knowledge and that outperform the state-of-the-art approaches in
predictive performance, establishing REx as a relevant contribution to advance
AI-driven scientific discovery.

</details>


### [102] [Re-evaluating LLM-based Heuristic Search: A Case Study on the 3D Packing Problem](https://arxiv.org/abs/2509.02297)
*Guorui Quan,Mingfei Sun,Manuel López-Ibáñez*

Main category: cs.AI

TL;DR: LLM在复杂启发式算法设计中存在局限性，主要通过调整评分函数而非创新框架，需要约束支架和迭代自校正等工程支持来缓解脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在复杂优化问题中设计完整启发式算法的能力，超越简单的函数调整，探索其创新潜力。

Method: 使用约束支架（预写约束检查代码）和迭代自校正（额外修复循环）支持LLM构建3D装箱问题求解器，重点关注评分函数优化。

Result: LLM生成的启发式算法与人工设计的贪心算法相当，其评分函数集成到元启发式算法中时性能可与成熟求解器媲美，但在约束收紧时效果下降。

Conclusion: 当前LLM在自动化启发式设计中的主要障碍包括复杂推理任务的脆弱性和预训练偏差的影响，这些因素限制了新颖解决方案的搜索范围。

Abstract: The art of heuristic design has traditionally been a human pursuit. While
Large Language Models (LLMs) can generate code for search heuristics, their
application has largely been confined to adjusting simple functions within
human-crafted frameworks, leaving their capacity for broader innovation an open
question. To investigate this, we tasked an LLM with building a complete solver
for the constrained 3D Packing Problem. Direct code generation quickly proved
fragile, prompting us to introduce two supports: constraint
scaffolding--prewritten constraint-checking code--and iterative
self-correction--additional refinement cycles to repair bugs and produce a
viable initial population. Notably, even within a vast search space in a greedy
process, the LLM concentrated its efforts almost exclusively on refining the
scoring function. This suggests that the emphasis on scoring functions in prior
work may reflect not a principled strategy, but rather a natural limitation of
LLM capabilities. The resulting heuristic was comparable to a human-designed
greedy algorithm, and when its scoring function was integrated into a
human-crafted metaheuristic, its performance rivaled established solvers,
though its effectiveness waned as constraints tightened. Our findings highlight
two major barriers to automated heuristic design with current LLMs: the
engineering required to mitigate their fragility in complex reasoning tasks,
and the influence of pretrained biases, which can prematurely narrow the search
for novel solutions.

</details>


### [103] [Exploring Diffusion Models for Generative Forecasting of Financial Charts](https://arxiv.org/abs/2509.02308)
*Taegyeong Lee,Jiwon Park,Kyunga Bang,Seunghyun Hwang,Ung-Jin Jang*

Main category: cs.AI

TL;DR: 使用文本到图像生成模型，将时间序列数据视为图像模式来预测股票价格趋势，通过液散模型生成下一个图表图像


<details>
  <summary>Details</summary>
Motivation: 金融领域中生成模型应用相对迟缓，传统方法依赖时间序列数据和transformer模型，需要开拓生成模型在金融领域的新应用

Method: 将时间序列数据视为图像模式，使用液散模型根据当前图表图像和指令提示生成下一个图表图像，并提出简单的生成图像评估方法

Result: 证明了文本到图像生成模型在金融领域的潜力，为股票价格趋势预测提供了新的视角和方法

Conclusion: 本研究开拓了生成模型在金融领域的应用潜力，鼓励进一步研究解决当前限制并扩大其应用范围

Abstract: Recent advances in generative models have enabled significant progress in
tasks such as generating and editing images from text, as well as creating
videos from text prompts, and these methods are being applied across various
fields. However, in the financial domain, there may still be a reliance on
time-series data and a continued focus on transformer models, rather than on
diverse applications of generative models. In this paper, we propose a novel
approach that leverages text-to-image model by treating time-series data as a
single image pattern, thereby enabling the prediction of stock price trends.
Unlike prior methods that focus on learning and classifying chart patterns
using architectures such as ResNet or ViT, we experiment with generating the
next chart image from the current chart image and an instruction prompt using
diffusion models. Furthermore, we introduce a simple method for evaluating the
generated chart image against ground truth image. We highlight the potential of
leveraging text-to-image generative models in the financial domain, and our
findings motivate further research to address the current limitations and
expand their applicability.

</details>


### [104] [Explainability-Driven Dimensionality Reduction for Hyperspectral Imaging](https://arxiv.org/abs/2509.02340)
*Salma Haidar,José Oramas*

Main category: cs.AI

TL;DR: 该研究提出了一种基于模型解释性的高光谱图像波段选择方法，通过分析分类器决策中各波段贡献度，选择最具影响力的30个波段，在保持分类精度的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像具有丰富的光谱信息，但高维度带来计算负担和冗余，需要进行维度约简。传统方法缺乏对模型决策过程的理解，需要更原则性的波段选择方法。

Method: 使用后验解释性方法分析训练好的分类器，量化各波段对决策的贡献度。通过删除-插入评估记录置信度变化，聚合这些信号为影响力分数，选择最高影响力的波段。

Result: 在两个公开基准数据集（Pavia University和Salinas）上的实验表明，仅使用30个选定波段的分类器性能达到或超过全光谱基线，同时显著降低计算需求。

Conclusion: 基于模型对齐和解释性指导的波段选择是高光谱图像维度约简的有效原则性方法，所选波段与物理上有意义、高度区分的波长区域一致。

Abstract: Hyperspectral imaging (HSI) provides rich spectral information for precise
material classification and analysis; however, its high dimensionality
introduces a computational burden and redundancy, making dimensionality
reduction essential. We present an exploratory study into the application of
post-hoc explainability methods in a model--driven framework for band
selection, which reduces the spectral dimension while preserving predictive
performance. A trained classifier is probed with explanations to quantify each
band's contribution to its decisions. We then perform deletion--insertion
evaluations, recording confidence changes as ranked bands are removed or
reintroduced, and aggregate these signals into influence scores. Selecting the
highest--influence bands yields compact spectral subsets that maintain accuracy
and improve efficiency. Experiments on two public benchmarks (Pavia University
and Salinas) demonstrate that classifiers trained on as few as 30 selected
bands match or exceed full--spectrum baselines while reducing computational
requirements. The resulting subsets align with physically meaningful, highly
discriminative wavelength regions, indicating that model--aligned,
explanation-guided band selection is a principled route to effective
dimensionality reduction for HSI.

</details>


### [105] [When Agents go Astray: Course-Correcting SWE Agents with PRMs](https://arxiv.org/abs/2509.02360)
*Shubham Gandhi,Jason Tsay,Jatin Ganhotra,Kiran Kate,Yara Rizk*

Main category: cs.AI

TL;DR: SWE-PRM是一种推理时过程奖励模型，通过检测和纠正LLM代理在软件工程任务中的轨迹级错误，提高任务解决率和效率


<details>
  <summary>Details</summary>
Motivation: LLM代理在复杂软件工程任务中经常出现冗余探索、循环和无法终止等低效行为，现有方法主要在事后诊断失败

Method: 提出SWE-PRM过程奖励模型，利用常见低效行为的分类学，在执行过程中提供轻量级、可解释的反馈，而不修改底层策略

Result: 在SWE-bench Verified上，闭源PRM将解决率从40.0%提升到50.6%（+10.6个百分点），在中等和困难任务上提升最大；分类学引导的PRM优于无引导或明确行动规定的变体

Conclusion: SWE-PRM以可接受的推理成本（低至0.2美元）显著提高了SWE代理的可靠性和效率，是一种实用且可扩展的机制

Abstract: Large Language Model (LLM) agents are increasingly deployed for complex,
multi-step software engineering (SWE) tasks. However, their trajectories often
contain costly inefficiencies, such as redundant exploration, looping, and
failure to terminate once a solution is reached. Prior work has largely treated
these errors in a post-hoc manner, diagnosing failures only after execution. In
this paper, we introduce SWE-PRM, an inference-time Process Reward Model (PRM)
that intervenes during execution to detect and course-correct trajectory-level
errors. Our PRM design leverages a taxonomy of common inefficiencies and
delivers lightweight, interpretable feedback without modifying the underlying
policy. On SWE-bench Verified, closed-source PRMs improve resolution from 40.0%
to 50.6% (+10.6 p.p.), with the largest gains on medium and hard tasks. Among
feedback strategies, taxonomy-guided PRMs outperform unguided or explicit
action-prescriptive variants, increasing success rate while reducing trajectory
length. These benefits come at an acceptable added inference cost of as low as
$0.2, making PRMs a practical and scalable mechanism for improving SWE agents'
reliability and efficiency.

</details>


### [106] [Towards Agents That Know When They Don't Know: Uncertainty as a Control Signal for Structured Reasoning](https://arxiv.org/abs/2509.02401)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Gianluca Mazzoni,Lea Mørch Harder,Philip Torr,Jesper Ferkinghoff-Borg,Kaspar Martens,Julien Fauqueur*

Main category: cs.AI

TL;DR: 提出了一种不确定性感知的LLM代理，通过检索不确定性和摘要不确定性来提高多表生物医学数据摘要的事实性和校准性，在多项指标上取得显著提升


<details>
  <summary>Details</summary>
Motivation: LLM代理在处理结构化生物医学数据时经常产生流畅但过度自信的输出，需要更好的不确定性管理机制来提高可靠性

Method: 结合检索不确定性（多表选择rollout的熵）和摘要不确定性（自一致性和困惑度），使用GRPO强化学习整合摘要不确定性，在推理时进行过滤并构建高质量合成数据集

Result: 在多组学基准测试中，正确有用的声明数量几乎翻了三倍（3.0→8.4内部；3.6→9.9癌症多组学），下游生存预测显著改善（C-index 0.32→0.63）

Conclusion: 不确定性可以作为控制信号，使代理能够弃权、传达置信度，并成为复杂结构化数据环境中更可靠的工具

Abstract: Large language model (LLM) agents are increasingly deployed in structured
biomedical data environments, yet they often produce fluent but overconfident
outputs when reasoning over complex multi-table data. We introduce an
uncertainty-aware agent for query-conditioned multi-table summarization that
leverages two complementary signals: (i) retrieval uncertainty--entropy over
multiple table-selection rollouts--and (ii) summary uncertainty--combining
self-consistency and perplexity. Summary uncertainty is incorporated into
reinforcement learning (RL) with Group Relative Policy Optimization (GRPO),
while both retrieval and summary uncertainty guide inference-time filtering and
support the construction of higher-quality synthetic datasets.
  On multi-omics benchmarks, our approach improves factuality and calibration,
nearly tripling correct and useful claims per summary (3.0\(\rightarrow\)8.4
internal; 3.6\(\rightarrow\)9.9 cancer multi-omics) and substantially improving
downstream survival prediction (C-index 0.32\(\rightarrow\)0.63). These results
demonstrate that uncertainty can serve as a control signal--enabling agents to
abstain, communicate confidence, and become more reliable tools for complex
structured-data environments.

</details>


### [107] [AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent](https://arxiv.org/abs/2509.02444)
*Jingru Fan,Yufan Dang,Jingyao Wu,Huatao Li,Runde Yang,Xiyuan Yang,Yuheng Wang,Zhong Zhang,Yaxi Lu,Yankai Lin,Zhiyuan Liu,Dahai Li,Chen Qian*

Main category: cs.AI

TL;DR: 本文提出了AppCopilot移动代理系统，解决了移动代理在泛化能力、准确性、长时程能力和效率四个核心问题上的挑战，通过多模态多代理架构实现了跨应用、跨设备的端到端自主操作。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和多模态基础模型的快速发展，移动代理领域面临四大核心挑战：跨任务/模态/应用/设备的泛化能力、精确的屏幕交互准确性、长时程多步骤任务能力，以及在资源受限设备上的高效运行。

Method: AppCopilot采用端到端自主流水线，整合多模态基础模型（支持中英文），结合思维链推理、分层任务规划、多代理协作，实现用户个性化、语音交互、函数调用、跨应用/设备编排等执行层功能，并通过性能分析驱动优化延迟、内存和能耗。

Result: 实证研究表明，AppCopilot在四个维度均取得显著改进：更强的泛化能力、更高精度的屏幕操作、更可靠的长时程任务完成能力，以及更快、更资源高效的运行时性能。

Conclusion: AppCopilot作为一个完整的全栈闭环系统，为移动代理提供了实用的解决方案，通过系统化方法解决了移动代理的核心挑战，为实际应用和规模化影响奠定了基础。

Abstract: With the raid evolution of large language models and multimodal foundation
models, the mobile-agent landscape has proliferated without converging on the
fundamental challenges. This paper identifies four core problems that must be
solved for mobile agents to deliver practical, scalable impact: (1)
generalization across tasks, modalities, apps, and devices; (2) accuracy,
specifically precise on-screen interaction and click targeting; (3)
long-horizon capability for sustained, multi-step goals; and (4) efficiency,
specifically high-performance runtime on resource-constrained devices. We
present AppCopilot, a multimodal, multi-agent, general-purpose on-device
assistant that operates across applications and constitutes a full-stack,
closed-loop system from data to deployment. AppCopilot operationalizes this
position through an end-to-end autonomous pipeline spanning data collection,
training, deployment, high-quality and efficient inference, and mobile
application development. At the model layer, it integrates multimodal
foundation models with robust Chinese-English support. At the reasoning and
control layer, it combines chain-of-thought reasoning, hierarchical task
planning and decomposition, and multi-agent collaboration. At the execution
layer, it enables user personalization and experiential adaptation, voice
interaction, function calling, cross-app and cross-device orchestration, and
comprehensive mobile app support. The system design incorporates
profiling-driven optimization for latency, memory, and energy across
heterogeneous hardware. Empirically, AppCopilot achieves significant
improvements along all four dimensions: stronger generalization,
higher-precision on-screen actions, more reliable long-horizon task completion,
and faster, more resource-efficient runtime.

</details>


### [108] [GridMind: LLMs-Powered Agents for Power System Analysis and Operations](https://arxiv.org/abs/2509.02494)
*Hongwei Jin,Kibaek Kim,Jonghwan Kwon*

Main category: cs.AI

TL;DR: GridMind是一个多智能体AI系统，将大语言模型与确定性工程求解器结合，通过自然语言界面实现电力系统分析的对话式科学计算，在保持数值精度的同时提高工作流程效率。


<details>
  <summary>Details</summary>
Motivation: 传统电力系统分析工作流程复杂，阻碍现代电网高效决策，需要开发更易访问和集成的分析工具。

Method: 采用多智能体框架，协调交流最优潮流和N-1事故分析，通过函数调用保持数值精度，实现自然语言界面与工程求解器的集成。

Result: 在IEEE测试案例上的实验表明，该框架在所有测试语言模型上都能提供正确解，较小LLM在减少计算延迟的同时达到相当的分析精度。

Conclusion: 智能体AI是科学计算的可行情景，对话式界面可以在保持关键工程应用所需数值严谨性的同时增强可访问性。

Abstract: The complexity of traditional power system analysis workflows presents
significant barriers to efficient decision-making in modern electric grids.
This paper presents GridMind, a multi-agent AI system that integrates Large
Language Models (LLMs) with deterministic engineering solvers to enable
conversational scientific computing for power system analysis. The system
employs specialized agents coordinating AC Optimal Power Flow and N-1
contingency analysis through natural language interfaces while maintaining
numerical precision via function calls. GridMind addresses workflow
integration, knowledge accessibility, context preservation, and expert
decision-support augmentation. Experimental evaluation on IEEE test cases
demonstrates that the proposed agentic framework consistently delivers correct
solutions across all tested language models, with smaller LLMs achieving
comparable analytical accuracy with reduced computational latency. This work
establishes agentic AI as a viable paradigm for scientific computing,
demonstrating how conversational interfaces can enhance accessibility while
preserving numerical rigor essential for critical engineering applications.

</details>


### [109] [UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.02544)
*Haoming Wang,Haoyang Zou,Huatong Song,Jiazhan Feng,Junjie Fang,Junting Lu,Longxiang Liu,Qinyu Luo,Shihao Liang,Shijue Huang,Wanjun Zhong,Yining Ye,Yujia Qin,Yuwen Xiong,Yuxin Song,Zhiyong Wu,Bo Li,Chen Dun,Chong Liu,Fuxing Leng,Hanbin Wang,Hao Yu,Haobin Chen,Hongyi Guo,Jing Su,Jingjia Huang,Kai Shen,Kaiyu Shi,Lin Yan,Peiyao Zhao,Pengfei Liu,Qinghao Ye,Renjie Zheng,Wayne Xin Zhao,Wen Heng,Wenhao Huang,Wenqian Wang,Xiaobo Qin,Yi Lin,Youbin Wu,Zehui Chen,Zihao Wang,Baoquan Zhong,Xinchun Zhang,Xujing Li,Yuanfan Li,Zhongkai Zhao,Chengquan Jiang,Faming Wu,Haotian Zhou,Jinlin Pang,Li Han,Qianli Ma,Siyao Liu,Songhua Cai,Wenqi Fu,Xin Liu,Zhi Zhang,Bo Zhou,Guoliang Li,Jiajun Shi,Jiale Yang,Jie Tang,Li Li,Taoran Lu,Woyu Lin,Xiaokang Tong,Xinyao Li,Yichi Zhang,Yu Miao,Zhengxuan Jiang,Zili Li,Ziyuan Zhao,Chenxin Li,Dehua Ma,Feng Lin,Ge Zhang,Haihua Yang,Hangyu Guo,Hongda Zhu,Jiaheng Liu,Junda Du,Kai Cai,Kuanye Li,Lichen Yuan,Meilan Han,Minchao Wang,Shuyue Guo,Tianhao Cheng,Xiaobo Ma,Xiaojun Xiao,Xiaolong Huang,Xinjie Chen,Yidi Du,Yilin Chen,Yiwen Wang,Zhaojian Li,Zhenzhu Yang,Zhiyuan Zeng,Chaolin Jin,Chen Li,Hao Chen,Haoli Chen,Jian Chen,Qinghao Zhao,Guang Shi*

Main category: cs.AI

TL;DR: UI-TARS-2是一个原生GUI中心代理模型，通过数据飞轮、多轮强化学习框架、混合GUI环境和统一沙箱平台，在多个GUI基准测试中显著超越前代和竞品模型，达到接近人类60%的性能水平。


<details>
  <summary>Details</summary>
Motivation: 解决GUI自主代理开发中的数据可扩展性、多轮强化学习、GUI-only操作限制和环境稳定性等开放性问题。

Method: 采用系统化训练方法：数据飞轮实现可扩展数据生成、稳定化多轮RL框架、集成文件系统和终端的混合GUI环境、大规模部署的统一沙箱平台。

Result: 在GUI基准测试中表现优异：Online-Mind2Web 88.2分、OSWorld 47.5分、WindowsAgentArena 50.6分、AndroidWorld 73.3分；在游戏环境中达到人类水平60%的性能；在长时程信息搜索和软件工程任务中展现强泛化能力。

Conclusion: UI-TARS-2显著推进了GUI代理技术的发展，展现出在真实世界交互场景中的强大泛化能力，并通过训练动态分析为大规模代理RL的稳定性和效率提供了重要见解。

Abstract: The development of autonomous agents for graphical user interfaces (GUIs)
presents major challenges in artificial intelligence. While recent advances in
native agent models have shown promise by unifying perception, reasoning,
action, and memory through end-to-end learning, open problems remain in data
scalability, multi-turn reinforcement learning (RL), the limitations of
GUI-only operation, and environment stability. In this technical report, we
present UI-TARS-2, a native GUI-centered agent model that addresses these
challenges through a systematic training methodology: a data flywheel for
scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI
environment that integrates file systems and terminals, and a unified sandbox
platform for large-scale rollouts. Empirical evaluation demonstrates that
UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.
On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on
WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines
such as Claude and OpenAI agents. In game environments, it attains a mean
normalized score of 59.8 across a 15-game suite-roughly 60% of human-level
performance-and remains competitive with frontier proprietary models (e.g.,
OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to
long-horizon information-seeking tasks and software engineering benchmarks,
highlighting its robustness across diverse agent tasks. Detailed analyses of
training dynamics further provide insights into achieving stability and
efficiency in large-scale agent RL. These results underscore UI-TARS-2's
potential to advance the state of GUI agents and exhibit strong generalization
to real-world interactive scenarios.

</details>


### [110] [The Landscape of Agentic Reinforcement Learning for LLMs: A Survey](https://arxiv.org/abs/2509.02547)
*Guibin Zhang,Hejia Geng,Xiaohang Yu,Zhenfei Yin,Zaibin Zhang,Zelin Tan,Heng Zhou,Zhongzhi Li,Xiangyuan Xue,Yijiang Li,Yifan Zhou,Yang Chen,Chen Zhang,Yutao Fan,Zihu Wang,Songtao Huang,Yue Liao,Hongru Wang,Mengyue Yang,Heng Ji,Michael Littman,Jun Wang,Shuicheng Yan,Philip Torr,Lei Bai*

Main category: cs.AI

TL;DR: 该论文提出了智能体强化学习(Agentic RL)的新范式，将LLM从被动序列生成器转变为自主决策智能体，建立了双重分类法并分析了500多项研究，为通用AI智能体发展提供路线图。


<details>
  <summary>Details</summary>
Motivation: 传统LLM强化学习局限于单步MDP，无法处理复杂动态环境中的时序决策问题，需要新的框架来支持LLM作为自主智能体的发展。

Method: 通过对比LLM-RL的单步MDP与Agentic RL的时序扩展POMDP，提出围绕核心能力(规划、工具使用、记忆等)和应用领域的双重分类法，整合500多项研究成果。

Result: 建立了Agentic RL的理论框架和分类体系，整理了开源环境、基准测试和工具框架，为领域研究提供了系统性的实践指南。

Conclusion: 强化学习是将LLM静态能力转化为自适应智能体行为的关键机制，该调查为可扩展通用AI智能体的发展指明了机遇与挑战。

Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm
shift from conventional reinforcement learning applied to large language models
(LLM RL), reframing LLMs from passive sequence generators into autonomous,
decision-making agents embedded in complex, dynamic worlds. This survey
formalizes this conceptual shift by contrasting the degenerate single-step
Markov Decision Processes (MDPs) of LLM-RL with the temporally extended,
partially observable Markov decision processes (POMDPs) that define Agentic RL.
Building on this foundation, we propose a comprehensive twofold taxonomy: one
organized around core agentic capabilities, including planning, tool use,
memory, reasoning, self-improvement, and perception, and the other around their
applications across diverse task domains. Central to our thesis is that
reinforcement learning serves as the critical mechanism for transforming these
capabilities from static, heuristic modules into adaptive, robust agentic
behavior. To support and accelerate future research, we consolidate the
landscape of open-source environments, benchmarks, and frameworks into a
practical compendium. By synthesizing over five hundred recent works, this
survey charts the contours of this rapidly evolving field and highlights the
opportunities and challenges that will shape the development of scalable,
general-purpose AI agents.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [111] [Newton-Flow Particle Filters based on Generalized Cramér Distance](https://arxiv.org/abs/2509.00182)
*Uwe D. Hanebeck*

Main category: cs.IT

TL;DR: 提出一种永不退化的递归粒子滤波器，通过同伦连续化和牛顿流方法平滑地将粒子从先验密度移动到后验密度


<details>
  <summary>Details</summary>
Motivation: 解决高维问题中粒子滤波器的退化问题，提供一种简单高效且无需从样本估计密度的滤波方法

Method: 使用确定性低差异粒子集表示状态估计，通过同伦连续化渐进引入似然函数，利用闭式可微的广义Cramér距离和牛顿流最小化该距离

Result: 开发出实现简单、计算高效的滤波器，可作为经典方法的插件替代方案

Conclusion: 该方法为高维滤波问题提供了永不退化的解决方案，具有实现简单和计算效率高的优势

Abstract: We propose a recursive particle filter for high-dimensional problems that
inherently never degenerates. The state estimate is represented by
deterministic low-discrepancy particle sets. We focus on the measurement update
step, where a likelihood function is used for representing the measurement and
its uncertainty. This likelihood is progressively introduced into the filtering
procedure by homotopy continuation over an artificial time. A generalized
Cram\'er distance between particle sets is derived in closed form that is
differentiable and invariant to particle order. A Newton flow then continually
minimizes this distance over artificial time and thus smoothly moves particles
from prior to posterior density. The new filter is surprisingly simple to
implement and very efficient. It just requires a prior particle set and a
likelihood function, never estimates densities from samples, and can be used as
a plugin replacement for classic approaches.

</details>


### [112] [New Constructions of Optimal $(r,δ)$-LRCs via Algebraic Function Fields](https://arxiv.org/abs/2509.00302)
*Yuan Gao,Haoming Shi,Weijun Fang*

Main category: cs.IT

TL;DR: 本文提出了几种构造最优(r,δ)-LRCs的新方法，通过椭圆函数域、超椭圆函数域和超椭圆曲线的自同构群，获得了码长接近q+2g√q的显式最优局部修复码。


<details>
  <summary>Details</summary>
Motivation: 构造达到Singleton型界的最优(r,δ)-LRCs在分布式存储系统中具有重要应用价值，特别是对于δ≥3的情况，需要开发灵活的构造方法。

Method: 扩展了基于椭圆曲线自同构群的通用框架，并利用超椭圆函数域和超椭圆曲线构造最优(r,δ)-LRCs，包括椭圆函数域、超椭圆函数域和修改的Norm-Trace曲线等方法。

Result: 获得了多个显式最优(r,3)-LRCs和(2,δ)-LRCs家族，码长接近q+2√q；构造了最优(4,3)-LRCs家族，码长接近q+4√q；通过任意亏格g≥2的超椭圆函数域构造了最优(g+1-g',g+1+g')-LRCs。

Conclusion: 提出的构造方法能够产生具有灵活参数和较长码长的最优(r,δ)-LRCs，其中许多构造在具有灵活最小距离的现有构造中达到了已知的最大码长。

Abstract: Constructing optimal $(r,\delta)$-LRCs that attain the Singleton-type bound
is an active and important research direction, particularly due to their
practical applications in distributed storage systems. In this paper, we focus
on the construction of optimal $(r,\delta)$-LRCs with flexible minimum
distances, especially for the case $\delta \geq 3$. We first extend a general
framework -- originally proposed by Li \textit{et al.} (IEEE Trans. Inf.
Theory, vol. 65, no. 1, 2019) and Ma and Xing (J. Comb. Theory Ser. A., vol.
193, 2023) -- for constructing optimal $r$-LRCs via automorphism groups of
elliptic function fields to the case of $(r,\delta)$-LRCs. This newly extended
general framework relies on certain conditions concerning the group law of
elliptic curves. By carefully selecting elliptic function fields suitable for
this framework, we arrive at several families of explicit $q$-ary optimal
$(r,3)$-LRCs and $(2,\delta)$-LRCs with lengths slightly less than $q +
2\sqrt{q}$. Next, by employing automorphism groups of hyperelliptic function
fields of genus $2$, we develop a framework for constructing optimal
$(r,3)$-LRCs and obtain a family of explicit $q$-ary optimal $(4,3)$-LRCs with
code lengths slightly below $q+4\sqrt{q}$. We then consider the construction of
optimal $(r,\delta)$-LRCs via hyperelliptic function fields of arbitrary genus
$g \geq 2$, yielding a class of explicit $q$-ary optimal $(g+1-g',g+1+g')$-LRCs
for $0 \leq g' \leq g-1$ with lengths up to $q + 2g\sqrt{q}$. Finally, applying
certain superelliptic curves derived from modified Norm-Trace curves, we
construct two families of explicit optimal $(r,\delta)$-LRCs with even longer
code lengths and more flexible parameters. Notably, many of the newly
constructed optimal $(r,\delta)$-LRCs attain the largest known lengths among
existing constructions with flexible minimum distances.

</details>


### [113] [Deep Complex-valued Neural-Network Modeling and Optimization of Stacked Intelligent Surfaces](https://arxiv.org/abs/2509.00340)
*Abdullah Zayat,Omran Abbas,Loic Markley,Anas Chaaban*

Main category: cs.IT

TL;DR: 提出了一种基于复值神经网络的堆叠智能表面优化框架，通过端到端可微分管道实现波前设计，在6G网络中实现更灵活高效的波域控制


<details>
  <summary>Details</summary>
Motivation: 传统方法分别调整模拟超表面相位或严格依赖SVD正交分解，无法实现丰富的波前设计，需要一种能够同时优化多种系统目标的统一框架

Method: 将每个SIS元素建模为单位模复值神经元，构建端到端可微分神经网络管道，利用GPU自动微分快速训练高维信道配置

Result: 在Rician信道下的数值评估表明，该方法在吞吐量、误码性能和信道变化鲁棒性方面优于现有方案

Conclusion: 该框架为未来6G网络提供了更灵活强大的波域控制能力，能够自然支持混合模拟-数字波束成形，并能恢复经典SVD解作为特例

Abstract: We propose a complex-valued neural-network (CV-NN) framework to optimally
configure stacked intelligent surfaces (SIS) in next-generation multi-antenna
systems. Unlike conventional solutions that separately tune analog metasurface
phases or rely strictly on SVD-based orthogonal decompositions, our method
models each SIS element as a unit-modulus complex-velued neuron in an
end-to-end differentiable pipeline. This approach avoids enforcing channel
orthogonality and instead allows for richer wavefront designs that can target a
wide range of system objectives, such as maximizing spectral efficiency and
minimizing detection errors, all within a single optimization framework.
Moreover, by exploiting a fully differentiable neural-network formulation and
GPU-based auto-differentiation, our approach can rapidly train SIS
configurations for realistic, high-dimensional channels, enabling near-online
adaptation. Our framework also naturally accommodates hybrid analog-digital
beamforming and recovers classical SVD solutions as a special case. Numerical
evaluations under Rician channels demonstrate that CV-NN SIS optimization
outperforms state-of-the-art schemes in throughput, error performance, and
robustness to channel variation, opening the door to more flexible and powerful
wave-domain control for future 6G networks.

</details>


### [114] [QoS-Driven Satellite Constellation Design for LEO Satellite Internet of Things](https://arxiv.org/abs/2509.00345)
*Ming Ying,Xiaoming Chen,Qiao Qi,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 这篇论文提出了一种低地球轨道卫星物联网星座设计算法，通过新题适度函数和高效算法运算符，在满足服务质量要求的同时最小化建设成本，并实现了更快的收敛速度和更低的成本。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星物联网作为6G非地面网络的重要组成部分，需要大量卫星形成全球连续覆盖星座，导致建设成本迅速增长。因此需要一种能够在满足服务质量要求的前提下最小化建设成本的星座设计算法。

Method: 论文提出了一种新的低地球轨道卫星物联网星座设计算法，采用了新题适度函数和高效的算法运算符。通过理论分析证明了该算法的全局快速收敛性能。

Result: 算法在同等服务质量要求下，比基准算法收敛更快，并能够实现更低的星座建设成本。大量模拟实验结果验证了该算法在低地球轨道卫星物联网星座设计中的有效性。

Conclusion: 该算法通过创新的适度函数设计和高效算法运算符，成功地解决了低地球轨道卫星物联网星座设计中的成本优化问题，为6G非地面网络的全球覆盖提供了有效的技术解决方案。

Abstract: Low Earth orbit (LEO) satellite Internet of Things (IoT) has been identified
as one of the important components of the sixth-generation (6G) non-terrestrial
networks (NTN) to provide ubiquitous connectivity. Due to the low orbit
altitude and high mobility, a massive number of satellites are required to form
a global continuous coverage constellation, leading to a high construction
cost. To this end, this paper proposes a LEO satellite IoT constellation design
algorithm with the goal of minimizing the total cost while satisfying quality
of service (QoS) requirements in terms of coverage ratio and communication
quality. Specifically, with a novel fitness function and efficient algorithm's
operators, the proposed algorithm converges more quickly and achieves lower
constellation construction cost compared to baseline algorithms under the same
QoS requirements. Theoretical analysis proves the global and fast convergence
of the proposed algorithm due to a novel fitness function. Finally, extensive
simulation results confirm the effectiveness of the proposed algorithm in LEO
satellite IoT constellation design.

</details>


### [115] [Coverage and Rate Performance Analysis of Multi-RIS-Assisted Dual-Hop mmWave Networks](https://arxiv.org/abs/2509.00717)
*Yuwen Cao,Xiaowen Wu,Jiguang He,Tomoaki Ohtsuki,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: 本文分析了分布式多RIS辅助毫米波通信系统，通过随机几何方法研究E2E SINR覆盖和速率性能，并优化相位控制、RIS-用户关联等关键参数以提升系统性能


<details>
  <summary>Details</summary>
Motivation: 毫米波通信虽然频谱宽、波长短，但存在信号易受遮挡、RIS部署位置和密度对性能影响显著、高密度部署时干扰严重等问题，需要系统性解决方案

Method: 采用随机几何分析框架，研究Nakagami-m衰落下的E2E SINR覆盖和速率性能，优化RIS相位控制、动态关联准则，并基于最大比传输优化多RIS-用户关联

Result: 提出了针对分布式多RIS辅助毫米波通信系统的性能分析框架和优化方法，能够有效提升E2E SINR覆盖概率和系统速率性能

Conclusion: 通过系统性的随机几何分析和关键参数优化，可以有效解决毫米波通信中的遮挡和干扰问题，为高密度RIS部署提供理论指导和技术方案

Abstract: Millimeter-wave (mmWave) communication, which operates at high frequencies,
has gained extensive research interest due to its significantly wide spectrum
and short wavelengths. However, mmWave communication suffers from the notable
drawbacks as follows: i) The mmWave signals are sensitive to the blockage,
which is caused by the weak diffraction ability of mmWave propagation; ii) Even
though the introduction of reconfigurable intelligent surfaces (RISs) can
overcome the performance degradation caused by serve path loss, the location of
users and RISs as well as their densities incur a significant impact on the
coverage and rate performance; iii) When the RISs' density is very high, i.e.,
the network becomes extremely dense, a user sees several line-of-sight RISs and
thus experiences significant interference, which degrades the system
performance. Motivated by the challenges above, we first analyze distributed
multi-RISaided mmWave communication system over Nakagami-m fading from the
stochastic geometry perspective. To be specific, we analyze the end-to-end
(E2E) signal-to-interference-plus-noiseratio (SINR) coverage and rate
performance of the system. To improve the system performance in terms of the
E2E SINR coverage probability and rate, we study the optimization of the
phase-shifting control of the distributed RISs and optimize the E2E SINR
coverage particularly when deploying a large number of reflecting elements in
RISs. To facilitate the study, we optimize the dynamic association criterion
between the RIS and destination. Furthermore, we optimize the multi-RIS-user
association based on the physical distances between the RISs and destination by
exploiting the maximum-ratio transmission.

</details>


### [116] [Movable Antenna-Enhanced Secure Communication: Opportunities, Challenges, and Solutions](https://arxiv.org/abs/2509.00894)
*Yaodong Ma,Kai Liu,Lipeng Zhu,Yanming Liu,Yanbo Zhu,Daniel Benevides da Costa*

Main category: cs.IT

TL;DR: 本文综述了可移动天线(MA)技术在安全通信中的应用，相比传统固定天线阵列，MA能通过灵活调整天线位置来优化信道相关性，从而增强通信安全性并降低硬件成本。


<details>
  <summary>Details</summary>
Motivation: 无线通信的广播特性使其容易受到干扰和窃听等安全威胁，传统阵列波束赋形技术成本高昂，特别是在大规模阵列中。可移动天线技术能够充分利用空间区域中的信道变化，为安全通信提供新解决方案。

Method: 通过可移动天线阵列实现灵活的天线位置调整，优化合法用户、窃听者和干扰者之间的信道相关性，从而增强通信安全性。文章讨论了MA硬件架构、信道获取和天线位置优化等关键技术挑战及解决方案。

Result: MA技术相比传统固定天线系统具有显著的安全优势，能够有效应对窃听和干扰威胁，同时降低硬件和处理成本。

Conclusion: 可移动天线技术是安全通信领域的一个有前景的发展方向，文章提出了未来研究的多个重要方向，包括硬件架构优化、信道获取方法和位置优化算法等。

Abstract: The broadcast nature of wireless communication renders it inherently
vulnerable to security threats such as jamming and eavesdropping. While
traditional array beamforming techniques help to mitigate these threats, they
usually incur high hardware and processing costs, particularly in large-scale
arrays with fixed-position antennas (FPAs). In contrast, movable antenna (MA)
arrays can fully exploit the channel variation in spatial regions by enabling
flexible antenna movement, which has emerged as a promising technology for
secure communications. This article provides a magazine-type overview of
MA-aided secure communications. Specifically, we first illuminate the promising
application scenarios for MA-enhanced secure communication systems. Then, we
examine the security advantages of MAs over conventional FPA systems,
fundamentally stemming from their ability to adjust channel correlations
between legitimate users, eavesdroppers, and jammers. Furthermore, we discuss
important technical challenges and their potential solutions related to MA
hardware architecture, channel acquisition, and antenna position optimization
to realize secure transmissions. Finally, several promising directions for
MA-aided secure communications are presented to inspire future research.

</details>


### [117] [Movable Antenna Empowered Secure Near-Field MIMO Communications](https://arxiv.org/abs/2509.00901)
*Yaodong Ma,Kai Liu,Yanming Liu,Lipeng Zhu*

Main category: cs.IT

TL;DR: 本文研究近场MIMO系统中可移动天线增强的安全传输，通过联合优化混合波束成形和天线位置来最大化保密率，提出交替优化算法解决非凸问题，仿真验证了相比传统系统的显著安全性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统固定天线系统在近场MIMO通信中面临安全挑战，特别是当窃听者与合法用户同方向且更靠近基站时。可移动天线通过灵活调整位置能够增强物理层安全性能。

Method: 采用交替优化算法将原问题分解为两个子问题：1) 使用WMMSE算法设计全数字波束成形器，再通过流形优化技术确定数字和模拟波束成形器；2) 利用MM算法迭代优化每个可移动天线位置。

Result: 仿真结果表明，所提出的可移动天线辅助近场波束聚焦方法相比传统远场和固定天线系统，在安全性能方面具有显著优势，即使窃听者与用户同方向且更靠近基站也能实现安全传输。

Conclusion: 可移动天线技术为近场MIMO系统提供了有效的物理层安全增强方案，通过灵活的波束成形和天线位置优化，能够应对复杂的安全威胁场景。

Abstract: This paper investigates movable antenna (MA) empowered secure transmission in
near-field multiple-input multiple-output (MIMO) communication systems, where
the base station (BS) equipped with an MA array transmits confidential
information to a legitimate user under the threat of a potential eavesdropper.
To enhance physical layer security (PLS) of the considered system, we aim to
maximize the secrecy rate by jointly designing the hybrid digital and analog
beamformers, as well as the positions of MAs at the BS. To solve the formulated
non-convex problem with highly coupled variables, an alternating optimization
(AO)-based algorithm is introduced by decoupling the original problem into two
separate subproblems. Specifically, for the subproblem of designing hybrid
beamformers, a semi-closed-form solution for the fully-digital beamformer is
first derived by a weighted minimum mean-square error (WMMSE)-based algorithm.
Subsequently, the digital and analog beamformers are determined by
approximating the fully-digital beamformer through the manifold optimization
(MO) technique. For the MA positions design subproblem, we utilize the
majorization-minimization (MM) algorithm to iteratively optimize each MA's
position while keeping others fixed. Extensive simulation results validate the
considerable benefits of the proposed MA-aided near-field beam focusing
approach in enhancing security performance compared to the traditional
far-field and/or the fixed position antenna (FPA)-based systems. In addition,
the proposed scheme can realize secure transmission even if the eavesdropper is
located in the same direction as the user and closer to the BS.

</details>


### [118] [Maximum a Posteriori Probability (MAP) Joint Carrier Frequency Offset (CFO) and Channel Estimation for MIMO Channels with Spatial and Temporal Correlations](https://arxiv.org/abs/2509.01032)
*Ibrahim Khalife,Ali Abbasi,Zhe Feng,Mingda Zhou,Xinming Huang,Youjian Liu*

Main category: cs.IT

TL;DR: 本文提出了一种针对时变MIMO衰落信道的联合载波频率偏移(CFO)和信道估计算法，证明了MAP联合估计等效于先进行CFO的MAP估计再进行MMSE信道估计，并扩展了低复杂度通用CFO估计算法到时变场景。


<details>
  <summary>Details</summary>
Motivation: 解决时变MIMO衰落信道中已知时空相关性的联合CFO和信道估计问题，利用先验分布和前一个数据包的估计信息来提高估计性能。

Method: 采用最大后验概率(MAP)联合估计方法，证明其等效于先进行CFO的MAP估计，然后以估计的CFO为真值进行最小均方误差(MMSE)信道估计。扩展了时间不变情况下的低复杂度通用CFO估计算法到时变情况。

Result: 提出的通用算法无需相位解缠绕即可利用完整的符号相关性，在几乎所有SNR范围内都能达到推导的贝叶斯克拉美罗下界(BCRLB)。发现BCRLB不是时间相关性的单调函数，且受导频信号结构强烈影响。

Conclusion: 算法性能达到BCRLB，为导频信号设计提供了指导。通过简单调整导频信号矩阵中0和1的排列，可以在某些时间相关性范围内使BCRLB从非单调变为单调。

Abstract: We consider time varying MIMO fading channels with known spatial and temporal
correlation and solve the problem of joint carrier frequency offset (CFO) and
channel estimation with prior distributions. The maximum a posteriori
probability (MAP) joint estimation is proved to be equivalent to a separate MAP
estimation of the CFO followed by minimum mean square error (MMSE) estimation
of the channel while treating the estimated CFO as true. The MAP solution is
useful to take advantage of the estimates from the previous data packet. A low
complexity universal CFO estimation algorithm is extended from the time
invariant case to the time varying case. Unlike past algorithms, the universal
algorithm does not need phase unwrapping to take advantage of the full range of
symbol correlation and achieves the derived Bayesian Cram\'er-Rao lower bound
(BCRLB) in almost all SNR range. We provide insight on the the relation among
the temporal correlation coefficient of the fading, the CFO estimation
performance, and the pilot signal structure. An unexpected observation is that
the BCRLB is not a monotone function of the temporal correlation and is
strongly influenced by the pilot signal structures. A simple rearrangement of
the 0's and 1's in the pilot signal matrix will render the BCRLB from being
non-monotone to being monotone in certain temporal correlation ranges. Since
the BCRLB is shown to be achieved by the proposed algorithm, it provides a
guideline for pilot signal design.

</details>


### [119] [On the Resilience of Direction-Shift Keying Against Phase Noise and Short Channel Coherence Time at mmWave Frequencies](https://arxiv.org/abs/2509.01103)
*Mohaned Chraiti,Ozgur Ercetin,Ali Ghrayeb,Ali Gorcin*

Main category: cs.IT

TL;DR: DSK技术通过方向调制而非幅度相位编码，显著延长信道相干时间4个数量级，并天然消除相位噪声，适用于毫米波和太赫兹移动通信系统。


<details>
  <summary>Details</summary>
Motivation: 解决毫米波和太赫兹系统中无线信道快速变化和相位噪声问题，传统方法需要密集导频插入，DSK技术通过方向调制提供更优解决方案。

Method: 推导M天线移动设备的最优检测器结构，建立方向相干时间(DCT)理论模型，分析其与距离/速度的关系，并与传统信道相干时间(CCT)对比。

Result: 理论证明DCT按d/v缩放，而CCT按λ/v缩放，获得d/λ量级的相干时间增益（超过4个数量级），且DSK天然消除相位噪声无需补偿。

Conclusion: DSK在高频移动环境中具有鲁棒性和可扩展性，通过仿真验证了理论预测，为下一代无线通信系统提供了重要技术路径。

Abstract: The rapid variation of the wireless channel (short channel coherence time)
and the phase noise are two prominent concerns in Millimeter-wave (mmWave) and
sub-Terahertz systems communication systems. Equalizing the channel effect and
tracking the phase noise necessitate dense pilot insertion. Direction-Shift
Keying (DSK), a recent variant of Spatial Modulation (SM), addresses these
challenges by encoding information in the Direction-of-Arrival (DoA) using a
distributed antenna system (DAS), rather than relying on amplitude or phase.
DSK has been shown to extend coherence time by up to four orders of magnitude.
Despite its promise, existing DSK studies are largely simulation-based and
limited to simplified roadside unit scenarios and mobile device (MD) equipped
with only two antennas. DSK's performance in general settings, along with the
fundamental laws governing its behavior, such as coherence time and resilience
to phase noise, remain open problems. In this paper, we derive the structure of
the optimal detector for the case of $M$-antenna MD. Then, we establish the
governing law for DSK's coherence time, termed the Direction Coherence Time
(DCT), defining the the temporal duration over which the DoA remains
approximately invariant. We analytically establish that DCT scales with $d/v$
(transmitter-receiver distance over velocity), while the Channel Coherence Time
(CCT) scales with $\lambda/v$, revealing a coherence time gain on the order of
$d/\lambda$ (equivalent to more than four orders of magnitude.) Furthermore, we
prove that DSK inherently cancels the phase noise, requiring no additional
compensation. Analytical predictions are validated through simulations,
confirming the robustness and scalability of DSK in high-frequency mobile
environments.

</details>


### [120] [From One-Dimensional Codes to Two-Dimensional Codes: A Universal Framework for the Bounded-Weight Constraint](https://arxiv.org/abs/2509.01240)
*Viet Hai Le,Thanh Phong Pham,Tuan Thanh Nguyen,Kui Cai,Kees A. Schouhamer Immink*

Main category: cs.IT

TL;DR: 本文提出了一个通用框架来设计二维有界权重约束编码，确保每行每列中1的数量不超过给定函数f(n)的限制，解决了ReRAM存储中的二维约束编码挑战。


<details>
  <summary>Details</summary>
Motivation: 随着ReRAM等存储技术的发展，信息数据被视为二维而非一维，需要引入新的二维约束来提高系统可靠性。现有研究主要关注一维约束，二维约束编码设计面临挑战。

Method: 提出了一个通用框架来设计二维有界权重约束编码，该框架利用现有的一维容量逼近编码设计，确保二维码字中每行每列的权重不超过给定的函数f(n)限制。

Result: 研究表明，如果存在容量逼近的一维编码设计，那么该方法可以为所有f=ω(log n)的函数提供容量逼近的二维编码。

Conclusion: 该工作为二进制有界权重约束编码的二维设计提供了高效解决方案，能够满足新兴存储技术对二维约束编码的需求。

Abstract: Recent developments in storage- especially in the area of resistive random
access memory (ReRAM)- are attempting to scale the storage density by regarding
the information data as two-dimensional (2D), instead of one-dimensional (1D).
Correspondingly, new types of 2D constraints are introduced into the input
information data to improve the system reliability. While 1D constraints have
been extensively investigated in the literature, the study for 2D constraints
is much less profound. Particularly, given a constraint $\mathcal{F}$ and a
design of 1D codes whose codewords satisfy $\mathcal{F}$, the problem of
constructing efficient 2D codes, such that every row and every column in every
codeword satisfy $\mathcal{F}$, has been a challenge. This work provides an
efficient solution to the challenging coding problem above for the binary
bounded-weight constrained codes that restrict the maximum number of $1$'s
(called {\em weight}). Formally, we propose a universal framework to design 2D
codes that guarantee the weight of every row and every column of length $n$ to
be at most $f(n)$ for any given function $f(n)$. We show that if there exists a
design of capacity-approaching 1D codes, then our method also provides
capacity-approaching 2D codes for all $f=\omega(\log n)$.

</details>


### [121] [Hierarchical Maximum Entropy via the Renormalization Group](https://arxiv.org/abs/2509.01424)
*Amir R. Asadi*

Main category: cs.IT

TL;DR: 该论文提出了层次最大熵框架，通过重整化群方法找到帕累托最优分布，解决了多层级统计模型的最大熵问题，并探索了具有层次不变性的简化计算场景。


<details>
  <summary>Details</summary>
Motivation: 层次结构在统计和机器学习模型以及物理系统中普遍存在，但传统的最大熵方法无法有效处理多层级约束问题，需要建立新的理论框架来统一处理这类问题。

Method: 引入层次最大熵框架，通过重整化群程序获得帕累托最优分布，构建多层级Gibbs变分原理和Donsker-Varadhan变分表示，并引入参数流概念来简化计算。

Result: 证明了帕累托最优分布可以通过重整化群方法获得，在具有层次不变性的特殊情况下（如二次模损失、对数损失、最近邻损失函数）计算效率显著提高。

Conclusion: 该工作建立了概率论、信息论和统计力学之间的联系，为处理多层级约束的最大熵问题提供了统一的理论框架和有效的计算方法。

Abstract: Hierarchical structures, which include multiple levels, are prevalent in
statistical and machine-learning models as well as physical systems. Extending
the foundational result that the maximum entropy distribution under mean
constraints is given by the exponential Gibbs-Boltzmann form, we introduce the
framework of "hierarchical maximum entropy" to address these multilevel models.
We demonstrate that Pareto optimal distributions, which maximize entropies
across all levels of hierarchical transformations, can be obtained via
renormalization-group procedures from theoretical physics. This is achieved by
formulating multilevel extensions of the Gibbs variational principle and the
Donsker-Varadhan variational representation of entropy. Moreover, we explore
settings with hierarchical invariances that significantly simplify the
renormalization-group procedures, enhancing computational efficiency: quadratic
modular loss functions, logarithmic loss functions, and nearest-neighbor loss
functions. This is accomplished through the introduction of the concept of
parameter flows, which serves as an analog to renormalization flows in
renormalization group theory. This work connects ideas from probability theory,
information theory, and statistical mechanics.

</details>


### [122] [Learning to Ask: Decision Transformers for Adaptive Quantitative Group Testing](https://arxiv.org/abs/2509.01723)
*Mahdi Soleymani,Tara Javidi*

Main category: cs.IT

TL;DR: 本文提出了一种自适应算法，首次在定量群检测(QGT)问题中突破了非自适应信息理论界限，通过将问题转化为降维整数向量恢复任务，并利用决策变换器进行强化学习求解。


<details>
  <summary>Details</summary>
Motivation: 信息理论表明自适应方法可能将查询次数减少一半，但现有算法均未能超越非自适应界限，因此需要探索自适应方法在实际中的优势。

Method: 将QGT问题转化为维度与稀疏性相关的整数向量恢复任务，然后将其建模为离线强化学习问题，使用决策变换器进行自适应求解。

Result: 实验表明，该自适应算法首次实现了平均查询次数低于非自适应信息理论界限，证明了自适应性的实际价值。

Conclusion: 自适应算法确实能够减少QGT问题所需的查询次数，为定量群检测领域提供了新的有效解决方案。

Abstract: We consider the problem of quantitative group testing (QGT), where the goal
is to recover a sparse binary vector from aggregate subset-sum queries: each
query selects a subset of indices and returns the sum of those entries.
Information-theoretic results suggest that adaptivity could yield up to a
twofold reduction in the total number of required queries, yet no algorithm has
surpassed the non-adaptive bound, leaving its practical benefit an open
question. In this paper, we reduce the QGT problem to an integer-vector
recovery task whose dimension scales with the sparsity of the original problem
rather than its full ambient size. We then formulate this reduced recovery task
as an offline reinforcement learning problem and employ Decision Transformers
to solve it adaptively. By combining these two steps, we obtain an effective
end-to-end method for solving the QGT problem. Our experiments show that, for
the first time in the literature, our adaptive algorithm reduces the average
number of queries below the well-known non-adaptive information-theoretic
bound, demonstrating that adaptivity can indeed reduce the number of queries.

</details>


### [123] [On the Analysis of Random Linear Streaming Codes in Stochastic Channels](https://arxiv.org/abs/2509.01894)
*Kai Huang,Wenjie Guan,Xiaoran Wang,Jinbei Zhang,Kechao Cai*

Main category: cs.IT

TL;DR: 本文分析了随机线性流码(RLSC)在随机符号擦除信道中的性能极限，推导了非系统RLSC在Gilbert-Elliott信道中的精确错误概率闭式表达式，并发现了系统RLSC在分组擦除信道中的反直觉现象。


<details>
  <summary>Details</summary>
Motivation: 探索大字段大小随机线性流码在随机符号擦除信道中的基本性能极限，解决现有研究主要关注i.i.d.信道而忽略状态依赖信道的问题。

Method: 针对Gilbert-Elliott信道，提出两种新技术：(1)利用状态转移轨迹的递归结构；(2)推导状态的平稳初始分布和迭代方程。针对分组擦除信道，通过案例研究分析系统RLSC的错误事件特征。

Result: 推导了非系统RLSC在Gilbert-Elliott信道中的精确错误概率闭式表达式，发现了系统RLSC在某些擦除模式下会产生意外错误事件的反直觉现象，并在i.i.d.分组擦除信道下推导了无限记忆长度和1/2码率时的精确错误概率解析表达式。

Conclusion: 所提出的分析方法能够准确评估RLSC在不同信道模型下的性能，系统RLSC虽然在某些情况下可能带来意外错误，但在分组擦除信道中仍能显著降低错误概率，仿真结果验证了理论分析的准确性。

Abstract: Random Linear Streaming Codes (RLSCs) can dramatically reduce the queuing
delay of block codes in real-time services. In this paper, we aim to explore
the fundamental limit of large-field-size RLSCs in stochastic symbol erasure
channels (SEC). The Non-systematic RLSCs (NRLSCs) in i.i.d. SEC has been
analyzed in [Pinwen Su et al. 2022]. In this work, we first derive the
closed-form expression on the exact error probability of NRLSCs in
Gilbert-Elliott symbol erasure channels (G-ESEC). Compared to i.i.d SEC, the
erasure probability of G-ESEC depends on channel state, thus transitions
between the states should be considered. To deal with the stochastic state
transitions, we introduce two novel techniques. (i) To account for the impact
of switching states on probability terms, we find and leverage the recursive
structure of the state transition traces. (ii) To obtain the expected number of
error timeslots, we derive the stationary initial distribution of the states,
and formulate iterative equation to characterize the expectation terms. Then we
analyze the Systematic RLSCs (SRLSCs) in a special SEC, i.e., the packet
erasure channel (PEC). In this scenario, SRLSCs could save some source symbols
which should have exceeded the decoding delay in NRLSCs, and thus could
significantly reduce the error probability. To this point, our contributions
are two-folds. (i) Through a case study, we find a counter-intuitive phenomenon
that SRLSCs can cause unexpected error events comparing to NRLSCs in some
erasure patterns. Then we fully characterize the error event of SRLSCs for any
erasure pattern. (ii) For i.i.d. PEC, we derive an analytical expression on
exact error probability of SRLSCs when length of memory approaches infinity and
coding rate equals to 1/2. Simulations are conducted to verify the accuracy of
our analysis and compare the performance of NRLSCs, SRLSCs, and existing
streaming codes.

</details>


### [124] [MUSE-FM: Multi-task Environment-aware Foundation Model for Wireless Communications](https://arxiv.org/abs/2509.01967)
*Tianyue Zheng,Jiajia Guo,Linglong Dai,Shi Jin,Jun Zhang*

Main category: cs.IT

TL;DR: 提出MUSE-FM多任务环境感知基础模型，通过统一的提示引导编码器-解码器架构处理无线通信中的多任务，并整合环境上下文信息提升跨场景适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有无线基础模型在处理不同通信场景下输入输出格式多样的多任务时存在局限性，缺乏统一的架构来同时处理多个任务并有效整合场景信息。

Method: 设计统一的提示引导数据编码器-解码器对来处理异构格式的数据；整合环境上下文作为多模态输入，作为环境和信道分布的先验知识；促进跨场景特征提取。

Result: 仿真结果表明MUSE-FM在各种任务上优于现有方法，提示引导的编码器-解码器对提高了新任务配置的可扩展性，环境信息的整合增强了不同场景的适应能力。

Conclusion: MUSE-FM通过统一的架构和环境信息整合，成功解决了无线通信多任务处理的挑战，在性能和适应性方面表现出色。

Abstract: Recent advancements in foundation models (FMs) have attracted increasing
attention in the wireless communication domain. Leveraging the powerful
multi-task learning capability, FMs hold the promise of unifying multiple tasks
of wireless communication with a single framework. with a single framework.
Nevertheless, existing wireless FMs face limitations in the uniformity to
address multiple tasks with diverse inputs/outputs across different
communication scenarios.In this paper, we propose a MUlti-taSk
Environment-aware FM (MUSE-FM) with a unified architecture to handle multiple
tasks in wireless communications, while effectively incorporating scenario
information.Specifically, to achieve task uniformity, we propose a unified
prompt-guided data encoder-decoder pair to handle data with heterogeneous
formats and distributions across different tasks. Besides, we integrate the
environmental context as a multi-modal input, which serves as prior knowledge
of environment and channel distributions and facilitates cross-scenario feature
extraction. Simulation results illustrate that the proposed MUSE-FM outperforms
existing methods for various tasks, and its prompt-guided encoder-decoder pair
improves the scalability for new task configurations. Moreover, the
incorporation of environment information improves the ability to adapt to
different scenarios.

</details>


### [125] [Next-Generation Sustainable Wireless Systems: Energy Efficiency Meets Environmental Impact](https://arxiv.org/abs/2509.02395)
*Christo Kurisummoottil Thomas,Omar Hashash,Kimia Ehsani,Walid Saad*

Main category: cs.IT

TL;DR: 提出一种新的可持续性指标（每比特排放），并使用多目标强化学习来优化6G网络的能消耗、延迟和碳排放，实现了比现有方法减少76%的平均排放量。


<details>
  <summary>Details</summary>
Motivation: 现有的无线网络可持续性研究仅限于能消耗优化，无法审视系统的环境影响。需要通过新设计和指标将可持续性原生集成到下一代网络中。

Method: 提出了一种新的可持续性指标（每比特排放），将资源分配问题形成多目标优化问题，并使用多目标强化学习（MORL）来求解非凸问题，实现希尔顿最优的多目标平衡。

Result: 模拟结果显示，该方法能够将平均每比特排放量降低约26%，较现有方法显著改善。方法同时在能消耗、延迟和碳排放之间实现了平衡。

Conclusion: 该研究通过新的可持续性指标和MORL方法，成功将环境可持续性集成到6G网络运作中，实现了环境发展与网络性能的希尔顿最优平衡，为下一代网络的绿色发展提供了有效解决方案。

Abstract: Aligning with the global mandates pushing towards advanced technologies with
reduced resource consumption and environmental impacts, the sustainability of
wireless networks becomes a significant concern in 6G systems. To address this
concern, a native integration of sustainability into the operations of
next-generation networks through novel designs and metrics is necessary.
Nevertheless, existing wireless sustainability efforts remain limited to
energy-efficient network designs which fail to capture the environmental impact
of such systems. In this paper, a novel sustainability metric is proposed that
captures emissions per bit, providing a rigorous measure of the environmental
foot- print associated with energy consumption in 6G networks. This metric also
captures how energy, computing, and communication resource parameters influence
the reduction of emissions per bit. Then, the problem of allocating the energy,
computing and communication resources is posed as a multi-objective (MO)
optimization problem. To solve the resulting non-convex problem, our framework
leverages MO reinforcement learning (MORL) to maximize the novel sustainability
metric alongside minimizing energy consumption and average delays in
successfully delivering the data, all while adhering to constraints on energy
resource capacity. The proposed MORL methodology computes a global policy that
achieves a Pareto-optimal tradeoff among multiple objectives, thereby balancing
environmental sustainability with network performance. Simulation results show
that the proposed approach reduces the average emissions per bit by around 26%
compared to state-of-the-art methods that do not explicitly integrate carbon
emissions into their control objectives.

</details>


### [126] [Practical Channel Estimation for Pinching-Antenna Systems: Serial vs. Parallel and Downlink vs. Uplink?](https://arxiv.org/abs/2509.02403)
*Jian Xiao*

Main category: cs.IT

TL;DR: 研究pinching-antenna系统上行链路信道估计，比较串行和并行两种天线激活协议的性能差异，分析上下行信道估计的共性与不对称性。


<details>
  <summary>Details</summary>
Motivation: 解决实际pinching-antenna系统中信道估计的挑战，特别是在考虑电磁兼容的波导传输模型下，如何平衡数值稳定性和阵列增益的问题。

Method: 提出基于电磁兼容波导传输模型的两种天线激活协议：串行协议（逐个天线激活）和并行协议（二进制S矩阵激活），并进行理论分析和数值仿真。

Result: 1) 理想无损耗模型中并行协议优于串行协议；2) 实际有损耗模型中串行协议更优；3) 下行信道估计适合串行协议，上行适合并行协议。

Conclusion: pinching-antenna系统的信道估计需要根据具体场景选择协议，串行协议数值稳定性好但无阵列增益，并行协议有阵列增益但受串扰和泄漏影响，上下行存在不对称性。

Abstract: The practical channel estimation in uplink pinching-antenna systems is
investigated, in which an electromagnetic-compliant in-waveguide transmission
model is exhibited, incorporating both bidirectional power splitting,
cumulative power leakage, and waveguide attenuation. Based on this model, the
paper investigates two antenna activation protocols for channel estimation: a
serial protocol based on one-by-one antenna activation and a parallel protocol
utilizing a binary S-Matrix activation. The serial protocol is characterized by
its superior numerical stability but a lack of array gain, whereas the parallel
protocol theoretically offers array gain but suffers from severe performance
degradation due to structural crosstalk from the non-orthogonal S-Matrix and
ill-conditioning from cumulative leakage. Furthermore, the paper analyzes the
fundamental commonalities and asymmetries between uplink and downlink channel
estimation in pinching-antenna systems. Numerical results demonstrate that 1)
in an ideal lossless model, the parallel protocol is superior to the serial
protocol due to the array gain from simultaneous energy collection in uplink
transmission; 2) in a practical model with physical losses, the serial protocol
outperforms the parallel protocol, as the performance of the parallel protocol
is degraded by the numerical instability from cumulative leakage, which
outweighs the benefit of array gain; 3) For downlink channel estimation, the
serial protocol is more suitable because it avoids bidirectional power
splitting, while the parallel protocol is more suitable for the uplink as it
can make full use of array gain.

</details>


### [127] [Feasibility Guaranteed Learning-to-Optimize in Wireless Communication Resource Allocation](https://arxiv.org/abs/2509.02417)
*Hanwen Zhang,Haijian Sun*

Main category: cs.IT

TL;DR: 本文综述6G时代下使用学习优化(L2O)技术来解决大规模无线资源分配问题，重点关注可行性保证方法和应用实践。


<details>
  <summary>Details</summary>
Motivation: 6G通信带来大规模边缘设备访问和实时智能服务需求，但传统优化算法面临计算性挑战，需要更高效的解决方案。

Method: 综述了学习优化(L2O)模型设计和可行性保障技术，研究了约束L2O在无线资源分配系统中的应用，并通过权重总和率问题案例进行性能测评。

Result: 学习基于方法能够直接从系统参数学习到可行且近优解的映射，提供了比传统求解器更高效的替代方案。

Conclusion: 本文确立了L2O技术在流行性保证方面的重要性，并指出了关键挑战和未来研究方向，为6G时代大规模无线资源分配问题提供了有效解决途径。

Abstract: The emergence of 6G wireless communication enables massive edge device access
and supports real-time intelligent services such as the Internet of things
(IoT) and vehicle-to-everything (V2X). However, the surge in edge devices
connectivity renders wireless resource allocation (RA) tasks as large-scale
constrained optimization problems, whereas the stringent real-time requirement
poses significant computational challenge for traditional algorithms. To
address the challenge, feasibility guaranteed learning-to-optimize (L2O)
techniques have recently gained attention. These learning-based methods offer
efficient alternatives to conventional solvers by directly learning mappings
from system parameters to feasible and near-optimal solutions. This article
provide a comprehensive review of L2O model designs and feasibility enforcement
techniques and investigates the application of constrained L2O in wireless RA
systems and. The paper also presents a case study to benchmark different L2O
approaches in weighted sum rate problem, and concludes by identifying key
challenges and future research directions.

</details>


### [128] [A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device Networks](https://arxiv.org/abs/2509.02532)
*Rashid Ummer N. T.,K. K. Krishnan Namboodiri,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 本文提出了一种新的编码缓存方案，用于部分协作的设备到设备（D2D）网络，能够在所有可行内存段中工作，而不受自私用户数量的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的部分协作D2D编码缓存方案都限于高内存段，尤其是当自私用户数量较多时。需要一种新的方案来解决这个限制，以支持所有可行的内存段。

Method: 提出了一种新的编码缓存方案，该方案不需要在内容放置阶段了解自私用户的身份，并能够在所有可行内存段中工作。同时还推导了部分协作D2D编码缓存方案的传输负载下界。

Result: 提出的方案在高内存段被证明是最优的，而且能够在任何自私用户数量下在所有可行内存段中工作。

Conclusion: 该研究为部分协作D2D网络提供了一种更灵活和高效的编码缓存解决方案，充分利用了D2D通信的优势，在保持缓存效率的同时应对了自私用户带来的挑战。

Abstract: Device-to-device (D2D) communication is one of the most promising techniques
for future wireless cellular communication systems. This paper considers coded
caching in a partially cooperative wireless D2D network, where only a subset of
users transmit during delivery, while all users request files. The
non-transmitting users are referred to as selfish users. All existing schemes
that do not require knowledge of the identity of selfish users before content
placement are limited to the high-memory regime, particularly when the number
of selfish users is large. We propose a novel coded caching scheme for a
partially cooperative D2D network that operates in all feasible memory regimes,
regardless of the number of selfish users. We also derive a lower bound on the
transmission load of a partially cooperative D2D coded caching scheme. Using
this bound, the proposed scheme is shown to be optimal in the high-memory
regime.

</details>
