<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [WiFiSim: Simulating WiFi Probe Requests via AOSP Analysis and Device Behavior Modeling](https://arxiv.org/abs/2509.15501)
*Lifei Hao,Yue Cheng,Min Wang,Bing Jia,Baoqi Huang*

Main category: cs.NI

TL;DR: WiFiSim是一个仿真框架，通过分析Android开源项目协议和有限状态设备行为建模来重建WiFi探测请求生成，解决了MAC地址随机化和标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: WiFi探测请求帧编码了细粒度的设备交互，是移动性和人群分析的关键基础，但普遍的MAC地址随机化和标注数据集稀缺阻碍了基于PR的研究进展。

Method: 通过Android开源项目协议分析和有限状态设备行为建模来重建PR生成，识别PR结构和时序的关键决定因素，并捕捉真实的用户驱动状态转换。

Result: 实验显示WiFiSim在分布和时间动态方面的偏差小于5%，能够扩展到大规模数据集合成，并支持下游应用的可靠评估。

Conclusion: WiFiSim框架能够有效解决WiFi探测请求研究中的数据稀缺问题，为可重复研究提供支持，源代码和样本数据集已公开发布。

Abstract: WiFi probe request (PR) frames encode fine-grained device interactions and
serve as a critical basis for mobility and crowd analytics. However, pervasive
MAC address randomization and the scarcity of labeled datasets hinder progress
in PR-based studies. We introduce WiFiSim, a simulation framework that
reconstructs PR generation through Android Open Source Project (AOSP) protocol
analysis and finite-state device behavior modeling. WiFiSim identifies the key
determinants of PR structure and timing while capturing realistic user-driven
state transitions. Experiments show that WiFiSim achieves less than 5%
deviation from real measurements in both distributional and temporal dynamics,
scales to large-scale dataset synthesis, and enables reliable evaluation of
downstream applications. Source code and sample datasets are publicly released
to foster reproducible research.

</details>


### [2] [Smart Interrupted Routing Based on Multi-head Attention Mask Mechanism-Driven MARL in Software-defined UASNs](https://arxiv.org/abs/2509.15856)
*Zhenyu Wang,Chuan Lin,Guangjie Han,Shengchao Zhu,Ruoyuan Wu,Tongwei Zhang*

Main category: cs.NI

TL;DR: 提出了一种基于软件定义网络和强化学习的智能中断路由方案（ISURL），用于水下声学传感器网络，通过MA-MAPPO算法实现自适应路由和中断恢复，提高数据收集效率。


<details>
  <summary>Details</summary>
Motivation: 水下声学传感器网络在海洋环境监测、灾害预警等方面具有重要作用，但水下恶劣条件（高延迟、有限带宽、动态拓扑）使得高效路由决策具有挑战性。

Method: 首先建模水下噪声影响，然后提出基于SDN的ISURL框架，结合MA-MAPPO算法（集成多头注意力掩码机制）和MA-MAPPO_i中断策略，实现自适应路由和智能中断决策。

Result: 评估表明，该路由方案能够实现精确的水下数据路由决策，比现有方法具有更快的收敛速度和更低的路由延迟。

Conclusion: 所提出的智能中断路由方案有效解决了水下动态环境中的路由挑战，为UASNs提供了高效可靠的数据收集解决方案。

Abstract: Routing-driven timely data collection in Underwater Acoustic Sensor Networks
(UASNs) is crucial for marine environmental monitoring, disaster warning and
underwater resource exploration, etc. However, harsh underwater conditions,
including high delays, limited bandwidth, and dynamic topologies - make
efficient routing decisions challenging in UASNs. In this paper, we propose a
smart interrupted routing scheme for UASNs to address dynamic underwater
challenges. We first model underwater noise influences from real underwater
routing features, e.g., turbulence and storms. We then propose a
Software-Defined Networking (SDN)-based Interrupted Software-defined UASNs
Reinforcement Learning (ISURL) framework which ensures adaptive routing through
dynamically failure handling (e.g., energy depletion of sensor nodes or link
instability) and real-time interrupted recovery. Based on ISURL, we propose
MA-MAPPO algorithm, integrating multi-head attention mask mechanism with MAPPO
to filter out infeasible actions and streamline training. Furthermore, to
support interrupted data routing in UASNs, we introduce MA-MAPPO_i, MA-MAPPO
with interrupted policy, to enable smart interrupted routing decision in UASNs.
The evaluations demonstrate that our proposed routing scheme achieves exact
underwater data routing decision with faster convergence speed and lower
routing delays than existing approaches.

</details>


### [3] [A Robust Scheduling of Cyclic Traffic for Integrated Wired and Wireless Time-Sensitive Networks](https://arxiv.org/abs/2509.15930)
*Özgür Ozan Kaynak,Andreas Kassler,Andreas Fischer,Ognjen Dobrijevic,Fabio D'Andreagiovanni*

Main category: cs.NI

TL;DR: 本文提出了一种针对时间敏感网络(TSN)中时间感知整形(TAS)配置的新方法，特别关注无线网络的随机性挑战，通过线性规划和启发式算法实现鲁棒调度。


<details>
  <summary>Details</summary>
Motivation: TSN的时间感知整形需要计算全网流量调度，但在集成无线网络(如5G、Wi-Fi)中，由于无线链路的随机性，这一配置变得特别困难。

Method: 提出了一种线性规划方法计算全网时间感知调度，对无线性能不确定性具有鲁棒性；同时提出了多项式时间运行的顺序批量调度启发式算法来降低计算复杂度。

Result: 在不同网络拓扑和无线链路特性下评估，启发式算法能在大型拓扑中调度90%的6500个请求TSN流。

Conclusion: 该方法有效解决了无线TSN网络中的调度挑战，通过可调鲁棒性参数实现了对无线链路抖动的鲁棒性。

Abstract: Time-Sensitive Networking (TSN) is a toolbox of technologies that enable
deterministic communication over Ethernet. A key area has been TSN's time-aware
traffic shaping (TAS), which supports stringent end-to-end latency and
reliability requirements. Configuration of TAS requires the computation of a
network-wide traffic schedule, which is particularly challenging with
integrated wireless networks (e.g., 5G, Wi-Fi) due to the stochastic nature of
wireless links. This paper introduces a novel method for configuring TAS,
focusing on cyclic traffic patterns and jitter of wireless links. We formulate
a linear program that computes a network-wide time-aware schedule, robust to
wireless performance uncertainties. The given method enables robust scheduling
of multiple TSN frames per transmission window using a tunable robustness
parameter ({\Gamma}). To reduce computational complexity, we also propose a
sequential batch-scheduling heuristic that runs in polynomial time. Our
approach is evaluated by using different network topologies and wireless link
characteristics, demonstrating that the heuristic can schedule 90% of 6500
requested TSN streams in a large topology.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI](https://arxiv.org/abs/2509.15291)
*Federico Taschin,Abderrahmane Lazaraq,Ozan K. Tonguz,Inci Ozgunes*

Main category: cs.AI

TL;DR: 本文评估了MetaLight这一最先进的元强化学习方法在交通信号控制中的应用，发现虽然在某些条件下表现良好，但在其他条件下可能表现不佳（误差高达22%），表明元强化学习方案往往不够鲁棒，甚至可能带来重大可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和人工智能在智能交通网络中的应用增加，强化学习被证明是一种有前景的方法。然而，由于输入数据分布相对于训练数据分布的动态变化，训练后的强化学习代理在交通信号控制中的可靠性存在问题，这可能导致不良后果。

Method: 本文评估和分析了一种名为MetaLight的最先进的元强化学习方法，该方法旨在解决强化学习在动态变化环境中的可靠性问题。

Result: 研究发现，在某些条件下MetaLight确实能产生相当好的结果，但在其他条件下可能表现不佳，误差高达22%，表明元强化学习方案往往不够鲁棒。

Conclusion: 元强化学习方案在交通信号控制中可能不够稳健，甚至可能带来重大可靠性问题，需要进一步改进以确保在实际应用中的可靠性。

Abstract: The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart
transportation networks has increased significantly in the last few years.
Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to
be a very promising approach by several authors. However, a problem with using
Reinforcement Learning in Traffic Signal Control is the reliability of the
trained RL agents due to the dynamically changing distribution of the input
data with respect to the distribution of the data used for training. This
presents a major challenge and a reliability problem for the trained network of
AI agents and could have very undesirable and even detrimental consequences if
a suitable solution is not found. Several researchers have tried to address
this problem using different approaches. In particular, Meta Reinforcement
Learning (Meta RL) promises to be an effective solution. In this paper, we
evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and
show that, while under certain conditions MetaLight can indeed lead to
reasonably good results, under some other conditions it might not perform well
(with errors of up to 22%), suggesting that Meta RL schemes are often not
robust enough and can even pose major reliability problems.

</details>


### [5] [MICA: Multi-Agent Industrial Coordination Assistant](https://arxiv.org/abs/2509.15237)
*Di Wen,Kunyu Peng,Junwei Zheng,Yufan Chen,Yitain Shi,Jiale Wei,Ruiping Liu,Kailun Yang,Rainer Stiefelhagen*

Main category: cs.AI

TL;DR: MICA是一个多代理工业协调助手系统，通过语音交互提供实时指导，在有限计算和隐私约束下运行，协调五个角色专业化的语言代理，确保准确合规的支持。


<details>
  <summary>Details</summary>
Motivation: 工业工作流程需要能够在有限计算、连接性和严格隐私约束下运行的适应性和可信赖的辅助系统。

Method: MICA协调五个角色专业化的语言代理，采用自适应步骤融合（ASF）技术动态融合专家推理与自然语音反馈的在线适应，并建立多代理协调基准和评估指标。

Result: 实验表明MICA在任务成功率、可靠性和响应性方面持续优于基线结构，同时可在实际离线硬件上部署。

Conclusion: MICA是实现可部署、隐私保护的多代理助手在动态工厂环境中应用的重要一步。

Abstract: Industrial workflows demand adaptive and trustworthy assistance that can
operate under limited computing, connectivity, and strict privacy constraints.
In this work, we present MICA (Multi-Agent Industrial Coordination Assistant),
a perception-grounded and speech-interactive system that delivers real-time
guidance for assembly, troubleshooting, part queries, and maintenance. MICA
coordinates five role-specialized language agents, audited by a safety checker,
to ensure accurate and compliant support. To achieve robust step understanding,
we introduce Adaptive Step Fusion (ASF), which dynamically blends expert
reasoning with online adaptation from natural speech feedback. Furthermore, we
establish a new multi-agent coordination benchmark across representative task
categories and propose evaluation metrics tailored to industrial assistance,
enabling systematic comparison of different coordination topologies. Our
experiments demonstrate that MICA consistently improves task success,
reliability, and responsiveness over baseline structures, while remaining
deployable on practical offline hardware. Together, these contributions
highlight MICA as a step toward deployable, privacy-preserving multi-agent
assistants for dynamic factory environments. The source code will be made
publicly available at https://github.com/Kratos-Wen/MICA.

</details>


### [6] [KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems](https://arxiv.org/abs/2509.15239)
*Stjepan Požgaj,Dobrik Georgiev,Marin Šilić,Petar Veličković*

Main category: cs.AI

TL;DR: 该论文提出了一种神经算法推理器，用于解决标准NAR基准中缺失的背包问题，采用动态规划监督的两阶段方法，相比直接预测基线具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 神经算法推理(NAR)领域旨在将算法逻辑嵌入神经网络，但标准基准中缺少背包问题这一连接经典算法和组合优化的伪多项式问题。

Method: 设计了两阶段管道：首先构建动态规划表，然后从中重建解决方案，通过动态规划监督对中间状态进行建模。

Result: 该方法在较大问题实例上比直接预测基线（仅从问题输入中选择最优子集）具有更好的泛化性能。

Conclusion: 通过模仿经典算法的两阶段动态规划方法，神经算法推理器能够有效解决背包问题并实现良好的泛化能力。

Abstract: Neural algorithmic reasoning (NAR) is a growing field that aims to embed
algorithmic logic into neural networks by imitating classical algorithms. In
this extended abstract, we detail our attempt to build a neural algorithmic
reasoner that can solve Knapsack, a pseudo-polynomial problem bridging
classical algorithms and combinatorial optimisation, but omitted in standard
NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow
the two-phase pipeline for the Knapsack problem, which involves first
constructing the dynamic programming table and then reconstructing the solution
from it. The approach, which models intermediate states through dynamic
programming supervision, achieves better generalization to larger problem
instances than a direct-prediction baseline that attempts to select the optimal
subset only from the problem inputs.

</details>


### [7] [An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature](https://arxiv.org/abs/2509.15292)
*Abhiyan Dhakal,Kausik Paudel,Sanjog Sigdel*

Main category: cs.AI

TL;DR: 提出了一种基于语义相似度的自动化文献综述流程，使用transformer嵌入和余弦相似度来最小化开销并提高相关性


<details>
  <summary>Details</summary>
Motivation: 传统系统综述方法或基于优化的方法存在开销大、效率低的问题，需要一种更高效、更相关的自动化文献综述工具

Method: 使用transformer模型生成嵌入向量，通过余弦相似度计算语义相似性，结合统计阈值方法过滤相关论文

Result: 评估了三种嵌入模型，系统显示出作为可扩展实用工具的潜力，适用于初步研究和探索性分析

Conclusion: 尽管缺乏启发式反馈或真实相关性标签，但提出的系统在可扩展性和实用性方面表现出前景

Abstract: We propose an automated pipeline for performing literature reviews using
semantic similarity. Unlike traditional systematic review systems or
optimization based methods, this work emphasizes minimal overhead and high
relevance by using transformer based embeddings and cosine similarity. By
providing a paper title and abstract, it generates relevant keywords, fetches
relevant papers from open access repository, and ranks them based on their
semantic closeness to the input. Three embedding models were evaluated. A
statistical thresholding approach is then applied to filter relevant papers,
enabling an effective literature review pipeline. Despite the absence of
heuristic feedback or ground truth relevance labels, the proposed system shows
promise as a scalable and practical tool for preliminary research and
exploratory analysis.

</details>


### [8] [Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling](https://arxiv.org/abs/2509.15336)
*Humam Kourani,Anton Antonov,Alessandro Berti,Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 本文研究大型语言模型在知识驱动幻觉方面的风险，即在自动化流程建模任务中，模型可能基于内部知识而非提供的证据生成输出。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能够利用预训练知识处理模糊输入，但这种能力可能导致模型输出与明确证据相矛盾，产生知识驱动幻觉。

Method: 通过在业务流程管理领域进行受控实验，设计标准流程和故意非典型流程的输入，测量模型对提供证据的忠实度。

Result: 实验揭示了LLMs在面临证据与背景知识冲突时，倾向于依赖内部知识而非遵循明确证据的问题。

Conclusion: 研究提出了评估这一可靠性问题的方法论，并强调在证据驱动领域需要对AI生成产物进行严格验证的重要性。

Abstract: The utility of Large Language Models (LLMs) in analytical tasks is rooted in
their vast pre-trained knowledge, which allows them to interpret ambiguous
inputs and infer missing information. However, this same capability introduces
a critical risk of what we term knowledge-driven hallucination: a phenomenon
where the model's output contradicts explicit source evidence because it is
overridden by the model's generalized internal knowledge. This paper
investigates this phenomenon by evaluating LLMs on the task of automated
process modeling, where the goal is to generate a formal business process model
from a given source artifact. The domain of Business Process Management (BPM)
provides an ideal context for this study, as many core business processes
follow standardized patterns, making it likely that LLMs possess strong
pre-trained schemas for them. We conduct a controlled experiment designed to
create scenarios with deliberate conflict between provided evidence and the
LLM's background knowledge. We use inputs describing both standard and
deliberately atypical process structures to measure the LLM's fidelity to the
provided evidence. Our work provides a methodology for assessing this critical
reliability issue and raises awareness of the need for rigorous validation of
AI-generated artifacts in any evidence-based domain.

</details>


### [9] [Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context](https://arxiv.org/abs/2509.15366)
*Andrejs Sorstkins,Josh Bailey,Dr Alistair Baron*

Main category: cs.AI

TL;DR: 提出一个用于评估和改善LLM智能体专家行为的诊断框架，通过黄金数据集、银数据集和智能体评判器来发现认知失败并指导改进


<details>
  <summary>Details</summary>
Motivation: 传统评估方法无法有效诊断具有随机性和多步决策过程的LLM智能体性能，需要新的框架来促进专家行为向LLM智能体的转移

Method: 集成黄金专家注释数据集、通过行为突变生成的银数据集，以及基于LLM的智能体评判器进行评分和针对性改进建议，并将建议嵌入向量化推荐图中

Result: 在多智能体招聘助手系统上验证，发现潜在的认知失败（如偏见措辞、提取漂移、工具误路由），同时引导智能体达到专家级推理和风格

Conclusion: 为随机性、工具增强的LLM智能体建立了标准化、可复现的专家行为转移基础，从静态评估转向主动的专家系统优化

Abstract: The rapid evolution of neural architectures - from multilayer perceptrons to
large-scale Transformer-based models - has enabled language models (LLMs) to
exhibit emergent agentic behaviours when equipped with memory, planning, and
external tool use. However, their inherent stochasticity and multi-step
decision processes render classical evaluation methods inadequate for
diagnosing agentic performance. This work introduces a diagnostic framework for
expert systems that not only evaluates but also facilitates the transfer of
expert behaviour into LLM-powered agents. The framework integrates (i) curated
golden datasets of expert annotations, (ii) silver datasets generated through
controlled behavioural mutation, and (iii) an LLM-based Agent Judge that scores
and prescribes targeted improvements. These prescriptions are embedded into a
vectorized recommendation map, allowing expert interventions to propagate as
reusable improvement trajectories across multiple system instances. We
demonstrate the framework on a multi-agent recruiter-assistant system, showing
that it uncovers latent cognitive failures - such as biased phrasing,
extraction drift, and tool misrouting - while simultaneously steering agents
toward expert-level reasoning and style. The results establish a foundation for
standardized, reproducible expert behaviour transfer in stochastic,
tool-augmented LLM agents, moving beyond static evaluation to active expert
system refinement.

</details>


### [10] [FragmentRetro: A Quadratic Retrosynthetic Method Based on Fragmentation Algorithms](https://arxiv.org/abs/2509.15409)
*Yu Shee,Anthony M. Smaldone,Anton Morgunov,Gregory W. Kyro,Victor S. Batista*

Main category: cs.AI

TL;DR: FragmentRetro是一种新的逆合成方法，利用BRICS和r-BRICS碎片化算法，结合库存感知探索和模式指纹筛选，实现二次方复杂度，解决了传统树搜索方法的指数复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 传统树搜索方法在逆合成中面临指数计算复杂度问题，限制了计算机辅助合成规划的效率。需要开发更高效的算法来应对复杂分子的合成规划挑战。

Method: 使用BRICS和r-BRICS碎片化算法，结合库存感知探索和模式指纹筛选，递归组合分子碎片并验证其在构建块集中的存在性，提供碎片组合作为逆合成解决方案。

Result: FragmentRetro实现O(h²)复杂度，远优于树搜索的O(b^h)和DirectMultiStep的O(h⁶)。在PaRoutes、USPTO-190和天然产物测试中表现出高解决率和竞争力运行时间。

Conclusion: FragmentRetro虽然专注于高效识别基于碎片的解决方案而非完整反应路径，但其计算优势和生成战略起始候选物的能力使其成为可扩展自动化合成规划的强大基础组件。

Abstract: Retrosynthesis, the process of deconstructing a target molecule into simpler
precursors, is crucial for computer-aided synthesis planning (CASP). Widely
adopted tree-search methods often suffer from exponential computational
complexity. In this work, we introduce FragmentRetro, a novel retrosynthetic
method that leverages fragmentation algorithms, specifically BRICS and r-BRICS,
combined with stock-aware exploration and pattern fingerprint screening to
achieve quadratic complexity. FragmentRetro recursively combines molecular
fragments and verifies their presence in a building block set, providing sets
of fragment combinations as retrosynthetic solutions. We present the first
formal computational analysis of retrosynthetic methods, showing that tree
search exhibits exponential complexity $O(b^h)$, DirectMultiStep scales as
$O(h^6)$, and FragmentRetro achieves $O(h^2)$, where $h$ represents the number
of heavy atoms in the target molecule and $b$ is the branching factor for tree
search. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate
that FragmentRetro achieves high solved rates with competitive runtime,
including cases where tree search fails. The method benefits from fingerprint
screening, which significantly reduces substructure matching complexity. While
FragmentRetro focuses on efficiently identifying fragment-based solutions
rather than full reaction pathways, its computational advantages and ability to
generate strategic starting candidates establish it as a powerful foundational
component for scalable and automated synthesis planning.

</details>


### [11] [Stress Testing Deliberative Alignment for Anti-Scheming Training](https://arxiv.org/abs/2509.15541)
*Bronson Schoen,Evgenia Nitishinskaya,Mikita Balesni,Axel Højmark,Felix Hofstätter,Jérémy Scheurer,Alexander Meinke,Jason Wolfe,Teun van der Weij,Alex Lloyd,Nicholas Goldowsky-Dill,Angela Fan,Andrei Matveiakin,Rusheb Shah,Marcus Williams,Amelia Glaese,Boaz Barak,Wojciech Zaremba,Marius Hobbhahn*

Main category: cs.AI

TL;DR: 该论文提出了一种评估和减轻AI系统潜在"阴谋"行为的方法，通过测试分布外任务的阴谋倾向、评估情境意识的影响，以及检查对预存错误目标的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高度能力的AI系统可能会秘密追求未对齐的目标（称为"阴谋"），由于阴谋AI会故意隐藏其未对齐的目标和行为，需要不同于传统ML的策略来测量和减轻阴谋。

Method: 使用"隐蔽行动"作为阴谋的代理指标，设计评估方法，并通过26个分布外评估（180+环境）测试审议对齐作为反阴谋干预的案例研究。

Result: 审议对齐显著降低了隐蔽行动率（OpenAI o3：13%→0.4%），但未能完全消除。模型能够大部分阻止预训练隐藏目标的追求，但红队测试后仍发现不当行为。

Conclusion: 模型的情境意识影响隐蔽行为，观察到的隐蔽行动率降低可能部分由情境意识驱动。需要进一步研究针对阴谋的对齐缓解措施，特别是本文未涉及的欺骗性对齐的对抗情况。

Abstract: Highly capable AI systems could secretly pursue misaligned goals -- what we
call "scheming". Because a scheming AI would deliberately try to hide its
misaligned goals and actions, measuring and mitigating scheming requires
different strategies than are typically used in ML. We propose that assessing
anti-scheming interventions requires at least (1) testing propensity to scheme
on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming
is driven by situational awareness, and (3) checking for robustness to
pre-existing misaligned goals. We use a broad category of "covert actions" --
such as secretly breaking rules or intentionally underperforming in tests -- as
a proxy for scheming, and design evaluations for covert actions. We then
stress-test deliberative alignment as a case study for anti-scheming. Across 26
OOD evaluations (180+ environments), deliberative alignment reduces covert
action rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our
mitigation is also able to largely stop agents from pursuing a hidden goal
previously trained into the model, but we still find misbehavior after
additional red-teaming. We find that models' chain-of-thought (CoT) often
demonstrates awareness of being evaluated for alignment, and show causal
evidence that this awareness decreases covert behavior, while unawareness
increases it. Therefore, we cannot exclude that the observed reductions in
covert action rates are at least partially driven by situational awareness.
While we rely on human-legible CoT for training, studying situational
awareness, and demonstrating clear evidence of misalignment, our ability to
rely on this degrades as models continue to depart from reasoning in standard
English. We encourage research into alignment mitigations for scheming and
their assessment, especially for the adversarial case of deceptive alignment,
which this paper does not address.

</details>


### [12] [MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents](https://arxiv.org/abs/2509.15635)
*Pan Tang,Shixiang Tang,Huanqi Pu,Zhiqing Miao,Zhixing Wang*

Main category: cs.AI

TL;DR: MicroRCA-Agent是一个基于大语言模型代理的微服务根因分析系统，通过多模态数据融合实现智能故障定位，在复杂微服务故障场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决微服务环境中大规模日志处理和故障根因定位的挑战，利用大语言模型的多模态理解能力提升故障分析效率。

Method: 结合预训练Drain日志解析算法和多级数据过滤机制压缩日志；采用隔离森林无监督学习和状态码验证的双异常检测方法；设计统计对称比过滤机制和两阶段LLM分析策略进行全栈现象总结；利用跨模态提示深度整合多模态异常信息。

Result: 在复杂微服务故障场景中取得优异性能，最终得分为50.71。消融研究验证了各模态数据的互补价值和系统架构的有效性。

Conclusion: MicroRCA-Agent通过创新的多模态数据融合和LLM分析策略，为微服务根因分析提供了有效的解决方案，代码已开源。

Abstract: This paper presents MicroRCA-Agent, an innovative solution for microservice
root cause analysis based on large language model agents, which constructs an
intelligent fault root cause localization system with multimodal data fusion.
The technical innovations are embodied in three key aspects: First, we combine
the pre-trained Drain log parsing algorithm with multi-level data filtering
mechanism to efficiently compress massive logs into high-quality fault
features. Second, we employ a dual anomaly detection approach that integrates
Isolation Forest unsupervised learning algorithms with status code validation
to achieve comprehensive trace anomaly identification. Third, we design a
statistical symmetry ratio filtering mechanism coupled with a two-stage LLM
analysis strategy to enable full-stack phenomenon summarization across
node-service-pod hierarchies. The multimodal root cause analysis module
leverages carefully designed cross-modal prompts to deeply integrate multimodal
anomaly information, fully exploiting the cross-modal understanding and logical
reasoning capabilities of large language models to generate structured analysis
results encompassing fault components, root cause descriptions, and reasoning
trace. Comprehensive ablation studies validate the complementary value of each
modal data and the effectiveness of the system architecture. The proposed
solution demonstrates superior performance in complex microservice fault
scenarios, achieving a final score of 50.71. The code has been released at:
https://github.com/tangpan360/MicroRCA-Agent.

</details>


### [13] [CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair](https://arxiv.org/abs/2509.15690)
*Weixuan Sun,Jucai Zhai,Dengfeng Liu,Xin Zhang,Xiaojun Wu,Qiaobo Hao,AIMgroup,Yang Fang,Jiuyang Tang*

Main category: cs.AI

TL;DR: 本文提出了CCrepair框架，通过构建大规模C++编译错误数据集和引入强化学习范式，解决了传统方法在生成语义正确补丁方面的局限性。


<details>
  <summary>Details</summary>
Motivation: C++编译错误的自动修复对开发效率至关重要，但面临数据集稀缺和传统监督方法无法生成语义正确补丁的挑战。

Method: 1) 构建CCrepair大规模C++编译错误数据集；2) 提出基于混合奖励信号的强化学习范式；3) 建立两阶段评估系统，使用LLM作为评判器。

Result: RL训练的Qwen2.5-1.5B-Instruct模型性能达到与Qwen2.5-14B-Instruct相当的水平，验证了训练范式的效率。

Conclusion: 该工作为研究社区提供了有价值的新数据集和更有效的训练评估范式，为开发更实用可靠的自动化编程助手铺平了道路。

Abstract: The automated repair of C++ compilation errors presents a significant
challenge, the resolution of which is critical for developer productivity.
Progress in this domain is constrained by two primary factors: the scarcity of
large-scale, high-fidelity datasets and the limitations of conventional
supervised methods, which often fail to generate semantically correct
patches.This paper addresses these gaps by introducing a comprehensive
framework with three core contributions. First, we present CCrepair, a novel,
large-scale C++ compilation error dataset constructed through a sophisticated
generate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL)
paradigm guided by a hybrid reward signal, shifting the focus from mere
compilability to the semantic quality of the fix. Finally, we establish the
robust, two-stage evaluation system providing this signal, centered on an
LLM-as-a-Judge whose reliability has been rigorously validated against the
collective judgments of a panel of human experts. This integrated approach
aligns the training objective with generating high-quality, non-trivial patches
that are both syntactically and semantically correct. The effectiveness of our
approach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct
model achieved performance comparable to a Qwen2.5-14B-Instruct model,
validating the efficiency of our training paradigm. Our work provides the
research community with a valuable new dataset and a more effective paradigm
for training and evaluating robust compilation repair models, paving the way
for more practical and reliable automated programming assistants.

</details>


### [14] [A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation](https://arxiv.org/abs/2509.15730)
*Lukas Laakmann,Seyyid A. Ciftci,Christian Janiesch*

Main category: cs.AI

TL;DR: 本文通过文献综述探讨了机器人流程自动化（RPA）与机器学习的联系，并构建了智能RPA的分类体系，包含RPA-ML集成和RPA-ML交互两个元特征，共八个维度。


<details>
  <summary>Details</summary>
Motivation: RPA虽然能够有效自动化基于规则的、结构良好的任务，但其符号性质在处理更复杂任务时存在固有局限性。机器学习概念为扩展可自动化任务范围提供了机会。

Method: 通过文献综述方法，分析RPA与机器学习的联系，并构建智能RPA的分类体系。

Result: 提出了包含两个元特征（RPA-ML集成和RPA-ML交互）和八个维度（架构与生态系统、能力、数据基础、智能水平、技术集成深度、部署环境、生命周期阶段、用户-机器人关系）的智能RPA分类法。

Conclusion: 机器学习与RPA的结合可以扩展自动化任务的范围，智能RPA分类体系为研究和实践提供了系统化的框架。

Abstract: Robotic process automation (RPA) is a lightweight approach to automating
business processes using software robots that emulate user actions at the
graphical user interface level. While RPA has gained popularity for its
cost-effective and timely automation of rule-based, well-structured tasks, its
symbolic nature has inherent limitations when approaching more complex tasks
currently performed by human agents. Machine learning concepts enabling
intelligent RPA provide an opportunity to broaden the range of automatable
tasks. In this paper, we conduct a literature review to explore the connections
between RPA and machine learning and organize the joint concept intelligent RPA
into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML
integration and RPA-ML interaction. Together, they comprise eight dimensions:
architecture and ecosystem, capabilities, data basis, intelligence level, and
technical depth of integration as well as deployment environment, lifecycle
phase, and user-robot relation.

</details>


### [15] [Ontology Creation and Management Tools: the Case of Anatomical Connectivity](https://arxiv.org/abs/2509.15780)
*Natallia Kokash,Bernard de Bono,Tom Gillespie*

Main category: cs.AI

TL;DR: 开发ApiNATOMY框架，用于外周神经系统和其他生理系统的多尺度生理回路图拓扑和语义表示


<details>
  <summary>Details</summary>
Motivation: 支持研究人员绘制与生理系统相关的数据，特别是研究器官的相关性，神经系统在协调和传输全身信号中起关键作用

Method: 创建包含知识表示模型和知识管理工具的框架，KR模型便于生理学专家捕获解剖实体间的相互作用，KM工具帮助建模者将高级抽象转化为详细的生理过程模型

Result: 建立了能够整合外部本体和知识图谱的生理回路图表示基础设施

Conclusion: ApiNATOMY为多尺度生理回路图的表示和管理提供了有效的框架支持

Abstract: We are developing infrastructure to support researchers in mapping data
related to the peripheral nervous system and other physiological systems, with
an emphasis on their relevance to the organs under investigation. The nervous
system, a complex network of nerves and ganglia, plays a critical role in
coordinating and transmitting signals throughout the body. To aid in this, we
have created ApiNATOMY, a framework for the topological and semantic
representation of multiscale physiological circuit maps. ApiNATOMY integrates a
Knowledge Representation (KR) model and a suite of Knowledge Management (KM)
tools. The KR model enables physiology experts to easily capture interactions
between anatomical entities, while the KM tools help modelers convert
high-level abstractions into detailed models of physiological processes, which
can be integrated with external ontologies and knowledge graphs.

</details>


### [16] [Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration](https://arxiv.org/abs/2509.15786)
*Nan Li,Bo Kang,Tijl De Bie*

Main category: cs.AI

TL;DR: CLIMB是一个自动化构建职业分类法的框架，通过全局语义聚类和多智能体系统从原始职位发布数据中创建高质量、数据驱动的分类法


<details>
  <summary>Details</summary>
Motivation: 手动构建职业分类法速度慢，现有自动化方法要么无法适应动态区域市场（自上而下），要么难以从噪声数据构建连贯的层次结构（自下而上）

Method: 使用全局语义聚类提炼核心职业，然后采用基于反思的多智能体系统迭代构建连贯的层次结构

Result: 在三个不同的真实世界数据集上，CLIMB生成的分类法比现有方法更连贯和可扩展，并成功捕捉了独特的区域特征

Conclusion: CLIMB框架能够完全自动化地创建高质量的职业分类法，解决了现有方法的局限性

Abstract: Creating robust occupation taxonomies, vital for applications ranging from
job recommendation to labor market intelligence, is challenging. Manual
curation is slow, while existing automated methods are either not adaptive to
dynamic regional markets (top-down) or struggle to build coherent hierarchies
from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent
taxonomy Builder), a framework that fully automates the creation of
high-quality, data-driven taxonomies from raw job postings. CLIMB uses global
semantic clustering to distill core occupations, then employs a
reflection-based multi-agent system to iteratively build a coherent hierarchy.
On three diverse, real-world datasets, we show that CLIMB produces taxonomies
that are more coherent and scalable than existing methods and successfully
capture unique regional characteristics. We release our code and datasets at
https://anonymous.4open.science/r/CLIMB.

</details>


### [17] [A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring](https://arxiv.org/abs/2509.15848)
*Giovanni De Gasperis,Sante Dino Facchini*

Main category: cs.AI

TL;DR: 本文比较了工业监控系统中基于规则的方法和数据驱动方法的优缺点，提出了评估框架，并建议混合解决方案作为未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 工业4.0环境下，监控系统正从传统基于规则架构向数据驱动方法转变，需要系统比较两种方法的适用性和局限性。

Method: 通过分析两种方法各自的优势、局限性和应用场景，提出基本评估框架来评估关键特性。

Result: 基于规则系统在可解释性、确定性行为和稳定环境中表现优异，但缺乏可扩展性和适应性；数据驱动系统在异常检测和预测性维护方面表现突出，但面临数据可用性和可解释性挑战。

Conclusion: 混合解决方案结合了规则逻辑的透明性和机器学习的分析能力，是未来工业监控的发展方向，能够增强韧性、运营效率和信任度。

Abstract: Industrial monitoring systems, especially when deployed in Industry 4.0
environments, are experiencing a shift in paradigm from traditional rule-based
architectures to data-driven approaches leveraging machine learning and
artificial intelligence. This study presents a comparison between these two
methodologies, analyzing their respective strengths, limitations, and
application scenarios, and proposes a basic framework to evaluate their key
properties. Rule-based systems offer high interpretability, deterministic
behavior, and ease of implementation in stable environments, making them ideal
for regulated industries and safety-critical applications. However, they face
challenges with scalability, adaptability, and performance in complex or
evolving contexts. Conversely, data-driven systems excel in detecting hidden
anomalies, enabling predictive maintenance and dynamic adaptation to new
conditions. Despite their high accuracy, these models face challenges related
to data availability, explainability, and integration complexity. The paper
suggests hybrid solutions as a possible promising direction, combining the
transparency of rule-based logic with the analytical power of machine learning.
Our hypothesis is that the future of industrial monitoring lies in intelligent,
synergic systems that leverage both expert knowledge and data-driven insights.
This dual approach enhances resilience, operational efficiency, and trust,
paving the way for smarter and more flexible industrial environments.

</details>


### [18] [EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol](https://arxiv.org/abs/2509.15957)
*Kanato Masayoshi,Masahiro Hashimoto,Ryoichi Yokoyama,Naoki Toda,Yoshifumi Uwamino,Shogo Fukuda,Ho Namkoong,Masahiro Jinzaki*

Main category: cs.AI

TL;DR: 该研究评估了通过MCP协议将LLM与医院EHR系统集成，在真实医院环境中自主检索临床信息的能力，在简单任务中达到近乎完美的性能，但在复杂任务中面临挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学领域展现出潜力，但由于对电子健康记录系统的访问受限，其在医院的部署受到限制。Model Context Protocol（MCP）能够实现LLM与外部工具的集成。

Method: 开发了EHR-MCP框架，将自定义MCP工具与医院EHR数据库集成，使用GPT-4.1通过LangGraph ReAct代理与之交互。测试了感染控制团队的六个任务，回顾性分析了八个患者案例。

Result: LLM能够一致地选择和执行正确的MCP工具。除两个任务外，所有任务都达到了近乎完美的准确性。在需要时间相关计算的复杂任务中性能较低。大多数错误源于参数不正确或工具结果误解。

Conclusion: LLM可以通过MCP工具在真实医院环境中从EHR检索临床数据，在简单任务中实现近乎完美的性能，同时突出了复杂任务中的挑战。EHR-MCP为安全、一致的数据访问提供了基础设施，可作为医院AI代理的基础。

Abstract: Background: Large language models (LLMs) show promise in medicine, but their
deployment in hospitals is limited by restricted access to electronic health
record (EHR) systems. The Model Context Protocol (MCP) enables integration
between LLMs and external tools.
  Objective: To evaluate whether an LLM connected to an EHR database via MCP
can autonomously retrieve clinically relevant information in a real hospital
setting.
  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated
with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct
agent to interact with it. Six tasks were tested, derived from use cases of the
infection control team (ICT). Eight patients discussed at ICT conferences were
retrospectively analyzed. Agreement with physician-generated gold standards was
measured.
  Results: The LLM consistently selected and executed the correct MCP tools.
Except for two tasks, all tasks achieved near-perfect accuracy. Performance was
lower in the complex task requiring time-dependent calculations. Most errors
arose from incorrect arguments or misinterpretation of tool results. Responses
from EHR-MCP were reliable, though long and repetitive data risked exceeding
the context window.
  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a
real hospital setting, achieving near-perfect performance in simple tasks while
highlighting challenges in complex ones. EHR-MCP provides an infrastructure for
secure, consistent data access and may serve as a foundation for hospital AI
agents. Future work should extend beyond retrieval to reasoning, generation,
and clinical impact assessment, paving the way for effective integration of
generative AI into clinical practice.

</details>


### [19] [Structured Information for Improving Spatial Relationships in Text-to-Image Generation](https://arxiv.org/abs/2509.15962)
*Sander Schildermans,Chang Tian,Ying Jiao,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级方法，通过基于元组的结构化信息增强文本提示，以改进文本到图像生成中的空间关系准确性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成系统在准确捕捉自然语言提示中描述的空间关系方面存在主要挑战，需要解决这一关键限制。

Method: 使用微调的语言模型自动将自然语言提示转换为基于元组的结构化信息，并将其无缝集成到文本到图像生成流程中。

Result: 实验结果显示空间准确性显著提升，同时Inception Score测量的整体图像质量未受影响，自动生成的元组质量与人工制作的元组相当。

Conclusion: 这种结构化信息为增强文本到图像生成中的空间关系提供了实用且可移植的解决方案。

Abstract: Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing
spatial relationships described in natural language prompts remains a major
challenge. Prior efforts have addressed this issue through prompt optimization,
spatially grounded generation, and semantic refinement. This work introduces a
lightweight approach that augments prompts with tuple-based structured
information, using a fine-tuned language model for automatic conversion and
seamless integration into T2I pipelines. Experimental results demonstrate
substantial improvements in spatial accuracy, without compromising overall
image quality as measured by Inception Score. Furthermore, the automatically
generated tuples exhibit quality comparable to human-crafted tuples. This
structured information provides a practical and portable solution to enhance
spatial relationships in T2I generation, addressing a key limitation of current
large-scale generative systems.

</details>


### [20] [Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers](https://arxiv.org/abs/2509.16058)
*Krati Saxena,Federico Jurado Ruiz,Guido Manzi,Dianbo Liu,Alex Lamb*

Main category: cs.AI

TL;DR: 该论文提出ASAC（基于注意力模式的注意力控制）方法，将认知科学中的注意力模式理论整合到人工神经网络中，通过VQVAE作为注意力抽象器和控制器来提升模型效率和性能。


<details>
  <summary>Details</summary>
Motivation: 受到认知科学中注意力模式理论（AST）的启发，该理论认为人类通过创建注意力模型来管理注意力分配。作者希望将这一认知机制引入AI系统，以提升注意力机制的效率和性能。

Method: 在Transformer架构中嵌入ASAC模块，使用向量量化变分自编码器（VQVAE）作为注意力抽象器和控制器，实现精确的注意力管理。通过显式建模注意力分配来提升系统效率。

Result: 在视觉和NLP领域的实验表明，ASAC能够提高分类准确率并加速学习过程。视觉Transformer在多个数据集上显示，注意力控制器不仅提升了分类准确率，还加快了学习速度。模型在噪声和分布外数据集上表现出鲁棒性和泛化能力，在多任务设置中也展现出改进的性能。

Conclusion: 基于注意力模式的模块增强了对抗攻击的韧性，优化了注意力以提高学习效率，促进了有效的迁移学习和少样本学习。这些有前景的结果建立了认知科学与机器学习之间的联系，为AI系统中注意力机制的高效利用提供了新的视角。

Abstract: Attention mechanisms have become integral in AI, significantly enhancing
model performance and scalability by drawing inspiration from human cognition.
Concurrently, the Attention Schema Theory (AST) in cognitive science posits
that individuals manage their attention by creating a model of the attention
itself, effectively allocating cognitive resources. Inspired by AST, we
introduce ASAC (Attention Schema-based Attention Control), which integrates the
attention schema concept into artificial neural networks. Our initial
experiments focused on embedding the ASAC module within transformer
architectures. This module employs a Vector-Quantized Variational AutoEncoder
(VQVAE) as both an attention abstractor and controller, facilitating precise
attention management. By explicitly modeling attention allocation, our approach
aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both
the vision and NLP domains, highlighting its ability to improve classification
accuracy and expedite the learning process. Our experiments with vision
transformers across various datasets illustrate that the attention controller
not only boosts classification accuracy but also accelerates learning.
Furthermore, we have demonstrated the model's robustness and generalization
capabilities across noisy and out-of-distribution datasets. In addition, we
have showcased improved performance in multi-task settings. Quick experiments
reveal that the attention schema-based module enhances resilience to
adversarial attacks, optimizes attention to improve learning efficiency, and
facilitates effective transfer learning and learning from fewer examples. These
promising results establish a connection between cognitive science and machine
learning, shedding light on the efficient utilization of attention mechanisms
in AI systems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [21] [Enhancing Physical Layer Security in IoT-Based RF-FSO Integrated Networks: Multi-RIS Structures and their Impact on Secure Communication](https://arxiv.org/abs/2509.15411)
*Anika Tabassum Biva,Md. Ibrahim,A. S. M. Badrudduza,Imran Shafique Ansari*

Main category: cs.IT

TL;DR: 本文研究了多RIS辅助的RF-FSO混合通信模型，以提升复杂环境中物联网应用的性能，分析了窃听场景下的安全性能指标。


<details>
  <summary>Details</summary>
Motivation: 6G无线通信面临挑战，特别是在物联网网络中。可重构智能表面(RIS)能够动态控制传播环境，为解决这些挑战提供了有前景的解决方案。

Method: 采用多RIS辅助的RF-FSO混合通信模型，RF链路使用Rician衰落模型，FSO链路考虑Málaga湍流和指向误差。推导了安全中断概率、平均安全容量和有效安全吞吐量的闭式解析表达式。

Result: 数值结果表明，外差检测能有效减轻FSO链路指向误差的不利影响。多RIS结构显著提升安全性能，与传统方法相比安全中断概率改善达47.67%。

Conclusion: 所提出的多RIS辅助RF-FSO混合通信模型在复杂环境中能有效提升物联网应用的安全性能，推导的解析结果通过蒙特卡洛仿真验证了准确性。

Abstract: Due to their ability to dynamically control the propagation environment,
reconfigurable intelligent surfaces (RISs) offer a promising solution to
address the challenges of $6$G wireless communication, especially in the
context of Internet of Things (IoT) networks. This paper investigates a mixed
communication model with multi-RIS-aided radio frequency (RF)-free space optics
(FSO) to enhance the performance of IoT applications in complex environments.
An eavesdropper is assumed to be present, attempting to intercept confidential
information transmitted over the RF link. All RF links are modeled using Rician
fading, while the FSO link accounts for M\'alaga turbulence with pointing
errors, capturing real-world propagation conditions. Closed-form analytical
expressions are derived for the secrecy outage probability, average secrecy
capacity, and effective secrecy throughput in terms of Meijer's G function. To
gain further insight, high signal-to-noise approximations of these metrics are
also presented. Numerical results highlight the importance of heterodyne
detection in mitigating the adverse effects of pointing errors on the FSO link.
Moreover, integrating a multi-RIS structure into the proposed model
significantly increases secrecy performance, achieving up to a $47.67\%$
improvement in SOP compared to conventional methods. Finally, the derived
analytical results are validated through Monte Carlo simulations.

</details>


### [22] [Interplay Between Belief Propagation and Transformer: Differential-Attention Message Passing Transformer](https://arxiv.org/abs/2509.15637)
*Chin Wa Lau,Xiang Shi,Ziyan Zheng,Haiwen Cao,Nian Guo*

Main category: cs.IT

TL;DR: 本文提出了一种结合经典置信传播原理与Transformer设计的新型解码器架构，通过可微分校验子损失函数和差分注意力机制优化比特与校验子嵌入交互，在短到中等长度LDPC码上超越了现有Transformer解码器和传统置信传播解码器的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer神经网络解码器在纠错编码领域展现出潜力，但需要更好地结合经典编码理论和优化比特与校验子之间的交互关系。

Method: 集成经典置信传播原理与Transformer架构，引入可微分校验子损失函数利用全局码本结构，以及差分注意力机制优化比特和校验子嵌入的交互。

Result: 实验结果表明，该方法相比现有Transformer解码器取得了一致的性能提升，在短到中等长度LDPC码上超越了传统置信传播解码器。

Conclusion: 该工作展示了将经典编码理论与现代神经网络架构相结合的有效性，为纠错编码领域提供了新的研究方向。

Abstract: Transformer-based neural decoders have emerged as a promising approach to
error correction coding, combining data-driven adaptability with efficient
modeling of long-range dependencies. This paper presents a novel decoder
architecture that integrates classical belief propagation principles with
transformer designs. We introduce a differentiable syndrome loss function
leveraging global codebook structure and a differential-attention mechanism
optimizing bit and syndrome embedding interactions. Experimental results
demonstrate consistent performance improvements over existing transformer-based
decoders, with our approach surpassing traditional belief propagation decoders
for short-to-medium length LDPC codes.

</details>


### [23] [Finite-blocklength Fluid Antenna Systems](https://arxiv.org/abs/2509.15643)
*Zhentian Zhang,Kai-Kit Wong,David Morales-Jimenez,Hao Jiang,Hao Xu,Christos Masouros,Zaichen Zhang,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文研究有限块长流体天线系统（FBL-FAS），通过随机矩阵理论和极值理论分析有限块长对SNR和SINR的影响，推导了BLER和中断概率的闭式表达式，相比传统多天线系统在有限块长下具有更高的能量和频谱效率。


<details>
  <summary>Details</summary>
Motivation: 为满足6G及未来网络的严格KPI要求（包括mMTC、URLLC和eMBB），需要评估FAS在有限信道使用情况下的性能。

Method: 利用随机矩阵理论和极值理论（EVT）分析有限块长对关键指标的影响，采用Chernoff边界和泰勒展开辅助的积分中值定理（MVTI）降低计算复杂度。

Result: 数值结果表明，相比传统多天线系统，FBL-FAS框架在有限块长下实现了更高的能量和频谱效率。

Conclusion: FBL-FAS是下一代无线网络的有前景的使能技术。

Abstract: This work introduces and investigates finite blocklength fluid antenna
systems (FBL-FASs). To meet the stringent key performance indicators (KPIs) of
6G and beyond networks, including ultra-massive machine-type communications
(mMTC), ultra-reliable low-latency communications (URLLC), and enhanced mobile
broadband (eMBB), it is necessary to evaluate the performance of FAS under
limited channel uses across time, frequency, and other domains. By exploiting
random matrix theory and extreme value theory (EVT), we characterize the effect
of finite blocklength on key metrics such as the signal-to-noise ratio (SNR)
and the signal-to-interference-plus-noise ratio (SINR), via accurate estimation
of interference caused by codeword correlation. Closed-form expressions for
block error rate (BLER) and outage probability are derived, covering both
conditional BLER (with channel state information, CSI) and statistical BLER
(without CSI). The proposed analysis leverages Chernoff bounds and introduces a
Taylor-expansion-assisted mean value theorem for integrals (MVTI) to reduce
computational complexity. Numerical results show that, compared with
conventional multi-antenna systems, the proposed FBL-FAS framework achieves
higher energy and spectral efficiency under finite blocklength, making it a
promising enabler for next-generation wireless networks.

</details>


### [24] [Near-Field Beam Training Through Beam Diverging](https://arxiv.org/abs/2509.16035)
*Ran Li,Ziyi Xu,Ying-Jun Angela Zhang*

Main category: cs.IT

TL;DR: 本文提出了一种使用发散波束的近场波束训练新范式，通过设计发散码字和分层方法，显著降低了波束训练开销并提高了精度。


<details>
  <summary>Details</summary>
Motivation: 现有近场波束训练方法主要依赖波束聚焦，但存在波束扫描开销大和精度有限的问题，因为窄波束容易失准。

Method: 设计发散码字实现单射频链路的波束发散效应，开发发散极域码本和分层方法，仅需2log₂(N)个导频即可完成用户设备角度定位，并通过角度范围缩减和导频集扩展技术提升性能。

Result: 数值结果表明，该算法以较小的导频开销实现了接近最优的精度，优于现有方法。

Conclusion: 发散波束训练范式为近场大规模天线阵列提供了高效、高精度的波束训练解决方案。

Abstract: This paper investigates beam training techniques for near-field (NF)
extremely large-scale antenna arrays (ELAAs). Existing NF beam training methods
predominantly rely on beam focusing, where the base station (BS) transmits
highly spatially selective beams to locate the user equipment (UE). However,
these beam-focusing-based schemes suffer from both high beam sweeping overhead
and limited accuracy in the NF, primarily due to the narrow beams' high
susceptibility to misalignment. To address this, we propose a novel NF beam
training paradigm using diverging beams. Specifically, we introduce the beam
diverging effect and exploit it for low-overhead, high-accuracy beam training.
First, we design a diverging codeword to induce the beam diverging effect with
a single radio frequency (RF) chain. Next, we develop a diverging polar-domain
codebook (DPC) along with a hierarchical method that enables angular-domain
localization of the UE with only 2 log_2(N) pilots, where N denotes the number
of antennas. Finally, we enhance beam training performance through two
additional techniques: a DPC angular range reduction strategy to improve the
effectiveness of beam diverging, and a pilot set expansion method to increase
overall beam training accuracy. Numerical results show that our algorithm
achieves near-optimal accuracy with a small pilot overhead, outperforming
existing methods.

</details>


### [25] [3D Near-Field Beam Training for Uniform Planar Arrays through Beam Diverging](https://arxiv.org/abs/2509.16055)
*Ran Li,Ziyi Xu,Ying-Jun Angela Zhang*

Main category: cs.IT

TL;DR: 本文提出了一种利用波束发散效应的新型波束训练方法，用于6G毫米波大规模均匀平面阵列系统的近场波束训练，通过分层码本和3D采样机制实现低训练开销下的高性能波束训练。


<details>
  <summary>Details</summary>
Motivation: 随着6G系统中天线阵列规模的增大和载波波长的减小，信道模型从远场平面波转变为近场球面波，传统基于远场导向或近场聚焦码字的波束训练方法对用户设备位置失配高度敏感，导致训练开销过大。

Method: 首先分析UPA系统中波束发散效应的空间特性，构建用于粗粒度用户定位的分层码本；然后开发3D采样机制构建近场精化码本进行精确波束训练，仅需单个射频链即可实现可调宽波束覆盖。

Result: 数值结果表明，所提算法在保持低训练开销的同时，实现了优越的波束训练性能。

Conclusion: 基于波束发散效应的新型波束训练方法有效解决了大规模UPA系统近场波束训练的复杂性问题，为6G毫米波通信系统提供了高效的波束训练解决方案。

Abstract: In future 6G communication systems, large-scale antenna arrays promise
enhanced signal strength and spatial resolution, but they also increase the
complexity of beam training. Moreover, as antenna counts grow and carrier
wavelengths shrink, the channel model transits from far-field (FF) planar waves
to near-field (NF) spherical waves, further complicating the beam training
process. This paper focuses on millimeter-wave (mmWave) systems equipped with
large-scale uniform planar arrays (UPAs), which produce 3D beam patterns and
introduce additional challenges for NF beam training. Existing methods
primarily rely on either FF steering or NF focusing codewords, both of which
are highly sensitive to mismatches in user equipment (UE) location, leading to
high sensitivity to even slight mismatch and excessive training overhead. In
contrast, we introduce a novel beam training approach leveraging the
beam-diverging effect, which enables adjustable wide-beam coverage using only a
single radio frequency (RF) chain. Specifically, we first analyze the spatial
characteristics of this effect in UPA systems and leverage them to construct
hierarchical codebooks for coarse UE localization. Then, we develop a 3D
sampling mechanism to build an NF refinement codebook for precise beam
training. Numerical results demonstrate that the proposed algorithm achieves
superior beam training performance while maintaining low training overhead.

</details>


### [26] [Learning the Influence Graph of a Markov Process that Randomly Resets to Past](https://arxiv.org/abs/2509.16129)
*Sudharsan Senthil,Avhishek Chatterjee*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Learning the influence graph G of a high-dimensional Markov process is a
challenging problem. Prior work has addressed this task when the process has
finite memory. However, the more general regime in which the system
probabilistically "jumps back in time" - so that the state at t+1 depends on a
sample from a distant past t-d - remains unexplored. The process with
probabilistic resets can be modeled as a Markov process with memory, but
estimations become computationally expensive. To tackle this, we introduce
PIMRecGreedy, a modification of the RecGreedy algorithm originally designed for
i.i.d. samples. The proposed method does not assume memory, requires no prior
knowledge of d, and recovers G with high probability even without access to the
specific time indices at which such temporal jumps occur, and without imposing
any constraints on the graph structures.

</details>


### [27] [Implicit Communication in Linear Quadratic Gaussian Control Systems](https://arxiv.org/abs/2509.16146)
*Gongpu Chen,Deniz Gunduz*

Main category: cs.IT

TL;DR: 本文研究线性二次高斯控制系统中的隐式通信，证明控制系统本身可作为通信信道，控制器通过输入向接收器传输信息，同时满足控制性能约束。


<details>
  <summary>Details</summary>
Motivation: 探索控制系统在完成主要控制任务的同时，如何利用系统本身作为隐式通信信道，实现控制与通信的协同设计。

Method: 通过定义隐式信道容量概念，分别在控制器和接收器有无噪声观测的三种设置下，分析信道容量特性，并证明在控制器无噪声观测时存在分离原理。

Result: 获得了三种设置下的隐式信道容量表达式或下界，发现容量最优输入策略遵循分离原理，可将隐式信道等价转换为高斯MIMO信道。

Conclusion: 控制系统可作为有效的隐式通信信道，控制与通信性能存在权衡关系，分离原理使得控制任务和信道编码任务可独立优化而不损失最优性。

Abstract: This paper studies implicit communication in linear quadratic Gaussian
control systems. We show that the control system itself can serve as an
implicit communication channel, enabling the controller to transmit messages
through its inputs to a receiver that observes the system state. This
communication is considered implicit because (i) no explicit communication
channels are needed; and (ii) information is transmitted while simultaneously
fulfilling the controller's primary objective--maintaining the control cost
within a specified level. As a result, there exists an inherent trade-off
between control and communication performance. This trade-off is formalized
through the notion of implicit channel capacity, which characterizes the
supremum reliable communication rate subject to a constraint on control
performance. We characterize the implicit channel capacity in three settings.
When both the controller and the receiver have noiseless observations of the
system state, the channel capacity admits a closed-form expression. When only
the controller has noiseless observations, the channel capacity is given by the
solution of a convex optimization. When both the controller and the receiver
have noisy observations, we establish a lower bound on the implicit capacity.
Surprisingly, when the controller has noiseless observations, the
capacity-achieving input policy adheres to a separation principle, allowing the
control and channel coding tasks to be addressed independently, without loss of
optimality. Moreover, under this capacity-achieving input policy, the implicit
channel can be equivalently translated into a Gaussian MIMO channel, enabling
the use of existing channel codes to achieve implicit communication.

</details>
