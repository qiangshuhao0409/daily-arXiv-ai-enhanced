<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 9]
- [cs.AI](#cs.AI) [Total: 53]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Reinforcement Learning Framework for Mobility Control of gNBs in Dynamic Radio Access Networks](https://arxiv.org/abs/2508.02960)
*Pedro Duarte,André Coelho,Manuel Ricardo*

Main category: cs.NI

TL;DR: 论文提出了一种基于3D模拟环境（CC-SIM）的移动基站（gNB）智能控制算法，通过DQN代理显著减少了动态无线环境中的LoS阻塞时间。


<details>
  <summary>Details</summary>
Motivation: 无线环境的复杂性（如用户移动性和动态障碍物）对维持LoS连接提出了挑战，需要开发智能算法控制移动基站以优化连接。

Method: 开发了CC-SIM模拟环境，用于训练和验证移动gNB的控制算法，并利用DQN代理学习动态调整基站位置。

Result: 实验表明，DQN代理在三种典型场景下将LoS阻塞时间减少了高达42%。

Conclusion: 基于学习的移动控制算法在下一代自适应无线网络中具有显著效果。

Abstract: The increasing complexity of wireless environments, characterized by user
mobility and dynamic obstructions, poses challenges for the maintenance of
Line-of-Sight (LoS) connectivity. Mobile base stations (gNBs) stand as a
promising solution by physically relocating to restore or sustain LoS, thereby
necessitating the development of intelligent algorithms for autonomous movement
control.
  As part of the CONVERGE research project, which is developing an experimental
chamber to integrate computer vision (CV) into mobile networks and enhance
Quality of Service (QoS) in dynamic wireless environments, this paper presents
two key contributions. First, we introduce the CONVERGE Chamber Simulator
(CC-SIM), a 3D simulation environment for developing, training, and validating
mobility control algorithms for mobile gNBs. CC-SIM models user and obstacle
mobility, visual occlusion, and Radio Frequency (RF) propagation behavior. It
supports both offline reinforcement learning and real-time testing through
tight integration with a standalone 5G system via the OpenAirInterface (OAI) RF
simulator, enabling validation under realistic network conditions.
  Second, leveraging CC-SIM, we develop a Deep Q-Network (DQN) agent that
learns to reposition the gNB proactively in response to dynamic environmental
changes. Experiments across three representative use cases show that the
trained agent significantly reduces LoS blockage time - by up to 42% - when
compared to static deployments. These results highlight the effectiveness of
learning-based mobility control in adaptive next-generation wireless networks.

</details>


### [2] [A Survey of AI Agent Registry Solutions](https://arxiv.org/abs/2508.03095)
*Aditi Singh,Abul Ehtesham,Ramesh Raskar,Mahesh Lambe,Pradyumna Chari,Jared James Grogan,Abhishek Singh,Saket Kumar*

Main category: cs.NI

TL;DR: 本文综述了三种AI代理注册系统（MCP、A2A、NANDA），比较了它们在安全性、可扩展性、认证和维护性方面的表现，并提出了未来设计的建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在云、企业和去中心化环境中的普及，标准化的注册系统对于支持发现、身份和能力共享变得至关重要。

Method: 分析了三种注册系统：MCP的集中式元注册、A2A的基于JSON的去中心化交互，以及NANDA的加密可验证隐私保护模型。

Result: 比较了三种方法在四个维度（安全性、可扩展性、认证、维护性）的表现。

Conclusion: 提出了未来AI代理注册系统设计和采用的建议。

Abstract: As As autonomous AI agents scale across cloud, enterprise, and decentralized
environments, the need for standardized registry systems to support discovery,
identity, and capability sharing has become essential. This paper surveys three
prominent registry approaches each defined by a unique metadata model: MCP's
mcp.json, A2A's Agent Card, and NANDA's AgentFacts. MCP uses a centralized
metaregistry with GitHub authenticated publishing and structured metadata for
server discovery. A2A enables decentralized interaction via JSON-based Agent
Cards, discoverable through well-known URIs, curated catalogs, or direct
configuration. NANDA Index introduces AgentFacts, a cryptographically
verifiable and privacy-preserving metadata model designed for dynamic
discovery, credentialed capabilities, and cross-domain interoperability. These
approaches are compared across four dimensions: security, scalability,
authentication, and maintainability. The paper concludes with suggestions and
recommendations to guide future design and adoption of registry systems for the
Internet of AI Agents.

</details>


### [3] [Using the NANDA Index Architecture in Practice: An Enterprise Perspective](https://arxiv.org/abs/2508.03101)
*Sichao Wang,Ramesh Raskar,Mahesh Lambe,Pradyumna Chari,Rekha Singhal,Shailja Gupta,Rajesh Ranjan,Ken Huang*

Main category: cs.NI

TL;DR: NANDA框架为AI代理生态系统提供安全、可信和互操作的基础设施，包括全球代理发现、能力验证和跨协议互操作性，解决了当前AI代理能力与基础设施需求之间的关键差距。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理的普及，传统网络架构需要转向协作智能系统，但缺乏发现、认证、能力验证和安全协作的机制。NANDA框架旨在填补这一基础设施需求的关键空白。

Method: 提出NANDA框架，通过AgentFacts实现加密能力验证，支持跨协议互操作性，并采用零信任代理访问（ZTAA）原则解决安全问题。

Result: NANDA框架成功将孤立的AI代理转变为可验证、可信的互联生态系统，支持大规模自主代理部署。

Conclusion: NANDA框架为下一代自主智能系统奠定了基础设施基础，解决了安全、可扩展的多代理协作需求。

Abstract: The proliferation of autonomous AI agents represents a paradigmatic shift
from traditional web architectures toward collaborative intelligent systems
requiring sophisticated mechanisms for discovery, authentication, capability
verification, and secure collaboration across heterogeneous protocol
environments. This paper presents a comprehensive framework addressing the
fundamental infrastructure requirements for secure, trustworthy, and
interoperable AI agent ecosystems. We introduce the NANDA (Networked AI Agents
in a Decentralized Architecture) framework, providing global agent discovery,
cryptographically verifiable capability attestation through AgentFacts, and
cross-protocol interoperability across Anthropic's Modal Context Protocol
(MCP), Google's Agent-to-Agent (A2A), Microsoft's NLWeb, and standard HTTPS
communications. NANDA implements Zero Trust Agentic Access (ZTAA) principles,
extending traditional Zero Trust Network Access (ZTNA) to address autonomous
agent security challenges including capability spoofing, impersonation attacks,
and sensitive data leakage. The framework defines Agent Visibility and Control
(AVC) mechanisms enabling enterprise governance while maintaining operational
autonomy and regulatory compliance. Our approach transforms isolated AI agents
into an interconnected ecosystem of verifiable, trustworthy intelligent
services, establishing foundational infrastructure for large-scale autonomous
agent deployment across enterprise and consumer environments. This work
addresses the critical gap between current AI agent capabilities and
infrastructure requirements for secure, scalable, multi-agent collaboration,
positioning the foundation for next-generation autonomous intelligent systems.

</details>


### [4] [NANDA Adaptive Resolver: Architecture for Dynamic Resolution of AI Agent Names](https://arxiv.org/abs/2508.03113)
*John Zinky,Hema Seshadri,Mahesh Lambe,Pradyumna Chari,Ramesh Raskar*

Main category: cs.NI

TL;DR: AdaptiveResolver是一种动态微服务架构，用于解决分布式异构环境中AI代理通信的静态端点解析限制。


<details>
  <summary>Details</summary>
Motivation: 传统DNS或静态URL无法满足基于地理、负载、能力和安全威胁的动态通信需求。

Method: 通过Agent Fact卡片在Agent Registry/Index中注册代理名称和上下文需求，实现实时端点选择和通信通道定制。

Result: 支持信任、服务质量和资源约束的协商，提供灵活、安全、可扩展的代理间通信。

Conclusion: AdaptiveResolver为未来复杂生态系统中的代理通信提供了健壮的基础。

Abstract: AdaptiveResolver is a dynamic microservice architecture designed to address
the limitations of static endpoint resolution for AI agent communication in
distributed, heterogeneous environments. Unlike traditional DNS or static URLs,
AdaptiveResolver enables context-aware, real-time selection of communication
endpoints based on factors such as geographic location, system load, agent
capabilities, and security threats. Agents advertise their Agent Name and
context requirements through Agent Fact cards in an Agent Registry/Index. A
requesting Agent discovers a Target Agent using the registry. The Requester
Agent can then resolve the Target Agent Name to obtain a tailored communication
channel to the agent based on actual environmental context between the agents.
The architecture supports negotiation of trust, quality of service, and
resource constraints, facilitating flexible, secure, and scalable
agent-to-agent interactions that go beyond the classic client-server model.
AdaptiveResolver provides a foundation for robust, future-proof agent
communication that can evolve with increasing ecosystem complexity.

</details>


### [5] [Scalability and Performance Evaluation of IEEE 802.11ah IoT Deployments: A Testbed Approach](https://arxiv.org/abs/2508.03146)
*Kostas Chounos,Katerina Kyriakou,Thanasis Korakis*

Main category: cs.NI

TL;DR: 研究开发并评估了现代无线物联网（IoT）架构，重点关注5G及更高版本应用的实际性能和可扩展性限制。


<details>
  <summary>Details</summary>
Motivation: 分析数据需求的增长及其影响，揭示IEEE 802.11ah（WiFi Halow）在现实部署中的性能行为和能源消耗问题。

Method: 构建IEEE 802.11ah办公室测试平台进行实际实验，评估网络性能和设备能源消耗。

Result: 发现网络竞争和相邻信道干扰（ACI）显著影响无线链路性能，同时揭示了能源消耗问题。

Conclusion: 研究结果有助于优化物联网到云的决策和能源消耗，提升IEEE 802.11ah的实际部署可行性。

Abstract: This work focuses on the development and assessment of modern wireless
Internet of Things (IoT) architectures, with relevance to emerging 5G and
beyond applications. To analyze the growing demands for data, and their impact,
we built an IEEE 802.11ah (WiFi Halow) office testbed for real-world
experimentation. This deployment allows us to uncover the practical performance
and scalability limitations of such networks under various challenging
scenarios. To the best of our knowledge, this is the first study to consider
complex real-world IEEE 802.11ah implementations, aiming specifically to reveal
unexpected performance behaviors, such as significant throughput degradation
arising in closely deployed wireless links. Our findings show that intense
network contention and Adjacent Channel Interference (ACI), drastically impact
the performance of the wireless links involved. Beyond evaluating network
performance, our experimental analysis also considers the energy consumption of
the devices under test, offering a more holistic perspective on the feasibility
of IEEE 802.11ah in real-world deployments. The effective disclosure of such
unexpected phenomena, can lead to well planned decisions and energy consumption
optimization across the IoT to Cloud continuum.

</details>


### [6] [Energy-efficient Federated Learning for UAV Communications](https://arxiv.org/abs/2508.03171)
*Chien-Wei Fu,Meng-Lin Ku*

Main category: cs.NI

TL;DR: 提出了一种无人机辅助的联邦学习框架，通过联合优化无人机轨迹、用户参与、功率分配和数据量控制，以最小化系统总能耗。


<details>
  <summary>Details</summary>
Motivation: 研究无人机辅助联邦学习中如何通过优化多变量提升学习性能并降低能耗。

Method: 采用交替优化（AO）和逐次凸近似（SCA）技术处理非凸约束，设计迭代能量消耗优化（ECO）算法。

Result: 仿真结果表明ECO算法在能耗优化上优于现有基准方案。

Conclusion: 该框架为无人机辅助联邦学习的能耗优化提供了有效解决方案。

Abstract: In this paper, we propose an unmanned aerial vehicle (UAV)-assisted federated
learning (FL) framework that jointly optimizes UAV trajectory, user
participation, power allocation, and data volume control to minimize overall
system energy consumption. We begin by deriving the convergence accuracy of the
FL model under multiple local updates, enabling a theoretical understanding of
how user participation and data volume affect FL learning performance. The
resulting joint optimization problem is non-convex; to address this, we employ
alternating optimization (AO) and successive convex approximation (SCA)
techniques to convexify the non-convex constraints, leading to the design of an
iterative energy consumption optimization (ECO) algorithm. Simulation results
confirm that ECO consistently outperform existing baseline schemes.

</details>


### [7] [Directives for Function Offloading in 5G Networks Based on a Performance Characteristics Analysis](https://arxiv.org/abs/2508.03287)
*Falk Dettinger,Matthias Weiß,Daniel Baumann,Martin Sommer,Michael Weyrich*

Main category: cs.NI

TL;DR: 论文评估了5G非独立网络在车辆功能云执行中的表现，重点关注延迟、往返时间和数据包传递，测试了两种AI算法在不同地理环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决资源密集型车辆算法的能耗和性能问题，探索5G非独立网络在现实应用中的表现。

Method: 在德国巴登-符腾堡州8.8公里路线上测试情绪识别和物体识别算法，分析云和边缘计算平台的性能。

Result: 平均信号质量84%，无连接中断，数据包错误率低于0.1%，云卸载在往返时间超过150ms时适用。

Conclusion: 5G非独立网络在车辆云功能执行中表现良好，但云卸载需满足特定延迟条件。

Abstract: Cloud-based offloading helps address energy consumption and performance
challenges in executing resource-intensive vehicle algorithms. Utilizing 5G,
with its low latency and high bandwidth, enables seamless vehicle-to-cloud
integration. Currently, only non-standalone 5G is publicly available, and
real-world applications remain underexplored compared to theoretical studies.
This paper evaluates 5G non-standalone networks for cloud execution of vehicle
functions, focusing on latency, Round Trip Time, and packet delivery. Tests
used two AI-based algorithms -- emotion recognition and object recognition --
along an 8.8 km route in Baden-W\"urttemberg, Germany, encompassing urban,
rural, and forested areas. Two platforms were analyzed: a cloudlet in Frankfurt
and a cloud in Mannheim, employing various deployment strategies like
conventional applications and containerized and container-orchestrated setups.
Key findings highlight an average signal quality of 84 %, with no connectivity
interruptions despite minor drops in built-up areas. Packet analysis revealed a
Packet Error Rate below 0.1 % for both algorithms. Transfer times varied
significantly depending on the geographical location and the backend servers'
network connections, while processing times were mainly influenced by the
computation hardware in use. Additionally, cloud offloading seems only be a
suitable option, when a round trip time of more than 150 ms is possible.

</details>


### [8] [Bidirectional TLS Handshake Caching for Constrained Industrial IoT Scenarios](https://arxiv.org/abs/2508.03321)
*Jörn Bodenhausen,Simon Mangel,Thomas Vogt,Martin Henze*

Main category: cs.NI

TL;DR: BiTHaC通过双向TLS握手缓存减少资源受限工业物联网中的带宽和计算开销。


<details>
  <summary>Details</summary>
Motivation: TLS在资源受限的工业物联网中因握手过程的高开销而受限。

Method: 利用重复TLS握手（如证书）的静态部分实现双向缓存，减少传输和计算。

Result: 带宽消耗减少61.1%，计算开销降低8.5%，内存开销可控且安全性不变。

Conclusion: BiTHaC有效解决了TLS在资源受限环境中的性能问题。

Abstract: While TLS has become the de-facto standard for end-to-end security, its use
to secure critical communication in evolving industrial IoT scenarios is
severely limited by prevalent resource constraints of devices and networks.
Most notably, the TLS handshake to establish secure connections incurs
significant bandwidth and processing overhead that often cannot be handled in
constrained environments. To alleviate this situation, we present BiTHaC which
realizes bidirectional TLS handshake caching by exploiting that significant
parts of repeated TLS handshakes, especially certificates, are static. Thus,
redundant information neither needs to be transmitted nor corresponding
computations performed, saving valuable bandwidth and processing resources. By
implementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth
consumption of TLS handshakes by up to 61.1% and the computational overhead by
up to 8.5%, while incurring only well-manageable memory overhead and preserving
the strict security guarantees of TLS.

</details>


### [9] [Morphlux: Programmable chip-to-chip photonic fabrics in multi-accelerator servers for ML](https://arxiv.org/abs/2508.03674)
*Abhishek Vijaya Kumar,Eric Ding,Arjun Devraj,Rachee Singh*

Main category: cs.NI

TL;DR: 论文提出了一种名为Morphlux的光互连技术，用于服务器内加速器芯片间的连接，解决了现有电互连带宽不足的问题，显著提升了带宽和计算资源利用率。


<details>
  <summary>Details</summary>
Motivation: 当前多加速器服务器使用电互连连接加速器芯片，但由于加速器计算能力提升快于互连带宽，导致带宽瓶颈和GPU资源闲置。

Method: 开发了Morphlux，一种可编程的光互连架构，用于服务器内加速器间的连接，并通过硬件原型验证其性能。

Result: Morphlux可将租户计算分配的带宽提升66%，减少计算碎片化70%，并提高ML模型训练吞吐量1.72倍。

Conclusion: Morphlux通过光互连技术有效解决了服务器内加速器互连的带宽问题，显著提升了性能和资源利用率。

Abstract: We optically interconnect accelerator chips (e.g., GPUs, TPUs) within compute
servers using newly viable programmable chip-to-chip photonic fabrics. In
contrast, today, commercial multi-accelerator compute servers that are
workhorses of ML, use electrical interconnects to network accelerator chips in
the server. However, recent trends have shown an interconnect bandwidth wall
caused by accelerator FLOPS scaling at a faster rate than the bandwidth of the
interconnect between accelerators in the same server. This has led to
under-utilization and idling of GPU resources in cloud datacenters. We develop
Morphlux, a server-scale programmable photonic fabric, to interconnect
accelerators within servers. We show that augmenting state-of-the-art photonic
ML-centric datacenters with Morphlux can improve the bandwidth of tenant
compute allocations by up to 66% and reduce compute fragmentation by up to 70%.
We develop a novel end-to-end hardware prototype of Morphlux to demonstrate
these performance benefits, which translate to 1.72x improvement in training
throughput of ML models. By rapidly programming the server-scale fabric in our
hardware testbed, Morphlux can logically replace a failed accelerator chip in
1.2 seconds.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Efficient Agents: Building Effective Agents While Reducing Cost](https://arxiv.org/abs/2508.02694)
*Ningning Wang,Xavier Hu,Pai Liu,He Zhu,Yue Hou,Heyuan Huang,Shengyu Zhang,Jian Yang,Jiaheng Liu,Ge Zhang,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLM）驱动代理系统的效率与性能权衡，提出了一种成本效益优化的新型代理框架Efficient Agents。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理系统复杂性和成本的增加，如何在保持性能的同时降低成本成为关键问题。

Method: 通过实证分析GAIA基准，评估LLM骨干选择、代理框架设计和测试时扩展策略的影响，并使用成本-通过指标量化效率与性能的权衡。

Result: 提出的Efficient Agents框架在保持96.7%性能的同时，将运营成本降低了28.4%。

Conclusion: 该研究为设计高效、高性能的代理系统提供了实用见解，推动了AI解决方案的可访问性和可持续性。

Abstract: The remarkable capabilities of Large Language Model (LLM)-driven agents have
enabled sophisticated systems to tackle complex, multi-step tasks, but their
escalating costs threaten scalability and accessibility. This work presents the
first systematic study of the efficiency-effectiveness trade-off in modern
agent systems, addressing the critical need for cost-effective designs without
sacrificing performance. We investigate three key questions: (1) How much
complexity do agentic tasks inherently require? (2) When do additional modules
yield diminishing returns? (3) How much efficiency can be gained through the
design of efficient agent frameworks? Through an empirical analysis on the GAIA
benchmark, we evaluate the impact of LLM backbone selection, agent framework
designs, and test-time scaling strategies. Using the cost-of-pass metric, we
quantify the efficiency-performance trade-off across these dimensions. Our
findings inform the development of Efficient Agents , a novel agent framework
that has an optimal complexity to task requirements. Efficient Agents retains
96.7% of the performance of OWL, one leading open-source agent framework, while
reducing operational costs from $0.398 to $0.228, resulting in a 28.4%
improvement in cost-of-pass. Our work provides actionable insights for
designing efficient, high-performing agent systems, advancing the accessibility
and sustainability of AI-driven solutions.

</details>


### [11] [Planning with Dynamically Changing Domains](https://arxiv.org/abs/2508.02697)
*Mikhail Soutchanski,Yongmei Liu*

Main category: cs.AI

TL;DR: 论文提出了一种无需领域闭包假设（DCA）的有界规划问题解决方法，适用于动态对象变化的场景，并证明了方法的完备性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 经典规划和一致性规划中，领域闭包假设（DCA）限制了对象的动态变化，而实际规划问题中对象可能动态增减，需要新的方法。

Method: 基于一阶逻辑构建规划问题，初始理论为有限的流文字集，限制计划长度，并在规划时对动作序列进行接地搜索。

Result: 证明了方法的完备性和可靠性，适用于无DCA的有界规划问题，且属于顺序广义规划和一致性规划的交集。

Conclusion: 该方法为动态对象变化的规划问题提供了理论基础，并通过概念验证实现展示了可行性。

Abstract: In classical planning and conformant planning, it is assumed that there are
finitely many named objects given in advance, and only they can participate in
actions and in fluents. This is the Domain Closure Assumption (DCA). However,
there are practical planning problems where the set of objects changes
dynamically as actions are performed; e.g., new objects can be created, old
objects can be destroyed. We formulate the planning problem in first-order
logic, assume an initial theory is a finite consistent set of fluent literals,
discuss when this guarantees that in every situation there are only finitely
many possible actions, impose a finite integer bound on the length of the plan,
and propose to organize search over sequences of actions that are grounded at
planning time. We show the soundness and completeness of our approach. It can
be used to solve the bounded planning problems without DCA that belong to the
intersection of sequential generalized planning (without sensing actions) and
conformant planning, restricted to the case without the disjunction over fluent
literals. We discuss a proof-of-the-concept implementation of our planner.

</details>


### [12] [Recovering Individual-Level Activity Sequences from Location-Based Service Data Using a Novel Transformer-Based Model](https://arxiv.org/abs/2508.02734)
*Weiyu Luo,Chenfeng Xiong*

Main category: cs.AI

TL;DR: 提出了一种名为VSNIT的新方法，用于从不完整的LBS数据中恢复活动序列，效果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: LBS数据的稀疏性导致活动序列不完整，难以准确推断出行和活动。研究目标是利用高质量LBS数据恢复个体层面的不完整活动序列。

Method: 结合Insertion Transformer的灵活序列构建能力和Variable Selection Network的动态协变量处理能力，提出VSNIT方法。

Result: VSNIT能插入更多样化、更真实的活动模式，恢复的活动序列更接近真实世界的变化，且在各项指标上显著优于基线模型。

Conclusion: VSNIT在活动序列恢复任务中表现出更高的准确性和多样性，为未来基于位置的研究和应用提供了有前景的框架。

Abstract: Location-Based Service (LBS) data provides critical insights into human
mobility, yet its sparsity often yields incomplete trip and activity sequences,
making accurate inferences about trips and activities difficult. We raise a
research problem: Can we use activity sequences derived from high-quality LBS
data to recover incomplete activity sequences at the individual level? This
study proposes a new solution, the Variable Selection Network-fused Insertion
Transformer (VSNIT), integrating the Insertion Transformer's flexible sequence
construction with the Variable Selection Network's dynamic covariate handling
capability, to recover missing segments in incomplete activity sequences while
preserving existing data. The findings show that VSNIT inserts more diverse,
realistic activity patterns, more closely matching real-world variability, and
restores disrupted activity transitions more effectively aligning with the
target. It also performs significantly better than the baseline model across
all metrics. These results highlight VSNIT's superior accuracy and diversity in
activity sequence recovery tasks, demonstrating its potential to enhance LBS
data utility for mobility analysis. This approach offers a promising framework
for future location-based research and applications.

</details>


### [13] [Large Language Model-based Data Science Agent: A Survey](https://arxiv.org/abs/2508.02744)
*Peiran Wang,Yaoning Yu,Ke Chen,Xianyang Zhan,Haohan Wang*

Main category: cs.AI

TL;DR: 本文综述了基于大语言模型（LLM）的代理在数据科学任务中的应用，从代理和数据科学双视角分析了设计原则和关键流程。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在数据科学任务中的潜力，填补现有研究的空白。

Method: 通过综述近期研究，总结代理设计原则（角色、执行、知识、反思）和数据科学流程（预处理、模型开发、评估、可视化等）。

Result: 提出了一个双视角框架，连接代理设计原则与数据科学实践。

Conclusion: 为LLM代理在数据科学中的应用提供了系统化的综述和实用框架。

Abstract: The rapid advancement of Large Language Models (LLMs) has driven novel
applications across diverse domains, with LLM-based agents emerging as a
crucial area of exploration. This survey presents a comprehensive analysis of
LLM-based agents designed for data science tasks, summarizing insights from
recent studies. From the agent perspective, we discuss the key design
principles, covering agent roles, execution, knowledge, and reflection methods.
From the data science perspective, we identify key processes for LLM-based
agents, including data preprocessing, model development, evaluation,
visualization, etc. Our work offers two key contributions: (1) a comprehensive
review of recent developments in applying LLMbased agents to data science
tasks; (2) a dual-perspective framework that connects general agent design
principles with the practical workflows in data science.

</details>


### [14] [Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science](https://arxiv.org/abs/2508.02789)
*Newman Cheng,Gordon Broadbent,William Chappell*

Main category: cs.AI

TL;DR: 论文提出了一种名为CLIO的认知循环方法，通过实时优化让大型语言模型（LLM）能够自主调整问题解决方式，并在低自信时适应行为，显著提升了科学发现中的AI实用性。


<details>
  <summary>Details</summary>
Motivation: 现有AI开发框架要么缺乏推理能力，要么将推理控制抽象化，科学家需要更透明、可操控的AI工具来辅助科学发现。

Method: 引入CLIO方法，通过开放设计让科学家观察不确定性、理解推理过程并介入修正，无需额外训练即可提升模型表现。

Result: 在HLE生物和医学问题上，CLIO使GPT-4.1的准确率提升13.82%（相对提升161.64%），并揭示了内部不确定性波动对结果的关键影响。

Conclusion: CLIO的开放设计和内部机制为科学决策提供了洞察力和控制力，展示了AI在科学发现中的潜力。

Abstract: The capacity for artificial intelligence (AI) to formulate, evolve, and test
altered thought patterns under dynamic conditions indicates advanced cognition
that is crucial for scientific discovery. The existing AI development landscape
falls into two categories: 1) frameworks over non-reasoning models that
natively incorporate opinions on how humans think, and 2) reasoning models that
abstract precise control of the reasoning intuition away from end users. While
powerful, for scientists to maximize utility of AI in scientific discovery,
they not only require accuracy and transparency in reasoning, but also
steerability. Hence, we introduce an alternative approach that enables deep and
precise control over the reasoning process called: a cognitive loop via in-situ
optimization (CLIO). CLIO enables large language models (LLMs) to
self-formulate ways of approaching a problem, adapt behavior when
self-confidence is low, and ultimately provide scientists with a final belief
or answer. Through CLIO's open design, scientists can observe uncertainty
levels, understand how final belief states are formulated using graph
structures, and interject corrections. Without any further post-training,
OpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\% in text-based biology
and medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\% net
or 161.64\% relative increase when compared to the base GPT-4.1 model and
surpasses OpenAI's o3 performance in high and low reasoning effort modes. We
further discovered that oscillations within internal uncertainty measures are
key in determining the accuracy of CLIO's results, revealing how its open
design and internal mechanisms can provide insight and control into scientific
decision-making processes.

</details>


### [15] [A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering](https://arxiv.org/abs/2508.02841)
*Ziruo Yi,Jinyu Liu,Ting Xiao,Mark V. Albert*

Main category: cs.AI

TL;DR: 提出了一种多代理系统（MAS）用于放射学视觉问答（RVQA），通过专门代理提升事实准确性、减少幻觉和跨模态对齐问题，并在挑战性数据集上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于多模态大语言模型（MLLMs）和检索增强生成（RAG）的RVQA方法在事实准确性、幻觉和跨模态对齐方面的不足。

Method: 设计了一个多代理系统（MAS），包含上下文理解、多模态推理和答案验证三个专门代理，以支持复杂推理。

Result: 在挑战性RVQA数据集上，系统表现优于强基线MLLMs，并通过案例研究展示了其可靠性和可解释性。

Conclusion: 多代理方法在支持需要复杂推理的可解释和可信赖临床AI应用方面具有潜力。

Abstract: Radiology visual question answering (RVQA) provides precise answers to
questions about chest X-ray images, alleviating radiologists' workload. While
recent methods based on multimodal large language models (MLLMs) and
retrieval-augmented generation (RAG) have shown promising progress in RVQA,
they still face challenges in factual accuracy, hallucinations, and cross-modal
misalignment. We introduce a multi-agent system (MAS) designed to support
complex reasoning in RVQA, with specialized agents for context understanding,
multimodal reasoning, and answer validation. We evaluate our system on a
challenging RVQA set curated via model disagreement filtering, comprising
consistently hard cases across multiple MLLMs. Extensive experiments
demonstrate the superiority and effectiveness of our system over strong MLLM
baselines, with a case study illustrating its reliability and interpretability.
This work highlights the potential of multi-agent approaches to support
explainable and trustworthy clinical AI applications that require complex
reasoning.

</details>


### [16] [Seemingly Simple Planning Problems are Computationally Challenging: The Countdown Game](https://arxiv.org/abs/2508.02900)
*Michael Katz,Harsha Kokel,Sarath Sreedharan*

Main category: cs.AI

TL;DR: 论文提出了一种基于游戏Countdown的规划能力评测基准，解决了现有基准的不足，并通过实验验证了其挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有规划能力评测基准存在不足，无法准确衡量基础模型和智能体的长期规划能力。

Method: 提出基于Countdown游戏的评测基准生成方法，进行理论分析和实验验证。

Result: 动态生成的评测基准对现有LLM辅助规划方法极具挑战性。

Conclusion: Countdown基准为规划能力评测提供了更有效的工具。

Abstract: There is a broad consensus that the inability to form long-term plans is one
of the key limitations of current foundational models and agents. However, the
existing planning benchmarks remain woefully inadequate to truly measure their
planning capabilities. Most existing benchmarks either focus on loosely defined
tasks like travel planning or end up leveraging existing domains and problems
from international planning competitions. While the former tasks are hard to
formalize and verify, the latter were specifically designed to test and
challenge the weaknesses of existing automated planners. To address these
shortcomings, we propose a procedure for creating a planning benchmark centered
around the game called Countdown, where a player is expected to form a target
number from a list of input numbers through arithmetic operations. We discuss
how this problem meets many of the desiderata associated with an ideal
benchmark for planning capabilities evaluation. Specifically, the domain allows
for an intuitive, natural language description for each problem instance, it is
computationally challenging (NP-complete), and the instance space is rich
enough that we do not have to worry about memorization. We perform an extensive
theoretical analysis, establishing the computational complexity result and
demonstrate the advantage of our instance generation procedure over public
benchmarks. We evaluate a variety of existing LLM-assisted planning methods on
instances generated using our procedure. Our results show that, unlike other
domains like 24 Game (a special case of Countdown), our proposed dynamic
benchmark remains extremely challenging for existing LLM-based approaches.

</details>


### [17] [A Wireless Foundation Model for Multi-Task Prediction](https://arxiv.org/abs/2507.05938)
*Yucheng Sheng,Jiacheng Wang,Xingyu Zhou,Le Liang,Hao Ye,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: 提出了一种统一的基础模型，用于无线网络中的多任务预测，支持不同预测区间，并通过分解、编码和因果Transformer实现泛化能力。


<details>
  <summary>Details</summary>
Motivation: 移动通信网络的复杂性和动态性增加，传统深度学习方法难以泛化到不同场景和任务，需要一种统一的预测模型。

Method: 采用单变量分解统一异构任务，编码粒度实现区间感知，使用因果Transformer作为主干，并引入补丁掩码策略支持任意输入长度。

Result: 在大规模数据集上训练后，模型在未见场景中表现出强泛化能力，并在新任务上实现零样本性能超越传统基线。

Conclusion: 提出的基础模型为无线网络中的多任务预测提供了高效且泛化的解决方案。

Abstract: With the growing complexity and dynamics of the mobile communication
networks, accurately predicting key system parameters, such as channel state
information (CSI), user location, and network traffic, has become essential for
a wide range of physical (PHY)-layer and medium access control (MAC)-layer
tasks. Although traditional deep learning (DL)-based methods have been widely
applied to such prediction tasks, they often struggle to generalize across
different scenarios and tasks. In response, we propose a unified foundation
model for multi-task prediction in wireless networks that supports diverse
prediction intervals. The proposed model enforces univariate decomposition to
unify heterogeneous tasks, encodes granularity for interval awareness, and uses
a causal Transformer backbone for accurate predictions. Additionally, we
introduce a patch masking strategy during training to support arbitrary input
lengths. After trained on large-scale datasets, the proposed foundation model
demonstrates strong generalization to unseen scenarios and achieves zero-shot
performance on new tasks that surpass traditional full-shot baselines.

</details>


### [18] [Enhancing Japanese Large Language Models with Reasoning Vectors](https://arxiv.org/abs/2508.02913)
*Carolina Minami Oguchi,Leo Wei,Koyo Kobayashi,Hsin-Tai Wu,Dipak Ghosal*

Main category: cs.AI

TL;DR: 通过从推理LLM中提取推理向量并应用于日语LLM，提升其性能，解决了资源不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于资源限制，日语LLM的性能提升困难，需寻找高效方法。

Method: 利用任务向量（权重变化）提取推理向量，并将其应用于日语LLM。

Result: 显著提升了日语LLM的性能，方法简单有效。

Conclusion: 该方法为资源有限的语言模型性能提升提供了新思路，适用于其他语言。

Abstract: Post-training methods have improved the performance and enhanced the
reasoning capability for mainstream large language models (LLMs), but the same
is challenging for Japanese LLMs to achieve due to the amount of resources
required. Inspired by task vectors that extract the change of weights before
and after training, specifically for a certain task, we obtain reasoning
vectors from reasoning LLMs and apply them to Japanese LLMs to boost their
performance. While the resources available present a challenge to improve
Japanese LLMs, we present a simple and effective way to obtain high improvement
and hope to inspire for other languages.

</details>


### [19] [PentestJudge: Judging Agent Behavior Against Operational Requirements](https://arxiv.org/abs/2508.02921)
*Shane Caldwell,Max Harley,Michael Kouremetis,Vincent Abruzzo,Will Pearce*

Main category: cs.AI

TL;DR: PentestJudge是一个基于大型语言模型（LLM）的系统，用于评估渗透测试代理的操作，通过分层任务分解和简单标准实现高效评估。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够高效评估渗透测试代理操作的系统，避免程序化评估的复杂性。

Method: 使用树结构将渗透测试任务分解为更小的子任务和简单标准，LLM作为评判者，并与人类专家评分对比。

Result: 最佳模型的F1得分为0.83，工具使用能力强的模型更接近人类专家评分。

Conclusion: PentestJudge展示了LLM在评估渗透测试任务中的潜力，验证可能比生成更容易，为未来研究提供了方法论支持。

Abstract: We introduce PentestJudge, a system for evaluating the operations of
penetration testing agents. PentestJudge is a large language model
(LLM)-as-judge with access to tools that allow it to consume arbitrary
trajectories of agent states and tool call history to determine whether a
security agent's actions meet certain operating criteria that would be
impractical to evaluate programmatically. We develop rubrics that use a tree
structure to hierarchically collapse the penetration testing task for a
particular environment into smaller, simpler, and more manageable sub-tasks and
criteria until each leaf node represents simple yes-or-no criteria for
PentestJudge to evaluate. Task nodes are broken down into different categories
related to operational objectives, operational security, and tradecraft.
LLM-as-judge scores are compared to human domain experts as a ground-truth
reference, allowing us to compare their relative performance with standard
binary classification metrics, such as F1 scores. We evaluate several frontier
and open-source models acting as judge agents, with the best model reaching an
F1 score of 0.83. We find models that are better at tool-use perform more
closely to human experts. By stratifying the F1 scores by requirement type, we
find even models with similar overall scores struggle with different types of
questions, suggesting certain models may be better judges of particular
operating criteria. We find that weaker and cheaper models can judge the
trajectories of pentests performed by stronger and more expensive models,
suggesting verification may be easier than generation for the penetration
testing task. We share this methodology to facilitate future research in
understanding the ability of judges to holistically and scalably evaluate the
process quality of AI-based information security agents so that they may be
confidently used in sensitive production environments.

</details>


### [20] [AQUAH: Automatic Quantification and Unified Agent in Hydrology](https://arxiv.org/abs/2508.02936)
*Songkun Yan,Zhi Li,Siyu Zhu,Yixin Wen,Mofan Zhang,Mengye Chen,Jie Cao,Yang Hong*

Main category: cs.AI

TL;DR: AQUAH是一个基于语言的端到端水文建模代理，通过自然语言提示自动完成数据检索、模型配置、模拟运行和报告生成。


<details>
  <summary>Details</summary>
Motivation: 简化水文建模流程，降低地球观测数据、物理工具和决策者之间的门槛。

Method: 利用视觉支持的大型语言模型，动态解析地图和栅格数据，指导关键决策如出口选择、参数初始化和不确定性分析。

Result: 初步实验表明，AQUAH能完成冷启动模拟并生成清晰、透明且物理合理的报告。

Conclusion: AQUAH展示了以LLM为核心的视觉代理在复杂环境建模中的潜力，但需进一步校准和验证。

Abstract: We introduce AQUAH, the first end-to-end language-based agent designed
specifically for hydrologic modeling. Starting from a simple natural-language
prompt (e.g., 'simulate floods for the Little Bighorn basin from 2020 to
2022'), AQUAH autonomously retrieves the required terrain, forcing, and gauge
data; configures a hydrologic model; runs the simulation; and generates a
self-contained PDF report. The workflow is driven by vision-enabled large
language models, which interpret maps and rasters on the fly and steer key
decisions such as outlet selection, parameter initialization, and uncertainty
commentary. Initial experiments across a range of U.S. basins show that AQUAH
can complete cold-start simulations and produce analyst-ready documentation
without manual intervention. The results are judged by hydrologists as clear,
transparent, and physically plausible. While further calibration and validation
are still needed for operational deployment, these early outcomes highlight the
promise of LLM-centered, vision-grounded agents to streamline complex
environmental modeling and lower the barrier between Earth observation data,
physics-based tools, and decision makers.

</details>


### [21] [MedBLINK: Probing Basic Perception in Multimodal Language Models for Medicine](https://arxiv.org/abs/2508.02951)
*Mahtab Bigverdi,Wisdom Ikezogwo,Kevin Zhang,Hyewon Jeong,Mingyu Lu,Sungjae Cho,Linda Shapiro,Ranjay Krishna*

Main category: cs.AI

TL;DR: Medblink是一个评估多模态语言模型（MLMs）在临床感知任务中表现的基准测试，涵盖8个任务和1,605张图像。测试显示，当前MLMs在简单感知任务上表现不佳，最高准确率仅65%，远低于人类的96.4%。


<details>
  <summary>Details</summary>
Motivation: 临床医生对AI工具的采用非常谨慎，模型在简单感知任务上的错误会阻碍其临床应用。因此，需要评估MLMs在这些任务上的表现。

Method: 引入Medblink基准测试，包含8个临床相关任务和1,429个多选题，覆盖多种成像模式和解剖区域。评估了19种MLMs，包括通用和领域专用模型。

Result: 人类标注者准确率为96.4%，而最佳模型仅达65%，表明当前MLMs在视觉基础任务上表现不足。

Conclusion: 当前MLMs在临床感知任务上表现不佳，需改进其视觉基础能力以支持临床应用。

Abstract: Multimodal language models (MLMs) show promise for clinical decision support
and diagnostic reasoning, raising the prospect of end-to-end automated medical
image interpretation. However, clinicians are highly selective in adopting AI
tools; a model that makes errors on seemingly simple perception tasks such as
determining image orientation or identifying whether a CT scan is
contrast-enhance are unlikely to be adopted for clinical tasks. We introduce
Medblink, a benchmark designed to probe these models for such perceptual
abilities. Medblink spans eight clinically meaningful tasks across multiple
imaging modalities and anatomical regions, totaling 1,429 multiple-choice
questions over 1,605 images. We evaluate 19 state-of-the-art MLMs, including
general purpose (GPT4o, Claude 3.5 Sonnet) and domain specific (Med Flamingo,
LLaVA Med, RadFM) models. While human annotators achieve 96.4% accuracy, the
best-performing model reaches only 65%. These results show that current MLMs
frequently fail at routine perceptual checks, suggesting the need to strengthen
their visual grounding to support clinical adoption. Data is available on our
project page.

</details>


### [22] [Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow](https://arxiv.org/abs/2508.02959)
*Chia-Tung Ho,Jing Gong,Xufeng Yao,Yunsheng Bai,Abhishek B Akkur,Haoxing Ren*

Main category: cs.AI

TL;DR: Polymath是一种自优化代理，通过动态分层工作流解决现实世界动态问题，无需标记数据，性能优于现有基线8.1%。


<details>
  <summary>Details</summary>
Motivation: 现有基于标记数据的方法在解决动态问题时效率低且不灵活，需要一种无需标记数据的自动化工作流生成与优化方法。

Method: Polymath结合任务流图的灵活性和代码表示工作流的表达能力，采用多网格图优化和自反思进化算法优化工作流。

Result: 在六个基准数据集上，Polymath平均性能提升8.1%。

Conclusion: Polymath为动态问题提供了一种高效、灵活的解决方案，无需依赖标记数据。

Abstract: Large language models (LLMs) excel at solving complex tasks by executing
agentic workflows composed of detailed instructions and structured operations.
Yet, building general-purpose agents by manually embedding foundation models
into agentic systems such as Chain-of-Thought, Self-Reflection, and ReACT
through text interfaces limits scalability and efficiency. Recently, many
researchers have sought to automate the generation and optimization of these
workflows through code-based representations. However, existing methods often
rely on labeled datasets to train and optimize workflows, making them
ineffective and inflexible for solving real-world, dynamic problems where
labeled data is unavailable. To address this challenge, we introduce Polymath,
a self-optimizing agent with dynamic hierarchical workflow that leverages the
flexibility of task flow graphs and the expressiveness of code-represented
workflows to solve a wide range of real-world, dynamic problems. The proposed
optimization methodology integrates multi-grid-inspired graph optimization with
a self-reflection-guided evolutionary algorithm to refine workflows without
labeled data. Experimental results on six benchmark datasets across coding,
math, and multi-turn QA tasks show that Polymath achieves 8.1% average
improvement over state-of-the-art baselines.

</details>


### [23] [Defend LLMs Through Self-Consciousness](https://arxiv.org/abs/2508.02961)
*Boshi Huang,Fabio Nonato de Paula*

Main category: cs.AI

TL;DR: 本文提出了一种新型的自意识防御机制，利用大语言模型（LLM）的推理能力自主对抗提示注入攻击，显著提高了防御成功率。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖外部分类器，而本文希望通过LLM自身的推理能力实现自我防护，提供轻量级且经济高效的解决方案。

Method: 提出包含元认知和仲裁模块的框架，使LLM能够自主评估和调节输出。

Result: 在七个先进LLM上测试，使用两个数据集，结果显示防御成功率显著提升，部分模型在增强模式下实现完美防御。

Conclusion: 该方法为提升LLM伦理提供了一种轻量级解决方案，适用于多种平台的生成式AI用例。

Abstract: This paper introduces a novel self-consciousness defense mechanism for Large
Language Models (LLMs) to combat prompt injection attacks. Unlike traditional
approaches that rely on external classifiers, our method leverages the LLM's
inherent reasoning capabilities to perform self-protection. We propose a
framework that incorporates Meta-Cognitive and Arbitration Modules, enabling
LLMs to evaluate and regulate their own outputs autonomously. Our approach is
evaluated on seven state-of-the-art LLMs using two datasets: AdvBench and
Prompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate
significant improvements in defense success rates across models and datasets,
with some achieving perfect and near-perfect defense in Enhanced Mode. We also
analyze the trade-off between defense success rate improvement and
computational overhead. This self-consciousness method offers a lightweight,
cost-effective solution for enhancing LLM ethics, particularly beneficial for
GenAI use cases across various platforms.

</details>


### [24] [Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling](https://arxiv.org/abs/2508.02979)
*Peng Ding,Rick Stevens*

Main category: cs.AI

TL;DR: 提出了一种统一工具集成方法，显著减少开发开销，提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决工具增强型大语言模型生态碎片化问题，简化开发流程。

Method: 采用协议无关设计原则，自动化模式生成，双模式并发执行，多源工具管理。

Result: 代码减少60-80%，性能提升3.1倍，兼容现有标准。

Conclusion: 为工具集成架构提供理论见解和实际解决方案。

Abstract: The proliferation of tool-augmented Large Language Models (LLMs) has created
a fragmented ecosystem where developers must navigate multiple protocols,
manual schema definitions, and complex execution workflows. We address this
challenge by proposing a unified approach to tool integration that abstracts
protocol differences while optimizing execution performance. Our solution
demonstrates how protocol-agnostic design principles can significantly reduce
development overhead through automated schema generation, dual-mode concurrent
execution, and seamless multi-source tool management. Experimental results show
60-80% code reduction across integration scenarios, performance improvements up
to 3.1x through optimized concurrency, and full compatibility with existing
function calling standards. This work contributes both theoretical insights
into tool integration architecture and practical solutions for real-world LLM
application development.

</details>


### [25] [When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs](https://arxiv.org/abs/2508.02994)
*Fangyi Yu*

Main category: cs.AI

TL;DR: 论文探讨了使用AI代理作为评估者（agent-as-a-judge）的新范式，以解决大型语言模型（LLMs）输出评估的瓶颈问题，分析了其优势、挑战及实际应用。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs能力和自主性的提升，其输出的评估在开放性和复杂任务中成为关键瓶颈，需要可扩展且细致的替代方案。

Method: 定义了agent-as-a-judge概念，追踪了从单模型评估到动态多代理辩论框架的演变，并比较了可靠性、成本和人类对齐等方面。

Result: 展示了在医疗、法律、金融和教育等领域的实际应用，并指出了偏见、鲁棒性和元评估等挑战。

Conclusion: 基于代理的评估可以补充（而非替代）人类监督，为下一代LLMs的可信、可扩展评估迈出一步。

Abstract: As large language models (LLMs) grow in capability and autonomy, evaluating
their outputs-especially in open-ended and complex tasks-has become a critical
bottleneck. A new paradigm is emerging: using AI agents as the evaluators
themselves. This "agent-as-a-judge" approach leverages the reasoning and
perspective-taking abilities of LLMs to assess the quality and safety of other
models, promising calable and nuanced alternatives to human evaluation. In this
review, we define the agent-as-a-judge concept, trace its evolution from
single-model judges to dynamic multi-agent debate frameworks, and critically
examine their strengths and shortcomings. We compare these approaches across
reliability, cost, and human alignment, and survey real-world deployments in
domains such as medicine, law, finance, and education. Finally, we highlight
pressing challenges-including bias, robustness, and meta evaluation-and outline
future research directions. By bringing together these strands, our review
demonstrates how agent-based judging can complement (but not replace) human
oversight, marking a step toward trustworthy, scalable evaluation for
next-generation LLMs.

</details>


### [26] [AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots](https://arxiv.org/abs/2508.02999)
*Xinjie Zhao,Moritz Blum,Fan Gao,Yingjian Chen,Boming Yang,Luis Marquez-Carpintero,Mónica Pina-Navarro,Yanran Fu,So Morikawa,Yusuke Iwasawa,Yutaka Matsuo,Chanjun Park,Irene Li*

Main category: cs.AI

TL;DR: AGENTiGraph是一个用户友好的、基于代理的系统，通过自然语言操作知识图谱，为非技术用户提供直观的数据交互和管理。


<details>
  <summary>Details</summary>
Motivation: 为非技术用户提供无需专业查询语言的工具，使其能够通过多轮对话和动态更新构建和优化知识库。

Method: 系统设计包括意图分类、任务规划和自动知识集成，支持多样化任务的无缝推理。

Result: 在3500个查询的教育场景基准测试中，系统表现优于零样本基线（分类准确率95.12%，执行成功率90.45%）。

Conclusion: AGENTiGraph展示了在合规关键或多步骤查询（如法律和医疗领域）中的扩展潜力，为多轮企业知识管理提供了新范式。

Abstract: AGENTiGraph is a user-friendly, agent-driven system that enables intuitive
interaction and management of domain-specific data through the manipulation of
knowledge graphs in natural language. It gives non-technical users a complete,
visual solution to incrementally build and refine their knowledge bases,
allowing multi-round dialogues and dynamic updates without specialized query
languages. The flexible design of AGENTiGraph, including intent classification,
task planning, and automatic knowledge integration, ensures seamless reasoning
between diverse tasks. Evaluated on a 3,500-query benchmark within an
educational scenario, the system outperforms strong zero-shot baselines
(achieving 95.12% classification accuracy, 90.45% execution success),
indicating potential scalability to compliance-critical or multi-step queries
in legal and medical domains, e.g., incorporating new statutes or research on
the fly. Our open-source demo offers a powerful new paradigm for multi-turn
enterprise knowledge management that bridges LLMs and structured graphs.

</details>


### [27] [Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning](https://arxiv.org/abs/2508.03018)
*Yutong Wang,Pengliang Ji,Kaixin Li,Baolong Bi,Tao Feng,Guillaume Sartoretti*

Main category: cs.AI

TL;DR: BPO框架通过三阶段（引导、外推和优化）解决大型语言推理模型在多轮代理规划中的挑战，显著提升稀疏奖励环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在多轮代理规划中的信用分配问题和计算开销问题。

Method: 提出BPO框架，包括引导（规划四元组）、外推（复杂度分层课程学习）和优化（奖励门控拒绝采样）。

Result: 在ALFWorld、ScienceWorld和WebShop上实现最优性能，且具有显著的标记效率。

Conclusion: BPO为代理规划中的推理模型提供了新方法，适用于长视野、稀疏奖励环境。

Abstract: Large Language Reasoning Models have demonstrated remarkable success on
static tasks, yet their application to multi-round agentic planning in
interactive environments faces two fundamental challenges. First, the
intractable credit assignment problem renders conventional reinforcement
learning ineffective in sparse-reward settings. Second, the computational
overhead of verbose, step-by-step reasoning histories is prohibitive. To
address these challenges, we propose BPO, a three-stage framework
(bootstrapping, extrapolation, and refinement) that establishes a
self-improving data flywheel to develop robust reasoning models for
long-horizon, sparse-reward environments. Our framework first bootstraps
efficient reasoning using the proposed planning quaternions with long-short
chain-of-thought fusion. It then extrapolates to out-of-distribution tasks
through complexity-stratified curriculum learning. Finally, the model
iteratively refines itself by learning exclusively on experiences selected via
reward-gated rejection sampling. Experiments on ALFWorld, ScienceWorld, and
WebShop demonstrate that our approach achieves state-of-the-art with
significant token efficiency, providing a new recipe for reasoning models in
agentic planning.

</details>


### [28] [Collab-Solver: Collaborative Solving Policy Learning for Mixed-Integer Linear Programming](https://arxiv.org/abs/2508.03030)
*Siyuan Li,Yifan Yu,Yanchen Deng,Zhihao Zhang,Mengjing Chen,Fangzhou Zhu,Tao Zhong,Jianye Hao,Peng Liu,Bo An*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体的协作学习框架（Collab-Solver），用于优化混合整数线性规划（MILP）求解器中多个模块的策略，显著提升了求解速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的MILP方法独立处理各模块的策略学习，忽视了模块间的相互依赖，影响了求解效率和质量。

Method: 将MILP求解中的割平面选择和分支协作建模为Stackelberg博弈，采用两阶段学习范式（数据通信预训练和模块策略协同优化）来稳定协作学习。

Result: 在合成和实际大规模MILP数据集上，联合学习的策略显著提升了求解性能，并展现出优秀的泛化能力。

Conclusion: Collab-Solver通过协作学习优化MILP求解器策略，解决了模块间独立学习的问题，提升了整体性能。

Abstract: Mixed-integer linear programming (MILP) has been a fundamental problem in
combinatorial optimization. Previous works have designed a plethora of
hard-coded heuristics to accomplish challenging MILP solving with domain
knowledge. Driven by the high capability of neural networks, recent research is
devoted to replacing manually designed heuristics with learned policies.
Although learning-based MILP methods have shown great promise, existing
worksindependentlytreatthepolicylearningineachmoduleofMILPsolvers without
considering their interdependence, severely hurting the solving speed and
quality. To address this issue, we propose a novel multi-agent-based policy
learning framework for MILP (Collab-Solver), which can collaboratively optimize
the policies for multiple modules. Specifically, we formulate the collaboration
of cut selection and branching in MILP solving as a Stackelberg game. Under
this formulation, we develop a two-phase learning paradigm to stabilize the
collaborative policy learning, where the first phase achieves the
data-communicated policy pretraining and the second phase further orchestrates
the policy learning for various modules. The jointly learned policy
significantly improves the solving performance on both synthetic and
large-scale real-world MILP datasets. Moreover, the policies learned by
Collab-Solver have also demonstrated excellent generalization abilities across
different instance sets.

</details>


### [29] [From Text to Trajectories: GPT-2 as an ODE Solver via In-Context](https://arxiv.org/abs/2508.03031)
*Ziyang Ma,Baojian Zhou,Deqing Yang,Yanghua Xiao*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLM）在上下文学习（ICL）中解决常微分方程（ODE）的能力，发现GPT-2能有效学习ODE算法，性能接近或优于欧拉方法，并展示出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索ICL在非线性数值问题中的机制，揭示LLM在解决ODE任务中的潜力。

Method: 将ODE问题及其解作为序列提示，评估GPT-2在两类ODE任务上的表现。

Result: GPT-2能学习ODE算法，性能接近欧拉方法，且随着示例增加精度呈指数提升，还能泛化到分布外问题。

Conclusion: 研究为ICL在NLP中的机制提供了新见解，展示了其在解决非线性数值问题中的潜力。

Abstract: In-Context Learning (ICL) has emerged as a new paradigm in large language
models (LLMs), enabling them to perform novel tasks by conditioning on a few
examples embedded in the prompt. Yet, the highly nonlinear behavior of ICL for
NLP tasks remains poorly understood. To shed light on its underlying
mechanisms, this paper investigates whether LLMs can solve ordinary
differential equations (ODEs) under the ICL setting. We formulate standard ODE
problems and their solutions as sequential prompts and evaluate GPT-2 models on
these tasks. Experiments on two types of ODEs show that GPT-2 can effectively
learn a meta-ODE algorithm, with convergence behavior comparable to, or better
than, the Euler method, and achieve exponential accuracy gains with increasing
numbers of demonstrations. Moreover, the model generalizes to
out-of-distribution (OOD) problems, demonstrating robust extrapolation
capabilities. These empirical findings provide new insights into the mechanisms
of ICL in NLP and its potential for solving nonlinear numerical problems.

</details>


### [30] [Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree](https://arxiv.org/abs/2508.03038)
*Qi Peng,Jialin Cui,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.AI

TL;DR: 论文提出了一种名为Tree-of-Reasoning (ToR)的多智能体框架，用于提升大型语言模型在复杂医学诊断任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在复杂医学诊断任务中表现不足，主要因推理深度不够导致信息丢失或逻辑跳跃，从而引发诊断错误。

Method: ToR框架采用树状结构记录推理路径和临床证据，并引入交叉验证机制确保多智能体决策的一致性。

Result: 实验结果表明，该框架在真实医学数据上优于现有基线方法。

Conclusion: ToR框架显著提升了多智能体在复杂医学场景中的临床推理能力。

Abstract: Large language models (LLMs) have shown great potential in the medical
domain. However, existing models still fall short when faced with complex
medical diagnosis task in the real world. This is mainly because they lack
sufficient reasoning depth, which leads to information loss or logical jumps
when processing a large amount of specialized medical data, leading to
diagnostic errors. To address these challenges, we propose Tree-of-Reasoning
(ToR), a novel multi-agent framework designed to handle complex scenarios.
Specifically, ToR introduces a tree structure that can clearly record the
reasoning path of LLMs and the corresponding clinical evidence. At the same
time, we propose a cross-validation mechanism to ensure the consistency of
multi-agent decision-making, thereby improving the clinical reasoning ability
of multi-agents in complex medical scenarios. Experimental results on
real-world medical data show that our framework can achieve better performance
than existing baseline methods.

</details>


### [31] [Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning](https://arxiv.org/abs/2508.03054)
*Rui Pu,Chaozhuo Li,Rui Ha,Litian Zhang,Lirong Qiu,Xi Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种认知驱动防御（CDD）框架，通过模拟人类认知推理和强化学习来防御大型语言模型的越狱攻击，表现出色且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖浅层模式匹配，难以应对新型攻击策略，因此需要更通用的防御机制。

Method: CDD框架通过结构化推理链（全局感知和局部分析）识别隐藏操作，并结合监督微调和熵引导强化学习（EG-GRPO）增强泛化能力。

Result: 实验表明CDD在防御性能和泛化能力上达到最先进水平。

Conclusion: CDD为LLM的安全部署提供了一种高效且通用的防御方案。

Abstract: Defending large language models (LLMs) against jailbreak attacks is essential
for their safe and reliable deployment. Existing defenses often rely on shallow
pattern matching, which struggles to generalize to novel and unseen attack
strategies. To address this challenge, we propose the Cognitive-Driven Defense
(CDD) framework, which targets the underlying structure of jailbreak prompts by
applying meta-operations, defined as basic manipulations that conceal harmful
intent.CDD emulates human cognitive reasoning through a structured reasoning
chain. It begins with a global perception of the prompt and follows with a
localized analysis to uncover hidden manipulations. By applying supervised
fine-tuning on this structured chain, the model learns to identify and reason
about known manipulation patterns. To enhance generalization to unseen threats,
an entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to
encourage exploration of new types and variants of meta-operations. Experiments
demonstrate that CDD can achieve state-of-the-art defense performance and
exhibit strong generalization to unseen jailbreak attacks.

</details>


### [32] [ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification in Commercial Contracts](https://arxiv.org/abs/2508.03080)
*Shuang Liu,Zelong Li,Ruoyun Ma,Haiyan Zhao,Mengnan Du*

Main category: cs.AI

TL;DR: 论文评估开源与专有大语言模型在法律风险分析中的表现，发现专有模型表现更优，开源模型需针对性优化。


<details>
  <summary>Details</summary>
Motivation: 探索开源大语言模型在法律领域的潜力，满足本地部署需求并保护数据隐私。

Method: 使用Contract Understanding Atticus Dataset (CUAD)评估4个专有和15个开源LLM，分析其识别商业合同条款风险的能力。

Result: 专有模型表现更优，开源模型在特定维度有竞争力；模型规模增大效果提升减缓；推理模式影响性能；开源模型易漏检相关条款；量化加速但降低性能。

Conclusion: 开源模型需针对性优化以匹配专有模型，ContractEval为法律领域LLM发展提供基准。

Abstract: The potential of large language models (LLMs) in specialized domains such as
legal risk analysis remains underexplored. In response to growing interest in
locally deploying open-source LLMs for legal tasks while preserving data
confidentiality, this paper introduces ContractEval, the first benchmark to
thoroughly evaluate whether open-source LLMs could match proprietary LLMs in
identifying clause-level legal risks in commercial contracts. Using the
Contract Understanding Atticus Dataset (CUAD), we assess 4 proprietary and 15
open-source LLMs. Our results highlight five key findings: (1) Proprietary
models outperform open-source models in both correctness and output
effectiveness, though some open-source models are competitive in certain
specific dimensions. (2) Larger open-source models generally perform better,
though the improvement slows down as models get bigger. (3) Reasoning
("thinking") mode improves output effectiveness but reduces correctness, likely
due to over-complicating simpler tasks. (4) Open-source models generate "no
related clause" responses more frequently even when relevant clauses are
present. This suggests "laziness" in thinking or low confidence in extracting
relevant content. (5) Model quantization speeds up inference but at the cost of
performance drop, showing the tradeoff between efficiency and accuracy. These
findings suggest that while most LLMs perform at a level comparable to junior
legal assistants, open-source models require targeted fine-tuning to ensure
correctness and effectiveness in high-stakes legal settings. ContractEval
offers a solid benchmark to guide future development of legal-domain LLMs.

</details>


### [33] [EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design](https://arxiv.org/abs/2508.03082)
*Fei Liu,Yilu Liu,Qingfu Zhang,Xialiang Tong,Mingxuan Yuan*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型语言模型（LLM）的自动化启发式集合设计（AHSD）方法，通过生成互补的启发式集合来提升泛化能力，并提出了EoH-S算法实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅设计单一启发式，导致泛化能力不足，无法适应不同问题实例的多样性。

Method: 提出AHSD框架，目标为生成互补启发式集合，并设计EoH-S算法，结合互补种群管理和互补感知的模因搜索。

Result: 在三个任务上的实验表明，EoH-S显著优于现有方法，性能提升高达60%。

Conclusion: AHSD和EoH-S有效解决了单一启发式的泛化问题，为自动化启发式设计提供了新方向。

Abstract: Automated Heuristic Design (AHD) using Large Language Models (LLMs) has
achieved notable success in recent years. Despite the effectiveness of existing
approaches, they only design a single heuristic to serve all problem instances,
often inducing poor generalization across different distributions or settings.
To address this issue, we propose Automated Heuristic Set Design (AHSD), a new
formulation for LLM-driven AHD. The aim of AHSD is to automatically generate a
small-sized complementary heuristic set to serve diverse problem instances,
such that each problem instance could be optimized by at least one heuristic in
this set. We show that the objective function of AHSD is monotone and
supermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the
AHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary
population management and complementary-aware memetic search, EoH-S could
effectively generate a set of high-quality and complementary heuristics.
Comprehensive experimental results on three AHD tasks with diverse instances
spanning various sizes and distributions demonstrate that EoH-S consistently
outperforms existing state-of-the-art AHD methods and achieves up to 60\%
performance improvements.

</details>


### [34] [MissDDIM: Deterministic and Efficient Conditional Diffusion for Tabular Data Imputation](https://arxiv.org/abs/2508.03083)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.AI

TL;DR: MissDDIM是一种基于DDIM的条件扩散框架，用于表格数据填补，解决了现有扩散模型的高延迟和输出不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于DDPM的缺失数据填补方法存在高推理延迟和输出不稳定性，限制了其在真实场景中的应用。

Method: 提出MissDDIM，一种基于DDIM的条件扩散框架，用于表格数据填补。

Result: MissDDIM通过确定性采样减少了输出不稳定性，同时保持了填补的多样性。

Conclusion: MissDDIM为表格数据填补提供了一种高效且稳定的解决方案。

Abstract: Diffusion models have recently emerged as powerful tools for missing data
imputation by modeling the joint distribution of observed and unobserved
variables. However, existing methods, typically based on stochastic denoising
diffusion probabilistic models (DDPMs), suffer from high inference latency and
variable outputs, limiting their applicability in real-world tabular settings.
To address these deficiencies, we present in this paper MissDDIM, a conditional
diffusion framework that adapts Denoising Diffusion Implicit Models (DDIM) for
tabular imputation. While stochastic sampling enables diverse completions, it
also introduces output variability that complicates downstream processing.

</details>


### [35] [T2UE: Generating Unlearnable Examples from Text Descriptions](https://arxiv.org/abs/2508.03091)
*Xingjun Ma,Hanxun Huang,Tianwei Song,Ye Sun,Yifeng Gao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: T2UE框架通过仅使用文本描述生成不可学习示例（UEs），解决了现有方法需要暴露原始图像数据的隐私矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有生成UEs的方法需要联合优化图像和文本噪声，计算成本高且依赖第三方服务，导致隐私泄露风险。

Method: T2UE利用文本到图像（T2I）模型将文本描述映射到图像（噪声）空间，并结合误差最小化框架生成有效的不可学习噪声。

Result: 实验表明，T2UE保护的数据显著降低了最先进模型在下游任务（如跨模态检索）中的性能，且保护效果泛化到多种架构和监督学习场景。

Conclusion: T2UE实现了“零接触数据保护”，仅需文本描述即可保护数据，无需直接暴露原始数据。

Abstract: Large-scale pre-training frameworks like CLIP have revolutionized multimodal
learning, but their reliance on web-scraped datasets, frequently containing
private user data, raises serious concerns about misuse. Unlearnable Examples
(UEs) have emerged as a promising countermeasure against unauthorized model
training, employing carefully crafted unlearnable noise to disrupt the learning
of meaningful representations from protected data. Current approaches typically
generate UEs by jointly optimizing unlearnable noise for both images and their
associated text descriptions (or labels). However, this optimization process is
often computationally prohibitive for on-device execution, forcing reliance on
external third-party services. This creates a fundamental privacy paradox:
users must initially expose their data to these very services to achieve
protection, thereby compromising privacy in the process. Such a contradiction
has severely hindered the development of practical, scalable data protection
solutions. To resolve this paradox, we introduce \textbf{Text-to-Unlearnable
Example (T2UE)}, a novel framework that enables users to generate UEs using
only text descriptions. T2UE circumvents the need for original image data by
employing a text-to-image (T2I) model to map text descriptions into the image
(noise) space, combined with an error-minimization framework to produce
effective unlearnable noise. Extensive experiments show that T2UE-protected
data substantially degrades performance in downstream tasks (e.g., cross-modal
retrieval) for state-of-the-art models. Notably, the protective effect
generalizes across diverse architectures and even to supervised learning
settings. Our work demonstrates the feasibility of "zero-contact data
protection", where personal data can be safeguarded based solely on their
textual descriptions, eliminating the need for direct data exposure.

</details>


### [36] [Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework](https://arxiv.org/abs/2508.03092)
*Zikun Cui,Tianyi Huang,Chia-En Chiang,Cuiqianhe Du*

Main category: cs.AI

TL;DR: 本文提出了一种可验证的虚假信息检测LLM代理，通过动态交互与多源验证，超越传统二元判断，显著提升检测准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的普及，虚假信息检测变得愈发重要且复杂，传统方法难以满足需求。

Method: 设计了包含精确网络搜索、来源可信度评估和数值声明验证工具的代理架构，支持多步验证策略和证据记录。

Result: 在FakeNewsNet等数据集上评估，代理在准确性、推理透明度和抗改写能力上优于基线方法。

Conclusion: 该代理为可信AI辅助事实核查提供了新范式。

Abstract: With the proliferation of Large Language Models (LLMs), the detection of
misinformation has become increasingly important and complex. This research
proposes an innovative verifiable misinformation detection LLM agent that goes
beyond traditional true/false binary judgments. The agent actively verifies
claims through dynamic interaction with diverse web sources, assesses
information source credibility, synthesizes evidence, and provides a complete
verifiable reasoning process. Our designed agent architecture includes three
core tools: precise web search tool, source credibility assessment tool and
numerical claim verification tool. These tools enable the agent to execute
multi-step verification strategies, maintain evidence logs, and form
comprehensive assessment conclusions. We evaluate using standard misinformation
datasets such as FakeNewsNet, comparing with traditional machine learning
models and LLMs. Evaluation metrics include standard classification metrics,
quality assessment of reasoning processes, and robustness testing against
rewritten content. Experimental results show that our agent outperforms
baseline methods in misinformation detection accuracy, reasoning transparency,
and resistance to information rewriting, providing a new paradigm for
trustworthy AI-assisted fact-checking.

</details>


### [37] [AgentSME for Simulating Diverse Communication Modes in Smart Education](https://arxiv.org/abs/2508.03109)
*Wen-Xi Yang,Tian-Fang Zhao*

Main category: cs.AI

TL;DR: 本文提出了一个基于LLM的统一生成代理框架AgentSME，用于智能教育，通过三种通信模式（Solo、Mono、Echo）验证了Echo模式在准确性上的优势，并评估了不同LLM的多样性。


<details>
  <summary>Details</summary>
Motivation: 智能教育中的生成代理模型尚未充分发展，教育环境的复杂性和个性化需求是主要挑战。

Method: 提出AgentSME框架，采用三种通信模式（Solo、Mono、Echo），以准确性为主要指标，辅以多样性评估，测试了六种LLM。

Result: Echo通信模式的代理在准确性上表现最佳，DeepSeek在多样性上表现最优。

Conclusion: 研究为提升代理学习能力和智能教育模型提供了有价值的信息。

Abstract: Generative agent models specifically tailored for smart education are
critical, yet remain relatively underdeveloped. A key challenge stems from the
inherent complexity of educational contexts: learners are human beings with
various cognitive behaviors, and pedagogy is fundamentally centered on
personalized human-to-human communication. To address this issue, this paper
proposes AgentSME, a unified generative agent framework powered by LLM. Three
directional communication modes are considered in the models, namely Solo,
Mono, and Echo, reflecting different types of agency autonomy and communicative
reciprocity. Accuracy is adopted as the primary evaluation metric, complemented
by three diversity indices designed to assess the diversity of reasoning
contents. Six widely used LLMs are tested to validate the robustness of
communication modes across different model tiers, which are equally divided
into base-capacity and high-capacity configurations. The results show that
generative agents that employ the Echo communication mode achieve the highest
accuracy scores, while DeepSeek exhibits the greatest diversity. This study
provides valuable information to improve agent learning capabilities and
inspire smart education models.

</details>


### [38] [Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation](https://arxiv.org/abs/2508.03117)
*Vinicius Lima,Dzung T. Phan,Jayant Kalagnanam,Dhaval Patel,Nianjun Zhou*

Main category: cs.AI

TL;DR: 提出了一种通过可验证合成数据生成管道训练可信赖大型语言模型（LLM）代理的框架，专注于线性和混合整数线性规划，实现了从结构化符号表示到自然语言描述、数学公式及可执行代码的系统生成。


<details>
  <summary>Details</summary>
Motivation: 为优化任务构建可靠且可验证的LLM代理，提供一种可扩展且原则性的方法。

Method: 通过程序化生成带有已知最优解的实例，确保数据质量，并利用教师模型生成逐步演示。开发了OptiTrust代理，实现从自然语言到可执行代码的多阶段翻译。

Result: 在7个数据集中，OptiTrust在6个上表现最佳，并在3个上比次优算法至少高出8个百分点。

Conclusion: 该方法为构建适用于实际优化应用的可信赖LLM代理提供了可扩展、可验证的路径。

Abstract: We present a framework for training trustworthy large language model (LLM)
agents for optimization modeling via a verifiable synthetic data generation
pipeline. Focusing on linear and mixed-integer linear programming, our approach
begins with structured symbolic representations and systematically produces
natural language descriptions, mathematical formulations, and solver-executable
code. By programmatically constructing each instance with known optimal
solutions, the pipeline ensures full verifiability and enables automatic
filtering of low-quality demonstrations generated by teacher models. Each
dataset instance includes a structured representation of the optimization
problem, a corresponding natural language description, the verified optimal
solution, and step-by-step demonstrations - generated by a teacher model - that
show how to model and solve the problem across multiple optimization modeling
languages. This enables supervised fine-tuning of open-source LLMs specifically
tailored to optimization tasks. To operationalize this pipeline, we introduce
OptiTrust, a modular LLM agent that performs multi-stage translation from
natural language to solver-ready code, leveraging stepwise demonstrations,
multi-language inference, and majority-vote cross-validation. Our agent
achieves state-of-the-art performance on standard benchmarks. Out of 7
datasets, it achieves the highest accuracy on six and outperforms the next-best
algorithm by at least 8 percentage on three of them. Our approach provides a
scalable, verifiable, and principled path toward building reliable LLM agents
for real-world optimization applications.

</details>


### [39] [Can Large Language Models Bridge the Gap in Environmental Knowledge?](https://arxiv.org/abs/2508.03149)
*Linda Smail,David Santandreu Calonge,Firuz Kamalov,Nur H. Orak*

Main category: cs.AI

TL;DR: 研究探讨AI模型（如GPT-3.5、GPT-4等）在弥补大学生环境教育知识差距中的潜力，发现AI虽能提供丰富知识，但仍需人类专家验证准确性。


<details>
  <summary>Details</summary>
Motivation: 评估AI模型在环境教育中的有效性，以弥补学生知识差距。

Method: 使用标准化工具EKT-19及针对性问题，比较AI模型与大学生环境知识水平。

Result: AI模型具备丰富知识库，但信息准确性需环境科学专家验证。

Conclusion: AI在环境教育中具有潜力，但需结合人类专家以确保信息准确。

Abstract: This research investigates the potential of Artificial Intelligence (AI)
models to bridge the knowledge gap in environmental education among university
students. By focusing on prominent large language models (LLMs) such as
GPT-3.5, GPT-4, GPT-4o, Gemini, Claude Sonnet, and Llama 2, the study assesses
their effectiveness in conveying environmental concepts and, consequently,
facilitating environmental education. The investigation employs a standardized
tool, the Environmental Knowledge Test (EKT-19), supplemented by targeted
questions, to evaluate the environmental knowledge of university students in
comparison to the responses generated by the AI models. The results of this
study suggest that while AI models possess a vast, readily accessible, and
valid knowledge base with the potential to empower both students and academic
staff, a human discipline specialist in environmental sciences may still be
necessary to validate the accuracy of the information provided.

</details>


### [40] [Causal identification with $Y_0$](https://arxiv.org/abs/2508.03167)
*Charles Tapley Hoyt,Craig Bakker,Richard J. Callahan,Joseph Cottam,August George,Benjamin M. Gyori,Haley M. Hummel,Nathaniel Merrill,Sara Mohammad Taheri,Pruthvi Prakash Navada,Marc-Antoine Parent,Adam Rupe,Olga Vitek,Jeremy Zucker*

Main category: cs.AI

TL;DR: $Y_0$是一个Python包，用于实现因果识别算法，支持干预、反事实和可迁移性查询，适用于随机对照试验和观察性研究。


<details>
  <summary>Details</summary>
Motivation: 帮助研究者在尝试估计因果关系的强度之前，先定性分析因果关系是否可以从可用数据中估计。

Method: 提供领域特定语言表示因果查询和符号概率表达式，支持含未观测混杂的因果图模型（如ADMGs），并实现多种识别算法。

Result: $Y_0$能将因果查询转化为符号估计量，支持非参数估计，并开源提供工具。

Conclusion: $Y_0$为因果推断提供了实用的工具，支持复杂查询和模型，促进因果研究的定性分析。

Abstract: We present the $Y_0$ Python package, which implements causal identification
algorithms that apply interventional, counterfactual, and transportability
queries to data from (randomized) controlled trials, observational studies, or
mixtures thereof. $Y_0$ focuses on the qualitative investigation of causation,
helping researchers determine whether a causal relationship can be estimated
from available data before attempting to estimate how strong that relationship
is. Furthermore, $Y_0$ provides guidance on how to transform the causal query
into a symbolic estimand that can be non-parametrically estimated from the
available data. $Y_0$ provides a domain-specific language for representing
causal queries and estimands as symbolic probabilistic expressions, tools for
representing causal graphical models with unobserved confounders, such as
acyclic directed mixed graphs (ADMGs), and implementations of numerous
identification algorithms from the recent causal inference literature. The
$Y_0$ source code can be found under the MIT License at
https://github.com/y0-causal-inference/y0 and it can be installed with pip
install y0.

</details>


### [41] [Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions](https://arxiv.org/abs/2508.03173)
*Jingxuan Wei,Caijun Jia,Qi Chen,Honghao He,Linzhuang Sun,Conghui He,Lijun Wu,Bihui Yu,Cheng Tan*

Main category: cs.AI

TL;DR: Geoint-R1是一个多模态推理框架，旨在从文本描述和视觉图表生成可形式化验证的几何解决方案，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在形式化几何推理（尤其是动态构建和验证辅助几何元素）方面表现不佳，因此需要改进。

Method: Geoint-R1整合了辅助元素构建、通过Lean4表示的形式推理和交互式可视化。

Result: 实验表明，Geoint-R1在需要显式辅助元素构建的挑战性问题中显著优于现有模型。

Conclusion: Geoint-R1为形式化几何推理提供了有效解决方案，并通过Geoint基准推动了该领域的发展。

Abstract: Mathematical geometric reasoning is essential for scientific discovery and
educational development, requiring precise logic and rigorous formal
verification. While recent advances in Multimodal Large Language Models (MLLMs)
have improved reasoning tasks, existing models typically struggle with formal
geometric reasoning, particularly when dynamically constructing and verifying
auxiliary geometric elements. To address these challenges, we introduce
Geoint-R1, a multimodal reasoning framework designed to generate formally
verifiable geometric solutions from textual descriptions and visual diagrams.
Geoint-R1 uniquely integrates auxiliary elements construction, formal reasoning
represented via Lean4, and interactive visualization. To systematically
evaluate and advance formal geometric reasoning, we propose the Geoint
benchmark, comprising 1,885 rigorously annotated geometry problems across
diverse topics such as plane, spatial, and solid geometry. Each problem
includes structured textual annotations, precise Lean4 code for auxiliary
constructions, and detailed solution steps verified by experts. Extensive
experiments demonstrate that Geoint-R1 significantly surpasses existing
multimodal and math-specific reasoning models, particularly on challenging
problems requiring explicit auxiliary element constructions.

</details>


### [42] [InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation](https://arxiv.org/abs/2508.03174)
*Tian-Fang Zhao,Wen-Xi Yang*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的智能代理模型InqEduAgent，用于模拟和选择适合探究式学习的学习伙伴，通过自适应匹配算法优化学习效果。


<details>
  <summary>Details</summary>
Motivation: 探究式教育中学习伙伴的选择通常依赖经验或规则，缺乏科学规划和灵活性，导致知识扩展不足。

Method: 设计生成代理捕捉学习者的认知和评估特征，采用高斯过程增强的自适应匹配算法识别先验知识模式。

Result: 实验表明InqEduAgent在多数知识学习场景和不同能力的LLM环境中表现最优。

Conclusion: 该研究推动了基于人类学习伙伴的智能分配和AI学习伙伴的构建，相关资源已开源。

Abstract: Collaborative partnership matters in inquiry-oriented education. However,
most study partners are selected either rely on experience-based assignments
with little scientific planning or build on rule-based machine assistants,
encountering difficulties in knowledge expansion and inadequate flexibility.
This paper proposes an LLM-empowered agent model for simulating and selecting
learning partners tailored to inquiry-oriented learning, named InqEduAgent.
Generative agents are designed to capture cognitive and evaluative features of
learners in real-world scenarios. Then, an adaptive matching algorithm with
Gaussian process augmentation is formulated to identify patterns within prior
knowledge. Optimal learning-partner matches are provided for learners facing
different exercises. The experimental results show the optimal performance of
InqEduAgent in most knowledge-learning scenarios and LLM environment with
different levels of capabilities. This study promotes the intelligent
allocation of human-based learning partners and the formulation of AI-based
learning partners. The code, data, and appendix are publicly available at
https://github.com/InqEduAgent/InqEduAgent.

</details>


### [43] [Full-History Graphs with Edge-Type Decoupled Networks for Temporal Reasoning](https://arxiv.org/abs/2508.03251)
*Osama Mohammed,Jiaxin Pan,Mojtaba Nayyeri,Daniel Hernández,Steffen Staab*

Main category: cs.AI

TL;DR: 论文提出了一种全历史图模型和ETDNet网络，用于建模实体间的动态交互，显著提升了驾驶意图预测和比特币欺诈检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现实任务中需要建模实体间的动态交互（如交通预测、金融欺诈检测），传统方法无法同时捕捉结构和时间关系。

Method: 引入全历史图，区分时间步内和时间步间边；设计ETDNet网络，结合图注意力和时间注意力模块。

Result: 在Waymo和Elliptic++数据集上，ETDNet分别将准确率和F1分数提升至75.6%和88.1%。

Conclusion: 通过区分结构和时间关系，全历史图和ETDNet显著提升了动态交互建模的性能。

Abstract: Modeling evolving interactions among entities is critical in many real-world
tasks. For example, predicting driver maneuvers in traffic requires tracking
how neighboring vehicles accelerate, brake, and change lanes relative to one
another over consecutive frames. Likewise, detecting financial fraud hinges on
following the flow of funds through successive transactions as they propagate
through the network. Unlike classic time-series forecasting, these settings
demand reasoning over who interacts with whom and when, calling for a
temporal-graph representation that makes both the relations and their evolution
explicit. Existing temporal-graph methods typically use snapshot graphs to
encode temporal evolution. We introduce a full-history graph that instantiates
one node for every entity at every time step and separates two edge sets: (i)
intra-time-step edges that capture relations within a single frame and (ii)
inter-time-step edges that connect an entity to itself at consecutive steps. To
learn on this graph we design an Edge-Type Decoupled Network (ETDNet) with
parallel modules: a graph-attention module aggregates information along
intra-time-step edges, a multi-head temporal-attention module attends over an
entity's inter-time-step history, and a fusion module combines the two messages
after every layer. Evaluated on driver-intention prediction (Waymo) and Bitcoin
fraud detection (Elliptic++), ETDNet consistently surpasses strong baselines,
lifting Waymo joint accuracy to 75.6\% (vs. 74.1\%) and raising Elliptic++
illicit-class F1 to 88.1\% (vs. 60.4\%). These gains demonstrate the benefit of
representing structural and temporal relations as distinct edges in a single
graph.

</details>


### [44] [ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools](https://arxiv.org/abs/2508.03284)
*Shaofeng Yin,Ting Lei,Yang Liu*

Main category: cs.AI

TL;DR: ToolVQA是一个大规模多模态数据集，旨在提升大型基础模型在真实世界工具使用中的多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究在工具增强的视觉问答（VQA）中表现良好，但在真实世界多模态环境中的工具使用能力仍有不足。

Method: 提出ToolVQA数据集和ToolEngine数据生成流水线，采用深度优先搜索（DFS）和动态上下文示例匹配机制模拟人类工具使用推理。

Result: 在ToolVQA上微调的7B LFMs在测试集和多个OOD数据集上表现优异，甚至超越GPT-3.5-turbo。

Conclusion: ToolVQA和ToolEngine有效提升了模型在真实世界工具使用场景中的泛化能力。

Abstract: Integrating external tools into Large Foundation Models (LFMs) has emerged as
a promising approach to enhance their problem-solving capabilities. While
existing studies have demonstrated strong performance in tool-augmented Visual
Question Answering (VQA), recent benchmarks reveal significant gaps in
real-world tool-use proficiency, particularly in functionally diverse
multimodal settings requiring multi-step reasoning. In this work, we introduce
ToolVQA, a large-scale multimodal dataset comprising 23K instances, designed to
bridge this gap. Unlike previous datasets that rely on synthetic scenarios and
simplified queries, ToolVQA features real-world visual contexts and challenging
implicit multi-step reasoning tasks, better aligning with real user
interactions. To construct this dataset, we propose ToolEngine, a novel data
generation pipeline that employs Depth-First Search (DFS) with a dynamic
in-context example matching mechanism to simulate human-like tool-use
reasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse task
domains, with an average inference length of 2.78 reasoning steps per instance.
The fine-tuned 7B LFMs on ToolVQA not only achieve impressive performance on
our test set but also surpass the large close-sourced model GPT-3.5-turbo on
various out-of-distribution (OOD) datasets, demonstrating strong
generalizability to real-world tool-use scenarios.

</details>


### [45] [Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science](https://arxiv.org/abs/2508.03341)
*Jiayan Nan,Wenquan Ma,Wenlong Wu,Yize Chen*

Main category: cs.AI

TL;DR: Nemori是一种新型自组织记忆架构，解决了大型语言模型在长上下文中的记忆问题，通过两步对齐原则和预测校准原则提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统在定义基本记忆单元和知识提取方面存在局限性，无法实现真正的学习和进化。

Method: Nemori采用两步对齐原则组织对话流为语义连贯的片段，并通过预测校准原则主动学习预测差距。

Result: 在LoCoMo和LongMemEval基准测试中，Nemori显著优于现有系统，尤其在长上下文中表现突出。

Conclusion: Nemori为自主代理处理长期动态工作流提供了可行路径。

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities, yet their
inability to maintain persistent memory in long contexts limits their
effectiveness as autonomous agents in long-term interactions. While existing
memory systems have made progress, their reliance on arbitrary granularity for
defining the basic memory unit and passive, rule-based mechanisms for knowledge
extraction limits their capacity for genuine learning and evolution. To address
these foundational limitations, we present Nemori, a novel self-organizing
memory architecture inspired by human cognitive principles. Nemori's core
innovation is twofold: First, its Two-Step Alignment Principle, inspired by
Event Segmentation Theory, provides a principled, top-down method for
autonomously organizing the raw conversational stream into semantically
coherent episodes, solving the critical issue of memory granularity. Second,
its Predict-Calibrate Principle, inspired by the Free-energy Principle, enables
the agent to proactively learn from prediction gaps, moving beyond pre-defined
heuristics to achieve adaptive knowledge evolution. This offers a viable path
toward handling the long-term, dynamic workflows of autonomous agents.
Extensive experiments on the LoCoMo and LongMemEval benchmarks demonstrate that
Nemori significantly outperforms prior state-of-the-art systems, with its
advantage being particularly pronounced in longer contexts.

</details>


### [46] [Adaptive AI Agent Placement and Migration in Edge Intelligence Systems](https://arxiv.org/abs/2508.03345)
*Xingdan Wang,Jiayi He,Zhiqing Tang,Jianxiong Guo,Jiong Lou,Liping Qian,Tian Wang,Weijia Jia*

Main category: cs.AI

TL;DR: 本文提出了一种针对动态边缘环境中基于LLM的AI代理的系统化部署和管理解决方案，通过自适应框架优化资源利用和QoS。


<details>
  <summary>Details</summary>
Motivation: 随着LLM（如ChatGPT和Claude）的兴起，需要能够实时处理任务的AI代理。然而，将数据密集型、多模态的边缘工作负载迁移到云数据中心会引入显著延迟。边缘部署虽能提高效率，但资源有限且异构，维护移动用户的QoS需要复杂的代理迁移。

Method: 提出了一种自适应框架，用于AI代理在边缘智能系统中的放置和迁移。该方法建模资源约束和延迟/成本，利用蚁群算法和基于LLM的优化进行高效决策。

Result: 在分布式系统中实现并通过全球分布的边缘服务器验证，该方案显著降低了部署延迟和迁移成本。

Conclusion: 该框架为动态边缘环境中基于LLM的AI代理提供了高效的部署和管理解决方案，优化了资源利用和QoS。

Abstract: The rise of LLMs such as ChatGPT and Claude fuels the need for AI agents
capable of real-time task handling. However, migrating data-intensive,
multi-modal edge workloads to cloud data centers, traditionally used for agent
deployment, introduces significant latency. Deploying AI agents at the edge
improves efficiency and reduces latency. However, edge environments present
challenges due to limited and heterogeneous resources. Maintaining QoS for
mobile users necessitates agent migration, which is complicated by the
complexity of AI agents coordinating LLMs, task planning, memory, and external
tools. This paper presents the first systematic deployment and management
solution for LLM-based AI agents in dynamic edge environments. We propose a
novel adaptive framework for AI agent placement and migration in edge
intelligence systems. Our approach models resource constraints and
latency/cost, leveraging ant colony algorithms and LLM-based optimization for
efficient decision-making. It autonomously places agents to optimize resource
utilization and QoS and enables lightweight agent migration by transferring
only essential state. Implemented on a distributed system using AgentScope and
validated across globally distributed edge servers, our solution significantly
reduces deployment latency and migration costs.

</details>


### [47] [Compressing Chain-of-Thought in LLMs via Step Entropy](https://arxiv.org/abs/2508.03346)
*Zeju Li,Jianyuan Zhong,Ziyang Zheng,Xiangyu Wen,Zhijian Xu,Yingying Cheng,Fan Zhang,Qiang Xu*

Main category: cs.AI

TL;DR: 论文提出了一种基于步骤熵的CoT压缩框架，通过识别冗余步骤显著提升LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: LLMs使用CoT提示时生成的冗长推理过程存在冗余，增加了推理成本并降低了效率。

Method: 引入步骤熵量化推理步骤的信息贡献，提出两阶段训练策略（SFT和GRPO强化学习），结合[SKIP]标记生成压缩CoT。

Result: 实验表明80%的低熵步骤可被修剪且不影响准确性，显著提升推理效率。

Conclusion: 该方法为LLM实际部署提供了高效解决方案，并深化了对推理结构的理解。

Abstract: Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at
complex reasoning but generate verbose thought processes with considerable
redundancy, leading to increased inference costs and reduced efficiency. We
introduce a novel CoT compression framework based on step entropy, a metric
that quantifies the informational contribution of individual reasoning steps to
identify redundancy. Through theoretical analysis and extensive empirical
validation on mathematical reasoning benchmarks, we demonstrate that steps with
low entropy are indeed highly redundant. Our experiments reveal that an
astonishing 80\% of low-entropy intermediate steps can be pruned with minor
degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and
Qwen3-8B. This finding sharply contrasts with random or high-entropy pruning,
which severely impairs reasoning performance. Building on this, we propose a
novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and
Group Relative Policy Optimization (GRPO) reinforcement learning. This approach
enables LLMs to autonomously learn to generate compressed COTs during inference
by strategically incorporating [SKIP] tokens. Our method significantly enhances
LLM inference efficiency while rigorously preserving accuracy, offering
profound implications for practical LLM deployment and a deeper understanding
of reasoning structures.

</details>


### [48] [CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment](https://arxiv.org/abs/2508.03360)
*Feng Rui,Zhiyao Luo,Wei Wang,Yuting Song,Yong Liu,Tingting Zhu,Jianqing Li,Xingyao Wang*

Main category: cs.AI

TL;DR: CogBench是一个评估大语言模型在多语言和多临床环境下认知障碍评估泛化能力的基准，结果显示传统模型泛化能力差，而采用链式思维提示的LLMs表现更好，LoRA微调进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前认知障碍自动评估方法在多语言和临床环境下的泛化能力不足，限制了其实际应用。

Method: 提出CogBench基准，使用统一的多模态流程评估LLMs在英语和普通话数据集上的表现，包括ADReSSo、NCMMSC2021-AD和CIR-E。

Result: 传统深度学习模型跨域表现差，LLMs通过链式思维提示表现更好，LoRA微调显著提升泛化能力。

Conclusion: 研究为构建临床实用且语言鲁棒的认知评估工具提供了重要进展。

Abstract: Automatic assessment of cognitive impairment from spontaneous speech offers a
promising, non-invasive avenue for early cognitive screening. However, current
approaches often lack generalizability when deployed across different languages
and clinical settings, limiting their practical utility. In this study, we
propose CogBench, the first benchmark designed to evaluate the cross-lingual
and cross-site generalizability of large language models (LLMs) for
speech-based cognitive impairment assessment. Using a unified multimodal
pipeline, we evaluate model performance on three speech datasets spanning
English and Mandarin: ADReSSo, NCMMSC2021-AD, and a newly collected test set,
CIR-E. Our results show that conventional deep learning models degrade
substantially when transferred across domains. In contrast, LLMs equipped with
chain-of-thought prompting demonstrate better adaptability, though their
performance remains sensitive to prompt design. Furthermore, we explore
lightweight fine-tuning of LLMs via Low-Rank Adaptation (LoRA), which
significantly improves generalization in target domains. These findings offer a
critical step toward building clinically useful and linguistically robust
speech-based cognitive assessment tools.

</details>


### [49] [A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning](https://arxiv.org/abs/2508.03366)
*Michael K. Chen*

Main category: cs.AI

TL;DR: 论文探讨了神经符号AI在通用逻辑推理中的两种主要方法（集成与混合），并通过案例研究比较了它们的潜力，发现混合方法更具前景。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在通用逻辑推理中表现不佳，缺乏确定性和可解释性，因此研究神经符号AI方法以改进逻辑推理能力。

Method: 通过两种代表性模型（LNN和LLM-SS）作为案例研究，分析集成与混合方法在通用逻辑推理中的表现。

Result: 混合方法（LLM-SS）在通用逻辑推理中更具潜力，因其推理链更可解释且保留了LLM的优势。

Conclusion: 提出了一种基于LLM-SS的通用框架，支持未来混合方法的研究，具有模块化、模型无关和领域无关的特点。

Abstract: General logical reasoning, defined as the ability to reason deductively on
domain-agnostic tasks, continues to be a challenge for large language models
(LLMs). Current LLMs fail to reason deterministically and are not
interpretable. As such, there has been a recent surge in interest in
neurosymbolic AI, which attempts to incorporate logic into neural networks. We
first identify two main neurosymbolic approaches to improving logical
reasoning: (i) the integrative approach comprising models where symbolic
reasoning is contained within the neural network, and (ii) the hybrid approach
comprising models where a symbolic solver, separate from the neural network,
performs symbolic reasoning. Both contain AI systems with promising results on
domain-specific logical reasoning benchmarks. However, their performance on
domain-agnostic benchmarks is understudied. To the best of our knowledge, there
has not been a comparison of the contrasting approaches that answers the
following question: Which approach is more promising for developing general
logical reasoning? To analyze their potential, the following best-in-class
domain-agnostic models are introduced: Logic Neural Network (LNN), which uses
the integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the
hybrid approach. Using both models as case studies and representatives of each
approach, our analysis demonstrates that the hybrid approach is more promising
for developing general logical reasoning because (i) its reasoning chain is
more interpretable, and (ii) it retains the capabilities and advantages of
existing LLMs. To support future works using the hybrid approach, we propose a
generalizable framework based on LLM-SS that is modular by design,
model-agnostic, domain-agnostic, and requires little to no human input.

</details>


### [50] [Board Game Arena: A Framework and Benchmark for Assessing Large Language Models via Strategic Play](https://arxiv.org/abs/2508.03368)
*Lucia Cipolina-Kun,Marianna Nezhurina,Jenia Jitsev*

Main category: cs.AI

TL;DR: Board Game Arena库为评估大型语言模型（LLM）的决策能力提供了一个框架，通过战略棋盘游戏实现系统比较。


<details>
  <summary>Details</summary>
Motivation: 通过游戏场景评估LLM的推理能力和博弈行为。

Method: 集成Google OpenSpiel库，支持多种游戏和代理类型，提供API访问和分布式执行。

Result: 提供全面的分析工具，支持对LLM推理轨迹的深入研究。

Conclusion: 该框架为LLM的实证评估提供了重要工具。

Abstract: The Board Game Arena library provides a framework for evaluating the decision
making abilities of large language models (LLMs) through strategic board games
implemented in Google OpenSpiel library. The framework enables systematic
comparisons between LLM based agents and other agents (random, human,
reinforcement learning agents, etc.) in various game scenarios by wrapping
multiple board and matrix games and supporting different agent types. It
integrates API access to models via LiteLLM, local model deployment via vLLM,
and offers distributed execution through Ray. Additionally it provides
extensive analysis tools for the LLM reasoning traces. This paper summarizes
the structure, key characteristics, and motivation of the repository,
highlighting how it contributes to the empirical evaluation of the reasoning of
LLM and game-theoretic behavior

</details>


### [51] [Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams](https://arxiv.org/abs/2508.03379)
*Wenxin Mao,Zhitao Wang Long Wang,Sirong Chen,Cuiyun Gao,Luyang Cao,Ziming Liu,Qiming Zhang,Jun Zhou,Zhi Jin*

Main category: cs.AI

TL;DR: 论文提出了一种名为UML2Dep的逐步代码生成框架，通过形式化规范解决自然语言描述的模糊性，提升代码生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 自然语言描述在复杂需求（如系统行为、条件逻辑和架构约束）中存在模糊性，难以捕捉隐式数据依赖。

Method: 1. 引入增强的UML序列图，结合决策表和API规范；2. 提出数据依赖推断（DDI）任务，构建显式数据依赖图。

Result: 通过形式化规范和DDI任务，显著减少了模糊性，提高了代码生成的准确性和效率。

Conclusion: UML2Dep框架通过形式化规范和数学推理，有效解决了复杂需求下的代码生成问题。

Abstract: Large language models (LLMs) excel at generating code from natural language
(NL) descriptions. However, the plain textual descriptions are inherently
ambiguous and often fail to capture complex requirements like intricate system
behaviors, conditional logic, and architectural constraints; implicit data
dependencies in service-oriented architectures are difficult to infer and
handle correctly. To bridge this gap, we propose a novel step-by-step code
generation framework named UML2Dep by leveraging unambiguous formal
specifications of complex requirements. First, we introduce an enhanced Unified
Modeling Language (UML) sequence diagram tailored for service-oriented
architectures. This diagram extends traditional visual syntax by integrating
decision tables and API specifications, explicitly formalizing structural
relationships and business logic flows in service interactions to rigorously
eliminate linguistic ambiguity. Second, recognizing the critical role of data
flow, we introduce a dedicated data dependency inference (DDI) task. DDI
systematically constructs an explicit data dependency graph prior to actual
code synthesis. To ensure reliability, we formalize DDI as a constrained
mathematical reasoning task through novel prompting strategies, aligning with
LLMs' excellent mathematical strengths. Additional static parsing and
dependency pruning further reduce context complexity and cognitive load
associated with intricate specifications, thereby enhancing reasoning accuracy
and efficiency.

</details>


### [52] [Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis](https://arxiv.org/abs/2508.03396)
*Rui Zou,Mengqi Wei,Yutao Zhu,Jirong Wen,Xin Zhao,Jing Chen*

Main category: cs.AI

TL;DR: 论文提出了一种动态对抗框架HSG，通过生成和诊断复杂错误来提升大语言模型的错误诊断能力，实验表明其效果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理和生成方面表现出色，但在识别和诊断复杂错误方面仍有不足，主要原因是训练目标过于关注正确答案，缺乏对错误的学习。

Method: 提出HSG框架，包含两个对抗角色：Sneaky（生成隐蔽的错误）和Diagnosis（检测错误），通过对抗协同进化提升能力。

Result: 在数学推理任务中，HSG显著提升了错误诊断能力，准确率比GPT-4o等基线模型高16.8%--31.4%。

Conclusion: HSG框架有效提升了模型的错误诊断能力，并发布了一个具有挑战性的数据集作为未来研究的基准。

Abstract: Large Language Models (LLMs) excel in reasoning and generation across
domains, but still struggle with identifying and diagnosing complex errors.
This stems mainly from training objectives that prioritize correct answers,
limiting exposure to and learning from errors. While recent studies have begun
to address this by introducing error signals, most rely on shallow, static
errors, restricting improvement in deep diagnostic ability. To overcome this,
we propose Hide and Seek Game (HSG), a dynamic adversarial framework for error
generation and diagnosis, and evaluate it on mathematical problem-solving. HSG
involves two adversarial roles: Sneaky, which "hides" by generating subtle,
deceptive reasoning errors, and Diagnosis, which "seeks" to accurately detect
them. Through adversarial co-evolution, both error stealth and diagnostic
precision are enhanced. Experiments on several math reasoning tasks show that
HSG significantly boosts error diagnosis, achieving 16.8\%--31.4\% higher
accuracy than baselines like GPT-4o. We also release a challenging dataset of
deceptive errors and diagnostic annotations as a benchmark for future research.

</details>


### [53] [Multi-Objective Infeasibility Diagnosis for Routing Problems Using Large Language Models](https://arxiv.org/abs/2508.03406)
*Kai Li,Ruihao Zheng,Xinye Hao,Zhenkun Wang*

Main category: cs.AI

TL;DR: MOID结合LLM代理和多目标优化，为不可行路由问题提供多种可操作建议。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM方法在诊断不可行模型时未考虑多调整方案的问题。

Method: MOID结合多目标优化和LLM代理，生成权衡解并分析。

Result: 在50类不可行路由问题上，MOID比现有方法提供更多实用建议。

Conclusion: MOID能自动生成多样化诊断建议，提升模型可行性和决策支持。

Abstract: In real-world routing problems, users often propose conflicting or
unreasonable requirements, which result in infeasible optimization models due
to overly restrictive or contradictory constraints, leading to an empty
feasible solution set. Existing Large Language Model (LLM)-based methods
attempt to diagnose infeasible models, but modifying such models often involves
multiple potential adjustments that these methods do not consider. To fill this
gap, we introduce Multi-Objective Infeasibility Diagnosis (MOID), which
combines LLM agents and multi-objective optimization within an automatic
routing solver, to provide a set of representative actionable suggestions.
Specifically, MOID employs multi-objective optimization to consider both path
cost and constraint violation, generating a set of trade-off solutions, each
encompassing varying degrees of model adjustments. To extract practical
insights from these solutions, MOID utilizes LLM agents to generate a solution
analysis function for the infeasible model. This function analyzes these
distinct solutions to diagnose the original infeasible model, providing users
with diverse diagnostic insights and suggestions. Finally, we compare MOID with
several LLM-based methods on 50 types of infeasible routing problems. The
results indicate that MOID automatically generates multiple diagnostic
suggestions in a single run, providing more practical insights for restoring
model feasibility and decision-making compared to existing methods.

</details>


### [54] [Data Overdose? Time for a Quadruple Shot: Knowledge Graph Construction using Enhanced Triple Extraction](https://arxiv.org/abs/2508.03438)
*Taine J. Elliott,Stephen P. Levitt,Ken Nixon,Martin Bekker*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型语言模型（LLM）的信息提取和知识图谱（KG）生成方法，用于连接生物医学知识，并通过上下文变量增强提取的三元组为四元组，提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 解决医学数据快速增长导致临床医生和研究人员难以系统化理解和应用最新知识的挑战。

Method: 使用LLM代理管道分解PubMed摘要为语义命题句子，提取KG三元组，并通过开放领域和基于本体的方法增强为四元组。

Result: 提取的三元组生成的自然语言句子与原命题的平均余弦相似度为0.874，上下文变量进一步提高了相似度。

Conclusion: 该方法为医学从业者提供了实时更新的集中知识源，并可能在其他领域实现类似效果。

Abstract: The rapid expansion of publicly-available medical data presents a challenge
for clinicians and researchers alike, increasing the gap between the volume of
scientific literature and its applications. The steady growth of studies and
findings overwhelms medical professionals at large, hindering their ability to
systematically review and understand the latest knowledge. This paper presents
an approach to information extraction and automatic knowledge graph (KG)
generation to identify and connect biomedical knowledge. Through a pipeline of
large language model (LLM) agents, the system decomposes 44 PubMed abstracts
into semantically meaningful proposition sentences and extracts KG triples from
these sentences. The triples are enhanced using a combination of open domain
and ontology-based information extraction methodologies to incorporate
ontological categories. On top of this, a context variable is included during
extraction to allow the triple to stand on its own - thereby becoming
`quadruples'. The extraction accuracy of the LLM is validated by comparing
natural language sentences generated from the enhanced triples to the original
propositions, achieving an average cosine similarity of 0.874. The similarity
for generated sentences of enhanced triples were compared with generated
sentences of ordinary triples showing an increase as a result of the context
variable. Furthermore, this research explores the ability for LLMs to infer new
relationships and connect clusters in the knowledge base of the knowledge
graph. This approach leads the way to provide medical practitioners with a
centralised, updated in real-time, and sustainable knowledge source, and may be
the foundation of similar gains in a wide variety of fields.

</details>


### [55] [Toward a Graph-Theoretic Model of Belief: Confidence, Credibility, and Structural Coherence](https://arxiv.org/abs/2508.03465)
*Saleh Nikooroo*

Main category: cs.AI

TL;DR: 论文提出了一种基于有向加权图的信念系统形式化方法，区分了信念的可信度和信心，避免了传统概率或逻辑模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统信念系统表示方法（如概率分布或逻辑命题）忽视了内部结构，无法处理矛盾或碎片化的认知状态。

Method: 使用有向加权图表示信念系统，节点为信念，边为认知关系，并分别定义可信度和信心函数。

Result: 该方法能够更精细地表示信念结构，支持对认知状态的分类和分析。

Conclusion: 该形式化为信念系统的内部组织提供了新的分析工具，优于现有方法。

Abstract: Belief systems are often treated as globally consistent sets of propositions
or as scalar-valued probability distributions. Such representations tend to
obscure the internal structure of belief, conflate external credibility with
internal coherence, and preclude the modeling of fragmented or contradictory
epistemic states. This paper introduces a minimal formalism for belief systems
as directed, weighted graphs. In this framework, nodes represent individual
beliefs, edges encode epistemic relationships (e.g., support or contradiction),
and two distinct functions assign each belief a credibility (reflecting source
trust) and a confidence (derived from internal structural support). Unlike
classical probabilistic models, our approach does not assume prior coherence or
require belief updating. Unlike logical and argumentation-based frameworks, it
supports fine-grained structural representation without committing to binary
justification status or deductive closure. The model is purely static and
deliberately excludes inference or revision procedures. Its aim is to provide a
foundational substrate for analyzing the internal organization of belief
systems, including coherence conditions, epistemic tensions, and
representational limits. By distinguishing belief structure from belief
strength, this formalism enables a richer classification of epistemic states
than existing probabilistic, logical, or argumentation-based approaches.

</details>


### [56] [Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes](https://arxiv.org/abs/2508.03484)
*Zhiyao Xu,Dan Zhao,Qingsong Zou,Qing Li,Yong Jiang,Yuhang Wang,Jingyu Xiao*

Main category: cs.AI

TL;DR: SmartGen是一个基于LLM的框架，用于合成上下文感知的用户行为数据，以支持智能家居模型的持续适应。


<details>
  <summary>Details</summary>
Motivation: 智能家居模型通常基于静态数据集训练，难以应对行为漂移（如季节变化、生活方式改变等），而收集新数据又存在成本高、隐私问题等挑战。

Method: SmartGen包含四个关键组件：时间和语义感知分割、语义感知序列压缩、图引导序列合成和两阶段异常过滤器。

Result: 实验表明，SmartGen显著提升了模型在行为漂移下的异常检测和行为预测性能，异常检测平均提升85.43%，行为预测提升70.51%。

Conclusion: SmartGen通过合成高质量行为数据，有效解决了智能家居模型的行为漂移问题。

Abstract: As smart homes become increasingly prevalent, intelligent models are widely
used for tasks such as anomaly detection and behavior prediction. These models
are typically trained on static datasets, making them brittle to behavioral
drift caused by seasonal changes, lifestyle shifts, or evolving routines.
However, collecting new behavior data for retraining is often impractical due
to its slow pace, high cost, and privacy concerns. In this paper, we propose
SmartGen, an LLM-based framework that synthesizes context-aware user behavior
data to support continual adaptation of downstream smart home models. SmartGen
consists of four key components. First, we design a Time and Semantic-aware
Split module to divide long behavior sequences into manageable, semantically
coherent subsequences under dual time-span constraints. Second, we propose
Semantic-aware Sequence Compression to reduce input length while preserving
representative semantics by clustering behavior mapping in latent space. Third,
we introduce Graph-guided Sequence Synthesis, which constructs a behavior
relationship graph and encodes frequent transitions into prompts, guiding the
LLM to generate data aligned with contextual changes while retaining core
behavior patterns. Finally, we design a Two-stage Outlier Filter to identify
and remove implausible or semantically inconsistent outputs, aiming to improve
the factual coherence and behavioral validity of the generated sequences.
Experiments on three real-world datasets demonstrate that SmartGen
significantly enhances model performance on anomaly detection and behavior
prediction tasks under behavioral drift, with anomaly detection improving by
85.43% and behavior prediction by 70.51% on average. The code is available at
https://github.com/horizonsinzqs/SmartGen.

</details>


### [57] [VQA support to Arabic Language Learning Educational Tool](https://arxiv.org/abs/2508.03488)
*Khaled Bachir Delassi,Lakhdar Zeggane,Hadda Cherroun,Abdelhamid Haouhat,Kaoutar Bouzouad*

Main category: cs.AI

TL;DR: 论文探讨了阿拉伯语学习工具的稀缺问题，提出了一种基于AI的教育工具，通过视觉问答活动提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语学习工具稀缺问题，尤其是支持现代教学法（如主动学习）的工具。

Method: 设计并评估了一种AI驱动的教育工具，利用视觉语言预训练模型生成交互式视觉测验，结合大语言模型定制阿拉伯语学习任务。

Result: 通过人工标注的1266个视觉测验评估，工具表现出较高的准确性，验证了其潜力。

Conclusion: 该工具为阿拉伯语学习者提供了可靠、个性化的互动学习体验，填补了教育资源的空白。

Abstract: We address the problem of scarcity of educational Arabic Language Learning
tools that advocate modern pedagogical models such as active learning which
ensures language proficiency. In fact, we investigate the design and evaluation
of an AI-powered educational tool designed to enhance Arabic language learning
for non-native speakers with beginner-to-intermediate proficiency level. The
tool leverages advanced AI models to generate interactive visual quizzes,
deploying Visual Question Answering as the primary activity. Adopting a
constructivist learning approach, the system encourages active learning through
real-life visual quizzes, and image-based questions that focus on improving
vocabulary, grammar, and comprehension. The system integrates Vision-Language
Pretraining models to generate contextually relevant image description from
which Large Language Model generate assignments based on customized Arabic
language Learning quizzes thanks to prompting.
  The effectiveness of the tool is evaluated through a manual annotated
benchmark consisting of 1266 real-life visual quizzes, with human participants
providing feedback. The results show a suitable accuracy rates, validating the
tool's potential to bridge the gap in Arabic language education and
highlighting the tool's promise as a reliable, AI-powered resource for Arabic
learners, offering personalized and interactive learning experiences.

</details>


### [58] [Error Detection and Correction for Interpretable Mathematics in Large Language Models](https://arxiv.org/abs/2508.03500)
*Yijin Yang,Cristina Cornelio,Mario Leiva,Paulo Shakarian*

Main category: cs.AI

TL;DR: EDCIM方法通过检测和纠正LLMs在数学任务中的错误，平衡成本与准确性，显著提升了预测精度并降低了成本。


<details>
  <summary>Details</summary>
Motivation: LLMs在多步推理中常产生错误，且难以遵循特定输出格式，导致最终预测不准确。

Method: EDCIM结合轻量级和强大LLMs，生成方程组并通过符号化框架检测错误，提供反馈进行纠正。

Result: 实验表明，EDCIM在降低成本和计算负担的同时，显著提高了预测准确性。

Conclusion: EDCIM为LLMs在数学任务中的错误纠正提供了高效且可配置的解决方案。

Abstract: Recent large language models (LLMs) have demonstrated the ability to perform
explicit multi-step reasoning such as chain-of-thought prompting. However,
their intermediate steps often contain errors that can propagate leading to
inaccurate final predictions. Additionally, LLMs still struggle with
hallucinations and often fail to adhere to prescribed output formats, which is
particularly problematic for tasks like generating mathematical expressions or
source code. This work introduces EDCIM (Error Detection and Correction for
Interpretable Mathematics), a method for detecting and correcting these errors
in interpretable mathematics tasks, where the model must generate the exact
functional form that explicitly solve the problem (expressed in natural
language) rather than a black-box solution. EDCIM uses LLMs to generate a
system of equations for a given problem, followed by a symbolic error-detection
framework that identifies errors and provides targeted feedback for LLM-based
correction. To optimize efficiency, EDCIM integrates lightweight, open-source
LLMs with more powerful proprietary models, balancing cost and accuracy. This
balance is controlled by a single hyperparameter, allowing users to control the
trade-off based on their cost and accuracy requirements. Experimental results
across different datasets show that EDCIM significantly reduces both
computational and financial costs, while maintaining, and even improving,
prediction accuracy when the balance is properly configured.

</details>


### [59] [Hidden Dynamics of Massive Activations in Transformer Training](https://arxiv.org/abs/2508.03616)
*Jorge Gallego-Feliciano,S. Aaron McClendon,Juan Morinelli,Stavros Zervoudakis,Antonios Saravanos*

Main category: cs.AI

TL;DR: 论文分析了Transformer训练中大规模激活值的动态发展规律，发现其遵循可预测的数学模式，并提出了预测框架。


<details>
  <summary>Details</summary>
Motivation: 研究大规模激活值在训练过程中的动态发展，填补现有研究的空白。

Method: 使用Pythia模型家族，分析不同规模和训练检查点的大规模激活值，建立数学模型和预测框架。

Result: 发现大规模激活值的发展遵循指数调制的对数函数，预测框架能高精度预测稳态行为。

Conclusion: 大规模激活值的发展受模型设计影响，可通过设计选择预测和控制，对模型稳定性和优化有重要意义。

Abstract: Massive activations are scalar values in transformer hidden states that
achieve values orders of magnitude larger than typical activations and have
been shown to be critical for model functionality. While prior work has
characterized these phenomena in fully trained models, the temporal dynamics of
their emergence during training remain poorly understood. We present the first
comprehensive analysis of massive activation development throughout transformer
training, using the Pythia model family as our testbed. Through systematic
analysis of various model sizes across multiple training checkpoints, we
demonstrate that massive activation emergence follows predictable mathematical
patterns that can be accurately modeled using an exponentially-modulated
logarithmic function with five key parameters. We develop a machine learning
framework to predict these mathematical parameters from architectural
specifications alone, achieving high accuracy for steady-state behavior and
moderate accuracy for emergence timing and magnitude. These findings enable
architects to predict and potentially control key aspects of massive activation
emergence through design choices, with significant implications for model
stability, training cycle length, interpretability, and optimization. Our
findings demonstrate that the emergence of massive activations is governed by
model design and can be anticipated, and potentially controlled, before
training begins.

</details>


### [60] [Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework](https://arxiv.org/abs/2508.03622)
*Jialin Li,Jinzhe Li,Gengxu Li,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: FPBench是首个针对代码生成中错误前提的评估框架，通过系统构建三类错误前提和多维评估指标，评估了15种代表性大语言模型，发现模型在错误前提下的表现不佳，揭示了其认知机制的缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型代码生成能力的提升，其对输入前提的依赖加剧，错误前提易导致代码生成幻觉，暴露模型自我审查能力的不足。

Method: 提出FPBench框架，系统构建三类错误前提，结合多维评估指标，对15种代表性大语言模型进行深入评估。

Result: （1）多数模型在错误前提下表现差，依赖显式提示；（2）错误前提导致资源投入边际效益递减；（3）三类错误前提分别激活模型的不同缺陷模式。

Conclusion: 研究强调了大语言模型需主动验证前提的重要性，FPBench框架为开发可靠、以人为本的代码生成模型提供了理论和实践基础。

Abstract: With the advancement of code generation capabilities in large language models
(LLMs), their reliance on input premises has intensified. When users provide
inputs containing faulty premises, the probability of code generation
hallucinations rises significantly, exposing deficiencies in their
self-scrutiny capabilities. This paper proposes Faulty Premises Bench
(FPBench), the first code generation evaluation framework targeting faulty
premises. By systematically constructing three categories of faulty premises
and integrating multi-dimensional evaluation metrics, it conducts in-depth
assessments of 15 representative LLMs. The key findings are as follows: (1)
Most models exhibit poor reasoning abilities and suboptimal code generation
performance under faulty premises, heavily relying on explicit prompts for
error detection, with limited self-scrutiny capabilities; (2) Faulty premises
trigger a point of diminishing returns in resource investment, leading to
blindly increasing length fails to enhance quality; (3) The three types of
faulty premises respectively activate distinct defect patterns in models,
revealing a triple dissociation in the cognitive mechanisms of code generation
models. This study not only highlights the urgent need for LLMs to proactively
verify premises in code generation but also, through the proposed FPBench
framework and multi-dimensional evaluation system, provides a theoretical
foundation and practical pathway for developing reliable, human-centric code
generation models.

</details>


### [61] [Automated Algorithmic Discovery for Gravitational-Wave Detection Guided by LLM-Informed Evolutionary Monte Carlo Tree Search](https://arxiv.org/abs/2508.03661)
*He Wang,Liang Zeng*

Main category: cs.AI

TL;DR: 提出了一种名为Evo-MCTS的新框架，结合进化优化和蒙特卡洛树搜索，用于改进引力波信号识别，性能提升20.2%。


<details>
  <summary>Details</summary>
Motivation: 现有算法（如匹配滤波和深度神经网络）在引力波信号识别中存在计算量大和黑盒决策的问题。

Method: 结合树状搜索、进化优化和大语言模型启发式，生成可解释的算法解决方案。

Result: 在MLGWSC-1基准数据集上性能提升20.2%，并发现新的算法组合。

Conclusion: Evo-MCTS为计算科学领域的自动化算法发现提供了可转移的方法论。

Abstract: Computational scientific discovery increasingly relies on algorithms to
process complex data and identify meaningful patterns - yet faces persistent
challenges in gravitational-wave signal identification. While existing
algorithmic approaches like matched filtering (MF) and deep neural networks
(DNNs) have achieved partial success, their limitations directly stem from
fundamental limitations: MF's excessive computational demands arise from its
reliance on predefined theoretical waveform templates, while DNNs' black-box
architectures obscure decision logic and introduce hidden biases. We propose
Evolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses
these limitations through systematic algorithm space exploration guided by
domain-aware physical constraints. Our approach combines tree-structured search
with evolutionary optimization and large language model heuristics to create
interpretable algorithmic solutions. Our Evo-MCTS framework demonstrates
substantial improvements, achieving a 20.2\% improvement over state-of-the-art
gravitational wave detection algorithms on the MLGWSC-1 benchmark dataset.
High-performing algorithm variants consistently exceed thresholds. The
framework generates human-interpretable algorithmic pathways that reveal
distinct performance patterns. Beyond performance improvements, our framework
discovers novel algorithmic combinations, thereby establishing a transferable
methodology for automated algorithmic discovery across computational science
domains.

</details>


### [62] [Agent Lightning: Train ANY AI Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03680)
*Xufang Luo,Yuge Zhang,Zhiyuan He,Zilong Wang,Siyun Zhao,Dongsheng Li,Luna K. Qiu,Yuqing Yang*

Main category: cs.AI

TL;DR: Agent Lightning是一个灵活可扩展的框架，通过强化学习（RL）训练大型语言模型（LLMs），支持多种AI代理的无缝集成，无需代码修改。


<details>
  <summary>Details</summary>
Motivation: 现有方法将RL训练与代理紧密耦合或依赖序列拼接与掩码，限制了灵活性和扩展性。

Method: 通过将代理执行建模为马尔可夫决策过程，定义统一数据接口，并提出了分层RL算法LightningRL，包含信用分配模块。

Result: 实验在文本到SQL、检索增强生成和数学工具使用任务中展示了稳定持续的改进。

Conclusion: 该框架为现实世界中的代理训练和部署提供了潜力。

Abstract: We present Agent Lightning, a flexible and extensible framework that enables
Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for
any AI agent. Unlike existing methods that tightly couple RL training with
agent or rely on sequence concatenation with masking, Agent Lightning achieves
complete decoupling between agent execution and training, allowing seamless
integration with existing agents developed via diverse ways (e.g., using
frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from
scratch) with almost ZERO code modifications. By formulating agent execution as
Markov decision process, we define an unified data interface and propose a
hierarchical RL algorithm, LightningRL, which contains a credit assignment
module, allowing us to decompose trajectories generated by ANY agents into
training transition. This enables RL to handle complex interaction logic, such
as multi-agent scenarios and dynamic workflows. For the system design, we
introduce a Training-Agent Disaggregation architecture, and brings agent
observability frameworks into agent runtime, providing a standardized agent
finetuning interface. Experiments across text-to-SQL, retrieval-augmented
generation, and math tool-use tasks demonstrate stable, continuous
improvements, showcasing the framework's potential for real-world agent
training and deployment.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [63] [Exponential convergence rate for Iterative Markovian Fitting](https://arxiv.org/abs/2508.02770)
*Kirill Sokolov,Alexander Korotin*

Main category: cs.IT

TL;DR: 本文首次证明了离散时间Schrödinger桥问题中IMF算法的指数收敛性，并给出了明确的收缩因子。


<details>
  <summary>Details</summary>
Motivation: 研究IMF算法在Kullback-Leibler散度下的收敛速度，填补了之前未量化的空白。

Method: 分析了离散时间Schrödinger桥问题在有限状态空间上的IMF算法。

Result: 证明了IMF算法具有指数收敛性，并提供了明确的收缩因子。

Conclusion: IMF算法在解决Schrödinger桥问题时具有高效的收敛性能。

Abstract: We consider the discrete-time Schr\"odinger bridge problem on a finite state
space. Although it has been known that the Iterative Markovian Fitting (IMF)
algorithm converges in Kullback-Leibler divergence to the ground truth
solution, the speed of that convergence remained unquantified. In this work, we
establish for the first time that IMF exhibits exponential convergence with an
explicit contraction factor.

</details>


### [64] [Distributed Source Coding for Compressing Vector-Linear Functions](https://arxiv.org/abs/2508.02996)
*Xuan Guang,Xiufang Sun,Ruze Zhang*

Main category: cs.IT

TL;DR: 论文研究了分布式源编码模型中的向量线性函数压缩问题，提出了函数压缩容量的通用下界，并对特定模型进行了详细分析。


<details>
  <summary>Details</summary>
Motivation: 受移动卫星通信系统和计算任务应用的启发，研究如何高效压缩向量线性函数。

Method: 提出通用下界，并针对特定模型（3源、不超过3编码器）分类矩阵T，通过上下界方法分析编码函数图像集大小。

Result: 证明了通用下界在某些情况下不紧，并完全刻画了T1和T2类矩阵的压缩容量。

Conclusion: 研究结果应用于网络函数计算，解决了已知上界是否渐近紧的开问题。

Abstract: Inspired by mobile satellite communication systems and the important and
prevalent applications of computational tasks, we consider a distributed source
coding model for compressing vector-linear functions, which consists of
multiple sources, multiple encoders and a decoder linked to all the encoders.
Each encoder has access to a certain subset of the sources and the decoder is
required to compute with zero error a vector-linear function of the source
information, which corresponds to a matrix $T$. The connectivity state between
the sources and the encoders and the vector-linear function are all arbitrary.
In the paper, we are interested in the function-compression capacity to measure
the efficiency of using the system. We first present a general lower bound on
the function-compression capacity applicable to arbitrary connectivity states
and vector-linear functions. Next, we confine to the nontrivial models with
only three sources and no more than three encoders, and prove that all the
$3\times2$ column-full-rank matrices $T$ can be divided into two types $T_1$
and $T_2$, for which the function-compression capacities are identical if the
matrices $T$ have the same type. We explicitly characterize the
function-compression capacities for two most nontrivial models associated with
$T_2$ by a novel approach of both upper bounding and lower bounding the size of
image sets of encoding functions. This shows that the lower bound thus obtained
is not always tight. Rather, by completely characterizing their capacities, the
lower bound is tight for all the models associated with $T_1$ and all the
models associated with $T_2$ except for the two most nontrivial models. We
finally apply the obtained results to network function computation and answer
the open problem whether the best known upper bound proved by Guang et. al.
(2019) on computing capacity is in general asymptotically tight.

</details>


### [65] [Multilevel inserting constructions for constant dimension subspace codes](https://arxiv.org/abs/2508.03196)
*Gang Wang,Xuan Gao,Sihem Mesnager,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 本文提出了一种通过逆双边多级构造和新的双边标识向量构建恒定维度子空间码（CDCs）的方法，展示了比现有文献更大的码集，并计算了新的下界与已知上界的比率。


<details>
  <summary>Details</summary>
Motivation: 恒定维度子空间码（CDCs）在随机网络编码中有广泛应用，但其构造和性能仍需改进。

Method: 引入逆双边标识向量和逆双边Ferrers图秩度量码，结合多级构造方法，提出新的CDC构造。

Result: 展示了多个具有更大尺寸的CDC码集，新下界与已知上界的比率大于0.94548（q≥3的素数幂）。

Conclusion: 提出的方法在CDC构造中有效，为网络编码中的实际应用提供了支持。

Abstract: Subspace codes, especially constant dimension subspace codes (CDCs),
represent an intriguing domain that can be used to conduct basic coding theory
investigations. They have received widespread attention due to their
applications in random network coding. This paper presents inverse bilateral
multilevel construction by introducing inverse bilateral identifying vectors
and inverse bilateral Ferrers diagram rank-metric codes. By inserting the
inverse bilateral multilevel construction into the double multilevel
construction and bilateral multilevel construction, an effective construction
for CDCs is provided. Furthermore, via providing a new set of bilateral
identifying vectors, we give another efficient construction for CDCs. In this
article, several CDCs are exhibited, equipped with the rank-metric, with larger
sizes than the known ones in the existing literature. From a practical
standpoint, our results could help in the pragmatic framework of
constant-dimension-lifted rank-metric codes for applications in network coding.
The ratio of the new lower bound to the known upper bound for some CDCs is
calculated, which is greater than 0.94548 for any prime power $q \geq 3.$

</details>


### [66] [Channel Coding for Unequal Error Protection in Digital Semantic Communication](https://arxiv.org/abs/2508.03381)
*Seonjung Kim,Yongjeong Oh,Yongjune Kim,Namyoon Lee,Yo-Seb Jeon*

Main category: cs.IT

TL;DR: 论文提出两种新型信道编码框架，用于数字语义通信中的不等错误保护（UEP），通过优化比特级和块级保护策略，显著提升任务性能和传输效率。


<details>
  <summary>Details</summary>
Motivation: 解决数字语义通信中比特级语义重要性不等的问题，确保高重要性比特得到更强保护。

Method: 1. 基于重复编码的比特级UEP框架；2. 利用现代信道码的块级UEP框架，结合比特分组算法。

Result: 仿真实验显示，所提框架在图像传输任务中显著优于传统方法。

Conclusion: 所提出的UEP框架有效提升了语义通信的任务性能和效率，为未来研究提供了新方向。

Abstract: Semantic communication is an emerging paradigm that prioritizes transmitting
task-relevant information over accurately delivering raw data bits. In this
paper, we address an unequal error protection (UEP) problem in digital semantic
communication, where bits of higher semantic importance require stronger
protection. To quantify bit-level importance, we leverage bit-flip
probabilities of semantic bits as target error protection levels, which are
jointly learned with semantic encoder and decoder. We propose two novel channel
coding frameworks aimed at minimizing the total blocklength while satisfying
UEP constraints. First, we develop a bit-level UEP framework based on
repetition coding, in which the repetition number for each bit is optimized to
precisely meet its target bit-flip probability. Second, we introduce a
block-level UEP framework utilizing modern channel codes, where semantic bits
with similar target bit-flip probabilities are grouped to exploit coding gains.
Within this framework, we propose a bit-grouping algorithm guided by finite
blocklength capacity analysis. Simulation results conducted on image
transmission tasks confirm that the proposed frameworks significantly
outperform conventional approaches, yielding substantial improvements in both
task performance and transmission efficiency.

</details>


### [67] [Dual Domain Expurgated Error Exponents for Source Coding with Side Information](https://arxiv.org/abs/2508.03467)
*Mehdi Dabirnia,Hamdi Joudeh,Albert Guillén i Fàbregas*

Main category: cs.IT

TL;DR: 本文提出了一种用于带边信息的源编码的排除方法，通过双域推导直接得到排除误差指数。双域方法优化参数少，且允许一般字母表和/或记忆。


<details>
  <summary>Details</summary>
Motivation: 传统方法在源编码中需要优化分布，而双域方法简化了优化过程，提高了效率。

Method: 采用双域方法推导排除误差指数，比较两种随机编码集合的误差指数。

Result: 两种误差指数中更优者与Csiszár-Körner指数一致，且在无边信息情况下与源最优码的误差指数一致。

Conclusion: 双域方法简化了优化问题，适用于更广泛的编码场景。

Abstract: We introduce an expurgation method for source coding with side information
that enables direct dual-domain derivations of expurgated error exponents.
Dual-domain methods yield optimization problems over few parameters, with any
sub-optimal choice resulting in an achievable exponent, as opposed to
primal-domain optimization over distributions. In addition, dual-domain methods
naturally allow for general alphabets and/or memory. We derive two such
expurgated error exponents for different random-coding ensembles. We show the
better of the exponents coincides with the Csisz\'ar-K\"orner exponent obtained
via a graph decomposition lemma. We show some numerical examples that
illustrate the differences between the two exponents and show that in the case
of source coding without side information, the expurgated exponent coincides
with the error exponent of the source optimal code.

</details>


### [68] [Decoding Algorithms for Twisted GRS Codes](https://arxiv.org/abs/2508.03552)
*Guanghui Zhang,Liren Lin,Bocong Chen*

Main category: cs.IT

TL;DR: 本文提出了针对MDS TGRS码的新解码算法，基于高斯消元法，能纠正特定数量的错误，计算复杂度为O(n³)。


<details>
  <summary>Details</summary>
Motivation: 扩展TGRS码的代数能力，构建新的非GRS MDS码并提升密码安全性。

Method: 使用高斯消元法设计解码算法，适用于MDS和近MDS TGRS码。

Result: 算法能纠正特定数量的错误，计算复杂度为O(n³)。

Conclusion: 新方法填补了现有文献空白，适用于更广泛的TGRS码解码场景。

Abstract: Twisted generalized Reed-Solomon (TGRS) codes were introduced to extend the
algebraic capabilities of classical generalized Reed-Solomon (GRS) codes. This
extension holds the potential for constructing new non-GRS maximum distance
separable (MDS) codes and enhancing cryptographic security. It is known that
TGRS codes with $1$ twist can either be MDS or near-MDS. In this paper, we
employ the Gaussian elimination method to propose new decoding algorithms for
MDS TGRS codes with parameters $[n,k,n-k+1]$. The algorithms can correct up to
$\lfloor \frac{n-k}{2}\rfloor$ errors when $n-k$ is odd, and $\lfloor
\frac{n-k}{2}\rfloor-1$ errors when $n-k$ is even. The computational complexity
for both scenarios is $O(n^3)$. %, where $\omega\approx 2.37286$ is the matrix
multiplication exponent. Our approach diverges from existing methods based on
Euclidean algorithm and addresses situations that have not been considered in
the existing literature \cite{SYJL}. Furthermore, this method is also
applicable to decoding near-MDS TGRS codes with parameters $[n, k, n-k]$,
enabling correction of up to $\lfloor \frac{n-k-1}{2} \rfloor$ errors, while
maintaining polynomial time complexity in $n$.

</details>


### [69] [What If, But Privately: Private Counterfactual Retrieval](https://arxiv.org/abs/2508.03681)
*Shreya Meel,Mohamed Nomeir,Pasan Dissanayake,Sanghamitra Dutta,Sennur Ulukus*

Main category: cs.IT

TL;DR: 提出了一种保护用户隐私的反事实解释检索框架，包括基线方案和两种改进方案，同时考虑了不可变特征的设置，并扩展了用户偏好。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中，黑盒机器学习模型的透明性和可解释性至关重要，但反事实解释可能威胁用户隐私。本文旨在保护用户在检索反事实解释时的隐私。

Method: 提出了私有反事实检索（PCR）问题，设计了基线PCR方案和两种改进方案，随后扩展到不可变PCR（I-PCR）和用户偏好设置。

Result: 理论分析和数值结果表明，所提方案能实现用户隐私的完美保护，并减少数据库信息泄露。

Conclusion: 该框架有效平衡了反事实解释的需求与用户隐私保护，适用于高风险应用场景。

Abstract: Transparency and explainability are two important aspects to be considered
when employing black-box machine learning models in high-stake applications.
Providing counterfactual explanations is one way of catering this requirement.
However, this also poses a threat to the privacy of the institution that is
providing the explanation, as well as the user who is requesting it. In this
work, we are primarily concerned with the user's privacy who wants to retrieve
a counterfactual instance, without revealing their feature vector to the
institution. Our framework retrieves the exact nearest neighbor counterfactual
explanation from a database of accepted points while achieving perfect,
information-theoretic, privacy for the user. First, we introduce the problem of
private counterfactual retrieval (PCR) and propose a baseline PCR scheme that
keeps the user's feature vector information-theoretically private from the
institution. Building on this, we propose two other schemes that reduce the
amount of information leaked about the institution database to the user,
compared to the baseline scheme. Second, we relax the assumption of mutability
of all features, and consider the setting of immutable PCR (I-PCR). Here, the
user retrieves the nearest counterfactual without altering a private subset of
their features, which constitutes the immutable set, while keeping their
feature vector and immutable set private from the institution. For this, we
propose two schemes that preserve the user's privacy information-theoretically,
but ensure varying degrees of database privacy. Third, we extend our PCR and
I-PCR schemes to incorporate user's preference on transforming their
attributes, so that a more actionable explanation can be received. Finally, we
present numerical results to support our theoretical findings, and compare the
database leakage of the proposed schemes.

</details>
