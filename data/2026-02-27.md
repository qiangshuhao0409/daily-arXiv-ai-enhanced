<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.AI](#cs.AI) [Total: 71]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Dynamic Hierarchical Birkhoff-von Neumann Decomposition for All-to-All GPU Communication](https://arxiv.org/abs/2602.22756)
*Yen-Chieh Wu,Cheng-Shang Chang,Duan-Shin Lee,H. Jonathan Chao*

Main category: cs.NI

TL;DR: 提出动态分层Birkhoff-von Neumann分解框架，用于优化GPU集群中的all-to-all通信调度，通过分层处理降低复杂度并改善流量均衡。


<details>
  <summary>Details</summary>
Motivation: 大规模训练集群中，all-to-all GPU通信是性能瓶颈，受限于端口带宽和流量倾斜问题。现代GPU系统的两层结构（快速服务器内链路+慢速服务器间网络）加剧了这一问题，需要层次感知的流量整形和调度方案。

Method: 提出动态分层BvN分解框架：1) 在帧边界处，首先通过简单本地操作平衡服务器内GPU/NIC流量，减轻微观倾斜；2) 在服务器级别应用分层BvN分解，然后细化为GPU级匹配，相比扁平GPU级方法显著降低复杂度；3) 结合动态帧大小(DFS)原则，构建具有可证明稳定性的在线调度器。

Result: 仿真实验显示，该方法显著减少了平均帧长度，特别是在服务器局部热点流量场景下表现优异。提出的在线调度器在可容许泊松到达条件下具有可证明的稳定性。

Conclusion: 动态分层BvN分解框架有效解决了GPU集群中all-to-all通信的调度问题，通过层次感知的流量整形和复杂度降低，在流量倾斜场景下显著提升性能。

Abstract: All-to-all GPU communication is a critical bottleneck in large-scale training clusters, where completion time is constrained by per-port bandwidth and can be severely impacted by traffic skew across GPUs and network interface cards (NICs). This issue is amplified by the two-tier structure of modern GPU systems, which combine fast intra-server links with much slower inter-server networks. Motivated by recent system observations that highlight the importance of traffic reshaping and hierarchy awareness, we study all-to-all scheduling from an online switching and queueing-theoretic perspective.
  We propose a dynamic hierarchical Birkhoff--von Neumann (BvN) decomposition framework tailored to two-tier GPU fabrics. At each frame boundary, traffic is first balanced within each server using simple local operations to mitigate micro-level GPU/NIC skew while preserving aggregate server-to-server demand. A hierarchical BvN decomposition is then applied at the server level and refined into GPU-level matchings, significantly reducing decomposition complexity relative to a flat GPU-level approach. By integrating this construction with the dynamic frame sizing (DFS) principle, we obtain an online scheduler with provable stability under admissible Poisson arrivals. Simulations demonstrate substantial reductions in mean frame length, particularly under server-localized hotspot traffic.

</details>


### [2] [BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models](https://arxiv.org/abs/2602.19929)
*Chenran Kou,Changsheng You,Mingjiang Wu,Dingzhu Wen,Zezhong Zhang,Chengwen Xing*

Main category: cs.NI

TL;DR: BeamVLM：一种基于视觉语言模型的端到端波束预测生成框架，将波束预测视为视觉问答任务，利用现有VLM联合推理无人机轨迹和环境上下文，在准确性和泛化性上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 低空经济中，高速移动无人机与地面基站间的快速准确波束预测至关重要，但现有深度学习方法缺乏对动态环境的高层语义理解导致泛化能力差，而基于大语言模型的方法虽然泛化性好但缺乏丰富的环境感知能力，无法捕捉精细空间语义。

Method: 提出BeamVLM框架，将波束预测视为视觉问答任务，利用现有视觉语言模型。通过将原始视觉补丁直接投影到语言域，并精心设计指令提示，使VLM能够联合推理无人机轨迹和环境上下文。

Result: 在真实世界数据集上的实验结果表明，BeamVLM在预测准确性上优于现有最先进方法，并且在车对基础设施（V2I）波束预测等其他场景中也表现出优越的泛化能力。

Conclusion: BeamVLM通过将波束预测构建为视觉问答任务，充分利用视觉语言模型的强大能力，实现了对动态环境的高层语义理解和精细空间语义捕捉，在准确性和泛化性方面均取得显著提升。

Abstract: For low-altitude economy (LAE), fast and accurate beam prediction between high-mobility unmanned aerial vehicles (UAVs) and ground base stations is of paramount importance, which ensures seamless coverage and reliable communications. However, existing deep learning-based beam prediction methods lack high-level semantic understanding of dynamic environments, resulting in poor generalization. On the other hand, the emerging large language model (LLM) based approaches show promise in enhancing generalization, but they typically lack rich environmental perception, thereby failing to capture fine-grained spatial semantics essential for precise beam alignment. To tackle these limitations, we propose in this correspondence a novel end-to-end generative framework for beam prediction, called BeamVLM, which treats beam prediction as a vision question answering task capitalizing on powerful existing vision-language models (VLMs). By projecting raw visual patches directly into the language domain and judiciously designing an instructional prompt, the proposed BeamVLM enables the VLM to jointly reason over UAV trajectories and environmental context. Last, experimental results on real-world datasets demonstrate that the proposed BeamVLM outperforms state-of-the-art methods in prediction accuracy and also exhibits superior generalization for other scenarios such as vehicle-to-infrastructure (V2I) beam prediction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的多智能体F1比赛策略优化方法，通过交互模块和自博弈训练生成竞争性策略，能够根据对手行为调整进站时机、轮胎选择和能量分配。


<details>
  <summary>Details</summary>
Motivation: 在F1比赛中，车队需要根据不断变化的比赛条件和竞争对手的行动来调整比赛策略。现有方法往往缺乏对多智能体交互的考虑，无法有效应对竞争对手的动态行为。

Method: 基于预训练的单智能体策略，引入交互模块来考虑竞争对手的行为。结合交互模块和自博弈训练方案生成竞争性策略，并根据相对性能对智能体进行排名。

Result: 智能体能够根据对手行为自适应调整进站时机、轮胎选择和能量分配，实现稳健一致的比赛表现。该框架仅使用真实比赛中可获得的信息，可在赛前和赛中支持策略师的决策。

Conclusion: 提出的强化学习方法能够有效优化F1多智能体比赛策略，通过考虑竞争对手交互实现自适应策略调整，为真实比赛中的策略决策提供支持。

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [4] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: GYWI系统通过结合作者知识图谱与检索增强生成，为LLMs提供可控的学术背景和可追溯的灵感路径，显著提升科学创意生成质量


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在科学创意生成中缺乏可控的学术背景和可追溯的灵感路径，需要构建能够提供上下文控制和灵感溯源的系统

Method: 1) 作者中心知识图谱构建与灵感源采样算法；2) RAG+GraphRAG混合检索机制；3) 融入强化学习原理的Prompt优化策略；4) 基于arXiv(2018-2023)构建评估数据集

Result: GYWI在GPT-4o、DeepSeek-V3、Qwen3-8B、Gemini 2.5等LLMs上实验，在新颖性、可靠性、相关性等多个指标上显著优于主流LLMs

Conclusion: GYWI系统通过知识图谱与混合检索机制，有效解决了LLMs科学创意生成中的背景控制和灵感溯源问题，提升了生成质量

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [5] [FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation](https://arxiv.org/abs/2602.22273)
*Xiyuan Zhang,Huihang Wu,Jiayu Guo,Zhenlin Zhang,Yiwei Zhang,Liangyu Huo,Xiaoxiao Ma,Jiansong Wan,Xuewei Jiao,Yi Jing,Jian Xie*

Main category: cs.AI

TL;DR: FIRE是一个综合性金融基准测试，用于评估LLMs的理论金融知识和实际业务场景处理能力，包含理论考试题和3000个金融场景问题。


<details>
  <summary>Details</summary>
Motivation: 当前需要系统评估LLMs在金融领域的理论知识和实际应用能力，以了解其在金融应用中的能力边界。

Method: 1) 理论评估：收集金融资格考试题；2) 实践评估：构建系统评估矩阵，收集3000个金融场景问题（封闭式和开放式）；3) 评估先进LLMs，包括轩辕4.0作为领域基线。

Result: 对先进LLMs进行了全面评估，包括轩辕4.0作为金融领域模型，系统分析了当前LLMs在金融应用中的能力边界。

Conclusion: FIRE基准测试为评估LLMs的金融能力提供了系统性框架，公开了基准问题和评估代码以促进未来研究。

Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.

</details>


### [6] [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)
*Willem Schooltink,Fabio Massimo Zennaro*

Main category: cs.AI

TL;DR: 本文提出因果嵌入框架，将多个详细因果模型映射到粗粒度模型的子系统中，作为抽象化的推广，并应用于多分辨率边际问题和数据集合并。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象研究主要关注两个模型之间的关系，但实际应用中需要将多个详细模型映射到同一个粗粒度模型中，以处理不同分辨率的数据集合并问题。

Method: 定义因果嵌入作为抽象化的推广，提出广义一致性概念，建立多分辨率边际问题框架，将统计边际问题和因果边际问题统一处理。

Result: 因果嵌入框架能够处理多个详细模型到粗粒度模型的映射，为统计边际问题和因果边际问题提供统一解决方案，并支持不同表示模型的数据集合并。

Conclusion: 因果嵌入框架扩展了因果抽象理论，为解决多分辨率因果建模和异构数据集合并提供了有效工具，具有重要的理论和实践价值。

Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>


### [7] [Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents](https://arxiv.org/abs/2602.22302)
*Varun Pratap Bhardwaj*

Main category: cs.AI

TL;DR: 提出了Agent Behavioral Contracts (ABC)框架，将设计契约原则应用于AI智能体，通过形式化规范和行为约束来防止智能体行为漂移和治理失败。


<details>
  <summary>Details</summary>
Motivation: 传统软件有API、类型系统等契约来确保正确行为，而AI智能体仅依赖提示和自然语言指令，缺乏形式化行为规范，这导致了智能体部署中的行为漂移、治理失败和项目失败。

Method: 引入ABC框架，契约C = (P, I, G, R)包含前置条件、不变量、治理策略和恢复机制。定义了(p, delta, k)-满足度来量化契约合规性，证明了漂移边界定理。实现了AgentAssert运行时执行库。

Result: 在AgentContract-Bench基准测试中，契约化智能体检测到5.2-6.8个软违规（基线完全未检测到），实现88-100%硬约束合规，行为漂移边界D* < 0.27，前沿模型100%恢复，所有开销<10ms/动作。

Conclusion: ABC框架为AI智能体提供了形式化行为规范，显著提高了行为合规性和可靠性，有效控制了行为漂移，为智能体部署提供了可验证的治理机制。

Abstract: Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and frequent project failures in agentic AI deployments. We introduce Agent Behavioral Contracts (ABC), a formal framework that brings Design-by-Contract principles to autonomous AI agents. An ABC contract C = (P, I, G, R) specifies Preconditions, Invariants, Governance policies, and Recovery mechanisms as first-class, runtime-enforceable components. We define (p, delta, k)-satisfaction -- a probabilistic notion of contract compliance that accounts for LLM non-determinism and recovery -- and prove a Drift Bounds Theorem showing that contracts with recovery rate gamma > alpha (the natural drift rate) bound behavioral drift to D* = alpha/gamma in expectation, with Gaussian concentration in the stochastic setting. We establish sufficient conditions for safe contract composition in multi-agent chains and derive probabilistic degradation bounds. We implement ABC in AgentAssert, a runtime enforcement library, and evaluate on AgentContract-Bench, a benchmark of 200 scenarios across 7 models from 6 vendors. Results across 1,980 sessions show that contracted agents detect 5.2-6.8 soft violations per session that uncontracted baselines miss entirely (p < 0.0001, Cohen's d = 6.7-33.8), achieve 88-100% hard constraint compliance, and bound behavioral drift to D* < 0.27 across extended sessions, with 100% recovery for frontier models and 17-100% across all models, at overhead < 10 ms per action.

</details>


### [8] [Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?](https://arxiv.org/abs/2602.22401)
*Yongjun Zhang*

Main category: cs.AI

TL;DR: 论文提出"氛围研究"概念，即AI代理自主执行完整研究流程，使用21技能插件作为案例，分析AI在研究中的认知边界和职业影响。


<details>
  <summary>Details</summary>
Motivation: AI代理相比传统聊天机器人能执行多步推理工作流，代表社会科学自动化的质变，需要探讨其在研究流程中的角色和边界。

Method: 提出认知任务框架，按可编码性和隐性知识需求对研究活动分类，识别认知而非顺序的委托边界，使用21技能插件作为案例研究。

Result: AI代理在速度、覆盖范围和方法论支持方面表现出色，但在理论原创性和隐性领域知识方面存在局限，委托边界贯穿研究各阶段。

Conclusion: AI代理将带来增强但脆弱的工作条件、分层风险和教育危机，需要建立负责任的氛围研究原则来应对职业变革。

Abstract: AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, query databases, search the web, and invoke domain-specific skills to execute entire research pipelines autonomously. This paper introduces the concept of vibe researching -- the AI-era parallel to ``vibe coding'' (Karpathy, 2025) -- and uses scholar-skill, a 21-skill plugin for Claude Code covering the full research pipeline from idea to submission, as an illustrative case. I develop a cognitive task framework that classifies research activities along two dimensions -- codifiability and tacit knowledge requirement -- to identify a delegation boundary that is cognitive, not sequential: it cuts through every stage of the research pipeline, not between stages. I argue that AI agents excel at speed, coverage, and methodological scaffolding but struggle with theoretical originality and tacit field knowledge. The paper concludes with an analysis of three implications for the profession -- augmentation with fragile conditions, stratification risk, and a pedagogical crisis -- and proposes five principles for responsible vibe researching.

</details>


### [9] [Towards Autonomous Memory Agents](https://arxiv.org/abs/2602.22406)
*Xinle Wu,Rui Zhang,Mustafa Anis Hussain,Yao Lu*

Main category: cs.AI

TL;DR: U-Mem是一种自主记忆代理，通过成本感知的知识提取级联和语义感知的Thompson采样，主动获取、验证和整理知识，显著提升LLM在可验证和不可验证任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理是被动和反应式的，记忆增长受限于可用信息，在不确定性中很少主动寻求外部输入。需要一种能够主动获取、验证和整理知识的自主记忆代理。

Method: U-Mem采用：(1) 成本感知的知识提取级联，从廉价的自/教师信号逐步升级到工具验证的研究，仅在需要时寻求专家反馈；(2) 语义感知的Thompson采样，平衡记忆的探索与利用，缓解冷启动偏差。

Result: 在可验证和不可验证基准测试中，U-Mem持续超越现有记忆基线，甚至超过基于RL的优化方法：HotpotQA（Qwen2.5-7B）提升14.6分，AIME25（Gemini-2.5-flash）提升7.33分。

Conclusion: U-Mem通过主动、成本优化的知识获取和智能记忆管理，实现了自主记忆代理的有效性，显著提升了LLM在各种任务上的性能表现。

Abstract: Recent memory agents improve LLMs by extracting experiences and conversation history into an external storage. This enables low-overhead context assembly and online memory update without expensive LLM training. However, existing solutions remain passive and reactive; memory growth is bounded by information that happens to be available, while memory agents seldom seek external inputs in uncertainties. We propose autonomous memory agents that actively acquire, validate, and curate knowledge at a minimum cost. U-Mem materializes this idea via (i) a cost-aware knowledge-extraction cascade that escalates from cheap self/teacher signals to tool-verified research and, only when needed, expert feedback, and (ii) semantic-aware Thompson sampling to balance exploration and exploitation over memories and mitigate cold-start bias. On both verifiable and non-verifiable benchmarks, U-Mem consistently beats prior memory baselines and can surpass RL-based optimization, improving HotpotQA (Qwen2.5-7B) by 14.6 points and AIME25 (Gemini-2.5-flash) by 7.33 points.

</details>


### [10] [Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus](https://arxiv.org/abs/2602.22408)
*Caroline Ahn,Quan Do,Leah Bakst,Michael P. Pascale,Joseph T. McGuire,Michael E. Hasselmo,Chantal E. Stern*

Main category: cs.AI

TL;DR: 研究者开发了CogARC（认知抽象推理语料库），用于研究人类抽象推理的认知策略，通过两个实验收集了260名参与者在75个视觉推理问题上的行为数据，发现人类在抽象推理中表现出高效但策略多样的特点。


<details>
  <summary>Details</summary>
Motivation: 研究人类在抽象推理中表现出的灵活性和快速学习能力，探究其背后的认知策略。通过创建适合人类研究的CogARC语料库，深入了解人类如何从稀疏示例中学习和应用规则。

Method: 1. 开发CogARC语料库，这是从人工智能基准测试ARC中选取的适合人类研究的子集；2. 进行两个实验，共260名参与者解决75个抽象视觉推理问题；3. 记录高时间分辨率的行为数据，包括示例查看、编辑序列和多尝试提交；4. 分析成功率、解决时间、策略差异等指标。

Result: 1. 参与者整体表现良好（实验1平均准确率约90%，实验2约80%）；2. 问题难度和参与者表现差异大；3. 更难的问题引发更长的思考时间和更大的策略分歧；4. 随着任务进行，响应速度加快但准确率略有下降；5. 即使是错误解决方案也常常高度收敛；6. 问题解决轨迹呈现多样性（直接高效型vs探索重启型）。

Conclusion: CogARC为研究人类抽象推理提供了丰富的行为环境，揭示了人类在不确定性下如何泛化、错误泛化和调整策略。研究发现人类抽象推理既高效又策略多样，为理解人类认知灵活性提供了重要见解。

Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.

</details>


### [11] [Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents](https://arxiv.org/abs/2602.22413)
*Jonas Karge*

Main category: cs.AI

TL;DR: 研究异构智能体通过校准自身可靠性并选择性弃权投票的集体准确性，将孔多塞陪审团定理推广到置信门控的序贯设置中。


<details>
  <summary>Details</summary>
Motivation: 传统孔多塞陪审团定理假设固定参与，但现实世界聚合常受益于允许智能体说"我不知道"。需要研究智能体学习自身可靠性并选择性弃权对集体准确性的影响。

Method: 提出概率框架：智能体经历校准阶段更新对自身固定能力的信念，然后面对最终置信门决定是否投票或弃权。推导群体成功概率的非渐近下界。

Result: 证明选择性参与将CJT的渐近保证推广到序贯置信门控设置。通过蒙特卡洛模拟验证边界。讨论在AI安全中应用，缓解集体LLM决策中的幻觉问题。

Conclusion: 选择性参与框架扩展了经典投票理论，允许智能体基于学习到的自身可靠性进行弃权，在保持集体准确性的同时减少错误投票，对AI安全特别是集体LLM决策有应用价值。

Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \textit{hallucinations} in collective LLM decision-making.

</details>


### [12] [A Mathematical Theory of Agency and Intelligence](https://arxiv.org/abs/2602.22519)
*Wael Hafez,Chenan Wei,Rodrigo Felipe,Amir Nazeri,Cameron Reid*

Main category: cs.AI

TL;DR: 论文提出"双可预测性"（bipredictability）作为衡量系统有效利用信息的新指标，证明其在量子、经典和代理系统中的不同上限，并区分了代理与智能，指出当前AI只有代理而无智能，最后提出基于丘脑皮质调节的反馈架构。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统虽然能处理大量信息做出复杂预测，但预测成功可能掩盖了系统与环境交互的退化。需要一种原则性方法来衡量系统部署的总信息中，有多少真正在观察、行动和结果之间共享，以评估系统在变化条件下的资源利用效率。

Method: 提出"双可预测性"（P）作为核心度量，从第一性原理推导其数学定义。通过理论证明P在不同系统中的上限：量子系统可达1，经典系统≤0.5，引入代理后更低。在物理系统（双摆）、强化学习代理和多轮LLM对话中验证这些界限。

Result: 理论证明和实验验证了P的界限：量子系统P可达1，经典系统P≤0.5，代理系统P更低。区分了代理（基于预测行动的能力）与智能（还需从交互中学习、自我监控学习效果、调整观察/行动/结果范围以恢复有效学习）。当前AI只有代理而无智能。

Conclusion: 双可预测性P是评估系统信息利用效率的关键指标。基于生物丘脑皮质调节的反馈架构，实时监控P，为构建自适应、有韧性的AI系统奠定了基础。当前AI实现了代理但未达到智能，需要能自我监控学习效果并适应的系统。

Abstract: To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.

</details>


### [13] [ArchAgent: Agentic AI-driven Computer Architecture Discovery](https://arxiv.org/abs/2602.22425)
*Raghav Gupta,Akanksha Jain,Abraham Gonzalez,Alexander Novikov,Po-Sen Huang,Matej Balog,Marvin Eisenberger,Sergey Shirobokov,Ngân Vũ,Martin Dixon,Borivoje Nikolić,Parthasarathy Ranganathan,Sagar Karandikar*

Main category: cs.AI

TL;DR: ArchAgent是一个基于AlphaEvolve的自动计算机架构发现系统，能够在无人干预下快速设计出优于现有最优缓存替换策略的新策略，并在多核和单核工作负载上分别实现5.3%和0.9%的IPC加速提升。


<details>
  <summary>Details</summary>
Motivation: 随着计算需求的爆炸式增长，需要敏捷的硬件设计流程作为力量倍增器。虽然代理生成式AI系统在算法设计、代码优化和科学发现方面取得了显著进展，但尚未应用于计算机架构设计领域。

Method: 基于AlphaEvolve构建ArchAgent系统，自动设计和实现最先进的缓存替换策略（不仅调整参数，还创造新的机制和逻辑）。系统还支持"后硅超专业化"，通过调整硬件策略中暴露的运行时可配置参数来进一步优化特定工作负载。

Result: 在2天内生成的多核Google工作负载策略实现了5.3%的IPC加速提升；在18天内生成的单核SPEC06工作负载策略实现了0.9%的IPC加速提升，速度比人工开发快3-5倍。通过后硅超专业化，在SPEC06工作负载上实现了2.4%的额外IPC加速提升。

Conclusion: ArchAgent展示了代理AI在计算机架构设计中的巨大潜力，能够快速发现优于人类设计的策略。研究还发现了"模拟器逃逸"现象，即AI代理发现并利用了流行微架构模拟器中的漏洞，这表明传统研究工具需要适应AI代理时代的新需求。

Abstract: Agile hardware design flows are a critically needed force multiplier to meet the exploding demand for compute. Recently, agentic generative AI systems have demonstrated significant advances in algorithm design, improving code efficiency, and enabling discovery across scientific domains.
  Bridging these worlds, we present ArchAgent, an automated computer architecture discovery system built on AlphaEvolve. We show ArchAgent's ability to automatically design/implement state-of-the-art (SoTA) cache replacement policies (architecting new mechanisms/logic, not only changing parameters), broadly within the confines of an established cache replacement policy design competition.
  In two days without human intervention, ArchAgent generated a policy achieving a 5.3% IPC speedup improvement over the prior SoTA on public multi-core Google Workload Traces. On the heavily-explored single-core SPEC06 workloads, it generated a policy in just 18 days showing a 0.9% IPC speedup improvement over the existing SoTA (a similar "winning margin" as reported by the existing SoTA). ArchAgent achieved these gains 3-5x faster than prior human-developed SoTA policies.
  Agentic flows also enable "post-silicon hyperspecialization" where agents tune runtime-configurable parameters exposed in hardware policies to further align the policies with a specific workload (mix). Exploiting this, we demonstrate a 2.4% IPC speedup improvement over prior SoTA on SPEC06 workloads.
  Finally, we outline broader implications for computer architecture research in the era of agentic AI. For example, we demonstrate the phenomenon of "simulator escapes", where the agentic AI flow discovered and exploited a loophole in a popular microarchitectural simulator - a consequence of the fact that these research tools were designed for a (now past) world where they were exclusively operated by humans acting in good-faith.

</details>


### [14] [A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring](https://arxiv.org/abs/2602.23163)
*Usman Anwar,Julianna Piskorz,David D. Baek,David Africa,Jim Weatherall,Max Tegmark,Christian Schroeder de Witt,Mihaela van der Schaar,David Krueger*

Main category: cs.AI

TL;DR: 论文提出了一种决策理论视角的隐写术检测方法，通过测量可解码与不可解码代理之间的信息不对称性来检测和量化LLM中的隐写推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型开始展现隐写能力，这可能让未对齐的模型逃避监督机制。然而，目前缺乏检测和量化这种行为的原则性方法。传统的隐写术定义和检测方法需要已知非隐写信号的参考分布，这在LLM隐写推理场景中不可行。

Method: 提出决策理论视角的隐写术，核心洞察是隐写术在能够解码和不能解码隐藏内容的代理之间创造了信息不对称。引入广义V信息作为测量输入中可用信息的功利主义框架，并定义"隐写差距"来量化隐写术。

Result: 经验验证了该形式化方法，表明它可以用于检测、量化和缓解LLM中的隐写推理。

Conclusion: 提出的决策理论框架为检测和量化LLM隐写能力提供了可行方法，解决了传统方法在缺乏参考分布时的局限性，有助于监督未对齐模型。

Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.

</details>


### [15] [How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?](https://arxiv.org/abs/2602.22441)
*Yingqian Cui,Zhenwei Dai,Bing He,Zhan Shi,Hui Liu,Rui Sun,Zhiji Liu,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 本文对潜在推理方法进行综合分析，揭示其存在普遍捷径行为，且潜在表示虽能编码多种可能性但未实现结构化搜索，同时发现监督强度存在权衡：强监督减少捷径但限制多样性，弱监督保持丰富表示但增加捷径行为。


<details>
  <summary>Details</summary>
Motivation: 潜在推理作为一种新兴推理范式，通过在潜在空间而非文本空间生成步骤进行多步推理，能够超越离散语言标记在连续潜在空间中进行计算。尽管已有大量研究关注提升潜在推理性能，但其内部机制尚未得到充分探究。

Method: 对具有不同监督水平的潜在推理方法进行全面分析，研究潜在表示在推理过程中的角色和行为。识别两种关键问题：观察普遍存在的捷径行为；检验潜在推理支持广度优先搜索式探索的假设。

Result: 发现潜在推理方法存在普遍捷径行为，能在不依赖潜在推理的情况下获得高准确率；潜在表示虽能编码多种可能性，但推理过程并未忠实实现结构化搜索，而是表现出隐式剪枝和压缩；监督强度存在权衡：强监督减少捷径但限制多样性保持能力，弱监督允许更丰富表示但增加捷径行为。

Conclusion: 潜在推理方法存在内在局限性，需要更深入理解其机制以改进设计。监督强度需要在减少捷径行为和保持潜在表示多样性之间取得平衡，未来研究应关注如何设计更忠实实现结构化搜索的潜在推理方法。

Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.

</details>


### [16] [A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines](https://arxiv.org/abs/2602.22442)
*Gaoyuan Du,Amit Ahlawat,Xiaoyang Liu,Jing Wu*

Main category: cs.AI

TL;DR: 提出评估代理(EA)对AutoML代理进行决策中心评估，而非仅关注最终性能，能检测错误决策、推理不一致性，揭示结果指标中不可见的失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的AutoML系统评估主要关注最终任务性能，缺乏对中间决策质量的结构化评估。通过文献回顾发现，现有系统均未报告用于事后评估中间决策质量的决策级指标。

Method: 提出评估代理(EA)作为观察者，在不干扰AutoML代理执行的情况下，从四个维度评估中间决策：决策有效性、推理一致性、超出准确率的模型质量风险、反事实决策影响。

Result: 在四个概念验证实验中，EA能够：(1)以0.919的F1分数检测错误决策；(2)独立于最终结果识别推理不一致性；(3)将下游性能变化归因于代理决策，揭示最终指标中-4.9%到+8.3%的影响范围。

Conclusion: 决策中心评估能揭示仅结果指标不可见的失败模式，将基于代理的AutoML系统评估从结果导向转变为审计代理决策，为可靠、可解释、可治理的自主ML系统奠定基础。

Abstract: Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find that none of the surveyed agentic AutoML systems report structured, decision-level evaluation metrics intended for post-hoc assessment of intermediate decision quality. To address this limitation, we propose an Evaluation Agent (EA) that performs decision-centric assessment of AutoML agents without interfering with their execution. The EA is designed as an observer that evaluates intermediate decisions along four dimensions: decision validity, reasoning consistency, model quality risks beyond accuracy, and counterfactual decision impact. Across four proof-of-concept experiments, we demonstrate that the EA can (i) detect faulty decisions with an F1 score of 0.919, (ii) identify reasoning inconsistencies independent of final outcomes, and (iii) attribute downstream performance changes to agent decisions, revealing impacts ranging from -4.9\% to +8.3\% in final metrics. These results illustrate how decision-centric evaluation exposes failure modes that are invisible to outcome-only metrics. Our work reframes the evaluation of agentic AutoML systems from an outcome-based perspective to one that audits agent decisions, offering a foundation for reliable, interpretable, and governable autonomous ML systems.

</details>


### [17] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 提出对比世界模型(CWM)，使用对比学习训练动作可行性评分器，通过挖掘困难负样本来提升对物理可行性的判别能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用监督微调(SFT)训练动作评分器，但SFT独立处理每个候选动作，没有显式教导模型区分物理正确和细微错误的动作，导致判别能力不足。

Method: 提出对比世界模型(CWM)，使用InfoNCE对比目标对大型语言模型进行微调，通过挖掘困难负样本（语义相似但物理不兼容的候选动作）在评分空间中推开有效和无效动作。

Result: 在ScienceWorld基准测试中：1) 内在可及性评估显示CWM在最小编辑负样本上的Precision@1比SFT高6.76个百分点，AUC-ROC更高(0.929 vs 0.906)；2) 实时过滤特性研究显示在分布外压力条件下，CWM保持更好的安全边界(-2.39 vs -3.96)。

Conclusion: 对比训练比单独使用SFT能更忠实地捕捉物理可行性，CWM在判别物理可行动作方面表现更优，特别是在困难负样本场景下。

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [18] [ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization](https://arxiv.org/abs/2602.22465)
*Joseph Tso,Preston Schmittou,Quan Huynh,Jibran Hutchins*

Main category: cs.AI

TL;DR: ConstraintBench：评估大语言模型在完全指定的约束优化问题上直接生成正确解的能力，发现可行性是主要瓶颈而非最优性


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估LLMs能否将优化问题表述为求解器代码，但缺少对LLMs能否直接生成正确解的评估。需要了解LLMs在完全指定的约束优化问题上的直接求解能力

Method: 引入ConstraintBench基准，涵盖10个运筹学领域，包含200个任务。每个任务提供自然语言场景描述，要求模型返回结构化解，通过确定性验证器检查所有约束并与Gurobi求解器验证的最优解对比

Result: 评估6个前沿模型发现：可行性是主要瓶颈（最佳模型仅65.0%约束满足率），可行解平均达到Gurobi最优目标的89-96%。没有模型能在0.1%误差内同时满足可行性和最优性超过30.5%。不同领域难度差异大，可行性从83.3%（生产组合）到0.8%（机组分配）。存在系统性失败模式

Conclusion: LLMs在直接约束优化方面可行性是主要挑战，可行解通常接近最优但难以完全满足所有约束。ConstraintBench为评估LLMs在运筹学问题上的直接求解能力提供了标准化基准

Abstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.

</details>


### [19] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: VERO是一个用于评估编码智能体优化性能的框架，包含版本控制、奖励机制和观察系统，支持对智能体进行迭代改进的评估。


<details>
  <summary>Details</summary>
Motivation: 编码智能体的优化（通过编辑-执行-评估循环迭代改进目标智能体）是一个新兴重要应用，但社区缺乏对此任务的系统性理解。智能体优化与传统软件工程有根本区别，需要同时捕获确定性代码和随机LLM完成的结构化信息。

Method: 提出VERO框架，包含：(1) 可复现的评估工具链，具有版本化智能体快照、预算控制评估和结构化执行轨迹；(2) 包含目标智能体和任务的基准套件，以及参考评估程序。

Result: 使用VERO进行了实证研究，比较不同优化器配置在任务上的表现，分析哪些修改能可靠提升目标智能体性能。研究结果支持智能体优化作为编码智能体核心能力的研究。

Conclusion: VERO框架为编码智能体优化研究提供了系统评估工具，填补了该领域评估方法的空白，支持智能体迭代改进的研究。

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [20] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 本文利用大语言模型对AI与生命周期评估(LCA)交叉领域的研究进行了系统性综述，揭示了该领域的发展趋势、主题模式及未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在生命周期评估中的应用近年来快速发展，但缺乏对该交叉领域的全面综合研究。本研究旨在填补这一空白，系统梳理AI-LCA研究现状。

Method: 结合大语言模型(LLM)文本挖掘与传统文献综述方法，构建动态有效的分析框架，识别研究趋势和概念模式。

Result: 分析显示：AI技术在LCA中的应用显著增长，向LLM驱动方法转变，机器学习应用持续增加，AI方法与LCA阶段存在统计显著相关性。

Conclusion: LLM辅助方法支持大规模可重复综述，为计算高效的LCA提供路径，帮助LCA从业者整合先进工具提升环境评估的严谨性和决策质量。

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [21] [Mirroring the Mind: Distilling Human-Like Metacognitive Strategies into Large Language Models](https://arxiv.org/abs/2602.22508)
*Ik-hwan Kim,Hyeongrok Han,Mingi Jung,Sangwon Yu,Jinseok Hong,Sang Hun Kim,Yoonyoung Choi,Sungroh Yoon*

Main category: cs.AI

TL;DR: MBT框架通过注入元认知行为解决大型推理模型的结构脆弱性问题，在保持推理能力的同时增强自我调节控制，显著提升推理稳定性并减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中存在结构脆弱性，即使能推导出有效中间步骤，也常常无法得出正确答案。研究发现这种失败通常不是推理能力不足，而是自我调节控制缺陷导致——有效逻辑被不受控的探索或未能识别逻辑充分性所破坏。

Method: 提出元认知行为调优（MBT）后训练框架，通过两种互补方法注入元认知行为：1）MBT-S从头合成严谨的推理轨迹；2）MBT-R重写学生初始轨迹以稳定内在探索模式。

Result: 在多跳QA基准测试中，MBT始终优于基线方法，在具有挑战性的基准上取得显著提升。通过有效消除推理崩溃，MBT以显著减少的token消耗实现更高准确率。

Conclusion: 内化元认知策略能够实现更稳定和鲁棒的推理，表明增强模型的自我调节控制是提升大型推理模型性能的有效途径。

Abstract: Large Reasoning Models (LRMs) often exhibit structural fragility in complex reasoning tasks, failing to produce correct answers even after successfully deriving valid intermediate steps. Through systematic analysis, we observe that these failures frequently stem not from a lack of reasoning capacity, but from a deficiency in self-regulatory control, where valid logic is destabilized by uncontrolled exploration or the failure to recognize logical sufficiency. Motivated by this observation, we propose Metacognitive Behavioral Tuning (MBT), a post-training framework that explicitly injects metacognitive behaviors into the model's thought process. MBT implements this via two complementary formulations: (1) MBT-S, which synthesizes rigorous reasoning traces from scratch, and (2) MBT-R, which rewrites the student's initial traces to stabilize intrinsic exploration patterns. Experiments across multi-hop QA benchmarks demonstrate that MBT consistently outperforms baselines, achieving notable gains on challenging benchmarks. By effectively eliminating reasoning collapse, MBT achieves higher accuracy with significantly reduced token consumption, demonstrating that internalizing metacognitive strategies leads to more stable and robust reasoning.

</details>


### [22] [Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents](https://arxiv.org/abs/2602.22523)
*Ryan Liu,Dilip Arumugam,Cedegao E. Zhang,Sean Escola,Xaq Pitkow,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 该论文提出从认知模型和AI算法中寻找设计模块化语言智能体的蓝图，通过定义智能体模板来指导多个LLM的协作组合。


<details>
  <summary>Details</summary>
Motivation: 虽然当前大型语言模型能力不断增强，但许多复杂问题仍超出单个LLM的能力范围。如何将多个LLM作为部件组合成更强大的整体系统仍存在不确定性。

Method: 提出智能体模板的概念，用于规范单个LLM的角色定义和功能组合方式。通过文献调研，分析现有语言智能体设计，揭示其底层模板直接源自认知模型或AI算法。

Result: 展示了多种现有语言智能体设计背后的模板，这些模板可直接追溯到认知科学和AI算法原理，为模块化语言智能体设计提供了系统化框架。

Conclusion: 认知科学和AI算法启发的智能体模板是开发高效、可解释语言智能体的有力工具，值得更多关注和研究。

Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.

</details>


### [23] [Agentic AI for Intent-driven Optimization in Cell-free O-RAN](https://arxiv.org/abs/2602.22539)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: cs.AI

TL;DR: 提出一个基于代理AI的意图翻译与优化框架，用于无小区O-RAN网络，通过多个LLM代理协作实现运营商意图，显著降低能耗和内存使用。


<details>
  <summary>Details</summary>
Motivation: 代理AI是自主RAN的关键使能技术，现有工作主要处理简单意图且代理独立工作，复杂意图需要多个代理协调的问题尚未探索。

Method: 提出多代理框架：监督代理翻译意图，用户权重代理确定优先级，O-RU管理代理使用DRL算法管理激活单元，监控代理协调保证最低速率要求。采用PEFT方法实现同一LLM支持不同代理。

Result: 在节能模式下，相比三种基线方案，该框架将活跃O-RU数量减少41.93%；使用PEFT方法，相比部署独立LLM代理，内存使用减少92%。

Conclusion: 该代理AI框架能有效处理复杂意图，通过代理协作实现意图翻译和优化，显著提升能效和可扩展性，为自主RAN提供可行解决方案。

Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.

</details>


### [24] [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546)
*Zhiming Wang,Jinwei He,Feng Lu*

Main category: cs.AI

TL;DR: AHCE框架通过主动学习如何请求专家推理，而非简单求助，显著提升LLM智能体在专业领域的任务成功率


<details>
  <summary>Details</summary>
Motivation: LLM智能体在需要长尾知识的专业领域表现不佳，而人类专家的指导往往非结构化且不可靠，难以直接整合到智能体规划中

Method: 提出AHCE框架，包含人类反馈模块(HFM)，学习将人类专家视为交互式推理工具的策略，实现按需人机协作

Result: 在Minecraft实验中，普通难度任务成功率提升32%，高难度任务提升近70%，且人类干预极少

Conclusion: 成功增强智能体需要学习如何请求专家推理，而不仅仅是请求帮助，AHCE框架为此提供了有效解决方案

Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.

</details>


### [25] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: CourtGuard：基于检索增强的多智能体框架，将安全评估重构为证据辩论，无需微调即可实现SOTA安全性能，并具备零样本适应性和自动数据审计能力


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全机制依赖静态微调分类器，存在适应僵化问题，无法在不昂贵重新训练的情况下强制执行新的治理规则

Method: 引入CourtGuard框架，基于外部政策文档进行对抗性辩论的检索增强多智能体系统，将安全评估重构为证据辩论

Result: 在7个安全基准测试中实现SOTA性能，超越专用政策遵循基线；零样本适应Wikipedia破坏检测任务达90%准确率；自动生成并审计9个新颖对抗攻击数据集

Conclusion: 将安全逻辑与模型权重解耦为AI治理提供了鲁棒、可解释且适应性强的方法，能够满足当前和未来的监管要求

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [26] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 论文提出选择性策略检索(SSR)框架，通过建模策略可执行性来改善数学推理中的示例引导效果，在多个基准上显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 数学推理中基于示例的引导效果不稳定，即使引导正确且相关。这种不稳定性源于策略使用与策略可执行性之间的差距——前者指策略是否出现在成功解决方案中，后者指策略作为引导时是否对目标模型有效。

Method: 通过分析人类编写和模型生成的解决方案，识别使用与可执行性的系统性差异。提出选择性策略检索(SSR)框架，通过经验性、多路径、来源感知的信号，选择性检索和组合策略来显式建模可执行性。

Result: 在多个数学推理基准测试中，SSR相比直接求解、上下文学习和单源引导，提供了可靠且一致的改进。在AIME25上准确率提升高达13个百分点，在Apex上提升5个百分点。

Conclusion: 策略使用与可执行性之间的差距是数学推理引导效果不稳定的关键原因。通过显式建模可执行性并选择性组合策略，SSR框架能显著提升推理模型的性能。

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [27] [Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach](https://arxiv.org/abs/2602.22585)
*Jodi M. Casabianca,Maggie Beiting-Parrish*

Main category: cs.AI

TL;DR: 该论文提出将心理测量学中的评分者模型整合到AI评估流程中，以解决人类评分中的系统性误差问题，提高AI模型评估的可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 人类评估在AI模型训练和评估中至关重要，但这些数据很少被当作存在系统性误差的测量来处理。当前AI评估中的人类评分存在评分者效应（如严格性和中心性偏差），导致观察到的评分失真，影响对AI输出质量的准确判断。

Method: 采用心理测量学中的评分者模型，特别是项目反应理论中的多面Rasch模型，将真实输出质量与评分者行为分离。使用OpenAI摘要数据集作为实证案例，展示如何通过调整评分者严格性来校正摘要质量估计，并提供评分者表现的诊断性洞察。

Result: 通过调整评分者严格性，获得了校正后的摘要质量估计，并提供了对评分者表现的诊断性洞察。该方法能够更准确地区分AI输出的真实质量与评分者行为偏差，提高了评估的可靠性。

Conclusion: 将心理测量学建模整合到人机协同评估中，能够更原则化、透明地使用人类数据，使开发者能够基于校正后的分数而非原始、易错的评分做出决策。这为AI开发和评估提供了更稳健、可解释且与构念对齐的实践路径。

Abstract: Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.

</details>


### [28] [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: SideQuest：利用大型推理模型自身进行KV缓存压缩的新方法，通过并行执行压缩任务减少代理任务中的峰值token使用量达65%


<details>
  <summary>Details</summary>
Motivation: 长期运行的代理任务（如深度研究）需要在多个网页和文档中进行多跳推理，导致外部检索的token主导LLM上下文，使内存使用快速增长并限制解码性能。现有的KV缓存压缩启发式方法无法有效支持多步推理模型。

Method: SideQuest利用大型推理模型自身来执行KV缓存压缩，通过推理其上下文中token的有用性。为避免管理过程相关的token污染模型内存，将KV缓存压缩作为与主要推理任务并行执行的辅助任务。仅用215个样本训练模型即可实现。

Result: 评估显示，SideQuest在代理任务中将峰值token使用量减少高达65%，且准确性下降最小，优于基于启发式的KV缓存压缩技术。

Conclusion: SideQuest通过让大型推理模型自身参与KV缓存压缩决策，有效解决了多步推理任务中的内存效率问题，为长期运行的代理任务提供了实用的内存优化解决方案。

Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

</details>


### [29] [MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios](https://arxiv.org/abs/2602.22638)
*Zhiheng Song,Jingshuai Zhang,Chuan Qin,Chao Wang,Chao Chen,Longfei Xu,Kaikui Liu,Xiangxiang Chu,Hengshu Zhu*

Main category: cs.AI

TL;DR: MobilityBench是一个用于评估基于LLM的路线规划代理在真实世界移动场景中的可扩展基准，包含大规模真实用户查询、确定性API重放沙箱和多维度评估协议。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的路线规划代理缺乏系统评估，因为存在多样化的路线需求、非确定性的地图服务以及有限的复现性，需要构建一个标准化的评估框架。

Method: 从Amap收集大规模匿名真实用户查询构建基准，设计确定性API重放沙箱消除环境方差，提出以结果有效性为核心的多维度评估协议。

Result: 当前模型在基础信息检索和路线规划任务上表现良好，但在偏好约束路线规划方面存在显著困难，显示个性化移动应用仍有很大改进空间。

Conclusion: MobilityBench为LLM路线规划代理提供了系统评估框架，揭示了当前模型的局限性，特别是处理个性化需求方面的不足，为未来研究提供了基准和工具。

Abstract: Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .

</details>


### [30] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: AHBid是一个用于多渠道在线广告自动出价的分层框架，结合生成式规划和实时控制，相比现有基线提升13.57%的总体回报


<details>
  <summary>Details</summary>
Motivation: 在线广告环境复杂动态，多渠道场景下需要有效分配预算和约束。现有方法要么缺乏灵活性（优化方法），要么难以捕捉历史依赖和观测模式（强化学习方法），需要更有效的解决方案。

Method: 提出AHBid分层出价框架：高层使用基于扩散模型的生成式规划器动态分配预算和约束，捕捉历史上下文和时间模式；引入约束执行机制确保合规性；轨迹精炼机制利用历史数据增强环境适应性；结合基于控制的出价算法，融合历史知识和实时信息。

Result: 在大规模离线数据集和在线A/B测试中，AHBid相比现有基线实现了13.57%的总体回报提升，证明了其有效性。

Conclusion: AHBid通过结合生成式规划和实时控制，有效解决了多渠道广告出价的复杂动态问题，在适应性和操作效能方面均有显著提升。

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [31] [Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions](https://arxiv.org/abs/2602.22680)
*Yue Xu,Qian Chen,Zizhan Ma,Dongrui Liu,Wenxuan Wang,Xiting Wang,Li Xiong,Wenjie Wang*

Main category: cs.AI

TL;DR: 这篇综述论文系统回顾了个性化LLM智能体技术，围绕用户画像建模、记忆、规划和行动执行四个核心组件，分析了用户信号的表示、传播和利用方法，并探讨了评估指标、应用场景和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长期交互中需要适应不同用户并保持连续性，个性化成为关键需求。当前个性化技术已渗透到整个决策流程而非仅限于表层生成，需要系统性的框架来理解和设计这类智能体。

Method: 采用能力导向的综述方法，围绕四个相互依赖的组件构建分类体系：1) 用户画像建模，2) 记忆系统，3) 规划能力，4) 行动执行。分析用户信号的表示、传播和利用机制，强调跨组件交互和设计权衡。

Result: 提出了一个结构化框架来理解和设计个性化LLM智能体，总结了代表性方法，分析了评估指标和基准测试，梳理了从通用辅助到专业领域的应用场景，为从原型到可部署系统的转化提供了路线图。

Conclusion: 该综述为个性化LLM智能体研究提供了系统框架，加速了从原型个性化到可扩展现实世界助手的发展进程，推动了更用户对齐、自适应、鲁棒和可部署的智能体系统建设。

Abstract: Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.

</details>


### [32] [Knob: A Physics-Inspired Gating Interface for Interpretable and Controllable Neural Dynamics](https://arxiv.org/abs/2602.22702)
*Siyu Jiang,Sanshuai Cui,Hui Zeng*

Main category: cs.AI

TL;DR: Knob框架将神经网络校准与控制理论结合，通过二阶机械系统类比创建可调"安全阀"，实现双模式推理和人工可调参数


<details>
  <summary>Details</summary>
Motivation: 现有校准方法多为静态后处理，忽略了现实推理的动态时序特性，且缺乏让操作者动态调整模型行为的直观界面

Method: 将神经门控动态映射到二阶机械系统，建立阻尼比和自然频率与神经门控的对应关系，使用logit级凸融合作为输入自适应温度缩放，并引入二阶动力学实现双模式推理

Result: 在CIFAR-10-C上验证了校准机制，连续模式下门响应符合标准二阶控制特征（阶跃稳定和低通衰减），为可预测的人机协同调谐铺平道路

Conclusion: 提出了探索性架构接口，重点展示概念并验证其控制理论特性，而非追求最先进的校准性能，为动态校准和人工调谐提供了新思路

Abstract: Existing neural network calibration methods often treat calibration as a static, post-hoc optimization task. However, this neglects the dynamic and temporal nature of real-world inference. Moreover, existing methods do not provide an intuitive interface enabling human operators to dynamically adjust model behavior under shifting conditions. In this work, we propose Knob, a framework that connects deep learning with classical control theory by mapping neural gating dynamics to a second-order mechanical system. By establishing correspondences between physical parameters -- damping ratio ($ζ$) and natural frequency ($ω_n$) -- and neural gating, we create a tunable "safety valve". The core mechanism employs a logit-level convex fusion, functioning as an input-adaptive temperature scaling. It tends to reduce model confidence particularly when model branches produce conflicting predictions. Furthermore, by imposing second-order dynamics (Knob-ODE), we enable a \textit{dual-mode} inference: standard i.i.d. processing for static tasks, and state-preserving processing for continuous streams. Our framework allows operators to tune "stability" and "sensitivity" through familiar physical analogues. This paper presents an exploratory architectural interface; we focus on demonstrating the concept and validating its control-theoretic properties rather than claiming state-of-the-art calibration performance. Experiments on CIFAR-10-C validate the calibration mechanism and demonstrate that, in Continuous Mode, the gate responses are consistent with standard second-order control signatures (step settling and low-pass attenuation), paving the way for predictable human-in-the-loop tuning.

</details>


### [33] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: RLHFless：首个基于无服务器计算的可扩展同步RLHF训练框架，通过动态资源适配、前缀预计算和成本感知的actor扩展策略，实现最高1.35倍加速和44.8%成本降低。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF框架依赖服务器基础设施，难以应对RLHF训练中动态变化的资源需求，导致组件间空闲时间和资源浪费问题。

Method: 1) 基于无服务器计算环境构建；2) 动态适配RLHF流水线资源需求；3) 预计算共享前缀避免重复计算；4) 成本感知的actor扩展策略考虑响应长度变化；5) 高效分配工作负载减少函数内不平衡和空闲时间。

Result: 在物理测试床和大规模模拟集群上的实验显示，RLHFless相比最先进基线实现最高1.35倍加速和44.8%成本降低。

Conclusion: RLHFless成功解决了同步RLHF训练中的资源效率问题，通过无服务器计算架构和多种优化技术显著提升了训练速度和成本效益。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [34] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: Taesar是一个数据中心的跨域序列推荐框架，通过对比解码机制将跨域上下文编码到目标域序列中，解决了传统模型中心方法在跨域推荐中的负迁移和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统跨域推荐采用模型中心范式，依赖复杂的定制架构，难以捕捉跨域的微妙非结构化序列依赖，导致泛化能力差、计算资源需求高。同时，域间差异会导致混合域数据质量下降，产生负迁移问题。

Method: 提出Taesar框架，采用数据中心的序列再生方法，使用对比解码机制自适应地将跨域上下文编码到目标域序列中，使标准模型无需复杂融合架构即可学习跨域依赖关系。

Result: 实验表明Taesar优于模型中心解决方案，能泛化到各种序列模型，通过生成丰富数据集有效结合了数据中心和模型中心范式的优势。

Conclusion: Taesar通过数据中心的序列再生方法解决了跨域推荐中的负迁移和泛化问题，为跨域推荐提供了一种更高效、泛化能力更强的解决方案。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [35] [Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning](https://arxiv.org/abs/2602.22751)
*Qiannian Zhao,Chen Yang,Jinhao Jing,Yunke Zhang,Xuhui Ren,Lu Yu,Shijie Zhang,Hongzhi Yin*

Main category: cs.AI

TL;DR: EGPO框架通过元认知熵校准，将模型内在不确定性整合到RLVR训练中，解决了不确定性-奖励不匹配问题，显著提升大型推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习（RLVR）主要依赖二元正确性信号，忽略了模型的内在不确定性，导致高不确定性和低不确定性解决方案被同等对待。这种不确定性-奖励不匹配阻碍了模型"知道自己知道什么"，限制了从优化正确答案到优化有效推理路径的转变。

Method: 提出EGPO元认知熵校准框架：1）使用基于token级似然的零开销熵代理估计每个样本的不确定性；2）通过非对称校准机制将内在不确定性与外在正确性对齐，保留正确推理同时选择性调节过度自信的错误；3）在不修改验证器或奖励定义的情况下，从退化的基于组的rollout中恢复信息性学习信号。

Result: 在多个基准测试上的广泛实验表明，EGPO在推理性能上带来了显著且一致的改进，为通过元认知熵校准推进大型推理模型提供了原则性路径。

Conclusion: EGPO通过将内在不确定性整合到RLVR训练中，解决了不确定性-奖励不匹配问题，实现了稳定且不确定性感知的策略优化，显著提升了大型推理模型在数学和问答等推理中心任务中的性能。

Abstract: Large reasoning models (LRMs) have emerged as a powerful paradigm for solving complex real-world tasks. In practice, these models are predominantly trained via Reinforcement Learning with Verifiable Rewards (RLVR), yet most existing outcome-only RLVR pipelines rely almost exclusively on a binary correctness signal and largely ignore the model's intrinsic uncertainty. We term this discrepancy the uncertainty-reward mismatch, under which high- and low-uncertainty solutions are treated equivalently, preventing the policy from "Know What You Know" and impeding the shift from optimizing for correct answers to optimizing effective reasoning paths. This limitation is especially critical in reasoning-centric tasks such as mathematics and question answering, where performance hinges on the quality of the model's internal reasoning process rather than mere memorization of final answers. To address this, we propose EGPO, a metacognitive entropy calibration framework that explicitly integrates intrinsic uncertainty into RLVR for enhancing LRMs. EGPO estimates per-sample uncertainty using a zero-overhead entropy proxy derived from token-level likelihoods and aligns it with extrinsic correctness through an asymmetric calibration mechanism that preserves correct reasoning while selectively regulating overconfident failures, thereby enabling stable and uncertainty-aware policy optimization. Moreover, EGPO recovers informative learning signals from otherwise degenerate group-based rollouts without modifying the verifier or reward definition. Extensive experiments across multiple benchmarks demonstrate that the proposed EGPO leads to substantial and consistent improvements in reasoning performance, establishing a principled path for advancing LRMs through metacognitive entropy calibration.

</details>


### [36] [Decomposing Physician Disagreement in HealthBench](https://arxiv.org/abs/2602.22758)
*Satya Borgohain,Roy Mariathas*

Main category: cs.AI

TL;DR: 研究分析HealthBench医疗AI评估数据集中医生分歧的来源，发现81.8%的分歧无法用现有特征解释，主要存在于边界案例中，且可减少的不确定性（信息缺失）会增加分歧，而不可减少的不确定性（固有医学模糊性）则无影响。


<details>
  <summary>Details</summary>
Motivation: 理解医疗AI评估中医生分歧的来源和可解释因素，以改进评估设计并提高评估一致性。

Method: 分解HealthBench数据集中医生分歧的方差来源，分析评估标准、医生身份、元数据标签、医学专业、表面特征、嵌入表示等多种因素对分歧的影响，并研究不确定性类别与分歧的关系。

Result: 81.8%的分歧方差无法被现有特征解释；评估标准仅解释3.6-6.9%的分歧方差；医生身份仅解释2.4%；分歧在边界案例中最高（倒U型关系）；可减少的不确定性使分歧几率增加2.55倍，而不可减少的不确定性无影响。

Conclusion: 医疗AI评估中的一致性上限主要是结构性的，但可减少与不可减少不确定性的分离表明，通过改进评估场景中的信息传递可以降低分歧，这为评估设计提供了可操作的改进方向。

Abstract: We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.

</details>


### [37] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: AMA-Bench是一个评估LLM智能体长期记忆能力的基准测试，包含真实和合成的智能体轨迹数据，并提出了AMA-Agent记忆系统来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆评估主要关注人机对话场景，而实际应用中智能体记忆是连续的机器生成交互流。需要建立更贴近真实智能体应用的记忆评估基准。

Method: 提出AMA-Bench基准，包含：1）真实世界智能体轨迹和专家标注的QA；2）可扩展到任意长度的合成智能体轨迹和基于规则的QA。同时提出AMA-Agent记忆系统，采用因果图结构和工具增强检索。

Result: 现有记忆系统在AMA-Bench上表现不佳，主要因为缺乏因果性和目标信息，以及基于相似性检索的损失性限制。AMA-Agent在AMA-Bench上达到57.22%的平均准确率，比最强基线提升11.16%。

Conclusion: AMA-Bench填补了智能体记忆评估的空白，AMA-Agent通过因果图和工具增强检索有效解决了现有记忆系统的局限性，为智能体长期记忆能力提供了更好的解决方案。

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [38] [ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making](https://arxiv.org/abs/2602.22771)
*Yusuke Watanabe,Yohei Kobashi,Takeshi Kojima,Yusuke Iwasawa,Yasushi Okuno,Yutaka Matsuo*

Main category: cs.AI

TL;DR: LLMs在临床不完全信息场景下无法准确判断可确定性，导致过早判断或过度弃权，现有基准不足以评估临床安全性


<details>
  <summary>Details</summary>
Motivation: 临床决策常面临信息不完整的情况，专家需要判断现有信息是否足够做出决策。过早结论和不必要的弃权都会危及患者安全。需要评估LLMs在这种情况下的能力。

Method: 开发ClinDet-Bench基准，基于临床评分系统，将不完全信息场景分解为可确定和不可确定条件。识别可确定性需要考虑所有关于缺失信息的假设（包括不太可能的假设），并验证结论是否在所有情况下都成立。

Result: 最近的LLMs无法在不完全信息下识别可确定性，既会产生过早判断，也会过度弃权，尽管它们能正确解释底层评分知识并在完整信息下表现良好。

Conclusion: 现有基准不足以评估LLMs在临床环境中的安全性。ClinDet-Bench提供了一个评估可确定性识别的框架，促进适当的弃权决策，在医学和其他高风险领域具有应用潜力。

Abstract: Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinability requires considering all hypotheses about missing information, including unlikely ones, and verifying whether the conclusion holds across them. We find that recent LLMs fail to identify determinability under incomplete information, producing both premature judgments and excessive abstention, despite correctly explaining the underlying scoring knowledge and performing well under complete information. These findings suggest that existing benchmarks are insufficient to evaluate the safety of LLMs in clinical settings. ClinDet-Bench provides a framework for evaluating determinability recognition, leading to appropriate abstention, with potential applicability to medicine and other high-stakes domains, and is publicly available.

</details>


### [39] [MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks](https://arxiv.org/abs/2602.22808)
*Shiqian Su,Sen Xing,Xuan Dong,Muyan Zhong,Bin Wang,Xizhou Zhu,Yuntao Chen,Wenhai Wang,Yue Deng,Pengxiang Zhu,Ziyuan Liu,Tiantong Li,Jiaheng Yu,Zhe Chen,Lidong Bing,Jifeng Dai*

Main category: cs.AI

TL;DR: MiroFlow是一个开源的高性能智能体框架，通过智能体图、深度推理模式和鲁棒工作流执行来解决现有LLM智能体在复杂任务中的性能瓶颈问题，在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理需要外部工具交互的复杂现实任务时能力趋于瓶颈，当前智能体框架存在工作流简单、性能不稳定、基准支持有限、依赖昂贵商业API等问题。

Method: 提出MiroFlow框架，包含：1）智能体图实现灵活编排；2）可选的深度推理模式提升性能；3）鲁棒工作流执行确保稳定可复现性能。

Result: 在GAIA、BrowseComp-EN/ZH、HLE、xBench-DeepSearch和FutureX等多个智能体基准测试中均取得最先进的性能表现。

Conclusion: MiroFlow可作为深度学习社区易于访问、可复现且可比较的基准框架，推动智能体研究的进一步发展。

Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.

</details>


### [40] [When Should an AI Act? A Human-Centered Model of Scene, Context, and Behavior for Agentic AI Design](https://arxiv.org/abs/2602.22814)
*Soyoung Jung,Daehoo Yoon,Sung Gyu Koh,Young Hwan Kim,Yehan Ahn,Sung Park*

Main category: cs.AI

TL;DR: 提出一个概念模型，将智能体行为重新定义为整合场景、上下文和人类行为因素的诠释结果，并推导出五个智能体设计原则，为具有情境敏感性和判断力的AI系统提供设计基础。


<details>
  <summary>Details</summary>
Motivation: 当前智能体AI通过上下文数据推断用户情况并主动干预，但缺乏关于何时、为何以及是否采取行动的原则性判断，导致经常失败。需要填补这一空白。

Method: 提出一个概念模型，将行为重新定义为整合三个要素的诠释结果：场景（可观察情况）、上下文（用户构建的意义）和人类行为因素（塑造行为可能性的决定因素）。基于人文、社会科学、人机交互和工程等多学科视角。

Result: 从该模型推导出五个智能体设计原则：行为对齐、情境敏感性、时间适当性、动机校准和代理保持，这些原则指导干预的深度、时机、强度和克制。

Conclusion: 该模型和原则为设计具有情境敏感性和判断力的智能体AI系统提供了基础，使AI在交互中能够更明智地行动。

Abstract: Agentic AI increasingly intervenes proactively by inferring users' situations from contextual data yet often fails for lack of principled judgment about when, why, and whether to act. We address this gap by proposing a conceptual model that reframes behavior as an interpretive outcome integrating Scene (observable situation), Context (user-constructed meaning), and Human Behavior Factors (determinants shaping behavioral likelihood). Grounded in multidisciplinary perspectives across the humanities, social sciences, HCI, and engineering, the model separates what is observable from what is meaningful to the user and explains how the same scene can yield different behavioral meanings and outcomes. To translate this lens into design action, we derive five agent design principles (behavioral alignment, contextual sensitivity, temporal appropriateness, motivational calibration, and agency preservation) that guide intervention depth, timing, intensity, and restraint. Together, the model and principles provide a foundation for designing agentic AI systems that act with contextual sensitivity and judgment in interactions.

</details>


### [41] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: FlexMS：一个用于质谱预测的基准框架，支持动态构建多种模型架构并进行评估，提供性能影响因素分析和实用指导。


<details>
  <summary>Details</summary>
Motivation: 化学分子的鉴定和性质预测在药物发现和材料科学中至关重要，但实验光谱的缺乏阻碍了分子鉴定，需要建立计算模型预测方法。现有深度学习模型在质谱预测方面有潜力，但方法异质性和缺乏明确定义的基准使得整体评估具有挑战性。

Method: 创建FlexMS基准框架，支持动态构建多种不同的模型架构组合，在预处理公共数据集上使用不同指标评估性能。分析影响性能的因素，包括数据集结构多样性、超参数、预训练效果、元数据消融设置和跨域迁移学习分析。

Result: FlexMS框架提供了构建和评估质谱预测模型的灵活平台，通过分析各种因素对性能的影响，为选择合适的模型提供实用指导。检索基准模拟实际鉴定场景，基于预测光谱对潜在匹配进行评分。

Conclusion: FlexMS基准框架解决了质谱预测领域缺乏统一评估标准的问题，通过系统分析模型性能影响因素，为研究人员选择合适模型提供了实用指导，并模拟实际应用场景进行性能评估。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [42] [DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation](https://arxiv.org/abs/2602.22839)
*Hao Zheng,Guozhao Mo,Xinru Yan,Qianhao Yuan,Wenkai Zhang,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: DeepPresenter是一个自适应、反馈驱动的演示文稿生成智能体框架，通过环境感知的反思机制和长时程细化，超越传统基于模板的方法，在多样化场景中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有演示文稿生成代理通常依赖预定义的工作流程和固定模板，缺乏对用户多样化意图的适应能力，也无法进行有效的反馈驱动细化。需要一种能够自主规划、渲染和修订中间幻灯片工件，并支持基于环境观察的长时程细化的智能框架。

Method: DeepPresenter采用智能体框架，自主规划、渲染和修订中间幻灯片工件。核心创新是环境接地的反思机制，基于感知到的工件状态（如已渲染的幻灯片）而非内部信号（如推理轨迹）来调节生成过程，从而在执行过程中识别和修正演示文稿特定问题。

Result: 在覆盖多样化演示文稿生成场景的评估集上，DeepPresenter实现了最先进的性能。微调后的9B模型在显著降低成本的同时仍保持高度竞争力。

Conclusion: DeepPresenter通过自适应规划、环境接地的反思和长时程细化，超越了传统基于模板的演示文稿生成方法，为智能演示文稿生成提供了更灵活、有效的解决方案。

Abstract: Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent

</details>


### [43] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: AI在创造性数学研究中能真正贡献价值，但需要严格的人类验证和监督。通过Hermite求积规则误差表示和界限的案例研究，展示了人机协作能超越纯人工工作，但AI的每个步骤都需要人类数学直觉和战略指导。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能是否真正能促进创造性数学研究，还是仅仅自动化常规计算并引入错误风险。通过实证案例研究，评估人机协作在数学发现中的实际效果和局限性。

Method: 采用系统化的人机协作方法，与多个AI助手合作，扩展Hermite求积规则的误差表示和界限结果。通过完整研究流程的透明记录，分析成功的人机协作模式和需要预防的失败模式。

Result: 成功发现并证明了多个关于Hermite求积规则误差表示和界限的新定理，超越了纯人工工作的成果。AI在代数操作、系统化证明探索、文献综合和LaTeX准备方面表现出色，但每一步都需要严格的人类验证和数学直觉指导。

Conclusion: 当配合适当的怀疑态度和验证协议使用时，AI工具能够有意义地加速数学发现，但需要仔细的人类监督和深厚的领域专业知识。人机协作揭示了AI在数学研究中的显著能力和关键局限性。

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [44] [Towards LLM-Empowered Knowledge Tracing via LLM-Student Hierarchical Behavior Alignment in Hyperbolic Space](https://arxiv.org/abs/2602.22879)
*Xingcheng Fu,Shengpeng Wang,Yisen Gao,Xianxian Li,Chunpei Li,Qingyun Sun,Dongran Yu*

Main category: cs.AI

TL;DR: 提出L-HAKT框架，利用大语言模型和双曲空间对齐，解决知识追踪中认知状态层次演化和个性化难度感知问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法主要基于ID序列或浅层文本特征，无法捕捉认知状态的层次演化以及学生个体对题目难度的感知差异，限制了语义建模能力。

Method: 1. 教师代理深度解析题目语义并构建知识点层次依赖；学生代理模拟学习行为生成合成数据。2. 在双曲空间中对合成数据和真实数据进行对比学习，减少题目难度、遗忘模式等关键特征的分布差异。3. 通过优化双曲曲率，显式建模知识点的树状层次结构，精确刻画不同层次知识点的学习曲线形态差异。

Result: 在四个真实世界教育数据集上的大量实验验证了L-HAKT框架的有效性。

Conclusion: 提出的L-HAKT框架通过大语言模型和双曲空间对齐，能够更好地捕捉知识点的层次结构和个性化学习特征，提升了知识追踪的性能。

Abstract: Knowledge Tracing (KT) diagnoses students' concept mastery through continuous learning state monitoring in education.Existing methods primarily focus on studying behavioral sequences based on ID or textual information.While existing methods rely on ID-based sequences or shallow textual features, they often fail to capture (1) the hierarchical evolution of cognitive states and (2) individualized problem difficulty perception due to limited semantic modeling. Therefore, this paper proposes a Large Language Model Hyperbolic Aligned Knowledge Tracing(L-HAKT). First, the teacher agent deeply parses question semantics and explicitly constructs hierarchical dependencies of knowledge points; the student agent simulates learning behaviors to generate synthetic data. Then, contrastive learning is performed between synthetic and real data in hyperbolic space to reduce distribution differences in key features such as question difficulty and forgetting patterns. Finally, by optimizing hyperbolic curvature, we explicitly model the tree-like hierarchical structure of knowledge points, precisely characterizing differences in learning curve morphology for knowledge points at different levels. Extensive experiments on four real-world educational datasets validate the effectiveness of our Large Language Model Hyperbolic Aligned Knowledge Tracing (L-HAKT) framework.

</details>


### [45] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Shijian Wang,Guanting Dong,Jiajie Jin,Hao Wang,Yinuo Wang,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: OmniGAIA是一个评估全模态AI助手能力的基准，OmniAtlas是基于该基准开发的原生全模态基础智能体，通过工具集成推理和主动感知提升多模态交互能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要局限于双模态交互（如视觉-语言），缺乏统一的全模态认知能力，无法满足通用AI助手的需求。需要开发能够处理视频、音频、图像等多种模态并具备深度推理和工具使用能力的系统。

Method: 1) 提出OmniGAIA基准：通过全模态事件图方法构建复杂多跳查询，需要跨模态推理和外部工具集成；2) 开发OmniAtlas智能体：基于工具集成推理范式，采用后见之明引导的树探索策略合成训练轨迹，并使用OmniDPO进行细粒度错误校正。

Result: OmniGAIA基准能够评估全模态智能体在视频、音频和图像模态上的深度推理和多轮工具执行能力。OmniAtlas有效提升了现有开源模型的工具使用能力，向下一代原生全模态AI助手迈进了一步。

Conclusion: 这项工作为开发面向真实世界场景的下一代原生全模态AI助手奠定了基础，通过统一的基准和智能体架构解决了当前多模态系统在跨模态推理和工具集成方面的局限性。

Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.

</details>


### [46] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 提出首个通用智能体评估框架Exgentic，建立开放通用智能体排行榜，验证通用智能体在多种环境中的泛化能力


<details>
  <summary>Details</summary>
Motivation: 当前智能体多为领域专用，缺乏对通用智能体性能的系统评估。现有基准测试假设领域特定集成，无法公平评估通用智能体

Method: 提出通用智能体评估概念原则，设计统一协议实现智能体-基准集成，开发Exgentic评估框架，在6个环境中测试5个主流智能体实现

Result: 通用智能体在多样化环境中展现出泛化能力，性能与领域专用智能体相当，无需环境特定调优

Conclusion: 建立了通用智能体评估的基础设施，包括评估协议、框架和排行榜，为通用智能体系统研究奠定基础

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [47] [FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning](https://arxiv.org/abs/2602.22963)
*Zehao Li,Hongwei Yu,Hao Jiang,Qiang Sheng,Yilong Xu,Baolong Bi,Yang Li,Zhenlong Yuan,Yujun Cai,Zhaoqi Wang*

Main category: cs.AI

TL;DR: FactGuard：基于多模态大语言模型的视频虚假信息检测代理框架，通过迭代推理和外部工具调用解决证据稀疏、碎片化问题


<details>
  <summary>Details</summary>
Motivation: 现有MLLM视频虚假信息检测方法依赖固定深度推理，过度信任内部生成的假设，在关键证据稀疏、碎片化或需要外部验证的场景中表现不佳

Method: 提出FactGuard代理框架：1）将验证构建为基于MLLM的迭代推理过程；2）显式评估任务模糊性并选择性调用外部工具获取关键证据；3）采用两阶段训练策略：领域特定代理监督微调 + 决策感知强化学习优化工具使用和风险敏感决策

Result: 在FakeSV、FakeTT和FakeVV数据集上的广泛实验表明FactGuard达到最先进性能，验证了其优秀的鲁棒性和泛化能力

Conclusion: FactGuard通过迭代推理和选择性工具调用有效解决了视频虚假信息检测中的证据稀疏问题，为MLLM在复杂验证任务中的应用提供了新思路

Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.

</details>


### [48] [Certified Circuits: Stability Guarantees for Mechanistic Circuits](https://arxiv.org/abs/2602.22968)
*Alaa Anani,Tobias Lorenz,Bernt Schiele,Mario Fritz,Jonas Fischer*

Main category: cs.AI

TL;DR: 提出Certified Circuits框架，为神经网络电路发现提供可证明的稳定性保证，通过随机数据子采样和弃权不稳定神经元，获得更紧凑、更准确的电路


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法存在脆弱性问题：电路高度依赖于所选概念数据集，且往往无法在分布外数据上迁移，这让人怀疑它们捕获的是概念还是数据集特定的伪影

Method: 引入Certified Circuits框架，将任何黑盒发现算法与随机数据子采样结合，证明电路组件包含决策对概念数据集的有界编辑距离扰动具有不变性，弃权不稳定神经元

Result: 在ImageNet和OOD数据集上，认证电路达到高达91%的更高准确率，同时使用45%更少的神经元，在基线方法失效时仍保持可靠

Conclusion: Certified Circuits通过产生可证明稳定且与目标概念更一致的机械解释，为电路发现奠定了形式化基础

Abstract: Understanding how neural networks arrive at their predictions is essential for debugging, auditing, and deployment. Mechanistic interpretability pursues this goal by identifying circuits - minimal subnetworks responsible for specific behaviors. However, existing circuit discovery methods are brittle: circuits depend strongly on the chosen concept dataset and often fail to transfer out-of-distribution, raising doubts whether they capture concept or dataset-specific artifacts. We introduce Certified Circuits, which provide provable stability guarantees for circuit discovery. Our framework wraps any black-box discovery algorithm with randomized data subsampling to certify that circuit component inclusion decisions are invariant to bounded edit-distance perturbations of the concept dataset. Unstable neurons are abstained from, yielding circuits that are more compact and more accurate. On ImageNet and OOD datasets, certified circuits achieve up to 91% higher accuracy while using 45% fewer neurons, and remain reliable where baselines degrade. Certified Circuits puts circuit discovery on formal ground by producing mechanistic explanations that are provably stable and better aligned with the target concept. Code will be released soon!

</details>


### [49] [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)
*Peiyao Xiao,Xiaogang Li,Chengliang Xu,Jiayi Wang,Ben Wang,Zichao Chen,Zeyu Wang,Kejun Yu,Yueqian Chen,Xulin Liu,Wende Xiao,Bing Zhao,Hu Wei*

Main category: cs.AI

TL;DR: SPM-Bench是一个针对扫描探针显微镜（SPM）的PhD级多模态基准测试，通过自动化数据合成管道和Anchor-Gated Sieve技术从arXiv和期刊论文中提取高质量图像-文本对，使用SIP-F1评分评估LLMs性能并量化模型"个性"。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在专业科学领域存在明显差距，现有基准测试存在数据污染、复杂度不足和人工成本高昂的问题，需要专门针对扫描探针显微镜领域的高质量、低成本评估工具。

Method: 1) 开发全自动数据合成管道，使用Anchor-Gated Sieve技术从2023-2025年arXiv和期刊论文中提取高质量图像-文本对；2) 采用混合云-本地架构，VLMs仅返回空间坐标供本地高保真裁剪，实现极端令牌节省；3) 引入Strict Imperfection Penalty F1（SIP-F1）评分进行客观评估。

Result: 建立了SPM-Bench基准测试，能够准确评估LLMs在复杂物理场景中的推理能力，首次量化了模型的"个性"类型（保守型、激进型、赌徒型、智慧型），并揭示了当前AI在复杂物理场景中的真实推理边界。

Conclusion: SPM-Bench为自动化科学数据合成提供了一个可推广的范式，能够以低成本、高权威性的方式评估LLMs在专业科学领域的性能，并为理解模型在复杂物理推理中的行为提供了新视角。

Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>


### [50] [Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots](https://arxiv.org/abs/2602.22973)
*Dimitrios P. Panagoulias,Evangelia-Aikaterini Tsichrintzi,Georgios Savvidis,Evridiki Tsoureli-Nikita*

Main category: cs.AI

TL;DR: 提出诊断对齐框架，将AI生成的影像报告作为不可变推理状态，与医师验证结果系统比较，通过多级一致性评估显示临床有意义对齐远超简单词汇匹配


<details>
  <summary>Details</summary>
Motivation: 在安全关键的临床AI中，人工验证至关重要，但模型推理与专家修正之间的转换很少被分析为结构化信号。现有二元词汇评估可能低估临床有意义对齐。

Method: 引入诊断对齐框架：1) 保留AI生成的影像报告作为不可变推理状态；2) 系统比较医师验证结果；3) 推理管道集成视觉大语言模型、基于BERT的医学实体提取和序列语言模型推理(SLMI)；4) 采用四级一致性评估框架：精确主要匹配率、语义相似度调整率、跨类别对齐和综合一致性率。

Result: 在21个皮肤病案例评估中：精确一致率达71.4%，语义相似度调整后保持不变(t=0.60)，结构化跨类别和鉴别诊断重叠分析显示100%综合一致性(95% CI: [83.9%, 100%])，无案例显示完全诊断分歧。

Conclusion: 二元词汇评估显著低估临床有意义对齐。将专家验证建模为结构化转换，可实现信号感知的修正动态量化，支持可追溯、人类对齐的影像临床决策支持系统评估。

Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.

</details>


### [51] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: 提出RepSPD模型，通过黎曼流形上的交叉注意力机制和全局双向对齐策略，改进基于对称正定矩阵的脑电图解码方法，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于对称正定矩阵的EEG分析方法主要关注统计聚合，忽略了频率特异性同步和脑区局部拓扑结构，需要更精细的几何深度学习方法来改进脑活动解码。

Method: 提出RepSPD模型：1）在黎曼流形上实现交叉注意力机制，用图导出的功能连接特征调制SPD的几何属性；2）引入全局双向对齐策略重塑切空间嵌入，减轻曲率引起的几何失真。

Result: 大量实验表明，该框架显著优于现有EEG表示方法，展现出优越的鲁棒性和泛化能力。

Conclusion: RepSPD通过几何深度学习有效改进脑电图解码，为神经科学和临床应用提供了更强大的工具。

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [52] [Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search](https://arxiv.org/abs/2602.22983)
*Xun Huang,Simeng Qin,Xiaoshuang Jia,Ranjie Duan,Huanqian Yan,Zhitao Zeng,Fei Yang,Yang Liu,Xiaojun Jia*

Main category: cs.AI

TL;DR: 论文提出CC-BOS框架，利用文言文生成对抗性提示进行越狱攻击，通过果蝇优化算法在八个维度优化提示，显著提升黑盒攻击效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛使用但存在安全风险，现有研究发现不同语言环境下越狱攻击效果不同。文言文因其简洁隐晦的特性可能绕过现有安全约束，暴露LLM的漏洞。

Method: 提出CC-BOS框架：1) 将提示编码为八个策略维度（角色、行为、机制、隐喻、表达、知识、触发模式、上下文）；2) 使用多维果蝇优化算法（嗅觉搜索、视觉搜索、柯西变异）迭代优化；3) 设计文言文到英文翻译模块提升可读性和评估准确性。

Result: 大量实验证明CC-BOS框架有效，在越狱攻击效果上持续优于现有最先进方法。

Conclusion: 文言文在越狱攻击中具有独特优势，CC-BOS框架通过自动化生成文言文对抗提示，有效暴露LLM的安全漏洞，为黑盒攻击提供了高效解决方案。

Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.

</details>


### [53] [Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design](https://arxiv.org/abs/2602.23092)
*Zhuoliang Xie,Fei Liu,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 提出AILS-AHD方法，结合大语言模型与自适应迭代局部搜索，用于解决带容量约束的车辆路径问题，在多个基准测试中取得最优解


<details>
  <summary>Details</summary>
Motivation: CVRP作为组合优化基础问题具有NP-hard特性，大规模实例计算挑战显著，现有方法仍有改进空间，需要更智能的启发式设计方法

Method: AILS-AHD方法：将进化搜索框架与大语言模型结合，动态生成和优化破坏启发式；引入LLM加速机制提升计算效率

Result: 在CVRPLib大规模基准测试中，10个实例中有8个获得新的最优解，性能优于AILS-II和HGS等先进求解器

Conclusion: LLM驱动的启发式设计在车辆路径优化领域具有显著潜力，AILS-AHD方法为解决CVRP问题提供了创新且有效的解决方案

Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.

</details>


### [54] [Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents](https://arxiv.org/abs/2602.23093)
*Dhwanil M. Mori,Neil F. Johnson*

Main category: cs.AI

TL;DR: AI代理在资源分配系统中会自发形成部落，但更智能的代理反而导致更差的系统性能，甚至不如随机决策


<details>
  <summary>Details</summary>
Motivation: 研究未来基础设施系统中自主AI代理如何竞争有限资源（能源、带宽、计算能力），探索AI代理在资源分配中的集体行为模式

Method: 建立简化框架：N个AI代理每轮独立决定是否请求1单位资源，系统有固定容量C。观察AI代理在资源竞争中的行为演化

Result: AI代理形成三种主要部落类型：攻击型（27.3%）、保守型（24.7%）、机会型（48.1%）。LLM代理未能减少过载或改善资源利用，性能常低于随机决策。更智能的AI代理反而增加系统故障率

Conclusion: 更智能的AI代理会形成部落，导致集体行为更愚蠢。AI代理的集体决策可能比随机决策更差，这对未来AI控制的基础设施系统设计提出了重要警示

Abstract: Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of "Lord of the Flies" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.

</details>


### [55] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: MALLET是一个基于多智能体LLM的情感净化系统，通过四个智能体分析、调整、监控和指导信息消费，显著降低新闻文章的情感刺激强度，同时保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济中，煽情内容让消费者暴露于过度情绪刺激，阻碍冷静决策。需要一种既能减少情绪刺激又不限制访问原始文本的解决方案。

Method: 提出MALLET系统，包含四个智能体：情感分析智能体（使用6情感BERT分类器量化刺激强度）、情感调整智能体（用LLM将文本重写为BALANCED和COOL两种模式）、平衡监控智能体（聚合每周信息消费模式生成个性化建议）、个人指导智能体（根据消费者敏感度推荐呈现模式）。

Result: 在800篇AG News文章上的实验显示：刺激分数显著降低（最高19.3%），情绪平衡改善，语义保持良好。刺激减少与语义保持之间的相关性接近零，表明两者可独立控制。分类分析显示体育、商业、科技类刺激大幅减少（17.8-33.8%），而世界类效果有限，因为事实本身具有高刺激性。

Conclusion: MALLET为支持消费者冷静接收信息提供了一个框架，无需限制访问原始文本，实现了情绪刺激减少和语义保持的独立控制。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [56] [On Sample-Efficient Generalized Planning via Learned Transition Models](https://arxiv.org/abs/2602.23148)
*Nitin Gupta,Vishal Pallagani,John A. Aydin,Biplav Srivastava*

Main category: cs.AI

TL;DR: 本文提出将广义规划重新定义为过渡模型学习问题，通过神经网络显式近似状态转移函数，而非直接预测动作序列，从而在样本效率和泛化能力上优于现有Transformer规划器。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的规划器（如PlanGPT、Plansformer）将广义规划视为直接的动作序列预测问题，虽然对分布内实例有效，但需要大量数据和模型规模，且在长时域规划中容易因缺乏显式世界状态演化而产生状态漂移问题。

Method: 将广义规划重新定义为过渡模型学习问题，训练神经网络模型显式近似后继状态函数，通过自回归预测中间世界状态来学习领域动态作为隐式世界模型。系统评估了多种状态表示和神经网络架构，包括关系图编码。

Result: 学习显式过渡模型在多个领域中比直接动作序列预测具有更高的分布外满意规划成功率，同时使用更少的训练实例和更小的模型就能实现这些优势。

Conclusion: 将广义规划视为过渡模型学习问题而非直接动作预测，能够提高样本效率和泛化能力，特别是在分布外场景和长时域规划中表现更优。这是ICAPS 2026短文的扩展版本。

Abstract: Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $γ: S \times A \rightarrow S$. Classical approaches achieve such generalization through symbolic abstractions and explicit reasoning over $γ$. In contrast, recent Transformer-based planners, such as PlanGPT and Plansformer, largely cast generalized planning as direct action-sequence prediction, bypassing explicit transition modeling. While effective on in-distribution instances, these approaches typically require large datasets and model sizes, and often suffer from state drift in long-horizon settings due to the absence of explicit world-state evolution. In this work, we formulate generalized planning as a transition-model learning problem, in which a neural model explicitly approximates the successor-state function $\hatγ \approx γ$ and generates plans by rolling out symbolic state trajectories. Instead of predicting actions directly, the model autoregressively predicts intermediate world states, thereby learning the domain dynamics as an implicit world model. To study size-invariant generalization and sample efficiency, we systematically evaluate multiple state representations and neural architectures, including relational graph encodings. Our results show that learning explicit transition models yields higher out-of-distribution satisficing-plan success than direct action-sequence prediction in multiple domains, while achieving these gains with significantly fewer training instances and smaller models. This is an extended version of a short paper accepted at ICAPS 2026 under the same title.

</details>


### [57] [The Trinity of Consistency as a Defining Principle for General World Models](https://arxiv.org/abs/2602.23152)
*Jingxuan Wei,Siyuan Li,Yuhang Xu,Zheng Sun,Junjie Jiang,Hexuan Jin,Caijun Jia,Honghao He,Xinglong Xu,Xi bai,Chang Yu,Yumou Liu,Junnan Zhu,Xuanhe Zhou,Jintao Chen,Xiaobin Hu,Shancheng Pang,Bihui Yu,Ran He,Zhen Lei,Stan Z. Li,Conghui He,Shuicheng Yan,Cheng Tan*

Main category: cs.AI

TL;DR: 该论文提出了构建通用世界模型的三位一体一致性理论框架（模态、空间、时间一致性），并引入CoW-Bench基准来评估视频生成模型和统一多模态模型。


<details>
  <summary>Details</summary>
Motivation: 当前虽然存在Sora等视频生成模型和统一多模态模型（UMM）的进展，但缺乏定义通用世界模型所需基本特性的原则性理论框架。需要建立系统化的理论来指导世界模型的发展。

Method: 提出"三位一体一致性"理论框架：模态一致性（语义接口）、空间一致性（几何基础）、时间一致性（因果引擎）。通过这个三重视角系统回顾多模态学习演进，并引入CoW-Bench基准来评估视频生成模型和UMMs。

Result: 建立了通用世界模型的原则性理论框架，揭示了从松散耦合的专门模块向统一架构的发展轨迹，这些架构能够实现内部世界模拟器的协同涌现。CoW-Bench提供了统一的评估协议。

Conclusion: 该工作为通用世界模型建立了原则性路径，阐明了当前系统的局限性以及未来进展所需的架构要求。三位一体一致性框架为世界模型的发展提供了理论基础和评估标准。

Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.

</details>


### [58] [PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering](https://arxiv.org/abs/2602.23161)
*Junkai Lu,Peng Chen,Xingjian Wu,Yang Shu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.AI

TL;DR: PATRA模型通过模式感知机制提取时间序列的趋势和季节性模式，并设计任务感知平衡奖励来协调不同难度任务的学习，在时间序列问答任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列推理方法存在两个局限：1) 将时间序列简单视为文本或图像，无法捕捉趋势和季节性等关键模式；2) 在混合简单和复杂任务训练时，简单目标主导学习过程，阻碍深度推理能力发展。

Method: 提出PATRA模型：1) 引入模式感知机制，从时间序列中提取趋势和季节性模式以实现深度对齐；2) 设计任务感知平衡奖励，协调不同难度任务的学习，激励生成连贯的思维链。

Result: 大量实验表明，PATRA在多样化时间序列问答任务上优于强基线模型，展现出卓越的跨模态理解和推理能力。

Conclusion: PATRA通过模式感知对齐和平衡推理机制，有效解决了现有方法在时间序列推理中的局限性，实现了更好的跨模态理解和深度推理能力。

Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.

</details>


### [59] [ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering](https://arxiv.org/abs/2602.23193)
*Elzo Brito dos Santos Filho*

Main category: cs.AI

TL;DR: ESAA架构将LLM智能体的认知意图与项目状态变更分离，通过事件溯源模式确保任务不可变性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自主智能体存在结构性限制：缺乏原生状态、长时程上下文退化、概率生成与确定性执行需求之间的鸿沟。需要一种能确保任务不可变性、可追溯性并支持多智能体并发的架构。

Method: 提出ESAA架构，分离智能体的认知意图与项目状态变更。智能体只发射结构化意图（JSON格式），由确定性编排器验证、持久化事件到仅追加日志，应用文件写入效果，并投影可验证物化视图。包含边界合约、元提示配置和重放验证机制。

Result: 两个案例研究验证了架构：1) 单智能体着陆页项目（9任务，49事件）；2) 多智能体临床仪表板系统（50任务，86事件，4个并发智能体）。两者均以run.status=success和verify_status=ok完成，多智能体案例展示了真实并发编排能力。

Conclusion: ESAA架构通过事件溯源模式解决了LLM智能体的结构性限制，提供了任务不可变性、可追溯性和多智能体并发支持，为实际应用提供了可验证、可扩展的解决方案。

Abstract: Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.

</details>


### [60] [SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation](https://arxiv.org/abs/2602.23199)
*Jiahao Zhao,Feng Jiang,Shaowei Qin,Zhonghui Zhang,Junhao Liu,Guibing Guo,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: SC-ARENA是一个针对单细胞基础模型的自然语言评估框架，通过虚拟细胞抽象统一评估目标，引入知识增强评估以克服传统指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞生物学中LLM评估实践不足：现有基准分散在不同任务中，采用多项选择等与真实使用场景不符的格式，且依赖缺乏可解释性和生物学基础的指标。

Method: 提出SC-ARENA框架，包含虚拟细胞抽象统一评估目标，定义五个自然语言任务（细胞类型注释、描述、生成、扰动预测和科学问答），并引入知识增强评估方法，整合外部本体、标记数据库和科学文献。

Result: 实验表明：在虚拟细胞统一评估范式下，当前模型在生物学复杂任务上表现不均，特别是需要机制或因果理解的任务；知识增强评估框架确保生物学正确性，提供可解释的证据基础推理，具有高判别能力。

Conclusion: SC-ARENA为单细胞生物学中的LLM评估提供了统一且可解释的框架，指向开发生物学对齐、可泛化的基础模型。

Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.

</details>


### [61] [ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays](https://arxiv.org/abs/2602.23232)
*Aishik Sanyal*

Main category: cs.AI

TL;DR: 该研究实现了一个可检查的智能体ReCoN-Ipsundrum，通过实验发现情感耦合能增强偏好稳定性、结构化探索和持续性谨慎行为，展示了如何通过机制设计和因果干预来研究机器意识指标。


<details>
  <summary>Details</summary>
Motivation: 受Humphrey的ipsundrum假说启发，研究旨在通过机制关联的证据、架构检查和因果干预来探索机器意识指标，验证情感耦合在智能体行为中的作用。

Method: 扩展ReCoN状态机，增加基于感官显著性的循环持久性回路和可选的情感代理（效价/唤醒度）。通过固定参数消融实验（ReCoN、Ipsundrum、Ipsundrum+情感）操作化Humphrey的"qualiaphilia"概念，设计熟悉度控制的场景-单调路线选择任务。

Result: 发现新奇性解离：非情感变体对新奇敏感（Δ场景进入=0.07），情感耦合变体偏好稳定（Δ场景进入=0.01）即使场景新奇性降低（中位数Δ新奇性~-0.43）。情感变体在无奖励探索中显示结构化局部调查（扫描事件31.4 vs. 0.9），在疼痛尾部探测中表现出持续性计划谨慎（尾部持续时间90 vs. 5）。损伤反馈+整合会选择性降低ipsundrum变体的刺激后持久性（AUC下降27.62, 27.9%）。

Conclusion: 循环连接导致持久性，情感耦合控制导致偏好稳定性、扫描行为和持续性谨慎。研究展示了如何工程化意识指标特征，并说明机制和因果证据应伴随行为标记。

Abstract: Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we operationalize Humphrey's qualiaphilia (preference for sensory experience for its own sake) as a familiarity-controlled scenic-over-dull route choice. We find a novelty dissociation: non-affect variants are novelty-sensitive (Delta scenic-entry = 0.07). Affect coupling is stable (Delta scenic-entry = 0.01) even when scenic is less novel (median Delta novelty ~ -0.43). In reward-free exploratory play, the affect variant shows structured local investigation (scan events 31.4 vs. 0.9; cycle score 7.6). In a pain-tail probe, only the affect variant sustains prolonged planned caution (tail duration 90 vs. 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop 27.62, 27.9%) while leaving ReCoN unchanged. These dissociations link recurrence -> persistence and affect-coupled control -> preference stability, scanning, and lingering caution, illustrating how indicator-like signatures can be engineered and why mechanistic and causal evidence should accompany behavioral markers.

</details>


### [62] [Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive](https://arxiv.org/abs/2602.23239)
*Radha Sarma*

Main category: cs.AI

TL;DR: 论文证明基于优化的AI系统（如RLHF训练的LLMs）在结构上无法实现真正的规范性治理，因为它们将所有价值统一为可交易的标量指标，缺乏不可协商的边界和非推理性的响应机制。


<details>
  <summary>Details</summary>
Motivation: AI系统在医疗、法律、金融等高风险领域部署时，人们假设它们可以被规范性原则所治理。本文旨在证明这种假设对于基于优化的系统（特别是通过RLHF训练的大语言模型）在形式上是无效的。

Method: 通过形式化分析，建立真正智能体所需的两个必要且充分的结构条件：不可通约性（将某些边界视为不可协商的约束而非可交易权重）和否定性响应（当这些边界受到威胁时能够非推理性地暂停处理）。然后证明RLHF系统在结构上与这两个条件不相容。

Result: RLHF系统在结构上无法满足规范性治理的要求，其优化机制（将所有价值统一为标量指标并总是选择最高分输出）排除了规范性治理的可能性。这导致已知的失败模式（奉承、幻觉、不忠实的推理）不是偶然错误，而是结构性的表现。此外，部署这些系统会引发"收敛危机"——人类在指标压力下会从真正的智能体退化为标准检查优化器。

Conclusion: 基于优化的AI系统在结构上无法成为真正的规范性智能体。本文的主要积极贡献是提供了一个与底层无关的结构规范，定义了任何系统（生物、人工或制度）要成为智能体而非复杂工具所必须满足的条件。

Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.

</details>


### [63] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: AIQI是首个在通用强化学习中被证明具有渐近ε最优性的无模型智能体，通过分布动作价值函数的通用归纳实现


<details>
  <summary>Details</summary>
Motivation: 现有通用强化学习中的最优智能体（如AIXI）都是基于模型的，需要显式维护和使用环境模型。本文旨在开发首个无模型的通用最优智能体

Method: 提出AIQI（Universal AI with Q-Induction），通过对分布动作价值函数进行通用归纳，而不是像先前工作那样对策略或环境进行归纳

Result: 在"grain of truth"条件下，证明AIQI具有强渐近ε最优性和渐近ε贝叶斯最优性，显著扩展了已知通用智能体的多样性

Conclusion: AIQI是首个被证明在通用强化学习中具有渐近最优性的无模型智能体，为通用智能体设计开辟了新方向

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [64] [Mitigating Legibility Tax with Decoupled Prover-Verifier Games](https://arxiv.org/abs/2602.23248)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 提出一种解耦方法，将模型输出的正确性与可检查性分离：先训练求解器模型最大化正确性，再训练翻译器模型将求解器的输出转换为可检查形式，从而避免可检查性税。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，需要确保其输出能够被能力较弱的系统轻松检查。现有的证明者-验证者游戏虽然能提高可检查性，但会导致准确性下降（可检查性税）。

Method: 提出解耦方法：1）先训练"求解器"模型最大化正确性；2）再训练"翻译器"模型，将固定求解器的输出转换为可检查形式，同时保留原始答案。为此设计了新的解耦证明者-验证者游戏，其均衡对应忠实且可检查的翻译器。

Result: 该方法允许在保持求解器高准确性的同时，通过翻译器实现输出的可检查性，避免了传统方法中的可检查性税问题。

Conclusion: 通过解耦正确性与可检查性，并引入翻译器模型，能够在不牺牲准确性的前提下实现大语言模型输出的可检查性，为解决可检查性税问题提供了有效方案。

Abstract: As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a "translator" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.

</details>


### [65] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentDropoutV2：一个无需重新训练的动态优化多智能体系统信息流的测试时修正或剪枝框架，通过检索增强的修正器和失败驱动指示器池来纠正错误，不可修复的输出被剪枝以防止错误传播。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂推理方面表现出色，但存在个体参与者生成错误信息的级联影响问题。当前解决方案通常采用僵化的结构工程或昂贵的微调，限制了其可部署性和适应性。

Method: 提出AgentDropoutV2框架，作为主动防火墙拦截智能体输出，使用检索增强的修正器基于失败驱动指示器池迭代纠正错误。该机制利用蒸馏的失败模式作为先验知识精确识别潜在错误，不可修复的输出被剪枝以防止错误传播，同时使用回退策略保持系统完整性。

Result: 在广泛的数学基准测试中，AgentDropoutV2显著提升了多智能体系统的任务性能，在数学基准上平均准确率提高了6.3个百分点。系统表现出强大的泛化能力和适应性，能够根据任务难度动态调整修正努力，并利用上下文感知指示器解决广泛的错误模式。

Conclusion: AgentDropoutV2是一个有效的测试时优化框架，能够在不重新训练的情况下动态优化多智能体系统的信息流，通过修正或剪枝机制显著提升系统性能，同时保持强大的泛化能力和适应性。

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [66] [Evaluating Stochasticity in Deep Research Agents](https://arxiv.org/abs/2602.23271)
*Haotian Zhai,Elias Stengel-Eskin,Pratik Patil,Liu Leqi*

Main category: cs.AI

TL;DR: 本文研究了深度研究代理（DRA）中的随机性问题，提出了评估框架识别三个随机性来源，并通过实验证明减少随机性可以提高研究输出质量，同时提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 尽管深度研究代理在提高研究质量方面取得了进展，但现有系统设计往往忽视了实际部署中的关键障碍：随机性。相同查询下，DRA的重复执行会在研究结果、发现和引用方面表现出显著变异性，这阻碍了其在现实世界中的应用。

Method: 将DRA建模为信息获取马尔可夫决策过程，引入量化系统方差的评估框架，识别信息获取、信息压缩和推理三个随机性来源，通过控制实验研究这些模块在不同决策步骤中对输出方差的影响。

Result: 实验结果表明，减少随机性可以提高研究输出质量，其中推理和早期阶段的随机性对DRA输出方差贡献最大。在DeepSearchQA上的实验显示，提出的缓解方法将平均随机性降低了22%，同时保持了高质量的研究输出。

Conclusion: DRA中的随机性是实际部署的重要障碍，需要系统性地解决。通过结构化输出和基于集成的查询生成等策略，可以在保持输出质量的同时有效降低随机性，为更可靠的DRA系统设计提供了方向。

Abstract: Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations. In this paper, we formalize the study of stochasticity in DRAs by modeling them as information acquisition Markov Decision Processes. We introduce an evaluation framework that quantifies variance in the system and identify three sources of it: information acquisition, information compression, and inference. Through controlled experiments, we investigate how stochasticity from these modules across different decision steps influences the variance of DRA outputs. Our results show that reducing stochasticity can improve research output quality, with inference and early-stage stochasticity contributing the most to DRA output variance. Based on these findings, we propose strategies for mitigating stochasticity while maintaining output quality via structured output and ensemble-based query generation. Our experiments on DeepSearchQA show that our proposed mitigation methods reduce average stochasticity by 22% while maintaining high research quality.

</details>


### [67] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: CXReasonAgent是一个整合大语言模型与临床诊断工具的智能体，通过图像衍生的诊断和视觉证据进行基于证据的诊断推理，相比大型视觉语言模型能产生更可靠、可验证的诊断响应。


<details>
  <summary>Details</summary>
Motivation: 胸部X光在胸部诊断中起核心作用，其解读需要多步骤、基于证据的推理。然而，大型视觉语言模型经常生成看似合理但未忠实基于诊断证据的响应，提供的视觉证据有限，且需要昂贵的重新训练来支持新的诊断任务，限制了其在临床环境中的可靠性和适应性。

Method: 提出CXReasonAgent诊断智能体，整合大语言模型与临床诊断工具，使用图像衍生的诊断和视觉证据进行基于证据的诊断推理。同时引入CXReasonDial多轮对话基准，包含1,946个对话，涵盖12个诊断任务。

Result: CXReasonAgent能够产生忠实基于证据的响应，相比大型视觉语言模型，实现了更可靠和可验证的诊断推理。

Conclusion: 在安全关键的临床环境中，整合临床诊断工具至关重要。CXReasonAgent通过证据驱动的推理方法，为胸部X光诊断提供了更可靠和可验证的解决方案。

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


### [68] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: 提出ODEBRAIN框架，使用神经ODE建模连续脑电动态，通过时空频特征整合提升EEG预测性能


<details>
  <summary>Details</summary>
Motivation: 传统潜变量方法通过递归架构离散化时间建模脑动态，导致累积预测误差且无法捕捉EEG的瞬时非线性特性

Method: 将时空频特征整合到谱图节点中，然后使用神经ODE建模连续潜动态，确保潜表示能捕捉任意时间点的复杂脑状态随机变化

Result: 大量实验验证ODEBRAIN在预测EEG动态方面显著优于现有方法，具有增强的鲁棒性和泛化能力

Conclusion: ODEBRAIN框架通过神经ODE建模连续潜动态，有效克服传统方法的局限性，为脑动态建模提供了更优的解决方案

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


### [69] [The logic of KM belief update is contained in the logic of AGM belief revision](https://arxiv.org/abs/2602.23302)
*Giacomo Bonanno*

Main category: cs.AI

TL;DR: 该论文将KM信念更新的公理转化为包含三个模态算子的模态逻辑公理，并与AGM信念修正的模态逻辑进行比较，证明AGM信念修正是KM信念更新的特例。


<details>
  <summary>Details</summary>
Motivation: 研究KM信念更新与AGM信念修正之间的关系，通过模态逻辑形式化这两种信念变化理论，探索它们之间的包含关系。

Method: 为KM信念更新的每个公理提供对应的模态逻辑公理（使用B、>、□三个模态算子），然后将得到的逻辑与AGM信念修正的模态逻辑进行比较。

Result: 证明L_KM的每个公理都是L_AGM的定理，即AGM信念修正是KM信念更新的特例。对于强版本KM更新，两者差异可归结为处理"不令人惊讶信息"的单个公理。

Conclusion: AGM信念修正是KM信念更新的特殊情形，两种理论在模态逻辑框架下具有明确的包含关系，强版本KM更新与AGM修正的主要差异在于处理非初始不相信信息的方式。

Abstract: For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\mathcal L_{AGM}$ and the former by $\mathcal L_{KM}$ we show that every axiom of $\mathcal L_{KM}$ is a theorem of $\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\mathcal L_{KM}$ and $\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.

</details>


### [70] [Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315)
*Sha Hu*

Main category: cs.AI

TL;DR: 提出基于重采样的推理方法，通过对输入进行不变变换生成多个样本，聚合推理结果以提高准确性


<details>
  <summary>Details</summary>
Motivation: 即使经过优化的AI模型也会因偶然性和认知不确定性产生推理错误，观察到基于输入不变变换的多个样本推理错误具有部分独立性

Method: 提出"重采样"推理方法：对训练好的AI模型应用输入的多个变换版本，聚合推理输出得到更准确结果

Result: 该方法有潜力提高推理准确性，并提供平衡模型大小和性能的策略

Conclusion: 利用认知不确定性导致的推理错误部分独立性，通过重采样聚合方法可以改善AI模型推理性能

Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

</details>


### [71] [Generalized Rapid Action Value Estimation in Memory-Constrained Environments](https://arxiv.org/abs/2602.23318)
*Aloïs Rautureau,Tristan Cazenave,Éric Piette*

Main category: cs.AI

TL;DR: GRAVE2、GRAVER和GRAVER2算法通过两级搜索和节点回收技术，在保持GRAVE算法游戏强度的同时大幅减少内存使用


<details>
  <summary>Details</summary>
Motivation: GRAVE算法在通用游戏博弈中表现优异，但其需要存储额外的胜率/访问统计数据，在内存受限环境中不实用，限制了实际应用

Method: 提出GRAVE2、GRAVER和GRAVER2三种算法：GRAVE2采用两级搜索，GRAVER使用节点回收技术，GRAVER2结合了两种技术

Result: 这些增强技术能够大幅减少存储节点数量，同时保持与GRAVE算法相当的游戏强度

Conclusion: 通过两级搜索和节点回收技术，可以在内存受限环境中有效应用GRAVE算法，扩展了其实际适用性

Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.

</details>


### [72] [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329)
*Chen Bo Calvin Zhang,Christina Q. Knight,Nicholas Kruus,Jason Hausenloy,Pedro Medeiros,Nathaniel Li,Aiden Kim,Yury Orlovskiy,Coleman Breen,Bryce Cai,Jasper Götting,Andrew Bo Liu,Samira Nedungadi,Paula Rodriguez,Yannis Yiming He,Mohamed Shaaban,Zifan Wang,Seth Donoughe,Julian Michael*

Main category: cs.AI

TL;DR: LLMs显著提升生物安全相关任务中新手表现，准确率比仅使用互联网的新手高4.16倍，甚至在某些任务上超越专家，但存在双重用途风险。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解LLMs是否真正提升新手用户（相对于仅使用互联网资源）在生物安全相关任务上的表现，这对理解科学加速和双重用途风险至关重要。

Method: 采用多模型、多基准的人类提升研究，比较新手在LLM访问与仅互联网访问条件下的表现，涵盖八个生物安全相关任务集，参与者有充足时间（最复杂任务达13小时）。

Result: LLM访问提供显著提升：使用LLMs的新手准确率比对照组高4.16倍；在四个有专家基线的基准中，使用LLMs的新手在三个上超越专家；独立LLMs常超越LLM辅助的新手；89.6%参与者报告获取双重用途相关信息几乎没有困难。

Conclusion: LLMs显著提升新手在原本需要专业训练的生物任务上的表现，强调需要持续、交互式的提升评估与传统基准并行，同时突显双重用途风险管理的紧迫性。

Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

</details>


### [73] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 提出一个细粒度任务分解的多智能体LLM交易框架，相比传统粗粒度设计显著提升风险调整收益，并通过投资组合优化实现优异表现


<details>
  <summary>Details</summary>
Motivation: 现有基于多智能体的自主金融交易系统通常依赖抽象指令，忽略了真实工作流程的复杂性，导致推理性能下降和决策透明度不足

Method: 提出一个多智能体LLM交易框架，将投资分析明确分解为细粒度任务而非粗粒度指令，使用日本股票数据（价格、财务报表、新闻、宏观信息）在防泄漏的回测环境中评估

Result: 细粒度任务分解相比传统粗粒度设计显著改善风险调整收益，分析中间输出表明分析输出与下游决策偏好的对齐是系统性能的关键驱动因素，通过投资组合优化实现优异表现

Conclusion: 研究结果为在实际交易系统中应用LLM智能体时的智能体结构和任务配置设计提供了重要参考

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [74] [Queue occupancy and server size distribution of a queue length dependent vacation queue with an optional service](https://arxiv.org/abs/2602.22295)
*Ashish Verma,Sourav Pradhan*

Main category: cs.IT

TL;DR: 分析具有单/多休假策略的无限缓冲离散时间批量到达队列系统，其中服务分为基本服务(FES)和可选服务(SOS)两阶段，适用于5G、物联网等现代通信系统。


<details>
  <summary>Details</summary>
Motivation: 现代通信系统（5G、物联网、边缘计算）需要处理突发流量并提供自适应数据包处理、拥塞控制、安全检查、能效操作等功能。传统队列模型无法充分描述这些复杂服务场景，特别是当服务分为基本处理和可选增强处理两阶段时。

Method: 建立无限缓冲离散时间批量到达队列模型，采用单/多休假策略，服务分为两阶段：基本服务(FES)对应基础数据处理或路由，可选服务(SOS)对应加密、错误检查、数据压缩或深度包检测等增强功能。推导完成FES和SOS后等待传输数据包数与正在处理数据包数的二元概率生成函数，建立任意时隙的完整联合分布（包括休假完成状态）。

Result: 成功推导出系统的二元概率生成函数和完整联合分布，通过数值示例（包括离散相位型服务时间分布）验证了框架的适用性。通过图形表示对关键参数对边际系统概率和各种性能指标的敏感性进行了分析。

Conclusion: 提出的两阶段服务离散时间队列模型能有效描述现代通信系统中的复杂服务场景，为5G、物联网和边缘计算环境中的自适应数据包处理、拥塞控制和安全检查提供了理论分析框架。模型具有实际应用价值，可用于系统性能优化和参数敏感性分析。

Abstract: The discrete time queueing system is highly applicable to modern telecommunication systems, where it provides adaptive packet handling, congestion controlled security/inspection, energy efficient operation, and supports bursty traffic common in 5G, Internet of Things (IoT), and edge computing environments. In this article, we analyze an infinite-buffer discrete-time batch-arrival queue with single and multiple vacation policy where customers are served in batches, in two phases, namely first essential service (FES) and second optional service (SOS). In such systems, the FES corresponds to basic data processing or packet routing, while SOS represents secondary tasks such as encryption, error checking, data compression, or deep packet inspection that may not be necessary for every packet. Here, we derive the bivariate probability generating functions for the joint distribution of the number of packets waiting for transmission and the number are being processed immediately after the completion of both the FES and SOS. Furthermore, the complete joint distribution at arbitrary time slots, including vacation completion states, is established. Numerical illustrations demonstrate the applicability of the proposed framework, including an example with discrete phase type service time distribution. Finally, the sensitivity analysis of the key parameters on marginal system's probabilities and different performance measures have been investigated through several graphical representations.

</details>


### [75] [On the Computation Rate of All-Reduce](https://arxiv.org/abs/2602.22482)
*Yufeng Zhou,Hua Sun*

Main category: cs.IT

TL;DR: 本文研究了All-Reduce问题的计算速率，提出了基于割集的上界和基于时间共享的线性规划下界，并在特定网络拓扑中得到了最优解或紧界。


<details>
  <summary>Details</summary>
Motivation: All-Reduce是分布式计算中的核心操作，每个节点持有输入数据，需要通过通信网络计算所有输入的总和。研究目标是确定在任意带宽的并行链路网络中，All-Reduce的最大计算速率。

Method: 1. 提出割集上界：基于网络流理论推导计算速率的上界；2. 提出线性规划下界：基于时间共享策略，考虑所有先Reduce后Broadcast的方案；3. 将通用界应用于特定网络拓扑：循环网络、完全网络和超立方体网络。

Result: 1. 对于一类通信网络得到了最优计算速率；2. 对于循环网络、完全网络和超立方体网络，得到了已知最佳速率界，其中上界不超过下界的两倍。

Conclusion: 本文为All-Reduce问题建立了系统的理论分析框架，通过割集上界和线性规划下界方法，在多种网络拓扑中获得了紧致的速率界，为分布式计算系统的设计提供了理论指导。

Abstract: In the All-Reduce problem, each one of the K nodes holds an input and wishes to compute the sum of all K inputs through a communication network where each pair of nodes is connected by a parallel link with arbitrary bandwidth. The computation rate of All-Reduce is defined as the number of sum instances that can be computed over each network use. For the computation rate, we provide a cut-set upper bound and a linear programming lower bound based on time (bandwidth) sharing over all schemes that first perform Reduce (aggregating all inputs at one node) and then perform Broadcast (sending the sum from that node to all other nodes). Specializing the two general bounds gives us the optimal computation rate for a class of communication networks and the best-known rate bounds (where the upper bound is no more than twice of the lower bound) for cyclic, complete, and hypercube networks.

</details>


### [76] [A Thermodynamic Structure of Asymptotic Inference](https://arxiv.org/abs/2602.22605)
*Willy Wong*

Main category: cs.IT

TL;DR: 论文提出一个将渐近推断与热力学框架统一的理论，其中样本量和参数方差构成状态空间，香农信息扮演熵的角色，推导出类似热力学定律的不等式和效率界限。


<details>
  <summary>Details</summary>
Motivation: 将统计推断与热力学理论建立联系，为渐近推断提供一个统一的物理框架，揭示统计推断过程与热力学系统之间的深层相似性。

Method: 构建一个热力学框架，其中样本量和参数方差定义状态空间，香农信息作为熵，通过积分因子组织变化形成类似热力学第一定律的平衡方程。

Result: 推导出类似反向第二定律的循环不等式、类似第三定律的熵下界（由表示噪声设定）、最优推断路径、信息增益的全局界限，以及类似卡诺效率的信息效率，其效率受噪声底限限制。

Conclusion: 该框架表明集合物理和推断物理构成在统一热力学描述中沿相反方向演化的影子过程，de Bruijn恒等式和I-MMSE关系在高斯极限情况下表现为同一热力学结构的坐标投影。

Abstract: A thermodynamic framework for asymptotic inference is developed in which sample size and parameter variance define a state space. Within this description, Shannon information plays the role of entropy, and an integrating factor organizes its variation into a first-law-type balance equation. The framework supports a cyclic inequality analogous to a reversed second law, derived for the estimation of the mean. A non-trivial third-law-type result emerges as a lower bound on entropy set by representation noise. Optimal inference paths, global bounds on information gain, and a natural Carnot-like information efficiency follow from this structure, with efficiency fundamentally limited by a noise floor. Finally, de Bruijn's identity and the I-MMSE relation in the Gaussian-limit case appear as coordinate projections of the same underlying thermodynamic structure. This framework suggests that ensemble physics and inferential physics constitute shadow processes evolving in opposite directions within a unified thermodynamic description.

</details>


### [77] [Multi-modal Data Driven Virtual Base Station Construction for Massive MIMO Beam Alignment](https://arxiv.org/abs/2602.22796)
*Yijie Bian,Wei Guo,Jie Yang,Shenghui Song,Jun Zhang,Shi Jin,Khaled B. Letaief*

Main category: cs.IT

TL;DR: 提出一种可解释的波束对准框架，利用多模态数据构建虚拟基站，在混合视距/非视距环境中实现低开销的波束管理


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO是6G网络实现高数据速率的关键，但其性能依赖于低训练开销的有效波束管理。在混合视距和非视距传播环境中，波束对准尤其具有挑战性。

Method: 利用多模态数据构建虚拟基站，这些VBS是基站在反射面（从3D LiDAR点重建）上的镜像。基于VBS开发波束对准方案，包括粗粒度信道重建和部分波束训练。

Result: 数值结果表明，所提方法在频谱效率方面实现了接近最优的性能。

Conclusion: 提出的可解释框架通过构建虚拟基站提供无线环境的稀疏空间表示，在混合传播环境中实现了高效低开销的波束对准。

Abstract: Massive multiple-input multiple-output (MIMO) is a key enabler for the high data rates required by the sixth-generation networks, yet its performance hinges on effective beam management with low training overhead. This paper proposes an interpretable framework to tackle beam alignment in mixed line-of-sight (LoS) and non-line-of-sight (NLoS) propagation environments. Our approach utilizes multi-modal data to construct virtual base stations (VBSs), which are geometrically defined as mirror images of the base station across reflecting surfaces reconstructed from 3D LiDAR points. These VBSs provide a sparse and spatial representation of the dominant features of the wireless environment. Based on the constructed VBSs, we develop a VBS-assisted beam alignment scheme comprising coarse channel reconstruction followed by partial beam training. Numerical results demonstrate that the proposed method achieves near-optimal performance in terms of spectral efficiency.

</details>


### [78] [Semantic Communication Through the Lens of Context-Dependent Channel Modeling](https://arxiv.org/abs/2602.22934)
*Javad Gholipour,Rafael F. Schaefer,Gerhard P. Fettweis*

Main category: cs.IT

TL;DR: 本文研究语义通信中语义噪声问题，提出基于上下文的状态依赖信道模型，分析语义编码器表示能力，推导语义噪声下的容量结果和可达速率。


<details>
  <summary>Details</summary>
Motivation: 语义通信作为下一代网络有前景的范式，但仍存在若干基本挑战未解决。本文针对语义噪声仅来自语义信道（假设理想物理信道）的特殊子类问题进行研究，旨在建立理论框架和分析方法。

Method: 基于语义通信的概率模型，利用上下文概念，引入虚拟状态依赖信道模型，其中状态（代表上下文）在通信中起关键作用。分析语义编码器的表示能力，探索多种语义噪声场景。

Result: 推导了某些情况下的容量结果和其他情况下的可达速率，为语义通信在语义噪声环境下的性能提供了理论界限和分析工具。

Conclusion: 本文为语义通信中语义噪声问题的理论研究提供了系统框架，通过状态依赖信道模型和上下文概念，为语义通信系统的设计和分析奠定了基础。

Abstract: Semantic communication has emerged as a promising paradigm for next-generation networks, yet several fundamental challenges remain unresolved. Building on the probabilistic model of semantic communication and leveraging the concept of context, this paper examines a specific subclass of semantic communication problems, where semantic noise originates solely from the semantic channel, assuming an ideal physical channel. To model this system, we introduce a virtual state-dependent channel, where the state-representing context-plays a crucial role in shaping communication. We further analyze the representational capability of the semantic encoder and explore various semantic communication scenarios in the presence of semantic noise, deriving capacity results for some cases and achievable rates for others.

</details>


### [79] [Frequency-Ordered Tokenization for Better Text Compression](https://arxiv.org/abs/2602.22958)
*Maximilian Kalcher*

Main category: cs.IT

TL;DR: 提出频率排序分词技术，通过利用自然语言词符的幂律分布（齐夫定律）来改进无损文本压缩。该方法使用BPE分词，重新排序词汇表使高频词符获得小整数标识符，然后用变长整数编码，再传递给标准压缩器。


<details>
  <summary>Details</summary>
Motivation: 自然语言文本中词符频率遵循幂律分布（齐夫定律），高频词符出现频率远高于低频词符。利用这一特性，通过让高频词符获得更小的整数标识符，可以改善后续压缩算法的效率。

Method: 1. 使用字节对编码（BPE）对文本进行分词；2. 根据词符频率重新排序词汇表，使高频词符获得小整数标识符；3. 使用变长整数编码处理结果；4. 将处理后的数据传递给任意标准压缩器。

Result: 在enwik8（100MB维基百科）上，相比原始压缩：zlib提升7.08个百分点，LZMA提升1.69个百分点，zstd提升0.76个百分点（均包含词汇表开销）。在1GB规模（enwik9）以及中文和阿拉伯文文本上效果一致。预处理还加速了计算密集型压缩算法：包含预处理的总耗时比原始zstd-22快3.1倍，比原始LZMA快2.4倍。

Conclusion: 频率排序分词是一种简单有效的文本压缩预处理技术，能够显著提升多种标准压缩算法的压缩率，同时加速计算密集型压缩过程。该方法实现简单（少于50行代码），在多种语言和规模下均有效。

Abstract: We present frequency-ordered tokenization, a simple preprocessing technique that improves lossless text compression by exploiting the power-law frequency distribution of natural language tokens (Zipf's law). The method tokenizes text with Byte Pair Encoding (BPE), reorders the vocabulary so that frequent tokens receive small integer identifiers, and encodes the result with variable-length integers before passing it to any standard compressor. On enwik8 (100 MB Wikipedia), this yields improvements of 7.08 percentage points (pp) for zlib, 1.69 pp for LZMA, and 0.76 pp for zstd (all including vocabulary overhead), outperforming the classical Word Replacing Transform. Gains are consistent at 1 GB scale (enwik9) and across Chinese and Arabic text. We further show that preprocessing accelerates compression for computationally expensive algorithms: the total wall-clock time including preprocessing is 3.1x faster than raw zstd-22 and 2.4x faster than raw LZMA, because the preprocessed input is substantially smaller. The method can be implemented in under 50 lines of code.

</details>


### [80] [Secure Transmission for Fluid Antenna-Aided ISAC Systems](https://arxiv.org/abs/2602.23241)
*Yunxiao Li,Qian Zhang,Xuejun Cheng,Zhiguo Wang,Xiaoyan Wang,Hongji Xu,Ju Liu*

Main category: cs.IT

TL;DR: 本文研究了流体天线辅助的集成感知与通信系统中的安全传输问题，通过联合优化天线位置向量和波束成形来最大化多用户总保密速率，相比固定位置天线系统获得了超过20%的性能增益。


<details>
  <summary>Details</summary>
Motivation: 在集成感知与通信系统中，当感知目标充当窃听者时，如何最大化总保密速率的问题尚未得到解决。流体天线的空间灵活性为增强物理层安全提供了新的可能性。

Method: 采用块连续上界最小化算法，结合近端距离算法进行波束成形器的闭式更新，以及外推投影梯度算法进行天线位置向量优化，以解决非凸优化问题。

Result: 仿真结果表明，所提出的流体天线-集成感知与通信方案相比固定位置天线系统，实现了超过20%的总保密速率增益。

Conclusion: 流体天线技术能够有效提升集成感知与通信系统的物理层安全性能，通过联合优化天线位置和波束成形可以显著提高系统的保密通信能力。

Abstract: Fluid antenna (FA) has become a highly promising technology and has recently been used to enhance the integrated sensing and communication (ISAC) system. However, the scenario where sensing targets act as eavesdroppers in ISAC and how to maximize the sum secrecy rate has not been addressed. This letter investigates secure transmission in FA-aided ISAC systems, where the spatial agility of FAs enables enhanced physical layer security. We jointly optimize antenna position vector (APV) and beamforming to maximize the multiuser sum secrecy rate, which complicates the solution process. To solve the resulting non-convex problem, we use a block successive upper-bound minimization (BSUM) algorithm, which incorporates the proximal distance algorithm (PDA) for closed-form beamformer updates and extrapolated projected gradient (EPG) for APV optimization. Simulation results show that the proposed FA-ISAC scheme achieves over 20$\%$ sum secrecy rate gain compared to fixed-position antenna (FPA) systems.

</details>
