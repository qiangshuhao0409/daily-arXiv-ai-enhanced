<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Frame-Based Zero-Shot Semantic Channel Equalization for AI-Native Communications](https://arxiv.org/abs/2507.17835)
*Simone Fiorellino,Claudio Battiloro,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.NI

TL;DR: 提出了一种零样本的Parseval Frame Equalizer（PFE），用于对齐异构编码器的潜在空间，解决语义信道噪声问题，并通过动态优化策略平衡资源消耗与性能。


<details>
  <summary>Details</summary>
Motivation: 未来AI原生无线网络中，独立设计的DNN编码器潜在空间不匹配会导致语义信道噪声，影响系统性能。

Method: 提出PFE方法，无需重新训练即可对齐潜在空间，并引入动态优化策略协调通信、计算和学习资源。

Result: 仿真验证了PFE在保持语义一致性和满足延迟与精度约束方面的有效性。

Conclusion: PFE能有效解决语义噪声问题，并在动态网络条件下优化系统性能。

Abstract: In future AI-native wireless networks, the presence of mismatches between the
latent spaces of independently designed and trained deep neural network (DNN)
encoders may impede mutual understanding due to the emergence of semantic
channel noise. This undermines the receiver's ability to interpret transmitted
representations, thereby reducing overall system performance. To address this
issue, we propose the Parseval Frame Equalizer (PFE), a zero-shot, frame-based
semantic channel equalizer that aligns latent spaces of heterogeneous encoders
without requiring system retraining. PFE enables dynamic signal compression and
expansion, mitigating semantic noise while preserving performance on downstream
tasks. Building on this capability, we introduce a dynamic optimization
strategy that coordinates communication, computation, and learning resources to
balance energy consumption, end-to-end (E2E) latency, and task performance in
multi-agent semantic communication scenarios. Extensive simulations confirm the
effectiveness of our approach in maintaining semantic consistency and meeting
long-term constraints on latency and accuracy under diverse and time-varying
network conditions.

</details>


### [2] [ARCADE: A RAN Diagnosis Methodology in a Hybrid AI Environment for 6G Networks](https://arxiv.org/abs/2507.17861)
*Daniel Ricardo Cunha Oliveira,Rodrigo Moreira,Flávio de Oliveira Silva*

Main category: cs.NI

TL;DR: 论文提出了一种名为ARCADE的方法，用于检测和评估蜂窝接入网络中的异常，并展示了在6G演进中如何通过混合架构增强AI在网络中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前5G网络中的NWDAF功能尚不足以全面支持网络自动化，尤其是在未充分探索的网络段中，因此需要更全面的方法。

Method: 提出了ARCADE方法，用于识别和诊断蜂窝接入网络中的异常，并结合混合架构的网络分析功能。

Result: 展示了ARCADE作为6G演进中AI应用的实例，证明了其有效性。

Conclusion: ARCADE方法及其混合架构为6G网络中AI的广泛应用提供了可行方案。

Abstract: Artificial Intelligence (AI) plays a key role in developing 6G networks.
While current specifications already include Network Data Analytics Function
(NWDAF) as a network element responsible for providing information about the
core, a more comprehensive approach will be needed to enable automation of
network segments that are not yet fully explored in the context of 5G. In this
paper, we present Automated Radio Coverage Anomalies Detection and Evaluation
(ARCADE), a methodology for identifying and diagnosing anomalies in the
cellular access network. Furthermore, we demonstrate how a hybrid architecture
of network analytics functions in the evolution toward 6G can enhance the
application of AI in a broader network context, using ARCADE as a practical
example of this approach.

</details>


### [3] [Talk with the Things: Integrating LLMs into IoT Networks](https://arxiv.org/abs/2507.17865)
*Alakesh Kalita*

Main category: cs.NI

TL;DR: 论文提出了一种基于边缘计算的框架，将大型语言模型（LLMs）与物联网（IoT）网络结合，实现自然语言控制、上下文感知决策和增强自动化。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs与IoT网络的融合，以构建智能、响应迅速且用户友好的系统。

Method: 采用轻量级检索增强生成（RAG）的LLMs，部署在边缘计算设备上，结合IoT网关实现本地化处理。

Result: 通过智能家居原型验证框架，展示模型精度与推理时间之间的权衡。

Conclusion: 讨论了LLM-based IoT系统的潜在应用及关键挑战。

Abstract: The convergence of Large Language Models (LLMs) and Internet of Things (IoT)
networks open new opportunities for building intelligent, responsive, and
user-friendly systems. This work presents an edge-centric framework that
integrates LLMs into IoT architectures to enable natural language-based
control, context-aware decision-making, and enhanced automation. The proposed
modular and lightweight Retrieval Augmented Generation (RAG)-based LLMs are
deployed on edge computing devices connected to IoT gateways, enabling local
processing of user commands and sensor data for reduced latency, improved
privacy, and enhanced inference quality. We validate the framework through a
smart home prototype using LLaMA 3 and Gemma 2B models for controlling smart
devices. Experimental results highlight the trade-offs between model accuracy
and inference time with respect to models size. At last, we also discuss the
potential applications that can use LLM-based IoT systems, and a few key
challenges associated with such systems.

</details>


### [4] [Enabling Scalability in Asynchronous and Bidirectional Communication in LPWAN](https://arxiv.org/abs/2507.17905)
*Mahbubur Rahman*

Main category: cs.NI

TL;DR: 论文提出了一种基于SNOW技术的LPWAN改进方法，通过D-OFDM和Gold码序列实现大规模异步传感器的高效并行数据传输，显著提升了网络的可扩展性和能效。


<details>
  <summary>Details</summary>
Motivation: 解决LPWAN在大规模传感器网络中数据传输效率低、延迟高的问题，以满足新兴物联网和CPS应用的需求。

Method: 利用分布式正交频分复用（D-OFDM）和Gold码伪随机噪声序列，实现基站对同一子载波上多个异步传感器数据的并行解码，同时支持多子载波并行传输。

Result: 实验表明，改进后的SNOW技术可提升约9倍的可扩展性，同时保持数据收集的及时性和传感器的能效。

Conclusion: 该方法为需要大规模传感器网络的新兴应用提供了高效、低延迟的解决方案，延长了传感器电池寿命并支持实时决策。

Abstract: LPWANs have become ubiquitous due to their ability to connect sensors over
large geographic areas in a single hop. It is, however, very challenging to
achieve massive scalability in LPWANs, where numerous sensors can transmit data
efficiently and with low latency, which emerging IoT and CPS applications may
require. In this paper, we address the above challenges by significantly
advancing an LPWAN technology called SNOW. SNOW exploits distributed orthogonal
frequency division multiplexing, D-OFDM, subcarriers to enable parallel
reception of data to a BS from multiple asynchronous sensors, each using a
different subcarrier. In this paper, we achieve massive scalability in SNOW by
enabling the BS to decode concurrent data from numerous asynchronous sensors on
the same subcarrier while parallelly decoding from other subcarriers as well.
Additionally, we enable numerous asynchronous sensors to receive distinct data
from the BS on the same subcarrier while other sensors also receive data
parallelly on other subcarriers. To do this, we develop a set of Gold
code-based pseudorandom noise or PN sequences that are mutually non-interfering
within and across the subcarriers. Each sensor uses its PN sequence from the
set for encoding or decoding data on its subcarriers, enabling massive
concurrency. Our evaluation results demonstrate that we can achieve
approximately 9x more scalability in SNOW while being timely in data collection
at the BS and energy efficient at the sensors. This may enable emerging IoT and
CPS applications requiring tens of thousands of sensors with longer battery
life and making data-driven, time-sensitive decisions.

</details>


### [5] [Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of Information Optimization in Vehicular Networks](https://arxiv.org/abs/2507.18328)
*Xiao Xu,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen,Khaled B. Letaief*

Main category: cs.NI

TL;DR: 论文研究了5G NR V2I Mode 2下车联网中的公平访问问题和信息新鲜度（AoI），提出了一种联合优化框架，通过调整SPS选择窗口和LLM-MOEA/D算法，平衡公平性和AoI。


<details>
  <summary>Details</summary>
Motivation: 车辆在相邻车道行驶时速度不同，导致RSU驻留时间和通信时长差异，从而引发网络资源访问不公平，可能影响驾驶安全。同时，信息新鲜度（AoI）对驾驶辅助至关重要。

Method: 提出了一种联合优化框架，定义公平性指数，并采用SHS建模AoI。通过调整Mode 2中的SPS选择窗口，结合LLM-MOEA/D算法优化公平性和AoI。

Result: 仿真结果表明，所提方案能有效平衡公平访问和最小化AoI。

Conclusion: 该研究为车联网中的公平访问和信息新鲜度问题提供了有效的解决方案，优化了驾驶辅助数据的及时性和相关性。

Abstract: In this paper, we consider the fair access problem and the Age of Information
(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in
vehicular networks. Specifically, vehicles follow Mode 2 to communicate with
Roadside Units (RSUs) to obtain accurate data for driving
assistance.Nevertheless, vehicles often have different velocity when they are
moving in adjacent lanes, leading to difference in RSU dwelltime and
communication duration. This results in unfair access to network resources,
potentially influencing driving safety. To ensure the freshness of received
data, the AoI should be analyzed. Mode 2 introduces a novel preemption
mechanism, necessitating simultaneous optimization of fair access and AoI to
guarantee timely and relevant data delivery. We propose a joint optimization
framework for vehicular network, defining a fairness index and employing
Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By
adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)
in Mode 2, we address the optimization of fairness and AoI. We apply a large
language model (LLM)-Based Multi-objective Evolutionary Algorithm Based on
Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate
the effectiveness of our scheme in balancing fair access and minimizing AoI.

</details>


### [6] [Improving Wi-Fi 8 Latency with Coordinated Spatial Reuse](https://arxiv.org/abs/2507.18480)
*David Nunez,Francesc Wilhelmi,Lorenzo Galati-Giordano,Giovanni Geraci,Boris Bellalta*

Main category: cs.NI

TL;DR: 论文研究了Wi-Fi 8网络中协调空间复用（Co-SR）的性能，提出了一种与Wi-Fi 8标准化工作一致的Co-SR实现，并通过仿真验证其在密集环境中的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为满足云游戏、扩展现实（XR）和视频流等高吞吐、低延迟、高可靠性应用的需求，IEEE 802.11网络需要优化频谱资源利用。

Method: 提出了一种与Wi-Fi 8标准化工作一致的Co-SR实现，并通过Wi-Fi模拟器在四AP的WLAN中评估其性能。

Result: 与分布式协调功能（DCF）相比，Co-SR在延迟上实现了31%至95%的降低。

Conclusion: Co-SR机制在密集环境中能显著提升网络性能，为Wi-Fi 8的标准化提供了有价值的参考。

Abstract: IEEE 802.11 networks continuously adapt to meet the stringent requirements of
emerging applications like cloud gaming, eXtended Reality (XR), and video
streaming services, which require high throughput, low latency, and high
reliability. To address these challenges, Coordinated Spatial Reuse (Co-SR) can
potentially contribute to optimizing spectrum resource utilization. This
mechanism is expected to enable simultaneous transmissions, thereby boosting
spectral efficiency in dense environments and increasing the overall network
performance. In this paper, we shed light on the performance of Co-SR for Wi-Fi
8 networks. For that, we propose an implementation of Co-SR aligned with
ongoing Wi-Fi 8 standardization efforts. The evaluation is done on a Wi-Fi
simulator, which allows us to study the performance of the proposed Co-SR
mechanisms in relevant scenarios. The results obtained in a Wireless Local Area
Network (WLAN) consisting of four APs show delay reduction with Co-SR ranging
from 31% to 95% when compared to Distributed Coordination Function (DCF).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics](https://arxiv.org/abs/2507.17777)
*Theofanis Aravanis,Grigorios Chrimatopoulos,Mohammad Ferdows,Michalis Xenos,Efstratios Em Tzirtzilakis*

Main category: cs.AI

TL;DR: 该研究将符号回归（SR）应用于三维不可压缩流动建模，结合答案集编程（ASP）提升模型的物理合理性，展示了SR在简化复杂流动行为中的潜力。


<details>
  <summary>Details</summary>
Motivation: 流体力学中理解流动物理与准确预测同样重要，SR无需先验模型假设，适合揭示复杂系统的数学关系。

Method: 使用PySR库从数值模拟数据中推导符号方程，并结合ASP框架确保模型的物理合理性。

Result: SR生成的方程不仅近似模拟了抛物线速度分布和压力降，还与文献中的解析解完全一致。

Conclusion: SR能简化复杂流动行为，结合知识表示方法可提升数据驱动模型的可靠性和领域一致性。

Abstract: Unlike conventional Machine-Learning (ML) approaches, often criticized as
"black boxes", Symbolic Regression (SR) stands out as a powerful tool for
revealing interpretable mathematical relationships in complex physical systems,
requiring no a priori assumptions about models' structures. Motivated by the
recognition that, in fluid mechanics, an understanding of the underlying flow
physics is as crucial as accurate prediction, this study applies SR to model a
fundamental three-dimensional (3D) incompressible flow in a rectangular
channel, focusing on the (axial) velocity and pressure fields under laminar
conditions. By employing the PySR library, compact symbolic equations were
derived directly from numerical simulation data, revealing key characteristics
of the flow dynamics. These equations not only approximate the parabolic
velocity profile and pressure drop observed in the studied fluid flow, but also
perfectly coincide with analytical solutions from the literature. Furthermore,
we propose an innovative approach that integrates SR with the
knowledge-representation framework of Answer Set Programming (ASP), combining
the generative power of SR with the declarative reasoning strengths of ASP. The
proposed hybrid SR/ASP framework ensures that the SR-generated symbolic
expressions are not only statistically accurate, but also physically plausible,
adhering to domain-specific principles. Overall, the study highlights two key
contributions: SR's ability to simplify complex flow behaviours into concise,
interpretable equations, and the potential of knowledge-representation
approaches to improve the reliability and alignment of data-driven SR models
with domain principles. Insights from the examined 3D channel flow pave the way
for integrating such hybrid approaches into efficient frameworks, [...] where
explainable predictions and real-time data analysis are crucial.

</details>


### [8] [I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis](https://arxiv.org/abs/2507.17874)
*SaiBarath Sundar,Pranav Satheesan,Udayaadithya Avadhanam*

Main category: cs.AI

TL;DR: I2I-STRADA是一种多智能体架构，通过结构化推理过程改进数据分析中的认知工作流，优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有数据分析系统缺乏结构化推理过程，无法满足实际分析中的认知需求。

Method: 提出I2I-STRADA架构，通过模块化子任务模拟分析推理的认知步骤。

Result: 在DABstep和DABench基准测试中，I2I-STRADA在规划一致性和洞察对齐方面表现更优。

Conclusion: 结构化认知工作流对数据分析智能体设计至关重要。

Abstract: Recent advances in agentic systems for data analysis have emphasized
automation of insight generation through multi-agent frameworks, and
orchestration layers. While these systems effectively manage tasks like query
translation, data transformation, and visualization, they often overlook the
structured reasoning process underlying analytical thinking. Reasoning large
language models (LLMs) used for multi-step problem solving are trained as
general-purpose problem solvers. As a result, their reasoning or thinking steps
do not adhere to fixed processes for specific tasks. Real-world data analysis
requires a consistent cognitive workflow: interpreting vague goals, grounding
them in contextual knowledge, constructing abstract plans, and adapting
execution based on intermediate outcomes. We introduce I2I-STRADA
(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an
agentic architecture designed to formalize this reasoning process. I2I-STRADA
focuses on modeling how analysis unfolds via modular sub-tasks that reflect the
cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench
benchmarks show that I2I-STRADA outperforms prior systems in planning coherence
and insight alignment, highlighting the importance of structured cognitive
workflows in agent design for data analysis.

</details>


### [9] [SMARTAPS: Tool-augmented LLMs for Operations Management](https://arxiv.org/abs/2507.17927)
*Timothy Tin Long Yu,Mahdi Mostajabdaveh,Jabo Serge Byusa,Rindra Ramamonjison,Giuseppe Carenini,Kun Mao,Zirui Zhou,Yong Zhang*

Main category: cs.AI

TL;DR: 论文介绍了一种基于增强型大语言模型的对话系统SmartAPS，旨在通过自然语言交互帮助供应链规划师更便捷地使用高级规划系统。


<details>
  <summary>Details</summary>
Motivation: 高级规划系统（APS）因定制和维护成本高昂，许多用户无法负担。为满足供应链规划师对更易用APS的需求，作者开发了SmartAPS。

Method: SmartAPS基于工具增强的大语言模型，提供自然语言聊天界面，支持查询、反事实推理、推荐和情景分析。

Result: 系统通过自然语言交互简化了APS的使用，提升了操作规划的效率。

Conclusion: SmartAPS通过LLM技术降低了APS的使用门槛，为供应链规划提供了更便捷的工具。

Abstract: Large language models (LLMs) present intriguing opportunities to enhance user
interaction with traditional algorithms and tools in real-world applications.
An advanced planning system (APS) is a sophisticated software that leverages
optimization to help operations planners create, interpret, and modify an
operational plan. While highly beneficial, many customers are priced out of
using an APS due to the ongoing costs of consultants responsible for
customization and maintenance. To address the need for a more accessible APS
expressed by supply chain planners, we present SmartAPS, a conversational
system built on a tool-augmented LLM. Our system provides operations planners
with an intuitive natural language chat interface, allowing them to query
information, perform counterfactual reasoning, receive recommendations, and
execute scenario analysis to better manage their operation. A short video
demonstrating the system has been released: https://youtu.be/KtIrJjlDbyw

</details>


### [10] [Synthesis of timeline-based planning strategies avoiding determinization](https://arxiv.org/abs/2507.17988)
*Dario Della Monica,Angelo Montanari,Pietro Sala*

Main category: cs.AI

TL;DR: 本文提出了一种定性时间线规划的分支，其计划存在性问题可直接映射到确定性有限自动机的非空性问题，从而避免复杂的确定性步骤。


<details>
  <summary>Details</summary>
Motivation: 定性时间线规划的计划存在性问题通常需要复杂的确定性步骤，本文旨在找到一种可直接映射到确定性有限自动机的方法，简化策略合成。

Method: 识别定性时间线规划的一个分支，其计划存在性问题可直接映射到确定性有限自动机的非空性问题，并确定符合该分支的Allen关系的最大子集。

Result: 成功识别了一个可直接映射到确定性有限自动机的规划分支，并确定了适用的Allen关系子集。

Conclusion: 该研究为简化时间线规划的策略合成提供了新方法，并明确了适用的约束范围。

Abstract: Qualitative timeline-based planning models domains as sets of independent,
but
  interacting, components whose behaviors over time, the timelines, are
governed
  by sets of qualitative temporal constraints (ordering relations), called
  synchronization rules.
  Its plan-existence problem has been shown to be PSPACE-complete; in
  particular, PSPACE-membership has been proved via reduction to the
  nonemptiness problem for nondeterministic finite automata.
  However, nondeterministic automata cannot be directly used to synthesize
  planning strategies as a costly determinization step is needed.
  In this paper, we identify a fragment of qualitative timeline-based planning
  whose plan-existence problem can be directly mapped into the nonemptiness
  problem of deterministic finite automata, which can then
  synthesize strategies.
  In addition, we identify a maximal subset of Allen's relations that fits into
  such a deterministic fragment.

</details>


### [11] [E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI](https://arxiv.org/abs/2507.18004)
*Yusen Peng,Shuhua Mao*

Main category: cs.AI

TL;DR: 论文提出E.A.R.T.H.框架，通过错误生成、放大、精炼选择、转换和反馈利用，将AI生成的错误转化为创意资产，显著提升创造力。


<details>
  <summary>Details</summary>
Motivation: 探索AI如何超越模仿实现真正的创造力，认为“创意潜力隐藏在失败中”。

Method: 采用五阶段生成管道（E.A.R.T.H.），结合结构化提示、语义评分和人类参与评估，使用多种模型（如LLaMA-2-7B-Chat、SBERT等）和复合奖励函数。

Result: 创造力评分提升70.4%，精炼后的标语更短、更新颖，跨模态测试显示强对齐性，人类评价中60%输出得分≥4.0。

Conclusion: 错误驱动和反馈驱动的生成方法能有效增强AI创造力，为自进化、人类对齐的创意AI提供可行路径。

Abstract: How can AI move beyond imitation toward genuine creativity? This paper
proposes the E.A.R.T.H. framework, a five-stage generative pipeline that
transforms model-generated errors into creative assets through Error
generation, Amplification, Refine selection, Transform, and Harness feedback.
Drawing on cognitive science and generative modeling, we posit that "creative
potential hides in failure" and operationalize this via structured prompts,
semantic scoring, and human-in-the-loop evaluation. Implemented using
LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the
pipeline employs a composite reward function based on novelty, surprise, and
relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to
1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%
improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a
4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment
(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs
scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones
(3.99). Feedback highlights stylistic precision and emotional resonance. These
results demonstrate that error-centered, feedback-driven generation enhances
creativity, offering a scalable path toward self-evolving, human-aligned
creative AI.

</details>


### [12] [Does visualization help AI understand data?](https://arxiv.org/abs/2507.18022)
*Victoria R. Li,Johnathan Sun,Martin Wattenberg*

Main category: cs.AI

TL;DR: AI系统在处理数据时，图表（如散点图）能提高其描述的精确性和准确性，尤其是在复杂数据集中。


<details>
  <summary>Details</summary>
Motivation: 探讨图表是否对AI系统分析数据有帮助。

Method: 使用GPT 4.1和Claude 3.5两种商业视觉语言模型，在三种代表性分析任务中测试图表（散点图）对数据描述的影响。

Result: 图表显著提升了AI系统对数据的描述能力，尤其是在复杂数据集中。

Conclusion: AI系统与人类类似，能从可视化中受益。

Abstract: Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.

</details>


### [13] [Multi-Agent Guided Policy Optimization](https://arxiv.org/abs/2507.18059)
*Yueheng Li,Guangming Xie,Zongqing Lu*

Main category: cs.AI

TL;DR: MAGPO是一种新型的多智能体强化学习框架，通过结合集中式指导和分散式执行，优化了CTDE范式，提供了理论保证和实际性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有CTDE方法未能充分利用集中式训练或缺乏理论保证，MAGPO旨在解决这些问题。

Method: MAGPO采用自回归联合策略进行可扩展的协调探索，并明确与分散策略对齐以确保可部署性。

Result: 在43个任务和6个环境中，MAGPO表现优于CTDE基线，甚至媲美完全集中式方法。

Conclusion: MAGPO为分散式多智能体学习提供了理论支持且实用的解决方案。

Abstract: Due to practical constraints such as partial observability and limited
communication, Centralized Training with Decentralized Execution (CTDE) has
become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning
(MARL). However, existing CTDE methods often underutilize centralized training
or lack theoretical guarantees. We propose Multi-Agent Guided Policy
Optimization (MAGPO), a novel framework that better leverages centralized
training by integrating centralized guidance with decentralized execution.
MAGPO uses an auto-regressive joint policy for scalable, coordinated
exploration and explicitly aligns it with decentralized policies to ensure
deployability under partial observability. We provide theoretical guarantees of
monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across
6 diverse environments. Results show that MAGPO consistently outperforms strong
CTDE baselines and matches or surpasses fully centralized approaches, offering
a principled and practical solution for decentralized multi-agent learning. Our
code and experimental data can be found in https://github.com/liyheng/MAGPO.

</details>


### [14] [AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/abs/2507.18074)
*Yixiu Liu,Yang Nan,Weixian Xu,Xiangkun Hu,Lyumanshan Ye,Zhen Qin,Pengfei Liu*

Main category: cs.AI

TL;DR: ASI-Arch是首个用于AI研究的超级智能系统，通过自主进行神经架构创新，突破了人类认知限制。


<details>
  <summary>Details</summary>
Motivation: AI研究受限于人类认知能力，ASI-Arch旨在通过自主创新解决这一瓶颈。

Method: ASI-Arch通过自主假设、实现、训练和验证新型架构，超越传统NAS。

Result: 系统在20,000 GPU小时内完成1,773次实验，发现106种创新架构，性能超越人工设计。

Conclusion: ASI-Arch展示了AI自主研究的潜力，为自加速AI系统提供了蓝图。

Abstract: While AI systems demonstrate exponentially improving capabilities, the pace
of AI research itself remains linearly bounded by human cognitive capacity,
creating an increasingly severe development bottleneck. We present ASI-Arch,
the first demonstration of Artificial Superintelligence for AI research
(ASI4AI) in the critical domain of neural architecture discovery--a fully
autonomous system that shatters this fundamental constraint by enabling AI to
conduct its own architectural innovation. Moving beyond traditional Neural
Architecture Search (NAS), which is fundamentally limited to exploring
human-defined spaces, we introduce a paradigm shift from automated optimization
to automated innovation. ASI-Arch can conduct end-to-end scientific research in
the domain of architecture discovery, autonomously hypothesizing novel
architectural concepts, implementing them as executable code, training and
empirically validating their performance through rigorous experimentation and
past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000
GPU hours, culminating in the discovery of 106 innovative, state-of-the-art
(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed
unexpected strategic insights invisible to human players, our AI-discovered
architectures demonstrate emergent design principles that systematically
surpass human-designed baselines and illuminate previously unknown pathways for
architectural innovation. Crucially, we establish the first empirical scaling
law for scientific discovery itself--demonstrating that architectural
breakthroughs can be scaled computationally, transforming research progress
from a human-limited to a computation-scalable process. We provide
comprehensive analysis of the emergent design patterns and autonomous research
capabilities that enabled these breakthroughs, establishing a blueprint for
self-accelerating AI systems.

</details>


### [15] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: 提出了一种自动化临床数据管道的Agentic AI框架，通过模块化代理处理从数据摄入到推理的全流程，减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 医疗领域机器学习解决方案的构建和部署成本高且劳动密集，主要由于预处理流程碎片化、模型兼容性问题及严格的数据隐私限制。

Method: 采用模块化、任务特定的代理系统，处理结构化和非结构化数据，自动完成特征选择、模型选择和预处理推荐。

Result: 在老年医学、姑息治疗和结肠镜成像的公开数据集上验证了框架的有效性，实现了从数据识别到模型推理的全自动化。

Conclusion: 该框架通过自动化机器学习生命周期中的高摩擦阶段，为临床环境中的AI操作提供了可扩展且经济高效的路径。

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [16] [Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes](https://arxiv.org/abs/2507.18123)
*Sedigh Khademi,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila,Jim Black*

Main category: cs.AI

TL;DR: 该研究利用自然语言处理（NLP）和主动学习技术，开发了一种从急诊科分诊笔记中快速检测疫苗安全问题的分类器，以弥补临床试验中安全数据收集的不足。


<details>
  <summary>Details</summary>
Motivation: 由于临床试验中安全数据收集时间有限，且疫苗广泛使用后需要快速监测安全问题，因此需要开发高效的监测系统。

Method: 结合自然语言处理、主动学习和数据增强技术，开发分类器以分析急诊科分诊笔记。

Result: 该方法能够更准确地识别疫苗安全问题，减少误报，并优化标注数据的质量。

Conclusion: 该研究为疫苗安全监测提供了一种高效、准确的解决方案，尤其适用于紧急情况下的快速响应。

Abstract: The rapid development of COVID-19 vaccines has showcased the global
communitys ability to combat infectious diseases. However, the need for
post-licensure surveillance systems has grown due to the limited window for
safety data collection in clinical trials and early widespread implementation.
This study aims to employ Natural Language Processing techniques and Active
Learning to rapidly develop a classifier that detects potential vaccine safety
issues from emergency department notes. ED triage notes, containing expert,
succinct vital patient information at the point of entry to health systems, can
significantly contribute to timely vaccine safety signal surveillance. While
keyword-based classification can be effective, it may yield false positives and
demand extensive keyword modifications. This is exacerbated by the infrequency
of vaccination-related ED presentations and their similarity to other reasons
for ED visits. NLP offers a more accurate and efficient alternative, albeit
requiring annotated data, which is often scarce in the medical field. Active
learning optimizes the annotation process and the quality of annotated data,
which can result in faster model implementation and improved model performance.
This work combines active learning, data augmentation, and active learning and
evaluation techniques to create a classifier that is used to enhance vaccine
safety surveillance from ED triage notes.

</details>


### [17] [Logical Characterizations of GNNs with Mean Aggregation](https://arxiv.org/abs/2507.18145)
*Moritz Schönherr,Carsten Lutz*

Main category: cs.AI

TL;DR: 研究了使用均值聚合函数的图神经网络（GNNs）的表达能力，发现其在非均匀设置下与比率模态逻辑等价，表达能力介于最大聚合和求和聚合之间；在均匀设置下，其表达能力低于求和和最大聚合。


<details>
  <summary>Details</summary>
Motivation: 探讨均值聚合GNNs的表达能力，明确其在不同设置下的逻辑等价性和相对表达能力。

Method: 通过非均匀和均匀设置下的理论分析，比较均值聚合GNNs与最大聚合和求和聚合GNNs的表达能力。

Result: 非均匀设置下均值GNNs与比率模态逻辑等价；均匀设置下其表达能力低于求和和最大聚合GNNs。

Conclusion: 均值聚合GNNs的表达能力受设置和假设条件影响，灵活性较高但相对较弱。

Abstract: We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.

</details>


### [18] [Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory](https://arxiv.org/abs/2507.18178)
*Mutian Yang,Jiandong Gao,Ji Wu*

Main category: cs.AI

TL;DR: 该论文提出了一种认知归因框架，将大语言模型（LLMs）的认知分解为知识检索和推理调整两个阶段，通过快速和慢速思维模式分析知识推理的贡献，并揭示了参数扩展和网络层次对知识和推理的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于区分LLMs中知识和推理的贡献，以提升模型的分析、可解释性和开发能力。

Method: 方法包括将LLMs的认知分解为知识检索（Phase 1）和推理调整（Phase 2），并通过快速和慢速思维模式量化两者的贡献。

Result: 结果显示推理调整具有领域特异性，参数扩展显著提升知识能力，且知识主要分布在网络低层，推理则在高层。

Conclusion: 该框架为理解LLMs提供了新视角，并对扩展规律、知识编辑和小模型推理的局限性提供了新见解。

Abstract: While large language models (LLMs) leverage both knowledge and reasoning
during inference, the capacity to distinguish between them plays a pivotal role
in model analysis, interpretability, and development. Inspired by dual-system
cognitive theory, we propose a cognition attribution framework to decouple the
contribution of knowledge and reasoning. In particular, the cognition of LLMs
is decomposed into two distinct yet complementary phases: knowledge retrieval
(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs
are prompted to generate answers under two different cognitive modes, fast
thinking and slow thinking, respectively. The performance under different
cognitive modes is analyzed to quantify the contribution of knowledge and
reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results
reveal: (1) reasoning adjustment is domain-specific, benefiting
reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and
potentially imparing knowledge-intensive domains. (2) Parameter scaling
improves both knowledge and reasoning, with knowledge improvements being more
pronounced. Additionally, parameter scaling make LLMs reasoning significantly
more prudent, while moderately more intelligent. (3) Knowledge primarily
resides in lower network layers, while reasoning operates in higher layers. Our
framework not only helps understand LLMs from a "decoupling" perspective, but
also provides new insights into existing research, including scaling laws,
hierarchical knowledge editing, and limitations of small-model reasoning.

</details>


### [19] [Comparing Non-minimal Semantics for Disjunction in Answer Set Programming](https://arxiv.org/abs/2507.18198)
*Felicidad Aguado,Pedro Cabalar,Brais Muñiz,Gilberto Pérez,Concepción Vidal*

Main category: cs.AI

TL;DR: 本文比较了四种不遵循模型最小化原则的析取语义，其中两种直接提供非最小语义，另两种引入新析取连接词。研究发现其中三种方法（Forks、Justified Models和DI语义的合理松弛）实际上一致，构成一种共同方法，且比第四种方法（Strongly Supported Models）更强。


<details>
  <summary>Details</summary>
Motivation: 探讨析取在Answer Set Programming中的不同语义，尤其是那些不遵循模型最小化原则的语义，以理解其异同和应用潜力。

Method: 比较四种析取语义：Justified Models、Strongly Supported Models、Forks和Determining Inference (DI)语义，分析其定义和关系。

Result: 证明Forks、Justified Models和DI语义的合理松弛实际上一致，且比Strongly Supported Models更强，同时包含程序的稳定模型。

Conclusion: 三种方法（Forks、Justified Models和DI语义的合理松弛）构成一种共同语义，比Strongly Supported Models更强大，为析取语义提供了新的理解。

Abstract: In this paper, we compare four different semantics for disjunction in Answer
Set Programming that, unlike stable models, do not adhere to the principle of
model minimality. Two of these approaches, Cabalar and Mu\~niz' \emph{Justified
Models} and Doherty and Szalas' \emph{Strongly Supported Models}, directly
provide an alternative non-minimal semantics for disjunction. The other two,
Aguado et al's \emph{Forks} and Shen and Eiter's \emph{Determining Inference}
(DI) semantics, actually introduce a new disjunction connective, but are
compared here as if they constituted new semantics for the standard disjunction
operator. We are able to prove that three of these approaches (Forks, Justified
Models and a reasonable relaxation of the DI semantics) actually coincide,
constituting a common single approach under different definitions. Moreover,
this common semantics always provides a superset of the stable models of a
program (in fact, modulo any context) and is strictly stronger than the fourth
approach (Strongly Supported Models), that actually treats disjunctions as in
classical logic.

</details>


### [20] [Foundations for Risk Assessment of AI in Protecting Fundamental Rights](https://arxiv.org/abs/2507.18290)
*Antonino Rotolo,Beatrice Ferrigno,Jose Miguel Angel Garcia Godinez,Claudio Novelli,Giovanni Sartor*

Main category: cs.AI

TL;DR: 该论文提出了一个定性风险评估AI的概念框架，结合定义性平衡和可废止推理，以解决法律合规和基本权利保护的复杂性。


<details>
  <summary>Details</summary>
Motivation: 在欧盟AI法案背景下，解决AI风险评估中的法律冲突和动态决策问题。

Method: 采用定义性平衡（比例分析）和可废止推理，分析AI部署场景及其对基本权利的多层次影响。

Result: 提供了AI风险分析的哲学基础，并提出了适用于高风险AI和通用AI系统的评估模型。

Conclusion: 未来工作将开发形式化模型和算法，以支持负责任的AI治理。

Abstract: This chapter introduces a conceptual framework for qualitative risk
assessment of AI, particularly in the context of the EU AI Act. The framework
addresses the complexities of legal compliance and fundamental rights
protection by itegrating definitional balancing and defeasible reasoning.
Definitional balancing employs proportionality analysis to resolve conflicts
between competing rights, while defeasible reasoning accommodates the dynamic
nature of legal decision-making. Our approach stresses the need for an analysis
of AI deployment scenarios and for identifying potential legal violations and
multi-layered impacts on fundamental rights. On the basis of this analysis, we
provide philosophical foundations for a logical account of AI risk analysis. In
particular, we consider the basic building blocks for conceptually grasping the
interaction between AI deployment scenarios and fundamental rights,
incorporating in defeasible reasoning definitional balancing and arguments
about the contextual promotion or demotion of rights. This layered approach
allows for more operative models of assessment of both high-risk AI systems and
General Purpose AI (GPAI) systems, emphasizing the broader applicability of the
latter. Future work aims to develop a formal model and effective algorithms to
enhance AI risk assessment, bridging theoretical insights with practical
applications to support responsible AI governance.

</details>


### [21] [The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams](https://arxiv.org/abs/2507.18337)
*Peter Baumgartner,Lachlan McGinness*

Main category: cs.AI

TL;DR: 提出了一种结合计算机代数系统、SMT求解器和项重写系统的自动评分方法，用于评估物理考试中学生答案的正确性。


<details>
  <summary>Details</summary>
Motivation: 解决评估学生物理考试答案正确性的挑战性问题。

Method: 结合计算机代数系统、SMT求解器和项重写系统，利用大型语言模型预处理学生答案，再通过自动推理技术评估正确性。

Result: 在2023年澳大利亚物理奥林匹克竞赛的1500多份真实学生答案上进行了评估。

Conclusion: 开发了有效的自动评分系统，并详细描述了项重写系统的设计与性质。

Abstract: We present our method for automatically marking Physics exams. The marking
problem consists in assessing typed student answers for correctness with
respect to a ground truth solution. This is a challenging problem that we seek
to tackle using a combination of a computer algebra system, an SMT solver and a
term rewriting system. A Large Language Model is used to interpret and remove
errors from student responses and rewrite these in a machine readable format.
Once formalized and language-aligned, the next step then consists in applying
automated reasoning techniques for assessing student solution correctness. We
consider two methods of automated theorem proving: off-the-shelf SMT solving
and term rewriting systems tailored for physics problems involving
trigonometric expressions. The development of the term rewrite system and
establishing termination and confluence properties was not trivial, and we
describe it in some detail in the paper. We evaluate our system on a rich pool
of over 1500 real-world student exam responses from the 2023 Australian Physics
Olympiad.

</details>


### [22] [Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios](https://arxiv.org/abs/2507.18368)
*Zhuang Qiang Bok,Watson Wei Khong Chua*

Main category: cs.AI

TL;DR: ConDiFi是一个评估LLMs在金融任务中发散和收敛思维的基准，包含607个发散性问题和990个收敛性问题。测试14个模型后发现，GPT-4o在新颖性和可操作性上表现不佳，而DeepSeek-R1和Cohere Command R+表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准过于关注事实准确性或逐步逻辑，而金融领域需要创造性思维和不确定性下的决策能力。

Method: ConDiFi包含607个宏观金融发散性问题和990个多跳对抗性多选题，用于评估LLMs的发散和收敛思维。

Result: 测试14个模型后，GPT-4o在新颖性和可操作性上表现不佳，而DeepSeek-R1和Cohere Command R+表现优异。

Conclusion: ConDiFi为评估LLMs在金融领域的安全和战略部署提供了新视角。

Abstract: Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step
logic. In finance, however, professionals must not only converge on optimal
decisions but also generate creative, plausible futures under uncertainty. We
introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent
thinking in LLMs for financial tasks.
  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990
multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we
evaluated 14 leading models and uncovered striking differences. Despite high
fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models
like DeepSeek-R1 and Cohere Command R+ rank among the top for generating
actionable, insights suitable for investment decisions. ConDiFi provides a new
perspective to assess reasoning capabilities essential to safe and strategic
deployment of LLMs in finance.

</details>


### [23] [Revisiting LLM Reasoning via Information Bottleneck](https://arxiv.org/abs/2507.18391)
*Shiye Lei,Zhihao Cheng,Kai Jia,Dacheng Tao*

Main category: cs.AI

TL;DR: 论文提出了一种基于信息瓶颈（IB）原则的理论框架IBRO，用于优化大语言模型（LLM）的推理能力，并通过轻量级的IB正则化方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLM推理方法缺乏理论支持，限制了方法的发展。

Method: 提出IBRO框架，利用信息瓶颈原则优化推理轨迹，并设计了一种轻量级的IB正则化方法。

Result: 在多个数学推理基准和强化学习算法中验证了IB正则化的有效性，性能一致提升。

Conclusion: IBRO框架为LLM推理提供了理论支持，且IB正则化方法简单高效，易于集成到现有系统中。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in reasoning capabilities through reinforcement learning with verifiable
rewards (RLVR). By leveraging simple rule-based rewards, RL effectively
incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning
trajectories, progressively guiding them toward correct answers. However,
existing approaches remain largely heuristic and intuition-driven, limiting the
development of principled methodologies. In this paper, we present a
theoretical characterization of LLM reasoning grounded in information
bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),
a framework that encourages reasoning trajectories to be both informative about
the final correct answer and generalizable across diverse prompts. We derive a
practical token-level surrogate objective and propose an efficient
approximation, resulting in the lightweight IB regularization method. This
technique integrates seamlessly into existing RL-based post-training frameworks
without additional computational overhead, requiring only a one-line code
modification. Empirically, we validate IB regularization across multiple
mathematical reasoning benchmarks and RL algorithms, demonstrating consistent
improvements in LLM reasoning performance.

</details>


### [24] [Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation](https://arxiv.org/abs/2507.18398)
*Kwong Ho Li,Wathsala Karunarathne*

Main category: cs.AI

TL;DR: 论文研究了强化学习在呼叫中心路由优化中的应用，比较了基于模型和无模型的两种方法，发现PPO方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 优化呼叫中心的路由策略，以减少客户等待时间和员工空闲时间。

Method: 比较了基于模型的值迭代（VI）和无模型的近端策略优化（PPO），并使用仿真模型进行评估。

Result: 经过1000次测试，PPO方法在奖励、客户等待时间和员工空闲时间上表现最优。

Conclusion: PPO方法在呼叫中心路由优化中效果最佳，尽管训练时间较长。

Abstract: This paper investigates the application of Reinforcement Learning (RL) to
optimise call routing in call centres to minimise client waiting time and staff
idle time. Two methods are compared: a model-based approach using Value
Iteration (VI) under known system dynamics, and a model-free approach using
Proximal Policy Optimisation (PPO) that learns from experience. For the
model-based approach, a theoretical model is used, while a simulation model
combining Discrete Event Simulation (DES) with the OpenAI Gym environment is
developed for model-free learning. Both models frame the problem as a Markov
Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with
Poisson client arrivals and exponentially distributed service and abandonment
times. For policy evaluation, random, VI, and PPO policies are evaluated using
the simulation model. After 1,000 test episodes, PPO consistently achives the
highest rewards, along with the lowest client waiting time and staff idle time,
despite requiring longer training time.

</details>


### [25] [GPU Accelerated Compact-Table Propagation](https://arxiv.org/abs/2507.18413)
*Enrico Santi,Fabio Tardivo,Agostino Dovier,Andrea Formisano*

Main category: cs.AI

TL;DR: 本文探讨了如何利用现代GPU的强大计算能力来增强Compact-Table（CT）算法，以处理大规模表约束问题。


<details>
  <summary>Details</summary>
Motivation: 传统CPU在处理包含数百或数千个有效案例的大规模表约束问题时效率不足，需要更高效的解决方案。

Method: 通过设计和实现GPU加速的CT算法，并将其集成到现有约束求解器中。

Result: 实验验证表明，GPU加速的CT算法在处理大规模表约束问题时表现优异。

Conclusion: GPU加速的CT算法为处理大规模表约束问题提供了高效且可行的解决方案。

Abstract: Constraint Programming developed within Logic Programming in the Eighties;
nowadays all Prolog systems encompass modules capable of handling constraint
programming on finite domains demanding their solution to a constraint solver.
This work focuses on a specific form of constraint, the so-called table
constraint, used to specify conditions on the values of variables as an
enumeration of alternative options. Since every condition on a set of finite
domain variables can be ultimately expressed as a finite set of cases, Table
can, in principle, simulate any other constraint. These characteristics make
Table one of the most studied constraints ever, leading to a series of
increasingly efficient propagation algorithms. Despite this, it is not uncommon
to encounter real-world problems with hundreds or thousands of valid cases that
are simply too many to be handled effectively with standard CPU-based
approaches. In this paper, we deal with the Compact-Table (CT) algorithm, the
state-of-the-art propagation algorithms for Table. We describe how CT can be
enhanced by exploiting the massive computational power offered by modern GPUs
to handle large Table constraints. In particular, we report on the design and
implementation of GPU-accelerated CT, on its integration into an existing
constraint solver, and on an experimental validation performed on a significant
set of instances.

</details>


### [26] [On the Performance of Concept Probing: The Influence of the Data (Extended Version)](https://arxiv.org/abs/2507.18550)
*Manuel de Sousa Ribeiro,Afonso Leote,João Leite*

Main category: cs.AI

TL;DR: 本文探讨了概念探测中训练数据对性能的影响，并提供了两个常用数据集的标签。


<details>
  <summary>Details</summary>
Motivation: 概念探测帮助解释神经网络，但研究多关注模型本身，忽视了训练数据的作用。

Method: 在图像分类任务中，研究训练数据对概念探测模型性能的影响。

Result: 分析了数据对性能的影响，并公开了两个数据集的标签。

Conclusion: 训练数据对概念探测模型性能至关重要，公开标签有助于进一步研究。

Abstract: Concept probing has recently garnered increasing interest as a way to help
interpret artificial neural networks, dealing both with their typically large
size and their subsymbolic nature, which ultimately renders them unfeasible for
direct human interpretation. Concept probing works by training additional
classifiers to map the internal representations of a model into human-defined
concepts of interest, thus allowing humans to peek inside artificial neural
networks. Research on concept probing has mainly focused on the model being
probed or the probing model itself, paying limited attention to the data
required to train such probing models. In this paper, we address this gap.
Focusing on concept probing in the context of image classification tasks, we
investigate the effect of the data used to train probing models on their
performance. We also make available concept labels for two widely used
datasets.

</details>


### [27] [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
*Shanghai AI Lab,:,Yicheng Bao,Guanxu Chen,Mingkang Chen,Yunhao Chen,Chiyu Chen,Lingjie Chen,Sirui Chen,Xinquan Chen,Jie Cheng,Yu Cheng,Dengke Deng,Yizhuo Ding,Dan Ding,Xiaoshan Ding,Yi Ding,Zhichen Dong,Lingxiao Du,Yuyu Fan,Xinshun Feng,Yanwei Fu,Yuxuan Gao,Ruijun Ge,Tianle Gu,Lujun Gui,Jiaxuan Guo,Qianxi He,Yuenan Hou,Xuhao Hu,Hong Huang,Kaichen Huang,Shiyang Huang,Yuxian Jiang,Shanzhe Lei,Jie Li,Lijun Li,Hao Li,Juncheng Li,Xiangtian Li,Yafu Li,Lingyu Li,Xueyan Li,Haotian Liang,Dongrui Liu,Qihua Liu,Zhixuan Liu,Bangwei Liu,Huacan Liu,Yuexiao Liu,Zongkai Liu,Chaochao Lu,Yudong Lu,Xiaoya Lu,Zhenghao Lu,Qitan Lv,Caoyuan Ma,Jiachen Ma,Xiaoya Ma,Zhongtian Ma,Lingyu Meng,Ziqi Miao,Yazhe Niu,Yuezhang Peng,Yuan Pu,Han Qi,Chen Qian,Xingge Qiao,Jingjing Qu,Jiashu Qu,Wanying Qu,Wenwen Qu,Xiaoye Qu,Qihan Ren,Qingnan Ren,Qingyu Ren,Jing Shao,Wenqi Shao,Shuai Shao,Dongxing Shi,Xin Song,Xinhao Song,Yan Teng,Xuan Tong,Yingchun Wang,Xuhong Wang,Shujie Wang,Xin Wang,Yige Wang,Yixu Wang,Yuanfu Wang,Futing Wang,Ruofan Wang,Wenjie Wang,Yajie Wang,Muhao Wei,Xiaoyu Wen,Fenghua Weng,Yuqi Wu,Yingtong Xiong,Xingcheng Xu,Chao Yang,Yue Yang,Yang Yao,Yulei Ye,Zhenyun Yin,Yi Yu,Bo Zhang,Qiaosheng Zhang,Jinxuan Zhang,Yexin Zhang,Yinqiang Zheng,Hefeng Zhou,Zhanhui Zhou,Pengyu Zhu,Qingzi Zhu,Yubo Zhu,Bowen Zhou*

Main category: cs.AI

TL;DR: SafeWork-R1是一种多模态推理模型，通过SafeLadder框架实现能力与安全性的协同进化，显著提升安全性表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有对齐方法（如RLHF）仅学习人类偏好而缺乏内在安全推理能力的问题。

Method: 采用SafeLadder框架，结合大规模渐进式安全导向强化学习后训练和多原则验证器。

Result: 在安全相关基准上平均提升46.54%，且不损害通用能力，优于GPT-4.1和Claude Opus 4。

Conclusion: SafeLadder框架证明安全与能力可协同进化，适用于构建稳健、可靠的通用AI。

Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [28] [Action-List Reinforcement Learning Syndrome Decoding for Binary Linear Block Codes](https://arxiv.org/abs/2507.17893)
*Milad Taghipour,Bane Vasic*

Main category: cs.IT

TL;DR: 本文提出了一种基于强化学习的线性分组码解码方法，通过优化翻转比特和决策过程提升性能，并设计了状态缩减和反馈增强策略。


<details>
  <summary>Details</summary>
Motivation: 提升线性分组码解码性能，同时降低强化学习模块的复杂度。

Method: 将迭代解码过程映射为马尔可夫决策过程（MDP），提出状态缩减方法（如截断MDP）和基于动作列表的解码方案，并利用Deep-Q网络和码的自同构群优化性能。

Result: 实验结果表明，所提方法在低密度奇偶校验码（LDPC）上显著提升了性能。

Conclusion: 强化学习方法可有效优化解码性能，同时通过状态缩减和反馈机制降低复杂度。

Abstract: This paper explores the application of reinforcement learning techniques to
enhance the performance of decoding of linear block codes based on flipping
bits and finding optimal decisions. We describe the methodology for mapping the
iterative decoding process into Markov Decision Processes (MDPs) and propose
different methods to reduce the number of states in the MDP. A truncated MDP is
proposed to reduce the number of states in the MDP by learning a Hamming ball
with a specified radius around codewords. We then propose a general scheme for
reinforcement learning based decoders applicable to any class of codes to
improve the performance of decoders. We call this scheme an action-list
decoding. We design an action-list decoder based on the Deep-Q network values
that substantially enhance performance. We also get benefit of automorphism
group of code to further improve the code performance. Additionally, we propose
a feedback-based method to exploit and enhance the performance of existing
high-performing decoders by applying reinforcement learning algorithms after
the existing decoders. These approaches effectively reduces the complexity of
the reinforcement learning block. Finally, we present experimental results for
the Low-Density Parity Check (LDPC) codes over the Binary Symmetric Channel
(BSC) to demonstrate the efficiency of the proposed methods.

</details>


### [29] [Minimax Data Sanitization with Distortion Constraint and Adversarial Inference](https://arxiv.org/abs/2507.17942)
*Amirarsalan Moatazedian,Yauhen Yakimenka,Rémi A. Chou,Jörg Kliewer*

Main category: cs.IT

TL;DR: 研究隐私保护数据共享，通过优化私有化器在满足重构器失真阈值的同时最大化对手的最小损失。


<details>
  <summary>Details</summary>
Motivation: 解决在多方信息共享中如何保护隐私，同时允许授权方准确重构数据的问题。

Method: 提出一种基于数据驱动的极小极大优化框架，交替更新私有化器、重构器和对手。

Result: 在高斯和二进制场景下获得理论最优解，并作为基准评估提出的优化方法。

Conclusion: 通过合作实现准确重构，同时最大化对手损失，类似于秘密共享但支持有损恢复。

Abstract: We study a privacy-preserving data-sharing setting where a privatizer
transforms private data into a sanitized version observed by an authorized
reconstructor and two unauthorized adversaries, each with access to side
information correlated with the private data.
  The reconstructor is evaluated under a distortion function, while each
adversary is evaluated using a separate loss function. The privatizer ensures
the reconstructor distortion remains below a fixed threshold while maximizing
the minimum loss across the two adversaries. This two-adversary setting models
cases where individual users cannot reconstruct the data accurately, but their
combined side information enables estimation within the distortion threshold.
The privatizer maximizes individual loss while permitting accurate
reconstruction only through collaboration. This echoes secret-sharing
principles, but with lossy rather than perfect recovery. We frame this as a
constrained data-driven minimax optimization problem and propose a data-driven
training procedure that alternately updates the privatizer, reconstructor, and
adversaries. We also analyze the Gaussian and binary cases as special scenarios
where optimal solutions can be obtained. These theoretical optimal results are
benchmarks for evaluating the proposed minimax training approach.

</details>


### [30] [Deep Learning-based Position-domain Channel Extrapolation for Cell-Free Massive MIMO](https://arxiv.org/abs/2507.17950)
*Jiajia Guo,Chao-Kai Wen,Xiao Li,Shi Jin*

Main category: cs.IT

TL;DR: 提出了一种基于深度学习的PCEnet框架，利用用户位置信息提升无蜂窝大规模MIMO系统的信道获取效率，减少开销。


<details>
  <summary>Details</summary>
Motivation: 减少信道获取的开销，利用用户位置信息作为不同信道间的桥梁，提升效率。

Method: 通过神经网络从获取的信道推断用户位置，利用位置信息设计导频符号并重构其他信道，提出简化策略和无位置标签方法。

Result: 仿真结果显示，PCEnet框架可将导频和反馈开销减少高达50%。

Conclusion: PCEnet框架显著提升了信道获取性能，同时降低了开销和延迟。

Abstract: To reduce channel acquisition overhead, spatial, time, and frequency-domain
channel extrapolation techniques have been widely studied. In this paper, we
propose a novel deep learning-based Position-domain Channel Extrapolation
framework (named PCEnet) for cell-free massive multiple-input multiple-output
(MIMO) systems. The user's position, which contains significant channel
characteristic information, can greatly enhance the efficiency of channel
acquisition. In cell-free massive MIMO, while the propagation environments
between different base stations and a specific user vary and their respective
channels are uncorrelated, the user's position remains constant and unique
across all channels. Building on this, the proposed PCEnet framework leverages
the position as a bridge between channels to establish a mapping between the
characteristics of different channels, thereby using one acquired channel to
assist in the estimation and feedback of others. Specifically, this approach
first utilizes neural networks (NNs) to infer the user's position from the
obtained channel. {The estimated position, shared among BSs through a central
processing unit (CPU)}, is then fed into an NN to design pilot symbols and
concatenated with the feedback information to the channel reconstruction NN to
reconstruct other channels, thereby significantly enhancing channel acquisition
performance. Additionally, we propose a simplified strategy where only the
estimated position is used in the reconstruction process without modifying the
pilot design, thereby reducing latency. Furthermore, we introduce a position
label-free approach that infers the relative user position instead of the
absolute position, eliminating the need for ground truth position labels during
the localization NN training. Simulation results demonstrate that the proposed
PCEnet framework reduces pilot and feedback overheads by up to 50%.

</details>


### [31] [A Novel Coded Computing Approach for Distributed Multi-Task Learning](https://arxiv.org/abs/2507.18025)
*Minquan Cheng,Yongkang Wang,Lingyu Zhang,Youlong Wu*

Main category: cs.IT

TL;DR: 提出了一种新的编码分布式多任务学习（DMTL）方案，通过矩阵分解和编码技术显著降低通信成本，适用于异构数据环境。


<details>
  <summary>Details</summary>
Motivation: 在大规模分布式多任务学习中，通信瓶颈限制了系统性能，尤其是在异构数据放置场景下。

Method: 将通信过程建模为矩阵分解问题，利用上行链路编码矩阵的结构特性和下行链路编码矩阵的MDS属性，提出了一种编码DMTL方案。

Result: 理论分析表明，该方案在温和条件下达到了通信开销的理论下限，适用于传统同构和异构计算环境。

Conclusion: 该方案不仅优化了DMTL的通信效率，还可扩展至其他分布式线性可分计算问题，为解决异构数据放置挑战提供了新思路。

Abstract: Distributed multi-task learning (DMTL) effectively improves model
generalization performance through the collaborative training of multiple
related models. However, in large-scale learning scenarios, communication
bottlenecks severely limit practical system performance. In this paper, we
investigate the communication bottleneck within a typical DMTL system that
employs non-linear global updates. This system involves distributed workers,
assisted by a central server, who collaboratively learn distinct models derived
from a non-linear aggregation of their local model parameters. We first
characterize the communication process as a matrix decomposition problem. It
transforms workers' data storage constraints into structural characteristics of
the uplink encoding matrix, and worker data retrieval demands into Maximum
Distance Separable (MDS) properties of the downlink encoding matrix. Building
on this, we propose a novel coded DTML scheme that can greatly reduce the
communication cost of the DTML with heterogeneous data placement. Theoretical
analysis demonstrates that the proposed scheme achieves the theoretical lower
bound for communication overhead under mild conditions. Remarkably, this
optimality holds for both traditional homogeneous computing environments and
various heterogeneous scenarios. Furthermore, our scheme is extensible to a
distributed linearly separable computation problem where the target function
involves multiple linear combinations of local update values. This indicates
that our scheme offers a new way of tackling heterogeneous data placement
challenges in various distributed applications.

</details>


### [32] [On the Role of Age and Semantics of Information in Remote Estimation of Markov Sources](https://arxiv.org/abs/2507.18514)
*Jiping Luo,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 研究有限状态马尔可夫链的语义感知远程估计，提出基于MAP估计器和AoCE/AoI指标的传输策略优化方法，并开发高效算法Insec-SPI。


<details>
  <summary>Details</summary>
Motivation: 优化远程估计性能，同时满足传输频率约束，结合AoCE和AoI提升估计质量。

Method: 采用MAP估计器，基于AoCE和AoI指标，将问题建模为无界成本的CMDP，提出混合策略和阈值策略。

Result: 证明混合策略的最优性，开发高效算法Insec-SPI，结合AoCE和AoI显著提升估计质量。

Conclusion: 结合AoCE和AoI的传输策略优于单一指标，Insec-SPI算法高效且性能优越。

Abstract: This paper investigates the semantics-aware remote estimation of a
finite-state Markov chain. We employ the maximum a posteriori (MAP) estimator
and aim to devise a transmission policy to optimize estimation performance
subject to a transmission frequency constraint. We leverage two metrics, namely
the Age of Consecutive Error (AoCE) and the Age of Information (AoI), to
quantify, respectively, the significance of estimation error at the transmitter
and the predictability of outdated information at the receiver. The optimal
transmission problem is formulated as a constrained Markov decision process
(CMDP) with unbounded costs. We show the existence of an optimal simple mixture
policy, which randomly selects between two deterministic switching policies
with a fixed probability. Notably, each switching policy triggers a
transmission only when the AoCE exceeds a threshold value that depends on both
the AoI and the instantaneous estimation error. We further derive sufficient
conditions under which the switching policy reduces to a simple threshold
policy; that is, it admits identical thresholds for all estimation errors.
Leveraging these results, we develop an efficient structure-aware algorithm,
Insec-SPI, that computes the optimal policy with reduced computation overhead.
Our results demonstrate that incorporating both AoI and AoCE yields
significantly improved estimation quality compared to using either metric
alone.

</details>


### [33] [Covert Communications in MEC-Based Networked ISAC Systems Towards Low-Altitude Economy](https://arxiv.org/abs/2507.18194)
*Weihao Mao,Yang Lu,Bo Ai,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: 本文研究了基于移动边缘计算（MEC）的网络化ISAC系统中的隐蔽传输设计，针对低空经济（LAE）场景，优化了通信、感知和计算资源以及无人机轨迹，以最小化总能耗。


<details>
  <summary>Details</summary>
Motivation: 低空经济（LAE）依赖ISAC、MEC和隐蔽通信，但现有研究未充分解决隐蔽传输与资源优化的联合问题。

Method: 提出了一种交替优化算法，分解为资源联合优化和无人机轨迹优化两个子问题，分别采用凸近似和信任域算法求解。

Result: 仿真验证了算法的有效性，揭示了通信、感知和计算之间的权衡。

Conclusion: 该算法为LAE系统中的隐蔽传输和资源优化提供了有效解决方案。

Abstract: Low-altitude economy (LAE) is an emerging business model, which heavily
relies on integrated sensing and communications (ISAC), mobile edge computing
(MEC), and covert communications. This paper investigates the convert
transmission design in MEC-based networked ISAC systems towards LAE, where an
MEC server coordinates multiple access points to simultaneously receive
computation tasks from multiple unmanned aerial vehicles (UAVs), locate a
target in a sensing area, and maintain UAVs' covert transmission against
multiple wardens. We first derive closed-form expressions for the detection
error probability (DEP) at wardens. Then, we formulate a total energy
consumption minimization problem by optimizing communication, sensing, and
computation resources as well as UAV trajectories, subject to the requirements
on quality of MEC services, DEP, and radar signal-to-interference-and-noise
ratio, and the causality of UAV trajectories. An alternating optimization based
algorithm is proposed to handle the considered problem, which decomposes it
into two subproblems: joint optimization of communication, sensing, and
computation resources, and UAV trajectory optimization. The former is addressed
by a successive convex approximation based algorithm, while the latter is
solved via a trust-region based algorithm. Simulations validate the
effectiveness of the proposed algorithm compared with various benchmarks, and
reveal the trade-offs among communication, sensing, and computation in LAE
systems.

</details>


### [34] [Hermitian hull of some GRS codes and new EAQMDS codes](https://arxiv.org/abs/2507.18361)
*Oisin Campion,Rodrigo San-José*

Main category: cs.IT

TL;DR: 研究了广义Reed-Solomon码的Hermitian壳维度，通过晶格计数问题提供显式公式，用于确定纠缠辅助量子纠错码所需的最大纠缠对数。


<details>
  <summary>Details</summary>
Motivation: 探索广义Reed-Solomon码的Hermitian壳维度，以优化纠缠辅助量子纠错码的设计。

Method: 将壳维度计算问题转化为晶格计数问题，并求解该问题。

Result: 提供了壳维度的显式公式，确定了纠缠辅助量子纠错码所需的最大纠缠对数，并获得了新的参数。

Conclusion: 该方法灵活，可用于构造多种纠缠辅助量子MDS码，并扩展了参数范围。

Abstract: We study the Hermitian hull of a particular family of generalized
Reed-Solomon codes. The problem of computing the dimension of the hull is
translated to a counting problem in a lattice. By solving this problem, we
provide explicit formulas for the dimension of the hull, which determines the
minimum number required of maximally entangled pairs for the associated
entanglement-assisted quantum error-correcting codes. This flexible
construction allows to obtain a wide range of entanglement-assisted quantum MDS
codes, as well as new parameters.

</details>


### [35] [AI/ML Life Cycle Management for Interoperable AI Native RAN](https://arxiv.org/abs/2507.18538)
*Chu-Hsiang Huang,Chao-Kai Wen,Geoffrey Ye Li*

Main category: cs.IT

TL;DR: 本文探讨了AI/ML在5G RAN中的应用及标准化生命周期管理框架的进展，从3GPP Release 16到20的演进，并提出了未来6G的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 解决AI/ML在5G RAN中大规模应用时面临的模型漂移、供应商锁定和透明度不足等问题。

Method: 通过3GPP Release 16-20逐步标准化AI/ML功能，包括模型传输、执行、性能监控和闭环控制。

Result: 提出了五块生命周期管理架构、KPI驱动监控机制和跨供应商协作方案。

Conclusion: 为6G的AI原生收发器奠定了基础，但仍需解决资源高效监控、环境漂移检测等挑战。

Abstract: Artificial intelligence (AI) and machine learning (ML) models are rapidly
permeating the 5G Radio Access Network (RAN), powering beam management, channel
state information (CSI) feedback, positioning, and mobility prediction.
However, without a standardized life-cycle management (LCM) framework,
challenges, such as model drift, vendor lock-in, and limited transparency,
hinder large-scale adoption. 3GPP Releases 16-20 progressively evolve AI/ML
from experimental features to managed, interoperable network functions.
Beginning with the Network Data Analytics Function (NWDAF) in Rel-16,
subsequent releases introduced standardized interfaces for model transfer,
execution, performance monitoring, and closed-loop control, culminating in
Rel-20's two-sided CSI-compression Work Item and vendor-agnostic LCM profile.
This article reviews the resulting five-block LCM architecture, KPI-driven
monitoring mechanisms, and inter-vendor collaboration schemes, while
identifying open challenges in resource-efficient monitoring, environment drift
detection, intelligent decision-making, and flexible model training. These
developments lay the foundation for AI-native transceivers as a key enabler for
6G.

</details>
