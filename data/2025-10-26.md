<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 38]
- [cs.IT](#cs.IT) [Total: 11]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks](https://arxiv.org/abs/2510.19973)
*Hatim Chergui,Farhad Rezazadeh,Merouane Debbah,Christos Verikoukis*

Main category: cs.NI

TL;DR: 本文探讨了6G网络中实现真正自主性的挑战，提出通过基于大语言模型的智能体AI来感知多模态遥测数据、进行推理和跨域协商，但需要解决人类设计带来的认知偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于KPI优化的方法只能实现有限自动化，无法达到真正的网络自主性。需要让AI智能体能够像人类一样感知和理解网络环境，但必须解决认知偏见对决策质量的影响。

Method: 提出基于大语言模型的智能体AI架构，能够感知多模态遥测数据、利用记忆进行推理、跨域协商并通过API执行动作。针对认知偏见问题，开发了锚定随机化、时间衰减和拐点奖励等技术来缓解特定偏见。

Result: 在6G切片间和跨域管理的实际用例中，通过偏见缓解技术，智能体决策质量显著提升，在第二个用例中实现了5倍延迟降低和约40%的能耗节省。

Conclusion: 实现6G网络真正自主性需要超越KPI优化，采用智能体AI方法，但必须系统性地识别和缓解认知偏见，才能确保决策的质量和公平性。

Abstract: The path to higher network autonomy in 6G lies beyond the mere optimization
of key performance indicators (KPIs). While KPIs have enabled automation gains
under TM Forum Levels 1--3, they remain numerical abstractions that act only as
proxies for the real essence of communication networks: seamless connectivity,
fairness, adaptability, and resilience. True autonomy requires perceiving and
reasoning over the network environment as it is. Such progress can be achieved
through \emph{agentic AI}, where large language model (LLM)-powered agents
perceive multimodal telemetry, reason with memory, negotiate across domains,
and act via APIs to achieve multi-objective goals. However, deploying such
agents introduces the challenge of cognitive biases inherited from human
design, which can distort reasoning, negotiation, tool use, and actuation.
Between neuroscience and AI, this paper provides a tutorial on a selection of
well-known biases, including their taxonomy, definition, mathematical
formulation, emergence in telecom systems and the commonly impacted agentic
components. The tutorial also presents various mitigation strategies tailored
to each type of bias. The article finally provides two practical use-cases,
which tackle the emergence, impact and mitigation gain of some famous biases in
6G inter-slice and cross-domain management. In particular, anchor
randomization, temporal decay and inflection bonus techniques are introduced to
specifically address anchoring, temporal and confirmation biases. This avoids
that agents stick to the initial high resource allocation proposal or decisions
that are recent and/or confirming a prior hypothesis. By grounding decisions in
a richer and fairer set of past experiences, the quality and bravery of the
agentic agreements in the second use-case, for instance, are leading to $\times
5$ lower latency and around $40\%$ higher energy saving.

</details>


### [2] [Rediscovering Recurring Routing Results](https://arxiv.org/abs/2510.20297)
*Xiao Song,John Heidemann*

Main category: cs.NI

TL;DR: Fenrir是一个新的路由分析系统，能够重新发现重复出现的路由结果，检测网络路由变化，量化路由变化程度，并识别可能重新出现的路由"模式"。


<details>
  <summary>Details</summary>
Motivation: 路由对网络性能至关重要，但理解和控制路由如何影响服务具有挑战性。运营商使用BGP进行流量工程优化网络性能，但最终结果受到整个互联网BGP策略的影响，而不仅仅是本地选择。

Method: Fenrir系统通过主动测量、数据清理和加权来发现网络路由变化，即使变化发生在观察者多跳之外。提供量化路由变化程度和识别路由模式的新方法。

Result: Fenrir可应用于多种场景：根DNS服务的任播捕获、多宿主企业的路由优化、顶级Web服务的网站选择。系统能够检测和量化变化，帮助回答运营问题。

Conclusion: Fenrir的检测和量化变化方法对网络运营很有帮助，能够识别流量工程的效果、第三方路由变化，以及判断当前路由是新模式还是之前见过的模式。

Abstract: Routing is central to networking performance, including: (1) latency in
anycast services and websites served from multiple locations,(2) networking
expenses and throughput in multi-homed enterprises, (3) the ability to keep
traffic domestic when considering data sovereignty. However, understanding and
managing how routing affects these services is challenging. Operators use
Traffic Engineering (TE) with BGP to optimize network performance, but what
they get is the result of all BGP policies throughout the Internet, not just
their local choices. Our paper proposes Fenrir, a new system to rediscover
recurring routing results. Fenrir can discover changes in network routing, even
when it happens multiple hops away from the observer. Fenrir also provides new
methods to quantify the degree of routing change, and to identify routing
"modes" that may reappear. Second, we show that Fenrir can be applied to many
different problems: we use five instances of three different types of systems
to illustrate the generalization: anycast catchments showing in a root DNS
service, route optimization for two multi-homed enterprises, and website
selection for two of the top-10 web services. Each type requires different
types of active measurements, data cleaning and weighting. We demonstrate
Fenrir's methods of detecting and quantifying change are helpful because they
all face similar operational questions: How much effect did traffic engineering
have? Did a third-party change alter my routing? In either case, is the current
routing new, or is it like a routing mode I saw before?

</details>


### [3] [Multicast-partitioning in Time-triggered Stream Planning for Time-Sensitive Networks](https://arxiv.org/abs/2510.20440)
*Heiko Geppert,Frank Dürr,Simon Naß,Kurt Rothermel*

Main category: cs.NI

TL;DR: 提出了一种新颖的多播分区技术，将多播树分割成更小的多播或单播树，在带宽利用率和流量调度难度之间实现更精细的权衡，从而提高动态系统中的可调度性。


<details>
  <summary>Details</summary>
Motivation: 多播通信虽然能节省网络带宽，但会复杂化流量规划，因为需要在多播树的所有分支上都有空闲队列或可用的下游出口端口。这限制了在时间敏感网络中的实际应用。

Method: 开发了一种多播分区技术，将大型多播树分割成较小的多播或单播树，从而在带宽利用和调度复杂度之间实现更好的平衡。

Result: 在不同网络拓扑和三种调度算法下进行评估，使用分区技术后，流拒绝率减少了5-15%，网络吞吐量提高了5-125%（取决于调度算法）。

Conclusion: 多播分区技术能够有效改善时间敏感网络中的可调度性，减少流拒绝率并显著提高网络吞吐量，为动态系统提供了更好的通信性能。

Abstract: Multicast allows sending a message to multiple recipients without having to
create and send a separate message for each recipient. This preserves network
bandwidth, which is particularly important in time-sensitive networks. These
networks are commonly used to provide latency-bounded communication for
real-time systems in domains like automotive, avionics, industrial internet of
things, automated shop floors, and smart energy grids. The preserved bandwidth
can be used to admit additional real-time messages with specific quality of
service requirements or to reduce the end-to-end latencies for messages of any
type. However, using multicast communication can complicate traffic planning,
as it requires free queues or available downstream egress ports on all branches
of the multicast tree. In this work, we present a novel multicast partitioning
technique to split multicast trees into smaller multicast or unicast trees.
This allows for a more fine-grained trade-off between bandwidth utilization and
traffic scheduling difficulty. Thus, schedulability in dynamic systems can be
improved, in terms the number of admitted streams and the accumulated network
throughput. We evaluated the multicast partitioning on different network
topologies and with three different scheduling algorithms. With the
partitioning, 5-15\% fewer streams were rejected, while achieving 5-125\% more
network throughput, depending on the scheduling algorithm.

</details>


### [4] [Trust, But Verify: An Empirical Evaluation of AI-Generated Code for SDN Controllers](https://arxiv.org/abs/2510.20703)
*Felipe Avencourt Soares,Muriel F. Franco,Eder J. Scheid,Lisandro Z. Granville*

Main category: cs.NI

TL;DR: 评估四种AI工具在生成POX网络控制器代码时的表现，ChatGPT和DeepSeek表现最佳，Copilot和BlackBox.ai需要更多调整。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在可编程网络等新领域的可靠性和功能性尚不明确，需要实证评估。

Method: 使用四种AI工具生成POX控制器代码，通过零样本和少样本提示技术，在Mininet模拟网络拓扑中测试功能性和正确性。

Result: 所有模型都能生成功能性控制器，但ChatGPT和DeepSeek在一致性和代码质量方面表现更好。

Conclusion: 生成式AI工具能够生成功能性网络控制器代码，但不同工具的性能存在差异，需要根据具体需求选择合适的工具。

Abstract: Generative Artificial Intelligence (AI) tools have been used to generate
human-like content across multiple domains (e.g., sound, image, text, and
programming). However, their reliability in terms of correctness and
functionality in novel contexts such as programmable networks remains unclear.
Hence, this paper presents an empirical evaluation of the source code of a POX
controller generated by different AI tools, namely ChatGPT, Copilot, DeepSeek,
and BlackBox.ai. To evaluate such a code, three networking tasks of increasing
complexity were defined and for each task, zero-shot and few-shot prompting
techniques were input to the tools. Next, the output code was tested in
emulated network topologies with Mininet and analyzed according to
functionality, correctness, and the need for manual fixes. Results show that
all evaluated models can produce functional controllers. However, ChatGPT and
DeepSeek exhibited higher consistency and code quality, while Copilot and
BlackBox.ai required more adjustments.

</details>


### [5] [AI-Enabled Digital Twins for Next-Generation Networks: Forecasting Traffic and Resource Management in 5G/6G](https://arxiv.org/abs/2510.20796)
*John Sengendo,Fabrizio Granelli*

Main category: cs.NI

TL;DR: 提出了一种基于LSTM神经网络的AI驱动数字孪生网络框架，用于预测网络流量模式并主动管理资源分配，在5G/6G网络中实现自主、自适应的高性能网络管理。


<details>
  <summary>Details</summary>
Motivation: 传统启发式资源管理技术无法满足5G/6G网络对敏捷性、可扩展性、弹性和实时服务精度的要求，需要数字孪生作为关键使能技术来克服这些限制。

Method: 将长短期记忆(LSTM)神经网络集成到数字孪生框架中，用于预测网络流量模式并主动管理资源分配。

Result: 通过分析实验，AI驱动的数字孪生框架相比基准方法表现出优越性能。

Conclusion: 在数字孪生中嵌入AI能力为未来移动网络中完全自主、自适应和高性能的网络管理铺平了道路。

Abstract: As 5G and future 6G mobile networks become increasingly more sophisticated,
the requirements for agility, scalability, resilience, and precision in
real-time service provisioning cannot be met using traditional and
heuristic-based resource management techniques, just like any advancing
technology. With the aim of overcoming such limitations, network operators are
foreseeing Digital Twins (DTs) as key enablers, which are designed as dynamic
and virtual replicas of network infrastructure, allowing operators to model,
analyze, and optimize various operations without any risk of affecting the live
network. However, for Digital Twin Networks (DTNs) to meet the challenges faced
by operators especially in line with resource management, a driving engine is
needed. In this paper, an AI (Artificial Intelligence)-driven approach is
presented by integrating a Long Short-Term Memory (LSTM) neural network into
the DT framework, aimed at forecasting network traffic patterns and proactively
managing resource allocation. Through analytical experiments, the AI-Enabled DT
framework demonstrates superior performance benchmarked against baseline
methods. Our study concludes that embedding AI capabilities within DTs paves
the way for fully autonomous, adaptive, and high-performance network management
in future mobile networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 该研究提出了分析可靠性基准(ARB)，这是首个用于量化能源系统分析中大型语言模型推理可靠性的标准化框架，包含五个子指标，并在四种前沿模型上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前AI在能源领域的应用缺乏标准化框架来评估系统推理的正确性，现有验证方法主要关注预测准确性或计算效率，而忽略了分析结论的逻辑完整性。

Method: 开发了ARB框架，整合准确性、推理可靠性、不确定性纪律、政策一致性和透明度五个子指标，使用开放技术经济数据集(NREL ATB 2024等)，在四种前沿模型(GPT-4/5、Claude 4.5 Sonnet、Gemini 2.5 Pro、Llama 3 70B)上进行确定性、概率性和认知性场景测试。

Result: GPT-4/5和Claude 4.5 Sonnet实现了稳定且符合政策的推理(分析可靠性指数大于90)，Gemini 2.5 Pro表现中等稳定，Llama 3 70B未达到专业阈值。统计验证确认这些差异显著且可重现。

Conclusion: ARB建立了能源文献中首个量化验证AI系统中因果、概率和政策驱动推理的方法，为全球能源转型中可信赖和透明的分析应用提供了参考框架。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [7] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: 提出了一种量子启发算法，使用矩阵乘积态和离散驱动调度来解决二次无约束二进制优化问题，在Sudoku和MaxCut问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决二次无约束二进制优化问题，该问题在数学上等同于寻找伊辛自旋玻璃哈密顿量的基态，传统方法难以处理大规模问题。

Method: 使用矩阵乘积态紧凑表示自旋构型的大叠加，结合离散驱动调度引导MPS向基态演化，采用密度矩阵重整化群方法迭代最小化系统能量。

Result: 算法可靠地识别全局最小值而非近似解，成功解决了包含200多个伊辛自旋的Sudoku谜题和包含251个节点、3265条边的MaxCut问题。

Conclusion: 该量子启发方法具有可扩展性、通用性和适用于工业规模QUBO应用的优势。

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [8] [Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory](https://arxiv.org/abs/2510.19838)
*Shiqi He,Yue Cui,Xinyu Ma,Yaliang Li,Bolin Ding,Mosharaf Chowdhury*

Main category: cs.AI

TL;DR: Branch-and-Browse是一个细粒度的网页代理框架，通过树结构探索、网页状态重放和页面动作记忆，显著提升了基于大语言模型的网页代理在复杂任务中的推理深度和执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有的网页代理方法在推理深度和效率上存在局限：线性方法无法处理多步推理且缺乏有效回溯，其他搜索策略则粒度粗且计算成本高。

Method: 采用显式子任务管理和树结构探索实现可控多分支推理；通过网页状态重放和后台推理引导探索；利用页面动作记忆在会话内外共享已探索的动作。

Result: 在WebArena基准测试中，任务成功率达到35.8%，执行时间相比最先进方法减少高达40.4%。

Conclusion: Branch-and-Browse是一个可靠且高效的基于大语言模型的网页代理框架。

Abstract: Autonomous web agents powered by large language models (LLMs) show strong
potential for performing goal-oriented tasks such as information retrieval,
report generation, and online transactions. These agents mark a key step toward
practical embodied reasoning in open web environments. However, existing
approaches remain limited in reasoning depth and efficiency: vanilla linear
methods fail at multi-step reasoning and lack effective backtracking, while
other search strategies are coarse-grained and computationally costly. We
introduce Branch-and-Browse, a fine-grained web agent framework that unifies
structured reasoning-acting, contextual memory, and efficient execution. It (i)
employs explicit subtask management with tree-structured exploration for
controllable multi-branch reasoning, (ii) bootstraps exploration through
efficient web state replay with background reasoning, and (iii) leverages a
page action memory to share explored actions within and across sessions. On the
WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\%
and reduces execution time by up to 40.4\% relative to state-of-the-art
methods. These results demonstrate that Branch-and-Browse is a reliable and
efficient framework for LLM-based web agents.

</details>


### [9] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: 本文提出了DAG-MATH框架，将思维链建模为基于规则的随机过程，通过逻辑紧密度评估LLMs的推理一致性，揭示了即使PASS@k指标相似时不同模型在推理保真度上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在数学问题上使用思维链表现出色，但尚不清楚这种成功是源于搜索、机械记忆还是规则一致的推理，需要新的评估框架来深入分析推理能力。

Method: 将思维链建模为有向无环图上的基于规则随机过程，提出逻辑紧密度指标，并构建DAG-MATH思维链格式的基准测试来评估LLMs的推理能力。

Result: 在标准数学推理数据集上，分析发现即使PASS@k指标可比，代表性LLM家族在推理保真度上存在统计显著差异，揭示了最终答案准确性与规则一致推导之间的差距。

Conclusion: 该框架在自由形式思维链和形式证明系统之间提供了平衡，为LLMs推理评估提供了可行的诊断工具，有助于更深入地理解模型的推理能力。

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [10] [Surfer 2: The Next Generation of Cross-Platform Computer Use Agents](https://arxiv.org/abs/2510.19949)
*Mathieu Andreux,Märt Bakler,Yanael Barbier,Hamza Ben Chekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Nathan Bout,Matthias Brunel,Aleix Cambray,Pierre-Louis Cedoz,Antoine Chassang,Gautier Cloix,Ethan Connelly,Alexandra Constantinou,Ramzi De Coster,Hubert de la Jonquiere,Aurélien Delfosse,Maxime Delpit,Alexis Deprez,Augustin Derupti,Mathieu Diaz,Shannon D'Souza,Julie Dujardin,Abai Edmund,Michael Eickenberg,Armand Fatalot,Wissem Felissi,Isaac Herring,Xavier Koegler,Erwan Le Jumeau de Kergaradec,Aurélien Lac,Maxime Langevin,Corentin Lauverjat,Antonio Loison,Avshalom Manevich,Axel Moyal,Axel Nguyen Kerbel,Marinela Parovic,Julien Revelle,Guillaume Richard,Mats Richter,Ronan Riochet,María Santos,Romain Savidan,Laurent Sifre,Maxime Theillard,Marc Thibault,Ivan Valentini,Tony Wu,Laura Yie,Kai Yuan,Jevgenij Zubovskij*

Main category: cs.AI

TL;DR: Surfer 2是一个纯视觉观察的统一架构，在Web、桌面和移动环境中实现最先进性能，无需特定任务微调即可超越所有先前系统。


<details>
  <summary>Details</summary>
Motivation: 解决现有代理系统依赖环境特定接口、限制跨平台部署的问题，构建能够在Web、桌面和移动环境中通用的智能代理。

Method: 集成层次化上下文管理、解耦的规划与执行、以及具有自适应恢复能力的自我验证机制，支持长任务周期的可靠操作。

Result: 在WebVoyager上达到97.1%准确率，WebArena 69.6%，OSWorld 60.1%，AndroidWorld 87.1%，多尝试情况下在所有基准测试中超越人类表现。

Conclusion: 系统化编排能够放大基础模型能力，仅通过视觉交互实现通用计算机控制，同时需要下一代视觉语言模型来实现帕累托最优的成本效益。

Abstract: Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.

</details>


### [11] [RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs](https://arxiv.org/abs/2510.19954)
*Joseph Meyer,Divyansha Lachi,Reza Mohammadi,Roshan Reddy Upendra,Eva L. Dyer,Mark Li,Tom Palczewski*

Main category: cs.AI

TL;DR: RELATE是一个模式无关的图神经网络特征编码器，使用共享的模态特定编码器和交叉注意力模块，在减少5倍参数的同时达到接近模式特定编码器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN需要为每种节点类型和特征列设计单独的特征编码器，这限制了可扩展性和参数共享。

Method: 使用共享的模态特定编码器处理分类、数值、文本和时间属性，然后通过Perceiver风格的交叉注意力模块将特征聚合成固定大小的节点表示。

Result: 在RelBench基准测试中，RELATE在ReLGNN和HGT上达到模式特定编码器97%的性能，同时参数减少5倍。

Conclusion: RELATE支持不同模式，为关系图数据的通用GNN预训练铺平了道路。

Abstract: Relational multi-table data is common in domains such as e-commerce,
healthcare, and scientific research, and can be naturally represented as
heterogeneous temporal graphs with multi-modal node attributes. Existing graph
neural networks (GNNs) rely on schema-specific feature encoders, requiring
separate modules for each node type and feature column, which hinders
scalability and parameter sharing. We introduce RELATE (Relational Encoder for
Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature
encoder that can be used with any general purpose GNN. RELATE employs shared
modality-specific encoders for categorical, numerical, textual, and temporal
attributes, followed by a Perceiver-style cross-attention module that
aggregates features into a fixed-size, permutation-invariant node
representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,
where it achieves performance within 3% of schema-specific encoders while
reducing parameter counts by up to 5x. This design supports varying schemas and
enables multi-dataset pretraining for general-purpose GNNs, paving the way
toward foundation models for relational graph data.

</details>


### [12] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: 生成式AI正在加剧保险欺诈，使大规模快速伪造事故证据变得容易。保险公司采用AI检测工具应对，但面临误报、漏报等挑战。本文提出UVeye分层解决方案来检测和威慑这种新型欺诈。


<details>
  <summary>Details</summary>
Motivation: 保险欺诈每年造成数百亿美元损失，传统欺诈手段包括伪造事故和文件。生成式AI（特别是深度伪造技术）使欺诈者能轻松制造逼真的碰撞照片和虚假身份，加剧了保险欺诈问题。

Method: 保险公司部署基于AI的深度伪造检测软件和增强验证流程。本文提出UVeye分层解决方案，通过多层检测机制来应对AI驱动的欺诈。

Result: 当前检测工具存在误报和漏报问题，欺诈者不断调整策略逃避检测。保险公司面临资源和成本障碍，AI驱动的保险欺诈仍然是持续挑战。

Conclusion: 生成式AI与检测技术之间的军备竞赛仍在继续。UVeye分层解决方案代表了在检测、缓解和威慑新型欺诈能力方面的重大进步，但应对AI驱动的保险欺诈仍需要持续努力。

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [13] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 该研究使用机器学习模型通过领导力人格特质预测学业成功，在129名环境工程硕士生中实现了最高87.50%的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 探索AI技术在个性化学习中的潜力，通过领导力人格特质预测学业表现，为早期识别学生优劣势和制定个性化学习策略提供机会。

Method: 对129名硕士生进行5项领导力人格测试（23个特征），结合学业成绩，使用7种机器学习算法（SVM、LR、KNN、DT、GB、RF、XGBoost、LightGBM）进行建模，采用皮尔逊相关系数进行特征选择。

Result: 随机森林分类器表现最佳，包含17个人格特征和领导力标记特征的模型准确率达87.50%，不包含该特征的模型准确率为85.71%。

Conclusion: 该研究为在早期教育阶段识别学生优劣势并选择最适合的个性化学习策略提供了有效方法。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [14] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: 该论文提出AGI发展需要经历身份固化阶段，从开放模仿转向稳定身份，并建立了检测指标。实验发现不同规模模型呈现不同固化效果，认为这是AGI可靠性的前提和关键安全控制点。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型过于开放和可操控，类比人类发展，假设AGI进步需要经历身份固化阶段，使目标结构、偏好和内部表征变得稳定且抵抗外部操控。

Method: 形式化身份固化阶段，将其与学习动态中的已知现象联系起来，提出操作性检测指标，并通过实验验证行为固化的非线性特征。

Result: 实验显示行为固化快速且非线性，但对通用能力的影响各异：小模型出现性能权衡，中等规模模型几乎无成本，大型量化模型出现暂时不稳定性。

Conclusion: 身份固化是AGI级可靠性的先决条件，也是关键安全控制点——身份可被工程化设计以提升可靠性，但也可能在扩展过程中自发形成，导致不可预测的目标和行为固化。

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [15] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [16] [AI PB: A Grounded Generative Agent for Personalized Investment Insights](https://arxiv.org/abs/2510.20099)
*Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh*

Main category: cs.AI

TL;DR: AI PB是一个部署在零售金融领域的生成式AI代理系统，通过组件化编排、混合检索管道和多阶段推荐机制，主动生成合规、个性化的投资洞察。


<details>
  <summary>Details</summary>
Motivation: 传统被动式聊天机器人无法满足金融领域对主动、合规、个性化投资洞察的需求，需要开发能够主动生成可信AI洞察的系统。

Method: 采用组件化编排层确定性地路由内部和外部LLM，使用OpenSearch和金融领域嵌入模型的混合检索管道，以及结合规则启发式、序列行为建模和上下文老虎机的多阶段推荐机制。

Result: 系统在24个NVIDIA H100 GPU上使用Docker Swarm和vLLM完全在韩国金融监管下本地部署，通过人工QA和系统指标验证了其可信性。

Conclusion: 通过显式路由和分层安全机制的接地生成可以在高风险金融领域提供可信的AI洞察。

Abstract: We present AI PB, a production-scale generative agent deployed in real retail
finance. Unlike reactive chatbots that answer queries passively, AI PB
proactively generates grounded, compliant, and user-specific investment
insights. It integrates (i) a component-based orchestration layer that
deterministically routes between internal and external LLMs based on data
sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the
finance-domain embedding model, and (iii) a multi-stage recommendation
mechanism combining rule heuristics, sequential behavioral modeling, and
contextual bandits. Operating fully on-premises under Korean financial
regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100
GPUs. Through human QA and system metrics, we demonstrate that grounded
generation with explicit routing and layered safety can deliver trustworthy AI
insights in high-stakes finance.

</details>


### [17] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: HCLA是一个以人为中心的多智能体系统，用于数字资产交易异常检测，通过自然语言交互提供可解释的检测结果。


<details>
  <summary>Details</summary>
Motivation: 提高金融取证中异常检测的透明度和可信度，让非专家用户能够理解检测过程和结果。

Method: 将解析、检测和解释三个角色连接成对话工作流，使用XGBoost作为基础检测器，通过自然语言交互提供基于特征的叙述性解释。

Result: 在比特币混币数据集上，基线检测器达到较高准确率，HCLA系统增加了可解释性和交互式优化能力。

Conclusion: 人机协作设计能够显著提升金融取证系统的透明度和用户信任度。

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [18] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: 论文质疑AI在法律实践中能大幅降低成本的乐观观点，提出了验证-价值悖论，认为AI使用带来的效率提升会被相应的验证需求所抵消，净价值往往微乎其微。


<details>
  <summary>Details</summary>
Motivation: 针对律师因提交AI生成的不准确内容而受处罚的案例，重新评估AI在法律实践中的使用范式，考虑AI与现实脱节、缺乏透明度以及律师的核心职责。

Method: 提出验证-价值悖论作为替代模型，分析AI使用效率提升与验证需求之间的平衡关系。

Result: AI在法律实践中的净价值往往可以忽略不计，因为效率提升被验证需求所抵消。

Conclusion: 需要重新思考AI在法律实践和教育中的使用，强调对真相的忠诚和公民责任等核心价值。

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [19] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出TRUST框架，一种透明、去中心化的大语言模型推理过程审计方法，通过共识机制、分层DAG分解、区块链账本和隐私保护技术解决现有集中式审计的四大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型审计方法存在集中化、不透明、难以扩展的问题，无法有效验证推理过程的忠实性和无害性，阻碍了专有模型在高风险领域的部署。

Method: TRUST框架采用：1) 多样化审计者共识机制；2) 分层有向无环图分解推理链；3) 区块链记录验证决策；4) 隐私保护的分段共享技术。

Result: 实验表明TRUST能有效检测推理缺陷，在30%恶意参与者情况下仍保持稳健，适用于数学、医疗、科学、人文等多种推理任务。

Conclusion: TRUST框架为去中心化AI审计开创了先河，为大语言模型的安全可信部署提供了可行路径。

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [20] [Merge and Conquer: Evolutionarily Optimizing AI for 2048](https://arxiv.org/abs/2510.20205)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.AI

TL;DR: 该论文研究了在2048游戏中优化AI的进化训练方法，比较了单智能体系统和双智能体元提示系统的性能。


<details>
  <summary>Details</summary>
Motivation: 优化AI在动态环境中的性能是机器学习研究的基本挑战，2048游戏结合了策略性和随机性元素，是研究决策制定、长期规划和动态适应的理想平台。

Method: 实现两种系统：双智能体元提示系统（思考者LLM优化执行者LLM的策略）和基于值函数优化的单智能体系统，配合有限蒙特卡洛树搜索和回滚功能。

Result: 单智能体系统取得显著改进，每周期平均增加473.2分，训练周期呈明显上升趋势（相关性ρ=0.607）；双智能体系统改进有限，显示元提示的固有局限性。

Conclusion: 进化优化技术在非确定性环境中具有提升AI性能的潜力，单智能体方法优于元提示方法。

Abstract: Optimizing artificial intelligence (AI) for dynamic environments remains a
fundamental challenge in machine learning research. In this paper, we examine
evolutionary training methods for optimizing AI to solve the game 2048, a 2D
sliding puzzle. 2048, with its mix of strategic gameplay and stochastic
elements, presents an ideal playground for studying decision-making, long-term
planning, and dynamic adaptation. We implemented two distinct systems: a
two-agent metaprompting system where a "thinker" large language model (LLM)
agent refines gameplay strategies for an "executor" LLM agent, and a
single-agent system based on refining a value function for a limited Monte
Carlo Tree Search. We also experimented with rollback features to avoid
performance degradation. Our results demonstrate the potential of evolutionary
refinement techniques in improving AI performance in non-deterministic
environments. The single-agent system achieved substantial improvements, with
an average increase of 473.2 points per cycle, and with clear upward trends
(correlation $\rho$=0.607) across training cycles. The LLM's understanding of
the game grew as well, shown in its development of increasingly advanced
strategies. Conversely, the two-agent system did not garner much improvement,
highlighting the inherent limits of meta-prompting.

</details>


### [21] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: 该论文提出个性化认知模拟(ICS)任务，评估不同认知表征方法在模拟个体思维过程方面的效果，通过小说数据集和11条件认知评估框架测试7个现成LLM的作者风格模仿能力。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM能模仿表面的人类行为，但其模拟更深层个性化认知过程的能力尚不清楚，需要开发评估框架来理解LLM在认知模拟方面的局限性。

Method: 构建基于近期小说（晚于测试LLM发布日期）的数据集，提出11条件认知评估框架，测试语言特征、概念映射和基于档案信息等不同认知表征方法。

Result: 概念和语言特征的结合在ICS中特别有效，在整体评估中优于基于静态档案的提示；LLM更擅长模仿语言风格而非叙事结构。

Conclusion: 研究为开发适应个体思维和表达方式的AI系统奠定了基础，推动了更个性化和人类对齐的创意技术的发展。

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [22] [Using Large Language Models for Abstraction of Planning Domains - Extended Version](https://arxiv.org/abs/2510.20258)
*Bita Banihashemi,Megh Patel,Yves Lespérance*

Main category: cs.AI

TL;DR: 使用大型语言模型通过上下文学习生成抽象PDDL领域和问题实例，以支持基于自然语言目标的规划、推理和解释。


<details>
  <summary>Details</summary>
Motivation: 动态领域的抽象生成对智能体的规划、推理和解释能力至关重要，但目前仍是一个挑战。

Method: 在PDDL中建模具体行为，使用LLM进行上下文学习，生成抽象PDDL领域和问题实例，并通过符号验证工具和专家评估。

Result: GPT-4o在简单设置下能有效合成规划领域抽象，但在动作抽象方面优于关联谓词的抽象。

Conclusion: LLM在生成规划领域抽象方面具有潜力，特别是在动作抽象方面表现良好，但在谓词抽象方面仍需改进。

Abstract: Generating an abstraction of a dynamic domain that aligns with a given
purpose remains a significant challenge given that the choice of such an
abstraction can impact an agent's ability to plan, reason, and provide
explanations effectively. We model the agent's concrete behaviors in PDDL and
investigate the use of in-context learning with large language models (LLMs)
for the generation of abstract PDDL domains and problem instances, given an
abstraction objective specified in natural language. The benchmark examples we
use are new and have not been part of the data any LLMs have been trained on.
We consider three categories of abstractions: abstraction of choice of
alternative concrete actions, abstraction of sequences of concrete actions, and
abstraction of action/predicate parameters, as well as combinations of these.
The generated abstract PDDL domains and problem instances are then checked by
symbolic validation tools as well as human experts. Our experiments show that
GPT-4o can generally synthesize useful planning domain abstractions in simple
settings, although it is better at abstracting over actions than over the
associated fluents.

</details>


### [23] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: STaBERT模型通过整合POI和时序信息来增强人类移动性预测，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有模型要么只建模位置序列，要么仅将时间信息作为辅助输入，未能充分利用兴趣点(POI)提供的丰富语义上下文。

Method: 提出STaBERT模型，在BERT-based移动性模型中融入时序描述符和POI嵌入，构建统一的语义增强移动性表示。

Result: 实验结果显示预测精度显著提升：单城市预测的GEO-BLEU分数从0.34提高到0.75；多城市预测从0.34提高到0.56。

Conclusion: 整合POI和时序信息能够更好地捕捉人类移动的语义基础，显著改善移动性预测性能。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [24] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: ToolEQA是一个集成外部工具和多步推理的EQA智能体，通过工具获取有用信息来改进探索方向，从而以更短探索距离生成更准确回答。


<details>
  <summary>Details</summary>
Motivation: 现有EQA方法直接使用VLMs探索环境而不进行显式思考或规划，限制了推理能力，导致探索效率低下和回答效果不佳。

Method: 提出ToolEQA智能体，集成外部工具与多步推理；设计自动生成EQA任务的数据生成管道，构建包含18K任务的EQA-RT数据集。

Result: 在EQA-RT-Seen和EQA-RT-Unseen测试集上，ToolEQA比最先进基线成功率提高9.2~20.2%，比零样本版本高10%；在HM-EQA、OpenEQA和EXPRESS-Bench数据集上也达到最优性能。

Conclusion: ToolEQA通过集成外部工具和多步推理，显著提升了EQA任务的性能，证明了其通用性和有效性。

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [25] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: 本文基于AI4HealthyAging项目经验，分析了临床数据收集中存在的多种偏见类型，并提出了改进医疗AI系统公平性和鲁棒性的实用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医疗领域前景广阔，但由于训练数据的质量和公平性问题，AI解决方案在真实临床实践中的整合仍然有限。主要障碍是数据收集过程中的偏见问题。

Method: 基于西班牙国家研发计划中的AI4HealthyAging项目，通过检测临床数据收集过程中的偏见，识别了历史偏见、代表性偏见和测量偏见等多种偏见类型。

Result: 在多个用例中发现了性别、年龄、居住环境、社会经济状况、设备和标签等变量上的偏见表现。

Conclusion: 提出了改进临床问题设计和数据收集公平性和鲁棒性的实用建议，为开发更公平的医疗AI系统提供指导。

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [26] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出了一种用于评估军事行动中AI系统目标打击附带损害的新模型，该模型整合了时间、空间和力量维度，采用知识表示与推理架构，通过实例化进行验证。


<details>
  <summary>Details</summary>
Motivation: 在AI系统在战场中作用日益增强的背景下，需要严格评估潜在附带效应以确保负责任的目标打击。

Method: 采用设计科学方法论，构建统一的知识表示与推理架构，整合时间、空间和力量维度，采用分层结构捕获AI系统类别、架构组件、打击向量和上下文方面，并考虑传播、严重性、可能性和评估指标。

Result: 模型通过实例化进行了演示和评估，为构建负责任和可信赖的智能系统奠定了基础。

Conclusion: 该模型为评估军事行动中打击AI系统产生的效应提供了透明推理机制，是构建负责任智能系统的重要步骤。

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [27] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: 本综述系统回顾了LLM赋能知识图谱构建的最新进展，分析了LLM如何重塑传统的本体工程、知识抽取和知识融合三层流水线，并探讨了基于模式和无模式两种构建范式的互补优势。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的出现，知识图谱构建正从基于规则和统计的流水线转向语言驱动和生成式框架，需要系统梳理这一范式转变的技术进展和发展方向。

Method: 从两个互补视角回顾新兴的LLM驱动方法：基于模式的范式强调结构、规范化和一致性；无模式范式强调灵活性、适应性和开放发现。

Result: 系统综合了各阶段的代表性框架，分析了技术机制并识别了局限性，为理解LLM与知识图谱的演进关系提供了清晰框架。

Conclusion: 通过系统综述，旨在阐明LLM与知识图谱之间的演进关系，弥合符号知识工程与神经语义理解，推动自适应、可解释和智能知识系统的发展。

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [28] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: 提出了IKnow框架，通过设计基于指令-响应对话格式的自监督目标，解决指令调优模型在持续预训练中语义表示退化的问题，无需依赖原始基础模型或外部知识库。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要访问原始基础模型或依赖外部领域知识库，这在基础模型权重因安全原因被保留或可靠外部语料不可用时存在现实障碍。

Method: IKnow框架在指令-响应对话格式中制定新颖的自监督目标，利用文本中嵌入的领域知识，在更深语义层次进行编码学习。

Result: 该方法能够适应指令调优模型到新领域，同时保持其指令跟随能力和语义表示质量。

Conclusion: IKnow提供了一个简单通用的框架，能够在无需外部资源的情况下实现语言模型的持续适应。

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [29] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: 提出了一种基于创造力理论的计算模型，用于生成更具新颖性的创新机会，在酒店业创新项目中验证了其优于Notebook LM和ChatGPT4o的表现。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在生成创新机会时难以平衡新颖性和实用性，需要开发专门的计算模型来产生既新颖又有用的创新机会。

Method: 开发了包含五个功能模块的计算模型，这些功能基于创造力理论和技巧设计，旨在提升创新机会的新颖性同时保持其实用性。

Result: 在酒店业创新项目评估中，该模型生成的机会比Notebook LM和ChatGPT4o更具新颖性和/或实用性，但并非所有功能模块都对提升新颖性有贡献。

Conclusion: 该计算模型在生成高新颖性创新机会方面表现优异，但部分功能需要进一步优化，为未来模型开发指明了新方向。

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [30] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 提出了一种名为EBR的神经推理器，通过嵌入近似符号推理器的结果，解决了传统描述逻辑推理器对不一致和错误数据不鲁棒的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号概念学习方法依赖描述逻辑推理器，但这些推理器对知识库中的不一致和错误数据不鲁棒，限制了在真实世界知识库中的应用。

Method: EBR使用嵌入来近似符号推理器的结果，仅需要检索原子概念和存在限制的实例，就能近似SHOIQ描述逻辑中任何概念的实例集合。

Result: 实验表明EBR对缺失和错误数据具有鲁棒性，优于现有推理器。

Conclusion: EBR为在真实世界知识库中部署神经符号概念学习提供了可行的解决方案。

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [31] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: FLORA是一种基于模糊逻辑的知识图谱对齐方法，无需训练数据，能同时对齐实体和关系，提供可解释的结果，并在主要基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱对齐方法主要关注实体级对齐，在嵌入空间中计算实体相似度，缺乏可解释性且需要训练数据。

Method: 基于模糊逻辑的迭代方法，提供实体和关系的整体对齐，允许存在无对应实体的悬挂实体，并具有可证明的收敛性。

Result: 在主要基准测试中取得了最先进的结果。

Conclusion: FLORA是一个简单有效的无监督知识图谱对齐方法，具有可解释性、收敛性和处理悬挂实体的能力。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [32] [Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI](https://arxiv.org/abs/2510.20568)
*Susan Ariel Aaronson,Michael Moreno*

Main category: cs.AI

TL;DR: 该研究比较了澳大利亚、哥伦比亚和美国三个国家在AI治理中的公众参与情况，发现政府未能建立有效的公众对话机制，参与率极低且反馈响应不足，导致参与式AI治理的承诺与实践之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨政府如何通过公众参与来建立对AI及其治理的信任，分析当前公众意见在AI政策制定过程中的实际影响。

Method: 采用景观分析方法，比较三个国家（澳大利亚、哥伦比亚、美国）征集公众对AI风险和政策的反馈方式，以及这些意见如何影响治理决策。

Result: 研究发现三个国家都未能建立有意义的公众对话，参与率均低于人口1%，政府缺乏多样声音吸引和反馈响应机制，导致参与式治理效果不佳。

Conclusion: 当前AI治理的公众参与方法无法建立信任或合法性，作者提出八项改进建议，包括提升AI素养、扩大参与范围、采用创新参与方法等。

Abstract: The worlds people have strong opinions about artificial intelligence (AI),
and they want policymakers to listen. Governments are inviting public comment
on AI, but as they translate input into policy, much of what citizens say is
lost. Policymakers are missing a critical opportunity to build trust in AI and
its governance. This paper compares three countries, Australia, Colombia, and
the United States, that invited citizens to comment on AI risks and policies.
Using a landscape analysis, the authors examined how each government solicited
feedback and whether that input shaped governance. Yet in none of the three
cases did citizens and policymakers establish a meaningful dialogue.
Governments did little to attract diverse voices or publicize calls for
comment, leaving most citizens unaware or unprepared to respond. In each
nation, fewer than one percent of the population participated. Moreover,
officials showed limited responsiveness to the feedback they received, failing
to create an effective feedback loop. The study finds a persistent gap between
the promise and practice of participatory AI governance. The authors conclude
that current approaches are unlikely to build trust or legitimacy in AI because
policymakers are not adequately listening or responding to public concerns.
They offer eight recommendations: promote AI literacy; monitor public feedback;
broaden outreach; hold regular online forums; use innovative engagement
methods; include underrepresented groups; respond publicly to input; and make
participation easier.

</details>


### [33] [Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting](https://arxiv.org/abs/2510.20591)
*Ali Rajaei,Peter Palensky,Jochen L. Cremer*

Main category: cs.AI

TL;DR: 提出一种图神经网络加速的网络拓扑优化方法，通过母线分裂缓解电网拥塞，在GOC 2000节点系统上实现4个数量级加速，1分钟内提供AC可行解，最优性差距仅2.3%。


<details>
  <summary>Details</summary>
Motivation: 现有求解器无法在近实时内解决大规模系统的混合整数非线性网络拓扑优化问题，机器学习方法在泛化到未见拓扑、变化运行条件和不同系统方面存在局限。

Method: 开发异构边缘感知消息传递神经网络，预测有效的母线分裂动作作为候选拓扑优化解，考虑线性化AC潮流，捕捉局部流量模式。

Result: 在GOC 2000节点系统上实现4个数量级加速，1分钟内提供AC可行解，最优性差距仅2.3%，展示了拓扑和跨系统泛化能力。

Conclusion: 该方法为实现大规模系统近实时网络拓扑优化迈出了重要一步，具有拓扑和跨系统泛化能力。

Abstract: Network topology optimization (NTO) via busbar splitting can mitigate
transmission grid congestion and reduce redispatch costs. However, solving this
mixed-integer non-linear problem for large-scale systems in near-real-time is
currently intractable with existing solvers. Machine learning (ML) approaches
have emerged as a promising alternative, but they have limited generalization
to unseen topologies, varying operating conditions, and different systems,
which limits their practical applicability. This paper formulates NTO for
congestion management problem considering linearized AC PF, and proposes a
graph neural network (GNN)-accelerated approach. We develop a heterogeneous
edge-aware message passing NN to predict effective busbar splitting actions as
candidate NTO solutions. The proposed GNN captures local flow patterns,
achieves generalization to unseen topology changes, and improves
transferability across systems. Case studies show up to 4 orders-of-magnitude
speed-up, delivering AC-feasible solutions within one minute and a 2.3%
optimality gap on the GOC 2000-bus system. These results demonstrate a
significant step toward near-real-time NTO for large-scale systems with
topology and cross-system generalization.

</details>


### [34] [What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation](https://arxiv.org/abs/2510.20603)
*Heejin Do,Jaehui Hwang,Dongyoon Han,Seong Joon Oh,Sangdoo Yun*

Main category: cs.AI

TL;DR: 该论文提出了因果逐步评估(CaSE)方法，通过评估推理步骤的相关性和连贯性来改进LLM推理能力，而不仅仅是关注最终答案的正确性。


<details>
  <summary>Details</summary>
Motivation: 当前仅评估大语言模型最终答案正确性的方法过于粗糙，无法提供有效的模型改进信号，且忽视了底层推理过程的质量。

Method: 提出CaSE方法，将推理质量分解为相关性和连贯性两个维度，通过仅使用前序上下文评估每个推理步骤来避免后见之明偏差。

Result: 在MRa-GSM8K和MRa-MATH基准上验证了CaSE与人工判断的一致性，并证明使用CaSE评估的数据进行训练能直接提升最终任务性能。

Conclusion: 该工作为分析、调试和改进LLM推理提供了可扩展框架，展示了超越有效性检查的实际价值。

Abstract: Evaluating large language models (LLMs) on final-answer correctness is the
dominant paradigm. This approach, however, provides a coarse signal for model
improvement and overlooks the quality of the underlying reasoning process. We
argue that a more granular evaluation of reasoning offers a more effective path
to building robust models. We decompose reasoning quality into two dimensions:
relevance and coherence. Relevance measures if a step is grounded in the
problem; coherence measures if it follows logically from prior steps. To
measure these aspects reliably, we introduce causal stepwise evaluation (CaSE).
This method assesses each reasoning step using only its preceding context,
which avoids hindsight bias. We validate CaSE against human judgments on our
new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we
show that curating training data with CaSE-evaluated relevance and coherence
directly improves final task performance. Our work provides a scalable
framework for analyzing, debugging, and improving LLM reasoning, demonstrating
the practical value of moving beyond validity checks.

</details>


### [35] [Efficient Algorithms for Computing Random Walk Centrality](https://arxiv.org/abs/2510.20604)
*Changan Liu,Zixuan Xie,Ahad N. Zehmakan,Zhongzhi Zhang*

Main category: cs.AI

TL;DR: 提出了两种近线性时间算法来计算随机游走中心性，解决了大规模网络计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 随机游走中心性能够捕捉丰富的图结构信息，但在大规模网络上的计算由于现有方法的计算需求而不切实际。

Method: 基于新的随机游走中心性公式，开发了两种算法：一种利用近似Cholesky分解和稀疏逆估计，另一种采样根生成树。

Result: 在包括超过1000万个节点的大型真实网络上的广泛实验证明了所提算法的效率和近似质量。

Conclusion: 提出的算法在近线性时间内运行并提供强近似保证，使大规模网络的随机游走中心性计算变得可行。

Abstract: Random walk centrality is a fundamental metric in graph mining for
quantifying node importance and influence, defined as the weighted average of
hitting times to a node from all other nodes. Despite its ability to capture
rich graph structural information and its wide range of applications, computing
this measure for large networks remains impractical due to the computational
demands of existing methods. In this paper, we present a novel formulation of
random walk centrality, underpinning two scalable algorithms: one leveraging
approximate Cholesky factorization and sparse inverse estimation, while the
other sampling rooted spanning trees. Both algorithms operate in near-linear
time and provide strong approximation guarantees. Extensive experiments on
large real-world networks, including one with over 10 million nodes,
demonstrate the efficiency and approximation quality of the proposed
algorithms.

</details>


### [36] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: MIMOSA框架是一个可解释性设计的方法论，旨在生成平衡可解释性与性能的预测模型，同时嵌入因果性、公平性和隐私性等关键伦理属性。


<details>
  <summary>Details</summary>
Motivation: 可解释性设计模型对于在现实应用中建立对自动化决策模型的信任、问责和安全采用至关重要。

Method: 形式化定义了监督学习设置，涵盖表格数据、时间序列、图像、文本等多种数据类型，并分析了特征重要性、规则和实例三类主要可解释模型家族。

Result: 为因果性、公平性和隐私性提供了正式定义、评估指标和验证程序，并研究了这些属性之间的权衡关系。

Conclusion: 通过在模型生成过程中评估伦理度量，该框架为开发不仅准确和可解释，而且公平、保护隐私和具有因果意识的AI系统奠定了理论基础。

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [37] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: EcomEval是一个全面的多语言多模态电子商务基准测试，覆盖6个类别37个任务，包括8个多模态任务，使用真实客户查询和交易日志数据，支持7种语言。


<details>
  <summary>Details</summary>
Motivation: 现有电子商务评估基准存在任务多样性有限、模态单一、数据合成化、语言覆盖窄等问题，无法可靠评估LLM在复杂真实购物场景中的表现。

Method: 采用半自动流程：大模型生成候选回答，50多名电子商务和多语言专家审核修改；定义难度等级；覆盖7种语言（包括5种东南亚低资源语言）。

Result: 构建了一个反映真实业务交互噪声和异质性的综合性基准测试，支持细粒度和面向挑战的评估。

Conclusion: EcomEval填补了电子商务领域评估基准的空白，为评估LLM在真实复杂购物场景中的能力提供了可靠工具。

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [38] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: 提出了Fluidity Index (FI)来衡量模型在动态扩展环境中的适应性，通过评估初始、当前和未来环境状态偏差来测试上下文切换和连续性能力。


<details>
  <summary>Details</summary>
Motivation: 需要量化模型在动态扩展环境中的适应能力，区分封闭式和开放式基准测试，优先使用闭环开放式真实世界基准来评估适应性。

Method: 引入Fluidity Index (FI)指标，基于环境状态偏差评估响应准确性，测量模型理解、预测和适应状态变化的能力。

Result: 建立了评估模型适应性的框架，强调真正超级智能模型应具备至少二阶适应性，通过数字补充实现自持计算以达到最佳流动性。

Conclusion: Fluidity Index为评估模型在动态环境中的适应性提供了量化标准，超级智能模型需要具备高阶自适应能力以维持最优性能。

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [39] [Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges](https://arxiv.org/abs/2510.20641)
*Andrea Agiollo,Andrea Omicini*

Main category: cs.AI

TL;DR: 本文对将机器学习集成到理性智能体架构中的现有方法进行了系统化梳理，以BDI范式为参考框架，分析了相关文献发展，并指出了关键研究机会和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型在感知和认知任务中展现出类人能力，将ML集成到理性智能体架构中的框架日益受到关注。但目前该领域研究分散且不连贯，往往只关注将ML嵌入通用智能体容器，而忽视了理性架构（如BDI智能体）的表达能力。

Method: 使用BDI范式作为参考框架，对现有方法进行细粒度系统化梳理，分析理性智能体增强ML的快速演进文献。

Result: 分析展示了将ML集成到理性智能体架构中的现有方法体系，揭示了该领域的发展趋势。

Conclusion: 识别了设计有效理性ML智能体的关键研究机会和开放挑战，为未来研究提供了方向指引。

Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML)
models in perceptual and cognitive tasks, frameworks integrating ML within
rational agent architectures are gaining traction. Yet, the landscape remains
fragmented and incoherent, often focusing on embedding ML into generic agent
containers while overlooking the expressive power of rational
architectures--such as Belief-Desire-Intention (BDI) agents. This paper
presents a fine-grained systematisation of existing approaches, using the BDI
paradigm as a reference. Our analysis illustrates the fast-evolving literature
on rational agents enhanced by ML, and identifies key research opportunities
and open challenges for designing effective rational ML agents.

</details>


### [40] [The Shape of Reasoning: Topological Analysis of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2510.20665)
*Xue Wen Tan,Nathaniel Tan,Galen Lee,Stanley Kok*

Main category: cs.AI

TL;DR: 提出基于拓扑数据分析的框架来评估大语言模型推理轨迹质量，相比传统图指标具有更高预测能力


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型推理轨迹质量的方法依赖专家标注，劳动密集且不可靠，需要自动化、高效的评估方法

Method: 使用拓扑数据分析(TDA)框架捕捉推理轨迹的几何结构，通过拓扑特征进行自动化评估

Result: 拓扑特征在评估推理质量方面比标准图指标具有显著更高的预测能力，表明有效推理由高维几何结构而非纯关系图更好捕捉

Conclusion: 紧凑稳定的拓扑特征集能可靠指示轨迹质量，为未来强化学习算法提供实用信号

Abstract: Evaluating the quality of reasoning traces from large language models remains
understudied, labor-intensive, and unreliable: current practice relies on
expert rubrics, manual annotation, and slow pairwise judgments. Automated
efforts are dominated by graph-based proxies that quantify structural
connectivity but do not clarify what constitutes high-quality reasoning; such
abstractions can be overly simplistic for inherently complex processes. We
introduce a topological data analysis (TDA)-based evaluation framework that
captures the geometry of reasoning traces and enables label-efficient,
automated assessment. In our empirical study, topological features yield
substantially higher predictive power for assessing reasoning quality than
standard graph metrics, suggesting that effective reasoning is better captured
by higher-dimensional geometric structures rather than purely relational
graphs. We further show that a compact, stable set of topological features
reliably indicates trace quality, offering a practical signal for future
reinforcement learning algorithms.

</details>


### [41] [Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs](https://arxiv.org/abs/2510.20691)
*Yanlin Song,Ben Liu,Víctor Gutiérrez-Basulto,Zhiwei Hu,Qianqian Xie,Min Peng,Sophia Ananiadou,Jeff Z. Pan*

Main category: cs.AI

TL;DR: Graph-RFT是一个两阶段强化微调KGQA框架，通过"计划-KG搜索-网络搜索-思考"范式，让LLM在不完整知识条件下自主规划并自适应调度KG和网络检索。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法难以充分利用KG丰富知识和LLM推理能力，假设KG覆盖完整且缺乏判断何时需要外部信息的机制，推理过程局部短视，无法维持连贯的多步规划。

Method: 提出两阶段方法：1) 链式思维微调激活结构化推理；2) 计划检索引导的强化学习，集成显式规划和检索动作，采用笛卡尔式规划模块分解复杂问题，逻辑表达式指导工具调用。

Result: 通过多奖励设计优化推理检索过程，结合结果和检索特定信号，使模型学习何时及如何有效结合KG和网络检索。

Conclusion: Graph-RFT解决了GRPO冷启动问题，实现了覆盖感知的检索调度和全局一致的多步推理。

Abstract: Knowledge Graph Question Answering aims to answer natural language questions
by reasoning over structured knowledge graphs. While large language models have
advanced KGQA through their strong reasoning capabilities, existing methods
continue to struggle to fully exploit both the rich knowledge encoded in KGs
and the reasoning capabilities of LLMs, particularly in complex scenarios. They
often assume complete KG coverage and lack mechanisms to judge when external
information is needed, and their reasoning remains locally myopic, failing to
maintain coherent multi-step planning, leading to reasoning failures even when
relevant knowledge exists. We propose Graph-RFT, a novel two-stage
reinforcement fine-tuning KGQA framework with a
'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to
perform autonomous planning and adaptive retrieval scheduling across KG and web
sources under incomplete knowledge conditions. Graph-RFT introduces a
chain-of-thought fine-tuning method with a customized plan-retrieval dataset
activates structured reasoning and resolves the GRPO cold-start problem. It
then introduces a novel plan-retrieval guided reinforcement learning process
integrates explicit planning and retrieval actions with a multi-reward design,
enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired
planning module to decompose complex questions into ordered subquestions, and
logical expression to guide tool invocation for globally consistent multi-step
reasoning. This reasoning retrieval process is optimized with a multi-reward
combining outcome and retrieval specific signals, enabling the model to learn
when and how to combine KG and web retrieval effectively.

</details>


### [42] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: 提出了一种基于广义均值积分的一致性感知AGI度量方法，替代了传统算术平均方法，能够惩罚能力不平衡并捕捉领域间依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有AGI定义基于CHC认知模型的算术平均，假设能力可补偿性，但真正的通用智能应该反映在所有关键领域的平衡能力。

Method: 使用广义均值在补偿性指数连续统上的积分作为AGI度量，涵盖算术、几何和调和均值等不同补偿性假设下的表现。

Result: 应用于GPT-4和GPT-5的CHC领域得分显示，尽管算术得分较高（如GPT-5达24%），但一致性调整后的AUC表明两者距离通用能力还很远。

Conclusion: 广义均值积分为AGI测量提供了原则性、可解释且更严格的基础，能更准确地评估向AGI的真正进展。

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [43] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: 提出了Real Deep Research (RDR)框架，用于系统分析AI和机器人领域的研究趋势，识别新兴趋势和跨领域机会，帮助研究人员应对每年上万篇论文的挑战。


<details>
  <summary>Details</summary>
Motivation: AI和机器人领域每年产生超过10,000篇论文，研究人员难以跟上快速发展的趋势、跨学科工作和探索新领域的需求。

Method: 构建了通用的RDR管道，能够系统分析任何研究领域，识别新兴趋势，发现跨领域机会，并为新研究提供具体起点。

Result: 将RDR框架应用于AI和机器人领域，特别关注基础模型和机器人技术进步，并扩展到其他科学领域。

Conclusion: RDR框架为AI及其他领域的研究人员提供了有价值的分析工具，帮助他们在快速发展的研究环境中保持更新和发现新机会。

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [44] [Information Gradient for Nonlinear Gaussian Channel with Applications to Task-Oriented Communication](https://arxiv.org/abs/2510.20179)
*Tadashi Wadayama*

Main category: cs.IT

TL;DR: 提出了基于梯度的参数化非线性高斯信道优化框架，通过互信息最大化实现。利用SFB方法推导出计算可行的信息梯度公式，包含输出分布的得分函数和前端函数的雅可比矩阵两个关键组件。


<details>
  <summary>Details</summary>
Motivation: 为了解决非线性高斯信道中互信息最大化参数优化的计算难题，开发一个实用的梯度框架，避免对输出分布进行显式计算。

Method: 使用SFB方法推导信息梯度公式，通过DSM学习边际输出分布的得分函数，利用VJP高效处理前端函数的雅可比矩阵，实现梯度上升的参数优化。

Result: 实验验证了信息梯度公式的正确性，证明该方法能有效优化线性和非线性信道达到目标。

Conclusion: 该框架实现了非线性前端的端到端优化，无需显式计算输出分布，在任务导向场景和信息瓶颈目标中具有良好应用前景。

Abstract: We propose a gradient-based framework for optimizing parametric nonlinear
Gaussian channels via mutual information maximization. Leveraging the
score-to-Fisher bridge (SFB) methodology, we derive a computationally tractable
formula for the information gradient that is the gradient of mutual information
with respect to the parameters of the nonlinear front-end. Our formula
expresses this gradient in terms of two key components: the score function of
the marginal output distribution, which can be learned via denoising score
matching (DSM), and the Jacobian of the front-end function, which is handled
efficiently using the vector-Jacobian product (VJP) within automatic
differentiation frameworks. This enables practical parameter optimization
through gradient ascent. Furthermore, we extend this framework to task-oriented
scenarios, deriving gradients for both task-specific mutual information, where
a task variable depends on the channel input, and the information bottleneck
(IB) objective. A key advantage of our approach is that it facilitates
end-to-end optimization of the nonlinear front-end without requiring explicit
computation on the output distribution. Extensive experimental validation
confirms the correctness of our information gradient formula against analytical
solutions and demonstrates its effectiveness in optimizing both linear and
nonlinear channels toward their objectives.

</details>


### [45] [New Second-Order Achievability Bounds for Coding with Side Information via Type Deviation Convergence](https://arxiv.org/abs/2510.20241)
*Xiang Li,Cheuk Ting Li*

Main category: cs.IT

TL;DR: 提出了一个适用于网络信息论的第二阶可达性框架——类型偏差收敛，特别适用于有损源编码和带成本的信道编码，改进了多个经典问题的第二阶可达性界。


<details>
  <summary>Details</summary>
Motivation: 现有网络信息论中的第二阶可达性分析存在局限性，需要一种更通用且能提供更紧界的框架来处理有损源编码和带成本的信道编码问题。

Method: 采用类型偏差收敛框架，通过分析概率分布的收敛特性来推导第二阶可达性界，应用于Wyner-Ziv问题、Heegard-Berger问题和Gelfand-Pinsker问题。

Result: 为Wyner-Ziv问题提供了优于所有已知界的第二阶可达性界，同时为Heegard-Berger问题和带成本的Gelfand-Pinsker问题改进了现有第二阶可达性界。

Conclusion: 类型偏差收敛框架是一个通用且有效的工具，能够显著改进网络信息论中多个重要问题的第二阶可达性分析结果。

Abstract: We propose a framework for second-order achievability, called type deviation
convergence, that is generally applicable to settings in network information
theory, and is especially suitable for lossy source coding and channel coding
with cost. We give a second-order achievability bound for lossy source coding
with side information at the decoder (Wyner-Ziv problem) that improves upon all
known bounds (e.g., Watanabe-Kuzuoka-Tan, Yassaee-Aref-Gohari and
Li-Anantharam). We also give second-order achievability bounds for lossy
compression where side information may be absent (Heegard-Berger problem) and
channels with noncausal state information at the encoder and cost constraint
(Gelfand-Pinsker problem with cost) that improve upon previous bounds.

</details>


### [46] [A Location-Aware Hybrid Deep Learning Framework for Dynamic Near-Far Field Channel Estimation in Low-Altitude UAV Communications](https://arxiv.org/abs/2510.20277)
*Wenli Yuan,Kan Yu,Xiaowu Liu,Kaixuan Li,Qixun Zhang,Zhiyong Feng*

Main category: cs.IT

TL;DR: 提出了一种基于位置感知混合深度学习架构的统一信道估计框架，用于解决低空无人机通信中混合近远场传播条件下的信道估计挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基于远场假设的信道估计方法无法捕捉近场场景中的复杂信道变化，且忽略了实时收发器位置等有价值的几何先验信息。

Method: 结合CNN进行空间特征提取、BiLSTM网络建模时间演化、多头自注意力机制增强对判别性信道分量的关注，并将实时收发器位置嵌入作为几何先验。

Result: 在归一化均方误差(NMSE)上平均至少减少30.25%，显著优于现有基准方法。

Conclusion: 所提出的位置感知混合深度学习框架能有效解决低空无人机通信中的信道估计问题，特别是在混合近远场传播条件下表现出优越性能。

Abstract: In low altitude UAV communications, accurate channel estimation remains
challenging due to the dynamic nature of air to ground links, exacerbated by
high node mobility and the use of large scale antenna arrays, which introduce
hybrid near and far field propagation conditions. While conventional estimation
methods rely on far field assumptions, they fail to capture the intricate
channel variations in near-field scenarios and overlook valuable geometric
priors such as real-time transceiver positions. To overcome these limitations,
this paper introduces a unified channel estimation framework based on a
location aware hybrid deep learning architecture. The proposed model
synergistically combines convolutional neural networks (CNNs) for spatial
feature extraction, bidirectional long short term memory (BiLSTM) networks for
modeling temporal evolution, and a multihead self attention mechanism to
enhance focus on discriminative channel components. Furthermore, real-time
transmitter and receiver locations are embedded as geometric priors, improving
sensitivity to distance under near field spherical wavefronts and boosting
model generalization. Extensive simulations validate the effectiveness of the
proposed approach, showing that it outperforms existing benchmarks by a
significant margin, achieving at least a 30.25% reduction in normalized mean
square error (NMSE) on average.

</details>


### [47] [Moving or Predicting? RoleAware-MAPP: A Role-Aware Transformer Framework for Movable Antenna Position Prediction to Secure Wireless Communications](https://arxiv.org/abs/2510.20293)
*Wenxu Wang,Xiaowu Liu,Wei Gong,Yujia Zhao,Kaixuan Li,Qixun Zhang,Zhiyong Feng,Kan Yu*

Main category: cs.IT

TL;DR: 本文提出RoleAware-MAPP框架，通过Transformer模型结合领域知识解决可移动天线定位问题，显著提升物理层安全性能。


<details>
  <summary>Details</summary>
Motivation: 可移动天线技术面临实时优化计算复杂度和机械运动与信道变化速度不匹配的挑战，现有学习方法忽视了通信领域知识，特别是合法用户与窃听者之间的非对称角色和对抗性交互。

Method: 将MA定位问题重新定义为预测任务，提出RoleAware-MAPP框架，包含三个关键组件：角色感知嵌入、物理信息语义特征和复合损失函数。

Result: 在3GPP兼容场景下，RoleAware-MAPP实现了0.3569 bps/Hz的平均保密速率和81.52%的严格正保密容量，分别比最强基线提高48.4%和5.39个百分点。

Conclusion: RoleAware-MAPP通过整合领域知识有效解决了可移动天线定位问题，在多种用户速度和噪声条件下保持稳健性能。

Abstract: Movable antenna (MA) technology provides a promising avenue for actively
shaping wireless channels through dynamic antenna positioning, thereby enabling
electromagnetic radiation reconstruction to enhance physical layer security
(PLS). However, its practical deployment is hindered by two major challenges:
the high computational complexity of real time optimization and a critical
temporal mismatch between slow mechanical movement and rapid channel
variations. Although data driven methods have been introduced to alleviate
online optimization burdens, they are still constrained by suboptimal training
labels derived from conventional solvers or high sample complexity in
reinforcement learning. More importantly, existing learning based approaches
often overlook communication-specific domain knowledge, particularly the
asymmetric roles and adversarial interactions between legitimate users and
eavesdroppers, which are fundamental to PLS. To address these issues, this
paper reformulates the MA positioning problem as a predictive task and
introduces RoleAware-MAPP, a novel Transformer based framework that
incorporates domain knowledge through three key components: role-aware
embeddings that model user specific intentions, physics-informed semantic
features that encapsulate channel propagation characteristics, and a composite
loss function that strategically prioritizes secrecy performance over mere
geometric accuracy. Extensive simulations under 3GPP-compliant scenarios show
that RoleAware-MAPP achieves an average secrecy rate of 0.3569 bps/Hz and a
strictly positive secrecy capacity of 81.52%, outperforming the strongest
baseline by 48.4% and 5.39 percentage points, respectively, while maintaining
robust performance across diverse user velocities and noise conditions.

</details>


### [48] [Ergodic Mutual Information and Outage Probability for SIM-Assisted Holographic MIMO Communications](https://arxiv.org/abs/2510.20307)
*Anastasios Papazafeiropoulos,Pandelis Kourtessis,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.IT

TL;DR: 本文研究了堆叠智能超表面(SIM)辅助MIMO系统的遍历互信息和中断概率，使用大随机矩阵理论推导了互信息分布，并基于统计CSI提出了中断概率的闭式表达式和优化算法。


<details>
  <summary>Details</summary>
Motivation: 堆叠智能超表面相比单层超表面能通过波传播提供更好的性能，但文献中缺乏对SIM辅助MIMO系统的遍历互信息和中断概率的研究。

Method: 使用大随机矩阵理论工具获得互信息分布，推导基于统计CSI的中断概率闭式表达式，并应用梯度下降法最小化中断概率。

Result: 仿真结果验证了理论分析，显示相比传统MIMO系统和单层超表面有性能提升，且所提优化算法比交替优化基准更快，节省了显著开销。

Conclusion: 本文为SIM辅助MIMO系统提供了理论分析和优化方法，证明了其性能优势和高效率。

Abstract: Stacked intelligent metasurface (SIM) is a promising enabler for
next-generation high-capacity networks that exhibit better performance compared
to its single-layer counterpart by means of just wave propagation. However, the
study of ergodic mutual information (EMI) and outage probability for
SIM-assisted multiple-input-multiple-output (MIMO) systems is not available in
the literature. To this end, we obtain the distribution of the MI by using
large random matrix theory (RMT) tools. Next, we derive a tight closed-form
expression for the outage probability based on statistical channel state
information (CSI). Moreover, we apply the gradient descent method for the
minimization of the outage probability. Simulation results verify the
analytical results and provide fundamental insights such as the performance
enhancements compared to conventional MIMO systems and the single-layer
counterpart. Notably the proposed optimization algorithm is faster than the
alternating optimization (AO) benchmark by saving significant overhead.

</details>


### [49] [Robust Analog Lagrange Coded Computing: Theory and Algorithms via Discrete Fourier Transforms](https://arxiv.org/abs/2510.20379)
*Rimpi Borah,J. Harshan*

Main category: cs.IT

TL;DR: 提出了一个安全的模拟拉格朗日编码计算框架，能够抵御拜占庭工作节点的完整性威胁，通过DFT码纠错算法提高计算精度，并利用信任配置文件优化任务分配。


<details>
  <summary>Details</summary>
Motivation: 现有的ALCC框架虽然能保护数据隐私并应对延迟节点，但对返回错误结果的拜占庭节点缺乏抵抗力，存在安全漏洞。

Method: 使用离散傅里叶变换码的纠错算法构建新的重建策略，并基于DFT解码器性能理论提出任务分配方法，利用工作节点的信任配置文件。

Result: 显著提高了在有限数量拜占庭节点存在时的计算精度，特别是在掌握工作节点信任信息的情况下。

Conclusion: 所提出的安全ALCC框架能有效应对拜占庭攻击，包括利用浮点实现固有精度噪声的合谋攻击策略。

Abstract: Analog Lagrange Coded Computing (ALCC) is a recently proposed computational
paradigm wherein certain computations over analog datasets are efficiently
performed using distributed worker nodes through floating point representation.
While the vanilla version of ALCC is known to preserve the privacy of the
datasets from the workers and also achieve resilience against stragglers, it is
not robust against Byzantine workers that return erroneous results.
Highlighting this vulnerability, we propose a secure ALCC framework that is
resilient against a wide range of integrity threats from the Byzantine workers.
As a foundational step, we use error-correction algorithms for Discrete Fourier
Transform (DFT) codes to build novel reconstruction strategies for ALCC thereby
improving its computational accuracy in the presence of a bounded number of
Byzantine workers. Furthermore, capitalizing on some theoretical results on the
performance of the DFT decoders, we propose novel strategies for distributing
the ALCC computational tasks to the workers, and show that such methods
significantly improve the accuracy when the workers' trust profiles are
available at the master server. Finally, we study the robustness of the
proposed framework against colluding attacks, and show that interesting attack
strategies can be executed by exploiting the inherent precision noise owing to
floating point implementation.

</details>


### [50] [Adversary-Aware Private Inference over Wireless Channels](https://arxiv.org/abs/2510.20518)
*Mohamed Seif,Malcolm Egan,Andrea J. Goldsmith,H. Vincent Poor*

Main category: cs.IT

TL;DR: 提出了一种用于保护隐私的AI感知框架，设备在将提取的特征传输到模型服务器之前应用特征变换，以防止敏感数据被重建。


<details>
  <summary>Details</summary>
Motivation: 在边缘网络中，传感器和模型服务器通常不位于同一位置，需要传输特征。由于攻击者可能重建敏感个人数据，因此需要对特征进行变换以降低隐私泄露风险。

Method: 设备在传输前对提取的特征应用变换，保护个体特征隐私。

Result: 该框架为保护AI感知中的隐私提供了一种新方法。

Conclusion: 提出的特征变换框架能够有效降低边缘AI感知系统中的隐私泄露风险。

Abstract: AI-based sensing at wireless edge devices has the potential to significantly
enhance Artificial Intelligence (AI) applications, particularly for vision and
perception tasks such as in autonomous driving and environmental monitoring. AI
systems rely both on efficient model learning and inference. In the inference
phase, features extracted from sensing data are utilized for prediction tasks
(e.g., classification or regression). In edge networks, sensors and model
servers are often not co-located, which requires communication of features. As
sensitive personal data can be reconstructed by an adversary, transformation of
the features are required to reduce the risk of privacy violations. While
differential privacy mechanisms provide a means of protecting finite datasets,
protection of individual features has not been addressed. In this paper, we
propose a novel framework for privacy-preserving AI-based sensing, where
devices apply transformations of extracted features before transmission to a
model server.

</details>


### [51] [Simultaneous Wireless Information and Power Transfer for Fluid Antenna Systems](https://arxiv.org/abs/2510.20569)
*Feilong Zhang,Jianxin Dai,Zhaohui Yang,Kai-Kit Wong,Lingyuxiu Li,Jianglin Ye*

Main category: cs.IT

TL;DR: 提出一种结合MISO流体天线与传统固定位置天线的新通信系统，通过天线位置优化提高能量收集效率。


<details>
  <summary>Details</summary>
Motivation: 流体天线技术通过改变天线位置来提升通信速率，但传统固定位置天线在能量收集效率方面存在局限。

Method: 采用SWIPT技术，从基站向信息接收器和能量接收器传输相同信号，通过优化发射和接收流体天线位置以及发射协方差矩阵来提升能量收集效率。

Result: 仿真结果表明，流体天线系统相比传统固定位置天线能显著提高能量接收器的能量收集效率。

Conclusion: 流体天线系统在能量收集方面具有显著优势，为无线通信中的能量传输提供了新的解决方案。

Abstract: Fluid antenna is a promising wireless communication technology that enhances
communication rate by changing the antenna positions. This article proposes a
new communication system that combines multiple-input single-output (MISO)
fluid antennas with traditional fixed-position antennas, utilizing antenna
position optimization to improve energy harvesting efficiency. In this model,
we consider simultaneous wireless information and power transfer (SWIPT) which
transmits identical signals from the base station to both information receiver
(IR) and energy receiver (ER). We strive to enhance the power delivered to the
ER by fine-tuning the positions of transmit and receive fluid antennas, along
with optimizing the transmit covariance matrix, subject to a given minimum
signal-to-interference-plus-noise ratio (SINR) constraint at the IR. Simulation
results indicate that fluid antenna systems significantly enhance the energy
harvesting efficiency of the ER compared to traditional fixed-position
antennas.

</details>


### [52] [Stacked Intelligent Metasurfaces for 6G Wireless Networks: Principles, Applications, and Research Directions](https://arxiv.org/abs/2510.20572)
*Enyu Shi,Jiayi Zhang,Zhilong Liu,Ziheng Liu,Arumugam Nallanathan,Merouane Debbah,Shi Jin,Bo Ai*

Main category: cs.IT

TL;DR: 本文综述了基于堆叠智能超表面(SIM)的分布式无线网络，探讨其在6G网络中的应用场景、系统架构和关键技术挑战，展示了性能增益案例，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要提供无处不在的连接、弹性覆盖和智能服务，分布式无线架构如无蜂窝大规模MIMO因其可扩展性和公平性受到关注。SIM作为可重构智能表面的演进，具有增强的可控性和空间自由度，集成到分布式网络中可实现先进的波域操作。

Method: 将SIM集成到分布式无线网络中，实现波域信号处理，包括分层框架、用户关联和联合预编码等关键技术。通过案例研究验证性能增益。

Result: SIM辅助的分布式无线网络能够实现高效的干扰管理、提升能量和频谱效率，并增强物理层安全性，在6G网络中展现出显著性能优势。

Conclusion: SIM与分布式无线网络的结合为6G网络提供了可扩展和智能化的解决方案，未来需要在硬件设计、能耗建模、算法开发和AI集成等方面进一步研究。

Abstract: The sixth-generation (6G) wireless networks are expected to deliver
ubiquitous connectivity, resilient coverage, and intelligence-driven services
in highly dynamic environments. To achieve these goals, distributed wireless
architectures such as cell-free massive multiple-input multiple-output (MIMO)
have attracted significant attention due to their scalability and fairness.
Recently, stacked intelligent metasurfaces (SIMs) have emerged as a promising
evolution of reconfigurable intelligent surfaces, offering multi-layer
electromagnetic domain processing with enhanced controllability and spatial
degrees of freedom. By integrating SIMs into distributed wireless networks,
advanced wave-domain operations can be realized, enabling efficient
interference management, improved energy and spectral efficiency, and robust
physical-layer security. This article provides a comprehensive overview of
SIM-aided distributed wireless networks, including their application scenarios,
classification, and system architectures. Key signal processing challenges,
such as hierarchical frameworks, user association, and joint precoding, are
discussed, followed by case studies demonstrating significant performance
gains. Finally, future research directions in hardware design, energy
consumption modeling, algorithm development, and artificial intelligence
integration are highlighted, aiming to pave the way for scalable and
intelligent 6G distributed wireless networks.

</details>


### [53] [Super-Linear Growth of the Capacity-Achieving Input Support for the Amplitude-Constrained AWGN Channel](https://arxiv.org/abs/2510.20723)
*Haiyang Wang*

Main category: cs.IT

TL;DR: 本文研究了幅度受限AWGN信道容量最优输入分布的支撑点数量增长问题，证明了当幅度约束A增大时，支撑点数量K(A)呈超线性增长。


<details>
  <summary>Details</summary>
Motivation: 虽然Smith(1971)已证明最优输入是有限个离散点，但关于支撑点数量K(A)随幅度约束A增大的紧界仍是一个开放问题。

Method: 结合输出分布到均匀律的总变差收敛性与高斯混合逼近的定量限制，推导新的解析下界。

Result: 证明了K(A)在A增大时呈超线性增长，提供了关于最优输入分布复杂性的新认识。

Conclusion: 幅度受限AWGN信道的最优输入分布随着约束增强而变得更加复杂，支撑点数量超线性增长。

Abstract: We study the growth of the support size of the capacity-achieving input
distribution for the amplitude-constrained additive white Gaussian noise (AWGN)
channel. While it is known since Smith (1971) that the optimal input is
discrete with finitely many mass points, tight bounds on the number of support
points $K(A)$ as the amplitude constraint $A$ increases remain open. Building
on recent work by Dytso \emph{et al.} (2019) and Mattingly \emph{et al.}
(2018), we derive a new analytical lower bound showing that $K(A)$ grows
super-linearly in $A$. Our approach combines total-variation convergence of the
output distribution to the uniform law with quantitative limits on Gaussian
mixture approximation.

</details>


### [54] [MIMO-Zak-OTFS with Superimposed Spread Pilots](https://arxiv.org/abs/2510.20734)
*Abhishek Bairwa,Ananthanarayanan Chockalingam*

Main category: cs.IT

TL;DR: 提出了一种用于MIMO-Zak-OTFS系统的叠加扩频导频设计和有效信道估计方法，通过在交叉模糊域分离导频序列并使用turbo迭代来减轻导频-数据干扰。


<details>
  <summary>Details</summary>
Motivation: 在MIMO-Zak-OTFS系统中，数据和扩频导频信号叠加在同一帧中，需要设计有效的导频方案来分离不同发射天线的导频序列，并实现准确的信道估计。

Method: 提出了一种在交叉模糊域分离导频序列的设计，通过简单的读取操作估计有效信道抽头，并采用信道估计和检测之间的turbo迭代来减轻导频-数据干扰。

Result: 在2×2和3×3 MIMO-Zak-OTFS系统的仿真中，使用高斯sinc脉冲整形滤波器和车载A信道模型，所提方案经过三次turbo迭代后能实现非常好的估计/检测性能。

Conclusion: 所提出的导频设计和估计方案能够有效解决MIMO-Zak-OTFS系统中的导频分离和信道估计问题，通过turbo迭代显著提升系统性能。

Abstract: In this paper, we consider the problem of spread pilot design and effective
channel estimation in multiple-input multiple-output Zak-OTFS (MIMO-Zak-OTFS)
with superimposed spread pilots, where data and spread pilot signals are
superimposed in the same frame. To achieve good estimation performance in a
MIMO setting, the spread pilots at different transmit antennas need to be
effectively separated at the receiver. Towards this, we propose a spread pilot
design that separates the pilot sequences in the cross-ambiguity domain and
enables the estimation of the effective channel taps by a simple read-off
operation. To further alleviate the effect of pilot-data interference on
performance, we carry out turbo iterations between channel estimation and
detection. Simulation results for $2\times 2$ and $3\times 3$ MIMO-Zak-OTFS
with Gaussian-sinc pulse shaping filter for vehicular-A channel model show that
the proposed pilot design and estimation scheme with three turbo iterations can
achieve very good estimation/detection performance.

</details>
