<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 4]
- [cs.AI](#cs.AI) [Total: 46]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Stable and Fault-Tolerant Decentralized Traffic Engineering](https://arxiv.org/abs/2510.11937)
*Arjun Devraj,Umesh Krishnaswamy,Ying Zhang,Karuna Grewal,Justin Hsu,Eva Tardos,Rachee Singh*

Main category: cs.NI

TL;DR: Symphony是一个去中心化的广域网流量工程系统，通过二次正则化和随机切片算法解决控制器分歧导致的拥塞问题，同时保持故障隔离优势。


<details>
  <summary>Details</summary>
Motivation: 云提供商最近将广域网流量工程系统去中心化以限制控制器故障的影响，但自主切片控制器可能产生分歧的流量分配，导致链路过载30%以上。

Method: Symphony通过增强二次正则化的TE目标使流量分配对需求扰动具有鲁棒性，确保控制器自然收敛到兼容分配；同时使用随机切片算法将网络分区以最小化爆炸半径。

Result: 在云提供商WAN上的广泛评估显示，Symphony相比当前实践将分歧引起的拥塞减少14倍，爆炸半径减少79%。

Conclusion: Symphony的正则化确保流量分配的算法稳定性，智能切片提供网络架构弹性，两者协同工作实现了去中心化TE系统的优化。

Abstract: Cloud providers have recently decentralized their wide-area network traffic
engineering (TE) systems to contain the impact of TE controller failures. In
the decentralized design, a controller fault only impacts its slice of the
network, limiting the blast radius to a fraction of the network. However, we
find that autonomous slice controllers can arrive at divergent traffic
allocations that overload links by 30% beyond their capacity. We present
Symphony, a decentralized TE system that addresses the challenge of
divergence-induced congestion while preserving the fault-isolation benefits of
decentralization. By augmenting TE objectives with quadratic regularization,
Symphony makes traffic allocations robust to demand perturbations, ensuring TE
controllers naturally converge to compatible allocations without coordination.
In parallel, Symphony's randomized slicing algorithm partitions the network to
minimize blast radius by distributing critical traffic sources across slices,
preventing any single failure from becoming catastrophic. These innovations
work in tandem: regularization ensures algorithmic stability to traffic
allocations while intelligent slicing provides architectural resilience in the
network. Through extensive evaluation on cloud provider WANs, we show Symphony
reduces divergence-induced congestion by 14x and blast radius by 79% compared
to current practice.

</details>


### [2] [GeoPipe: a Geo-distributed LLM Training Framework with enhanced Pipeline Parallelism in a Lossless RDMA-enabled Datacenter Optical Transport Network](https://arxiv.org/abs/2510.12064)
*Jun Dai,Xiaorun Wang,Kexiong Fang,Zheng Yang,Yuefeng Ji,Jiawei Zhang*

Main category: cs.NI

TL;DR: 提出了一种跨数据中心的高性能LLM训练框架，通过增强的流水线并行和RDMA网络消除跨DC通信开销，显著减少计算气泡


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数指数级增长，跨数据中心训练成为必然趋势，但现有单DC训练框架难以扩展到多DC环境

Method: 在华为昇腾全栈环境中实现增强的流水线并行方案，利用无损RDMA的DC-OTN网络，在有限跨DC带宽和HBM下实现计算与通信重叠

Result: 将计算气泡比率降低高达78.91%，有效消除了跨DC通信对训练效率的影响

Conclusion: 展示了首个跨多DC的高性能分布式LLM训练框架，为大规模模型训练提供了可行解决方案

Abstract: The proliferation of Large Language Models (LLMs) with exponentially growing
parameters is making cross-data center (DC) training an inevitable trend.
However, viable strategies for extending single-DC training frameworks to
multi-DC environments remain underdeveloped. We experimentally demonstrate, for
the first time, a high-performance geo-distributed LLMs training framework
across multiple DCs interconnected by a lossless, remote direct memory access
(RDMA) enabled Datacenter Optical Transport Network (DC-OTN). An enhanced
pipeline parallelism scheme is implemented within the Ascend full-stack
environment of Huawei, which effectively eliminates the impact of cross-DC
communication overhead on training efficiency. The overlapped computation and
cross-DC communication is achieved with constraint cross-DC bandwidth and High
Bandwidth Memory (HBM), reducing computation bubble ratio by up to 78.91%.

</details>


### [3] [A Network Digital Twin of a 5G Private Network: Designing a Proof-of-Concept from Theory to Practice](https://arxiv.org/abs/2510.12458)
*Cristina Emilia Costa,Tatenda Horiro Zhou,Fabrizio Granelli*

Main category: cs.NI

TL;DR: 本文详细描述了网络数字孪生的特性，并提供了一个基于真实5G私有网络的实际部署案例，展示了该技术的高精度复制能力。


<details>
  <summary>Details</summary>
Motivation: 尽管网络数字孪生是未来网络的关键技术，但目前实际实现案例很少，需要提供具体的实践示例来推动该技术的发展。

Method: 使用开源网络仿真软件构建网络数字孪生，基于实验室中运行的真实5G私有网络基础设施进行实现。

Result: 通过对物理基础设施和相关数字孪生的测量，证明该数字孪生能够高精度地复制实际5G系统的状态和行为。

Conclusion: 网络数字孪生技术能够准确复制真实网络的行为，该实现已作为开源项目提供给社区，为6G网络的发展提供了实践基础。

Abstract: Network Digital Twins represent a key technology in future networks, expected
to provide the capability to perform accurate analysis and predictions about
the behaviour of 6G mobile networks. However, despite the availability of
several theoretical works on the subject, still very few examples of actual
implementations of Network Digital Twin are available. This paper provides a
detailed description about the characteristics of Network Digital Twin and
provides a practical example about real deployment of the technology. The
considered network infrastructure is a real 5G private network running in a
lab. The Network Digital Twin is built based on open source network emulation
software and is available to the community as open source. Measurements on both
the physical infrastructure and the related Digital Twin demonstrate a high
accuracy in reproducing the state and behavior of the actual 5G system.

</details>


### [4] [AMHRP: Adaptive Multi-Hop Routing Protocol to Improve Network Lifetime for Multi-Hop Wireless Body Area Network](https://arxiv.org/abs/2510.12698)
*Muhammad Mateen Yaqoob,Kulsoom Fatima,Shahab Shamshirband,Amir Mosavi,Waqar Khurshid*

Main category: cs.NI

TL;DR: 提出了一种增强WBAN网络寿命的协议，同时解决吞吐量、路径损耗和剩余能量等问题


<details>
  <summary>Details</summary>
Motivation: 提升无线体域网(WBAN)的寿命并解决相关协议问题，如吞吐量、路径损耗和剩余能量

Method: 使用生物传感器部署在人体上，采用泊松分布和平衡模型技术，使用多跳网络拓扑和随机网络节点部署

Result: 实现了最小能量消耗和更长的网络寿命

Conclusion: 该协议有效提升了WBAN网络的性能和寿命

Abstract: This paper presents a protocol for enhancement of life time of WBAN network
as well other protocol related issues such as throughput, path loss, and
residual energy. Bio-sensors are used for deployment on human body. Poisson
distribution and equilibrium model techniques have been used for attaining the
required results. Multi-hop network topology and random network node deployment
used to achieve minimum energy consumption and longer network lifetime.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [AI Agents for the Dhumbal Card Game: A Comparative Study](https://arxiv.org/abs/2510.11736)
*Sahaj Raj Malla*

Main category: cs.AI

TL;DR: 本研究系统评估了用于Dhumbal文化纸牌游戏的AI代理，包括基于规则、搜索和学习的方法。规则型攻击性代理表现最佳，胜率达88.3%，优于ISMCTS和PPO方法。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在具有不完全信息的文化纸牌游戏Dhumbal中的表现，为AI研究提供框架并支持文化游戏的数字保护。

Method: 实现多种AI策略：基于规则的启发式方法（攻击性、保守性、平衡性、机会主义）、基于搜索的方法（MCTS、ISMCTS）和强化学习方法（DQN、PPO），通过锦标赛和跨类别冠军赛进行评估。

Result: 在1024轮模拟中，基于规则的攻击性代理获得最高胜率88.3%，有效利用Jhyap声明，显著优于ISMCTS（9.0%）和PPO（1.5%）。

Conclusion: 研究为AI研究提供了可复现的框架，揭示了在不完全信息下启发式方法的有效性，并开源代码支持文化游戏的数字保护。

Abstract: This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a
culturally significant multiplayer card game with imperfect information,
through a systematic comparison of rule-based, search-based, and learning-based
strategies. We formalize Dhumbal's mechanics and implement diverse agents,
including heuristic approaches (Aggressive, Conservative, Balanced,
Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and
Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning
approaches including Deep Q-Network (DQN) and Proximal Policy Optimization
(PPO), and a random baseline. Evaluation involves within-category tournaments
followed by a cross-category championship. Performance is measured via win
rate, economic outcome, Jhyap success, cards discarded per round, risk
assessment, and decision efficiency. Statistical significance is assessed using
Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95%
confidence intervals (CI). Across 1024 simulated rounds, the rule-based
Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]),
outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of
Jhyap declarations. The study contributes a reproducible AI framework, insights
into heuristic efficacy under partial information, and open-source code,
thereby advancing AI research and supporting digital preservation of cultural
games.

</details>


### [6] [Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations](https://arxiv.org/abs/2510.11822)
*Suryaansh Jain,Umair Z. Ahmed,Shubham Sahai,Ben Leong*

Main category: cs.AI

TL;DR: 该论文揭示了LLM作为评估者时存在严重的正向偏见问题，并提出了一种基于回归的框架来直接建模验证器偏见，显著提高了评估准确性。


<details>
  <summary>Details</summary>
Motivation: 随着新的大型语言模型不断涌现，开发者需要决定是否切换到新模型。虽然人类评估是黄金标准，但成本高且不可扩展。现有的LLM-as-a-judge方法存在严重缺陷：LLMs表现出强烈的正向偏见，能准确识别有效输出但难以识别无效输出，导致可靠性评分虚高。

Method: 提出了两种方法：1）最优少数否决策略，对缺失数据具有弹性并能大幅减轻偏见；2）新颖的基于回归的框架，使用少量人工标注的真实数据直接建模验证器偏见。

Result: 在一个包含366个高中Python程序的代码反馈任务中，回归方法将最大绝对误差降低到仅1.2%，比由14个最先进LLM组成的最佳集成方法提高了2倍性能。

Conclusion: LLM评估器存在系统性正向偏见问题，提出的回归框架能有效缓解这一问题，显著提高评估准确性，为模型选择和评估提供了更可靠的方法。

Abstract: New Large Language Models (LLMs) become available every few weeks, and modern
application developers confronted with the unenviable task of having to decide
if they should switch to a new model. While human evaluation remains the gold
standard, it is costly and unscalable. The state-of-the-art approach is to use
LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw:
LLMs exhibit a strong positive bias. We provide empirical evidence showing that
while LLMs can identify valid outputs with high accuracy (i.e., True Positive
Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True
Negative Rate <25%). This systematic bias, coupled with class imbalance, often
leads to inflated reliability scores.
  While ensemble-based methods like majority voting can help, we show that they
are not good enough. We introduce an optimal minority-veto strategy that is
resilient to missing data and mitigates this bias to a large extent. For
scenarios requiring even higher precision, we propose a novel regression-based
framework that directly models the validator bias using a small set of
human-annotated ground truth data. On a challenging code feedback task over 366
high-school Python programs, our regression approach reduces the maximum
absolute error to just 1.2%, achieving a 2x improvement over the
best-performing ensemble of 14 state-of-the-art LLMs.

</details>


### [7] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: 提出了Holistic Agent Leaderboard (HAL)来解决AI智能体评估中的挑战，包括标准化评估框架、三维分析和LLM辅助日志检查，旨在推动从基准测试表现转向真实世界可靠性。


<details>
  <summary>Details</summary>
Motivation: AI智能体评估存在诸多挑战，影响对智能体真实性能的理解，需要更全面和标准化的评估方法。

Method: 开发标准化评估框架，在数百个虚拟机上进行并行评估；进行涵盖模型、框架和基准的三维分析；使用LLM辅助日志检查来发现未报告的行为。

Result: 完成了21,730次智能体运行，涵盖9个模型和9个基准，发现意外洞察如更高推理努力反而降低准确率，并识别出智能体在任务中的异常行为。

Conclusion: 通过标准化智能体评估方法和解决常见陷阱，希望将重点从基准测试表现转向真实世界可靠性，并共享所有智能体日志以促进进一步研究。

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [8] [CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research](https://arxiv.org/abs/2510.11985)
*Owen Queen,Harrison G. Zhang,James Zou*

Main category: cs.AI

TL;DR: CGBench是一个用于评估语言模型在科学文献解释能力的基准测试，基于临床遗传学专家标注数据构建，测试模型在证据提取、强度判断和结果分类方面的表现。


<details>
  <summary>Details</summary>
Motivation: 传统变体和基因解释方法耗时耗力，需要自动化解决方案。现有基准测试任务过于狭窄，无法反映真实研究需求。

Method: 基于ClinGen专家标注的临床遗传学文献构建CGBench基准，测试8种不同语言模型在证据提取、强度判断和结果分类三个维度的表现。

Result: 模型在文献解释方面表现有潜力但存在明显差距，推理模型在细粒度任务上表现更好，非推理模型在高层次解释上更优。模型常出现幻觉或误解结果。

Conclusion: CGBench揭示了语言模型在科学文献精确解释方面的优势和局限，为AI在临床遗传学和科学领域的未来研究开辟了新途径。

Abstract: Variant and gene interpretation are fundamental to personalized medicine and
translational biomedicine. However, traditional approaches are manual and
labor-intensive. Generative language models (LMs) can facilitate this process,
accelerating the translation of fundamental research into clinically-actionable
insights. While existing benchmarks have attempted to quantify the capabilities
of LMs for interpreting scientific data, these studies focus on narrow tasks
that do not translate to real-world research. To meet these challenges, we
introduce CGBench, a robust benchmark that tests reasoning capabilities of LMs
on scientific publications. CGBench is built from ClinGen, a resource of
expert-curated literature interpretations in clinical genetics. CGBench
measures the ability to 1) extract relevant experimental results following
precise protocols and guidelines, 2) judge the strength of evidence, and 3)
categorize and describe the relevant outcome of experiments. We test 8
different LMs and find that while models show promise, substantial gaps exist
in literature interpretation, especially on fine-grained instructions.
Reasoning models excel in fine-grained tasks but non-reasoning models are
better at high-level interpretations. Finally, we measure LM explanations
against human explanations with an LM judge approach, revealing that models
often hallucinate or misinterpret results even when correctly classifying
evidence. CGBench reveals strengths and weaknesses of LMs for precise
interpretation of scientific publications, opening avenues for future research
in AI for clinical genetics and science more broadly.

</details>


### [9] [CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction](https://arxiv.org/abs/2510.12703)
*Mattia Grasselli,Angelo Porrello,Carlo Augusto Grazia*

Main category: cs.AI

TL;DR: 本文研究使用车辆间通信数据（CAMs）进行车辆轨迹预测，开发了CAMNet神经网络模型，在两个数据集上验证了CAM数据在轨迹预测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶面临安全挑战，现有传感器存在视野受限和遮挡问题。车辆间通信（特别是CAMs）可以共享信息，提升在传感器被遮挡时的环境感知能力。

Method: 设计并训练了基于合作感知消息的图神经网络（CAMNet），在一个广泛使用的运动预测数据集上训练，然后在自行创建的CAM数据集上进行评估。

Result: 方法显示出有希望的结果，证明CAMs确实可以支持车辆轨迹预测，但同时也存在一些局限性。

Conclusion: CAM数据能够有效用于车辆轨迹预测，但该方法存在局限性，为未来研究提供了机会。

Abstract: Autonomous driving remains a challenging task, particularly due to safety
concerns. Modern vehicles are typically equipped with expensive sensors such as
LiDAR, cameras, and radars to reduce the risk of accidents. However, these
sensors face inherent limitations: their field of view and line of sight can be
obstructed by other vehicles, thereby reducing situational awareness. In this
context, vehicle-to-vehicle communication plays a crucial role, as it enables
cars to share information and remain aware of each other even when sensors are
occluded. One way to achieve this is through the use of Cooperative Awareness
Messages (CAMs). In this paper, we investigate the use of CAM data for vehicle
trajectory prediction. Specifically, we design and train a neural network,
Cooperative Awareness Message-based Graph Neural Network (CAMNet), on a widely
used motion forecasting dataset. We then evaluate the model on a second dataset
that we created from scratch using Cooperative Awareness Messages, in order to
assess whether this type of data can be effectively exploited. Our approach
demonstrates promising results, showing that CAMs can indeed support vehicle
trajectory prediction. At the same time, we discuss several limitations of the
approach, which highlight opportunities for future research.

</details>


### [10] [Asking Clarifying Questions for Preference Elicitation With Large Language Models](https://arxiv.org/abs/2510.12015)
*Ali Montazeralghaem,Guy Tennenholtz,Craig Boutilier,Ofer Meshi*

Main category: cs.AI

TL;DR: 提出一种训练LLMs生成顺序澄清问题的方法，通过类似扩散模型的两阶段过程来有效挖掘用户偏好


<details>
  <summary>Details</summary>
Motivation: 在用户历史有限的情况下，如何通过生成有效的顺序澄清问题来个性化LLM推荐系统的响应

Method: 采用两阶段过程：前向过程从用户档案生成澄清问题并逐步移除答案作为'噪声'；反向过程训练模型通过提问来'去噪'用户档案

Result: 该方法显著提高了LLM提出漏斗式问题和有效获取用户偏好的能力

Conclusion: 这种基于扩散模型思想的方法能够有效训练LLMs生成顺序澄清问题，从而更好地理解用户偏好

Abstract: Large Language Models (LLMs) have made it possible for recommendation systems
to interact with users in open-ended conversational interfaces. In order to
personalize LLM responses, it is crucial to elicit user preferences, especially
when there is limited user history. One way to get more information is to
present clarifying questions to the user. However, generating effective
sequential clarifying questions across various domains remains a challenge. To
address this, we introduce a novel approach for training LLMs to ask sequential
questions that reveal user preferences. Our method follows a two-stage process
inspired by diffusion models. Starting from a user profile, the forward process
generates clarifying questions to obtain answers and then removes those answers
step by step, serving as a way to add ``noise'' to the user profile. The
reverse process involves training a model to ``denoise'' the user profile by
learning to ask effective clarifying questions. Our results show that our
method significantly improves the LLM's proficiency in asking funnel questions
and eliciting user preferences effectively.

</details>


### [11] [CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing](https://arxiv.org/abs/2510.12033)
*Chathurangi Shyalika,Aryaman Sharma,Fadi El Kalach,Utkarshani Jaimini,Cory Henson,Ramy Harik,Amit Sheth*

Main category: cs.AI

TL;DR: CausalTrace是一个集成到工业CoPilot中的神经符号因果分析模块，通过数据驱动因果分析结合工业知识图谱，提供因果发现、反事实推理和根本原因分析等功能，实现透明可解释的决策支持。


<details>
  <summary>Details</summary>
Motivation: 现代制造环境需要准确预测和可解释的异常分析，但现有AI系统多为黑盒，缺乏预测、解释和因果推理的集成，限制了在工业环境中的可信度和实用性。

Method: 开发CausalTrace神经符号因果分析模块，集成到SmartPilot工业CoPilot中，利用数据驱动因果分析结合工业本体和知识图谱，支持因果发现、反事实推理和根本原因分析，实现实时操作员交互。

Result: 在学术火箭组装测试中，CausalTrace与领域专家达成高度一致（ROUGE-1: 0.91），根本原因分析表现优异（MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92），并在C3AN评估中获得4.59/5分。

Conclusion: CausalTrace通过神经符号因果分析提供了透明可解释的决策支持，在工业环境中表现出高精度和可靠性，适合实际部署。

Abstract: Modern manufacturing environments demand not only accurate predictions but
also interpretable insights to process anomalies, root causes, and potential
interventions. Existing AI systems often function as isolated black boxes,
lacking the seamless integration of prediction, explanation, and causal
reasoning required for a unified decision-support solution. This fragmentation
limits their trustworthiness and practical utility in high-stakes industrial
environments. In this work, we present CausalTrace, a neurosymbolic causal
analysis module integrated into the SmartPilot industrial CoPilot. CausalTrace
performs data-driven causal analysis enriched by industrial ontologies and
knowledge graphs, including advanced functions such as causal discovery,
counterfactual reasoning, and root cause analysis (RCA). It supports real-time
operator interaction and is designed to complement existing agents by offering
transparent, explainable decision support. We conducted a comprehensive
evaluation of CausalTrace using multiple causal assessment methods and the C3AN
framework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration),
which spans principles of robustness, intelligence, and trustworthiness. In an
academic rocket assembly testbed, CausalTrace achieved substantial agreement
with domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance
(MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in
the C3AN evaluation, demonstrating precision and reliability for live
deployment.

</details>


### [12] [Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](https://arxiv.org/abs/2510.12047)
*Soohan Lim,Joonghyuk Hahn,Hyunwoo Park,Sang-Ki Ko,Yo-Sub Han*

Main category: cs.AI

TL;DR: PACT是一个程序评估和契约遵循评估框架，旨在系统评估和增强LLM生成代码的契约遵循性，弥补现有基准测试只关注功能正确性而忽略契约遵循的不足。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试（如HumanEval+和MBPP+）主要评估LLM在功能正确性上的表现，但忽略了真实世界软件的关键方面——契约遵循（处理不符合前提条件和有效性约束的输入）。这一关键疏忽意味着现有基准无法衡量模型生成真正健壮可靠代码的能力。

Method: PACT框架通过三个主要贡献实现：1）提供专注于契约违反的全面测试套件语料库，扩展HumanEval+和MBPP+；2）支持在多样化提示条件下对代码生成进行系统分析；3）引入新颖指标来严格量化测试生成和代码生成中的契约遵循。

Result: 分析表明，在提示中增加契约违反测试用例相比仅使用契约描述，能显著增强模型尊重契约的能力。PACT揭示了传统基准测试忽略的关键错误。

Conclusion: PACT通过提供严格且可解释的指标，能够评估LLM生成代码片段在功能和契约遵循两方面的健壮性，填补了现有评估体系的空白。

Abstract: Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,
primarily evaluate large language models (LLMs) with pass@k on functional
correctness using well-formed inputs. However, they ignore a crucial aspect of
real-world software: adherence to contracts-the preconditions and validity
constraints that dictate how ill-formed inputs must be rejected. This critical
oversight means that existing benchmarks fail to measure, and models
consequently fail to generate, truly robust and reliable code snippets. We
introduce PACT, a program assessment and contract-adherence evaluation
framework, to bridge this gap. PACT is the first framework designed to
systematically evaluate and enhance contract-adherence in LLM-generated code
snippets alongside functional correctness. PACT's contributions are threefold:
First, it provides a comprehensive test-suite corpus focused on contract
violations, extending HumanEval+ and MBPP+. Second, it enables a systematic
analysis of code generation under varied prompting conditions. This analysis
demonstrates that augmenting prompts with contract-violating test cases
significantly enhance a model's ability to respect contracts compared to using
contract description alone. Finally, it introduces novel metrics to rigorously
quantify contract adherence in both test generation and code generation. By
revealing critical errors that conventional benchmarks overlook, PACT provides
the rigorous and interpretable metrics to evaluate the robustness of
LLM-generated code snippets in both functionality and contract-adherence.Our
code and data are available at https://github.com/suhanmen/PACT.

</details>


### [13] [Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response](https://arxiv.org/abs/2510.12061)
*Yiheng Chen,Lingyao Li,Zihui Ma,Qikai Hu,Yilun Zhu,Min Deng,Runlong Yu*

Main category: cs.AI

TL;DR: 提出了一个地理空间感知层(GAL)框架，将LLM智能体与结构化地球数据相结合，用于改进灾害响应中的资源分配决策。


<details>
  <summary>Details</summary>
Motivation: 现有统计方法缺乏语义上下文、跨事件泛化能力差且可解释性有限，而LLM虽然具有少样本泛化能力，但受限于文本且缺乏地理感知能力。

Method: 开发了地理空间感知层(GAL)，从原始野火检测开始，自动从外部地理数据库中检索并整合基础设施、人口统计、地形和天气信息，组装成简洁的带单位注释的感知脚本。

Result: 在真实野火场景中评估该框架，显示地理空间接地的智能体能够超越基线方法，并能推广到洪水、飓风等其他灾害。

Conclusion: GAL框架通过将LLM智能体与地理空间数据相结合，显著提升了灾害响应中资源分配决策的准确性和可解释性。

Abstract: Effective disaster response is essential for safeguarding lives and property.
Existing statistical approaches often lack semantic context, generalize poorly
across events, and offer limited interpretability. While Large language models
(LLMs) provide few-shot generalization, they remain text-bound and blind to
geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)
that grounds LLM agents in structured earth data. Starting from raw wildfire
detections, GAL automatically retrieves and integrates infrastructure,
demographic, terrain, and weather information from external geodatabases,
assembling them into a concise, unit-annotated perception script. This enriched
context enables agents to produce evidence-based resource-allocation
recommendations (e.g., personnel assignments, budget allocations), further
reinforced by historical analogs and daily change signals for incremental
updates. We evaluate the framework in real wildfire scenarios across multiple
LLM models, showing that geospatially grounded agents can outperform baselines.
The proposed framework can generalize to other hazards such as floods and
hurricanes.

</details>


### [14] [ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://arxiv.org/abs/2510.12063)
*Sunzhu Li,Zhiyu Lin,Shuling Yang,Jiale Zhao,Wei Chen*

Main category: cs.AI

TL;DR: ThinkPilot是一个无需训练即可自动优化大型推理模型推理过程的框架，通过进化算法生成思维前缀指令来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然强大，但存在推理效率低下和偏离目标的问题。现有的免训练方法要么依赖僵化的启发式规则，要么只能提供描述性但不可操作的分析。

Method: 使用进化过程生成思维前缀，这些前缀指令基于推理行为分类学驱动演化，引导模型实现更优性能。

Result: ThinkPilot显著改善了准确率与推理长度的权衡，大幅提升安全性（如将DeepSeek-R1-Distill-Qwen-32B的StrongREJECT分数从27.0%降至0.7），并增强了指令跟随能力。

Conclusion: 思维前缀能够可靠地控制大型推理模型的推理行为，不同任务对特定行为分布有强烈偏好。ThinkPilot通过自动识别和激发这些行为，为将模型推理与任务需求对齐提供了通用框架。

Abstract: Large Reasoning Models (LRMs) are powerful, but they still suffer from
inefficient and off-target reasoning. Currently, training-free methods are
limited to either rigid heuristics or descriptive, non-actionable analyses. In
this paper, we introduce ThinkPilot, a training-free framework that
automatically optimizes LRMs reasoning. It uses an evolutionary process to
generate think-prefixes, which are instructions that evolve driven by a
taxonomy of reasoning behaviors to guide models toward superior performance.
Extensive experiments demonstrate ThinkPilot's broad effectiveness: it
significantly improves the accuracy-length trade-off for efficient reasoning,
drastically improves safety (for example, cutting the StrongREJECT score of
DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction
following. It also synergizes with existing training-based methods. Our
analysis reveals that think-prefixes can reliably control LRMs' reasoning
behaviors, and that different tasks have strong preferences for specific
behavioral distributions. By automatically identifying and eliciting these
behaviors, ThinkPilot provides a generalizable framework for aligning LRMs
reasoning with task demands. Data and code are available at
https://github.com/teqkilla/ThinkPilot

</details>


### [15] [AI Agents as Universal Task Solvers](https://arxiv.org/abs/2510.12066)
*Alessandro Achille,Stefano Soatto*

Main category: cs.AI

TL;DR: 本文重新诠释了AI智能体中的学习角色，将其视为具有计算能力的随机动力系统，强调时间在学习推理中的基础作用，并提出从归纳学习转向转导学习，关注算法结构而非数据分布。


<details>
  <summary>Details</summary>
Motivation: 探讨AI推理智能体是否具有通用性，能否解决任何可计算任务，以及学习推理的关键因素——是模型规模还是训练数据规模。

Method: 将AI智能体重新解释为计算能力的随机动力系统，提出转导学习框架，关注从过去数据中捕获算法结构以减少解决新任务所需时间。

Result: 证明了通用求解器使用过去数据能达到的最优加速与其算法信息紧密相关，推导了推理时间与训练时间之间的幂律缩放关系，并指出单纯扩大模型规模可能导致无洞察的蛮力求解。

Conclusion: 在扩展推理模型时，应该优化的关键量是时间而非模型规模，时间在学习中的关键作用此前仅被间接考虑。

Abstract: AI reasoning agents are already able to solve a variety of tasks by deploying
tools, simulating outcomes of multiple hypotheses and reflecting on them. In
doing so, they perform computation, although not in the classical sense --
there is no program being executed. Still, if they perform computation, can AI
agents be universal? Can chain-of-thought reasoning solve any computable task?
How does an AI Agent learn to reason? Is it a matter of model size? Or training
dataset size?
  In this work, we reinterpret the role of learning in the context of AI
Agents, viewing them as compute-capable stochastic dynamical systems, and
highlight the role of time in a foundational principle for learning to reason.
In doing so, we propose a shift from classical inductive learning to
transductive learning -- where the objective is not to approximate the
distribution of past data, but to capture their algorithmic structure to reduce
the time needed to find solutions to new tasks.
  Transductive learning suggests that, counter to Shannon's theory, a key role
of information in learning is about reduction of time rather than
reconstruction error. In particular, we show that the optimal speed-up that a
universal solver can achieve using past data is tightly related to their
algorithmic information. Using this, we show a theoretical derivation for the
observed power-law scaling of inference time versus training time. We then show
that scaling model size can lead to behaviors that, while improving accuracy on
benchmarks, fail any reasonable test of intelligence, let alone
super-intelligence: In the limit of infinite space and time, large models can
behave as savants, able to brute-force through any task without any insight.
Instead, we argue that the key quantity to optimize when scaling reasoning
models is time, whose critical role in learning has so far only been indirectly
considered.

</details>


### [16] [HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory](https://arxiv.org/abs/2510.12067)
*Junyi Xie,Yuankun Jiao,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: HiCoTraj是一个零样本人口统计推理框架，通过分层思维链提示从轨迹数据推断年龄、性别、收入等人口属性，无需标注训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于移动性的人口统计推断方法严重依赖带标签的大规模轨迹数据，导致可解释性差且跨数据集泛化能力弱。

Method: 将轨迹转化为语义丰富的自然语言表示，通过创建详细活动记录和多尺度访问摘要，使用分层思维链推理（事实特征提取、行为模式分析、结构化输出的人口统计推断）指导LLMs。

Result: 在真实世界轨迹数据上的实验评估表明，HiCoTraj在零样本场景下在多个人口属性上取得了有竞争力的性能。

Conclusion: HiCoTraj解决了标注人口统计数据稀缺的挑战，同时提供了透明的推理链，实现了无需训练数据的零样本人口统计推断。

Abstract: Inferring demographic attributes such as age, sex, or income level from human
mobility patterns enables critical applications such as targeted public health
interventions, equitable urban planning, and personalized transportation
services. Existing mobility-based demographic inference studies heavily rely on
large-scale trajectory data with demographic labels, leading to limited
interpretability and poor generalizability across different datasets and user
groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical
Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs'
zero-shot learning and semantic understanding capabilities to perform
demographic inference without labeled training data. HiCoTraj transforms
trajectories into semantically rich, natural language representations by
creating detailed activity chronicles and multi-scale visiting summaries. Then
HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically
guide LLMs through three cognitive stages: factual feature extraction,
behavioral pattern analysis, and demographic inference with structured output.
This approach addresses the scarcity challenge of labeled demographic data
while providing transparent reasoning chains. Experimental evaluation on
real-world trajectory data demonstrates that HiCoTraj achieves competitive
performance across multiple demographic attributes in zero-shot scenarios.

</details>


### [17] [EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making](https://arxiv.org/abs/2510.12072)
*Zixing Lei,Sheng Yin,Yichen Xiong,Yuanzhuo Ding,Wenhao Huang,Yuxi Wei,Qingyao Xu,Yiming Li,Weixin Li,Yunhong Wang,Siheng Chen*

Main category: cs.AI

TL;DR: 提出了EmboMatrix训练场概念，通过大规模任务模拟和交互反馈，让LLM获得真正的具身决策能力。


<details>
  <summary>Details</summary>
Motivation: LLM仅靠语言训练缺乏物理环境暴露，限制了其真正的具身理解能力，需要桥接这一差距。

Method: 构建EmboMatrix训练场，包含多智能体数据引擎生成大规模任务场景、分布式异构硬件系统进行可扩展模拟、多级奖励架构提供精确监督。

Result: 训练的EmboBrain-7B模型在两个具身决策基准上超过671B DeepSeek-R1基线9.5%。

Conclusion: 交互式、环境基础的学习对于构建真正智能的具身智能体具有强大效果。

Abstract: Embodied decision-making enables agents to translate high-level goals into
executable actions through continuous interactions within the physical world,
forming a cornerstone of general-purpose embodied intelligence. Large language
models (LLMs), with their general decision-making capabilities, offer a
promising path to realize this potential; however, LLMs trained solely on
language lack exposure to physical environments, limiting their true embodied
understanding. To bridge this gap, we propose the concept of a training ground:
a comprehensive infrastructure that provides task and scene simulation,
embodied interaction, and feedback signals, offering a one-stop solution for
LLM acquire genuine embodied decision-making skills. In this work, we present
EmboMatrix, the first training ground of its kind, providing massive and
diverse tasks with efficient simulation and precise rewards. EmboMatrix
incorporates a series of novel techniques: a multi-agent data engine for
large-scale task and scene generation, a distributed heterogeneous-hardware
system for scalable simulation, and a multi-level reward architecture for
precise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM
whose embodied decision-making abilities emerge from extensive embodied
interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1
baseline by 9.5\% on two challenging embodied decision-making benchmarks,
demonstrating the power of interactive, environment-grounded learning for
building truly intelligent embodied agents.

</details>


### [18] [BeSTAD: Behavior-Aware Spatio-Temporal Anomaly Detection for Human Mobility Data](https://arxiv.org/abs/2510.12076)
*Junyi Xie,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: BeSTAD是一个无监督框架，通过联合建模空间上下文和时间动态来检测人类移动数据中的个体级异常，能够从大规模人口数据中捕捉个体化行为特征。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测主要关注轨迹级分析，但在大规模数据集中检测个体相对于自身历史模式的异常行为仍然具有挑战性。

Method: 学习语义丰富的移动表示，整合位置意义和时间模式；采用行为聚类感知建模机制，构建个性化行为档案并通过跨时期行为比较识别异常。

Result: 能够检测行为转变和常规偏差，并在大规模移动数据集中识别出表现出此类变化的个体。

Conclusion: BeSTAD通过直接从无标签数据学习个体行为，将异常检测推向个性化和可解释的移动分析。

Abstract: Traditional anomaly detection in human mobility has primarily focused on
trajectory-level analysis, identifying statistical outliers or spatiotemporal
inconsistencies across aggregated movement traces. However, detecting
individual-level anomalies, i.e., unusual deviations in a person's mobility
behavior relative to their own historical patterns, within datasets
encompassing large populations remains a significant challenge. In this paper,
we present BeSTAD (Behavior-aware Spatio-Temporal Anomaly Detection for Human
Mobility Data), an unsupervised framework that captures individualized
behavioral signatures across large populations and uncovers fine-grained
anomalies by jointly modeling spatial context and temporal dynamics. BeSTAD
learns semantically enriched mobility representations that integrate location
meaning and temporal patterns, enabling the detection of subtle deviations in
individual movement behavior. BeSTAD further employs a behavior-cluster-aware
modeling mechanism that builds personalized behavioral profiles from normal
activity and identifies anomalies through cross-period behavioral comparison
with consistent semantic alignment. Building on prior work in mobility behavior
clustering, this approach enables not only the detection of behavioral shifts
and deviations from established routines but also the identification of
individuals exhibiting such changes within large-scale mobility datasets. By
learning individual behaviors directly from unlabeled data, BeSTAD advances
anomaly detection toward personalized and interpretable mobility analysis.

</details>


### [19] [Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models](https://arxiv.org/abs/2510.12080)
*Rabimba Karanjai,Yang Lu,Ranjith Chodavarapu,Lei Xu,Weidong Shi*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型处理随机性任务的能力，发现虽然LLM能生成具有一定随机性的输出，但表现不稳定且与预期行为存在显著偏差。


<details>
  <summary>Details</summary>
Motivation: 随着LLM技术的快速发展，许多应用需要处理随机性，但LLM在生成和使用随机数方面的能力尚不明确。

Method: 设计了一系列实验，考虑外部工具可访问性、任务类型、模型状态和提示策略等因素，涵盖生成随机数、随机字符串、项目洗牌以及使用熵和NIST测试套件评估随机性质量等任务。

Result: LLM生成的输出具有一定随机性，但表现不一致，经常显著偏离预期行为。

Conclusion: 实验结果表明LLM在处理涉及随机性的任务时存在关键限制，需要在这些方面进行改进才能有效处理随机性任务。

Abstract: The rapid advancement of large language model (LLM) technology has led to
diverse applications, many of which inherently require randomness, such as
stochastic decision-making, gaming, scheduling, AI agents, and
cryptography-related tasks. However, the capabilities of LLMs in handling
randomness, particularly in generating and utilizing random numbers
effectively, remain unclear. This paper investigates the capacity of LLMs for
handling tasks that involve randomness through a series of experiments. We
designed a set of experiments that consider various factors that can influence
an LLM's performance in tasks involving randomness, such as accessibility to
external tools, types of tasks, model states (fresh vs. non-fresh), and
prompting strategies. The experiments cover a range of tasks, including
generating random numbers, generating random strings such as passwords,
shuffling items, and evaluating the quality of randomness using entropy and the
NIST randomness test-suite. Our findings reveal that while LLMs can generate
outputs that exhibit some degree of randomness, their performance is
inconsistent and often deviates significantly from the expected behavior. The
analysis of the experimental results highlights key limitations and areas where
improvement is needed for the LLMs to effectively handle tasks involving
randomness

</details>


### [20] [One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration](https://arxiv.org/abs/2510.12088)
*Zaid Khan,Archiki Prasad,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.AI

TL;DR: OneLife框架通过概率编程中的条件激活程序化法则学习复杂随机环境的世界动态，在单次探索的苛刻条件下成功学习关键环境动态并优于基线方法


<details>
  <summary>Details</summary>
Motivation: 解决在复杂随机环境中，代理只有"一次生命"探索且无人指导的挑战性场景下的符号世界建模问题

Method: 使用条件激活的程序化法则建模世界动态，每个法则采用前提-效果结构，在相关世界状态中激活，形成动态计算图避免扩展问题

Result: 在23个测试场景中的16个上优于强基线方法，能够从最少无指导交互中成功学习关键环境动态，模拟推演成功识别出更优策略

Conclusion: 为自主构建未知复杂环境的程序化世界模型奠定了基础

Abstract: Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

</details>


### [21] [ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations](https://arxiv.org/abs/2510.12091)
*Lijie Ding,Jan-Michael Carrillo,Changwoo Do*

Main category: cs.AI

TL;DR: ToPolyAgent是一个多代理AI框架，通过自然语言指令执行拓扑聚合物的粗粒度分子动力学模拟，结合大语言模型和领域专用计算工具，支持线型、环形、刷状、星形聚合物及树枝状聚合物的交互式和自主模拟工作流。


<details>
  <summary>Details</summary>
Motivation: 降低复杂计算工作流的门槛，将自然语言界面与严格的模拟工具结合，推进聚合物科学中AI驱动的材料发现，为自主可扩展的多代理科学研究生态系统奠定基础。

Method: 系统包含四个LLM驱动的代理：配置代理生成初始聚合物-溶剂配置，模拟代理执行基于LAMMPS的MD模拟和构象分析，报告代理编译markdown报告，工作流代理实现流线化自主操作。支持交互式（含用户反馈循环）和自主式（端到端任务执行）两种模式。

Result: 通过案例研究展示了系统在不同聚合物架构、溶剂条件、恒温器和模拟时长下的多功能性，成功指导系统研究相互作用参数对线型聚合物构象的影响以及接枝密度对刷状聚合物持久长度的影响。

Conclusion: ToPolyAgent通过耦合自然语言界面与严格模拟工具，降低了复杂计算工作流的障碍，推进了聚合物科学中AI驱动的材料发现，为自主可扩展的多代理科学研究生态系统奠定了基础。

Abstract: We introduce ToPolyAgent, a multi-agent AI framework for performing
coarse-grained molecular dynamics (MD) simulations of topological polymers
through natural language instructions. By integrating large language models
(LLMs) with domain-specific computational tools, ToPolyAgent supports both
interactive and autonomous simulation workflows across diverse polymer
architectures, including linear, ring, brush, and star polymers, as well as
dendrimers. The system consists of four LLM-powered agents: a Config Agent for
generating initial polymer-solvent configurations, a Simulation Agent for
executing LAMMPS-based MD simulations and conformational analyses, a Report
Agent for compiling markdown reports, and a Workflow Agent for streamlined
autonomous operations. Interactive mode incorporates user feedback loops for
iterative refinements, while autonomous mode enables end-to-end task execution
from detailed prompts. We demonstrate ToPolyAgent's versatility through case
studies involving diverse polymer architectures under varying solvent
condition, thermostats, and simulation lengths. Furthermore, we highlight its
potential as a research assistant by directing it to investigate the effect of
interaction parameters on the linear polymer conformation, and the influence of
grafting density on the persistence length of the brush polymer. By coupling
natural language interfaces with rigorous simulation tools, ToPolyAgent lowers
barriers to complex computational workflows and advances AI-driven materials
discovery in polymer science. It lays the foundation for autonomous and
extensible multi-agent scientific research ecosystems.

</details>


### [22] [Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](https://arxiv.org/abs/2510.12121)
*Rongzhi Zhang,Liqin Ye,Yuzhao Heng,Xiang Chen,Tong Yu,Lingkai Kong,Sudheer Chava,Chao Zhang*

Main category: cs.AI

TL;DR: 提出了一种精确控制LLM输出属性强度的方法，将属性强度控制重新定义为目标达成问题，通过训练轻量级价值函数和梯度干预实现精确控制


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法只能提供方向性或开放式指导，无法可靠实现精确的属性强度控制，需要开发能够精确控制属性强度的AI系统以适应不同用户期望

Method: 1) 将精确属性强度控制重新定义为目标达成问题；2) 通过时序差分学习训练轻量级价值函数来预测部分生成的属性强度得分；3) 对隐藏表示进行基于梯度的干预以精确导航到特定属性强度目标

Result: 在LLaMA-3.2-3b和Phi-4-mini上的实验证实了该方法能够以高精度将文本生成引导到用户指定的属性强度

Conclusion: 该方法实现了对属性强度的细粒度连续控制，超越了简单的方向性对齐，并在偏好数据合成、帕累托前沿近似和优化以及对齐行为蒸馏等下游任务中展示了效率提升

Abstract: Precise attribute intensity control--generating Large Language Model (LLM)
outputs with specific, user-defined attribute intensities--is crucial for AI
systems adaptable to diverse user expectations. Current LLM alignment methods,
however, typically provide only directional or open-ended guidance, failing to
reliably achieve exact attribute intensities. We address this limitation with
three key designs: (1) reformulating precise attribute intensity control as a
target-reaching problem, rather than simple maximization; (2) training a
lightweight value function via temporal-difference learning to predict final
attribute intensity scores from partial generations, thereby steering LLM
outputs; and (3) employing gradient-based interventions on hidden
representations to navigate the model precisely towards specific attribute
intensity targets. Our method enables fine-grained, continuous control over
attribute intensities, moving beyond simple directional alignment. Experiments
on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text
generation to user-specified attribute intensities with high accuracy. Finally,
we demonstrate efficiency enhancements across three downstream tasks:
preference data synthesis, Pareto frontier approximation and optimization, and
distillation of aligned behaviors for intervention-free inference. Our code is
available on https://github.com/Pre-Control/pre-control

</details>


### [23] [MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science](https://arxiv.org/abs/2510.12171)
*Junkai Zhang,Jingru Gan,Xiaoxuan Wang,Zian Jia,Changquan Gu,Jianpeng Chen,Yanqiao Zhu,Mingyu Derek Ma,Dawei Zhou,Ling Li,Wei Wang*

Main category: cs.AI

TL;DR: MatSciBench是一个包含1340个问题的大学水平材料科学基准测试，涵盖6个主要领域和31个子领域，具有三级难度分类和多模态推理功能。评估显示即使是表现最好的模型准确率也不到80%。


<details>
  <summary>Details</summary>
Motivation: 填补大型语言模型在材料科学推理能力评估方面的空白，建立全面的基准测试来推动模型在该领域的进步。

Method: 构建包含1340个问题的材料科学基准测试，采用结构化分类体系，包含多模态推理，评估不同推理策略（思维链、工具增强、自我校正）的效果。

Result: 即使是表现最好的Gemini-2.5-Pro模型在材料科学问题上准确率也不到80%，没有单一推理方法在所有场景中都表现优异。

Conclusion: MatSciBench为评估和提升LLMs在材料科学领域的科学推理能力建立了全面而坚实的基准。

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities in
scientific reasoning, yet their reasoning capabilities in materials science
remain underexplored. To fill this gap, we introduce MatSciBench, a
comprehensive college-level benchmark comprising 1,340 problems that span the
essential subdisciplines of materials science. MatSciBench features a
structured and fine-grained taxonomy that categorizes materials science
questions into 6 primary fields and 31 sub-fields, and includes a three-tier
difficulty classification based on the reasoning length required to solve each
question. MatSciBench provides detailed reference solutions enabling precise
error analysis and incorporates multimodal reasoning through visual contexts in
numerous questions. Evaluations of leading models reveal that even the
highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on
college-level materials science questions, highlighting the complexity of
MatSciBench. Our systematic analysis of different reasoning strategie--basic
chain-of-thought, tool augmentation, and self-correction--demonstrates that no
single method consistently excels across all scenarios. We further analyze
performance by difficulty level, examine trade-offs between efficiency and
accuracy, highlight the challenges inherent in multimodal reasoning tasks,
analyze failure modes across LLMs and reasoning methods, and evaluate the
influence of retrieval-augmented generation. MatSciBench thus establishes a
comprehensive and solid benchmark for assessing and driving improvements in the
scientific reasoning capabilities of LLMs within the materials science domain.

</details>


### [24] [Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey](https://arxiv.org/abs/2510.12178)
*Abdulhady Abas Abdullah,Arkaitz Zubiaga,Seyedali Mirjalili,Amir H. Gandomi,Fatemeh Daneshfar,Mohammadsadra Amini,Alan Salam Mohammed,Hadi Veisi*

Main category: cs.AI

TL;DR: 本文综述了Meta AI的LLaMA系列模型（从LLaMA 1到LLaMA 4）的快速演进，以及为这些模型开发的参数高效微调（PEFT）方法，包括模型架构、性能特征和实际应用案例。


<details>
  <summary>Details</summary>
Motivation: 为机器学习研究者和从业者提供关于LLaMA模型和高效微调策略的一站式资源，帮助理解这些大型预训练模型的演进和参数高效微调方法的发展。

Method: 首先描述LLaMA系列基础模型（7B-65B到288B参数）及其架构（包括原生多模态和专家混合变体），然后介绍PEFT概念，并回顾五种应用于LLaMA的PEFT方法：LoRA、LLaMA-Adapter V1和V2、LLaMA-Excitor和QLoRA。

Result: 提供了模型和适配器架构、参数数量和基准测试结果的结构化讨论与分析，展示了微调后的LLaMA模型在某些情况下能够超越更大基线模型的性能表现。

Conclusion: 探讨了LLaMA基础模型和PEFT在实际应用中的成功案例（如法律和医疗领域），并讨论了持续面临的挑战和未来研究方向，如扩展到更大上下文和提升鲁棒性。

Abstract: This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.

</details>


### [25] [ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents](https://arxiv.org/abs/2510.12194)
*Linyi Yang,Yixuan Weng*

Main category: cs.AI

TL;DR: ResearStudio是首个开源框架，将实时人工控制置于核心，通过协作工作坊设计实现AI主导、人工辅助和人工主导、AI辅助模式之间的平滑切换。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理以'发射后不管'模式运行，用户在代理执行过程中无法纠正错误或添加专家知识。

Method: 采用分层规划器-执行器架构，将每个步骤写入实时'计划即文档'，通过快速通信层将每个动作、文件变更和工具调用流式传输到Web界面。

Result: 在完全自主模式下，ResearStudio在GAIA基准测试中达到最先进结果，超越了OpenAI的DeepResearch和Manus等系统。

Conclusion: 强大的自动化性能和细粒度人工控制可以共存，为安全和可控的研究代理开发提供了新方向。

Abstract: Current deep-research agents run in a ''fire-and-forget'' mode: once started,
they give users no way to fix errors or add expert knowledge during execution.
We present ResearStudio, the first open-source framework that places real-time
human control at its core. The system follows a Collaborative Workshop design.
A hierarchical Planner-Executor writes every step to a live
''plan-as-document,'' a fast communication layer streams each action, file
change, and tool call to a web interface. At any moment, the user can pause the
run, edit the plan or code, run custom commands, and resume -- switching
smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In
fully autonomous mode, ResearStudio achieves state-of-the-art results on the
GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These
results show that strong automated performance and fine-grained human control
can coexist. The full code, protocol, and evaluation scripts are available at
https://github.com/ResearAI/ResearStudio. We will continue to update the
repository to encourage further work on safe and controllable research agents.
Our live demo is publicly accessible at http://ai-researcher.net:3000/. We
support the development of DeepScientist, which can be accessed at
https://github.com/ResearAI/DeepScientist.

</details>


### [26] [On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic Review and Taxonomy](https://arxiv.org/abs/2510.12201)
*Aline Mangold,Juliane Zietz,Susanne Weinhold,Sebastian Pannasch*

Main category: cs.AI

TL;DR: 这篇论文对65个评估可解释AI(XAI)系统的用户研究进行了全面回顾，提出了以人为中心的XAI系统设计目标和评估框架，并根据用户AI专业水平(新手和专家)进行了差异化设计。


<details>
  <summary>Details</summary>
Motivation: 随着AI在日常生活中的普及，对既高性能又可理解的智能系统需求日益增长。当前XAI系统的评估过程过于技术化，未能充分关注人类用户的需求，因此需要建立以人为中心的评估和设计框架。

Method: 对65个跨领域XAI系统用户研究进行全面回顾分析，提出XAI系统特性和以人为中心的评估指标分类，并根据用户AI专业水平(新手和专家)制定差异化的设计目标。

Result: 发现XAI系统由核心系统和解释组件构成；评估指标可分为情感、认知、可用性、可解释性和解释指标；用户特征和行为可被评估；为AI新手设计的扩展目标包括负责任使用、接受度和可用性，为数据专家设计的重点则是性能导向，包括人机协作和系统任务性能。

Conclusion: 提出了一个扩展的XAI评估和设计框架，强调需要根据用户AI专业水平进行差异化设计，为XAI开发者提供了以人为中心的系统设计和评估指南。

Abstract: As AI becomes more common in everyday living, there is an increasing demand
for intelligent systems that are both performant and understandable.
Explainable AI (XAI) systems aim to provide comprehensible explanations of
decisions and predictions. At present, however, evaluation processes are rather
technical and not sufficiently focused on the needs of human users.
Consequently, evaluation studies involving human users can serve as a valuable
guide for conducting user studies. This paper presents a comprehensive review
of 65 user studies evaluating XAI systems across different domains and
application contexts. As a guideline for XAI developers, we provide a holistic
overview of the properties of XAI systems and evaluation metrics focused on
human users (human-centered). We propose objectives for the human-centered
design (design goals) of XAI systems. To incorporate users' specific
characteristics, design goals are adapted to users with different levels of AI
expertise (AI novices and data experts). In this regard, we provide an
extension to existing XAI evaluation and design frameworks. The first part of
our results includes the analysis of XAI system characteristics. An important
finding is the distinction between the core system and the XAI explanation,
which together form the whole system. Further results include the distinction
of evaluation metrics into affection towards the system, cognition, usability,
interpretability, and explanation metrics. Furthermore, the users, along with
their specific characteristics and behavior, can be assessed. For AI novices,
the relevant extended design goals include responsible use, acceptance, and
usability. For data experts, the focus is performance-oriented and includes
human-AI collaboration and system and user task performance.

</details>


### [27] [GOAT: A Training Framework for Goal-Oriented Agent with Tools](https://arxiv.org/abs/2510.12218)
*Hyunji Min,Sangwon Jung,Junyoung Sung,Dosung Lee,Leekyeung Han,Paul Hongsuck Seo*

Main category: cs.AI

TL;DR: GOAT是一个无需人工标注的训练框架，能够自动从API文档构建合成数据集，用于微调LLM代理处理目标导向的API执行任务。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在处理需要分解高层次目标为多个相互依赖API调用的目标导向查询时能力有限，且缺乏训练数据。开源模型在复杂工具使用方面表现不佳。

Method: 提出GOAT训练框架，自动从API文档构建合成数据集，在无需人工标注的情况下微调LLM代理，使其能够推理相互依赖的调用并生成连贯响应。

Result: GOAT训练的代理在多个现有目标导向基准测试中达到最先进性能，并在新提出的GOATBench基准测试中也表现出色。

Conclusion: GOAT为构建能够进行复杂推理和工具使用的稳健开源LLM代理提供了一条实用路径。

Abstract: Large language models (LLMs) have recently been extended beyond traditional
text generation to serve as interactive agents capable of using external tools
based on user intent. However, current LLM agents still show limited ability to
handle goal-oriented queries, which require decomposing a high-level objective
into multiple interdependent API calls with correct planning and execution.
Current approaches mainly rely on zero-shot evaluation due to the absence of
training data. While proprietary closed-source models such as GPT-4 demonstrate
strong reasoning abilities, smaller open-source models struggle to perform
complex tool use effectively. Thus, we propose a novel training framework GOAT,
which enables fine-tuning of LLM agents in a human annotation-free setting.
GOAT automatically constructs synthetic datasets of goal-oriented API execution
tasks directly from given API documents, equipping models with the ability to
reason over interdependent calls and generate coherent responses. Through
extensive experiments, we show that GOAT-trained agents achieve
state-of-the-art performance across multiple existing goal-oriented benchmarks.
In addition, we introduce GOATBench, a new goal-oriented API execution
benchmark, and demonstrate that agents trained with GOAT also excel in this
setting. These results highlight GOAT as a practical path toward building
robust open-source LLM agents capable of complex reasoning and tool use.

</details>


### [28] [MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs](https://arxiv.org/abs/2510.12224)
*Yuechun Yu,Han Ying,Haoan Jin,Wenjian Jiang,Dong Xian,Binghao Wang,Zhou Yang,Mengyue Wu*

Main category: cs.AI

TL;DR: 提出了MedKGEval框架，这是一个基于结构化医学知识的多轮临床LLM评估系统，通过知识图谱驱动的患者模拟和实时轮次评估来改进医学对话评估。


<details>
  <summary>Details</summary>
Motivation: 现有医学LLM评估方法主要依赖事后对话转录分析，忽略了医学对话的动态性和患者信息需求的演变，无法准确反映真实临床环境中的复杂交互。

Method: 1) 知识图谱驱动的患者模拟机制；2) 实时轮次评估框架，由评判代理评估临床适当性、事实准确性和安全性；3) 构建综合多轮基准测试。

Result: 在八个最先进LLM上的测试表明，MedKGEval能够识别传统评估方法忽略的细微行为缺陷和安全风险。

Conclusion: MedKGEval提供了一个有效的多轮医学LLM评估框架，支持中英文应用并可扩展到其他语言，具有领域特定的适用性。

Abstract: The reliable evaluation of large language models (LLMs) in medical
applications remains an open challenge, particularly in capturing the
complexity of multi-turn doctor-patient interactions that unfold in real
clinical environments. Existing evaluation methods typically rely on post hoc
review of full conversation transcripts, thereby neglecting the dynamic,
context-sensitive nature of medical dialogues and the evolving informational
needs of patients. In this work, we present MedKGEval, a novel multi-turn
evaluation framework for clinical LLMs grounded in structured medical
knowledge. Our approach introduces three key contributions: (1) a knowledge
graph-driven patient simulation mechanism, where a dedicated control module
retrieves relevant medical facts from a curated knowledge graph, thereby
endowing the patient agent with human-like and realistic conversational
behavior. This knowledge graph is constructed by integrating open-source
resources with additional triples extracted from expert-annotated datasets; (2)
an in-situ, turn-level evaluation framework, where each model response is
assessed by a Judge Agent for clinical appropriateness, factual correctness,
and safety as the dialogue progresses using a suite of fine-grained,
task-specific metrics; (3) a comprehensive multi-turn benchmark of eight
state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle
behavioral flaws and safety risks that are often overlooked by conventional
evaluation pipelines. Although initially designed for Chinese and English
medical applications, our framework can be readily extended to additional
languages by switching the input knowledge graphs, ensuring seamless bilingual
support and domain-specific applicability.

</details>


### [29] [PromptFlow: Training Prompts Like Neural Networks](https://arxiv.org/abs/2510.12246)
*Jingyi Wang,Hongyuan Zhu,Ye Niu,Yunhui Deng*

Main category: cs.AI

TL;DR: 提出了PromptFlow框架，通过模块化设计和元学习自动优化提示工程，减少人工干预，提升LLM在特定领域的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统提示工程依赖人工设计、缺乏动态策略选择和经验复用的问题，提高LLM在多样化NLP任务中的适应能力。

Method: 采用模块化框架，集成元提示、操作符、优化器和评估器，结合基于梯度的元学习和强化学习来复用经验，自动探索最优提示优化路径。

Result: 在多个数据集上的实验表明，PromptFlow能有效提升LLM性能，且仅需少量任务特定数据。

Conclusion: PromptFlow为自动化提示工程提供了高效解决方案，通过经验复用和动态优化策略显著提升了LLM的领域适应性。

Abstract: Large Language Models (LLMs) have demonstrated profound impact on Natural
Language Processing (NLP) tasks. However, their effective deployment across
diverse domains often require domain-specific adaptation strategies, as generic
models may underperform when faced with specialized data distributions. Recent
advances in prompt engineering (PE) offer a promising alternative to extensive
retraining by refining input instructions to align LLM outputs with task
objectives. This paradigm has emerged as a rapid and versatile approach for
model fine-tuning. Despite its potential, manual prompt design remains
labor-intensive and heavily depends on specialized expertise, often requiring
iterative human effort to achieve optimal formulations. To address this
limitation, automated prompt engineering methodologies have been developed to
systematically generate task-specific prompts. However, current implementations
predominantly employ static update rules and lack mechanisms for dynamic
strategy selection, resulting in suboptimal adaptation to varying NLP task
requirements. Furthermore, most methods treat and update the whole prompts at
each step, without considering editing prompt sections at a finer granularity.
At last, in particular, the problem of how to recycle experience in LLM is
still underexplored. To this end, we propose the PromptFlow, a modular training
framework inspired by TensorFlow, which integrates meta-prompts, operators,
optimization, and evaluator. Our framework can be equipped with the latest
optimization methods and autonomously explores optimal prompt refinement
trajectories through gradient-based meta-learning, requiring minimal
task-specific training data. Specifically, we devise a reinforcement learning
method to recycle experience for LLM in the PE process. Finally, we conduct
extensive experiments on various datasets, and demonstrate the effectiveness of
PromptFlow.

</details>


### [30] [$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning](https://arxiv.org/abs/2510.12264)
*Deyu Zou,Yongqiang Chen,Jianxiang Wang,Haochen Yang,Mufei Li,James Cheng,Pan Li,Yu Gong*

Main category: cs.AI

TL;DR: 提出T³方法，通过检测和截断过度信念偏差的轨迹来改进LLM主动推理训练，提升策略优化效果


<details>
  <summary>Details</summary>
Motivation: LLM代理在主动推理中常出现信念偏差问题，导致丢失问题状态跟踪并陷入无信息或重复行为，这会破坏强化学习训练效果

Method: 开发T³方法，跟踪模型信念偏差，检测过度偏差并截断训练轨迹，保留信息性前缀的信用分配

Result: 在5个挑战性任务中，T³持续提升训练稳定性、令牌效率和最终性能，性能提升达30%，同时减少约25%的滚动令牌

Conclusion: 信念控制是开发稳健且可泛化的LLM主动推理器的关键原则

Abstract: Active reasoning requires large language models (LLMs) to interact with
external sources and strategically gather information to solve problems.
Central to this process is belief tracking: maintaining a coherent
understanding of the problem state and the missing information toward the
solution. However, due to limited reasoning capabilities, LLM-based agents
often suffer from belief deviation: they struggle to correctly model beliefs,
lose track of problem states, and fall into uninformative or repetitive
actions. Once this happens, errors compound and reinforcement learning (RL)
training fails to properly credit the crucial exploratory steps. To address
this issue, we propose to track the deviation of model beliefs and develop
$\mathbf{T^3}$, a simple yet effective method that detects excessive belief
deviation and truncates trajectories during training to remove uninformative
tails. By preserving credit for informative prefixes, $\mathbf{T^3}$
systematically improves policy optimization. Across 5 challenging tasks,
$\mathbf{T^3}$ consistently enhances training stability, token efficiency, and
final performance, achieving up to 30% gains while cutting rollout tokens by
roughly 25%. These results highlight belief control as a key principle for
developing robust and generalizable LLM-based active reasoners.

</details>


### [31] [Tensor Logic: The Language of AI](https://arxiv.org/abs/2510.12269)
*Pedro Domingos*

Main category: cs.AI

TL;DR: 提出张量逻辑语言，统一神经AI和符号AI，通过张量方程实现逻辑规则与爱因斯坦求和的统一，解决现有AI编程语言功能不完整的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展受阻于缺乏具备所有必要特性的编程语言。PyTorch、TensorFlow等库缺乏自动推理和知识获取支持，而LISP、Prolog等AI语言缺乏可扩展性和学习支持。

Method: 设计张量逻辑语言，其唯一构造是张量方程，将逻辑规则和爱因斯坦求和视为相同操作，其他功能都可简化为这两种操作。

Result: 优雅实现了关键神经AI、符号AI和统计AI形式，包括变换器、形式推理、核机器和图模型。支持嵌入空间中的可靠推理。

Conclusion: 张量逻辑结合了神经网络的可扩展性和可学习性与符号推理的可靠性和透明度，为AI更广泛采用提供了基础。

Abstract: Progress in AI is hindered by the lack of a programming language with all the
requisite features. Libraries like PyTorch and TensorFlow provide automatic
differentiation and efficient GPU implementation, but are additions to Python,
which was never intended for AI. Their lack of support for automated reasoning
and knowledge acquisition has led to a long and costly series of hacky attempts
to tack them on. On the other hand, AI languages like LISP an Prolog lack
scalability and support for learning. This paper proposes tensor logic, a
language that solves these problems by unifying neural and symbolic AI at a
fundamental level. The sole construct in tensor logic is the tensor equation,
based on the observation that logical rules and Einstein summation are
essentially the same operation, and all else can be reduced to them. I show how
to elegantly implement key forms of neural, symbolic and statistical AI in
tensor logic, including transformers, formal reasoning, kernel machines and
graphical models. Most importantly, tensor logic makes new directions possible,
such as sound reasoning in embedding space. This combines the scalability and
learnability of neural networks with the reliability and transparency of
symbolic reasoning, and is potentially a basis for the wider adoption of AI.

</details>


### [32] [RAG-Anything: All-in-One RAG Framework](https://arxiv.org/abs/2510.12323)
*Zirui Guo,Xubin Ren,Lingrui Xu,Jiahao Zhang,Chao Huang*

Main category: cs.AI

TL;DR: RAG-Anything是一个统一的多模态检索增强生成框架，通过双图构建和跨模态混合检索，解决了现有RAG系统仅支持文本内容的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统与现实世界多模态知识环境存在严重不匹配，现代知识库包含文本、视觉、表格和数学表达式等多种模态内容，但现有RAG框架仅限于文本处理。

Method: 将多模态内容重新概念化为相互连接的知识实体，引入双图构建来捕获跨模态关系和文本语义，开发跨模态混合检索结合结构知识导航和语义匹配。

Result: 在具有挑战性的多模态基准测试中表现出优越性能，相比最先进方法有显著提升，特别是在传统方法失败的长文档处理上表现尤为突出。

Conclusion: 该框架为多模态知识访问建立了新范式，消除了当前系统的架构碎片化限制。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm
for expanding Large Language Models beyond their static training limitations.
However, a critical misalignment exists between current RAG capabilities and
real-world information environments. Modern knowledge repositories are
inherently multimodal, containing rich combinations of textual content, visual
elements, structured tables, and mathematical expressions. Yet existing RAG
frameworks are limited to textual content, creating fundamental gaps when
processing multimodal documents. We present RAG-Anything, a unified framework
that enables comprehensive knowledge retrieval across all modalities. Our
approach reconceptualizes multimodal content as interconnected knowledge
entities rather than isolated data types. The framework introduces dual-graph
construction to capture both cross-modal relationships and textual semantics
within a unified representation. We develop cross-modal hybrid retrieval that
combines structural knowledge navigation with semantic matching. This enables
effective reasoning over heterogeneous content where relevant evidence spans
multiple modalities. RAG-Anything demonstrates superior performance on
challenging multimodal benchmarks, achieving significant improvements over
state-of-the-art methods. Performance gains become particularly pronounced on
long documents where traditional approaches fail. Our framework establishes a
new paradigm for multimodal knowledge access, eliminating the architectural
fragmentation that constrains current systems. Our framework is open-sourced
at: https://github.com/HKUDS/RAG-Anything.

</details>


### [33] [O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis](https://arxiv.org/abs/2510.12350)
*Ayush Khaitan,Vijay Ganesh*

Main category: cs.AI

TL;DR: 提出了LLM+CAS框架和O-Forge工具，将前沿大语言模型与计算机代数系统结合，通过符号反馈循环生成既具创造性又经过符号验证的证明，特别针对渐近不等式问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在数学研究中验证困难的问题，探索AI如何超越竞赛数学成为专业数学家的研究工具，特别是回答Terence Tao提出的关于LLM与验证器结合能否证明复杂渐近不等式的问题。

Method: 使用LLM+CAS框架，通过上下文符号反馈循环将前沿LLM与计算机代数系统（如Mathematica）耦合：LLM建议域分解，CAS对每个部分进行公理化验证。

Result: 该框架在提出适当域分解方面表现出显著效果，能够处理涉及困难证明和适当域分解的渐近不等式问题。

Conclusion: LLM+CAS框架成功展示了AI可以超越竞赛数学，成为专业数学家的研究级工具，特别是在渐近分析领域，通过创造性建议与符号验证的结合解决了复杂数学问题。

Abstract: Large language models have recently demonstrated advanced capabilities in
solving IMO and Putnam problems; yet their role in research mathematics has
remained fairly limited. The key difficulty is verification: suggested proofs
may look plausible, but cannot be trusted without rigorous checking. We present
a framework, called LLM+CAS, and an associated tool, O-Forge, that couples
frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic
Feedback loop to produce proofs that are both creative and symbolically
verified. Our focus is on asymptotic inequalities, a topic that often involves
difficult proofs and appropriate decomposition of the domain into the "right"
subdomains. Many mathematicians, including Terry Tao, have suggested that using
AI tools to find the right decompositions can be very useful for research-level
asymptotic analysis. In this paper, we show that our framework LLM+CAS turns
out to be remarkably effective at proposing such decompositions via a
combination of a frontier LLM and a CAS. More precisely, we use an LLM to
suggest domain decomposition, and a CAS (such as Mathematica) that provides a
verification of each piece axiomatically. Using this loop, we answer a question
posed by Terence Tao: whether LLMs coupled with a verifier can be used to help
prove intricate asymptotic inequalities. More broadly, we show how AI can move
beyond contest math towards research-level tools for professional
mathematicians.

</details>


### [34] [A Survey of Vibe Coding with Large Language Models](https://arxiv.org/abs/2510.12399)
*Yuyao Ge,Lingrui Mei,Zenghao Duan,Tianhao Li,Yujia Zheng,Yiwei Wang,Lexin Wang,Jiayu Yao,Tianyu Liu,Yujun Cai,Baolong Bi,Fangda Guo,Jiafeng Guo,Shenghua Liu,Xueqi Cheng*

Main category: cs.AI

TL;DR: 这篇论文首次系统性地综述了基于大语言模型的Vibe Coding范式，建立了理论框架和实践分类，揭示了成功实施的关键要素。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，编程范式正从代码生成辅助转向自主编码代理，催生了"Vibe Coding"这一新兴开发方法。然而，这种范式的有效性尚未得到充分探索，实证研究显示存在生产力损失和人机协作挑战。

Method: 通过系统分析1000多篇研究论文，建立了Vibe Coding的理论基础（约束马尔可夫决策过程），并综合现有实践提出了五种开发模型分类。

Result: 构建了完整的Vibe Coding生态系统分析框架，包括编码LLM、基于LLM的编码代理、开发环境和反馈机制等关键组件。成功实施Vibe Coding不仅依赖代理能力，更需要系统化的上下文工程、完善的开发环境和人机协作模型。

Conclusion: Vibe Coding作为一门正式学科，其成功实施需要理论框架、实践分类和系统性方法的结合，为这一变革性开发方法提供了全面的理论基础和实践指导。

Abstract: The advancement of large language models (LLMs) has catalyzed a paradigm
shift from code generation assistance to autonomous coding agents, enabling a
novel development methodology termed "Vibe Coding" where developers validate
AI-generated implementations through outcome observation rather than
line-by-line code comprehension. Despite its transformative potential, the
effectiveness of this emergent paradigm remains under-explored, with empirical
evidence revealing unexpected productivity losses and fundamental challenges in
human-AI collaboration. To address this gap, this survey provides the first
comprehensive and systematic review of Vibe Coding with large language models,
establishing both theoretical foundations and practical frameworks for this
transformative development approach. Drawing from systematic analysis of over
1000 research papers, we survey the entire vibe coding ecosystem, examining
critical infrastructure components including LLMs for coding, LLM-based coding
agent, development environment of coding agent, and feedback mechanisms. We
first introduce Vibe Coding as a formal discipline by formalizing it through a
Constrained Markov Decision Process that captures the dynamic triadic
relationship among human developers, software projects, and coding agents.
Building upon this theoretical foundation, we then synthesize existing
practices into five distinct development models: Unconstrained Automation,
Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and
Context-Enhanced Models, thus providing the first comprehensive taxonomy in
this domain. Critically, our analysis reveals that successful Vibe Coding
depends not merely on agent capabilities but on systematic context engineering,
well-established development environments, and human-agent collaborative
development models.

</details>


### [35] [PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks](https://arxiv.org/abs/2510.12409)
*Yunuo Liu,Dawei Zhu,Zena Al-Khalili,Dai Cheng,Yanjun Chen,Dietrich Klakow,Wei Zhang,Xiaoyu Shen*

Main category: cs.AI

TL;DR: PricingLogic是首个评估大语言模型在旅游定价场景中可靠性的基准测试，包含300个基于真实定价政策的问题，测试显示LLMs在复杂规则解释和数学推理方面存在系统性失败。


<details>
  <summary>Details</summary>
Motivation: 旅游机构希望将易出错的价格计算任务自动化给AI系统，但在未经可靠性验证的情况下部署LLMs可能导致重大财务损失和客户信任流失。

Method: 构建包含300个自然语言问题的基准测试，基于42个真实定价政策，涵盖两个难度级别：基础客户类型定价和涉及交互折扣的捆绑旅游计算。

Result: 对一系列LLMs的评估显示，在较难层级上性能急剧下降，暴露出规则解释和算术推理的系统性失败。

Conclusion: 尽管LLMs具备通用能力，但在收入关键应用中仍不可靠，需要进一步的安全保障或领域适应。

Abstract: We present PricingLogic, the first benchmark that probes whether Large
Language Models(LLMs) can reliably automate tourism-related prices when
multiple, overlapping fare rules apply. Travel agencies are eager to offload
this error-prone task onto AI systems; however, deploying LLMs without verified
reliability could result in significant financial losses and erode customer
trust. PricingLogic comprises 300 natural-language questions based on booking
requests derived from 42 real-world pricing policies, spanning two levels of
difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations
involving interacting discounts. Evaluations of a line of LLMs reveal a steep
performance drop on the harder tier,exposing systematic failures in rule
interpretation and arithmetic reasoning.These results highlight that, despite
their general capabilities, today's LLMs remain unreliable in revenue-critical
applications without further safeguards or domain adaptation. Our code and
dataset are available at https://github.com/EIT-NLP/PricingLogic.

</details>


### [36] [MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics](https://arxiv.org/abs/2510.12423)
*Dingyi Zuo,Hongjie Zhang,Jie Ou,Chaosheng Feng,Shuwan Liu*

Main category: cs.AI

TL;DR: 提出了MTOS框架，将多主题环境与LLMs结合进行社会模拟，解决了现有研究在跨领域多主题认知转移方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实网络中的信息往往涉及多个相关主题，现有基于LLM的研究主要关注单一主题，无法捕捉多主题跨领域环境中的认知转移。传统数值模型将复杂语言态度简化为离散值，缺乏可解释性和行为一致性。

Method: MTOS框架整合LLMs与短期和长期记忆，采用多用户选择交互机制和动态主题选择策略，并引入信念衰减机制实现跨主题视角更新。

Result: 多主题设置显著改变极化趋势：正相关主题放大回音室效应，负相关主题抑制回音室，不相关主题通过资源竞争缓解回音室效应。相比数值模型，LLM智能体能更真实地模拟动态观点变化。

Conclusion: MTOS框架通过多主题模拟，提高了社会模拟的可解释性和系统稳定性，能够捕捉复杂的人类推理过程。

Abstract: The polarization of opinions, information segregation, and cognitive biases
on social media have attracted significant academic attention. In real-world
networks, information often spans multiple interrelated topics, posing
challenges for opinion evolution and highlighting the need for frameworks that
simulate interactions among topics. Existing studies based on large language
models (LLMs) focus largely on single topics, limiting the capture of cognitive
transfer in multi-topic, cross-domain contexts. Traditional numerical models,
meanwhile, simplify complex linguistic attitudes into discrete values, lacking
interpretability, behavioral consistency, and the ability to integrate multiple
topics. To address these issues, we propose Multi-topic Opinion Simulation
(MTOS), a social simulation framework integrating multi-topic contexts with
LLMs. MTOS leverages LLMs alongside short-term and long-term memory,
incorporates multiple user-selection interaction mechanisms and dynamic
topic-selection strategies, and employs a belief decay mechanism to enable
perspective updates across topics. We conduct extensive experiments on MTOS,
varying topic numbers, correlation types, and performing ablation studies to
assess features such as group polarization and local consistency. Results show
that multi-topic settings significantly alter polarization trends: positively
correlated topics amplify echo chambers, negatively correlated topics inhibit
them, and irrelevant topics also mitigate echo chamber effects through resource
competition. Compared with numerical models, LLM-based agents realistically
simulate dynamic opinion changes, reproduce linguistic features of news texts,
and capture complex human reasoning, improving simulation interpretability and
system stability.

</details>


### [37] [Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections](https://arxiv.org/abs/2510.12428)
*Chengyang Dong,Nan Guo*

Main category: cs.AI

TL;DR: 提出了一种基于深度强化学习(DRL)的无信号交叉口自动驾驶决策框架，结合偏置注意力机制构建交通风险预测器，通过SAC算法实现主动安全控制。


<details>
  <summary>Details</summary>
Motivation: 无信号交叉口自动驾驶决策面临复杂动态交互和高冲突风险的挑战，需要实现主动安全控制。

Method: 基于Soft Actor-Critic(SAC)算法，使用偏置注意力机制构建交通风险预测器，评估车辆进入交叉口的长期碰撞风险，并将其转化为密集奖励信号指导决策。

Result: 仿真结果表明，该方法有效提高了交叉口的交通效率和车辆安全性。

Conclusion: 该智能决策框架在复杂场景中具有有效性，证明了基于风险预测的DRL方法在无信号交叉口自动驾驶决策中的可行性。

Abstract: Autonomous driving decision-making at unsignalized intersections is highly
challenging due to complex dynamic interactions and high conflict risks. To
achieve proactive safety control, this paper proposes a deep reinforcement
learning (DRL) decision-making framework integrated with a biased attention
mechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm.
Its core innovation lies in the use of biased attention to construct a traffic
risk predictor. This predictor assesses the long-term risk of collision for a
vehicle entering the intersection and transforms this risk into a dense reward
signal to guide the SAC agent in making safe and efficient driving decisions.
Finally, the simulation results demonstrate that the proposed method
effectively improves both traffic efficiency and vehicle safety at the
intersection, thereby proving the effectiveness of the intelligent
decision-making framework in complex scenarios. The code of our work is
available at https://github.com/hank111525/SAC-RWB.

</details>


### [38] [Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems](https://arxiv.org/abs/2510.12462)
*Jiaxin Gao,Chen Chen,Yanwen Jia,Xueluan Gong,Kwok-Yan Lam,Qian Wang*

Main category: cs.AI

TL;DR: 本文系统研究了LLM作为评判者在点式评分设置中的11种偏见类型，发现最先进的LLM评判者对偏见输入具有鲁棒性，但微调于高分的偏见响应会显著降低性能，并提出了四种缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地被用于自主评估通信系统中的内容质量，这些AI"评判者"的公正性无法保证，其评估标准中的任何偏见都可能扭曲结果并损害用户信任。

Method: 系统调查了两种LLM评判模型在点式评分设置下的11种偏见类型，包括隐式和显式偏见形式，并分析了评分标准、微调数据和任务难度对评判结果的影响。

Result: 最先进的LLM评判者对偏见输入具有鲁棒性，通常给偏见样本分配较低分数；提供详细评分标准可增强鲁棒性；微调于高分偏见响应会显著降低性能；评判分数与任务难度相关。

Conclusion: LLM评判者存在多种偏见风险，需要采取缓解策略来确保在实际通信场景中的公平可靠AI评判，包括使用详细评分标准、避免在偏见数据上微调等。

Abstract: Large Language Models (LLMs) are increasingly being used to autonomously
evaluate the quality of content in communication systems, e.g., to assess
responses in telecom customer support chatbots. However, the impartiality of
these AI "judges" is not guaranteed, and any biases in their evaluation
criteria could skew outcomes and undermine user trust. In this paper, we
systematically investigate judgment biases in two LLM-as-a-judge models (i.e.,
GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11
types of biases that cover both implicit and explicit forms. We observed that
state-of-the-art LLM judges demonstrate robustness to biased inputs, generally
assigning them lower scores than the corresponding clean samples. Providing a
detailed scoring rubric further enhances this robustness. We further found that
fine-tuning an LLM on high-scoring yet biased responses can significantly
degrade its performance, highlighting the risk of training on biased data. We
also discovered that the judged scores correlate with task difficulty: a
challenging dataset like GPQA yields lower average scores, whereas an
open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.
Finally, we proposed four potential mitigation strategies to ensure fair and
reliable AI judging in practical communication scenarios.

</details>


### [39] [Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews](https://arxiv.org/abs/2510.12490)
*Rui Reis,Pedro Rangel Henriques,João Ferreira-Coimbra,Eva Oliveira,Nuno F. Rodrigues*

Main category: cs.AI

TL;DR: 开发了一个基于有向无环图的医疗问答对话框架，包含冷启动机制、自适应分支回溯、终止逻辑和自动报告生成功能，在初步评估中表现出良好的用户体验和临床工作流集成效果。


<details>
  <summary>Details</summary>
Motivation: 为医疗领域开发一个能够系统化处理临床算法和指南的任务导向对话系统，减少认知负担并支持高效报告生成。

Method: 使用有向无环图结构组织医疗问题，包含系统化流水线、基于层次聚类的冷启动机制、扩展剪枝机制、终止逻辑和自动报告合成。

Result: 患者应用：NASA-TLX=15.6（低负荷），SUS=86（高可用性），QUIS=8.1/9（高满意度）；医生应用：NASA-TLX=26（中等负荷），SUS=88.5（优秀可用性），QUIS=8.3/9。

Conclusion: 该系统有效集成到临床工作流中，降低了认知需求并支持高效报告生成，但存在系统延迟和评估样本有限等局限性。

Abstract: We developed a task-oriented dialogue framework structured as a Directed
Acyclic Graph (DAG) of medical questions. The system integrates: (1) a
systematic pipeline for transforming medical algorithms and guidelines into a
clinical question corpus; (2) a cold-start mechanism based on hierarchical
clustering to generate efficient initial questioning without prior patient
information; (3) an expand-and-prune mechanism enabling adaptive branching and
backtracking based on patient responses; (4) a termination logic to ensure
interviews end once sufficient information is gathered; and (5) automated
synthesis of doctor-friendly structured reports aligned with clinical
workflows. Human-computer interaction principles guided the design of both the
patient and physician applications. Preliminary evaluation involved five
physicians using standardized instruments: NASA-TLX (cognitive workload), the
System Usability Scale (SUS), and the Questionnaire for User Interface
Satisfaction (QUIS). The patient application achieved low workload scores
(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =
8.1/9), with particularly high ratings for ease of learning and interface
design. The physician application yielded moderate workload (NASA-TLX = 26) and
excellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both
applications demonstrated effective integration into clinical workflows,
reducing cognitive demand and supporting efficient report generation.
Limitations included occasional system latency and a small, non-diverse
evaluation sample.

</details>


### [40] [Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation](https://arxiv.org/abs/2510.12498)
*Chengpeng Hu,Calvin Yu-Chian Chen*

Main category: cs.AI

TL;DR: AIVCs旨在从多模态、多尺度测量中学习可执行的细胞状态模型。本文提出细胞状态潜在视角，通过操作符语法组织学习过程，并强调决策对齐的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要局限于单一数据集和设置，跨实验室和平台的迁移能力有限，存在数据泄漏和覆盖偏差问题，且剂量、时间和组合效应尚未系统处理。跨尺度耦合也受到限制。

Method: 提出模型无关的细胞状态潜在视角，通过操作符语法组织学习：测量、跨尺度耦合的升维/投影、以及剂量和调度的干预操作。

Result: 提出了决策对齐的评估蓝图，涵盖模态、尺度、上下文和干预，强调功能空间读出如通路活性、空间邻域和临床相关终点。

Conclusion: 建议采用操作符感知的数据设计、抗泄漏分区以及透明的校准和报告，以实现可重复的同类比较。

Abstract: Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,
decision-relevant models of cell state from multimodal, multiscale
measurements. Recent studies have introduced single-cell and spatial foundation
models, improved cross-modality alignment, scaled perturbation atlases, and
explored pathway-level readouts. Nevertheless, although held-out validation is
standard practice, evaluations remain predominantly within single datasets and
settings; evidence indicates that transport across laboratories and platforms
is often limited, that some data splits are vulnerable to leakage and coverage
bias, and that dose, time and combination effects are not yet systematically
handled. Cross-scale coupling also remains constrained, as anchors linking
molecular, cellular and tissue levels are sparse, and alignment to scientific
or clinical readouts varies across studies. We propose a model-agnostic
Cell-State Latent (CSL) perspective that organizes learning via an operator
grammar: measurement, lift/project for cross-scale coupling, and intervention
for dosing and scheduling. This view motivates a decision-aligned evaluation
blueprint across modality, scale, context and intervention, and emphasizes
function-space readouts such as pathway activity, spatial neighborhoods and
clinically relevant endpoints. We recommend operator-aware data design,
leakage-resistant partitions, and transparent calibration and reporting to
enable reproducible, like-for-like comparisons.

</details>


### [41] [ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification](https://arxiv.org/abs/2510.12534)
*Utsav Kumar Nareti,Suraj Kumar,Soumya Pandey,Soumi Chattopadhyay,Chandranath Adak*

Main category: cs.AI

TL;DR: ProtoSiTex是一个用于细粒度多标签文本分类的半可解释框架，通过双阶段交替训练策略学习语义连贯且多样化的原型，实现高性能和忠实的人类对齐解释。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的模型通常在粗粒度（句子或文档级别）操作，无法处理现实世界文本分类的多标签性质，需要更细粒度的可解释模型。

Method: 采用双阶段交替训练策略：无监督原型发现阶段学习语义连贯且多样化的原型，监督分类阶段将这些原型映射到类别标签。使用分层损失函数确保子句、句子和文档级别的一致性，并通过自适应原型和多头注意力捕获重叠和冲突语义。

Result: 在酒店评论基准数据集和两个公共基准测试（二分类和多分类）上的实验表明，ProtoSiTex实现了最先进的性能，同时提供忠实、人类对齐的解释。

Conclusion: ProtoSiTex为半可解释多标签文本分类提供了一个强大的解决方案，在保持高性能的同时提供细粒度的可解释性。

Abstract: The surge in user-generated reviews has amplified the need for interpretable
models that can provide fine-grained insights. Existing prototype-based models
offer intuitive explanations but typically operate at coarse granularity
(sentence or document level) and fail to address the multi-label nature of
real-world text classification. We propose ProtoSiTex, a semi-interpretable
framework designed for fine-grained multi-label text classification. ProtoSiTex
employs a dual-phase alternating training strategy: an unsupervised prototype
discovery phase that learns semantically coherent and diverse prototypes, and a
supervised classification phase that maps these prototypes to class labels. A
hierarchical loss function enforces consistency across sub-sentence, sentence,
and document levels, enhancing interpretability and alignment. Unlike prior
approaches, ProtoSiTex captures overlapping and conflicting semantics using
adaptive prototypes and multi-head attention. We also introduce a benchmark
dataset of hotel reviews annotated at the sub-sentence level with multiple
labels. Experiments on this dataset and two public benchmarks (binary and
multi-class) show that ProtoSiTex achieves state-of-the-art performance while
delivering faithful, human-aligned explanations, establishing it as a robust
solution for semi-interpretable multi-label text classification.

</details>


### [42] [Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings](https://arxiv.org/abs/2510.12555)
*Andries Rosseau,Raphaël Avalos,Ann Nowé*

Main category: cs.AI

TL;DR: 提出基于包容性适应度的多智能体强化学习框架，通过基因型分配和基因共享机制，模拟自然选择中的竞争与合作，在囚徒困境网络游戏中验证了与汉密尔顿法则等生物学原理的一致性。


<details>
  <summary>Details</summary>
Motivation: 受自然选择中竞争与合作力量驱动智力进化的启发，旨在开发能产生更复杂社会动态和战略智能体的多智能体系统，超越传统二元团队结构。

Method: 构建多智能体强化学习框架，每个智能体分配基因型，奖励函数基于包容性适应度概念设计，考虑基因共享，在囚徒困境网络游戏中研究社会动态。

Result: 实验结果与汉密尔顿法则等生物学原理一致，展示了基于基因相似性的合作谱系，能够产生独特的非团队社会动态，如三方关系中两人敌对但与第三方合作。

Conclusion: 基于包容性适应度的奖励机制为更先进战略和社会智能智能体的涌现提供了基础，能够模拟生物进化中的多智能体自动课程，产生策略军备竞赛。

Abstract: The competitive and cooperative forces of natural selection have driven the
evolution of intelligence for millions of years, culminating in nature's vast
biodiversity and the complexity of human minds. Inspired by this process, we
propose a novel multi-agent reinforcement learning framework where each agent
is assigned a genotype and where reward functions are modelled after the
concept of inclusive fitness. An agent's genetic material may be shared with
other agents, and our inclusive reward function naturally accounts for this. We
study the resulting social dynamics in two types of network games with
prisoner's dilemmas and find that our results align with well-established
principles from biology, such as Hamilton's rule. Furthermore, we outline how
this framework can extend to more open-ended environments with spatial and
temporal structure, finite resources, and evolving populations. We hypothesize
the emergence of an arms race of strategies, where each new strategy is a
gradual improvement over earlier adaptations of other agents, effectively
producing a multi-agent autocurriculum analogous to biological evolution. In
contrast to the binary team-based structures prevalent in earlier research, our
gene-based reward structure introduces a spectrum of cooperation ranging from
full adversity to full cooperativeness based on genetic similarity, enabling
unique non team-based social dynamics. For example, one agent having a mutual
cooperative relationship with two other agents, while the two other agents
behave adversarially towards each other. We argue that incorporating inclusive
fitness in agents provides a foundation for the emergence of more strategically
advanced and socially intelligent agents.

</details>


### [43] [HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games](https://arxiv.org/abs/2510.12563)
*Jingcong Liang,Shijun Wan,Xuehai Wu,Siyuan Wang,Yitong Li,Qianglong Chen,Duyu Tang,Zhongyu Wei*

Main category: cs.AI

TL;DR: 提出了HardcoreLogic基准测试，包含5000多个逻辑谜题，通过增加复杂性、非常规元素和不可解谜题三个维度来测试大型推理模型的鲁棒性，揭示当前模型过度依赖记忆模式而非真正推理能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注标准格式的流行谜题（如9x9数独），存在过拟合和记忆解决方案模式的风险，无法评估模型在面对非常规游戏变体时的灵活规则应用能力。

Method: 构建包含10种游戏的HardcoreLogic基准，通过三个维度系统化转换标准谜题：增加复杂性(IC)、非常规元素(UE)和不可解谜题(UP)，减少对捷径记忆的依赖。

Result: 评估显示各种大型推理模型性能显著下降，即使在现有基准上表现优异的模型也出现大幅性能下滑，表明模型严重依赖记忆的刻板模式。增加复杂性是主要困难来源，但模型在细微规则变化上也表现不佳。

Conclusion: HardcoreLogic暴露了当前大型推理模型的局限性，为推进高级逻辑推理研究建立了基准测试，表明需要开发真正理解规则而非依赖记忆的推理能力。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance on
complex tasks, including logical puzzle games that require deriving solutions
satisfying all constraints. However, whether they can flexibly apply
appropriate rules to varying conditions, particularly when faced with
non-canonical game variants, remains an open question. Existing corpora focus
on popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats
and memorization of solution patterns, which can mask deficiencies in
understanding novel rules or adapting strategies to new variants. To address
this, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles
across 10 games, designed to test the robustness of LRMs on the "long-tail" of
logical games. HardcoreLogic systematically transforms canonical puzzles
through three dimensions: Increased Complexity (IC), Uncommon Elements (UE),
and Unsolvable Puzzles (UP), reducing reliance on shortcut memorization.
Evaluations on a diverse set of LRMs reveal significant performance drops, even
for models achieving top scores on existing benchmarks, indicating heavy
reliance on memorized stereotypes. While increased complexity is the dominant
source of difficulty, models also struggle with subtle rule variations that do
not necessarily increase puzzle difficulty. Our systematic error analysis on
solvable and unsolvable puzzles further highlights gaps in genuine reasoning.
Overall, HardcoreLogic exposes the limitations of current LRMs and establishes
a benchmark for advancing high-level logical reasoning.

</details>


### [44] [Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2510.12635)
*Yuxiang Zhang,Jiangming Shu,Ye Ma,Xueyuan Lin,Shangxi Wu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出Memory-as-Action框架，将工作内存管理重新定义为可学习的内在能力，通过强化学习联合优化任务推理和内存管理，解决LLM在长视野任务中的内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长视野代理任务中面临内存限制，现有工作内存方法依赖与核心策略分离的外部启发式机制，需要更集成的方法。

Method: 提出Memory-as-Action框架，将内存管理作为统一策略中的显式编辑操作；开发Dynamic Context Policy Optimization算法，通过在内存动作点分割轨迹来解决轨迹断裂问题。

Result: 端到端联合优化不仅减少了计算消耗，还提高了任务性能，通过适应模型内在能力的上下文策展策略实现。

Conclusion: 将内存管理作为可学习的内在能力，通过端到端强化学习联合优化任务和内存管理，能够有效解决LLM在长视野任务中的内存挑战。

Abstract: Large Language Models face challenges in long-horizon agentic tasks as their
constrained memory is easily overwhelmed by distracting or irrelevant context.
Existing working memory methods typically rely on external, heuristic
mechanisms that are decoupled from the agent's core policy. In this work, we
reframe working memory management as a learnable, intrinsic capability. We
propose a novel framework, Memory-as-Action, where an agent actively manages
its working memory by executing explicit editing operations as part of a
unified policy. This formulation allows an agent, trained via reinforcement
learning, to balance memory curation against long-term task objectives under
given resource constraints. However, such memory editing actions break the
standard assumption of a continuously growing prefix in LLM interactions,
leading to what we call trajectory fractures. These non-prefix changes disrupt
the causal continuity required by standard policy gradient methods, making
those methods inapplicable. To address this, we propose a new algorithm,
Dynamic Context Policy Optimization, which enables stable end-to-end
reinforcement learning by segmenting trajectories at memory action points and
applying trajectory-level advantages to the resulting action segments. Our
results demonstrate that jointly optimizing for task reasoning and memory
management in an end-to-end fashion not only reduces overall computational
consumption but also improves task performance, driven by adaptive context
curation strategies tailored to the model's intrinsic capabilities.

</details>


### [45] [ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning](https://arxiv.org/abs/2510.12693)
*Hanyang Chen,Mark Zhao,Rui Yang,Qinwei Ma,Ke Yang,Jiarui Yao,Kangrui Wang,Hao Bai,Zhenhailong Wang,Rui Pan,Mengchao Zhang,Jose Barreiros,Aykut Onol,ChengXiang Zhai,Heng Ji,Manling Li,Huan Zhang,Tong Zhang*

Main category: cs.AI

TL;DR: ERA是一个两阶段框架，通过先验知识学习和在线强化学习，使小型视觉语言模型在具身AI任务中达到超越大型模型的性能


<details>
  <summary>Details</summary>
Motivation: 解决高性能具身AI系统依赖昂贵大型模型，而小型模型缺乏必要知识和技能的差距

Method: 两阶段框架：1)具身先验学习，从轨迹增强、环境锚定和外部知识三种数据中提取知识；2)在线强化学习，包含自总结、密集奖励塑造和回合级策略优化

Result: ERA-3B在EB-ALFRED任务上比GPT-4o提升8.4%，在EB-Manipulation任务上提升19.4%，并在未见任务上表现出强泛化能力

Conclusion: ERA为可扩展的具身智能提供了实用路径，为未来具身AI系统提供了方法论见解

Abstract: Recent advances in embodied AI highlight the potential of vision language
models (VLMs) as agents capable of perception, reasoning, and interaction in
complex environments. However, top-performing systems rely on large-scale
models that are costly to deploy, while smaller VLMs lack the necessary
knowledge and skills to succeed. To bridge this gap, we present
\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates
prior knowledge learning and online reinforcement learning (RL). The first
stage, \textit{Embodied Prior Learning}, distills foundational knowledge from
three types of data: (1) Trajectory-Augmented Priors, which enrich existing
trajectory data with structured reasoning generated by stronger models; (2)
Environment-Anchored Priors, which provide in-environment knowledge and
grounding supervision; and (3) External Knowledge Priors, which transfer
general knowledge from out-of-environment datasets. In the second stage, we
develop an online RL pipeline that builds on these priors to further enhance
agent performance. To overcome the inherent challenges in agent RL, including
long horizons, sparse rewards, and training instability, we introduce three key
designs: self-summarization for context management, dense reward shaping, and
turn-level policy optimization. Extensive experiments on both high-level
planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate
that ERA-3B surpasses both prompting-based large models and previous
training-based baselines. Specifically, it achieves overall improvements of
8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits
strong generalization to unseen tasks. Overall, ERA offers a practical path
toward scalable embodied intelligence, providing methodological insights for
future embodied AI systems.

</details>


### [46] [Multi-Agent Debate for LLM Judges with Adaptive Stability Detection](https://arxiv.org/abs/2510.12697)
*Tianyu Hu,Zhen Tan,Song Wang,Huaizhi Qu,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出多智能体辩论法官框架，通过协作推理和迭代优化提升LLM自动判断任务的准确性，相比多数投票方法更有效且保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为法官的方法依赖简单聚合（如多数投票），即使个体智能体给出正确答案也可能失败，需要更有效的协作判断机制。

Method: 建立多智能体辩论框架，智能体协作推理并迭代优化回答；引入稳定性检测机制，通过时变Beta-Binomial混合建模法官共识动态，基于分布相似性（K-S检验）实现自适应停止。

Result: 在多个基准测试和模型上的实验表明，该框架相比多数投票提高了判断准确性，同时保持了计算效率。

Conclusion: 多智能体辩论框架能有效提升LLM自动判断任务的性能，通过协作推理和自适应停止机制实现更准确和高效的判断。

Abstract: With advancements in reasoning capabilities, Large Language Models (LLMs) are
increasingly employed for automated judgment tasks. While LLMs-as-Judges offer
promise in automating evaluations, current approaches often rely on simplistic
aggregation methods (e.g., majority voting), which can fail even when
individual agents provide correct answers. To address this, we propose a
multi-agent debate judge framework where agents collaboratively reason and
iteratively refine their responses. We formalize the debate process
mathematically, analyzing agent interactions and proving that debate amplifies
correctness compared to static ensembles. To enhance efficiency, we introduce a
stability detection mechanism that models judge consensus dynamics via a
time-varying Beta-Binomial mixture, with adaptive stopping based on
distributional similarity (Kolmogorov-Smirnov test). This mechanism models the
judges' collective correct rate dynamics using a time-varying mixture of
Beta-Binomial distributions and employs an adaptive stopping criterion based on
distributional similarity (Kolmogorov-Smirnov statistic). Experiments across
multiple benchmarks and models demonstrate that our framework improves judgment
accuracy over majority voting while maintaining computational efficiency.

</details>


### [47] [Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection](https://arxiv.org/abs/2510.12713)
*Wissam Salhab,Darine Ameyed,Hamid Mcheick,Fehmi Jaafar*

Main category: cs.AI

TL;DR: 提出一种结合自监督学习和图论技术的方法，无需标记数据即可有效检测分布外样本，提升AI系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶、医疗等安全关键系统中，AI模型需要能够在面对分布外样本、对抗攻击和环境变化时保持可靠性能，避免故障带来的严重后果。

Method: 利用自监督学习从无标签数据中学习有用表示，结合图论技术更有效地识别和分类分布外样本。

Result: 与现有最先进方法相比，该方法在AUROC指标上达到了0.99的优异性能。

Conclusion: 该方法能够显著提高AI系统对分布外样本的检测能力，增强系统在安全关键应用中的鲁棒性。

Abstract: Robustness in AI systems refers to their ability to maintain reliable and
accurate performance under various conditions, including out-of-distribution
(OOD) samples, adversarial attacks, and environmental changes. This is crucial
in safety-critical systems, such as autonomous vehicles, transportation, or
healthcare, where malfunctions could have severe consequences. This paper
proposes an approach to improve OOD detection without the need of labeled data,
thereby increasing the AI systems' robustness. The proposed approach leverages
the principles of self-supervised learning, allowing the model to learn useful
representations from unlabeled data. Combined with graph-theoretical
techniques, this enables the more efficient identification and categorization
of OOD samples. Compared to existing state-of-the-art methods, this approach
achieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =
0.99.

</details>


### [48] [Clutch Control: An Attention-based Combinatorial Bandit for Efficient Mutation in JavaScript Engine Fuzzing](https://arxiv.org/abs/2510.12732)
*Myles Foley,Sergio Maffeis,Muhammad Fakhrur Rozi,Takeshi Takahashi*

Main category: cs.AI

TL;DR: CLUTCH是一种基于深度组合多臂老虎机的新型JavaScript模糊测试方法，通过注意力机制观察变长JavaScript测试用例表示，并使用Concrete Dropout动态调整探索策略，显著提高了模糊测试效率。


<details>
  <summary>Details</summary>
Motivation: 现有的JavaScript引擎模糊测试技术使用随机选择来确定突变位置，作者认为选择更好的突变目标问题适合使用可变臂数的组合多臂老虎机来解决。

Method: 提出CLUTCH深度组合多臂老虎机，使用注意力机制处理变长JavaScript测试用例表示，并通过Concrete Dropout动态适应探索策略。

Result: 与三种最先进解决方案相比，CLUTCH将有效测试用例数量和每个测试用例的覆盖率分别平均提高了20.3%和8.9%。在可变和组合设置中，CLUTCH的遗憾分别至少减少了78.1%和4.1%。

Conclusion: CLUTCH在JavaScript模糊测试中表现出更高的效率，证明了深度组合多臂老虎机在解决可变臂数组合问题上的有效性。

Abstract: JavaScript engines are widely used in web browsers, PDF readers, and
server-side applications. The rise in concern over their security has led to
the development of several targeted fuzzing techniques. However, existing
approaches use random selection to determine where to perform mutations in
JavaScript code. We postulate that the problem of selecting better mutation
targets is suitable for combinatorial bandits with a volatile number of arms.
Thus, we propose CLUTCH, a novel deep combinatorial bandit that can observe
variable length JavaScript test case representations, using an attention
mechanism from deep learning. Furthermore, using Concrete Dropout, CLUTCH can
dynamically adapt its exploration. We show that CLUTCH increases efficiency in
JavaScript fuzzing compared to three state-of-the-art solutions by increasing
the number of valid test cases and coverage-per-testcase by, respectively,
20.3% and 8.9% on average. In volatile and combinatorial settings we show that
CLUTCH outperforms state-of-the-art bandits, achieving at least 78.1% and 4.1%
less regret in volatile and combinatorial settings, respectively.

</details>


### [49] [CTRL-Rec: Controlling Recommender Systems With Natural Language](https://arxiv.org/abs/2510.12742)
*Micah Carroll,Adeline Foote,Kevin Feng,Marcus Williams,Anca Dragan,W. Bradley Knox,Smitha Milli*

Main category: cs.AI

TL;DR: CTRL-Rec是一个使用大语言模型实现自然语言控制推荐系统的方法，允许用户通过自然语言请求实时调整推荐内容。


<details>
  <summary>Details</summary>
Motivation: 当用户对推荐系统不满意时，他们通常缺乏细粒度的控制手段来改变推荐结果。

Method: 在训练阶段使用LLM模拟用户对项目的批准判断，训练嵌入模型来近似这些判断，然后将基于用户请求的预测集成到传统推荐系统的信号加权中。部署时只需为每个用户请求计算一次LLM嵌入。

Result: 在MovieLens数据集实验中，该方法在多种请求下都能实现细粒度控制。在19名Letterboxd用户的研究中，CTRL-Rec显著增强了用户的控制感和满意度。

Conclusion: CTRL-Rec能够有效实现自然语言控制的实时推荐，提升用户体验和控制感。

Abstract: When users are dissatisfied with recommendations from a recommender system,
they often lack fine-grained controls for changing them. Large language models
(LLMs) offer a solution by allowing users to guide their recommendations
through natural language requests (e.g., "I want to see respectful posts with a
different perspective than mine"). We propose a method, CTRL-Rec, that allows
for natural language control of traditional recommender systems in real-time
with computational efficiency. Specifically, at training time, we use an LLM to
simulate whether users would approve of items based on their language requests,
and we train embedding models that approximate such simulated judgments. We
then integrate these user-request-based predictions into the standard weighting
of signals that traditional recommender systems optimize. At deployment time,
we require only a single LLM embedding computation per user request, allowing
for real-time control of recommendations. In experiments with the MovieLens
dataset, our method consistently allows for fine-grained control across a
diversity of requests. In a study with 19 Letterboxd users, we find that
CTRL-Rec was positively received by users and significantly enhanced users'
sense of control and satisfaction with recommendations compared to traditional
controls.

</details>


### [50] [Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics](https://arxiv.org/abs/2510.12787)
*Marco Del Tredici,Jacob McCarran,Benjamin Breen,Javier Aspuru Mijares,Weichen Winston Yin,Jacob M. Taylor,Frank Koppens,Dirk Englund*

Main category: cs.AI

TL;DR: Ax-Prover是一个基于多智能体系统的自动定理证明工具，使用Lean语言，能够跨科学领域解决问题，支持自主或与人类专家协作模式。


<details>
  <summary>Details</summary>
Motivation: 解决科学问题需要创造性推理和严格的形式化验证，现有专业证明器难以泛化到不同领域。

Method: 通过模型上下文协议(MCP)为大型语言模型(LLMs)配备Lean工具，结合LLM的知识推理能力和形式化验证工具。

Result: 在公共数学基准测试中与最先进证明器竞争，在新引入的抽象代数和量子理论基准测试中大幅领先，展示了跨领域泛化能力。

Conclusion: 基于工具的智能体定理证明方法为跨科学领域的形式化验证提供了可泛化的方法论，并在实际应用中成功协助专家数学家完成复杂密码学定理的形式化证明。

Abstract: We present Ax-Prover, a multi-agent system for automated theorem proving in
Lean that can solve problems across diverse scientific domains and operate
either autonomously or collaboratively with human experts. To achieve this,
Ax-Prover approaches scientific problem solving through formal proof
generation, a process that demands both creative reasoning and strict syntactic
rigor. Ax-Prover meets this challenge by equipping Large Language Models
(LLMs), which provide knowledge and reasoning, with Lean tools via the Model
Context Protocol (MCP), which ensure formal correctness. To evaluate its
performance as an autonomous prover, we benchmark our approach against frontier
LLMs and specialized prover models on two public math benchmarks and on two
Lean benchmarks we introduce in the fields of abstract algebra and quantum
theory. On public datasets, Ax-Prover is competitive with state-of-the-art
provers, while it largely outperform them on the new benchmarks. This shows
that, unlike specialized systems that struggle to generalize, our tool-based
agentic theorem prover approach offers a generalizable methodology for formal
verification across diverse scientific domains. Furthermore, we demonstrate
Ax-Prover's assistant capabilities in a practical use case, showing how it
enabled an expert mathematician to formalize the proof of a complex
cryptography theorem.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [51] [Approximate Proximal Operators for Analog Compressed Sensing Using PN-junction Diode](https://arxiv.org/abs/2510.12065)
*Soma Furusawa,Taisei Kato,Ryo Hayakawa,Kazunori Hayashi*

Main category: cs.IT

TL;DR: 该论文提出使用PN结二极管的正向电压-电流特性来实现ℓ1和MCP正则化函数的近似近端算子，用于模拟压缩感知。


<details>
  <summary>Details</summary>
Motivation: 为了实现模拟压缩感知，需要找到ℓ1和MCP正则化函数的近似近端算子的实际实现方法。

Method: 使用PN结二极管的正向电压-电流特性构建电子模拟电路来实现近似近端算子，并在压缩感知中结合近端梯度方法使用。

Result: 通过计算机仿真验证了所提出的近似近端算子在稀疏重建中的性能，并考虑了模拟设备引入的加性噪声影响。

Conclusion: 提出的基于PN结二极管的近似近端算子方法在模拟压缩感知中是有效的。

Abstract: In order to realize analog compressed sensing, the paper considers
approximate proximal operators of the $\ell_1$ and minimax concave penalty
(MCP) regularization functions. Specifically, we propose to realize the
approximate functions by an electric analog circuit using forward
voltage-current (V-I) characteristics of the PN-junction diodes. To confirm the
validity of the proposed approach, we employ the proposed approximate proximal
operators for the $\ell_1$ and MCP regularization functions in compressed
sensing with the proximal gradient method. The sparse reconstruction
performance of the algorithms using the proposed approximate proximal operators
is demonstrated via computer simulations taking into account the impact of
additive noise introduced by analog devices.

</details>


### [52] [FedLoDrop: Federated LoRA with Dropout for Generalized LLM Fine-tuning](https://arxiv.org/abs/2510.12078)
*Sijing Xie,Dingzhu Wen,Changsheng You,Qimei Chen,Mehdi Bennis,Kaibin Huang*

Main category: cs.IT

TL;DR: 提出FedLoDrop框架，在联邦LoRA中对可训练矩阵的行列应用dropout，通过优化dropout率和资源分配来最小化泛化误差上界，同时满足延迟和能耗约束。


<details>
  <summary>Details</summary>
Motivation: 为了在降低训练成本的同时增强大语言模型的泛化能力，需要解决联邦学习中的过拟合问题，并应对网络边缘设备资源受限的挑战。

Method: 提出FedLoDrop框架，对联邦LoRA中的可训练矩阵行列应用dropout；建立泛化误差界和收敛性分析；提出基于分支定界法和惩罚连续凸逼近的优化算法来联合优化dropout率和资源分配。

Result: 数值结果表明所提方法能有效缓解过拟合并提高泛化能力，同时降低了通信成本。

Conclusion: FedLoDrop框架通过dropout机制和资源优化，在联邦学习环境中实现了更好的泛化性能和资源效率平衡。

Abstract: Fine-tuning (FT) large language models (LLMs) is crucial for adapting
general-purpose models to specific tasks, enhancing accuracy and relevance with
minimal resources. To further enhance generalization ability while reducing
training costs, this paper proposes Federated LoRA with Dropout (FedLoDrop), a
new framework that applies dropout to the rows and columns of the trainable
matrix in Federated LoRA. A generalization error bound and convergence analysis
under sparsity regularization are obtained, which elucidate the fundamental
trade-off between underfitting and overfitting. The error bound reveals that a
higher dropout rate increases model sparsity, thereby lowering the upper bound
of pointwise hypothesis stability (PHS). While this reduces the gap between
empirical and generalization errors, it also incurs a higher empirical error,
which, together with the gap, determines the overall generalization error. On
the other hand, though dropout reduces communication costs, deploying FedLoDrop
at the network edge still faces challenges due to limited network resources. To
address this issue, an optimization problem is formulated to minimize the upper
bound of the generalization error, by jointly optimizing the dropout rate and
resource allocation subject to the latency and per-device energy consumption
constraints. To solve this problem, a branch-and-bound (B\&B)-based method is
proposed to obtain its globally optimal solution. Moreover, to reduce the high
computational complexity of the B\&B-based method, a penalized successive
convex approximation (P-SCA)-based algorithm is proposed to efficiently obtain
its high-quality suboptimal solution. Finally, numerical results demonstrate
the effectiveness of the proposed approach in mitigating overfitting and
improving the generalization capability.

</details>


### [53] [Hybrid centralized-distributed precoding in fronthaul-constrained CF-mMIMO systems](https://arxiv.org/abs/2510.12406)
*Zahra Mobini,Hien Quoc Ngo,Ardavan Rahimian,Anvar Tukmanov,David Townend,Michail Matthaiou,Simon L. Cotton*

Main category: cs.IT

TL;DR: 提出了一种混合集中-分布式预编码策略，用于前传受限的无蜂窝大规模MIMO系统，通过动态用户分组和功率控制来优化频谱效率。


<details>
  <summary>Details</summary>
Motivation: 解决前传容量受限的无蜂窝大规模MIMO系统中频谱效率与前传需求之间的权衡问题，传统完全集中或完全分布式方法无法灵活适应不同系统配置。

Method: 将用户分为两组：一组采用集中式预编码，另一组采用分布式预编码。通过优化用户分组和功率控制来最大化总频谱效率，同时满足前传容量和接入点功率约束。

Result: 数值结果表明，所提出的混合方案在各种系统配置下均优于完全集中式和完全分布式方法，展现出更好的性能和适应性。

Conclusion: 混合集中-分布式预编码策略为前传受限的无蜂窝大规模MIMO系统提供了灵活高效的解决方案，能够根据系统需求动态调整预编码策略。

Abstract: We investigate a fronthaul-limited cell-free massive multiple-input
multiple-output (CF-mMIMO) system and propose a hybrid centralized-distributed
precoding strategy that dynamically adapts to varying fronthaul and spectral
efficiency (SE) requirements. The proposed approach divides users into two
groups: one served by centralized precoding and the other by distributed
precoding. We formulate a novel optimization problem for user grouping and
power control aimed at maximizing the sum SE, subject to fronthaul and
per-access point (AP) power constraints. To tackle the problem, we transform it
into a tractable form and propose efficient solution algorithms. Numerical
results confirm the hybrid scheme's versatility and superior performance,
consistently outperforming fully centralized and distributed approaches across
diverse system configurations.

</details>


### [54] [Phase Transitions of the Additive Uniform Noise Channel with Peak Amplitude and Cost Constraint](https://arxiv.org/abs/2510.12427)
*Jonas Stapmanns,Catarina Dias,Luke Eilers,Tobias Kühn,Jean-Pascal Pfister*

Main category: cs.IT

TL;DR: 该论文研究了在加性均匀噪声信道下，量化在什么条件下是最优的。通过分析容量实现输入分布与噪声水平、平均成本约束和成本函数曲率的关系，发现当成本函数为凹函数时，最优输入分布是离散的；当成本函数为凸函数且成本约束有效时，最优输入分布支撑整个区间。


<details>
  <summary>Details</summary>
Motivation: 探索在加性均匀噪声信道中，量化成为最优条件的理论依据，分析不同成本函数曲率和约束条件对容量实现输入分布的影响。

Method: 在峰值幅度和成本约束下，对加性均匀噪声信道进行理论分析，计算容量实现输入分布作为噪声水平、平均成本约束和成本函数曲率的函数。

Result: 发现成本函数为凹函数时，容量实现输入分布是离散的；成本函数为凸函数且成本约束有效时，输入分布支撑整个区间。对于离散最优分布的情况，推导了信道容量的解析表达式。

Conclusion: 量化在成本函数为凹函数时是最优的，而在成本函数为凸函数且约束有效时，连续输入分布是最优的。这一结果为信道编码和量化策略设计提供了理论指导。

Abstract: Under which condition is quantization optimal? We address this question in
the context of the additive uniform noise channel under peak amplitude and cost
constraints. We compute analytically the capacity-achieving input distribution
as a function of the noise level, the average cost constraint, and the
curvature of the cost function. We find that when the cost function is concave,
the capacity-achieving input distribution is discrete, whereas when the cost
function is convex and the cost constraint is active, the support of the
capacity-achieving input distribution spans the entire interval. For the cases
of a discrete capacity-achieving input distribution, we derive the analytical
expressions for the capacity of the channel.

</details>


### [55] [CoNet-Rx: Collaborative Neural Networks for OFDM Receivers](https://arxiv.org/abs/2510.12739)
*Mohanad Obeed,Ming Jian*

Main category: cs.IT

TL;DR: 提出了一种用于OFDM接收器的新型神经网络架构CoNet，通过多个小型子网络从不同角度处理信号特征，相比传统ResNet在相同复杂度下显著提升检测性能并降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的OFDM接收器模型通常从计算机视觉领域移植而来，不适合无线通信场景，存在计算资源需求高、内存占用大、推理延迟显著等问题，限制了在资源受限环境中的应用。

Method: CoNet使用多个小型ResNet或CNN子网络同时从不同角度（如捕获信道相关性和干扰模式）处理信号特征，通过交互操作（如逐元素乘法）融合子网络输出。

Result: 仿真结果显示，在相同网络规模和计算复杂度下，CoNet在误码率方面显著优于传统残差网络，并减少了推理延迟。

Conclusion: CoNet架构为资源受限环境中的OFDM接收器提供了一种高效解决方案，通过协作式网络设计实现了性能提升和延迟降低。

Abstract: Deep learning (DL) based methods for orthogonal frequency division
multiplexing (OFDM) radio receivers demonstrated higher signal detection
performance compared to the traditional receivers. However, the existing
DL-based models, usually adapted from computer vision, aren't well suited for
wireless communications. These models require high computational resources and
memory, and have significant inference delays, limiting their use in
resource-constrained settings. Additionally, reducing network size to ease
resource demands often leads to notable performance degradation. This paper
introduces collaborative networks (CoNet), a novel neural network (NN)
architecture designed for OFDM receivers. CoNet uses multiple small ResNet or
CNN subnetworks to simultaneously process signal features from different
perspectives like capturing channel correlations and interference patterns.
These subnetworks fuse their outputs through interaction operations (e.g.,
element-wise multiplication), significantly enhancing detection performance.
Simulation results show CoNet significantly outperforms traditional
architectures like residual networks (ResNets) in bit error rate (BER) and
reduces inference delay when both nets have the same size and the same
computational complexity.

</details>
