<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Fingerprinting Deep Packet Inspection Devices by Their Ambiguities](https://arxiv.org/abs/2509.09081)
*Diwen Xue,Armin Huremagic,Wayne Wang,Ram Sundara Raman,Roya Ensafi*

Main category: cs.NI

TL;DR: dMAP是一个远程测量框架，通过行为指纹识别技术来区分和聚类DPI设备，解决了DPI设备难以测量的问题。


<details>
  <summary>Details</summary>
Motivation: 全球用户面临日益严重的网络干扰（如审查、限速和拦截），DPI设备的商品化和普及使得网络运营商都能大规模干扰流量，但我们对DPI设备及其部署的了解仍然有限。

Method: 基于差分模糊测试，dMAP系统地发现、选择和部署专门探针，将DPI内部解析行为转化为外部可观察的指纹。通过解析和解释流量时的歧义性来创建可测量的DPI差异。

Result: 应用dMAP到全球DPI部署，证明其实用可行性，仅需20-40个判别性探针就能可靠区分各种DPI实现，包括主要国家审查基础设施和商业DPI产品。

Conclusion: 该指纹识别方法可推广到审查之外的其他形式的针对性干扰，为互联网上DPI的主动侦察提供了第一步。

Abstract: Users around the world face escalating network interference such as
censorship, throttling, and interception, largely driven by the commoditization
and growing availability of Deep Packet Inspection (DPI) devices. Once reserved
for a few well-resourced nation-state actors, the ability to interfere with
traffic at scale is now within reach of nearly any network operator. Despite
this proliferation, our understanding of DPIs and their deployments on the
Internet remains limited -- being network intermediary leaves DPI unresponsive
to conventional host-based scanning tools, and DPI vendors actively obscuring
their products further complicates measurement efforts.
  In this work, we present a remote measurement framework, dMAP (DPI Mapper),
that derives behavioral fingerprints for DPIs to differentiate and cluster
these otherwise indistinguishable middleboxes at scale, as a first step toward
active reconnaissance of DPIs on the Internet. Our key insight is that parsing
and interpreting traffic as network intermediaries inherently involves
ambiguities -- from under-specified protocol behaviors to differing RFC
interpretations -- forcing DPI vendors into independent implementation choices
that create measurable variance among DPIs. Based on differential fuzzing, dMAP
systematically discovers, selects, and deploys specialized probes that
translate DPI internal parsing behaviors into externally observable
fingerprints. Applying dMAP to DPI deployments globally, we demonstrate its
practical feasibility, showing that even a modest set of 20-40 discriminative
probes reliably differentiates a wide range of DPI implementations, including
major nation-state censorship infrastructures and commercial DPI products. We
discuss how our fingerprinting methodology generalizes beyond censorship to
other forms of targeted interference.

</details>


### [2] [AI Reasoning for Wireless Communications and Networking: A Survey and Perspectives](https://arxiv.org/abs/2509.09193)
*Haoxiang Luo,Yu Yan,Yanhui Bian,Wenjiao Feng,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Gang Sun,Dusit Niyato,Hongfang Yu,Abbas Jamalipour,Shiwen Mao*

Main category: cs.NI

TL;DR: 这篇论文综述了基于理性AI（特别是大语言模型）在无线通信网络中的应用，分析了从物理层到应用层的各层次挑战和解决方案，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在无线通信网络优化中并不能有效处理复杂的多步骤决策问题，需要具有结构化理性能力的AI技术来动态优化网络运行。

Method: 通过建立分类系统，对物理层、数据链路层、网络层、传输层和应用层进行逐层分析，使用LLM基于的代理结合理性、长期规划、记忆、工具使用和自主跨层控制等技术。

Result: 研究展示了AI理性方法如何提升基于AI的无线通信性能，为各层次提供了具体的挑战解决方案。

Conclusion: 本论文为整合理性技术到下一代无线网络指明了方向，通过通信与AI领域的融合，推动无线通信网络向更智能、自主的方向发展。

Abstract: Artificial Intelligence (AI) techniques play a pivotal role in optimizing
wireless communication networks. However, traditional deep learning approaches
often act as closed boxes, lacking the structured reasoning abilities needed to
tackle complex, multi-step decision problems. This survey provides a
comprehensive review and outlook of reasoning-enabled AI in wireless
communication networks, with a focus on Large Language Models (LLMs) and other
advanced reasoning paradigms. In particular, LLM-based agents can combine
reasoning with long-term planning, memory, tool utilization, and autonomous
cross-layer control to dynamically optimize network operations with minimal
human intervention. We begin by outlining the evolution of intelligent wireless
networking and the limitations of conventional AI methods. We then introduce
emerging AI reasoning techniques. Furthermore, we establish a classification
system applicable to wireless network tasks. We also present a layer-by-layer
examination for AI reasoning, covering the physical, data link, network,
transport, and application layers. For each part, we identify key challenges
and illustrate how AI reasoning methods can improve AI-based wireless
communication performance. Finally, we discuss key research directions for AI
reasoning toward future wireless communication networks. By combining insights
from both communications and AI, this survey aims to chart a path for
integrating reasoning techniques into the next-generation wireless networks.

</details>


### [3] [Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN Deployments](https://arxiv.org/abs/2509.09343)
*Mohammed M. H. Qazzaz,Abdelaziz Salama,Maryam Hafeez,Syed A. R. Zaidi*

Main category: cs.NI

TL;DR: 基于机器学习的O-RAN负载均衡与能源效率联合优化框架，通过多阈值策略平衡节能与性能，随机森林模型达到98.3%的F1综合指标


<details>
  <summary>Details</summary>
Motivation: 解决O-RAN中传统AI/机器学习方法在实现动态睡眠节能时导致的负载不均衡问题，这会影响用户设备的吞吐性能

Method: 提出一个多类分类系统，预测性地评估潜在的RU配置，将网络条件映射到三个负载均衡类别（良好均衡、中度均衡、不均衡），采用多阈值策略来满足不同的节能与性能优先级需求

Result: 使用426万个实际网络测量数据进行实验评估，随机森林模型达到了98.3%的F1-macro性能，较传统基准策略提升195%的性能收益

Conclusion: 该框架能够在O-RAN部署中有效地实现负载均衡与能源效率的联合优化，通过灵活的阈值设置满足不同运营需求，为5G网络的绿色发展提供了重要技术支撑

Abstract: Open Radio Access Network (O-RAN) architecture provides an intrinsic
capability to exploit key performance monitoring (KPM) within Radio
Intelligence Controller (RIC) to derive network optimisation through xApps.
These xApps can leverage KPM knowledge to dynamically switch on/off the
associated RUs where such a function is supported over the E2 interface.
Several existing studies employ artificial intelligence (AI)/Machine Learning
(ML) based approaches to realise such dynamic sleeping for increased energy
efficiency (EE). Nevertheless, most of these approaches rely upon offloading
user equipment (UE) to carve out a sleeping opportunity. Such an approach
inherently creates load imbalance across the network. Such load imbalance may
impact the throughput performance of offloaded UEs as they might be allocated a
lower number of physical resource blocks (PRBs). Maintaining the same PRB
allocation while addressing the EE at the network level is a challenging task.
To that end, in this article, we present a comprehensive ML-based framework for
joint optimisation of load balancing and EE for ORAN deployments. We formulate
the problem as a multi-class classification system that predictively evaluates
potential RU configurations before optimising the EE, mapping network
conditions to three load balance categories (Well Balanced, Moderately
Balanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate,
Aggressive) accommodates different operational priorities between energy
savings and performance assurance. Experimental evaluation using 4.26 million
real network measurements from simulations demonstrates that our Random Forest
model achieves 98.3% F1-macro performance, representing 195% improvement over
traditional baseline strategies.

</details>


### [4] [Toward quantum-safe scalable networks: an open, standards-aware key management framework](https://arxiv.org/abs/2509.09453)
*Ane Sanz,Asier Atutxa,David Franco,Jasone Astorga,Eduardo Jacob,Diego López*

Main category: cs.NI

TL;DR: 本文提出一种集成软件定义网络(SDN)原理的量子密码分配(QKD)网络架构，通过虚拟密钥管理系统(vKMS)和量子安全控制器(QuSeC)解决KMS识别、中继路径发现和网络可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 量子计算对通信安全构成威胁，QKD网络虽能提供无条件安全密钥，但当前技术在可扩展性和长距离实现方面仍面临挑战。便件中继节点可部分解决距离问题，但中继路径建立仍缺乏有效解决方案。

Method: 通过集成SDN原理，在每个节点中建立高层次虚拟KMS(vKMS)，并创建新的量子安全控制器(QuSeC)实体。vKMS处理用户密钥请求和管理节点内多个KMS，QuSeC基于网络拓扑和状态的高层次视图计算端到端中继路径并应用安全策略。

Result: 该方案能够有效解决QKD网络中的KMS识别、中继路径发现和可扩展性问题，提供了完整的安全分析，识别了架构的安全等级并分析了核心网络安全属性。

Conclusion: 集成SDN的QKD网络架构通过vKMS和QuSeC的创新设计，为量子安全通信网络提供了可扩展、高效的密钥管理和中继路径发现方案，有力推动量子密码技术的实际应用。

Abstract: With the advent of quantum computing, the increasing threats to security
poses a great challenge to communication networks. Recent innovations in this
field resulted in promising technologies such as Quantum Key Distribution
(QKD), which enables the generation of unconditionally secure keys,
establishing secure communications between remote nodes. Additionally, QKD
networks enable the interconnection of multinode architectures, extending the
point-to-point nature of QKD. However, due to the limitations of the current
state of technology, the scalability of QKD networks remains a challenge toward
feasible implementations. When it comes to long-distance implementations,
trusted relay nodes partially solve the distance issue through the forwarding
of the distributed keys, allowing applications that do not have a direct QKD
link to securely share key material. Even though the relay procedure itself has
been extensively studied, the establishment of the relaying node path still
lacks a solution. This paper proposes an innovative network architecture that
solves the challenges of Key Management System (KMS) identification, relay path
discovery, and scalability of QKD networks by integrating Software-Defined
Networking (SDN) principles, and establishing high-level virtual KMSs (vKMS) in
each node and creating a new entity called the Quantum Security Controller
(QuSeC). The vKMS serves the end-user key requests, managing the multiple KMSs
within the node and abstracting the user from discovering the correct KMS.
Additionally, based on the high-level view of the network topology and status,
the QuSeC serves the path discovery requests from vKMSs, computing the
end-to-end (E2E) relay path and applying security policies. The paper also
provides a security analysis of the proposal, identifying the security levels
of the architecture and analyzing the core networking security properties.

</details>


### [5] [PARROT: Portable Android Reproducible traffic Observation Tool](https://arxiv.org/abs/2509.09537)
*Andrea Jimenez-Berenguel,Celeste Campo,Marta Moure-Garrido,Carlos Garcia-Rubio,Daniel Díaz-Sanchez,Florina Almenares*

Main category: cs.NI

TL;DR: PARROT是一个可重现的移动应用流量采集系统，使用Android虚拟设备进行自动化流量捕获，支持SSL/TLS解密和多种捕获模式。通过分析2021年和2025年的数据集，发现TLSv1.3协议使用率从6.7%增至90.0%，QUIC协议采用率大幅提升，DNS通信从非加密Do53转向加密DoT协议。


<details>
  <summary>Details</summary>
Motivation: 移动安全协议的快速发展和当前数据集的有限性限制了应用流量分析研究，需要可重现的流量采集系统来支持系统性研究。

Method: 开发PARROT系统，使用Android虚拟设备进行自动化环境设置、流量记录管理和带标签的流量捕获，集成mitmproxy进行可选流量解密，支持有/无流量拦截的灵活捕获模式。

Result: 收集了80个应用的流量数据集，比较分析显示：TLSv1.3在TCP加密流量中占比从6.7%增至90.0%；所有50个常见应用都生成QUIC流量（2021年仅30个）；DNS通信从91.0%的Do53协议转变为81.1%的DoT加密协议。

Conclusion: PARROT系统为研究社区提供了可重现的应用流量捕获能力，揭示了移动应用安全协议的显著演进趋势，特别是向TLSv1.3、QUIC和加密DNS协议的迁移。

Abstract: The rapid evolution of mobile security protocols and limited availability of
current datasets constrains research in app traffic analysis. This paper
presents PARROT, a reproducible and portable traffic capture system for
systematic app traffic collection using Android Virtual Devices. The system
provides automated environment setup, configurable Android versions, traffic
recording management, and labeled captures extraction with human-in-the-loop
app interaction. PARROT integrates mitmproxy for optional traffic decryption
with automated SSL/TLS key extraction, supporting flexible capture modes with
or without traffic interception. We collected a dataset of 80 apps selected
from the MAppGraph dataset list, providing traffic captures with corresponding
SSL keys for decryption analysis. Our comparative analysis between the
MAppGraph dataset (2021) and our dataset (2025) reveals app traffic pattern
evolution across 50 common apps. Key findings include migration from TLSv1.2 to
TLSv1.3 protocol, with TLSv1.3 comprising 90.0\% of TCP encrypted traffic in
2025 compared to 6.7\% in 2021. QUIC protocol adoption increased substantially,
with all 50 common apps generating QUIC traffic under normal network conditions
compared to 30 apps in 2021. DNS communications evolved from predominantly
unencrypted Do53 protocol (91.0\% in 2021) to encrypted DoT protocol (81.1\% in
2025). The open-source PARROT system enables reproducible app traffic capture
for research community adoption and provides insights into app security
protocol evolution.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文提出了区间二型贝叶斯定理，通过保守方法处理输入区间的不一致性，并开发了将专家提供的区间编码为二型模糊隶属函数的新算法。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断假设精确输入值产生精确输出，但现实应用中专家通常提供区间范围估计，需要扩展贝叶斯定理处理区间不确定性。

Method: 开发了区间二型贝叶斯定理，使用保守方法避免输入不一致性；提出新颖算法将专家提供的区间编码为二型模糊隶属函数。

Result: 成功扩展了贝叶斯定理到区间二型版本，能够处理区间输入概率，避免了传统方法可能产生的无效输出结果。

Conclusion: 该方法为处理现实世界中不确定的专家知识提供了有效工具，扩展了贝叶斯定理在不确定环境下的应用能力。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [7] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 通过精调的LLaMA-3模型和多模态LLM技术，将游戏设计文档自动转换为Unity游戏原型，提高了代码生成的质量和效率


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助游戏开发中从设计到实现的转换问题，缩短游戏开发周期，提高开发效率

Method: 使用细调的LLaMA-3模型专门用于Unity代码生成，结合自定制Unity集成包，构建了一个从GDD解析到代码生成的端到端系统

Result: 在编译成功率、GDD遵循度、最佳实践采用和代码模块化等指标上表现优异，平均得分4.8/5.0，显著超过基准模型

Conclusion: 该框架有效解决了AI辅助游戏开发的关键问题，将LLM定位为从游戏设计到实现过渡的价值工具

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [8] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 通过多个专门的LLM模型分工合作，将自然语言描述的优化问题转换为MiniZinc模型，每个模型专门处理特定类型的全局约束，最后组装成完整模型


<details>
  <summary>Details</summary>
Motivation: 自然语言描述向MiniZinc模型的转换需要逻辑推理和约束编程专业知识，这一过程很具挑战性

Method: 采用多个专门的LLM模型以代理方式工作，按全局约束类型进行任务分解，每个模型专注于检测和生成某类约束的代码，最后由组装模型整合成完整的MiniZinc模型

Result: 初步实验显示，该框架在多个LLM上都表现出更好的性能，超过了一次性提示和思维链提示等基准方法

Conclusion: 通过任务分解和专门化模型的代理方式，可以有效降低复杂性并提高MiniZinc模型生成的准确性，未来还有较大的改进空间

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [9] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 本文提出了一种截断交叉熵（TCE）损失函数来缓解生成式AI模型在合成数据上重复训练时出现的模型崩溃问题，通过降低对高置信度预测的权重来显著延迟模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型对合成数据的依赖增加，重复训练会导致模型崩溃现象，现有缓解策略有限。研究发现模型对自生成数据的过度自信是崩溃的关键驱动因素。

Method: 提出置信度感知的损失函数TCE，在训练过程中降低高置信度预测的权重，建立了与模型崩溃缓解相关的模型无关框架。

Result: TCE显著延迟了递归训练中的模型崩溃，可将模型保真度区间延长2.3倍以上，且该方法在不同模态上都具有通用性。

Conclusion: 损失函数设计为在合成数据时代保持生成模型质量提供了简单而强大的工具。

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [10] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 本文研究了解释性AI中的不确定性解释和全局解释方法，通过结合多种概念的算法来验证其对用户信任的影响和解释性效果。


<details>
  <summary>Details</summary>
Motivation: 虽然解释性AI已成为研究热点，但不确定性解释和全局解释方面仍有缺口。研究者希望构建通用的XAI指南，并重点关注通过直观可视化方法来提高用户满意度和人类可解释性。

Method: 选择了一种能够同时涵盖不确定性、稳健性和全局XAI等多个概念的算法，并测试其调节用户信任的能力。测试了虽然复杂但能提供直观可视化理解的算法是否能够提高用户满意度和人类可解释性。

Result: 文中没有明确提供具体的实验结果数据，但描述了研究方法和目标。

Conclusion: 研究重点在于开发能够同时处理多种XAI概念的算法，并验证通过直观可视化方法提高解释性的效果，以为构建通用的XAI指南提供基础。

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [11] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于上下文条件提示的方法来解决推荐系统中的冷启动用户问题，通过优化大语言模型的指令提示和示例注入，在低数据设置下显著提升了推荐精度。


<details>
  <summary>Details</summary>
Motivation: 冷启动用户问题限制了推荐系统的有效性，因为缺乏历史行为信息。需要一种有效的方法来优化大语言模型在推荐任务中的表现，特别是在少样本设置下。

Method: 提出了上下文条件提示公式P(u, Ds)→R̂，其中u是冷启动用户档案，Ds是精选支持集，R̂是预测的项目排名列表。使用基于transformer的自回归大语言模型（BioGPT、LLaMA-2、GPT-4），采用令牌级对齐和嵌入空间正则化技术。

Result: 实验证明，最优示例注入和指令结构可以显著提高precision@k和NDCG分数。提示的及时组合不仅具有语法功能，还能直接控制注意力规模和解码器行为。

Conclusion: 基于提示的适应方法可以视为解决基于大语言模型流水线中冷启动推荐问题的一种有效途径。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [12] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 这篇论文在动态多代理协商环境中比较了人类、大语言模型和贝叶斯统计代理的表现，发现虽然绩效相近但行为差异显著


<details>
  <summary>Details</summary>
Motivation: 随着协调任务趋向自动化，需要评估不仅是代理的绩效结果，还包括其在动态多代理环境中的协商过程和行为动态

Method: 在动态协商设置中对人类(N=216)、LLM(GPT-4o, Gemini 1.5 Pro)和贝叶斯代理进行直接可比测试，捐捕绩效结果和行为动态

Result: 贝叶斯代理通过敌对性优化获得最高剩余，但拒绝率高；人类和LLM总体绩效相似，但LLM偏向保守进退交易，人类则更具战略性、风险承受和公平导向

Conclusion: 绩效平等这种常见评测标准可能隐藏了过程和对齐方面的根本性差异，这对实际部署至关重要

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [13] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 本文提出了一种用于反洗钱风险识别的机器学习流水线，在竞赛中获得了第二名，AUROC达到0.961。


<details>
  <summary>Details</summary>
Motivation: 反洗钱是金融机构的优先事项，机器学习在此领域具有巨大潜力，需要开发系统化的方法来识别高风险银行客户。

Method: 采用16步设计和统计分析流程，构建SQLite数据库，开发基于SQL的特征工程算法，集成预训练模型并提供可解释AI模块。

Result: 流水线在包含195,789个客户ID的数据集上实现了0.961的平均AUROC（标准差0.005），在竞赛中获得第二名。

Conclusion: 提出的综合方法成功构建了稳健的机器学习流水线，为金融机构的反洗钱工作提供了有效的技术解决方案。

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [14] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 这篇论文提出了一个基于神经科学原理的计算框架，用于提升人工智能系统的空间推理能力，包含六个核心模块，以缩小人类空间智能与AI系统之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的自主AI系统在空间推理能力上存在显著差距，主要限于符号和序列处理，而人类空间智能则基于多感知、空间记忆咈认知地图，能够在非结构化环境中进行灵活的上下文感知决策。缩小这一差距对于推进自主空间智能至关重要。

Method: 从计算神经科学角度分析空间神经模型，提出了基于神经科学原理的计算框架。该框架将核心生物功能映射到六个计算模块：生物启发的多模态感知、多感官整合、自我中心-绝对坐标转换、人工认知地图、空间记忆和空间推理。

Result: 论文对最新方法进行了框架导向的分析，评估了它们与各模块的相关性，并识别了阻碍更加基于神经科学的空间推理模块发展的关键空白。同时评估了新兴的测试标准和数据集，探讨了从虚拟到体现系统的应用领域。

Conclusion: 该研究为人工智能空间推理能力的发展提供了一个基于神经科学的视角和结构化路径，希望能够推动空间推理能力在动态或非结构化环境中的普适性发展，为研究社区带来改进。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [15] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 提出ProgD方法，通过动态异构图建模和渐进多尺度解码策略，解决多智能体运动预测中交互关系动态演化的问题，在多个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了多智能体交互关系的动态演化特性，无法有效处理未来运动中的不确定性

Method: 使用动态异构图进行场景建模，设计渐进式建模策略处理时空依赖关系，结合多尺度解码逐步消除未来运动不确定性

Result: 在INTERACTION多智能体预测基准排名第一，在Argoverse 2多世界预测基准也取得优异表现

Conclusion: ProgD方法通过显式建模动态交互关系和多尺度渐进解码，有效提升了多智能体运动预测的准确性和一致性

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [16] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 基于区块链的多代理协作监管架构，通过三个核心模块实现代理行为追踪、动态声誉评估和恶意行为预测


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的自主代理在金融、医疗等领域带来机遇的同时，其不可预测行为和异构能力造成了重大的监管和责任挑战

Method: 提出了一种区块链启用的层级监管架构，包含代理层、区块链数据层和监管应用层，设计了代理行为追踪中裁、动态声誉评估和恶意行为预测三个核心模块

Result: 为大规模代理生态系统建立了可信、弹性和可扩展监管机制的系统性基础

Conclusion: 该框架为多代理系统中的区块链监管提供了有效解决方案，并提出了未来研究方向

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [17] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 通过从实际Jupyter笔记本提取高质量工具基于数据分析任务，构建NbQA数据集，并使用MCTS搜索框架Jupiter提升多步推理能力，在数据分析任务上达到或超越GPT-4o的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数据科学工作流中对多步推理和工具使用的能力还有限，限制了在复杂数据分析任务上的效果。

Method: 从实际Jupyter笔记本提取标准化的任务-解决方案对，构建NbQA数据集；提出Jupiter框架，将数据分析形式化为搜索问题，使用蒙特卡罗树搜索(MCTS)生成多样化解决轨迹进行价值模型学习。

Result: Qwen2.5-7B和14B-Instruct模型在NbQA上分别解决了InfiAgent-DABench中77.82%和86.38%的任务，达到或超越了GPT-4o和先进代理框架的性能。

Conclusion: 该方法能够有效提升大语言模型在数据分析任务中的多步推理和工具使用能力，并在多种多步推理任务上展现出更好的统一性和更强的工具使用推理能力。

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [18] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 知识图构建与LLM集成的技术比较研究，评估spaCy、OpenIE和GraphRAG三种方法在问答系统中的性能表现


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理复杂长文本的主题性和整体理解方面存在限制，需要知识图来提升问答系统的深度分析能力

Method: 进行三种知识图构建方法（spaCy、Stanford CoreNLP-OpenIE、GraphRAG）的综合技术比较研究，分析它们与LLM集成的效果、可行性和适应性

Result: 实验结果显示OpenIE提供了最全面的三元组覆盖，而GraphRAG在推理能力方面表现最优

Conclusion: 讨论了各方法的优势和限制，并提供了未来改进知识图基于问答系统的研究方向

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [19] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 将MCTS轨迹重新用于基于偏好的强化学习策略优化，提出分阶段GRPO训练范式，通过树结构优势估计改善策略学习，但面临优势饱和和奖励信号崩溃等挑战


<details>
  <summary>Details</summary>
Motivation: 探索如何将传统用于训练价值或奖励模型的MCTS衍生轨迹重新用于改进基于偏好的强化学习中的策略优化，特别是在不需要价值网络的情况下实现偏好一致的策略学习

Method: 提出分阶段GRPO训练范式，使用部分揭示的MCTS rollout生成补全，引入新颖的树结构优势估计设置，产生前缀条件奖励信号

Result: 结构化优势估计可以稳定更新并更好地反映组合推理质量，但仍面临优势饱和和奖励信号崩溃等挑战

Conclusion: 提出了启发式和统计解决方案来缓解这些问题，并讨论了在分阶段或树状奖励结构下学习的开放挑战

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [20] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: LightAgent是一个轻量级但功能强大的多智能体框架，解决了现有框架在灵活性和简单性之间的权衡问题，集成了内存、工具和思维树等核心功能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，多智能体系统在各种应用场景中取得了显著进展，但在设计通用、鲁棒和高效的智能体部署平台方面仍存在重大挑战。

Method: 提出了LightAgent框架，集成了Memory (mem0)、Tools和Tree of Thought (ToT)等核心功能，同时保持极轻量级结构，是一个完全开源的解决方案。

Result: LightAgent能够无缝集成到主流聊天平台，使开发者能够轻松构建自学习智能体。

Conclusion: LightAgent有效解决了现有框架在灵活性和简单性之间的权衡问题，为多智能体系统提供了一个轻量级但功能强大的部署框架。

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [21] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 该论文研究如何为锦标赛规则中的获胜者提供经过认证的解释，通过识别最小支持子锦标赛来证明候选者为何获胜，并分析了多种常见锦标赛规则的算法复杂性和解释效果。


<details>
  <summary>Details</summary>
Motivation: 锦标赛模型广泛应用于表示候选者之间的成对优势关系，但缺乏对获胜者为何获胜的认证解释。研究旨在为各种锦标赛规则提供形式化的可解释AI方法，通过最小支持子锦标赛来证明候选者的必然获胜性。

Method: 提出最小支持概念，即候选者在其中必然获胜的最小子锦标赛。针对六种常见锦标赛规则（top cycle、uncovered set、Copeland、Borda、maximin、weighted uncovered set）进行分析，设计多项式时间算法计算最小支持（除weighted uncovered set外），并证明NP完全性。

Result: 确定了各规则最小支持的最小尺寸，为除加权未覆盖集外的所有规则提供了多项式时间算法，证明了加权未覆盖集问题的NP完全性。展示了最小支持能够产生紧凑、经过认证且直观的解释。

Conclusion: 最小支持概念为锦标赛获胜者提供了有效的认证解释框架，在可解释AI领域具有重要应用价值，能够为各种锦标赛规则生成可信且易于理解的获胜理由说明。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [22] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 研究空间协调三个维度（探索多样性、移动专业化和自适应空间邻近性）对团队在受限通信的协作搜索救援任务中绩效的影响


<details>
  <summary>Details</summary>
Motivation: 许多团队（消防员、军事、执法、应急响应）需要在没有视觉提示或大量显式通信的情况下在物理空间中协调移动，但现有研究主要关注同地同步团队或知识工作协调

Method: 分析34个四人团队（136名参与者）在搜索救援任务中的空间邻近性、分布模式和移动对齐等空间协调指标数据

Result: 空间专业化正向预测绩效，自适应空间邻近性呈现边际倒U型关系（适度适应水平最优），这些指标的时态动态能够区分高低绩效团队

Conclusion: 研究揭示了基于角色的团队工作中隐性空间协调的重要性，强调了平衡自适应策略的价值，对培训和AI辅助团队支持系统具有启示意义

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [23] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: TAM Bench是一个用于评估基于LLM的端到端机器学习代理的多样化、结构化基准测试，包含150个AutoML任务，具有浏览器自动化采集、基于排行榜的难度建模和多维度评估框架三大创新。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在任务覆盖、领域多样性、难度建模和评估严谨性方面存在不足，无法充分评估LLM代理在真实环境中的端到端机器学习工作流能力。

Method: 1) 基于浏览器自动化和LLM的任务采集系统从Kaggle等平台自动收集结构化ML挑战；2) 基于参与者数量和分数离散度的排行榜驱动难度建模机制；3) 包含性能、格式合规性、约束遵循和任务泛化的多维度评估框架。

Result: 构建了包含150个AutoML任务的基准测试，提供Lite(18任务)、Medium和Full三个不同规模的子集，其中Lite版本在模态和难度级别上平衡覆盖，适合日常基准测试和比较研究。

Conclusion: TAM Bench提供了一个全面、真实的结构化基准，能够更好地评估LLM代理在端到端机器学习任务中的能力，解决了现有基准的局限性。

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [24] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 通过集成视觉-语言模型的层状奖励函数和课程学习策略，提出了一种资源高效的深度强化学习架构，实现了更好的语义探索能力和战略性VLM查询决策。


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习方法在语义探索中的局限性，特别是小型策略的认知能力不足导致的探索效率低下和人工干预问题。

Method: 集成视觉-语言模型的常识知识，通过层状奖励函数将VLM查询模型化为专门动作，结合课程学习策略在不同复杂度级别指导学习。

Result: 实验结果显示该方法显著提高了物体发现率，能够有效导航到语义丰富区域，并学会战略性地决定何时主动获取外部环境信息。

Conclusion: 该研究提供了一种实用且可扩展的方法，将常识语义推理嵌入自主代理体，为实现完全智能自主探索提供了新方向。

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [25] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: TORSO方法通过模板导向推理，让大语言模型无需人工构建的few-shot示例就能激发内在推理能力，在多个任务上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 现有few-shot提示方法过度依赖提供的示例，限制了模型内在推理能力的发挥，且构建任务特定的few-shot提示成本高且在不同任务间存在不一致性

Method: 提出Template-Oriented Reasoning (TORSO)方法，通过模板导向的方式激发模型利用内部推理能力生成适当响应，无需人工构建的few-shot示例

Result: 实验结果表明TORSO在多样化的大语言模型基准测试中实现了强劲性能，并产生了合理的推理过程

Conclusion: TORSO提供了一种有效的方法来激发大语言模型的内在推理能力，无需依赖成本高昂的人工few-shot示例构建，在多个任务上表现优异

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [26] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 论文分析了法律领域LLM的“幽灵现象”问题，评估了RAG策略的有效性和局限性，建议采用重视真实性和可追溯性的“咨询式AI”范式


<details>
  <summary>Details</summary>
Motivation: 解决法律领域大语言模型中出现假信息的问题，探索有效的减少幽灵现象的方法

Method: 分析幽灵现象的原因和表现形式，评估RAG(retrieval-augmented generation)策略的效果和局限性，提出整体优化方案

Result: 识别了RAG策略的不足，建议一种更加重视真实性和可追溯性的咨询式AI范式

Conclusion: 解决方案不在于求优生成式模型，而在于采用能够增强专业判断而非替代人类监督的咨询式AI方法

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [27] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM是一个自演化的分布式内存框架，通过可验证写入、动态内存调度和跨域知识扩散来解决多智能体系统中内存管理的噪声积累、内存膨胀和泛化限制问题。


<details>
  <summary>Details</summary>
Motivation: 长期多智能体系统产生大量轨迹和交互历史，现有基于向量检索和分层存储的方法存在噪声积累、内存膨胀失控和跨域泛化有限的问题，需要更高效的内存管理机制。

Method: SEDM框架包含三个核心组件：基于可重现回放的可验证写入准入、根据经验效用动态排序和整合条目的自调度内存控制器、以及抽象可重用见解支持跨异构任务迁移的跨域知识扩散。

Result: 在基准数据集上的评估表明，SEDM相比强内存基线提高了推理准确性，同时减少了token开销，并且能够将从事实验证中提取的知识增强多跳推理能力。

Conclusion: SEDM作为一个可扩展和可持续的内存机制，将内存从被动存储库转变为主动自优化组件，适用于开放式的多智能体协作。

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [28] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 量子模型在组合泛化任务中表现优于经典组合模型，特别是在多热编码图像表示上取得了良好的概念验证结果


<details>
  <summary>Details</summary>
Motivation: 组合泛化是人类认知的关键能力，但当前AI工具（如视觉语言模型）缺乏这种能力。量子模型训练效率更高，有望提升组合泛化任务的性能

Method: 将组合张量模型的表示解释在希尔伯特空间中，使用变分量子电路学习这些表示。采用两种图像编码技术：多热编码（MHE）和基于CLIP模型的角/幅度编码

Result: 使用噪声MHE编码获得了良好的概念验证结果。在CLIP图像向量上的表现较为复杂，但仍优于经典组合模型

Conclusion: 量子模型在组合泛化任务中展现出潜力，特别是在特定编码方式下表现优异，为量子AI在复杂认知任务中的应用提供了可能性

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [29] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: Auras是一个算法-系统协同设计的推理框架，通过解耦感知和生成模块并实现管道并行化，显著提升具身AI系统的推理频率和吞吐量，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 传统顺序计算模式在处理高频输入输出需求时存在频率限制，无法满足具身AI系统在动态环境中的实时应用需求。

Method: 将感知和生成模块解耦，提供受控的管道并行化，建立共享的公共上下文来解决数据陈旧性问题。

Result: 平均吞吐量提升2.54倍，同时达到原始准确率的102.7%。

Conclusion: Auras框架有效克服了顺序计算的限制，为具身AI系统提供了高吞吐量的推理能力。

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [30] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 大语言模型在长任务执行中存在自我条件化效应，即模型在上下文中看到自己的错误后更容易出错，而续扩大模型规模并不能解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在长距离任务中的执行能力，解释为什么LLM能处理复杂推理问题却在简单但长任务中失败，并探索模型扩缩是否带来逐步收益减少。

Method: 通过明确提供知识和计划来隔离执行能力，分析不同规模模型在长任务中的每步准确率变化，对比传统LLM与思维模型的表现。

Result: 发现模型执行能力存在自我条件化效应，上下文中的错误会导致后续更多错误，而模型扩缩并不能消除这种效应。思维模型能在单轮中执行更长任务且不会自我条件化。

Conclusion: 重点关注执行能力能够解释LLM在长任务中的表现问题，扩缩模型规模和增加序列测试时计算对长距离任务有重大收益。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [31] [Improved Receiver Chain Performance via Error Location Inference](https://arxiv.org/abs/2509.08869)
*Michael Greenwood,Robert Hunter*

Main category: cs.IT

TL;DR: 使用机器学习模型预测字节级坏死概率，通过标记删除来提升RS解码效果，在不改变载具硬件的情况下获得0.3dB收益


<details>
  <summary>Details</summary>
Motivation: 现代宇航通信系统依赖于卷积码和里德-索羅问码的约束编码方案，需要在不改变硬件和编码标准的前提下提升数据恢复能力

Method: 在解码端使用机器学习模型估计接收数据帧中字节级坏死的概率，并将这些估计用于在RS解码前标记删除

Result: 方法在信号条件恶化时能够提高数据恢复能力，获得0.3分贝的性能收益

Conclusion: 该机器学习基于的删除标记方法为宇航通信系统提供了一种无需硬件改造的性能提升方案

Abstract: Modern spacecraft communication systems rely on concatenated error correction
schemes, typically combining convolutional and Reed-Solomon (RS) codes. This
paper presents a decoder-side method that uses a machine learning model to
estimate the likelihood of byte-level corruption in received data frames. These
estimates are used to mark erasures prior to RS decoding, enhancing its
correction capacity without requiring changes to spacecraft hardware or
encoding standards. The approach enables improved data recovery under degraded
signal conditions at a gain of 0.3 decibels.

</details>


### [32] [Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?](https://arxiv.org/abs/2509.09411)
*Rui Xu,Yinghui Ye,Xiaoli Chu,Guangyue Lu,Farshad Rostami Ghadi,Kai-Kit Wong*

Main category: cs.IT

TL;DR: 本文探讨了在流体天线系统(FAS)中使用颜色相关矩阵替代系数相关矩阵来评估断询性能的优势，并在Nakagami-m衰落下验证了其更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 之前的研究使用Jake模型的频道系数相关矩阵来近似高斯偶合的协方差矩阵，但因为多元正态随机变量是通过变换相关频道包络生成的，所以使用频道包络相关矩阵可能更为准确。

Method: 在完全相关的Nakagami-m衰落条件下，研究了使用频道包络相关矩阵的优势，并开发了一种生成这种衰落频道的方法用于Monte Carlo模拟，作为验证理论结果的基准。

Result: 模拟结果证实了所提出频道建模方法的有效性，并证明了使用频道包络相关矩阵具有更高的准确性，尤其在稀疏端口部署和低断询区域。

Conclusion: 使用频道包络相关矩阵比使用频道系数相关矩阵能够提供更准确的FAS性能评估，这一方法在实际应用中具有重要意义。

Abstract: Gaussian copula has been employed to evaluate the outage performance of Fluid
Antenna Systems (FAS), with the covariance matrix reflecting the dependence
among multivariate normal random variables (RVs). While prior studies
approximate this matrix using the channel coefficient correlation matrix from
Jake's model, this work instead employs the channel envelope correlation
matrix, motivated by the fact that the multivariate normal RVs are generated by
transforming correlated channel envelopes. This raises an open question of
whether using the coefficient- or envelope-level correlation matrix yields
better accuracy in accessing FAS performance. Toward this end, this paper
explores the benefits of using the envelope-level correlation matrix under
fully correlated Nakagami-m fading, and develops a method for generating such
fading channels for Monte Carlo simulations, which serve as a benchmark for
validating the theoretical results. Simulation results confirm the
effectiveness of the proposed channel modeling approach and demonstrate the
superior accuracy of using the envelope-level correlation matrix, particularly
in sparse port deployment and low-outage regime.

</details>


### [33] [Mixture of Semantics Transmission for Generative AI-Enabled Semantic Communication Systems](https://arxiv.org/abs/2509.09499)
*Junjie Ni,Tong Wu,Zhiyong Chen,Yin Xu,Meixia Tao,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出基于生成式AI的语义混合(MoS)传输策略，将图像分为ROI和RONI区域分别处理语义信息，通过扩散模型重建图像，实现信道资源的高效利用


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式AI的语义通信方法在信道资源利用效率方面存在不足，需要在视觉保真度和语义相关性之间取得更好平衡

Method: 在发送端将图像分为ROI和RONI区域分别提取语义信息，ROI分配更多带宽，RONI采用紧凑表示；接收端使用扩散模型基于接收到的语义信息重建完整图像

Result: 实验结果表明适当的ROI-RONI分配至关重要，MoS在ROI的PSNR和RONI的CLIP分数方面取得了显著的性能提升

Conclusion: MoS策略通过语义信息的分层处理实现了更高效的信道资源利用，在保持语义相关性的同时提升了视觉质量

Abstract: In this paper, we propose a mixture of semantics (MoS) transmission strategy
for wireless semantic communication systems based on generative artificial
intelligence (AI). At the transmitter, we divide an image into regions of
interest (ROI) and reigons of non-interest (RONI) to extract their semantic
information respectively. Semantic information of ROI can be allocated more
bandwidth, while RONI can be represented in a compact form for transmission. At
the receiver, a diffusion model reconstructs the full image using the received
semantic information of ROI and RONI. Compared to existing generative AI-based
methods, MoS enables more efficient use of channel resources by balancing
visual fidelity and semantic relevance. Experimental results demonstrate that
appropriate ROI-RONI allocation is critical. The MoS achieves notable
performance gains in peak signal-to-noise ratio (PSNR) of ROI and CLIP score of
RONI.

</details>


### [34] [Fast Polarisation-Aware Decoder for Non-Binary Polar Codes](https://arxiv.org/abs/2509.09554)
*Joseph Jabbour,Ali Chamas Al-Ghouwayel,Emmanuel Boutillon*

Main category: cs.IT

TL;DR: 非二进制极化码低复杂度解码器研究，通过核定制和计算负载优化，实现了显著的复杂度降低而性能损失很小


<details>
  <summary>Details</summary>
Motivation: 优化非二进制极化码解码器的复杂度，提高解码效率，减少计算资源消耗

Method: 提出FSC-PA算法，通过离线分析定制每个核，最小化具有相同输入极化水平的奇偶检查节点的计算负载

Result: 与现有最优扩展最小和算法相比，FSC-PA算法实现了域加法减少60%和实数加法减少30%，性能损失仅为0.2dB以下

Conclusion: FSC-PA算法通过核定制设计有效降低了非二进制极化码解码的复杂度，在保持良好性能的同时显著提高了解码效率

Abstract: The paper investigates the emerging field of low-complexity non-binary polar
code (NB-PC) decoders. It shows that customizing each kernel of an NB-PC
decoder through offline analysis can significantly reduce the overall decoding
complexity. The proposed decoder, referred to as the Fast Successive
Cancellation-Polarization Aware (FSC-PA) scheme, achieves this by minimizing
the computational load of parity-check nodes that share the same level of input
polarization. The NB polar decoder is developed for both BPSK and CCSK
modulations. Compared to the state-of-the-art extended min-sum algorithm, the
FSC-PA algorithm achieves an overall reduction of 60 percents in field
additions and 30 percents in real additions, while incurring only a negligible
performance loss (less than 0.2 dB degradation).

</details>


### [35] [RSMA-Enhanced Data Collection in RIS-Assisted Intelligent Consumer Transportation Systems](https://arxiv.org/abs/2509.09644)
*Chunjie Wang,Xuhui Zhang,Wenchao Liu,Jinke Ren,Shuqiang Wang,Yanyan Shen,Kejiang Ye,Kim Fung Tsang*

Main category: cs.IT

TL;DR: 提出RIS赋能的智能交通系统数据收集增强框架，通过联合优化RIS相移、功率分配、计算资源和时隙分配，最大化最小处理数据量


<details>
  <summary>Details</summary>
Motivation: 解决智能交通系统中数据收集和处理效率问题，利用RIS技术增强通信性能，提高交通数据的处理能力

Method: 采用混合RSMA和TDMA协议，提出交替优化和序列秩一约束松弛的迭代算法，联合优化RIS相移、发射功率、计算资源和时隙分配

Result: 仿真结果表明所提算法在不同场景下显著优于基线方案，有效提升了智能交通应用的数据处理性能

Conclusion: 该框架和算法能够有效增强智能交通系统的数据收集和处理能力，为智能交通应用提供了有效的解决方案

Abstract: This paper investigates the data collection enhancement problem in a
reconfigurable intelligent surface (RIS)-empowered intelligent consumer
transportation system (ICTS). We propose a novel framework where a data center
(DC) provides energy to pre-configured roadside unit (RSU) pairs during the
downlink stage. While in the uplink stage, these RSU pairs utilize a hybrid
rate-splitting multiple access (RSMA) and time-division multiple access (TDMA)
protocol to transmit the processed data to the DC, while simultaneously
performing local data processing using the harvested energy. Our objective is
to maximize the minimal processed data volume of the RSU pairs by jointly
optimizing the RIS downlink and uplink phase shifts, the transmit power of the
DC and RSUs, the RSU computation resource allocation, and the time slot
allocation. To address the formulated non-convex problem, we develop an
efficient iterative algorithm integrating alternating optimization and
sequential rank-one constraint relaxation methods. Extensive simulations
demonstrate that the proposed algorithm significantly outperforms baseline
schemes under diverse scenarios, validating its effectiveness in enhancing the
data processing performance for intelligent transportation applications.

</details>
