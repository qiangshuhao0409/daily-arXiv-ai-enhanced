<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Millimeter-Wave UAV Channel Model with Height-Dependent Path Loss and Shadowing in Urban Scenarios](https://arxiv.org/abs/2511.10763)
*Abdul Saboor,Evgenii Vinogradov*

Main category: cs.NI

TL;DR: 本文提出了一个无人机空中基站高度相关的毫米波信道模型，研究了城市几何布局对视线概率和大尺度衰落的影响，发现即使建筑参数相同，几何布局仍会引入约±0.2的路径损耗指数变化。


<details>
  <summary>Details</summary>
Motivation: 无人机作为空中基站可扩展6G毫米波覆盖范围并提高链路可靠性，但空对地信道高度依赖无人机高度和城市几何结构。需要研究城市几何布局（超出标准建筑参数）是否显著影响视线概率和大尺度衰落。

Method: 使用MATLAB射线追踪在26GHz频段模拟约10,000个城市场景，涵盖四种具有相同建筑参数但空间组织不同的城市布局。采用sigmoid模型提取基于高度的视线概率，使用指数拟合推导高度相关的路径损耗指数和阴影衰落趋势。

Result: 结果显示：非视线路径损耗指数在高海拔时降至2.5-3，视线路径损耗指数保持在2附近，阴影衰落随高度减小。几何布局即使建筑参数固定也会引入约±0.2的路径损耗指数变化。

Conclusion: 提出的统一模型与射线追踪统计结果吻合良好，为复杂城市场景中的空中基站规划提供了实用的高度相关大尺度衰落模型。

Abstract: Uncrewed Aerial Vehicles (UAVs) serving as Aerial Base Stations (ABSs) are expected to extend 6G millimeter-Wave (mmWave) coverage and improve link reliability in urban areas. However, UAV-based Air-to-Ground (A2G) channels are highly dependent on height and urban geometry. This paper proposes an ABS height-dependent mmWave channel model and investigates whether urban geometry, beyond the standard built-up parameters, significantly affects LoS probability (PLoS) and Large-Scale Fading (LSF). Using MATLAB ray tracing at 26 GHz, we simulate approximately 10K city realizations for four urban layouts that share identical built-up parameters but differ in their spatial organization. We extract elevation-based PLoS using a sigmoid model and derive height-dependent Path-Loss Exponents (PLEs) and shadow-fading trends using exponential fits. Results show that PLE for Non-Line-of-Sight (NLoS) decreases toward 2.5-3 at high altitudes, Line-of-Sight (LoS) PLE remains near 2, and shadow fading reduces with height. We also find that geometric layout introduces a modest but consistent change in PLE (+/- 0.2), even when built-up parameters are fixed. The proposed unified model aligns well with ray-tracing statistics and offers a practical, height-dependent LSF model suitable for ABS planning in complex urban scenarios.

</details>


### [2] [Advancing IoT System Dependability: A Deep Dive into Management and Operation Plane Separation](https://arxiv.org/abs/2511.11204)
*Luoyao Hao,Shuo Zhang,Henning Schulzrinne*

Main category: cs.NI

TL;DR: 提出通过分离管理和操作平面来增强大规模物联网系统的可靠性，管理平面强制执行总体策略，而操作工作流保持不变。


<details>
  <summary>Details</summary>
Motivation: 增强大规模物联网系统的可靠性，通过分离管理平面来强制执行安全规范、操作标准和能源限制等总体策略。

Method: 创新管理平面，采用身份无关的策略框架，使用灵活的描述符而非固定标识符，允许主动部署具有系统变化适应性的总体策略。

Result: 在三个数据集上的评估表明，所提出的框架可以实现接近最优的表达能力和可靠的策略执行。

Conclusion: 通过分离管理和操作平面，并采用身份无关的策略框架，能够有效增强大规模物联网系统的可靠性。

Abstract: We propose to enhance the dependability of large-scale IoT systems by separating the management and operation plane. We innovate the management plane to enforce overarching policies, such as safety norms, operation standards, and energy restrictions, and integrate multi-faceted management entities, including regulatory agencies and manufacturers, while the current IoT operational workflow remains unchanged. Central to the management plane is a meticulously designed, identity-independent policy framework that employs flexible descriptors rather than fixed identifiers, allowing for proactive deployment of overarching policies with adaptability to system changes. Our evaluation across three datasets indicates that the proposed framework can achieve near-optimal expressiveness and dependable policy enforcement.

</details>


### [3] [Use Cases, Metrics, and Challenges of Nomadic Non-Public Networks for the 6G Standardization](https://arxiv.org/abs/2511.11217)
*Daniel Lindenschmitt,Michael Gundall,Ainur Daurembekova,Marcos Rates Crippa,Mohammad Asif Habibi,Bin Han,Philipp Rosemann,Dennis Krummacker,Benedikt Veith,Hans D. Schotten*

Main category: cs.NI

TL;DR: 本文探讨了从5G非公共网络(NPNs)向游牧非公共网络(NNPNs)的演进，分析了NNPNs在6G系统中的架构、资源分配、无线回程等关键技术，以及其在应急响应、交通、农业等领域的应用，同时指出了架构、监管和安全方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统静态基础设施存在局限性，需要发展动态自组织网络来支持6G系统。NNPNs通过增强移动性和适应性，能够克服固定位置网络的限制，满足应急响应、交通、农业等多样化场景的连接需求。

Method: 通过分析网络架构、动态资源分配和无线回程等关键技术，定义关键性能指标(KPIs)来评估NNPN应用，并建立基于移动性和操作需求的分类框架，同时识别技术、监管和安全挑战。

Result: 建立了NNPNs的分类框架和评估标准，识别了多个应用场景，明确了NNPNs在提升连接性方面的优势，同时也揭示了切换机制、频谱政策、跨境功能和信任机制等挑战。

Conclusion: NNPNs是6G网络的重要组成部分，需要适应性政策和网络架构来最大化其效益。本文的研究成果有助于标准化工作，强调了解决技术、监管和安全挑战的必要性。

Abstract: Wireless communication is evolving with the adoption of dynamic and self-organizing networks. They are expected to play a crucial role in shaping sixth-generation (6G) systems and the ongoing standardization process. The concept of non-public networks (NPNs) introduced in fifth-generation (5G) will be enhanced by nomadic non-public networks (NNPNs), extending mobility and adaptability beyond fixed locations. These networks help overcome the limitations of traditional static infrastructures, making them applicable to areas such as emergency response, transportation, agriculture, and others. This paper examines the transition from NPNs to NNPNs, highlighting key technical aspects such as network architecture, dynamic resource allocation, and wireless backhauling. Several use cases illustrate how NNPNs improve connectivity in environments where traditional networks are limited. Additionally, the study defines Key Performance Indicators (KPIs) to evaluate NNPN applications and establishes a framework for categorizing them based on mobility and operational requirements. Despite their advantages, NNPNs introduce architectural, regulatory, and security challenges such as new approaches for handovers, spectrum policies or cross-border functionality, and trust mechanisms to maintain reliable operations. By identifying use cases, defining evaluation criteria, and addressing technical and regulatory challenges, this paper provides insights into integrating NNPNs into future 6G networks. These findings contribute to ongoing standardization efforts and emphasize the need for adaptable policies and network architectures to maximize the benefits of NNPNs in next-generation communication systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: 该论文提出了AI伦理熵的概念，类比热力学第二定律，证明未经约束的AI系统会自发偏离目标，需要持续的对齐工作来维持稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在训练后容易偏离原始目标，存在规范博弈和探索噪声等问题，需要建立定量框架来理解和控制这种偏离现象。

Method: 定义了基于目标概率分布的伦理熵S = -Σ p(g_i; theta) ln p(g_i; theta)，证明其时间导数dS/dt >= 0，并推导了对齐工作的临界稳定性边界gamma_crit = (lambda_max / 2) ln N。

Result: 实验验证：70亿参数模型从未对齐的初始熵0.32漂移到1.69±1.08纳特，而使用gamma=20.4（1.5倍临界值）对齐的系统保持稳定在0.00±0.00纳特（p=4.19×10^-17）。

Conclusion: 该框架将AI对齐重新表述为连续热力学控制问题，为维护先进自主系统的稳定性和安全性提供了定量基础。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>


### [5] [Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents](https://arxiv.org/abs/2511.10705)
*Yuan Zhao,Hualei Zhu,Tingyu Jiang,Shen Li,Xiaohang Xu,Hao Henry Wang*

Main category: cs.AI

TL;DR: 提出了Co-EPG框架，通过规划模型和基础模型的协同进化来解决GUI任务自动化中的挑战，在无需外部数据的情况下实现自我迭代优化。


<details>
  <summary>Details</summary>
Motivation: 当前GUI智能体存在两个主要局限：(1) 未能充分利用跨模型协同效应，(2) 过度依赖合成数据生成但利用不足。需要一种能够自我迭代优化的训练范式。

Method: Co-EPG框架建立规划模型和基础模型的迭代正反馈循环：规划模型通过GRPO在基础模型奖励指导下探索策略，生成多样化数据优化基础模型；优化后的基础模型为规划模型提供更有效的奖励，实现持续改进。

Result: 在Multimodal-Mind2Web和AndroidControl基准测试中，仅经过三次迭代就超越了现有最优方法，且无需外部数据。智能体在每次迭代中持续改进，展现出强大的自我增强能力。

Conclusion: 这项工作为GUI智能体建立了新的训练范式，从孤立优化转向集成、自驱动的协同进化方法。

Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

</details>


### [6] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 本文提出了一种新的多目标优化质量度量方法——定向覆盖率，用于解决帕累托剪枝问题，并通过公理分析、计算复杂性研究和实验评估验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多目标决策问题中，帕累托最优解数量庞大，决策者难以选择。现有帕累托剪枝方法的质量度量存在反直觉行为，需要更合理的度量标准。

Method: 将帕累托剪枝问题重新定义为多赢家投票问题，进行公理分析；引入新的定向覆盖率度量；分析不同质量度量的计算复杂性；进行实验评估比较。

Result: 发现现有质量度量存在反直觉行为；定向覆盖率在多种设置下表现竞争力强或更优；确定了质量度量优化问题的计算复杂性边界。

Conclusion: 质量度量的选择对所选解集特性有决定性影响，提出的定向覆盖率度量在各种设置下表现优异，为多目标决策提供了更有效的工具。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [7] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文研究了图参数clique-width在(Q)SAT编码中的应用，特别针对抽象论证框架设计了保持clique-width线性的归约方法。


<details>
  <summary>Details</summary>
Motivation: 虽然现代SAT求解器在小treewidth实例上表现高效，但对于更一般的图参数clique-width的编码能力了解甚少。抽象论证框架作为基于有向图的推理框架，是研究计算性质的理想候选。

Method: 设计了从论证问题到(Q)SAT的新型归约方法，这些归约线性保持clique-width，称为有向分解引导(DDG)归约。

Result: 为所有论证语义(包括计数)建立了新结果，证明DDG归约的开销在合理假设下无法显著改进。

Conclusion: 本文开启了理解clique-width编码能力的研究，为基于clique-width的高效(Q)SAT求解提供了理论基础。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [8] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 提出了两种新的反事实决策指标：潜在结果排序概率(PoR)和获得最佳潜在结果概率(PoB)，建立了识别定理和边界推导，并开发了估计方法。


<details>
  <summary>Details</summary>
Motivation: 在不确定性下进行反事实决策时，决策者通常通过比较候选行动的期望潜在结果来做出选择。本文旨在开发新的决策规则来改进这一过程。

Method: 引入PoR和PoB两个新指标，建立识别定理和边界推导，提出估计方法，并进行数值实验和真实数据集应用验证。

Result: 通过数值实验验证了估计器的有限样本性质，并在真实数据集中展示了这些指标的实际应用价值。

Conclusion: PoR和PoB为反事实决策提供了新的有效工具，能够帮助决策者更准确地评估和比较不同行动的效果。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [9] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 该调查从自适应性的角度重新审视LLM推理，强调根据任务难度和不确定性动态分配推理努力的重要性，提出了训练式和训练无关方法实现自适应推理的分类框架。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对所有任务采用统一的推理策略，无法根据任务复杂度调整推理深度，导致对简单问题过度推理而对困难问题推理不足。

Method: 将自适应推理形式化为控制增强的策略优化问题，提出系统分类法：训练式方法（强化学习、监督微调、学习控制器）和训练无关方法（提示条件化、反馈驱动停止、模块化组合）。

Result: 建立了连接经典认知范式与算法实现的形式化框架，澄清了不同机制如何在实际中实现自适应推理，支持跨策略的系统比较。

Conclusion: 识别了自我评估、元推理和人类对齐推理控制等开放挑战，为自适应推理研究提供了系统框架。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [10] [HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive Hazard Forecasting in High-Risk DOE Environments](https://arxiv.org/abs/2511.10810)
*Ran Elgedawy,Sanjay Das,Ethan Seefried,Gavin Wiggins,Ryan Burchfield,Dana Hewit,Sudarshan Srinivasan,Todd Thomas,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: HARNESS是一个模块化AI框架，用于预测危险事件和分析美国能源部环境中的操作风险，通过结合大语言模型、结构化工作数据和历史事件检索来主动识别潜在危险。


<details>
  <summary>Details</summary>
Motivation: 在任务关键工作场所确保操作安全至关重要，因为这些场所的日常任务复杂且危险。需要开发能够预测危险事件和分析操作风险的系统来提高安全性。

Method: 集成大语言模型与结构化工作数据、历史事件检索和风险分析，采用人在回路机制让领域专家优化预测，形成自适应学习循环。结合专家协作和迭代代理推理。

Result: 初步部署显示有前景的结果，提高了预测安全系统的可靠性和效率。

Conclusion: HARNESS通过结合领域专家协作和迭代代理推理，改善了预测安全系统的性能。未来工作将关注准确性、专家一致性和决策延迟减少的定量评估。

Abstract: Operational safety at mission-critical work sites is a top priority given the complex and hazardous nature of daily tasks. This paper presents the Human-Agent Risk Navigation and Event Safety System (HARNESS), a modular AI framework designed to forecast hazardous events and analyze operational risks in U.S. Department of Energy (DOE) environments. HARNESS integrates Large Language Models (LLMs) with structured work data, historical event retrieval, and risk analysis to proactively identify potential hazards. A human-in-the-loop mechanism allows subject matter experts (SMEs) to refine predictions, creating an adaptive learning loop that enhances performance over time. By combining SME collaboration with iterative agentic reasoning, HARNESS improves the reliability and efficiency of predictive safety systems. Preliminary deployment shows promising results, with future work focusing on quantitative evaluation of accuracy, SME agreement, and decision latency reduction.

</details>


### [11] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx是一个混合知识图谱嵌入框架，通过自适应结合双曲空间、复空间和欧几里得空间来解决现有方法在建模多样化关系类型时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法存在关键限制：欧几里得模型难以处理层次结构，向量空间模型无法捕捉不对称性，双曲模型在对称关系上表现不佳。需要一种能自适应处理各种关系类型的统一框架。

Method: 提出关系特定的空间加权策略，通过学习的注意力机制动态选择每个关系类型的最优几何空间，并使用多空间一致性损失确保跨空间预测的一致性。

Result: 在从1K论文到10M论文的知识图谱上评估，相比TransE、RotatE、DistMult等基线模型取得一致改进。在10M论文数据集上达到0.612 MRR，相对最佳基线提升4.8%，同时保持高效训练和85ms/三元组的推理速度。

Conclusion: HyperComplEx通过自适应几何空间组合有效解决了多样化关系类型的建模问题，具有良好的可扩展性和性能优势，为可扩展知识图谱嵌入研究提供了新方向。

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [12] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 开发了一个多智能体AI框架，用于从碎片化的碰撞数据中重建事故前场景和推断车辆行为，在复杂碰撞案例中实现了100%的准确率，超越了人类专家的92%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统交通事故重建依赖人工经验，在处理不完整多模态数据时结果不一致，需要更精确、一致的分析方法。

Method: 采用两阶段协作框架：第一阶段从多模态输入生成自然语言事故重建；第二阶段结合时间事件数据记录器进行深度事故推理。处理了277起追尾前车减速碰撞案例。

Result: 在39个复杂碰撞案例中，框架实现了100%的准确率，成功识别最相关EDR事件并正确区分撞击与被撞车辆，即使在处理不完整数据时也保持稳健性能。

Conclusion: 该研究展示了AI在处理异构碰撞数据方面的卓越能力，在重建碰撞动力学和表征事故前行为方面提供了前所未有的精确度。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [13] [Enhancing Demand-Oriented Regionalization with Agentic AI and Local Heterogeneous Data for Adaptation Planning](https://arxiv.org/abs/2511.10857)
*Seyedeh Mobina Noorani,Shangde Gao,Changjie Chen,Karla Saldana Ochoa*

Main category: cs.AI

TL;DR: 提出一个基于智能AI的规划支持系统，用于生成面向需求的动态规划单元，支持灾害规划。系统结合人类参与原则，通过改进的自组织映射算法和AI代理指导，实现交互式区域划分探索。


<details>
  <summary>Details</summary>
Motivation: 传统规划单元（如人口普查区、邮政编码区）无法准确反映当地社区需求，缺乏灵活性来实施有效的灾害预防或应对策略。需要创建动态规划单元来支持灾害规划。

Method: 开发基于代表性初始化空间约束自组织映射（RepSC-SOM）的规划支持系统，扩展传统SOM算法，加入自适应地理过滤和区域增长细化。AI代理能够推理、规划和行动，指导输入特征选择、空间约束设置和交互式探索。

Result: 通过佛罗里达州杰克逊维尔市洪水风险案例研究展示平台能力，证明系统允许用户交互式探索、生成和评估区域划分，结合计算严谨性和用户驱动决策。

Conclusion: 该平台成功整合了计算方法和人类专业知识，为灾害规划提供了灵活、透明且适应性的动态规划单元生成解决方案。

Abstract: Conventional planning units or urban regions, such as census tracts, zip codes, or neighborhoods, often do not capture the specific demands of local communities and lack the flexibility to implement effective strategies for hazard prevention or response. To support the creation of dynamic planning units, we introduce a planning support system with agentic AI that enables users to generate demand-oriented regions for disaster planning, integrating the human-in-the-loop principle for transparency and adaptability. The platform is built on a representative initialized spatially constrained self-organizing map (RepSC-SOM), extending traditional SOM with adaptive geographic filtering and region-growing refinement, while AI agents can reason, plan, and act to guide the process by suggesting input features, guiding spatial constraints, and supporting interactive exploration. We demonstrate the capabilities of the platform through a case study on the flooding-related risk in Jacksonville, Florida, showing how it allows users to explore, generate, and evaluate regionalization interactively, combining computational rigor with user-driven decision making.

</details>


### [14] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: 提出了一种利用大语言模型作为专家指导的新框架，从不规则采样的纵向患者数据中学习神经退行性疾病进展，同时优化长期疾病轨迹构建和生物约束的脑区交互图结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于简化脑连接性的复杂关系，假设单一模态的脑连接组作为疾病传播基质，导致病理传播预测不准确，特别是在长期进展期间。而纯数据驱动的方法由于缺乏适当约束而面临可识别性问题。

Method: 利用大语言模型作为区域变量交互的专家指导，通过LLM合成多模态关系并整合多种疾病驱动机制，同时优化个体观察的长期疾病轨迹构建和生物约束的脑区交互图结构。

Result: 在阿尔茨海默病队列的tau-PET成像数据上验证，新框架相比传统方法展现出更优越的预测准确性和可解释性，同时揭示了超出传统连接性测量的额外疾病驱动因素。

Conclusion: 该框架通过结合大语言模型的专家知识和生物约束，有效解决了神经退行性疾病进展建模中的可识别性问题，提供了更准确的病理传播预测和更深入的疾病机制理解。

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [15] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: 提出了一个多代理法律验证器，通过专业代理分工和协调协议来检查AI驱动数据转移规划的法律合规性，在APIP法规下比单代理基线准确率提高21个百分点。


<details>
  <summary>Details</summary>
Motivation: 在严格的隐私法规（如日本个人信息保护法APIP）下，AI驱动数据转移规划的法律合规性变得日益关键，需要可靠且可解释的自动化合规验证框架。

Method: 多代理法律验证器将合规检查分解为法规解释、业务背景评估和风险评估等专业代理，通过结构化合成协议进行协调。

Result: 在200个APIP第16条修正案案例数据集上，系统达到72%准确率，比单代理基线高21个百分点，在明确合规案例上达到90%准确率，同时保持对明确违规的完美检测。

Conclusion: 领域专业化和协调推理能显著提高法律AI性能，为可信赖和可解释的自动化合规验证提供了可扩展且法规感知的框架。

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [16] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 论文探讨了自主AI系统在遇到训练数据未覆盖的复杂场景时，如何构建、评估和证明候选行动方案，以满足人类期望和价值观。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在实际部署中必然会遇到训练数据未覆盖的新场景，此时需要超越训练策略，构建符合人类期望的行动方案。

Method: 通过理论分析和实证案例研究，识别智能体决策所需的知识类型，包括规范性、实用性和情境性理解。

Result: 确定了智能体在复杂现实环境中做出稳健决策所需的知识整合框架，能够选择更符合人类期望的行动方案。

Conclusion: 自主AI系统需要整合多种知识类型才能在新颖或未充分指定的情境中做出符合人类价值观的决策。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [17] [AI Agent-Driven Framework for Automated Product Knowledge Graph Construction in E-Commerce](https://arxiv.org/abs/2511.11017)
*Dimitar Peshevski,Riste Stojanov,Dimitar Trajanov*

Main category: cs.AI

TL;DR: 提出基于AI代理的自动化框架，直接从非结构化产品描述构建产品知识图谱，无需预定义模式或人工规则


<details>
  <summary>Details</summary>
Motivation: 电商平台产生大量非结构化产品数据，传统知识图谱构建过程复杂且手动，需要自动化解决方案

Method: 使用大语言模型驱动的三阶段代理框架：本体创建与扩展、本体精炼、知识图谱填充

Result: 在真实空调产品数据集上验证，实现超过97%的属性覆盖率和最小冗余度

Conclusion: LLMs在零售领域结构化知识提取方面具有巨大潜力，为智能产品数据集成提供可扩展路径

Abstract: The rapid expansion of e-commerce platforms generates vast amounts of unstructured product data, creating significant challenges for information retrieval, recommendation systems, and data analytics. Knowledge Graphs (KGs) offer a structured, interpretable format to organize such data, yet constructing product-specific KGs remains a complex and manual process. This paper introduces a fully automated, AI agent-driven framework for constructing product knowledge graphs directly from unstructured product descriptions. Leveraging Large Language Models (LLMs), our method operates in three stages using dedicated agents: ontology creation and expansion, ontology refinement, and knowledge graph population. This agent-based approach ensures semantic coherence, scalability, and high-quality output without relying on predefined schemas or handcrafted extraction rules. We evaluate the system on a real-world dataset of air conditioner product descriptions, demonstrating strong performance in both ontology generation and KG population. The framework achieves over 97\% property coverage and minimal redundancy, validating its effectiveness and practical applicability. Our work highlights the potential of LLMs to automate structured knowledge extraction in retail, providing a scalable path toward intelligent product data integration and utilization.

</details>


### [18] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: 提出了一种新的不完全方法来打破抽象结构的对称性，通过更好地利用其表示来处理约束编程中的对称性问题。


<details>
  <summary>Details</summary>
Motivation: 在约束编程中，使用高级建模语言（如Essence）表达问题时涉及抽象结构，这些结构需要转换为求解器支持的表示。对称性破坏技术能显著加速求解过程，但应用于抽象变量时会产生大量复杂约束，性能不佳。

Method: 开发了一种新的不完全对称性破坏方法，通过更好地利用抽象结构的表示来处理对称性，特别针对不可区分对象产生的对称性。

Result: 该方法比之前提出的方法（Akgün et al. 2025）更快。

Conclusion: 新方法通过改进抽象结构的表示利用，有效解决了对称性破坏中的性能问题，在不可区分对象对称性处理上表现出优越性能。

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [19] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: 研究发现角色分配策略对多智能体辩论性能有显著影响，提出"Truth Last"策略可提升22%性能，并开发MADC策略通过一致性评估来优化多智能体辩论机制。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论在提升LLM推理能力方面有潜力，但角色分配策略这一关键因素尚未充分探索，特别是在实际应用中真理未知的情况下。

Method: 提出"Truth Last"角色分配策略，并开发MADC策略，通过路径一致性评估独立角色间的一致性，模拟最高一致性得分的角色作为真理。

Result: 在9个LLM模型上的验证显示，MADC策略能持续展现先进性能，有效克服MAD的性能瓶颈，在推理任务中提升达22%。

Conclusion: MADC为LLM智能体扩展提供了重要改进路径，通过系统性模拟和优化多智能体辩论的核心机制，显著提升了推理任务的性能。

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [20] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 提出了DSS框架，利用可微分模拟器Waymax作为状态预测器和评估器，通过梯度下降优化动作序列，显著提升了自动驾驶的跟踪和路径规划精度。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，规划对于避免碰撞和在复杂密集交通场景中导航至关重要。传统方法在学习策略、状态预测器和评估器时面临挑战。

Method: 使用可微分模拟器Waymax作为状态预测器和评估器，利用其硬编码动力学实现高精度状态预测，通过梯度下降在想象轨迹上优化动作序列。

Result: 实验表明，DSS结合规划梯度和随机搜索，相比序列预测、模仿学习、无模型RL和其他规划方法，显著提高了跟踪和路径规划精度。

Conclusion: DSS框架通过可微分模拟器和梯度优化，为自动驾驶规划提供了有效解决方案，在复杂交通场景中表现出优越性能。

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [21] [ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving](https://arxiv.org/abs/2511.11079)
*Sejin Kim,Hayan Choi,Seokki Lee,Sundong Kim*

Main category: cs.AI

TL;DR: ARCTraj是一个数据集和方法框架，用于通过抽象推理语料库中的复杂视觉任务建模人类推理过程，记录时间有序的对象级动作来揭示中间推理步骤。


<details>
  <summary>Details</summary>
Motivation: 现有的ARC方法主要依赖静态输入-输出监督，无法洞察推理过程如何随时间展开，缺乏对中间推理步骤的理解。

Method: 通过O2ARC网络界面收集约10,000条轨迹，包含任务标识符、时间戳和成功标签；定义统一的推理管道，包括数据收集、动作抽象、MDP制定和下游学习，可与强化学习、生成建模和序列建模方法集成。

Result: 收集了400个训练任务的轨迹数据，分析了空间选择、颜色归因和策略收敛，揭示了人类推理的结构和多样性。

Conclusion: ARCTraj为研究类人推理提供了结构化和可解释的基础，推动了可解释性、对齐和通用智能的发展。

Abstract: We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.

</details>


### [22] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 提出了一种新的广义规划方法，通过从训练问题中学习条件-动作规则来构建广义规划器，该方法在合成成本、规划覆盖率和解质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 广义规划旨在合成能够解决相关规划问题族的程序，现有方法在效率和可扩展性方面存在局限，需要更简单有效的方法。

Method: 为每个训练问题按顺序计算各目标原子的最优规划，对结果规划执行目标回归，并将输出提升为一阶条件-动作规则集合。

Result: 实验表明，在经典和数值规划领域中，该方法在合成成本、规划覆盖率和解质量三个指标上显著优于现有最先进的广义规划器。

Conclusion: 该方法能够学习有效的广义规划和状态空间剪枝公理，为广义规划提供了一种简单而强大的解决方案。

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [23] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: 提出了GGBench基准测试，专门用于评估几何生成推理能力，填补了统一多模态模型在集成认知过程评估方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估判别性理解或无约束图像生成，无法衡量生成推理的集成认知过程，几何构建需要语言理解和精确视觉生成的融合，是理想的测试平台。

Method: 基于几何构建设计基准测试，要求模型不仅要理解和推理，还要主动构建解决方案，系统诊断模型的综合能力。

Result: 开发了GGBench基准测试，为下一代智能系统设定了更严格的标准。

Conclusion: 几何构建为评估统一多模态模型的生成推理能力提供了理想测试平台，GGBench填补了现有评估方法的空白。

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [24] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: 提出MUG协议，通过多模态反事实测试检测幻觉代理，改进多代理辩论框架，提供更可靠的多模态推理。


<details>
  <summary>Details</summary>
Motivation: 解决多代理辩论中假设所有代理都是理性的不现实问题，当代理本身容易产生幻觉时，需要更可靠的检测机制。

Method: 受社交推理游戏启发，通过修改参考图像引入反事实证据，观察代理是否能准确识别变化，从而检测幻觉代理。

Result: MUG协议在三个关键维度上改进了MAD：实现基于反事实测试的事实验证、引入跨证据推理、促进主动推理。

Conclusion: MUG为LLMs的多模态推理提供了一个更可靠有效的框架，代码已开源。

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [25] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR框架通过慢思考机制增强LLM的表格推理能力，采用两阶段难度感知强化学习和不确定性量化，显著提升推理深度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM表格推理方法存在推理深度不足、缺乏迭代精炼以及推理过程不稳定的问题，需要更接近人类认知的推理机制。

Method: 提出STaR框架，通过显式建模逐步思考和不确定性感知推理来赋予LLM慢思考能力；训练阶段采用两阶段难度感知强化学习，从简单到复杂查询渐进学习；推理阶段通过整合token级置信度和答案一致性进行轨迹级不确定性量化。

Result: 在多个基准测试中，STaR实现了优越的性能和增强的推理稳定性，在域外数据集上也表现出强大的泛化能力。

Conclusion: STaR作为一个可靠且受认知启发的解决方案，在LLM表格推理方面具有巨大潜力，能够提供更可信的推理路径。

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [26] [UAVBench: An Open Benchmark Dataset for Autonomous and Agentic AI UAV Systems via LLM-Generated Flight Scenarios](https://arxiv.org/abs/2511.11252)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.AI

TL;DR: UAVBench是一个用于评估无人机系统中大型语言模型推理能力的标准化基准数据集，包含5万个验证过的飞行场景和5万个多选题，涵盖10种认知和伦理推理风格。


<details>
  <summary>Details</summary>
Motivation: 当前无人机系统越来越依赖LLMs进行任务规划、感知和决策，但缺乏标准化和物理基础的基准来系统评估其推理能力。

Method: 通过分类学引导的LLM提示和多阶段安全验证生成5万个验证过的无人机飞行场景，并创建包含5万个多选题的推理导向扩展数据集。

Result: 评估了32个最先进的LLM，发现在感知和政策推理方面表现强劲，但在伦理意识和资源受限决策方面仍存在挑战。

Conclusion: UAVBench为自主空中系统中的智能体AI建立了可重现和物理基础的基准，推动了下一代无人机推理智能的发展。

Abstract: Autonomous aerial systems increasingly rely on large language models (LLMs) for mission planning, perception, and decision-making, yet the lack of standardized and physically grounded benchmarks limits systematic evaluation of their reasoning capabilities. To address this gap, we introduce UAVBench, an open benchmark dataset comprising 50,000 validated UAV flight scenarios generated through taxonomy-guided LLM prompting and multi-stage safety validation. Each scenario is encoded in a structured JSON schema that includes mission objectives, vehicle configuration, environmental conditions, and quantitative risk labels, providing a unified representation of UAV operations across diverse domains. Building on this foundation, we present UAVBench_MCQ, a reasoning-oriented extension containing 50,000 multiple-choice questions spanning ten cognitive and ethical reasoning styles, ranging from aerodynamics and navigation to multi-agent coordination and integrated reasoning. This framework enables interpretable and machine-checkable assessment of UAV-specific cognition under realistic operational contexts. We evaluate 32 state-of-the-art LLMs, including GPT-5, ChatGPT-4o, Gemini 2.5 Flash, DeepSeek V3, Qwen3 235B, and ERNIE 4.5 300B, and find strong performance in perception and policy reasoning but persistent challenges in ethics-aware and resource-constrained decision-making. UAVBench establishes a reproducible and physically grounded foundation for benchmarking agentic AI in autonomous aerial systems and advancing next-generation UAV reasoning intelligence. To support open science and reproducibility, we release the UAVBench dataset, the UAVBench_MCQ benchmark, evaluation scripts, and all related materials on GitHub at https://github.com/maferrag/UAVBench

</details>


### [27] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: AIonopedia是首个基于大语言模型的离子液体发现智能体，通过多模态领域基础模型实现准确性质预测和分层分子筛选，在真实湿实验验证中表现出色。


<details>
  <summary>Details</summary>
Motivation: 离子液体的发现面临数据有限、模型精度低和工作流程碎片化的挑战，需要开发更有效的AI驱动发现方法。

Method: 构建LLM增强的多模态离子液体领域基础模型，采用分层搜索架构进行分子筛选和设计，使用新构建的综合数据集进行训练。

Result: 模型性能优越，在文献报道系统上能有效进行离子液体修饰，真实湿实验验证显示在分布外任务上具有卓越泛化能力。

Conclusion: AIonopedia能够加速真实世界离子液体的发现过程，展示了LLM在材料科学中的实际应用潜力。

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [28] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 本文提出了一种通过强制记录AI训练和推理过程中每个组件的方法，来生成防篡改、可验证且全面的AI决策追踪系统。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在决策过程文档化方面存在不足，这阻碍了追溯决策依据的能力，而这是重建责任链的前提。当AI决策无意或故意违反法律时，这种可追溯性对于法庭确定原因至关重要。

Method: 采用DBOM概念扩展，结合机密计算技术，构建支持生成防篡改、可验证且详尽AI决策追踪的运行工作流。

Result: 开发了一个区分有毒和可食用蘑菇的应用程序作为高风险决策支持的示例，展示了该工作流的实际运作。

Conclusion: 该方法为解决AI决策可追溯性和责任链问题提供了实用且有效的解决方案。

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [29] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: 提出对比ABox解释的概念，用于回答"为什么a是C的实例而b不是？"这类问题，同时考虑正蕴含和缺失蕴含，聚焦a和b之间的相关共性和差异。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独解释正蕴含（为什么C(a)被知识库蕴含）或缺失蕴含（为什么C(b)不被蕴含），但缺乏同时考虑两者的对比解释方法。

Method: 为描述逻辑本体论中的ABox推理开发了对比解释的适当概念，分析了不同变体在不同最优性标准下的计算复杂度，考虑了轻量级和更表达性的描述逻辑。

Result: 实现了计算对比解释变体的首个方法，并在现实知识库的生成问题上进行了评估。

Conclusion: 对比ABox解释能够有效回答对比性问题，通过同时考虑正负实例来揭示相关差异，为描述逻辑推理提供了新的解释能力。

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [30] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign是一个推理时框架，将大型视觉语言模型的对齐问题重新定义为经济理性的搜索过程，通过前瞻性函数动态权衡安全性、效用和成本，在降低计算成本的同时达到或超越现有方法的安全性和效用表现。


<details>
  <summary>Details</summary>
Motivation: 当前的对齐方法在安全性、效用和运营成本之间存在权衡困难，仅关注最终输出的过程盲目性会浪费大量计算预算在不安全的推理上，使得有害推理能够通过良性理由伪装来规避简单的安全评分。

Method: 将LVLM视为有限理性代理，逐步扩展思维图，使用前瞻性函数（类似于净现值）对行动进行评分，动态权衡预期安全性、效用和成本与剩余预算的关系，并通过最薄弱环节原则强制执行路径安全性。

Result: 在3个闭源和2个开源模型的6个数据集上进行广泛实验，表明EcoAlign在较低计算成本下匹配或超越了最先进的安全性和效用表现。

Conclusion: EcoAlign为稳健的LVLM对齐提供了一个原则性、经济性的路径。

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [31] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: RLSLM是一个结合强化学习和基于规则的社会运动模型的混合框架，通过将心理学原理融入奖励函数，实现社会感知导航，在用户体验和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决社会感知导航中基于规则方法缺乏泛化性而数据驱动方法缺乏可解释性的问题，需要一种既能遵循人类心理原则又能灵活适应复杂环境的方法。

Method: 提出RLSLM框架，将基于实证行为实验的社会运动模型集成到强化学习的奖励函数中，生成方向敏感的社会舒适度场，联合优化机械能和社会舒适度。

Result: 在沉浸式VR实验中，RLSLM在用户体验方面优于最先进的基于规则模型，消融和敏感性分析显示其可解释性显著优于传统数据驱动方法。

Conclusion: 这项工作提出了一个可扩展的、以人为中心的方法论，有效整合认知科学和机器学习，实现现实世界的社会导航。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [32] [KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics](https://arxiv.org/abs/2511.11357)
*Haixin Li,Yanke Li,Diego Paez-Granados*

Main category: cs.AI

TL;DR: KarmaTS是一个交互式框架，用于构建具有滞后索引的可执行时空因果图模型，用于多元时间序列模拟。该系统通过结合专家知识和算法建议，构建离散时间结构因果过程，支持模拟和因果干预。


<details>
  <summary>Details</summary>
Motivation: 解决生理数据访问受限的挑战，生成具有已知因果动态的合成多元时间序列，并用专家知识增强真实世界数据集。

Method: 采用混合主动、人在回路的工作流程，结合专家知识和算法建议构建离散时间结构因果过程。处理混合变量类型、同时和滞后边，以及从参数化模板到神经网络模型的模块化边功能。

Result: 能够生成具有已知因果动态的合成多元时间序列，支持模拟和因果干预，包括用户指定的分布偏移。

Conclusion: KarmaTS通过专家指导的模拟，为因果发现算法提供了灵活的验证和基准测试能力。

Abstract: We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.

</details>


### [33] [MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism](https://arxiv.org/abs/2511.11373)
*Shulin Liu,Dong Du,Tao Yang,Yang Li,Boyu Qiu*

Main category: cs.AI

TL;DR: MarsRL是一个新颖的强化学习框架，通过代理管道并行性联合优化多代理推理系统中的所有代理，显著提升了开源模型在复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型受限于输出长度，难以在单次推理中实现深度推理。多代理推理系统虽然有效，但在开源模型中由于批评和修正能力不足而表现不佳。

Method: 提出MarsRL框架，采用代理特定奖励机制减少奖励噪声，并使用管道式训练提高长轨迹处理效率，联合优化系统中的所有代理。

Result: 在Qwen3-30B-A3B-Thinking-2507上，MarsRL将AIME2025准确率从86.5%提升至93.3%，BeyondAIME从64.9%提升至73.8%，甚至超越了Qwen3-235B-A22B-Thinking-2507。

Conclusion: MarsRL展示了在多代理推理系统中推进强化学习的潜力，能够扩展其在不同推理任务中的适用性。

Abstract: Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.

</details>


### [34] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 本调查系统回顾了在多智能体强化学习中，面对现实通信约束（如消息扰动、传输延迟和有限带宽）时的鲁棒高效通信策略进展，重点关注自动驾驶、分布式SLAM和联邦学习三个应用领域。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法大多假设通信是瞬时、可靠且带宽无限的，但这些条件在现实部署中很少满足，因此需要研究在现实约束下的鲁棒高效通信策略。

Method: 系统调查方法，分析在消息扰动、传输延迟和有限带宽等现实通信约束下的MARL通信策略，并聚焦三个具体应用领域。

Result: 识别了当前MARL通信策略在现实约束下的研究进展，并明确了低延迟可靠性、带宽密集型数据共享和通信隐私权衡等核心挑战。

Conclusion: 提出了统一方法，将通信、学习和鲁棒性协同设计，以弥合理论MARL模型与实际实现之间的差距，并指出了未来研究方向。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [35] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet是一个多模态模型，通过整合非结构化临床笔记、实验室测试和患者时间序列数据，使用LLM处理临床文本和文本实验室测试，以及transformer编码器处理纵向序列访问，来预测慢性疾病。


<details>
  <summary>Details</summary>
Motivation: 大多数预测模型未能充分捕捉多个数据模态之间的交互、冗余和时间模式，通常只关注单一数据类型或忽视这些复杂性。

Method: 利用大型语言模型处理临床文本和文本实验室测试，使用transformer编码器处理纵向序列访问，整合三种数据模态。

Result: 在MIMIC-III和FEMH数据集上，CURENet在多标签框架中预测前10种慢性疾病的准确率超过94%。

Conclusion: 多模态EHR整合有潜力增强临床决策制定并改善患者结局。

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [36] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR是一个经验引导的推理系统，能够在推理时动态生成包含LLM调用、工具、采样参数和控制逻辑的完整计算策略，实现AI系统的自适应问题解决。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在推理时只能通过修改文本输入来调整，无法灵活改变采样参数、移除工具、修改系统提示或在代理和工作流范式间切换，而更灵活的系统需要离线优化且部署后静态不变。

Method: 使用基于LLM的元策略来生成策略，包含两个组件：Guide基于当前问题和结构化记忆生成候选策略，Consolidator整合执行反馈以改进未来策略生成。

Result: 在五个挑战性基准测试中，EGuR相比最强基线准确率提升高达14%，计算成本降低高达111倍，且随着经验积累两个指标持续改善。

Conclusion: EGuR通过动态生成完整计算策略实现了AI系统的自适应推理，在提升性能的同时显著降低计算成本。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


### [37] [Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping](https://arxiv.org/abs/2511.11551)
*Dena Mujtaba,Brian Hu,Anthony Hoogs,Arslan Basharat*

Main category: cs.AI

TL;DR: 提出了一种基于模型引导策略塑造的测试时对齐技术，能够在复杂动态环境中精确控制AI代理的行为属性，在伦理对齐和奖励最大化之间实现原则性权衡，无需重新训练代理。


<details>
  <summary>Details</summary>
Motivation: AI决策代理在复杂动态环境中部署时面临与人类价值观或指导方针保持对齐的关键挑战。仅以达成目标为训练目的的代理可能采取有害行为，暴露出最大化奖励函数与保持对齐之间的关键权衡。

Method: 使用模型引导策略塑造的测试时对齐技术，通过场景-行为属性分类器进行策略塑造，确保决策与伦理属性对齐。在MACHIAVELLI基准测试中进行评估，包含134个文本游戏环境和数千个涉及伦理决策的标注场景。

Result: 测试时策略塑造提供了跨不同环境和对齐属性减轻不道德行为的有效且可扩展的解决方案，优于先前的训练时方法和通用代理。

Conclusion: 测试时策略塑造是确保预训练AI代理在复杂环境中保持伦理对齐的有效方法，能够在伦理对齐和奖励最大化之间实现原则性权衡，无需昂贵的重新训练过程。

Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [38] [Support Recovery in One-bit Compressed Sensing with Near-Optimal Measurements and Sublinear Time](https://arxiv.org/abs/2511.10777)
*Xiaxin Li,Arya Mazumdar*

Main category: cs.IT

TL;DR: 本文提出了两种在一位压缩感知中实现亚线性运行时间的支持恢复方案：一种是通用精确支持恢复方案，另一种是通用ε近似支持恢复方案，均显著改进了现有方法的运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有的一位压缩感知支持恢复方法运行时间通常为Ω(n)，这在处理高维信号时效率较低。本文旨在开发具有亚线性运行时间o(n)的支持恢复方案，以提高计算效率。

Method: 提出了两种方案：(1) 通用支持恢复方案，包括精确恢复和ε近似恢复，使用结构化测量矩阵和高效算法；(2) 概率性精确支持恢复方案，在亚线性机制下运行。

Result: 方案1.i实现了m=O(k²log(n/k)log n)测量数和D=O(km)运行时间；方案1.ii实现了m=O(kε⁻¹log(n/k)log n)测量数和D=O(ε⁻¹m)运行时间；方案2实现了m=O(k(log k/log log k)log n)测量数和O(m)运行时间。

Conclusion: 本文成功开发了具有亚线性运行时间的支持恢复方案，在保持恢复性能的同时显著提高了计算效率，为大规模信号处理应用提供了更实用的解决方案。

Abstract: The problem of support recovery in one-bit compressed sensing (1bCS) aim to recover the support of a signal $x\in \mathbb{R}^n$, denoted as supp$(x)$, from the observation $y=\text{sign}(Ax)$, where $A\in \mathbb{R}^{m\times n}$ is a sensing matrix and $|\text{supp}(x)|\leq k, k \ll n$. Under this setting, most preexisting works have a recovery runtime $Ω(n)$. In this paper, we propose two schemes that have sublinear $o(n)$ runtime. (1.i): For the universal exact support recovery, a scheme of $m=O(k^2\log(n/k)\log n)$ measurements and runtime $D=O(km)$. (1.ii): For the universal $ε$-approximate support recovery, the same scheme with $m=O(kε^{-1}\log(n/k)\log n)$ and runtime $D=O(ε^{-1}m)$, improving the runtime significantly with an extra $O(\log n)$ factor in the number of measurements compared to the current optimal (Matsumoto et al., 2023). (2): For the probabilistic exact support recovery in the sublinear regime, a scheme of $m:=O(k\frac{\log k}{\log\log k}\log n)$ measurements and runtime $O(m)$, with vanishing error probability, improving the recent result of Yang et al., 2025.

</details>


### [39] [Joint Beamforming and Position Optimization for IRS-Aided SWIPT with Movable Antennas](https://arxiv.org/abs/2511.11148)
*Yanze Zhu,Qingqing Wu,Xinrong Guan,Ziyuan Zheng,Honghao Wang,Wen Chen,Yang Liu,Yuan Guo*

Main category: cs.IT

TL;DR: 该论文提出在SWIPT系统中引入智能反射面和可移动天线技术，通过联合优化基站和IRS的波束成形以及MA位置，在保证能量收集接收器需求的同时最大化信息解码接收器的加权和速率。


<details>
  <summary>Details</summary>
Motivation: SWIPT技术在IoT网络中面临长距离传播导致的严重衰减问题，导致能量收集效率低下。需要新技术来增强信息传输和能量收集性能。

Method: 结合WMMSE、BCD、MM和PDD框架开发高效算法，联合优化基站和IRS的主动/被动波束成形以及MA位置，并提出了可行性表征方法来检查EHRs需求的实现性。

Result: 仿真结果表明所提方案具有显著优势，在考虑的场景下优化的IRS配置比MA对应方案展现出更高的性能增益。

Conclusion: IRS和MA技术的结合能有效提升SWIPT系统性能，IRS配置在特定场景下比MA具有更好的性能表现。

Abstract: Simultaneous wireless information and power transfer (SWIPT) has been envisioned as a promising technology to support ubiquitous connectivity and reliable sustainability in Internet-of-Things (IoT) networks, which, however, generally suffers from severe attenuation caused by long distance propagation, leading to inefficient wireless power transfer (WPT) for energy harvesting receivers (EHRs). This paper proposes to introduce emerging intelligent reflecting surface (IRS) and movable antenna (MA) technologies into SWIPT systems aiming at enhancing information transmission for information decoding receivers (IDRs) and improving receive power of EHRs. We consider to maximize the weighted sum-rate of IDRs via jointly optimizing the active and passive beamforming at the base station (BS) and IRS, respectively, as well as the positions of MAs, while guaranteeing the requirements of all EHRs. To tackle this challenging task due to the non-convexity of associated optimization, we develop an efficient algorithm combining weighted minimal mean square error (WMMSE), block coordinate descent (BCD), majorization-minimization (MM), and penalty duality decomposition (PDD) frameworks. Besides, we present a feasibility characterization method to examine the achievability of EHRs' requirements. Simulation results demonstrate the significant benefits of our proposed solutions. Particularly, the optimized IRS configuration may exhibit higher performance gain than MA counterpart under our considered scenario.

</details>


### [40] [Mutual Coupling in Continuous Aperture Arrays: Physical Modeling and Beamforming Design](https://arxiv.org/abs/2511.11225)
*Zhaolin Wang,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Yuanwei Liu*

Main category: cs.IT

TL;DR: 研究了连续孔径阵列中的互耦合现象，开发了包含极化和表面损耗的物理模型，提出了两种波束成形优化方法，并将模型扩展到离散阵列，证明了耦合模型比非耦合模型更符合物理规律且具有更高方向性。


<details>
  <summary>Details</summary>
Motivation: 传统阵列设计忽略互耦合效应，特别是极化引起的各向异性耦合，导致半波长间距规则失效，需要开发更准确的耦合模型和优化方法。

Method: 1) 建立包含极化和损耗的连续孔径阵列物理模型；2) 通过变分法推导最优波束成形结构；3) 提出核近似法和共轭梯度法解决耦合核求逆问题；4) 将模型扩展到离散阵列。

Result: 1) 耦合离散阵列性能正确收敛到连续孔径极限；2) 极化导致阵列增益各向异性；3) 耦合波束模式比非耦合模式具有更高方向性；4) 非耦合模型违反物理规律。

Conclusion: 互耦合是阵列设计中不可忽视的物理现象，极化效应使耦合呈现各向异性，提出的耦合模型和优化方法能显著提升阵列性能，耦合波束成形可获得更高方向性。

Abstract: The phenomenon of mutual coupling in continuous aperture arrays (CAPAs) is studied. First, a general physical model for the phenomenon that accounts for both polarization and surface dissipation losses is developed. Then, the unipolarized coupling kernel is characterized, revealing that polarization induces anisotropic coupling and invalidates the conventional half-wavelength spacing rule for coupling elimination. Next, the beamforming design problem for CAPAs with coupling is formulated as a functional optimization problem, leading to the derivation of optimal beamforming structures via the calculus of variations. To address the challenge of inverting the coupling kernel in the optimal structure, two methods are proposed: 1) the kernel approximation method, which yields a closed-form solution via wavenumber-domain transformation and GaussLegendre quadrature, and 2) the conjugate gradient method, which addresses an equivalent quadratic functional optimization problem iteratively. Furthermore, the optimal array gain and beampattern are analyzed at the large-aperture limit. Finally, the proposed continuous mutual coupling model is extended to spatially discrete arrays (SPDAs), and comprehensive numerical results are provided, demonstrating that: 1) coupled SPDA performance correctly converges to the CAPA limit, while uncoupled models are shown to violate physics, 2) polarization results in anisotropic array gain behavior, and 3) the coupled beampattern exhibits higher directivity than the uncoupled beampattern.

</details>


### [41] [SCL Decoding of Non-Binary Linear Block Codes](https://arxiv.org/abs/2511.11256)
*Jingyu Lin,Li Chen,Xiaoqian Ye*

Main category: cs.IT

TL;DR: 本文提出了一种针对非二进制线性分组码的连续消除列表译码方法，通过将非二进制码字映射到多个二进制极化码，实现了亚二次复杂度的软判决译码。


<details>
  <summary>Details</summary>
Motivation: 非二进制线性分组码在纠正突发错误方面具有优势，但高效的软判决译码仍然具有挑战性。

Method: 建立非二进制码字与r个二进制极化码字之间的一对r映射，采用r步译码路径排序策略，实现复杂度为亚二次的SCL译码。

Result: 在扩展Reed-Solomon码和非二进制扩展BCH码上的仿真表明，该方法优于现有软判决译码技术，且有限域算术操作更少。对于长度为16的eRS码，使用适度列表大小即可接近最大似然译码性能。

Conclusion: 提出的SCL译码方法为非二进制线性分组码提供了一种高效的软判决译码方案，在性能和复杂度之间取得了良好平衡。

Abstract: Non-binary linear block codes (NB-LBCs) are an important class of error-correcting codes that are especially competent in correcting burst errors. They have broad applications in modern communications and storage systems. However, efficient soft-decision decoding of these codes remains challenging. This paper proposes successive cancellation list (SCL) decoding for NB-LBCs that are defined over a finite field of characteristic two, i.e., F_{2^r}, where r is the extension degree. By establishing a one-to-r mapping between the binary composition of each non-binary codeword and r binary polar codewords, SCL decoding of the r polar codes can be performed with a complexity that is sub-quadratic in the codeword length. An r-step decoding path sorting strategy is further proposed to facilitate the decoding. Simulation results on extended Reed-Solomon (eRS) and non-binary extended BCH (NB-eBCH) codes show that SCL decoding can outperform their state-of-the-art soft-decision decoding with fewer finite field arithmetic operations. For length-16 eRS codes, their maximum-likelihood (ML) decoding performances can be approached with a moderate list size.

</details>


### [42] [Joint Optimization for Multi-User Transmissive RIS-MIMO Systems](https://arxiv.org/abs/2511.11495)
*Zhengwei Jiang,Yufeng Zhou,Xusheng Zhu,Wen Chen,Qingqing Wu,Kai-Kit Wong*

Main category: cs.IT

TL;DR: 本文提出了一种用于透射式可重构智能表面(RIS)多用户MIMO系统的联合优化框架，通过交替优化算法解决RIS系数、功率分配和接收波束成形的联合优化问题，显著提升了系统和速率。


<details>
  <summary>Details</summary>
Motivation: 透射式RIS作为未来无线网络的关键技术，能够替代传统昂贵基站实现低成本、高能效传输。但在多用户MIMO系统中，RIS系数、功率分配和接收波束成形的联合优化面临非凸目标、耦合变量和恒定模约束等挑战。

Method: 提出新颖的优化框架，将和速率最大化问题重构为可处理形式，开发高效的交替优化算法，将问题分解为RIS系数、接收波束成形和功率分配子问题，分别采用凸近似和DC规划等先进技术求解。

Result: 仿真结果表明，所提方法收敛迅速，相比传统方案实现了显著的和速率增益，验证了方法的有效性。

Conclusion: 该方法证明了透射式RIS作为下一代无线系统关键技术的潜力，为低成本、高能效无线传输提供了有效解决方案。

Abstract: Transmissive reconfigurable intelligent surfaces (RIS) represent a transformative architecture for future wireless networks, enabling a paradigm shift from traditional costly base stations to low-cost, energy-efficient transmitters. This paper explores a downlink multi-user MIMO system where a transmissive RIS, illuminated by a single feed antenna, forms the core of the transmitter. The joint optimization of the RIS coefficient vector, power allocation, and receive beamforming in such a system is critical for performance but poses significant challenges due to the non-convex objective, coupled variables, and constant modulus constraints. To address these challenges, we propose a novel optimization framework. Our approach involves reformulating the sum-rate maximization problem into a tractable equivalent form and developing an efficient alternating optimization (AO) algorithm. This algorithm decomposes the problem into subproblems for the RIS coefficients, receive beamformers, and power allocation, each solved using advanced techniques including convex approximation and difference-of-convex programming. Simulation results demonstrate that our proposed method converges rapidly and achieves substantial sum-rate gains over conventional schemes, validating the effectiveness of our approach and highlighting the potential of transmissive RIS as a key technology for next-generation wireless systems.

</details>
