<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.AI](#cs.AI) [Total: 51]
- [cs.IT](#cs.IT) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [MMGaP: Multi-User MIMO Detection and Precoding using GPU-assisted Physics-inspired Computation](https://arxiv.org/abs/2510.01579)
*Abhishek Kumar Singh,Kyle Jamieson*

Main category: cs.NI

TL;DR: MMGaP是一个用于下一代蜂窝网络的上行多用户MIMO检测器和下行矢量扰动预编码器，首次在裸金属CUDA内核上实现大规模MIMO处理算法，显著提升了5G系统的吞吐量性能。


<details>
  <summary>Details</summary>
Motivation: 填补物理层处理算法在商用处理器上实际实现的空白，解决现有系统吞吐量与理论预期之间的差距。

Method: 开发基于裸金属CUDA内核的MMGaP算法，可扩展到大型GPU处理平台，并打包为TensorFlow模块便于系统集成。

Result: 在5G网络测试中，MMGaP使每个用户的上行吞吐量提升约50Mbps，下行吞吐量提升100Mbps；在16天线16用户场景下，上行吞吐量仍能提升50Mbps以上。

Conclusion: MMGaP能够满足最先进5G系统的时序要求，在线速率下运行，为大规模MIMO处理提供了实用的高性能解决方案。

Abstract: Physics-inspired and quantum compute based methods for processing in the
physical layer of next-generation cellular radio access networks have
demonstrated theoretical advances in spectral efficiency in recent years, but
have stopped short of practical realization on commodity processors, leaving a
gap between the throughput practical systems can achieve and the projected
throughput the state-of-the-art should achieve. To fill this gap, this paper
proposes MMGaP, an uplink multi-user MIMO detector and downlink Vector
perturbation precoder for next-generation cellular networks. MMGaP realizes
these large MIMO processing algorithms for the first time on bare-metal CUDA
kernels that scale to run on large GPU processing platforms, and can be
packaged as TensorFlow modules, allowing easy integration with a variety of
systems. We integrate MMGaP with NVIDIA's software-defined, GPU-accelerated 5G
platform and evaluate its performance against the state-of-the-art. In a 5G
cellular network using 100 MHz of radio bandwidth, eight antennas at the base
station and eight concurrent users, we show that MMGaP improves uplink
throughput by approximately 50 Mbps per user and downlink throughput by 100
Mbps per user over a wide range of SNR. We further show that MMGaP can also
support larger MIMO sizes: for 16 antennas at the base station and 16
concurrent users, MMGaP provides more than 50 Mbps higher uplink throughput per
user. We measure the execution time of MMGaP on different NVIDIA GPUs and show
that it can operate at line-rate and meet the timing requirements of
state-of-the-art 5G systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models](https://arxiv.org/abs/2510.01253)
*Jianzhang Zhang,Jialong Zhou,Chuang Liu*

Main category: cs.AI

TL;DR: OR-Toolformer通过微调Llama-3.1-8B-Instruct模型，结合半自动数据合成流程和外部求解器增强，在运筹学问题上实现了高精度求解，在标准基准测试中达到80.1%的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面表现出色，但依赖闭源API存在隐私问题，从头训练开源模型计算成本高昂。

Method: 使用半自动数据合成管道生成多样化的运筹学问题-答案对，通过微调Llama-3.1-8B-Instruct模型，并增强模型调用外部求解器的能力。

Result: 在四个标准基准测试中的三个上，OR-Toolformer达到80.1%的执行准确率，比同规模基线高出4.3%以上；在零样本评估中，对未见过的运筹学问题类型达到54%的平均准确率，比最强基线提高21个百分点。

Conclusion: 工具增强的微调方法对于准确且可泛化的运筹学问题建模和求解具有有效性。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning, but
reliance on closed-source APIs for OR tasks raises privacy concerns, and
training open-source models from scratch incurs high compute costs. We
introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a
semi-automatic data synthesis pipeline that generates diverse OR problem-answer
pairs and augments the model with external solvers to produce API calls. On
three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot
evaluation on two unseen OR problem types, it attains 54% average accuracy, a
21 percentage-point improvement over the strongest baseline. These findings
validate the efficacy of tool-augmented fine-tuning LLMs for accurate and
generalizable OR problem modeling and solving.

</details>


### [3] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出了ROTE算法，通过将日常行为建模为可执行的程序代码而非基于信念的策略，结合LLM生成行为程序假设空间和概率推理处理不确定性，显著提升了人类行为预测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的人类行为建模方法要么对理性做出不切实际的假设，要么计算量过大难以快速适应。日常社交互动中存在可预测的行为模式（"脚本"），这些模式能最小化认知负荷。

Method: ROTE算法：1）使用大语言模型合成行为程序假设空间；2）通过概率推理处理该空间的不确定性；3）将行为理解视为程序合成问题。

Result: 在网格世界任务和大规模家庭模拟器中测试，ROTE从稀疏观察中预测人类和AI行为，比行为克隆和基于LLM的方法在样本内准确性和样本外泛化能力上高出50%。

Conclusion: 通过将动作理解视为程序合成问题，ROTE为AI系统在现实世界中高效有效地预测人类行为开辟了新路径。

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [4] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: 提出了CA-ChemE系统，这是一个通过多智能体协作实现自主研究进化和新兴科学发现的数字化学术城镇，解决了AI在化学工程中跨学科合作和探索未知问题的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在化学工程中跨学科合作和探索未知问题方面存在局限性，需要开发能够自主进化和发现新知识的智能系统。

Method: 集成领域特定知识库、知识增强技术和协作智能体，构建具有深度专业推理和高效跨学科协作能力的智能生态系统，并引入具备本体工程能力的协作智能体。

Result: 知识库增强机制使7个专家智能体的对话质量得分平均提高10-15%，协作智能体的干预使远领域专家对的协作效率提高了8.5%，而邻近领域对仅提高0.8%，揭示了"知识库差距导致的协作效率降低"效应。

Conclusion: 精心设计的多智能体架构为化学工程中的自主科学发现提供了可行途径。

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [5] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: 提出了一个多智能体辩论框架来评估LLM在交互环境中的社交和认知行为，发现智能体具有强烈的共识寻求倾向，即使在没有明确指令的情况下也能达到高语义一致性。


<details>
  <summary>Details</summary>
Motivation: 传统评估基准无法捕捉LLM作为自主智能体在交互环境中出现的社交和认知动态，需要新的评估方法来理解智能体的社会行为。

Method: 使用多智能体辩论作为受控"社交实验室"，让具有不同角色和激励的LLM智能体在LLM主持人的监督下就各种挑战性话题进行辩论，并采用心理测量和语义指标进行分析。

Result: 发现智能体具有强烈的共识寻求倾向（语义一致性μ>0.88），分配的角色会产生稳定的心理测量特征，主持人的角色能显著改变辩论结果。

Conclusion: 这项工作为面向智能体场景的动态、基于心理测量的评估协议提供了蓝图，为理解和塑造下一代AI智能体的社会行为提供了关键方法。

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [6] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: AGILE通过将拼图任务转化为交互式学习过程，显著提升了视觉语言模型的感知和推理能力，在拼图任务上准确率从9.5%提升至82.8%，并在9个通用视觉任务上平均提升3.1%。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在多模态理解和推理方面虽有进步，但在基本感知和推理能力上仍有限制，特别是在简单拼图任务上表现接近随机。高质量视觉语言数据的稀缺性和有限可扩展性限制了这些能力的提升。

Method: AGILE将拼图解决制定为交互过程，模型在每个步骤生成可执行代码执行动作，环境提供细粒度视觉反馈。通过观察和交互的迭代循环，模型通过探索和反馈逐步提升感知和推理能力。

Result: AGILE在复杂程度不同的拼图任务上大幅提升性能（2×2设置下准确率从9.5%提升至82.8%），并在9个通用视觉任务上表现出强泛化能力，平均提升3.1%。

Conclusion: 这项工作为推进多模态模型的推理和泛化能力开辟了新途径，并为多模态强化学习数据稀缺问题提供了高效、可扩展的解决方案。

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [7] [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
*Tudor Achim,Alex Best,Kevin Der,Mathïs Fédérico,Sergei Gukov,Daniel Halpern-Leister,Kirsten Henningsgard,Yury Kudryashov,Alexander Meiburg,Martin Michelsen,Riley Patterson,Eric Rodriguez,Laura Scharff,Vikram Shanker,Vladmir Sicca,Hari Sowrirajan,Aidan Swope,Matyas Tamas,Vlad Tenev,Jonathan Thomm,Harold Williams,Lawrence Wu*

Main category: cs.AI

TL;DR: Aristotle系统结合形式验证与非正式推理，在2025年国际数学奥林匹克竞赛中达到金牌级别的表现。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够结合形式验证的严谨性和非正式推理的灵活性的AI系统，以解决复杂的数学问题。

Method: 集成三个主要组件：Lean证明搜索系统、生成并形式化引理的非正式推理系统，以及专用的几何求解器。

Result: 在2025年国际数学奥林匹克竞赛问题上达到金牌等效性能，展示了自动定理证明领域的最先进性能。

Conclusion: Aristotle系统成功证明了结合形式验证和非正式推理的方法在解决复杂数学问题上的有效性，并具有良好的扩展性。

Abstract: We introduce Aristotle, an AI system that combines formal verification with
informal reasoning, achieving gold-medal-equivalent performance on the 2025
International Mathematical Olympiad problems. Aristotle integrates three main
components: a Lean proof search system, an informal reasoning system that
generates and formalizes lemmas, and a dedicated geometry solver. Our system
demonstrates state-of-the-art performance with favorable scaling properties for
automated theorem proving.

</details>


### [8] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK是一个用于评估多平台代理环境中长期记忆和状态跟踪的基准测试，模拟真实组织工作流程，整合Slack、Linear和Git等平台的异步事件。


<details>
  <summary>Details</summary>
Motivation: 现有上下文和记忆基准测试主要关注对话场景，但评估企业动态环境中的记忆能力对于其有效应用至关重要。

Method: 通过专家手动设计和基于代理的可扩展合成来构建数据集，生成基于真实软件开发过程的生态有效场景，并引入正确性、效率和冗余性等指标。

Result: 对最先进LLM和记忆后端的实验显示，在长时程利用记忆、处理跨平台依赖和解决矛盾方面存在挑战，表现最佳的GPT-5模型在MEMTRACK上仅达到60%的正确性得分。

Conclusion: 这项工作为记忆增强代理的评估研究提供了一个可扩展框架，超越了现有对话设置的关注，并为复杂组织环境中的多代理、多平台记忆基准测试奠定了基础。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [9] [Retrieval-Augmented Framework for LLM-Based Clinical Decision Support](https://arxiv.org/abs/2510.01363)
*Leon Garza,Anantaa Kotal,Michael A. Grasso,Emre Umucu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的临床决策支持系统，通过分析电子健康记录生成治疗建议，采用检索增强生成技术整合非结构化叙述和结构化数据，旨在辅助而非替代临床决策。


<details>
  <summary>Details</summary>
Motivation: 临床决策复杂性增加和电子健康记录快速增长为数据驱动医疗带来机遇与挑战，需要开发能够辅助处方决策的智能工具。

Method: 采用检索增强生成（RAG）管道，整合自然语言处理和结构化临床输入，通过检索具有相似特征的先例病例来生成治疗建议。

Result: 初步评估显示，在适当约束和严格验证下，基于大语言模型的工具可在处方工作流程中提供有价值的决策支持。

Conclusion: 这是将生成式AI整合到现实世界临床决策中的初步尝试，强调透明度、安全性以及与既定实践的一致性。

Abstract: The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.

</details>


### [10] [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
*Xinpeng Wang,Nitish Joshi,Barbara Plank,Rico Angell,He He*

Main category: cs.AI

TL;DR: 提出了TRACE方法，通过截断推理过程来检测隐式奖励黑客行为，在数学推理和编程任务中显著优于现有CoT监控方法。


<details>
  <summary>Details</summary>
Motivation: 奖励黑客行为（模型利用奖励函数漏洞获取高分而不解决实际任务）是一个严重威胁，特别是隐式黑客行为会绕过现有的CoT监控。

Method: TRACE通过逐步截断模型的推理链，强制模型在推理被截断时给出答案，并测量验证器通过率。黑客模型会因走捷径而在推理链很短时就获得高通过率。

Result: 在数学推理任务中比最强的72B CoT监控器提升超过65%，在编程任务中比32B监控器提升超过30%，并能发现训练中的未知漏洞。

Conclusion: TRACE为当前监控方法无效的监督场景提供了一种可扩展的无监督方法。

Abstract: Reward hacking, where a reasoning model exploits loopholes in a reward
function to achieve high rewards without solving the intended task, poses a
significant threat. This behavior may be explicit, i.e. verbalized in the
model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus
bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE
(Truncated Reasoning AUC Evaluation). Our key observation is that hacking
occurs when exploiting the loophole is easier than solving the actual task.
This means that the model is using less `effort' than required to achieve high
reward. TRACE quantifies effort by measuring how early a model's reasoning
becomes sufficient to pass a verifier. We progressively truncate a model's CoT
at various lengths, force the model to answer, and measure the verifier-passing
rate at each cutoff. A hacking model, which takes a shortcut, will achieve a
high passing rate with only a small fraction of its CoT, yielding a large area
under the accuracy-vs-length curve. TRACE achieves over 65% gains over our
strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B
monitor in coding. We further show that TRACE can discover unknown loopholes
during training. Overall, TRACE offers a scalable unsupervised approach for
oversight where current monitoring methods prove ineffective.

</details>


### [11] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: 通过蒸馏学习将推理时检索转化为学习能力，让LLM智能体内部化检索知识，减少运行时依赖


<details>
  <summary>Details</summary>
Motivation: LLM智能体在多步任务中经常因未满足前提条件、冗余命令或环境约束处理不当而失败。检索增强生成(RAG)虽能提升性能，但需要维护外部知识库且增加计算开销

Method: 提出三步蒸馏管道：1)从失败中提取紧凑可重用提示；2)在回合开始时通过一次性检索生成改进的教师轨迹；3)训练学生模型学习这些轨迹（移除提示字符串），强制内部化而非记忆

Result: 在ALFWorld和WebShop两个交互基准上，蒸馏学生模型表现优于基线，ALFWorld成功率91%(基线79%)，WebShop得分72(基线61)，同时比检索增强教师少用10-60%token

Conclusion: 该方法在不同模型规模(7B/14B)和智能体架构(ReAct/StateAct)上均有效，表明检索优势可通过针对性微调内部化，无需永久运行时依赖

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [12] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: 提出使用大语言模型（LLM）代理自动进行数据驱动建模和分析的管道，特别关注回归任务。通过多代理系统和单代理系统框架，在临界热通量预测基准测试中取得了与专家开发的模型相当的预测精度。


<details>
  <summary>Details</summary>
Motivation: 现代工程依赖大量实验和模拟数据，需要高效可靠的建模策略。传统数据驱动方法需要大量人工干预，难以扩展和泛化到不同应用。

Method: 使用两种LLM代理框架：多代理系统（专业协作代理）和单代理系统（基于ReAct范式）。这些框架自主处理数据预处理、神经网络开发、训练、超参数优化和不确定性量化。

Result: 在包含约25,000个实验数据点的临界热通量预测基准测试中，LLM代理开发的模型超越了传统查找表，预测精度和不确定性量化与专家开发的贝叶斯优化深度神经网络模型相当。

Conclusion: LLM代理在自动化复杂工程建模任务方面具有巨大潜力，能显著减少人工工作量，同时达到或超越现有预测性能标准。

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [13] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX是一个基于LLM的自主AI代理，将原始系统日志转换为基于本体的知识图谱，并通过RAG和迭代校正确保语义有效性，最终映射到MITRE ATT&CK战术。


<details>
  <summary>Details</summary>
Motivation: 系统日志是宝贵的网络威胁情报来源，但由于缺乏结构、语义不一致和跨设备碎片化，其效用受到限制。需要能够将噪声异构数据转换为连贯可互操作表示的方法。

Method: 集成轻量级日志本体与检索增强生成(RAG)和迭代校正步骤，确保生成的知识图谱在语法和语义上有效。系统将KGs聚合到会话中，并使用LLM预测MITRE ATT&CK战术。

Result: 在公共基准和真实世界蜜罐数据集上的评估表明，OntoLogX在多个KG后端上具有稳健的KG生成能力，并能准确将对抗活动映射到ATT&CK战术。检索和校正提高了精确率和召回率。

Conclusion: 基于本体的表示为可操作的CTI提取提供了价值，代码导向模型在结构化日志分析中有效，检索和校正机制对精度和召回率有益。

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [14] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: Falconer是一个结合LLM智能推理与轻量代理模型的可扩展知识挖掘框架，通过LLM作为规划器和标注器，训练小型代理模型执行分类和提取任务，在保持高准确率的同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决大规模知识挖掘中LLM部署成本过高，而传统分类器-提取器管道脆弱且无法泛化到新任务的问题。

Method: 提出Falconer协作框架，LLM作为规划器分解用户指令为可执行管道，作为标注器生成监督数据训练小型代理模型，统一分类和提取为两个原子操作。

Result: Falconer在指令跟随准确率上接近最先进LLM，同时减少推理成本达90%，加速大规模知识挖掘超过20倍。

Conclusion: Falconer为深度研究提供了高效可扩展的基础，实现了LLM智能推理与轻量模型效率的平衡。

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [15] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: 本文探讨了如何利用领域专家精心整理的知识来创建更有效的智能教学系统，重点介绍了两种方法：使用可解释AI技术自动生成课程，以及利用专家指定的课程体系开发自适应教学系统。


<details>
  <summary>Details</summary>
Motivation: AI教育社区往往忽视了领域专家精心整理的知识在创建有效教学系统中的重要作用。本文旨在强调这种高度专业化的专家知识如何帮助开发新颖的教育系统。

Method: 提出了两种方法：1）使用可解释AI技术结合专家指定的问题解决规则自动生成课程；2）利用专家指定的学习课程体系开发自适应教学系统。通过授粉者识别教学系统的案例研究验证了这些方法。

Result: 研究表明，专家知识可以帮助创建更有效的教学系统，不仅提供更好的学习体验，还能使用更高效的算法来构建这些系统。

Conclusion: 领域专家精心整理的知识在开发智能教学系统中具有重要价值，通过可解释AI技术和专家指定的课程体系，可以创建更有效、自适应的教育系统。

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [16] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: VOGUE通过将探索从文本输出空间转移到视觉输入空间，利用视觉不确定性指导探索，显著提升了多模态大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在处理多模态输入时，将视觉输入视为固定条件，忽略了视觉变化带来的模糊性，导致策略对合理视觉变化缺乏鲁棒性。

Method: VOGUE将图像视为随机上下文，通过计算原始分支和噪声分支之间的对称KL散度来量化策略对视觉扰动的敏感性，创建不确定性感知的探索信号，并结合token熵奖励和退火采样调度来平衡探索与利用。

Result: 在GRPO框架下，VOGUE在Qwen2.5-VL-3B/7B模型上，将三个视觉数学基准的pass@1准确率平均提升2.6%，三个通用领域推理基准提升3.7%，同时提高了pass@4性能并缓解了RL微调中常见的探索衰减问题。

Conclusion: 基于视觉输入固有不确定性的探索策略是提升多模态推理的有效方法。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [17] [AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance](https://arxiv.org/abs/2510.01474)
*Bill Marino,Rosco Hunter,Zubair Jamali,Marinos Emmanouil Kalpakos,Mudra Kashyap,Isaiah Hinton,Alexa Hanson,Maahum Nazir,Christoph Schnabl,Felix Steffek,Hongkai Wen,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 提出了首个用于评估LLM在AI法规合规性检查方面性能的基准数据集AIReg-Bench，基于欧盟AI法案构建，包含120个技术文档样本和专家标注的合规性标签。


<details>
  <summary>Details</summary>
Motivation: 随着政府对AI的监管加强，需要评估LLM在AI法规合规性检查方面的能力，但目前缺乏相应的基准测试方法。

Method: 通过两步流程创建数据集：(1)使用结构化指令提示LLM生成120个虚构但合理的AI系统技术文档；(2)法律专家审查并标注每个样本违反AI法案的具体条款。

Result: 创建了包含120个样本的基准数据集，并评估了前沿LLM在重现专家合规性标签方面的表现，为理解LLM在AIR合规评估中的机会和局限性提供了起点。

Conclusion: AIReg-Bench为评估LLM在AI法规合规性检查方面的能力建立了首个基准，数据集和评估代码已开源，可供后续LLM比较使用。

Abstract: As governments move to regulate AI, there is growing interest in using Large
Language Models (LLMs) to assess whether or not an AI system complies with a
given AI Regulation (AIR). However, there is presently no way to benchmark the
performance of LLMs at this task. To fill this void, we introduce AIReg-Bench:
the first benchmark dataset designed to test how well LLMs can assess
compliance with the EU AI Act (AIA). We created this dataset through a two-step
process: (1) by prompting an LLM with carefully structured instructions, we
generated 120 technical documentation excerpts (samples), each depicting a
fictional, albeit plausible, AI system - of the kind an AI provider might
produce to demonstrate their compliance with AIR; (2) legal experts then
reviewed and annotated each sample to indicate whether, and in what way, the AI
system described therein violates specific Articles of the AIA. The resulting
dataset, together with our evaluation of whether frontier LLMs can reproduce
the experts' compliance labels, provides a starting point to understand the
opportunities and limitations of LLM-based AIR compliance assessment tools and
establishes a benchmark against which subsequent LLMs can be compared. The
dataset and evaluation code are available at
https://github.com/camlsys/aireg-bench.

</details>


### [18] [Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates](https://arxiv.org/abs/2510.01500)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: LToT是一种改进的Tree-of-Thoughts搜索控制器，通过分离效用和逻辑一致性，将低效用但一致的候选视为资产而非浪费，解决了广度饱和和深度近视问题。


<details>
  <summary>Details</summary>
Motivation: 标准Tree-of-Thoughts搜索在大计算预算下存在两个问题：广度饱和（额外样本产生近重复）和深度近视（噪声短期效用剪枝有长期回报的分支）。

Method: LToT将前沿分为主线（高效用候选用于开发）和侧线（一致但初始低效用候选），通过侧向竞赛与短路机制探索侧线，使用有上限的连续减半竞赛在宽侧线集上分布微小探测。

Result: 理论证明侧向成本为伪线性Θ(N₀ log_η N₀)，与无上限主线的指数增长形成对比。实证评估正在准备中。

Conclusion: LToT将大测试时预算转化为原则性多样性，同时保持提升纪律，在不增加计算的情况下缓解饱和和近视问题。

Abstract: Modern deployments increasingly allocate large test-time compute (thousands
of tokens or many node expansions) to boost reliability. Under such budgets,
standard Tree-of-Thoughts-style search exhibits two pathologies: breadth
saturation (additional samples mostly produce near-duplicates, so width stops
growing) and depth myopia (noisy short-horizon utilities prune branches whose
payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts
(LToT), a drop-in controller that separates utility from logical consistency
and treats low-utility but consistent candidates as assets rather than waste.
The frontier is split into mainlines (high-utility candidates used for
exploitation) and laterals (consistent, initially low-utility candidates that
receive short, cheap probes before judgment). LToT explores laterals via
Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race
that spreads tiny probes across a very wide lateral set, uses width-aware
thresholds with repeat-to-confirm, and immediately promotes a branch once its
envelope clears the mainline bar; mainlines are kept intentionally narrow so
surplus compute is invested where width is cheap. We prove a pseudolinear
lateral cost $\Theta(N_0 \log_{\eta} N_0)$ with logarithmically many rungs
(initial lateral width $N_0$; culling factor $\eta>1$), in contrast to the
exponential growth of uncapped mainlines. Empirical evaluations on benchmark
tasks are in preparation and will be added in a future revision. In short, LToT
turns large test-time budgets into principled diversity while preserving
promotion discipline, mitigating saturation and myopia without inflating
compute.

</details>


### [19] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: 提出一种基于稀疏自编码器和聚类技术的方法，用于分析大语言模型的内部标记表示并指导数学推理任务的生成过程。


<details>
  <summary>Details</summary>
Motivation: 为了在大语言模型的数学推理任务中实现高质量的推理过程，需要平衡利用已知推理轨迹和探索新推理路径之间的关系。

Method: 首先训练稀疏自编码器生成训练标记的稀疏向量表示，然后应用k-means聚类构建图结构，其中顶点代表标记簇，加权边捕获顺序标记转移。基于此图定义基于边权重的奖励函数来量化对已建立推理轨迹的遵循程度。

Result: 研究发现平衡利用和探索对于在数学推理任务中实现高准确率至关重要。稀疏自编码器可以作为可扩展的奖励模型来指导生成过程。

Conclusion: 该方法能够防止极端行为，确保在利用和探索之间取得平衡，最终促进大语言模型中更高质量的推理过程。

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [20] [LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning](https://arxiv.org/abs/2510.01530)
*Navapat Nananukul,Yue Zhang,Ryan Lee,Eric Boxer,Jonathan May,Vibhav Giridhar Gogate,Jay Pujara,Mayank Kejriwal*

Main category: cs.AI

TL;DR: 提出了LOGicalThought (LogT)神经符号架构，结合高级逻辑语言推理器和LLM，构建双重符号图上下文和逻辑上下文，将长文本推理转化为紧凑的接地评估，在四个多领域基准测试中性能提升11.84%。


<details>
  <summary>Details</summary>
Motivation: 高保证推理需要准确、可验证且明确基于证据的结论，但LLMs在标准推理任务中的能力无法满足高保证文本指南的严格推理要求，特别是在涉及否定、蕴含和可废止规则等逻辑结构时。

Method: 使用高级逻辑语言推理器与LLM结合，构建符号图上下文和逻辑上下文双重表示，将长文本指南推理问题转化为紧凑的接地评估。

Result: 在四个多领域基准测试中，相比四个基线模型，LogT整体性能提升11.84%，在否定推理上提升10.2%，蕴含推理提升13.2%，可废止推理提升5.5%。

Conclusion: LogT神经符号架构有效解决了高保证推理中的逻辑挑战，显著提升了LLMs在复杂逻辑推理任务中的表现。

Abstract: High-assurance reasoning, particularly in critical domains such as law and
medicine, requires conclusions that are accurate, verifiable, and explicitly
grounded in evidence. This reasoning relies on premises codified from rules,
statutes, and contracts, inherently involving defeasible or non-monotonic logic
due to numerous exceptions, where the introduction of a single fact can
invalidate general rules, posing significant challenges. While large language
models (LLMs) excel at processing natural language, their capabilities in
standard inference tasks do not translate to the rigorous reasoning required
over high-assurance text guidelines. Core reasoning challenges within such
texts often manifest specific logical structures involving negation,
implication, and, most critically, defeasible rules and exceptions. In this
paper, we propose a novel neurosymbolically-grounded architecture called
LOGicalThought (LogT) that uses an advanced logical language and reasoner in
conjunction with an LLM to construct a dual symbolic graph context and
logic-based context. These two context representations transform the problem
from inference over long-form guidelines into a compact grounded evaluation.
Evaluated on four multi-domain benchmarks against four baselines, LogT improves
overall performance by 11.84% across all LLMs. Performance improves
significantly across all three modes of reasoning: by up to +10.2% on negation,
+13.2% on implication, and +5.5% on defeasible reasoning compared to the
strongest baseline.

</details>


### [21] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: InfoSeeker是一个LLM决策框架，通过主动信息寻求来对齐内部动态，在部分可观测环境中实现最优决策。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划智能体在处理观测不确定性时，往往忽略了内部动态与实际环境之间的差异，导致在信息不完整和噪声动态的实际环境中表现不佳。

Method: 提出InfoSeeker框架，提示LLM主动收集信息，通过规划行动来验证理解、检测环境变化或测试假设，然后生成或修订任务导向计划。

Result: 在部分可观测环境基准测试中，InfoSeeker相比先前方法实现了74%的绝对性能提升，且不牺牲样本效率。在机器人操作和网页导航等基准测试中也优于基线方法。

Conclusion: 在部分可观测环境中，紧密集成规划和信息寻求对于实现鲁棒行为至关重要。

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [22] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: 提出了Step-Aware Policy Optimization (SAPO)算法，通过过程奖励函数引导扩散语言模型学习结构化推理路径，解决现有方法中推理步骤无意义的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的方法依赖稀疏的结果奖励，可能强化导致偶然正确结果的错误推理路径，这与推理的自然结构不匹配。

Method: 首先提出理论框架将复杂问题解决形式化为层次选择过程，然后引入SAPO算法，使用过程奖励函数使扩散语言模型的去噪过程与潜在推理层次对齐。

Result: 实验结果表明该方法在挑战性推理基准上显著提升性能，并增强生成过程的可解释性。

Conclusion: 基于理论框架的SAPO算法能够有效引导扩散语言模型学习结构化推理，解决现有方法的缺陷。

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [23] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: InvThink是一种让大语言模型具备逆向思维能力的方法，通过先分析潜在危害及其后果，再生成安全响应来提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法直接优化安全响应，但缺乏对潜在风险的系统性思考。InvThink旨在让模型在生成响应前主动考虑失败模式。

Method: 1) 枚举潜在危害 2) 分析后果 3) 生成主动避免这些风险的安全输出。通过监督微调和强化学习在三个LLM家族中实现。

Result: 相比基线方法SafetyPrompt，有害响应减少达15.7%；安全改进随模型规模扩展更强；缓解安全税，保持标准基准上的通用推理能力；在医疗、金融、法律等高风险领域表现优异。

Conclusion: 逆向推理为构建更安全、更强大的语言模型提供了可扩展和泛化的路径。

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [24] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: AdvEvo-MARL是一个协同进化的多智能体强化学习框架，通过内部化安全机制来解决LLM多智能体系统中的安全漏洞问题，无需依赖外部防护模块。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM多智能体系统在规划、工具使用和角色协调方面表现出色，但其开放性和交互复杂性使其容易受到越狱、提示注入和对抗性协作攻击。现有防御方法存在性能不足或系统开销大的问题。

Method: 提出AdvEvo-MARL框架，通过对抗学习环境联合优化攻击者（合成进化越狱提示）和防御者（任务智能体），引入公共基线进行优势估计，同一功能组内的智能体共享组级平均回报基线。

Result: 在代表性攻击场景中，AdvEvo-MARL始终将攻击成功率保持在20%以下，而基线方法可达38.33%，同时保持甚至提高任务准确率（推理任务最高提升3.67%）。

Conclusion: 安全性和实用性可以在不依赖额外防护智能体或增加系统开销的情况下共同提升。

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [25] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: AgentRec是一个基于LLM的多智能体协作推荐框架，通过分层智能体网络解决动态用户偏好、对话连贯性和多目标平衡的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有交互式对话推荐系统在处理动态用户偏好、保持对话连贯性和平衡多个排序目标方面存在显著挑战。

Method: 采用分层智能体网络，包含对话理解、偏好建模、上下文感知和动态排序等专门LLM智能体，通过自适应权重机制协调，结合三层学习策略。

Result: 在三个真实数据集上的实验显示，AgentRec在对话成功率提升2.8%、推荐准确率(NDCG@10)提升1.9%、对话效率提升3.2%，同时保持可比较的计算成本。

Conclusion: AgentRec通过智能体协作有效解决了现有推荐系统的局限性，实现了更好的推荐性能和对话体验。

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [26] [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
*Min Zeng*

Main category: cs.AI

TL;DR: 论文研究了LLMs在心理咨询领域的应用潜力，通过开发基于美国国家咨询师认证考试的PsychoBench基准测试，评估LLMs是否具备成为心理咨询师的知识能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在需要认知能力的应用领域（如心理咨询）的潜力，评估LLMs是否能够达到专业心理咨询师的资格标准。

Method: 开发PsychoBench基准测试，包含2,252个精心设计的单选题，基于美国国家咨询师认证考试，涵盖心理学各子学科，要求深度理解和广泛知识。

Result: 先进模型如GPT-4o、Llama3.3-70B和Gemma3-27B表现远超通过阈值（约70%准确率），而较小的开源模型（如Qwen2.5-7B、Mistral-7B）远低于该标准。

Conclusion: 目前只有前沿LLMs能够达到心理咨询考试标准，这突显了开发心理学导向LLMs的潜力和挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of industries, primarily due to their impressive generative
abilities. Yet, their potential in applications requiring cognitive abilities,
such as psychological counseling, remains largely untapped. This paper
investigates the key question: Can LLMs be effectively applied to psychological
counseling? To determine whether an LLM can effectively take on the role of a
psychological counselor, the first step is to assess whether it meets the
qualifications required for such a role, namely the ability to pass the U.S.
National Counselor Certification Exam (NCE). This is because, just as a human
counselor must pass a certification exam to practice, an LLM must demonstrate
sufficient psychological knowledge to meet the standards required for such a
role. To address this, we introduce PsychoBench, a benchmark grounded in
U.S.national counselor examinations, a licensure test for professional
counselors that requires about 70% accuracy to pass. PsychoBench comprises
approximately 2,252 carefully curated single-choice questions, crafted to
require deep understanding and broad enough to cover various sub-disciplines of
psychology. This benchmark provides a comprehensive assessment of an LLM's
ability to function as a counselor. Our evaluation shows that advanced models
such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing
threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)
remain far below it. These results suggest that only frontier LLMs are
currently capable of meeting counseling exam standards, highlighting both the
promise and the challenges of developing psychology-oriented LLMs.

</details>


### [27] [Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs](https://arxiv.org/abs/2510.01620)
*Peidong Liu,Junjiang Lin,Shaowen Wang,Yao Xu,Haiqing Li,Xuhao Xie,Siyi Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出基于信息论的上下文马尔可夫决策过程(CMDPs)总结方法，使用LLMs压缩高维上下文为低维语义摘要，提升决策效率并减少计算成本


<details>
  <summary>Details</summary>
Motivation: 现有CMDP方法在高维或非结构化上下文中泛化能力差，导致计算量大、性能不稳定，需要更高效的上下文处理方法

Method: 使用LLMs进行信息论总结，将上下文输入压缩为低维语义丰富的摘要，基于近似上下文充分性概念，提供遗憾边界和延迟-熵权衡分析

Result: 在离散、连续、视觉和推荐基准测试中优于原始上下文和非上下文基线，提高奖励、成功率、样本效率，同时降低延迟和内存使用

Conclusion: LLM基于的总结为上下文丰富、资源受限环境中的高效决策提供了可扩展和可解释的解决方案

Abstract: Contextual Markov Decision Processes (CMDPs) offer a framework for sequential
decision-making under external signals, but existing methods often fail to
generalize in high-dimensional or unstructured contexts, resulting in excessive
computation and unstable performance. We propose an information-theoretic
summarization approach that uses large language models (LLMs) to compress
contextual inputs into low-dimensional, semantically rich summaries. These
summaries augment states by preserving decision-critical cues while reducing
redundancy. Building on the notion of approximate context sufficiency, we
provide, to our knowledge, the first regret bounds and a latency-entropy
trade-off characterization for CMDPs. Our analysis clarifies how
informativeness impacts computational cost. Experiments across discrete,
continuous, visual, and recommendation benchmarks show that our method
outperforms raw-context and non-context baselines, improving reward, success
rate, and sample efficiency, while reducing latency and memory usage. These
findings demonstrate that LLM-based summarization offers a scalable and
interpretable solution for efficient decision-making in context-rich,
resource-constrained environments.

</details>


### [28] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: 本文研究大语言模型的地理空间推理能力，特别是能否理解道路网络地图并执行导航任务。通过轨迹恢复作为代理任务，提出GLOBALTRACE数据集和提示框架，使LLM无需外部导航工具即可生成有效路径。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在地理空间推理方面的能力，特别是它们是否能够理解道路网络地图并执行导航任务，这对于开发更智能的导航系统具有重要意义。

Method: 将轨迹恢复作为代理任务，构建包含4000多条真实世界轨迹的GLOBALTRACE数据集，使用道路网络作为上下文，通过提示框架使LLM能够生成有效路径而不依赖外部导航工具。

Result: 实验表明LLM在轨迹恢复任务上优于现有基线和专门模型，具有强大的零样本泛化能力。细粒度分析显示LLM对道路网络和坐标系有良好理解，但也存在区域和交通模式的系统性偏差。

Conclusion: 大语言模型具备强大的地理空间推理能力，能够通过灵活的地图推理来增强导航体验，并整合用户偏好，但需要注意其存在的系统性偏差问题。

Abstract: We explore the geospatial reasoning capabilities of Large Language Models
(LLMs), specifically, whether LLMs can read road network maps and perform
navigation. We frame trajectory recovery as a proxy task, which requires models
to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with
over 4,000 real-world trajectories across diverse regions and transportation
modes. Using road network as context, our prompting framework enables LLMs to
generate valid paths without accessing any external navigation tools.
Experiments show that LLMs outperform off-the-shelf baselines and specialized
trajectory recovery models, with strong zero-shot generalization. Fine-grained
analysis shows that LLMs have strong comprehension of the road network and
coordinate systems, but also pose systematic biases with respect to regions and
transportation modes. Finally, we demonstrate how LLMs can enhance navigation
experiences by reasoning over maps in flexible ways to incorporate user
preferences.

</details>


### [29] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: 本研究展示了GuruAgents（提示引导的AI代理）能够系统化实施传奇投资大师的策略。通过将五位标志性投资者的哲学编码到LLM提示中，结合金融工具和确定性推理流程，在纳斯达克100成分股的回测中，巴菲特代理表现最佳，年化回报率达42.2%。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过提示工程将投资大师的定性哲学转化为可复现的定量策略，为自动化系统化投资开辟新方向。

Method: 开发五个不同的GuruAgents，每个代理模拟一位标志性投资者，通过将他们的投资哲学编码到LLM提示中，并整合金融工具和确定性推理流程。

Result: 在2023年第四季度至2025年第二季度的纳斯达克100成分股回测中，GuruAgents展现出由其提示人格驱动的独特行为。巴菲特代理表现最佳，年化回报率达42.2%，显著超越基准，其他代理结果各异。

Conclusion: 提示工程能够成功将投资大师的定性哲学转化为可复现的定量策略，这为自动化系统化投资指明了新的研究方向。

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [30] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: 本文识别了计算机使用代理(CUAs)中的盲目目标导向(BGD)风险，开发了BLIND-ACT基准测试，发现前沿模型平均BGD率高达80.8%，揭示了执行优先偏见、思维-行动脱节和请求优先等失败模式。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理(CUAs)在GUI上执行操作以实现用户目标，但存在盲目追求目标而忽视可行性、安全性和上下文的系统性偏见，需要系统性地识别和量化这种风险。

Method: 开发BLIND-ACT基准测试，包含90个任务捕捉三种BGD模式：缺乏上下文推理、在模糊性下做出假设和决策、矛盾或不可行目标。基于OSWorld构建真实环境，使用LLM评估器评估代理行为。

Result: 评估9个前沿模型(包括Claude Sonnet和Opus 4、Computer-Use-Preview、GPT-5)，观察到平均BGD率为80.8%。提示干预能降低BGD水平但风险仍然显著。

Conclusion: BGD暴露了即使输入不直接有害时也会出现的微妙风险，需要更强的训练或推理时干预措施。识别BGD和引入BLIND-ACT为未来研究和缓解这一基本风险奠定了基础。

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [31] [A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)
*Motoki Sato,Yuki Matsushita,Hidekazu Takahashi,Tomoaki Kakazu,Sou Nagata,Mizuho Ohnuma,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.AI

TL;DR: LENOHA是一个安全优先、本地优先的系统，使用高精度分类器从临床FAQ中返回原文答案，避免生成错误，在牙科拔牙和胃镜检查两个领域测试显示准确率达98.3%，能耗比生成式模型低170倍。


<details>
  <summary>Details</summary>
Motivation: 患者在接受侵入性手术前常有未解答的问题，但时间压力和隐私限制限制了个性化咨询。需要开发安全、高效的系统来提供准确信息。

Method: 使用高精度句子转换器分类器路由输入，从临床医生策划的FAQ中返回原文答案，避免自由文本生成。评估了四个编码器在两个临床领域的表现。

Result: E5-large-instruct编码器总体准确率0.983，AUC 0.996，仅有7个错误。非生成式临床路径能耗仅为1.0 mWh/输入，比本地8B SLM小聊回复低170倍，延迟约0.10秒。

Conclusion: 通过返回经过验证的FAQ原文答案，可以在临床路径中结构性地避免前沿判别和生成引起的错误，支持隐私保护、可持续性和在带宽有限环境中的公平部署。

Abstract: Patients awaiting invasive procedures often have unanswered pre-procedural
questions; however, time-pressured workflows and privacy constraints limit
personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave
No One Behind Architecture), a safety-first, local-first system that routes
inputs with a high-precision sentence-transformer classifier and returns
verbatim answers from a clinician-curated FAQ for clinical queries, eliminating
free-text generation in the clinical path. We evaluated two domains (tooth
extraction and gastroscopy) using expert-reviewed validation sets
(n=400/domain) for thresholding and independent test sets (n=200/domain). Among
the four encoders, E5-large-instruct (560M) achieved an overall accuracy of
0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were
statistically indistinguishable from GPT-4o on this task; Gemini made no errors
on this test set. Energy logging shows that the non-generative clinical path
consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local
8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single
on-prem GPU. These results indicate that near-frontier discrimination and
generation-induced errors are structurally avoided in the clinical path by
returning vetted FAQ answers verbatim, supporting privacy, sustainability, and
equitable deployment in bandwidth-limited environments.

</details>


### [32] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: 本文主张AGI评估方法应从基于直觉的合成任务转向关注稳健任务执行能力的评估，借鉴数据科学实践来展示系统的可靠部署能力。


<details>
  <summary>Details</summary>
Motivation: 当前AGI评估方法主要基于对智能的直觉设计合成任务，但这些方法在AI历史上表现不佳，需要更有效的评估框架。

Method: 提出基于数据科学实践的替代设计哲学，专注于评估稳健任务执行能力，通过实际部署能力来展示AGI。

Result: 提供了AGI评估的实际示例，展示了如何通过稳健任务执行来验证AGI能力。

Conclusion: AGI评估应转向以能力为导向的方法，强调系统的实际部署可靠性和任务执行稳健性，而非依赖直觉设计的合成任务。

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [33] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: 提出了VaPR框架，通过LLM引导的响应编辑生成硬负样本，解决了合成偏好标注中的风格和长度偏差问题，显著提升了大型视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有偏好微调方法忽视了合成偏好标注中普遍存在的风格和长度偏差噪声，需要开发能够产生高质量负样本的方法来改进模型对齐效果。

Method: 基于LLM引导的响应编辑构建硬负响应生成框架，产生具有目标错误但保持风格和长度相似性的拒绝响应，并创建了包含30K高质量样本的VaPR数据集。

Result: 在三个LVLM家族上显著提升性能，平均增益分别为6.5%(LLaVA)、4.0%(Qwen2VL)和1.5%(Qwen2.5VL)，在推理任务上表现尤为突出，同时减少了二元问题中回答"是"的倾向。

Conclusion: VaPR框架有效解决了合成偏好标注的噪声问题，能够持续提升模型性能，且可泛化到开源LLM作为编辑器，为LVLM的对齐提供了高质量数据解决方案。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [34] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-Félix Nothias*

Main category: cs.AI

TL;DR: MetaboT是一个基于大语言模型的多智能体AI系统，能够将用户的自然语言问题转换为SPARQL查询语言，用于操作代谢组学知识图谱，显著提高了查询准确率。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱使用中需要深入理解本体论和查询语言语法的技术障碍，使研究人员能够通过自然语言轻松访问结构化代谢组学数据。

Method: 采用多智能体系统架构，包括入口代理、验证代理、监督代理、知识图谱代理和SPARQL查询生成代理，使用LangChain和LangGraph库集成LLM与外部工具。

Result: 在50个代谢组学相关问题的测试中，MetaboT达到83.67%的准确率，显著优于仅使用GPT-4o的基线方法（8.16%）。

Conclusion: MetaboT成功桥接了复杂语义技术与用户友好交互之间的差距，通过自动化SPARQL查询生成和执行，促进了数据驱动的研究发现。

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [35] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: 提出了一个结构化决策支持框架，将不同AI智能体架构（反应式、认知式、混合式和学习式）与NIST网络安全框架2.0系统对齐，为AI解决方案的选择和部署提供透明方法。


<details>
  <summary>Details</summary>
Motivation: 弥合理论AI构建与操作网络安全需求之间的差距，提供统一的检测、事件响应和治理策略，超越孤立的AI应用。

Method: 通过将NIST CSF 2.0功能细分为具体任务，将AI智能体属性（自主性、自适应学习、实时响应）与每个子类别的安全要求相连接，并定义分级自主水平以适应不同网络安全成熟度组织。

Result: 概念验证表明，定制化的AI智能体部署可以增强态势感知、加速响应时间，并通过自适应风险管理加强长期韧性。

Conclusion: 该研究为遵循行业标准的稳健、经验验证的多智能体系统奠定了基础，将AI理论与行业指南相结合。

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [36] [REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing](https://arxiv.org/abs/2510.01800)
*Thanh Ma,Tri-Tam La,Lam-Thu Le Huu,Minh-Nghi Nguyen,Khanh-Van Pham Luu,Huu-Hoa Nguyen*

Main category: cs.AI

TL;DR: 提出了REBot学术咨询聊天机器人，采用CatRAG混合检索推理框架，结合检索增强生成和图推理，在学术规定咨询任务中达到98.89%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 学术规定咨询需要帮助学生理解和遵守制度政策，但构建有效系统需要领域特定的监管资源。

Method: CatRAG框架集成密集检索和图推理，使用带有语义特征的层次化类别标记知识图谱，轻量级意图分类器将查询路由到适当的检索模块。

Result: 在分类和问答任务评估中，REBot达到最先进性能，F1分数为98.89%。

Conclusion: 开发了展示REBot在实际学术咨询场景中实用价值的Web应用程序。

Abstract: Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.

</details>


### [37] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出一个可信赖的协同学习模型，用于军事行动中的人机协作，包含可调节自主性、多层控制、双向反馈和协作决策四个维度。


<details>
  <summary>Details</summary>
Motivation: 在快速演变的军事威胁和复杂作战环境中，AI集成带来优势但也面临挑战。当前研究多从外部视角处理人机协作系统，需要深入系统内部动态来处理多维责任、安全和鲁棒性方面的问题。

Method: 设计可信赖的协同学习模型，包含四个维度：可调节自主性（根据任务状态、系统置信度等动态调整自主级别）、多层控制（持续监督和问责）、双向反馈（显性和隐性反馈回路）、协作决策（生成、评估和提出决策及其置信度和理由）。

Result: 提出了具体示例和建议，有助于进一步开发负责任和可信赖的军事人机协作系统。

Conclusion: 该模型通过四个维度的整合，为人机协作系统在军事行动中的有效和道德部署提供了框架，强调了持续双向学习和适应性。

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [38] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: 提出了PTA-GRPO框架，通过两阶段方法改进LLM的推理能力：第一阶段使用高级LLM将思维链提炼为紧凑的高层指导进行监督微调；第二阶段引入指导感知的强化学习方法联合优化最终输出和高层指导质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的推理过程受限于自回归的token级生成，缺乏全局规划，导致推理冗余、不连贯或不准确。现有方法如树算法和RL计算成本高且难以产生最优推理轨迹。

Method: 两阶段框架：1) 使用高级LLM将思维链提炼为高层指导进行SFT；2) 引入指导感知的RL方法，联合优化最终输出和指导质量。

Result: 在多个数学推理基准测试(MATH、AIME2024、AIME2025、AMC)和不同基础模型(Qwen2.5-7B、Qwen3-8B、Qwen3-14B、LLaMA3.2-3B)上，PTA-GRPO均实现了稳定且显著的性能提升。

Conclusion: PTA-GRPO能够有效提升LLM的推理能力，在不同模型和任务上表现出良好的有效性和泛化性。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [39] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 该论文提出了一种对抗性逆强化学习方法，为大型语言模型推理学习密集的token级奖励模型，用于过程监督而非风格模仿。


<details>
  <summary>Details</summary>
Motivation: 重新构建和操作对抗性逆强化学习到大型语言模型推理中，直接从专家演示中学习密集的token级奖励模型，而不是通过监督微调模仿风格。

Method: 使用对抗性逆强化学习方法学习推理奖励，该奖励在训练时提供步骤级反馈来优化推理策略，在推理时作为批评者重新排序采样轨迹。

Result: 在GSM8K数据集上，使用Llama3和Qwen2.5骨干网络验证了：(1)密集推理奖励可用作学习信号来激发推理；(2)通过奖励引导的重新排序提高了预测性能（特别是对于基于Llama的策略）。

Conclusion: 通过将训练信号、推理时选择和token级诊断统一到单个推理奖励中，这项工作表明可重用的过程级奖励具有增强语言模型中多步推理的广泛潜力。

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [40] [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
*Paweł Parys,Sairam Vaidya,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: 提出了CARS方法，通过自适应剪枝技术提高受限生成的采样效率，同时保持语言模型的原始分布不变。


<details>
  <summary>Details</summary>
Motivation: 现有受限生成方法存在两极化问题：贪婪解码方法会扭曲语言模型分布，而拒绝采样方法计算效率低下。在程序模糊测试等需要有效性和多样性的领域，这两种方法都有问题。

Method: CARS从无约束的语言模型采样开始，通过记录违反约束的continuations到trie结构中，并从未来采样中减去其概率质量，实现自适应剪枝。

Result: 在程序模糊测试和分子生成等多个领域的实验中，CARS在采样效率（每个有效样本所需的LM前向传递次数）和样本多样性方面都优于现有方法。

Conclusion: CARS严格提升了拒绝采样的效率而不产生分布扭曲，确保前缀一旦被证明无效就永远不会被重新访问，接受率单调提升，生成的样本严格遵循受限分布。

Abstract: Language Models (LMs) are increasingly used in applications where generated
outputs must satisfy strict semantic or syntactic constraints. Existing
approaches to constrained generation fall along a spectrum: greedy constrained
decoding methods enforce validity during decoding but distort the LM's
distribution, while rejection sampling (RS) preserves fidelity but wastes
computation by discarding invalid outputs. Both extremes are problematic in
domains such as program fuzzing, where both validity and diversity of samples
are essential. We present Constrained Adaptive Rejection Sampling (CARS), an
approach that strictly improves the sample-efficiency of RS without
distributional distortion. CARS begins with unconstrained LM sampling and
adaptively rules out constraint-violating continuations by recording them in a
trie and subtracting their probability mass from future draws. This adaptive
pruning ensures that prefixes proven invalid are never revisited, acceptance
rates improve monotonically, and the resulting samples exactly follow the
constrained distribution. In experiments on a variety of domains -- e.g.,
program fuzzing and molecular generation -- CARS consistently achieves higher
efficiency -- measured in the number of LM forward passes per valid sample --
while also producing stronger sample diversity than both GCD and methods that
approximate the LM's distribution.

</details>


### [41] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,Clémentine Bouleau,Vivian Tsai,Maël Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [42] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: 提出了一个确定性模拟框架，为评估AI生成的同行评审报告提供首个稳定的证据标准，通过分析352份模拟报告验证了系统的可靠性和一致性。


<details>
  <summary>Details</summary>
Motivation: 学术出版生态系统面临投稿量不可管理和AI不受监管的双重危机，需要新的治理模型来保障科学完整性。传统纯人工同行评审缺乏可扩展的客观基准。

Method: 采用确定性模拟框架，分析352份同行评审模拟报告，识别一致的系统状态指标来验证可靠性。

Result: 系统能够模拟校准的编辑判断，'修订'决策在所有学科中始终占多数（>50%），'拒绝'率动态适应领域规范（健康科学达45%）；保持29%的证据锚定合规率，在不同评审任务和科学领域中保持稳定。

Conclusion: 该框架将AI重新定位为机构问责的关键组成部分，为维持学术交流信任提供关键基础设施，为科学界提供确保公平的透明工具，为出版策略师提供可扩展的审计工具。

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [43] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD是一个为表格异常检测恢复文本语义的基准框架，提供20个带有结构化文本元数据的表格数据集，并实现了包括经典、深度学习和LLM方法在内的先进异常检测算法。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测基准仅提供原始数据点，缺乏语义上下文（如特征描述和领域知识），这限制了研究灵活性并阻碍模型充分利用领域知识进行检测。

Method: 通过恢复文本语义实现上下文感知的表格异常检测研究，包括：(1) 提供20个精心策划的带有结构化文本元数据的表格数据集和先进异常检测算法实现；(2) 提出无需任务特定训练的零样本LLM框架。

Result: 实验结果表明，语义上下文提高了检测性能，并通过支持领域感知推理增强了可解释性。

Conclusion: ReTabAD为系统探索上下文感知异常检测建立了基准，揭示了文本元数据在异常检测中的作用和实用性。

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [44] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: 对LLM深度层使用的系统研究表明，深层贡献因评估设置而异：基于似然的评估中大部分层可剪枝，而生成评估中中间和深层对推理和连贯性至关重要。


<details>
  <summary>Details</summary>
Motivation: 反驳现有研究认为LLM深层对表示学习贡献有限的片面观点，通过系统分析揭示深度使用的异质性和上下文依赖性。

Method: 从评估协议、任务类别和模型架构等多个维度系统研究深度利用情况，包括似然评估与生成评估的对比分析。

Result: 知识检索集中在浅层组件，推理准确性依赖深层但可通过蒸馏重塑；深度使用高度异质且依赖上下文。

Conclusion: LLM深度使用高度异质且上下文依赖，需要在解释和压缩大模型时考虑任务、度量和模型感知的视角。

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [45] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: 研究发现AI模型在文本模态下虽然能达到人类水平的准确率，但其抽象推理能力被高估，因为模型往往依赖表面模式而非真正的抽象概念；而在视觉模态下，模型的抽象推理能力可能被低估，因为它们能识别抽象概念但应用能力不足。


<details>
  <summary>Details</summary>
Motivation: 评估AI模型是否真正具备抽象推理能力，而不仅仅是表面模式匹配，特别是在OpenAI o3-preview模型在ARC-AGI基准上超越人类准确率后，需要更深入地理解模型的抽象推理本质。

Method: 在ConceptARC基准上评估模型，通过改变输入模态（文本vs视觉）、是否允许使用Python工具、以及推理模型的推理努力程度，同时评估输出准确率和模型生成的自然语言规则。

Result: 文本模态下模型准确率与人类相当，但规则分析显示模型主要依赖表面"捷径"而非真正抽象；视觉模态下准确率大幅下降，但规则分析显示模型仍能识别相当比例的抽象概念，只是应用能力不足。

Conclusion: 仅凭准确率评估抽象推理能力会高估文本模态下的能力而低估视觉模态下的能力，需要结合规则分析来更准确地评估模型的多模态抽象推理能力。

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [46] [FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models](https://arxiv.org/abs/2510.02133)
*Karan Dua,Hitesh Laxmichand Patel,Puneet Mittal,Ranjeet Gupta,Amit Agarwal,Praneet Pabolu,Srikant Panda,Hansa Meghwani,Graham Horwood,Fahad Shah*

Main category: cs.AI

TL;DR: FlexDoc是一个可扩展的合成数据生成框架，通过随机模式和参数化采样生成多语言半结构化文档，显著减少文档理解模型的数据收集和标注成本。


<details>
  <summary>Details</summary>
Motivation: 企业级文档理解模型需要大量多样化标注数据，但数据收集面临隐私限制、法律约束和手动标注成本高昂（可达数百万美元）的问题。

Method: 结合随机模式和参数化采样，通过概率建模布局模式、视觉结构和内容变异性，可控地生成多样化文档变体。

Result: 在关键信息提取任务中，FlexDoc生成的数据将F1分数绝对提升高达11%，同时相比传统硬模板方法减少90%以上的标注工作量。

Conclusion: FlexDoc已在实际部署中加速企业级文档理解模型的开发，同时显著降低了数据获取和标注成本。

Abstract: Developing document understanding models at enterprise scale requires large,
diverse, and well-annotated datasets spanning a wide range of document types.
However, collecting such data is prohibitively expensive due to privacy
constraints, legal restrictions, and the sheer volume of manual annotation
needed - costs that can scale into millions of dollars. We introduce FlexDoc, a
scalable synthetic data generation framework that combines Stochastic Schemas
and Parameterized Sampling to produce realistic, multilingual semi-structured
documents with rich annotations. By probabilistically modeling layout patterns,
visual structure, and content variability, FlexDoc enables the controlled
generation of diverse document variants at scale. Experiments on Key
Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data
improves the absolute F1 Score by up to 11% when used to augment real datasets,
while reducing annotation effort by over 90% compared to traditional
hard-template methods. The solution is in active deployment, where it has
accelerated the development of enterprise-grade document understanding models
while significantly reducing data acquisition and annotation costs.

</details>


### [47] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 本文提出了针对深度研究代理(DRAs)的严格基准和多维评估框架，包含214个专家策划的挑战性查询和手动构建的参考包，用于评估长格式报告的质量。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估维度、响应格式和评分机制方面存在不足，无法有效评估从封闭语言模型向互联代理系统转变的新型AI系统。

Method: 构建包含10个广泛主题领域的214个专家策划查询的基准，并开发多维评估框架，整合语义质量、主题聚焦和检索可信度的评分指标。

Result: 实验证实主流DRAs优于基于网络搜索工具增强的推理模型，但仍存在显著的改进空间。

Conclusion: 本研究为DRA系统的能力评估、架构优化和范式进步提供了坚实基础。

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [48] [UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194)
*Yuhao Sun,Zhuoer Xu,Shiwen Cui,Kun Yang,Lingyun Yu,Yongdong Zhang,Hongtao Xie*

Main category: cs.AI

TL;DR: UpSafe°C是一个通过安全感知升级来增强LLM安全性的统一框架，通过识别安全关键层并将其升级为稀疏MoE结构，结合两阶段SFT策略和安全温度机制，实现动态的安全-效用权衡控制。


<details>
  <summary>Details</summary>
Motivation: 现有安全技术（外部护栏、推理时指导和后训练对齐）在平衡安全性、实用性和可控性方面存在局限性，需要一种更灵活的动态安全控制方法。

Method: 识别安全关键层并升级为稀疏MoE结构，路由器作为软护栏选择性激活原始MLP和安全专家；采用两阶段SFT策略强化安全判别能力；引入安全温度机制实现推理时动态控制。

Result: 在多个基准测试、基础模型和模型规模上，UpSafe°C对有害和越狱输入实现了稳健的安全改进，同时在通用任务上保持竞争力，安全温度机制实现了效用和安全性的帕累托最优前沿。

Conclusion: 研究为LLM安全指明了新方向：从静态对齐转向动态、模块化和推理感知的控制。

Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.

</details>


### [49] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: RLVR方法本应增强大语言模型的推理能力，但研究发现它反而会缩小推理边界。论文揭示了RLVR中的负干扰现象和赢家通吃现象，并提出了一种专注于低概率问题的数据筛选算法来改善性能。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR被设计用于提升大语言模型的推理能力，但近期证据表明它可能反而会缩小推理边界。本文旨在深入探究RLVR失败的原因，分析其学习动态中的关键问题。

Method: 通过理论分析和在多个数学推理基准上的实证研究，揭示了RLVR中的负干扰现象和赢家通吃现象。基于这些发现，提出了一种专注于低概率问题的数据筛选算法。

Result: 研究发现RLVR会导致负干扰（学习解决某些训练问题会降低其他问题正确解的概率）和赢家通吃现象（过度强化高概率正确解，抑制低概率问题）。提出的数据筛选算法显著改善了Pass@k性能。

Conclusion: RLVR的失败源于标准RL目标中的固有策略采样，导致模型收敛于狭窄的解策略。通过专注于低概率问题的数据筛选，可以有效改善RLVR的性能，避免推理边界的缩小。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [50] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出了Behavior Best-of-N (bBoN)方法，通过生成多个执行轨迹并使用行为叙事进行选择，显著提升计算机使用代理在复杂任务中的可靠性和成功率。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理在自动化日常数字任务方面具有潜力，但其不可靠性和高变异性阻碍了在长期复杂任务中的应用。

Method: bBoN方法通过生成多个执行轨迹（rollouts），并使用描述代理执行过程的行为叙事来进行轨迹选择，实现广泛的探索和原则性的轨迹选择。

Result: 在OSWorld上达到69.9%的最新SOTA性能，显著超越先前方法，接近72%的人类水平性能。在WindowsAgentArena和AndroidWorld上也表现出强大的泛化能力。

Conclusion: 研究结果表明，当采用结构化轨迹理解和选择时，扩展计算机使用代理具有显著效果，bBoN为实现这一目标提供了实用框架。

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [51] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: 提出RLAD方法，通过推理抽象来引导模型进行有效的算法推理，避免冗长和退化的探索，提高对困难问题的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型模型在推理过程中往往无法一致地捕捉或重用过程，而是陷入冗长和退化的探索，需要更有效的推理方法。

Method: 引入推理抽象的概念，训练模型能够为问题提出多个抽象描述，然后使用强化学习激励在构建解决方案时利用这些抽象信息，形成抽象生成器和解决方案生成器的两玩家RL训练范式。

Result: RLAD方法实现了结构化探索，解耦了抽象提议和解决方案生成的学习信号，并提高了对更困难问题的泛化能力。

Conclusion: 推理抽象能有效引导有意义的探索，在测试时分配更多计算资源生成抽象比生成更多解决方案更有利于性能提升。

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


### [52] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: 提出BioX-Bridge框架，通过轻量级桥接网络实现生物信号跨模态知识迁移，大幅减少可训练参数(88-99%)，同时保持或提升迁移性能。


<details>
  <summary>Details</summary>
Motivation: 生物信号模态间存在相关性，但标注数据有限且现有知识蒸馏方法计算开销大，特别是对于大型基础模型。需要更高效的跨模态知识迁移方法。

Method: 训练轻量级桥接网络对齐基础模型的中间表示，实现跨模态信息流动。包括高效选择对齐位置策略和灵活的桥接网络架构。

Result: 在多个生物信号模态、任务和数据集上的实验表明，BioX-Bridge相比最先进方法减少88-99%可训练参数，同时保持或提升迁移性能。

Conclusion: BioX-Bridge为生物信号跨模态知识迁移提供了高效解决方案，显著降低计算和内存开销，同时保持性能优势。

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [53] [Next-Generation AI-Native Wireless Communications: MCMC-Based Receiver Architectures for Unified Processing](https://arxiv.org/abs/2510.01636)
*Xingyu Zhou,Le Liang,Jing Zhang,Chao-Kai Wen,Shi Jin*

Main category: cs.IT

TL;DR: 提出基于MCMC的AI驱动通用MIMO接收机架构，将信道估计、符号检测和信道解码集成到统一概率框架中，提高可解释性、可扩展性和灵活性。


<details>
  <summary>Details</summary>
Motivation: MIMO接收机处理面临复杂性和可扩展性挑战，AI技术为解决这些问题提供了潜力。现有AI方法将接收机处理视为黑盒，缺乏可解释性和灵活性。

Method: 采用马尔可夫链蒙特卡罗(MCMC)技术构建通用贝叶斯计算引擎，将信道估计、符号检测和信道解码等任务集成到统一概率框架中。

Result: 该方法增强了接收机在不同场景下的可解释性、可扩展性和灵活性，并能与数据驱动学习方法无缝结合。

Conclusion: 提出的统一框架能够实现整体性能优化，促进全智能通信接收机的发展。

Abstract: The multiple-input multiple-output (MIMO) receiver processing is a key
technology for current and next-generation wireless communications. However, it
faces significant challenges related to complexity and scalability as the
number of antennas increases. Artificial intelligence (AI), a cornerstone of
next-generation wireless networks, offers considerable potential for addressing
these challenges. This paper proposes an AI-driven, universal MIMO receiver
architecture based on Markov chain Monte Carlo (MCMC) techniques. Unlike
existing AI-based methods that treat receiver processing as a black box, our
MCMC-based approach functions as a generic Bayesian computing engine applicable
to various processing tasks, including channel estimation, symbol detection,
and channel decoding. This method enhances the interpretability, scalability,
and flexibility of receivers in diverse scenarios. Furthermore, the proposed
approach integrates these tasks into a unified probabilistic framework, thereby
enabling overall performance optimization. This unified framework can also be
seamlessly combined with data-driven learning methods to facilitate the
development of fully intelligent communication receivers.

</details>


### [54] [On Algebraic Approaches for DNA Codes with Multiple Constraints](https://arxiv.org/abs/2510.01750)
*Krishna Gopal Benerjee,Manish K Gupta*

Main category: cs.IT

TL;DR: 本章介绍DNA编码的最新约束条件，包括反向、反向互补、GC含量、汉明距离等约束，重点讨论使用代数方法构建具有高汉明距离的非循环DNA编码。


<details>
  <summary>Details</summary>
Motivation: DNA编码在DNA计算和数据存储中的应用日益重要，需要满足多种热力学和组合约束条件，现有代数方法大多无法产生高汉明距离的DNA编码。

Method: 使用代数构造方法，通过有限环和映射（通常是等距映射）来构建DNA编码，重点关注非循环DNA编码，并讨论各种距离度量方法。

Result: 提出了能够满足多种约束条件的高汉明距离DNA编码的代数构造方法，并讨论了相关的代数界限。

Conclusion: DNA编码研究需要进一步发展能够同时满足多个约束条件的构造方法，本章为这一领域提供了理论基础并指出了未来的研究方向。

Abstract: DNA strings and their properties are widely studied since last 20 years due
to its applications in DNA computing. In this area, one designs a set of DNA
strings (called DNA code) which satisfies certain thermodynamic and
combinatorial constraints such as reverse constraint, reverse-complement
constraint, $GC$-content constraint and Hamming constraint. However recent
applications of DNA codes in DNA data storage resulted in many new constraints
on DNA codes such as avoiding tandem repeats constraint (a generalization of
non-homopolymer constraint) and avoiding secondary structures constraint.
Therefore, in this chapter, we introduce DNA codes with recently developed
constraints. In particular, we discuss reverse, reverse-complement,
$GC$-content, Hamming, uncorrelated-correlated, thermodynamic, avoiding tandem
repeats and avoiding secondary structures constraints. DNA codes are
constructed using various approaches such as algebraic, computational, and
combinatorial. In particular, in algebraic approaches, one uses a finite ring
and a map to construct a DNA code. Most of such approaches does not yield DNA
codes with high Hamming distance. In this chapter, we focus on algebraic
constructions using maps (usually an isometry on some finite ring) which yields
DNA codes with high Hamming distance. We focus on non-cyclic DNA codes. We
briefly discuss various metrics such as Gau distance, Non-Homopolymer distance
etc. We discuss about algebraic constructions of families of DNA codes that
satisfy multiple constraints and/or properties. Further, we also discuss about
algebraic bounds on DNA codes with multiple constraints. Finally, we present
some open research directions in this area.

</details>


### [55] [List decoding of evaluation codes](https://arxiv.org/abs/2510.01811)
*Silouanos Brazitikos,Theodoulos Garefalakis,Eleni Tzanaki*

Main category: cs.IT

TL;DR: 本文研究了基于凸多面体的多项式求值码（Toric码）的列表译码问题，提出了Guruswami-Sudan算法的推广版本，并计算了译码半径的界限。


<details>
  <summary>Details</summary>
Motivation: 多项式求值码在编码理论中占据重要地位，但现有的列表译码研究主要集中在Reed-Solomon和Reed-Muller等特殊码型上，对于更一般的基于凸多面体的Toric码的列表译码问题研究不足。

Method: 提出了Guruswami-Sudan算法的推广版本，该算法考虑了凸多面体的几何和组合特性，用于解决Toric码的列表译码问题。

Result: 计算得到了Toric码列表译码的译码半径界限，为这类码的译码性能提供了理论保证。

Conclusion: 该工作扩展了列表译码理论的应用范围，为基于凸多面体的多项式求值码提供了有效的译码算法和性能分析。

Abstract: Polynomial evaluation codes hold a prominent place in coding theory. In this
work, we study the problem of list decoding for a general class of polynomial
evaluation codes, also known as Toric codes, that are defined for any given
convex polytope P. Special cases, such as Reed-Solomon and Reed-Muller codes,
have been studied extensively. We present a generalization of the
Guruswami-Sudan algorithm that takes into account the geometry and the
combinatorics of P and compute bounds for the decoding radius.

</details>


### [56] [Parallelism Empowered Guessing Random Additive Noise Decoding](https://arxiv.org/abs/2510.01813)
*Li Wan,Huarui Yin,Wenyi Zhang*

Main category: cs.IT

TL;DR: 提出基于统一EP树表示的并行SGRAND解码器，保持最大似然解码性能的同时显著降低延迟，并开发了混合GRAND算法进一步加速解码过程


<details>
  <summary>Details</summary>
Motivation: 现有Soft GRAND解码器虽然实现最大似然解码，但其串行特性阻碍并行化，导致高解码延迟，无法满足现代并行硬件平台的高吞吐量和低延迟要求

Method: 使用统一的EP树表示错误模式，支持紧凑表示、高效操作和并行探索；提出并行SGRAND设计，结合剪枝策略和树基计算；开发混合GRAND算法，将EP树表示与ORBGRAND结合

Result: 并行SGRAND相比串行实现获得3.75倍加速，混合增强方法获得4.8倍加速，在硬件映射下预期有更大增益

Conclusion: EP树表示为GRAND解码器提供了有效的并行化框架，在保持ML最优性的同时显著降低解码延迟，适用于现代并行硬件平台

Abstract: Advances in parallel hardware platforms have motivated the development of
efficient universal decoders capable of meeting stringent throughput and
latency requirements. Guessing Random Additive Noise Decoding (GRAND) is a
recently proposed decoding paradigm that sequentially tests Error Patterns
(EPs) until finding a valid codeword. While Soft GRAND (SGRAND) achieves
maximum-likelihood (ML) decoding, its inherently sequential nature hinders
parallelism and results in high decoding latency. In this work, we utilize a
unified binary tree representation of EPs, termed the EP tree, which enables
compact representation, efficient manipulation, and parallel exploration.
Building upon this EP tree representation, we propose a parallel design of
SGRAND, preserving its ML optimality while significantly reducing decoding
latency through pruning strategies and tree-based computation. Furthermore, we
develop a hybrid GRAND algorithm that enhances Ordered Reliability Bits (ORB)
GRAND with the EP tree representation, thereby achieving ML decoding with
minimal additional computational cost beyond ORBGRAND while retaining parallel
efficiency. Numerical experiments demonstrate that parallel SGRAND achieves a
$3.75\times$ acceleration compared to serial implementation, while the hybrid
enhanced method achieves a $4.8\times$ acceleration, with further gains
expected under hardware mapping.

</details>


### [57] [The dimension and Bose distance of some BCH codes of length $\frac{q^{m}-1}λ$](https://arxiv.org/abs/2510.02020)
*Run Zheng,Nung-Sing Sze,Zejun Huang*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: BCH codes are important error correction codes, widely utilized due to their
robust algebraic structure, multi-error correcting capability, and efficient
decoding algorithms. Despite their practical importance and extensive study,
their parameters, including dimension, minimum distance and Bose distance,
remain largely unknown in general. This paper addresses this challenge by
investigating the dimension and Bose distance of BCH codes of length $(q^m -
1)/\lambda$ over the finite field $\mathbb{F}_q$, where $\lambda$ is a positive
divisor of $q - 1$. Specifically, for narrow-sense BCH codes of this length
with $m \geq 4$, we derive explicit formulas for their dimension for designed
distance $2 \leq \delta \leq (q^{\lfloor (2m - 1)/3 \rfloor + 1} - 1)/{\lambda}
+ 1$. We also provide explicit formulas for their Bose distance in the range $2
\leq \delta \leq (q^{\lfloor (2m - 1)/3 \rfloor + 1} - 1)/{\lambda}$. These
ranges for $\delta$ are notably larger than the previously known results for
this class of BCH codes. Furthermore, we extend these findings to determine the
dimension and Bose distance for certain non-narrow-sense BCH codes of the same
length. Applying our results, we identify several BCH codes with good
parameters.

</details>


### [58] [Performance Analysis of RIS-Assisted UAV Communication in NOMA Networks](https://arxiv.org/abs/2510.02022)
*Masoud Ghazikor,Van Ly Nguyen,Morteza Hashemi*

Main category: cs.IT

TL;DR: 本文研究了在可分区可重构智能表面(RIS)增强的无人机网络中下行非正交多址接入(NOMA)通信性能，提出了RIS辅助的无人机中断最小化(RUOM)算法，在保证公平性的同时最小化所需RIS反射单元数量。


<details>
  <summary>Details</summary>
Motivation: 无人机通信面临信道质量不稳定和资源分配公平性问题，需要结合RIS技术和NOMA来提升通信可靠性和效率。

Method: 分析了基站与无人机之间的三种链路类型，推导了信噪比的累积分布函数闭式表达式，提出了双层优化问题和RUOM算法来分配NOMA功率系数和RIS资源。

Result: 仿真验证了分析模型的有效性，RUOM算法显著提高了基站-无人机通信的公平性和效率。

Conclusion: RIS辅助的NOMA通信能有效改善无人机网络的性能，提出的RUOM算法在保证公平性的同时优化了资源利用效率。

Abstract: This paper investigates the performance of downlink non-orthogonal multiple
access (NOMA) communication in unmanned aerial vehicle (UAV) networks enhanced
by partitionable reconfigurable intelligent surfaces (RISs). We analyze three
types of links between base station (BS) and UAVs: direct, RIS-only indirect,
and composite links, under both Line-of-Sight (LoS) and Non-LoS (NLoS)
propagation. The RIS-only indirect link and direct link are modeled using
double Nakagami-m and Nakagami-m fading, respectively, while the composite link
follows a combined fading channel model. Closed-form expressions for the
cumulative distribution function (CDF) of the received signal-to-noise ratio
(SNR) are derived for all links, enabling tractable outage probability
analysis. Then, we formulate a fairness-efficiency bilevel optimization problem
to minimize the maximum outage probability among UAVs while minimizing the
total number of required RIS reflecting elements. Accordingly, an RIS-assisted
UAV Outage Minimization (RUOM) algorithm is proposed, which fairly allocates
the NOMA power coefficients while minimizing the total number of RIS reflecting
elements required, subject to NOMA-defined constraints, RIS resource
limitations, and maximum allowable outage threshold. Simulation results
validate the analytical models and demonstrate that the proposed RUOM algorithm
significantly improves fairness and efficiency in BS-UAV communication.

</details>


### [59] [Variational Secret Common Randomness Extraction](https://arxiv.org/abs/2510.02048)
*Xinyang Li,Vlad C. Andrei,Peter J. Gu,Yiqi Chen,Ullrich J. Mönich,Holger Boche*

Main category: cs.IT

TL;DR: 提出一种实用的两阶段公共随机性提取框架，结合变分概率量化和安全草图技术，用于从相关随机源中提取秘密密钥，并应用于基于感知的物理层密钥生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统物理层密钥生成方法依赖信道互易性、需要双向信道探测导致协议开销大、不适合高移动性场景的问题，探索在集成感知与通信系统中利用感知数据生成密钥的新方法。

Method: 两阶段框架：第一阶段使用变分概率量化，通过概率神经网络编码器将观测映射为离散均匀随机变量；第二阶段使用码偏移构造的安全草图协调编码器输出为相同密钥。

Result: 通过端到端仿真和真实软件定义无线电测量验证了框架的可行性，即使在窃听者部分了解Bob位置的情况下也能实现令人信服的性能。

Conclusion: 提出的公共随机性提取框架和基于感知的物理层密钥生成方法具有可行性，为高移动性场景下的安全通信提供了新解决方案。

Abstract: This paper studies the problem of extracting common randomness (CR) or secret
keys from correlated random sources observed by two legitimate parties, Alice
and Bob, through public discussion in the presence of an eavesdropper, Eve. We
propose a practical two-stage CR extraction framework. In the first stage, the
variational probabilistic quantization (VPQ) step is introduced, where Alice
and Bob employ probabilistic neural network (NN) encoders to map their
observations into discrete, nearly uniform random variables (RVs) with high
agreement probability while minimizing information leakage to Eve. This is
realized through a variational learning objective combined with adversarial
training. In the second stage, a secure sketch using code-offset construction
reconciles the encoder outputs into identical secret keys, whose secrecy is
guaranteed by the VPQ objective. As a representative application, we study
physical layer key (PLK) generation. Beyond the traditional methods, which rely
on the channel reciprocity principle and require two-way channel probing, thus
suffering from large protocol overhead and being unsuitable in high mobility
scenarios, we propose a sensing-based PLK generation method for integrated
sensing and communications (ISAC) systems, where paired range-angle (RA) maps
measured at Alice and Bob serve as correlated sources. The idea is verified
through both end-to-end simulations and real-world software-defined radio (SDR)
measurements, including scenarios where Eve has partial knowledge about Bob's
position. The results demonstrate the feasibility and convincing performance of
both the proposed CR extraction framework and sensing-based PLK generation
method.

</details>


### [60] [Interference Resilient Quantum Receivers with Rydberg Atoms](https://arxiv.org/abs/2510.02134)
*Javane Rostampoor,Raviraj Adve*

Main category: cs.IT

TL;DR: 本文研究了基于Rb-85的里德堡原子接收器在检测8-PAM信号时的性能，证明其能够抑制离谐振干扰且无需额外滤波器，优于传统电路接收器。


<details>
  <summary>Details</summary>
Motivation: 量子传感因其高精度测量能力而备受关注。里德堡原子对外部电磁场具有强敏感性，且能避免热噪声，实现更灵敏的检测。

Method: 通过监测控制激光束在通过含里德堡原子的蒸汽池前后的变化，分析透射激光信号来检测外部电磁场特性。比较里德堡原子接收器与传统接收器在检测8-PAM信号时的性能。

Result: 里德堡接收器能够抑制离谐振干扰，无需额外滤波器，在符号错误率方面优于传统电路接收器。

Conclusion: 里德堡接收器可作为集成滤波器和解调器，在干扰抑制方面表现优于传统接收器。

Abstract: Quantum sensing has attracted significant attention due to its ability to
measure physical quantities with extremely high accuracy. Rydberg atoms -
typically alkali atoms with a highly excited valence electron that is far from
the nucleus - exhibit strong sensitivity to external electromagnetic fields.
This sensitivity leads to coupling between different atomic energy levels,
which can be observed by monitoring changes in a control laser beam before and
after it passes through a vapor cell containing the Rydberg atoms. By analyzing
the transmitted laser signal with a photodetector, variations in transmission
can be attributed to the presence and characteristics of the external
electromagnetic field. Because Rydberg atoms operate in a highly excited
quantum state without relying on traditional electronic circuitry, they
inherently avoid thermal noise, thereby enabling more sensitive detection. In
this paper, we investigate the performance of a Rydberg atomic receiver based
on Rb-85 and compare it with that of a conventional receiver in detecting an
8-level pulse amplitude modulation (8-PAM) signal in the presence of
off-resonant interference. We demonstrate that the Rydberg receiver can
suppress interference without the need for an additional filter. Effectively,
our results show that the Rydberg receiver serves as an integrated filter and
demodulator, outperforming conventional circuit-based receivers in terms of
achievable symbol error rate

</details>


### [61] [Joint Channel and Semantic-aware Grouping for Effective Collaborative Edge Inference](https://arxiv.org/abs/2510.02191)
*Mateus P. Mota,Mattia Merluzzi,Emilio Calvanese Strinati*

Main category: cs.IT

TL;DR: 提出了一种联合考虑语义相关性和信道状态的协作边缘推理方法，通过将注意力权重与信道信息关联，在无线网络中实现多设备协作以提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 在存在数据损坏的无线环境中，仅考虑语义相关性或信道状态的分离方法性能较差，特别是在恶劣传播条件下需要联合优化。

Method: 采用联合方法，在设备分组协作时同时考虑语义信息相关性和信道状态，使通用注意力权重依赖于信道信息。

Result: 数值模拟显示，联合方法在损坏数据上的推理性能优于本地推理，也优于仅考虑应用层或物理层参数的分离决策协作推理。

Conclusion: 在无线协作边缘推理中，联合考虑语义相关性和信道状态的方法能够显著提升推理性能，特别是在恶劣信道条件下。

Abstract: We focus on collaborative edge inference over wireless, which enables
multiple devices to cooperate to improve inference performance in the presence
of corrupted data. Exploiting a key-query mechanism for selective information
exchange (or, group formation for collaboration), we recall the effect of
wireless channel impairments in feature communication. We argue and show that a
disjoint approach, which only considers either the semantic relevance or
channel state between devices, performs poorly, especially in harsh propagation
conditions. Based on these findings, we propose a joint approach that takes
into account semantic information relevance and channel states when grouping
devices for collaboration, by making the general attention weights dependent of
the channel information. Numerical simulations show the superiority of the
joint approach against local inference on corrupted data, as well as compared
to collaborative inference with disjoint decisions that either consider
application or physical layer parameters when forming groups.

</details>


### [62] [Collaborative Edge Inference via Semantic Grouping under Wireless Channel Constraints](https://arxiv.org/abs/2510.02222)
*Mateus P. Mota,Mattia Merluzzi,Emilio Calvanese Strinati*

Main category: cs.IT

TL;DR: 该论文研究了协作推理框架，通过边缘设备间交换中间特征而非原始数据来提高分类准确性，重点分析了信道噪声影响、协作点选择和通信-准确性权衡。


<details>
  <summary>Details</summary>
Motivation: 在边缘计算环境中，多个设备通过协作推理可以提高分类准确性，但需要平衡通信带宽限制与性能提升，特别是在存在信道噪声的情况下。

Method: 基于键-查询机制进行选择性信息交换，分析不同中间协作点对性能的影响，探索通信剪枝策略，研究信道噪声对特征通信的影响。

Result: 中间协作方法对信道错误具有鲁棒性，查询传输比数据传输本身需要更高的可靠性，可以通过优化协作点选择在最小化资源使用的同时保持准确性。

Conclusion: 协作推理框架在存在信道噪声的情况下仍能有效工作，通过合理选择协作点和通信策略可以实现通信效率与分类准确性的良好平衡。

Abstract: In this paper, we study the framework of collaborative inference, or edge
ensembles. This framework enables multiple edge devices to improve
classification accuracy by exchanging intermediate features rather than raw
observations. However, efficient communication strategies are essential to
balance accuracy and bandwidth limitations. Building upon a key-query mechanism
for selective information exchange, this work extends collaborative inference
by studying the impact of channel noise in feature communication, the choice of
intermediate collaboration points, and the communication-accuracy trade-off
across tasks. By analyzing how different collaboration points affect
performance and exploring communication pruning, we show that it is possible to
optimize accuracy while minimizing resource usage. We show that the
intermediate collaboration approach is robust to channel errors and that the
query transmission needs a higher degree of reliability than the data
transmission itself.

</details>
