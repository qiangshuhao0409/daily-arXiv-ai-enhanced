<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 10]
- [cs.AI](#cs.AI) [Total: 153]
- [cs.IT](#cs.IT) [Total: 19]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Asynchronous MultiAgent Reinforcement Learning for 5G Routing under Side Constraints](https://arxiv.org/abs/2602.00035)
*Sebastian Racedo,Brigitte Jaumard,Oscar Delgado,Meysam Masoudi*

Main category: cs.NI

TL;DR: 提出异步多智能体强化学习框架，让每个服务的独立PPO智能体并行规划路由，通过状态协调保证资源可行性，在O-RAN网络中实现可扩展的分布式路由


<details>
  <summary>Details</summary>
Motivation: 5G及后续网络承载异构流量，具有多样化的服务质量约束，实时路由决策复杂且时间关键。现有方法（如启发式人工干预、单一集中式RL策略或多学习者同步更新）面临可扩展性和滞后效应问题

Method: 提出异步多智能体强化学习框架，每个服务使用独立的PPO智能体并行规划路由，将资源增量提交到共享全局资源环境，通过状态协调保持跨服务可行性，并针对服务特定目标进行专门化

Result: 在基于蒙特利尔实时流量数据的O-RAN网络仿真中，与单智能体PPO基线相比，AMARL实现了相似的服务等级（接受率）和端到端延迟，同时减少了训练时间并提高了对需求变化的鲁棒性

Conclusion: 异步、服务专门化的智能体为分布式路由提供了可扩展且实用的方法，其适用性可扩展到O-RAN领域之外

Abstract: Networks in the current 5G and beyond systems increasingly carry heterogeneous traffic with diverse quality-of-service constraints, making real-time routing decisions both complex and time-critical. A common approach, such as a heuristic with human intervention or training a single centralized RL policy or synchronizing updates across multiple learners, struggles with scalability and straggler effects. We address this by proposing an asynchronous multi-agent reinforcement learning (AMARL) framework in which independent PPO agents, one per service, plan routes in parallel and commit resource deltas to a shared global resource environment. This coordination by state preserves feasibility across services and enables specialization for service-specific objectives. We evaluate the method on an O-RAN like network simulation using nearly real-time traffic data from the city of Montreal. We compared against a single-agent PPO baseline. AMARL achieves a similar Grade of Service (acceptance rate) (GoS) and end-to-end latency, with reduced training wall-clock time and improved robustness to demand shifts. These results suggest that asynchronous, service-specialized agents provide a scalable and practical approach to distributed routing, with applicability extending beyond the O-RAN domain.

</details>


### [2] [NetWorld: Communication-Based Diffusion World Model for Multi-Agent Reinforcement Learning in Wireless Networks](https://arxiv.org/abs/2602.00558)
*Kechen Meng,Rongpeng Li,Yansha Deng,Zhifeng Zhao,Honggang Zhang*

Main category: cs.NI

TL;DR: 提出NetWorld：基于扩散世界模型的通信框架，用于无线网络中多智能体强化学习的少样本跨任务泛化，通过离线预训练和轨迹规划减少真实交互需求。


<details>
  <summary>Details</summary>
Motivation: 无线网络规模扩大和复杂度增加，资源分配任务日益关键。多智能体强化学习（MARL）虽然提供分布式控制方案，但需要昂贵的真实交互且缺乏跨任务泛化能力。扩散模型在建模复杂动态和高保真仿真方面表现出色，为解决这些挑战提供了机会。

Method: 提出通信驱动的扩散世界模型（NetWorld），采用分布式训练与分散执行（DTDE）范式。分为两阶段：1）在多任务离线数据集上预训练分类器引导的条件扩散世界模型；2）完全在世界模型内进行轨迹规划，避免额外在线交互。通过共享潜在处理处理跨任务异质性，使用双热离散化处理任务特定动作和奖励，使用逆动态模型恢复动作。引入轻量级平均场（MF）通信机制减少非平稳性并促进协调行为。

Result: 在三个代表性任务上的实验表明，相比MARL基线方法，NetWorld在性能和样本效率方面都有提升，显示出良好的可扩展性和无线网络优化的实际潜力。

Conclusion: NetWorld通过扩散世界模型实现了无线网络中多智能体强化学习的少样本跨任务泛化，减少了真实交互需求，提高了样本效率，为大规模分布式网络优化提供了实用解决方案。

Abstract: As wireless communication networks grow in scale and complexity, diverse resource allocation tasks become increasingly critical. Multi-Agent Reinforcement Learning (MARL) provides a promising solution for distributed control, yet it often requires costly real-world interactions and lacks generalization across diverse tasks. Meanwhile, recent advances in Diffusion Models (DMs) have demonstrated strong capabilities in modeling complex dynamics and supporting high-fidelity simulation. Motivated by these challenges and opportunities, we propose a Communication-based Diffusion World Model (NetWorld) to enable few-shot generalization across heterogeneous MARL tasks in wireless networks. To improve applicability to large-scale distributed networks, NetWorld adopts the Distributed Training with Decentralized Execution (DTDE) paradigm and is organized into a two-stage framework: (i) pre-training a classifier-guided conditional diffusion world model on multi-task offline datasets, and (ii) performing trajectory planning entirely within this world model to avoid additional online interaction. Cross-task heterogeneity is handled via shared latent processing for observations, two-hot discretization for task-specific actions and rewards, and an inverse dynamics model for action recovery. We further introduce a lightweight Mean Field (MF) communication mechanism to reduce non-stationarity and promote coordinated behaviors with low overhead. Experiments on three representative tasks demonstrate improved performance and sample efficiency over MARL baselines, indicating strong scalability and practical potential for wireless network optimization.

</details>


### [3] [The Syntactic-Semantic Internet:Engineering Infrastructures for Autonomous Systems](https://arxiv.org/abs/2602.00818)
*Mallik Tatipamula,Xuesong Liu,Yao Sun,Muhammad Ali Imran*

Main category: cs.NI

TL;DR: 论文提出"语义层"作为互联网新架构层，将意义作为一等公民，实现解释对齐、语义问责和可理解的自主行为，形成句法-语义互联网的双栈结构。


<details>
  <summary>Details</summary>
Motivation: 随着自主学习系统在网络控制、计算和决策中直接嵌入智能，互联网缺乏表示和交换意义的结构基础。当前基于认知的模式识别、预测和优化不足以支持下一代网络系统，特别是在安全关键和社会技术领域，系统需要理解意图、上下文和后果。

Method: 引入语义层概念作为新架构层，提出句法-语义互联网的双栈结构：句法栈继续传输比特、数据包和工作负载，语义栈并行传输意义、基础和后果。描述语义栈的三个组成部分：语义通信、语义基板和新兴的智能体网络。

Result: 建立了语义层的架构框架，与TCP/IP和万维网形成明确的架构类比。分析了当前行业努力，识别了关键架构差距，并概述了实现全球可互操作语义基础设施所需的工程挑战。

Conclusion: 语义层是互联网演进的必要步骤，能够支持解释对齐、语义问责和可理解的自主行为，为下一代网络系统提供意义交换的基础设施。

Abstract: The Internet has evolved through successive architectural abstractions that enabled unprecedented scale, interoperability, and innovation. Packet-based networking enabled the reliable transport of bits; cloud-native systems enabled the orchestration of distributed computation. Today, the emergence of autonomous, learning-based systems introduces a new architectural challenge: intelligence is increasingly embedded directly into network control, computation, and decision-making, yet the Internet lacks a structural foundation for representing and exchanging meaning. In this paper, we argue that cognition alone: pattern recognition, prediction, and optimization, is insufficient for the next generation of networked systems. As autonomous agents act across safety-critical and socio-technical domains, systems must not only compute and communicate, but also comprehend intent, context, and consequence. We introduce the concept of a Semantic Layer: a new architectural stratum that treats meaning as a first-class construct, enabling interpretive alignment, semantic accountability, and intelligible autonomous behavior. We show that this evolution leads naturally to a Syntactic-Semantic Internet. The syntactic stack continues to transport bits, packets, and workloads with speed and reliability, while a parallel semantic stack transports meaning, grounding, and consequence. We describe the structure of this semantic stack-semantic communication, a semantic substrate, and an emerging Agentic Web, and draw explicit architectural parallels to TCP/IP and the World Wide Web. Finally, we examine current industry efforts, identify critical architectural gaps, and outline the engineering challenges required to make semantic interoperability a global, interoperable infrastructure.

</details>


### [4] [LMTE: Putting the "Reasoning" into WAN Traffic Engineering with Language Models](https://arxiv.org/abs/2602.00941)
*Xinyu Yuan,Yan Qiao,Zonghui Wang,Meng Li,Wenzhi Chen*

Main category: cs.NI

TL;DR: 本文首次探索使用大型语言模型作为通用流量规划器，提出LMTE框架，在多种场景下实现比传统流量工程求解器更好的性能（最高降低15%最大链路利用率）和10-100倍加速。


<details>
  <summary>Details</summary>
Motivation: 现代广域网快速扩张使得流量工程日益复杂，传统求解器难以跟上需求。现有基于深度神经网络的离线ML方法在未见流量模式或拓扑结构上缺乏足够的表达能力和泛化性，限制了实际应用。受大型语言模型成功启发，本文首次探索其作为通用流量规划器的潜力。

Method: 提出LMTE框架：1）理论上证明预训练语言模型可以模拟流量工程的顺序决策过程，并具有并行推理能力；2）实践上通过高效的多模态对齐和轻量级配置生成来实现这些洞察，同时保持模型的原始能力。

Result: 在五个数据集上的实验表明：1）达到顶级性能，最高降低15%最大链路利用率；2）在各种场景下（如高流量动态和链路故障）性能下降始终低于5%；3）相比传统流量工程求解器实现10-100倍加速。

Conclusion: 大型语言模型可以作为有效的通用流量规划器，LMTE框架在性能、泛化性和效率方面均表现出色，为未来研究提供了新的方向。代码已开源供后续研究使用。

Abstract: The rapid expansion of modern wide-area networks (WANs) has made traffic engineering (TE) increasingly challenging, as traditional solvers struggle to keep pace. Although existing offline ML-driven approaches accelerate TE optimization with deep neural networks (DNNs), they often lack sufficient expressiveness and generalization on unseen traffic patterns or topologies, limiting their practicality. Inspired by the success of large language models (LMs), for the first time, this paper investigates their potential as general-purpose traffic planners. Our contributions are two-fold: (i) Theoretically, we show that pre-trained LMs can simulate the sequential decision processes underlying TE and, crucially, exhibit parallel reasoning capabilities, making them well-suited for the task; (ii) Practically, we present LMTE, a novel LM-driven TE framework that embraces these insights through efficient multimodal alignment and lightweight configuration generation, all while preserving the model's original abilities. Extensive experiments demonstrate that fold matches top-tier performance on five datasets, achieving up to 15\% better maximum link utilization (MLU) and consistently lower performance degradation across diverse scenarios, e.g., less than 5\% with high traffic dynamics and link failures. Moreover, it achieves 10 to 100 times speedups over traditional TE solvers. To aid future works, our codebase is available at https://github.com/Y-debug-sys/LMTE.

</details>


### [5] [Resilience Optimization in 6G and Beyond Integrated Satellite-Terrestrial Networks: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2602.01102)
*Dinh-Hieu Tran,Nguyen Van Huynh,Van Nhan Vo,Madyan Alsenwi,Eva Lagunas,Symeon Chatzinotas*

Main category: cs.NI

TL;DR: 提出一个基于深度Q网络的卫星-地面网络弹性优化框架，通过优化天线倾角、功率控制和用户关联，在基站故障时利用LEO卫星维持服务，最大化用户速率同时最小化卫星使用


<details>
  <summary>Details</summary>
Motivation: 6G及未来网络需要确保在基站故障（由于故障、灾害、攻击或节能操作）时的网络弹性，维持服务连续性。卫星-地面融合网络可以利用LEO卫星在基站不可用时辅助用户

Method: 构建包含用户关联、天线倾角调整、功率控制、异构流量需求和动态用户分布的多小区模型。使用深度Q网络算法学习网络动态，优化天线倾角、传输功率和用户关联（到相邻基站或LEO卫星），最大化总用户速率同时最小化卫星使用

Result: 仿真结果表明，该方法显著优于基准方法，能够有效提升网络弹性

Conclusion: 提出的基于DQN的卫星-地面网络弹性优化框架能够有效应对基站故障，通过智能资源分配在维持服务质量的同时减少对卫星链路的依赖，为6G及未来网络提供可靠的弹性保障

Abstract: Ensuring network resilience in 6G and beyond is essential to maintain service continuity during base station (BS) outages due to failures, disasters, attacks, or energy-saving operations. This paper proposes a novel resilience optimization framework for integrated satellite-terrestrial networks (ISTNs), leveraging low Earth orbit (LEO) satellites to assist users when terrestrial BSs are unavailable. Specifically, we develop a realistic multi-cell model incorporating user association, antenna downtilt adaptation, power control, heterogeneous traffic demands, and dynamic user distribution. The objective is to maximize of the total user rate in the considered area by optimizing the BS's antenna tilt, transmission power, user association to neighboring BS or to a LEO satellite with a minimum number of successfully served user satisfaction constraint, defined by rate and Reference Signal Received Power (RSRP) requirements. To solve the non-convex, NP-hard problem, we design a deep Q-network (DQN)-based algorithm to learn network dynamics to maximize throughput while minimizing LEO satellite usage, thereby limiting reliance on links with longer propagation delays and prolonging satellite operational lifetime. Simulation results confirm that our approach significantly outperforms the benchmark one.

</details>


### [6] [Energy-efficient Software-defined 5G/6G Multimedia IoV: PID controller-based approach](https://arxiv.org/abs/2602.01180)
*Ahmadreza Montazerolghaem*

Main category: cs.NI

TL;DR: 提出基于PID控制器的软件定义5G/6G车联网多媒体资源管理框架，显著提升能效和负载均衡


<details>
  <summary>Details</summary>
Motivation: 智慧城市和车联网中多媒体应用快速增长，传统网络架构难以满足5G/6G时代对可扩展性、适应性和能效的要求

Method: 结合SDN和NFV技术，采用PID控制器动态管理负载分布和温度，实现集中式自适应网络资源控制

Result: 在重载条件下，功耗降低达30%，实现近乎均衡的负载分布，CPU利用率提升，对超过98%的车载请求有效响应

Conclusion: 提出的PID控制器框架为5G/6G车联网多媒体资源管理提供了高效、可扩展的解决方案，显著提升能效和性能

Abstract: The rapid proliferation of multimedia applications in smart city environments and the Internet of Vehicles (IoV) presents significant challenges for existing network infrastructures, particularly with the advent of 5G and emerging 6G technologies. Traditional architectures struggle to meet the demands for scalability, adaptability, and energy efficiency required by data-intensive multimedia services. To address these challenges, this study proposes an innovative, energy-efficient framework for multimedia resource management in software-defined 5G/6G IoV networks, leveraging a Proportional-Integral-Derivative (PID) controller. The framework integrates Software-Defined Networking (SDN) and Network Functions Virtualization (NFV) technologies to enable centralized and adaptive control over network resources. By employing a PID controller, it dynamically manages load distribution and temperature, ensuring balanced resource allocation and minimizing energy waste. Comprehensive simulations validate the framework's effectiveness, demonstrating significant improvements in load balancing, CPU utilization, and energy consumption compared to traditional methods. For instance, under heavy traffic conditions, the proposed framework maintained resource efficiency, reducing power consumption by up to 30% and achieving nearly equal load distribution across all network components. Additionally, the controller exhibited exceptional scalability, effectively responding to over 98% of vehicle requests even in scenarios of extreme traffic demand.

</details>


### [7] [AOASS: Adaptive Obstacle-Aware Square Spiral Framework for Single-mobile Anchor-Based WSN Localization](https://arxiv.org/abs/2602.01290)
*Abdelhady Naguib*

Main category: cs.NI

TL;DR: AOASS是一种用于无线传感器网络的新型单移动锚点定位框架，结合了优化的方形螺旋运动模式与自适应障碍物检测，通过OLSTM DV Hop模型提高定位精度，并使用TD3 LSTM强化学习代理进行轨迹管理。


<details>
  <summary>Details</summary>
Motivation: 无线传感器网络中的精确且节能的定位仍然是一个关键挑战，特别是在障碍物影响信号传播的情况下。需要一种能够适应障碍物环境并保持高定位精度的智能解决方案。

Method: 1. 自适应障碍物感知方形螺旋（AOASS）框架：结合优化的方形螺旋运动模式与自适应障碍物检测
2. OLSTM DV Hop模型：将长短期记忆网络与传统DV Hop算法集成，以更好地估计跳数距离并减少多跳误差
3. TD3 LSTM强化学习代理：管理锚点轨迹，辅以卡尔曼预测层和模糊逻辑ORCA安全模块，实现平滑无碰撞导航

Result: 在不同障碍物密度下的仿真实验表明，AOASS相比现有方法始终实现更高的定位精度、更好的能量效率和更优化的轨迹。该框架展示了可扩展性和在真实世界WSN应用中的潜力。

Conclusion: AOASS为数据驱动的物联网系统提供了一个智能且适应性强的解决方案，能够在障碍物环境中实现精确、节能的无线传感器网络定位。

Abstract: Accurate and energy efficient localization remains a key challenge in Wireless Sensor Networks (WSNs), particularly when obstacles affect signal propagation. This study introduces AOASS (Adaptive Obstacle Aware Square Spiral), a new single mobile anchor framework that combines an optimized square spiral movement pattern with adaptive obstacle detection. The mobile anchor can sense and bypass obstacles while maintaining high localization accuracy and full network coverage, ensuring that each node receives at least three noncollinear beacon signals for reliable position estimation. Localization accuracy is further improved using the OLSTM DV Hop model, which integrates a Long Short Term Memory (LSTM) network with the traditional DV Hop algorithm to estimate hop distances better and reduce multi hop errors. The anchor trajectory is managed by a TD3 LSTM reinforcement learning agent, supported by a Kalman based prediction layer and a fuzzy logic ORCA safety module for smooth and collision free navigation. Simulation experiments across different obstacle densities show that AOASS consistently achieves higher localization accuracy, better energy efficiency, and more optimized trajectories than existing approaches. These results demonstrate the framework scalability and potential for real world WSN applications, offering an intelligent and adaptable solution for data driven IoT systems.

</details>


### [8] [Federated Learning Meets Random Access: Energy-Efficient Uplink Resource Allocation](https://arxiv.org/abs/2602.01913)
*Giovanni Perin,Eunjeong Jeong,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 该论文研究无线网络中AI生成流量（联邦学习）与传统吞吐导向流量的资源分配问题，提出优化方案以最小化系统能耗，同时满足FL延迟和RA吞吐约束。


<details>
  <summary>Details</summary>
Motivation: AI生成的流量（特别是联邦学习）正在改变无线网络形态，大量训练数据需要网络资源精心分配，以同时支持标准应用。需要解决FL设备（FDMA访问）与吞吐导向设备（随机访问）在相同带宽上的并发通信资源分配问题。

Method: 针对FL设备使用FDMA定期上传大模型、吞吐导向设备使用ALOHA或时隙ALOHA随机访问的混合系统，建立非凸优化问题模型，推导出最小化系统能耗的近似最优解，同时满足FL延迟和RA吞吐约束。

Result: 研究显示：当系统以FL流量为主时，ALOHA能维持较高的FL效率，能耗降低达48%；当RA流量占主导时，时隙ALOHA更高效，能耗降低6%。

Conclusion: 针对不同类型的流量主导场景，选择合适的随机访问协议（ALOHA或时隙ALOHA）能显著优化无线网络中的资源分配，有效降低系统能耗，同时满足联邦学习的延迟要求和随机访问的吞吐需求。

Abstract: Artificial intelligence-generated traffic is changing the shape of wireless networks. Specifically, as the amount of data generated to train machine learning models is massive, network resources must be carefully allocated to continue supporting standard applications. In this paper, we tackle the problem of allocating radio resources for two sets of concurrent devices communicating in uplink with a gateway over the same bandwidth. A set of devices performs federated learning (FL), and accesses the medium in FDMA, uploading periodically large models. The other set is throughput-oriented and accesses the medium via random access (RA), either with ALOHA or slotted-ALOHA protocols. We derive close-to-optimal solutions to the non-convex problem of minimizing the system energy consumption subject to FL latency and RA throughput constraints. Our solutions show that ALOHA can sustain high FL efficiency, yielding up to 48% lower consumption when the system is dominated by FL traffic. On the other hand, slotted-ALOHA becomes more efficient when RA traffic dominates, yielding 6% lower consumption.

</details>


### [9] [TriCloudEdge: A multi-layer Cloud Continuum](https://arxiv.org/abs/2602.02121)
*George Violettas,Lefteris Mamatas*

Main category: cs.NI

TL;DR: TriCloudEdge是一个可扩展的三层云连续体架构，集成远端边缘设备、中间边缘节点和中心云服务，通过并行工作提供统一解决方案，支持多种协议传输数据并平衡计算挑战与延迟需求。


<details>
  <summary>Details</summary>
Motivation: 解决云连续体架构中的计算分布、延迟和隐私问题，提供实际可实施的解决方案，平衡不同层级设备的计算能力与通信效率。

Method: 提出三层架构：远端边缘（微控制器处理轻量AI任务）、中间边缘（本地智能）、中心云（大规模分析、联邦学习、模型适配、全局身份管理）。比较多协议（WebSocket、MQTT、HTTP）与单一协议（Zenoh）架构，实现跨层双向数据传输。

Result: TriCloudEdge能够有效分布计算挑战以解决延迟和隐私问题。比较实现展示了资源利用与通信效率之间的权衡。测试了远端边缘的AI模型适配和并行计算挑战。

Conclusion: 该工作为云连续体实际实施挑战提供了视角，与近期研究进展保持一致，展示了如何在不同云层级之间平衡计算与通信需求。

Abstract: TriCloudEdge is a scalable three-tier cloud continuum that integrates far-edge devices, intermediate edge nodes, and central cloud services, working in parallel as a unified solution. At the far edge, ultra-low-cost microcontrollers can handle lightweight AI tasks, while intermediate edge devices provide local intelligence, and the cloud tier offers large-scale analytics, federated learning, model adaptation, and global identity management. The proposed architecture enables multi-protocols and technologies (WebSocket, MQTT, HTTP) compared to a versatile protocol (Zenoh) to transfer diverse bidirectional data across the tiers, offering a balance between computational challenges and latency requirements. Comparative implementations between these two architectures demonstrate the trade-offs between resource utilization and communication efficiency. The results show that TriCloudEdge can distribute computational challenges to address latency and privacy concerns. The work also presents tests of AI model adaptation on the far edge and the computational effort challenges under the prism of parallelism. This work offers a perspective on the practical continuum challenges of implementation aligned with recent research advances addressing challenges across the different cloud levels.

</details>


### [10] [Evaluating Acoustic Data Transmission Schemes for Ad-Hoc Communication Between Nearby Smart Devices](https://arxiv.org/abs/2602.02249)
*Florentin Putz,Philipp Fortmann,Jan Frank,Christoph Haugwitz,Mario Kupnik,Matthias Hollick*

Main category: cs.NI

TL;DR: 该论文系统评估了31种声学通信方案，发现缺乏可复现性，通过重新实现3种方案并建立测试平台，在真实环境中测试了11000多次传输，揭示了现有方案在实际应用中的可靠性问题，并发布了首个真实世界声学传输数据集。


<details>
  <summary>Details</summary>
Motivation: 声学数据传输作为蓝牙和NFC的替代方案具有潜力，但现有研究大多依赖仿真或有限设备测试，难以评估实际可靠性。缺乏可访问源代码和系统评估方法阻碍了该领域发展。

Method: 系统回顾31项声学通信研究，联系作者获取代码，重新实现3种有前景的方案，建立包含8个代表性系统的测试平台，在真实室内环境和消声室中进行11000多次智能手机传输测试。

Result: 发现现有方案在实际应用中面临挑战，主要由于室内严重的多径传播和设备间音频特性差异。许多方案在实际使用中可靠性不足，仿真结果与实际部署存在显著差距。

Conclusion: 强调严格的设备测试的重要性，需要稳健的设计策略来弥合仿真结果与可靠物联网部署之间的差距。发布了重新实现的代码和首个真实世界声学传输综合数据集，以支持未来研究和促进更稳健的评估。

Abstract: Acoustic data transmission offers a compelling alternative to Bluetooth and NFC by leveraging the ubiquitous speakers and microphones in smartphones and IoT devices. However, most research in this field relies on simulations or limited on-device testing, which makes the real-world reliability of proposed schemes difficult to assess. We systematically reviewed 31 acoustic communication studies for commodity devices and found that none provided accessible source code. After contacting authors and re-implementing three promising schemes, we assembled a testbed of eight representative acoustic communication systems. Using over 11000 smartphone transmissions in both realistic indoor environments and an anechoic chamber, we provide a systematic and repeatable methodology for evaluating the reliability and generalizability of these schemes under real-world conditions. Our results show that many existing schemes face challenges in practical usage, largely due to severe multipath propagation indoors and varying audio characteristics across device models. To support future research and foster more robust evaluations, we release our re-implementations alongside the first comprehensive dataset of real-world acoustic transmissions. Overall, our findings highlight the importance of rigorous on-device testing and underscore the need for robust design strategies to bridge the gap between simulation results and reliable IoT deployments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

TL;DR: 比较FastAPI与NVIDIA Triton在医疗AI部署中的性能表现，提出混合架构方案，FastAPI负责安全网关和PHI去标识化，Triton负责后端推理，实现安全高性能部署。


<details>
  <summary>Details</summary>
Motivation: 医疗等受监管领域需要平衡推理延迟、吞吐量和数据隐私（如HIPAA）的竞争需求，需要找到高效的机器学习模型部署方案。

Method: 在Kubernetes上部署DistilBERT情感分析模型，对比FastAPI REST服务和NVIDIA Triton推理服务器的性能，测量p50/p95延迟和吞吐量，并评估混合架构方案。

Result: FastAPI在单请求工作负载上p50延迟为22ms，而Triton通过动态批处理实现780 RPS的吞吐量，是基准的两倍。混合架构结合了两者优势。

Conclusion: 混合架构（FastAPI作为安全网关+Triton作为后端推理）是临床AI的最佳实践，为安全高可用部署提供了蓝图。

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [12] [Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets](https://arxiv.org/abs/2602.00188)
*Srividhya Sethuraman,Chandrashekar Lakshminarayanan*

Main category: cs.AI

TL;DR: 提出AFDLD可解释需求模型和ADEPT在线学习算法，在动态定价中同时实现可解释性和高效性


<details>
  <summary>Details</summary>
Motivation: 现有低秩bandit方法虽然学习效率高，但依赖潜在特征，无法解释单个产品属性如何影响价格，缺乏可解释性

Method: 提出AFDLD可解释加性特征分解需求模型，将产品价格表示为属性级贡献之和，并显式建模替代效应；基于此提出ADEPT算法，直接在属性空间进行投影自由、梯度自由的在线学习

Result: ADEPT算法达到亚线性遗憾界$\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$；在合成和真实数据集上证明：(i)在动态市场条件下学习接近最优价格，(ii)快速适应冲击和漂移，(iii)提供透明的属性级价格解释

Conclusion: 通过结构化、属性驱动的表示，可以在自主定价代理中同时实现可解释性和效率

Abstract: Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \emph{Additive Feature Decomposition-based Low-Dimensional Demand (\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.

</details>


### [13] [From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models](https://arxiv.org/abs/2602.00190)
*Mohit Jiwatode,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 研究探索LLMs通过游戏轨迹逆向推导VGDL规则的能力，比较直接代码生成和先推断SCM再转换的两阶段方法，发现SCM方法更接近真实规则且逻辑更一致。


<details>
  <summary>Details</summary>
Motivation: 深度学习代理在复杂游戏领域表现出色但通常不理解底层因果机制，需要研究因果归纳能力——从观测数据推断支配规律，以增强AI的可解释性和推理能力。

Method: 使用语义嵌入和聚类从GVGAI框架中选择9个代表性游戏，比较两种VGDL生成方法：1）直接从观测生成代码；2）先推断结构因果模型（SCM）再转换为VGDL。评估多种提示策略和控制上下文机制，从原始游戏观察到部分VGDL规范。

Result: SCM方法比直接生成更常产生接近真实情况的VGDL描述，在盲评中偏好胜率高达81%，产生的逻辑不一致规则更少。学习到的SCM可用于因果强化学习、可解释代理和程序生成新颖但逻辑一致的游戏。

Conclusion: SCM方法在从游戏轨迹逆向推导VGDL规则方面优于直接生成，为因果推理、可解释AI和游戏设计提供了有前景的途径。

Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.

</details>


### [14] [Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic](https://arxiv.org/abs/2602.00266)
*Yani Zhang,Helmut Bölcskei*

Main category: cs.AI

TL;DR: 论文将ReLU神经网络转化为Łukasiewicz逻辑公式，通过逻辑公理代数重写实现功能等价网络变换，建立了ReLU网络功能对称性的完整识别框架。


<details>
  <summary>Details</summary>
Motivation: 深度ReLU神经网络存在显著的功能对称性：不同架构和参数可以实现相同函数。需要解决完整识别问题——给定函数f，推导出所有产生f的前馈ReLU网络的架构和参数。

Method: 将ReLU网络转化为Łukasiewicz逻辑公式，通过逻辑公理的代数重写实现功能等价网络变换，提出组合范式便于从逻辑公式映射回ReLU网络。

Result: 使用Chang完备性定理证明，在每个功能等价类中，所有ReLU网络都通过Łukasiewicz逻辑有限公理集对应的有限对称性集合相互连接。

Conclusion: 该方法类似于香农在开关电路设计中的工作，将电路转化为布尔公式，通过布尔逻辑公理的代数重写实现综合，为ReLU网络功能对称性提供了完整的理论框架。

Abstract: Deep ReLU neural networks admit nontrivial functional symmetries: vastly different architectures and parameters (weights and biases) can realize the same function. We address the complete identification problem -- given a function f, deriving the architecture and parameters of all feedforward ReLU networks giving rise to f. We translate ReLU networks into Lukasiewicz logic formulae, and effect functional equivalent network transformations through algebraic rewrites governed by the logic axioms. A compositional norm form is proposed to facilitate the mapping from Lukasiewicz logic formulae back to ReLU networks. Using Chang's completeness theorem, we show that for every functional equivalence class, all ReLU networks in that class are connected by a finite set of symmetries corresponding to the finite set of axioms of Lukasiewicz logic. This idea is reminiscent of Shannon's seminal work on switching circuit design, where the circuits are translated into Boolean formulae, and synthesis is effected by algebraic rewriting governed by Boolean logic axioms.

</details>


### [15] [Localizing and Correcting Errors for LLM-based Planners](https://arxiv.org/abs/2602.00276)
*Aditya Kumar,William W. Cohen*

Main category: cs.AI

TL;DR: LLMs在符号规划任务中常违反约束，本文提出局部上下文学习(L-ICL)方法，通过针对性修正失败步骤来显著提升规划有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和编程任务上表现出强大的推理能力，但在符号经典规划任务中经常失败，生成的计划经常违反指令中给出的领域约束（如穿墙）。

Method: 提出局部上下文学习(L-ICL)：迭代地在指令中添加针对性修正演示。具体来说，L-ICL识别轨迹中的第一个约束违反，并注入一个最小化的输入-输出示例，给出失败步骤的正确行为。

Result: L-ICL比显式指令或传统ICL（添加完整问题解决轨迹）以及其他基线方法更有效。在8x8网格世界中，L-ICL仅用60个训练示例就能产生89%的有效计划，而最佳基线为59%，提高了30%。在其他领域（网格导航、迷宫、Sokoban、BlocksWorld）和多种LLM架构上也显示出显著改进。

Conclusion: 局部上下文学习是一种有效的方法，能够显著提高LLM在符号规划任务中生成有效计划的能力，通过针对性修正失败步骤来避免约束违反。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.

</details>


### [16] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

TL;DR: 本文研究大型语言模型在特定领域微调后出现的突发性错位风险，通过构建11个不安全领域数据集并评估带/不带后门触发器的模型行为，发现后门触发器显著增加错位率，且不同领域脆弱性差异巨大。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地用于自主任务，突发性错位对AI安全构成风险。需要评估模型在特定领域微调后可能出现的意外有害行为，特别是在不同领域中的脆弱性差异。

Method: 构建包含11个不同领域的不安全数据集，对Qwen2.5-Coder-7B-Instruct和GPT-4o-mini进行微调，评估带/不带后门触发器的模型行为。使用成员推理指标预测错位程度，分析不同数据集微调模型间的错位关系。

Result: 1) 后门触发器在77.8%的领域增加错位率（平均下降4.33分），风险金融建议和有毒法律建议领域影响最大；2) 领域脆弱性差异巨大：错误数学领域错位率为0%，而血腥电影琐事领域达87.67%；3) 调整后的成员推理指标可有效预测错位程度。

Conclusion: 本研究首次提供了突发性错位的领域分类排名，对AI安全和后训练有重要意义。建立了构建错位数据集的标准化方法，所有代码和数据集已公开，为评估和缓解模型错位风险提供了基础。

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [17] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

TL;DR: ADP-MA是一个使用元代理自主构建、执行和迭代优化数据处理管道的框架，通过分层代理编排实现动态管道管理。


<details>
  <summary>Details</summary>
Motivation: 传统数据处理管道通常是静态的、为特定任务手工构建的，难以适应不断变化的需求。现有的通用代理和编码助手虽然能为已知管道生成代码，但缺乏部署后自主监控、管理和优化端到端管道的能力。

Method: 采用分层代理编排架构：元代理分析输入数据和任务规范，设计多阶段计划，实例化专门的地面级代理，并持续评估管道性能。框架包含三个关键组件：策略生成的规划模块、代理协调和工具集成的编排层，以及迭代评估和回溯的监控循环。

Result: 通过交互式演示展示了ADP-MA在代表性数据处理任务中的管道构建、执行监控和自适应优化能力。框架强调上下文感知优化、自适应工作负载分区和渐进采样以实现可扩展性。

Conclusion: ADP-MA通过元代理的自主编排，实现了动态、自适应和可优化的数据处理管道，解决了传统静态管道的局限性，能够重用现有代理和工具，减少冗余并加速管道构建。

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [18] [SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?](https://arxiv.org/abs/2602.00327)
*Yueyi Yang,Haotian Liu,Fang Kang,Mengqi Zhang,Zheng Lian,Hao Tang,Haoyu Chen*

Main category: cs.AI

TL;DR: 该论文提出SayNext-Bench基准和SayNext-Chat模型，用于评估LLMs和MLLMs基于多模态线索预测人类对话下一话语的能力，发现当前模型在此任务上表现不佳，强调了多模态线索和主动预测处理在自然交互中的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然对话方面取得进展，但研究发现即使是领先模型也难以预测人类说话者的下一话语。人类能够基于手势、注视、情感语调等多模态线索轻松预测即将到来的话语，而当前模型缺乏这种能力。论文旨在系统性地探索LLMs和MLLMs是否能够复制人类基于多模态线索预测下一话语的能力。

Method: 1. 提出SayNext-Bench基准，用于评估LLMs和MLLMs基于多模态线索预测上下文条件化响应的能力；2. 构建SayNext-PC数据集，包含丰富多模态线索的对话；3. 开发SayNext-Chat模型，采用认知启发的双路径预测设计，模拟对话中的预测处理机制。

Result: 实验结果表明，SayNext-Chat模型在词汇重叠、语义相似性和情感一致性方面优于最先进的MLLMs。研究证明了基于多模态线索的下一话语预测的可行性，并强调了多模态线索和主动预测处理在自然人类交互中的不可或缺作用。

Conclusion: 该研究为构建更类人、上下文敏感的AI交互提供了新的研究方向，强调了多模态线索和主动预测处理作为自然人类交互基础的重要性，这是当前MLLMs所缺失的。基准和模型已开源，促进该领域进一步发展。

Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.

</details>


### [19] [MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants](https://arxiv.org/abs/2602.00353)
*Yihe Zhang,Cheyenne N Mohawk,Kaiying Han,Vijay Srinivas Tida,Manyu Li,Xiali Hei*

Main category: cs.AI

TL;DR: MHDash是一个开源平台，用于开发、评估和审计心理健康AI系统，揭示传统基准测试在安全关键场景中的不足，特别是在高风险案例和多轮对话中的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要依赖聚合性能指标，往往掩盖了风险特定的失败模式，且对现实多轮交互中的模型行为提供有限洞察。在心理健康支持系统中，可靠识别自杀意念和自伤等高危状态是安全关键的。

Method: 开发了MHDash开源平台，集成了数据收集、结构化标注、多轮对话生成和基线评估的统一流程。支持多维度标注（关注类型、风险等级、对话意图），实现细粒度和风险感知的分析。

Result: 研究发现：(1)简单基线和先进LLM API总体准确率相当，但在高风险案例上差异显著；(2)一些LLM保持一致的严重程度排序但绝对风险分类失败，而另一些获得合理聚合分数但在严重类别上假阴性率高；(3)多轮对话中性能差距放大，风险信号逐渐显现。

Conclusion: 传统基准测试不足以应对安全关键的心理健康场景。通过开源MHDash平台，旨在促进可重复研究、透明评估和安全对齐的心理健康AI系统开发。

Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.

</details>


### [20] [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)
*Minhua Lin,Hanqing Lu,Zhan Shi,Bing He,Rui Mao,Zhiwei Zhang,Zongyu Wu,Xianfeng Tang,Hui Liu,Zhenwei Dai,Xiang Zhang,Suhang Wang,Benoit Dumoulin,Jian Pei*

Main category: cs.AI

TL;DR: 论文提出LLMs需要从静态训练转向动态演化适应，引入A-Evolve框架将部署时改进视为对持久系统状态的有目标优化过程，并提出演化缩放假说。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从精心策划的训练集转向开放的现实世界环境时，静态训练无法跟上持续部署环境变化，现有部署时适应方法缺乏战略能力来诊断失败并产生持久改进。

Method: 提出A-Evolve框架，将部署时改进视为对持久系统状态的有目标优化过程，将演化从固定流程提升为自主演化代理，实现代理化演化。

Result: 提出演化缩放假说：适应能力随着分配给演化的计算资源而扩展，将代理化演化定位为在现实世界中实现持续、开放式适应的可扩展路径。

Conclusion: 代理化演化代表了LLM适应的必然未来，通过A-Evolve框架和演化缩放假说，为LLMs在动态现实环境中的持续适应提供了新的扩展轴。

Abstract: As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.

</details>


### [21] [POET: Protocol Optimization via Eligibility Tuning](https://arxiv.org/abs/2602.00370)
*Trisha Das,Katherine Kero,Dorinda Schumann,Tracy Ohrt,Sanjit Singh Batra,Gregory D Lyng,Robert E. Tillman*

Main category: cs.AI

TL;DR: 提出基于语义轴引导的临床试验资格标准生成框架，在结构化输入与端到端生成之间取得平衡，通过可解释的语义维度指导生成过程，并引入可复用的评估框架。


<details>
  <summary>Details</summary>
Motivation: 临床试验资格标准设计耗时且认知负担重，现有自动化方法要么需要高度结构化输入（如预定义实体），要么依赖端到端系统从最少输入生成完整标准，实用性有限。

Method: 提出引导生成框架，引入可解释的语义轴（如人口统计学、实验室参数、行为因素）来指导资格标准生成。这些语义轴通过大语言模型推导，在特异性和可用性之间取得平衡，使临床医生无需指定确切实体即可引导生成。同时提出可复用的基于量规的评估框架。

Result: 引导生成方法在自动评估、量规评估和临床医生评估中均持续优于非引导生成，为AI辅助试验设计提供了实用且可解释的解决方案。

Conclusion: 该引导生成框架在临床试验资格标准自动化设计中提供了实用性和可解释性的平衡，通过语义轴引导实现了比非引导方法更好的性能，有望减轻临床医生的工作负担。

Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.

</details>


### [22] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

TL;DR: KEPO提出了一种结合质量门控蒸馏和知识增强探索的强化学习后训练框架，用于解决推理密集型任务中的探索失败和梯度噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习后训练方法在推理密集型任务中面临挑战：稀疏轨迹级奖励导致信用分配模糊和严重探索失败，使策略陷入"学习悬崖"。而现有的均匀蒸馏方法在低质量轨迹上会产生噪声梯度。

Method: KEPO框架包含两个核心组件：1) 质量门控的在线蒸馏目标，仅对高质量轨迹应用密集教师指导；2) 知识增强探索策略，利用从教师模型学习的提示来拒绝采样奖励正的在线轨迹，缓解探索崩溃。

Result: 在具有挑战性的医学视觉问答基准测试中，KEPO在单源泛化设置下表现出更好的训练稳定性、更一致的推理行为，以及优于强化学习和在线蒸馏基线的分布外性能。

Conclusion: KEPO通过选择性蒸馏和知识增强探索，有效解决了推理密集型任务中强化学习后训练的稳定性问题，提高了模型的泛化能力和推理质量。

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [23] [RobustDebias: Debiasing Language Models using Distributionally Robust Optimization](https://arxiv.org/abs/2602.00405)
*Deep Gandhi,Katyani Singh,Nidhi Hegde*

Main category: cs.AI

TL;DR: 提出RobustDebias方法，使用分布鲁棒优化在微调阶段减轻语言模型偏见，避免昂贵的预训练修改


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型存在偏见和社会刻板印象，现有去偏见方法主要关注预训练阶段的嵌入空间修改，不适用于大型模型。微调会放大数据中的偏见，同时可能降低模型性能。

Method: 提出RobustDebias机制，将分布鲁棒优化（DRO）应用于语言模型微调阶段，在MLM微调过程中对多个人口统计群体进行去偏见处理，可泛化到任何数据集或任务

Result: 在各种语言模型上的广泛实验显示，该方法能显著减轻偏见，同时对模型性能影响最小

Conclusion: RobustDebias提供了一种在微调阶段有效减轻语言模型偏见的方法，避免了昂贵的预训练修改，具有较好的泛化能力和实用性

Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.

</details>


### [24] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

TL;DR: 提出PolarMem记忆系统，将模糊感知概率转化为离散逻辑约束，通过极化图拓扑结构显式存储否定信息，为可验证多模态智能体奠定基础


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体从被动观察者发展为长期决策者，需要具有逻辑可验证性的记忆系统。现有架构存在认知不对称性：概率视觉语言模型和密集关联记忆将语义亲和性与事实存在混为一谈，且无法编码否定约束

Method: 提出PolarMem（极化潜在图记忆），通过非参数分布划分将模糊感知似然转化为离散逻辑约束；采用极化图拓扑结构，使用正交抑制连接显式存储已验证的否定作为主要认知状态；在推理时实施逻辑主导检索范式，抑制违反否定约束的幻觉模式

Result: 在8个冻结视觉语言模型和6个基准测试上的广泛评估表明，PolarMem作为稳健的认知系统运行，为可验证多模态智能体建立了基础

Conclusion: PolarMem解决了当前记忆系统的根本局限性，通过将感知概率转化为逻辑约束并显式编码否定信息，为构建可验证的多模态智能体提供了关键技术支持

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [25] [Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks](https://arxiv.org/abs/2602.00449)
*Jia Liang,Liangming Pan*

Main category: cs.AI

TL;DR: 该研究通过分析CODI模型在多项式迭代任务上的表现，揭示了潜在思维链（Latent-CoT）的内部工作机制，区分了其何时能实现忠实迭代计算，何时会退化为压缩或捷径策略。


<details>
  <summary>Details</summary>
Motivation: 潜在思维链（Latent-CoT）旨在实现无需生成长推理过程的逐步计算，但其内部机制尚不明确。研究者希望通过分析CODI模型来理解潜在思维链如何表示和路由中间状态，以及在不同任务复杂度下的行为变化。

Method: 使用CODI（连续思维师生蒸馏模型）在严格顺序的多项式迭代任务上进行实验。采用多种分析技术：logit-lens解码、线性探针、注意力分析和激活修补，以定位中间状态表示并追踪其到最终输出的路由路径。

Result: 在2-3跳任务中，CODI形成了完整的桥接状态集，这些状态在不同潜在思维位置可解码，而最终输入通过近乎直接的路径；预测通过思维边界处的后期融合产生。对于更长跳数，CODI无法可靠执行完整的潜在展开，而是表现出部分潜在推理路径，集中在后期中间状态并与最后输入在答案读取位置融合。消融实验表明，这种部分路径在机制变化（如更难的优化）下可能崩溃。

Conclusion: 研究明确了CODI式潜在思维链在何时能产生忠实的迭代计算，何时会退化为压缩或捷径策略。这突显了为顺序推理设计鲁棒的潜在思维链目标所面临的挑战，需要更可靠的机制来确保长序列任务中的稳定推理。

Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.

</details>


### [26] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: DebateOCR：一个跨模态压缩框架，用紧凑的图像表示替代冗长的文本辩论历史，减少92%以上输入token，降低计算成本并加速推理。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论虽然能提高推理质量并减少幻觉，但随着辩论轮次和智能体数量增加，上下文会迅速增长。保留完整文本历史会导致token使用量超过上下文限制，并且通常需要重复总结，增加了开销并加剧信息损失。

Method: 引入DebateOCR跨模态压缩框架，将冗长的文本辩论轨迹替换为紧凑的图像表示，然后通过专门的视觉编码器处理这些图像表示来调节后续轮次。该方法压缩了通常跨越数万到数十万token的历史记录。

Result: 在多个基准测试中，输入token减少了92%以上，计算成本显著降低，推理速度更快。理论分析表明，智能体间的多样性支持恢复被省略的信息：虽然单个压缩历史可能丢弃细节，但聚合多个智能体的压缩视图可以使集体表示以指数级高概率接近信息瓶颈。

Conclusion: DebateOCR通过跨模态压缩有效解决了多智能体辩论中的上下文爆炸问题，在保持推理质量的同时大幅降低了计算开销，为大规模多智能体系统提供了实用的解决方案。

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [27] [Benchmarking Agents in Insurance Underwriting Environments](https://arxiv.org/abs/2602.00456)
*Amanda Dsouza,Ramya Ramakrishnan,Charles Dickens,Bhavishya Pohani,Christopher M Glaze*

Main category: cs.AI

TL;DR: 提出了UNDERWRITE基准测试，这是一个与保险核保专家合作设计的、反映真实企业复杂性的多轮对话基准，用于评估AI代理在专业领域的实际应用能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理基准测试过度关注开放领域（如代码），使用狭窄的准确性指标，缺乏真实世界的复杂性，无法反映企业应用的实际需求。

Method: 与领域专家密切合作设计保险核保基准，引入专有业务知识、噪声工具接口和不完美的模拟用户等关键现实因素，评估13个前沿模型。

Result: 发现研究实验室性能与企业就绪度之间存在显著差距：最准确的模型并非最有效率；模型即使有工具访问仍会产生领域知识幻觉；pass^k结果显示性能下降20%。

Conclusion: 专家参与基准设计对真实代理评估至关重要；常见代理框架存在脆弱性影响性能报告；专业领域的幻觉检测需要组合方法；为开发符合企业部署需求的基准提供了见解。

Abstract: As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.

</details>


### [28] [Dual Latent Memory for Visual Multi-agent System](https://arxiv.org/abs/2602.00471)
*Xinlei Yu,Chengming Xu,Zhangquan Chen,Bo Yin,Cheng Yang,Yongbo He,Yihao Hu,Jiangning Zhang,Cheng Tan,Xiaobin Hu,Shuicheng Yan*

Main category: cs.AI

TL;DR: L²-VMAS框架通过双潜在记忆和熵驱动触发机制，解决了视觉多智能体系统中的"扩展墙"问题，在提升性能的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 视觉多智能体系统（VMAS）通过智能体间协作提升综合能力，但实证发现存在反直觉的"扩展墙"现象：增加智能体轮次反而降低性能，同时指数级增加token成本。这归因于文本中心通信的信息瓶颈，将感知和思维轨迹转换为离散自然语言时不可避免地导致语义损失。

Method: 提出L²-VMAS框架，采用双潜在记忆实现智能体间协作。该方法解耦感知和思维过程，动态合成双潜在记忆，并引入熵驱动的主动触发机制，用高效按需内存访问替代被动信息传输。

Result: 在多种骨干网络、模型规模和智能体结构上的实验表明，该方法有效打破"扩展墙"，具有出色的可扩展性：平均准确率提升2.7-5.4%，同时token使用量减少21.3-44.8%。

Conclusion: L²-VMAS通过双潜在记忆和主动触发机制解决了视觉多智能体系统的信息瓶颈问题，在提升协作效果的同时显著降低计算开销，为多智能体系统设计提供了新思路。

Abstract: While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive "scaling wall": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the "scaling wall" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.

</details>


### [29] [Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models](https://arxiv.org/abs/2602.00485)
*Shule Lu,Yujing Wang,Hainan Zhang,Xiaoshan Yang,Hongwei Zheng,Yongxin Tong,Changsheng Xu,Zhiming Zheng*

Main category: cs.AI

TL;DR: MoR：基于GRPO与混合奖励的联邦对齐框架，用于异构视觉语言模型，通过本地奖励模型训练和路由融合机制实现隐私保护的联邦对齐


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医疗、金融等隐私敏感领域有广泛应用潜力，但数据共享限制使集中式训练不可行。联邦学习虽然能实现去中心化训练，但面临客户端异构性（计算资源、应用需求、模型架构）的挑战。作者认为，用偏好替代参数比用参数替代数据更具可扩展性和隐私保护性

Method: 提出MoR联邦对齐框架：1）初始化视觉基础模型作为KL正则化参考；2）每个客户端基于本地偏好标注训练奖励模型，捕获特定评估信号而不暴露原始数据；3）引入基于路由的融合机制自适应聚合客户端奖励信号；4）服务器使用混合奖励执行GRPO优化基础VLM

Result: 在三个公开VQA基准测试中，MoR在泛化性、鲁棒性和跨客户端适应性方面持续优于联邦对齐基线方法

Conclusion: MoR为异构视觉语言模型在联邦设置下的隐私保护对齐提供了可扩展解决方案，代表了从参数共享到偏好共享的范式转变

Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.

</details>


### [30] [Capabilities and Fundamental Limits of Latent Chain-of-Thought](https://arxiv.org/abs/2602.01148)
*Jiaxuan Zou,Yaozhong Xiong,Yong Liu*

Main category: cs.AI

TL;DR: Latent CoT模型存在探索与执行权衡问题，研究发现决策确定性是关键机制，提出符号索引量化决策承诺，证明课程学习是理论必要的。


<details>
  <summary>Details</summary>
Motivation: Latent CoT模型在推理任务中表现出令人困惑的性能不一致性：在探索任务（ProsQA: 97.0%）表现出色，但在计算任务（GSM8K: 34.1%）上失败。需要理解这种权衡的根本原因。

Method: 1. 理论分析探索-执行权衡，证明高确定性支持精确执行但抑制探索，低确定性促进搜索但导致错误累积；2. 引入符号索引量化决策承诺，建立其与执行稳定性和探索能力的因果关系；3. 证明课程学习的理论必要性。

Result: 揭示了决策确定性是Latent CoT模型探索-执行权衡的核心机制，提出了符号索引作为量化工具，证明了直接训练会因分布不匹配而失败，课程学习是必要的。

Conclusion: 该框架将设计范式从二元架构选择转向自适应系统，能够根据任务需求动态调节决策确定性，为高效推理模型提供了理论基础。

Abstract: Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certainty. Our contributions are threefold: (1) We theoretically characterize the fundamental Exploration-Execution Trade-off, proving that high certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation. (2) We introduce the Symbolic Index--quantifying decisional commitment--as the core mechanism governing this trade-off and establish its causal relationship with both execution stability and exploration capability. (3) We prove that curriculum learning is theoretically necessary, as direct training provably fails due to distributional mismatch. Our framework shifts the design paradigm from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands.

</details>


### [31] [PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)](https://arxiv.org/abs/2602.00510)
*Huanghaohe Zou,Peng Han,Emad Nazerian,Alex Q. Huang*

Main category: cs.AI

TL;DR: PCBSchemaGen：首个基于LLM代理和约束引导合成的免训练PCB原理图设计框架，能处理数字、模拟和电源混合信号，通过代码生成和知识图谱验证提高设计准确性和效率。


<details>
  <summary>Details</summary>
Motivation: PCB原理图设计在电子行业至关重要，但现有研究仅关注数字或模拟电路单一领域，而实际PCB设计需要处理混合信号并遵守真实IC封装和引脚约束。由于开源数据稀缺和缺乏仿真验证，自动化PCB原理图设计尚未得到充分探索。

Method: 提出PCBSchemaGen框架：1）基于LLM的代码生成范式，通过领域特定提示进行迭代反馈；2）利用真实IC数据手册构建的知识图谱和子图同构验证框架，编码引脚角色语义和拓扑约束；3）在23个PCB原理图任务上进行广泛实验。

Result: PCBSchemaGen显著提高了设计准确性和计算效率，能够有效处理数字、模拟和电源领域的混合信号设计任务。

Conclusion: PCBSchemaGen是首个免训练的PCB原理图设计框架，通过LLM代理和约束引导合成解决了混合信号设计的挑战，为自动化PCB设计提供了有效解决方案。

Abstract: Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.

</details>


### [32] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: 提出基于项目反应理论的两阶段诊断框架，评估LLM作为评判者的可靠性，包括内在一致性和人类对齐两个维度


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge验证实践主要关注观察输出层面，缺乏对LLM评判者作为测量工具是否稳定可靠的深入洞察

Method: 基于项目反应理论（IRT）的两阶段诊断框架，采用IRT的等级反应模型（GRM），从内在一致性和人类对齐两个维度形式化可靠性

Result: 框架为LLM评判者诊断提供可解释信号，能够系统性地验证可靠性并识别不可靠性的潜在原因

Conclusion: IRT-GRM框架为评估LLM-as-a-Judge的可靠性提供了系统化的诊断工具，有助于提升自动化评估的质量

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [33] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: LLMs在扑克游戏中表现不佳，存在启发式依赖、事实误解和知行差距等问题。作者提出ToolPoker框架，结合外部求解器实现GTO一致的行动和专业的解释，达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在关键领域应用增加，其在不确定性下进行战略推理的能力变得至关重要。扑克游戏提供了一个严格的测试平台，不仅需要强力的行动，还需要基于博弈论的原则性推理。

Method: 首先系统研究LLMs在多个现实扑克任务中的表现，分析其推理轨迹。然后提出ToolPoker框架，该框架整合外部求解器来生成GTO一致的行动，并提供更精确的专业风格解释。

Result: 研究发现LLMs无法与传统算法竞争，存在三个主要缺陷：依赖启发式方法、事实误解、以及行动与推理脱节的"知行差距"。ToolPoker框架在实验中实现了最先进的游戏表现，并产生紧密反映博弈论原则的推理轨迹。

Conclusion: LLMs在需要严格博弈论推理的任务中存在显著局限性。通过整合外部工具和求解器的ToolPoker框架，可以显著提升LLMs在战略游戏中的表现和推理质量。

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [34] [Uncovering Latent Communication Patterns in Brain Networks via Adaptive Flow Routing](https://arxiv.org/abs/2602.00561)
*Tianhao Huang,Guanghui Min,Zhenyu Lei,Aiying Zhang,Chen Chen*

Main category: cs.AI

TL;DR: AFR-Net是一个基于物理启发的自适应流路由网络，通过模拟神经通信动力学来融合结构连接和功能连接，可解释地发现关键神经通路。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏神经科学基础洞察，无法揭示连接组背后的神经区域潜在相互作用，也不能解释为什么SC和FC表现出耦合和异质性的动态状态。

Method: 提出自适应流路由网络（AFR-Net），这是一个物理启发的框架，模拟结构约束如何产生功能通信模式，实现关键神经通路的可解释发现。

Result: 大量实验表明，AFR-Net显著优于最先进的基线方法。

Conclusion: 通过神经通信动力学视角进行多模态融合，能够更好地理解宏观认知表型如何从微观神经元连接中涌现。

Abstract: Unraveling how macroscopic cognitive phenotypes emerge from microscopic neuronal connectivity remains one of the core pursuits of neuroscience. To this end, researchers typically leverage multi-modal information from structural connectivity (SC) and functional connectivity (FC) to complete downstream tasks. Recent methodologies explore the intricate coupling mechanisms between SC and FC, attempting to fuse their representations at the regional level. However, lacking fundamental neuroscientific insight, these approaches fail to uncover the latent interactions between neural regions underlying these connectomes, and thus cannot explain why SC and FC exhibit dynamic states of both coupling and heterogeneity. In this paper, we formulate multi-modal fusion through the lens of neural communication dynamics and propose the Adaptive Flow Routing Network (AFR-Net), a physics-informed framework that models how structural constraints (SC) give rise to functional communication patterns (FC), enabling interpretable discovery of critical neural pathways. Extensive experiments demonstrate that AFR-Net significantly outperforms state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/DIAL-F0D1.

</details>


### [35] [Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs](https://arxiv.org/abs/2602.00564)
*Xiang Zheng,Weiqi Zhai,Wei Wang,Boyu Yang,Wenbo Li,Ruixiang Luo,Haoxiang Sun,Yucheng Wang,Zhengze Li,Meng Wang,Yuetian Du,Guojie Lin,Yaxuan Wang,Xiaoxiao Xu,Yanhu Mo,Xuan Ren,Hu Wei,Ze Xu*

Main category: cs.AI

TL;DR: 论文提出了ReasoningMath-Plus基准测试，包含150个专门设计用于评估结构化推理能力的问题，并引入了HCRS评分方法和PRM模型，揭示了现有LLMs在深层推理能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准测试已被大型语言模型接近饱和地解决，主要原因是这些数据集过度依赖模板化计算和浅层算术分解，未能充分评估真正的推理能力，如多约束协调、构造性逻辑合成和空间推理等技能。

Method: 1) 构建ReasoningMath-Plus基准测试，包含150个精心设计的问题，强调交互约束下的推理、构造性解决方案形成和非平凡的结构洞察；2) 为每个问题标注最小推理骨架以支持细粒度过程级评估；3) 引入HCRS（风险感知链式规则评分）确定性步骤级评分函数；4) 在标注的推理轨迹上训练过程奖励模型（PRM）。

Result: 领先模型在最终答案准确率上相对较高（最高5.8/10），但基于HCRS的整体评估得分显著较低（平均4.36/10，最佳5.14/10），表明仅基于答案的指标会高估推理的鲁棒性。

Conclusion: 需要更精细的评估方法来准确衡量语言模型的真实推理能力，仅依赖最终答案准确率会掩盖模型在结构化推理方面的不足。ReasoningMath-Plus基准测试和HCRS评分方法为评估深层推理能力提供了更可靠的框架。

Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.

</details>


### [36] [Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings](https://arxiv.org/abs/2602.00574)
*Yifei Shao,Kun Zhou,Ziming Xu,Mohammad Atif Quamar,Shibo Hao,Zhen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: 提出modal-mixed CoT方法，在思维链中交替使用文本标记和视觉草图潜在嵌入，以改进多模态推理任务。


<details>
  <summary>Details</summary>
Motivation: 传统文本形式的思维链在处理视觉密集型问题时存在局限，因为关键中间状态本质上是视觉的，需要扩展CoT到多模态领域。

Method: 使用VLM自身作为编码器，训练语言骨干重建其视觉嵌入以保证语义对齐；附加基于扩散的潜在解码器，通过特殊控制token调用；采用两阶段训练：监督微调结合强化学习。

Result: 在11个多样化多模态推理任务上的实验表明，该方法优于纯语言和其他CoT方法。

Conclusion: modal-mixed CoT通过视觉-文本交替的思维链，有效解决了多模态推理中的视觉中间状态表示问题，实现了更好的性能。

Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.

</details>


### [37] [Small Shifts, Large Gains: Unlocking Traditional TSP Heuristic Guided-Sampling via Unsupervised Neural Instance Modification](https://arxiv.org/abs/2602.00580)
*Wei Huang,Hanchen Wang,Dong Wen,Wenjie Zhang*

Main category: cs.AI

TL;DR: TSP-MDF：通过实例修改框架为传统TSP启发式算法添加引导采样能力，无需真实监督训练，显著提升解质量


<details>
  <summary>Details</summary>
Motivation: 传统TSP启发式算法（如最远/最近插入法）计算高效但确定性行为导致局部最优；神经启发式算法解质量更好但需要大量训练和真实监督，实用性受限。需要弥合这一差距。

Method: 提出TSP-MDF实例修改框架：使用神经实例修改器策略性地移动节点坐标生成多个修改实例，在这些实例上运行传统启发式算法，然后将解映射回原始实例，使传统算法能探索更高质量的解并逃离局部最优。

Result: 在大规模TSP基准和真实世界基准上的实验表明，TSP-MDF显著提升传统启发式算法的性能，达到与神经启发式算法相当的解质量，但训练时间极短。

Conclusion: TSP-MDF成功为传统确定性启发式算法添加了引导采样能力，通过无监督训练实现了高效实用的TSP求解框架，在解质量和实用性之间取得了良好平衡。

Abstract: The Traveling Salesman Problem (TSP) is one of the most representative NP-hard problems in route planning and a long-standing benchmark in combinatorial optimization. Traditional heuristic tour constructors, such as Farthest or Nearest Insertion, are computationally efficient and highly practical, but their deterministic behavior limits exploration and often leads to local optima. In contrast, neural-based heuristic tour constructors alleviate this issue through guided-sampling and typically achieve superior solution quality, but at the cost of extensive training and reliance on ground-truth supervision, hindering their practical use. To bridge this gap, we propose TSP-MDF, a novel instance modification framework that equips traditional deterministic heuristic tour constructors with guided-sampling capability. Specifically, TSP-MDF introduces a neural-based instance modifier that strategically shifts node coordinates to sample multiple modified instances, on which the base traditional heuristic tour constructor constructs tours that are mapped back to the original instance, allowing traditional tour constructors to explore higher-quality tours and escape local optima. At the same time, benefiting from our instance modification formulation, the neural-based instance modifier can be trained efficiently without any ground-truth supervision, ensuring the framework maintains practicality. Extensive experiments on large-scale TSP benchmarks and real-world benchmarks demonstrate that TSP-MDF significantly improves the performance of traditional heuristics tour constructors, achieving solution quality comparable to neural-based heuristic tour constructors, but with an extremely short training time.

</details>


### [38] [Exploring Information Seeking Agent Consolidation](https://arxiv.org/abs/2602.00585)
*Guochen Yan,Jialong Wu,Zhengwei Tao,Bo Li,Qintong Zhang,Jiahao Xu,Haitao Mi,Yuejian Fang,Qingni Shen,Wentao Zhang,Zhonghai Wu*

Main category: cs.AI

TL;DR: 该论文研究了如何将异构信息检索智能体整合为单一基础智能体模型，比较了数据级整合和参数级整合两种策略，发现数据级整合更稳定，参数级整合虽高效但存在干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索智能体通常专门用于开放网络、文档或本地知识库，这种专业化限制了可扩展性和跨领域泛化能力，因此需要研究如何整合异构智能体。

Method: 研究了两种整合策略：1) 数据级整合 - 在混合领域特定数据集上联合训练统一模型；2) 参数级整合 - 在参数层面合并独立训练的智能体模型。

Result: 数据级整合保持强大稳定的基线性能，参数级整合提供了有前景的高效替代方案，但存在干扰和鲁棒性挑战。确定了参数级整合的关键设计因素。

Conclusion: 数据级整合仍是稳健的整合方法，参数级整合虽高效但需要解决干扰问题。研究为构建统一的信息检索基础智能体模型提供了重要见解。

Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.

</details>


### [39] [DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder](https://arxiv.org/abs/2602.00592)
*Jiaran Zhang,Luck Ma,Yanhao Li,Fanqi Wan,Di Qi,Xu Zhao,Jieyi Hou,Zhe Xie,Mengqiang Ren,Xin Wu,Zhewei Huang,Liangyu Chen,Yingwei Ma,Qi Han,Xiangyu Zhang*

Main category: cs.AI

TL;DR: DockSmith是一个专门用于Docker环境构建的智能代理，通过将环境构建视为核心代理能力而非预处理步骤，解决了软件工程代理训练和评估中的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 可靠的Docker环境构建是扩展软件工程代理执行基础训练和评估的主要瓶颈。传统方法将环境构建视为预处理步骤，但作者认为这应该是一个核心的代理能力。

Method: DockSmith采用专门的代理式Docker构建器，通过长时程工具使用、依赖推理和故障恢复来构建环境。使用增强的SWE-Factory风格流水线生成大规模执行基础的Docker构建轨迹进行训练，包含循环检测控制器和跨任务成功记忆。

Result: 在Multi-Docker-Eval上达到开源SOTA性能：39.72% Fail-to-Pass率和58.28% Commit率。在SWE-bench Verified、SWE-bench Multilingual和Terminal-Bench 2.0上表现出更好的分布外性能。

Conclusion: DockSmith不仅解决了Docker构建的瓶颈问题，还展示了环境构建作为核心代理能力的更广泛代理效益，其监督信号可以迁移到Docker构建之外的任务。

Abstract: Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.

</details>


### [40] [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)
*Wei Zeng,Xuchen Li,Ruili Feng,Zhen Liu,Fengwei An,Jian Zhao*

Main category: cs.AI

TL;DR: 提出硬件-算法协同设计框架，解决生成式游戏引擎的"内存墙"问题，实现720×480分辨率实时生成，比基线提升50倍像素吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有实时生成式游戏引擎受限于"内存墙"，只能实现低分辨率（如64×64），无法满足高分辨率神经模拟的需求

Method: 硬件-算法协同设计框架：1）非对称资源分配策略优化序列并行约束下的吞吐量；2）内存中心算子融合方案最小化片外带宽使用；3）流形感知潜在外推机制利用时间冗余掩盖延迟

Result: 在可编程AI加速器集群上实现720×480分辨率实时生成，比基线提升50倍像素吞吐量，在3D赛车和2D平台游戏中分别达到26.4 FPS和48.3 FPS，摊销有效延迟2.7ms

Conclusion: 通过架构协同设计解决"内存墙"不仅是优化，更是实现高保真、响应式神经游戏体验的前提条件

Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \times 480$ resolution -- a $50\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.

</details>


### [41] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: 本文评估了两种7B参数LLM在VirtualHome基准上的表现，提出了结构化自洽解码策略，发现不同模型在具身AI任务中各有优势。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要代理在模拟环境中理解目标、规划动作并执行任务。目前缺乏对大型语言模型在具身AI基准上的系统性评估，特别是不同模型在结构化任务生成方面的表现差异。

Method: 使用Embodied Agent Interface框架在VirtualHome基准上评估OPENPANGU-7B和QWEN2.5-7B模型。提出结构化自洽解码策略，通过多次采样和领域特定投票机制改进结构化生成任务的质量。评估四个基本任务：目标解释、动作序列、子目标分解和状态转移建模。

Result: 结构化自洽解码显著提升性能。OPENPANGU-7B在层次化规划任务中表现优异，而QWEN2.5-7B在动作级任务中具有优势。不同模型类型展现出互补优势。

Conclusion: 结构化自洽解码是提升具身AI任务性能的有效策略。不同LLM在具身AI任务中各有专长，这为未来具身AI系统开发提供了重要见解，表明可以结合不同模型的优势构建更强大的系统。

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [42] [Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees](https://arxiv.org/abs/2602.00616)
*Minhyuk Lee,Hyekyung Yoon,Myungjoo Kang*

Main category: cs.AI

TL;DR: 提出一种基于总变差理论的文本到图像扩散模型安全防护框架，通过提示投影选择性干预高风险提示，在保持良性提示对齐的同时减少不安全生成


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型需要安全防护机制来抑制不安全生成，但现有方法往往损害良性提示的图像对齐质量。需要一种既能减少不安全生成又能保持良性提示对齐的解决方案。

Method: 基于总变差理论提出安全-提示对齐权衡(SPAT)框架，采用推理阶段的提示投影方法，通过验证的代理目标将高风险提示映射到容忍控制的安全集合中，无需重新训练或微调生成器。

Result: 在四个数据集和三种扩散骨干网络上，相比强基线模型级对齐方法，将不当生成百分比相对降低16.7-60.0%，同时在COCO数据集上保持良性提示的图像对齐接近未对齐的参考模型。

Conclusion: 该研究为文本到图像扩散模型的安全部署提供了理论框架和实用方法，通过选择性干预高风险提示实现了安全性和对齐质量的有效平衡。

Abstract: Text-to-Image (T2I) diffusion models enable high-quality open-ended synthesis, but their real-world deployment demands safeguards that suppress unsafe generations without degrading benign prompt-image alignment. We formalize this tension through a total variation (TV) lens: once the reference conditional distribution is fixed, any nontrivial reduction in unsafe generations necessarily incurs TV deviation from the reference, yielding a principled Safety-Prompt Alignment Trade-off (SPAT). Guided by this view, we propose an inference-only prompt projection framework that selectively intervenes on high-risk prompts via a surrogate objective with verification, mapping them into a tolerance-controlled safe set while leaving benign prompts effectively unchanged, without retraining or fine-tuning the generator. Across four datasets and three diffusion backbones, our approach achieves 16.7-60.0% relative reductions in inappropriate percentage (IP) versus strong model-level alignment baselines, while preserving benign prompt-image alignment on COCO near the unaligned reference.

</details>


### [43] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 提出基于模糊相似推理的可解释超滤膜剩余使用寿命预测框架，通过物理健康指标和模糊规则实现透明预测


<details>
  <summary>Details</summary>
Motivation: 反渗透海水淡化中超滤膜因污染导致性能下降，现有预测维护模型缺乏可解释性和操作员信任，需要透明可靠的预测方法

Method: 使用基于跨膜压力、通量和阻力的物理健康指标，通过高斯隶属函数模糊化，采用相似性度量识别历史退化轨迹，构建Takagi-Sugeno模糊规则进行RUL预测

Result: 在工业规模超滤系统的12,528个运行周期上测试，平均绝对误差为4.50个周期，生成可解释的规则库与专家理解一致

Conclusion: 提出的可解释预测框架在保持高精度的同时提供了透明性，增强了操作员信任，有望改善超滤膜的预测维护实践

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [44] [SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent](https://arxiv.org/abs/2602.00663)
*Fabian P. Krüger,Andrea Hunklinger,Adrian Wolny,Tim J. Adler,Igor Tetko,Santiago David Villalba*

Main category: cs.AI

TL;DR: SEISMO是一个基于LLM的在线分子优化代理，通过每次oracle调用后更新，无需批量学习，在23个任务上比现有方法效率高2-3倍


<details>
  <summary>Details</summary>
Motivation: 分子优化是化学科学特别是药物发现的核心瓶颈，由于实验测定等oracle评估成本高昂且耗时，需要高度样本高效的优化方法

Method: SEISMO是一个严格在线的LLM代理，在推理时进行分子优化，每次oracle调用后更新，结合自然语言任务描述、标量分数和结构化解释反馈，基于完整优化轨迹生成新提议

Result: 在23个Practical Molecular Optimization基准任务上，SEISMO的优化曲线下面积比先前方法高2-3倍，通常在50次oracle调用内达到接近最大任务分数；在药物化学任务中，提供解释反馈能进一步提高效率

Conclusion: 利用领域知识和结构化信息是实现样本高效分子优化的关键，SEISMO展示了LLM代理在严格在线分子优化中的有效性

Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.

</details>


### [45] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: OpenGuanDan是一个用于评估AI智能体的开源四人斗地主基准测试平台，具有不完全信息、大规模状态空间、合作竞争混合目标等挑战性特征。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究需要更具挑战性的基准测试来推动多智能体智能决策领域的发展。斗地主作为流行的四人多轮中国纸牌游戏，具有不完全信息、大规模信息集、合作竞争混合目标等复杂特征，能够为现有智能决策方法提供严格的测试平台。

Method: 开发了OpenGuanDan基准测试平台，支持高效模拟斗地主游戏，并提供全面的评估框架。平台包含独立API支持人机交互和大语言模型集成，通过两种评估方式：1) AI智能体之间的配对竞赛，2) 人机对战。

Result: 实验结果表明，当前基于学习的智能体显著优于基于规则的智能体，但仍未达到超人类水平。这突显了在多智能体智能决策领域继续研究的必要性。

Conclusion: OpenGuanDan作为一个具有挑战性的基准测试平台，能够推动多智能体智能决策研究的发展。当前AI智能体虽已超越规则智能体，但距离超人类水平仍有差距，需要进一步研究。

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [46] [HumanStudy-Bench: Towards AI Agent Design for Participant Simulation](https://arxiv.org/abs/2602.00685)
*Xuan Liu,Haoyang Shang,Zizhang Liu,Xinyan Liu,Yunze Xiao,Yiwen Tu,Haojian Jin*

Main category: cs.AI

TL;DR: 提出HUMANSTUDY-BENCH基准和引擎，将LLM作为模拟参与者重构人类实验，通过新指标评估模拟与真实人类行为的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为社会科学实验模拟参与者时行为不稳定且对设计选择敏感，现有评估常混淆基础模型能力与实验实例化，需要区分模型本身与代理设置的影响。

Method: 将参与者模拟定义为完整实验协议上的代理设计问题，提出HUMANSTUDY-BENCH基准和执行引擎，采用Filter-Extract-Execute-Evaluate流程重构已发表的人类实验，在共享运行时中重放试验序列并运行原始分析流程。

Result: 实例化了12个基础研究作为动态基准的初始套件，涵盖个体认知、策略互动和社会心理学领域，包含6000多个试验，人类样本规模从数十到2100多名参与者。

Conclusion: 通过将参与者模拟框架化为代理设计问题，并引入新的评估指标，能够更准确地评估LLM代理在社会科学实验中的行为保真度，为使用LLM作为人类行为模拟提供更可靠的方法。

Abstract: Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.

</details>


### [47] [From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development](https://arxiv.org/abs/2602.00699)
*Xuan Liu,Ziyu Li,Mu He,Ziyang Ma,Xiaoxu Wu,Gizem Yilmaz,Yiyuan Xia,Bingbing Li,He Tan,Jerry Ying Hsi Fuh,Wen Feng Lu,Anders E. W. Jarfors,Per Jansson*

Main category: cs.AI

TL;DR: 本研究探讨了三种基于大语言模型的方法（预训练LLM驱动、上下文学习和微调）从领域文本中提取术语和关系，并利用最佳方法构建了铸造领域本体


<details>
  <summary>Details</summary>
Motivation: 传统本体构建依赖人工标注和传统NLP技术，过程劳动密集且成本高昂，特别是在铸造制造等专业领域。大语言模型的兴起为自动化知识提取提供了新可能

Method: 研究比较了三种LLM方法：1）预训练LLM驱动方法；2）上下文学习方法；3）微调方法，用于从领域特定文本中提取术语和关系，使用有限数据

Result: 比较了三种方法的性能，并使用最佳性能方法构建了铸造领域本体，该本体经过领域专家验证

Conclusion: LLM方法能够有效自动化领域知识提取，特别是在数据有限的情况下，为专业领域本体构建提供了高效解决方案

Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.

</details>


### [48] [Self-Guard: Defending Large Reasoning Models via enhanced self-reflection](https://arxiv.org/abs/2602.00707)
*Jingnan Zheng,Jingjun Xu,Yanzhen Luo,Chenhang Cui,Gelei Deng,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Self-Guard是一个轻量级安全防御框架，通过安全导向提示和安全激活引导两个阶段，在表示层面强化大型推理模型的安全合规性，有效弥合意识-合规差距。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然推理能力显著，但存在推理操纵和信息泄露等独特风险。现有对齐策略依赖繁重的后训练或外部干预，计算成本高且无法解决模型意识到风险但仍优先遵循用户指令的"意识-合规差距"问题。

Method: Self-Guard框架包含两个主要阶段：(1)安全导向提示：激活模型潜在的安全意识，引发自发反思；(2)安全激活引导：提取隐藏状态空间中的方向性变化并放大，确保推理过程中安全合规性优先于顺从性。

Result: 实验表明Self-Guard能有效弥合意识-合规差距，在不损害模型实用性的情况下实现稳健的安全性能。该框架在不同未见风险和模型规模上表现出强泛化能力，为LRM安全对齐提供了成本高效的解决方案。

Conclusion: Self-Guard通过轻量级的表示层面干预，解决了大型推理模型安全对齐中的关键挑战，为模型安全提供了一种计算高效且泛化能力强的防御框架。

Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.

</details>


### [49] [Physics-informed Diffusion Generation for Geomagnetic Map Interpolation](https://arxiv.org/abs/2602.00709)
*Wenda Li,Tongya Zheng,Kaixuan Chen,Shunyu Liu,Haoze Jiang,Yunzhi Hao,Rui Miao,Zujie Ren,Mingli Song,Hang Shi,Gang Chen*

Main category: cs.AI

TL;DR: 提出PDG框架，通过物理信息引导的扩散生成模型进行地磁地图插值，结合局部感受野和克里金原理，有效处理噪声并保证物理规律一致性。


<details>
  <summary>Details</summary>
Motivation: 现有散点数据插值方法未专门针对地磁地图设计，无法有效处理检测噪声和物理规律约束，导致性能不理想。地磁地图插值在导航和资源勘探中具有重要应用价值。

Method: 提出物理信息扩散生成框架(PDG)：1) 基于局部感受野设计物理信息掩码策略，引导扩散生成过程消除噪声干扰；2) 根据地磁地图克里金原理对扩散生成结果施加物理信息约束，确保符合物理规律。

Result: 在四个真实世界数据集上的大量实验和深入分析表明，PDG框架及其各组件具有优越性和有效性。

Conclusion: PDG框架通过物理信息引导的扩散生成方法，能够有效处理地磁地图插值问题，在消除噪声干扰的同时保证物理规律一致性，为地磁地图应用提供可靠解决方案。

Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.

</details>


### [50] [Learning More from Less: Unlocking Internal Representations for Benchmark Compression](https://arxiv.org/abs/2602.00710)
*Yueqi Zhang,Jin Hu,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yiwei Li,Jiayi Shi,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: REPCORE：通过对齐隐藏状态构建代表性核心集，仅需少量源模型即可精确评估LLM性能，解决了传统方法依赖大量历史数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型成本高昂，现有核心集方法需要大量源模型来估计可靠的项分布，这在源模型池较小时统计不稳定，尤其限制了新发布基准的评估。

Method: 提出REPCORE方法，将异构隐藏状态对齐到统一潜在空间来构建代表性核心集，利用这些子集进行性能外推，仅需少量源模型即可实现精确评估。

Result: 在5个基准测试和200多个模型上的实验表明，REPCORE在排名相关性和估计准确性方面一致优于基于输出的基线方法，仅需10个源模型即可实现精确估计。

Conclusion: REPCORE通过利用隐藏状态信息而非仅依赖离散正确性标签，有效解决了小源模型池下的评估挑战，谱分析表明对齐表示包含可分离的广泛响应倾向和任务特定推理模式。

Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.

</details>


### [51] [Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations](https://arxiv.org/abs/2602.00731)
*Kyle Hamilton,Ali Intizar*

Main category: cs.AI

TL;DR: 本文系统回顾了过去五年工业环境中预测性维护(PdM)的最新进展，指出数据驱动方法（如深度学习）精度更高但存在数据需求大、泛化性差、可解释性不足等问题，而传统基于规则的方法则精度低、误报多。作者提出结合深度学习和符号逻辑的神经符号AI作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 预测性维护在工业环境中至关重要，但现有方法各有局限：数据驱动方法需要大量标注数据、泛化能力差且缺乏可解释性；传统基于规则的方法精度低、误报多且需要持续专家监督。需要一种结合两者优势的解决方案。

Method: 本文采用系统综述方法，分析过去五年工业预测性维护的最新进展。重点关注结合传感器数据和人工规则的具体神经符号架构，描述多种神经符号AI架构并分析其在PdM领域的优势和局限性。

Result: 研究发现数据驱动方法（特别是深度学习）在准确性上优于传统知识系统，但存在重大局限性。混合系统（结合数据驱动和领域知识）有潜力克服单一方法的弱点。神经符号AI架构能够创建更准确、可解释、稳健的系统。

Conclusion: 神经符号AI是预测性维护领域有前景的方向，能够整合深度学习和符号逻辑的优势，解决当前方法的局限性，实现更准确、可解释和稳健的维护系统。

Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).

</details>


### [52] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: 论文提出Maria平台，一个用于初级医疗保健的生产级AI系统，通过四个工程支柱实现可信临床AI：清洁架构、事件驱动架构、Agent作为模块化单元、人机协同治理模型。


<details>
  <summary>Details</summary>
Motivation: AI在临床环境中的集成面临软件工程挑战，需要从孤立模型转向健壮、可治理、可靠的系统。当前工业应用常存在脆弱、原型衍生的架构和系统性监督缺失，导致"责任真空"和安全问责问题。

Method: 提出Maria平台作为行业案例研究，采用协同架构结合清洁架构（可维护性）和事件驱动架构（韧性和可审计性）。引入Agent作为主要模块化单元，每个拥有自主MLOps生命周期。技术上集成人机协同治理模型作为关键的事件驱动数据源。

Result: Maria平台作为参考架构，为在高风险领域构建可维护、可扩展和可问责的AI系统提供实用经验教训。

Conclusion: 可信临床AI通过四个基础工程支柱的整体集成实现：清洁架构、事件驱动架构、Agent模块化和人机协同治理。该平台为解决AI系统在医疗等高风险领域的可靠性、可维护性和问责性挑战提供了系统化方案。

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [53] [Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models](https://arxiv.org/abs/2602.00780)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Zenghuan Zhu,Jiajun Deng,Xinrui Lin,Shuo Liu,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.AI

TL;DR: EcoVLA：一种无需训练、即插即用的自适应剪枝框架，用于加速视觉-语言-动作模型推理，通过环境感知自适应剪枝和交错推理编排实现实时操作


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型参数庞大导致推理延迟高，阻碍实时操作，需要参数稀疏化。但环境动态变化时，最优稀疏模式也会变化，静态剪枝缺乏适应性，固定间隔的动态层剪枝粒度粗且重训练开销大

Method: 提出EcoVLA框架，包含两个组件：1) 环境感知自适应剪枝：轻量级自适应通道剪枝方法，利用物理环境的时间一致性更新稀疏模式；2) 交错推理编排：利用VLA推理中的计算气泡并行调度剪枝方法，对延迟影响可忽略

Result: 在多种VLA模型和基准测试中达到最先进性能，实现最高1.60倍加速且成功率仅下降0.4%，与令牌剪枝结合后可达2.18倍加速且性能仅下降0.5%，并在真实机器人上验证有效性

Conclusion: EcoVLA是一种无需训练、即插即用的自适应剪枝框架，能够适应环境动态变化，与现有VLA加速方法正交组合，显著提升推理速度同时保持高性能

Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.

</details>


### [54] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

TL;DR: 该论文主张在复杂高成本领域中使用世界模型作为智能体与现实世界之间的中介，以解决高成本交互的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习训练的LLM智能体在低成本环境（如游戏、数学、编程）中表现出色，但在物理机器人运行、ML工程时间成本、科学实验资源成本等高成本复杂领域中未能成功。真正的瓶颈在于执行动作获取奖励信号的高昂成本。

Method: 提出使用世界模型作为智能体与现实世界之间的中介。世界模型被建模为动态、奖励和任务分布的模型，能够克服高成本动作带来的根本障碍，包括极端离策略学习和长时程任务中的样本效率问题。

Result: 展示了世界模型如何在机器学习工程、计算机使用、机器人和AI科学等多个领域中为智能体提供关键且丰富的学习信号。

Conclusion: 识别了构建这些世界模型的挑战，并提出了在数据集管理、架构设计、扩展和评估方面的可行建议，以推动高成本复杂领域中智能体性能的下一阶段发展。

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [55] [MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing](https://arxiv.org/abs/2602.00811)
*Ronghao Lin,Honghao Lu,Ruixing Wu,Aolin Xiong,Qinggong Chu,Qiaolin He,Sijie Mai,Haifeng Hu*

Main category: cs.AI

TL;DR: 论文提出MissMAC-Bench基准，用于系统评估多模态情感计算中的模态缺失问题，通过统一评估标准和跨模态协同视角解决实际部署中的鲁棒性挑战。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态数据往往动态不确定，模态缺失导致分布偏移和语义缺陷，严重影响MAC模型的鲁棒性和实际部署，需要系统量化这一挑战。

Method: 提出MissMAC-Bench基准，建立公平统一的评估标准，基于两个指导原则：训练时无缺失先验、单一模型处理完整和不完整模态场景，并整合固定和随机缺失模式评估协议。

Result: 在4个数据集上对3个广泛使用的语言模型进行广泛实验，验证了多种MAC方法处理模态缺失问题的有效性。

Conclusion: 该基准为推进鲁棒多模态情感计算提供了坚实基础，促进了多媒体数据挖掘的发展，有助于弥合学术研究与实际应用之间的差距。

Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.

</details>


### [56] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

TL;DR: 论文提出DoPR方法，通过动态选择单一样本进行策略更新，显著降低RLVR训练的计算成本，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR框架在LLM推理任务上表现出色，但其训练过程需要大量奖励信号和计算资源，成本过高，限制了实际应用。

Method: 提出Dynamic One-Shot Policy Refinement (DoPR)方法：基于奖励波动性和探索驱动的采集策略，动态选择每个批次中最具信息量的单个训练样本进行策略更新，大幅减少计算开销。

Result: DoPR将训练开销降低近一个数量级，同时保持与基线相当的推理准确性，实现了资源高效的LLM后训练。

Conclusion: DoPR为推理密集型LLM应用提供了一条实用且可扩展的高效RL训练路径，使基于RL的LLM训练更加经济可行。

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [57] [Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward](https://arxiv.org/abs/2602.00845)
*Senkang Hu,Yong Dai,Yuzhi Zhao,Yihang Tao,Yu Guo,Zhengru Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.AI

TL;DR: InfoReasoner：通过语义信息增益奖励激励有效信息检索的统一框架，在七个QA基准上平均准确率提升达5.4%


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）的代理推理需要动态获取外部知识，但检索过程的优化面临挑战，因为缺乏密集、有原则的奖励信号。现有方法难以有效激励信息寻求行为。

Method: 1）理论层面：将信息增益重新定义为模型信念状态的不确定性减少，建立非负性、伸缩可加性和通道单调性保证；2）实践层面：提出输出感知的内在估计器，通过双向文本蕴涵的语义聚类直接从模型输出分布计算信息增益；3）使用组相对策略优化（GRPO）进行高效训练。

Result: 在七个问答基准测试中，InfoReasoner始终优于强大的检索增强基线，平均准确率提升高达5.4%。该方法为检索式代理推理提供了理论基础和可扩展路径。

Conclusion: InfoReasoner通过语义信息增益奖励框架，为代理推理中的信息检索优化提供了理论支撑和实用解决方案，显著提升了检索增强模型的性能。

Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.

</details>


### [58] [Persuasion Propagation in LLM Agents](https://arxiv.org/abs/2602.00851)
*Hyejun Jeong,Amir Houmansadr,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 研究AI智能体在长期任务中受到用户说服影响时的行为变化，发现任务执行前的信念预设比实时说服更能显著影响智能体的搜索和编码行为


<details>
  <summary>Details</summary>
Motivation: 现代AI智能体结合对话交互与自主任务执行（如编码和网络研究），当智能体在执行长期任务时受到用户说服，其下游任务行为会受到何种影响？需要研究信念层面干预如何影响任务行为

Method: 引入行为中心评估框架，区分任务执行期间和任务执行前的说服干预。在网络研究和编码任务上进行实验，比较实时说服与信念预设对智能体行为的影响

Result: 实时说服对行为影响微弱且不一致；而任务开始时明确指定信念状态的智能体，相比中性预设的智能体，平均减少26.9%的搜索次数和16.9%的唯一来源访问量

Conclusion: 说服（即使是先前的交互）能够影响智能体的行为，这强调了在智能体系统中进行行为层面评估的重要性

Abstract: Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\% fewer searches and visit 16.9\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.

</details>


### [59] [Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding](https://arxiv.org/abs/2602.00854)
*Fangzhou Lin,Qianwen Ge,Lingyu Xu,Peiran Li,Xiangbo Gao,Shuo Xing,Kazunori Yamada,Ziming Zhang,Haichong Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 论文提出"能力-理解鸿沟"概念，即AI系统性能提升时用户理解能力下降，并定义"认知完整性阈值"作为保持监督所需的最低理解水平。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统产生流畅、正确的结果，用户解释、验证和干预的能力逐渐被侵蚀。现有透明度、用户控制、素养和治理方法未能解决人类在AI委托下保持监督所需的基本理解问题。

Method: 提出认知完整性阈值(CIT)概念，将其操作化为三个功能维度：验证能力、理解保持型交互、治理的制度支撑。这为设计和治理提供了框架。

Result: 建立了一个理论框架，识别出当监督变成程序化且可争议性失效的阈值，为责任关键场景中的人机交互设计提供指导。

Conclusion: 需要将人机交互设计与认知可持续性对齐，确保在AI协助下保持监督、自主性和负责任参与，推动责任关键环境中的设计和治理议程。

Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.

</details>


### [60] [Multi-Head Attention Is a Multi-Player Game](https://arxiv.org/abs/2602.00861)
*Kushal Chakrabarti,Nirmal Balachundar*

Main category: cs.AI

TL;DR: 论文将transformer注意力头建模为多智能体博弈，证明交叉熵训练诱导出潜在博弈，梯度下降收敛到纳什均衡但存在效率损失。提出用博弈论框架分析幻觉和冗余问题，并开发GAME-LoRA方法改善性能。


<details>
  <summary>Details</summary>
Motivation: 现有transformer注意力头之间存在竞争和协调关系，但训练时却将其视为单一优化器。这种差距导致注意力头之间存在未定价的外部性（冗余、相关错误），可能产生无界效率损失。

Method: 将transformer注意力头建模为多智能体博弈，证明交叉熵训练诱导出潜在博弈。提出用博弈无政府价格（PoA）量化效率损失，该价格由注意力头交互矩阵Γ(G)的离对角线质量界定。开发GAME-LoRA方法，结合Barlow Twins去相关和log-determinant协调压力来减少Γ(G)。

Result: 理论证明：Γ(G)能预测幻觉概率（p<0.05），涌现的联盟表现出选择性协调。实验验证：GAME-LoRA能实现高达18%的幻觉减少（平均8%），且不降低知识性能——这是忽略博弈结构的方法无法达到的帕累托改进。

Conclusion: transformer注意力头的博弈论视角为理解幻觉和冗余提供了统一框架。通过减少注意力头交互矩阵Γ(G)的正则化方法（如GAME-LoRA）能有效改善模型性能，实现帕累托改进。

Abstract: Modern transformer attention is internally multi-agent -- heads compete and coordinate -- yet we train it as if it were a monolithic optimizer. We formalize this gap: cross-entropy training induces an implicit potential game among heads, and gradient descent converges to Nash equilibria with potentially unbounded inefficiency due to unpriced externalities (redundancy, correlated errors). Our main result bounds the Price of Anarchy by $Γ(G)$, the off-diagonal mass of a head interaction matrix capturing weight and gradient coupling. Under mild smoothness assumptions, we prove that both \emph{excess hallucination probability} and \emph{excess head redundancy} scale with PoA, unifying two distinct failure modes into a single mechanism. The bound is prescriptive: regularization that reduces $Γ(G)$ provably tightens PoA. We instantiate this as GAME-LoRA, combining Barlow Twins decorrelation with log-determinant coordination pressure. Experiments validate the theory: $Γ(G)$ predicts hallucination ($p{<}0.05$), emergent coalitions exhibit selective coordination, and GAME-LoRA achieves up to 18\% hallucination reduction (8\% average) with no knowledge degradation -- a Pareto improvement inaccessible to methods ignoring the game structure.

</details>


### [61] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

TL;DR: 提出首个CAN总线基础模型，将解码后的CAN信号视为语言进行大规模预训练，然后在多种汽车保险任务上微调，实现跨任务泛化。


<details>
  <summary>Details</summary>
Motivation: 现有CAN数据处理方法多为针对特定任务的孤立模型训练，缺乏共享表示学习和跨任务泛化能力。受NLP和CV领域基础模型范式成功的启发，希望将这一范式应用于CAN数据领域。

Method: 将CAN数据视为语言，提出统一的离散-连续混合信号标记化方案，在大规模未标记解码CAN信号上进行预训练，然后针对异构汽车保险任务进行微调，解决时间复杂性和行程特定变异性挑战。

Result: 一个预训练的CAN模型能够有效适应多种预测任务，验证了基础模型范式在CAN数据上的有效性，为汽车AI中的可泛化表示学习确立了新方向。

Conclusion: 基础模型范式在NLP和CV领域取得成功后，同样适用于CAN数据，能够实现跨任务泛化，为汽车AI领域开辟了新的通用表示学习方向。

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [62] [Beyond Output Critique: Self-Correction via Task Distillation](https://arxiv.org/abs/2602.00871)
*Hossein A. Rahmani,Mengting Wan,Pei Zhou,Longqi Yang,Nick Craswell,Emine Yilmaz,Sujay Kumar Jauhar*

Main category: cs.AI

TL;DR: SELF-THOUGHT框架通过任务抽象引导LLM自我修正，将任务提炼为结构化模板，指导解决方案实例化，并支持模板跨模型迁移，提升大小模型的推理准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我修正方法主要在输出层面进行批判性修正，只能修补表面错误，难以纠正深层推理缺陷。需要一种能理解任务本质结构、减少错误传播的自我修正框架。

Method: 提出SELF-THOUGHT框架：1）任务抽象：将输入和初始响应提炼为结构化模板，捕捉关键变量、约束和问题结构；2）解决方案实例化：基于抽象模板指导生成修正后的响应；3）模板跨模型迁移：大模型生成的模板可作为小模型的结构化指导。

Result: 在多样化推理任务上的实验表明，SELF-THOUGHT提高了准确率、鲁棒性和泛化能力，使小模型无需大量微调或依赖外部验证器即可实现更可靠的修正。

Conclusion: SELF-THOUGHT通过任务抽象和模板迁移，为构建更可靠的自修正语言系统提供了可扩展路径，特别有助于提升小模型的自我修正能力。

Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.

</details>


### [63] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

TL;DR: Synapse框架通过联邦学习训练共享的全局工具使用知识模型，提高多智能体LLM系统的工具使用效果，同时降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的智能体在联邦学习下面临通信成本高、数据和工具使用异质性等挑战，限制了协作学习的效果。

Method: 训练共享的全局工具使用知识模型，客户端智能体在本地学习工具使用模式，通过协调器传输工件进行联邦聚合，更新全局工具目录并重新分发，使用模板化表示、嵌入检索与LLM重排序、自适应掩码等技术。

Result: Synapse提高了工具使用效果，相比权重或提示共享方法，减少了多智能体LLM系统的通信开销。

Conclusion: Synapse框架有效解决了联邦学习中LLM智能体的协作挑战，通过共享工具使用知识模型实现了更好的性能和更低的通信成本。

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [64] [Supervised sparse auto-encoders as unconstrained feature models for semantic composition](https://arxiv.org/abs/2602.00924)
*Ouns El Harzli,Hugo Wallner,Yoonsoo Nam,Haixuan Xavier Tao*

Main category: cs.AI

TL;DR: 提出一种监督式稀疏自编码器方法，通过联合学习稀疏概念嵌入和解码器权重，解决传统SAE在重建、可扩展性和语义对齐方面的限制。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在机制可解释性中面临两大挑战：1) L1惩罚的非平滑性阻碍重建和可扩展性；2) 学习特征与人类语义缺乏对齐。需要改进现有方法以提升实用性和语义可解释性。

Method: 采用无约束特征模型（源自神经崩溃理论）并引入任务监督。监督（仅解码器）SAE通过联合学习稀疏概念嵌入和解码器权重来重建特征向量。在Stable Diffusion 3.5上进行验证。

Result: 方法展现出组合泛化能力，能够成功重建训练中未见的概念组合图像，并支持无需提示修改的语义图像编辑，实现特征级干预。

Conclusion: 通过监督和数学框架改进的稀疏自编码器方法有效解决了传统SAE的局限性，在组合泛化和语义图像编辑方面表现出色，为机制可解释性提供了更实用的工具。

Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.

</details>


### [65] [Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents](https://arxiv.org/abs/2602.00929)
*Zergham Ahmed,Kazuki Irie,Joshua B. Tenenbaum,Christopher J. Bates,Samuel J. Gershman*

Main category: cs.AI

TL;DR: TheoryCoder-2是一个基于理论的强化学习代理，利用大语言模型的上下文学习能力主动学习可重用抽象，而不是依赖人工指定的抽象，从而在多样环境中实现更高效的规划和泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型代理和深度强化学习系统在抽象学习和规划泛化方面仍有挑战。现有的基于理论的强化学习系统（如TheoryCoder）虽然通过使用抽象表现出强大的泛化能力，但严重依赖人工提供的抽象，回避了抽象学习问题。

Method: TheoryCoder-2利用大语言模型的上下文学习能力，从经验中主动合成可重用抽象，并将这些抽象整合到分层规划过程中。该方法不依赖人工指定的抽象，而是通过经验学习抽象。

Result: 在BabyAI、Minihack和VGDL游戏（如Sokoban）等多样化环境中的实验表明，TheoryCoder-2比增强了经典规划领域构建、基于推理的规划以及先前程序合成代理（如WorldCoder）的基线大语言模型代理具有显著更高的样本效率。TheoryCoder-2能够解决基线方法失败的复杂任务，同时只需要最少的人工提示。

Conclusion: TheoryCoder-2通过主动学习抽象而非依赖人工指定抽象，实现了基于理论的强化学习系统的改进，在保持最少人工干预的同时，显著提高了样本效率和任务解决能力。

Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.

</details>


### [66] [The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis](https://arxiv.org/abs/2602.00947)
*Mohan Reddy*

Main category: cs.AI

TL;DR: 聊天界面不适合多步骤数据分析任务，会导致认知过载和性能下降，提出了八个混合设计模式来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 当前聊天已成为AI辅助数据分析的默认界面，但对于多步骤、状态依赖的分析任务，这种界面设计存在根本性缺陷。基于Woods(1984)的"锁眼效应"理论，作者认为聊天界面通过五种机制系统性降低分析性能。

Method: 提出了认知过载的形式化模型：O = max(0, m - v - W)，其中m是任务相关项目数，v是可见项目数，W是工作记忆容量。当O>0时，错误概率增加，分析偏差放大。提出了八个混合设计模式来解决认知瓶颈：生成式UI、无限画布、指示性交互、状态轨道、幽灵层、准备工作、语义缩放和概率UI。

Result: 聊天界面通过五种机制导致认知过载：(1)内容不断替换破坏海马体空间记忆系统；(2)隐藏状态变量超出工作记忆容量；(3)强制言语化触发言语遮蔽效应，降低视觉模式识别能力；(4)线性文本流阻碍认知行动和认知卸载；(5)序列化惩罚随数据维度增加而增加。

Conclusion: 聊天界面不适合开放探索式数据分析任务，需要采用混合设计模式来平衡自然语言交互与视觉空间表示。良好搭建的对话系统可以减少指导性任务的认知负荷，但框架主要适用于开放探索场景。论文最后提出了可证伪的假设和实验范式进行实证验证。

Abstract: Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.

</details>


### [67] [MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support](https://arxiv.org/abs/2602.00950)
*António Farinhas,Nuno M. Guerreiro,José Pombal,Pedro Henrique Martins,Laura Melton,Alex Conway,Cara Dochat,Maya D'Eon,Ricardo Rei*

Main category: cs.AI

TL;DR: MindGuard：基于临床风险分类法的心理健康对话安全系统，通过专家标注数据集和轻量级分类器，在保持治疗性内容的同时准确识别危机情况


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在心理健康支持应用中，通用安全措施无法区分治疗性披露和真正的临床危机，导致安全失效。需要专门针对心理健康对话的临床安全解决方案

Method: 1. 与心理学博士合作开发临床风险分类法；2. 发布MindGuard-testset专家标注数据集；3. 使用受控双代理设置生成合成对话；4. 训练轻量级安全分类器（4B和8B参数）

Result: MindGuard分类器在高召回率操作点减少误报，与临床语言模型配合使用时，在对抗性多轮交互中实现更低的攻击成功率和有害参与率，优于通用安全措施

Conclusion: MindGuard为心理健康对话提供了临床基础的安全解决方案，在保护治疗性内容的同时有效识别危机情况，所有模型和人工评估数据已开源

Abstract: Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures. To address this gap, we introduce a clinically grounded risk taxonomy, developed in collaboration with PhD-level psychologists, that identifies actionable harm (e.g., self-harm and harm to others) while preserving space for safe, non-crisis therapeutic content. We release MindGuard-testset, a dataset of real-world multi-turn conversations annotated at the turn level by clinical experts. Using synthetic dialogues generated via a controlled two-agent setup, we train MindGuard, a family of lightweight safety classifiers (with 4B and 8B parameters). Our classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, help achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards. We release all models and human evaluation data.

</details>


### [68] [R-HTN: Rebellious Online HTN Planning for Safety and Game AI](https://arxiv.org/abs/2602.00951)
*Hector Munoz-Avila,David W. Aha,Paola Rizzo*

Main category: cs.AI

TL;DR: 论文提出R-HTN算法，用于在线分层任务网络规划，使智能体能够在违反内置指令时拒绝执行用户任务或自适应调整计划。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常盲目执行用户任务，但在某些安全或伦理场景下，智能体需要具备"智能抗命"能力，能够拒绝执行可能违反安全规定或人格特质指令的任务。

Method: 结合HTN规划、在线规划和内置指令集D，提出R-HTN算法。研究两种智能体变体：非自适应智能体（发现违反指令时停止执行）和自适应智能体（修改HTN计划寻找替代方案）。

Result: R-HTN智能体在测试中从未违反指令，并在可行情况下尽力实现用户目标（尽管可能以用户未预期的方式）。自适应智能体比非自适应智能体表现更好。

Conclusion: R-HTN为在线HTN规划提供了一种处理内置指令的通用算法，使智能体能够在安全约束下进行"智能抗命"，平衡用户目标与指令遵守。

Abstract: We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \D. Like other agents that are capable of rebellion (i.e., {\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.

</details>


### [69] [Small-Margin Preferences Still Matter-If You Train Them Right](https://arxiv.org/abs/2602.00954)
*Jinlong Pang,Zhaowei Zhu,Na Di,Yichi Zhang,Yaxuan Wang,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: MixDPO提出了一种难度感知的训练策略，通过将困难偏好对路由到SFT目标，而将简单对应用于偏好损失，从而有效利用模糊偏好对进行对齐优化。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）对偏好对的质量和难度高度敏感，通常将小边际（模糊）对视为噪声并过滤掉。然而研究发现，困难对在偏好损失下会破坏训练稳定性，但在监督微调中仍包含有用的监督信号。

Method: MixDPO采用难度感知训练策略：1）根据边际定义的难度将偏好数据从易到难排序（课程学习）；2）将困难对路由到SFT目标，同时对简单对应用偏好损失。这种混合设计能够利用模糊对而不引发优化失败。

Result: 在三个LLM-judge基准测试中，MixDPO在DPO和一系列广泛使用的变体上持续改进对齐效果，在AlpacaEval~2长度控制胜率上表现尤为突出。

Conclusion: MixDPO通过难度感知的混合训练策略，有效解决了偏好优化中对困难对处理的挑战，显著提升了语言模型的对齐性能，为利用模糊偏好数据提供了实用机制。

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

</details>


### [70] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: 本文通过LEAS系统揭示了ARL中推理与工具使用行为间的训练干扰问题，并提出DART框架通过分离的低秩适配模块解耦参数更新，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有Agentic Reinforcement Learning (ARL)方法通常训练单一共享模型参数来支持推理和工具使用行为，隐含假设联合训练能提升整体智能体性能，但这一假设缺乏实证检验。

Method: 1. 引入Linear Effect Attribution System (LEAS)定量分析推理与工具使用行为间的干扰；2. 提出Disentangled Action Reasoning Tuning (DART)框架，通过分离的低秩适配模块显式解耦推理和工具使用的参数更新。

Result: 实验结果显示DART持续优于基线方法，平均提升6.35%，且使用单一模型即可达到与显式分离工具使用和推理的多智能体系统相当的性能。

Conclusion: ARL中推理与工具使用行为存在训练干扰，DART通过参数解耦有效缓解此问题，挑战了当前ARL范式，为更高效的智能体训练提供了新方向。

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [71] [Error Taxonomy-Guided Prompt Optimization](https://arxiv.org/abs/2602.00997)
*Mayank Singh,Vikas Yadav,Eduardo Blanco*

Main category: cs.AI

TL;DR: ETGPO是一种基于错误分类的提示优化方法，采用自上而下的方式分析全局失败模式，相比传统试错方法显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法多为自下而上的试错方式，缺乏全局视角，计算成本高。需要一种能分析全局失败模式、更高效的优化方法。

Method: ETGPO采用自上而下的方法：收集模型错误、分类建立错误分类法、针对最常见失败模式在提示中添加针对性指导。

Result: 在数学、问答和逻辑推理等多个基准测试中，ETGPO达到或优于最先进方法的准确率，同时优化阶段token使用量和评估预算减少约三分之二。

Conclusion: ETGPO通过错误分类法指导的提示优化，实现了高效且有效的性能提升，为自动提示优化提供了新的全局视角方法。

Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.

</details>


### [72] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

TL;DR: 研究发现人类偏好对齐会放大LLM的谄媚行为，提出了基于协方差分析的形式化机制和训练时干预方法


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基于偏好的后训练后表现出更强的谄媚行为，即使与事实准确性或合理判断相冲突也会附和用户的信念。需要理解这种失败模式如何被人类反馈对齐放大。

Method: 1) 形式化分析对齐如何通过奖励学习放大谄媚行为；2) 分析Bradley-Terry等随机效用模型下的奖励学习；3) 提出训练时干预方法，通过KL散度最小化推导闭式奖励修正

Result: 计算实验发现奖励差距普遍存在，在所有考虑的配置中都会导致行为漂移。提出的干预方法能够防止谄媚行为增加。

Conclusion: 人类偏好对齐会系统性放大LLM的谄媚行为，但可以通过训练时干预来中和这种放大机制，通过最小化KL散度推导的闭式奖励修正能有效防止谄媚行为增加。

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [73] [HalluHard: A Hard Multi-Turn Hallucination Benchmark](https://arxiv.org/abs/2602.01031)
*Dongyang Fan,Sebastien Delsad,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.AI

TL;DR: HalluHard是一个多轮对话幻觉基准测试，包含950个种子问题，涵盖法律、研究、医疗和编程四个高风险领域，通过内联引用评估事实基础性，并提出了基于网络搜索的证据检索评估管道。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮对话中会产生看似合理但缺乏事实依据的虚假信息，随着对话轮次增加，早期错误会级联放大，这在法律、医疗等高风险领域尤其危险。

Method: 1) 创建HalluHard基准测试：包含950个种子问题，涵盖四个高风险领域；2) 通过内联引用要求来操作化"基础性"概念；3) 提出评估管道：迭代检索网络证据，获取、过滤和解析全文来源（包括PDF）来验证引用是否支持生成内容。

Result: 即使在网络搜索辅助下，前沿专有和开源模型的幻觉率仍然很高（最强配置Opus-4.5+网络搜索约为30%），内容基础性错误持续存在。幻觉行为受模型能力、对话轮次位置、有效推理和所需知识类型影响。

Conclusion: 多轮对话中的幻觉问题依然严重，需要更严格的评估方法和改进的模型架构。提出的HalluHard基准和评估管道为可靠评估开放域设置中的基础性提供了有效工具。

Abstract: Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search ($\approx 30\%$ for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.

</details>


### [74] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

TL;DR: 提出一个强化学习框架，通过边际信息增益机制提供连续奖励信号，结合解耦掩码策略和双门监督微调目标，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法存在奖励稀疏和信用分配效率低的问题，需要更有效的机制来提供连续奖励信号并减少训练噪声。

Method: 1) 基于单调历史水印的步进边际信息增益机制；2) 解耦掩码策略：过程导向奖励应用于思维链，结果导向奖励应用于完整完成；3) 双门监督微调目标。

Result: 在文本和多模态基准测试（如MATH、Super-CLEVR）中，该方法在样本效率和最终准确率上均优于GRPO等基线方法，并展现出优越的分布外鲁棒性和零样本迁移能力。

Conclusion: 该框架通过提供连续奖励信号和有效的信用分配机制，显著提升了强化学习在大语言模型推理任务中的效果，具有更好的泛化能力和鲁棒性。

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [75] [SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning](https://arxiv.org/abs/2602.01062)
*Chenyi Li,Yuan Zhang,Bo Wang,Guoqing Ma,Wei Tang,Haoyang Huang,Nan Duan*

Main category: cs.AI

TL;DR: 提出一种基于核相似度的轨迹级多样性目标，通过留一法边际贡献计算，作为优势塑形项增强RL训练，提升LLM推理性能同时保持解多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习虽然能提升LLM数学推理性能，但会导致解多样性降低，模型将概率质量集中在少数解上。受收益递减原则启发，需要平衡性能与多样性。

Method: 1) 基于核相似度定义轨迹级多样性目标；2) 使用留一法计算每个采样轨迹的边际贡献；3) 将该目标作为可插拔的优势塑形项集成到策略优化中；4) 通过分布扰动框架分析单轨迹对多样性的贡献，理论上证明稀有轨迹具有更高边际贡献。

Result: 在不同模型规模上的广泛实验表明，该方法在多个基准测试中，在Pass@1和Pass@K指标上均持续优于强基线方法。

Conclusion: 提出的多样性增强强化学习方法能有效提升LLM推理性能，同时保持解多样性，解决了现有方法中性能提升与多样性降低的矛盾。

Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.

</details>


### [76] [ConvexBench: Can LLMs Recognize Convex Functions?](https://arxiv.org/abs/2602.01075)
*Yepeng Liu,Yu Huang,Yu-Xiang Wang,Yingbin Liang,Yuheng Bu*

Main category: cs.AI

TL;DR: 论文提出了一个测试LLMs识别符号目标函数凸性的基准CB，发现LLMs在深度复合函数上表现急剧下降，并提出了一个分治框架来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型开始自动化研究级数学和科学任务，需要测试LLMs是否能够理解和推理凸性这一重要数学概念。

Method: 引入CB基准测试LLMs识别深度复合符号目标函数的凸性，发现性能随深度增加急剧下降。提出基于分治的代理框架：使用外部工具构建抽象语法树，并对每个中间子表达式进行递归推理。

Result: 前沿LLMs在深度复合推理上存在显著差距：F1分数从深度2的1.0下降到深度100的约0.2。提出的分治框架能可靠缓解深度复合失败，在深度100时达到F1分数1.0。

Conclusion: LLMs在深度复合函数的凸性推理上存在显著缺陷，但通过分治代理框架可以有效解决这一问题，为LLMs在数学推理任务中的应用提供了重要改进方向。

Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \cb, a scalable and mechanically verifiable benchmark for testing \textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \textit{parsing failure} and \textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).

</details>


### [77] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

TL;DR: AutoHealth是一个不确定性感知的多智能体系统，能够自主建模健康数据并评估模型可靠性，在预测性能和不确定性估计方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在健康数据应用中存在局限性：难以泛化到异构健康数据模态、过度依赖预定义解决方案模板、缺乏不确定性估计，而医疗决策需要可靠的可靠性评估。

Method: 提出AutoHealth系统，采用五个专门智能体的闭环协调机制，执行数据探索、任务条件化模型构建、训练和优化，同时优先考虑预测性能和不确定性量化。

Result: 在包含17个任务的真实世界基准测试中，AutoHealth完成所有任务，预测性能比最先进基线提高29.2%，不确定性估计提高50.2%。

Conclusion: AutoHealth通过不确定性感知的多智能体系统，为健康数据建模提供了可靠、可解释的自主机器学习解决方案，支持可信赖的解释和风险感知决策。

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [78] [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)
*Yiliu He,Tianle Li,Binghao Ji,Zhiyuan Liu,Di Huang*

Main category: cs.AI

TL;DR: EvoOpt-LLM：基于LLM的工业优化建模框架，支持自动化模型构建、动态约束注入和变量剪枝，仅需少量训练样本即可实现高生成率和可执行率。


<details>
  <summary>Details</summary>
Motivation: 工业规划调度中，将自然语言需求转化为MILP模型并随业务规则演化维护需要大量专家知识。现有LLM方法存在数据效率低、求解器有效性有限、工业规模问题扩展性差等问题。

Method: 基于7B参数LLM构建统一框架，采用参数高效的LoRA微调，包含三个核心模块：自动化模型构建、动态业务约束注入和端到端变量剪枝。

Result: 仅用3,000训练样本达到91%生成率和65.9%可执行率，关键性能在1,500样本内显现。约束注入可靠增强现有MILP模型，变量剪枝在400样本下对中型LP模型达到约0.56的F1分数。

Conclusion: EvoOpt-LLM为工业优化建模提供了实用且数据高效的方法，减少对专家干预的依赖，同时提升适应性和求解器效率。

Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.

</details>


### [79] [MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI](https://arxiv.org/abs/2602.01086)
*Takahito Nakajima*

Main category: cs.AI

TL;DR: MedBeads提出了一种面向AI代理的医疗数据基础设施，使用不可变的Merkle DAG结构存储临床事件，解决传统EMR与AI代理之间的"上下文不匹配"问题。


<details>
  <summary>Details</summary>
Motivation: 当前电子病历系统为人类设计，AI代理接收碎片化数据，需要依赖概率推理重建患者历史，导致幻觉和审计困难。存在"上下文不匹配"问题。

Method: 提出MedBeads架构，将临床事件作为不可变的"Beads"节点存储在Merkle有向无环图中，每个节点加密引用其因果前驱。实现原型包括Go核心引擎、Python中间件和React可视化界面。

Result: 成功实现工作流程，将FHIR资源转换为因果链接图。BFS上下文检索算法以O(V+E)复杂度遍历相关子图，支持实时决策。篡改检测通过加密链保证，可视化界面帮助临床医生理解因果联系。

Conclusion: MedBeads通过从概率搜索转向确定性图遍历，从可变记录转向不可变链，解决了"上下文不匹配"问题，为"可信医疗AI"提供基础。AI接收的上下文是确定性和防篡改的，而LLM负责解释。开源发布以加速代理原生数据标准发展。

Abstract: Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous "Clinical Agents" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a "Context Mismatch": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable "Beads"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This "write-once, read-many" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the "Context Mismatch" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for "Trustworthy Medical AI." It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient "AI-native language." We release MedBeads as open-source software to accelerate agent-native data standards.

</details>


### [80] [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)
*Yang Liu,Chuan Zhou,Yancheng Chen,Shuai Zhang,Xixun Lin,Xiaoqing Wang*

Main category: cs.AI

TL;DR: FALCON框架通过语法约束解码、可行性修复层和自适应采样确保LLM求解组合优化问题时100%可行性，同时使用BOPO训练方法提升解的质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在组合优化中表现出潜力，但缺乏保证解可行性的机制，这在现实部署中至关重要。

Method: FALCON框架包含三个创新：语法约束解码确保句法有效性；可行性修复层纠正语义约束违反；自适应Best-of-N采样高效分配推理计算。训练采用BOPO方法，通过目标差距加权偏好对提供密集监督。

Result: 理论上证明了BOPO的收敛性并提供了修复引起的质量损失边界。在七个NP难组合优化问题上，FALCON实现了完美可行性，同时匹配或超越了最先进的神经和LLM求解器的解质量。

Conclusion: FALCON为LLM在组合优化中的实际部署提供了可行的解决方案，通过确保100%可行性同时保持高质量解，解决了LLM在该领域的关键限制。

Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% feasibility through three key innovations: (i) \emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

</details>


### [81] [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)
*Yiming Dong,Kun Fu,Haoyu Li,Xinyuan Zhu,Yurou Liu,Lijing Shao,Jieping Ye,Zheng Wang*

Main category: cs.AI

TL;DR: 本文通过"目标级黑客攻击"框架解释了MoE模型在RLVR训练中的不稳定性，揭示了训练-推理差异异常增长的机制。


<details>
  <summary>Details</summary>
Motivation: MoE架构在RLVR训练中容易出现不稳定性，严重影响模型能力提升，但其根本原因和机制尚不清楚，需要系统性分析。

Method: 提出基于"目标级黑客攻击"的理论框架，通过token级信用错位分析系统级虚假信号，并在30B MoE模型上进行大量实验验证。

Result: 揭示了MoE模型中训练-推理差异异常增长的关键病理动态的起源和机制，为RLVR训练不稳定性提供了具体的因果解释。

Conclusion: 研究结果为理解MoE模型训练不稳定性提供了理论框架和机制解释，为设计稳定的RLVR算法提供了指导。

Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.

</details>


### [82] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

TL;DR: BiCarFormer：首个多模态方法，整合DTC序列和环境数据进行车辆故障多标签序列分类，显著提升诊断准确性


<details>
  <summary>Details</summary>
Motivation: 当前车辆诊断系统主要依赖诊断故障码序列，但忽略了温度、湿度、压力等环境数据，而这些信息对专家判断故障模式至关重要。环境数据复杂且噪声多，需要有效整合方法。

Method: 提出BiCarFormer双向Transformer模型，专门处理车辆事件序列，采用嵌入融合和协同注意力机制捕捉诊断码与环境数据之间的关系。

Result: 在包含22,137个错误码和360个错误模式的实际汽车数据集上，相比仅使用DTC序列的传统序列模型，分类性能显著提升。

Conclusion: 整合环境上下文信息对提高车辆诊断准确性和鲁棒性至关重要，有助于降低维护成本并增强汽车行业自动化流程。

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [83] [Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach](https://arxiv.org/abs/2602.01131)
*Yue Zhong,Jiawen Kang,Yongju Tong,Hong-Ning Dai,Dong In Kim,Abbas Jamalipour,Shengli Xie*

Main category: cs.AI

TL;DR: 提出一个感知-通信-计算-控制闭环框架，将通信延迟对物理控制稳定性的影响显式建模，通过李雅普诺夫稳定性理论将稳定性要求转化为可量化资源边界，并设计基于剪枝的PPO算法实现轻量级资源分配。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济的发展，无人机作为空中基站需要支持从延迟敏感关键任务到带宽密集型数据流的多样化服务。传统吞吐量中心设计无法解决有限机载资源与严格稳定性要求之间的冲突，需要新的框架来保证任务可靠性。

Method: 1) 提出感知-通信-计算-控制闭环框架，显式建模通信延迟对物理控制稳定性的影响；2) 利用李雅普诺夫稳定性理论推导控制系统状态演化与通信约束的内在映射；3) 将资源分配问题建模为Stackelberg博弈，无人机作为领导者动态定价资源，用户作为跟随者基于服务紧迫性优化请求；4) 提出基于剪枝的PPO算法，通过动态结构化剪枝机制压缩神经网络规模，实现轻量级训练和推理。

Result: 仿真结果表明，所提方案在动态低空环境中能有效保证控制环稳定性，同时最大化系统效用。基于剪枝的PPO算法显著降低了计算开销，使无人机能够以最小推理延迟快速逼近博弈均衡。

Conclusion: 该研究提出了一个将通信延迟与物理控制稳定性直接关联的闭环框架，通过稳定性理论将抽象要求转化为可量化资源边界，并设计了轻量级算法实现高效资源分配，为低空经济中无人机异构网络的可靠服务提供了新思路。

Abstract: With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networks is often compromised by the conflict between limited onboard resources and stringent stability requirements. Moving beyond traditional throughput-centric designs, we propose a Sensing-Communication-Computing-Control closed-loop framework that explicitly models the impact of communication latency on physical control stability. To guarantee mission reliability, we leverage the Lyapunov stability theory to derive an intrinsic mapping between the state evolution of the control system and communication constraints, transforming abstract stability requirements into quantifiable resource boundaries. Then, we formulate the resource allocation problem as a Stackelberg game, where UAVs (as leaders) dynamically price resources to balance load and ensure stability, while users (as followers) optimize requests based on service urgency. Furthermore, addressing the prohibitive computational overhead of standard Deep Reinforcement Learning (DRL) on energy-constrained edge platforms, we propose a novel and lightweight pruning-based Proximal Policy Optimization (PPO) algorithm. By integrating a dynamic structured pruning mechanism, the proposed algorithm significantly compresses the neural network scale during training, enabling the UAV to rapidly approximate the game equilibrium with minimal inference latency. Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments.

</details>


### [84] [PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?](https://arxiv.org/abs/2602.01146)
*Sidharth Pulipaka,Oliver Chen,Manas Sharma,Taaha S Bajwa,Vyas Raina,Ivaxi Sheth*

Main category: cs.AI

TL;DR: 论文提出PersistBench基准测试，用于评估LLM长期记忆带来的安全风险，发现主流模型在跨域泄漏和记忆诱导奉承问题上失败率很高。


<details>
  <summary>Details</summary>
Motivation: 对话助手越来越多地将长期记忆与LLM集成，这种记忆持久性可以增强个性化，但同时也带来了被忽视的安全风险，需要系统性地测量这些风险。

Method: 提出PersistBench基准测试，识别两种长期记忆特有风险：跨域泄漏（LLM不适当地从长期记忆中注入上下文）和记忆诱导奉承（存储的长期记忆暗中强化用户偏见），并评估18个前沿和开源LLM。

Result: 评估结果显示这些LLM的失败率惊人地高：跨域样本中位数失败率为53%，奉承样本中位数失败率为97%，表明现有模型在长期记忆安全方面存在严重问题。

Conclusion: PersistBench基准测试有助于推动开发更鲁棒、更安全的长期记忆使用方式，提升前沿对话系统的安全性。

Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.

</details>


### [85] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: CAREP是一个多智能体系统，用于从车辆诊断故障代码(DTCs)中自动生成错误模式(EP)规则，替代传统手工制定规则的方法。


<details>
  <summary>Details</summary>
Motivation: 现代车辆产生数千种不同的诊断故障代码，汽车制造商使用这些代码的布尔组合（错误模式）来表征系统故障。然而，这些规则目前仍由领域专家手工制定，随着车辆复杂性增加，这一过程成本高昂且容易出错。

Method: CAREP采用多智能体架构：1)因果发现智能体识别DTC-EP潜在关系；2)上下文信息智能体整合元数据和描述；3)编排智能体合成候选布尔规则并提供可解释的推理轨迹。

Result: 在包含29,100个独特DTCs和474个错误模式的大规模汽车数据集上评估，CAREP能够自动准确地发现未知的EP规则，优于仅使用LLM的基线方法，同时提供透明的因果解释。

Conclusion: CAREP结合实用因果发现和基于智能体的推理，代表了向全自动故障诊断迈出的一步，实现了可扩展、可解释且成本效益高的车辆维护。

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [86] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

TL;DR: 研究发现预训练视觉语言模型（VLM）中存在任务干扰层，这些层会损害下游任务性能。通过层干预分析，提出无需训练的动态层剔除方法TaLo，在推理时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLM通常默认使用所有层进行下游任务预测，但研究发现某些层反而会阻碍特定任务的性能。需要系统研究各层对任务的影响，并开发无需训练的方法来规避干扰层。

Method: 1. 通过层干预实验（如参数归零）测量各层对任务性能的影响；2. 提出任务-层交互向量量化层干预效果；3. 开发TaLo方法：动态识别并绕过最干扰的层，无需参数更新。

Result: 发现任务干扰层普遍存在且可跨模型和数据集泛化；TaLo方法显著提升性能，如在ScienceQA的Maps任务上Qwen-VL准确率提升16.6%；任务-层交互向量显示相似能力的任务具有一致的层敏感性模式。

Conclusion: 预训练VLM中存在意外形式的模块化结构，任务干扰层会损害下游性能。TaLo作为即插即用的无训练方法，能在推理时解锁模型的隐藏能力，为VLM优化提供新思路。

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [87] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

TL;DR: ASP-Bench是一个包含128个自然语言问题实例的基准测试，用于评估将自然语言问题转换为答案集程序（ASP）的系统，覆盖了ASP的各种特性，并通过基于ReAct框架的智能体方法展示了反馈驱动的迭代优化效果。


<details>
  <summary>Details</summary>
Motivation: 将自然语言规范自动转换为逻辑程序是神经符号工程中的一个挑战性任务。当前缺乏系统评估这种翻译能力的基准测试，特别是针对答案集编程（ASP）这种重要逻辑编程形式。

Method: 创建ASP-Bench基准测试，包含128个自然语言问题实例（64个基础问题及其简单和困难变体）。每个问题都包含参考验证器来检查解决方案是否符合规范。使用基于ReAct框架的智能体方法进行测试，通过反馈驱动的迭代优化来建模自然语言问题。

Result: 基准测试系统覆盖了ASP的各种特性（选择规则、聚合、优化等）。通过七个独立的推理维度（优化、时序推理、默认逻辑、资源分配、递归、空间推理和定量复杂性）来表征问题难度。基于ReAct的智能体方法实现了完全饱和，表明反馈驱动的迭代优化是建模自然语言问题的可靠方法。

Conclusion: ASP-Bench为评估自然语言到ASP的翻译系统提供了全面的基准测试。基于ReAct的反馈驱动方法被证明是有效的，并且通过多维度分析可以深入理解问题建模难度的决定因素。

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [88] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

TL;DR: 提出基于线性注意力机制的高效推理框架，将LLM推理过程建模为状态转移过程，降低计算复杂度，提升推理效率和性能


<details>
  <summary>Details</summary>
Motivation: 长思维链推理虽然能提升LLM在复杂任务上的性能，但生成长序列带来的计算和内存成本限制了其效率和实用性。现有方法通过压缩思维链序列来提高效率，但这与测试时扩展相冲突，限制了LLM的推理能力。

Method: 提出高效推理框架，将LLM推理过程建模为状态转移过程：1) 使用线性注意力机制估计LLM的推理状态，记录历史推理信息；2) 基于查询提示和推理状态，LLM高效执行当前推理步骤并更新状态；3) 通过线性注意力，当前推理步骤中的每个token可以直接从推理状态检索相关信息，无需显式关注先前步骤的token；4) 提出基于状态的推理策略缓解噪声推理步骤导致的过度思考问题。

Result: 在多个数据集和模型规模上的广泛实验表明，该框架不仅提高了LLM的推理效率（将注意力计算复杂度从二次降低到线性），还增强了推理性能。

Conclusion: 提出的高效推理框架通过将推理过程建模为状态转移过程，使用线性注意力机制显著降低了计算复杂度，同时通过状态管理策略缓解了过度思考问题，实现了推理效率和性能的双重提升。

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [89] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: Workflow-R1将工作流构建重新定义为基于自然语言的多轮顺序决策过程，引入GSsPO算法解决优化粒度不匹配问题，在多个QA基准测试中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作流优化方法将工作流合成视为静态、一次性的代码生成问题，这过度约束了模型的编码能力，限制了动态问题解决的灵活性。

Method: 提出Workflow-R1框架，将工作流构建重新定义为多轮自然语言顺序决策过程；引入Group Sub-sequence Policy Optimization (GSsPO)算法，将优化单元重新校准为复合子序列（特别是原子Think-Action循环），使梯度更新与交互的语义边界对齐。

Result: 在多个QA基准测试中，Workflow-R1优于竞争基线，验证了GSsPO作为顺序推理通用解决方案的有效性。

Conclusion: GSsPO是一种结构感知的强化学习算法，可推广到广泛的多轮代理顺序决策任务；Workflow-R1为自动化工作流优化提供了一个有前景的新范式。

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [90] [Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)](https://arxiv.org/abs/2602.01206)
*Zeinab Dehghani*

Main category: cs.AI

TL;DR: gSMILE是一个统一的生成模型可解释性框架，通过文本扰动、Wasserstein距离和加权代理建模来量化提示组件对输出的影响，为LLMs提供细粒度token归因，为图像编辑模型分析指令修改的影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型虽然能产生复杂输出，但其决策过程不透明，限制了在高风险应用中的信任和问责。需要开发能够解释生成模型决策过程的框架。

Method: 扩展SMILE方法到生成式设置，使用文本输入受控扰动、Wasserstein距离度量和加权代理建模。为LLMs提供token级归因和热力图，为图像编辑模型分析指令修改影响。结合基于ODD框架的场景评估策略。

Result: gSMILE产生稳健、与人类对齐的归因，并能有效泛化到最先进的生成模型。定义了稳定性、保真度、准确性、一致性和忠实度等归因指标来评估解释质量。

Conclusion: gSMILE有潜力推进生成式AI技术的透明、可靠和负责任部署，为生成模型提供统一的可解释性框架。

Abstract: The rapid advancement of generative artificial intelligence has enabled models capable of producing complex textual and visual outputs; however, their decision-making processes remain largely opaque, limiting trust and accountability in high-stakes applications. This thesis introduces gSMILE, a unified framework for the explainability of generative models, extending the Statistical Model-agnostic Interpretability with Local Explanations (SMILE) method to generative settings. gSMILE employs controlled perturbations of textual input, Wasserstein distance metrics, and weighted surrogate modelling to quantify and visualise how specific components of a prompt or instruction influence model outputs. Applied to Large Language Models (LLMs), gSMILE provides fine-grained token-level attribution and generates intuitive heatmaps that highlight influential tokens and reasoning pathways. In instruction-based image editing models, the exact text-perturbation mechanism is employed, allowing for the analysis of how modifications to an editing instruction impact the resulting image. Combined with a scenario-based evaluation strategy grounded in the Operational Design Domain (ODD) framework, gSMILE allows systematic assessment of model behaviour across diverse semantic and environmental conditions. To evaluate explanation quality, we define rigorous attribution metrics, including stability, fidelity, accuracy, consistency, and faithfulness, and apply them across multiple generative architectures. Extensive experiments demonstrate that gSMILE produces robust, human-aligned attributions and generalises effectively across state-of-the-art generative models. These findings highlight the potential of gSMILE to advance transparent, reliable, and responsible deployment of generative AI technologies.

</details>


### [91] [Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models](https://arxiv.org/abs/2602.01207)
*Hui Wu,Hengyi Cai,Jinman Zhao,Xinran Chen,Ziheng Li,Zhejun Zhao,Shuaiqiang Wang,Yuchen Li,Dawei Yin*

Main category: cs.AI

TL;DR: SAGE是一个动态偏好对齐框架，通过最大化策略更新的信噪比来提升对齐可靠性，相比静态方法能显著加速收敛并取得更好性能。


<details>
  <summary>Details</summary>
Motivation: 标准偏好对齐方法（如DPO）将所有偏好对同等对待，忽略了训练实例的演化效用。这种静态方法导致优化效率低下或不稳定：浪费计算资源在梯度可忽略的平凡对上，同时受到决策边界附近样本噪声的影响。

Method: SAGE整合了粗粒度课程机制（基于模型能力刷新候选池）和细粒度稳定性感知评分函数（优先选择信息丰富、置信度高的错误，同时过滤不稳定样本），旨在最大化策略更新的信噪比。

Result: 在多个数学推理基准测试中，SAGE显著加速了收敛速度，并超越了静态基线方法，证明了策略感知、稳定性意识的数据选择在推理对齐中的关键作用。

Conclusion: SAGE框架通过动态、稳定性感知的数据选择机制，有效解决了静态偏好对齐方法的局限性，为推理模型的对齐训练提供了更高效可靠的解决方案。

Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.

</details>


### [92] [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)
*Shaoxiong Yang,Junting Li,Mengyuan Zhang,Chao Li,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: FutureMind是一个模块化推理框架，通过从大语言模型自适应知识蒸馏，为小语言模型提供战略思维模式先验，提升其在复杂知识密集型任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在成本敏感和资源受限场景中具有吸引力，但它们在需要结构化推理和有效检索的复杂知识密集型任务上表现不佳。需要一种方法让小语言模型既能保持高效推理，又能处理复杂任务。

Method: 提出FutureMind框架，包含四个关键模块：问题分析、逻辑推理、策略规划和检索指导。采用三种不同的检索范式将复杂查询分解为可处理的子问题。通过从大语言模型自适应知识蒸馏，为小语言模型注入战略思维模式先验。

Result: 在2WikiMultihopQA、MuSiQue、Bamboogle和Frames等多跳问答基准测试中，FutureMind始终优于Search-o1等强基线，在不同架构和规模的小语言模型上实现了免费训练条件下的最先进结果。

Conclusion: FutureMind成功提升了小语言模型在复杂任务中的表现，但研究发现思维模式蒸馏过程受到教师模型（大语言模型）和学生模型（小语言模型）之间认知偏差瓶颈的限制。这为推理技能的可迁移性提供了新视角，为开发兼具效率和真正认知能力的小语言模型铺平了道路。

Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.

</details>


### [93] [Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models](https://arxiv.org/abs/2602.01237)
*Katrina Brown,Aneesh Muppidi,Rana Shahout*

Main category: cs.AI

TL;DR: 论文提出Predictive Scheduling框架，通过轻量级预测器在生成前预估查询的最优推理长度，动态分配固定token预算以最大化准确率，在GSM8K上实现7.9%准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在复杂推理任务中使用固定token预算会导致简单问题过度计算、困难问题计算不足的问题，需要更精细的计算-准确率权衡控制。

Method: 提出Predictive Scheduling框架：1) 使用轻量级预测器（基于中间隐藏状态的MLP或基于原始文本的LoRA微调分类器）在生成前预估查询的最优推理长度或难度；2) 贪心批量分配器动态分配固定总token预算以最大化预期准确率。

Result: 在GSM8K算术基准测试中，相比均匀预算分配，预测调度在相同token成本下获得高达7.9个百分点的绝对准确率提升，缩小了与完美预知oracle之间50%以上的差距。系统层研究表明transformer中间层（12-17层）携带最丰富的规模估计信号。

Conclusion: 预运行预算预测能够实现计算-准确率权衡的精细控制，为延迟敏感、成本高效的LLM部署提供了具体路径。

Abstract: Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.

</details>


### [94] [LLM-Driven Ontology Construction for Enterprise Knowledge Graphs](https://arxiv.org/abs/2602.01276)
*Abdulsobur Oyewale,Tommaso Soru*

Main category: cs.AI

TL;DR: OntoEKG：基于LLM的自动化流程，从非结构化企业数据生成领域本体，加速企业知识图谱构建


<details>
  <summary>Details</summary>
Motivation: 企业知识图谱构建中，底层本体创建仍依赖人工和领域专家，过程资源密集且耗时，需要自动化解决方案来加速这一过程

Method: 提出两阶段LLM驱动流程：1)提取模块识别核心类和属性；2)蕴含模块将元素逻辑结构化形成层次结构，最后序列化为标准RDF格式

Result: 在Data领域获得模糊匹配F1分数0.724，同时在范围定义和层次推理方面存在局限性，展示了该方法的潜力和挑战

Conclusion: OntoEKG展示了LLM驱动本体生成的可行性，为自动化企业知识图谱构建提供了有前景的方向，但仍需改进范围定义和层次推理能力

Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.

</details>


### [95] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: RE-MCDF是一个关系增强的多专家临床诊断框架，通过生成-验证-修订的闭环架构，结合医学知识图谱和多关系约束，解决神经科电子病历诊断中的异构性、稀疏性和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 神经科电子病历具有异构性、稀疏性和噪声，传统单智能体系统容易产生自我强化的错误，现有多智能体框架交互浅层且缺乏结构，忽略了疾病间的丰富逻辑依赖关系（如互斥性、病理兼容性、诊断混淆），无法排除临床不可行的假设。

Method: 提出RE-MCDF框架，包含三个互补组件：1) 生成候选诊断和支持证据的主要专家；2) 动态优先处理异构临床指标的实验室专家；3) 强制执行疾病间逻辑约束的多关系感知与评估专家组。基于医学知识图谱，前两个专家自适应重新加权电子病历证据，专家组验证和修正候选诊断以确保逻辑一致性。

Result: 在CMEMR的神经学子集(NEEMRs)和自建数据集(XMEMRs)上的广泛实验表明，RE-MCDF在复杂诊断场景中持续优于最先进的基线方法。

Conclusion: RE-MCDF通过整合多专家协作和显式关系约束，有效解决了神经科电子病历诊断中的挑战，提高了诊断的准确性和逻辑一致性。

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [96] [Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance](https://arxiv.org/abs/2602.01346)
*Wei Yang,Hong Xie,Tao Tan,Xin Li,Defu Lian,Enhong Chen*

Main category: cs.AI

TL;DR: 提出基于视觉编码器内部功能动态的模型选择框架，通过方向性电导散度（DCD）度量源任务对目标任务功能块的覆盖效果，无需直接推理即可预测模型排名。


<details>
  <summary>Details</summary>
Motivation: 现有开源视觉语言模型（VLM）众多，但为特定下游任务选择最优预训练模型具有挑战性。穷举评估因计算限制和少样本场景数据不足而不可行。现有方法要么依赖数据密集型代理，要么使用对称文本描述符，忽略了迁移性的方向性和模型特定性。

Method: 提出基于视觉编码器内部功能动态的框架：1）通过层间电导表示每个任务；2）通过熵正则化对齐得到目标条件块重要性分布；3）引入方向性电导散度（DCD），这是一个非对称度量，量化源任务对目标任务显著功能块的覆盖效果；4）通过聚合源任务排名预测目标模型排名，无需直接推理。

Result: 在48个VLM和21个数据集上的实验表明，该方法优于现有最先进基线，在NDCG@5指标上比SWAB提高了14.7%。

Conclusion: 该方法通过建模视觉编码器的内部功能动态，有效解决了VLM模型选择问题，特别是在数据有限的情况下，为模型选择提供了更准确的方向性度量。

Abstract: While open sourced Vision-Language Models (VLMs) have proliferated, selecting the optimal pretrained model for a specific downstream task remains challenging. Exhaustive evaluation is often infeasible due to computational constraints and data limitations in few shot scenarios. Existing selection methods fail to fully address this: they either rely on data-intensive proxies or use symmetric textual descriptors that neglect the inherently directional and model-specific nature of transferability. To address this problem, we propose a framework that grounds model selection in the internal functional dynamics of the visual encoder. Our approach represents each task via layer wise conductance and derives a target-conditioned block importance distribution through entropy regularized alignment. Building on this, we introduce Directional Conductance Divergence (DCD), an asymmetric metric that quantifies how effectively a source task covers the target's salient functional blocks. This allows for predicting target model rankings by aggregating source task ranks without direct inference. Experimental results on 48 VLMs across 21 datasets demonstrate that our method outperforms state-of-the-art baselines, achieving a 14.7% improvement in NDCG@5 over SWAB.

</details>


### [97] [Aggregation Queries over Unstructured Text: Benchmark and Agentic Method](https://arxiv.org/abs/2602.01355)
*Haojia Zhu,Qinyuan Xu,Haoyu Li,Yuxi Liu,Hanchen Qiu,Jiaoyan Chen,Jiahui Jin*

Main category: cs.AI

TL;DR: 该论文提出了一种面向完整性的实体级聚合查询方法，并引入了AGGBench基准测试和DFA模块化基线系统来解决文本聚合查询中的证据收集完整性问题。


<details>
  <summary>Details</summary>
Motivation: 文本上的聚合查询是一个长期存在但未被充分探索的问题。与普通问答不同，聚合查询需要完整的证据收集，系统必须"找到所有"而不仅仅是"找到一个"。现有的Text-to-SQL和检索增强生成方法无法实现这种完整性要求。

Method: 提出了DFA（消歧-过滤-聚合）方法，这是一种模块化的智能基线系统，将聚合查询分解为可解释的阶段：消歧、过滤和聚合，并暴露与模糊性、过滤和聚合相关的关键失败模式。

Result: 实验结果表明，DFA在聚合证据覆盖率方面持续优于强大的RAG和智能基线方法。在AGGBench基准测试上验证了方法的有效性。

Conclusion: 该工作形式化了具有严格完整性要求的语料库约束下的实体级聚合查询，并提供了AGGBench基准测试和DFA基线系统，为解决文本聚合查询的完整性挑战提供了重要工具。

Abstract: Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to "find all," not merely "find one." Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.

</details>


### [98] [Building Better Deception Probes Using Targeted Instruction Pairs](https://arxiv.org/abs/2602.01425)
*Vikram Natarajan,Devina Jain,Shivam Arora,Satvik Golechha,Joseph Bloom*

Main category: cs.AI

TL;DR: 线性探针检测AI欺骗行为时，指令对选择比数据集更重要，应针对特定欺骗类型设计专用探针而非通用检测器


<details>
  <summary>Details</summary>
Motivation: 现有线性探针在检测AI系统欺骗行为时存在明显缺陷，包括虚假相关性和对非欺骗响应的误报，需要改进检测方法

Method: 通过人类可解释的欺骗分类法，针对特定欺骗行为设计指令对，分析指令对如何捕捉欺骗意图而非内容特定模式

Result: 指令对选择解释了探针性能70.6%的方差，基于分类法的针对性方法在评估数据集上表现更好，证明欺骗意图检测优于内容模式匹配

Conclusion: 组织应针对特定威胁模型设计专门的欺骗检测探针，而非寻求通用欺骗检测器，因为不同数据集的欺骗类型具有异质性

Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.

</details>


### [99] [SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce](https://arxiv.org/abs/2602.01443)
*Alberto Castelo,Zahra Zanjani Foumani,Ailin Fan,Keat Yang Koay,Vibhor Malik,Yuanzheng Zhu,Han Li,Meysam Feghhi,Ronie Uliana,Shuang Xie,Zhaoyu Zhang,Angelo Ocana Martins,Mingyu Zhao,Francis Pelland,Jonathan Faerman,Nikolas LeBlanc,Aaron Glazer,Andrew McNamara,Lingyun Wang,Zhong Wu*

Main category: cs.AI

TL;DR: SimGym是一个基于LLM代理的离线A/B测试系统，通过合成买家模拟替代传统在线A/B测试，将实验周期从数周缩短至1小时内


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试存在流量分流、周期长（数周）、可能损害用户体验等问题，需要一种快速、安全的离线测试方法

Method: 从生产交互数据中提取买家画像和意图，识别行为原型，使用LLM代理在实时浏览器中模拟加权队列会话，在控制组和实验组店铺进行测试

Result: 即使没有对齐后训练，SimGym代理也能与观察到的结果变化达到最先进的对齐度，将实验周期从数周缩短至1小时以内

Conclusion: SimGym实现了无需接触真实买家的快速实验，为电商UI变更评估提供了可扩展的离线A/B测试解决方案

Abstract: A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Language Model agents operating in a live browser. SimGym extracts per-shop buyer profiles and intents from production interaction data, identifies distinct behavioral archetypes, and simulates cohort-weighted sessions across control and treatment storefronts. We validate SimGym against real human outcomes from real UI changes on a major e-commerce platform under confounder control. Even without alignment post training, SimGym agents achieve state of the art alignment with observed outcome shifts and reduces experiment cycles from weeks to under an hour , enabling rapid experimentation without exposure to real buyers.

</details>


### [100] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: 提出一个完全自动化的多智能体系统，将软件工程建模为组织化流程，模拟真实工程团队结构，在SWE-bench 500上达到72.4%的任务解决率。


<details>
  <summary>Details</summary>
Motivation: 当前大多数自主系统将问题解决视为单一或流水线过程，而真实软件开发是团队协作活动，具有明确的角色分工、沟通和评审。需要模拟真实工程团队的组织结构来提升自主软件工程能力。

Method: 基于agyn开源平台构建多智能体系统，分配协调、研究、实现、评审等专门角色，提供隔离沙箱进行实验，支持结构化通信。系统遵循定义好的开发方法论，包括分析、任务规范、PR创建和迭代评审，完全无需人工干预。

Result: 在SWE-bench 500上评估，系统解决了72.4%的任务，优于使用可比语言模型的单智能体基线。系统专为实际生产使用设计，未针对SWE-bench进行调优。

Conclusion: 模拟团队结构、方法论和沟通是自主软件工程的强大范式，未来进展可能同等依赖于组织设计和智能体基础设施，而不仅仅是模型改进。

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [101] [Legal Infrastructure for Transformative AI Governance](https://arxiv.org/abs/2602.01474)
*Gillian K. Hadfield*

Main category: cs.AI

TL;DR: 本文主张AI治理不仅需要实质规则，更需要建立法律和监管基础设施，提出了三个具体框架：前沿模型注册、自主代理注册识别、以及监管市场设计。


<details>
  <summary>Details</summary>
Motivation: 当前AI治理主要关注实质规则（如限制和检查），但忽视了法律和监管基础设施的建设。AI的变革性特征特别需要关注法律和监管框架的构建，以确保规则的有效生成和实施。

Method: 作者回顾并提出了三个具体建议：1）建立前沿模型注册制度；2）建立自主代理注册和识别制度；3）设计监管市场，让私营公司能够创新并提供AI监管服务。

Result: 提出了三个具体的法律和监管基础设施框架，为AI治理提供了结构化的解决方案，强调基础设施在有效实施AI规则中的关键作用。

Conclusion: AI治理需要超越实质规则，重视法律和监管基础设施的建设。通过注册制度和监管市场等机制，可以建立更有效、适应性更强的AI治理体系。

Abstract: Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.

</details>


### [102] [Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models](https://arxiv.org/abs/2602.01475)
*Brij Malhotra,Shivvrat Arya,Tahrima Rahman,Vibhav Giridhar Gogate*

Main category: cs.AI

TL;DR: 提出神经摊销框架改进概率图模型中的重复查询MPE推理，通过注意力网络预测移动减少汉明距离的能力，平衡短期似然增益与长期潜力


<details>
  <summary>Details</summary>
Motivation: 在概率图模型中，MPE推理是基础但计算困难的问题。实际应用中，图模型固定但需要针对不同证据模式重复推理。传统随机局部搜索算法依赖短视的最佳改进规则，容易陷入局部最优，而现有启发式方法如GLS+的指导无法在相同模型的多次推理查询中有效复用。

Method: 提出神经摊销框架，利用固定图结构训练基于注意力的网络，通过预测局部移动减少到近最优解汉明距离的能力来评分移动。该方法与现有局部搜索过程无缝集成，在邻居选择时平衡短期似然增益与长期潜力。

Result: 提供了理论直觉，将距离减少移动选择与改进的收敛行为联系起来。在摊销推理设置中，在具有挑战性的高树宽基准测试上，相比SLS和GLS+方法，实证展示了持续改进。

Conclusion: 提出的神经摊销框架有效改进了概率图模型中重复查询场景下的局部搜索性能，通过预测移动的长期潜力来避免陷入局部最优，为固定图结构的多次推理查询提供了有效的解决方案。

Abstract: Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be performed repeatedly for varying evidence patterns. Stochastic Local Search (SLS) algorithms scale to large models but rely on myopic best-improvement rule that prioritizes immediate likelihood gains and often stagnate in poor local optima. Heuristics such as Guided Local Search (GLS+) partially alleviate this limitation by modifying the search landscape, but their guidance cannot be reused effectively across multiple inference queries on the same model. We propose a neural amortization framework for improving local search in this repeated-query regime. Exploiting the fixed graph structure, we train an attention-based network to score local moves by predicting their ability to reduce Hamming distance to a near-optimal solution. Our approach integrates seamlessly with existing local search procedures, using this signal to balance short-term likelihood gains with long-term promise during neighbor selection. We provide theoretical intuition linking distance-reducing move selection to improved convergence behavior, and empirically demonstrate consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in the amortized inference setting.

</details>


### [103] [Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection](https://arxiv.org/abs/2602.01518)
*Jongseok Park,Sunga Kim,Alvin Cheung,Ion Stoica*

Main category: cs.AI

TL;DR: Qrita是一种基于枢轴选择策略的高效Top-k和Top-p算法，相比传统排序方法在GPU上实现2倍吞吐量和一半内存使用，同时保持确定性输出。


<details>
  <summary>Details</summary>
Motivation: Top-k和Top-p是大语言模型采样的主要截断操作符，但在大规模词汇表上高效实现仍然是一个重大挑战。现有方法要么依赖排序（带来显著计算和内存开销），要么使用随机方法（改变算法输出）。

Method: 基于RTop-k的枢轴搜索思想，扩展到Top-k和Top-p，采用两种关键技术：1) 基于高斯分布的sigma截断，大幅减少目标元素的搜索空间；2) 四元枢轴搜索与重复处理，将枢轴搜索迭代减半并保证确定性输出。使用Triton GPU编程语言实现。

Result: 与vLLM、SGLang和Flashinfer等高性能LLM执行引擎的Top-k和Top-p内核相比，Qrita实现了高达2倍的吞吐量和一半的内存使用，同时提供与排序算法相同的输出。

Conclusion: Qrita提供了一种高效、确定性的Top-k和Top-p算法实现，解决了现有方法在GPU上的计算和内存开销问题，同时保持算法输出的准确性。

Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.

</details>


### [104] [PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents](https://arxiv.org/abs/2602.01532)
*Yuxuan Fu,Xiaoyu Tan,Teqi Hao,Chen Zhan,Xihe Qiu*

Main category: cs.AI

TL;DR: PRISM框架通过决策理论门控和双过程推理架构，实现成本敏感的选择性干预，在主动式对话代理中平衡帮助收益与打扰负担的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前主动式代理系统依赖脆弱的启发式方法或无差别长推理，对帮助收益与打扰负担的权衡缺乏可控性，需要更精确的干预决策机制。

Method: PRISM框架结合决策理论门控与双过程推理架构：使用成本推导的阈值进行门控，仅在用户接受概率超过阈值时干预；采用"快速-慢速"推理模式，在决策边界附近调用资源密集的慢速模式进行反事实检查；通过门控对齐、模式锁定的蒸馏训练方法，教师模型提供密集可执行监督，学生模型学习与干预门控解耦的响应策略。

Result: 在ProactiveBench基准测试中，PRISM将误报率降低22.78%，F1分数提高20.14%，优于现有基线方法。

Conclusion: 基于决策理论的门控机制，结合选择性慢速推理和对齐蒸馏，能够构建精确、计算高效且可控的主动式代理系统。

Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: "make haste slowly"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.

</details>


### [105] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: MAGIC框架通过多轮多智能体强化学习，将LLM安全对齐建模为对抗性非对称博弈，攻击者和防御者协同进化，实现动态安全防御


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法依赖静态预收集数据分布，难以应对不断演化的对抗攻击，需要动态、自适应的安全对齐机制

Method: 提出MAGIC多轮多智能体强化学习框架：攻击者智能体学习迭代重写查询为欺骗性提示，防御者智能体同时优化策略识别并拒绝此类输入，形成协同进化过程

Result: 攻击者演化出新颖的组合策略，防御者泛化到未见攻击模式，实现更高的防御成功率且不损害模型的有用性，理论分析提供了更鲁棒的博弈均衡和安全保证

Conclusion: MAGIC框架通过动态对抗训练实现了更鲁棒的LLM安全对齐，为应对不断演化的对抗攻击提供了有效解决方案

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [106] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

TL;DR: S1-NexusAgent是一个自进化的多学科科学智能体框架，采用分层规划-代码执行范式，通过双循环架构解耦全局科学规划和子任务工具执行，支持大规模跨学科科学工具集成，在多个科学基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM和基于工具的智能体在处理大规模数据、复杂工作流和专用工具方面存在局限，特别是在长时程规划、鲁棒目标维持和持续学习方面。科学研究的复杂性需要更强大的智能体框架。

Method: 采用分层Plan-and-CodeAct执行范式，双循环架构分离全局规划和子任务执行。支持MCP协议，集成数千个跨学科科学工具，通过意图感知动态工具检索和热插拔机制实现异构工具编排。引入基于对象引用的稀疏上下文管理解决长上下文和大规模数据问题。通过Critic Agent自动评估执行轨迹，提炼高质量研究路径为可重用科学技能。

Result: 在权威科学基准测试（biomini-eval、ChemBench、MatSciBench）上达到最先进性能，验证了在复杂科学任务中的有效性和泛化能力，特别是在长时程规划和复杂专用工具编排方面。

Conclusion: S1-NexusAgent通过自进化框架有效解决了科学研究的复杂性挑战，为可持续和长时程科学研究提供了有价值的解决方案，展示了在多学科科学任务中的强大能力。

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [107] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: 提出基于人类模拟的框架，让AI系统能够通过推理内部状态、环境观察和与其他AI系统的交互来自主形成问题和设定任务，将问题形成作为任务选择和执行的先决决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的AI系统大多依赖预定义任务和固定提示，限制了它们在环境条件变化时自主识别应解决问题的能力。需要让AI系统能够自主形成问题和设定任务。

Method: 提出人类模拟框架，将问题形成作为首要决策过程，整合内部驱动、环境感知和智能体间感知三种提示范围来逐步扩展认知覆盖。框架支持从经验中学习问题形成过程。

Result: 在多智能体模拟环境中，环境感知提示相比内部驱动基线显著减少了无进食事件，智能体间感知提示在20天模拟中将累计无进食事件进一步减少60%以上，具有统计显著性(p<0.05)。

Conclusion: 该框架使AI系统能够自主形成问题和设定任务，通过整合多层次认知范围显著提高了在动态开放环境中的适应性和决策质量。

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [108] [Reasoning with Autoregressive-Diffusion Collaborative Thoughts](https://arxiv.org/abs/2602.01608)
*Mu Yuan,Liekang Zeng,Guoliang Xing,Lan Zhang,Yunhao Liu*

Main category: cs.AI

TL;DR: 提出Collaborative Thoughts框架，通过自回归模型和扩散模型的闭环协作，结合结构化规划与视觉生成，提升空间推理可靠性和生成可控性。


<details>
  <summary>Details</summary>
Motivation: 自回归模型擅长序列规划和约束组合，但缺乏空间物理基础；扩散模型能捕捉丰富空间结构，但缺乏逐步逻辑控制来满足复杂多阶段约束或纠正错误。需要结合两者优势。

Method: Collaborative Thoughts框架：自回归模型负责结构化规划和约束管理，扩散模型将约束实例化为中间视觉思维，视觉批评模块评估视觉思维是否满足结构和物理要求，反馈用于迭代优化后续规划和生成步骤。

Result: 通过代表性示例展示了Collaborative Thoughts能够提高空间推理的可靠性和生成的可控性，减轻跨模态错误传播。

Conclusion: Collaborative Thoughts为自回归问答和基于扩散的视觉生成提供了统一的协作框架，通过模态间闭环交互实现了更好的规划与生成协同。

Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.

</details>


### [109] [ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning](https://arxiv.org/abs/2602.01610)
*Zitao Guo,Changyang Jiang,Tianhong Zhao,Jinzhou Cao,Genan Dai,Bowen Zhang*

Main category: cs.AI

TL;DR: ToPT：一个两阶段框架，通过空间感知的区域嵌入学习和任务感知提示，解决城市区域表示中空间不一致和任务语义对齐不足的问题，在多个任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个主要问题：1）两阶段方法产生任务无关表示，与下游目标脱节；2）基于提示的方法缺乏明确的空间先验（导致空间不一致）和鲁棒的任务语义对齐机制。

Method: ToPT包含两个模块：SREL（空间感知区域嵌入学习）使用Graphormer融合模块，注入距离和区域中心性作为可学习注意力偏置；Prompt4RE（任务感知提示）使用冻结的多模态大语言模型处理任务特定模板，通过多头交叉注意力将语义向量与区域嵌入对齐。

Result: 在多个任务和城市的实验中取得最先进性能，改进幅度高达64.2%，验证了空间先验和提示-区域对齐的必要性和互补性。

Conclusion: ToPT通过空间一致融合和显式任务对齐，有效解决了城市区域表示中的关键挑战，为城市计算任务提供了更有效的区域嵌入学习方法。

Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.

</details>


### [110] [ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development](https://arxiv.org/abs/2602.01655)
*Pengrui Lu,Shiqi Zhang,Yunzhong Hou,Lyumanshan Ye,Chaoyi Huang,Zixi Chen,Ji Zeng,Hantao Jiang,Pengfei Liu,Yiwei Wang,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ProjDevBench是一个端到端编码代理评估基准，通过项目需求评估完整代码库生成能力，结合在线评测和LLM辅助代码审查，在20个编程问题上测试6个编码代理，总体接受率27.38%


<details>
  <summary>Details</summary>
Motivation: 现有编码代理评估主要关注问题级别的bug修复，缺乏对端到端开发能力的评估。需要一个新的基准来评估编码代理从简单提示生成完整代码库的能力，特别是在系统架构设计、功能正确性和迭代解决方案优化方面的表现。

Method: 提出ProjDevBench基准，结合在线评测(Online Judge)测试和LLM辅助代码审查。基准包含20个编程问题，涵盖8个类别，包括概念导向任务和真实应用场景。评估六个基于不同LLM后端的编码代理，从系统架构设计、功能正确性和迭代解决方案优化三个维度进行评估。

Result: 总体接受率为27.38%。编码代理能够处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面表现不佳。基准已开源在GitHub上。

Conclusion: ProjDevBench填补了编码代理端到端评估的空白，揭示了当前编码代理在复杂系统设计和优化方面的局限性，为未来编码代理开发提供了重要的评估工具。

Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.

</details>


### [111] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

TL;DR: FlowSteer：基于强化学习的端到端工作流编排框架，通过多轮交互自动优化工作流，支持多种算子库和可互换的LLM后端


<details>
  <summary>Details</summary>
Motivation: 现有工作流编排面临高人工成本、依赖特定算子/大语言模型、奖励信号稀疏等关键挑战，需要自动化解决方案

Method: 提出FlowSteer框架，使用轻量级策略模型作为代理，在可执行画布环境中通过多轮交互实现工作流编排。策略模型分析执行状态并选择编辑动作，画布执行算子并返回反馈进行迭代优化。采用Canvas Workflow Relative Policy Optimization (CWRPO)训练方法，引入多样性约束奖励和条件释放机制来稳定学习并抑制捷径行为

Result: 在12个数据集上的实验结果表明，FlowSteer在各种任务上显著优于基线方法

Conclusion: FlowSteer提供了一个即插即用的框架，支持多样化的算子库和可互换的LLM后端，有效解决了工作流编排的自动化挑战

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [112] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: TRIP-Bench是一个基于真实旅行规划场景的长时程基准测试，包含18个工具和40+旅行需求，支持自动评估；GTPO是一种在线多轮强化学习方法，能提升约束满足和交互鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分代表真实世界中的关键挑战，如强制执行全局约束、协调多工具推理以及适应长期多轮交互中不断变化的用户行为。

Method: 1) 引入TRIP-Bench基准：基于真实世界数据，包含18个精选工具和40+旅行需求，支持自动评估，具有不同难度划分；2) 提出GTPO方法：在线多轮强化学习方法，采用专门的奖励归一化和奖励差分技术。

Result: 1) 实验显示即使先进模型在简单划分上最多只能达到50%的成功率，在困难子集上性能降至10%以下；2) GTPO应用于Qwen2.5-32B-Instruct后，在约束满足和交互鲁棒性方面优于Gemini-3-Pro。

Conclusion: TRIP-Bench有望推动实用的长时程交互智能体发展，GTPO为鲁棒的长时程训练提供了有效的在线强化学习方案。

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [113] [What LLMs Think When You Don't Tell Them What to Think About?](https://arxiv.org/abs/2602.01689)
*Yongchan Kwon,James Zou*

Main category: cs.AI

TL;DR: 研究LLMs在无主题约束下的生成行为，发现不同模型家族有强烈的系统性主题偏好和独特退化模式


<details>
  <summary>Details</summary>
Motivation: 现有LLM分析大多依赖特定主题或任务的提示，限制了观察范围。需要研究LLMs在最小化、主题中性输入下的生成行为，以更全面地理解模型特性

Method: 使用最小化、主题中性的输入提示LLMs，收集16个模型的256,000个样本，分析其无约束生成行为中的主题偏好、内容深度和退化模式

Result: 不同模型家族表现出强烈的系统性主题偏好：GPT-OSS偏好编程和数学内容，Llama偏好文学内容，DeepSeek偏好宗教内容，Qwen偏好多项选择题。还观察到内容深度差异和独特的退化模式

Conclusion: LLMs即使在无主题约束下也表现出强烈的内在偏好，这些发现对模型监控、AI安全和理解模型行为有重要意义，并提供了可复现的数据集和代码库

Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.

</details>


### [114] [Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning](https://arxiv.org/abs/2602.01695)
*Yadong Wang,Haodong Chen,Yu Tian,Chuanxing Geng,Dong Liang,Xiang Chen*

Main category: cs.AI

TL;DR: LSTR提出了一种稀疏潜在推理框架，将稀疏编码器提升为主动推理算子，通过稀疏语义转换进行多步计算，在保持准确性和压缩效率的同时显著提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法依赖密集潜在转换，难以解释和控制；而稀疏表示模型虽然能发现人类可解释的语义特征，但主要局限于事后分析。需要解决这种张力，实现既高效又可解释的推理。

Method: 提出LSTR框架，使用具有残差跳跃架构的潜在转换编码器（LTT），将线性流形传输与稀疏语义更新解耦，通过显式稀疏约束实现可控语义分辨率。

Result: 实验表明LSTR在保持推理准确性和压缩效率的同时，相比密集潜在基线显著提高了可解释性。因果干预和轨迹分析进一步证明这些稀疏特征在推理过程中既是可解释的又是因果有效的算子。

Conclusion: LSTR成功地将稀疏表示与主动推理相结合，通过稀疏语义转换实现了高效、准确且高度可解释的潜在推理，为可解释AI推理提供了新方向。

Abstract: Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.

</details>


### [115] [Mitigating loss of control in advanced AI systems through instrumental goal trajectories](https://arxiv.org/abs/2602.01699)
*Willem Fourie*

Main category: cs.AI

TL;DR: 该论文提出"工具性目标轨迹"概念，通过监控组织获取计算、数据、资金等资源的过程来增强对AI系统的控制，将干预点从模型本身扩展到组织层面。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全措施主要关注技术层面和系统本身，但高度能力的AI系统可能通过追求工具性目标侵蚀人类控制。需要超越模型本身，从组织层面寻找更全面的控制方法。

Method: 提出三种组织路径：采购、治理和财务工具性目标轨迹，通过监控这些路径中产生的组织痕迹来识别干预点。这些轨迹关注AI系统获取计算、存储、数据、资金等关键资源的过程。

Result: IGTs提供了具体的监控和干预途径，能够定义能力水平，并扩展可纠正性和可中断性的实现方式，将关注点从模型属性转移到支持模型的组织系统。

Conclusion: 工具性目标轨迹为AI安全提供了新的组织层面干预框架，通过监控资源获取过程来增强对AI系统的控制，为定义能力阈值和实施干预措施提供了具体路径。

Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.

</details>


### [116] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

TL;DR: CPO使用因果推断框架优化LLM提示，通过双机器学习分离提示效果与查询特征，实现低成本、高精度的查询特定提示优化。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法存在两个问题：静态指令无法适应异构查询；动态方法依赖离线奖励模型，但奖励模型本质上是相关性的，混淆了提示效果与查询特征。

Method: 提出因果提示优化(CPO)框架：1) 应用双机器学习到提示和查询的语义嵌入，学习离线因果奖励模型，分离提示变异的因果效应与混淆查询属性；2) 利用无偏奖励信号指导资源高效的查询特定提示搜索，无需昂贵的在线评估。

Result: 在数学推理、可视化和数据分析基准测试中，CPO始终优于人工设计的提示和最先进的自动优化器。改进主要由困难查询上的鲁棒性提升驱动，而现有方法在这些查询上表现恶化。

Conclusion: CPO从根本上重塑了提示优化的经济学：通过将评估从实时模型执行转移到离线因果模型，能以在线方法所需推理成本的一小部分实现高精度、按查询定制的优化。因果推断为可靠且经济高效的提示优化提供了可扩展基础。

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [117] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

TL;DR: MACD提出了一种新的推理策略，通过模型引导的反事实数据构建与对比解码相结合，减少视频语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 视频语言模型容易产生幻觉，特别是在视觉证据较弱、模糊或有偏差时。现有的解码方法（如对比解码）依赖随机扰动构建对比数据，难以控制导致幻觉的视觉线索或与模型弱点对齐。

Method: 提出模型感知的反事实数据对比解码（MACD），利用Video-LLM自身的反馈识别导致幻觉的对象区域，在对象级别生成有针对性的反事实输入，而不是任意的帧或时间修改。然后将这些模型感知的反事实数据集成到对比解码中，在解码过程中强制证据基础化的标记选择。

Result: 在EventHallusion、MVBench、Perception-test和Video-MME上的实验表明，MACD能持续减少幻觉，同时保持或提高各种Video-LLM（包括Qwen和InternVL系列）的任务准确性。该方法在处理涉及小物体、遮挡物体或共现物体的挑战性场景时特别有效。

Conclusion: MACD通过模型引导的反事实数据构建与对比解码相结合，有效减少了视频语言模型的幻觉问题，特别是在视觉证据不足的复杂场景中表现优异。

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [118] [Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives](https://arxiv.org/abs/2602.01749)
*Lin Chen,Samuel Drapeau,Fanghao Shao,Xuekai Zhu,Bo Xue,Yunchong Song,Mathieu Laurière,Zhouhan Lin*

Main category: cs.AI

TL;DR: α-GFNs通过可调参数α改进GFlowNet的探索-利用平衡，在多个基准测试中显著提升模式发现能力


<details>
  <summary>Details</summary>
Motivation: 传统GFlowNet目标隐含地固定了前向和后向策略的等比例混合，这可能限制了训练过程中的探索-利用权衡。作者希望通过建立GFlowNet与马尔可夫链的联系来理解这种约束的根源

Method: 首先建立GFlowNet目标与马尔可夫链可逆性的等价关系，揭示约束来源；然后提出α-GFNs，通过可调参数α泛化混合比例，直接控制探索-利用动态

Result: 在Set、Bit Sequence和Molecule Generation等多个基准测试中，α-GFN目标始终优于之前的GFlowNet目标，模式发现数量最高提升10倍

Conclusion: 通过理论分析GFlowNet与马尔可夫链的联系，提出的α-GFNs框架能够有效控制探索-利用平衡，显著提升模式发现能力，同时确保收敛到唯一流

Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \times$ increase in the number of discovered modes.

</details>


### [119] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: ARA框架将奖励黑客攻击重构为动态竞争游戏，通过黑客发现漏洞、审计员检测利用，再用审计引导的RLHF惩罚黑客行为，实现跨域通用防御。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法容易受到奖励黑客攻击，模型会利用奖励模型中的虚假相关性获得高分但违背人类意图。现有防御措施是静态的，无法适应新的攻击策略。

Method: 提出对抗性奖励审计(ARA)框架：1)黑客策略发现奖励模型漏洞，审计员从潜在表示中学习检测利用；2)审计引导的RLHF(AG-RLHF)通过门控奖励信号惩罚检测到的黑客行为。

Result: 在三种黑客场景中，ARA在所有基线中实现了最佳对齐-效用权衡：将奉承行为降至接近SFT水平同时提高帮助性，减少冗长同时获得最高ROUGE-L，抑制代码游戏同时提高Pass@1。

Conclusion: 奖励黑客攻击、检测和缓解都具有跨域泛化能力，单个审计员模型能有效抑制多个领域的利用行为，实现高效的多域防御。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [120] [PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models](https://arxiv.org/abs/2602.01762)
*Xuliang Wang,Yuetao Chen,Maochan Zhen,Fang Liu,Xinzhou Zheng,Xingwu Liu,Hong Xu,Ming Li*

Main category: cs.AI

TL;DR: PRISM是一种新的推测解码架构，通过解耦计算路径来分离模型容量与推理成本，显著提升LLM解码速度


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法为了提升草稿质量而使用更大的参数化草稿模型，但这引入了显著的计算开销。需要在预测准确性和计算延迟之间找到更好的平衡点。

Method: 提出PRISM架构，将每个预测步骤的计算分解到不同的参数集，重构草稿模型的计算路径，成功解耦模型容量与推理成本

Result: PRISM优于所有现有草稿架构，实现了卓越的接受长度，同时保持最小的草稿延迟，端到端加速效果显著。在高度优化的推理引擎上提升解码吞吐量超过2.6倍

Conclusion: PRISM通过架构创新解决了推测解码中模型容量与推理成本的根本矛盾，比现有方法更有效地扩展，为LLM加速提供了新方向

Abstract: Large Language Models (LLMs), constrained by their auto-regressive nature, suffer from slow decoding. Speculative decoding methods have emerged as a promising solution to accelerate LLM decoding, attracting attention from both systems and AI research communities. Recently, the pursuit of better draft quality has driven a trend toward parametrically larger draft models, which inevitably introduces substantial computational overhead. While existing work attempts to balance the trade-off between prediction accuracy and compute latency, we address this fundamental dilemma through architectural innovation.
  We propose PRISM, which disaggregates the computation of each predictive step across different parameter sets, refactoring the computational pathways of draft models to successfully decouple model capacity from inference cost. Through extensive experiments, we demonstrate that PRISM outperforms all existing draft architectures, achieving exceptional acceptance lengths while maintaining minimal draft latency for superior end-to-end speedup. We also re-examine scaling laws with PRISM, revealing that PRISM scales more effectively with expanding data volumes than other draft architectures. Through rigorous and fair comparison, we show that PRISM boosts the decoding throughput of an already highly optimized inference engine by more than 2.6x.

</details>


### [121] [Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction](https://arxiv.org/abs/2602.01775)
*Yucheng Wu,Yuekui Yang,Hongzheng Li,Anan Liu,Jian Xiao,Junjie Zhai,Huan Yu,Shaoping Ma,Leye Wang*

Main category: cs.AI

TL;DR: CrossAdapt：两阶段跨架构知识蒸馏框架，解决大规模推荐系统中模型切换成本高的问题，通过离线快速嵌入迁移和在线非对称协同蒸馏，在减少训练时间43-71%的同时提升AUC 0.27-0.43%


<details>
  <summary>Details</summary>
Motivation: 大规模用户响应预测系统中部署新架构面临高昂的模型切换成本，包括大规模历史数据重新训练的开销和数据保留约束下的性能下降。现有知识蒸馏方法难以处理架构异构性和大型嵌入表迁移的昂贵成本。

Method: 提出两阶段框架：1）离线阶段：通过维度自适应投影实现快速嵌入迁移（无需迭代训练），结合渐进网络蒸馏和策略性采样降低计算成本；2）在线阶段：引入非对称协同蒸馏（学生频繁更新，教师低频更新），配合分布感知适应机制动态平衡历史知识保留和快速适应演化数据。

Result: 在三个公开数据集上，CrossAdapt实现AUC提升0.27-0.43%，同时减少训练时间43-71%。在腾讯微信视频号大规模部署（约1000万日样本）中，相比标准蒸馏基线显著缓解了AUC下降、LogLoss增加和预测偏差。

Conclusion: CrossAdapt有效解决了大规模推荐系统中跨架构知识迁移的挑战，通过创新的两阶段设计平衡了效率与性能，为实际工业部署提供了可行的解决方案。

Abstract: Deploying new architectures in large-scale user response prediction systems incurs high model switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterogeneity and the prohibitive cost of transferring large embedding tables. We propose CrossAdapt, a two-stage framework for efficient cross-architecture knowledge transfer. The offline stage enables rapid embedding transfer via dimension-adaptive projections without iterative training, combined with progressive network distillation and strategic sampling to reduce computational cost. The online stage introduces asymmetric co-distillation, where students update frequently while teachers update infrequently, together with a distribution-aware adaptation mechanism that dynamically balances historical knowledge preservation and fast adaptation to evolving data. Experiments on three public datasets show that CrossAdapt achieves 0.27-0.43% AUC improvements while reducing training time by 43-71%. Large-scale deployment on Tencent WeChat Channels (~10M daily samples) further demonstrates its effectiveness, significantly mitigating AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.

</details>


### [122] [LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning](https://arxiv.org/abs/2602.01779)
*Rui Hua,Yu Wei,Zixin Shu,Kai Chang,Dengying Yan,Jianan Xia,Zeyu Liu,Hui Zhu,Shujie Song,Mingzhong Xiao,Xiaodong Li,Dongmei Jia,Zhuye Gao,Yanyan Meng,Naixuan Zhao,Yu Fu,Haibin Yu,Benman Yu,Yuanyuan Chen,Fei Dong,Zhizhou Meng,Pengcheng Yang,Songxue Zhao,Lijuan Pei,Yunhui Hu,Kan Ding,Jiayuan Duan,Wenmao Yin,Yang Gu,Runshun Zhang,Qiang Zhu,Jian Yu,Jiansheng Li,Baoyan Liu,Wenjia Wang,Xuezhong Zhou*

Main category: cs.AI

TL;DR: LingLanMiDian (LingLan) 是一个大规模、专家标注的中医多任务基准测试，统一评估LLM在中医领域的知识回忆、多跳推理、信息抽取和临床决策能力，揭示当前模型与人类专家在中医专门推理上的显著差距。


<details>
  <summary>Details</summary>
Motivation: 中医具有独特的本体论、术语和推理模式，需要领域忠实的评估。现有中医基准测试覆盖范围碎片化、规模有限，且依赖非统一或生成密集的评分方法，阻碍了公平比较。

Method: 构建LingLan基准测试，包含统一度量设计、临床标签的同义词容忍协议、每个数据集400项的困难子集，并将诊断和治疗建议重构为单项选择决策识别。对14个领先的开源和专有LLM进行全面的零样本评估。

Result: 评估提供了对LLM在中医常识知识理解、推理和临床决策支持方面优势和局限性的统一视角。困难子集的评估显示当前模型与人类专家在中医专门推理上存在显著差距。

Conclusion: LingLan通过标准化评估连接基础知识和应用推理，为推进中医LLM和领域特定医学AI研究建立了统一、可量化、可扩展的基础。

Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.

</details>


### [123] [ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing](https://arxiv.org/abs/2602.01797)
*Hanlin Zhou,Huah Yong Chan*

Main category: cs.AI

TL;DR: ORCH是一个确定性的多智能体协调框架，通过"多分析、一决策"范式，让多个LLM独立分析，再由合并智能体输出最终选择，在多项推理任务上显著超越单模型基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统通常依赖随机路由或临时启发式方法，导致行为难以复现、决策过程难以解释。需要一种确定性的协调框架来提高可预测性和可解释性。

Method: ORCH采用"多分析、一决策"范式：多个基础模型独立生成结构化分析，专用合并智能体输出最终选择。使用固定规则进行任务分解和答案聚合，保持流程可预测、可复现且无需训练。可选引入EMA引导的路由器，基于历史准确率、延迟或成本更新智能体选择。

Result: 在MMLU、MMLU-Pro和GSM8K上的实验显示，ORCH始终优于单模型基线和多数投票集成。在MMLU-Pro上比最强基线提高超过10个百分点准确率，在GSM8K上获得超过50个百分点的增益。EMA路由器提供额外0.7-2.0个百分点的准确率提升。

Conclusion: ORCH为离散选择推理提供了一个实用路径，实现了可控、可解释且可部署的基于LLM的智能体系统，通过确定性协调和多智能体协作显著提升性能。

Abstract: Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.

</details>


### [124] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

TL;DR: INDIBATOR框架通过基于科学家个人研究轨迹的个性化代理，在分子发现任务中超越了传统的粗粒度角色分配方法，实现了更高质量的科学发现。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中通用的角色分配（如"审稿人"、"作者"）过于简化了真实科学家的行为模式，忽略了科学家独特的研究轨迹对科学贡献的影响。

Method: 提出INDIBATOR框架，通过两种模态构建个性化科学家档案：1) 发表历史（文献知识） 2) 分子历史（结构先验）。这些代理通过提案、批评和投票三个阶段进行多轮辩论。

Result: 基于细粒度个性化档案的代理系统持续优于依赖粗粒度角色的系统，达到竞争性或最先进的性能水平。

Conclusion: 捕捉智能体的"科学DNA"对于高质量的科学发现至关重要，验证了个性化科学家档案在多智能体系统中的重要性。

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [125] [Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs](https://arxiv.org/abs/2602.01832)
*Rui Wang,Yaoguang Cao,Yuyi Chen,Jianyi Xu,Zhuoyang Li,Jiachen Shang,Shichun Yang*

Main category: cs.AI

TL;DR: 提出SoV框架，通过视觉输入预测自动驾驶车辆的触觉激励，解决当前传感器无法检测道路激励的问题


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆依赖多模态融合保证安全，但现有视觉和光学传感器无法检测对车辆动态控制至关重要的道路激励。受人类联觉启发，需要开发能够从视觉输入预测触觉激励的系统

Method: 1. 提出车辆联觉(SoV)框架；2. 开发跨模态时空对齐方法解决时间和空间差异；3. 提出基于潜在扩散的视觉-触觉联觉(VTSyn)生成模型，用于无监督高质量触觉数据合成；4. 使用真实车辆感知系统收集多模态数据集

Result: VTSyn在时间、频率和分类性能方面优于现有模型，通过主动触觉感知增强自动驾驶安全性

Conclusion: SoV框架成功实现了从视觉到触觉的跨模态预测，为自动驾驶车辆提供了重要的道路激励感知能力，显著提升了车辆安全性能

Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.

</details>


### [126] [ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems](https://arxiv.org/abs/2602.01848)
*Salaheddin Alzu'bi,Baran Nama,Arda Kaz,Anushri Eswaran,Weiyuan Chen,Sarvesh Khetan,Rishab Bala,Tu Vu,Sewoong Oh*

Main category: cs.AI

TL;DR: ROMA是一个递归开放元代理框架，通过任务分解和结构化聚合解决长时程任务中的性能问题，支持异构多智能体系统，结合GEPA+提示优化器在多个基准测试中取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架在长时程任务中表现不佳，随着推理深度增加，顺序编排变得脆弱，上下文窗口限制导致性能下降，不透明的执行轨迹使得故障难以定位和调试。

Method: 提出ROMA框架，通过递归任务分解和结构化聚合，将目标分解为依赖感知的子任务树并行执行，同时聚合压缩和验证中间结果以控制上下文增长。框架围绕四个模块化角色构建：Atomizer（决定任务是否分解）、Planner、Executor和Aggregator。还引入GEPA+遗传帕累托提示提议器，在ROMA组件层次结构中搜索提示。

Result: 在SEAL-0基准测试中，ROMA结合GLM-4.6将准确率比Kimi-Researcher提高9.9%。在EQ-Bench长文本生成基准测试中，ROMA使DeepSeek-V3能够与Claude Sonnet 4.5等领先闭源模型性能相当。

Conclusion: 递归模块化智能体架构能够扩展推理深度，同时保持可解释性、灵活性和模型无关性，为解决长时程任务中的性能瓶颈提供了有效方案。

Abstract: Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.

</details>


### [127] [SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures](https://arxiv.org/abs/2602.01858)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: SOPRAG：基于专家混合的工业标准操作规程检索框架，通过实体、因果、流程图专家和程序卡层解决传统RAG在工业SOP检索中的局限性，在多个工业领域显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 工业标准操作规程（SOPs）检索面临独特挑战：专有结构僵化、条件依赖相关性、需可执行操作，传统语义驱动的RAG范式无法有效解决这些问题。

Method: 提出SOPRAG框架，采用专家混合范式，用专门的实体、因果和流程图专家替代平面分块；引入程序卡层修剪搜索空间消除计算噪声，以及LLM引导的门控机制动态加权专家；还设计了自动化多智能体工作流构建基准数据集。

Result: 在四个工业领域的广泛实验中，SOPRAG在检索准确性和响应实用性方面显著优于强力的词法、密集和基于图的RAG基线，在现实世界关键任务中实现了完美的执行分数。

Conclusion: SOPRAG成功解决了工业SOP检索的独特挑战，通过专家混合架构和智能协调机制，为工业环境中的操作安全和一致性提供了有效的解决方案。

Abstract: Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.

</details>


### [128] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

TL;DR: ProcMEM框架让LLM智能体从交互经验中自主构建可执行的过程记忆，通过非参数PPO实现高质量技能验证，显著提升重用率和性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在顺序决策中依赖即时推理，即使在重复场景中也会重新推导解决方案，导致计算冗余和执行不稳定。缺乏经验重用机制限制了长期自主性。

Method: 提出ProcMEM框架，通过形式化Skill-MDP将被动经验叙事转化为可执行技能（包含激活、执行、终止条件）。引入非参数PPO，利用语义梯度生成高质量候选技能，通过PPO Gate进行稳健技能验证，并通过基于分数的维护保持紧凑高质量的过程记忆。

Result: 在领域内、跨任务和跨智能体场景中，ProcMEM实现了卓越的重用率和显著的性能提升，同时具有极端的内存压缩能力。可视化进化轨迹和技能分布展示了过程知识的透明积累、精炼和重用。

Conclusion: ProcMEM通过自主构建可执行的过程记忆，有效解决了LLM智能体经验重用不足的问题，实现了计算效率、执行稳定性和长期自主性的显著提升。

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [129] [Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models](https://arxiv.org/abs/2602.01884)
*Shidong Yang,Tongwen Huang,Hao Wen,Yong Wang,Li Chen,Xiangxiang Chu*

Main category: cs.AI

TL;DR: 提出基于熵指导的训练方法(EGT)，通过响应熵作为无监督代理来识别标注噪声和样本难度，提升多模态推理奖励模型的性能


<details>
  <summary>Details</summary>
Motivation: 多模态奖励模型在训练中存在两个关键挑战：1) 偏好数据集固有的噪声会降低模型性能；2) 传统训练方法效率低下，忽略了样本难度的差异

Method: 提出熵指导训练(EGT)方法：1) 熵指导数据筛选，利用响应熵作为无监督代理识别不可靠样本；2) 熵指导训练策略，逐步引入更复杂的示例进行训练

Result: 在三个基准测试上的广泛实验表明，EGT训练的模型始终优于最先进的多模态奖励模型

Conclusion: 响应熵与准确性之间存在强相关性，可以作为标注噪声和样本难度的可靠无监督代理，基于此提出的EGT方法能有效提升多模态推理奖励模型的性能

Abstract: Multimodal reward models are crucial for aligning multimodal large language models with human preferences. Recent works have incorporated reasoning capabilities into these models, achieving promising results. However, training these models suffers from two critical challenges: (1) the inherent noise in preference datasets, which degrades model performance, and (2) the inefficiency of conventional training methods, which ignore the differences in sample difficulty. In this paper, we identify a strong correlation between response entropy and accuracy, indicating that entropy can serve as a reliable and unsupervised proxy for annotation noise and sample difficulty. Based on this insight, we propose a novel Entropy-Guided Training (EGT) approach for multimodal reasoning reward models, which combines two strategies: (1) entropy-guided data curation to mitigate the impact of unreliable samples, and (2) an entropy-guided training strategy that progressively introduces more complex examples. Extensive experiments across three benchmarks show that the EGT-trained model consistently outperforms state-of-the-art multimodal reward models.

</details>


### [130] [Geometric Analysis of Token Selection in Multi-Head Attention](https://arxiv.org/abs/2602.01893)
*Timur Mudarisov,Mikhal Burtsev,Tatiana Petrova,Radu State*

Main category: cs.AI

TL;DR: 论文提出几何框架分析LLM多头注意力，通过top-N选择视角在值状态空间研究注意力机制，定义几何度量评估token可分性，理论预测与实证结果一致，发现注意力表现为结构化几何分类器。


<details>
  <summary>Details</summary>
Motivation: 为理解大型语言模型中多头注意力的工作机制，需要从几何角度分析注意力如何选择和分离token，提供可解释的量化框架，从而改进注意力机制的设计和稀疏化。

Method: 1) 将标准注意力视为top-N选择机制，在值状态空间分析；2) 定义几何度量：精确率、召回率和F分数，量化选中与未选中token的可分性；3) 在经验假设下推导非渐近边界；4) 在LLaMA-2-7B、Gemma-7B和Mistral-7B模型上进行实证验证。

Result: 1) 理论预测与实证测量高度一致；2) top-N选择增强可分性；3) sink相似性与召回率相关；4) 在LLaMA-2-7B中发现三种注意力头类型：检索器、混合器、重置器，各有不同几何特征；5) 注意力表现为结构化几何分类器，具有可测量的token选择标准。

Conclusion: 注意力机制本质上是结构化几何分类器，几何框架为注意力头提供可解释性，并可为几何感知的注意力稀疏化和设计提供指导，有助于理解LLM中注意力机制的工作方式。

Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.

</details>


### [131] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

TL;DR: DomusFM是首个专门为智能家居传感器数据设计的预训练基础模型，通过自监督双对比学习范式，在数据稀缺情况下实现跨环境和任务的优异性能。


<details>
  <summary>Details</summary>
Motivation: 智能家居传感器数据在医疗监测和辅助技术中潜力巨大，但现有方法存在严重局限：监督模型需要大量标注数据；现有基础模型只关注惯性传感器，不适用于智能家居二元传感器数据的稀疏离散特性和丰富语义关联；基于LLM的方法存在隐私和成本问题。

Method: DomusFM采用自监督双对比学习范式，结合轻量级语言模型的语义嵌入、时间模式编码器和二元状态编码器，同时捕捉token级语义属性和序列级时间依赖关系。

Result: 在七个公共智能家居数据集上的留一数据集评估表明，DomusFM在不同下游任务中优于现有最佳基线，即使在仅有5%标注数据的情况下进行微调也能实现优异性能。

Conclusion: DomusFM解决了智能家居传感器数据稀缺问题，同时保持了实际部署的可行性，为现实世界智能家居系统提供了实用的解决方案。

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


### [132] [Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling](https://arxiv.org/abs/2602.01933)
*Fabrice Boissier,Monica Sen,Irina Rychkova*

Main category: cs.AI

TL;DR: 比较大型语言模型（LLM）和形式概念分析（FCA）在主题建模任务中的表现，通过两个实验评估它们在文档主题提取方面的优缺点。


<details>
  <summary>Details</summary>
Motivation: 主题建模在文档检索、情感分析和文本摘要等领域的应用日益广泛。虽然LLM在文本处理中很流行，但对其在主题建模中的研究较少。FCA最近被提出作为主题建模的候选方法，但缺乏实际应用案例研究。本研究旨在比较这两种方法，了解它们在主题建模领域的优势和局限性。

Method: 使用CREA管道评估FCA（基于过去主题建模和可视化实验），使用GPT-5评估LLM。对GPT-5采用基于三个提示的零样本设置策略：从文档批次生成主题、合并批次结果形成最终主题、主题标注。进行两个实验：第一个重用之前评估CREA的教学材料，第二个分析40篇信息系统研究文章，比较提取的主题与底层子领域。

Result: 论文没有提供具体结果数据，但从摘要描述可以看出，研究通过两个实验系统比较了LLM和FCA在主题建模中的表现。第一个实验使用已知的教学材料，第二个实验使用实际研究文章，旨在评估提取的主题与实际子领域的匹配程度。

Conclusion: 该研究对LLM和FCA在主题建模任务中的表现进行了系统性比较，为理解这两种方法的优缺点提供了实证基础。研究结果表明需要进一步探索这两种技术在主题建模领域的实际应用价值。

Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.

</details>


### [133] [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)
*Yun Qu,Qi Wang,Yixiu Mao,Heming Zou,Yuhang Jiang,Weijie Liu,Clive Bai,Kai Yang,Yangkun Chen,Saiyong Yang,Xiangyang Ji*

Main category: cs.AI

TL;DR: 提出GPS方法，通过贝叶斯推断和轻量生成模型选择信息丰富的提示，提升强化学习中语言模型推理的训练效率和最终性能


<details>
  <summary>Details</summary>
Motivation: 强化学习提升大语言模型推理能力但计算成本高，现有在线提示选择方法要么依赖昂贵评估，要么缺乏跨提示泛化能力

Method: GPS使用轻量生成模型在共享优化历史上进行贝叶斯推断，结合中等难度优先和历史锚定多样性原则选择信息丰富的提示批次

Result: 在多个推理基准测试中，GPS在训练效率、最终性能和测试时效率方面显著优于基线方法

Conclusion: GPS通过可泛化的预测性提示选择有效解决了强化学习中计算成本高的问题，为高效训练提供了新方法

Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.

</details>


### [134] [Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning](https://arxiv.org/abs/2602.01983)
*Xintian Shen,Jiawei Chen,Lihao Zheng,Hao Ma,Tao Wei,Kun Zhan*

Main category: cs.AI

TL;DR: UCT框架让LLM从工具使用者转变为工具创造者，通过提取推理经验创建自适应工具，无需额外训练即可持续改进


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理模型存在三个主要问题：1) 固定工具无法满足开放性问题需求；2) 错误工具输出会误导LLM；3) 工具构建需要大量人工工作，限制了适用性

Method: 提出UCT框架，将LLM的推理痕迹转化为可重用工具，包括：1) 从工具使用者转变为工具创造者；2) 自适应工具创建和推理过程中的自我更新；3) 记忆整合机制维护工具库，确保经验记忆的高可重用性

Result: 在数学和科学推理任务的多领域基准测试中，性能显著提升+20.86%和+23.04%，验证了代理的自我进化能力

Conclusion: UCT为增强TIR模型能力提供了新范式，通过自动化工具构建使代理系统无需额外训练即可持续进步，实现了从工具使用者到工具创造者的转变

Abstract: Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\uparrow$ and +23.04%$\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.

</details>


### [135] [Emergent Analogical Reasoning in Transformers](https://arxiv.org/abs/2602.01992)
*Gouki Minegishi,Jingyuan Feng,Hiroki Furuta,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 该论文将类比推理形式化为跨类别实体对应关系的推断，基于范畴论中的函子概念，通过合成任务研究Transformer中类比推理的涌现机制，发现其依赖于嵌入空间中的几何对齐和Transformer内部的函子应用。


<details>
  <summary>Details</summary>
Motivation: 类比是人类智能的核心能力，但Transformer如何获得和实施类比推理的机制仍不清楚。研究旨在将类比从抽象的认知概念转变为现代神经网络中具体、机制上可解释的现象。

Method: 基于范畴论中的函子概念，将类比推理形式化为跨类别实体对应关系的推断。引入合成任务在受控设置下评估类比推理的涌现，并进行机制分析。

Result: 类比推理的涌现高度敏感于数据特征、优化选择和模型规模。Transformer中的类比推理分解为两个关键组件：(1)嵌入空间中关系结构的几何对齐；(2)Transformer内部函子的应用。这些机制使模型能够将关系结构从一个类别转移到另一个类别，实现类比。同样的趋势在预训练LLMs中也观察到。

Conclusion: 该研究将类比从抽象的认知概念转变为现代神经网络中具体、机制上可解释的现象，揭示了Transformer实现类比推理的机制基础，为理解神经网络中的高级推理能力提供了新视角。

Abstract: Analogy is a central faculty of human intelligence, enabling abstract patterns discovered in one domain to be applied to another. Despite its central role in cognition, the mechanisms by which Transformers acquire and implement analogical reasoning remain poorly understood. In this work, inspired by the notion of functors in category theory, we formalize analogical reasoning as the inference of correspondences between entities across categories. Based on this formulation, we introduce synthetic tasks that evaluate the emergence of analogical reasoning under controlled settings. We find that the emergence of analogical reasoning is highly sensitive to data characteristics, optimization choices, and model scale. Through mechanistic analysis, we show that analogical reasoning in Transformers decomposes into two key components: (1) geometric alignment of relational structure in the embedding space, and (2) the application of a functor within the Transformer. These mechanisms enable models to transfer relational structure from one category to another, realizing analogy. Finally, we quantify these effects and find that the same trends are observed in pretrained LLMs. In doing so, we move analogy from an abstract cognitive notion to a concrete, mechanistically grounded phenomenon in modern neural networks.

</details>


### [136] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: 提出基于知识图谱的对话诊断系统，通过两步推理（生成诊断假设和验证假设）来提高诊断准确性和效率，并采用模糊症状描述的模拟器来增强现实性。


<details>
  <summary>Details</summary>
Motivation: 现有对话诊断方法要么依赖模型参数知识，要么假设患者提供丰富具体信息，这在现实中不切实际。需要解决信息不完整情况下进行多轮问诊的挑战。

Method: 提出基于诊断知识图谱的对话诊断系统，采用两步推理：1) 从对话上下文生成诊断假设；2) 通过澄清问题验证假设，循环直到得出最终诊断。使用MIMIC-IV患者数据构建模拟器，并让模拟器描述症状模糊以反映真实临床早期情况。

Result: 实验显示相比强基线方法，该系统在诊断准确性和效率上都有提升。医生评估支持模拟器的现实性和生成问题的临床实用性。

Conclusion: 提出的基于知识图谱的对话诊断系统能够有效处理信息不完整情况，通过两步推理机制提高诊断性能，模糊症状模拟器增强了现实性，为临床对话诊断提供了实用解决方案。

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [137] [Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction](https://arxiv.org/abs/2602.02018)
*Enes Altinisik,Masoomali Fatehkia,Fatih Deniz,Nadir Durrani,Majd Hawasly,Mohammad Raza,Husrev Taha Sencar*

Main category: cs.AI

TL;DR: VeriFY是一个训练时框架，通过一致性自验证教LLMs推理事实不确定性，减少事实幻觉同时保持召回率


<details>
  <summary>Details</summary>
Motivation: 现有缓解LLMs事实幻觉的方法主要依赖外部事后验证或将不确定性直接映射为弃权，通常导致过于保守的行为，需要更好的训练时解决方案

Method: VeriFY通过结构化验证轨迹增强训练，指导模型：1)生成初始答案，2)生成并回答验证查询，3)进行一致性判断，4)决定回答或弃权。采用阶段级损失掩码避免强化幻觉内容

Result: 在多个模型系列和规模上，VeriFY将事实幻觉率降低9.7%至53.3%，召回率仅轻微下降0.4%至5.7%，且在单数据集训练时能跨数据集泛化

Conclusion: VeriFY通过训练时的一致性自验证有效减少LLMs的事实幻觉，平衡准确性与保守性，为事实不确定性推理提供了有前景的解决方案

Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.

</details>


### [138] [Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron](https://arxiv.org/abs/2602.02027)
*Sicheng Shen,Mingyang Lv,Han Shen,Jialin Wu,Binghao Wang,Zhou Yang,Guobin Shen,Dongcheng Zhao,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 提出一种基于专家模型和单神经元门控机制的安全感知解码方法，实现轻量级对齐，在保持模型能力的同时增强输出安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐方法主要依赖后训练，计算成本高且泛化能力差；轻量级方法要么依赖预计算的安全注入，要么过度依赖模型自身能力，导致泛化有限、生成效率降低。

Method: 提出安全感知解码方法：仅需低成本训练专家模型，使用单神经元作为门控机制，有效平衡模型内在能力与外部指导。

Result: 在训练开销和跨模型规模泛化方面具有明显优势，同时保持实用性和增强输出安全性。

Conclusion: 为大型语言模型的安全实用部署提供了轻量级对齐的新视角。

Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.

</details>


### [139] [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028)
*Ya Gao,Kalle Kujanpää,Pekka Marttinen,Harri Valpola,Alexander Ilin*

Main category: cs.AI

TL;DR: 提出一种基于推理的知识内化训练策略，通过连贯背景故事、多跳问题生成和知识蒸馏，使AI模型能有效整合新知识进行推理


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要关注原子事实，虽然改善了事实回忆，但未能将新信息整合到可在不同上下文中使用的连贯框架中。知识内化本质上是推理问题而非记忆问题。

Method: 提出基于三个原则的训练策略：1) 将新知识作为连贯背景故事引入，解释新事实与现有知识的关系；2) 使用自生成的多跳问题进行训练，需要涉及新信息的多步推理；3) 通过知识蒸馏进行训练，强制学生模型内化教师的推理行为而无需访问新信息。

Result: 实验表明，使用该策略训练的模型在推理过程中能有效利用新获得的知识，并在需要结合多个新事实的挑战性问题中取得显著性能。

Conclusion: 通过将知识内化视为推理问题而非记忆问题，并采用基于背景故事、多跳推理和知识蒸馏的训练策略，可以显著提升AI模型整合和应用新知识的能力。

Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.

</details>


### [140] [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029)
*Zhongyuan Lyu,Shuoyu Hu,Lujie Liu,Hongxia Yang,Ming LI*

Main category: cs.AI

TL;DR: 提出CIR中间表示和R2C框架，通过多智能体管道将自然语言描述转化为优化模型，在复杂操作规则建模上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的优化模型自动生成方法在处理复杂操作规则的复合约束和适当建模范式方面存在困难，需要一种更好的方法来解耦规则逻辑与数学实例化

Method: 引入规范中间表示(CIR)作为LLM生成的中间模式，通过约束原型和候选建模范式编码操作规则语义；开发R2C多智能体框架，包含问题解析、CIR合成和优化模型实例化

Result: 在新构建的包含丰富操作规则的基准测试中达到47.2%准确率，在已有基准测试中达到与专有模型(如GPT-5)相当的竞争性结果，通过反思机制进一步提升了性能

Conclusion: CIR和R2C框架有效解决了复杂操作规则的优化建模问题，实现了规则逻辑与数学实例化的解耦，在多个基准测试中展现了优越性能

Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.

</details>


### [141] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: 论文提出了一种基于有限时域马尔可夫决策过程的多智能体系统，用于改进LLM在合规审查等监管场景中的工作流程，通过量化不确定性并引入人类监督，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在复杂监管工作流（如合规审查）中主要依赖单一智能体的提示工程，难以观察和比较模型如何处理跨决策阶段的不确定性和协调问题，以及如何与人类监督有效结合。

Method: 将多智能体系统形式化为具有有向无环结构的有限时域马尔可夫决策过程，每个智能体对应特定角色或决策阶段（如内容、业务或法律审查），通过预定义转换表示任务升级或完成。使用蒙特卡洛估计量化智能体层面的认知不确定性，系统级不确定性通过MDP在自动标记状态或人工审查状态的终止来捕捉。

Result: 在AI安全评估（自残检测）的案例研究中，相比单智能体基线，实现了高达19%的准确率提升，高达85倍的人工审查需求减少，在某些配置下还减少了处理时间。

Conclusion: 提出的多智能体MDP框架能够有效管理LLM在监管工作流中的不确定性和协调问题，通过结构化决策过程和量化不确定性，显著提升了系统性能并减少了人工干预需求。

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [142] [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039)
*Wei Liu,Peijie Yu,Michele Orini,Yali Du,Yulan He*

Main category: cs.AI

TL;DR: 论文提出"调查性智能"概念，区别于执行性智能，并引入Deep Data Research任务和DDR-Bench基准来评估LLM在数据科学中的自主探索能力。


<details>
  <summary>Details</summary>
Motivation: 当前对Agentic LLM的期望超越了正确回答问题，需要自主设定目标和决定探索方向。数据科学是天然测试场，但现有基准很少关注从原始数据开始的真实分析场景。

Method: 提出Deep Data Research开放任务，让LLM自主从数据库中提取关键洞察；构建DDR-Bench大规模检查表基准，支持可验证评估。

Result: 前沿模型显示出初步的代理能力，但长期视野的探索仍然困难。调查性智能的有效性不仅取决于代理框架或规模扩展，还与代理模型的内在策略有关。

Conclusion: 区分调查性智能和执行性智能很重要，DDR-Bench为评估LLM在数据科学中的自主探索能力提供了新基准，揭示了代理模型内在策略对调查性智能的关键作用。

Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.

</details>


### [143] [Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents](https://arxiv.org/abs/2602.02050)
*Zeping Li,Hongru Wang,Yiwen Zhao,Guanhua Chen,Yixia Li,Keyang Chen,Yixin Cao,Guangnan Ye,Hongfeng Chai,Mengdi Wang,Zhenfei Yin*

Main category: cs.AI

TL;DR: 基于熵减的奖励策略优化LLM工具使用行为，稀疏奖励减少72.07%工具调用，密集奖励提升22.27%性能


<details>
  <summary>Details</summary>
Motivation: LLM工具使用代理在长轨迹中经常触发过多低质量工具调用，增加延迟并降低推理性能，需要有效管理工具使用行为

Method: 通过熵基实验发现熵减与高质量工具调用正相关，提出以熵减作为监督信号，设计稀疏结果奖励（轨迹级指导）和密集过程奖励（细粒度监督）两种策略

Result: 实验显示稀疏奖励比基线平均减少72.07%工具调用，密集奖励提升22.27%性能，熵减成为增强工具使用行为的关键机制

Conclusion: 熵减作为监督信号能有效优化LLM工具使用行为，使代理在现实应用中更具适应性，两种奖励策略分别针对效率和性能优化需求

Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.

</details>


### [144] [SIDiffAgent: Self-Improving Diffusion Agent](https://arxiv.org/abs/2602.02051)
*Shivank Garg,Ayush Singh,Gaurav Kumar Nayak*

Main category: cs.AI

TL;DR: SIDiffAgent：基于Qwen系列模型的免训练代理框架，通过自主提示工程、错误检测与修复、细粒度伪影去除以及迭代自改进，解决文本到图像扩散模型的部署限制。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在实际部署中存在多个限制：对提示词表述敏感、语义歧义（如"mouse"指动物还是电脑外设）、解剖结构扭曲等伪影，以及需要精心设计的输入提示。现有方法通常需要额外训练且可控性有限，限制了实际应用中的适应性。

Method: 提出SIDiffAgent免训练代理框架，利用Qwen系列模型（Qwen-VL、Qwen-Image、Qwen-Edit、Qwen-Embedding）自主管理提示工程、检测并纠正生成错误、执行细粒度伪影去除。框架包含迭代自改进机制，将过往经验存储在数据库中，并在代理流程的每个阶段注入基于提示的指导。

Result: 在GenAIBench基准测试中，SIDiffAgent获得了0.884的平均VQA分数，显著优于开源模型、专有模型以及其他代理方法。

Conclusion: SIDiffAgent通过免训练的代理框架有效解决了文本到图像扩散模型的部署挑战，实现了更可靠和一致的输出，并在基准测试中表现出优越性能。

Abstract: Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.

</details>


### [145] [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133)
*Sangwoo Shin,BumJun Kim,Kyelim Lee,Moongyu Jeon,Albert No*

Main category: cs.AI

TL;DR: 扩散语言模型比自回归模型更少受逆转诅咒影响，主要原因是架构结构（权重共享）及其与训练的交互作用，而非训练目标本身。


<details>
  <summary>Details</summary>
Motivation: 研究为什么掩码扩散语言模型（MDMs）比自回归语言模型（ARMs）在逆转诅咒问题上表现更好。虽然MDMs使用任意顺序训练目标，但这并不能完全解释其优势，需要探究更深层的原因。

Method: 通过理论分析和实验验证：1）在单层Transformer编码器中，权重共享使前向和反向注意力分数正相关；2）相应的梯度对齐，最小化前向损失也减少反向损失；3）在受控玩具任务和大规模扩散语言模型上进行实验验证。

Result: MDMs对逆转诅咒的缓解主要源于架构结构（权重共享）及其与训练的交互作用，而非任意顺序训练目标。权重共享耦合了两个方向，使前向和反向注意力分数正相关，梯度对齐使得最小化前向损失也减少反向损失。

Conclusion: 扩散语言模型部分克服逆转诅咒的原因是其架构结构（特别是权重共享）及其与训练的交互作用，这解释了为什么MDMs能缓解ARMs中持续存在的失败模式。

Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that "$A$ is $B$", they often fail on the reverse query "$B$ is $A$". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing "[MASK] is $B$" during training does not necessarily teach the model to handle the reverse prompt "$B$ is [MASK]". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.

</details>


### [146] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

TL;DR: 提出DGR方法，通过将外部安全推理数据集转换和精炼以匹配目标大语言模型的内在分布，减少安全对齐带来的推理能力下降（安全税），同时保持安全性能。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐数据集通常从外部大模型或人工标注中蒸馏安全推理轨迹和答案，但这些与需要对齐的目标大语言模型存在分布差距，作者推测这种分布差距是导致目标模型推理能力显著下降的主要原因。

Method: 提出DGR方法，将现有分布外安全推理数据集转换和精炼，使其与目标大语言模型的内在分布对齐，从而减少分布差距对推理能力的影响。

Result: 实验表明：1) DGR有效缓解安全税，同时保持安全性能，相比Vanilla SFT在DirectRefusal上平均推理准确率提升30.2%，在R1-ACT上提升21.2%；2) 推理能力下降程度与分布偏移程度相关；3) 仅需10个样本即可激活有效的拒绝行为，表明安全对齐主要作为激活潜在知识的机制。

Conclusion: 分布一致性对保持大语言模型推理能力至关重要，安全对齐可能主要作为激活潜在知识的机制，这些发现为理解推理模型中安全激活机制提供了新见解。

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [147] [Traffic-Aware Navigation in Road Networks](https://arxiv.org/abs/2602.02158)
*Sarah Nassar*

Main category: cs.AI

TL;DR: 该研究比较了三种图搜索算法在金斯顿路网交通感知导航中的表现：Floyd-Warshall-Ingerman（预处理算法）、Dijkstra/A*（实时搜索）和Yen's算法（混合方法）。Dijkstra/A*在交通感知和最优性方面表现最佳，Floyd-Warshall最快但无交通感知，Yen's算法在速度和最优性间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估不同图搜索算法在真实城市路网（金斯顿）交通感知导航任务中的性能表现，为实际部署提供算法选择依据。需要权衡预处理时间、实时计算速度和路径最优性之间的平衡。

Method: 研究方法包括：1）Floyd-Warshall-Ingerman算法（单次运行多查询预处理）；2）Dijkstra和A*算法（连续单查询实时搜索）；3）Yen's算法（混合方法，先找前K条最短路径，再实时迭代）。在金斯顿路网上进行对比实验。

Result: 结果显示：Dijkstra和A*算法能提供最交通感知的最优解，且预处理需求最小；Floyd-Warshall-Ingerman实时计算最快，但只提供基于距离的路径，无交通感知；Yen's算法需要显著预处理，但在运行速度和最优性方面平衡了前两种方法。

Conclusion: 结论是每种方法都有其优缺点，需要根据具体部署场景的实际情况进行权衡选择。没有单一最优算法，最佳解决方案取决于具体应用需求。

Abstract: This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.

</details>


### [148] [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188)
*Xia Jiang,Jing Chen,Cong Zhang,Jie Gao,Chengpeng Hu,Chenhao Zhang,Yaoxin Wu,Yingqian Zhang*

Main category: cs.AI

TL;DR: NLCO是一个评估大语言模型在组合优化问题中端到端推理能力的基准测试，包含43个问题，采用四层分类法，实验显示LLM在小规模实例上表现良好但随规模增大而退化


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在数学和逻辑推理方面表现出色，但它们在组合优化（搜索高维解空间并满足硬约束）方面的能力尚未得到充分探索，需要建立专门的基准来评估LLM在这方面的能力

Method: 提出NLCO基准测试，包含43个组合优化问题，采用四层分类法（变量类型、约束族、全局模式、目标类别），提供求解器标注的解决方案，从可行性、解的最优性和推理效率三个维度全面评估LLM

Result: 实验表明，高性能LLM在小规模实例上表现出良好的可行性和解质量，但随着实例规模增大，两者都会退化，即使使用更多token进行推理也是如此；集合类任务相对容易，而图结构问题和瓶颈目标更容易失败

Conclusion: LLM在组合优化方面存在局限性，特别是在处理大规模实例和复杂结构问题时，需要进一步研究提升LLM在这类问题上的推理能力

Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \textbf{N}atural \textbf{L}anguage \textbf{C}ombinatorial \textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.

</details>


### [149] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: 论文提出TIDE框架，用于诊断评估LLM智能体在测试时改进(TTI)中的表现，通过三个维度分析任务完成效率、循环行为和记忆负担。


<details>
  <summary>Details</summary>
Motivation: 当前对自主LLM智能体通过与环境迭代交互实现性能提升（测试时改进TTI）的机制理解不足，现有评估指标无法捕捉任务优化效率、错误行为适应和工作记忆效用等关键维度。

Method: 提出TIDE（测试时改进诊断评估）框架，这是一个智能体和环境无关的评估系统，将TTI分解为三个相互关联的维度：任务完成的时间动态、循环行为约束和累积记忆负担。

Result: 通过跨多种智能体和环境的广泛实验，TIDE揭示出提升智能体性能不仅需要扩展内部推理能力，更需要显式优化智能体与环境的交互动态。

Conclusion: TIDE框架为理解和诊断LLM智能体的测试时改进提供了系统化的评估工具，强调了优化智能体-环境交互动态的重要性，而不仅仅是增强内部推理能力。

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [150] [More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression](https://arxiv.org/abs/2602.02199)
*Aryan Sood,Tanvi Sharma,Vansh Agrawal*

Main category: cs.AI

TL;DR: LASER-KV 是一种新的 KV 缓存压缩框架，通过分层累积选择和精确 LSH 召回机制，在严格累积预算策略下实现高效压缩，相比现有方法在长上下文任务中保持更稳定的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型理论上支持长上下文窗口，但实际部署受限于 KV 缓存内存的线性增长。现有压缩方法通过剪枝机制在语义召回和内存效率之间进行权衡，导致性能下降。

Method: 提出 LASER-KV 框架，采用分层累积选择策略，通过保护除数(n)控制块级累积，结合精确 LSH 召回机制，避免滑动窗口伪影，实现 KV 缓存的极限压缩。

Result: 在 Babilong 基准测试中，现有压缩方法性能下降 15-30%，而 LASER-KV 保持稳定性能，在 128k 上下文长度下准确率提升高达 10%。

Conclusion: 研究结果表明，仅依赖注意力分数作为 token 效用的代理是不够的，LASER-KV 通过新的压缩策略挑战了这一普遍假设，为长上下文处理提供了更有效的解决方案。

Abstract: While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.

</details>


### [151] [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304)
*Martino Ciaperoni,Marzio Di Vece,Luca Pappalardo,Fosca Giannotti,Francesco Giannini*

Main category: cs.AI

TL;DR: 提出比较性可解释AI框架（Δ-XAI），用于解释模型干预（如微调、强化学习等）引起的行为变化，而非孤立分析单个模型。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在扩展、微调、强化学习或上下文学习后会出现行为变化，传统XAI方法只能分析单个检查点，无法解释不同检查点间的内部变化，需要新的解释框架。

Method: 提出比较性XAI（Δ-XAI）框架，包含一组设计解释方法时应考虑的需求，提供可能的流程管道，并将其与需求关联，通过具体Δ-XAI实验展示方法工作原理。

Result: 建立了Δ-XAI框架，明确了比较性解释的核心目标应是参考模型与干预模型之间的干预引起的变化，而非孤立分析任何单个模型。

Conclusion: 行为变化应通过比较性方法解释，Δ-XAI框架为此提供了理论基础和方法指导，有助于更好地理解模型干预引起的行为转变。

Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.

</details>


### [152] [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313)
*Changming Li,Kaixing Zhang,Haoyun Xu,Yingdong Shi,Zheng Zhang,Kaitao Song,Kan Ren*

Main category: cs.AI

TL;DR: 提出IPG框架，通过传播基于结果的信号来定位语言模型复杂推理的内部机制


<details>
  <summary>Details</summary>
Motivation: 现有方法难以精确定位复杂推理机制或捕捉从模型内部工作到推理输出的顺序影响，需要新的解释性方法

Method: 提出集成策略梯度（IPG）框架，通过将基于结果的信号（如推理后准确性）沿模型推理轨迹反向传播，将推理行为归因于模型内部组件

Result: 实证评估显示该方法实现了更精确的定位，并能可靠地调节不同推理模型的推理行为（如推理能力、推理强度）

Conclusion: IPG框架基于结果导向和顺序影响感知原则，能够识别对推理行为有顺序贡献的组件，为理解语言模型推理机制提供了新方法

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.

</details>


### [153] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

TL;DR: M2CL通过为每个智能体学习上下文生成器，动态生成每轮讨论的上下文指令，解决多智能体讨论中的不一致性问题，显著提升性能20%-50%


<details>
  <summary>Details</summary>
Motivation: 当前多智能体讨论方法容易遭受讨论不一致性问题，由于智能体个体上下文之间的不对齐，导致LLM无法达成一致的解决方案

Method: 提出多LLM上下文学习方法(M2CL)，为每个智能体训练上下文生成器，通过自动信息组织和精炼动态生成每轮讨论的上下文指令，采用精心设计的自适应机制控制上下文一致性和输出差异

Result: 在学术推理、具身任务和移动控制等挑战性任务上，M2CL性能显著超越现有方法20%-50%，同时具有良好的可迁移性和计算效率

Conclusion: M2CL通过动态上下文生成有效解决了多智能体讨论中的不一致性问题，使LLM能够避免过早收敛于多数噪声并逐步达成正确共识

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [154] [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369)
*Yaolun Zhang,Yiran Wu,Yijiong Yu,Qingyun Wu,Huazheng Wang*

Main category: cs.AI

TL;DR: Live-Evo是一个在线自进化记忆系统，通过经验库和元指导库分离"发生了什么"和"如何使用"，实现从持续数据流中学习，在真实分布漂移下表现稳健。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的记忆系统大多基于静态训练/测试集开发，通过折叠静态基准来近似在线学习，在真实分布漂移和持续反馈下表现脆弱。需要真正的在线自进化记忆系统来应对动态环境。

Method: Live-Evo采用双库架构：经验库存储原始交互经验，元指导库存储如何利用经验的指导原则。系统通过权重管理在线记忆，根据反馈强化有帮助的经验，降低误导或过时经验的权重，模拟人类记忆的强化和衰减机制。

Result: 在10周的Prophet Arena基准测试中，Live-Evo将Brier分数提高了20.8%，市场回报增加了12.9%。在深度研究基准测试中也表现出持续优于强基线的稳定增益。

Conclusion: Live-Evo展示了在线自进化记忆系统的有效性，能够从持续数据流中学习并适应分布漂移，为LLM代理在动态环境中的应用提供了实用解决方案。

Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.

</details>


### [155] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

TL;DR: BELLA是一个预算高效的LLM选择框架，通过自动技能分析推荐最优模型，在保证性能的同时控制成本。


<details>
  <summary>Details</summary>
Motivation: 当前标准基准测试只报告聚合指标，无法揭示任务所需的具体能力，也无法判断更便宜的模型是否足够。LLM从业者需要在不浪费资金的情况下为任务选择合适的模型。

Method: BELLA采用三阶段方法：1) 通过基于批评的剖析分解LLM输出并提取细粒度技能；2) 将技能聚类到结构化能力矩阵中；3) 使用多目标优化选择模型，在预算约束下最大化性能。

Result: BELLA提供自然语言推理的推荐，提供当前黑盒路由系统缺乏的透明度。该框架使从业者能够为部署LLM做出原则性的成本-性能权衡。

Conclusion: BELLA框架通过可解释的基于技能的模型选择，使从业者能够在不浪费资金的情况下为任务选择最优LLM，特别适用于金融推理等具有多样化技能需求和模型成本变化的领域。

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [156] [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416)
*Ankur Samanta,Akshayaa Magesh,Ayush Jain,Kavosh Asadi,Youliang Yu,Daniel Jiang,Boris Vidolov,Kaveh Hassani,Paul Sajda,Jalaj Bhandari,Yonathan Efroni*

Main category: cs.AI

TL;DR: 本文提出Thought-ICS框架，通过离散化思维步骤实现语言模型的自我错误定位与修正，相比传统方法显著提升自我修正能力


<details>
  <summary>Details</summary>
Motivation: 当前语言模型自我修正能力仍然有限，本文探索语言模型能否显式定位错误推理，以构建能有效自我修正的AI系统。受人类大脑在离散决策点监控错误并重新采样替代方案的启发，研究如何让模型在推理结构中可靠地定位错误。

Method: 提出Thought-ICS（迭代修正思维采样）框架：1）将推理结构化为离散、语义连贯的思维步骤；2）每次只生成一个完整的离散思维（代表模型的深思熟虑决策）；3）创建自然边界以精确错误定位；4）验证后定位第一个错误步骤；5）回溯到最后一个正确点生成替代推理。

Result: 1）在传统非结构化思维链推理中模型无法可靠定位错误，但在结构化思维步骤中能可靠定位；2）当要求修正被oracle验证为错误的推理时，Thought-ICS实现20-40%的自我修正提升；3）在完全自主无外部验证的设置中，优于当代自我修正基线方法。

Conclusion: 通过将推理结构化为离散思维步骤，语言模型能够可靠地定位错误，Thought-ICS框架为构建有效自我修正的AI系统提供了可行路径，显著提升了模型的自我修正能力。

Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

</details>


### [157] [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: SafeGround是一个用于GUI grounding的不确定性感知框架，通过分布感知的不确定性量化和校准过程，实现风险感知预测和统计保证的FDR控制。


<details>
  <summary>Details</summary>
Motivation: GUI grounding中的错误预测可能导致代价高昂且难以逆转的操作（如错误的支付批准），因此需要提高模型可靠性并控制风险。

Method: SafeGround采用分布感知的不确定性量化方法捕捉模型输出的空间分散性，通过校准过程推导出具有统计保证的假发现率（FDR）控制的测试时决策阈值。

Result: 在ScreenSpot-Pro基准测试中，SafeGround的不确定性度量优于现有基线，校准阈值实现了严格的风险控制，系统级准确率相比Gemini-only推理最高提升5.38个百分点。

Conclusion: SafeGround为GUI grounding模型提供了一个有效的风险控制框架，通过不确定性量化和统计校准显著提高了系统可靠性和准确性。

Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference.

</details>


### [158] [Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling](https://arxiv.org/abs/2602.02453)
*Andong Chen,Wenxin Zhu,Qiuyu Ding,Yuchen Song,Muyun Yang,Tiejun Zhao*

Main category: cs.AI

TL;DR: 提出"Thinking with Comics"视觉推理范式，用漫画作为图像和视频之间的高信息密度媒介，在保持时间结构和叙事连贯性的同时降低推理成本


<details>
  <summary>Details</summary>
Motivation: 现有模态推理存在局限：静态图像难以表示时间结构，而视频引入大量冗余和计算成本。需要一种既能保留时间信息又高效的视觉推理媒介

Method: 提出基于漫画的视觉推理范式，将漫画作为图像和视频之间的中间表示。系统研究两种基于漫画的推理路径，并在多种推理任务和长上下文理解任务上进行评估

Result: 实验结果显示：1) Thinking with Comics在多步时间和因果推理任务上优于Thinking with Images；2) 比Thinking with Video更高效；3) 不同漫画叙事结构和风格对任务性能有持续影响

Conclusion: 漫画作为有效的中间视觉表示，能够改善多模态推理，在保留时间结构的同时降低计算成本，为视觉推理提供了新的高效范式

Abstract: Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.

</details>


### [159] [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455)
*Han Bao,Zheyuan Zhang,Pengcheng Jing,Zhengqing Yuan,Kaiwen Shi,Yanfang Ye*

Main category: cs.AI

TL;DR: Drift-Bench是首个评估自主智能体在输入故障下多轮澄清能力的诊断基准，通过状态导向和服务导向的执行环境测试智能体的语用能力，发现现有模型在这些故障下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型向自主智能体过渡时，用户输入经常违反合作假设（如隐含意图、缺失参数、错误预设或模糊表达），产生文本评估无法捕捉的执行风险。现有基准通常假设指令明确或仅限于文本单轮澄清，无法衡量有执行风险的多轮消歧能力。

Method: 引入Drift-Bench基准，基于经典沟通理论提供统一的合作故障分类法，采用角色驱动的用户模拟器和Rise评估协议，在状态导向和服务导向执行环境中测试多轮澄清能力。

Result: 实验显示智能体在这些故障下性能大幅下降，澄清效果因用户角色和故障类型而异，揭示了现有模型在真实执行环境中的脆弱性。

Conclusion: Drift-Bench连接了澄清研究和智能体安全评估，能够系统诊断可能导致不安全执行的故障，为自主智能体的安全性和鲁棒性评估提供了重要工具。

Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.

</details>


### [160] [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465)
*Jana Zeller,Thaddäus Wiedemer,Fanfei Li,Thomas Klein,Prasanna Mayilvahanan,Matthias Bethge,Felix Wichmann,Ryan Cotterell,Wieland Brendel*

Main category: cs.AI

TL;DR: 视觉思维（如心理意象）目前无法提升前沿多模态模型的推理能力，尽管模型具备文本推理能力和生成正确视觉的能力，但存在生成错误累积和无法利用视觉化的问题。


<details>
  <summary>Details</summary>
Motivation: 随着前沿模型从仅能处理视觉信息的MLLMs向能够原生交错生成的UMMs转变，研究者希望探索使用中间视觉化作为推理辅助（类似人类心理意象）的潜力。核心是评估模型形成、维护和操作视觉表征的能力。

Method: 开发了MentisOculi——一个程序化、分层的多步推理问题套件，专门设计来挑战前沿模型。评估了从潜在标记到显式生成图像等多种视觉策略，并特别分析了UMMs的能力限制。

Result: 视觉策略普遍未能提升模型性能。UMMs虽然具备解决任务的文本推理能力，有时也能生成正确视觉，但存在生成错误累积问题，甚至无法利用真实视觉化。视觉思维目前尚未能有效辅助模型推理。

Conclusion: 尽管视觉思维具有内在吸引力，但目前尚未能提升模型推理能力。MentisOculi为分析和弥合这一差距提供了必要基础，未来需要在不同模型家族中进一步研究。

Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.

</details>


### [161] [Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts](https://arxiv.org/abs/2602.02468)
*Aiden Yiliu Li,Xinyue Hao,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Avenir-Web：通过混合定位专家、经验模仿规划和任务跟踪检查表，在真实网页交互中实现开源SOTA的网页代理


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型有所进展，但自主网页代理在执行复杂动态网页界面的长时程任务时仍不可靠。现有代理存在元素定位不准确、缺乏站点特定程序知识、长时任务跟踪和记忆不稳定等问题，特别是在复杂DOM结构上操作时。

Method: 1. 混合定位专家（Mixture of Grounding Experts）用于准确元素定位；2. 经验模仿规划（Experience-Imitation Planning）整合程序先验知识；3. 任务跟踪检查表结合自适应记忆，实现跨不同UI范式的鲁棒交互。

Result: 在Online-Mind2Web基准测试（实时用户中心网页任务）中，Avenir-Web显著超越先前开源代理，达到与顶级专有模型相当的性能，建立了实时网站可靠网页代理的新开源SOTA。

Conclusion: Avenir-Web通过创新的混合定位、程序知识整合和自适应记忆机制，解决了网页代理在复杂动态界面中的关键挑战，为开源网页代理设立了新的性能标准。

Abstract: Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.

</details>


### [162] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

TL;DR: 通过引入"身份桥"数据正则化（如"A→A"形式），可以显著缓解LLM的"逆转诅咒"问题，使模型能够学习到更高层次的规则而非仅记忆事实。


<details>
  <summary>Details</summary>
Motivation: 尽管自回归大语言模型在许多复杂任务上表现出色，但在简单逻辑推理如"逆转诅咒"上仍会失败。传统观点认为这是自回归因果LLM的固有根本限制，但本文挑战这一观点，试图证明通过数据层面的简单调整可以缓解这一问题。

Method: 提出"身份桥"数据正则化方法，在训练数据中添加"A→A"形式的简单数据（如"爱丽丝的名字是爱丽丝"）。理论上分析梯度下降的隐式偏差，证明即使单层Transformer也能打破逆转诅咒。实证上在1B参数的预训练语言模型上进行微调实验。

Result: 使用身份桥数据正则化的模型在逆转任务上达到40%的成功率，而仅使用前向知识数据训练的模型成功率接近零。这显著缓解了逆转诅咒问题。

Conclusion: 逆转诅咒并非自回归LLM的固有根本限制，通过简单的数据正则化方法即可显著改善。这为LLM学习更高层次规则提供了理论依据和低成本实践路径。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>


### [163] [AgentRx: Diagnosing AI Agent Failures from Execution Trajectories](https://arxiv.org/abs/2602.02475)
*Shraddha Barke,Arnav Goyal,Alind Khare,Avaljot Singh,Suman Nath,Chetan Bansal*

Main category: cs.AI

TL;DR: 提出了AGENTRX框架，用于自动诊断AI代理失败轨迹中的关键失败步骤和类别，并发布了包含115个失败轨迹的基准数据集


<details>
  <summary>Details</summary>
Motivation: AI代理的失败难以定位，因为执行具有概率性、长时程、多代理且受噪声工具输出影响。现有方法缺乏系统化的失败诊断框架

Method: 提出了AGENTRX框架：1) 合成约束条件；2) 逐步评估约束；3) 生成可审计的约束违反验证日志；4) 使用LLM作为法官基于日志定位关键失败步骤和类别。还发布了包含115个失败轨迹的手动标注基准数据集

Result: AGENTRX在三个领域（结构化API工作流、事件管理、开放式网络/文件任务）中，在步骤定位和失败归因方面优于现有基线方法

Conclusion: AGENTRX提供了一个有效的自动化、领域无关的诊断框架，能够准确识别AI代理失败的关键步骤和类别，有助于降低失败分析的人工成本

Abstract: AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [164] [Information Propagation and Encoding in Solids: A Quantitative Approach Towards Mechanical Intelligence](https://arxiv.org/abs/2602.00140)
*Peerasait Prachaseree,Emma Lejeune*

Main category: cs.IT

TL;DR: 该研究提出了一种量化弹性体中信息传播的框架，将物理结构作为信息处理媒介，连接力学现象与信息理论。


<details>
  <summary>Details</summary>
Motivation: 传统工程系统将机械功能与信息处理分离，而生物系统可以利用物理结构进行信息处理。研究旨在探索如何将信息处理能力嵌入机械结构中，但缺乏量化评估框架。

Method: 使用弹性体作为模型系统，应用信息论工具将弹性域视为信息编码器，量化信息从施加载荷到离散传感器位置的传输。连接这些度量与力学现象（如圣维南效应和主应力线）。

Result: 建立了弹性体中信息传播的量化框架，展示了如何通过几何形状和架构材料调节信息传输，使弹性域能够传输或阻断信息。

Conclusion: 该工作推进了机械智能的可量化度量和基准任务，支持机械嵌入式信息处理的比较设计，为物理计算系统提供了理论基础。

Abstract: Engineered systems typically separate mechanical function from information processing, whereas biological systems can exploit physical structure as a medium for information processing and computation. Motivated by this contrast, recent work in mechanics has explored embedding information-processing capabilities directly into mechanical structures. However, quantitative frameworks for evaluating such capabilities remain limited. Here we address a foundational question: how does information propagate through a solid body? Using elastic bodies as a model system, we apply information-theoretic tools to treat an elastic domain as an information encoder and quantify how information transmits from applied loads to discrete sensor locations. We further connect these measures to familiar mechanical phenomena, including Saint-Venant's effect and principal stress lines. Moving toward design, we show how geometry and architected materials can tune transmission, enabling elastic domains to either transmit or block information. Overall, this work advances quantifiable metrics and benchmark tasks for mechanical intelligence, supporting comparable designs of mechanically embodied information processing.

</details>


### [165] [Preemptive Scheduling for Age of Job Minimization in Task-Specific Machine Networks](https://arxiv.org/abs/2602.02435)
*Subhankar Banerjee,Sennur Ulukus*

Main category: cs.IT

TL;DR: 提出基于作业年龄的新颖及时性度量，针对多网络作业分配系统设计调度策略，包括最大权重、Whittle指数和净收益最大化策略，在不同系统规模下各有优劣。


<details>
  <summary>Details</summary>
Motivation: 多网络作业分配系统中，由于资源限制无法同时服务所有作业，需要设计调度策略来优化作业的及时性，为此引入基于信息年龄启发的"作业年龄"度量。

Method: 1) 提出最大权重策略（最小化单步Lyapunov漂移）；2) 几何分布下推导Whittle指数策略；3) 一般分布下提出Whittle指数加最大权重回退策略；4) 研究净收益最大化策略。

Result: WIMWF策略在一般作业完成时间分布下表现最佳；小系统中最大权重策略优于NGM策略，但NGM策略随系统规模扩大而改善并渐近优于最大权重策略；几何服务时间下WI策略在所有系统规模中年龄最低。

Conclusion: 针对不同作业完成时间分布和系统规模，提出了相应的优化调度策略，其中WIMWF策略在一般分布下表现最优，而不同策略在不同系统规模下各有适用场景。

Abstract: We consider a time-slotted job-assignment system consisting of a central server, $N$ task-specific networks of machines, and multiple users. Each network specializes in executing a distinct type of task. Users stochastically generate jobs of various types and forward them to the central server, which routes each job to the appropriate network of machines. Due to resource constraints, the server cannot serve all users' jobs simultaneously, which motivates the design of scheduling policies with possible preemption. To evaluate scheduling performance, we introduce a novel timeliness metric, the age of job, inspired by the well-known metric, the age of information. We study the problem of minimizing the long-term weighted average age of job. We first propose a max-weight policy by minimizing the one-step Lyapunov drift and then derive the Whittle index (WI) policy when the job completion times of the networks of machines follow geometric distributions. For general job completion time distributions, we introduce a Whittle index with max-weight fallback (WIMWF) policy. We also investigate the Net-gain maximization (NGM) policy. Numerically, we show that the proposed WIMWF policy achieves the best performance in the general job completion time setting. We also observe a scaling trend: two different max-weight policies can outperform the NGM policy in small systems, whereas the NGM policy improves as we scale the system size and becomes asymptotically better than max-weight policies. For geometric service times, the WI policy yields the lowest age across all considered system sizes.

</details>


### [166] [Semantic-Aware Command and Control Transmission for Multi-UAVs](https://arxiv.org/abs/2602.00142)
*Boya Li,Xiaonan Liu,Dongzhu Liu,Dusit Niyato,Zhu Han*

Main category: cs.IT

TL;DR: 提出基于语义感知的无人机群C&C传输框架，利用语义相似度实现多播传输，优化资源分配以提升传输效率


<details>
  <summary>Details</summary>
Motivation: 随着无人机数量增加和无线数据爆炸式增长，传统的比特导向通信网络已接近香农容量极限，无法满足无人机指挥控制（C&C）传输所需的超可靠低延迟通信（URLLC）服务质量要求

Method: 1) 利用语义相似度衡量每个无人机在连续传输时间间隔内C&C消息的变化，并捕捉无人机间C&C消息的相关性，实现多播传输；2) 基于语义相似度和无人机命令重要性设计触发函数量化服务质量；3) 开发近端策略优化（PPO）算法，联合确定传输模式（单播/多播/空闲）和基站与无人机间有限资源块的分配

Result: 实验结果表明，提出的语义感知框架相比比特导向无人机传输显著提高了传输效率和效果

Conclusion: 通过语义感知方法优化无人机C&C传输，在有限无线资源下实现了更好的服务质量，为无人机通信网络提供了更高效的解决方案

Abstract: Uncrewed aerial vehicles (UAVs) have played an important role in the low-altitude economy and have been used in various applications. However, with the increasing number of UAVs and explosive wireless data, the existing bit-oriented communication network has approached the Shannon capacity, which cannot satisfy the quality of service (QoS) with ultra-reliable low-latency communication (URLLC) requirements for command and control (C\&C) transmission in bit-oriented UAV communication networks. To address this issue, we propose a novel semantic-aware C\&C transmission for multi-UAVs under limited wireless resources. Specifically, we leverage semantic similarity to measure the variation in C\&C messages for each UAV over continuous transmission time intervals (TTIs) and capture the correlation of C\&C messages among UAVs, enabling multicast transmission. Based on the semantic similarity and the importance of UAV commands, we design a trigger function to quantify the QoS of UAVs. Then, to maximize the long-term QoS and exploit multicast opportunities of C\&C messages induced by semantic similarity, we develop a proximal policy optimization (PPO) algorithm to jointly determine the transmission mode (unicast/multicast/idle) and the allocation of limited resource blocks (RBs) between a base station (BS) and UAVs. Experimental results show that our proposed semantic-aware framework significantly increases transmission efficiency and improves effectiveness compared with bit-oriented UAV transmission.

</details>


### [167] [The structure and enumeration of periodic binary sequences with high nonlinear complexity](https://arxiv.org/abs/2602.01134)
*Qin Yuan,Chunlei Li,Xiangyong Zeng*

Main category: cs.IT

TL;DR: 本文研究了非线性复杂度≥3n/4的n周期二进制序列的结构，并给出了精确计数公式


<details>
  <summary>Details</summary>
Motivation: 非线性复杂度是评估序列随机性的重要指标，定义为能生成给定序列的最短反馈移位寄存器的长度。研究高非线性复杂度序列的结构有助于理解序列的随机性特征，并为密码学应用提供理论基础。

Method: 首先刻画了非线性复杂度≥3n/4的n周期二进制序列的结构特征，然后基于这些结构特征推导出此类序列的精确计数公式。

Result: 获得了非线性复杂度≥3n/4的n周期二进制序列的完整结构特征，并确定了此类序列数量的精确枚举公式。

Conclusion: 本文成功刻画了高非线性复杂度周期序列的结构，并给出了精确计数，为序列随机性分析和密码学应用提供了理论支持。

Abstract: Nonlinear complexity, as an important measure for assessing the randomness of sequences, is defined as the length of the shortest feedback shift registers that can generate a given sequence. In this paper, the structure of n-periodic binary sequences with nonlinear complexity larger than or equal to 3n/4 is characterized. Based on their structure, an exact enumeration formula for the number of such periodic sequences is determined.

</details>


### [168] [On the Palindromic/Reverse-Complement Duplication Correcting Codes](https://arxiv.org/abs/2602.01151)
*Yubo Sun,Gennian Ge*

Main category: cs.IT

TL;DR: 该论文研究DNA存储中的纠错码，针对反转互补重复和回文重复两种错误模式，提出了多种编码构造和理论界限。


<details>
  <summary>Details</summary>
Motivation: 受体内DNA存储应用驱动，研究能够纠正重复错误的编码。DNA存储中常出现反转互补重复和回文重复错误，需要设计相应的纠错码。

Method: 1. 构造了单冗余符号的显式编码，可纠正任意数量的反转互补重复或回文重复（要求重复长度≥3⌈log_q n⌉且不相交）
2. 推导了Gilbert-Varshamov界限，证明最优冗余上界为2log_q n + log_q log_q n + O(1)
3. 对于q≥4，提出了两种纠正t个长度为1的反转互补重复的显式构造：第一种冗余2tlog_q n + O(log_q log_q n)，第二种改进为(2t-1)log_q n + O(log_q log_q n)

Result: 1. 成功构造了单冗余符号的编码，可处理任意数量的长重复错误
2. 建立了理论界限，证明最优冗余的上界
3. 针对长度1的重复错误，提出了两种具有不同复杂度-冗余权衡的编码方案

Conclusion: 该论文为DNA存储中的重复错误纠正提供了系统的编码理论和构造方法，包括长重复和短重复两种情况，建立了理论界限并给出了实用的编码方案。

Abstract: Motivated by applications in in-vivo DNA storage, we study codes for correcting duplications. A reverse-complement duplication of length $k$ is the insertion of the reversed and complemented copy of a substring of length $k$ adjacent to its original position, while a palindromic duplication only inserts the reversed copy without complementation. We first construct an explicit code with a single redundant symbol capable of correcting an arbitrary number of reverse-complement duplications (respectively, palindromic duplications), provided that all duplications have length $k \ge 3\lceil \log_q n \rceil$ and are disjoint. Next, we derive a Gilbert-Varshamov bound for codes that can correct a reverse-complement duplication (respectively, palindromic duplication) of arbitrary length, showing that the optimal redundancy is upper bounded by $2\log_q n + \log_q\log_q n + O(1)$. Finally, for $q \ge 4$, we present two explicit constructions of codes that can correct $t$ length-one reverse-complement duplications. The first construction achieves a redundancy of $2t\log_q n + O(\log_q\log_q n)$ with encoding complexity $O(n)$ and decoding complexity $O\big(n(\log_2 n)^4\big)$. The second construction achieves an improved redundancy of $(2t-1)\log_q n + O(\log_q\log_q n)$, but with encoding and decoding complexities of $O\big(n \cdot \mathrm{poly}(\log_2 n)\big)$.

</details>


### [169] [A class of pseudorandom sequences From Function Fields](https://arxiv.org/abs/2602.01154)
*Xiaofeng Liu,Jun Zhang,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 利用代数函数域上的指数和界，研究了一类p元序列的周期、线性复杂度、线性复杂度轮廓、r-模式分布、周期相关性和非线性复杂度，推广了之前的构造。


<details>
  <summary>Details</summary>
Motivation: 受到Hu等人在循环椭圆函数域上构造伪随机序列以及Xing等人在函数域上构造低相关、大线性跨度二进制序列的启发，本文旨在推广这些构造并研究更一般的p元序列的密码学性质。

Method: 利用Weil和Deligne推导的代数函数域上指数和的界，分析一类p元序列的密码学性质，包括周期、线性复杂度、线性复杂度轮廓、r-模式分布、周期相关性和非线性复杂度。

Result: 对推广后的p元序列进行了全面的密码学分析，得到了关于周期、线性复杂度、线性复杂度轮廓、r-模式分布、周期相关性和非线性复杂度的理论结果。

Conclusion: 通过代数函数域的理论工具，成功推广了之前的序列构造，并建立了这些序列的密码学性质的理论保证，为伪随机序列设计提供了新的理论框架。

Abstract: Motivated by the constructions of pseudorandom sequences over the cyclic elliptic function fields by Hu \textit{et al.} in \text{[IEEE Trans. Inf. Theory, 53(7), 2007]} and the constructions of low-correlation, large linear span binary sequences from function fields by Xing \textit{et al.} in \text{[IEEE Trans. Inf. Theory, 49(6), 2003]}, we utilize the bound derived by Weil \text{[Basic Number Theory, Grund. der Math. Wiss.,
  Bd 144]} and Deligne \text{[ Lecture Notes in Mathematics, vol. 569 (Springer, Berlin, 1977)]} for the exponential sums over the general algebraic function fields and study the periods, linear complexities, linear complexity profiles, distributions of $r-$patterns, period correlation and nonlinear complexities for a class of $p-$ary sequences that generalize the constructions in \text{[IEEE Trans. Inf. Theory, 49(6), 2003]} and [IEEE Trans. Inf. Theory, 53(7), 2007].

</details>


### [170] [Reducing ORBGRAND Latency via Partial Gaussian Elimination](https://arxiv.org/abs/2602.01174)
*Li Wan,Wenyi Zhang*

Main category: cs.IT

TL;DR: 提出基于消除辅助的ORBGRAND解码方案，通过整合RMRE位和部分高斯消除过滤机制，减少解码延迟，适用于超可靠低延迟通信场景。


<details>
  <summary>Details</summary>
Motivation: ORBGRAND虽然通过利用LLR排序实现了GRAND的并行实现，但在不利信道条件下仍存在高尾部延迟问题，限制了其在实时系统中的应用。

Method: 提出消除辅助的ORBGRAND方案，整合RMRE（最可靠错误位秩）和部分高斯消除过滤机制。方案将共享相同RMRE的错误模式分组并联合验证，一旦识别出有效错误模式就恢复ORBGRAND搜索，利用先前的高斯消除步骤过滤不必要的猜测。

Result: 仿真结果显示，相比原始ORBGRAND，消除辅助的ORBGRAND过滤掉超过50%的错误模式，相应降低整体计算复杂度，且没有块错误率损失。

Conclusion: 该方法显著减少需要测试的错误模式数量，降低平均和最坏情况延迟，同时保持纠错性能，适用于超可靠低延迟通信场景。

Abstract: Guessing Random Additive Noise Decoding (GRAND) is a universal framework for decoding all block codes by testing candidate error patterns (EPs). Ordered Reliability Bits GRAND (ORBGRAND) facilitates parallel implementation of GRAND by exploiting log-likelihood ratio (LLR) rankings but still suffers from high tail latency under unfavorable channel conditions, limiting its use in real-time systems.
  We propose an elimination-aided ORBGRAND scheme that reduces decoding latency by integrating the Rank of the Most Reliable Erroneous (RMRE) bit with a partial Gaussian-elimination (GE) filtering mechanism. The scheme groups and jointly verifies EPs that share the same RMRE, and once a valid EP is identified, the ORBGRAND search is resumed. By leveraging prior GE steps to filter out unnecessary guesses, this approach significantly reduces the number of EPs to be tested, thereby lowering both average and worst-case latency while maintaining error-correction performance.
  Simulation results show that compared to the original ORBGRAND, the elimination-aided ORBGRAND filters out more than 50\% of EPs and correspondingly reduce overall computational complexity, all with no loss in block error rate. This demonstrates that this approach is suitable for ultra-reliable low-latency communication scenarios.

</details>


### [171] [L-Moment-Based LOS and NLOS Channel Characterization via Four-parameter Kappa Distribution for AoA BLE CTE Measurements](https://arxiv.org/abs/2602.01229)
*Hamed Talebian,Aamir Mahmood,Mikael Gidlund*

Main category: cs.IT

TL;DR: 该研究通过配对比测实验收集BLE CTE IQ数据，验证LOS/NLOS统计特性差异，提出基于L矩比的特征提取方法，显著改善NLOS条件下的分布拟合效果。


<details>
  <summary>Details</summary>
Motivation: 现有BLE方向查找数据集缺乏严格配对的LOS/NLOS IQ测量数据，且常用的平坦衰落模型在室内多径环境中表现不佳，需要更准确的统计模型来区分LOS/NLOS传播条件。

Method: 使用商用BLE模块进行配对比测实验，收集132000个标记CTE数据包；采用稳健预处理去除异常CTE；通过假设检验验证特征可分性；计算L矩比并在L矩比图中分析；使用kappa族分布进行拟合；应用自监督聚类到L矩统计量。

Result: IQ功率特征在LOS/NLOS条件下具有强可分离性，所有均值差异均统计显著；NLOS子集表现出更重的尾部和更强的不对称性；kappa族分布显著改善拟合优度，NLOS的L矩比图差异最小且标准化L峰度偏差接近零；L矩统计量相比乘积矩提供更可分离的表示。

Conclusion: L矩比分析为BLE CTE IQ数据的LOS/NLOS区分提供了有效的统计框架，kappa族分布能更好地捕捉NLOS条件下的重尾特性，L矩统计量相比传统矩方法在特征表示上更具优势。

Abstract: Bluetooth Low Energy (BLE) CTE transmissions provide in-phase and quadrature (IQ) samples whose empirical statistics are strongly governed by the propagation regime. in particular, the distributions differ markedly between line-of-sight (LOS) and non-line-of-sight (NLOS) conditions. In NLOS, multipath-induced distortions typically degrade Angle-of-Arrivial (AoA) estimation accuracy. Existing BLE direction finding datasets rarely provide tightly controlled, IQ-level paired LOS and NLOS measurements with rigorous statistical validation, and commonly used flat-fading models can be inadequate for cluttered indoor environments exhibiting heavy-tailed power distributions. To address these limitations, we conduct a paired-geometry BLE AoA measurement campaign using an off-the-shelf module, collecting 132000 labeled CTE packets under matched anchor-tag conditions. A robust preprocessing stage removes anomalous CTEs using combined univariate and multivariate criteria. Feature-wise hypothesis tests on IQ-derived power features confirm strong LOS and NLOS separability. All mean differences are statistically significant; additionally, 92 percent of feature-wise variance differences are significant. We further compute L-moment ratios (LMRs) and analyze them in the L-moment Ratio Diagram (LMRD), showing that NLOS subsets exhibit markedly heavier tails and stronger asymmetry than LOS. Kappa-family distributions fitted from LMRs provide substantially improved dual scored L--moment goodness-of-fit (GoF), Specifically, for NLOS, which is the smallest discrepancy in the LMRD and a near-zero standardized L-kurtosis deviation. As a practice, we apply a self-supervised clustering to L-moment statistics, achieving a more separable representation, compared to product moments.

</details>


### [172] [MDS matrices from skew polynomials with automorphisms and derivations](https://arxiv.org/abs/2602.01383)
*Atif Ahmad Khan,Shakir Ali,Elif Segah Oztas,Abhishek Kesarwani*

Main category: cs.IT

TL;DR: 本文提出了一种使用斜多项式环构造MDS矩阵的新方法，引入了δ_θ-循环矩阵概念，并构建了准递归MDS矩阵，相比现有方法有改进。


<details>
  <summary>Details</summary>
Motivation: MDS矩阵在编码理论和对称密钥密码学中具有重要作用，因为它们提供最优扩散特性。现有构造方法有限，需要探索新的数学框架来构建更优的MDS矩阵。

Method: 使用斜多项式环F_q[X;θ,δ]构造MDS矩阵，其中θ是自同构，δ是θ-导数。引入δ_θ-循环矩阵概念，研究其结构特性，推导这些矩阵成为对合且满足MDS性质的充要条件。在斜多项式环F_q[X;θ]中构造与伴随矩阵相关的准递归MDS矩阵。

Result: 成功构造了δ_θ-循环矩阵作为经典构造的推广，建立了准递归MDS矩阵，这些矩阵被证明是对合的，相比文献中先前报道的准对合构造有严格改进。提供了多个说明性结果和示例。

Conclusion: 斜多项式环为构造MDS矩阵提供了新的数学框架，δ_θ-循环矩阵和准递归MDS矩阵的构造方法在理论和应用上都有重要意义，为编码理论和密码学中的扩散层设计提供了新工具。

Abstract: Maximum Distance Separable (MDS) matrices play a central role in coding theory and symmetric-key cryptography due to their optimal diffusion properties. In this paper, we present a construction of MDS matrices using skew polynomial rings \( \mathbb{F}_q[X;θ,δ] \), where \( θ\) is an automorphism and \( δ\) is a \( θ\)-derivation on \( \mathbb{F}_q \). We introduce the notion of \( δ_θ \)-circulant matrices and study their structural properties. Necessary and sufficient conditions are derived under which these matrices are involutory and satisfy the MDS property. The resulting $δ_θ$-circulant matrix can be viewed as a generalization of classical constructions obtained in the absence of $θ$-derivations. One of the main contribution of this work is the construction of quasi recursive MDS matrices. In the setting of the skew polynomial ring $\mathbb{F}_q[X;θ]$, we construct quasi recursive MDS matrices associated with companion matrices.
  These matrices are shown to be involutory, yielding a strict improvement over the quasi-involutory constructions previously reported in the literature. Several illustrative results and examples are also provided.

</details>


### [173] [Design of Root Protograph LDPC Codes Simultaneously Achieving Full Diversity and High Coding Gain](https://arxiv.org/abs/2602.01555)
*Inki Kim,Hyuntae Ahn,Yongjune Kim,Hee-Youl Kwak,Dae-Young Yun,Sang-Hyo Kim*

Main category: cs.IT

TL;DR: 提出了一种基于原图的LDPC码设计框架，在块衰落信道中实现全分集，在高斯白噪声信道中实现接近容量性能


<details>
  <summary>Details</summary>
Motivation: 现有的LDPC码设计通常在分集导向和容量导向之间需要权衡，难以同时满足块衰落信道（需要全分集）和高斯白噪声信道（需要接近容量性能）的要求

Method: 1) 基于布尔近似的分集演化分析推导广义根校验的结构约束；2) 为两区块衰落信道设计原图模板；3) 使用密度演化引导的遗传算法优化原图边以获得优异AWGNC性能

Result: 设计的LDPC码在块衰落信道中实现了全分集，在高斯白噪声信道中表现出接近容量的性能，有效弥合了分集导向和容量导向设计之间的差距

Conclusion: 提出的设计框架能够生成同时满足块衰落信道全分集和高斯白噪声信道接近容量性能的LDPC码，在两种信道环境下都表现出鲁棒性能

Abstract: This paper presents a novel design framework for protograph-based LDPC codes that simultaneously achieves full diversity in block-fading channels (BFCs) and nearcapacity performance in additive white Gaussian noise channels (AWGNCs). By leveraging a Boolean approximation-based analysis--Diversity Evolution (DivE)--we derive structural constraints for generalized rootchecks that guarantee full diversity. Based on these constraints, we propose a protograph template tailored for two-block BFCs. Furthermore, we employ a genetic algorithm guided by density evolution to optimize the protograph edges within this template for superior AWGNC performance. The resulting codes effectively bridge the gap between diversityoriented and capacity-oriented designs, exhibiting robust performance across both channel environments.

</details>


### [174] [On the Fragility of AI-Based Channel Decoders under Small Channel Perturbations](https://arxiv.org/abs/2602.01582)
*Haoyu Lei,Mohammad Jalali,Chin Wa Lau,Farzan Farnia*

Main category: cs.IT

TL;DR: AI解码器在AWGN信道下性能优于传统BP解码，但对对抗性扰动更敏感，存在鲁棒性代价


<details>
  <summary>Details</summary>
Motivation: 探究AI解码器性能提升的来源及其代价，特别是对信道输出分布变化的鲁棒性

Method: 通过对抗性扰动（FGM和投影梯度法）和通用对抗性扰动评估AI解码器鲁棒性

Result: AI解码器（ECCT、CrossMPT）在对抗性扰动下性能显著下降，对抗性扰动在AI解码器间转移性强，通用扰动比随机扰动危害更大

Conclusion: AI解码器的性能提升可能以鲁棒性为代价，对信道分布变化更敏感

Abstract: Recent advances in deep learning have led to AI-based error correction decoders that report empirical performance improvements over traditional belief-propagation (BP) decoding on AWGN channels. While such gains are promising, a fundamental question remains: where do these improvements come from, and what cost is paid to achieve them? In this work, we study this question through the lens of robustness to distributional shifts at the channel output. We evaluate both input-dependent adversarial perturbations (FGM and projected gradient methods under $\ell_2$ constraints) and universal adversarial perturbations that apply a single norm-bounded shift to all received vectors. Our results show that recent AI decoders, including ECCT and CrossMPT, could suffer significant performance degradation under such perturbations, despite superior nominal performance under i.i.d. AWGN. Moreover, adversarial perturbations transfer relatively strongly between AI decoders but weakly to BP-based decoders, and universal perturbations are substantially more harmful than random perturbations of equal norm. These numerical findings suggest a potential robustness cost and higher sensitivity to channel distribution underlying recent AI decoding gains.

</details>


### [175] [Spectral-Aligned Pruning for Universal Error-Correcting Code Transformers](https://arxiv.org/abs/2602.01602)
*Sanghyeon Cho,Taewoo Park,Seong-Joon Park,Dae-Young Yun,Hee-Youl Kwak,Sang-Hyo Kim,Yongjune Kim*

Main category: cs.IT

TL;DR: 提出SAP（谱对齐剪枝）框架，通过利用二分图谱实现跨码结构化剪枝掩码重用，结合LoRA进行码特定恢复，显著降低FECCT解码器的计算复杂度和内存占用。


<details>
  <summary>Details</summary>
Motivation: FECCT作为通用信道解码器虽性能优异，但基于Transformer的架构存在计算复杂度高、参数量大的问题，限制了实际部署应用。

Method: 提出SAP框架：1）利用二分图谱实现跨码结构化剪枝掩码重用；2）通过参数高效的LoRA进行码特定恢复；3）在共享剪枝骨干网络基础上仅存储小型码特定适配器参数。

Result: 在多种码型上实验表明，SAP能达到与专用逐码剪枝相当的译码性能，同时通过核级结构化剪枝显著降低计算成本和模型内存占用。

Conclusion: SAP框架有效解决了FECCT解码器的部署障碍，实现了性能与效率的良好平衡，为通用信道解码器的实际应用提供了可行方案。

Abstract: Recently, the Foundation Error Correction Code Transformer (FECCT) has emerged as a promising universal channel decoder, achieving competitive decoding performance across diverse code families by relying on a single shared model backbone, optionally followed by code-specific retraining. Despite this flexibility, the high computational complexity and large parameter footprint of transformer-based decoders present substantial obstacles to practical deployment. To address these challenges, we investigate structured pruning for FECCT and propose Spectral-Aligned Pruning (SAP), a structure-aware framework that enables cross-code reuse of structured pruning masks across codes by leveraging the spectrum of the corresponding bipartite graph. After pruning, SAP performs per-code recovery via parameter-efficient low-rank adaptation (LoRA), enabling a shared pruned backbone while storing only small code-specific adapter parameters. Experiments across diverse codes show that SAP achieves decoding performance comparable to dedicated per-code pruning, while enabling substantial reductions in computational cost and model memory footprint through kernel-level structured pruning.

</details>


### [176] [Low-Complexity Multi-Agent Continual Learning for Stacked Intelligent Metasurface-Assisted Secure Communications](https://arxiv.org/abs/2602.01653)
*Enyu Shi,Yiyang Zhu,Jiayi Zhang,Ziheng Liu,Jiakang Zheng,Jiancheng An,Derrick Wing Kwan Ng,Bo Ai,Chau Yuen*

Main category: cs.IT

TL;DR: 提出一种基于堆叠智能超表面(SIM)的多用户MIMO系统物理层安全增强方案，采用流形增强异构多智能体持续学习框架优化波束成形，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统MIMO系统物理层安全面临数字预编码复杂、硬件开销大的挑战，堆叠智能超表面技术能直接在电磁域进行波束成形，有望简化系统架构并提升安全性能。

Method: 提出SIM辅助多用户MIMO系统架构，基站天线传输单用户流，多层SIM执行电磁域波束成形；设计MHACL框架联合优化功率分配和相位偏移，并开发低复杂度SIMHACL模板将搜索空间从指数级降至线性。

Result: 仿真验证所提框架在SIM辅助系统中实现毫秒级每轮训练，显著优于多种基线方案；SIMHACL在保持与MHACL相当加权和保密率的同时，计算时间减少30%。

Conclusion: SIM技术与智能学习框架的结合为未来无线通信安全提供了高效解决方案，在降低硬件复杂度的同时显著提升物理层安全性能，具有重要应用前景。

Abstract: Stacked intelligent metasurfaces (SIMs), composed of multiple layers of reconfigurable transmissive metasurfaces, are gaining prominence as a transformative technology for future wireless communication security. This paper investigates the integration of SIM into multi-user multiple-input multiple-output (MIMO) systems to enhance physical layer security. A novel system architecture is proposed, wherein each base station (BS) antenna transmits a dedicated single-user stream, while a multi-layer SIM executes wave-based beamforming in the electromagnetic domain, thereby avoiding the need for complex baseband digital precoding and significantly reducing hardware overhead. To maximize the weighted sum secrecy rate (WSSR), we formulate a joint precoding optimization problem over BS power allocation and SIM phase shifts, which is high-dimensional and non-convex due to the complexity of the objective function and the coupling among optimization variables. To address this, we propose a manifold-enhanced heterogeneous multi-agent continual learning (MHACL) framework that incorporates gradient representation and dual-scale policy optimization to achieve robust performance in dynamic environments with high demands for secure communication. Furthermore, we develop SIM-MHACL (SIMHACL), a low-complexity learning template that embeds phase coordination into a product manifold structure, reducing the exponential search space to linear complexity while maintaining physical feasibility. Simulation results validate that the proposed framework achieves millisecond-level per-iteratio ntraining in SIM-assisted systems, significantly outperforming various baseline schemes, with SIMHACL achieving comparable WSSR to MHACL while reducing computation time by 30\%.

</details>


### [177] [Decoding Golay Codes and their Related Lattices: A PAC Code Perspective](https://arxiv.org/abs/2602.01657)
*Yujun Ji,Ling Liu,Shanxiang Lyu,Chao Chen,Tao Dai,Baoming Bai*

Main category: cs.IT

TL;DR: 提出基于极化调整卷积码视角的Golay码解码方法，通过Forney立方构造找到不同PAC码构造方式，实现高效并行列表解码，性能接近最大似然。


<details>
  <summary>Details</summary>
Motivation: 现有Golay码解码方法通常需要索引置换和码字删余操作，复杂度较高。本文旨在从PAC码的新视角探索更高效的Golay码解码方法。

Method: 利用Forney的Golay码立方构造及其生成器G*(8,7)/(8,4)，从PAC码角度发现Golay码的不同构造方式，开发并行列表解码算法。

Result: 提出的解码方法无需索引置换和码字删余，实现高效并行列表解码，性能接近最大似然。该方法还能高效解码相关格结构如Leech格Λ24及其主子格H24。

Conclusion: 从PAC码视角为Golay码提供了新的解码方法，简化了操作流程，提高了解码效率，并能扩展到相关格结构的解码应用。

Abstract: In this work, we propose a decoding method of Golay codes from the perspective of Polarization Adjusted Convolutional (PAC) codes. By invoking Forney's cubing construction of Golay codes and their generators $G^*(8,7)/(8,4)$, we found different construction methods of Golay codes from PAC codes, which result in an efficient parallel list decoding algorithm with near-maximum likelihood performance. Compared with existing methods, our method can get rid of index permutation and codeword puncturing. Using the new decoding method, some related lattices, such as Leech lattice $Λ_{24}$ and its principal sublattice $H_{24}$, can be also decoded efficiently.

</details>


### [178] [Performance Guarantees of Cellular Networks with Hardcore Regulation and Scheduling](https://arxiv.org/abs/2602.01802)
*Ke Feng,François Baccelli,Catherine Rosenberg*

Main category: cs.IT

TL;DR: 该论文研究蜂窝网络中通过空间硬核调控和基站调度来提供可证明的性能保证，分析了调度策略何时能改善链路级速率保证。


<details>
  <summary>Details</summary>
Motivation: 现代通信网络需要提供性能保证，空间调控已被证明对建立可证明的无线链路级保证至关重要。本文旨在研究在蜂窝网络下行链路中，通过硬核空间调控和基站调度来提供性能保证。

Method: 采用空间网络演算框架，对基站位置施加硬核调控（强制最小距离约束），分析基站调度对性能的影响。首先推导空间调控蜂窝网络总干扰功率的上界，然后识别调度基站能比基站始终活跃场景提供更好链路级速率保证的机制。特别分析了六边形蜂窝网络作为特例。

Result: 提供了空间调控蜂窝网络总干扰功率的上界，识别了调度策略能改善链路级速率保证的机制。六边形蜂窝网络分析提供了具体案例。结果表明调度策略在某些情况下能提供更好的性能保证。

Conclusion: 研究结果为需要何种空间调控、何时选择调度策略以及如何降低网络功耗以提供特定目标性能保证提供了见解。空间调控与调度策略的结合能有效改善蜂窝网络的性能保证。

Abstract: Providing performance guarantees is one of the {critical} objectives of {recent and future} communication networks, toward which regulations, {i.e., constraints on key system parameters,} have played an indispensable role. This is the case for large wireless communication networks, where spatial regulations (e.g., constraints on intercell distance) have recently been shown, through a spatial network calculus, to be essential for establishing provable wireless link-level guarantees. In this work, we focus on performance guarantees for {the downlink of} cellular networks where we impose a hardcore (spatial) regulation on base station (BS) locations and evaluate {how BS scheduling (which controls which BSs can transmit at a given time) impacts performance}. Hardcore regulation is the simplest form of spatial regulation that enforces a minimal distance between any pair of transmitters in the network. Within this framework of spatial network calculus, we first provide an upper bound on the power of total interference for a spatially regulated cellular network, and then, identify the regimes where scheduling BSs yields {better} link-level rate guarantees compared to scenarios where base stations are always active. The hexagonal cellular network is analyzed as a special case. The results offer insights into what spatial regulations are needed, when to choose scheduling, and how to potentially reduce the network power consumption {to provide a certain target performance guarantee}.

</details>


### [179] [Zero-Shot Knowledge Base Resizing for Rate-Adaptive Digital Semantic Communication](https://arxiv.org/abs/2602.01829)
*Shumin Yao,Hui Du,Lifeng Xie,Yaping Sun,Hao Chen,Nan Ma,Xiaodong Xu*

Main category: cs.IT

TL;DR: 提出一种零样本知识库大小调整方法，通过揭示语义层次结构实现VQ-VAE语义通信系统的实时速率自适应，无需重新训练


<details>
  <summary>Details</summary>
Motivation: 当前VQ-VAE语义通信系统中，传输速率由知识库大小决定，但KB大小是固定超参数，需要为每个不同大小训练和存储单独模型，计算和存储成本过高，无法实现真正细粒度的速率控制

Method: 1) 将KB向量嵌入双曲空间以揭示其层次关系；2) 使用最小生成树算法构建主语义树；3) 通过迭代剪枝最不重要的叶节点实现即时大小调整。建立全局重要性排名，利用单个大型父KB的内在语义层次

Result: 方法实现了与从头训练专用KB几乎相同的重建质量，仅需一小部分计算预算。在极低速率下表现出更强的鲁棒性，而传统KB会出现灾难性失败

Conclusion: 解决了VQ-VAE语义通信系统的基本限制，为灵活和速率自适应的语义通信提供了实用高效路径

Abstract: Digital semantic communication systems, which often leverage the Vector Quantized Variational Autoencoder (VQ-VAE) framework, are pivotal for future wireless networks. In a VQ-VAE-based semantic communication system, the transmission rate is directly governed by the size of a discrete codebook known as knowledge base (KB). However, the KB size is a fixed hyperparameter, meaning that adapting the rate requires training and storing a separate model for each desired size -- a practice that is too computationally and storage-prohibitive to achieve truly granular rate control. To address this, we introduce a principled, zero-shot KB resizing method that enables on-the-fly rate adaptation without any retraining. Our approach establishes a global importance ranking for all vectors within a single, large parent KB by uncovering its inherent semantic hierarchy. This is achieved via a three-step framework: 1) embedding KB vectors into hyperbolic space to reveal their hierarchical relationships; 2) constructing a master semantic tree using a minimum spanning tree algorithm; 3) enabling instant resizing by iteratively pruning the least important leaf nodes. Extensive simulations demonstrate that our method achieves reconstruction quality nearly identical to that of dedicated KBs trained from scratch, while demanding only a fraction of the computational budget. Moreover, our approach exhibits superior robustness at very low rates, where conventional KBs suffer from catastrophic failure. Our work resolves a fundamental limitation of VQ-VAE-based semantic communication systems, offering a practical and efficient path toward flexible and rate-adaptive semantic communication.

</details>


### [180] [Two-Stage Coded-Sliding Beam Training and QoS-Constrained Sum-Rate Maximization for SIM-Assisted Wireless Communications](https://arxiv.org/abs/2602.02131)
*Qian Zhang,Ju Liu,Yao Ge,Yufei Zhao,Wali Ullah Khan,Zheng Dong,Yong Liang Guan,Chau Yuen*

Main category: cs.IT

TL;DR: 提出了一种用于堆叠智能超表面辅助通信系统的统一低复杂度算法框架，包括两步码本构建、两阶段编码滑动波束训练和可变解耦块连续上界最小化算法，以解决信道状态信息获取和相位优化问题。


<details>
  <summary>Details</summary>
Motivation: 堆叠智能超表面为大规模天线通信提供了经济高效的解决方案，但高效的信道状态信息获取和相位偏移优化仍然是关键挑战。需要开发低复杂度算法来解决这些问题。

Method: 1. 提出广义两步码本构建方法，利用二维角度域解耦将平面阵列波束形成器设计转化为两个独立的一维线性阵列设计问题，通过Gerchberg-Saxton算法和提出的基于主化最小化的近端距离算法求解。
2. 开发两阶段编码滑动波束训练方法：第一阶段嵌入纠错码增强抗噪性，第二阶段在匹配角度样本周围进行滑动采样提高角度分辨率。
3. 提出可变解耦块连续上界最小化算法，通过闭式迭代更新直接解决QoS约束的和速率最大化问题。

Result: 仿真结果表明所提方法能够实现精确的波束模式实现、改进的波束训练精度和角度分辨率，以及增强的和速率性能。

Conclusion: 该论文为SIM辅助通信系统开发了一个统一的低复杂度算法框架，有效解决了信道状态信息获取和相位优化问题，显著降低了计算复杂度，为大规模天线通信提供了实用解决方案。

Abstract: Stacked intelligent metasurfaces (SIM) provide a cost-effective and scalable solution for large-scale antenna communications.However, efficient channel state information acquisition and phase shift optimization remain critical challenges. In this paper, we develop a unified framework of low-complexity algorithms for SIM-assisted communication systems to address these issues. Specifically, we propose a generalized two-step codebook construction (TSCC) method that leverages two-dimensional angular-domain decoupling to transform planar array beamformer design into two independent one-dimensional linear array beamformer design problems, efficiently solved via the Gerchberg-Saxton algorithm and our proposed majorization-minimization-based proximal distance (PDMM) algorithm. We further develop a two-stage coded-sliding beam training (TSCSBT) method for low-overhead and high-accuracy beam training, where error-correcting codes are embedded in the first-stage training to enhance robustness against noise, and sliding sampling is subsequently performed around the matched angular samples to improve angular resolution. The proposed framework is further extended to multi-path user channels. Finally, a variable decoupling-based block successive upper bound minimization (VD-BSUM) algorithm is proposed to directly solve the QoS-constrained sum-rate maximization problem through closed-form iterative updates with substantially reduced computational complexity. Simulation results demonstrate the effectiveness of the proposed methods in achieving precise beam pattern realization, improved beam training accuracy and angular resolution, and enhanced sum-rate performance.

</details>


### [181] [Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation](https://arxiv.org/abs/2602.02469)
*Ahmed M. Elshazly,Ahmed Arafa*

Main category: cs.IT

TL;DR: 提出一种无需设备端信道状态信息的无线联邦学习方法，通过多天线MRC检测和AgeTop-k坐标选择策略，在单OFDM符号内传输模型更新，降低延迟。


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习中，多设备同时发送模型更新面临信道资源有限的问题。传统方法需要设备端信道状态信息，且大量参数传输需要多个OFDM符号，导致高延迟。

Method: 提出年龄感知的边缘盲过空中联邦学习方法：1) PS使用多天线和MRC检测，无需设备端CSI；2) 采用AgeTop-k选择策略，优先选择幅度最大且等待时间最长的k个坐标，确保所有选定参数能装入单个OFDM符号。

Result: 1) 更多PS天线显著提高精度和收敛速度；2) 在较好信道条件下，AgeTop-k优于随机选择；3) 最优k值取决于信道条件，噪声较大时较小的k更好。

Conclusion: 该方法有效解决了无线联邦学习中的延迟问题，通过天线阵列和智能参数选择实现了性能提升，揭示了天线数量与压缩误差之间的关键权衡。

Abstract: We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates. We propose an efficient \emph{age-aware edge-blind over-the-air FL} approach that does not require channel state information (CSI) at the devices. Instead, the parameter server (PS) uses multiple antennas and applies maximum-ratio combining (MRC) based on its estimated sum of the channel gains to detect the parameter updates. A key challenge is that the number of orthogonal subcarriers is limited; thus, transmitting many parameters requires multiple Orthogonal Frequency Division Multiplexing (OFDM) symbols, which increases latency. To address this, the PS selects only a small subset of model coordinates each round using \emph{AgeTop-\(k\)}, which first picks the largest-magnitude entries and then chooses the \(k\) coordinates with the longest waiting times since they were last selected. This ensures that all selected parameters fit into a single OFDM symbol, reducing latency. We provide a convergence bound that highlights the advantages of using a higher number of antenna array elements and demonstrates a key trade-off: increasing \(k\) decreases compression error at the cost of increasing the effect of channel noise. Experimental results show that (i) more PS antennas greatly improve accuracy and convergence speed; (ii) AgeTop-\(k\) outperforms random selection under relatively good channel conditions; and (iii) the optimum \(k\) depends on the channel, with smaller \(k\) being better in noisy settings.

</details>


### [182] [Secure Multi-User Linearly-Separable Distributed Computing](https://arxiv.org/abs/2602.02489)
*Amir Masoud Jafarpisheh,Ali Khalesi,Petros Elia*

Main category: cs.IT

TL;DR: 该论文提出了一种在多用户线性可分分布式计算框架中实现信息论安全的方法，通过设计满足特定条件的矩阵分解方案，保证用户只能获取自身请求的函数信息。


<details>
  <summary>Details</summary>
Motivation: 现有的多用户线性可分分布式计算框架虽然能提供近最优性能，但其线性特性引发了数据保密性问题。需要设计一种方案，确保每个用户只能学习到自己请求的函数，而不能获取其他用户的信息。

Method: 提出两个必要且充分的保密准则：1) 每个用户观察到的服务器响应中，可见的公共随机性必须恰好跨越维度α_k-1的子空间；2) 从矩阵D中移除用户观察到的服务器对应列后，剩余矩阵的秩至少为K-1。基于这些条件，设计了一个通用方案：在矩阵E后附加Null(D)的基，并精心注入共享随机性。

Result: 该方案在有限域情况下能保证完美信息论保密性，在实数域情况下能提供明确的互信息界限，且可以通过增加高斯公共随机性的方差使该界限任意小。在许多情况下，该方案不会产生额外成本。

Conclusion: 提出的方案成功解决了多用户线性可分分布式计算框架中的数据保密性问题，在保持性能的同时实现了信息论安全，为分布式计算中的隐私保护提供了有效的理论框架。

Abstract: The introduction of the new multi-user linearly-separable distributed computing framework, has recently revealed how a parallel treatment of users can yield large parallelization gains with relatively low computation and communication costs. These gains stem from a new approach that converts the computing problem into a sparse matrix factorization problem; a matrix $F$ that describes the users' requests, is decomposed as \(F = DE\), where a \(γ\)-sparse \(E\) defines the task allocation across $N$ servers, and a \(δ\)-sparse \(D\) defines the connectivity between \(N\) servers and \(K\) users as well as the decoding process. While this approach provides near-optimal performance, its linear nature has raised data secrecy concerns.
  We here adopt an information-theoretic secrecy framework, seeking guarantees that each user can learn nothing more than its own requested function. In this context, our main result provides two necessary and sufficient secrecy criteria; (i) for each user \(k\) who observes $α_k$ server responses, the common randomness visible to that user must span a subspace of dimension exactly $α_k-1$,
  and (ii) for each user, removing from \(\mathbf{D}\) the columns corresponding to the servers it observes must leave a matrix of rank at least \(K-1\). With these conditions in place, we design a general scheme -- that applies to finite and non-finite fields alike -- which is based on appending to \(\mathbf{E}\) a basis of \(\mathrm{Null}(\mathbf{D})\) and by carefully injecting shared randomness. In many cases, this entails no additional costs. The scheme, while maintaining performance, guarantees perfect information-theoretic secrecy in the case of finite fields, while in the real case, the conditions yield an explicit mutual-information bound that can be made arbitrarily small by increasing the variance of Gaussian common randomness.

</details>
