<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.IT](#cs.IT) [Total: 17]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [X-raying the arXiv: A Large-Scale Analysis of arXiv Submissions' Source Files](https://arxiv.org/abs/2601.11385)
*Giovanni Apruzzese,Aurore Fass*

Main category: cs.NI

TL;DR: arXiv论文源文件中平均27%的数据是冗余的，包含不当内容和研究机密，作者提出自动化检测工具改善数据卫生


<details>
  <summary>Details</summary>
Motivation: arXiv作为最大的开放获取科学文献库，作者上传源文件时会包含与发表论文无关的数据（如图表、文档、评论），这些数据可能无意中泄露机密信息或浪费存储空间。研究旨在探究arXiv提交的源文件中存在哪些不必要的内容。

Method: 对2015-2025年间约60万篇arXiv提交进行纵向分析，检查每个提交的上传源文件，量化并描述生成相应PDF不需要的数据。同时进行定性检查，识别不当内容和研究细节。

Result: 平均每个提交中27%的数据是不必要的，整个数据集总计超过580GB的冗余内容。定性检查发现存在攻击性/不当文本（如"WTF does this mean?"）和可能泄露正在进行研究的实验细节。已联系arXiv领导团队和相关作者提醒这些问题。

Conclusion: arXiv源文件中存在大量不必要数据，可能泄露机密信息和浪费存储空间。作者提出建议和自动化工具，用于大规模检测和分析arXiv提交的残留数据，旨在改善arXiv生态系统的数据卫生。

Abstract: arXiv is the largest open-access repository for scientific literature. When submitting a paper, authors upload the manuscript's source files, from which the final PDF is compiled. These source files are also publicly downloadable, potentially exposing data unrelated to the published paper -- such as figures, documents, or comments -- that may unintentionally reveal confidential information or simply waste storage space. We thus ask ourselves: "What can be found within the source files of arXiv submissions?"
  We present a longitudinal analysis of ~600,000 submissions appeared on arXiv between 2015--2025. For each submission, we examine the uploaded source files to quantify and characterize data not required for producing the respective PDF. On average, 27% of the data in each submission are unnecessary, totaling >580 GB of redundant content across our dataset. Qualitative inspection reveals the presence of offensive/inappropriate text (e.g., "WTF does this mean?") and experimental details that could disclose ongoing research. We have contacted arXiv's leadership team, as well as the authors of affected papers to alert them of these issues. Finally, we propose recommendations and an automated tool to detect and analyze arXiv submissions residual data at scale, aiming to improve data hygiene in the arXiv's ecosystem.

</details>


### [2] [Indoor Neutral-Host Networks Over Shared Spectrum and Shared Infrastructure: A Comparison Study of Real-World Deployments](https://arxiv.org/abs/2601.11457)
*Joshua Roy Palathinkal,Muhammad Iqbal Rochman,Vanlin Sathya,Mehmet Yavuz,Monisha Ghosh*

Main category: cs.NI

TL;DR: 中性主机网络通过室内部署和频谱共享，显著提升室内覆盖和上行性能，优于传统运营商宏站但上行仍不及Wi-Fi


<details>
  <summary>Details</summary>
Motivation: 室内高容量连接受限于建筑穿透损耗和宏站上行功率限制，传统运营商在低频和中频频谱优化中仍存在上行性能不对称问题

Method: 多站点测量研究，比较CBRS中性主机网络与公共运营商4G/5G宏站部署及Wi-Fi的性能

Result: 1) 建筑穿透损耗显著；2) 中性主机网络室内RSRP提升30dB，下行匹配运营商室外性能，上行超越运营商；3) 中性主机上行效率更高；4) 运营商依赖低频，中性主机保持中频高性能；5) 中性主机端到端吞吐量优于运营商，但上行吞吐量和延迟不及Wi-Fi

Conclusion: 中性主机网络通过室内部署和频谱共享有效解决室内覆盖和上行性能问题，是传统运营商宏站部署的有力补充方案

Abstract: Indoor high-capacity connectivity is frequently constrained by significant building penetration loss and the inherent uplink power limitations of a typical outdoor macro-cell deployment. While Mobile Network Operators (MNOs) must optimize spectrum across low-band (<1 GHz) and mid-band (1-7 GHz) frequencies, uplink performance remains disproportionately degraded due to link budget asymmetry. Neutral-host (NH) networking provides a scalable alternative by transparently offloading MNO subscribers via spectrum sharing and shared infrastructure. We present a multi-site measurement study comparing Citizens Broadband Radio Service (CBRS)-enabled NH networks against public MNO 4G/5G macro deployments and Wi-Fi. Our results show: (i) significant building penetration loss with up to 15.5 dB in low-bands and 17.9 dB in mid-bands, resulting in a ~10 dB RSRP deficit for MNO mid-bands compared to low-bands; (ii) NH networks provide a 30 dB higher median indoor RSRP with indoor NH normalized downlink throughput matches MNO outdoor performance, while its uplink performance exceeds MNO levels in both indoor and outdoor settings; (iii) NH proximity enables superior uplink efficiency, utilizing 64-QAM for 56% of transmissions (versus <6% for MNOs) and reducing median UE transmit power by 5 dB; (iv) MNOs rely on low-band spectrum for indoor uplink transmissions, while the NH deployment maintains high-performance mid-band connectivity; and (v) NH outperforms MNOs in end-to-end throughput but trails Wi-Fi in uplink throughput and latency due to packet routing overhead to the MNO core.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Japanese AI Agent System on Human Papillomavirus Vaccination: System Design](https://arxiv.org/abs/2601.10718)
*Junyu Liu,Siwen Yang,Dexiu Ma,Qian Niu,Zequn Zhang,Momoko Nagai-Tanima,Tomoki Aoyama*

Main category: cs.AI

TL;DR: 开发了一个双功能AI代理系统，通过对话界面提供HPV疫苗验证信息，同时基于用户互动和社交媒体生成医疗机构分析报告


<details>
  <summary>Details</summary>
Motivation: 日本HPV疫苗犹豫问题严重，2013-2021年主动推荐暂停导致信息缺口，社交媒体错误信息加剧问题，传统方法无法同时处理个人查询和监测群体讨论

Method: 构建包含向量数据库（整合学术论文、政府来源、新闻媒体和社交媒体）、基于ReAct架构的多工具编排检索增强生成聊天机器人，以及自动报告生成系统（新闻分析、研究综合、社交媒体情感分析、用户互动模式识别）

Result: 单轮评估：相关性4.83、路由4.89、参考质量4.50、正确性4.90、专业身份4.88（总体4.80）；多轮评估：上下文保留4.94、主题连贯性5.00、总体4.98；报告系统完整性4.00-5.00、正确性4.00-5.00、有用性3.67-5.00，所有时期参考有效性均为5.00

Conclusion: 证明了集成AI代理系统在双向HPV疫苗沟通中的可行性，该架构能够提供带来源归属的验证信息，同时提供系统的公共话语分析，具有可转移到其他医疗环境的框架

Abstract: Human papillomavirus (HPV) vaccine hesitancy poses significant public health challenges, particularly in Japan where proactive vaccination recommendations were suspended from 2013 to 2021. The resulting information gap is exacerbated by misinformation on social media, and traditional ways cannot simultaneously address individual queries while monitoring population-level discourse. This study aimed to develop a dual-purpose AI agent system that provides verified HPV vaccine information through a conversational interface while generating analytical reports for medical institutions based on user interactions and social media. We implemented a system comprising: a vector database integrating academic papers, government sources, news media, and social media; a Retrieval-Augmented Generation chatbot using ReAct agent architecture with multi-tool orchestration across five knowledge sources; and an automated report generation system with modules for news analysis, research synthesis, social media sentiment analysis, and user interaction pattern identification. Performance was assessed using a 0-5 scoring scale. For single-turn evaluation, the chatbot achieved mean scores of 4.83 for relevance, 4.89 for routing, 4.50 for reference quality, 4.90 for correctness, and 4.88 for professional identity (overall 4.80). Multi-turn evaluation yielded higher scores: context retention 4.94, topic coherence 5.00, and overall 4.98. The report generation system achieved completeness 4.00-5.00, correctness 4.00-5.00, and helpfulness 3.67-5.00, with reference validity 5.00 across all periods. This study demonstrates the feasibility of an integrated AI agent system for bidirectional HPV vaccine communication. The architecture enables verified information delivery with source attribution while providing systematic public discourse analysis, with a transferable framework for adaptation to other medical contexts.

</details>


### [4] [Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models](https://arxiv.org/abs/2601.10719)
*Gerard Yeo,Svetlana Churina,Kokil Jaidka*

Main category: cs.AI

TL;DR: LLMs在预训练中隐式编码了心理信任信号，无需显式监督即可区分高/低信任文本，为可信AI系统设计提供基础


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否以心理一致的方式表示在线信息可信度，这对于LLMs在搜索、推荐和对话系统中的可信度至关重要

Method: 使用PEACE-Reviews数据集分析指令调优LLMs（Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B）在类似网络叙事中编码可信度的方式，通过层和头激活差异、探测分析等方法

Result: 模型在层和头激活上能系统区分高/低信任文本，信任信号可线性解码，微调会优化而非重构表示，最强关联出现在公平性、确定性和自我问责等人类信任形成维度

Conclusion: 现代LLMs无需显式监督即可内化心理基础的信任信号，为设计可信、透明AI系统提供了表示基础

Abstract: Perceived trustworthiness underpins how users navigate online information, yet it remains unclear whether large language models (LLMs),increasingly embedded in search, recommendation, and conversational systems, represent this construct in psychologically coherent ways. We analyze how instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B) encode perceived trustworthiness in web-like narratives using the PEACE-Reviews dataset annotated for cognitive appraisals, emotions, and behavioral intentions. Across models, systematic layer- and head-level activation differences distinguish high- from low-trust texts, revealing that trust cues are implicitly encoded during pretraining. Probing analyses show linearly de-codable trust signals and fine-tuning effects that refine rather than restructure these representations. Strongest associations emerge with appraisals of fairness, certainty, and accountability-self -- dimensions central to human trust formation online. These findings demonstrate that modern LLMs internalize psychologically grounded trust signals without explicit supervision, offering a representational foundation for designing credible, transparent, and trust-worthy AI systems in the web ecosystem. Code and appendix are available at: https://github.com/GerardYeo/TrustworthinessLLM.

</details>


### [5] [Building AI Agents to Improve Job Referral Requests to Strangers](https://arxiv.org/abs/2601.10726)
*Ross Chu,Yuting Huang*

Main category: cs.AI

TL;DR: 开发AI助手帮助求职者撰写有效的职位推荐请求，通过改进代理重写请求、评估代理预测成功率，结合RAG技术提升弱请求成功率14%而不影响强请求表现。


<details>
  <summary>Details</summary>
Motivation: 在专业在线社区中，求职者需要撰写有效的推荐请求以获得工作机会，但许多人缺乏撰写高质量请求的技能。AI助手可以提供低成本、可扩展的帮助，提升求职成功率。

Method: 采用双代理系统：改进代理使用LLM重写推荐请求；评估代理使用训练好的模型预测接收推荐的概率。结合检索增强生成(RAG)技术优化LLM编辑效果，防止对强请求的负面修改。

Result: LLM修订能提升弱请求的预测成功率，但会降低强请求的成功率。结合RAG后，能防止对强请求的负面影响，同时将弱请求的预测成功率提升14%。

Conclusion: AI助手能有效提升求职推荐请求的质量，特别是对弱请求的改善效果显著。虽然模型预测的改进不一定保证实际获得更多推荐，但为低成本测试有前景的功能提供了信号，可在进行高风险真实用户实验前进行验证。

Abstract: This paper develops AI agents that help job seekers write effective requests for job referrals in a professional online community. The basic workflow consists of an improver agent that rewrites the referral request and an evaluator agent that measures the quality of revisions using a model trained to predict the probability of receiving referrals from other users. Revisions suggested by the LLM (large language model) increase predicted success rates for weaker requests while reducing them for stronger requests. Enhancing the LLM with Retrieval-Augmented Generation (RAG) prevents edits that worsen stronger requests while it amplifies improvements for weaker requests. Overall, using LLM revisions with RAG increases the predicted success rate for weaker requests by 14\% without degrading performance on stronger requests. Although improvements in model-predicted success do not guarantee more referrals in the real world, they provide low-cost signals for promising features before running higher-stakes experiments on real users.

</details>


### [6] [ORBITFLOW: SLO-Aware Long-Context LLM Serving with Fine-Grained KV Cache Reconfiguration](https://arxiv.org/abs/2601.10729)
*Xinyue Ma,Heelim Hong,Taegeon Um,Jongseop Lee,Seoyeong Choy,Woo-Yeon Lee,Myeongjae Jeon*

Main category: cs.AI

TL;DR: ORBITFLOW是一个细粒度自适应KV缓存管理系统，通过动态调整KV缓存位置和回退机制，解决长上下文LLM服务中的内存波动和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM服务面临挑战：请求长度和批次组成在token生成过程中变化，导致运行时内存占用大幅波动。现有的静态预定义卸载策略无法适应快速变化的内存需求，导致过多的CPU-GPU KV传输、延迟峰值和频繁的SLO违规。

Method: 1. 使用轻量级ILP求解器为每个请求决定哪些层的KV缓存保留在GPU上（在内存容量约束内）；2. 基于运行时反馈持续优化KV放置策略；3. 在高负载时调用回退机制，暂时推迟内存占用大的请求以保持整体SLO达成。

Result: ORBITFLOW将TPOT和TBT的SLO达成率分别提高了66%和48%，将第95百分位延迟降低了38%，与现有卸载方法相比实现了高达3.3倍的吞吐量提升。

Conclusion: ORBITFLOW通过细粒度自适应KV缓存管理，有效解决了长上下文LLM服务中的内存波动问题，显著提高了SLO达成率、降低了延迟并提升了吞吐量。

Abstract: Serving long-context LLMs is challenging because request lengths and batch composition vary during token generation, causing the memory footprint to fluctuate significantly at runtime. Offloading KV caches to host memory limits effective memory usage, but existing static and predetermined offloading strategies cannot adapt to the rapidly shifting memory demands of long-context serving. This often leads to excessive CPU-to-GPU KV transfers that translate into latency spikes and frequent SLO violations. To address these challenges, we introduce ORBITFLOW, a fine-grained and adaptive KV cache management system that meets latency SLOs in long-context LLM serving. ORBITFLOW employs a lightweight ILP solver to decide which layers' KV caches to retain on the GPU for each request, within memory capacity constraints. It continuously refines KV placements based on runtime feedback when the active plan becomes suboptimal during token generation. Under heavy load, ORBITFLOW invokes a fallback mechanism to temporarily defer in-flight requests with large memory footprints, preserving overall SLO attainment. Our experiments demonstrate that ORBITFLOW improves SLO attainment for TPOT and TBT by up to 66% and 48%, respectively, while reducing the 95th percentile latency by 38% and achieving up to 3.3x higher throughput compared to existing offloading methods.

</details>


### [7] [CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems](https://arxiv.org/abs/2601.10738)
*Percy Jardine*

Main category: cs.AI

TL;DR: CTHA是一个约束性时间分层架构，通过在层级间通信空间施加结构化约束来恢复协调稳定性，减少47%的失败级联，提升2.3倍样本效率。


<details>
  <summary>Details</summary>
Motivation: 多时间尺度智能体架构虽然提升了性能，但破坏了统一智能体系统的协调稳定性，导致严重的层级间冲突、无界错误传播和可扩展性受限。

Method: 提出约束性时间分层架构(CTHA)，包含三个关键约束：1)消息契约约束，通过类型化摘要、计划和策略包形式化层级间信息流；2)权威流形约束，根据时间范围限制每层的决策空间；3)仲裁器解决约束，保证多层决策的无冲突组合。

Result: 实验证明CTHA在复杂任务执行中有效，相比无约束分层基线减少47%的失败级联，提升2.3倍样本效率，并具有更优的可扩展性。

Conclusion: CTHA作为时间分层架构的原则性扩展，将有助于深入理解多智能体协调，并为鲁棒自主系统的演进提供有前景的方向。

Abstract: Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.

</details>


### [8] [Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration](https://arxiv.org/abs/2601.10744)
*Sen Wang,Bangwei Liu,Zhenkun Gao,Lizhuang Ma,Xuhong Wang,Yuan Xie,Xin Tan*

Main category: cs.AI

TL;DR: 提出LMEE框架和MemoryExplorer方法，通过强化学习微调多模态大语言模型，实现主动记忆查询和探索，在长期具身任务中取得显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能体主要关注任务完成结果，忽视了探索过程和记忆利用的重要性。理想的具身智能体应具备终身学习能力，利用长期情景记忆优化决策，处理长期复杂任务。

Method: 提出LMEE框架统一探索认知和决策行为，构建LMEE-Bench数据集和基准。提出MemoryExplorer方法，通过强化学习微调多模态大语言模型，使用包含动作预测、前沿选择和问答的多任务奖励函数，促进主动记忆查询和探索。

Result: 与最先进的具身探索模型相比，该方法在长期具身任务中取得了显著优势，证明了主动记忆查询和探索的有效性。

Conclusion: LMEE框架和MemoryExplorer方法通过强化学习促进主动记忆利用，为具身智能体的终身学习提供了有效解决方案，在长期复杂任务中表现出色。

Abstract: An ideal embodied agent should possess lifelong learning capabilities to handle long-horizon and complex tasks, enabling continuous operation in general environments. This not only requires the agent to accurately accomplish given tasks but also to leverage long-term episodic memory to optimize decision-making. However, existing mainstream one-shot embodied tasks primarily focus on task completion results, neglecting the crucial process of exploration and memory utilization. To address this, we propose Long-term Memory Embodied Exploration (LMEE), which aims to unify the agent's exploratory cognition and decision-making behaviors to promote lifelong learning.We further construct a corresponding dataset and benchmark, LMEE-Bench, incorporating multi-goal navigation and memory-based question answering to comprehensively evaluate both the process and outcome of embodied exploration. To enhance the agent's memory recall and proactive exploration capabilities, we propose MemoryExplorer, a novel method that fine-tunes a multimodal large language model through reinforcement learning to encourage active memory querying. By incorporating a multi-task reward function that includes action prediction, frontier selection, and question answering, our model achieves proactive exploration. Extensive experiments against state-of-the-art embodied exploration models demonstrate that our approach achieves significant advantages in long-horizon embodied tasks.

</details>


### [9] [Optimisation of complex product innovation processes based on trend models with three-valued logic](https://arxiv.org/abs/2601.10768)
*Nina Bočková,Barbora Volná,Mirko Dohnal*

Main category: cs.AI

TL;DR: 使用基于启发式的趋势模型研究复杂产品创新过程，通过简单趋势（增加、减少、恒定）作为最小信息量化器，避免数值依赖，用场景和转移图表示系统行为


<details>
  <summary>Details</summary>
Motivation: 研究复杂产品创新过程需要简化的量化方法，避免对数值或粗糙集的依赖，寻求用最小信息强度的方式描述系统行为

Method: 基于启发式建立趋势模型，每个启发式用简单趋势（增加、减少、恒定）表达，构建场景集合和转移图，系统行为通过图中的路径表示

Result: 提出了一种用趋势模型描述复杂产品创新过程的方法，通过转移图可以描绘系统所有可能的未来或过去行为路径

Conclusion: 基于简单趋势的启发式模型为复杂产品创新过程分析提供了有效的简化框架，避免了传统数值方法的复杂性

Abstract: This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends -- increasing, decreasing, or constant -- which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.

</details>


### [10] [ARC Prize 2025: Technical Report](https://arxiv.org/abs/2601.10904)
*François Chollet,Mike Knoop,Gregory Kamradt,Bryan Landers*

Main category: cs.AI

TL;DR: ARC-AGI-2竞赛显示AI在抽象推理方面仍有局限，当前最佳方法依赖任务特定的精炼循环，前沿AI模型的表现仍受知识覆盖限制，存在基准污染问题。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI基准系列是衡量AI在陌生任务上少样本泛化能力的关键指标，反映了流体智力和抽象推理的核心能力。随着ARC-AGI-2数据集复杂度提升和竞赛参与度增加，需要系统分析当前最佳方法、评估AGI进展状态。

Method: 论文对ARC-AGI-2竞赛进行系统性调查，分析精炼循环（包括进化程序合成和商业AI系统应用层优化）的关键作用，研究零预训练深度学习方法的进展，评估前沿AI实验室的公开表现。

Result: 竞赛最佳成绩仅达24%，显示AI抽象推理能力仍有限。精炼循环成为主导方法，但前沿AI模型表现仍受知识覆盖限制，出现新的基准污染形式。小参数模型（7M参数）已能取得竞争性表现。

Conclusion: 当前AI推理能力仍依赖知识覆盖而非真正的抽象推理，精炼循环是重要进展但仍有局限。ARC-AGI-3将引入交互式推理挑战，需要探索、规划、记忆等更全面的认知能力，推动AGI向更真实智能发展。

Abstract: The ARC-AGI benchmark series serves as a critical measure of few-shot generalization on novel tasks, a core aspect of intelligence. The ARC Prize 2025 global competition targeted the newly released ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. The Kaggle competition attracted 1,455 teams and 15,154 entries, with the top score reaching 24% on the ARC-AGI-2 private evaluation set. Paper submissions nearly doubled year-over-year to 90 entries, reflecting the growing research interest in fluid intelligence and abstract reasoning. The defining theme of 2025 is the emergence of the refinement loop -- a per-task iterative program optimization loop guided by a feedback signal. Refinement loops come in a variety of forms, in particular evolutionary program synthesis approaches and application-layer refinements to commercial AI systems. Such refinement loops are also possible in weight space, as evidenced by zero-pretraining deep learning methods which are now achieving competitive performance with remarkably small networks (7M parameters). In parallel, four frontier AI labs (Anthropic, Google DeepMind, OpenAI, and xAI) reported ARC-AGI performance in public model cards in 2025, establishing ARC-AGI as an industry standard benchmark for AI reasoning. However, our analysis indicates that current frontier AI reasoning performance remains fundamentally constrained to knowledge coverage, giving rise to new forms of benchmark contamination. In this paper, we survey the top-performing methods, examine the role of refinement loops in AGI progress, discuss knowledge-dependent overfitting, and preview ARC-AGI-3, which introduces interactive reasoning challenges that require exploration, planning, memory, goal acquisition, and alignment capabilities.

</details>


### [11] [What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge](https://arxiv.org/abs/2601.10922)
*Yosub Shin,Michael Buriek,Boris Sobolev,Pavel Bushuyeu,Vikas Kumar,Haoyang Xu,Samuel Watson,Igor Molybog*

Main category: cs.AI

TL;DR: 该研究通过NeurIPS 2025 DCVLR挑战赛探索多模态推理的数据策展，发现基于难度的样本选择是性能提升的主要驱动力，而数据集大小增加主要减少方差而非提升平均准确率


<details>
  <summary>Details</summary>
Motivation: 研究多模态推理中的数据策展问题，通过固定模型和训练协议来隔离数据集选择的影响，探索在数据有限情况下如何优化多模态推理性能

Method: 使用NeurIPS 2025 DCVLR挑战赛框架，固定模型和训练协议，主要基于Walton Multimodal Cold Start数据集构建紧凑的策展数据集，并进行后竞赛消融实验分析不同数据策展策略的效果

Result: 基于难度的样本选择是性能提升的主要驱动力；增加数据集大小主要减少运行方差而非提升平均准确率；常用的多样性和合成增强启发式方法无额外益处且常降低性能；该研究在挑战赛中获得了第一名

Conclusion: DCVLR是一个饱和状态评估，强调了对齐和难度在数据高效多模态推理中的核心作用，为多模态数据策展提供了重要见解

Abstract: We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.

</details>


### [12] [AdaMARP: An Adaptive Multi-Agent Interaction Framework for General Immersive Role-Playing](https://arxiv.org/abs/2601.11007)
*Zhenhua Xu,Dongsheng Chen,Shuo Wang,Jian Li,Chengjie Wang,Meng Han,Yabiao Wang*

Main category: cs.AI

TL;DR: AdaMARP：自适应多智能体角色扮演框架，通过沉浸式消息格式和场景管理器解决现有系统沉浸感不足、适应性差的问题，在角色一致性、环境接地和叙事连贯性方面显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM角色扮演系统存在沉浸感有限和适应性不足的问题，通常对环境动态信息建模不足，假设场景和角色基本静态，对多角色编排、场景转换和实时角色引入支持不够。

Method: 提出自适应多智能体角色扮演框架AdaMARP，包含：1）沉浸式消息格式，交织[思考]、(动作)、<环境>和对话；2）显式场景管理器，通过离散动作（初始化场景、选择发言者、切换场景、添加角色、结束）及相应理由来管理角色扮演。

Result: 实验表明：AdaRPSet训练集提升了角色一致性、环境接地和叙事连贯性，8B参数演员模型超越多个商业LLM；AdaSMSet训练集实现了更流畅的场景转换和更自然的角色引入，仅用14B LLM就超越了Claude Sonnet 4.5。

Conclusion: AdaMARP框架通过结构化沉浸式消息格式和显式场景管理，显著提升了多智能体角色扮演的沉浸感和适应性，为动态叙事环境中的角色扮演提供了有效解决方案。

Abstract: LLM role-playing aims to portray arbitrary characters in interactive narratives, yet existing systems often suffer from limited immersion and adaptability. They typically under-model dynamic environmental information and assume largely static scenes and casts, offering insufficient support for multi-character orchestration, scene transitions, and on-the-fly character introduction. We propose an adaptive multi-agent role-playing framework, AdaMARP, featuring an immersive message format that interleaves [Thought], (Action), <Environment>, and Speech, together with an explicit Scene Manager that governs role-playing through discrete actions (init_scene, pick_speaker, switch_scene, add_role, end) accompanied by rationales. To train these capabilities, we construct AdaRPSet for the Actor Model and AdaSMSet for supervising orchestration decisions, and introduce AdaptiveBench for trajectory-level evaluation. Experiments across multiple backbones and model scales demonstrate consistent improvements: AdaRPSet enhances character consistency, environment grounding, and narrative coherence, with an 8B actor outperforming several commercial LLMs, while AdaSMSet enables smoother scene transitions and more natural role introductions, surpassing Claude Sonnet 4.5 using only a 14B LLM.

</details>


### [13] [Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics](https://arxiv.org/abs/2601.11012)
*Jiahao Wang,Shuangjia Zheng*

Main category: cs.AI

TL;DR: HADES是一种基于贝叶斯优化的蛋白质序列设计方法，利用哈密顿动力学进行高效采样，通过结构感知的后验近似和位置离散化过程，在保持蛋白质结构相似性的同时优化功能性质。


<details>
  <summary>Details</summary>
Motivation: 现有基于序列的蛋白质优化方法难以处理高维复杂性，主要因为存在上位效应（epistasis）和忽视结构约束。需要一种能够同时考虑蛋白质结构和序列相互约束的优化方法。

Method: 提出HADES方法：1）利用哈密顿动力学进行贝叶斯优化采样，通过动量和不确定性实现快速向有前景区域的转移；2）引入位置离散化过程，从连续状态系统提出离散蛋白质序列；3）采用两阶段编码器-解码器框架构建后验代理模型，学习突变邻居间的结构-功能关系，获得平滑的采样景观。

Result: 在计算机模拟评估中，HADES在大多数指标上优于现有最先进基线方法。该方法特别优势在于能够利用蛋白质结构和序列的相互约束，设计出结构相似但性质优化的蛋白质序列。

Conclusion: HADES通过结合哈密顿动力学和结构感知的贝叶斯优化，有效解决了蛋白质序列设计中的高维复杂性问题，为生物技术和医学中的蛋白质工程提供了新的优化框架。

Abstract: The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we propose HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior. Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas. A position discretization procedure is introduced to propose discrete protein sequences from such a continuous state system. The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships between mutant neighbors, consequently learning a smoothed landscape to sample from. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines in in-silico evaluations across most metrics. Remarkably, our approach offers a unique advantage by leveraging the mutual constraints between protein structure and sequence, facilitating the design of protein sequences with similar structures and optimized properties. The code and data are publicly available at https://github.com/GENTEL-lab/HADES.

</details>


### [14] [BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search](https://arxiv.org/abs/2601.11037)
*Shiyu Liu,Yongjing Yin,Jianhao Yan,Yunbo Tang,Qinggang Zhang,Bei Li,Xin Chen,Jingang Wang,Xunliang Cai,Jinsong Su*

Main category: cs.AI

TL;DR: BAPO：通过边界感知策略优化提升基于RL的代理搜索可靠性，使LLM在证据不足时承认"我不知道"


<details>
  <summary>Details</summary>
Motivation: 基于RL的代理搜索虽然通过大规模强化学习显著提升了准确性，但存在可靠性缺陷：这些代理无法识别其推理边界，即使在证据不足或推理达到极限时也很少承认"我不知道"。这种可靠性缺失导致看似合理但不可靠的答案，在许多现实场景中带来重大风险。

Method: 提出边界感知策略优化（BAPO）框架，包含两个关键组件：1）基于群体的边界感知奖励，仅在推理达到极限时鼓励IDK响应；2）自适应奖励调节器，在早期探索阶段战略性地暂停此奖励，防止模型将IDK作为捷径利用。

Result: 在四个基准测试上的广泛实验表明，BAPO显著提升了代理搜索的整体可靠性。

Conclusion: BAPO框架能够在保持准确性的同时，培养可靠的边界意识，解决了当前基于RL的代理搜索中存在的可靠性问题。

Abstract: RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.

</details>


### [15] [AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts](https://arxiv.org/abs/2601.11044)
*Keyu Li,Junhao Shi,Yang Xiao,Mohan Jiang,Jie Sun,Yunze Wu,Shijie Xia,Xiaojie Cai,Tianze Xu,Weiye Si,Wenjie Li,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: AgencyBench是一个全面的自主智能体基准测试，从日常AI使用中提取，评估6种核心智能体能力，包含32个真实场景的138个任务，支持自动化评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注单一智能体能力，无法捕捉长视野的真实世界场景，且依赖人工反馈造成可扩展性瓶颈，阻碍自动化评估。

Method: 构建包含32个真实场景、138个任务的基准测试，平均每个任务需要90次工具调用、100万token和数小时执行时间。使用用户模拟智能体提供迭代反馈，Docker沙箱进行视觉和功能评估。

Result: 闭源模型显著优于开源模型（48.4% vs 32.1%），在资源效率、反馈驱动自我修正和特定工具使用偏好方面存在显著差异。专有模型在其原生生态系统中表现更优。

Conclusion: AgencyBench为下一代智能体提供了关键测试平台，强调了模型架构与智能体框架协同优化的必要性，揭示了自主智能体的未来发展方向。

Abstract: Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.

</details>


### [16] [MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic Forecasting](https://arxiv.org/abs/2601.11089)
*Suhan Guo,Jiahong Deng,Furao Shen*

Main category: cs.AI

TL;DR: MiCA是一个轻量级的流行病预测模块，通过因果发现推断移动关系，并通过门控残差混合将其整合到时序预测模型中，在数据有限和噪声条件下提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 人类移动性对流行病空间传播至关重要，但移动数据通常噪声大、间接且难以与疾病记录可靠整合。同时，流行病病例时间序列通常较短且时间分辨率粗糙，这限制了依赖干净丰富数据的参数密集型移动感知预测器的效果。

Method: 提出Mobility-Informed Causal Adapter (MiCA)，一个轻量级且架构无关的模块。通过因果发现推断移动关系，并通过门控残差混合将其整合到时序预测模型中，避免引入图神经网络或完全注意力等重型关系组件。

Result: 在四个真实世界流行病数据集（COVID-19发病率、COVID-19死亡率、流感和登革热）上的实验表明，MiCA能持续改进轻量级时序主干模型，在预测时间范围内平均相对误差降低7.5%，性能与最先进的时空模型相当但保持轻量级。

Conclusion: MiCA为流行病预测提供了一种轻量级解决方案，能在数据有限和噪声条件下有效利用移动性信息，提升预测性能而不增加过多计算负担。

Abstract: Accurate forecasting of infectious disease dynamics is critical for public health planning and intervention. Human mobility plays a central role in shaping the spatial spread of epidemics, but mobility data are noisy, indirect, and difficult to integrate reliably with disease records. Meanwhile, epidemic case time series are typically short and reported at coarse temporal resolution. These conditions limit the effectiveness of parameter-heavy mobility-aware forecasters that rely on clean and abundant data. In this work, we propose the Mobility-Informed Causal Adapter (MiCA), a lightweight and architecture-agnostic module for epidemic forecasting. MiCA infers mobility relations through causal discovery and integrates them into temporal forecasting models via gated residual mixing. This design allows lightweight forecasters to selectively exploit mobility-derived spatial structure while remaining robust under noisy and data-limited conditions, without introducing heavy relational components such as graph neural networks or full attention. Extensive experiments on four real-world epidemic datasets, including COVID-19 incidence, COVID-19 mortality, influenza, and dengue, show that MiCA consistently improves lightweight temporal backbones, achieving an average relative error reduction of 7.5\% across forecasting horizons. Moreover, MiCA attains performance competitive with SOTA spatio-temporal models while remaining lightweight.

</details>


### [17] [ReCreate: Reasoning and Creating Domain Agents Driven by Experience](https://arxiv.org/abs/2601.11100)
*Zhezheng Hao,Hong Wang,Jian Luo,Jianqing Zhang,Yuyan Zhou,Qiang Lin,Can Wang,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: ReCreate：一个基于经验驱动的自动创建领域智能体框架，通过智能体交互历史学习成功与失败原因，实现自动优化和适应


<details>
  <summary>Details</summary>
Motivation: 当前大多数实用智能体仍由人工设计，任务差异大导致构建成本高。现有自动化方法将智能体生成视为黑盒过程，仅依赖最终性能指标，忽略了成功/失败的关键证据，且计算成本高。

Method: 提出ReCreate框架，采用智能体即优化器范式，包含三个核心组件：1) 经验存储与检索机制；2) 推理-创建协同管道，将执行经验映射为脚手架编辑；3) 分层更新，将实例级细节抽象为可重用领域模式。

Result: 在多个不同领域的实验中，ReCreate始终优于人工设计的智能体和现有自动化智能体生成方法，即使从最小种子脚手架开始也能取得良好效果。

Conclusion: ReCreate通过系统利用智能体交互历史中的具体信号，成功解决了自动创建和适应领域智能体的挑战，为智能体自动化生成提供了有效解决方案。

Abstract: Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.

</details>


### [18] [Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems](https://arxiv.org/abs/2601.11147)
*Zixu Wang,Bingbing Xu,Yige Yuan,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出SCALE框架，通过任务级工作流生成和自预测评估，显著降低多智能体系统的令牌消耗，性能仅轻微下降。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统通常通过工作流协调智能体，但现有方法在任务级和查询级工作流生成之间的相对成本和效益不明确。查询级工作流生成并非总是必要，而基于执行的任务级评估既昂贵又不可靠。

Method: 提出SCALE框架：通过少量样本校准的自预测优化器进行评估，替代完整的验证执行。该方法在任务级生成少量最优工作流，覆盖大部分查询需求，避免昂贵的执行评估。

Result: SCALE在多个数据集上保持竞争力，平均性能仅下降0.61%，同时将总体令牌使用量减少高达83%。

Conclusion: 任务级工作流生成配合自预测评估是高效的多智能体系统构建方法，能在显著降低计算成本的同时维持性能。

Abstract: Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \textbf{SCALE}, which means \underline{\textbf{S}}elf prediction of the optimizer with few shot \underline{\textbf{CAL}}ibration for \underline{\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\%.

</details>


### [19] [TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech](https://arxiv.org/abs/2601.11178)
*Girish A. Koushik,Helen Treharne,Diptesh Kanojia*

Main category: cs.AI

TL;DR: TANDEM：一个统一框架，将视听仇恨检测从二元分类任务转变为结构化推理问题，通过跨模态强化学习实现精确的时间定位和目标识别。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体上长格式多模态内容中的有害叙述通过音频、视觉和文本线索的复杂交互构建。现有的自动化仇恨检测系统虽然准确率高，但作为"黑箱"无法提供可解释的证据（如精确时间戳和目标身份），难以支持有效的人机协同审核。

Method: 提出TANDEM框架，采用新颖的串联强化学习策略，其中视觉-语言和音频-语言模型通过自约束的跨模态上下文相互优化，在不需要密集帧级监督的情况下稳定地对扩展时间序列进行推理。

Result: 在三个基准数据集上的实验表明，TANDEM显著优于零样本和上下文增强基线，在HateMM数据集上目标识别F1达到0.73（比最先进方法提升30%），同时保持精确的时间定位。研究发现二元检测稳健，但在多类别设置中区分冒犯性和仇恨内容仍具挑战性。

Conclusion: 即使在复杂的多模态环境中，结构化的可解释对齐也是可以实现的，这为下一代透明且可操作的在线安全审核工具提供了蓝图。

Abstract: Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as "black boxes" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.

</details>


### [20] [Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling Problems](https://arxiv.org/abs/2601.11189)
*Sofiene Lassoued,Asrat Gobachew,Stefan Lier,Andreas Schwung*

Main category: cs.AI

TL;DR: 提出基于策略的深度强化学习超启发式框架解决作业车间调度问题，通过动作预过滤和承诺机制优化调度规则切换，在标准基准测试中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 作业车间调度问题（JSSP）是经典的NP-hard组合优化问题，传统启发式方法需要专家知识且难以适应动态环境。现有方法在调度规则切换和约束处理方面存在局限性，需要更智能的自适应调度框架

Method: 提出基于策略的深度强化学习超启发式框架，包含两个关键机制：1) 动作预过滤：限制决策于可行的低层动作，使低层启发式独立于环境约束进行评估；2) 承诺机制：调节启发式切换频率，研究从步级切换到完整周期承诺的不同策略。同时比较确定性贪婪选择和随机采样两种动作选择策略

Result: 在标准JSSP基准测试上的计算实验表明，该方法优于传统启发式、元启发式和最近的基于神经网络的调度方法

Conclusion: 提出的深度强化学习超启发式框架通过动作预过滤和承诺机制有效解决了JSSP问题，展示了自适应调度规则的潜力，为复杂调度问题提供了新的解决方案

Abstract: This paper proposes a policy-based deep reinforcement learning hyper-heuristic framework for solving the Job Shop Scheduling Problem. The hyper-heuristic agent learns to switch scheduling rules based on the system state dynamically. We extend the hyper-heuristic framework with two key mechanisms. First, action prefiltering restricts decision-making to feasible low-level actions, enabling low-level heuristics to be evaluated independently of environmental constraints and providing an unbiased assessment. Second, a commitment mechanism regulates the frequency of heuristic switching. We investigate the impact of different commitment strategies, from step-wise switching to full-episode commitment, on both training behavior and makespan. Additionally, we compare two action selection strategies at the policy level: deterministic greedy selection and stochastic sampling. Computational experiments on standard JSSP benchmarks demonstrate that the proposed approach outperforms traditional heuristics, metaheuristics, and recent neural network-based scheduling methods

</details>


### [21] [Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning](https://arxiv.org/abs/2601.11252)
*Qianyue Wang,Jinwu Hu,Yufeng Wang,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Mingkui Tan*

Main category: cs.AI

TL;DR: 提出Think-with-Me交互式推理范式，在推理过程中引入外部反馈干预，通过暂停推理获取反馈来减少冗余推理，在有限上下文窗口下实现准确率与推理长度的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在过度思考和推理偏移问题，导致计算成本增加和性能下降。现有高效推理方法缺乏外部干预机制来指导推理过程。

Method: 1) 利用过渡连词作为自然干预点，在推理过程中暂停获取外部反馈；2) 基于多标准评估（合理性和完整性）生成反馈；3) 使用Group Relative Policy Optimization训练模型适应交互模式。

Result: 在AIME24上，Think-with-Me比QwQ-32B准确率提升7.19%，同时平均推理长度减少81%（8K窗口下）。该范式在安全和创意任务中也表现出优势。

Conclusion: Think-with-Me通过引入外部反馈干预，有效解决了大型推理模型的效率问题，在有限上下文窗口下实现了准确率与推理效率的更好平衡，为交互式推理提供了新范式。

Abstract: Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.

</details>


### [22] [XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making](https://arxiv.org/abs/2601.11286)
*Weihong Qi,Fan Huang,Rasika Muralidharan,Jisun An,Haewoon Kwak*

Main category: cs.AI

TL;DR: XChoice是一个可解释的框架，用于评估约束决策中AI与人类的对齐程度，通过机制建模而非表面结果来诊断对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注准确性、F1分数等结果一致性，缺乏对决策机制的深入理解，无法诊断AI与人类在约束决策中的根本对齐问题。

Method: XChoice为人类数据和LLM生成的决策拟合基于机制的决策模型，恢复可解释参数（决策因素相对重要性、约束敏感性、隐含权衡），通过比较参数向量来评估对齐。

Result: 在美国时间分配研究中，发现不同模型和活动之间存在异质性对齐，黑人和已婚群体中存在显著错位；通过不变性分析验证了鲁棒性，RAG干预展示了针对性缓解效果。

Conclusion: XChoice提供基于机制的度量标准，能够诊断错位并支持超越表面结果匹配的知情改进，为AI与人类对齐评估提供了更深入的分析框架。

Abstract: We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.

</details>


### [23] [AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems](https://arxiv.org/abs/2601.11354)
*Weiyi Wang,Xinchi Chen,Jingjing Gong,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: AstroReason-Bench：用于评估太空规划问题中智能体规划能力的基准测试，发现当前智能体在物理约束下的表现远不如专用求解器。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准主要关注符号或弱接地环境，缺乏对物理约束现实世界领域的评估。太空规划问题具有异构目标、严格物理约束和长时程决策等特点，需要专门的评估基准。

Method: 引入AstroReason-Bench基准，整合地面站通信和敏捷地球观测等多种调度机制，提供统一的智能体导向交互协议，评估最先进的开放和闭源智能体LLM系统。

Result: 当前智能体在太空规划问题上的表现显著低于专用求解器，突显了在现实约束下通用规划的关键局限性。

Conclusion: AstroReason-Bench为未来智能体研究提供了一个具有挑战性和诊断性的测试平台，有助于推动智能体在物理约束现实世界领域的发展。

Abstract: Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.

</details>


### [24] [Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs](https://arxiv.org/abs/2601.11468)
*Alessandro Padella,Massimiliano de Leoni,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文扩展了基于LLM的预测性过程监控框架，评估其在多KPI下的泛化能力、语义利用和推理机制，在数据稀缺场景中LLM表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 预测性过程监控旨在预测进行中过程的结果，现有方法多采用机器学习和深度学习架构。本文旨在扩展先前基于LLM的框架，全面评估其在不同关键性能指标下的表现，特别是在数据稀缺环境中的能力。

Method: 扩展了先前基于LLM的预测性过程监控框架，通过提示工程实现多关键性能指标预测。在三个不同事件日志上进行实证评估，重点关注总时间和活动发生频率两个KPI，分析LLM在数据稀缺环境下的表现。

Result: 在仅有100条轨迹的数据稀缺设置中，LLM超越了基准方法。实验表明LLM能够利用其先验知识和训练轨迹间的内部相关性。模型不仅复制现有预测方法，而是执行高阶推理来生成预测。

Conclusion: 基于LLM的预测性过程监控框架在数据稀缺环境中具有优势，能够利用语义知识和推理能力，为过程挖掘领域提供了新的有效方法。

Abstract: Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.

</details>


### [25] [Hyperparameter Optimization of Constraint Programming Solvers](https://arxiv.org/abs/2601.11389)
*Hedieh Haddad,Thibault Falque,Pierre Talbot,Pascal Bouvry*

Main category: cs.AI

TL;DR: 提出"探针与求解"两阶段框架，通过贝叶斯优化自动调优约束规划求解器超参数，在CPMpy库中实现，显著提升求解性能


<details>
  <summary>Details</summary>
Motivation: 约束规划求解器的性能对超参数选择高度敏感，手动寻找最佳配置需要专家知识且耗时，需要自动化超参数优化方法

Method: 提出"探针与求解"两阶段框架：1) 探针阶段使用可配置的超参数优化方法探索不同参数集；2) 求解阶段使用找到的最佳配置在剩余时间内解决问题。在CPMpy库中实现并比较贝叶斯优化和汉明距离搜索两种方法

Result: 贝叶斯优化方法优于默认配置：ACE求解器在25.4%实例中提升求解质量，57.9%实例中与默认性能相当；Choco求解器在38.6%实例中取得更好结果。贝叶斯优化也始终优于同一框架内的汉明距离搜索

Conclusion: "探针与求解"算法提供了一种实用、资源感知的约束求解器调优方法，在多种问题类型上都能获得稳健的改进，模型化探索优于简单局部搜索

Abstract: The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.
  We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.
  Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.

</details>


### [26] [Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning](https://arxiv.org/abs/2601.11479)
*Yohai Trabelsi,Guojun Xiong,Fentabil Getnet,Stéphane Verguet,Milind Tambe*

Main category: cs.AI

TL;DR: 该论文提出了一个结合大语言模型和扩展贪心算法的混合框架（LEG），用于优化埃塞俄比亚农村卫生站升级的优先级排序，在保证人口覆盖理论保证的同时融入专家定性指导。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚卫生部需要升级卫生站以改善农村地区基本医疗服务可及性，但资源有限需要优先排序。传统优化方法需要明确的量化目标，而利益相关者的标准通常用自然语言表达且难以形式化，需要一种能结合专家知识和优化技术的方法。

Method: 开发了LEG（大语言模型和扩展贪心）框架，结合了人口覆盖优化的可证明近似算法与LLM驱动的迭代优化，通过人机对齐确保解决方案既反映专家定性指导，又保持覆盖保证。

Result: 在埃塞俄比亚三个地区的真实数据上进行实验，证明了该框架的有效性，展示了其在促进公平、数据驱动的卫生系统规划方面的潜力。

Conclusion: LEG框架成功地将专家定性知识与优化技术相结合，为资源有限的卫生系统规划提供了既能保证理论覆盖保证又能反映实际专家意见的实用解决方案。

Abstract: Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.

</details>


### [27] [BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics](https://arxiv.org/abs/2601.11492)
*Kaiwen Wang,Kaili Zheng,Rongrong Deng,Qingmin Fan,Milin Zhang,Zongrui Li,Xuesi Zhou,Bo Han,Liren Chen,Chenyi Guo,Ji Wu*

Main category: cs.AI

TL;DR: BoxMind：一个基于图神经网络的拳击AI专家系统，通过解析比赛视频为战术指标，预测比赛结果并生成战术建议，在2024巴黎奥运会帮助中国队获得3金2银。


<details>
  <summary>Details</summary>
Motivation: 格斗类运动（如拳击）的战术分析在AI领域发展不足，主要因为动作动态复杂且缺乏结构化战术表示。需要将非结构化视频数据转化为战略智能，弥合计算机视觉与竞技体育决策支持之间的差距。

Method: 1. 定义原子拳击事件（精确时空和技术属性）；2. 将比赛视频解析为18个层次化技术-战术指标；3. 提出基于图的预测模型，融合显性技术-战术特征与可学习的时变潜在嵌入；4. 将比赛结果建模为技术-战术指标的可微函数，将获胜概率梯度转化为可执行的战术调整。

Result: 1. 结果预测模型达到SOTA性能：BoxerGraph测试集准确率69.8%，奥运比赛准确率87.5%；2. 系统生成的战略建议达到人类专家水平；3. 在2024巴黎奥运会闭环部署，直接帮助中国国家队获得3金2银的历史性成绩。

Conclusion: BoxMind建立了将非结构化视频数据转化为战略智能的可复制范式，成功弥合了计算机视觉与竞技体育决策支持之间的差距，为格斗类运动的AI分析提供了有效框架。

Abstract: Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [28] [On the Entropy of a Random Geometric Graph](https://arxiv.org/abs/2601.10778)
*Praneeth Kumar Vippathalla,Justin P. Coon,Mihai-Alin Badiu*

Main category: cs.IT

TL;DR: 该论文研究了硬随机几何图(RGG)的熵，推导了在不同空间域和连接半径下的熵上界，并在某些情况下获得了精确的渐近特征。


<details>
  <summary>Details</summary>
Motivation: 随机几何图是空间网络的常用模型，但其信息熵特性尚未得到充分研究。理解RGG的熵有助于量化其结构复杂性，并为网络分析和信息论提供理论基础。

Method: 通过分析两个空间域（d维单位立方体[0,1]^d和d维单位环面𝕋^d）上的硬RGG模型，推导熵的上界。在某些情况下，通过证明紧下界获得熵的精确渐近特征。

Result: 主要结果：1) 在𝕋^d上，当0<r≤1/4时，H(G_m)∼dm log₂m；2) 在一维[0,1]上，对所有0<r<1，熵表现为m log m；3) 推断出𝕋^d上未标记RGG的结构熵为Ω((d-1)m log₂m)。

Conclusion: 论文成功刻画了硬随机几何图在不同参数设置下的熵行为，为理解空间网络的信息复杂度提供了理论框架，并提出了其他情况下的渐近行为猜想。

Abstract: In this paper, we study the entropy of a hard random geometric graph (RGG), a commonly used model for spatial networks, where the connectivity is governed by the distances between the nodes. Formally, given a connection range $r$, a hard RGG $G_m$ on $m$ vertices is formed by drawing $m$ random points from a spatial domain, and then connecting any two points with an edge when they are within a distance $r$ from each other. The two domains we consider are the $d$-dimensional unit cube $[0,1]^d$ and the $d$-dimensional unit torus $\mathbb{T}^d$. We derive upper bounds on the entropy $H(G_m)$ for both these domains and for all possible values of $r$. In a few cases, we obtain an exact asymptotic characterization of the entropy by proving a tight lower bound. Our main results are that $H(G_m) \sim dm \log_2m$ for $0 < r \leq 1/4$ in the case of $\mathbb{T}^d$ and that the entropy of a one-dimensional RGG on $[0,1]$ behaves like $m\log m$ for all $0<r<1$. As a consequence, we can infer that the asymptotic structural entropy of an RGG on $\mathbb{T}^d$, which is the entropy of an unlabelled RGG, is $Ω((d-1)m \log_2m)$ for $0 < r \leq 1/4$. For the rest of the cases, we conjecture that the entropy behaves asymptotically as the leading order terms of our derived upper bounds.

</details>


### [29] [Efficient LLR-Domain Decoding of ABS+ Polar Codes](https://arxiv.org/abs/2601.10808)
*Mikhail Chernikov,Peter Trifonov*

Main category: cs.IT

TL;DR: ABS+极码的LLR域SCL解码器实现与优化，相比经典极码在相同FER下减少算术运算


<details>
  <summary>Details</summary>
Motivation: ABS+极码作为Arikan极码的推广，具有更快的极化速度，但需要高效的解码器实现来充分发挥其优势

Method: 提出了ABS+极码的LLR域SCL解码器实现，并优化了SCL算法以降低LLR计算的复杂度需求

Result: 与经典极码相比，所提方法在高信噪比区域获得相同帧错误率时，SCL解码器所需的算术运算次数更少

Conclusion: LLR域SCL解码器优化有效降低了ABS+极码的解码复杂度，使其在实际应用中更具优势

Abstract: ABS+ polar codes are a generalization of Arikan polar codes that provides much faster polarization. We present an LLR-domain implementation of the SCL decoder of ABS+ polar codes. Furthermore, we optimize the SCL algorithm in order to reduce the complexity requirements for the LLRs computation. In comparison with classical polar codes, the proposed approach requires less number of arithmetic operations in the SCL decoder to obtain the fixed frame error rate (FER) at high-SNR region.

</details>


### [30] [Fundamental Limits of Quantum Semantic Communication via Sheaf Cohomology](https://arxiv.org/abs/2601.10958)
*Christo Kurisummoottil Thomas,Mingzhe Chen*

Main category: cs.IT

TL;DR: 该论文提出了一个量子语义通信的信息论框架，利用层上同调理论建模多智能体语义网络，证明了语义对齐所需的最小通信速率与同调空间维数的对数成正比，并展示了量子纠缠可以超越经典界限。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，当智能体采用异构的感知模态和AI架构时，完美的比特级传输不再保证相互理解。尽管深度学习的语义压缩方法有所进展，但异构性下语义对齐的信息论极限仍然不清楚。语义模糊性与量子上下文性具有相同的数学结构，都源于上同调障碍，这促使了量子语义通信的提出。

Method: 使用层上同调理论构建量子语义通信的信息论框架。将多智能体语义网络建模为量子层，其中智能体的意义空间是希尔伯特空间，通过量子信道连接。第一层上同调群被用来表征不可约的语义模糊性。

Result: 证明了语义对齐所需的最小通信速率与同调空间维数的对数成正比，建立了语义的香农极限类比。对于纠缠辅助信道，可实现容量严格超过经典界限，每个共享的纠缠比特可以减少一比特的经典通信需求。量子上下文性可以减少同调障碍，并建立了量子不和谐与集成语义信息之间的对偶性。

Conclusion: 该框架为自主多智能体系统中的量子增强语义通信提供了严格的理论基础，将量子相关性连接到不可约的语义内容，为异构智能体之间的高效语义对齐提供了新的理论工具。

Abstract: Semantic communication (SC) enables bandwidth-efficient coordination in multi-agent systems by transmitting meaning rather than raw bits. However, when agents employ heterogeneous sensing modalities and AI architectures, perfect bit-level transmission no longer guarantees mutual understanding. Although deep learning methods for semantic compression have advanced, the information-theoretic limits of semantic alignment under heterogeneity remain poorly understood. Notably, semantic ambiguity shares the same mathematical structure as quantum contextuality, as both arise from cohomological obstructions, motivating a quantum formulation of SC. In this paper, an information-theoretic framework for quantum semantic communication is proposed using sheaf cohomology. Multi-agent semantic networks are modeled as quantum sheaves, where agents meaning spaces are Hilbert spaces connected by quantum channels. The first sheaf cohomology group is shown to characterize irreducible semantic ambiguity, representing a fundamental obstruction to alignment that no local processing can resolve. The minimum communication rate required for semantic alignment is proven to scale with the logarithm of the dimension of the cohomological space, establishing a semantic analog of Shannon limits. For entanglement-assisted channels, the achievable capacity is shown to strictly exceed classical bounds, with each shared ebit reducing the required classical communication by one bit, providing a rigorous interpretation of shared context. Additionally, quantum contextuality is shown to reduce cohomological obstructions, and a duality between quantum discord and integrated semantic information is established, linking quantum correlations to irreducible semantic content. This framework provides rigorous foundations for quantum-enhanced semantic communication in autonomous multi-agent systems.

</details>


### [31] [A Differential Geometry and Algebraic Topology Based Public-Key Cryptographic Algorithm in Presence of Quantum Adversaries](https://arxiv.org/abs/2601.10883)
*Andrea Rondelli*

Main category: cs.IT

TL;DR: Z-Sigil是一种基于功能分析、微分几何和代数拓扑的非对称公钥密码算法，旨在抵抗经典和量子攻击，通过在紧致Calabi-Yau流形的切丛上构建密码系统来实现安全性。


<details>
  <summary>Details</summary>
Motivation: 将古代印章的信任、保密和完整性传统延续到量子计算机、经典超级计算机和人工智能时代，开发能够抵抗经典和量子攻击的新型密码系统。

Method: 在紧致Calabi-Yau流形的切纤维丛上构建密码系统，密钥是切纤维向量元素，在基流形切空间上定义二元运算形成群胚结构。加密解密在消息块上迭代执行，采用串行架构限制量子并行性，每个块依赖于秘密几何和分析数据。

Result: 算法被证明具有正确性和可逆性，任何无私钥的明文恢复尝试都会导致攻击搜索空间指数增长，即使存在量子加速。该方法与基于离散代数假设的现有量子安全密码方案有本质区别。

Conclusion: Z-Sigil通过连续几何结构、非线性算子组合和强制块串行化，提供了一种新的量子安全密码方法，能够抵抗经典和量子攻击，延续了印章在数字时代的信任和安全传统。

Abstract: In antiquity, the seal embodied trust, secrecy, and integrity in safeguarding the exchange of letters and messages. The purpose of this work is to continue this tradition in the contemporary era, characterized by the presence of quantum computers, classical supercomputers, and increasingly sophisticated artificial intelligence. We introduce Z-Sigil, an asymmetric public-key cryptographic algorithm grounded in functional analysis, differential geometry, and algebraic topology, with the explicit goal of achieving resistance against both classical and quantum attacks. The construction operates over the tangent fiber bundle of a compact Calabi-Yau manifold [13], where cryptographic keys are elements of vector tangent fibers, with a binary operation defined on tangent spaces of the base manifold giving rise to a groupoid structure. Encryption and decryption are performed iteratively on message blocks, enforcing a serial architecture designed to limit quantum parallelism [9,10]. Each block depends on secret geometric and analytic data, including a randomly chosen base point on the manifold, a selected section of the tangent fiber bundle, and auxiliary analytic data derived from operator determinants and Zeta function regularization [11]. The correctness and invertibility of the proposed algorithm are proven analytically. Furthermore, any adversarial attempt to recover the plaintext without the private key leads to an exponential growth of the adversarial search space,even under quantum speedups. The use of continuous geometric structures,non-linear operator compositions,and enforced blockwise serialization distinguishes this approach from existing quantum-safe cryptographic proposals based on primary discrete algebraic assumptions.

</details>


### [32] [Convergence Properties of Good Quantum Codes for Classical Communication](https://arxiv.org/abs/2601.11498)
*Alptug Aytekin,Mohamed Nomeir,Lei Hu,Sennur Ulukus*

Main category: cs.IT

TL;DR: 将经典信道容量理论中关于好码输出统计特性的结果推广到量子信道中的经典通信场景


<details>
  <summary>Details</summary>
Motivation: 经典信息论中已有关于达到容量的码的输出统计特性研究，包括其经验分布与信道容量问题中最优输入诱导的输出分布的比较。本文旨在将类似结果推广到量子信道中用于经典通信的量子码

Method: 1. 首先证明最优输出分布的唯一性；2. 使用类似经典方法将渐近消失错误概率结果推广到量子情况；3. 利用基于量子广义去极化半群超压缩性的二阶逆定理，将非消失错误概率结果推广到量子分组码

Result: 成功将经典信息论中关于好码输出统计特性的结果扩展到量子信道中的经典通信场景，包括渐近消失和非消失错误概率情况

Conclusion: 本文建立了量子信道中经典通信码的输出统计特性理论，将经典信息论中的相关结果系统地推广到量子领域

Abstract: An important part of the information theory folklore had been about the output statistics of codes that achieve the capacity and how the empirical distributions compare to the output distributions induced by the optimal input in the channel capacity problem. Results for a variety of such empirical output distributions of good codes have been known in the literature, such as the comparison of the output distribution of the code to the optimal output distribution in vanishing and non-vanishing error probability cases. Motivated by these, we aim to achieve similar results for the quantum codes that are used for classical communication, that is the setting in which the classical messages are communicated through quantum codewords that pass through a noisy quantum channel. We first show the uniqueness of the optimal output distribution, to be able to talk more concretely about the optimal output distribution. Then, we extend the vanishing error probability results to the quantum case, by using techniques that are close in spirit to the classical case. We also extend non-vanishing error probability results to the quantum case on block codes, by using the second-order converses for such codes based on hypercontractivity results for the quantum generalized depolarizing semi-groups.

</details>


### [33] [A PAC-Bayesian Analysis of Channel-Induced Degradation in Edge Inference](https://arxiv.org/abs/2601.10915)
*Yangshuo He,Guanding Yu,Jingge Zhu*

Main category: cs.IT

TL;DR: 提出一种针对边缘推理中无线信道噪声的神经网络训练方法，通过理论分析信道引起的性能恶化，并设计信道感知训练算法提升推理精度


<details>
  <summary>Details</summary>
Motivation: 边缘推理中神经网络在无噪声环境下训练，但实际部署时面临无线信道噪声，导致性能不匹配问题

Method: 1) 提出增强型神经网络模型，将信道统计特性融入权重空间；2) 推导PAC-Bayesian泛化界量化无线失真影响；3) 针对实际信道给出闭式表达式；4) 基于理论结果设计信道感知训练算法

Result: 仿真表明提出的算法能有效利用信道统计特性提升推理精度，无需端到端重新训练

Conclusion: 通过理论分析信道引起的泛化误差并设计信道感知训练，能有效解决边缘推理中训练-部署环境不匹配问题

Abstract: In the emerging paradigm of edge inference, neural networks (NNs) are partitioned across distributed edge devices that collaboratively perform inference via wireless transmission. However, standard NNs are generally trained in a noiseless environment, creating a mismatch with the noisy channels during edge deployment. In this paper, we address this issue by characterizing the channel-induced performance deterioration as a generalization error against unseen channels. We introduce an augmented NN model that incorporates channel statistics directly into the weight space, allowing us to derive PAC-Bayesian generalization bounds that explicitly quantifies the impact of wireless distortion. We further provide closed-form expressions for practical channels to demonstrate the tractability of these bounds. Inspired by the theoretical results, we propose a channel-aware training algorithm that minimizes a surrogate objective based on the derived bound. Simulations show that the proposed algorithm can effectively improve inference accuracy by leveraging channel statistics, without end-to-end re-training.

</details>


### [34] [Asymmetric Encoding-Decoding Schemes for Lossless Data Compression](https://arxiv.org/abs/2601.10991)
*Hirosuke Yamamoto,Ken-ichi Iwata*

Main category: cs.IT

TL;DR: 本文提出了一种新的无损数据压缩编码方案AEDS（非对称编解码方案），作为tANS的推广，具有更广的编码类别，在某些条件下能获得比霍夫曼编码更短的平均码长。


<details>
  <summary>Details</summary>
Motivation: tANS（非对称数字系统的表格化变体）在数据压缩中表现出色，但其编码类别有限。本文旨在提出一种更通用的编码方案，扩展tANS的能力，同时保持其高效性。

Method: 提出AEDS方案，数据序列采用反向顺序编码（从后向前），但保持与tANS相同的正向解码顺序。通过理论分析证明AEDS具有比tANS更广的编码类别，并针对i.i.d.源推导了平均码长的上界。

Result: 1) 对于i.i.d.源，当霍夫曼编码树根节点的子节点概率大于0.61803时，2状态AEDS的平均码长比霍夫曼编码更短；当概率大于0.56984时，5状态AEDS表现更优。2) 推导了AEDS和tANS平均码长的多个上界。3) 证明了最优AEDS和tANS的平均码长以O(1/N)的速度收敛到源熵。

Conclusion: AEDS作为tANS的推广，提供了更灵活的编码方案，在某些条件下能超越霍夫曼编码的性能，同时保持了与源熵的渐近收敛性，为无损数据压缩提供了新的理论框架。

Abstract: This paper proposes a new lossless data compression coding scheme named an asymmetric encoding-decoding scheme (AEDS), which can be considered as a generalization of tANS (tabled variant of asymmetric numeral systems). In the AEDS, a data sequence $\bm{s}=s_1s_2\cdots s_n$ is encoded in backward order $s_t, t=n, \cdots, 2,1$, while $\bm{s}$ is decoded in forward order $s_t, t=1, 2, \cdots, n$ in the same way as the tANS. But, the code class of the AEDS is much broader than that of the tANS. We show for i.i.d.~sources that an AEDS with 2 states (resp.~5 states) can attain a shorter average code length than the Huffman code if a child of the root in the Huffman code tree has a probability weight larger than 0.61803 (resp.~0.56984). Furthermore, we derive several upper bounds on the average code length of the AEDS, which also hold for the tANS, and we show that the average code length of the optimal AEDS and tANS with $N$ states converges to the source entropy with speed $O(1/N)$ as $N$ increases.

</details>


### [35] [PEMNet: Towards Autonomous and Enhanced Environment-Aware Mobile Networks](https://arxiv.org/abs/2601.11025)
*Lei Li,Yanqing Xu,Ye Xue,Feng Yin,Chao Shen,Rui Zhang,Tsung-Hui Chang*

Main category: cs.IT

TL;DR: 提出感知嵌入地图(PEM)，通过联合嵌入信道统计和流量模式，为5G/6G网络提供低开销的环境感知优化方案


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络需要在动态环境中做出低延迟、低能耗的决策，但需要先了解无线信道和流量需求的空间-时间变化。现有方法缺乏联合信道-流量表示，且部署成本高。

Method: 提出感知嵌入地图(PEM)框架，将细粒度信道统计与网格级空间-时间流量模式联合嵌入基站覆盖区域。基于标准测量报告（如测量报告、调度/QoS日志）构建，可实现低成本大规模部署。

Result: PEM支持跨PHY、MAC和网络层的环境感知优化，显著降低训练开销和信令。相比现有站点特定信道地图和数字孪生副本，PEM强调联合信道-流量嵌入和基于标准测量的实用构建。

Conclusion: PEM为5G/6G网络提供了一种实用的联合信道-流量表示框架，能够在保证保真度的同时实现网络自主性，平衡了性能与成本。

Abstract: With 5G deployment and the evolution toward 6G, mobile networks must make decisions in highly dynamic environments under strict latency, energy, and spectrum constraints. Achieving this goal, however, depends on prior knowledge of spatial-temporal variations in wireless channels and traffic demands. This motivates a joint, site-specific representation of radio propagation and user demand that is queryable at low online overhead. In this work, we propose the perception embedding map (PEM), a localized framework that embeds fine-grained channel statistics together with grid-level spatial-temporal traffic patterns over a base station's coverage. PEM is built from standard-compliant measurements -- such as measurement report and scheduling/quality-of-service logs -- so it can be deployed and maintained at scale with low cost. Integrated into PEM, this joint knowledge supports enhanced environment-aware optimization across PHY, MAC, and network layers while substantially reducing training overhead and signaling. Compared with existing site-specific channel maps and digital-twin replicas, PEM distinctively emphasizes (i) joint channel-traffic embedding, which is essential for network optimization, and (ii) practical construction using standard measurements, enabling network autonomy while striking a favorable fidelity-cost balance.

</details>


### [36] [Sensing Mutual Information for Communication Signal with Deterministic Pilots and Random Data Payloads](https://arxiv.org/abs/2601.11149)
*Lei Xie,Hengtao He,Jun Tong,Fan Liu,Shenghui Song*

Main category: cs.IT

TL;DR: 该论文研究了集成感知与通信系统中，针对包含导频和数据载荷的混合信号结构的感知互信息分析与预编码设计。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC研究主要关注纯随机或纯确定性波形，忽略了实际通信标准中普遍使用的导频（确定性）与数据载荷（随机）混合结构，需要填补这一理论空白。

Method: 利用随机矩阵理论推导混合信号感知互信息的闭式表达式，基于此建立预编码优化问题，采用交替方向乘子法框架求解。

Result: 仿真验证了理论结果的准确性，并证明所提预编码设计在感知性能上优于传统基准方法。

Conclusion: 该研究为实际ISAC系统中混合信号结构的感知能力量化提供了理论框架和优化方案，填补了现有文献的空白。

Abstract: The recent emergence of the integrated sensing and communication (ISAC) framework has sparked significant interest in quantifying the sensing capabilities inherent in communication signals. However, existing literature has mainly focused on scenarios involving either purely random or purely deterministic waveforms. This overlooks a critical reality: operational communication standards invariably utilize a hybrid structure comprising both deterministic pilots for channel estimation and random payloads for data transmission. To bridge this gap, this paper investigates the sensing mutual information (SMI) and precoding design specifically for ISAC systems employing communication signals with both pilots and data payloads. First, by utilizing random matrix theory (RMT), we derive a tractable closed-form expression for the SMI that accurately accounts for the statistical properties of the hybrid signal. Building upon this theoretical foundation, we formulate a precoding optimization problem to maximize SMI with constraints on the transmit power and communication rate, which is solved via an efficient alternating direction method of multipliers framework. Simulation results validate the accuracy of the theoretical results and demonstrate the superiority of the proposed precoding design over conventional benchmarks.

</details>


### [37] [Performance Analysis of Cell-Free Massive MIMO under Imperfect LoS Phase Tracking](https://arxiv.org/abs/2601.11179)
*Noor Ul Ain,Lorenzo Miretti,Renato L. G. Cavalcante,Sławomir Stańczak*

Main category: cs.IT

TL;DR: 该论文研究了非完美视距相位跟踪对无蜂窝大规模MIMO网络性能的影响，提出了包含相位误差的Rician衰落模型和线性MMSE信道估计器，并开发了可处理的集中式和分布式波束成形方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常假设视距相位完全已知或完全未知，但实际系统中由于硬件损伤、移动性和同步误差，相位估计存在残余不确定性。需要建立一个更现实的模型来评估实际相位跟踪限制对网络性能的影响。

Method: 提出了一种Rician衰落模型，其中视距分量被不完美的相位估计旋转，并通过确定性相位误差惩罚因子衰减。推导了捕获统计相位误差的线性MMSE信道估计器，并引入了保持信道估计二阶统计特性的虚拟上行链路模型，从而推导出可处理的集中式和分布式MMSE波束成形器。

Result: 数值结果表明，该框架在理想化假设和实际跟踪限制之间建立了桥梁，为6G无蜂窝网络提供了严格的性能基准和设计见解。提出的方法在完美相位知识和无相位知识两种极端情况下都能简化为现有结果。

Conclusion: 该研究为评估实际相位跟踪限制对无蜂窝大规模MIMO网络性能的影响提供了系统框架，填补了现有研究在非完美相位跟踪方面的空白，对6G网络设计具有重要指导意义。

Abstract: We study the impact of imperfect line-of-sight (LoS) phase tracking on the performance of cell-free massive MIMO networks. Unlike prior works that assume perfectly known or completely unknown phases, we consider a realistic regime where LoS phases are estimated with residual uncertainty due to hardware impairments, mobility, and synchronization errors. To this end, we propose a Rician fading model where LoS components are rotated by imperfect phase estimates and attenuated by a deterministic phase-error penalty factor. We derive a linear MMSE channel estimator that captures statistical phase errors and unifies prior results, reducing to the Bayesian MMSE estimator with perfect phase knowledge and to a zero-mean model in the absence of phase knowledge. To address the non-Gaussian setting, we introduce a virtual uplink model that preserves second-order statistics of channel estimation, enabling the derivation of tractable centralized and distributed MMSE beamformers. To ensure fair assessment of the network performance, we apply these beamformers to the true uplink model and compute the spectral efficiency bounds available in the literature. Numerical results show that our framework bridges idealized assumptions and practical tracking limitations, providing rigorous performance benchmarks and design insights for 6G cell-free networks.

</details>


### [38] [Rate-Distortion-Perception Tradeoff for the Gray-Wyner Problem](https://arxiv.org/abs/2601.11257)
*Yu Yang,Yingxin Zhang,Weijie Yuan,Lin Zhou*

Main category: cs.IT

TL;DR: 本文扩展了Gray-Wyner有损信源编码问题，首次推导了在重建序列上施加感知约束时的一阶渐近最优率失真感知区域。


<details>
  <summary>Details</summary>
Motivation: 图像和视频压缩等实际应用中，不仅需要压缩效率，还需要重建序列的分布与原始源序列分布接近。先前研究主要关注单源序列的压缩和重建，本文将其推广到Gray-Wyner问题的多终端设置。

Method: 通过将随机循环移位算子直接集成到编码和解码过程中，统一分析失真和感知约束。推导出由共同信息和两个条件率失真感知函数控制的互信息项。

Result: 获得了具有感知约束的Gray-Wyner系统的一阶渐近最优率失真感知区域，展示了压缩率、失真度和感知质量之间的最优权衡关系。

Conclusion: 成功将点对点系统的率失真感知分析推广到多终端Gray-Wyner设置，为相关源序列的联合压缩提供了理论框架，对实际图像视频压缩应用具有重要意义。

Abstract: We revisit the Gray-Wyner lossy source coding problem and derive the first-order asymptotic optimal rate-distortion-perception region when additional perception constraints are imposed on reproduced source sequences. The optimal trade-off is shown to be governed by a mutual information term involving common information and two conditional rate-distortion-perception functions. The perception constraint requires that the distribution of each reproduced sequence is close to that of the original source sequence, which is motivated by practical applications in image and video compression. Prior studies usually focus on the compression and reconstruction of a single source sequence. In this paper, we generalize the prior results for point-to-point systems to the representative multi-terminal setting of the Gray-Wyner problem with two correlated source sequences. In particular, we integrate the analyses of the distortion and the perception constraints by including the random circular shift operator in the encoding and decoding process directly.

</details>


### [39] [Joint Antenna Rotation and IRS Beamforming for Multi-User Uplink Communications](https://arxiv.org/abs/2601.11291)
*Guoying Zhang,Qingqing Wu,Ziyuan Zheng,Qiaoyan Peng,Yanze Zhu,Wen Chen,Penghui Huang*

Main category: cs.IT

TL;DR: 提出一种可旋转天线与智能反射面协同的多用户上行系统，通过联合优化天线3D旋转、接收波束成形和IRS相位偏移来最大化和速率，显著提升传统固定天线系统性能


<details>
  <summary>Details</summary>
Motivation: 传统可旋转天线在物理遮挡下性能下降，而智能反射面在基站天线旁瓣区域部署时存在角度失配问题。需要一种新系统来同时解决这两个问题

Method: 提出RA-enabled IRS辅助多用户上行系统，基站天线可灵活调整3D方向以对准IRS。采用交替优化算法：1) 投影梯度上升更新天线旋转；2) 闭式解计算接收波束成形；3) 分数规划优化IRS相位偏移

Result: 数值结果表明，与传统固定天线系统相比，所提系统在角度失配较大时能带来显著的性能增益

Conclusion: 通过联合优化天线3D旋转、接收波束成形和IRS相位偏移，可旋转天线与智能反射面协同的系统能有效解决角度失配问题，提升无线覆盖性能

Abstract: Rotatable antenna (RA) enhances wireless coverage through directional gain steering, yet suffers from performance degradation under physical blockages. Intelligent reflecting surface (IRS) establishes reflective paths to bypass obstacles, but suffers from angular mismatch when deployed in the side-lobe region of base station (BS) antennas. To address this issue, we propose a new RA-enabled IRS-assisted multi-user uplink system, in which the BS antennas are capable of flexibly adjusting their 3D orientations to align their boresights with the IRS. We formulate a sum rate maximization problem by jointly optimizing the antenna 3D rotations, receive beamforming and IRS phase shifts. To tackle this non-convex problem, we propose an efficient alternating optimization (AO) algorithm. Specifically, we iteratively update the antenna rotations via projected gradient ascent (PGA), compute the receive beamforming via a closed-form solution, and optimize the IRS phase shifts via fractional programming (FP). Numerical results demonstrate that the proposed system yields significant performance gains over conventional fixed-antenna systems, especially under large angular misalignments.

</details>


### [40] [Information Theoretic Perspective on Representation Learning](https://arxiv.org/abs/2601.11334)
*Deborah Pereg*

Main category: cs.IT

TL;DR: 提出信息论框架分析最后一层嵌入表示，研究回归任务中学习表示的信息理论极限


<details>
  <summary>Details</summary>
Motivation: 需要理论框架来分析回归任务中最后一层嵌入表示的信息容量和可靠性极限，理解输入-输出信息表示的基本限制

Method: 引入信息论框架，定义表示率、表示容量（扰动设置）、表示率失真（压缩输出），推导可达容量、可达表示率及其逆定理

Result: 推导了输入源熵决定的输入-输出信息表示可靠性极限，建立了表示容量和表示率失真的理论界限

Conclusion: 提出了统一的信息论框架分析最后一层嵌入表示，为回归任务中的表示学习提供了理论基础和性能界限

Abstract: An information-theoretic framework is introduced to analyze last-layer embedding, focusing on learned representations for regression tasks. We define representation-rate and derive limits on the reliability with which input-output information can be represented as is inherently determined by the input-source entropy. We further define representation capacity in a perturbed setting, and representation rate-distortion for a compressed output. We derive the achievable capacity, the achievable representation-rate, and their converse. Finally, we combine the results in a unified setting.

</details>


### [41] [Polar Orbit Decoding: Universal Parallel Soft Decoding via Automorphism Orbits](https://arxiv.org/abs/2601.11373)
*Pin-Jing Li,Yu-Chih Huang*

Main category: cs.IT

TL;DR: 提出Polar Orbit Decoding (POD)框架，通过并行解码二进制线性分组码的自同构轨道，在保持ML性能的同时显著降低解码延迟。


<details>
  <summary>Details</summary>
Motivation: 现有二进制线性分组码(BLBCs)没有单一码族能同时优化所有性能指标，导致标准中广泛使用多码架构，需要多个解码器，硬件复杂度高。虽然已有基于极化变换的通用解码框架，但其并行化尚未讨论。

Method: 提出Polar Orbit Decoding (POD)框架：1) 利用BLBCs的自同构生成置换轨道，在极化变换后产生具有相同动态冻结约束的多样化解码轨迹；2) 并行解码自同构轨道，实现延迟-性能权衡；3) 使用Schreier-Sims算法以基和强生成集(BSGS)形式表示自同构群，实现多项式时间的离线系统计算。

Result: 在扩展BCH码和扩展Golay码上的仿真结果表明，POD能够达到最大似然(ML)性能，同时相比传统的连续取消列表解码显著降低解码延迟。

Conclusion: POD为二进制线性分组码提供了一个通用的并行解码框架，通过利用自同构轨道实现高效的延迟-性能权衡，无需冻结集重新适配或额外的穷举置换搜索，具有实际应用价值。

Abstract: Binary linear block codes (BLBCs) form the foundation of modern communication systems, yet no single code family simultaneously optimizes all performance aspects. This leads to the widely used multi-code architecture in the standard, significantly increasing the hardware complexity since multiple decoders are required in each piece of equipment. A universal decoding framework based on polar transformations has recently been proposed to unify BLBC decoding under polar-style decoders, but its parallelization has not yet been discussed. In this work, we propose Polar Orbit Decoding (POD), a universal parallel decoding framework for BLBCs. We identify that the automorphisms of BLBCs generate an orbit of permutations that induce diverse decoding trajectories with identical dynamic-frozen constraints after the polar transformations. By decoding over this automorphism orbit in parallel, POD achieves substantial latency-performance tradeoffs without requiring frozen-set readaptation or extra exhaustive permutation searches. Moreover, to enable efficient orbit traversal in the implementation, we represent the automorphism group in a base and strong generating set (BSGS) form using Schreier-Sims algorithms, making offline systematic computation accessible in polynomial time. Simulation results on extended BCH and extended Golay codes demonstrate that POD can achieve maximum-likelihood performance while significantly reducing the decoding latency compared to conventional successive cancellation list decoding.

</details>


### [42] [Efficient Channel Autoencoders for Wideband Communications leveraging Walsh-Hadamard interleaving](https://arxiv.org/abs/2601.11407)
*Cel Thys,Rodney Martinez Alonso,Sofie Pollin*

Main category: cs.IT

TL;DR: 该论文研究了如何利用Walsh-Hadamard交织转换器实现能量高效的宽带通信端到端信道自动编码器。WH交织通过模拟WH变换实现高采样率模数转换并降低功耗，E2E训练的神经编码调制可透明适应WH收发器硬件。在短块长度下，WH-AE在0.14dB内接近5G Polar码性能，同时系统功耗相当或更低，相比最佳神经基线平均能效提高29%。


<details>
  <summary>Details</summary>
Motivation: 研究动机是实现能量高效的宽带通信系统。传统高采样率模数转换器功耗大，而Walsh-Hadamard交织转换器能降低模拟转换功耗。需要探索如何将端到端学习的神经编码调制与这种硬件高效结合，在系统层面平衡计算复杂度、信噪比和模拟功耗。

Method: 采用Walsh-Hadamard交织转换器实现高采样率低功耗模数转换。训练端到端WH域自动编码器，使其透明适应WH收发器硬件而无需算法重新设计。在短块长度下训练WH-AE，并与标准神经基线和传统基线（包括5G Polar码）进行基准测试。量化系统级能量权衡：基带计算、信道信噪比和模拟转换器功耗。

Result: WH-AE系统能在0.14dB内接近传统Polar码的信噪比性能，同时消耗相当或更低的系统功率。相比最佳神经基线，WH-AE平均能效（bit/J）提高29%。WH域学习通过显式平衡计算复杂度、信噪比和模拟功耗，为能量高效、高吞吐宽带通信提供了可行路径。

Conclusion: Walsh-Hadamard域学习是实现能量高效、高吞吐宽带通信的可行途径。端到端训练的神经编码调制能透明适应WH收发器硬件，在系统层面优化能量效率。该方法在保持接近传统编码性能的同时，显著提高能效，为下一代通信系统设计提供了新思路。

Abstract: This paper investigates how end-to-end (E2E) channel autoencoders (AEs) can achieve energy-efficient wideband communications by leveraging Walsh-Hadamard (WH) interleaved converters. WH interleaving enables high sampling rate analog-digital conversion with reduced power consumption using an analog WH transformation. We demonstrate that E2E-trained neural coded modulation can transparently adapt to the WH-transceiver hardware without requiring algorithmic redesign. Focusing on the short block length regime, we train WH-domain AEs and benchmark them against standard neural and conventional baselines, including 5G Polar codes. We quantify the system-level energy tradeoffs among baseband compute, channel signal-to-noise ratio (SNR), and analog converter power. Our analysis shows that the proposed WH-AE system can approach conventional Polar code SNR performance within 0.14dB while consuming comparable or lower system power. Compared to the best neural baseline, WH-AE achieves, on average, 29% higher energy efficiency (in bit/J) for the same reliability. These findings establish WH-domain learning as a viable path to energy-efficient, high-throughput wideband communications by explicitly balancing compute complexity, SNR, and analog power consumption.

</details>


### [43] [Coding Schemes for the Noisy Torn Paper Channel](https://arxiv.org/abs/2601.11501)
*Frederik Walter,Maria Abu-Sini,Nils Weinhardt,Antonia Wachter-Zeh*

Main category: cs.IT

TL;DR: 该论文研究DNA存储中的衰减过程，将其建模为概率性噪声撕纸信道，并提出使用静态标记和数据依赖标记的编码方案来应对噪声和序列断裂问题。


<details>
  <summary>Details</summary>
Motivation: 为了使DNA成为适合档案数据存储的介质，必须考虑DNA存储系统中观察到的链衰减过程。DNA存储中的衰减会导致比特替换和序列断裂，需要有效的编码方案来应对这些挑战。

Method: 将DNA衰减过程建模为概率性噪声撕纸信道，该信道首先通过替换概率性地破坏传输序列的比特，然后将序列分解为一组无序的噪声子字符串。设计了两种编码方案：1）在传输序列中嵌入静态标记；2）使用与数据连接的标记（哈希函数形式）。

Result: 模拟显示：静态标记在较高替换概率下表现更好，而数据依赖标记在较低噪声水平下更优。两种方法都实现了超过99%的重建率，且未观察到错误解码，主要受计算资源限制。

Conclusion: 通过嵌入标记的编码方案可以有效应对DNA存储中的噪声撕纸信道问题。静态标记和数据依赖标记在不同噪声环境下各有优势，为DNA档案存储提供了实用的编码解决方案。

Abstract: To make DNA a suitable medium for archival data storage, it is essential to consider the decay process of the strands observed in DNA storage systems. This paper studies the decay process as a probabilistic noisy torn paper channel (TPC), which first corrupts the bits of the transmitted sequence in a probabilistic manner by substitutions, then breaks the sequence into a set of noisy unordered substrings. The present work devises coding schemes for the noisy TPC by embedding markers in the transmitted sequence. We investigate the use of static markers and markers connected to the data in the form of hash functions. These two tools have also been recently exploited to tackle the noiseless TPC. Simulations show that static markers excel at higher substitution probabilities, while data-dependent markers are superior at lower noise levels. Both approaches achieve reconstruction rates exceeding $99\%$ with no false decodings observed, primarily limited by computational resources.

</details>


### [44] [Empirical Coordination over Markov Channel with Independent Source](https://arxiv.org/abs/2601.11520)
*Mengyuan Zhao,Maël Le Treust,Tobias J. Oechtering*

Main category: cs.IT

TL;DR: 研究马尔可夫信道下的联合信源信道编码，通过经验协调框架分析编码方案可诱导的信源和信道符号经验分布，建立了可实现联合分布的单字母内外界


<details>
  <summary>Details</summary>
Motivation: 传统基于块独立的编码方案在处理马尔可夫信道时可能不是最优的，需要直接利用马尔可夫信道结构来改进性能，特别是在严格因果编码器无法访问过去信道状态的情况下

Method: 提出输入驱动马尔可夫典型性新概念，建立其基本性质，利用经验协调框架分析编码方案，考虑严格因果编码器（无法访问过去信道状态）驱动当前马尔可夫状态演化

Result: 建立了可实现联合分布（协调网络中所有符号）的单字母内外界，新方法直接利用马尔可夫信道结构，超越了基于独立性的传统论证

Conclusion: 通过输入驱动马尔可夫典型性新框架，为马尔可夫信道下的联合信源信道编码提供了更优的分析方法，改进了传统基于块独立的编码方案

Abstract: We study joint source-channel coding over Markov channels through the empirical coordination framework. More specifically, we aim at determining the empirical distributions of source and channel symbols that can be induced by a coding scheme. We consider strictly causal encoders that generate channel inputs, without access to the past channel states, henceforth driving the current Markov state evolution. Our main result is the single-letter inner and outer bounds of the set of achievable joint distributions, coordinating all the symbols in the network. To establish the inner bound, we introduce a new notion of typicality, the input-driven Markov typicality, and develop its fundamental properties. Contrary to the classical block-Markov coding schemes that rely on blockwise independence for discrete memoryless channels, our analysis directly exploits the Markov channel structure and improves beyond the independence-based arguments.

</details>
