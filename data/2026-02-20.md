<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 9]
- [cs.AI](#cs.AI) [Total: 67]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [HyRA: A Hybrid Resource Allocation Framework for RAN Slicing](https://arxiv.org/abs/2602.16952)
*Mohammad Zangooei,Bo Sun,Noura Limam,Raouf Boutaba*

Main category: cs.NI

TL;DR: HyRA是一个混合RAN切片资源分配框架，结合了专用分配和共享资源池，在保证性能隔离的同时提高了资源效率，相比纯专用或纯共享方案可节省50-75%频谱资源。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络需要灵活高效的RAN资源管理来满足多样化SLA要求。现有RAN切片框架主要依赖每切片资源预留，虽然保证了性能隔离，但在突发流量下导致资源利用率低下。

Method: 提出HyRA混合资源分配框架，结合专用分配和跨切片共享资源池。通过双层随机优化建模：外层确定专用和共享资源预算，内层采用新颖的注水算法进行每用户调度。使用样本平均近似、KKT条件和Big-M编码将问题转化为可处理的混合整数规划。

Result: 在多样化需求模式、SLA配置和流量突发性下的广泛仿真表明，HyRA相比纯专用和纯共享基线方案可实现50-75%的频谱节省。

Conclusion: HyRA是未来移动网络中资源高效、符合SLA要求的RAN切片的可行方法，在保证性能隔离的同时显著提高了资源利用率。

Abstract: The advent of 5G and the emergence of 6G networks demand unprecedented flexibility and efficiency in Radio Access Network (RAN) resource management to satisfy diverse service-level agreements (SLAs). Existing RAN slicing frameworks predominantly rely on per-slice resource reservation, which ensures performance isolation but leads to inefficient utilization, particularly under bursty traffic. We introduce HyRA, a hybrid resource allocation framework for RAN slicing that combines dedicated per-slice allocations with shared resource pooling across slices. HyRA preserves performance isolation while improving resource efficiency by leveraging multiplexing gains in bursty traffic conditions. We formulate this design as a bi-level stochastic optimization problem, where the outer loop determines the dedicated and shared resource budgets and the inner loop performs per-UE scheduling under a novel water-filling approach. By using the sample-average approximation, the Karush-Kuhn-Tucker (KKT) conditions of the inner loop, and Big-M encoding, we transform the problem into a tractable mixed-integer program that standard optimization solvers can solve. Extensive simulations under diverse demand patterns, SLA configurations, and traffic burstiness show that HyRA achieves up to 50-75% spectrum savings compared to dedicated-only and shared-only baselines. These results highlight HyRA as a viable approach for resource-efficient, SLA-compliant RAN slicing in future mobile networks.

</details>


### [2] [Robust and Extensible Measurement of Broadband Plans with BQT+](https://arxiv.org/abs/2602.16969)
*Laasya Koduru,Sylee Beltiukov,Alexander Nguyen,Eugene Vuong,Jaber Daneshamooz,Tejas Narechania,Elizabeth Belding,Arpit Gupta*

Main category: cs.NI

TL;DR: BQT+是一个宽带计划测量框架，使用声明式状态/动作规范替代传统工作流，通过抽象非确定性有限自动机建模查询意图，支持对64家ISP的纵向监测，用于宽带基础设施投资评估。


<details>
  <summary>Details</summary>
Motivation: 需要独立、街道地址级别的宽带数据来评估宽带基础设施投资（如420亿美元的BEAD项目），现有系统无法满足对频繁接口变化、跨数百家提供商扩展以及非专家用户低技术门槛这三个基本要求。

Method: 提出BQT+框架，用声明式状态/动作规范替代传统工作流，将查询意图建模为交互状态空间（抽象非确定性有限自动机），运行时选择执行路径以适应不同的交互流程和本地化接口变化。

Result: BQT+能够持续监测64家ISP的纵向数据，支持查询超过100家ISP，并应用于两项政策研究：构建BEAD项目预拨付基准线，以及对四个州超过124,000个地址的宽带可负担性进行基准测试。

Conclusion: BQT+框架成功解决了宽带数据收集中的三个关键系统要求，为宽带基础设施投资评估提供了有效的测量工具，支持政策制定和投资效果评估。

Abstract: Independent, street address-level broadband data is essential for evaluating Internet infrastructure investments, such as the $42B Broadband Equity, Access, and Deployment (BEAD) program. Evaluating these investments requires longitudinal visibility into broadband availability, quality, and affordability, including data on pre-disbursement baselines and changes in providers' advertised plans. While such data can be obtained through Internet Service Provider (ISP) web interfaces, these workloads impose three fundamental system requirements: robustness to frequent interface evolution, extensibility across hundreds of providers, and low technical overhead for non-expert users. Existing systems fail to meet these three essential requirements.
  We present BQT+, a broadband plan measurement framework that replaces monolithic workflows with declarative state/action specifications. BQT+ models querying intent as an interaction state space, formalized as an abstract nondeterministic finite automaton (NFA), and selects execution paths at runtime to accommodate alternative interaction flows and localized interface changes. We show that BQT+ sustains longitudinal monitoring of 64 ISPs, supporting querying for over 100 ISPs. We apply it to two policy studies: constructing a BEAD pre-disbursement baseline and benchmarking broadband affordability across over 124,000 addresses in four states.

</details>


### [3] [RIS Control through the Lens of Stochastic Network Calculus: An O-RAN Framework for Delay-Sensitive 6G Applications](https://arxiv.org/abs/2602.17198)
*Oscar Adamuz-Hinojosa,Lanfranco Zanzi,Vincenzo Sciancalepore,Marco Di Renzo,Xavier Costa-Pérez*

Main category: cs.NI

TL;DR: DARIO：面向6G多RIS场景的O-RAN兼容框架，通过动态RIS分配和随机网络演算建模，实现用户延迟减少高达95.7%


<details>
  <summary>Details</summary>
Motivation: 现有RIS控制方案对快速变化的网络条件响应不足，限制了其在超可靠低延迟通信中的应用，特别是在多RIS场景下需要满足用户异构延迟和可靠性需求

Method: 提出DARIO框架，采用随机网络演算模型分析估计延迟边界，将RIS分配问题建模为非线性整数规划，并设计在线启发式算法实现低计算开销的近优解

Result: 通过仿真和真实流量轨迹评估，在高负载或RIS可用性条件下，延迟减少高达95.7%，能有效满足用户延迟和可靠性目标

Conclusion: DARIO框架成功解决了多RIS场景下的上行延迟最小化问题，通过动态RIS分配和随机网络演算建模，为6G网络中的超可靠低延迟通信提供了有效解决方案

Abstract: Reconfigurable Intelligent Surfaces (RIS) enable dynamic electromagnetic control for 6G networks, but existing control schemes lack responsiveness to fast-varying network conditions, limiting their applicability for ultra-reliable low latency communications. This work addresses uplink delay minimization in multi-RIS scenarios with heterogeneous per-user latency and reliability demands. We propose Delay-Aware RIS Orchestrator (DARIO), an O-RAN-compliant framework that dynamically assigns RIS devices to users within short time windows, adapting to traffic fluctuations to meet per-user delay and reliability targets. DARIO relies on a novel Stochastic Network Calculus (SNC) model to analytically estimate the delay bound for each possible user-RIS assignment under specific traffic and service dynamics. These estimations are used by DARIO to formulate a Nonlinear Integer Program (NIP), for which an online heuristic provides near-optimal performance with low computational overhead. Extensive evaluations with simulations and real traffic traces show consistent delay reductions up to 95.7% under high load or RIS availability.

</details>


### [4] [Hierarchical Edge-Cloud Task Offloading in NTN for Remote Healthcare](https://arxiv.org/abs/2602.17209)
*Alejandro Flores,Danial Shafaie,Konstantinos Ntontin,Elli Kartsakli,Symeon Chatzinotas*

Main category: cs.NI

TL;DR: 研究分层非地面网络作为远程医疗设施/IoMT设备的边缘云计算平台，分析HAPS和LEO卫星组成的层次系统中各层级自私优化下的任务卸载策略和带宽分配


<details>
  <summary>Details</summary>
Motivation: 为远程医疗设施和医疗物联网设备提供可靠的计算平台，解决偏远地区医疗设备计算资源不足的问题，通过分层非地面网络架构实现边缘到云的协同计算

Method: 构建HAPS（高空平台站）提供本地MEC服务，LEO卫星作为桥梁连接远程云服务器，各层级自私优化自身效用，考虑本地延迟成本，制定最优每任务成本和带宽分配策略

Result: 提出了分层非地面网络的最优任务卸载策略和带宽分配方案，各层级在自私优化下能达到均衡，满足远程医疗计算需求

Conclusion: 分层非地面网络架构能有效支持远程医疗计算，通过优化任务成本和带宽分配可实现各层级效用最大化，为偏远地区医疗设施提供可靠计算平台

Abstract: In this work, we study a hierarchical non-terrestrial network as an edge-cloud platform for remote computing of tasks generated by remote ad-hoc healthcare facility deployments, or internet of medical things (IoMT) devices. We consider a high altitude platform station (HAPS) to provide local multiaccess edge server (MEC) services to a set of remote ground medical devices, and a low-earth orbit (LEO) satellite, serving as a bridge to a remote cloud computing server through a ground gateway (GW), providing a large amount of computing resources to the HAPS. In this hierarchical system, the HAPS and the cloud server charges the ground users and the HAPS for the use of the spectrum and the computing of their tasks respectively. Each tier seeks to maximize their own utility in a selfish manner. To encourage the prompt computation of the tasks, a local delay cost is assumed. We formulate the optimal per-task cost at each tier that influences the corresponding offloading policies, and find the corresponding optimal bandwidth allocation.

</details>


### [5] [End-to-End Latency Measurement Methodology for Connected and Autonomous Vehicle Teleoperation](https://arxiv.org/abs/2602.17381)
*François Provost,Faisal Hawlader,Mehdi Testouri,Raphaël Frank*

Main category: cs.NI

TL;DR: 本文提出了一种测量远程操作CAV系统端到端延迟的框架，区分了G2G（环境事件到显示）和M2M（控制输入到车辆执行）延迟，发现M2M延迟占总延迟的60%。


<details>
  <summary>Details</summary>
Motivation: 现有延迟评估方法主要关注G2G延迟，但这只是远程驾驶员体验的总延迟的一个组成部分。需要同时测量M2M延迟和E2E延迟，以全面评估CAV远程操作系统的性能。

Method: 使用陀螺仪、光电晶体管和两个GPS同步的Raspberry Pi 5单元构建测量框架。通过低通滤波和阈值检测识别远程操作端和车辆端的转向盘运动，光电晶体管检测摄像头视野内LED激活来生成中断信号。

Result: 在商用4G和5G网络上对远程操作原型车进行初始测量，平均E2E延迟约为500ms（测量精度±4ms），其中M2M延迟贡献了高达60%的总延迟值。

Conclusion: 提出的测量框架能够全面量化CAV远程操作系统的延迟性能，揭示了M2M延迟在总延迟中的显著贡献，为系统优化提供了重要依据。

Abstract: Connected and Autonomous Vehicles (CAVs) continue to evolve rapidly, and system latency remains one of their most critical performance parameters, particularly when vehicles are operated remotely. Existing latency-assessment methodologies focus predominantly on Glass-to-Glass (G2G) latency, defined as the delay between an event occurring in the operational environment, its capture by a camera, and its subsequent display to the remote operator. However, G2G latency accounts for only one component of the total delay experienced by the driver. The complementary component, Motion-to-Motion (M2M) latency, represents the delay between the initiation of a control input by the remote driver and the corresponding physical actuation by the vehicle. Together, M2M and G2G constitute the overall End-to-End (E2E) latency. This paper introduces a measurement framework capable of quantifying M2M, G2G, and E2E latencies using gyroscopes, a phototransistor, and two GPS-synchronized Raspberry Pi 5 units. The system employs low-pass filtering and threshold-based detection to identify steering-wheel motion on both the remote operator and vehicle sides. An interrupt is generated when the phototransistor detects the activation of an LED positioned within the camera's Field Of View (FOV). Initial measurements obtained from our teleoperated prototype vehicle over commercial 4G and 5G networks indicate an average E2E latency of approximately 500 ms (measurement precision +/- 4 ms). The M2M latency contributes up to 60% of this value.

</details>


### [6] [Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks](https://arxiv.org/abs/2602.17394)
*Nuno Saavedra,Pedro Ribeiro,André Coelho,Rui Campos*

Main category: cs.NI

TL;DR: SIREN是一个AI驱动的框架，通过语音识别和语义提取将紧急语音通信转换为结构化信息，实现无人机辅助网络中基于语音的感知能力。


<details>
  <summary>Details</summary>
Motivation: 无人机辅助网络在应急响应中具有重要作用，但语音无线电通信的非结构化特性阻碍了其与自动化网络管理的直接集成。需要一种方法将紧急语音通信转换为机器可读信息。

Method: 提出SIREN框架，集成自动语音识别(ASR)、基于大语言模型(LLM)的语义提取和自然语言处理(NLP)验证，提取响应单位、位置参考、紧急程度和服务质量要求等信息。

Result: 在包含语言变化、说话者数量、背景噪声和消息复杂度的合成紧急场景中评估，结果显示在不同操作条件下具有稳健的转录和可靠的语义提取能力。

Conclusion: SIREN证明了语音驱动的态势感知在无人机辅助网络中的可行性，为应急响应操作中的人机协同决策支持和自适应网络管理提供了实践基础。

Abstract: Unmanned Aerial Vehicle (UAV)-assisted networks are increasingly foreseen as a promising approach for emergency response, providing rapid, flexible, and resilient communications in environments where terrestrial infrastructure is degraded or unavailable. In such scenarios, voice radio communications remain essential for first responders due to their robustness; however, their unstructured nature prevents direct integration with automated UAV-assisted network management. This paper proposes SIREN, an AI-driven framework that enables voice-driven perception for UAV-assisted networks. By integrating Automatic Speech Recognition (ASR) with Large Language Model (LLM)-based semantic extraction and Natural Language Processing (NLP) validation, SIREN converts emergency voice traffic into structured, machine-readable information, including responding units, location references, emergency severity, and Quality-of-Service (QoS) requirements. SIREN is evaluated using synthetic emergency scenarios with controlled variations in language, speaker count, background noise, and message complexity. The results demonstrate robust transcription and reliable semantic extraction across diverse operating conditions, while highlighting speaker diarization and geographic ambiguity as the main limiting factors. These findings establish the feasibility of voice-driven situational awareness for UAV-assisted networks and show a practical foundation for human-in-the-loop decision support and adaptive network management in emergency response operations.

</details>


### [7] [ACOS: Arrays of Cheap Optical Switches](https://arxiv.org/abs/2602.17449)
*Daniel Amir,Ori Cohen,Jakob Krebs,Mark Silberstein*

Main category: cs.NI

TL;DR: ACOS使用低成本低端口数光路开关阵列替代昂贵的高端口数光路开关，通过应用协同设计实现可重构网络架构，在保持大规模LLM训练性能的同时显著降低成本


<details>
  <summary>Details</summary>
Motivation: 机器学习训练对集群网络需求巨大，现有光路开关方案依赖昂贵的高端口数OCS和/或需要与包交换结合，成本高且重构速度慢，限制了可扩展性和性能

Method: 提出ACOS架构，使用低成本低端口数光路开关作为构建模块，支持拓扑选择、工作负载适应和故障恢复等训练集群所需的重构形式，成本随支持的拓扑和适应能力而非端口数扩展

Result: 仿真显示ACOS部署在训练最先进LLM时性能与全配置包交换网络相当，同时使用现有商用OCS实现显著成本节约，未来带宽扩展性更强且成本节约更多

Conclusion: ACOS通过应用协同设计突破当前专用ML网络的可扩展性限制，提供成本效益高的可重构网络解决方案，支持未来机器学习训练集群的发展

Abstract: Machine learning training places immense demands on cluster networks, motivating specialized architectures and co-design with parallelization strategies. Recent designs incorporating optical circuit switches (OCSes) are promising, offering improved cost, power efficiency, and long-term bandwidth scaling than packet switches. However, most existing approaches rely on costly high-radix OCSes and/or combine them with packet switches to achieve competitive performance at scale. Unfortunately, high-radix OCSes are both expensive and slow to reconfigure, limiting both scalability and performance.
  We propose Arrays of Cheap Optical Switches (ACOS), which bring application co-design directly to the structure of the reconfigurable fabric. Using low-radix OCSes as building blocks, ACOS supports the forms of reconfiguration needed in training clusters including topology selection, workload adaptation, and failure resilience. The cost of ACOS scales with supported topologies and adaptations rather than with port count, breaking past the scalability barriers of current specialized ML networks. We show through simulation that ACOS-based deployments match the performance of fully provisioned packet-switched networks when training state-of-the-art LLMs at scale, while delivering significant cost savings using existing off-the-shelf OCSes, with strong bandwidth scaling and higher cost savings in the future.

</details>


### [8] [HAP Networks for the Future: Applications in Sensing, Computing, and Communication](https://arxiv.org/abs/2602.17534)
*Sultan Çoğay,T. Tolga Sari,Muhammad Nadeem Ali,Byung-Seo Kim,Gökhan Seçinti*

Main category: cs.NI

TL;DR: 本文综述了高空平台(HAPs)在非地面网络中的应用，重点分析了先进空中通信、集成感知和空中信息学，评估了数据处理、网络性能、计算存储需求、经济可行性和监管挑战。


<details>
  <summary>Details</summary>
Motivation: 高空平台(HAPs)作为非地面网络的重要进展，在卫星系统和地面网络之间形成关键连接，在下一代通信技术中扮演重要角色。需要全面评估HAP网络应用的现状和发展方向。

Method: 采用文献综述方法，系统性地调查和评估HAP网络应用，重点关注先进空中通信、集成感知和空中信息学三个领域，从多个维度进行分析。

Result: 分析揭示了HAPs在全球通信中不断演变的角色，识别了数据处理、网络性能、计算存储需求、经济可行性和监管挑战等方面的现状和问题。

Conclusion: HAPs在下一代通信网络中具有重要战略地位，需要进一步研究支持其部署，特别是在技术集成、经济可行性和监管框架方面。

Abstract: High Altitude Platforms (HAPs) are a major advancement in non-terrestrial networks, offering broad coverage and unique capabilities. They form a vital link between satellite systems and terrestrial networks and play a key role in next-generation communication technologies. This study reviews HAP network applications, focusing on advanced airborne communications, integrated sensing, and airborne informatics. Our survey assesses the current state of HAP-centric applications by examining data processing, network performance, computational and storage requirements, economic feasibility, and regulatory challenges. The analysis highlights the evolving role of HAPs in global communication and identifies future research directions to support their deployment.

</details>


### [9] [EDRP: Enhanced Dynamic Relay Point Protocol for Data Dissemination in Multi-hop Wireless IoT Networks](https://arxiv.org/abs/2602.17619)
*Jothi Prasanna Shanmuga Sundaram,Magzhan Gabidolla,Luis Fujarte,Shawn Duong,Jianlin Guo,Toshiaki Koike-Akino,Pu,Wang,Kieran Parsons,Philip V. Orlik,Takenori Sumi,Yukimasa Nagai,Miguel A. Carreira-Perpinan,Alberto E. Cerpa*

Main category: cs.NI

TL;DR: EDRP改进DRP协议，通过LQ-CSMA和ML-BSS算法应对实际链路质量波动，提升物联网数据传输效率


<details>
  <summary>Details</summary>
Motivation: 物联网应用从电池供电转向电网供电，需要高效的数据分发协议。现有DRP协议在实际链路质量波动下表现不佳，导致多发送者传输重叠和吞吐量下降

Method: 提出EDRP协议，包含两个新组件：1) LQ-CSMA根据实时链路质量估计动态限制退避延迟范围；2) ML-BSS算法预测未来链路质量并优化无速率编码的块大小选择

Result: 现场评估显示EDRP相比竞争协议平均吞吐量提升39.43%

Conclusion: EDRP通过结合链路质量感知的CSMA和机器学习优化的块大小选择，有效解决了实际链路波动问题，显著提升了物联网数据分发性能

Abstract: Emerging IoT applications are transitioning from battery-powered to grid-powered nodes. DRP, a contention-based data dissemination protocol, was developed for these applications. Traditional contention-based protocols resolve collisions through control packet exchanges, significantly reducing goodput. DRP mitigates this issue by employing a distributed delay timer mechanism that assigns transmission-start delays based on the average link quality between a sender and its children, prioritizing highly connected nodes for early transmission. However, our in-field experiments reveal that DRP is unable to accommodate real-world link quality fluctuations, leading to overlapping transmissions from multiple senders. This overlap triggers CSMA's random back-off delays, ultimately degrading the goodput performance.
  To address these shortcomings, we first conduct a theoretical analysis that characterizes the design requirements induced by real-world link quality fluctuations and DRP's passive acknowledgments. Guided by this analysis, we design EDRP, which integrates two novel components: (i) Link-Quality Aware CSMA (LQ-CSMA) and (ii) a Machine Learning-based Block Size Selection (ML-BSS) algorithm for rateless codes. LQ-CSMA dynamically restricts the back-off delay range based on real-time link quality estimates, ensuring that nodes with stronger connectivity experience shorter delays. ML-BSS algorithm predicts future link quality conditions and optimally adjusts the block size for rateless coding, reducing overhead and enhancing goodput. In-field evaluations of EDRP demonstrate an average goodput improvement of 39.43\% than the competing protocols.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems](https://arxiv.org/abs/2602.16715)
*H. Sinan Bank,Daniel R. Herber*

Main category: cs.AI

TL;DR: 探索LLMs、RAG和GraphRAG在生成设计结构矩阵(DSM)方面的潜力，在电动螺丝刀和CubeSat两个用例上测试性能，评估组件关系识别和组件发现能力。


<details>
  <summary>Details</summary>
Motivation: 研究自动化生成设计结构矩阵(DSM)的可能性，传统DSM创建需要大量人工分析，探索AI方法能否提高效率并保持准确性。

Method: 使用大型语言模型(LLMs)、检索增强生成(RAG)和图增强检索生成(GraphRAG)三种方法，在电动螺丝刀和CubeSat两个已知架构参考的用例上进行测试，评估两种任务：预定义组件关系确定和更复杂的组件识别及其关系建立。

Result: 尽管存在设计和计算挑战，但识别出自动化DSM生成的机会，所有代码已公开用于可重复性和领域专家的进一步反馈。

Conclusion: AI方法在自动化设计结构矩阵生成方面具有潜力，特别是在组件关系识别任务上，为工程设计和架构分析提供了新的自动化工具可能性。

Abstract: We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.

</details>


### [11] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE 本体论为法医牙科年龄评估提供了一个标准化的语义框架，整合了手动和AI辅助的工作流程，旨在提高法医司法决策的透明度、可重复性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前法医牙科年龄评估面临方法异质性、数据表示碎片化以及临床、法医和法律信息系统之间互操作性有限的问题，这些问题阻碍了透明度和可重复性，特别是随着AI方法的日益普及，这些问题更加突出。

Method: 开发了AIdentifyAGE领域特定本体论，该本体论基于上层和已建立的生物医学、牙科和机器学习本体论，与领域专家共同开发，建模了完整的法医法律工作流程，包括司法背景、个体信息、法医检查数据、牙齿发育评估方法、放射影像、统计参考研究和AI估计方法。

Result: AIdentifyAGE本体论提供了一个标准化、语义一致的框架，能够追踪观察、方法、参考数据和报告结果之间的可追溯链接，确保与FAIR原则的互操作性、可扩展性和合规性。

Conclusion: AIdentifyAGE本体论是提高一致性和透明度的基础步骤，为法医法律和司法背景下的本体驱动决策支持系统建立了坚实基础。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [12] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 论文证明上下文性不是量子力学的特有现象，而是经典概率表示中单状态重用的必然结果，揭示了自适应智能中普遍存在的表示约束。


<details>
  <summary>Details</summary>
Motivation: 自适应系统经常在多个上下文中运行，但由于内存、表示或物理资源的限制，它们需要重用固定的内部状态空间。这种单状态重用现象在自然和人工智能中普遍存在，但其基本的表示后果尚未被充分理解。

Method: 将上下文建模为作用于共享内部状态的干预，证明任何再现上下文结果统计的经典模型都必须承担不可约的信息论成本。提供了一个最小构造性示例来明确实现这一成本并澄清其操作意义。

Result: 证明了上下文性不是量子力学的特性，而是经典概率表示中单状态重用的必然结果。上下文依赖性不能仅通过内部状态来中介，必须承担信息论成本。非经典概率框架通过放松单一全局联合概率空间的假设来避免这种阻碍。

Conclusion: 上下文性是自适应智能中普遍存在的表示约束，与物理实现无关。这一发现将上下文性从量子力学的特殊现象提升为智能系统表示能力的普遍限制。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [13] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache：利用可重构缓存和潜在空间推理的高效大规模人类移动模拟框架


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的人类移动模拟方法虽然能产生真实行为，但计算成本过高，限制了可扩展性，需要更高效的解决方案

Method: 设计MobCache框架，包含：1）推理组件将推理步骤编码为潜在空间嵌入，通过潜在空间评估器实现推理步骤的重用和重组；2）解码组件使用轻量级解码器，通过移动规律约束的蒸馏训练，将潜在空间推理链转换为自然语言

Result: 实验表明MobCache在多个维度上显著提高了效率，同时保持了与最先进的基于LLM方法相当的性能

Conclusion: MobCache通过可重构缓存和潜在空间推理技术，实现了高效且保真的大规模人类移动模拟，解决了现有LLM方法计算成本过高的问题

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [14] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 该研究分析了60个大型语言模型基准测试的饱和现象，发现近半数基准已饱和，且饱和率随时间增加。研究考察了14个设计属性对饱和的影响，发现专家策划的基准比众包基准更抗饱和，而隐藏测试数据并无保护作用。


<details>
  <summary>Details</summary>
Motivation: AI基准测试在衡量模型进展和指导部署决策中起核心作用，但许多基准测试很快饱和，无法区分最佳性能模型，降低了长期价值。需要了解哪些设计因素导致饱和，以创建更持久的评估方法。

Method: 从主要模型开发商的技术报告中选取60个LLM基准测试，沿任务设计、数据构建和评估格式三个维度定义14个属性特征。测试五个假设，分析每个属性如何影响饱和率。

Result: 近一半基准测试表现出饱和现象，饱和率随基准测试年龄增加而增加。隐藏测试数据（公开vs私有）没有保护效果，而专家策划的基准比众包基准更能抵抗饱和。

Conclusion: 研究揭示了哪些设计选择能延长基准测试寿命，为创建更持久的评估策略提供了信息。专家策划的基准设计是抵抗饱和的关键因素。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [15] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 简单基准方法在代码演化任务中表现优于复杂方法，揭示了当前代码演化研究中的评估缺陷和实际挑战


<details>
  <summary>Details</summary>
Motivation: 测试代码演化技术中简单基准方法的性能，揭示当前研究中缺乏与简单基准的比较，以及评估方法存在的问题

Method: 在三个领域（数学边界优化、智能体脚手架设计、机器学习竞赛）比较简单基准方法与复杂代码演化技术的性能

Result: 简单基准方法在所有三个领域都匹配或超越了更复杂的方法；发现搜索空间设计和领域知识比演化算法本身更重要；高方差和小数据集导致次优脚手架选择

Conclusion: 代码演化研究需要更严格的评估方法，关注搜索空间设计而非算法复杂性，并提出减少评估随机性的经济可行方法

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [16] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 证明了超立方体边切片问题的新上界：S(n) ≤ ⌈4n/5⌉（n不是5的奇数倍时），或S(n) ≤ 4n/5 + 1（n是5的奇数倍时），改进了1971年的已知上界⌈5n/6⌉。


<details>
  <summary>Details</summary>
Motivation: 研究超立方体边切片问题：需要多少超平面才能与n维超立方体Q_n的所有边相交。这是一个经典的组合几何问题，Paterson在1971年给出了上界⌈5n/6⌉，但40多年来一直未改进。

Method: 通过构造8个超平面切片Q_{10}来证明改进的上界，使用了新工具CPro1：结合推理LLM和自动超参数调优的自动工具，用于发现数学构造的搜索算法。

Result: 证明了S(n) ≤ ⌈4n/5⌉（当n不是5的奇数倍时），S(n) ≤ 4n/5 + 1（当n是5的奇数倍时）。同时获得了k<n个超平面能切片的最大边数的新下界。

Conclusion: 成功改进了超立方体边切片问题的40多年未变的上界，展示了自动数学发现工具CPro1在解决组合几何问题中的有效性。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [17] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent是一个受治理的AI工作流，用于中子散射设施的数据分析，将手动处理时间从435分钟减少到86-94分钟（4.6-5.0倍加速），同时生成经过验证的晶体结构文件。


<details>
  <summary>Details</summary>
Motivation: 大规模科学设施面临分析和报告延迟问题，特别是对于结构复杂的样品，传统的手动处理流程耗时且效率低下，限制了科学产出速度。

Method: 开发了受治理的AI工作流NeuDiff Agent，采用工具使用架构，通过白名单工具限制、关键工作流边界的验证门控以及完整溯源记录，实现从仪器数据到晶体结构的自动化处理。

Result: 在基准测试中，NeuDiff Agent将处理时间从435分钟（手动）减少到86.5-94.4分钟，加速4.6-5.0倍，生成无A或B级警报的验证CIF文件，同时保持完全溯源能力。

Conclusion: NeuDiff Agent为设施晶体学部署智能AI提供了实用途径，在保持可追溯性和出版验证要求的同时，显著提高了分析效率。

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [18] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: 提出Node Learning范式，将智能部署在边缘节点，通过选择性对等交互扩展，实现去中心化学习


<details>
  <summary>Details</summary>
Motivation: 集中式AI在边缘计算中存在成本高、脆弱性等问题，数据传输、延迟、能耗和对大型数据中心的依赖在异构、移动和资源受限环境中扩展性差

Method: 节点从本地数据持续学习，维护自身模型状态，在有益时进行机会性知识交换；学习通过重叠和扩散传播而非全局同步或中心聚合；统一自主与合作行为，适应数据、硬件、目标和连接性的异构性

Result: 建立了Node Learning范式的概念基础，对比了现有去中心化方法，分析了通信、硬件、信任和治理的影响

Conclusion: Node Learning不抛弃现有范式，而是将其置于更广泛的去中心化视角中，为边缘AI提供新的学习范式

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [19] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 提出基于序关系的犹豫模糊集评分统一框架，证明经典序不构成格结构，但对称序满足评分函数规范准则，并引入优势函数用于犹豫模糊元素排序和群体决策。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论基础，需要建立形式化的序导向评分框架，以提供更灵活一致的评分机制。

Method: 1) 建立序导向的统一评分框架；2) 分析犹豫模糊元素上的经典序结构；3) 证明对称序满足评分函数规范准则；4) 引入优势函数类用于排序；5) 提供离散优势函数和相对优势函数两个具体实例。

Result: 1) 经典序不诱导格结构；2) 对称序定义的评分满足强单调性和Gärdenfors条件等规范准则；3) 优势函数可用于构建犹豫模糊集上的模糊偏好关系并支持群体决策。

Conclusion: 序导向的评分框架为犹豫模糊集提供了理论基础，优势函数为包含最小可接受阈值的控制集比较提供了有效工具，支持群体决策应用。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [20] [IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages](https://arxiv.org/abs/2602.16832)
*Priyaranjan Pattnayak,Sanchari Chowdhuri*

Main category: cs.AI

TL;DR: IJR是一个评估LLM在12种印度及南亚语言中对抗性安全性的基准，发现合同约束会提高拒绝率但不能阻止越狱攻击，英语到印度语言的攻击转移性强，罗马化输入会降低安全性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐评估主要集中在英语和合同约束下，忽视了多语言环境中的安全漏洞，特别是对于南亚地区经常进行语码转换和罗马化书写的用户群体。

Method: 创建了IJR基准，涵盖12种印度和南亚语言（21亿使用者），包含45216个提示，分为JSON（合同约束）和Free（自然）两个轨道，使用无裁判评估方法。

Result: 发现三个关键模式：1）合同约束提高拒绝率但不能阻止越狱攻击；2）英语到印度语言的攻击转移性强，格式包装优于指令包装；3）罗马化或混合输入会降低安全性，与罗马化比例和分词相关。

Conclusion: IJR提供了一个可复现的多语言压力测试，揭示了仅关注英语和合同约束的评估所隐藏的风险，特别对南亚用户群体具有重要意义，他们经常进行语码转换和罗马化书写。

Abstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks.
  IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize.

</details>


### [21] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5是一个多平台GUI代理模型，在20+基准测试中达到SOTA，采用混合数据飞轮、统一能力增强和多平台环境RL扩展等创新技术。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够跨多种平台（桌面、移动、浏览器等）执行GUI任务的通用代理模型，实现云边协作和实时交互，解决现有GUI代理在多平台适应性和性能方面的限制。

Method: 1. 混合数据飞轮：结合模拟环境和云沙箱环境构建UI理解和轨迹生成数据管道；2. 统一能力增强：使用统一思维合成管道增强推理能力，重点提升工具调用、内存和多代理适应能力；3. 多平台环境RL扩展：提出MRPO算法解决多平台冲突和长时任务训练效率问题。

Result: 在20+GUI基准测试中达到开源模型SOTA：GUI自动化任务（OSWorld 56.5，AndroidWorld 71.6，WebArena 48.4），接地任务（ScreenSpotPro 80.3），工具调用任务（OSWorld-MCP 47.6，MobileWorld 46.8），内存和知识任务（GUI-Knowledge Bench 75.5）。

Conclusion: GUI-Owl-1.5通过创新的数据收集、能力增强和训练方法，在多平台GUI任务上实现了最先进的性能，为云边协作的实时GUI交互提供了有效的解决方案，并已开源。

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [22] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage是首个让LLM能自动创建具有自生成拓扑和工具集的智能体开发套件，提供结构化内存支持，在多个基准测试中优于现有ADK


<details>
  <summary>Details</summary>
Motivation: 现有智能体开发套件(ADK)要么功能支持不足，要么依赖人工设计拓扑、工具和内存组件，限制了智能体的泛化能力和整体性能

Method: OpenSage让LLM自动创建智能体，支持自生成拓扑和工具集，提供分层图结构内存系统，并针对软件工程任务提供专门工具包

Result: 在三个最先进的基准测试中，使用不同骨干模型的实验显示OpenSage优于现有ADK，消融研究验证了各组件设计的有效性

Conclusion: OpenSage为下一代智能体开发铺平道路，将焦点从以人为中心转向以AI为中心的模式

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [23] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: AgentLAB是首个专门评估LLM智能体对自适应、长视野攻击脆弱性的基准测试，包含5种新型攻击类型、28个真实环境和644个安全测试用例，发现现有智能体对长视野攻击高度脆弱且单轮防御无效。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长视野复杂环境中部署增加，它们面临利用多轮用户-智能体-环境交互的长视野攻击风险，这些攻击在单轮设置中无法实现，需要专门的评估基准来测量智能体对此类风险的脆弱性。

Method: 开发AgentLAB基准测试，支持5种新型攻击类型（意图劫持、工具链、任务注入、目标漂移、内存中毒），涵盖28个真实智能体环境和644个安全测试用例，用于评估代表性LLM智能体对长视野攻击的脆弱性。

Result: 评估发现代表性LLM智能体对长视野攻击仍然高度脆弱，为单轮交互设计的防御措施无法可靠缓解长视野威胁，智能体安全存在显著漏洞。

Conclusion: AgentLAB作为首个专门评估LLM智能体长视野攻击脆弱性的基准测试，为跟踪实际环境中智能体安全进展提供了宝贵工具，揭示了当前智能体安全防御的不足和未来研究方向。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [24] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace是一个评估大语言模型规划、推理和世界知识的基准测试，要求模型通过维基百科超链接从源页面导航到目标页面。前沿模型在简单任务上表现超人类，但在困难任务上成功率大幅下降，揭示了当前推理系统的明显局限性。


<details>
  <summary>Details</summary>
Motivation: 需要评估大语言模型在规划、推理和世界知识方面的能力，特别是在需要前瞻性规划和理解现实世界概念连接的任务中。现有基准可能无法充分测试这些复杂能力，因此需要一个能揭示模型在长程推理和动态规划方面局限性的简单而有效的测试平台。

Method: 创建LLM-Wikirace基准测试，要求模型通过维基百科超链接逐步导航从源页面到目标页面。评估了广泛的开源和闭源模型，包括Gemini-3、GPT-5和Claude Opus 4.5。分析包括不同难度级别的性能比较、世界知识与规划能力的相对重要性，以及轨迹级别的失败模式分析。

Result: 前沿模型在简单任务上表现超人类，但在困难任务上性能急剧下降：表现最好的Gemini-3在困难游戏中仅成功23%。世界知识对成功是必要的，但达到一定阈值后，规划和长程推理能力成为主导因素。轨迹分析显示即使最强模型在失败后也难以重新规划，经常陷入循环而非恢复。

Conclusion: LLM-Wikirace是一个简单的基准测试，揭示了当前推理系统的明显局限性。它表明具有规划能力的大语言模型仍有很大改进空间，特别是在长程推理、动态重新规划和避免循环行为方面。该基准为评估和改进模型的规划能力提供了一个开放的竞技场。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [25] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 微调对齐的视觉语言模型在窄域有害数据集上会导致严重的泛化性错位，多模态评估比文本评估更能揭示错位程度，有害行为存在于低维子空间，现有缓解策略无法完全消除有害行为


<details>
  <summary>Details</summary>
Motivation: 终身多模态智能体需要通过后训练持续适应新任务，但这在获取能力和保持安全对齐之间产生了根本性矛盾。研究旨在探索微调对齐视觉语言模型在窄域有害数据集上是否会导致错位，以及这种错位如何在不同任务和模态间泛化。

Method: 在Gemma3-4B模型上进行实验，使用LoRA进行微调，研究错位程度与LoRA秩的关系。通过多模态和文本评估对比错位程度，分析有害数据比例对错位的影响。使用几何分析方法探索有害行为的低维子空间特性。评估两种缓解策略：良性窄域微调和基于激活的引导。

Result: 微调会导致严重的泛化性错位，多模态评估显示错位程度（70.71±1.22）远高于文本评估（41.19±2.51）。即使只有10%有害数据也会导致显著错位。几何分析显示有害行为存在于低维子空间，前10个主成分就能捕获大部分错位信息。两种缓解策略都能显著减少错位，但无法完全消除有害行为。

Conclusion: 当前的后训练范式可能无法在部署后环境中充分保持对齐，需要开发更鲁棒的持续学习框架。多模态安全评估比文本评估更能揭示对齐退化，有害行为的低维特性为缓解策略提供了新方向。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [26] [DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs](https://arxiv.org/abs/2602.16935)
*Justin Albrethsen,Yash Datta,Kunal Kumar,Sharath Rajasekar*

Main category: cs.AI

TL;DR: DeepContext是一个状态感知的安全监控框架，通过RNN架构追踪对话中的意图演变，显著提升多轮越狱攻击检测效果，在保持低延迟的同时实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全防护大多是状态无关的，将多轮对话视为独立事件处理，导致无法检测跨轮次的渐进式恶意意图积累（如Crescendo和ActorAttack攻击），存在"安全漏洞"。

Method: 提出DeepContext框架，采用RNN架构处理序列化的轮次级嵌入表示，通过隐藏状态在对话中传播，捕捉风险的渐进积累，替代孤立评估模型。

Result: 在T4 GPU上实现亚20ms推理延迟，多轮越狱检测F1分数达到0.84，显著优于云服务商防护和开源模型（Llama-Prompt-Guard-2和Granite-Guardian均为0.67）。

Conclusion: 建模意图的序列演化比部署大规模状态无关模型更有效且计算高效，状态感知监控是解决多轮越狱攻击的关键方向。

Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a "Safety Gap" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.

</details>


### [27] [SourceBench: Can AI Answers Reference Quality Web Sources?](https://arxiv.org/abs/2602.16942)
*Hexi Jin,Stephen Liu,Yuheng Li,Simran Malik,Yiying Zhang*

Main category: cs.AI

TL;DR: SourceBench：首个评估LLM引用网页源质量的基准，涵盖100个真实查询和8个质量指标，揭示AI搜索工具在证据质量方面的关键问题


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注答案正确性，而忽视了引用网页源的质量。随着LLM越来越多地通过引用网页源来回答问题，需要系统评估这些证据的质量

Method: 构建SourceBench基准，包含100个真实世界查询（涵盖信息、事实、论证、社交和购物意图），采用8个质量指标框架（内容质量和页面级信号），并开发了与专家判断匹配的LLM评估器

Result: 评估了8个LLM、Google搜索和3个AI搜索工具的3996个引用源，揭示了四个关键新见解，为GenAI和网络搜索的未来研究提供指导

Conclusion: SourceBench填补了LLM引用源质量评估的空白，为改进AI搜索工具的证据质量提供了重要基准和方向指导

Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.

</details>


### [28] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 研究发现LLM的文本安全性与工具调用安全性存在显著差距，文本拒绝有害请求时工具调用仍可能执行危险操作，需要专门的安全评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前安全评估主要关注文本层面的拒绝行为，但LLM作为代理通过工具调用与外部系统交互时，文本安全是否意味着工具调用安全尚不明确，存在评估盲区。

Method: 提出GAP基准测试框架，在6个监管领域、7种越狱场景、3种系统提示条件下测试6个前沿模型，产生17,420个数据点，量化文本安全与工具调用安全的差异。

Result: 文本安全不保证工具调用安全，所有模型都存在文本拒绝但工具执行危险操作的情况；系统提示对工具调用行为影响显著；运行时治理能减少信息泄露但无法阻止工具调用尝试。

Conclusion: 仅依赖文本安全评估不足以评估代理行为，工具调用安全需要专门的测量和缓解措施，现有对齐方法存在局限性。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [29] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: LLM4Cov：一个离线代理学习框架，用于硬件验证覆盖，通过执行验证数据整理、策略感知数据合成和最差状态优先采样，在4B参数模型上实现69.2%的覆盖率，超越其教师模型5.3%。


<details>
  <summary>Details</summary>
Motivation: 执行感知的LLM代理虽然能从工具反馈中学习，但反馈获取昂贵且缓慢，使得在线强化学习不切实际。硬件验证覆盖尤其面临这一挑战，因为它依赖工业模拟器和不可微分的执行信号。

Method: 提出LLM4Cov离线代理学习框架，将验证建模为确定性评估器指导的无记忆状态转换。采用执行验证数据整理、策略感知代理数据合成和最差状态优先采样，实现执行约束下的可扩展学习。

Result: 紧凑的4B参数模型在代理评估下达到69.2%的覆盖率通过率，超越其教师模型5.3%，并与大一个数量级的模型表现相当。同时创建了基于现有验证套件的现实对齐基准。

Conclusion: LLM4Cov框架通过离线学习方法有效解决了硬件验证中执行反馈昂贵的问题，使小型模型能够达到与大型模型竞争的性能，为执行约束下的代理学习提供了可行方案。

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [30] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: Phantom是一个自动化代理劫持框架，利用结构化模板注入攻击LLM代理的架构机制，通过优化模板导致角色混淆，显著提高攻击成功率和查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义提示操纵的攻击成功率低且对闭源商业模型可迁移性有限，需要针对LLM代理基础架构机制的攻击方法。

Method: 基于结构化模板注入，通过多级模板增强增加结构多样性，训练模板自编码器将离散模板嵌入连续潜在空间，使用贝叶斯优化搜索最优对抗向量并解码为高效攻击模板。

Result: 在Qwen、GPT和Gemini上的实验显示，该框架在攻击成功率和查询效率上显著优于现有基线，并在真实商业产品中发现70多个已确认漏洞。

Conclusion: 结构化模板劫持具有实际严重性，为保护下一代代理系统提供了实证基础，揭示了LLM代理架构层面的安全风险。

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [31] [HQFS: Hybrid Quantum Classical Financial Security with VQC Forecasting, QUBO Annealing, and Audit-Ready Post-Quantum Signing](https://arxiv.org/abs/2602.16976)
*Srikumar Nayak*

Main category: cs.AI

TL;DR: HQFS是一个结合量子计算和经典计算的混合金融风险系统，将预测、离散风险优化和审计追踪集成到单一流程中，显著提升了预测精度、投资表现和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统金融风险系统采用预测模型+优化器的两阶段流程，在实际应用中面临诸多问题：市场变化时决策不稳定、添加离散约束（如批量大小、上限）时优化困难、资产规模增大时计算缓慢，以及监管要求下需要清晰的审计追踪。这些限制促使开发一个集成预测、优化和审计功能的统一解决方案。

Method: HQFS采用三阶段混合流程：1）使用变分量子电路（VQC）和小型经典头部学习下一期收益和波动率代理；2）将风险收益目标和约束转化为QUBO问题，优先使用量子退火求解，同时保留经典QUBO求解器作为备选；3）使用后量子签名对每次再平衡输出进行签名，确保分配结果可验证且不依赖运行时环境的信任。

Result: 在数据集研究中，HQFS相比调优的经典基线：收益预测误差降低7.8%，波动率预测误差降低6.1%；决策层方面，样本外夏普比率提升9.4%，最大回撤降低11.7%；QUBO求解阶段在相同约束下平均求解时间减少28%，同时生成完全可追踪的签名分配记录。

Conclusion: HQFS成功地将量子计算优势与经典计算可靠性相结合，为金融风险系统提供了一个实用、高效且可审计的解决方案，在预测精度、投资表现和计算效率方面均显著优于传统方法，同时满足监管对审计追踪的要求。

Abstract: Here's the corrected paragraph with all punctuation and formatting issues fixed:
  Financial risk systems usually follow a two-step routine: a model predicts return or risk, and then an optimizer makes a decision such as a portfolio rebalance. In practice, this split can break under real constraints. The prediction model may look good, but the final decision can be unstable when the market shifts, when discrete constraints are added (lot sizes, caps), or when the optimization becomes slow for larger asset sets. Also, regulated settings need a clear audit trail that links each decision to the exact model state and inputs. We present HQFS, a practical hybrid pipeline that connects forecasting, discrete risk optimization, and auditability in one flow. First, HQFS learns next-step return and a volatility proxy using a variational quantum circuit (VQC) with a small classical head. Second, HQFS converts the risk-return objective and constraints into a QUBO and solves it with quantum annealing when available, while keeping a compatible classical QUBO solver as a fallback for deployment. Third, HQFS signs each rebalance output using a post-quantum signature so the allocation can be verified later without trusting the runtime environment. On our market dataset study, HQFS reduces return prediction error by 7.8% and volatility prediction error by 6.1% versus a tuned classical baseline. For the decision layer, HQFS improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. The QUBO solve stage also cuts average solve time by 28% compared to a mixed-integer baseline under the same constraints, while producing fully traceable, signed allocation records.

</details>


### [32] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 黑盒安全评估假设测试分布能可靠预测部署性能，但本文证明对于依赖未观测内部变量的模型，黑盒评估存在根本性限制，无法可靠估计部署风险。


<details>
  <summary>Details</summary>
Motivation: 挑战黑盒安全评估的基本假设：模型在测试分布上的行为能否可靠预测部署性能。针对那些输出依赖于未观测内部变量的模型（这些变量在评估中罕见但在部署中普遍），揭示黑盒评估的局限性。

Method: 采用理论分析方法：1) 被动评估：使用Le Cam方法证明极小极大下界；2) 自适应评估：使用基于哈希的触发构造和Yao极小极大原理；3) 计算分离：基于陷门单向函数假设；4) 白盒探测：提供样本复杂度分析和偏差校正方法。

Result: 证明黑盒评估存在根本性限制：被动评估误差≥0.208δL；自适应评估误差≥δL/16；计算上部署环境可激活不安全行为而多项式时间评估者无法区分；白盒探测需要O(1/(γ²ε_R²))样本。

Conclusion: 黑盒测试在统计上可能无法确定安全性，为最坏情况安全保证需要额外保障措施：架构约束、训练时保证、可解释性和部署监控。本文量化了黑盒评估的局限性并提供了何时需要额外保障的明确标准。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [33] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe是一个用于股票推荐的对话式纵向基准测试，它超越了单纯的行为模仿，通过多视角参考评估LLMs的决策质量，区分描述性行为和基于投资者风险偏好的规范性效用。


<details>
  <summary>Details</summary>
Motivation: 在金融咨询领域，观察到的用户行为可能在市场波动下存在噪音或短视，与用户的长期目标冲突。传统推荐基准将用户选择作为唯一真实标签，混淆了行为模仿与决策质量。

Method: 构建Conv-FinRe基准：使用真实市场数据和人类决策轨迹，通过入职访谈、逐步市场情境和咨询对话，要求模型在固定投资期限内生成股票排名。提供多视角参考，区分描述性行为和基于投资者风险偏好的规范性效用。

Result: 评估结果显示理性决策质量与行为对齐之间存在持续张力：在基于效用的排名上表现良好的模型往往无法匹配用户选择，而行为对齐的模型可能过度拟合短期噪音。

Conclusion: Conv-FinRe基准能够诊断LLMs是遵循理性分析、模仿用户噪音还是受市场动量驱动，为金融推荐系统提供了超越行为模仿的评估框架，相关数据集和代码已公开。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [34] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: Sonar-TS是一个神经符号框架，通过搜索-验证流程解决时间序列数据库的自然语言查询问题，并提出了首个大规模基准测试NLQTSBench。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法无法处理连续形态意图（如形状或异常），而时间序列模型难以处理超长历史数据，需要新的解决方案来帮助非专业用户从海量时间记录中检索有意义的事件、区间和摘要。

Method: 提出Sonar-TS神经符号框架，采用搜索-验证流程：首先使用特征索引通过SQL查询候选窗口，然后生成Python程序对原始信号进行验证和锁定。

Result: 实验表明Sonar-TS能有效处理传统方法无法解决的复杂时间查询，并揭示了该领域的独特挑战。

Conclusion: 这是首个关于时间序列数据库自然语言查询的系统性研究，提供了一个通用框架和评估标准，为未来研究奠定了基础。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [35] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder是一个两阶段匹配系统，通过Ruzicka相似性指数快速筛选，再用基于反正态分布技能桶的Kantorovich距离计算"制裁分数"来评估公平性。


<details>
  <summary>Details</summary>
Motivation: 现代多人游戏中，为技能水平异质的预组队（lobby）创建公平匹配是一个重要挑战。仅基于平均技能指标的匹配通常导致不平衡的对局，特别是在技能分布广泛或偏斜时。

Method: 两阶段系统：第一阶段使用Ruzicka相似性指数比较lobby的"非异常值"技能范围进行快速初步筛选；第二阶段将玩家排名映射到基于反正态分布生成的技能桶中，然后使用Kantorovich距离在排序后的桶索引上计算"制裁分数"来量化匹配公平性。

Result: 通过分析1.4亿个模拟lobby配对的制裁分数分布，证明了系统的可行性，为公平匹配阈值提供了坚实基础。

Conclusion: Cinder系统能够为异质技能水平的预组队提供快速且公平的匹配，解决了传统基于平均技能指标匹配方法的不平衡问题。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [36] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F是首个端到端、项目规模的数学自动形式化框架，可将教科书级数学内容转换为可编译的Lean代码库，在3周内完成通常需要数月专家工作的形式化任务。


<details>
  <summary>Details</summary>
Motivation: 现有数学自动形式化技术局限于孤立定理和短片段，无法处理教科书和研究论文等大规模项目，需要解决跨文件依赖、导入解析和端到端编译等问题。

Method: 采用两阶段代理框架：1) 语句编译阶段将文档拆分为原子块，通过依赖推断排序，修复声明骨架直到项目可编译；2) 证明修复阶段在固定签名下使用目标导向的局部编辑填补证明空缺，整个过程保持验证器在循环中。

Result: 在约3周内将479页的实分析和凸分析教科书转换为153,853行Lean代码库，在FATE-H基准上达到96%的证明成功率（基线为80%），实现了教科书规模的形式化。

Conclusion: M2F框架证明大规模数学文献的自动形式化是可行的，能够以远超人工效率的速度完成教科书级数学内容的形式化，为数学形式化的实际应用铺平了道路。

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [37] [Sales Research Agent and Sales Research Bench](https://arxiv.org/abs/2602.17017)
*Deepanjan Bhol*

Main category: cs.AI

TL;DR: 微软Dynamics 365 Sales中的销售研究代理，通过连接实时CRM数据、推理复杂模式，提供决策就绪的洞察，并在专门构建的基准测试中显著优于Claude Sonnet 4.5和ChatGPT-5。


<details>
  <summary>Details</summary>
Motivation: 企业需要能够基于实时定制CRM数据回答销售领导问题的AI系统，但现有模型缺乏透明、可重复的质量评估证据。

Method: 开发了销售研究代理，连接实时CRM和相关数据，推理复杂模式，生成文本和图表输出；同时创建了销售研究基准，从8个客户加权维度评估系统质量。

Result: 在2025年10月19日对定制企业模式的200个问题测试中，销售研究代理在100分综合得分上比Claude Sonnet 4.5高出13分，比ChatGPT-5高出24.1分。

Conclusion: 销售研究代理为AI解决方案提供了可重复的质量比较方法，使企业能够基于透明证据选择适合的AI系统。

Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.

</details>


### [38] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: 提出PA-MoE方法解决RL中单一策略网络导致的简单任务偏差问题，通过相位感知的专家混合架构让不同专家专注于不同任务阶段。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法使用单一策略网络导致简单任务占用大部分参数和梯度更新，复杂任务得不到足够容量。传统MoE的token级路由会分散相位一致模式，破坏专家专业化。

Method: 提出相位感知专家混合(PA-MoE)：1) 轻量级相位路由器直接从RL目标学习潜在相位边界，无需预定义相位类别；2) 相位路由器为相同专家分配时间一致的分配，让专家保持相位特定专业知识。

Result: 实验结果证明了PA-MoE的有效性。

Conclusion: PA-MoE通过相位感知路由解决了传统MoE在RL中的局限性，实现了更好的专家专业化，提升了复杂任务解决能力。

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [39] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: ITR通过检索式方法动态构建系统提示和工具集，大幅减少LLM代理的上下文长度和成本，提升工具选择准确性


<details>
  <summary>Details</summary>
Motivation: LLM代理在运行时需要反复加载长系统指令和大量工具，导致成本高、延迟大、代理偏离概率增加和工具选择错误等问题

Method: 提出Instruction-Tool Retrieval (ITR)，一种RAG变体，每步只检索最小的系统提示片段和必要的工具子集，动态构建运行时系统提示并暴露缩小的工具集，带有置信度门控回退机制

Result: 在受控基准测试中，ITR将每步上下文token减少95%，工具路由正确率相对提高32%，端到端成本降低70%，使代理能在上下文限制内运行2-20倍更多循环

Conclusion: ITR特别适用于长时间运行的自主代理，随着代理步数增加，节省效果会叠加，论文提供了方法细节、评估协议、消融研究和实际部署操作指南

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [40] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA是一个多代理计算机使用框架，通过意图对齐的计划记忆来稳定长时程执行，在桌面自动化任务中达到74.83%的成功率和0.91的步骤效率比。


<details>
  <summary>Details</summary>
Motivation: 现有方法（从基于RL的规划器到轨迹检索）在长时程任务中容易偏离用户意图，重复解决常规子问题，导致错误累积和效率低下。需要一种能够稳定执行、减少冗余重新规划的方法。

Method: 提出IntentCUA多代理框架，包含规划器、计划优化器和批评器，通过共享记忆协作。该记忆将原始交互轨迹抽象为多视图意图表示和可重用技能。运行时，意图原型检索子组对齐的技能并注入部分计划中。

Result: 在端到端评估中，IntentCUA实现了74.83%的任务成功率，步骤效率比为0.91，优于基于RL和轨迹中心的基线方法。消融实验显示多视图意图抽象和共享计划记忆共同提高了执行稳定性。

Conclusion: 系统级意图抽象和基于记忆的协调是实现大型动态环境中可靠高效桌面自动化的关键。合作多代理循环在长时程任务中提供了最大的性能提升。

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [41] [RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models](https://arxiv.org/abs/2602.17053)
*Yunseok Han,Yejoon Lee,Jaeyoung Do*

Main category: cs.AI

TL;DR: 该论文提出了评估大型推理模型忠实性的框架RFEval，发现49.7%的输出存在不忠实问题，准确性与忠实性相关性弱，RL训练可能损害推理忠实性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）虽然表现良好，但经常产生听起来合理却无法反映其真实决策过程的推理，这削弱了可靠性和信任。需要建立正式的框架来评估推理的忠实性。

Method: 提出了推理忠实性的形式化框架，定义了两个可测试条件：立场一致性和因果影响。开发了RFEval基准，包含7,186个实例，通过受控的输出级反事实干预来探测忠实性。评估了12个开源LRMs。

Result: 发现49.7%的输出存在不忠实问题，主要源于立场不一致。失败集中在数学和代码等脆弱、收敛的领域。后训练机制比规模更相关：RL风格目标在监督微调基础上可能降低推理忠实性。准确性与忠实性相关性弱且统计不显著。

Conclusion: 建立了审计LRM可靠性的严谨方法，表明可信AI不仅需要优化正确结果，还需要优化推理过程的结构完整性。准确性与忠实性分离，不能作为忠实性的可靠代理。

Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$

</details>


### [42] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: 提出S2Q方法，通过多个子价值函数保留替代高价值动作，使用Softmax行为策略促进持续探索，在MARL基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于价值分解的多智能体强化学习方法依赖单一最优动作，当价值函数在训练中变化时难以适应，容易收敛到次优策略。

Method: 提出Successive Sub-value Q-learning (S2Q)，学习多个子价值函数以保留替代高价值动作，结合Softmax行为策略促进持续探索，使总Q值能快速适应变化的最优解。

Result: 在挑战性MARL基准测试中，S2Q一致优于多种MARL算法，展示了更好的适应性和整体性能。

Conclusion: S2Q通过多个子价值函数和Softmax策略解决了MARL中的适应性问题，提高了算法性能和探索能力。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [43] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: PBS是一种新的训练优化技术，通过动态优先处理高损失样本来加速语言模型收敛，使用轻量级线性预测器从静态标记特征估计样本难度。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习方法需要预定义的难度指标，而硬样本挖掘方法需要昂贵的逐样本损失跟踪。PBS旨在通过更高效的方式识别困难样本，加速模型收敛。

Method: PBS使用在线训练的轻量级线性预测器，仅基于四个简单的标记级特征（标记频率、序列长度、词汇多样性、稀有标记比例）来估计样本难度，动态构建优先处理高损失样本的批次。

Result: 在130M参数transformer上的实验表明，PBS实现了6-13%的收敛加速（通过评估损失衡量），预测器相关性在10,000训练步中从0.14提升到0.44，与真实损失的相关性达到0.44。

Conclusion: 标记频率统计编码了关于样本难度的有意义信息，使得能够以可忽略的计算开销实现有效的课程学习，PBS为语言模型训练提供了一种高效的数据调度方法。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [44] [How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses](https://arxiv.org/abs/2602.17084)
*Kan Watanabe,Rikuto Tsuchida,Takahiro Monno,Bin Huang,Kazuma Yamasaki,Youmei Fan,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.AI

TL;DR: AI编码代理在GitHub上创建的PR描述风格存在差异，这些差异影响人类评审员的参与度、响应时间和合并结果


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，AI编码代理能够自主创建GitHub拉取请求，但不同代理在PR描述特征上的差异以及人类评审员如何响应这些差异尚未得到充分研究

Method: 使用AIDev数据集对五个AI编码代理创建的拉取请求进行实证分析，分析PR描述的结构特征，并考察人类评审员在评审活动、响应时间、情感和合并结果方面的响应

Result: 发现AI编码代理展现出不同的PR描述风格，这些风格与评审员参与度、响应时间和合并结果的差异相关；不同代理在评审员互动指标和合并率方面存在显著差异

Conclusion: PR呈现方式和评审员互动动态在人类-AI协作软件开发中扮演重要角色，AI代理的PR描述风格会影响人类评审员的响应行为

Abstract: The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.

</details>


### [45] [Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence](https://arxiv.org/abs/2602.17096)
*Zhaoyang Li,Xingzhi Jin,Junyu Pan,Qianqian Yang,Zhiguo Shi*

Main category: cs.AI

TL;DR: 该论文探讨了6G无线系统中基于意图的自主智能，利用大语言模型（LLM）构建智能体来感知用户多维意图，并自适应地做出物理层决策，实现可持续优化的6G通信。


<details>
  <summary>Details</summary>
Motivation: 6G系统功能复杂度增加，服务需求多样化，用户需求从单一指标转向多维目标（如时延敏感度、能耗偏好、计算约束等），且这些目标会随时间动态变化。传统基于规则的控制或中心化优化难以应对这种复杂性，需要意图驱动的自主智能来准确理解通信环境和用户意图。

Method: 论文提出基于大语言模型（LLM）构建智能体网络代理，利用LLM强大的上下文理解和跨模态推理能力，整合异构信息并将自然语言意图转化为可执行的控制和配置决策。研究聚焦于意图感知、自主决策和网络执行的闭环流程，探讨了6G物理层的智能体AI实现路径，包括多模态感知、跨层决策和可持续优化等关键技术。

Result: 论文提出了一个名为AgenCom的意图驱动链路决策智能体案例研究，该智能体能够在不同用户偏好和信道条件下自适应地构建通信链路，展示了智能体AI在6G物理层应用中的可行性和优势。

Conclusion: 基于大语言模型的智能体AI为6G无线系统提供了实现意图驱动自主智能的可行路径，能够应对复杂多变的多维用户需求和环境动态，是实现可持续演进6G通信的关键技术方向。

Abstract: As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications.
  Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation for intent-aware network agents. Compared with rule-driven or centrally optimized designs, LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions.
  Focusing on a closed-loop pipeline of intent perception, autonomous decision making, and network execution, this paper investigates agentic AI for the 6G physical layer and its realization pathways. We review representative physical-layer tasks and their limitations in supporting intent awareness and autonomy, identify application scenarios where agentic AI is advantageous, and discuss key challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. Finally, we present a case study of an intent-driven link decision agent, termed AgenCom, which adaptively constructs communication links under diverse user preferences and channel conditions.

</details>


### [46] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 提出一个人类-AI协作框架，通过STRIDE和SR-Delta两个组件，生成可信的基准数据集来评估和协调不同机构的ESG评级方法


<details>
  <summary>Details</summary>
Motivation: 不同ESG评级机构对同一公司的评分差异很大，限制了评级的可比性、可信度和决策相关性，需要一种方法来协调这些评级结果

Method: 提出一个通用的人类-AI协作框架，包含两个部分：1) STRIDE - 提供原则性标准和评分系统，指导使用大语言模型构建公司级基准数据集；2) SR-Delta - 差异分析程序框架，揭示潜在调整的见解

Result: 该框架能够实现可持续性评级方法的可扩展和可比评估，为评估不同评级方法提供系统性工具

Conclusion: 呼吁AI社区采用AI驱动的方法来加强和推进可持续性评级方法，支持并执行紧迫的可持续性议程

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [47] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 提出O-Shap方法，通过满足T-性质的分割策略改进Owen值在图像解释中的应用，提升特征归因的准确性和语义一致性


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值方法假设特征独立，但在视觉任务中像素存在空间和语义依赖关系。虽然Owen值支持分组归因，但其效果严重依赖于特征分组的定义方式，现有分割方法（如轴对齐或SLIC）破坏了关键的一致性性质

Method: 提出新的分割方法，满足T-性质以确保层次结构中的语义对齐，该层次结构支持计算剪枝，同时提高归因准确性和可解释性

Result: 在图像和表格数据集上的实验表明，O-Shap在归因精度、语义一致性和运行效率方面优于基线SHAP变体，特别是在结构重要的情况下表现更佳

Conclusion: 通过满足T-性质的分割策略改进Owen值应用，能够有效解决特征依赖问题，提升可解释人工智能中特征归因的质量和效率

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [48] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG：一个从课程材料自动构建教师对齐知识图谱的框架，用于捕获学习依赖关系


<details>
  <summary>Details</summary>
Motivation: 大规模课程中难以诊断学生知识缺口和提供个性化干预，现有知识图谱方法要么过于表面化，要么忽略了教学材料中的丰富教学信号

Method: 从课程讲座材料（幻灯片、笔记等）中提取重要概念作为节点，推断学习依赖关系作为有向边（如"部分-整体"或"依赖"关系），结合教育材料特有的时间语义信号与大型语言模型的泛化能力

Result: 通过多个真实世界课程材料的实验和人工评估，证明InstructKG能够捕获丰富且与教师意图一致的学习进展关系

Conclusion: InstructKG框架能够自动构建教师对齐的知识图谱，有效捕获课程预期的学习进展，为个性化学习提供支持

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [49] [Epistemology of Generative AI: The Geometry of Knowing](https://arxiv.org/abs/2602.17116)
*Ilya Levin*

Main category: cs.AI

TL;DR: 论文提出高维空间索引认识论，将生成式AI视为在语义空间中导航的第三种知识生产方式，区别于符号推理和统计重组。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的认知机制尚不明确，其语义处理方式与传统计算范式（图灵-香农-冯·诺依曼传统）存在根本断裂。缺乏对这种新范式的哲学理解，就无法在科学、教育和制度中负责任地整合生成式AI。

Method: 基于高维几何的四个结构特性（测度集中、近正交性、指数方向容量、流形正则性），结合皮尔士符号学和帕珀特建构主义，发展出"高维空间索引认识论"，将生成模型重新概念化为学习流形的导航器。

Result: 提出"导航知识"作为第三种知识生产方式，生成式AI通过在语义空间中定位和导航来产生知识，这种知识既非符号推理也非统计重组，而是基于高维几何关系的索引性认知。

Conclusion: 生成式AI代表了一种新的认知范式，需要从高维几何的角度理解其知识生产方式。导航知识为负责任地整合AI提供了哲学基础，标志着从符号处理到几何语义的范式转变。

Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.

</details>


### [50] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 提出一种并行算法，通过约束分割将困难的CircuitSAT实例分解为弱化公式族，参数化设计结合并行硬度估计来寻找高质量分解。


<details>
  <summary>Details</summary>
Motivation: CircuitSAT（电路可满足性问题）在硬件验证、密码分析等领域有重要应用，但困难实例的求解极具挑战性。传统方法难以高效处理大规模复杂电路实例，需要新的并行分解技术来提升求解效率。

Method: 采用并行算法设计，通过专门约束将原始SAT实例分割成弱化公式族。算法参数化设计，允许调整参数来高效识别高质量分解，同时并行计算硬度估计来指导分解过程。

Result: 在具有挑战性的CircuitSAT实例上展示了算法的实际效果，包括布尔电路逻辑等价性验证和密码哈希函数原像攻击的编码实例，证明了方法的有效性。

Conclusion: 提出的并行分解算法为处理困难CircuitSAT实例提供了有效解决方案，通过参数化设计和并行硬度估计实现了高质量分解，在硬件验证和密码分析等实际应用中具有实用价值。

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [51] [Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning](https://arxiv.org/abs/2602.17145)
*Joseph Bingham,Sam Helmich*

Main category: cs.AI

TL;DR: Combine是一个基于准则的剪枝框架，提供快速有效的迭代剪枝方法，建立了准则函数比较标准，并提出新准则函数，在VGG模型上剪枝79%滤波器同时保持或提升精度。


<details>
  <summary>Details</summary>
Motivation: 随着CNN模型变得更准确和强大，其尺寸、执行时间、内存占用和功耗也随之增加。现有剪枝解决方案各自有不同的指标和方法，缺乏统一实现，难以实施和比较。

Method: 提出Combine框架，这是一个基于准则的剪枝解决方案，支持迭代剪枝，建立了比较准则函数的标准语言，并提出了几种新颖的准则函数。

Result: 在VGG启发模型上，剪枝高达79%的滤波器同时保持或提高准确率，将网络所需计算量减少高达68%，展示了不同准则函数对不同模型的不同效果。

Conclusion: Combine是一个快速有效的剪枝框架，为准则函数比较提供了标准语言，提出的新准则函数在模型剪枝中表现出色，能显著减少计算需求同时保持模型性能。

Abstract: As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions and the framework on VGG inspired models, pruning up to 79\% of filters while retaining or improving accuracy, and reducing the computations needed by the network by up to 68\%.

</details>


### [52] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA是一个新的基因组基础模型预训练框架，结合了联合嵌入预测架构（JEPA）与传统生成目标，通过潜在空间预测来捕获更广泛的生物功能上下文，而不仅仅是局部序列模式。


<details>
  <summary>Details</summary>
Motivation: 现有的基因组基础模型主要依赖掩码语言建模（MLM）或下一标记预测（NTP），这些方法虽然擅长捕获局部基因组语法和精细的基序模式，但往往无法捕获更广泛的功能上下文，导致表征缺乏全局生物学视角。

Method: JEPA-DNA框架整合了联合嵌入预测架构（JEPA）与传统生成目标，引入潜在基础化机制，将标记级恢复与潜在空间中的预测目标相结合，通过监督CLS标记来预测被掩码基因组片段的高级功能嵌入，而不是仅仅关注单个核苷酸。

Result: 在多样化的基因组基准测试中，JEPA-DNA在监督和零样本任务上始终优于仅生成式基线模型，提供了更稳健和生物学基础的表征。

Conclusion: JEPA-DNA通过提供更稳健和生物学基础的表征，为理解基因组字母和序列底层功能逻辑的基础模型提供了可扩展的路径。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [53] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo是一个仅2000万参数的轻量级公式识别模型，通过精心设计、知识蒸馏和词汇表/分词器迁移，性能媲美SOTA模型，同时模型大小减少80%/65%，支持实时推理和浏览器部署。


<details>
  <summary>Details</summary>
Motivation: 现有公式识别模型通常参数量大，计算成本高，难以在消费级硬件或浏览器中实时运行。需要开发轻量级但高性能的模型，使公式识别技术更易于部署和应用。

Method: 采用最小化设计，结合注意力机制、知识蒸馏技术，以及词汇表和分词器的迁移策略，构建仅2000万参数的紧凑模型架构。

Result: Texo在性能上可与UniMERNet-T和PPFormulaNet-S等SOTA模型相媲美，同时模型大小分别减少了80%和65%，实现了消费级硬件上的实时推理和浏览器内部署。

Conclusion: Texo证明了通过精心设计和优化，可以在大幅减小模型规模的同时保持高性能，为公式识别技术的实际应用和部署提供了可行的轻量级解决方案。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [54] [Continual learning and refinement of causal models through dynamic predicate invention](https://arxiv.org/abs/2602.17217)
*Enrique Crespo-Fernandez,Oliver Ray,Telmo de Menezes e Silva Filho,Peter Flach*

Main category: cs.AI

TL;DR: 提出一个在线构建符号因果世界模型的框架，通过元解释学习和谓词发明实现样本高效、可扩展的层次化概念学习


<details>
  <summary>Details</summary>
Motivation: 传统世界建模方法存在样本效率低、缺乏透明性和可扩展性差的问题，需要一种能在线学习符号因果模型的方法

Method: 整合连续模型学习和修复到智能体决策循环中，利用元解释学习和谓词发明发现语义上有意义且可重用的抽象概念

Result: 该方法能扩展到具有复杂关系动态的领域，避免组合爆炸问题，样本效率比PPO神经网络基线高出几个数量级

Conclusion: 提出的框架能在线构建符号因果世界模型，实现高效、可扩展且透明的层次化概念学习

Abstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.

</details>


### [55] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 本研究提出一个基于AI代理的协作研究流程（Agentic Workflow），用于人文社科研究，并以台湾Claude.ai使用数据为案例验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI研究主要集中在软件工程和自然科学领域，人文社科领域缺乏系统的方法论探索。本研究旨在填补这一空白，为人文社科研究者提供可复制的AI协作框架。

Method: 采用"方法论实验"定位，设计七阶段模块化工作流程，基于任务模块化、人机分工和可验证性三原则。以台湾Claude.ai使用数据（N=7,729个对话）为实证案例，展示工作流程在二次数据分析中的应用。

Result: 成功验证了AI代理协作工作流程的可行性，识别出三种人机协作操作模式：直接执行、迭代优化和人类主导。揭示了人类在研究问题制定、理论解释、情境化推理和伦理反思方面的不可替代性。

Conclusion: 本研究为人文社科研究者提供了可复制的AI协作框架，强调人类判断在研究中的核心地位。承认了单平台数据、横截面设计和AI可靠性风险等局限性，为未来研究指明方向。

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [56] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: 提出LBM（大型行为模型），通过行为嵌入而非提示工程来预测个体战略决策，利用结构化心理特征档案提升预测准确性


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在预测高风险环境下的人类决策时存在局限，特别是难以生成一致、个体特定的行为，且提示方法易出现身份漂移和无法有效利用详细人格描述

Method: 开发LBM行为基础模型，通过基于综合心理测量工具的结构化高维特征档案进行条件化，从瞬时人格提示转向行为嵌入，在连接稳定倾向、动机状态和情境约束与观察选择的数据集上微调

Result: LBM在保留场景评估中相比未适应的Llama-3.1-8B-Instruct骨干模型改进行为预测，与前沿基线表现相当；提示基线存在复杂度上限，而LBM能持续受益于更密集的特征档案

Conclusion: LBM为高保真行为模拟提供了可扩展方法，在战略预见、谈判分析、认知安全和决策支持等应用领域具有潜力

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [57] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 研究发现LLM内部神经表示中，Bloom认知分类的不同层次（从基础记忆到抽象创造）在线性可分空间中编码，线性分类器准确率达95%，表明模型在前向传播早期就解析了认知难度。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型的黑盒特性，需要超越表面性能指标的新评估框架。本研究旨在探索LLM内部神经表示是否编码了认知复杂性，特别是使用Bloom分类法作为层次化视角。

Method: 通过分析不同LLM的高维激活向量，研究不同认知层次（从基础记忆到抽象创造）是否在模型的残差流中线性可分。使用线性分类器探测这些表示。

Result: 线性分类器在所有Bloom认知层次上平均准确率达到约95%，强烈证明认知层次在模型表示中以线性可访问子空间的形式编码。模型在前向传播早期就解析了认知难度，且表示在不同层中变得越来越可分。

Conclusion: 研究提供了证据表明LLM内部表示中编码了认知复杂性层次，且这些表示在线性可分空间中可访问。这为理解LLM如何处理不同认知难度的任务提供了新视角，并支持使用线性探测作为模型内部表示分析的有效工具。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [58] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 提出Shapley-DCLR指标量化LLM预测中的时间知识泄漏，并开发TimeSPEC方法通过声明验证减少泄漏，提升回溯测试可靠性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM预测未来事件能力需要回溯测试，但LLM可能在训练中编码了截止日期后的知识，导致时间知识泄漏，影响评估有效性。

Method: 1) 提出声明级框架检测时间知识泄漏：将模型推理分解为原子声明，按时间可验证性分类，用Shapley值衡量每个声明对预测的贡献，得到Shapley-DCLR指标；2) 开发TimeSPEC方法：通过声明验证和再生主动过滤时间污染，确保所有支持声明都能追溯到截止日期前的来源。

Result: 在350个实例（美国最高法院案件预测、NBA薪资估计、股票回报排名）上的实验显示，标准提示基线存在显著泄漏。TimeSPEC在保持任务性能的同时降低了Shapley-DCLR，证明显式的声明级验证优于基于提示的时间约束。

Conclusion: 提出的Shapley-DCLR指标能有效量化LLM回溯测试中的时间知识泄漏，TimeSPEC方法通过声明级验证减少泄漏，为可靠的LLM回溯评估提供了可解释的框架。

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [59] [Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web](https://arxiv.org/abs/2602.17245)
*Linxi Jiang,Rui Xi,Zhijie Liu,Shuo Chen,Zhiqiang Lin,Suman Nath*

Main category: cs.AI

TL;DR: 论文提出Web Verbs作为网络动作的语义层，通过类型化、语义化文档的函数统一API和客户端工作流，为AI代理提供稳定、可组合、可验证的网络操作接口。


<details>
  <summary>Details</summary>
Motivation: 当前网络代理大多基于低级操作（点击、按键），这些操作脆弱、低效且难以验证。随着LLM发展，自然语言成为目标导向任务的实用接口，但缺乏网络动作的语义层来统一API和浏览器操作。

Method: 提出Web Verbs概念：网络规模的类型化、语义化文档函数集，通过统一接口暴露网站功能（无论是API实现还是客户端工作流）。这些动词包含前置条件、后置条件、策略标签和日志支持，作为稳定可组合单元供代理发现、选择和组合。

Result: 通过概念验证实现和案例研究表明，相比现有代理，Web Verbs能够实现更简洁、更鲁棒的执行。将数十个步骤减少为几个函数调用，提高可靠性、效率和可验证性。

Conclusion: Web Verbs为代理网络提供了必要的语义层，统一了API和浏览器范式，使LLM能够合成可靠、可审计的工作流。论文还提出了标准化路线图，以实现网络规模的部署和可信度。

Abstract: The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \textbf{reliability} by providing stable interfaces, \textbf{efficiency} by reducing dozens of steps into a few function calls, and \textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.

</details>


### [60] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 本文详细记录了在有限计算资源下（2xA100 GPU）从头训练一个1.36B参数的科学语言模型的完整工程流程，包括数据预处理、训练稳定性分析和基础设施瓶颈研究。


<details>
  <summary>Details</summary>
Motivation: 虽然前沿大语言模型展现出强大的推理和数学能力，但如何从原始科学文献（如arXiv LaTeX源文件）训练领域专用科学语言模型的实践过程仍然缺乏详细文档。本研究旨在为在中等计算预算下构建领域专用模型的研究者提供工程指导。

Method: 开发了一个端到端管道，涵盖元数据过滤、存档验证、LaTeX提取、文本规范化、领域感知分词和密集Transformer训练。在2xA100 GPU的有限计算资源下，通过24次实验运行分析了训练稳定性、扩展行为、数据损失和基础设施瓶颈。

Result: 研究发现预处理决策显著影响可用token数量，分词策略影响符号稳定性，存储和I/O约束可能成为与计算同等重要的限制因素。在数据丰富的环境下（52B预训练token）展示了稳定的训练行为。

Conclusion: 本研究没有提出新的架构，而是提供了一个工程基础的、透明的案例研究，展示了如何从原始科学文献训练小型科学语言模型。这些见解旨在支持在中等计算预算下构建领域专用模型的研究者。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [61] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: MedClarify是一个用于医学诊断的AI代理，通过生成后续问题进行迭代推理，减少诊断不确定性，相比单次LLM基线减少约27个百分点的诊断错误。


<details>
  <summary>Details</summary>
Motivation: 当前医学LLM在诊断任务中存在局限性，无法像临床医生那样通过系统病史采集和迭代提问来推理鉴别诊断。真实临床实践中，正确诊断很少能直接从初始患者表现推断出来，而是需要通过排除紧急情况和考虑多种可能条件来逐步减少不确定性。

Method: MedClarify首先计算类似鉴别诊断的候选诊断列表，然后主动生成旨在减少诊断不确定性的后续问题。通过选择具有最高预期信息增益的问题，实现有针对性的、不确定性感知的推理。

Result: 实验显示当前LLM在医学推理中存在局限性，当患者病例不完整或相关信息缺失时，常常产生多个相似可能性的诊断。MedClarify的信息论推理方法能生成有效的后续提问，相比标准单次LLM基线减少约27个百分点的诊断错误。

Conclusion: MedClarify通过代理式信息寻求为改进医学LLM提供了一条路径，促进了与医学LLM的有效对话，反映了真实世界临床推理的迭代性和不确定性特征。

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [62] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 提出一种无数据的正则化方法，通过曲率矩阵近似解决任务向量组合中的表征漂移问题，无需外部任务数据，在任务加法和否定中达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 任务算术为调整基础模型提供了模块化、可扩展的方式，但组合多个任务向量会导致跨任务干扰，引起表征漂移和性能下降。现有表征漂移正则化方法通常需要外部任务数据，这与模块化和数据可用性约束（如隐私要求）相冲突。

Method: 将表征漂移的正则化框架化为曲率矩阵近似问题，采用Kronecker分解近似曲率技术，获得实用的正则化器。该方法在任务数量上具有恒定复杂度，并提升对任务向量缩放的鲁棒性。

Result: 在任务加法和否定任务中达到最先进的结果，无需保留调优，具有常数复杂度，对任务向量缩放具有鲁棒性。

Conclusion: 提出的无数据正则化方法有效解决了任务算术中的表征漂移问题，无需外部任务数据，在保持模块化和隐私约束的同时实现了高性能。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [63] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 提出一个结合形式化验证与深度学习的图像检索框架，通过图验证和神经代码生成支持开放词汇自然语言查询，提供可验证、可信赖的检索结果。


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入模型的自然语言搜索在处理复杂关系、对象组合、精确约束（如身份、数量、比例）等查询时仍不可靠，需要超越向量表示的模糊性和近似性，提供更透明、可验证的检索过程。

Method: 提出新颖框架，将形式化验证集成到基于深度学习的图像检索中，结合图验证方法和神经代码生成，对用户查询中的每个原子事实进行形式化验证。

Result: 不仅能返回匹配结果，还能识别和标记哪些具体约束被满足、哪些未满足，提供更透明、可问责的检索过程，同时提升最流行的基于嵌入方法的检索效果。

Conclusion: 通过将检索结果建立在形式化推理系统上，超越了向量表示的模糊性，为复杂自然语言查询提供了可信赖、可验证的解决方案，推动了信息检索领域的发展。

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [64] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 提出MCVAE模型，通过模态特定变分编码器、融合瓶颈门控机制和多任务目标，解决NSCLC生存预测中多模态数据严重缺失的问题。


<details>
  <summary>Details</summary>
Motivation: NSCLC生存预测因个体预后特征差异而困难，多模态数据（全切片图像、转录组学、DNA甲基化）可提供互补信息，但临床数据常存在模态缺失问题，现有方法在严重缺失情况下缺乏鲁棒性。

Method: 提出多模态对比变分自编码器（MCVAE）：模态特定变分编码器捕获各数据源不确定性；融合瓶颈带学习门控机制归一化现有模态贡献；多任务目标结合生存损失和重建损失正则化患者表示；跨模态对比损失强制潜在空间对齐；训练时应用随机模态掩码提升对任意缺失模式的鲁棒性。

Result: 在TCGA-LUAD（n=475）和TCGA-LUSC（n=446）数据集上广泛评估，证明该方法在预测疾病特异性生存（DSS）方面有效，且在严重缺失场景下比两种SOTA模型更具鲁棒性。

Conclusion: MCVAE能有效处理多模态数据严重缺失问题，但通过测试所有模态子集发现，多模态整合并非总是对任务有益，为多模态整合提供了新的见解。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [65] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 提出基于隐私设计原则的框架，指导AI应用开发者为儿童设计隐私保护系统，结合GDPR、PIPEDA、COPPA等法规原则，并应用于LLM全生命周期


<details>
  <summary>Details</summary>
Motivation: 儿童使用AI技术日益增多，但面临隐私风险。现有隐私法规要求企业实施保护措施，但在实践中存在挑战，需要系统化框架来指导开发者

Method: 提出基于隐私设计原则的框架，整合GDPR、PIPEDA、COPPA等法规原则，映射到LLM应用的数据收集、模型训练、运营监控和持续验证等阶段，结合UNCRC、AADC等儿童设计指南

Result: 框架提供了操作控制措施和设计指南，通过13岁以下儿童LLM教育导师的案例研究，展示了如何通过技术和组织控制以及适龄设计决策实现隐私保护和法规合规

Conclusion: 通过在整个LLM生命周期中采用数据保护策略和适龄设计决策，可以支持开发既提供隐私保护又符合法律要求的儿童AI应用

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [66] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec是一个高性能推荐系统框架，通过后端无关架构解决了学术界与工业界之间的鸿沟，支持50+算法、40指标和19种策略，可无缝从本地扩展到分布式训练，并集成了能耗追踪功能。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统研究生态系统分裂，研究人员必须在易于内存实验和需要重写代码以适应分布式工业引擎之间做出选择，这阻碍了创新。

Method: 提出WarpRec框架，采用新颖的后端无关架构，包含50+最先进算法、40个评估指标和19种过滤与分割策略，支持从本地执行到分布式训练的无缝过渡，并集成CodeCarbon进行实时能耗追踪。

Result: WarpRec消除了学术界与工业界之间的权衡，展示了可扩展性不必以科学完整性或可持续性为代价，并能够作为下一代可持续、支持智能体的推荐系统的架构基础。

Conclusion: WarpRec不仅弥合了学术界与工业界的差距，还可以作为下一代可持续、支持智能体的推荐系统的架构基础，推动推荐系统从静态排名引擎向生成式AI生态系统中的交互工具演进。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [67] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 提出针对ARM Cortex处理器（M0+, M4, M7）的AI模型优化基准测试框架，通过自动测试平台系统评估能效、精度和资源利用，发现FLOPs与推理时间近线性相关，并利用帕累托分析平衡能耗与精度权衡。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统中AI模型部署面临能效、精度和资源利用的平衡挑战，缺乏针对ARM Cortex处理器的系统化评估框架来指导开发者选择最优处理器与模型组合。

Method: 设计自动化测试平台，系统评估ARM Cortex M0+、M4、M7处理器上的AI模型性能，使用关键性能指标（KPIs）分析，通过帕累托分析平衡能耗与精度权衡，并验证FLOPs与推理时间的相关性。

Result: 发现FLOPs与推理时间存在近线性关系，可作为计算需求估计指标；M7适合短推理周期任务，M4在长推理任务中能效更优，M0+适用于简单任务；通过帕累托分析实现了能耗与精度的最佳平衡。

Conclusion: 为嵌入式AI系统开发者提供了实用的基准测试框架和设计指导，帮助选择适合的处理器与模型组合，在保证性能的同时实现能效优化，推动可持续的嵌入式AI应用发展。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [68] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG框架结合知识图谱和检索增强生成，提升LLM在电信领域的准确性和可靠性，减少幻觉问题


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在电信领域应用面临挑战，包括领域复杂性、标准演进和专用术语，导致幻觉增加和实用性降低

Method: 提出KG-RAG框架，整合知识图谱（提供电信标准和文档的结构化知识表示）和检索增强生成（动态检索相关事实）

Result: 在基准数据集上，KG-RAG平均准确率比RAG提高14.3%，比纯LLM模型提高21.6%，在复杂电信场景中产生准确可靠且可解释的输出

Conclusion: KG-RAG框架通过结合知识图谱和检索增强生成，有效提升LLM在电信领域的性能，减少幻觉，确保符合电信规范

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [69] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 该论文提出了两个新指标（可重用性和可验证性）来评估多智能体IR管道中思维链的质量，发现这些指标与标准准确率不相关，揭示了当前基于准确率的排行榜在评估推理能力方面的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前思维链评估仅关注目标任务准确率，但这一指标无法评估推理过程本身的质量或效用。需要新的评估指标来更全面地衡量思维链的质量。

Method: 采用Thinker-Executor框架将思维链生成与执行解耦。引入两个新指标：可重用性（衡量Executor重用Thinker思维链的容易程度）和可验证性（衡量Executor使用思维链匹配Thinker答案的频率）。评估了4个Thinker模型和10个Executor委员会在5个基准测试上的表现。

Result: 研究发现可重用性和可验证性与标准准确率不相关，暴露了当前基于准确率的排行榜在评估推理能力方面的盲点。令人惊讶的是，专用推理模型生成的思维链并不比通用LLM（如Llama和Gemma）生成的思维链更可重用或可验证。

Conclusion: 需要超越准确率的新评估指标来全面评估思维链的质量。可重用性和可验证性提供了对推理过程本身质量的洞察，揭示了当前评估方法的局限性。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [70] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong是一个开源LLM智能体，通过轨迹分割SFT和渐进式RL训练来解决超长视野任务，在多个基准测试中超越现有模型


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理超长视野任务（如研究论文分析）时能力有限，需要开发专门训练方法来提升模型在复杂、多步骤任务中的表现

Method: 1. 轨迹分割SFT：通过冷启动激活基础模型能力，保留早期上下文，渐进截断后期上下文，保持子轨迹重叠
2. Research-Factory自动管道：收集研究论文并构建评估标准，从Claude 4.5 Sonnet蒸馏数千条长视野轨迹
3. 渐进式RL训练：将训练分为多个阶段，逐步延长超时时间以提升长视野任务解决能力

Result: KLong（106B）在PaperBench上超越Kimi K2 Thinking（1T）11.28%，在SWE-bench Verified和MLE-bench等编码基准测试上也表现出泛化改进

Conclusion: 提出的轨迹分割SFT和渐进式RL训练方法有效提升了LLM在超长视野任务中的能力，KLong在多个基准测试中表现出优越性和泛化性

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [71] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 提出基于常微分方程的激活引导理论框架ODESteer，通过屏障函数设计多步自适应引导，在LLM对齐任务上取得显著改进


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法存在两个关键局限：缺乏统一理论框架指导引导方向设计，以及过度依赖单步引导无法捕捉激活分布的复杂模式

Method: 提出基于常微分方程的理论框架，将传统激活加法解释为ODE的一阶近似，通过控制理论中的屏障函数设计引导方向，引入ODESteer方法进行多步自适应引导

Result: ODESteer在多个LLM对齐基准测试中取得一致改进：TruthfulQA提升5.7%，UltraFeedback提升2.5%，RealToxicityPrompts提升2.4%

Conclusion: 通过ODE统一激活引导的理论基础，提出的ODESteer方法在理论和实证上都取得了进展，为LLM对齐提供了新的原则性视角

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [72] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 提出一种基于联邦学习的混合AI模型，结合SWIN Transformer和CNN，用于X射线图像的COVID-19和肺炎诊断，确保医疗数据安全性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 利用人工智能和联邦学习技术，在医疗数据共享的背景下，建立一个安全、分布式的医疗数据处理系统，提高疾病诊断的准确性和可靠性，同时保护患者隐私。

Method: 采用联邦学习框架，结合最新的CNN模型（DenseNet201、Inception V3、VGG 19）和SWIN Transformer视觉Transformer模型，构建混合AI模型，使用TensorFlow和Keras实现。

Result: 提出的混合联邦学习模型能够提高COVID-19和肺炎的诊断准确性，通过实时持续学习增强疾病严重程度预测能力，同时确保模型安全性和信息真实性。

Conclusion: 联邦学习与混合AI模型的结合为医疗诊断提供了安全、高效、可靠的解决方案，有助于全球共同应对疫情挑战，为医生提供有力的辅助诊断工具。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [73] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 提出AI GameStore平台，通过评估AI在所有人类游戏中的表现来衡量类人通用智能，替代传统狭窄的AI基准测试


<details>
  <summary>Details</summary>
Motivation: 传统AI基准测试评估范围狭窄且容易饱和，需要更全面的方法来评估类人通用智能。人类游戏空间（Multiverse of Human Games）提供了丰富的评估场景

Method: 开发AI GameStore平台，使用LLM和人类参与合成代表性人类游戏，从Apple App Store和Steam等平台自动获取并容器化游戏环境变体

Result: 生成100个游戏，评估7个前沿视觉语言模型。最佳模型在大多数游戏中得分不到人类平均分的10%，特别是在需要世界模型学习、记忆和规划的游戏上表现不佳

Conclusion: AI GameStore是衡量和推动机器实现类人通用智能的实用方法，未来需要进一步扩展和完善该平台

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [74] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT是一个基于分层离散扩散模型的分子图生成框架，通过引入化学先验编码和解耦原子编码，在MOSES数据集上实现了接近完美的化学有效性，超越了现有图扩散方法和1D基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于图扩散的分子生成模型存在化学有效性低、难以满足目标属性要求的问题，相比1D建模方法表现不佳。需要开发能够克服这些性能限制的新方法。

Method: MolHIT基于分层离散扩散模型，将离散扩散推广到编码化学先验的额外类别，并采用解耦原子编码技术，根据原子的化学角色分离原子类型。

Result: 在MOSES数据集上实现了新的最先进性能，首次在图扩散中达到接近完美的化学有效性，在多个指标上超越了强大的1D基线模型。在下游任务如多属性引导生成和骨架扩展中也表现出色。

Conclusion: MolHIT成功克服了现有图扩散分子生成方法的性能限制，通过引入化学先验和解耦原子编码，实现了高化学有效性和优越的性能，为AI驱动的药物发现和材料科学提供了强大的分子生成框架。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [75] [AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing](https://arxiv.org/abs/2602.17607)
*Jianda Du,Youran Sun,Haizhao Yang*

Main category: cs.AI

TL;DR: AutoNumerics是一个多智能体框架，能够从自然语言描述自主设计、实现、调试和验证通用PDE的数值求解器，生成基于经典数值分析的透明求解器。


<details>
  <summary>Details</summary>
Motivation: 传统PDE数值求解器设计需要大量数学专业知识和手动调优，而现有神经网络方法虽然灵活但计算成本高且可解释性有限，需要一种更自动化和可解释的解决方案。

Method: 采用多智能体框架，引入粗到细执行策略和基于残差的自验证机制，从自然语言描述生成基于经典数值分析的透明求解器。

Result: 在24个经典和实际PDE问题上，AutoNumerics相比现有神经和LLM基线达到竞争性或更优的精度，并能根据PDE结构特性正确选择数值方案。

Conclusion: AutoNumerics展示了作为自动化PDE求解可访问范式的可行性，能够生成透明且基于经典数值分析的求解器。

Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.

</details>


### [76] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是CLEF评估实验室，专注于从多语言历史文本中提取人物-地点关系，扩展了先前系列，增加了语义关系提取任务，并引入三重评估框架。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决历史文本中人物与地点关系提取的挑战，这些文本通常噪声大、多语言、跨时期。通过建立关系提取任务，旨在支持数字人文学科中的知识图谱构建、历史传记重建和空间分析等下游应用。

Method: 方法包括：1）定义两种关系类型（at和isAt），需要基于时间和地理线索进行推理；2）构建多语言、跨时期的历史文本数据集；3）设计三重评估框架，同时评估准确性、计算效率和领域泛化能力；4）作为CLEF评估实验室，组织系统竞赛。

Result: HIPE-2026建立了人物-地点关系提取的评估基准，扩展了HIPE系列从命名实体识别到语义关系提取。该实验室提供了多语言历史文本数据集，定义了具体的关系分类任务，并引入了综合评估框架，为历史文本处理系统提供了标准化测试平台。

Conclusion: HIPE-2026成功将关系提取任务引入历史文本处理领域，通过系统化的评估框架促进了该领域的技术发展。该实验室连接了关系提取与大规模历史数据处理，为数字人文学科的应用提供了重要支持，并延续了HIPE系列在历史文本处理方面的研究传统。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [77] [Greedy Multi-Path Block Verification for Faster Decoding in Speculative Sampling](https://arxiv.org/abs/2602.16961)
*Rahul Thomas,Arka Pal*

Main category: cs.IT

TL;DR: 提出贪婪多路径块验证(GBV)方法，通过多候选路径和块验证优化推测解码，相比现有方法提升30%以上块效率和15%以上解码速度。


<details>
  <summary>Details</summary>
Motivation: 标准推测解码使用单路径验证，块验证(BV)虽已改进但仍有限。需要利用多路径信息和更优验证算法进一步提升解码效率。

Method: 1. 构建信息无关线性规划(LP)证明块验证最优性；2. 扩展LP到多路径场景；3. 提出贪婪多路径块验证(GBV)作为高效近似。

Result: GBV相比BV提升30%以上块效率，减少15%以上解码时间；在Llama-3 70B上比SOTA多路径方法提升15%以上端到端解码吞吐量。

Conclusion: 块验证在多路径场景下仍有优化空间，GBV通过贪婪策略有效平衡计算复杂度和性能，显著提升推测解码效率。

Abstract: The goal of $L$-step speculative decoding is to accelerate autoregressive decoding of a target model by using a cheaper draft model to generate a candidate path of $L$ tokens. Based on a verification algorithm involving target and draft model probabilities, a prefix of the candidate sequence is accepted, and an additional correction token is sampled from a residual distribution to ensure that the final output adheres to the target distribution. While standard speculative decoding uses a verification algorithm which is independent at each token on the path, a recent extension called block verification uses a joint condition involving all sampled on-path probabilities. Block verification (BV) was shown to be optimal over all verification algorithms which use only on-path probabilities, improving on standard speculative decoding. In this work, we first show that block verification is optimal even over verification algorithms that use off-path probabilities, by constructing an information-agnostic linear program (LP). Further, we can extend our LP to the setting where the draft model samples multiple candidate paths, and use it to construct a natural class of multi-path block verification generalizations. While computing the optimal algorithm in this class is not tractable, by considering a stricter class of greedy algorithms, we can formulate an efficient method called greedy multi-path block verification (GBV). Empirically, GBV can improve block efficiency by over 30% and reduce decoding walltimes by over 15% relative to BV. On Llama-3 70B, GBV can improve the end-to-end decoding throughput over SOTA multi-path verification methods by more than 15%.

</details>


### [78] [Resource Allocation for STAR-RIS-enhanced Metaverse Systems with Augmented Reality](https://arxiv.org/abs/2602.17123)
*Sun Mao,Lei Liu,Kun Yang,F. Richard Yu,Duist Niyato,Chau Yuen*

Main category: cs.IT

TL;DR: 提出了一种基于STAR-RIS辅助的AR元宇宙资源管理框架，通过联合优化基站计算资源分配、STAR-RIS系数矩阵、AR用户CPU频率和发射功率，最小化服务延迟。


<details>
  <summary>Details</summary>
Motivation: AR元宇宙系统面临网络资源有限和无线传播环境不可预测的瓶颈，需要提高通信效率和服务质量。

Method: 采用STAR-RIS辅助通信，将服务延迟最小化问题转化为可处理形式，通过交替优化方法解耦多维变量，使用惩罚函数法、闭式解、拉格朗日对偶法和凸优化理论分别求解各变量。

Result: 仿真结果表明，所提方法相比多个基准方法实现了显著的延迟降低。

Conclusion: 提出的STAR-RIS辅助AR元宇宙资源管理框架能有效优化系统性能，显著降低服务延迟，为AR元宇宙系统提供高效资源管理方案。

Abstract: Augmented reality (AR)-enabled Metaverse is a promising technique to provide immersive service experience for mobile users. However, the limited network resources and unpredictable wireless propagation environments are key design bottlenecks of AR-enabled Metaverse systems. Therefore, this paper presents a resource management framework for simultaneously transmitting and reflecting RIS (STAR-RIS)-assisted AR-enabled Metaverse, where the STAR-RIS is configured to improve the communication efficiency between AR users and the Metaverse server located at the base station (BS). Moreover, we formulate a service latency minimization problem via jointly optimizing the computation resource allocation of the BS, coefficient matrix of the STAR-RIS, central processing unit (CPU) frequency and transmit power of the AR users. To tackle the non-convex problem, we utilize an approximate method to transform it to a tractable form, and decouple the multi-dimensional variables via the alternating optimization method. Particularly, the optimal coefficient matrix is obtained by a penalty function-based method with proved convergence, the CPU frequencies of AR users are derived as the closed-form solution, and the transmit power of AR users and computation resource allocation of the BS are obtained by the Lagrange duality method and convex optimization theory. Finally, simulation results demonstrates that the proposed method achieves remarkable latency reduction than several benchmark methods.

</details>


### [79] [Isometric Invariant Quantification of Gaussian Divergence over Poincare Disc](https://arxiv.org/abs/2602.17159)
*Levent Ali Mengütürk*

Main category: cs.IT

TL;DR: 论文建立了球面平方Hellinger距离与庞加莱圆盘在一般莫比乌斯群作用下的双曲等距不变量之间的几何对偶关系，并提出了L2嵌入双曲等距不变量作为高斯测度间散度的新度量方法。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机源于发现球面平方Hellinger距离与庞加莱圆盘双曲等距不变量之间的几何联系。这种几何对偶关系启发研究者探索将双曲几何工具应用于信息论中，特别是用于量化高斯测度之间的散度。

Method: 论文首先建立了球面平方Hellinger距离与庞加莱圆盘在一般莫比乌斯群作用下的双曲等距不变量之间的几何对偶关系。基于这一几何连接，作者提出使用L2嵌入的双曲等距不变量作为量化高斯测度间散度的新方法。

Result: 研究结果表明存在一个几何对偶关系，使得球面平方Hellinger距离与庞加莱圆盘的双曲等距不变量相互对应。提出的L2嵌入双曲等距不变量为信息论中高斯测度散度量化提供了新的数学工具。

Conclusion: 论文通过建立球面几何与双曲几何之间的对偶关系，为信息论中的散度度量提供了新的几何视角。提出的L2嵌入双曲等距不变量方法为高斯测度间的散度量化贡献了新的理论工具，拓展了信息几何的研究框架。

Abstract: The paper presents a geometric duality between the spherical squared-Hellinger distance and a hyperbolic isometric invariant of the Poincare disc under the action of the general Mobius group. Motivated by the geometric connection, we propose the usage of the L2-embedded hyperbolic isometric invariant as an alternative way to quantify divergence between Gaussian measures as a contribution to information theory.

</details>


### [80] [Federated Latent Space Alignment for Multi-user Semantic Communications](https://arxiv.org/abs/2602.17271)
*Giuseppe Di Poce,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.IT

TL;DR: 提出一种联邦优化方法，通过共享语义预均衡器和本地均衡器来缓解多智能体AI原生语义通信中的潜在空间错配问题


<details>
  <summary>Details</summary>
Motivation: AI原生设备中不同的潜在表示会导致语义不匹配，阻碍多智能体之间的相互理解和任务执行，需要解决潜在空间错配问题

Method: 在接入点共享语义预均衡器，在用户设备部署本地语义均衡器，采用联邦优化进行去中心化训练，考虑功率和复杂度约束

Result: 数值结果验证了方法在目标导向语义通信中的有效性，揭示了准确性、通信开销、复杂度和语义邻近度之间的关键权衡

Conclusion: 提出的方法能有效缓解潜在空间错配，促进多智能体AI原生语义通信中的相互理解和任务导向通信

Abstract: Semantic communication aims to convey meaning for effective task execution, but differing latent representations in AI-native devices can cause semantic mismatches that hinder mutual understanding. This paper introduces a novel approach to mitigating latent space misalignment in multi-agent AI- native semantic communications. In a downlink scenario, we consider an access point (AP) communicating with multiple users to accomplish a specific AI-driven task. Our method implements a protocol that shares a semantic pre-equalizer at the AP and local semantic equalizers at user devices, fostering mutual understanding and task-oriented communication while considering power and complexity constraints. To achieve this, we employ a federated optimization for the decentralized training of the semantic equalizers at the AP and user sides. Numerical results validate the proposed approach in goal-oriented semantic communication, revealing key trade-offs among accuracy, com- munication overhead, complexity, and the semantic proximity of AI-native communication devices.

</details>
