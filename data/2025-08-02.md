<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [PRIME: Pseudo-Random Integrated Multi-Part Entropy for Adaptive Packet Spraying in AI/ML Data centers](https://arxiv.org/abs/2507.23012)
*Ashkan Sobhani,Sogand Sadrhaghighi,Xingjun Chu*

Main category: cs.NI

TL;DR: PRIME是一种伪随机轮询分组喷洒方法，通过考虑网络拓扑优化负载分布和性能，显著提升AI/ML工作负载的网络性能。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式训练对网络基础设施提出高要求，现有解决方案（如ECMP）在处理低熵、突发性和长寿命流时表现不佳，且现有分组喷洒方案会导致缓冲区膨胀和尾部延迟增加。

Method: PRIME采用伪随机轮询分组喷洒方法，利用拥塞信号动态调整负载，避免网络热点。

Result: 实验表明，PRIME在排列流量和网络退化场景下分别提升15%和27%的性能。

Conclusion: PRIME通过动态负载平衡有效解决了现有分组喷洒方案的性能问题，适用于大规模AI/ML工作负载。

Abstract: Large-scale distributed training in production data centers place significant
demands on network infrastructure. In particular, significant load balancing
challenges arise when processing AI/ML workloads, consisting of low-entropy,
bursty and long-lived flows. Existing solutions designed for Ethernet, such as
Equal-Cost Multi-Path (ECMP) struggle to maintain high network utilization.
While major industry players (e.g., Ultra Ethernet Consortium) and parts of
academia have proposed packet spraying to enhance AI/ML workload performance,
we argue that existing packet spraying solutions lead to buffer inflation over
time, negatively affecting network performance. Specifically, when ACK
coalescing is used, these solutions lead to stale information, degrading
network performance. Additionally, in asymmetric network conditions- such as
mix of ordered an unordered traffic, or link degradation and failures- existing
packet spraying solutions often lead to increased tail latency. In this paper,
we present the design and evaluation of PRIME, a pseudo-randomized round-robin
approach to packet spraying that considers the network topology to optimize
load distribution and performance. PRIME uses congestion as an indicator to
re-balance the load. To this extent, PRIME takes into account various
congestion signals, accounting for congestion severity, and their decay times
to avoid network hotspots. We extensively evaluated PRIME using large-scale
production-level simulator. Our results indicate that, compared to existing
solutions, PRIME leads to up to 15% improvement for permutation traffic and up
to 27% improvement in network degradation scenarios

</details>


### [2] [InterfO-RAN: Real-Time In-band Cellular Uplink Interference Detection with GPU-Accelerated dApps](https://arxiv.org/abs/2507.23177)
*Neagin Neasamoni Santhi,Davide Villa,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: 论文提出InterfO-RAN，一种基于CNN的实时解决方案，用于检测5G网络中上行链路的带内干扰，准确率超过91%，并在真实环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 5G及超密集网络中的上行链路干扰会显著降低信号质量，影响协议操作和网络性能，尤其是在小区边缘和毫米波系统中。

Method: 利用卷积神经网络（CNN）处理gNB物理层的I/Q样本，实时检测干扰，并通过GPU加速实现高效运行。

Result: 在真实环境中测试了超过700万个NR上行链路时隙，干扰检测准确率超过91%，处理时间低于650微秒。

Conclusion: InterfO-RAN是一种高效的实时干扰检测解决方案，适用于密集部署的5G网络，能够显著提升网络性能。

Abstract: Ultra-dense fifth generation (5G) and beyond networks leverage spectrum
sharing and frequency reuse to enhance throughput, but face unpredictable
in-band uplink (UL) interference challenges that significantly degrade Signal
to Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases
(gNBs). This is particularly problematic at cell edges, where overlapping
regions force User Equipments (UEs) to increase transmit power, and in
directional millimeter wave systems, where beamforming sidelobes can create
unexpected interference. The resulting signal degradation disrupts protocol
operations, including scheduling and resource allocation, by distorting quality
indicators like Reference Signal Received Power (RSRP) and Received Signal
Strength Indicator (RSSI), and can compromise critical functions such as
channel state reporting and Hybrid Automatic Repeat Request (HARQ)
acknowledgments. To address this problem, this article introduces InterfO-RAN,
a real-time programmable solution that leverages a Convolutional Neural Network
(CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical
layer, detecting in-band interference with accuracy exceeding 91% in under 650
us. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics
Processing Unit (GPU), coexisting with the 5G NR physical layer processing of
NVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial
Radio Units (RUs) and smartphones, our solution was trained and tested on more
than 7 million NR UL slots collected from real-world environments,
demonstrating robust interference detection capabilities essential for
maintaining network performance in dense deployments.

</details>


### [3] [Optimal Packetization Towards Low Latency in Random Access Networks (extended version)](https://arxiv.org/abs/2507.23286)
*Zihong Li,Anshan Yuan,Xinghua Sun*

Main category: cs.NI

TL;DR: 本文研究了随机接入网络中包化对排队延迟的影响，提出了优化包化策略以减少延迟的方法，并分析了不同网络参数的影响。


<details>
  <summary>Details</summary>
Motivation: 随着低延迟服务的需求增长，研究包化对排队延迟的影响变得重要，尤其是在秒级延迟的精确度量方面。

Method: 建立了包化与平均排队延迟的数学关系，并通过数值方法找到最优包大小，同时分析了网络参数的影响。

Result: 确定了最优平均排队延迟及其对应的包大小，并通过仿真研究了包化对延迟抖动的影响。

Conclusion: 通过包化视角重新评估了无连接和基于连接方案的权衡，并将分析应用于NTN场景中的RA-SDT。

Abstract: As the demand for low-latency services grows, ensuring the delay performance
of random access (RA) networks has become a priority. Existing studies on the
queueing delay performance of the Aloha model universally treat packets as
atomic transmission units, focusing primarily on delay measured in time slots.
However, the impact of packetization on queueing delay has been consistently
overlooked, particularly for the mean queueing delay measured in seconds, which
serves as a more precise and practically relevant performance metric than its
slot-based counterpart. Here, packetization refers to the process of
determining the number of bits assembled into a packet. To optimize queueing
delay from the perspective of packetization, this paper establishes the
mathematical relationship between packetization and mean queueing delay in
seconds for both connection-free and connection-based Aloha schemes, and
explores the optimal packetization strategy to minimize this delay. We identify
the optimal mean queueing delay and its corresponding packet size via numerical
methods, and further analyze the influence of various network parameters. We
further use simulations to investigate the similar impact of packetization on
jitter of queueing delay. We then apply our analysis to re-evaluate the complex
trade-off between the connection-free and connection-based schemes through the
new perspective of packetization. Furthermore, recognizing that an analysis of
the queueing delay performance for RA-SDT in NTN scenarios, especially from a
packetization perspective, also remains an unexplored area, we apply the
analysis to this scenario as a case study.

</details>


### [4] [FAST-LoRa: An Efficient Simulation Framework for Evaluating LoRaWAN Networks and Transmission Parameter Strategies](https://arxiv.org/abs/2507.23342)
*Laura Acosta García,Juan Aznar Poveda,Fabian Margreiter,Antonio-Javier García Sánchez,Joan García Haro,Thomas Fahringer,José Lorente López,José-Víctor Rodríguez*

Main category: cs.NI

TL;DR: FAST-LoRa是一种新型仿真框架，旨在快速高效地评估LoRaWAN网络和传输参数选择，显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有仿真框架虽能准确模拟真实场景，但计算开销大且耗时，需要一种轻量级工具来优化传输参数。

Method: FAST-LoRa通过分析模型和高效矩阵运算简化计算，避免复杂的包级仿真。

Result: 在复杂场景中，FAST-LoRa与现有仿真器相比，准确性相近（PDR的MAE为0.940×10⁻²，EE为0.040 bits/mJ），计算时间减少三个数量级。

Conclusion: FAST-LoRa是一种轻量级且准确的近似工具，适用于稳定流量模式和上行通信场景。

Abstract: The Internet of Things (IoT) has transformed many industries, and LoRaWAN
(Long Range Wide Area Network), built on LoRa (Long Range) technology, has
become a crucial solution for enabling scalable, low-cost, and energy-efficient
communication in wide-area networks. Simulation tools are essential for
optimizing the transmission parameters and, therefore, the energy efficiency
and performance of LoRaWAN networks. While existing simulation frameworks
accurately replicate real-world scenarios by including multiple layers of
communication protocols, they often imply significant computational overhead
and simulation times. To address this issue, this paper introduces FAST-LoRa, a
novel simulation framework designed to enable fast and efficient evaluation of
LoRaWAN networks and selection of transmission parameters. FAST-LoRa
streamlines computation by relying on analytical models without complex
packet-level simulations and implementing gateway reception using efficient
matrix operations. Rather than aiming to replace discrete-event simulators,
FAST-LoRa is intended as a lightweight and accurate approximation tool for
evaluating transmission parameter strategies in scenarios with stable traffic
patterns and uplink-focused communications. In our evaluation, we compare
FAST-LoRa with a well-established simulator using multiple network
configurations with varying numbers of end devices and gateways. The results
show that FAST-LoRa achieves similar accuracy in estimating key network
metrics, even in complex scenarios with interference and multi-gateway
reception, with a Mean Absolute Error (MAE) of 0.940 $\times 10^{-2}$ for the
Packet Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while
significantly reducing computational time by up to three orders of magnitude.

</details>


### [5] [Dual-Mode Wireless Devices for Adaptive Pull and Push-Based Communication](https://arxiv.org/abs/2507.23421)
*Sara Cavallero,Fabio Saggese,Junya Shiraishi,Israel Leyva-Mayorga,Shashi Raj Pandey,Chiara Buratti,Petar Popovski*

Main category: cs.NI

TL;DR: 论文提出了一种双模通信框架，结合查询驱动（拉取）和事件驱动（推送）传输，通过统一的时帧结构实现高效数据传输。


<details>
  <summary>Details</summary>
Motivation: 解决无线设备在不同网络条件下实现及时、上下文感知且高效数据传输的需求。

Method: 采用唤醒无线电机制和定制化的MAC协议，支持不同通信类别的数据流量，并进行系统级分析。

Result: 相比传统方法，能耗降低30%，同时保持对推送和拉取通信的可靠支持。

Conclusion: 提出的双模通信框架在能耗和性能上表现优异，适用于多种网络条件。

Abstract: This paper introduces a dual-mode communication framework for wireless
devices that integrates query-driven (pull) and event-driven (push)
transmissions within a unified time-frame structure. Devices typically respond
to information requests in pull mode, but if an anomaly is detected, they
preempt the regular response to report the critical condition. Additionally,
push-based communication is used to proactively send critical data without
waiting for a request. This adaptive approach ensures timely, context-aware,
and efficient data delivery across different network conditions. To achieve
high energy efficiency, we incorporate a wake-up radio mechanism and we design
a tailored medium access control (MAC) protocol that supports data traffic
belonging to the different communication classes. A comprehensive system-level
analysis is conducted, accounting for the wake-up control operation and
evaluating three key performance metrics: the success probability of anomaly
reports (push traffic), the success probability of query responses (pull
traffic) and the total energy consumption. Numerical results characterize the
system's behavior and highlight the inherent trade-off in success probabilities
between push- and pull-based traffic as a function of allocated communication
resources. Our analysis demonstrates that the proposed approach reduces energy
consumption by up to 30% compared to a traditional approach, while maintaining
reliable support for both communication paradigms.

</details>


### [6] [From Timestamps to Versions: Version AoI in Single- and Multi-Hop Networks](https://arxiv.org/abs/2507.23433)
*Erfan Delfani,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 论文分析了通信网络中数据传播的及时性和信息性，提出了版本信息年龄（VAoI）的静态分布分析，并推导了多种调度策略下的闭式解。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注平均指标值，而忽略了多跳网络中的完整分布，因此需要更全面的分析。

Method: 通过随机静态、均匀和基于阈值的调度策略，分析单跳和多跳网络中VAoI的静态分布，并推导闭式解。

Result: 得出了静态分布和平均VAoI的闭式表达式，并确定了基于阈值调度下的最优阈值和最小VAoI。

Conclusion: 数值验证了分析结果，为高效通信网络设计提供了基于VAoI的实用见解。

Abstract: Timely and informative data dissemination in communication networks is
essential for enhancing system performance and energy efficiency, as it reduces
the transmission of outdated or redundant data. Timeliness metrics, such as Age
of Information (AoI), effectively quantify data freshness; however, these
metrics fail to account for the intrinsic informativeness of the content
itself. To address this limitation, content-based metrics have been proposed
that combine both timeliness and informativeness. Nevertheless, existing
studies have predominantly focused on evaluating average metric values, leaving
the complete distribution-particularly in multi-hop network scenarios-largely
unexplored. In this paper, we provide a comprehensive analysis of the
stationary distribution of the Version Age of Information (VAoI), a
content-based metric, under various scheduling policies, including randomized
stationary, uniform, and threshold-based policies, with transmission
constraints in single-hop and multi-hop networks. We derive closed-form
expressions for the stationary distribution and average VAoI under these
scheduling approaches. Furthermore, for threshold-based scheduling, we
analytically determine the optimal threshold value that minimizes VAoI and
derive the corresponding optimal VAoI in closed form. Numerical evaluations
verify our analytical findings, providing valuable insights into leveraging
VAoI in the design of efficient communication networks.

</details>


### [7] [Networked Physical Computing: A New Paradigm for Effective Task Completion via Hypergraph Aided Trusted Task-Resource Matching](https://arxiv.org/abs/2507.23556)
*Botao Zhu,Xianbin Wang*

Main category: cs.NI

TL;DR: 本文提出了一种基于超图的信任任务-资源匹配框架（TTR-matching），用于解决复杂系统中任务与资源匹配的挑战，通过整合物理属性和信任关系，实现价值驱动的任务完成。


<details>
  <summary>Details</summary>
Motivation: 由于计算资源和任务的物理属性多样化，在复杂系统中实现高效的任务与资源匹配变得更具挑战性。本文旨在通过整合物理属性和任务特定的信任关系，提升任务完成的价值。

Method: 提出了一种超图辅助的TTR-matching框架，包括定义任务特定的信任资源超图和任务超图，并设计超图匹配算法以实现任务与资源的精准匹配。

Result: 实验结果表明，TTR-matching框架在识别任务特定的可信合作者和最大化任务完成价值方面优于对比算法。

Conclusion: 本文提出的TTR-matching框架通过超图建模和匹配算法，有效解决了任务与资源匹配的挑战，提升了任务完成的价值。

Abstract: Due to the diverse physical attributes of computing resources and tasks,
developing effective mechanisms to facilitate task and resource matching in
complex connected systems for value-oriented task completion has become
increasingly challenging. To address the challenge, this paper proposes a
networked physical computing system that integrates the physical attributes of
computing resources and tasks as well as task-specific trust relationships
among devices to enable value-driven task completion. Specifically, we propose
a state-of-the-art hypergraph-aided trusted task-resource matching
(TTR-matching) framework to achieve the envisioned physical computing. First, a
task-specific trusted physical resource hypergraph is defined, which integrates
task-specific trust, the physical attributes of resources, and task types. This
enables accurate modeling of device collaboration dependencies under specific
task types. Next, a task hypergraph is generated to associate the task
initiator with the physical attributes of the corresponding tasks. Based on
these two hypergraphs, a hypergraph matching algorithm is designed to
facilitate task-specific trusted collaborator selection and accurate
task-resource matching for value-maximizing task completion. Extensive
experimental results demonstrate that the proposed TTR-matching framework
outperforms comparison algorithms in identifying task-specific trustworthy
collaborators and maximizing the average value of task completion.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Unifying Post-hoc Explanations of Knowledge Graph Completions](https://arxiv.org/abs/2507.22951)
*Alessandro Lonardi,Samy Badreddine,Tarek R. Besold,Pablo Sanchez Martin*

Main category: cs.AI

TL;DR: 本文提出了一种统一的知识图谱补全（KGC）事后解释性框架，通过多目标优化平衡解释的有效性和简洁性，并改进了评估协议。


<details>
  <summary>Details</summary>
Motivation: 当前KGC的事后解释性缺乏形式化和一致的评估，阻碍了研究的可重复性和跨研究比较。

Method: 提出一个通用框架，通过多目标优化统一现有的事后解释性算法，并改进评估协议，使用Mean Reciprocal Rank和Hits@k等指标。

Result: 框架统一了现有方法，并通过实证支持改进的评估协议，强调解释性对终端用户查询的实用性。

Conclusion: 通过统一方法和改进评估标准，本研究旨在提高KGC解释性研究的可重复性和影响力。

Abstract: Post-hoc explainability for Knowledge Graph Completion (KGC) lacks
formalization and consistent evaluations, hindering reproducibility and
cross-study comparisons. This paper argues for a unified approach to post-hoc
explainability in KGC. First, we propose a general framework to characterize
post-hoc explanations via multi-objective optimization, balancing their
effectiveness and conciseness. This unifies existing post-hoc explainability
algorithms in KGC and the explanations they produce. Next, we suggest and
empirically support improved evaluation protocols using popular metrics like
Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of
interpretability as the ability of explanations to address queries meaningful
to end-users. By unifying methods and refining evaluation standards, this work
aims to make research in KGC explainability more reproducible and impactful.

</details>


### [9] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 本文探讨了数据准备原则（DRAI）在领导级科学数据集中的应用，提出了一个二维准备框架，用于指导AI训练的数据处理。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决科学数据在AI训练中的预处理挑战，特别是在高性能计算环境中。

Method: 方法包括分析四个典型领域的数据工作流，并提出一个结合数据准备级别和处理阶段的两维框架。

Result: 结果是一个成熟度矩阵，用于评估和指导科学数据的AI准备。

Conclusion: 结论是该框架有助于实现跨领域、可扩展和可复现的科学AI支持。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


### [10] [FairReason: Balancing Reasoning and Social Bias in MLLMs](https://arxiv.org/abs/2507.23067)
*Zhenyu Pan,Yutong Zhang,Jianshu Zhang,Haoran Lu,Haozheng Luo,Yuwei Han,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 研究发现，通过强化学习以1:4比例混合去偏见和推理样本，能在减少10%偏见的同时保留88%推理准确性。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型（MLLMs）在推理能力提升与偏见缓解之间的权衡关系。

Method: 比较监督微调（SFT）、知识蒸馏（KD）和基于规则的强化学习（RL）三种偏见缓解策略，并调整样本比例以分析推理与偏见的权衡。

Result: 强化学习在1:4混合比例下，偏见分数降低10%，推理准确性保留88%。

Conclusion: 研究为平衡MLLMs的公平性与能力提供了具体指导。

Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art
results across a wide range of tasks and modalities. To push their reasoning
ability further, recent studies explore advanced prompting schemes and
post-training fine-tuning. Although these techniques improve logical accuracy,
they frequently leave the models' outputs burdened with pronounced social
biases. Clarifying how reasoning gains interact with bias mitigation-and
whether the two objectives inherently trade off-therefore remains an open and
pressing research problem. Our study begins by benchmarking three
bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation
(KD), and rule-based reinforcement learning (RL)-under identical conditions,
establishing their baseline strengths and weaknesses. Building on these
results, we vary the proportion of debias-focused and reasoning-centric samples
within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps
reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement
learning cuts stereotype scores by 10% while retaining 88% of the model's
original reasoning accuracy, offering concrete guidance for balancing fairness
and capability in MLLMs.

</details>


### [11] [Moravec's Paradox: Towards an Auditory Turing Test](https://arxiv.org/abs/2507.23091)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 当前AI系统在人类轻松完成的听觉任务上表现糟糕，失败率超过93%，揭示了其在选择性注意力、噪声鲁棒性和上下文适应方面的不足。


<details>
  <summary>Details</summary>
Motivation: 受Moravec悖论启发，研究旨在量化并分析AI在复杂听觉场景中的失败原因，推动人类水平机器听觉的发展。

Method: 引入包含917项挑战的听觉图灵测试，评估GPT-4和Whisper等先进音频模型在七类任务中的表现。

Result: 最佳模型准确率仅为6.9%，远低于人类的52%，暴露了AI在听觉场景分析中的根本缺陷。

Conclusion: 研究为机器听觉进步提供了诊断框架，强调需整合选择性注意力、物理音频理解和上下文感知的新方法。

Abstract: This research work demonstrates that current AI systems fail catastrophically
on auditory tasks that humans perform effortlessly. Drawing inspiration from
Moravec's paradox (i.e., tasks simple for humans often prove difficult for
machines, and vice versa), we introduce an auditory Turing test comprising 917
challenges across seven categories: overlapping speech, speech in noise,
temporal distortion, spatial audio, coffee-shop noise, phone distortion, and
perceptual illusions. Our evaluation of state-of-the-art audio models including
GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate
exceeding 93%, with even the best-performing model achieving only 6.9% accuracy
on tasks that humans solved at 7.5 times higher success (52%). These results
expose focusing failures in how AI systems process complex auditory scenes,
particularly in selective attention, noise robustness, and contextual
adaptation. Our benchmark not only quantifies the human-machine auditory gap
but also provides insights into why these failures occur, suggesting that
current architectures lack fundamental mechanisms for human-like auditory scene
analysis. The traditional design of audio CAPTCHAs highlights common filters
that humans evolved but machines fail to select in multimodal language models.
This work establishes a diagnostic framework for measuring progress toward
human-level machine listening and highlights the need for novel approaches
integrating selective attention, physics-based audio understanding, and
context-aware perception into multimodal AI systems.

</details>


### [12] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 论文提出并形式化定义了预测中的‘论证一致性’属性，要求预测者的推理与预测一致。通过实验证明，过滤不一致预测能提高人类和LLM预测的准确性，但用户实验显示用户通常不遵循这一属性。


<details>
  <summary>Details</summary>
Motivation: 研究论证一致性在判断性预测中的价值，尤其是在人类和LLM预测中，以提高预测准确性。

Method: 定义论证一致性，并在人类和LLM预测中评估其影响；通过众包用户实验验证用户对一致性的认知。

Result: 过滤不一致预测显著提高了人类和LLM的预测准确性；用户实验显示用户通常不遵循一致性。

Conclusion: 论证一致性对提高预测准确性有实际价值，需在基于论证的判断性预测中引入机制过滤不一致意见。

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


### [13] [Tractable Responsibility Measures for Ontology-Mediated Query Answering](https://arxiv.org/abs/2507.23191)
*Meghyn Bienvenu,Diego Figueira,Pierre Lafourcade*

Main category: cs.AI

TL;DR: 本文研究了基于Shapley值的责任度量（WSMS）在基于本体的查询回答中的计算复杂性，发现其在某些查询类别中具有多项式数据复杂性，但在其他情况下是困难的。


<details>
  <summary>Details</summary>
Motivation: 量化查询答案中事实的贡献是解释性数据库研究的重要问题，而基于本体的查询回答增加了复杂性。

Method: 利用数据库设置的结果，分析WSMS度量的计算复杂性，包括数据复杂性和组合复杂性。

Result: 在可一阶重写的查询类别中，WSMS具有多项式数据复杂性；但在支持合取或可达性查询的本体语言中，计算变得困难。

Conclusion: 研究揭示了WSMS计算的复杂性边界，并识别了在DL-Lite方言中可高效计算的查询类别。

Abstract: Recent work on quantitative approaches to explaining query answers employs
responsibility measures to assign scores to facts in order to quantify their
respective contributions to obtaining a given answer. In this paper, we study
the complexity of computing such responsibility scores in the setting of
ontology-mediated query answering, focusing on a very recently introduced
family of Shapley-value-based responsibility measures defined in terms of
weighted sums of minimal supports (WSMS). By exploiting results from the
database setting, we can show that such measures enjoy polynomial data
complexity for classes of ontology-mediated queries that are
first-order-rewritable, whereas the problem becomes "shP"-hard when the
ontology language can encode reachability queries (via axioms like $\exists R.
A \sqsubseteq A$). To better understand the tractability frontier, we next
explore the combined complexity of WSMS computation. We prove that
intractability applies already to atomic queries if the ontology language
supports conjunction, as well as to unions of `well-behaved' conjunctive
queries, even in the absence of an ontology. By contrast, our study yields
positive results for common DL-Lite dialects: by means of careful analysis, we
identify classes of structurally restricted conjunctive queries (which
intuitively disallow undesirable interactions between query atoms) that admit
tractable WSMS computation.

</details>


### [14] [Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification](https://arxiv.org/abs/2507.23197)
*Yuke Liao,Blaise Genest,Kuldeep Meel,Shaan Aryaman*

Main category: cs.AI

TL;DR: 论文提出了一种基于分治法的混合MILP方法，通过选择关键ReLU变量（使用SAS评分）来减少二进制变量数量，提升验证效率。


<details>
  <summary>Details</summary>
Motivation: 处理复杂实例时，传统方法效率低，需优化ReLU变量的选择以减少计算成本。

Method: 结合分治法，提出SAS评分和全局评分（GS）选择关键ReLU变量，并采用混合MILP方法（先使用α,β-CROWN，再部分MILP）。

Result: SAS评分减少二进制变量数量6倍，保持准确性；混合方法将未决实例减少40%，运行时间合理。

Conclusion: SAS评分和混合MILP方法显著提升验证效率，适用于大规模网络。

Abstract: To handle complex instances, we revisit a divide-and-conquer approach to
break down the complexity: instead of few complex BaB calls, we rely on many
small {\em partial} MILP calls. The crucial step is to select very few but very
important ReLUs to treat using (costly) binary variables. The previous attempts
were suboptimal in that respect. To select these important ReLU variables, we
propose a novel {\em solution-aware} ReLU scoring ({\sf SAS}), as well as adapt
the BaB-SR and BaB-FSB branching functions as {\em global} ReLU scoring ({\sf
GS}) functions. We compare them theoretically as well as experimentally, and
{\sf SAS} is more efficient at selecting a set of variables to open using
binary variables. Compared with previous attempts, SAS reduces the number of
binary variables by around 6 times, while maintaining the same level of
accuracy. Implemented in {\em Hybrid MILP}, calling first $\alpha,\beta$-CROWN
with a short time-out to solve easier instances, and then partial MILP,
produces a very accurate yet efficient verifier, reducing by up to $40\%$ the
number of undecided instances to low levels ($8-15\%$), while keeping a
reasonable runtime ($46s-417s$ on average per instance), even for fairly large
CNNs with 2 million parameters.

</details>


### [15] [How Far Are AI Scientists from Changing the World?](https://arxiv.org/abs/2507.23276)
*Qiujie Xie,Yixuan Weng,Minjun Zhu,Fuchen Shen,Shulin Huang,Zhen Lin,Jiahui Zhou,Zilan Mao,Zijie Yang,Linyi Yang,Jian Wu,Yue Zhang*

Main category: cs.AI

TL;DR: 本文探讨了基于大语言模型（LLM）的AI科学家系统在科学发现中的潜力，分析了当前成就、瓶颈及未来目标。


<details>
  <summary>Details</summary>
Motivation: 研究AI科学家系统如何推动科学发现，评估其距离改变世界和重塑科研范式的距离。

Method: 通过前瞻性综述，全面分析AI科学家系统的现状、关键瓶颈及所需组件。

Result: 指出AI科学家系统已取得进展，但仍需突破瓶颈以实现重大科学发现。

Conclusion: 本文旨在明确当前系统的局限，指出未来目标，为科学AI的发展提供方向。

Abstract: The emergence of large language models (LLMs) is propelling automated
scientific discovery to the next level, with LLM-based Artificial Intelligence
(AI) Scientist systems now taking the lead in scientific research. Several
influential works have already appeared in the field of AI Scientist systems,
with AI-generated research papers having been accepted at the ICLR 2025
workshop, suggesting that a human-level AI Scientist capable of uncovering
phenomena previously unknown to humans, may soon become a reality. In this
survey, we focus on the central question: How far are AI scientists from
changing the world and reshaping the scientific research paradigm? To answer
this question, we provide a prospect-driven review that comprehensively
analyzes the current achievements of AI Scientist systems, identifying key
bottlenecks and the critical components required for the emergence of a
scientific agent capable of producing ground-breaking discoveries that solve
grand challenges. We hope this survey will contribute to a clearer
understanding of limitations of current AI Scientist systems, showing where we
are, what is missing, and what the ultimate goals for scientific AI should be.

</details>


### [16] [AI Must not be Fully Autonomous](https://arxiv.org/abs/2507.23330)
*Tosin Adewumi,Lama Alkhaled,Florent Imbert,Hui Han,Nudrat Habib,Karl Löwenmark*

Main category: cs.AI

TL;DR: 论文主张AI不应完全自主，提出3级自主AI分类，强调人类监督的必要性，并提供理论和证据支持。


<details>
  <summary>Details</summary>
Motivation: 探讨AI自主性的风险，尤其是超级智能（ASI）可能带来的威胁，主张人类监督的重要性。

Method: 通过理论分析、12个论点、6个反驳及其回应，以及15个AI价值错位的案例支持观点。

Result: 提出3级自主AI分类，强调完全自主AI（第3级）的风险，并论证人类监督的必要性。

Conclusion: AI不应完全自主，人类监督是降低风险的关键。

Abstract: Autonomous Artificial Intelligence (AI) has many benefits. It also has many
risks. In this work, we identify the 3 levels of autonomous AI. We are of the
position that AI must not be fully autonomous because of the many risks,
especially as artificial superintelligence (ASI) is speculated to be just
decades away. Fully autonomous AI, which can develop its own objectives, is at
level 3 and without responsible human oversight. However, responsible human
oversight is crucial for mitigating the risks. To ague for our position, we
discuss theories of autonomy, AI and agents. Then, we offer 12 distinct
arguments and 6 counterarguments with rebuttals to the counterarguments. We
also present 15 pieces of recent evidence of AI misaligned values and other
risks in the appendix.

</details>


### [17] [DSBC : Data Science task Benchmarking with Context engineering](https://arxiv.org/abs/2507.23336)
*Ram Mohan Rao Kadiyala,Siddhant Gupta,Jebish Purbey,Giulio Martini,Suman Debnath,Hamza Farooq*

Main category: cs.AI

TL;DR: 论文提出了一种针对数据科学代理的全面基准测试，评估了三种LLM模型在不同方法下的性能，揭示了实际部署中的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管数据科学代理在自动化分析任务中迅速普及，但缺乏系统性评估其效能和局限性的基准。

Method: 通过商业应用观察用户交互，设计了反映真实场景的基准测试，评估了三种LLM模型在三种方法下的表现。

Result: 发现不同模型和方法之间存在明显的性能差异，并探讨了温度参数对结果的影响。

Conclusion: 提出的基准数据集和评估框架为未来研究更强大、高效的数据科学代理奠定了基础。

Abstract: Recent advances in large language models (LLMs) have significantly impacted
data science workflows, giving rise to specialized data science agents designed
to automate analytical tasks. Despite rapid adoption, systematic benchmarks
evaluating the efficacy and limitations of these agents remain scarce. In this
paper, we introduce a comprehensive benchmark specifically crafted to reflect
real-world user interactions with data science agents by observing usage of our
commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,
Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with
context engineering, multi-step with context engineering, and with SmolAgent.
Our benchmark assesses performance across a diverse set of eight data science
task categories, additionally exploring the sensitivity of models to common
prompting issues, such as data leakage and slightly ambiguous instructions. We
further investigate the influence of temperature parameters on overall and
task-specific outcomes for each model and approach. Our findings reveal
distinct performance disparities among the evaluated models and methodologies,
highlighting critical factors that affect practical deployment. The benchmark
dataset and evaluation framework introduced herein aim to provide a foundation
for future research of more robust and effective data science agents.

</details>


### [18] [LLM4Rail: An LLM-Augmented Railway Service Consulting Platform](https://arxiv.org/abs/2507.23377)
*Zhuo Li,Xianghuai Deng,Chiwei Feng,Hanmeng Li,Shenjie Wang,Haichao Zhang,Teng Jia,Conlin Chen,Louis Linchun Wu,Jia Wang*

Main category: cs.AI

TL;DR: LLM4Rail是一个基于大语言模型（LLM）的铁路服务平台，通过QTAO提示框架整合语言推理与任务导向行动，提供个性化铁路服务。


<details>
  <summary>Details</summary>
Motivation: 满足日益增长的个性化铁路服务需求，如票务、餐饮推荐、天气信息等。

Method: 提出QTAO提示框架，结合语言推理与任务导向行动；构建CRFD-25铁路餐饮数据集；开发零样本对话推荐系统。

Result: LLM4Rail能生成精准响应，推荐系统与CRFD-25数据集对齐。

Conclusion: LLM4Rail通过LLM和QTAO框架，有效提升铁路服务的个性化和智能化水平。

Abstract: Large language models (LLMs) have significantly reshaped different walks of
business. To meet the increasing demands for individualized railway service, we
develop LLM4Rail - a novel LLM-augmented railway service consulting platform.
Empowered by LLM, LLM4Rail can provide custom modules for ticketing, railway
food & drink recommendations, weather information, and chitchat. In LLM4Rail,
we propose the iterative "Question-Thought-Action-Observation (QTAO)" prompting
framework. It meticulously integrates verbal reasoning with task-oriented
actions, that is, reasoning to guide action selection, to effectively retrieve
external observations relevant to railway operation and service to generate
accurate responses. To provide personalized onboard dining services, we first
construct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible
takeout dataset tailored for railway services. CRFD-25 covers a wide range of
signature dishes categorized by cities, cuisines, age groups, and spiciness
levels. We further introduce an LLM-based zero-shot conversational recommender
for railway catering. To address the unconstrained nature of open
recommendations, the feature similarity-based post-processing step is
introduced to ensure all the recommended items are aligned with CRFD-25
dataset.

</details>


### [19] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: 本文介绍了一种基于大型语言模型（LLM）的代理，用于与工业级ERP系统交互，通过自然语言查询生成SQL语句。


<details>
  <summary>Details</summary>
Motivation: 解决工业级ERP系统中自然语言查询的复杂性和可靠性问题。

Method: 提出了一种新颖的双代理架构，结合推理和批判阶段以提高查询生成的可靠性。

Result: 代理能够成功将自然语言查询转化为可执行的SQL语句。

Conclusion: 双代理架构显著提升了查询生成的准确性和可靠性。

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [20] [Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440)
*Mingzhe Li,Xin Lu,Yanyan Zhao*

Main category: cs.AI

TL;DR: Self-Foveate是一种创新的LLM驱动方法，通过多级注视方法提升指令合成的多样性和难度。


<details>
  <summary>Details</summary>
Motivation: 传统指令合成方法依赖人工标注或自动化方法，但存在多样性和难度不足的问题。

Method: 提出Micro-Scatter-Macro多级注视方法，引导LLM从无监督文本中挖掘细粒度信息。

Result: 在多个无监督语料库和模型架构上的实验验证了方法的有效性和优越性。

Conclusion: Self-Foveate显著提升了指令合成的多样性和难度，数据与代码已公开。

Abstract: Large language models (LLMs) with instruction following capabilities have
demonstrated impressive problem-solving abilities. While synthesizing
instructional data from unsupervised text has become a common approach for
training such models, conventional methods rely heavily on human effort for
data annotation. Although existing automated synthesis paradigms have
alleviated this constraint, they still exhibit significant limitations in
ensuring adequate diversity and difficulty of synthesized instructions. To
address these challenges, we propose Self-Foveate, an innovative LLM-driven
method for instruction synthesis. This approach introduces a
"Micro-Scatter-Macro" multi-level foveation methodology that effectively guides
the LLM to deeply excavate fine-grained information embedded in unsupervised
text, thereby enhancing both the diversity and difficulty of synthesized
instructions. Comprehensive experiments across multiple unsupervised corpora
and diverse model architectures validate the effectiveness and superiority of
our proposed method. We publicly release our data and codes:
https://github.com/Mubuky/Self-Foveate

</details>


### [21] [Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery](https://arxiv.org/abs/2507.23488)
*Kacper Kadziolka,Saber Salehkaleybar*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型在因果推断中的表现，提出了一种基于Tree-of-Thoughts和Chain-of-Thoughts的模块化方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 因果推断是大型语言模型的核心挑战，现有模型在数据扰动下表现不佳，因此研究先进推理模型在因果发现任务中的潜力。

Method: 使用OpenAI的o系列和DeepSeek-R模型，结合Tree-of-Thoughts和Chain-of-Thoughts方法，设计模块化上下文管道。

Result: 该方法在Corr2Cause基准上实现了三倍于传统基线的性能提升，并通过分析推理链长度和复杂性验证了其有效性。

Conclusion: 先进推理模型结合结构化上下文框架能显著提升因果发现能力，为跨领域应用提供了通用蓝图。

Abstract: Causal inference remains a fundamental challenge for large language models.
Recent advances in internal reasoning with large language models have sparked
interest in whether state-of-the-art reasoning models can robustly perform
causal discovery-a task where conventional models often suffer from severe
overfitting and near-random performance under data perturbations. We study
causal discovery on the Corr2Cause benchmark using the emergent OpenAI's
o-series and DeepSeek-R model families and find that these reasoning-first
architectures achieve significantly greater native gains than prior approaches.
To capitalize on these strengths, we introduce a modular in-context pipeline
inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding
nearly three-fold improvements over conventional baselines. We further probe
the pipeline's impact by analyzing reasoning chain length, complexity, and
conducting qualitative and quantitative comparisons between conventional and
reasoning models. Our findings suggest that while advanced reasoning models
represent a substantial leap forward, carefully structured in-context
frameworks are essential to maximize their capabilities and offer a
generalizable blueprint for causal discovery across diverse domains.

</details>


### [22] [Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification](https://arxiv.org/abs/2507.23497)
*David A Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果关系的图像分类器解释方法，兼具形式化严谨性和黑盒算法适用性，并引入了对比性和置信度感知的完整因果解释。


<details>
  <summary>Details</summary>
Motivation: 现有图像分类器解释方法缺乏形式化严谨性，而逻辑解释虽严谨但假设严格，不适用于图像分类器。

Method: 提出因果解释方法，证明其形式化性质，引入对比性因果解释和完整因果解释（置信度感知）。

Result: 实验表明不同模型在充分性、对比性和完整性上表现不同，算法高效（平均6秒/图像）且完全黑盒。

Conclusion: 因果解释兼具形式化严谨性和实用性，适用于图像分类器，且无需模型内部信息。

Abstract: Existing algorithms for explaining the outputs of image classifiers are based
on a variety of approaches and produce explanations that lack formal rigor. On
the other hand, logic-based explanations are formally and rigorously defined
but their computability relies on strict assumptions about the model that do
not hold on image classifiers.
  In this paper, we show that causal explanations, in addition to being
formally and rigorously defined, enjoy the same formal properties as
logic-based ones, while still lending themselves to black-box algorithms and
being a natural fit for image classifiers. We prove formal properties of causal
explanations and introduce contrastive causal explanations for image
classifiers. Moreover, we augment the definition of explanation with confidence
awareness and introduce complete causal explanations: explanations that are
classified with exactly the same confidence as the original image.
  We implement our definitions, and our experimental results demonstrate that
different models have different patterns of sufficiency, contrastiveness, and
completeness. Our algorithms are efficiently computable, taking on average 6s
per image on a ResNet50 model to compute all types of explanations, and are
totally black-box, needing no knowledge of the model, no access to model
internals, no access to gradient, nor requiring any properties, such as
monotonicity, of the model.

</details>


### [23] [DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer](https://arxiv.org/abs/2507.23554)
*Ruoyu Wang,Junda Wu,Yu Xia,Tong Yu,Ryan A. Rossi,Julian McAuley,Lina Yao*

Main category: cs.AI

TL;DR: 论文提出DICE框架，通过动态选择上下文示例提升大语言模型代理的性能，解决了现有方法依赖启发式或任务特定设计的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文学习的代理在复杂推理和工具使用任务中表现优异，但其性能高度依赖示例选择，缺乏通用且理论支持的选择标准。

Method: 提出DICE框架，通过因果视角分解示例知识为可转移和不可转移部分，并提出逐步选择标准，确保代理性能提升。

Result: 实验证明DICE在多样化领域中的有效性和通用性，显著提升了代理的稳健性和效率。

Conclusion: DICE是一种无需额外训练成本的通用解决方案，强调了基于理论的上下文感知示例选择对代理性能的重要性。

Abstract: Large language model-based agents, empowered by in-context learning (ICL),
have demonstrated strong capabilities in complex reasoning and tool-use tasks.
However, existing works have shown that the effectiveness of ICL is highly
sensitive to the choice of demonstrations, with suboptimal examples often
leading to unstable or degraded performance. While prior work has explored
example selection, including in some agentic or multi-step settings, existing
approaches typically rely on heuristics or task-specific designs and lack a
general, theoretically grounded criterion for what constitutes an effective
demonstration across reasoning steps. Therefore, it is non-trivial to develop a
principled, general-purpose method for selecting demonstrations that
consistently benefit agent performance. In this paper, we address this
challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a
theoretically grounded ICL framework for agentic tasks that selects the most
relevant demonstrations at each step of reasoning. Our approach decomposes
demonstration knowledge into transferable and non-transferable components
through a causal lens, showing how the latter can introduce spurious
dependencies that impair generalization. We further propose a stepwise
selection criterion with a formal guarantee of improved agent performance.
Importantly, DICE is a general, framework-agnostic solution that can be
integrated as a plug-in module into existing agentic frameworks without any
additional training cost. Extensive experiments across diverse domains
demonstrate our method's effectiveness and generality, highlighting the
importance of principled, context-aware demo selection for robust and efficient
LLM agents.

</details>


### [24] [Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI](https://arxiv.org/abs/2507.23565)
*Botao Zhu,Xianbin Wang,Dusit Niyato*

Main category: cs.AI

TL;DR: 提出一种基于语义信任链的自主信任编排方法，利用智能代理和超图优化分布式协作中的信任评估。


<details>
  <summary>Details</summary>
Motivation: 解决分布式协作中因任务复杂性、资源动态性和评估开销导致的信任评估效率低下问题。

Method: 结合智能代理和超图技术，通过历史数据和空闲期评估设备信任，并利用语义超图管理协作关系。

Result: 实验证明该方法实现了资源高效的信任评估。

Conclusion: 提出的方法在资源利用和信任准确性之间取得了平衡，适用于大规模协作系统。

Abstract: In collaborative systems, the effective completion of tasks hinges on
task-specific trust evaluations of potential devices for distributed
collaboration. However, the complexity of tasks, the spatiotemporal dynamism of
distributed device resources, and the inevitable assessment overhead
dramatically increase the complexity and resource consumption of the trust
evaluation process. As a result, ill-timed or overly frequent trust evaluations
can reduce utilization rate of constrained resources, negatively affecting
collaborative task execution. To address this challenge, this paper proposes an
autonomous trust orchestration method based on a new concept of semantic
chain-of-trust. Our technique employs agentic AI and hypergraph to establish
and maintain trust relationships among devices. By leveraging its strengths in
autonomous perception, task decomposition, and semantic reasoning, we propose
agentic AI to perceive device states and autonomously perform trust evaluations
of collaborators based on historical performance data only during device idle
periods, thereby enabling efficient utilization of distributed resources. In
addition, agentic AI performs task-specific trust evaluations on collaborator
resources by analyzing the alignment between resource capabilities and task
requirements. Moreover, by maintaining a trust hypergraph embedded with trust
semantics for each device, agentic AI enables hierarchical management of
collaborators and identifies collaborators requiring trust evaluation based on
trust semantics, thereby achieving a balance between overhead and trust
accuracy. Furthermore, local trust hypergraphs from multiple devices can be
chained together to support multi-hop collaboration, enabling efficient
coordination in large-scale systems. Experimental results demonstrate that the
proposed method achieves resource-efficient trust evaluation.

</details>


### [25] [MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying](https://arxiv.org/abs/2507.23633)
*Qian Zhao,Zhuo Sun,Bin Guo,Zhiwen Yu*

Main category: cs.AI

TL;DR: 论文提出了一种策略引导的代理辅助记忆回忆方法，通过设计5W召回图和分层召回树优化策略选择与响应生成，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统代理辅助记忆回忆方法受限于内存模块大小，无法完整获取记忆，影响回忆效果。受记忆理论启发，作者希望通过有效线索主动激活记忆。

Method: 设计了5W召回图分类记忆查询，定义15种召回策略模式，结合蒙特卡洛树搜索算法优化策略选择和响应生成，并微调开源大语言模型开发MemoCue代理。

Result: 在三个数据集上，MemoCue在回忆启发方面优于基于大语言模型的方法17.74%，人类评估也显示其在记忆回忆应用中的优势。

Conclusion: 策略引导的代理辅助记忆回忆方法通过优化策略选择和响应生成，显著提升了记忆回忆效果，具有实际应用潜力。

Abstract: Agent-assisted memory recall is one critical research problem in the field of
human-computer interaction. In conventional methods, the agent can retrieve
information from its equipped memory module to help the person recall
incomplete or vague memories. The limited size of memory module hinders the
acquisition of complete memories and impacts the memory recall performance in
practice. Memory theories suggest that the person's relevant memory can be
proactively activated through some effective cues. Inspired by this, we propose
a novel strategy-guided agent-assisted memory recall method, allowing the agent
to transform an original query into a cue-rich one via the judiciously designed
strategy to help the person recall memories. To this end, there are two key
challenges. (1) How to choose the appropriate recall strategy for diverse
forgetting scenarios with distinct memory-recall characteristics? (2) How to
obtain the high-quality responses leveraging recall strategies, given only
abstract and sparsely annotated strategy patterns? To address the challenges,
we propose a Recall Router framework. Specifically, we design a 5W Recall Map
to classify memory queries into five typical scenarios and define fifteen
recall strategy patterns across the corresponding scenarios. We then propose a
hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to
optimize the selection of strategy and the generation of strategy responses. We
construct an instruction tuning dataset and fine-tune multiple open-source
large language models (LLMs) to develop MemoCue, an agent that excels in
providing memory-inspired responses. Experiments on three representative
datasets show that MemoCue surpasses LLM-based methods by 17.74% in recall
inspiration. Further human evaluation highlights its advantages in
memory-recall applications.

</details>


### [26] [Personalized Education with Ranking Alignment Recommendation](https://arxiv.org/abs/2507.23664)
*Haipeng Liu,Yuxuan Liu,Ting Long*

Main category: cs.AI

TL;DR: 论文提出了一种名为RAR的个性化问题推荐方法，通过结合协作思想改进探索机制，提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索效率上存在不足，难以在训练期间为每个学生找到最佳问题。

Method: 提出Ranking Alignment Recommendation (RAR)，将协作思想融入探索机制。

Result: 实验表明RAR显著提升了推荐性能，且框架适用于任何基于强化学习的问题推荐系统。

Conclusion: RAR通过改进探索机制，有效解决了现有方法的局限性，提升了推荐效果。

Abstract: Personalized question recommendation aims to guide individual students
through questions to enhance their mastery of learning targets. Most previous
methods model this task as a Markov Decision Process and use reinforcement
learning to solve, but they struggle with efficient exploration, failing to
identify the best questions for each student during training. To address this,
we propose Ranking Alignment Recommendation (RAR), which incorporates
collaborative ideas into the exploration mechanism, enabling more efficient
exploration within limited training episodes. Experiments show that RAR
effectively improves recommendation performance, and our framework can be
applied to any RL-based question recommender. Our code is available in
https://github.com/wuming29/RAR.git.

</details>


### [27] [TextQuests: How Good are LLMs at Text-Based Video Games?](https://arxiv.org/abs/2507.23701)
*Long Phan,Mantas Mazeika,Andy Zou,Dan Hendrycks*

Main category: cs.AI

TL;DR: TextQuests是一个基于交互式文本游戏的基准测试，旨在评估AI代理在长上下文推理和自主问题解决中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分评估AI代理在探索性环境中的自主推理能力，因此需要更全面的测试工具。

Method: 利用Infocom的交互式小说游戏作为基准，限制外部工具使用，专注于内在推理能力。

Result: TextQuests提供了一个有效的测试平台，用于评估AI代理在长上下文和自主问题解决中的表现。

Conclusion: TextQuests填补了现有基准测试的空白，推动了AI代理在复杂环境中的发展。

Abstract: Evaluating AI agents within complex, interactive environments that mirror
real-world challenges is critical for understanding their practical
capabilities. While existing agent benchmarks effectively assess skills like
tool use or performance on structured tasks, they often do not fully capture an
agent's ability to operate autonomously in exploratory environments that demand
sustained, self-directed reasoning over a long and growing context. To spur the
development of agents capable of more robust intrinsic reasoning over long
horizons, we introduce TextQuests, a benchmark based on the Infocom suite of
interactive fiction games. These text-based adventures, which can take human
players over 30 hours and require hundreds of precise actions to solve, serve
as an effective proxy for evaluating AI agents on focused, stateful tasks. The
benchmark is specifically designed to assess an LLM agent's capacity for
self-contained problem-solving by precluding the use of external tools, thereby
focusing on intrinsic long-context reasoning capabilities in an exploratory
environment characterized by the need for trial-and-error learning and
sustained problem-solving within a single interactive session. We release
TextQuests at https://textquests.ai.

</details>


### [28] [Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving](https://arxiv.org/abs/2507.23726)
*Luoxin Chen,Jinming Gu,Liankai Huang,Wenhao Huang,Zhicheng Jiang,Allan Jie,Xiaoran Jin,Xing Jin,Chenggang Li,Kaijing Ma,Cheng Ren,Jiawei Shen,Wenlei Shi,Tong Sun,He Sun,Jiahui Wang,Siran Wang,Zhihong Wang,Chenrui Wei,Shufa Wei,Yonghui Wu,Yuchen Wu,Yihang Xia,Huajian Xin,Fan Yang,Huaiyuan Ying,Hongyi Yuan,Zheng Yuan,Tianyang Zhan,Chi Zhang,Yue Zhang,Ge Zhang,Tianyun Zhao,Jianqiu Zhao,Yichi Zhou,Thomas Hanwen Zhu*

Main category: cs.AI

TL;DR: Seed-Prover是一种基于Lean反馈的定理证明模型，通过迭代优化证明，在IMO级问题上表现优异，并引入Seed-Geometry解决几何问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在定理证明中因缺乏明确监督信号而表现不佳，而专用语言如Lean能提供清晰的验证反馈。

Method: 提出Seed-Prover模型，利用Lean反馈、已证明引理和自我总结迭代优化证明，并设计三种推理策略。

Result: Seed-Prover在IMO问题上达到78.1%成功率，优于现有技术，并在IMO 2025中成功解决5/6问题。

Conclusion: 结合形式化验证与长链推理，显著提升了自动数学推理能力。

Abstract: LLMs have demonstrated strong mathematical reasoning abilities by leveraging
reinforcement learning with long chain-of-thought, yet they continue to
struggle with theorem proving due to the lack of clear supervision signals when
solely using natural language. Dedicated domain-specific languages like Lean
provide clear supervision via formal verification of proofs, enabling effective
training through reinforcement learning. In this work, we propose
\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover
can iteratively refine its proof based on Lean feedback, proved lemmas, and
self-summarization. To solve IMO-level contest problems, we design three
test-time inference strategies that enable both deep and broad reasoning.
Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F,
and achieves over 50\% on PutnamBench, outperforming the previous
state-of-the-art by a large margin. To address the lack of geometry support in
Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which
outperforms previous formal geometry engines. We use these two systems to
participate in IMO 2025 and fully prove 5 out of 6 problems. This work
represents a significant advancement in automated mathematical reasoning,
demonstrating the effectiveness of formal verification with long
chain-of-thought reasoning.

</details>


### [29] [CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks](https://arxiv.org/abs/2507.23751)
*Ping Yu,Jack Lanchantin,Tianlu Wang,Weizhe Yuan,Olga Golovneva,Ilia Kulikov,Sainbayar Sukhbaatar,Jason Weston,Jing Xu*

Main category: cs.AI

TL;DR: CoT-Self-Instruct是一种合成数据生成方法，通过Chain-of-Thought（CoT）引导LLMs生成高质量合成提示，显著提升推理和指令跟随任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据集在可验证推理和指令跟随任务中表现不足，需要更高质量的合成数据来提升LLM性能。

Method: 基于种子任务，通过CoT引导LLMs生成合成提示，并使用自动指标过滤高质量数据。

Result: 在可验证推理任务中优于现有数据集（如s1k、OpenMathReasoning），在非可验证指令跟随任务中超越人类或标准自提示方法。

Conclusion: CoT-Self-Instruct能有效生成高质量合成数据，显著提升LLM在多种任务中的表现。

Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that
instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the
given seed tasks, and then to generate a new synthetic prompt of similar
quality and complexity for use in LLM training, followed by filtering for
high-quality data with automatic metrics. In verifiable reasoning, our
synthetic data significantly outperforms existing training datasets, such as
s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For
non-verifiable instruction-following tasks, our method surpasses the
performance of human or standard self-instruct prompts on both AlpacaEval 2.0
and Arena-Hard.

</details>


### [30] [SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model](https://arxiv.org/abs/2507.23773)
*Mingkai Deng,Jinyu Hou,Yilin Shen,Hongxia Jin,Graham Neubig,Zhiting Hu,Eric Xing*

Main category: cs.AI

TL;DR: SimuRA是一种基于世界模型的通用AI代理架构，通过模拟规划克服自回归推理的局限性，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理多为单任务设计，缺乏通用性和扩展性，而人类通过模拟行动结果进行推理，SimuRA旨在实现更通用的智能代理。

Method: SimuRA引入世界模型进行规划，利用LLM的潜在语言空间灵活适应多种环境，实验验证其在网页浏览任务中的优势。

Result: 在航班搜索任务中，SimuRA将成功率从0%提升至32.2%，基于世界模型的规划比自回归规划优势高达124%。

Conclusion: SimuRA展示了世界模型模拟作为推理范式的潜力，为训练通用智能代理提供了可能。

Abstract: AI agents built on large language models (LLMs) hold enormous promise, but
current practice focuses on a one-task-one-agent approach, which not only falls
short of scalability and generality, but also suffers from the fundamental
limitations of autoregressive LLMs. On the other hand, humans are general
agents who reason by mentally simulating the outcomes of their actions and
plans. Moving towards a more general and powerful AI agent, we introduce
SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based
on a principled formulation of optimal agent in any environment, \modelname
overcomes the limitations of autoregressive reasoning by introducing a world
model for planning via simulation. The generalized world model is implemented
using LLM, which can flexibly plan in a wide range of environments using the
concept-rich latent space of natural language. Experiments on difficult web
browsing tasks show that \modelname improves the success of flight search from
0\% to 32.2\%. World-model-based planning, in particular, shows consistent
advantage of up to 124\% over autoregressive planning, demonstrating the
advantage of world model simulation as a reasoning paradigm. We are excited
about the possibility for training a single, general agent model based on LLMs
that can act superintelligently in all environments. To start, we make SimuRA,
a web-browsing agent built on \modelname with pretrained LLMs, available as a
research demo for public testing.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [31] [From Link Diversity to Cross-Band Feedback Collaboration: A New Perspective on Hybrid Optical-RF Systems](https://arxiv.org/abs/2507.23686)
*Menghan Li,Yulin Shao,Runxin Zhang,Lu Lu*

Main category: cs.IT

TL;DR: 论文提出了一种新的混合光-射频（O-RF）系统架构O-RF-CBF，通过利用光学下行链路反馈来增强射频上行链路的可靠性，突破了传统多样性网络的局限。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为混合O-RF系统主要是多样性驱动的网络，而本文旨在探索光学下行链路作为实时反馈通道的新用途，以提升上行链路的性能。

Method: 提出O-RF-CBF架构，利用光学下行链路反馈指导射频上行链路的编码策略，实现跨频带协作。

Result: 数值结果显示，O-RF-CBF相比传统O-RF系统显著提升了上行链路的吞吐量。

Conclusion: 研究强调了跨频带协同而非冗余是释放混合无线网络潜力的关键。

Abstract: We suggest a re-examination of the conventional view that hybrid
optical-radio frequency (O-RF) systems are primarily diversity-driven networks
that switch between RF and optical links for robustness. Instead, we uncover a
new architectural opportunity: repurposing the optical downlink to enable
real-time feedback channel coding over the RF uplink, where structured decoder
feedback is delivered from the access point to guide the transmitter's coding
strategy. This insight marks a conceptual paradigm shift from passive link
diversity to active cross-band collaboration, where the wideband,
interference-free optical wireless communication (OWC) is no longer merely a
downlink backup but a functional enabler of uplink reliability. To realize this
vision, we propose a novel architecture, O-RF with Cross-Band Feedback
(O-RF-CBF), that exploits the optical downlink feedback to facilitate adaptive
RF uplink coding. Numerical results reveal that O-RF-CBF achieves significant
uplink throughput gains over traditional O-RF systems. Our findings highlight
that inter-band synergy, not redundancy, is the key to unlocking the full
potential of hybrid wireless networks.

</details>


### [32] [A CPFSK Transceiver with Hybrid CSS-DSSS Spreading for LPWAN PHY Communication](https://arxiv.org/abs/2507.23029)
*Wenkun Wen,Ruiqi Zhang,Peiran Wu,Tierui Min,Minghua Xia*

Main category: cs.IT

TL;DR: 本文提出了一种新型LPWAN收发器，通过改进前导码设计和接收方法，实现了高接收灵敏度和低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统LPWAN收发器在覆盖范围和数据速率之间存在权衡，本文旨在解决这一问题。

Method: 采用CSS前导码和CPFSK调制，结合DSSS技术，优化接收端的同步和解调方案。

Result: 仿真和实地测试表明，该收发器在接收灵敏度和复杂度上优于传统方案。

Conclusion: 该设计为LPWAN提供了一种高性能且低成本的解决方案。

Abstract: Traditional low-power wide-area network (LPWAN) transceivers typically
compromise data rates to achieve deep coverage. This paper presents a novel
transceiver that achieves high receiver sensitivity and low computational
complexity. At the transmitter, we replace the conventional direct sequence
spread spectrum (DSSS) preamble with a chirp spread spectrum (CSS) preamble,
consisting of a pair of down-chirp and up-chirp signals that are conjugate to
each other, simplifying packet synchronization. For enhanced coverage, the
payload incorporates continuous phase frequency shift keying (CPFSK) to
maintain a constant envelope and phase continuity, in conjunction with DSSS to
achieve a high spreading gain. At the receiver, we develop a double-peak
detection method to improve synchronization and a non-coherent joint
despreading and demodulation scheme that increases receiver sensitivity while
maintaining simplicity in implementation. Furthermore, we optimize the preamble
detection threshold and spreading sequences for maximum non-coherent receiver
performance. The software-defined radio (SDR) prototype, developed using GNU
Radio and USRP, along with operational snapshots, showcases its practical
engineering applications. Extensive Monte Carlo simulations and field-test
trials demonstrate that our transceiver outperforms traditional ones in terms
of receiver sensitivity, while also being low in complexity and cost-effective
for LPWAN requirements.

</details>


### [33] [Optimal compressed sensing for mixing stochastic processes](https://arxiv.org/abs/2507.23175)
*Yonatan Gutman,Adam Śpiewak*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Jalali and Poor introduced an asymptotic framework for compressed sensing of
stochastic processes, demonstrating that any rate strictly greater than the
mean information dimension serves as an upper bound on the number of random
linear measurements required for (universal) almost lossless recovery of
$\psi^*$-mixing processes, as measured in the normalized $L^2$ norm. In this
work, we show that if the normalized number of random linear measurements is
strictly less than the mean information dimension, then almost lossless
recovery of a $\psi^*$-mixing process is impossible by any sequence of
decompressors. This establishes the mean information dimension as the
fundamental limit for compressed sensing in this setting (and, in fact, the
precise threshold for the problem). To this end, we introduce a new quantity,
related to techniques from geometric measure theory: the correlation dimension
rate, which is shown to be a lower bound for compressed sensing of arbitrary
stationary stochastic processes.

</details>


### [34] [The Construction of Near-optimal Universal Coding of Integers](https://arxiv.org/abs/2507.23180)
*Wei Yan,Yunghsiang S. Han*

Main category: cs.IT

TL;DR: 该论文研究了通用整数编码（UCI）的最小扩展因子，提出了一种名为ν码的近最优UCI，将最优UCI的最小扩展因子范围缩小至2≤C_C^*≤2.0386，并构造了新的Δδ码。


<details>
  <summary>Details</summary>
Motivation: 研究UCI的最小扩展因子以优化压缩性能，填补现有理论空白。

Method: 提出ν码和Δδ码，并通过理论证明其最优性。

Result: ν码的最小扩展因子为2.0386，Δδ码和ν码是目前最优的UCI。

Conclusion: 论文缩小了最优UCI的最小扩展因子范围，并证明了其下界为2。

Abstract: Universal Coding of Integers (UCI) is suitable for discrete memoryless
sources with unknown probability distributions and infinitely countable
alphabet sizes. The UCI is a class of prefix codes, such that the ratio of the
average codeword length to $\max\{1, H(P)\}$ is within a constant expansion
factor $K_{\mathcal{C}}$ for any decreasing probability distribution $P$, where
$H(P)$ is the entropy of $P$. For any UCI code $\mathcal{C}$, define \emph{the
minimum expansion factor} $K_{\mathcal{C}}^{*}$ to represent the infimum of the
set of extension factors of $\mathcal{C}$. Each $\mathcal{C}$ has a unique
corresponding $K_{\mathcal{C}}^{*}$, and the smaller $K_{\mathcal{C}}^{*}$ is,
the better the compression performance of $\mathcal{C}$ is. A class of UCI
$\mathcal{C}$ (or family $\{\mathcal{C}_i\}_{i=1}^{\infty}$) achieving the
smallest $K_{\mathcal{C}}^{*}$ is defined as the \emph{optimal UCI}. The best
result currently is that the range of $C_{\mathcal{C}}^{*}$ for the optimal UCI
is $2\leq C_{\mathcal{C}}^{*}\leq 2.5$. In this paper, we prove that there
exists a class of near-optimal UCIs, called $\nu$ code, to achieve
$K_\nu=2.0386$. This narrows the range of the minimum expansion factor for
optimal UCI to $2\leq C_{\mathcal{C}}^{*}\leq 2.0386$. Another new class of
UCI, called $\Delta\delta$ code, is specifically constructed. We show that the
$\Delta\delta$ code and $\nu$ code are currently optimal in terms of minimum
expansion factor. In addition, we propose a new proof that shows the minimum
expansion factor of the optimal UCI is lower bounded by $2$.

</details>


### [35] [Efficient DFT of Zadoff-Chu Sequences using lmFH Pattern](https://arxiv.org/abs/2507.23200)
*Fanping Du*

Main category: cs.IT

TL;DR: 本文展示了Zadoff-Chu (ZC)序列的DFT和IDFT计算，提出了基于微频率跳变（lmFH）的直观方法，并引入了一种新的累积和计算方法。


<details>
  <summary>Details</summary>
Motivation: 探索ZC序列的DFT和IDFT计算方式，揭示其与微频率跳变（lmFH）的关联。

Method: 利用lmFH模式直观计算ZC序列的DFT和IDFT，并引入广义二次高斯和计算累积和。

Result: 发现ZC序列的DFT可转化为lmFH符号，且可通过累积频率点计算。

Conclusion: ZC序列的DFT计算可通过lmFH模式简化，为相关应用提供了新思路。

Abstract: Having established that Zadoff-Chu (ZC) sequences are inherently linear
micro-frequency hopping (lmFH) symbols, this paper first presents an intuitive
and visual exposition of the computation of the DFT and IDFT of ZC sequences
using the lmFH pattern. This yields interesting results. Subsequently, an
alternative form for computing the cumulative sum of ZC sequences using the
Generalized Quadratic Gauss Sum is introduced. Furthermore, building on the
micro-frequency hopping (mFH) concept, this paper shows that the DFT of ZC
sequences can be transformed into an lmFH symbol with frequency shift and phase
offset. Therefore, the DFT of ZC sequences can be computed via cumulative
frequency points, similar to the computation of normal mFH symbols.

</details>


### [36] [Secure Integrated Sensing and Communication Networks: Stochastic Performance Analysis](https://arxiv.org/abs/2507.23234)
*Marziyeh Soltani,Mahtab Mirmohseni,Rahim Tafazolli*

Main category: cs.IT

TL;DR: 本文分析了MIMO ISAC系统在下行链路场景中的随机安全性能，考虑了通信和感知的多功能信号传输，并评估了窃听威胁下的性能。


<details>
  <summary>Details</summary>
Motivation: 研究MIMO ISAC系统在随机网络中的安全性能，解决通信与感知的权衡问题，同时保障安全和隐私。

Method: 通过推导ESR和CRB，分析SNR和CRB的概率密度函数，利用中心极限定理简化计算。

Result: 表征了CRB-保密率区域的边界，揭示了通信与感知之间的性能权衡。

Conclusion: 在随机ISAC网络中，通过多功能信号传输，可以同时实现通信、感知和安全性能的平衡。

Abstract: This paper analyzes the stochastic security performance of a multiple-input
multiple-output (MIMO) integrated sensing and communication (ISAC) system in a
downlink scenario. A base station (BS) transmits a multi-functional signal to
simultaneously communicate with a user, sense a target's angular location, and
counteract eavesdropping threats. The attack model considers a passive
single-antenna communication eavesdropper intercepting communication data, as
well as a multi-antenna sensing eavesdropper attempting to infer the target's
location. We also consider a malicious target scenario where the target plays
the role of the communication eavesdropper. The BS-user and BS-eavesdroppers
channels follow Rayleigh fading, while the target's azimuth angle is uniformly
distributed. To evaluate the performance in this random network, we derive the
ergodic secrecy rate (ESR) and the ergodic Cramer-Rao lower bound (CRB), for
target localization, at both the BS and the sensing eavesdropper. This involves
computing the probability density functions (PDFs) of the signal-to-noise ratio
(SNR) and CRB, leveraging the central limit theorem for tractability. We
characterize the boundary of the CRB-secrecy rate region, and interpret the
performance tradeoffs between communication and sensing while guaranteeing a
level of security and privacy in the random ISAC networks.

</details>


### [37] [Exploiting Movable Elements of Intelligent Reflecting Surface for Enhancement of Integrated Sensing and Communication](https://arxiv.org/abs/2507.23296)
*Xingyu Peng,Qin Tao,Yong Liang Guan,Xiaoming Chen*

Main category: cs.IT

TL;DR: 利用智能反射面（IRS）的可移动元素提升集成感知与通信（ISAC）系统性能，提出单用户和多用户场景下的联合波束成形与元素位置优化方案。


<details>
  <summary>Details</summary>
Motivation: 提升ISAC系统的通信速率和感知精度，扩展其覆盖范围。

Method: 通过性能分析揭示可移动元素的作用，设计单用户和多用户场景下的联合波束成形与元素位置优化方案。

Result: 仿真结果表明，IRS元素的移动可提高通信速率和感知精度，并扩展ISAC的覆盖范围。

Conclusion: IRS的可移动元素能显著提升ISAC系统的整体性能。

Abstract: In this paper, we propose to exploit movable elements of intelligent
reflecting surface (IRS) to enhance the overall performance of integrated
sensing and communication (ISAC) systems. Firstly, focusing on a single-user
scenario, we reveal the function of movable elements by performance analysis,
and then design a joint beamforming and element position optimization scheme.
Further, we extend it to a general multi-user scenario, and also propose an
element position optimization scheme according to the derived performance
expressions. Finally, simulation results confirm that the movement of IRS
elements can improve the communication rate and the sensing accuracy, and
especially broaden the coverage of ISAC.

</details>


### [38] [Hybrid Generative Semantic and Bit Communications in Satellite Networks: Trade-offs in Latency, Generation Quality, and Computation](https://arxiv.org/abs/2507.23528)
*Chong Huang,Gaojie Chen,Jing Zhu,Qu Luo,Pei Xiao,Wei Huang,Rahim Tafazolli*

Main category: cs.IT

TL;DR: 提出了一种多层混合比特与生成语义通信框架，用于动态卫星通信网络，并引入新的语义通信效率指标（SEM）和深度强化学习算法GRPO优化资源分配。


<details>
  <summary>Details</summary>
Motivation: 卫星通信在未来无线网络中日益重要，但存在链路预算有限的问题，语义通信虽能解决此问题，但增加了计算资源消耗。

Method: 提出多层混合比特与生成语义通信框架，引入SEM指标评估延迟、计算消耗和语义重建质量的权衡，并使用GRPO算法优化资源分配。

Result: 仿真结果表明所提框架的灵活性及SEM指标的有效性，揭示了语义通信指标间的关系。

Conclusion: 所提框架和SEM指标能有效解决卫星通信中的资源分配问题，平衡效率与性能。

Abstract: As satellite communications play an increasingly important role in future
wireless networks, the issue of limited link budget in satellite systems has
attracted significant attention in current research. Although semantic
communications emerge as a promising solution to address these constraints, it
introduces the challenge of increased computational resource consumption in
wireless communications. To address these challenges, we propose a multi-layer
hybrid bit and generative semantic communication framework which can adapt to
the dynamic satellite communication networks. Furthermore, to balance the
semantic communication efficiency and performance in satellite-to-ground
transmissions, we introduce a novel semantic communication efficiency metric
(SEM) that evaluates the trade-offs among latency, computational consumption,
and semantic reconstruction quality in the proposed framework. Moreover, we
utilize a novel deep reinforcement learning (DRL) algorithm group relative
policy optimization (GRPO) to optimize the resource allocation in the proposed
network. Simulation results demonstrate the flexibility of our proposed
transmission framework and the effectiveness of the proposed metric SEM,
illustrate the relationships among various semantic communication metrics.

</details>


### [39] [Cell-Free Massive MIMO SWIPT with Beyond Diagonal Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2507.23702)
*Duc Thien Hua,Mohammadali Mohammadi,Hien Quoc Ngo,Michail Matthaiou*

Main category: cs.IT

TL;DR: 论文研究了在无小区大规模MIMO系统中集成超对角可重构智能表面（BDRIS）以增强同时无线信息和能量传输（SWIPT），通过优化AP选择、功率控制和BDRIS散射矩阵设计，显著提高了能量收集效率。


<details>
  <summary>Details</summary>
Motivation: 解决在同时支持信息接收器（IRs）和能量接收器（ERs）时，如何在不牺牲时频资源的情况下提升能量传输效率的问题。

Method: 采用保护性部分零强迫预编码技术，优化AP选择、功率控制和BDRIS散射矩阵设计，并提出启发式搜索、逐次凸逼近和深度强化学习算法。

Result: BDRIS在组或全连接架构下比传统对角RIS显著提高了能量收集效率，启发式散射矩阵设计使平均能量收集量提升七倍。

Conclusion: BDRIS在SWIPT系统中具有显著优势，能够高效支持多用户需求，为未来无线通信系统提供了新的优化方向。

Abstract: We investigate the integration of beyond diagonal reconfigurable intelligent
surfaces (BDRISs) into cell free massive multiple input multiple output
(CFmMIMO) systems to enhance simultaneous wireless information and power
transfer (SWIPT). To simultaneously support two groups of users energy
receivers (ERs) and information receivers (IRs) without sacrificing time
frequency resources, a subset of access points (APs) is dedicated to serving
ERs with the aid of a BDRIS, while the remaining APs focus on supporting IRs. A
protective partial zero forcing precoding technique is implemented at the APs
to manage the non coherent interference between the ERs and IRs. Subsequently,
closed form expressions for the spectral efficiency of the IRs and the average
sum of harvested energy at the ERs are leveraged to formulate a comprehensive
optimization problem. This problem jointly optimizes the AP selection, AP power
control, and scattering matrix design at the BDRIS, all based on long term
statistical channel state information. This challenging problem is then
effectively transformed into more tractable forms. To solve these sub problems,
efficient algorithms are proposed, including a heuristic search for the
scattering matrix design, as well as successive convex approximation and deep
reinforcement learning methods for the joint AP mode selection and power
control design. Numerical results show that a BDRIS with a group or fully
connected architecture achieves significant energy harvesting gains over the
conventional diagonal RIS, especially delivering up to a seven fold increase in
the average sum of harvested energy when a heuristic based scattering matrix
design is employed.

</details>
