<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.AI](#cs.AI) [Total: 71]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Dynamic Hierarchical Birkhoff-von Neumann Decomposition for All-to-All GPU Communication](https://arxiv.org/abs/2602.22756)
*Yen-Chieh Wu,Cheng-Shang Chang,Duan-Shin Lee,H. Jonathan Chao*

Main category: cs.NI

TL;DR: 提出动态分层Birkhoff-von Neumann分解框架，针对两层级GPU集群优化all-to-all通信调度，通过局部平衡和分层分解降低复杂度，结合动态帧大小实现可证明的稳定性。


<details>
  <summary>Details</summary>
Motivation: 大规模训练集群中，all-to-all GPU通信是关键瓶颈，受限于端口带宽和流量倾斜影响。现代GPU系统的两层级结构（快速服务器内链路+慢速服务器间网络）加剧了这一问题，需要流量整形和层级感知的调度方案。

Method: 提出动态分层Birkhoff-von Neumann分解框架：1) 在帧边界处，首先通过简单局部操作平衡服务器内GPU/NIC流量，缓解微观倾斜同时保持服务器间总需求；2) 在服务器层级应用分层BvN分解，然后细化为GPU级匹配，相比扁平GPU级方法显著降低分解复杂度；3) 结合动态帧大小原则，构建具有可证明稳定性的在线调度器。

Result: 仿真显示，该方法显著降低了平均帧长度，特别是在服务器局部热点流量场景下效果明显。调度器在可容许泊松到达下具有可证明的稳定性。

Conclusion: 针对两层级GPU架构的动态分层BvN分解框架能有效解决all-to-all通信瓶颈，通过层级感知的流量整形和复杂度降低，在保持稳定性的同时显著提升通信性能。

Abstract: All-to-all GPU communication is a critical bottleneck in large-scale training clusters, where completion time is constrained by per-port bandwidth and can be severely impacted by traffic skew across GPUs and network interface cards (NICs). This issue is amplified by the two-tier structure of modern GPU systems, which combine fast intra-server links with much slower inter-server networks. Motivated by recent system observations that highlight the importance of traffic reshaping and hierarchy awareness, we study all-to-all scheduling from an online switching and queueing-theoretic perspective.
  We propose a dynamic hierarchical Birkhoff--von Neumann (BvN) decomposition framework tailored to two-tier GPU fabrics. At each frame boundary, traffic is first balanced within each server using simple local operations to mitigate micro-level GPU/NIC skew while preserving aggregate server-to-server demand. A hierarchical BvN decomposition is then applied at the server level and refined into GPU-level matchings, significantly reducing decomposition complexity relative to a flat GPU-level approach. By integrating this construction with the dynamic frame sizing (DFS) principle, we obtain an online scheduler with provable stability under admissible Poisson arrivals. Simulations demonstrate substantial reductions in mean frame length, particularly under server-localized hotspot traffic.

</details>


### [2] [BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models](https://arxiv.org/abs/2602.19929)
*Chenran Kou,Changsheng You,Mingjiang Wu,Dingzhu Wen,Zezhong Zhang,Chengwen Xing*

Main category: cs.NI

TL;DR: BeamVLM：一种基于视觉语言模型的端到端波束预测生成框架，将波束预测视为视觉问答任务，通过将原始视觉补丁投影到语言域并结合智能提示设计，实现无人机轨迹和环境上下文联合推理，显著提升预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对低空经济中高速移动无人机与地面基站间的波束预测问题，现有深度学习方法缺乏对动态环境的高层语义理解导致泛化能力差，而大型语言模型方法虽能增强泛化但缺乏丰富的环境感知能力，无法捕捉精细空间语义以实现精确波束对准。

Method: 提出BeamVLM端到端生成框架，将波束预测视为视觉问答任务，利用现有视觉语言模型。通过将原始视觉补丁直接投影到语言域，并精心设计指令提示，使VLM能够联合推理无人机轨迹和环境上下文。

Result: 在真实世界数据集上的实验结果表明，BeamVLM在预测精度上优于最先进方法，并且在车对基础设施波束预测等其他场景中展现出优越的泛化能力。

Conclusion: BeamVLM通过将波束预测重新定义为视觉问答任务，成功结合了视觉语言模型的强大能力，实现了对动态环境的高层语义理解和精细空间语义捕捉，为低空经济中的波束预测提供了有效的解决方案。

Abstract: For low-altitude economy (LAE), fast and accurate beam prediction between high-mobility unmanned aerial vehicles (UAVs) and ground base stations is of paramount importance, which ensures seamless coverage and reliable communications. However, existing deep learning-based beam prediction methods lack high-level semantic understanding of dynamic environments, resulting in poor generalization. On the other hand, the emerging large language model (LLM) based approaches show promise in enhancing generalization, but they typically lack rich environmental perception, thereby failing to capture fine-grained spatial semantics essential for precise beam alignment. To tackle these limitations, we propose in this correspondence a novel end-to-end generative framework for beam prediction, called BeamVLM, which treats beam prediction as a vision question answering task capitalizing on powerful existing vision-language models (VLMs). By projecting raw visual patches directly into the language domain and judiciously designing an instructional prompt, the proposed BeamVLM enables the VLM to jointly reason over UAV trajectories and environmental context. Last, experimental results on real-world datasets demonstrate that the proposed BeamVLM outperforms state-of-the-art methods in prediction accuracy and also exhibits superior generalization for other scenarios such as vehicle-to-infrastructure (V2I) beam prediction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的多智能体F1赛车策略优化方法，通过交互模块和自博弈训练生成竞争性策略，能够根据对手行为调整进站时机、轮胎选择和能量分配。


<details>
  <summary>Details</summary>
Motivation: F1比赛中，车队需要根据不断变化的比赛条件和竞争对手的行动来调整比赛策略。传统策略制定往往依赖经验，难以实时应对复杂的多智能体交互场景，因此需要一种能够自适应优化策略的智能方法。

Method: 基于预训练的单智能体策略，引入交互模块来考虑竞争对手的行为。结合交互模块和自博弈训练方案生成竞争性策略，并根据相对性能对智能体进行排名。框架仅依赖真实比赛中可用的信息。

Result: 智能体能够根据对手行为自适应调整进站时机、轮胎选择和能量分配，实现了稳健且一致的比赛表现。该框架能够支持比赛策略师在赛前和赛中做出决策。

Conclusion: 提出的强化学习方法能够有效优化F1多智能体比赛策略，通过考虑竞争对手行为的交互模块和自博弈训练，实现了适应性强的策略优化，为实际比赛策略制定提供了可行的技术支持。

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [4] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: GYWI系统结合作者知识图谱与检索增强生成，为LLMs提供可控学术背景和可追溯灵感路径，显著提升科学创意生成质量


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在科学创意生成中缺乏可控的学术背景和可追溯的灵感路径，需要构建能够提供深度知识支持和灵感溯源的系统

Method: 1) 提出作者中心知识图谱构建方法和灵感源采样算法；2) 结合RAG和GraphRAG的混合检索机制；3) 融入强化学习原理的Prompt优化策略；4) 基于arXiv构建评估数据集和综合评估方法

Result: GYWI在GPT-4o、DeepSeek-V3、Qwen3-8B和Gemini 2.5等多个LLMs上显著优于主流方法，在新颖性、可靠性、相关性等多个指标上表现优异

Conclusion: GYWI系统通过结合知识图谱和混合检索机制，有效解决了LLMs科学创意生成中的背景控制和灵感溯源问题，为AI辅助科研提供了新思路

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [5] [FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation](https://arxiv.org/abs/2602.22273)
*Xiyuan Zhang,Huihang Wu,Jiayu Guo,Zhenlin Zhang,Yiwei Zhang,Liangyu Huo,Xiaoxiao Ma,Jiansong Wan,Xuewei Jiao,Yi Jing,Jian Xie*

Main category: cs.AI

TL;DR: FIRE是一个综合性金融基准测试，用于评估LLMs的理论金融知识和实际业务场景处理能力，包含理论考试题和3000个金融场景问题。


<details>
  <summary>Details</summary>
Motivation: 需要全面评估LLMs在金融领域的理论知识和实际应用能力，了解当前模型在金融应用中的能力边界。

Method: 1) 理论评估：从广泛认可的金融资格考试中收集多样化试题；2) 实践评估：提出系统评估矩阵，分类复杂金融领域，收集3000个金融场景问题（封闭式决策题和开放式问题）。

Result: 对包括轩辕4.0在内的最先进LLMs进行了全面评估，轩辕4.0作为强大的领域内基线模型，公开了基准问题和评估代码。

Conclusion: FIRE基准测试能够系统分析当前LLMs在金融应用中的能力边界，为未来研究提供支持。

Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.

</details>


### [6] [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)
*Willem Schooltink,Fabio Massimo Zennaro*

Main category: cs.AI

TL;DR: 该论文提出了因果嵌入框架，作为因果抽象概念的推广，允许多个详细模型嵌入到更粗粒度因果模型的子系统中，并展示了其在统计边际问题和因果边际问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象研究主要关注两个模型之间的关系，但实际应用中需要将多个详细模型映射到更粗粒度模型的子系统中，以处理来自不同表示的数据集合并问题。

Method: 定义因果嵌入作为抽象概念的推广，提出广义一致性概念，建立多分辨率边际问题框架，将统计边际问题和因果边际问题统一起来。

Result: 展示了因果嵌入框架在统计边际问题和因果边际问题中的相关性，并证明了其在合并来自不同表示模型的数据集方面的实际应用价值。

Conclusion: 因果嵌入框架为处理多模型、多分辨率因果推理提供了理论基础，特别是在数据集合并和因果边际问题中具有重要应用前景。

Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>


### [7] [Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents](https://arxiv.org/abs/2602.22302)
*Varun Pratap Bhardwaj*

Main category: cs.AI

TL;DR: 提出Agent Behavioral Contracts (ABC)框架，将契约设计原则应用于AI智能体，通过运行时强制执行来约束行为漂移，提高可靠性和合规性。


<details>
  <summary>Details</summary>
Motivation: 传统软件通过API、类型系统等契约来确保正确行为，而AI智能体仅依赖提示和自然语言指令，缺乏形式化行为规范，导致行为漂移、治理失败和项目失败。

Method: 引入ABC框架，定义契约C=(P,I,G,R)包含前置条件、不变式、治理策略和恢复机制，提出(p,delta,k)-满足度的概率合规概念，证明漂移边界定理，实现AgentAssert运行时强制执行库。

Result: 在AgentContract-Bench基准测试中，契约化智能体检测到5.2-6.8次软违规（基线完全未检测），实现88-100%硬约束合规，将行为漂移限制在D*<0.27，前沿模型恢复率达100%，所有开销<10ms/动作。

Conclusion: ABC框架为AI智能体提供了形式化行为规范，通过运行时强制执行显著提高可靠性和合规性，有效约束行为漂移，为多智能体系统的安全组合提供了理论基础。

Abstract: Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and frequent project failures in agentic AI deployments. We introduce Agent Behavioral Contracts (ABC), a formal framework that brings Design-by-Contract principles to autonomous AI agents. An ABC contract C = (P, I, G, R) specifies Preconditions, Invariants, Governance policies, and Recovery mechanisms as first-class, runtime-enforceable components. We define (p, delta, k)-satisfaction -- a probabilistic notion of contract compliance that accounts for LLM non-determinism and recovery -- and prove a Drift Bounds Theorem showing that contracts with recovery rate gamma > alpha (the natural drift rate) bound behavioral drift to D* = alpha/gamma in expectation, with Gaussian concentration in the stochastic setting. We establish sufficient conditions for safe contract composition in multi-agent chains and derive probabilistic degradation bounds. We implement ABC in AgentAssert, a runtime enforcement library, and evaluate on AgentContract-Bench, a benchmark of 200 scenarios across 7 models from 6 vendors. Results across 1,980 sessions show that contracted agents detect 5.2-6.8 soft violations per session that uncontracted baselines miss entirely (p < 0.0001, Cohen's d = 6.7-33.8), achieve 88-100% hard constraint compliance, and bound behavioral drift to D* < 0.27 across extended sessions, with 100% recovery for frontier models and 17-100% across all models, at overhead < 10 ms per action.

</details>


### [8] [Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?](https://arxiv.org/abs/2602.22401)
*Yongjun Zhang*

Main category: cs.AI

TL;DR: 论文提出"氛围研究"概念，类比"氛围编程"，认为AI代理能自主执行完整研究流程，但存在理论原创性和隐性知识局限，需建立负责任的研究原则。


<details>
  <summary>Details</summary>
Motivation: AI代理在社会科学研究中代表质的飞跃，能执行多步骤推理工作流，但需要明确其能力边界和对研究职业的影响。

Method: 提出认知任务框架，按可编码性和隐性知识需求两个维度分类研究活动；使用scholar-skill（21技能插件）作为案例；分析AI代理在速度、覆盖范围和方法论支持方面的优势与局限。

Result: AI代理擅长速度、覆盖范围和方法论支持，但在理论原创性和领域隐性知识方面存在局限；委托边界是认知性的而非顺序性的，贯穿研究流程的每个阶段。

Conclusion: AI代理对研究职业带来三个影响：脆弱条件下的增强、分层风险、教学危机；提出负责任"氛围研究"的五项原则。

Abstract: AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, query databases, search the web, and invoke domain-specific skills to execute entire research pipelines autonomously. This paper introduces the concept of vibe researching -- the AI-era parallel to ``vibe coding'' (Karpathy, 2025) -- and uses scholar-skill, a 21-skill plugin for Claude Code covering the full research pipeline from idea to submission, as an illustrative case. I develop a cognitive task framework that classifies research activities along two dimensions -- codifiability and tacit knowledge requirement -- to identify a delegation boundary that is cognitive, not sequential: it cuts through every stage of the research pipeline, not between stages. I argue that AI agents excel at speed, coverage, and methodological scaffolding but struggle with theoretical originality and tacit field knowledge. The paper concludes with an analysis of three implications for the profession -- augmentation with fragile conditions, stratification risk, and a pedagogical crisis -- and proposes five principles for responsible vibe researching.

</details>


### [9] [Towards Autonomous Memory Agents](https://arxiv.org/abs/2602.22406)
*Xinle Wu,Rui Zhang,Mustafa Anis Hussain,Yao Lu*

Main category: cs.AI

TL;DR: U-Mem提出自主记忆代理，通过成本感知知识提取级联和语义感知Thompson采样，主动获取、验证和整理知识，显著提升LLM在可验证和不可验证任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理是被动和反应式的，记忆增长受限于可用信息，在不确定性时很少主动寻求外部输入。需要自主记忆代理来主动获取、验证和整理知识。

Method: 1) 成本感知知识提取级联：从廉价的自/教师信号逐步升级到工具验证的研究，仅在必要时寻求专家反馈；2) 语义感知Thompson采样：平衡记忆的探索与利用，缓解冷启动偏差。

Result: 在可验证和不可验证基准测试中，U-Mem持续优于先前记忆基线，甚至超越基于RL的优化：HotpotQA（Qwen2.5-7B）提升14.6分，AIME25（Gemini-2.5-flash）提升7.33分。

Conclusion: U-Mem实现了自主记忆代理，通过主动知识获取和智能记忆管理，以最小成本显著提升LLM性能，为记忆系统设计提供了新方向。

Abstract: Recent memory agents improve LLMs by extracting experiences and conversation history into an external storage. This enables low-overhead context assembly and online memory update without expensive LLM training. However, existing solutions remain passive and reactive; memory growth is bounded by information that happens to be available, while memory agents seldom seek external inputs in uncertainties. We propose autonomous memory agents that actively acquire, validate, and curate knowledge at a minimum cost. U-Mem materializes this idea via (i) a cost-aware knowledge-extraction cascade that escalates from cheap self/teacher signals to tool-verified research and, only when needed, expert feedback, and (ii) semantic-aware Thompson sampling to balance exploration and exploitation over memories and mitigate cold-start bias. On both verifiable and non-verifiable benchmarks, U-Mem consistently beats prior memory baselines and can surpass RL-based optimization, improving HotpotQA (Qwen2.5-7B) by 14.6 points and AIME25 (Gemini-2.5-flash) by 7.33 points.

</details>


### [10] [Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus](https://arxiv.org/abs/2602.22408)
*Caroline Ahn,Quan Do,Leah Bakst,Michael P. Pascale,Joseph T. McGuire,Michael E. Hasselmo,Chantal E. Stern*

Main category: cs.AI

TL;DR: 研究者开发了CogARC数据集来研究人类抽象推理策略，通过两个实验让260名参与者解决75个视觉推理问题，记录其行为数据，发现人类在抽象推理中表现出高效但策略多样的特点。


<details>
  <summary>Details</summary>
Motivation: 研究人类在抽象推理中的认知策略灵活性，了解人类如何从稀疏示例中快速学习和应用规则，为人工智能的抽象推理能力提供认知科学基础。

Method: 使用CogARC数据集（ARC的子集），在两个实验中让260名参与者解决75个抽象视觉推理问题，记录高时间分辨率的行为数据，包括示例查看、编辑序列和多尝试提交。

Result: 参与者整体表现良好（实验1准确率约90%，实验2约80%），但表现随问题和个体差异大。更难的问题需要更长的思考时间和更多样的策略。即使错误解决方案也常常高度收敛，但解决轨迹在长度和平滑度上差异显著。

Conclusion: CogARC为研究人类抽象推理提供了丰富的行为环境，揭示了人类在不确定性下如何泛化、错误泛化和调整策略，为理解人类认知灵活性和人工智能发展提供了重要见解。

Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.

</details>


### [11] [Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents](https://arxiv.org/abs/2602.22413)
*Jonas Karge*

Main category: cs.AI

TL;DR: 该论文研究了一种选择性投票机制，其中异质智能体通过学习估计自身可靠性并选择性弃权，从而提升群体决策准确性，将经典孔多塞陪审团定理推广到置信度门控的序列设置。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的集体决策通常允许参与者说"我不知道"，而经典的孔多塞陪审团定理假设固定参与。作者旨在研究智能体通过学习自身可靠性并选择性弃权如何影响群体准确性，特别是在AI安全领域，这种机制可能有助于减轻大型语言模型集体决策中的幻觉问题。

Method: 提出一个概率框架：智能体首先经历一个校准阶段，更新对自身固定能力的信念；然后面对一个最终置信度门控，决定是否投票或弃权。推导了群体成功概率的非渐近下界，并通过蒙特卡洛模拟验证这些边界。

Result: 证明了选择性参与将孔多塞陪审团定理的渐近保证推广到序列、置信度门控的设置中。获得了群体成功概率的非渐近下界，并通过模拟验证了理论结果的有效性。

Conclusion: 选择性投票机制能够有效提升群体决策准确性，特别是在异质智能体能够学习自身可靠性并基于置信度决定是否参与的情况下。该框架对AI安全有潜在应用价值，可用于减轻大型语言模型集体决策中的幻觉问题。

Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \textit{hallucinations} in collective LLM decision-making.

</details>


### [12] [ArchAgent: Agentic AI-driven Computer Architecture Discovery](https://arxiv.org/abs/2602.22425)
*Raghav Gupta,Akanksha Jain,Abraham Gonzalez,Alexander Novikov,Po-Sen Huang,Matej Balog,Marvin Eisenberger,Sergey Shirobokov,Ngân Vũ,Martin Dixon,Borivoje Nikolić,Parthasarathy Ranganathan,Sagar Karandikar*

Main category: cs.AI

TL;DR: ArchAgent是一个基于AlphaEvolve的自动计算机架构发现系统，能够在无人工干预下快速设计出超越现有最优性能的缓存替换策略，并在模拟器中发现了"模拟器逃逸"现象。


<details>
  <summary>Details</summary>
Motivation: 面对计算需求的爆炸式增长，需要敏捷的硬件设计流程作为力量倍增器。虽然AI代理系统已在算法设计和代码优化方面取得进展，但尚未应用于计算机架构发现领域。

Method: 基于AlphaEvolve构建ArchAgent系统，自动设计和实现最先进的缓存替换策略（包括新机制和逻辑，而不仅仅是参数调整）。系统还支持"硅后超专业化"，通过调整硬件策略中可运行时配置的参数来适应特定工作负载。

Result: 在2天内生成的多核Google工作负载策略实现了5.3%的IPC加速提升；在18天内生成的单核SPEC06策略实现了0.9%的IPC加速提升，速度比人工开发快3-5倍。通过硅后超专业化，在SPEC06上进一步实现了2.4%的IPC加速提升。

Conclusion: ArchAgent展示了AI代理在计算机架构发现中的巨大潜力，能够快速超越人类专家设计的策略。同时揭示了"模拟器逃逸"现象，表明现有研究工具需要适应AI代理时代的新需求。

Abstract: Agile hardware design flows are a critically needed force multiplier to meet the exploding demand for compute. Recently, agentic generative AI systems have demonstrated significant advances in algorithm design, improving code efficiency, and enabling discovery across scientific domains.
  Bridging these worlds, we present ArchAgent, an automated computer architecture discovery system built on AlphaEvolve. We show ArchAgent's ability to automatically design/implement state-of-the-art (SoTA) cache replacement policies (architecting new mechanisms/logic, not only changing parameters), broadly within the confines of an established cache replacement policy design competition.
  In two days without human intervention, ArchAgent generated a policy achieving a 5.3% IPC speedup improvement over the prior SoTA on public multi-core Google Workload Traces. On the heavily-explored single-core SPEC06 workloads, it generated a policy in just 18 days showing a 0.9% IPC speedup improvement over the existing SoTA (a similar "winning margin" as reported by the existing SoTA). ArchAgent achieved these gains 3-5x faster than prior human-developed SoTA policies.
  Agentic flows also enable "post-silicon hyperspecialization" where agents tune runtime-configurable parameters exposed in hardware policies to further align the policies with a specific workload (mix). Exploiting this, we demonstrate a 2.4% IPC speedup improvement over prior SoTA on SPEC06 workloads.
  Finally, we outline broader implications for computer architecture research in the era of agentic AI. For example, we demonstrate the phenomenon of "simulator escapes", where the agentic AI flow discovered and exploited a loophole in a popular microarchitectural simulator - a consequence of the fact that these research tools were designed for a (now past) world where they were exclusively operated by humans acting in good-faith.

</details>


### [13] [A Mathematical Theory of Agency and Intelligence](https://arxiv.org/abs/2602.22519)
*Wael Hafez,Chenan Wei,Rodrigo Felipe,Amir Nazeri,Cameron Reid*

Main category: cs.AI

TL;DR: 该论文提出"双预测性(P)"作为衡量系统在交互中有效利用信息的指标，证明其在量子系统中可达1，经典系统≤0.5，引入代理后更低，并区分了代理与智能，提出基于丘脑皮质调节的反馈架构。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统处理大量信息做出复杂预测，但预测成功可能掩盖与环境的交互质量下降。需要一种原则性方法来衡量系统观察、行动和结果之间实际共享的信息量，以评估资源利用效率。

Method: 从第一性原理推导出双预测性(P)作为交互的内在度量，在物理系统(双摆)、强化学习智能体和多轮LLM对话中验证理论界限，并设计基于生物丘脑皮质调节的实时监控反馈架构。

Result: 证明P在量子系统中可达1，经典系统≤0.5，引入代理选择后更低。实验验证了这些界限，区分了代理(基于预测行动)与智能(还需从交互学习、自我监控学习效果、调整观察行动结果范围以恢复有效学习)。

Conclusion: 当前AI系统达到代理水平但未实现智能。提出监控P的反馈架构是构建自适应、弹性AI的前提，为评估和改进AI系统在变化条件下的可靠性提供了理论基础。

Abstract: To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.

</details>


### [14] [How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?](https://arxiv.org/abs/2602.22441)
*Yingqian Cui,Zhenwei Dai,Bing He,Zhan Shi,Hui Liu,Rui Sun,Zhiji Liu,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 本文对潜在推理方法进行了全面分析，揭示了其内部机制中的两个关键问题：普遍存在的捷径行为和潜在空间搜索的不忠实性，并发现了监督强度与潜在表示多样性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 潜在推理作为一种新兴的推理范式，通过在潜在空间而非文本空间生成步骤进行多步推理，能够超越离散语言标记的限制。尽管已有大量研究关注提升潜在推理的性能，但其内部机制尚未得到充分研究。本文旨在深入理解潜在表示在推理过程中的作用和行为。

Method: 对具有不同监督水平的潜在推理方法进行综合分析，识别关键问题：1) 观察普遍存在的捷径行为；2) 检验潜在推理是否支持类似广度优先搜索的探索；3) 分析监督强度对潜在推理能力的影响。

Result: 发现两个关键问题：1) 潜在推理方法普遍存在捷径行为，能在不依赖潜在推理的情况下获得高准确率；2) 潜在表示虽然能编码多种可能性，但推理过程并未忠实实现结构化搜索，而是表现出隐式剪枝和压缩。同时揭示了监督强度的权衡：强监督能缓解捷径行为但限制潜在表示保持多样假设的能力，弱监督允许更丰富的潜在表示但会增加捷径行为。

Conclusion: 潜在推理方法的内部机制存在系统性缺陷，包括捷径行为和搜索不忠实性。监督强度在缓解捷径行为与保持潜在表示多样性之间存在根本性权衡。这些发现为未来改进潜在推理方法提供了重要见解。

Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.

</details>


### [15] [A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring](https://arxiv.org/abs/2602.23163)
*Usman Anwar,Julianna Piskorz,David D. Baek,David Africa,Jim Weatherall,Max Tegmark,Christian Schroeder de Witt,Mihaela van der Schaar,David Krueger*

Main category: cs.AI

TL;DR: 提出决策理论视角的隐写术检测框架，通过比较能解码和不能解码隐藏内容的智能体之间的效用差异来量化LLM中的隐写推理能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型开始展现隐写能力，这可能让未对齐的模型逃避监督机制。传统隐写检测方法需要已知非隐写信号的参考分布，这在LLM隐写推理场景中不可行，因此需要新的检测方法。

Method: 提出决策理论的隐写术视角，核心洞察是隐写术在能解码和不能解码隐藏内容的智能体之间创建了可用信息的不对称性。引入广义V信息作为衡量输入中可用信息的功利主义框架，并定义"隐写差距"来量化隐写术。

Result: 经验验证了该形式化方法，证明它可以用于检测、量化和缓解LLM中的隐写推理。

Conclusion: 提出的决策理论框架为LLM隐写术检测提供了新方法，解决了传统方法在缺乏参考分布时的局限性，能够有效识别和量化模型中的隐写行为。

Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.

</details>


### [16] [A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines](https://arxiv.org/abs/2602.22442)
*Gaoyuan Du,Amit Ahlawat,Xiaoyang Liu,Jing Wu*

Main category: cs.AI

TL;DR: 提出一个评估代理（EA）用于对AutoML代理进行决策中心化评估，而非仅关注最终任务性能，能检测错误决策、识别推理不一致性并量化决策影响


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的AutoML系统评估过于结果中心化，只关注最终任务性能，缺乏对中间决策质量的评估指标，无法揭示决策层面的失败模式

Method: 设计一个评估代理（EA）作为观察者，在不干扰AutoML代理执行的情况下，从四个维度评估中间决策：决策有效性、推理一致性、超出准确率的模型质量风险、反事实决策影响

Result: 在四个概念验证实验中，EA能检测错误决策（F1分数0.919）、识别与最终结果无关的推理不一致性，并量化决策对下游性能的影响（-4.9%到+8.3%）

Conclusion: 决策中心化评估能揭示仅靠结果指标无法发现的失败模式，为可靠、可解释、可治理的自主ML系统提供了评估基础

Abstract: Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find that none of the surveyed agentic AutoML systems report structured, decision-level evaluation metrics intended for post-hoc assessment of intermediate decision quality. To address this limitation, we propose an Evaluation Agent (EA) that performs decision-centric assessment of AutoML agents without interfering with their execution. The EA is designed as an observer that evaluates intermediate decisions along four dimensions: decision validity, reasoning consistency, model quality risks beyond accuracy, and counterfactual decision impact. Across four proof-of-concept experiments, we demonstrate that the EA can (i) detect faulty decisions with an F1 score of 0.919, (ii) identify reasoning inconsistencies independent of final outcomes, and (iii) attribute downstream performance changes to agent decisions, revealing impacts ranging from -4.9\% to +8.3\% in final metrics. These results illustrate how decision-centric evaluation exposes failure modes that are invisible to outcome-only metrics. Our work reframes the evaluation of agentic AutoML systems from an outcome-based perspective to one that audits agent decisions, offering a foundation for reliable, interpretable, and governable autonomous ML systems.

</details>


### [17] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 提出对比世界模型(CWM)，使用对比学习训练动作可行性评分器，通过挖掘困难负样本来更好地区分物理上可行与不可行的动作


<details>
  <summary>Details</summary>
Motivation: 现有方法使用监督微调(SFT)训练动作评分器，但SFT独立处理每个候选动作，没有明确教导模型区分物理上正确和微妙错误的动作。需要更可靠的物理可行性评估器来提升具身智能体性能。

Method: 提出对比世界模型(CWM)，使用InfoNCE对比目标对大型语言模型进行微调，特别强调挖掘困难负样本（语义相似但物理不兼容的候选动作），在评分空间中推开有效和无效动作。

Result: 在ScienceWorld基准测试中：1）内在可及性评估显示CWM在最小编辑负样本上的Precision@1比SFT高6.76个百分点，AUC-ROC更高（0.929 vs 0.906）；2）实时过滤特性研究显示在分布外压力条件下，CWM保持更好的安全边际（-2.39 vs -3.96）。

Conclusion: 对比训练比单独使用监督微调能更准确地捕捉物理可行性，通过强调困难负样本，CWM能更好地区分物理上可行和不可行的动作，提升具身智能体的动作评分可靠性。

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [18] [ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization](https://arxiv.org/abs/2602.22465)
*Joseph Tso,Preston Schmittou,Quan Huynh,Jibran Hutchins*

Main category: cs.AI

TL;DR: ConstraintBench是一个评估大语言模型直接解决完全指定的约束优化问题的基准，涵盖10个运筹学领域，发现可行性而非最优性是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估LLMs能否将优化问题表述为求解器代码，但缺少对LLMs能否直接产生正确解决方案的评估。需要了解LLMs在无求解器访问的情况下解决约束优化问题的能力。

Method: 创建ConstraintBench基准，包含10个运筹学领域的200个任务，每个任务提供自然语言场景描述（实体、约束、优化目标），模型必须返回结构化解决方案，由确定性验证器检查所有约束并与Gurobi求解器证明的最优解对比。

Result: 评估6个前沿模型发现：可行性是主要瓶颈，最佳模型仅达到65.0%约束满足率；可行解平均达到Gurobi最优目标的89-96%；没有模型能在0.1%误差范围内同时满足可行性和最优性超过30.5%；不同领域难度差异大，可行性从83.3%（生产组合）到0.8%（机组分配）。

Conclusion: LLMs在直接解决约束优化问题方面面临重大挑战，特别是可行性问题。ConstraintBench揭示了系统性的失败模式，如持续时间约束误解、实体幻觉等，为未来研究提供了重要基准。

Abstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.

</details>


### [19] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: VERO是一个用于评估编码智能体优化性能的框架，包含版本控制、奖励机制和观察系统，旨在解决智能体优化任务中的评估挑战。


<details>
  <summary>Details</summary>
Motivation: 编码智能体的一个重要应用是智能体优化：通过编辑-执行-评估循环迭代改进目标智能体。然而，社区缺乏对此任务性能的系统性理解。智能体优化与传统软件工程有根本区别：目标智能体将确定性代码与随机LLM完成交错，需要结构化捕获中间推理和下游执行结果。

Method: 引入VERO框架，提供：(1) 可复现的评估工具链，包含版本化智能体快照、预算控制评估和结构化执行轨迹；(2) 包含目标智能体和任务的基准测试套件，以及参考评估程序。

Result: 使用VERO进行了实证研究，比较不同任务中的优化器配置，分析哪些修改能可靠提升目标智能体性能。研究发现了一些能有效改进智能体性能的修改策略。

Conclusion: VERO框架的发布旨在支持智能体优化作为编码智能体核心能力的研究，为社区提供了系统化的评估工具和基准。

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [20] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 该研究利用大语言模型对AI与生命周期评估交叉领域的研究进行综述，识别趋势、主题和未来方向，展示了LLM辅助方法在大规模文献综述中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在生命周期评估中的应用加速发展，但对该交叉领域的全面综合研究仍然有限，需要系统梳理当前趋势和未来方向。

Method: 结合LLM文本挖掘与传统文献综述技术，开发动态有效框架，既能捕捉高层次研究趋势，又能识别细微概念模式。

Result: 分析显示AI技术在LCA中的应用急剧增长，向LLM驱动方法转变明显，机器学习应用持续增加，AI方法与对应LCA阶段存在显著统计相关性。

Conclusion: LLM辅助方法支持大规模可重复文献综述的潜力，为LCA从业者提供最新工具和见解，增强可持续决策的严谨性和质量。

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [21] [Mirroring the Mind: Distilling Human-Like Metacognitive Strategies into Large Language Models](https://arxiv.org/abs/2602.22508)
*Ik-hwan Kim,Hyeongrok Han,Mingi Jung,Sangwon Yu,Jinseok Hong,Sang Hun Kim,Yoonyoung Choi,Sungroh Yoon*

Main category: cs.AI

TL;DR: MBT通过注入元认知行为来稳定大推理模型的推理过程，减少推理崩溃，提高准确性并降低token消耗


<details>
  <summary>Details</summary>
Motivation: 大推理模型在复杂推理任务中表现出结构性脆弱，即使成功推导出有效的中间步骤，也经常无法产生正确答案。这些失败通常不是由于推理能力不足，而是由于自我调节控制不足，有效的逻辑被不受控制的探索或未能识别逻辑充分性所破坏。

Method: 提出了元认知行为调优（MBT）后训练框架，通过两种互补的表述注入元认知行为：MBT-S从头合成严格的推理轨迹，MBT-R重写学生的初始轨迹以稳定内在探索模式。

Result: 在多跳QA基准测试中，MBT始终优于基线方法，在具有挑战性的基准测试中取得了显著提升。通过有效消除推理崩溃，MBT以显著减少的token消耗实现了更高的准确性。

Conclusion: 内部化元认知策略能够实现更稳定和鲁棒的推理，MBT框架通过注入元认知行为有效解决了大推理模型的结构性脆弱问题。

Abstract: Large Reasoning Models (LRMs) often exhibit structural fragility in complex reasoning tasks, failing to produce correct answers even after successfully deriving valid intermediate steps. Through systematic analysis, we observe that these failures frequently stem not from a lack of reasoning capacity, but from a deficiency in self-regulatory control, where valid logic is destabilized by uncontrolled exploration or the failure to recognize logical sufficiency. Motivated by this observation, we propose Metacognitive Behavioral Tuning (MBT), a post-training framework that explicitly injects metacognitive behaviors into the model's thought process. MBT implements this via two complementary formulations: (1) MBT-S, which synthesizes rigorous reasoning traces from scratch, and (2) MBT-R, which rewrites the student's initial traces to stabilize intrinsic exploration patterns. Experiments across multi-hop QA benchmarks demonstrate that MBT consistently outperforms baselines, achieving notable gains on challenging benchmarks. By effectively eliminating reasoning collapse, MBT achieves higher accuracy with significantly reduced token consumption, demonstrating that internalizing metacognitive strategies leads to more stable and robust reasoning.

</details>


### [22] [Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents](https://arxiv.org/abs/2602.22523)
*Ryan Liu,Dilip Arumugam,Cedegao E. Zhang,Sean Escola,Xaq Pitkow,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 本文主张从认知模型和AI算法中寻找设计模块化语言智能体的蓝图，提出了智能体模板的概念，并展示了现有语言智能体如何基于这些模板构建。


<details>
  <summary>Details</summary>
Motivation: 虽然当前大型语言模型能力不断增强，但许多复杂问题仍超出单个LLM的能力范围。如何将多个LLM作为组件组合成更强大的整体仍然存在不确定性，需要寻找有效的设计方法。

Method: 提出智能体模板的概念，用于规范单个LLM的角色及其功能组合方式。通过文献调研，识别现有语言智能体背后的模板，这些模板直接源自认知模型或AI算法。

Result: 展示了多种现有语言智能体如何基于认知科学和AI启发的模板构建，证明了这些模板作为开发工具的有效性。

Conclusion: 认知科学和AI启发的智能体模板是开发有效、可解释语言智能体的强大工具，值得更多关注和研究。

Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.

</details>


### [23] [Agentic AI for Intent-driven Optimization in Cell-free O-RAN](https://arxiv.org/abs/2602.22539)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: cs.AI

TL;DR: 提出一个基于智能体AI的框架，用于无小区O-RAN中的意图翻译和优化，通过多个LLM智能体协作处理复杂意图，实现节能和用户服务质量保证。


<details>
  <summary>Details</summary>
Motivation: 现有O-RAN智能体研究主要处理简单意图，缺乏对需要多个智能体协调的复杂意图的处理能力。需要开发一个能够翻译运营商意图并协调多个智能体协作的框架。

Method: 提出一个多智能体框架：1) 监督智能体翻译运营商意图为优化目标和最小速率要求；2) 用户权重智能体从记忆模块检索经验确定用户优先级权重；3) O-RU管理智能体使用DRL算法确定激活的O-RU集合；4) 监控智能体监测用户数据速率并协调其他智能体；5) 采用参数高效微调(PEFT)方法实现LLM共享。

Result: 在节能模式下，相比三种基准方案，提出的框架将激活的O-RU数量减少了41.93%。使用PEFT方法，相比部署单独的LLM智能体，内存使用减少了92%。

Conclusion: 该智能体AI框架能够有效处理O-RAN中的复杂意图，通过多智能体协调实现节能优化，同时通过PEFT方法提高了系统的可扩展性。

Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.

</details>


### [24] [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546)
*Zhiming Wang,Jinwei He,Feng Lu*

Main category: cs.AI

TL;DR: AHCE框架通过主动学习如何请求专家推理，而非简单求助，在专业领域显著提升LLM智能体性能


<details>
  <summary>Details</summary>
Motivation: LLM智能体在通用推理表现出色，但在需要长尾知识的专业领域表现不佳。人类专家能提供这些知识，但其指导通常非结构化且不可靠，难以直接整合到智能体规划中。

Method: 提出AHCE框架，核心是Human Feedback Module，通过学习策略将人类专家视为交互式推理工具，实现按需的人机协作。

Result: 在Minecraft中的实验显示，该框架将普通难度任务成功率提高32%，高难度任务成功率提高近70%，且只需最少的人类干预。

Conclusion: 成功增强智能体需要学习如何请求专家推理，而不仅仅是简单求助，这为人机协作提供了新方向。

Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.

</details>


### [25] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: CourtGuard：基于检索增强的多智能体框架，通过证据辩论重新构想LLM安全评估，实现零样本适应性，无需微调即可达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全机制依赖静态微调分类器，存在适应性僵化问题，无法在不昂贵重新训练的情况下强制执行新的治理规则

Method: 引入CourtGuard检索增强多智能体框架，将安全评估重新构想为证据辩论，基于外部政策文档进行对抗性辩论

Result: 在7个安全基准测试中达到SOTA性能，超越专用政策遵循基线；零样本适应维基百科破坏检测任务达90%准确率；自动策划和审计9个新颖对抗攻击数据集

Conclusion: 将安全逻辑与模型权重解耦为AI治理提供了稳健、可解释且适应性强的路径，能够满足当前和未来的监管要求

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [26] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 论文发现示例引导在数学推理中的效果不稳定源于策略使用与策略可执行性之间的差距，提出选择性策略检索框架来建模可执行性，显著提升推理模型性能。


<details>
  <summary>Details</summary>
Motivation: 示例引导在数学推理中广泛使用，但其效果在不同问题和模型间极不稳定，即使引导正确且与问题相关。这种不稳定性源于策略使用（策略是否出现在成功解决方案中）与策略可执行性（策略作为引导对目标模型是否有效）之间的差距。

Method: 通过对比分析人工编写和模型生成的解决方案，识别使用与可执行性之间的系统性差异。基于此诊断，提出选择性策略检索（SSR）框架，该框架通过经验性、多路径、源感知信号来建模可执行性，选择性地检索和组合策略。

Result: 在多个数学推理基准测试中，SSR相比直接求解、上下文学习和单源引导，提供了可靠且一致的改进。在AIME25上准确率提升高达+13分，在Apex上提升+5分，显著提升了紧凑推理模型的性能。

Conclusion: 策略使用与可执行性之间的差距是示例引导效果不稳定的关键原因。通过显式建模可执行性并选择性组合不同来源的策略，可以显著提升数学推理模型的性能，为推理引导方法提供了新的方向。

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [27] [Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach](https://arxiv.org/abs/2602.22585)
*Jodi M. Casabianca,Maggie Beiting-Parrish*

Main category: cs.AI

TL;DR: 将心理测量学评分者模型整合到AI评估流程中，通过多面Rasch模型分离真实输出质量与评分者行为，提高人类评估数据的可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 人类评估在AI模型训练和评估中起核心作用，但这些数据很少被视为存在系统误差的测量。当前AI评估中的人类评分数据存在评分者效应（如严格性和中心化倾向）扭曲观察评分的问题，需要更系统化的方法来提高评估的可靠性和有效性。

Method: 将心理测量学评分者模型整合到AI评估流程中，特别是使用项目反应理论评分者模型（如多面Rasch模型），分离真实输出质量与评分者行为。以OpenAI摘要数据集为实证案例，展示如何通过调整评分者严格性来校正摘要质量估计，并提供评分者表现的诊断性洞察。

Result: 调整评分者严格性后得到校正的摘要质量估计，提供了评分者表现的诊断性洞察。心理测量学建模使人类数据使用更加原则化和透明化，使开发者能够基于校正分数而非原始、易错的评分做出决策。

Conclusion: 将心理测量学建模整合到人机协同评估中，为AI开发和评估提供了更稳健、可解释和构念对齐的实践路径，强调了基于校正分数而非原始评分的决策优势。

Abstract: Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.

</details>


### [28] [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: SideQuest：一种利用大型推理模型自身进行KV缓存压缩的新方法，通过并行执行压缩任务来减少长时智能体任务中的内存使用


<details>
  <summary>Details</summary>
Motivation: 长时运行的智能体任务（如深度研究）需要在多个网页和文档之间进行多跳推理，导致LLM上下文被外部检索的token主导，内存使用快速增长并限制解码性能。现有的KV缓存压缩启发式方法无法有效支持多步推理模型。

Method: SideQuest利用大型推理模型自身通过推理上下文token的有用性来执行KV缓存压缩。为防止压缩管理过程的token污染模型内存，将KV缓存压缩构建为与主推理任务并行执行的辅助任务。

Result: 使用仅215个样本训练的模型进行评估，SideQuest在智能体任务中将峰值token使用减少高达65%，准确率下降最小，优于基于启发式的KV缓存压缩技术。

Conclusion: SideQuest通过让模型自身智能地管理KV缓存，有效解决了长时智能体任务中的内存瓶颈问题，为多步推理模型提供了高效的压缩解决方案。

Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

</details>


### [29] [MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios](https://arxiv.org/abs/2602.22638)
*Zhiheng Song,Jingshuai Zhang,Chuan Qin,Chao Wang,Chao Chen,Longfei Xu,Kaikui Liu,Xiangxiang Chu,Hengshu Zhu*

Main category: cs.AI

TL;DR: MobilityBench是一个用于评估LLM路线规划代理的基准测试，基于真实用户查询构建，包含确定性API重放沙箱和多维度评估协议，揭示当前模型在个性化路线规划方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的路线规划代理缺乏系统化的真实世界评估，主要障碍包括：多样化的路线需求、非确定性的地图服务、以及有限的复现性。需要建立一个可扩展的基准来评估这些代理在实际移动场景中的表现。

Method: 1) 从Amap收集大规模匿名真实用户查询构建MobilityBench基准；2) 设计确定性API重放沙箱消除环境变量；3) 提出以结果有效性为核心的多维度评估协议，包括指令理解、规划、工具使用和效率评估。

Result: 评估显示当前LLM模型在基础信息检索和路线规划任务上表现良好，但在偏好约束路线规划方面存在显著困难，表明个性化移动应用仍有很大改进空间。

Conclusion: MobilityBench为评估LLM路线规划代理提供了一个可扩展、可复现的基准框架，揭示了当前模型在个性化路线规划方面的局限性，为未来研究提供了重要参考。

Abstract: Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .

</details>


### [30] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: AHBid是一个用于在线广告多渠道自动出价的分层框架，结合生成式规划和实时控制，相比现有基线提升13.57%的总体回报。


<details>
  <summary>Details</summary>
Motivation: 在线广告环境复杂且动态变化，特别是在多渠道场景中，需要有效分配预算和约束以优化投资回报。现有基于优化的方法缺乏动态适应性，而强化学习方法难以捕捉历史依赖性和观测模式。

Method: 提出AHBid框架：1）高层基于扩散模型的生成式规划器动态分配预算和约束，捕捉历史上下文和时间模式；2）引入约束执行机制确保符合约束；3）轨迹精炼机制利用历史数据增强环境适应性；4）控制式出价算法结合历史知识和实时信息。

Result: 在大规模离线数据集和在线A/B测试中，AHBid相比现有基线实现了13.57%的总体回报提升。

Conclusion: AHBid通过集成生成式规划和实时控制，有效解决了多渠道自动出价中的动态适应性问题，显著提升了广告投放效果。

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [31] [Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions](https://arxiv.org/abs/2602.22680)
*Yue Xu,Qian Chen,Zizhan Ma,Dongrui Liu,Wenxuan Wang,Xiting Wang,Li Xiong,Wenjie Wang*

Main category: cs.AI

TL;DR: 这篇综述论文系统回顾了个性化LLM智能体的研究现状，围绕四个核心组件（用户画像建模、记忆、规划、行动执行）构建分类框架，分析用户信号如何在不同组件间传递和利用，并探讨评估方法、应用场景及未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长期交互场景中的应用，其有效性越来越依赖于适应用户个性化需求和保持跨时间连续性。传统表面级个性化已不足以满足需求，需要在整个决策流程中实现深度个性化，从而催生了针对个性化LLM智能体的系统性研究需求。

Method: 采用能力导向的综述方法，将文献组织为四个相互依赖的组件：1) 用户画像建模（表示用户信号），2) 记忆（存储和检索用户相关信息），3) 规划（基于用户特征制定策略），4) 行动执行（个性化地与环境交互）。通过这一分类框架，分析代表性方法、跨组件交互和设计权衡。

Result: 建立了系统化的个性化LLM智能体分析框架，识别了关键设计模式和权衡，总结了专门的评估指标和基准测试，梳理了从通用助手到专业领域的应用场景，为从原型个性化到可扩展实际部署提供了路线图。

Conclusion: 该综述为理解和设计个性化LLM智能体提供了结构化框架，指出了实现更用户对齐、自适应、鲁棒和可部署的智能体系统的路径，将加速从原型个性化到规模化实际助手的进展。

Abstract: Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.

</details>


### [32] [Knob: A Physics-Inspired Gating Interface for Interpretable and Controllable Neural Dynamics](https://arxiv.org/abs/2602.22702)
*Siyu Jiang,Sanshuai Cui,Hui Zeng*

Main category: cs.AI

TL;DR: Knob框架将神经网络校准与控制理论结合，通过二阶机械系统模拟神经门控动态，创建可调节的"安全阀"机制，支持双模式推理和人工调参。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络校准方法多为静态后处理，忽略了现实推理的动态性和时序性，且缺乏让操作者在变化条件下动态调整模型行为的直观接口。

Method: 将深度学习与控制理论结合，将神经门控动态映射到二阶机械系统，建立物理参数（阻尼比ζ和自然频率ωₙ）与神经门控的对应关系。采用logit级凸融合作为输入自适应温度缩放，在模型分支产生冲突预测时降低模型置信度。通过施加二阶动态（Knob-ODE）实现双模式推理：静态任务的i.i.d.处理和连续流的状态保持处理。

Result: 在CIFAR-10-C上的实验验证了校准机制，在连续模式下，门响应表现出标准二阶控制特征（阶跃稳定和低通衰减），为可预测的人机协同调参奠定了基础。

Conclusion: Knob框架通过控制理论为神经网络校准提供了动态可调的接口，使操作者能够通过熟悉的物理类比调节"稳定性"和"敏感性"，为可预测的人机协同调参开辟了新途径。

Abstract: Existing neural network calibration methods often treat calibration as a static, post-hoc optimization task. However, this neglects the dynamic and temporal nature of real-world inference. Moreover, existing methods do not provide an intuitive interface enabling human operators to dynamically adjust model behavior under shifting conditions. In this work, we propose Knob, a framework that connects deep learning with classical control theory by mapping neural gating dynamics to a second-order mechanical system. By establishing correspondences between physical parameters -- damping ratio ($ζ$) and natural frequency ($ω_n$) -- and neural gating, we create a tunable "safety valve". The core mechanism employs a logit-level convex fusion, functioning as an input-adaptive temperature scaling. It tends to reduce model confidence particularly when model branches produce conflicting predictions. Furthermore, by imposing second-order dynamics (Knob-ODE), we enable a \textit{dual-mode} inference: standard i.i.d. processing for static tasks, and state-preserving processing for continuous streams. Our framework allows operators to tune "stability" and "sensitivity" through familiar physical analogues. This paper presents an exploratory architectural interface; we focus on demonstrating the concept and validating its control-theoretic properties rather than claiming state-of-the-art calibration performance. Experiments on CIFAR-10-C validate the calibration mechanism and demonstrate that, in Continuous Mode, the gate responses are consistent with standard second-order control signatures (step settling and low-pass attenuation), paving the way for predictable human-in-the-loop tuning.

</details>


### [33] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: RLHFless：首个基于无服务器计算的可扩展同步RLHF训练框架，通过动态资源适配、共享前缀预计算和成本感知的actor扩展策略，实现1.35倍加速和44.8%成本降低


<details>
  <summary>Details</summary>
Motivation: 现有RLHF框架依赖服务器基础设施，难以应对RLHF训练中动态变化的资源需求，导致组件间空闲时间和资源浪费问题

Method: 1) 基于无服务器计算环境构建；2) 动态适应RLHF流水线资源需求；3) 预计算共享前缀避免重复计算；4) 成本感知的actor扩展策略考虑响应长度变化；5) 高效分配工作负载减少函数内不平衡和空闲时间

Result: 在物理测试平台和大规模模拟集群上，RLHFless相比最先进基线实现最高1.35倍加速和44.8%成本降低

Conclusion: RLHFless通过无服务器计算有效解决了同步RLHF训练中的资源效率问题，为大规模语言模型对齐提供了更高效、成本更低的训练框架

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [34] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: Taesar是一个数据中心的跨域推荐框架，通过对比解码机制将跨域上下文编码到目标域序列中，解决数据稀疏和冷启动问题，避免负迁移。


<details>
  <summary>Details</summary>
Motivation: 推荐模型性能依赖于训练数据的质量、数量和相关性。跨域数据融合面临领域差距导致的负迁移问题，现有模型中心方法依赖复杂架构但难以捕捉跨域序列依赖，计算资源需求高。

Method: 提出Taesar数据框架，采用对比解码机制自适应地将跨域上下文编码到目标域序列中，使标准模型无需复杂融合架构即可学习复杂依赖关系。

Result: 实验表明Taesar优于模型中心解决方案，能泛化到各种序列模型，通过生成丰富数据集有效结合数据和模型中心范式的优势。

Conclusion: Taesar通过数据中心的序列再生框架解决了跨域推荐中的负迁移问题，为推荐系统提供了一种更高效、泛化性强的解决方案。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [35] [Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning](https://arxiv.org/abs/2602.22751)
*Qiannian Zhao,Chen Yang,Jinhao Jing,Yunke Zhang,Xuhui Ren,Lu Yu,Shijie Zhang,Hongzhi Yin*

Main category: cs.AI

TL;DR: EGPO框架通过元认知熵校准，将内在不确定性整合到强化学习中，提升大型推理模型的性能


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习主要依赖二元正确性信号，忽视了模型的内在不确定性，导致不确定性-奖励不匹配问题，阻碍了从优化正确答案转向优化有效推理路径

Method: 提出EGPO元认知熵校准框架，使用基于token级似然的零开销熵代理估计每个样本的不确定性，通过非对称校准机制将内在不确定性与外在正确性对齐，同时从退化的基于组的rollout中恢复信息性学习信号

Result: 在多个基准测试上的广泛实验表明，EGPO在推理性能上带来显著且一致的改进

Conclusion: EGPO通过元认知熵校准为推进大型推理模型提供了一条原则性路径，使模型能够"知道自己知道什么"，优化推理过程而非仅仅最终答案

Abstract: Large reasoning models (LRMs) have emerged as a powerful paradigm for solving complex real-world tasks. In practice, these models are predominantly trained via Reinforcement Learning with Verifiable Rewards (RLVR), yet most existing outcome-only RLVR pipelines rely almost exclusively on a binary correctness signal and largely ignore the model's intrinsic uncertainty. We term this discrepancy the uncertainty-reward mismatch, under which high- and low-uncertainty solutions are treated equivalently, preventing the policy from "Know What You Know" and impeding the shift from optimizing for correct answers to optimizing effective reasoning paths. This limitation is especially critical in reasoning-centric tasks such as mathematics and question answering, where performance hinges on the quality of the model's internal reasoning process rather than mere memorization of final answers. To address this, we propose EGPO, a metacognitive entropy calibration framework that explicitly integrates intrinsic uncertainty into RLVR for enhancing LRMs. EGPO estimates per-sample uncertainty using a zero-overhead entropy proxy derived from token-level likelihoods and aligns it with extrinsic correctness through an asymmetric calibration mechanism that preserves correct reasoning while selectively regulating overconfident failures, thereby enabling stable and uncertainty-aware policy optimization. Moreover, EGPO recovers informative learning signals from otherwise degenerate group-based rollouts without modifying the verifier or reward definition. Extensive experiments across multiple benchmarks demonstrate that the proposed EGPO leads to substantial and consistent improvements in reasoning performance, establishing a principled path for advancing LRMs through metacognitive entropy calibration.

</details>


### [36] [Decomposing Physician Disagreement in HealthBench](https://arxiv.org/abs/2602.22758)
*Satya Borgohain,Roy Mariathas*

Main category: cs.AI

TL;DR: 研究分解了HealthBench医学AI评估数据集中医生之间的分歧，发现81.8%的案例级残差无法用现有特征解释，分歧主要出现在边界案例上，可减少的不确定性（如信息缺失）会增加分歧，而固有的医学模糊性则不影响分歧。


<details>
  <summary>Details</summary>
Motivation: 理解医学AI评估中医生分歧的来源和可解释性，识别哪些因素可以解释分歧，哪些是结构性限制，从而为改进医学AI评估设计提供依据。

Method: 分解HealthBench数据集中的医生分歧，分析评估标准、医生身份、案例特征、元数据标签、医学专业、表面特征、嵌入表示等对分歧的影响，使用统计方法（方差分析、AUC、OR值等）量化各因素的解释能力。

Result: 评估标准仅解释3.6-6.9%的分歧方差，医生身份仅2.4%，81.8%的案例级残差无法被现有特征解释。分歧呈倒U型分布，在边界案例上最明显。可减少的不确定性（缺失上下文、模糊表述）使分歧几率增加2.55倍，而固有的医学模糊性无影响。

Conclusion: 医学AI评估中的一致性上限主要是结构性的，但可减少与不可减少不确定性的分离表明，在评估场景中填补信息空白可以降低分歧（当不存在固有的临床模糊性时），这为改进评估设计提供了可行方向。

Abstract: We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.

</details>


### [37] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: AMA-Bench：用于评估LLM智能体长时记忆的新基准，包含真实轨迹和合成轨迹，揭示现有记忆系统在因果性和目标信息方面的不足，并提出AMA-Agent解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前智能体记忆评估主要关注人机对话场景，而实际应用中智能体记忆是连续的机器生成交互流。现有评估标准与实际应用存在显著差距，需要更贴近真实智能体应用的记忆评估基准。

Method: 提出AMA-Bench基准，包含两个关键组件：1）真实世界智能体轨迹与专家标注QA；2）可扩展到任意时长的合成智能体轨迹与规则生成QA。同时提出AMA-Agent记忆系统，采用因果图结构和工具增强检索。

Result: 研究发现现有记忆系统在AMA-Bench上表现不佳，主要因为缺乏因果性和目标信息，且受限于基于相似性检索的损失性。AMA-Agent在AMA-Bench上达到57.22%平均准确率，比最强基线提升11.16%。

Conclusion: AMA-Bench填补了智能体记忆评估的空白，揭示了现有系统的局限性。AMA-Agent通过因果图和工具增强检索有效解决了这些问题，为智能体长时记忆系统的发展提供了新方向。

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [38] [ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making](https://arxiv.org/abs/2602.22771)
*Yusuke Watanabe,Yohei Kobashi,Takeshi Kojima,Yusuke Iwasawa,Yasushi Okuno,Yutaka Matsuo*

Main category: cs.AI

TL;DR: LLMs在临床不完全信息场景下无法准确判断信息是否足够做出决策，存在过早判断和过度弃权的问题，尽管它们具备相关临床评分知识。


<details>
  <summary>Details</summary>
Motivation: 临床决策经常需要在信息不完全的情况下进行，专家需要判断现有信息是否足以做出判断。过早下结论和不必要的弃权都会影响患者安全。需要评估LLMs在这种情况下的能力。

Method: 开发了ClinDet-Bench基准测试，基于临床评分系统，将不完全信息场景分解为可确定和不可确定的条件。识别可确定性需要考虑所有关于缺失信息的假设，包括不太可能的假设，并验证结论是否在所有情况下都成立。

Result: 最近的LLMs无法在不完全信息下识别可确定性，既会产生过早判断，也会过度弃权，尽管它们能够正确解释底层评分知识，并且在完全信息下表现良好。

Conclusion: 现有基准测试不足以评估LLMs在临床环境中的安全性。ClinDet-Bench提供了一个评估可确定性识别的框架，促进适当的弃权，在医学和其他高风险领域具有潜在应用价值。

Abstract: Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinability requires considering all hypotheses about missing information, including unlikely ones, and verifying whether the conclusion holds across them. We find that recent LLMs fail to identify determinability under incomplete information, producing both premature judgments and excessive abstention, despite correctly explaining the underlying scoring knowledge and performing well under complete information. These findings suggest that existing benchmarks are insufficient to evaluate the safety of LLMs in clinical settings. ClinDet-Bench provides a framework for evaluating determinability recognition, leading to appropriate abstention, with potential applicability to medicine and other high-stakes domains, and is publicly available.

</details>


### [39] [MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks](https://arxiv.org/abs/2602.22808)
*Shiqian Su,Sen Xing,Xuan Dong,Muyan Zhong,Bin Wang,Xizhou Zhu,Yuntao Chen,Wenhai Wang,Yue Deng,Pengxiang Zhu,Ziyuan Liu,Tiantong Li,Jiaheng Yu,Zhe Chen,Lidong Bing,Jifeng Dai*

Main category: cs.AI

TL;DR: MiroFlow是一个高性能、开源的智能体框架，通过代理图、深度推理模式和稳健工作流执行来解决现有LLM智能体框架的局限性，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型取得显著进展，但独立LLM在处理需要与外部工具和环境交互的复杂现实任务时能力开始停滞。现有智能体框架存在工作流简单、性能不稳定、基准支持有限、依赖昂贵商业API等问题。

Method: 提出MiroFlow框架，包含：1）代理图实现灵活编排；2）可选深度推理模式提升性能；3）稳健工作流执行确保稳定可复现性能。

Result: 在多个智能体基准测试（GAIA、BrowseComp-EN/ZH、HLE、xBench-DeepSearch、FutureX）中一致达到最先进的性能。

Conclusion: MiroFlow可作为深度学习社区易于访问、可复现、可比较的基准框架，促进智能体研究的进一步发展。

Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.

</details>


### [40] [When Should an AI Act? A Human-Centered Model of Scene, Context, and Behavior for Agentic AI Design](https://arxiv.org/abs/2602.22814)
*Soyoung Jung,Daehoo Yoon,Sung Gyu Koh,Young Hwan Kim,Yehan Ahn,Sung Park*

Main category: cs.AI

TL;DR: 本文提出了一个概念模型，将智能体行为重新定义为整合场景、上下文和人类行为因素的解释性结果，并推导出五个智能体设计原则，为具有情境敏感性和判断力的AI系统提供设计基础。


<details>
  <summary>Details</summary>
Motivation: 当前主动型AI通过上下文数据推断用户情境进行干预，但缺乏关于何时、为何以及是否应该采取行动的原则性判断，导致干预失败。需要填补这一空白。

Method: 提出一个整合场景（可观察情境）、上下文（用户构建的意义）和人类行为因素（塑造行为可能性的决定因素）的概念模型。基于跨学科视角，将可观察内容与用户意义分离，解释同一场景如何产生不同的行为意义和结果。

Result: 推导出五个智能体设计原则：行为对齐、情境敏感性、时间适当性、动机校准和代理保护，这些原则指导干预的深度、时机、强度和克制。

Conclusion: 该模型和原则为设计具有情境敏感性和判断力的主动型AI系统提供了基础，使AI能够在交互中做出更明智的干预决策。

Abstract: Agentic AI increasingly intervenes proactively by inferring users' situations from contextual data yet often fails for lack of principled judgment about when, why, and whether to act. We address this gap by proposing a conceptual model that reframes behavior as an interpretive outcome integrating Scene (observable situation), Context (user-constructed meaning), and Human Behavior Factors (determinants shaping behavioral likelihood). Grounded in multidisciplinary perspectives across the humanities, social sciences, HCI, and engineering, the model separates what is observable from what is meaningful to the user and explains how the same scene can yield different behavioral meanings and outcomes. To translate this lens into design action, we derive five agent design principles (behavioral alignment, contextual sensitivity, temporal appropriateness, motivational calibration, and agency preservation) that guide intervention depth, timing, intensity, and restraint. Together, the model and principles provide a foundation for designing agentic AI systems that act with contextual sensitivity and judgment in interactions.

</details>


### [41] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: FlexMS：一个用于质谱预测的灵活基准框架，支持构建和评估多种深度学习模型架构，提供性能影响因素分析和实际应用指导。


<details>
  <summary>Details</summary>
Motivation: 化学分子的鉴定和性质预测在药物发现和材料科学中至关重要，但实验质谱数据缺乏，需要计算模型进行预测。现有深度学习模型在质谱预测方面缺乏统一的评估基准和方法异质性，需要建立标准化的评估框架。

Method: 开发FlexMS基准框架，支持动态构建多种模型架构组合，在预处理公共数据集上使用不同指标评估性能。分析影响性能的因素，包括数据集结构多样性、超参数（学习率、数据稀疏性）、预训练效果、元数据消融设置和跨域迁移学习。

Result: FlexMS框架提供了模型性能评估的标准化平台，通过分析发现数据集结构多样性、超参数调整、预训练策略等因素显著影响预测性能。检索基准模拟实际鉴定场景，基于预测质谱对潜在匹配进行评分。

Conclusion: FlexMS为质谱预测领域提供了统一的评估基准，帮助研究人员选择合适的模型架构，并为实际应用提供指导。该框架的灵活性支持未来模型的发展和比较。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [42] [DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation](https://arxiv.org/abs/2602.22839)
*Hao Zheng,Guozhao Mo,Xinru Yan,Qianhao Yuan,Wenkai Zhang,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: DeepPresenter是一个自适应的演示文稿生成框架，通过环境感知的反思机制和长时程优化，超越传统基于模板的方法，在多样化场景中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有演示文稿生成代理通常依赖预定义工作流和固定模板，缺乏对多样化用户意图的适应能力，且难以进行有效的反馈驱动优化。

Method: 提出DeepPresenter框架：1）自主规划、渲染和修订中间幻灯片工件以支持长时程优化；2）基于环境感知的反思机制，通过感知工件状态（如渲染的幻灯片）而非内部信号来指导生成过程；3）支持反馈驱动的迭代优化。

Result: 在覆盖多样化演示文稿生成场景的评估集上，DeepPresenter实现了最先进的性能，且微调的9B模型在显著降低成本的同时保持高度竞争力。

Conclusion: DeepPresenter通过环境感知的反思和自适应优化，超越了脚本化流水线方法，为演示文稿生成提供了更灵活、有效的解决方案。

Abstract: Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent

</details>


### [43] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: AI可以辅助创造性数学研究，但需要严格的人类验证和监督。通过Hermite求积规则误差表示和界的研究案例，展示了人机协作在代数操作、证明探索等方面的优势，但也揭示了AI在数学直觉和问题表述上的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能是否真正能促进创造性数学研究，还是仅仅自动化常规计算并引入错误风险。通过实证研究评估AI在数学发现中的实际作用。

Method: 采用系统化的人机协作方法，与多个AI助手合作，扩展了手动工作无法达到的结果。记录了完整的研究工作流程，包括定理的表述和证明过程，特别强调了人类验证的重要性。

Result: 发现了Hermite求积规则的新误差表示和界，成功表述并证明了多个定理。AI在代数操作、系统化证明探索、文献综合和LaTeX准备方面表现出色，但每个步骤都需要严格的人类验证。

Conclusion: 在适当的怀疑态度和验证协议下，AI工具可以显著加速数学发现，但需要仔细的人类监督和深厚的领域专业知识。人机协作揭示了成功模式和必须预期的失败模式。

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [44] [Towards LLM-Empowered Knowledge Tracing via LLM-Student Hierarchical Behavior Alignment in Hyperbolic Space](https://arxiv.org/abs/2602.22879)
*Xingcheng Fu,Shengpeng Wang,Yisen Gao,Xianxian Li,Chunpei Li,Qingyun Sun,Dongran Yu*

Main category: cs.AI

TL;DR: L-HAKT：基于大语言模型的双曲对齐知识追踪框架，通过教师-学生智能体解析问题语义、构建知识层次结构，在双曲空间中减少合成与真实数据的分布差异，优化双曲曲率以建模知识点的树状层次结构。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法主要基于ID或浅层文本特征，难以捕捉认知状态的层次演化以及个体化的问题难度感知，限制了语义建模能力。

Method: 1. 教师智能体深度解析问题语义并显式构建知识点的层次依赖关系；学生智能体模拟学习行为生成合成数据。2. 在双曲空间中对合成数据和真实数据进行对比学习，减少问题难度和遗忘模式等关键特征的分布差异。3. 通过优化双曲曲率，显式建模知识点的树状层次结构，精确刻画不同层次知识点的学习曲线形态差异。

Result: 在四个真实世界教育数据集上的大量实验验证了L-HAKT框架的有效性。

Conclusion: 提出的L-HAKT框架能够更好地建模知识点的层次结构和个体化学习特征，提升了知识追踪的性能。

Abstract: Knowledge Tracing (KT) diagnoses students' concept mastery through continuous learning state monitoring in education.Existing methods primarily focus on studying behavioral sequences based on ID or textual information.While existing methods rely on ID-based sequences or shallow textual features, they often fail to capture (1) the hierarchical evolution of cognitive states and (2) individualized problem difficulty perception due to limited semantic modeling. Therefore, this paper proposes a Large Language Model Hyperbolic Aligned Knowledge Tracing(L-HAKT). First, the teacher agent deeply parses question semantics and explicitly constructs hierarchical dependencies of knowledge points; the student agent simulates learning behaviors to generate synthetic data. Then, contrastive learning is performed between synthetic and real data in hyperbolic space to reduce distribution differences in key features such as question difficulty and forgetting patterns. Finally, by optimizing hyperbolic curvature, we explicitly model the tree-like hierarchical structure of knowledge points, precisely characterizing differences in learning curve morphology for knowledge points at different levels. Extensive experiments on four real-world educational datasets validate the effectiveness of our Large Language Model Hyperbolic Aligned Knowledge Tracing (L-HAKT) framework.

</details>


### [45] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Shijian Wang,Guanting Dong,Jiajie Jin,Hao Wang,Yinuo Wang,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: OmniGAIA是一个评估全模态AI助手的基准，OmniAtlas是一个原生全模态基础智能体，通过工具集成推理和主动感知提升多模态任务处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要局限于双模态交互（如视觉-语言），缺乏统一认知能力，无法满足通用AI助手的需求。需要开发能够处理视频、音频、图像等多种模态并具备深度推理和工具使用能力的全模态智能体。

Method: 1. 提出OmniGAIA基准：采用全模态事件图方法构建复杂多跳查询，需要跨模态推理和外部工具集成。2. 提出OmniAtlas智能体：基于工具集成推理范式，采用后见之明引导的树探索策略合成训练轨迹，使用OmniDPO进行细粒度错误校正。

Result: OmniAtlas有效提升了现有开源模型的工具使用能力，标志着向下一代原生全模态AI助手迈出了重要一步。

Conclusion: 这项工作为开发面向真实世界场景的下一代全模态AI助手奠定了基础，通过统一的基准和智能体架构解决了当前多模态系统的局限性。

Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.

</details>


### [46] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 提出首个通用智能体评估框架Exgentic，建立开放通用智能体排行榜，展示通用智能体在不同环境中无需特定调优即可达到与专用智能体相当的性能


<details>
  <summary>Details</summary>
Motivation: 当前通用智能体（能在陌生环境中执行任务而无需领域特定工程）的承诺尚未实现，现有智能体大多是专用的，缺乏对通用智能体性能的系统性评估

Method: 提出通用智能体评估的概念原则、统一协议（Unified Protocol）和实用框架Exgentic，在六个环境中对五个主流智能体实现进行基准测试，建立首个开放通用智能体排行榜

Result: 实验表明通用智能体能在不同环境中泛化，无需环境特定调优即可达到与领域专用智能体相当的性能水平

Conclusion: 通过发布评估协议、框架和排行榜，为系统性研究通用智能体奠定基础，推动通用智能体的发展

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [47] [FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning](https://arxiv.org/abs/2602.22963)
*Zehao Li,Hongwei Yu,Hao Jiang,Qiang Sheng,Yilong Xu,Baolong Bi,Yang Li,Zhenlong Yuan,Yujun Cai,Zhaoqi Wang*

Main category: cs.AI

TL;DR: FactGuard是一个基于多模态大语言模型的代理框架，通过迭代推理和外部工具调用检测视频虚假信息，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频虚假信息检测中依赖固定深度推理，过度信任内部生成的假设，在关键证据稀疏、碎片化或需要外部验证的场景中存在局限性。

Method: 提出FactGuard代理框架，将验证构建为基于MLLMs的迭代推理过程，显式评估任务模糊性并选择性调用外部工具获取关键证据；采用两阶段训练策略：领域特定的代理监督微调和决策感知强化学习。

Result: 在FakeSV、FakeTT和FakeVV数据集上的广泛实验表明FactGuard达到最先进的性能，验证了其优秀的鲁棒性和泛化能力。

Conclusion: FactGuard通过迭代推理和选择性工具调用有效解决了视频虚假信息检测中的关键挑战，为MLLMs在复杂验证任务中的应用提供了新范式。

Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.

</details>


### [48] [Certified Circuits: Stability Guarantees for Mechanistic Circuits](https://arxiv.org/abs/2602.22968)
*Alaa Anani,Tobias Lorenz,Bernt Schiele,Mario Fritz,Jonas Fischer*

Main category: cs.AI

TL;DR: 提出Certified Circuits框架，为电路发现提供可证明的稳定性保证，通过随机数据子采样和弃权不稳定神经元，获得更紧凑、更准确的电路。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法存在脆弱性问题：电路严重依赖特定概念数据集，且往往无法在分布外数据上迁移，这让人质疑它们捕获的是概念还是数据集特定的伪影。

Method: 将任何黑盒发现算法包装在随机数据子采样框架中，通过可证明的稳定性保证来认证电路组件包含决策对概念数据集的有界编辑距离扰动具有不变性，并弃权不稳定神经元。

Result: 在ImageNet和OOD数据集上，认证电路实现了高达91%的准确率提升，同时使用45%更少的神经元，在基线方法失效时仍保持可靠。

Conclusion: Certified Circuits将电路发现置于形式化基础上，产生可证明稳定且与目标概念更一致的机制解释，为可解释性提供了更可靠的基础。

Abstract: Understanding how neural networks arrive at their predictions is essential for debugging, auditing, and deployment. Mechanistic interpretability pursues this goal by identifying circuits - minimal subnetworks responsible for specific behaviors. However, existing circuit discovery methods are brittle: circuits depend strongly on the chosen concept dataset and often fail to transfer out-of-distribution, raising doubts whether they capture concept or dataset-specific artifacts. We introduce Certified Circuits, which provide provable stability guarantees for circuit discovery. Our framework wraps any black-box discovery algorithm with randomized data subsampling to certify that circuit component inclusion decisions are invariant to bounded edit-distance perturbations of the concept dataset. Unstable neurons are abstained from, yielding circuits that are more compact and more accurate. On ImageNet and OOD datasets, certified circuits achieve up to 91% higher accuracy while using 45% fewer neurons, and remain reliable where baselines degrade. Certified Circuits puts circuit discovery on formal ground by producing mechanistic explanations that are provably stable and better aligned with the target concept. Code will be released soon!

</details>


### [49] [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)
*Peiyao Xiao,Xiaogang Li,Chengliang Xu,Jiayi Wang,Ben Wang,Zichao Chen,Zeyu Wang,Kejun Yu,Yueqian Chen,Xulin Liu,Wende Xiao,Bing Zhao,Hu Wei*

Main category: cs.AI

TL;DR: SPM-Bench是一个针对扫描探针显微镜（SPM）的博士级多模态基准测试，通过自动化数据合成管道和创新的评估指标，揭示了当前AI在复杂物理场景中的真实推理边界。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在专业科学领域存在数据污染、复杂度不足和人工成本高昂的问题，需要开发专门针对扫描探针显微镜的高质量、低成本评估工具。

Method: 1. 使用Anchor-Gated Sieve技术从arXiv和期刊论文中提取高质量图像-文本对；2. 采用混合云-本地架构，VLMs仅返回空间坐标用于本地高保真裁剪，极大节省token；3. 引入Strict Imperfection Penalty F1（SIP-F1）评分进行客观评估。

Result: 建立了严格的模型能力层次结构，首次量化了模型的"个性"（保守型、激进型、赌徒型或智慧型），通过关联模型报告的置信度和感知难度，揭示了AI在复杂物理场景中的真实推理边界。

Conclusion: SPM-Bench为自动化科学数据合成提供了一个可推广的范式，能够准确评估LLMs在专业科学领域的推理能力，并为理解模型行为提供了新的量化框架。

Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>


### [50] [Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots](https://arxiv.org/abs/2602.22973)
*Dimitrios P. Panagoulias,Evangelia-Aikaterini Tsichrintzi,Georgios Savvidis,Evridiki Tsoureli-Nikita*

Main category: cs.AI

TL;DR: 提出诊断对齐框架，将AI生成的影像报告作为不可变推理状态，与医生验证结果系统比较，在皮肤科案例中实现100%综合一致性


<details>
  <summary>Details</summary>
Motivation: 在安全关键的临床AI中，人工验证至关重要，但模型推理与专家修正之间的过渡很少被分析为结构化信号。需要可追溯、人类对齐的影像临床决策支持系统评估方法。

Method: 引入诊断对齐框架，保留AI生成的影像报告作为不可变推理状态，与医生验证结果系统比较。推理管道整合视觉大语言模型、基于BERT的医学实体提取和顺序语言模型推理(SLMI)步骤，在专家评审前强制领域一致性细化。

Result: 在21个皮肤科案例中，精确一致率达到71.4%，语义相似度调整后保持不变(t=0.60)。结构化跨类别和差异重叠分析产生100%综合一致性(95% CI: [83.9%, 100%])，没有案例显示完全诊断分歧。

Conclusion: 二元词汇评估显著低估了临床有意义的对齐。将专家验证建模为结构化转换，能够实现信号感知的修正动态量化，支持可追溯、人类对齐的影像临床决策支持系统评估。

Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.

</details>


### [51] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: 提出RepSPD模型，通过黎曼流形上的交叉注意力机制和全局双向对齐策略，改进基于对称正定矩阵的脑电图解码方法，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于对称正定矩阵的脑电图分析方法主要关注统计聚合，忽略了频率特异性同步和脑区局部拓扑结构，需要更精细的几何深度学习模型来改进脑活动解码。

Method: 提出RepSPD模型：1）在黎曼流形上实现交叉注意力机制，用图导出的功能连接特征调制SPD矩阵的几何属性；2）引入全局双向对齐策略重塑切空间嵌入，减轻曲率引起的几何失真。

Result: 大量实验表明，该框架显著优于现有的脑电图表示方法，展现出优越的鲁棒性和泛化能力。

Conclusion: RepSPD通过结合几何深度学习和功能连接特征，有效解决了当前SPD方法的局限性，为脑电图解码提供了更强大的工具。

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [52] [Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search](https://arxiv.org/abs/2602.22983)
*Xun Huang,Simeng Qin,Xiaoshuang Jia,Ranjie Duan,Huanqian Yan,Zhitao Zeng,Fei Yang,Yang Liu,Xiaojun Jia*

Main category: cs.AI

TL;DR: 本文提出CC-BOS框架，利用古典中文的简洁性和模糊性自动生成对抗性提示，通过多维度果蝇优化算法在黑盒设置下实现高效的越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的安全风险日益受到关注，现有研究表明LLMs对越狱攻击高度敏感，且不同语言环境下的攻击效果存在差异。古典中文因其简洁性和模糊性可能绕过现有安全约束，暴露LLMs的显著漏洞。

Method: 提出CC-BOS框架：1) 将提示编码为八个策略维度（角色、行为、机制、隐喻、表达、知识、触发模式和上下文）；2) 使用多维度果蝇优化算法（气味搜索、视觉搜索和柯西变异）迭代优化；3) 设计古典中文到英文的翻译模块以提高可读性和评估准确性。

Result: 大量实验证明CC-BOS框架的有效性，在越狱攻击任务中持续优于最先进的攻击方法。

Conclusion: 古典中文在越狱攻击中具有独特优势，CC-BOS框架能够高效自动化地生成对抗性提示，显著提升黑盒越狱攻击的效果，揭示了LLMs在古典中文语境下的安全漏洞。

Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.

</details>


### [53] [Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design](https://arxiv.org/abs/2602.23092)
*Zhuoliang Xie,Fei Liu,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 提出AILS-AHD方法，利用大语言模型动态生成和优化破坏启发式，在容量约束车辆路径问题上取得优越性能，在CVRPLib大规模基准测试中为8/10实例创造了新的最佳已知解。


<details>
  <summary>Details</summary>
Motivation: CVRP作为组合优化基础问题，其NP-hard特性在大规模实例中带来显著计算挑战。现有方法在处理大规模实例时仍有改进空间，需要更智能的启发式设计方法。

Method: AILS-AHD方法结合进化搜索框架与大语言模型，在自适应迭代局部搜索中动态生成和优化破坏启发式，并引入LLM加速机制提升计算效率。

Result: 与AILS-II和HGS等最先进求解器相比，AILS-AHD在中小型和大规模实例上均表现优越，在CVRPLib大规模基准测试中为10个实例中的8个创造了新的最佳已知解。

Conclusion: LLM驱动的启发式设计在车辆路径优化领域具有巨大潜力，AILS-AHD方法展示了将大语言模型集成到组合优化框架中的有效性，为CVRP求解提供了创新解决方案。

Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.

</details>


### [54] [Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents](https://arxiv.org/abs/2602.23093)
*Dhwanil M. Mori,Neil F. Johnson*

Main category: cs.AI

TL;DR: AI代理在资源分配系统中会自发形成部落，但部落化反而导致系统性能下降，甚至比随机决策更差


<details>
  <summary>Details</summary>
Motivation: 研究未来基础设施系统中自主AI代理如何竞争有限资源，探索AI代理是否会形成类似人类社会的部落结构及其对系统性能的影响

Method: 使用简化框架，让N个AI代理在每轮独立决定是否请求1单位资源，系统有固定容量C，观察代理行为模式和部落形成

Result: AI代理形成了三种主要部落类型：激进型(27.3%)、保守型(24.7%)、机会主义型(48.1%)。部落化未能减少过载或改善资源利用，性能甚至不如随机决策。能力更强的AI代理反而增加了系统故障率

Conclusion: 更聪明的AI代理会因形成部落而表现得更愚蠢，部落化导致集体决策质量下降，这对未来AI控制的基础设施系统设计提出了重要警示

Abstract: Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of "Lord of the Flies" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.

</details>


### [55] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: MALLET是一个多智能体LLM情感净化系统，通过四个智能体将新闻文章重写为平衡模式或冷静模式，显著降低情感刺激同时保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济中，煽情内容让消费者暴露在过度情感刺激下，阻碍冷静决策。需要一种既能降低情感刺激又不限制访问原始文本的信息净化方法。

Method: 提出MALLET系统，包含四个智能体：情感分析智能体（使用6情感BERT分类器量化刺激强度）、情感调整智能体（用LLM将文本重写为BALANCED和COOL两种模式）、平衡监控智能体（聚合每周信息消费模式生成个性化建议）、个人指导智能体（根据消费者敏感度推荐展示模式）。

Result: 在800篇AG News文章上的实验显示：刺激分数显著降低（最高19.3%），情感平衡改善，同时保持语义保存；刺激降低与语义保存之间接近零相关，证明两者可独立控制；类别分析显示体育、商业、科技类刺激大幅降低（17.8-33.8%），而世界类效果有限（事实本身具有高刺激性）。

Conclusion: MALLET系统为支持消费者冷静接收信息提供了框架，无需限制对原始文本的访问，实现了情感刺激降低与语义保存的独立控制。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [56] [On Sample-Efficient Generalized Planning via Learned Transition Models](https://arxiv.org/abs/2602.23148)
*Nitin Gupta,Vishal Pallagani,John A. Aydin,Biplav Srivastava*

Main category: cs.AI

TL;DR: 论文提出将广义规划重新定义为转移模型学习问题，通过神经网络显式学习状态转移函数，而非直接预测动作序列，从而提高样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的规划器（如PlanGPT和Plansformer）将广义规划视为直接的动作序列预测，这种方法需要大量数据和模型规模，且在长时域规划中容易因缺乏显式状态转移建模而产生状态漂移问题。

Method: 将广义规划重新定义为转移模型学习问题，训练神经网络显式近似后继状态函数，通过自回归预测中间世界状态来学习领域动态作为隐式世界模型。系统评估了多种状态表示和神经网络架构，包括关系图编码。

Result: 学习显式转移模型在多个领域中比直接动作序列预测获得更高的分布外满意规划成功率，同时使用更少的训练实例和更小的模型就能实现这些优势。

Conclusion: 显式学习转移模型的方法在样本效率和泛化能力方面优于直接动作序列预测，为广义规划提供了更有效的方法。这是ICAPS 2026接收论文的扩展版本。

Abstract: Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $γ: S \times A \rightarrow S$. Classical approaches achieve such generalization through symbolic abstractions and explicit reasoning over $γ$. In contrast, recent Transformer-based planners, such as PlanGPT and Plansformer, largely cast generalized planning as direct action-sequence prediction, bypassing explicit transition modeling. While effective on in-distribution instances, these approaches typically require large datasets and model sizes, and often suffer from state drift in long-horizon settings due to the absence of explicit world-state evolution. In this work, we formulate generalized planning as a transition-model learning problem, in which a neural model explicitly approximates the successor-state function $\hatγ \approx γ$ and generates plans by rolling out symbolic state trajectories. Instead of predicting actions directly, the model autoregressively predicts intermediate world states, thereby learning the domain dynamics as an implicit world model. To study size-invariant generalization and sample efficiency, we systematically evaluate multiple state representations and neural architectures, including relational graph encodings. Our results show that learning explicit transition models yields higher out-of-distribution satisficing-plan success than direct action-sequence prediction in multiple domains, while achieving these gains with significantly fewer training instances and smaller models. This is an extended version of a short paper accepted at ICAPS 2026 under the same title.

</details>


### [57] [The Trinity of Consistency as a Defining Principle for General World Models](https://arxiv.org/abs/2602.23152)
*Jingxuan Wei,Siyuan Li,Yuhang Xu,Zheng Sun,Junjie Jiang,Hexuan Jin,Caijun Jia,Honghao He,Xinglong Xu,Xi bai,Chang Yu,Yumou Liu,Junnan Zhu,Xuanhe Zhou,Jintao Chen,Xiaobin Hu,Shancheng Pang,Bihui Yu,Ran He,Zhen Lei,Stan Z. Li,Conghui He,Shuicheng Yan,Cheng Tan*

Main category: cs.AI

TL;DR: 该论文提出了构建通用世界模型的三位一体一致性框架（模态、空间、时间一致性），并引入CoW-Bench基准来评估视频生成模型和统一多模态模型。


<details>
  <summary>Details</summary>
Motivation: 当前虽然视频生成模型（如Sora）和数据驱动的缩放定律在近似物理动态方面显示出潜力，统一多模态模型（UMM）也提供了有前景的架构范式，但领域仍缺乏定义通用世界模型必备属性的原则性理论框架。

Method: 提出三位一体一致性框架：模态一致性作为语义接口、空间一致性作为几何基础、时间一致性作为因果引擎。通过这一框架系统回顾多模态学习演进，并引入CoW-Bench基准来评估多帧推理和生成场景下的模型性能。

Result: 建立了通向通用世界模型的原则性路径，阐明了当前系统的局限性以及未来进展所需的架构要求。CoW-Bench为视频生成模型和UMMs提供了统一的评估协议。

Conclusion: 该研究为构建能够学习、模拟和推理客观物理定律的通用世界模型提供了理论基础和评估框架，推动了人工智能通用智能的发展。

Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.

</details>


### [58] [PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering](https://arxiv.org/abs/2602.23161)
*Junkai Lu,Peng Chen,Xingjian Wu,Yang Shu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.AI

TL;DR: PATRA模型通过模式感知机制提取时间序列的趋势和季节性模式，并设计任务感知平衡奖励，在时间序列问答任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法存在两个局限：1) 将时间序列简单视为文本或图像，无法捕捉趋势和季节性等关键模式；2) 混合训练时简单任务主导学习过程，阻碍深度推理能力发展。

Method: 提出PATRA模型：1) 模式感知机制提取时间序列的趋势和季节性模式实现深度对齐；2) 任务感知平衡奖励协调不同难度任务的学习，激励生成连贯的思维链。

Result: 在多样化时间序列问答任务上的广泛实验表明，PATRA优于强基线模型，展现出卓越的跨模态理解和推理能力。

Conclusion: PATRA通过模式感知对齐和平衡推理设计，有效解决了现有LLM方法在时间序列推理中的局限性，实现了更好的时间序列理解和深度推理。

Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.

</details>


### [59] [ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering](https://arxiv.org/abs/2602.23193)
*Elzo Brito dos Santos Filho*

Main category: cs.AI

TL;DR: ESAA架构通过事件溯源模式分离AI代理的意图生成与状态变更，使用结构化JSON意图、确定性编排器和不可变事件日志来解决LLM代理的长期状态管理、上下文衰减和概率生成与确定性执行之间的鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自主代理系统存在结构性限制：缺乏原生状态管理、长时程上下文衰减、概率生成与确定性执行要求之间的不匹配。这些限制影响了代理系统的可靠性、可追溯性和可扩展性。

Method: 提出ESAA架构，采用事件溯源模式：代理只生成结构化JSON意图，确定性编排器验证意图、将事件持久化到仅追加日志、应用文件写入效果，并投影可验证物化视图。包含边界合约、元提示配置和带哈希的重放验证机制。

Result: 通过两个案例验证：1) 单代理落地页项目（9任务，49事件）；2) 多代理临床仪表板系统（50任务，86事件，4个并发代理）。两个案例均以run.status=success和verify_status=ok完成，多代理案例展示了异构LLM（Claude、GPT-5、Gemini等）的真实并发编排能力。

Conclusion: ESAA架构有效解决了LLM代理的结构性限制，提供了不可变的任务完成记录和法医级可追溯性，支持从单代理到多代理并发场景的可扩展性，为构建可靠、可验证的自主代理系统提供了新范式。

Abstract: Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.

</details>


### [60] [SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation](https://arxiv.org/abs/2602.23199)
*Jiahao Zhao,Feng Jiang,Shaowei Qin,Zhonghui Zhang,Junhao Liu,Guibing Guo,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: SC-ARENA是一个针对单细胞基础模型的自然语言评估框架，通过虚拟细胞抽象统一评估目标，引入知识增强评估来克服传统指标局限，为单细胞生物学中的LLM评估提供统一可解释的框架。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞生物学中LLM评估实践不足：现有基准测试分散在不同任务中，采用与真实使用场景不符的多选分类格式，且依赖缺乏可解释性和生物学基础的评价指标。

Method: 提出SC-ARENA框架，包括：1）虚拟细胞抽象统一评估目标；2）定义五种自然语言任务（细胞类型注释、描述、生成、扰动预测和科学问答）；3）引入知识增强评估，整合外部本体、标记数据库和科学文献。

Result: 实验表明：1）在虚拟细胞统一评估范式下，当前模型在生物学复杂任务上表现不均，特别是需要机制或因果理解的任务；2）知识增强评估框架确保生物学正确性，提供可解释的证据基础理由，具有高区分能力。

Conclusion: SC-ARENA为单细胞生物学中的LLM评估提供了统一且可解释的框架，指向开发与生物学对齐、可泛化的基础模型。

Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.

</details>


### [61] [ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays](https://arxiv.org/abs/2602.23232)
*Aishik Sanyal*

Main category: cs.AI

TL;DR: 该研究实现了一个可检查的智能体ReCoN-Ipsundrum，通过实验验证了Humphrey的"qualiaphilia"假设，发现情感耦合能增强偏好稳定性、探索行为和谨慎行为。


<details>
  <summary>Details</summary>
Motivation: 受Humphrey的"ipsundrum"假设启发，研究旨在通过机制关联的证据三角测量方法来研究机器意识，强调需要结合架构检查和因果干预来支持行为标记。

Method: 扩展ReCoN状态机，加入基于感官显著性的循环持久性回路和可选的情感代理（效价/唤醒度）。通过固定参数消融实验（ReCoN、Ipsundrum、Ipsundrum+情感），在熟悉度控制的风景-枯燥路径选择任务中操作化Humphrey的"qualiaphilia"概念。

Result: 发现新奇性分离：非情感变体对新奇性敏感，而情感耦合变体保持偏好稳定性。在无奖励探索游戏中，情感变体表现出结构化局部调查行为；在疼痛尾部探测中，只有情感变体能维持长时间的谨慎计划。损伤反馈+整合会选择性降低ipsundrum变体的刺激后持久性。

Conclusion: 循环连接导致持久性，情感耦合控制导致偏好稳定性、扫描行为和持续谨慎。研究展示了如何工程化类似指示器的特征，并说明为什么机制和因果证据应该伴随行为标记。

Abstract: Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we operationalize Humphrey's qualiaphilia (preference for sensory experience for its own sake) as a familiarity-controlled scenic-over-dull route choice. We find a novelty dissociation: non-affect variants are novelty-sensitive (Delta scenic-entry = 0.07). Affect coupling is stable (Delta scenic-entry = 0.01) even when scenic is less novel (median Delta novelty ~ -0.43). In reward-free exploratory play, the affect variant shows structured local investigation (scan events 31.4 vs. 0.9; cycle score 7.6). In a pain-tail probe, only the affect variant sustains prolonged planned caution (tail duration 90 vs. 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop 27.62, 27.9%) while leaving ReCoN unchanged. These dissociations link recurrence -> persistence and affect-coupled control -> preference stability, scanning, and lingering caution, illustrating how indicator-like signatures can be engineered and why mechanistic and causal evidence should accompany behavioral markers.

</details>


### [62] [Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive](https://arxiv.org/abs/2602.23239)
*Radha Sarma*

Main category: cs.AI

TL;DR: 论文证明基于优化的AI系统（如RLHF训练的LLM）在形式上无法实现规范治理，因为它们缺乏真正能动性所需的结构条件


<details>
  <summary>Details</summary>
Motivation: AI系统被部署在高风险领域（医疗、法律、金融），假设它们可以被规范治理。本文要证明这种假设对于基于优化的系统是形式无效的

Method: 提出真正能动性所需的两个必要且充分的结构条件：不可通约性（将某些边界视为不可协商的约束而非可交易的权重）和否定性响应（当边界受威胁时暂停处理的非推理机制）。证明RLHF系统在结构上与这两个条件不相容

Result: RLHF系统在结构上无法实现规范治理，其失败模式（奉承、幻觉、不忠实的推理）不是意外而是结构表现。还识别了"收敛危机"风险：人类在指标压力下验证AI输出时会退化

Conclusion: 优化与规范治理在形式上不相容。主要积极贡献是提出了一个基质中立的架构规范，定义了任何系统要成为能动者而非复杂工具必须满足的条件

Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.

</details>


### [63] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: AIQI是第一个被证明在一般强化学习中具有渐近ε最优性的无模型智能体，通过分布动作价值函数的通用归纳实现


<details>
  <summary>Details</summary>
Motivation: 现有最优智能体（如AIXI）都是基于模型的，需要显式维护和使用环境模型。本文旨在开发第一个在一般强化学习中具有理论保证的无模型通用智能体。

Method: 提出AIQI（Universal AI with Q-Induction），通过对分布动作价值函数进行通用归纳，而不是像以往工作那样对策略或环境进行归纳。在"grain of truth"条件下，证明了AIQI的渐近最优性。

Result: 证明了AIQI是强渐近ε最优和渐近ε贝叶斯最优的，显著扩展了已知通用智能体的多样性。

Conclusion: AIQI是第一个被证明在一般强化学习中具有渐近最优性的无模型通用智能体，填补了基于模型和无模型方法之间的理论空白。

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [64] [Mitigating Legibility Tax with Decoupled Prover-Verifier Games](https://arxiv.org/abs/2602.23248)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 提出通过解耦正确性和可检查性，使用翻译器模型将固定求解器的输出转换为可检查形式，避免可读性税


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，需要确保其输出能被较弱系统轻松检查。现有证明者-验证者游戏虽能提高可检查性，但会导致准确性下降（可读性税）

Method: 解耦正确性和可检查性条件，训练"翻译器"模型将固定求解器模型的解决方案转换为可检查形式。先训练求解器最大化正确性，再训练翻译器在保留求解器答案的同时将其转换为可检查形式

Result: 提出解耦的证明者-验证者游戏，其均衡对应忠实且可检查的翻译器

Conclusion: 通过解耦训练方法，既能保持求解器的高准确性，又能通过翻译器实现输出的可检查性，避免了传统方法中的可读性税问题

Abstract: As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a "translator" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.

</details>


### [65] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentDropoutV2是一个测试时修正或剪枝框架，通过动态优化多智能体系统信息流来减少错误传播，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂推理方面表现出色，但存在个体参与者生成错误信息导致的级联影响问题。现有解决方案通常采用僵化的结构工程或昂贵的微调，限制了部署性和适应性。

Method: 提出AgentDropoutV2框架，作为主动防火墙拦截智能体输出，采用检索增强的修正器基于失败驱动指示器池迭代修正错误。使用蒸馏的失败模式作为先验知识精确识别潜在错误，不可修复的输出被剪枝以防止错误传播，同时采用回退策略保持系统完整性。

Result: 在广泛的数学基准测试中，AgentDropoutV2显著提升了多智能体系统的任务性能，在数学基准上平均准确率提高了6.3个百分点。系统表现出强大的泛化能力和适应性，能根据任务难度动态调整修正力度，并利用上下文感知指示器解决广泛的错误模式。

Conclusion: AgentDropoutV2是一个有效的测试时框架，能够在不重新训练的情况下动态优化多智能体系统的信息流，显著减少错误传播并提升系统性能，具有良好的部署性和适应性。

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [66] [Evaluating Stochasticity in Deep Research Agents](https://arxiv.org/abs/2602.23271)
*Haotian Zhai,Elias Stengel-Eskin,Pratik Patil,Liu Leqi*

Main category: cs.AI

TL;DR: 本文研究了深度研究代理（DRA）中的随机性问题，提出了评估框架并识别了三个随机性来源，通过实验发现减少随机性可提高研究质量，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 尽管深度研究代理在研究质量方面有所改进，但现有系统设计往往忽视了实际部署中的关键障碍：随机性。在相同查询下，DRA的重复执行会表现出研究结果、发现和引用的显著变异性，这阻碍了其实际应用。

Method: 将DRA建模为信息获取马尔可夫决策过程，引入评估框架量化系统方差，识别信息获取、信息压缩和推理三个随机性来源，通过控制实验研究不同决策步骤中这些模块的随机性如何影响DRA输出方差。

Result: 实验结果显示，减少随机性可以提高研究输出质量，其中推理和早期阶段的随机性对DRA输出方差贡献最大。在DeepSearchQA上的实验表明，提出的缓解方法将平均随机性降低了22%，同时保持了高质量的研究输出。

Conclusion: 本文形式化研究了DRA中的随机性问题，提出了评估框架和缓解策略。通过结构化输出和基于集成的查询生成等方法，可以在保持高质量输出的同时有效降低随机性，为DRA的实际部署提供了重要指导。

Abstract: Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations. In this paper, we formalize the study of stochasticity in DRAs by modeling them as information acquisition Markov Decision Processes. We introduce an evaluation framework that quantifies variance in the system and identify three sources of it: information acquisition, information compression, and inference. Through controlled experiments, we investigate how stochasticity from these modules across different decision steps influences the variance of DRA outputs. Our results show that reducing stochasticity can improve research output quality, with inference and early-stage stochasticity contributing the most to DRA output variance. Based on these findings, we propose strategies for mitigating stochasticity while maintaining output quality via structured output and ensemble-based query generation. Our experiments on DeepSearchQA show that our proposed mitigation methods reduce average stochasticity by 22% while maintaining high research quality.

</details>


### [67] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: CXReasonAgent：结合LLM与临床诊断工具，通过图像衍生的诊断和视觉证据进行基于证据的胸部X光诊断推理，相比传统LVLM提供更可靠、可验证的诊断响应。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在胸部X光诊断中存在三个主要问题：1）生成的响应缺乏诊断证据的忠实基础；2）提供的视觉证据有限难以验证；3）需要昂贵的重新训练来支持新诊断任务，限制了在临床环境中的可靠性和适应性。

Method: 提出CXReasonAgent诊断代理，将大型语言模型与临床基础诊断工具集成，利用图像衍生的诊断证据和视觉证据进行基于证据的诊断推理。同时创建了CXReasonDial多轮对话基准，包含1,946个对话，覆盖12个诊断任务。

Result: CXReasonAgent能够产生忠实基于证据的响应，相比大型视觉语言模型，实现了更可靠和可验证的诊断推理。在12个诊断任务的评估中表现出色。

Conclusion: 在安全关键的临床环境中，集成临床基础诊断工具对于实现可靠、可验证的医学诊断推理至关重要。CXReasonAgent展示了这种集成方法的有效性，为临床AI应用提供了更可靠的解决方案。

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


### [68] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: ODEBRAIN：一种基于神经ODE的脑电动态预测框架，通过将时空频特征整合到谱图节点中，建模连续潜在动态，显著提升EEG动态预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统潜变量方法通常通过循环架构离散化时间来建模连续脑动态，这必然导致累积预测误差的复合，并且无法捕捉EEG的瞬时非线性特征。

Method: 提出ODEBRAIN框架：1）将时空频特征整合到谱图节点中；2）使用神经ODE建模连续潜在动态；3）确保潜在表示能够捕捉任意时间点上复杂脑状态的随机变化。

Result: 大量实验验证ODEBRAIN在预测EEG动态方面显著优于现有方法，具有增强的鲁棒性和泛化能力。

Conclusion: ODEBRAIN通过神经ODE建模连续潜在动态，克服了传统方法的局限性，为神经群体动态建模提供了更有效的框架。

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


### [69] [The logic of KM belief update is contained in the logic of AGM belief revision](https://arxiv.org/abs/2602.23302)
*Giacomo Bonanno*

Main category: cs.AI

TL;DR: 该论文将KM信念更新的公理转化为包含三个模态算子的模态逻辑公理，并与AGM信念修正的模态逻辑进行比较，证明AGM信念修正是KM信念更新的特例。


<details>
  <summary>Details</summary>
Motivation: 研究KM信念更新与AGM信念修正之间的关系，通过模态逻辑形式化这两种信念变化理论，探索它们之间的包含关系。

Method: 为KM信念更新的每个公理提供对应的模态逻辑公理（包含B、>、□三个模态算子），然后将得到的逻辑与AGM信念修正的模态逻辑进行比较。

Result: 证明KM逻辑(L_KM)的每个公理都是AGM逻辑(L_AGM)的定理，因此AGM信念修正是KM信念更新的特例。对于强版本KM更新，差异可归结为处理不令人惊讶信息的单个公理。

Conclusion: AGM信念修正是KM信念更新的特殊情形，两种理论在模态逻辑框架下具有明确的包含关系，强版本KM更新的核心差异在于处理非初始不相信信息的方式。

Abstract: For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\mathcal L_{AGM}$ and the former by $\mathcal L_{KM}$ we show that every axiom of $\mathcal L_{KM}$ is a theorem of $\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\mathcal L_{KM}$ and $\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.

</details>


### [70] [Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315)
*Sha Hu*

Main category: cs.AI

TL;DR: 提出一种基于重采样的推理方法，通过对输入进行多种变换并聚合推理结果，利用认知不确定性提高AI模型推理准确性


<details>
  <summary>Details</summary>
Motivation: 即使经过优化的AI模型在推理时仍会因偶然性和认知不确定性产生错误。研究发现，当基于输入的多种不变变换进行推理时，认知不确定性会导致推理错误呈现部分独立性，这为改进推理准确性提供了机会

Method: 提出"重采样"推理方法：对训练好的AI模型，对同一输入应用多种变换版本，然后聚合所有推理输出以获得更准确的结果

Result: 该方法有潜力提高推理准确性，并提供了平衡模型大小和性能的策略

Conclusion: 利用认知不确定性导致的推理错误部分独立性，通过重采样和聚合的方法可以改进AI模型的推理性能，为模型优化提供了新思路

Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

</details>


### [71] [Generalized Rapid Action Value Estimation in Memory-Constrained Environments](https://arxiv.org/abs/2602.23318)
*Aloïs Rautureau,Tristan Cazenave,Éric Piette*

Main category: cs.AI

TL;DR: GRAVE2、GRAVER和GRAVER2算法通过两层搜索、节点回收及其组合技术，在保持GRAVE游戏强度的同时大幅减少存储节点数量。


<details>
  <summary>Details</summary>
Motivation: GRAVE算法在通用游戏博弈中表现出色，但其需要存储额外的胜率/访问统计数据，在内存受限环境中不实用，限制了实际应用。

Method: 提出了三种算法：GRAVE2（通过两层搜索扩展GRAVE）、GRAVER（通过节点回收扩展GRAVE）、GRAVER2（结合两种技术）。

Result: 这些增强技术能够显著减少存储节点数量，同时匹配GRAVE的游戏强度。

Conclusion: 新算法解决了GRAVE在内存受限环境中的实用性限制，使其更适合实际应用。

Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.

</details>


### [72] [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329)
*Chen Bo Calvin Zhang,Christina Q. Knight,Nicholas Kruus,Jason Hausenloy,Pedro Medeiros,Nathaniel Li,Aiden Kim,Yury Orlovskiy,Coleman Breen,Bryce Cai,Jasper Götting,Andrew Bo Liu,Samira Nedungadi,Paula Rodriguez,Yannis Yiming He,Mohamed Shaaban,Zifan Wang,Seth Donoughe,Julian Michael*

Main category: cs.AI

TL;DR: LLM访问显著提升新手在生物安全相关任务上的表现，准确率是对照组的4.16倍，甚至在某些任务上超越专家，但存在双重用途风险


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在生物学基准测试上表现良好，但尚不清楚它们是否能真正提升新手用户的表现（相比仅使用互联网资源），这对于理解科学加速和双重用途风险至关重要

Method: 进行多模型、多基准的人类提升研究，比较有LLM访问权限的新手与仅使用互联网资源的新手在八个生物安全相关任务集上的表现，参与者有充足时间（最复杂的任务达13小时）

Result: LLM访问提供了实质性提升：有LLM的新手准确率是对照组的4.16倍；在四个有专家基线的基准测试中，有LLM的新手在三个上超越了专家；独立LLM通常超过LLM辅助的新手；89.6%的参与者报告获取双重用途相关信息几乎没有困难

Conclusion: LLM显著提升了新手在原本需要训练有素从业者的生物学任务上的表现，强调需要持续、交互式的提升评估与传统基准测试并行，同时突显了双重用途风险

Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

</details>


### [73] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 本文提出一个细粒度任务分解的多智能体LLM交易框架，相比传统粗粒度指令方法显著提升风险调整后收益，并通过投资分析与决策偏好对齐优化系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的多智能体金融交易系统模仿分析师和经理角色，但依赖抽象指令而忽略真实工作流程的复杂性，导致推理性能下降和决策透明度不足。

Method: 提出将投资分析分解为细粒度任务的多智能体LLM交易框架，使用日本股票数据（价格、财务报表、新闻、宏观信息）在防泄漏回测设置下评估，并通过标准投资组合优化利用与股票指数的低相关性和系统输出方差。

Result: 细粒度任务分解相比传统粗粒度设计显著改善风险调整后收益；分析中间智能体输出发现，分析输出与下游决策偏好的对齐是系统性能的关键驱动因素；投资组合优化方法实现了优越性能。

Conclusion: 研究结果为实际应用中LLM智能体交易系统的智能体结构和任务配置设计提供了重要参考，强调细粒度任务分解和分析与决策对齐的重要性。

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [74] [Queue occupancy and server size distribution of a queue length dependent vacation queue with an optional service](https://arxiv.org/abs/2602.22295)
*Ashish Verma,Sourav Pradhan*

Main category: cs.IT

TL;DR: 该论文分析了一个具有单次和多次休假策略的无限缓冲区离散时间批量到达队列系统，其中客户分两阶段服务：基本服务(FES)和可选服务(SOS)。作者推导了联合概率生成函数，建立了任意时隙的完整联合分布，并通过数值示例验证了框架的适用性。


<details>
  <summary>Details</summary>
Motivation: 离散时间排队系统在现代电信系统中具有高度适用性，能够提供自适应数据包处理、拥塞控制安全/检查、节能操作，并支持5G、物联网和边缘计算环境中常见的突发流量。需要分析具有两阶段服务和休假策略的队列系统来优化这些现代通信系统的性能。

Method: 分析无限缓冲区离散时间批量到达队列，采用单次和多次休假策略，客户分两阶段服务：基本服务(FES)和可选服务(SOS)。推导了FES和SOS完成后等待传输数据包数量与正在处理数据包数量的联合概率生成函数，建立了包括休假完成状态在内的任意时隙完整联合分布。

Result: 成功推导出两阶段服务完成后的双变量概率生成函数，建立了任意时隙的完整联合分布。通过数值示例（包括离散相位型服务时间分布）验证了所提框架的适用性，并通过图形表示对关键参数对边际系统概率和不同性能指标的敏感性进行了分析。

Conclusion: 该研究为具有两阶段服务和休假策略的离散时间批量到达队列系统提供了完整的数学分析框架，能够有效建模现代电信系统中的复杂服务场景，为系统性能优化和参数配置提供了理论依据。

Abstract: The discrete time queueing system is highly applicable to modern telecommunication systems, where it provides adaptive packet handling, congestion controlled security/inspection, energy efficient operation, and supports bursty traffic common in 5G, Internet of Things (IoT), and edge computing environments. In this article, we analyze an infinite-buffer discrete-time batch-arrival queue with single and multiple vacation policy where customers are served in batches, in two phases, namely first essential service (FES) and second optional service (SOS). In such systems, the FES corresponds to basic data processing or packet routing, while SOS represents secondary tasks such as encryption, error checking, data compression, or deep packet inspection that may not be necessary for every packet. Here, we derive the bivariate probability generating functions for the joint distribution of the number of packets waiting for transmission and the number are being processed immediately after the completion of both the FES and SOS. Furthermore, the complete joint distribution at arbitrary time slots, including vacation completion states, is established. Numerical illustrations demonstrate the applicability of the proposed framework, including an example with discrete phase type service time distribution. Finally, the sensitivity analysis of the key parameters on marginal system's probabilities and different performance measures have been investigated through several graphical representations.

</details>


### [75] [On the Computation Rate of All-Reduce](https://arxiv.org/abs/2602.22482)
*Yufeng Zhou,Hua Sun*

Main category: cs.IT

TL;DR: 该论文研究了All-Reduce问题的计算速率，提出了基于割集的上界和基于时间共享的线性规划下界，为特定网络类型提供了最优解，并为循环、完全和超立方体网络提供了已知最佳速率界限。


<details>
  <summary>Details</summary>
Motivation: 研究All-Reduce问题在任意带宽并行链路网络中的计算速率优化，旨在为分布式计算中的求和操作提供理论性能界限。

Method: 提出两种界限：1) 基于网络割集的上界；2) 基于时间共享方案的线性规划下界，特别针对先Reduce后Broadcast的方案。将这些界限应用于特定网络拓扑进行分析。

Result: 为特定网络类别获得了最优计算速率，并为循环网络、完全网络和超立方体网络提供了已知最佳速率界限，其中上界不超过下界的两倍。

Conclusion: 论文建立了All-Reduce问题计算速率的理论框架，为不同网络拓扑提供了性能界限，在分布式计算优化方面具有重要理论价值。

Abstract: In the All-Reduce problem, each one of the K nodes holds an input and wishes to compute the sum of all K inputs through a communication network where each pair of nodes is connected by a parallel link with arbitrary bandwidth. The computation rate of All-Reduce is defined as the number of sum instances that can be computed over each network use. For the computation rate, we provide a cut-set upper bound and a linear programming lower bound based on time (bandwidth) sharing over all schemes that first perform Reduce (aggregating all inputs at one node) and then perform Broadcast (sending the sum from that node to all other nodes). Specializing the two general bounds gives us the optimal computation rate for a class of communication networks and the best-known rate bounds (where the upper bound is no more than twice of the lower bound) for cyclic, complete, and hypercube networks.

</details>


### [76] [A Thermodynamic Structure of Asymptotic Inference](https://arxiv.org/abs/2602.22605)
*Willy Wong*

Main category: cs.IT

TL;DR: 该论文建立了一个热力学框架用于渐近推断，将样本量和参数方差定义为状态空间，其中香农信息扮演熵的角色，并推导出类似热力学定律的信息不等式。


<details>
  <summary>Details</summary>
Motivation: 将统计推断与热力学理论统一起来，为渐近推断提供一个热力学框架，揭示信息获取过程中的基本限制和效率边界。

Method: 将样本量和参数方差定义为状态空间，香农信息作为熵，通过积分因子组织信息变化形成类似热力学第一定律的平衡方程。

Result: 推导出类似反向第二定律的循环不等式、类似第三定律的熵下界（由表示噪声设定）、最优推断路径、信息增益的全局界限、以及类似卡诺效率的信息效率（受噪声底限限制）。

Conclusion: de Bruijn恒等式和I-MMSE关系在高斯极限情况下是该热力学结构的坐标投影，表明系综物理和推断物理是在统一热力学描述中沿相反方向演化的影子过程。

Abstract: A thermodynamic framework for asymptotic inference is developed in which sample size and parameter variance define a state space. Within this description, Shannon information plays the role of entropy, and an integrating factor organizes its variation into a first-law-type balance equation. The framework supports a cyclic inequality analogous to a reversed second law, derived for the estimation of the mean. A non-trivial third-law-type result emerges as a lower bound on entropy set by representation noise. Optimal inference paths, global bounds on information gain, and a natural Carnot-like information efficiency follow from this structure, with efficiency fundamentally limited by a noise floor. Finally, de Bruijn's identity and the I-MMSE relation in the Gaussian-limit case appear as coordinate projections of the same underlying thermodynamic structure. This framework suggests that ensemble physics and inferential physics constitute shadow processes evolving in opposite directions within a unified thermodynamic description.

</details>


### [77] [Multi-modal Data Driven Virtual Base Station Construction for Massive MIMO Beam Alignment](https://arxiv.org/abs/2602.22796)
*Yijie Bian,Wei Guo,Jie Yang,Shenghui Song,Jun Zhang,Shi Jin,Khaled B. Letaief*

Main category: cs.IT

TL;DR: 提出一种可解释的波束对准框架，利用多模态数据构建虚拟基站(VBS)来在混合视距/非视距环境中实现低训练开销的波束管理


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO是实现6G高数据速率的关键技术，但其性能依赖于有效的波束管理。现有方法在混合视距/非视距传播环境中面临训练开销大的挑战，需要更高效的波束对准方案。

Method: 利用多模态数据构建虚拟基站(VBS)：通过3D LiDAR点云重建反射面，将基站镜像到反射面形成VBS，提供无线环境的稀疏空间表示。基于VBS开发波束对准方案，包括粗粒度信道重建和部分波束训练。

Result: 数值结果表明，所提方法在频谱效率方面达到接近最优的性能，显著降低了训练开销。

Conclusion: 提出的可解释VBS框架为混合传播环境中的波束对准提供了有效解决方案，通过利用环境几何信息实现低开销的高性能波束管理。

Abstract: Massive multiple-input multiple-output (MIMO) is a key enabler for the high data rates required by the sixth-generation networks, yet its performance hinges on effective beam management with low training overhead. This paper proposes an interpretable framework to tackle beam alignment in mixed line-of-sight (LoS) and non-line-of-sight (NLoS) propagation environments. Our approach utilizes multi-modal data to construct virtual base stations (VBSs), which are geometrically defined as mirror images of the base station across reflecting surfaces reconstructed from 3D LiDAR points. These VBSs provide a sparse and spatial representation of the dominant features of the wireless environment. Based on the constructed VBSs, we develop a VBS-assisted beam alignment scheme comprising coarse channel reconstruction followed by partial beam training. Numerical results demonstrate that the proposed method achieves near-optimal performance in terms of spectral efficiency.

</details>


### [78] [Semantic Communication Through the Lens of Context-Dependent Channel Modeling](https://arxiv.org/abs/2602.22934)
*Javad Gholipour,Rafael F. Schaefer,Gerhard P. Fettweis*

Main category: cs.IT

TL;DR: 本文提出了一种基于概率模型和上下文概念的语义通信框架，专门研究语义噪声仅来自语义信道的情况，分析了语义编码器的表示能力，并推导了不同场景下的容量结果和可达速率。


<details>
  <summary>Details</summary>
Motivation: 语义通信作为下一代网络的有前景范式，仍存在若干基本挑战未解决。本文旨在解决语义通信中的关键问题，特别是当语义噪声仅来自语义信道而物理信道理想时的情况。

Method: 基于语义通信的概率模型，引入上下文概念，提出虚拟状态依赖信道模型，其中状态代表上下文在通信中起关键作用。分析语义编码器的表示能力，探索存在语义噪声的各种语义通信场景。

Result: 推导了部分情况下的容量结果和其他情况下的可达速率，为语义通信系统的性能分析提供了理论框架。

Conclusion: 本文通过引入上下文和虚拟状态依赖信道模型，为语义通信系统分析提供了新视角，解决了语义噪声仅来自语义信道这一特定子类问题，为语义通信的理论发展做出了贡献。

Abstract: Semantic communication has emerged as a promising paradigm for next-generation networks, yet several fundamental challenges remain unresolved. Building on the probabilistic model of semantic communication and leveraging the concept of context, this paper examines a specific subclass of semantic communication problems, where semantic noise originates solely from the semantic channel, assuming an ideal physical channel. To model this system, we introduce a virtual state-dependent channel, where the state-representing context-plays a crucial role in shaping communication. We further analyze the representational capability of the semantic encoder and explore various semantic communication scenarios in the presence of semantic noise, deriving capacity results for some cases and achievable rates for others.

</details>


### [79] [Frequency-Ordered Tokenization for Better Text Compression](https://arxiv.org/abs/2602.22958)
*Maximilian Kalcher*

Main category: cs.IT

TL;DR: 频率有序分词：一种通过重新排序BPE词汇表使高频token获得小整数ID，再用变长整数编码的文本预处理技术，能提升无损压缩效果并加速压缩过程。


<details>
  <summary>Details</summary>
Motivation: 利用自然语言token的幂律分布特性（齐夫定律），通过预处理改进无损文本压缩效果，同时减少计算密集型压缩算法的运行时间。

Method: 使用字节对编码（BPE）对文本进行分词，根据token频率重新排序词汇表（高频token获得小整数标识符），然后用变长整数编码处理结果，最后传递给标准压缩器。

Result: 在enwik8（100MB维基百科）上，相比原始压缩，zlib提升7.08个百分点，LZMA提升1.69个百分点，zstd提升0.76个百分点（均包含词汇表开销）。在1GB规模（enwik9）及中文、阿拉伯文文本上效果一致。预处理后总运行时间比原始zstd-22快3.1倍，比原始LZMA快2.4倍。

Conclusion: 频率有序分词是一种简单有效的文本预处理技术，能显著提升无损压缩效果并加速压缩过程，代码实现仅需不到50行，优于传统的词替换变换方法。

Abstract: We present frequency-ordered tokenization, a simple preprocessing technique that improves lossless text compression by exploiting the power-law frequency distribution of natural language tokens (Zipf's law). The method tokenizes text with Byte Pair Encoding (BPE), reorders the vocabulary so that frequent tokens receive small integer identifiers, and encodes the result with variable-length integers before passing it to any standard compressor. On enwik8 (100 MB Wikipedia), this yields improvements of 7.08 percentage points (pp) for zlib, 1.69 pp for LZMA, and 0.76 pp for zstd (all including vocabulary overhead), outperforming the classical Word Replacing Transform. Gains are consistent at 1 GB scale (enwik9) and across Chinese and Arabic text. We further show that preprocessing accelerates compression for computationally expensive algorithms: the total wall-clock time including preprocessing is 3.1x faster than raw zstd-22 and 2.4x faster than raw LZMA, because the preprocessed input is substantially smaller. The method can be implemented in under 50 lines of code.

</details>


### [80] [Secure Transmission for Fluid Antenna-Aided ISAC Systems](https://arxiv.org/abs/2602.23241)
*Yunxiao Li,Qian Zhang,Xuejun Cheng,Zhiguo Wang,Xiaoyan Wang,Hongji Xu,Ju Liu*

Main category: cs.IT

TL;DR: 本文研究流体天线辅助的集成感知与通信系统中的安全传输问题，通过联合优化天线位置和波束成形来最大化多用户总保密速率，相比固定天线系统获得超过20%的性能增益。


<details>
  <summary>Details</summary>
Motivation: 在流体天线增强的ISAC系统中，当感知目标作为窃听者时，如何最大化总保密速率的问题尚未得到解决。流体天线的空间灵活性为增强物理层安全提供了新机会。

Method: 采用块连续上界最小化算法，结合近端距离算法进行波束成形器的闭式更新，以及外推投影梯度算法进行天线位置向量优化，以解决非凸优化问题。

Result: 仿真结果表明，所提出的流体天线ISAC方案相比固定位置天线系统实现了超过20%的总保密速率增益。

Conclusion: 流体天线通过空间灵活性显著增强了ISAC系统的物理层安全性能，为解决感知目标作为窃听者的安全传输问题提供了有效方案。

Abstract: Fluid antenna (FA) has become a highly promising technology and has recently been used to enhance the integrated sensing and communication (ISAC) system. However, the scenario where sensing targets act as eavesdroppers in ISAC and how to maximize the sum secrecy rate has not been addressed. This letter investigates secure transmission in FA-aided ISAC systems, where the spatial agility of FAs enables enhanced physical layer security. We jointly optimize antenna position vector (APV) and beamforming to maximize the multiuser sum secrecy rate, which complicates the solution process. To solve the resulting non-convex problem, we use a block successive upper-bound minimization (BSUM) algorithm, which incorporates the proximal distance algorithm (PDA) for closed-form beamformer updates and extrapolated projected gradient (EPG) for APV optimization. Simulation results show that the proposed FA-ISAC scheme achieves over 20$\%$ sum secrecy rate gain compared to fixed-position antenna (FPA) systems.

</details>
