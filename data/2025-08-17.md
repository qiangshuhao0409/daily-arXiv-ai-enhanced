<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Rethinking Reliability Using Network Coding: a Practical 5G Evaluation](https://arxiv.org/abs/2508.10247)
*Laura Landon,Vipindev Adat Vasudevan,Junmo Sung,Muriel Médard*

Main category: cs.NI

TL;DR: 本文介绍了一种集成到5G测试床IP层的实时网络编码系统，替代传统的基于重传的可靠性机制（如ARQ和HARQ）。通过随机线性网络编码（RLNC）实现前向纠错，并在实际流量中评估其性能。结果表明，RLNC在适当码率下能以更少的传输次数恢复丢包，并在中高丢包率下保持高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统的基于重传的可靠性机制（如ARQ和HARQ）在无线系统中效率较低，尤其是在高丢包率下。本文旨在探索网络编码（如RLNC）作为替代方案的潜力，以提高资源利用率和系统性能。

Method: 采用基于netfilter的数据包拦截框架，在5G测试床的gNB和UE之间的3GPP射频链路上注入RLNC前向纠错。评估了块编码方案对吞吐量、抖动和资源使用的影响。

Result: 实验结果显示，RLNC在适当码率下能以比ARQ/HARQ更少的传输次数完全恢复丢包，并在中高丢包率下保持高吞吐量。

Conclusion: 网络编码（如RLNC）可以有效地替代基于重传的可靠性机制，未来无线系统中有望实现更高效的资源利用。

Abstract: This work presents the design and implementation of a real-time network
coding system integrated into the IP layer of a 5G testbed, offering an
alternative to conventional retransmission-based reliability mechanisms such as
ARQ and HARQ. Using a netfilter-based packet interception framework, we inject
forward erasure correction using Random Linear Network Coding (RLNC) into live
traffic between a gNB and UE over a 3GPP RF link. We evaluate a block coding
scheme, analyzing its impact on throughput, jitter, and resource usage. Results
show that with appropriate code rate selection, RLNC can fully recover from
packet losses using fewer transmissions than ARQ/HARQ and maintain a high
throughput, particularly under moderate-to-high packet loss rates. These
findings demonstrate that network coding can effectively replace
retransmission-based reliability in future wireless systems, with the potential
for more efficient resource utilization.

</details>


### [2] [Design of a Timer Queue Supporting Dynamic Update Operations](https://arxiv.org/abs/2508.10283)
*Zekun Wang,Binghao Yue,Weitao Pan,Jiangyi Shi,Yue Hao*

Main category: cs.NI

TL;DR: 提出了一种基于脉动阵列和移位寄存器的混合架构硬件优先级队列，用于高效管理计时器队列，解决了传统实现中计时精度低和计算开销高的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模计时器在网络处理中普遍存在，但传统实现存在计时精度低和计算开销高的局限性。

Method: 设计了一种支持五种操作的混合架构硬件优先级队列，利用集中式布尔逻辑编码和创新的push-first操作。

Result: 实验结果显示，该设计在FPGA上运行频率超过400 MHz，资源消耗比现有最优实现减少2.2-2.8倍。

Conclusion: 该设计首次实现了队列内优先级更新，显著提升了计时器队列管理的效率和性能。

Abstract: Large-scale timers are ubiquitous in network processing, including flow table
entry expiration control in software defined network (SDN) switches, MAC
address aging in Ethernet bridges, and retransmission timeout management in
TCP/IP protocols. Conventional implementations suffer from critical
limitations: low timing accuracy due to large-scale timer traversal and high
computational overhead for new timer insertion. This paper presents a
hybrid-architecture hardware priority queue based on systolic arrays and shift
registers for efficient timer queue management. The design uniquely supports
five operations: enqueue, dequeue, delete, update, and peek.To the best of our
knowledge, it is the first hardware priority queue enabling in-queue priority
updates. By leveraging centralized Boolean logic encoding within systolic
blocks, the design efficiently generates set/shift control signals while the
novel push-first operation ensures FIFO ordering for same-priority timers
without additional metadata. Experimental results demonstrate that the design
operates at over 400 MHz on FPGAs, achieving a 2.2-2.8x reduction in resource
consumption compared to state-of-the-art implementations.

</details>


### [3] [Near-realtime Earth Observation Via Starlink LEO Satellite Constellation](https://arxiv.org/abs/2508.10338)
*Bo Wu,Pengfei Zhou*

Main category: cs.NI

TL;DR: 论文探讨了利用Starlink卫星基础设施支持地球观测卫星数据传输的可行性，提出了一种名为“Starlink Space User”（SSU）的新系统，通过优化算法显著减少了数据积压。


<details>
  <summary>Details</summary>
Motivation: 地球观测卫星数据传输面临地面站数量有限和通信窗口短暂的问题，而Starlink等新兴LEO星座提供了连续连接的可能性。

Method: 提出SSU系统，将观测卫星视为Starlink的空间用户，设计优化算法进行链路和PoP选择及系统调度。

Result: 通过仿真和实际测量，SSU显著减少了每颗卫星的中位数数据积压。

Conclusion: SSU系统利用Starlink基础设施，有效解决了地球观测卫星数据传输的瓶颈问题。

Abstract: Earth observation (EO) satellites in Low Earth Orbit (LEO) are collecting
vast amounts of data, which are invaluable for applications such as monitoring
forest fires. However, data downloading from EO satellites faces significant
challenges due to the limited number of ground stations and the brief
communication windows with them. Conversely, emerging LEO constellations like
Starlink have enabled continuous connectivity and revolutionized access for
ordinary users globally, who can connect via a simple satellite dish. In this
paper, we study the feasibility of supporting EO satellites with Starlink
satellite infrastructure and introduce a novel data delivery system, designated
as "Starlink Space User" (SSU), for relaying data from observation satellites.
SSU treats EO satellites as space users of Starlink, facilitating efficient
data transfer to Earth. At the core of SSU is a novel class of algorithms
designed for link and PoP selection, as well as system scheduling optimization,
that operate effectively atop Starlink's proprietary infrastructure. We assess
the performance of SSU using trace-driven simulations alongside real-world
Starlink performance measurements. Our results demonstrate that the proposed
Starlink-aided design can significantly reduce the median backlog (data not
delivered) per satellite.

</details>


### [4] [Probabilistic Latency Analysis of the Data Distribution Service in ROS 2](https://arxiv.org/abs/2508.10413)
*Sanghoon Lee,Hyung-Seok Park,Jiyeong Chae,Kyung-Joon Park*

Main category: cs.NI

TL;DR: 论文提出了一种概率延迟分析（PLA）方法，用于建模ROS 2 DDS通信的可靠传输过程，解决了无线网络中参数调优的挑战。


<details>
  <summary>Details</summary>
Motivation: ROS 2 DDS在无线网络中因心跳周期、IP分片和重传间隔的紧密耦合导致延迟行为不明确，缺乏调优指导。

Method: 采用离散状态方法建模ROS 2 DDS通信的可靠传输过程，分析中间件和传输层事件，计算未确认消息的稳态概率分布和重传延迟。

Result: 在270种场景下验证PLA，结果显示分析预测与实验结果高度吻合。

Conclusion: PLA为无线工业机器人中可靠性、延迟和性能的系统优化提供了理论基础。

Abstract: Robot Operating System 2 (ROS 2) is now the de facto standard for robotic
communication, pairing UDP transport with the Data Distribution Service (DDS)
publish-subscribe middleware. DDS achieves reliability through periodic
heartbeats that solicit acknowledgments for missing samples and trigger
selective retransmissions. In lossy wireless networks, the tight coupling among
heartbeat period, IP fragmentation, and retransmission interval obscures end to
end latency behavior and leaves practitioners with little guidance on how to
tune these parameters. To address these challenges, we propose a probabilistic
latency analysis (PLA) that analytically models the reliable transmission
process of ROS 2 DDS communication using a discrete state approach. By
systematically analyzing both middleware level and transport level events, PLA
computes the steady state probability distribution of unacknowledged messages
and the retransmission latency. We validate our PLA across 270 scenarios,
exploring variations in packet delivery ratios, message sizes, and both
publishing and retransmission intervals, demonstrating a close alignment
between analytical predictions and experimental results. Our findings establish
a theoretical basis to systematically optimize reliability, latency, and
performance in wireless industrial robotics.

</details>


### [5] [Federated Learning Over LoRa Networks: Simulator Design and Performance Evaluation](https://arxiv.org/abs/2508.10574)
*Anshika Singh,Siddhartha S. Borkotoky*

Main category: cs.NI

TL;DR: 论文开发了一个Python模拟器，结合Flower和LoRaSim框架，评估了基于LoRa网络的联邦学习性能，重点研究了FEC对学习收敛和设备通信时间的影响。


<details>
  <summary>Details</summary>
Motivation: 解决在LoRa低功耗广域网络中实现联邦学习时面临的带宽限制、干扰和严格占空比约束等挑战。

Method: 开发了一个集成Flower和LoRaSim框架的Python模拟器，详细建模了LoRa信道上的FL更新传输，包括接收灵敏度、干扰特性、块衰落效应等。

Result: 数值结果表明传输参数（扩频因子、FEC速率）和干扰对FL性能的影响，FEC在LoRa网络中实现FL的关键作用。

Conclusion: FEC对FL收敛和设备通信时间有显著影响，为LoRa网络上的FL通信协议设计提供了重要见解。

Abstract: Federated learning (FL) over long-range (LoRa) low-power wide area networks
faces unique challenges due to limited bandwidth, interference, and strict
duty-cycle constraints. We develop a Python-based simulator that integrates and
extends the Flower and LoRaSim frameworks to evaluate centralized FL over LoRa
networks. The simulator employs a detailed link-level model for FL update
transfer over LoRa channels, capturing LoRa's receiver sensitivity,
interference characteristics, block-fading effects, and constraints on the
maximum transmission unit. It supports update sparsification, quantization,
compression, forward frame-erasure correction (FEC), and duty cycling.
Numerical results illustrate the impact of transmission parameters (spreading
factor, FEC rate) and interference on FL performance. Demonstrating the
critical role of FEC in enabling FL over LoRa networks, we perform an in-depth
evaluation of the impact of FEC on FL convergence and device airtime, providing
insights for communication protocol design for FL over LoRa networks.

</details>


### [6] [Balancing the Energy Consumption and Latency of Over-the-Air Firmware Updates in LoRaWAN](https://arxiv.org/abs/2508.10588)
*Siddhartha S. Borkotoky*

Main category: cs.NI

TL;DR: 提出了一种灵活的LoRaWAN固件更新方案，通过调整扩频因子实现能耗与延迟的可调权衡。


<details>
  <summary>Details</summary>
Motivation: 解决LoRaWAN终端设备在能耗受限和传输周期限制下，固件更新的能耗与延迟问题。

Method: 采用LoRa扩频因子顺序传输更新帧，通过调整最小扩频因子和每个扩频因子的传输次数，实现能耗与延迟的权衡。

Result: 方案可根据需求选择低延迟高能耗或高延迟低能耗的更新策略。

Conclusion: 该方案为LoRaWAN固件更新提供了灵活且高效的解决方案。

Abstract: Over-the-air firmware updates are crucial for mitigating security threats and
maintaining up-to-date device functionality in Long Range Wide Area Networks
(LoRaWANs). LoRaWAN end devices are usually energy-constrained, and LoRaWAN
transmissions are subject to duty-cycle restrictions. Consequently, controlling
the energy expenditure and update-delivery latency of FUOTA are key challenges.
We propose a flexible scheme that achieves a tunable trade-off between the
energy consumption and delivery delay. The scheme employs the LoRa spreading
factors sequentially to transmit update-carrying frames, sending a fixed number
of frames with a given spreading factor before moving to the next. By adjusting
the smallest spreading factor to be used and the number of transmissions per
spreading factor, a suitable energy-delay trade-off can be achieved. Thus,
time-sensitive updates, such as security patches, may be sent with a
low-delay-high-energy setting, whereas a more energy-efficient but higher-delay
setting may be used for non-critical updates.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: 本文综述了利用大语言模型（LLMs）自动化数学建模过程的最新进展，包括数据合成、模型微调、推理框架、基准数据集和性能评估，并构建了新的公平评估排行榜和在线资源门户。


<details>
  <summary>Details</summary>
Motivation: 优化建模需要大量专业知识，而LLMs的出现为自动化数学建模提供了新机会。

Method: 综述了技术栈的各个方面，包括数据合成、模型微调、推理框架等，并清理了基准数据集，构建了新的排行榜和在线门户。

Result: 发现基准数据集错误率高，清理后构建了公平评估排行榜和资源门户。

Conclusion: 指出了当前方法的局限性，并提出了未来研究方向。

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [8] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 亚马逊Nova AI挑战赛通过对抗性竞赛推动AI安全技术发展，大学团队开发了先进的自动化红队和安全对齐方法。


<details>
  <summary>Details</summary>
Motivation: 解决AI在软件开发中的安全性问题，推动安全AI技术的发展。

Method: 通过对抗性竞赛（红队与AI助手对话）和高质量标注数据，评估和改进安全对齐方法。

Result: 团队开发了推理式安全对齐、模型护栏、多轮越狱等先进技术。

Conclusion: 该挑战赛提升了AI安全性，展示了协作努力在推动技术进步中的重要性。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [9] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 提出了一种多智能体系统，通过关系提取检测新闻标题和短文本中的虚假信息，准确率达95.3%，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 数字平台上虚假信息的广泛传播对信息完整性构成挑战，需要高效检测方法。

Method: 结合四种智能体（机器学习、维基百科知识检查、一致性检测、网络数据抓取分析），通过模型上下文协议（MCP）协调，实现共享上下文和实时学习。

Result: 多智能体系统准确率95.3%，F1分数0.964，显著优于单智能体和传统方法。加权聚合方法优于算法阈值优化。

Conclusion: 模块化架构使系统易于扩展，同时保留决策过程细节，为虚假信息检测提供了高效解决方案。

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [10] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: 本文系统回顾和比较了Agentic AI框架，分析了其架构、通信机制等，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）带来的Agentic AI范式，评估现有框架的优缺点，推动自主AI系统的发展。

Method: 对CrewAI、LangGraph等框架进行系统回顾和比较分析，深入研究了通信协议如CNP、A2A等。

Result: 建立了Agentic AI系统的基础分类法，并提出了增强可扩展性、鲁棒性和互操作性的研究方向。

Conclusion: 本文为研究人员和从业者提供了全面的参考，推动了下一代自主AI系统的进步。

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [11] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 本文研究了开源深度研究代理（ODR）与闭源系统在BrowseComp-Small基准上的性能比较，并通过改进提出了ODR+模型，实现了10%的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理（DRAs）多为闭源系统，缺乏开源选项，限制了学术研究。本文旨在填补这一空白，并通过改进开源系统提升性能。

Method: 使用BrowseComp-Small基准测试ODR与闭源系统性能，提出三种改进策略（ODR+），并通过消融实验验证其贡献。

Result: 所有测试系统在60个问题的测试集上初始准确率为0%，改进后的ODR+达到10%的成功率，优于其他系统。

Conclusion: 开源ODR+在性能上可与闭源系统竞争，为学术研究提供了可行的开源解决方案。

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [12] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为LCPO的方法，通过控制生成长度来优化大型推理模型的效率，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务中表现优异，但冗长的输出增加了计算成本并可能导致过度思考，需要平衡推理效果与效率。

Method: 分析了生成路径分布和难度估计，提出了基于Bradley-Terry损失框架的长度控制偏好优化（LCPO），直接平衡与NLL损失相关的隐式奖励。

Result: 实验表明，LCPO在多个基准测试中将平均输出长度减少50%以上，同时保持推理性能。

Conclusion: LCPO展示了在有限数据和训练下学习长度偏好的潜力，为高效推理提供了计算高效的方法。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [13] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI是一个新型AutoML框架，通过动态解决方案空间探索和RAG技术提升性能，解决了现有LLM-based AutoML系统的探索局限和执行瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based AutoML系统存在探索策略单一和执行效率低下的问题，限制了其性能。

Method: KompeteAI采用动态解决方案空间探索，引入合并阶段整合候选方案，并利用RAG技术扩展假设空间。此外，通过预测评分模型和加速调试方法优化执行效率。

Result: KompeteAI在MLE-Bench基准测试中平均性能提升3%，且加速管道评估6.9倍。

Conclusion: KompeteAI通过创新方法显著提升了AutoML系统的性能和效率，并在新提出的Kompete-bench上取得了领先成果。

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [14] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 本文提出了一种基于事件熵势的概念，用于增强AI中的不确定性量化、决策和可解释性，结合物理学和信息理论，适用于多种AI应用。


<details>
  <summary>Details</summary>
Motivation: 旨在通过事件熵势的概念，统一和强化智能系统中的不确定性建模，提升AI的决策能力和可解释性。

Method: 将物理学中的熵势概念调整为AI框架，引入事件中心度量，形式化定义并强调条件期望以处理反事实场景。

Result: 在策略评估、内在奖励设计、可解释AI和异常检测等应用中展示了熵势框架的潜力。

Conclusion: 熵势框架为AI中的不确定性管理提供了理论基础、可解释且多功能的方法，融合了热力学、信息理论和机器学习的原理。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [15] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: 论文认为大语言模型（LLMs）的“理解能力”和“推理能力”是虚幻的，本质上是其工作原理的限制所致。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否真正具备理解和推理能力，揭示其局限性。

Method: 通过分析LLMs的工作原理，论证其无法实现真正的正确推理。

Result: LLMs因本质限制无法具备真正的理解和推理能力。

Conclusion: LLMs的所谓“能力”是虚幻的，其工作原理决定了无法实现真正的推理。

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [16] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 论文提出了一种基于规则的可验证逐步奖励机制（VSRM），通过奖励有效推理步骤并惩罚无效步骤，解决了大型推理模型（LRMs）的过度思考问题，显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务中表现优异，但存在过度思考问题，导致效率低下。现有方法需要预设预算或选择推理模式，缺乏灵活性。

Method: 提出VSRM机制，根据推理轨迹中中间状态的表现分配奖励，结合PPO和Reinforce++进行实验验证。

Result: 在AIME24和AIME25等数学推理基准测试中，VSRM显著减少了输出长度，同时保持了推理性能。

Conclusion: VSRM有效抑制了无效推理步骤，鼓励有效推理，从根本上缓解了过度思考问题。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [17] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: 论文介绍了Dianping-Trust-Safety团队在META CRAG-MM挑战中的解决方案，通过结合视觉大语言模型、强化学习和外部知识检索，在多模态多轮问答任务中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决多模态多轮问答任务中的复杂查询和上下文理解问题，提升系统的回答准确性和减少幻觉现象。

Method: 1. 任务1：基于视觉大语言模型，通过GPT-4.1的知识蒸馏进行监督微调，并应用课程学习策略优化强化学习。2. 任务2和3：结合知识图谱和网络搜索API，整合外部知识处理复杂查询和多轮对话。

Result: 任务1以52.38%的优势获得第一名，任务3获得第三名，验证了方法的有效性。

Conclusion: 结合课程学习与强化学习的训练框架在多模态问答任务中表现优异，为复杂场景下的问答系统提供了有效解决方案。

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [18] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: HATRPO-W和HATRPO-G通过动态分配KL阈值，提升了HATRPO在异构多智能体强化学习中的性能，分别实现了超过22.5%的最终性能提升。


<details>
  <summary>Details</summary>
Motivation: 在异构多智能体强化学习中，固定KL阈值可能导致更新缓慢和局部最优，需要更灵活的阈值分配方法。

Method: 提出两种KL阈值分配方法：基于KKT的HATRPO-W和基于贪心算法的HATRPO-G。

Result: 实验显示两种方法显著提升了HATRPO的性能，收敛更快且最终奖励更高，HATRPO-W还表现出更稳定的学习动态。

Conclusion: 动态KL阈值分配方法有效解决了异构多智能体强化学习中的性能瓶颈，HATRPO-W和HATRPO-G均表现出色。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [19] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）在信息稀疏环境中的想象力推理能力，提出了基于“Turtle Soup”游戏的框架，包括新基准、代理和评估协议，揭示了LLMs的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准多为静态或聚焦社交推理，无法捕捉动态探索性推理过程，因此需要新框架填补这一空白。

Method: 提出TurtleSoup-Bench基准和Mosaic-Agent代理，开发多维度评估协议，测试LLMs在800个双语谜题中的表现。

Result: 实验显示LLMs在想象力推理中存在明显能力限制，与人类表现差距显著。

Conclusion: 研究为探索性代理行为提供了新见解和未来研究方向。

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [20] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: LeanRAG通过知识聚合和检索策略的协作设计，解决了传统知识图谱RAG方法中语义孤岛和检索效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱RAG方法存在语义孤岛和检索效率低的问题，限制了其效果。

Method: LeanRAG采用语义聚合算法构建实体集群和显式关系，并结合自底向上的结构引导检索策略。

Result: 在四个QA基准测试中，LeanRAG显著优于现有方法，减少46%的检索冗余。

Conclusion: LeanRAG通过优化知识聚合和检索策略，提升了RAG的效果和效率。

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [21] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: HiRef框架结合医学本体层次结构和电子健康记录（EHR）共现模式，通过双曲空间嵌入和稀疏正则化提升药物推荐的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决EHR数据中罕见实体和不完整记录导致的模型泛化问题。

Method: 结合医学本体层次结构和EHR共现模式，使用双曲空间嵌入和稀疏正则化优化模型。

Result: 在MIMIC-III和MIMIC-IV基准测试中表现优异，对未见医疗代码保持高准确率。

Conclusion: HiRef通过本体和共现模式的互补结构，显著提升了药物推荐的鲁棒性和泛化能力。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [22] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: MM-Food-100K是一个公开的多模态食品数据集，包含10万样本，来源于120万质量合格的食品图像，标注了多种信息。数据通过社区贡献和AI辅助质量检查收集，支持溯源。通过微调视觉语言模型验证了其有效性，并在公开子集上展示了性能提升。


<details>
  <summary>Details</summary>
Motivation: 构建一个高质量、可溯源的多模态食品数据集，以支持食品智能研究，并为社区和商业用途提供资源。

Method: 使用Codatta贡献模型，结合社区众包和AI辅助质量检查，收集并标注食品图像。数据通过链下账本溯源，并计划链上协议。

Result: 微调视觉语言模型（如ChatGPT 5、Qwen-Max）在营养预测任务上表现优于基线，验证了数据集的实用性。

Conclusion: MM-Food-100K是一个高质量、可溯源的食品数据集，公开部分免费，其余保留商业用途，为食品智能研究提供了重要资源。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [23] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: We-Math 2.0是一个统一系统，通过结构化数学知识体系、模型中心数据空间建模和强化学习训练范式，提升多模态大语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究在复杂数学推理上表现不足，忽视了知识驱动设计和数据空间建模。

Method: 构建五层数学知识体系、开发标准与专业数据集、提出两阶段强化学习框架、引入全面评估基准。

Result: 在四个基准测试中表现优异，并在MathBookEval上展现出强泛化能力。

Conclusion: We-Math 2.0通过系统化方法显著提升了数学推理能力，具有广泛的应用潜力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>


### [24] [FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs](https://arxiv.org/abs/2508.10467)
*Xueli Pan,Victor de Boer,Jacco van Ossenbruggen*

Main category: cs.AI

TL;DR: FIRESPARQL是一个模块化框架，通过微调LLM和结合RAG与SPARQL查询修正层，解决了LLM在SKG上生成SPARQL查询时的结构不一致和语义不准确问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在SKG上生成SPARQL查询时的结构不一致和语义不准确问题。

Method: 提出FIRESPARQL框架，结合微调LLM、RAG和SPARQL查询修正层，并在SciQA Benchmark上评估不同配置的性能。

Result: 微调配置表现最佳，ROUGE-L达到0.90，RelaxedEM达到0.85。

Conclusion: FIRESPARQL通过微调LLM显著提升了在SKG上的问答性能。

Abstract: Question answering over Scholarly Knowledge Graphs (SKGs) remains a
challenging task due to the complexity of scholarly content and the intricate
structure of these graphs. Large Language Model (LLM) approaches could be used
to translate natural language questions (NLQs) into SPARQL queries; however,
these LLM-based approaches struggle with SPARQL query generation due to limited
exposure to SKG-specific content and the underlying schema. We identified two
main types of errors in the LLM-generated SPARQL queries: (i) structural
inconsistencies, such as missing or redundant triples in the queries, and (ii)
semantic inaccuracies, where incorrect entities or properties are shown in the
queries despite a correct query structure. To address these issues, we propose
FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core
component, with optional context provided via retrieval-augmented generation
(RAG) and a SPARQL query correction layer. We evaluate the framework on the
SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,
one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance
with baseline and state-of-the-art approaches. We measure query accuracy using
BLEU and ROUGE metrics, and query result accuracy using relaxed exact
match(RelaxedEM), with respect to the gold standards containing the NLQs,
SPARQL queries, and the results of the queries. Experimental results
demonstrate that fine-tuning achieves the highest overall performance, reaching
0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the
test set.

</details>


### [25] [SEQ-GPT: LLM-assisted Spatial Query via Example](https://arxiv.org/abs/2508.10486)
*Ivan Khai Ze Lim,Ningyi Liao,Yiming Yang,Gerald Wei Yong Yip,Siqiang Luo*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型语言模型（LLM）的空间查询系统SEQ-GPT，用于通过自然语言进行更灵活的空间范例查询（SEQ）。


<details>
  <summary>Details</summary>
Motivation: 现有空间服务（如在线地图）主要依赖用户查询进行位置搜索，但在执行复杂任务（如同时搜索多个相关位置）时用户体验有限。

Method: 引入SEQ-GPT系统，利用LLM的自然语言能力实现交互式SEQ搜索，包括澄清查询细节和动态调整搜索。提出了一种定制化的LLM适配流程，通过对话合成和多模型协作将自然语言与结构化空间数据对齐。

Result: SEQ-GPT展示了通过自然语言扩展空间搜索的端到端解决方案，适用于实际数据和场景。

Conclusion: SEQ-GPT为复杂空间查询提供了更灵活和交互式的解决方案，提升了用户体验。

Abstract: Contemporary spatial services such as online maps predominantly rely on user
queries for location searches. However, the user experience is limited when
performing complex tasks, such as searching for a group of locations
simultaneously. In this study, we examine the extended scenario known as
Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly
searched based on user-specified examples. We introduce SEQ-GPT, a spatial
query system powered by Large Language Models (LLMs) towards more versatile SEQ
search using natural language. The language capabilities of LLMs enable unique
interactive operations in the SEQ process, including asking users to clarify
query details and dynamically adjusting the search based on user feedback. We
also propose a tailored LLM adaptation pipeline that aligns natural language
with structured spatial data and queries through dialogue synthesis and
multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for
broadening spatial search with realistic data and application scenarios.

</details>


### [26] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出了一种新的AI主导的诊断范式DxDirector-7B，能够从模糊主诉开始驱动全流程诊断，显著减少医生工作量并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI在临床诊断中仅作为医生助手，无法驱动全流程诊断，限制了其减轻医生负担和提升效率的潜力。

Method: 提出DxDirector-7B，一种具备深度思考能力的LLM，能够主导诊断流程并建立责任框架。

Result: 在罕见、复杂和真实案例中，DxDirector-7B显著优于现有医疗LLM和通用LLM，减少医生工作量并提高准确性。

Conclusion: DxDirector-7B标志着AI从助手转变为诊断主导者，为高效准确的诊断提供了新解决方案。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [27] [PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning](https://arxiv.org/abs/2508.10501)
*Yushi Feng,Junye Du,Yingying Hong,Qifan Wang,Lequan Yu*

Main category: cs.AI

TL;DR: PASS是一个多模态框架，解决了现有工具增强代理系统在医疗领域中的黑盒推理、多模态整合不足和计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有系统在医疗任务中存在黑盒推理、多模态整合不足和计算效率低的问题，限制了其实际应用。

Method: PASS通过自适应采样代理工作流，利用概率注释路径，结合多工具图和三阶段训练方法优化性能与成本。

Result: PASS在多个指标上显著优于基线，同时平衡计算成本。

Conclusion: PASS为医疗代理系统提供了可解释、自适应和多模态的新范式。

Abstract: Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.

</details>


### [28] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 论文探讨了语言模型（LM）对齐人类偏好的方法，发现动态数据（on-policy）与静态数据的效果差异显著，并提出对齐阶段假设来解释这一现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于优化LM对齐方法，以更高效地利用静态和动态偏好数据，提升模型性能。

Method: 提出对齐阶段假设，将对齐过程分为偏好注入和偏好微调两个阶段，并通过实验验证其有效性。

Result: 实验表明，动态数据在不同模型上的效果差异显著（如Llama-3效果提升3倍，Zephyr降低0.4倍），验证了假设的普适性。

Conclusion: 结论指出，对齐阶段假设为LM对齐提供了新视角，并提出了有效的阶段边界识别算法。

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [29] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 论文提出ComMCS方法，通过结合蒙特卡洛采样步骤的估计器，降低方差，提升大语言模型在数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂领域（如数学）的推理能力仍有不足，现有方法因蒙特卡洛采样成本高导致估计误差。

Method: 提出ComMCS方法，线性组合当前和后续步骤的蒙特卡洛估计器，降低方差且保持无偏估计。

Result: 在MATH-500和GSM8K基准测试中，ComMCS优于回归优化方法2.8分，优于基线方法2.2分。

Conclusion: ComMCS有效降低估计误差，提升推理性能，且无需额外计算成本。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


### [30] [MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models](https://arxiv.org/abs/2508.10599)
*Xinyan Jiang,Lin Zhang,Jiayi Zhang,Qingsong Yang,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: MSRS通过正交子空间分配和混合子空间组合策略，有效减少多属性控制中的干扰，提升模型行为调控的精确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多属性联合调控时存在干扰和权衡问题，MSRS旨在解决这一问题。

Method: 采用正交子空间分配和混合子空间组合策略，结合动态权重函数和令牌级调控机制。

Result: 实验显示MSRS显著减少属性冲突，优于现有方法，并能泛化到多样化任务。

Conclusion: MSRS为多属性调控提供了一种高效且精确的解决方案。

Abstract: Activation steering offers a promising approach to controlling the behavior
of Large Language Models by directly manipulating their internal activations.
However, most existing methods struggle to jointly steer multiple attributes,
often resulting in interference and undesirable trade-offs. To address this
challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel
framework for effective multi-attribute steering via subspace representation
fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal
subspaces to each attribute, isolating their influence within the model's
representation space. MSRS also incorporates a hybrid subspace composition
strategy: it combines attribute-specific subspaces for unique steering
directions with a shared subspace for common steering directions. A dynamic
weighting function learns to efficiently integrate these components for precise
control. During inference, MSRS introduces a token-level steering mechanism
that dynamically identifies and intervenes on the most semantically relevant
tokens, enabling fine-grained behavioral modulation. Experimental results show
that MSRS significantly reduces attribute conflicts, surpasses existing methods
across a range of attributes, and generalizes effectively to diverse downstream
tasks.

</details>


### [31] [STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation](https://arxiv.org/abs/2508.10669)
*Zhenye Yang,Jinpeng Chen,Huan Li,Xiongnan Jin,Xuanyang Li,Junwei Zhang,Hongbo Gao,Kaimin Wei,Senzhang Wang*

Main category: cs.AI

TL;DR: STEP是一种基于预训练语言模型的对话推荐系统，通过课程引导的上下文-知识融合和轻量级任务特定提示调优，解决了现有系统在捕捉用户偏好深度语义和对话上下文方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统难以有效整合外部知识图谱信息，导致推荐结果与用户期望不符。

Method: STEP采用三阶段课程逐步对齐对话上下文与知识图谱实体，并通过双重提示（对话前缀和推荐前缀）注入冻结的语言模型。

Result: 在两个公开数据集上，STEP在推荐精度和对话质量上优于主流方法。

Conclusion: STEP通过课程引导的上下文-知识融合和双重提示调优，显著提升了对话推荐系统的性能。

Abstract: Conversational recommender systems (CRSs) aim to proactively capture user
preferences through natural language dialogue and recommend high-quality items.
To achieve this, CRS gathers user preferences via a dialog module and builds
user profiles through a recommendation module to generate appropriate
recommendations. However, existing CRS faces challenges in capturing the deep
semantics of user preferences and dialogue context. In particular, the
efficient integration of external knowledge graph (KG) information into
dialogue generation and recommendation remains a pressing issue. Traditional
approaches typically combine KG information directly with dialogue content,
which often struggles with complex semantic relationships, resulting in
recommendations that may not align with user expectations.
  To address these challenges, we introduce STEP, a conversational recommender
centered on pre-trained language models that combines curriculum-guided
context-knowledge fusion with lightweight task-specific prompt tuning. At its
heart, an F-Former progressively aligns the dialogue context with
knowledge-graph entities through a three-stage curriculum, thus resolving
fine-grained semantic mismatches. The fused representation is then injected
into the frozen language model via two minimal yet adaptive prefix prompts: a
conversation prefix that steers response generation toward user intent and a
recommendation prefix that biases item ranking toward knowledge-consistent
candidates. This dual-prompt scheme allows the model to share cross-task
semantics while respecting the distinct objectives of dialogue and
recommendation. Experimental results show that STEP outperforms mainstream
methods in the precision of recommendation and dialogue quality in two public
datasets.

</details>


### [32] [GenOM: Ontology Matching with Description Generation and Large Language Model](https://arxiv.org/abs/2508.10703)
*Yiping Song,Jiaoyan Chen,Renate A. Schmidt*

Main category: cs.AI

TL;DR: GenOM是一个基于大型语言模型（LLM）的本体对齐框架，通过生成文本定义丰富本体概念的语义表示，利用嵌入模型检索对齐候选，并结合精确匹配工具提高精度。在OAEI Bio-ML测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学领域异构知识源之间的语义互操作性和集成问题，该领域包含大量复杂概念。

Method: 使用LLM生成文本定义丰富语义表示，嵌入模型检索候选对齐，结合精确匹配工具。

Result: 在OAEI Bio-ML测试中表现优于传统OM系统和近期LLM方法。

Conclusion: GenOM框架展示了语义丰富和少样本提示的有效性，具有鲁棒性和适应性。

Abstract: Ontology matching (OM) plays an essential role in enabling semantic
interoperability and integration across heterogeneous knowledge sources,
particularly in the biomedical domain which contains numerous complex concepts
related to diseases and pharmaceuticals. This paper introduces GenOM, a large
language model (LLM)-based ontology alignment framework, which enriches the
semantic representations of ontology concepts via generating textual
definitions, retrieves alignment candidates with an embedding model, and
incorporates exact matching-based tools to improve precision. Extensive
experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often
achieve competitive performance, surpassing many baselines including
traditional OM systems and recent LLM-based methods. Further ablation studies
confirm the effectiveness of semantic enrichment and few-shot prompting,
highlighting the framework's robustness and adaptability.

</details>


### [33] [Agentic Design Review System](https://arxiv.org/abs/2508.10745)
*Sayan Nag,K J Joseph,Koustava Goswami,Vlad I Morariu,Balaji Vasan Srinivasan*

Main category: cs.AI

TL;DR: 提出了一种多智能体协作的设计评审系统（AgenticDRS），通过图匹配和提示扩展方法提升评审效果，并在DRS-BENCH基准上验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图形设计评审需要多角度评估，但目前缺乏系统化的方法。本文旨在通过多智能体协作解决这一问题。

Method: 提出AgenticDRS系统，利用图匹配和提示扩展方法使智能体具备设计感知能力，并通过元智能体协调多智能体协作。

Result: 实验表明，AgenticDRS在DRS-BENCH基准上优于现有方法，并能生成可操作的反馈。

Conclusion: AgenticDRS为图形设计评审提供了一种有效方法，并呼吁关注这一实用但未充分探索的研究方向。

Abstract: Evaluating graphic designs involves assessing it from multiple facets like
alignment, composition, aesthetics and color choices. Evaluating designs in a
holistic way involves aggregating feedback from individual expert reviewers.
Towards this, we propose an Agentic Design Review System (AgenticDRS), where
multiple agents collaboratively analyze a design, orchestrated by a meta-agent.
A novel in-context exemplar selection approach based on graph matching and a
unique prompt expansion method plays central role towards making each agent
design aware. Towards evaluating this framework, we propose DRS-BENCH
benchmark. Thorough experimental evaluation against state-of-the-art baselines
adapted to the problem setup, backed-up with critical ablation experiments
brings out the efficacy of Agentic-DRS in evaluating graphic designs and
generating actionable feedback. We hope that this work will attract attention
to this pragmatic, yet under-explored research direction.

</details>


### [34] [Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning](https://arxiv.org/abs/2508.10747)
*Sangwoo Jeon,Juchul Shin,Gyeong-Tae Kim,YeonJe Cho,Seongwoo Kim*

Main category: cs.AI

TL;DR: 提出了一种稀疏、目标感知的GNN表示方法，解决了传统全连接图在大型规划问题中的组合爆炸和信息稀疏问题，显著提升了策略泛化能力和成功率。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用全连接图表示规划状态，导致信息稀疏和内存需求爆炸，难以应对大规模问题。

Method: 采用稀疏、目标感知的GNN表示，选择性编码局部相关关系并显式整合目标空间特征。

Result: 实验表明，该方法能有效扩展到更大规模的网格环境，显著提升策略泛化和成功率。

Conclusion: 为大规模广义规划任务提供了实用基础。

Abstract: Generalized planning using deep reinforcement learning (RL) combined with
graph neural networks (GNNs) has shown promising results in various symbolic
planning domains described by PDDL. However, existing approaches typically
represent planning states as fully connected graphs, leading to a combinatorial
explosion in edge information and substantial sparsity as problem scales grow,
especially evident in large grid-based environments. This dense representation
results in diluted node-level information, exponentially increases memory
requirements, and ultimately makes learning infeasible for larger-scale
problems. To address these challenges, we propose a sparse, goal-aware GNN
representation that selectively encodes relevant local relationships and
explicitly integrates spatial features related to the goal. We validate our
approach by designing novel drone mission scenarios based on PDDL within a grid
world, effectively simulating realistic mission execution environments. Our
experimental results demonstrate that our method scales effectively to larger
grid sizes previously infeasible with dense graph representations and
substantially improves policy generalization and success rates. Our findings
provide a practical foundation for addressing realistic, large-scale
generalized planning tasks.

</details>


### [35] [Modeling Human Responses to Multimodal AI Content](https://arxiv.org/abs/2508.10769)
*Zhiqi Shen,Shaojing Fan,Danni Xu,Terence Sim,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 论文研究了AI生成内容对人类行为和感知的影响，提出了MhAIM数据集和T-Lens系统，通过量化信任度、影响力和开放性等新指标，增强LLM的人类感知能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的普及，其带来的错误信息风险增加。现有研究多关注内容真实性，而较少探讨其对人类行为和感知的影响。

Method: 引入MhAIM数据集（154,552条在线帖子，其中111,153条为AI生成），提出三个新指标（信任度、影响力和开放性），并开发T-Lens系统（基于HR-MCP协议）。

Result: 研究发现，当帖子包含文本和视觉内容时，人们更容易识别AI生成内容（尤其是两者不一致时）。T-Lens系统能更好地预测人类反应。

Conclusion: 研究为LLM提供了人类感知能力的实证工具，揭示了AI、人类认知和信息接收的复杂关系，提出了缓解AI错误信息风险的可操作策略。

Abstract: As AI-generated content becomes widespread, so does the risk of
misinformation. While prior research has primarily focused on identifying
whether content is authentic, much less is known about how such content
influences human perception and behavior. In domains like trading or the stock
market, predicting how people react (e.g., whether a news post will go viral),
can be more critical than verifying its factual accuracy. To address this, we
take a human-centered approach and introduce the MhAIM Dataset, which contains
154,552 online posts (111,153 of them AI-generated), enabling large-scale
analysis of how people respond to AI-generated content. Our human study reveals
that people are better at identifying AI content when posts include both text
and visuals, particularly when inconsistencies exist between the two. We
propose three new metrics: trustworthiness, impact, and openness, to quantify
how users judge and engage with online content. We present T-Lens, an LLM-based
agent system designed to answer user queries by incorporating predicted human
responses to multimodal information. At its core is HR-MCP (Human Response
Model Context Protocol), built on the standardized Model Context Protocol
(MCP), enabling seamless integration with any LLM. This integration allows
T-Lens to better align with human reactions, enhancing both interpretability
and interaction capabilities. Our work provides empirical insights and
practical tools to equip LLMs with human-awareness capabilities. By
highlighting the complex interplay among AI, human cognition, and information
reception, our findings suggest actionable strategies for mitigating the risks
of AI-driven misinformation.

</details>


### [36] [The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference](https://arxiv.org/abs/2508.10777)
*Maël Jullien,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: 论文通过临床试验自然语言推理基准测试，发现当前大语言模型虽能准确获取知识，但在结构化推理任务上表现不佳，揭示了其内部表征的局限性。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否通过数据和参数扩展获得结构化、可泛化的内部表征。

Method: 引入临床试验自然语言推理基准测试（四种推理类型），并设计GKMRV探针以区分知识获取与推理失败。评估六种当代大语言模型在直接和思维链提示下的表现。

Result: 模型在GKMRV任务上准确率接近天花板（0.918），但在主要推理任务上表现差（0.25）。输出推理高度一致（0.87），表明其依赖启发式和捷径。

Conclusion: 当前大语言模型虽具备临床知识，但缺乏结构化表征能力，无法可靠整合约束、权衡证据或模拟反事实。GKMRV框架为高风险领域模型可靠性提供了有效评估方法。

Abstract: Large language models are often assumed to acquire increasingly structured,
generalizable internal representations simply by scaling data and parameters.
We interrogate this assumption by introducing a Clinical Trial Natural Language
Inference benchmark comprising four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction.
Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning
Verification (GKMRV) probe, allowing us to dissociate failures of factual
access from failures of inference. We evaluate six contemporary LLMs under both
direct and chain of thought prompting.
  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform
poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,
output inferences are highly consistent across samples (mean 0.87), indicating
a systematic application of underlying heuristics and shortcuts.
  These results reveal fundamental structural and representational limitations:
current LLMs often possess the relevant clinical knowledge but lack the
structured, composable internal representations needed to deploy it reliably
(e.g., integrating constraints, weighing evidence, or simulating
counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this
dissociation explicit and measurable, providing an effective framework for
probing the reliability of LLMs in high-stakes domains.

</details>


### [37] [Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems](https://arxiv.org/abs/2508.10806)
*Maria J. P. Peixoto,Akriti Pandey,Ahsan Zaman,Peter R. Lewis*

Main category: cs.AI

TL;DR: 论文探讨了可解释AI（XAI）的可访问性问题，特别是针对视觉障碍用户，提出了一个四部分的方法论框架，并发现简化解释和多模态呈现更有效。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在关键领域决策中的应用增加，可解释性成为提升用户理解和决策能力的手段，但XAI的可访问性（尤其是对视觉障碍用户）研究不足。

Method: 通过文献综述（79项研究）和四部分方法论（分类AI系统、定义人物角色、原型设计与实现、专家与用户评估）进行研究。

Result: 初步结果表明，简化解释比详细解释更易于非视觉用户理解，多模态呈现有助于提升公平的可解释性。

Conclusion: XAI设计需更多关注可访问性，简化解释和多模态呈现是提升视觉障碍用户体验的关键。

Abstract: As AI systems are increasingly deployed to support decision-making in
critical domains, explainability has become a means to enhance the
understandability of these outputs and enable users to make more informed and
conscious choices. However, despite growing interest in the usability of
eXplainable AI (XAI), the accessibility of these methods, particularly for
users with vision impairments, remains underexplored. This paper investigates
accessibility gaps in XAI through a two-pronged approach. First, a literature
review of 79 studies reveals that evaluations of XAI techniques rarely include
disabled users, with most explanations relying on inherently visual formats.
Second, we present a four-part methodological proof of concept that
operationalizes inclusive XAI design: (1) categorization of AI systems, (2)
persona definition and contextualization, (3) prototype design and
implementation, and (4) expert and user assessment of XAI techniques for
accessibility. Preliminary findings suggest that simplified explanations are
more comprehensible for non-visual users than detailed ones, and that
multimodal presentation is required for more equitable interpretability.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [38] [Using nonassociative algebras to classify skew polycyclic codes up to isometry and equivalence](https://arxiv.org/abs/2508.10139)
*Susanne Pumpluen*

Main category: cs.IT

TL;DR: 提出新的斜多环码等价和等距定义，减少已知分类数量，明确不同概念重合条件。


<details>
  <summary>Details</summary>
Motivation: 改进现有斜多环码的分类方法，减少重复代码，提高分类精确性。

Method: 利用斜多环码生成器与主左理想生成器的一一对应关系，通过代数同构保持汉明距离进行分类。

Result: 减少了等价和等距类的数量，明确了分类条件，适用于非结合代数环境。

Conclusion: 新定义提高了斜多环码分类的精确性和适用性，同时支持秩度量线性码的分类。

Abstract: We propose new definitions of equivalence and isometry for skew polycyclic
codes that will lead to tighter classifications than existing ones. This helps
to reduce the number of previously known isometry and equivalence classes, and
state precisely when these different notions coincide. In the process, we
classify classes of skew $(f,\sigma,\delta)$-polycyclic codes with the same
performance parameters, to avoid duplicating already existing codes.
  We exploit that the generator of a skew polycyclic code is in one-one
correspondence with the generator of a principal left ideal in its ambient
algebra. Algebra isomorphisms that preserve the Hamming distance (called
isometries) map generators of principal left ideals to generators of principal
left ideals and preserve length, dimension and Hamming distance of the codes.
We allow the ambient algebras to be nonassociative, thus eliminating the need
on restrictions on the length of the codes. The isometries between the ambient
algebras can also be used to classify corresponding linear codes equipped with
the rank metric.

</details>


### [39] [Space-time Coded Differential Modulation for Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2508.10244)
*Jiawei Qiu,Harry Leib*

Main category: cs.IT

TL;DR: 论文提出了一种结合差分时空调制（DSTM）和差分反射调制（DRM）的方案，以绕过RIS系统中的CSI需求，并提高误码率性能。


<details>
  <summary>Details</summary>
Motivation: RIS技术在6G系统中具有潜力，但CSI获取困难。本文旨在解决这一问题。

Method: 采用基于酉群码的DSTM方案，并与DRM结合，扩展了反射模式数量（K=2,3,4），研究了编解码复杂度。

Result: 仿真结果表明，DRM-DSTM编码系统在准静态瑞利衰落信道中优于未编码的DRM。

Conclusion: DRM-DSTM方案有效解决了RIS系统中的CSI需求问题，并提升了性能。

Abstract: Reconfigurable Intelligent Surfaces (RIS) hold the promise of improving
significantly coverage, as well as spectral and energy efficiency in wireless
communication systems. Techniques based on RIS form a key technology for 6G
systems. An important issue in RIS technology is Channel State Information
(CSI), which is much more difficult to acquire in such systems. This work
introduces a Differential Space-Time Modulation (DSTM) scheme integrated with
Differential Reflecting Modulation (DRM) to bypass the requirement for CSI in
such systems, while providing error rate gains. The DSTM scheme is based on
unitary group codes. We first consider uncoded DRM for RIS to serve as a
reference point. Next we provide an overview of DSTM and outline the procedures
for its integration with DRM. Furthermore, we explore the extension of both the
original DRM and the coded DRM-DSTM scheme to a larger number of RIS reflecting
patterns $K$, and provide tables of codes for $K= 2, 3, 4$. Encoding and
decoding complexities are studied as well. Extensives simulation results over
quasi-static Rayleigh fading channels confirm the effectiveness of the DRM-DSTM
coded system, illustrating its advantages over uncoded DRM with proper system
parameters.

</details>


### [40] [The Conditional Regret-Capacity Theorem for Batch Universal Prediction](https://arxiv.org/abs/2508.10282)
*Marco Bondaschi,Michael Gastpar*

Main category: cs.IT

TL;DR: 本文推导了经典遗憾容量定理的条件版本，用于通用预测中以找到最小批量遗憾的下界，并应用于二元无记忆源类。此外，定理被推广到Rényi信息度量，揭示了条件Rényi散度与条件Sibson互信息之间的深层联系。


<details>
  <summary>Details</summary>
Motivation: 研究如何在批量训练数据可用时，为预测器提供更通用的平均遗憾下界。

Method: 推导条件版本的遗憾容量定理，并将其应用于二元无记忆源类，进一步推广到Rényi信息度量。

Result: 揭示了条件Rényi散度与条件Sibson互信息之间的深层联系，并提供了最小批量遗憾的下界。

Conclusion: 条件遗憾容量定理及其推广为通用预测和信息理论提供了新的工具和见解。

Abstract: We derive a conditional version of the classical regret-capacity theorem.
This result can be used in universal prediction to find lower bounds on the
minimal batch regret, which is a recently introduced generalization of the
average regret, when batches of training data are available to the predictor.
As an example, we apply this result to the class of binary memoryless sources.
Finally, we generalize the theorem to R\'enyi information measures, revealing a
deep connection between the conditional R\'enyi divergence and the conditional
Sibson's mutual information.

</details>


### [41] [Energy-Efficient Index and Code Index Modulations for Spread CPM Signals in Internet of Things](https://arxiv.org/abs/2508.10290)
*Long Yuan,Wenkun Wen,Junlin Liu,Peiran Wu,Minghua Xia*

Main category: cs.IT

TL;DR: 论文提出两种新型调制方案（IM-CPM-SS和CIM-CPM-SS），结合连续相位调制（CPM）与扩频技术（SS），以解决物联网技术中的低功耗、高效频谱利用等需求。


<details>
  <summary>Details</summary>
Motivation: 物联网技术发展面临四大需求：超低功耗、高频谱效率、低成本实现和大规模连接支持。

Method: 提出IM-CPM-SS和CIM-CPM-SS两种方案，分别利用索引调制（IM）和代码索引调制（CIM）优化频谱效率。

Result: 两种方案在误码率、频谱和能量效率等方面优于传统方法，且在非线性放大器条件下表现稳健。

Conclusion: 新方案在保持恒定包络和连续相位优势的同时，显著提升了性能，适用于下行非正交多址（NOMA）系统。

Abstract: The evolution of Internet of Things technologies is driven by four key
demands: ultra-low power consumption, high spectral efficiency, reduced
implementation cost, and support for massive connectivity. To address these
challenges, this paper proposes two novel modulation schemes that integrate
continuous phase modulation (CPM) with spread spectrum (SS) techniques. We
begin by establishing the quasi-orthogonality properties of CPM-SS sequences.
The first scheme, termed IM-CPM-SS, employs index modulation (IM) to select
spreading sequences from the CPM-SS set, thereby improving spectral efficiency
while maintaining the constant-envelope property. The second scheme, referred
to as CIM-CPM-SS, introduces code index modulation (CIM), which partitions the
input bits such that one subset is mapped to phase-shift keying symbols and the
other to CPM-SS sequence indices. Both schemes are applied to downlink
non-orthogonal multiple access (NOMA) systems. We analyze their performance in
terms of bit error rate (BER), spectral and energy efficiency, computational
complexity, and peak-to-average power ratio characteristics under nonlinear
amplifier conditions. Simulation results demonstrate that both schemes
outperform conventional approaches in BER while preserving the benefits of
constant-envelope, continuous-phase signaling. Furthermore, they achieve higher
spectral and energy efficiency and exhibit strong resilience to nonlinear
distortions in downlink NOMA scenarios.

</details>


### [42] [Integrated Communication and Remote Sensing in LEO Satellite Systems: Protocol, Architecture and Prototype](https://arxiv.org/abs/2508.10317)
*Yichao Xu,Xiaoming Chen,Ming Ying,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 提出了一种基于ODDM波形的LEO卫星通信与SAR遥感集成架构，支持实时SAR成像与信息传输。


<details>
  <summary>Details</summary>
Motivation: 解决卫星信道高动态性和载荷处理能力有限的问题，实现通信与遥感功能的集成。

Method: 采用ODDM波形和集成收发器，设计兼容5G NR标准的传输协议，提出统一信号处理框架。

Result: 通过分析与仿真验证了系统性能，并开发了SDR原型验证毫米波频段的有效性。

Conclusion: 该集成系统在实时SAR成像和信息传输方面表现出色，适用于卫星直连用户设备场景。

Abstract: In this paper, we explore the integration of communication and synthetic
aperture radar (SAR)-based remote sensing in low Earth orbit (LEO) satellite
systems to provide real-time SAR imaging and information transmission.
Considering the high-mobility characteristics of satellite channels and limited
processing capabilities of satellite payloads, we propose an integrated
communication and remote sensing architecture based on an orthogonal
delay-Doppler division multiplexing (ODDM) signal waveform. Both communication
and SAR imaging functionalities are achieved with an integrated transceiver
onboard the LEO satellite, utilizing the same waveform and radio frequency (RF)
front-end. Based on such an architecture, we propose a transmission protocol
compatible with the 5G NR standard using downlink pilots for joint channel
estimation and SAR imaging. Furthermore, we design a unified signal processing
framework for the integrated satellite receiver to simultaneously achieve
high-performance channel sensing, low-complexity channel equalization and
interference-free SAR imaging. Finally, the performance of the proposed
integrated system is demonstrated through comprehensive analysis and extensive
simulations in the sub-6 GHz band. Moreover, a software-defined radio (SDR)
prototype is presented to validate its effectiveness for real-time SAR imaging
and information transmission in satellite direct-connect user equipment (UE)
scenarios within the millimeter-wave (mmWave) band.

</details>


### [43] [Predictive Position Control for Movable Antenna Arrays in UAV Communications: A Spatio-Temporal Transformer-LSTM Framework](https://arxiv.org/abs/2508.10720)
*Kan Yu,Kaixuan Li,Xiaowu Liu,Qixun Zhang,Zhiyong Feng*

Main category: cs.IT

TL;DR: 提出了一种预测性MA-UAV协作控制框架，通过Transformer增强的LSTM网络预测天线位置，显著提高了通信可靠性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决城市低空通信中动态障碍和多径效应导致的链路衰减和覆盖盲区问题，同时克服MA技术与UAV移动性之间的速度不匹配问题。

Method: 1. 通过保密率最大化确定最优天线位置；2. 使用Transformer增强的LSTM网络预测未来天线位置。

Result: 仿真显示预测准确性显著提高（NMSE降低超过49%），通信可靠性优于现有基准。

Conclusion: 提出的框架有效解决了MA-UAV协作中的实时链路优化和安全保障问题。

Abstract: In complex urban environments, dynamic obstacles and multipath effects lead
to significant link attenuation and pervasive coverage blind spots.
Conventional approaches based on large-scale fixed antenna arrays and UAV
trajectory optimization struggle to balance energy efficiency, real-time
adaptation, and spatial flexibility. The movable antenna (MA) technology has
emerged as a promising solution, offering enhanced spatial flexibility and
reduced energy consumption to overcome the bottlenecks of urban low-altitude
communications. However, MA deployment faces a critical velocity mismatch
between UAV mobility and mechanical repositioning latency, undermining
real-time link optimization and security assurance. To overcome this, we
propose a predictive MA-UAV collaborative control framework. First, optimal
antenna positions are derived via secrecy rate maximization. Second, a
Transformer-enhanced long short-term memory (LSTM) network predicts future MA
positions by capturing spatio-temporal correlations in antenna trajectories.
Extensive simulations demonstrate superior prediction accuracy (NMSE reduction
exceeds 49\%) and communication reliability versus current popular benchmarks.

</details>


### [44] [MapLibre Tile: A Next Generation Vector Tile Format](https://arxiv.org/abs/2508.10791)
*Markus Tremmel,Roland Zink*

Main category: cs.IT

TL;DR: 论文介绍了MapLibre Tile (MLT)格式，一种新型矢量瓦片规范，旨在解决Mapbox Vector Tile (MVT)的局限性，提供更高的压缩比和解码速度，并支持下一代地图渲染器。


<details>
  <summary>Details</summary>
Motivation: MVT格式已有十年历史，无法完全适应新型地理空间数据源的需求，如数据量激增和AI自动检测。

Method: 设计并实现MLT格式，通过实验模拟用户会话，对比MVT和MLT在压缩比、解码速度和性能上的表现。

Result: MLT在压缩比上比MVT高出三倍，某些大瓦片上甚至超过六倍；解码速度快三倍，处理性能显著提升。

Conclusion: MLT为下一代地图渲染器奠定了基础，支持GPU处理，克服摩尔定律停滞问题。

Abstract: The Mapbox Vector Tile (MVT) format is widely considered the leading open
standard for large-scale map visualization, as evidenced by its widespread
adoption by major technology companies such as AWS, Meta, and Microsoft for
their products and services. However, MVT was developed nearly a decade ago
and, consequently, does not fully align with the capabilities of new geospatial
data sources that are characterized by rapidly increasing data volumes due to
advancements in geospatial sensors and automated detection through artificial
intelligence. In this paper, we introduce the MapLibre Tile (MLT) format, a
novel vector tile specification designed from the ground up to address the
limitations of MVT. Our experiments, simulating user sessions on widely used
basemap datasets, demonstrate that MLT achieves up to three times better
compression ratios compared to MVT on encoded tilesets, with over six times
better on certain large tiles. Additionally, MLT offers decoding speeds that
are up to three times faster and significantly enhances processing performance.
MLT also introduces new functionalities and is specifically designed to lay the
foundation for the next generation of map renderers, which we expect to
entirely offload processing to the GPU, thereby overcoming the stagnation of
Moore`s law.

</details>
