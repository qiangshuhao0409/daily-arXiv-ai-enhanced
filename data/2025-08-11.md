<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.IT](#cs.IT) [Total: 3]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Hierarchical Placement Learning for Network Slice Provisioning](https://arxiv.org/abs/2508.06432)
*Jesutofunmi Ajayi,Antonio Di Maio,Torsten Braun*

Main category: cs.NI

TL;DR: 论文提出了一种基于分层多臂老虎机的解决方案，用于优化边缘移动网络中的切片资源配置，提高请求接受率并降低节点资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘移动网络中切片资源配置的挑战，目标是最大化请求接受率并最小化节点资源利用率。

Method: 采用分层多臂老虎机问题框架，提出一种两级分层老虎机解决方案，以在线方式学习可扩展的资源配置策略。

Result: 在两种真实网络拓扑上的仿真显示，该方法平均节点资源利用率为5%，在某些场景下比基线方法多接受25%的切片请求。

Conclusion: 所提出的分层老虎机方法能有效优化切片资源配置，显著提升网络性能。

Abstract: In this work, we aim to address the challenge of slice provisioning in
edge-based mobile networks. We propose a solution that learns a service
function chain placement policy for Network Slice Requests, to maximize the
request acceptance rate, while minimizing the average node resource
utilization. To do this, we consider a Hierarchical Multi-Armed Bandit problem
and propose a two-level hierarchical bandit solution which aims to learn a
scalable placement policy that optimizes the stated objectives in an online
manner. Simulations on two real network topologies show that our proposed
approach achieves 5% average node resource utilization while admitting over 25%
more slice requests in certain scenarios, compared to baseline methods.

</details>


### [2] [An Online Multi-dimensional Knapsack Approach for Slice Admission Control](https://arxiv.org/abs/2508.06468)
*Jesutofunmi Ajayi,Antonio Di Maio,Torsten Braun,Dimitrios Xenakis*

Main category: cs.NI

TL;DR: 网络切片技术通过共享物理网络基础设施实现多租户通信和服务，但资源需求不确定性是主要挑战。本文提出两种基于预留的在线切片准入控制策略，通过模拟验证其能提升收入并优化资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决网络切片中资源需求不确定性问题，优化长期收入。

Method: 将切片准入控制建模为在线多维背包问题，提出两种预留策略及其算法。

Result: 模拟结果显示，相比先到先服务策略，新策略提升收入12.9%，降低资源消耗1.7%。

Conclusion: 提出的在线准入策略在经济不平等加剧时能显著提升基础设施提供商的收入。

Abstract: Network Slicing has emerged as a powerful technique to enable cost-effective,
multi-tenant communications and services over a shared physical mobile network
infrastructure. One major challenge of service provisioning in slice-enabled
networks is the uncertainty in the demand for the limited network resources
that must be shared among existing slices and potentially new Network Slice
Requests. In this paper, we consider admission control of Network Slice
Requests in an online setting, with the goal of maximizing the long-term
revenue received from admitted requests. We model the Slice Admission Control
problem as an Online Multidimensional Knapsack Problem and present two
reservation-based policies and their algorithms, which have a competitive
performance for Online Multidimensional Knapsack Problems. Through Monte Carlo
simulations, we evaluate the performance of our online admission control method
in terms of average revenue gained by the Infrastructure Provider, system
resource utilization, and the ratio of accepted slice requests. We compare our
approach with those of the online First Come First Serve greedy policy. The
simulation's results prove that our proposed online policies increase revenues
for Infrastructure Providers by up to 12.9 % while reducing the average
resource consumption by up to 1.7% In particular, when the tenants' economic
inequality increases, an Infrastructure Provider who adopts our proposed online
admission policies gains higher revenues compared to an Infrastructure Provider
who adopts First Come First Serve.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: 本文提出了一种结合主动推理原则与大型语言模型（LLMs）的新型框架，用于开发安全的人工通用智能（AGI）。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全方法（如事后可解释性和奖励工程）存在根本性局限，因此需要一种将安全性融入系统核心设计的新方法。

Method: 通过透明信念表示和分层价值对齐，构建多智能体系统，利用自然语言表示和操作信念，实现人类直接监督。

Result: 提出了一种具有安全性保证的架构，包括信念与偏好的明确分离、资源感知的自由能最小化以及模块化智能体结构。

Conclusion: 该框架为AGI开发提供了一条更安全的新路径，并通过ARC基准测试提出了验证安全性的研究议程。

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [4] [A "good regulator theorem" for embodied agents](https://arxiv.org/abs/2508.06326)
*Nathaniel Virgo,Martin Biehl,Manuel Baltieri,Matteo Capucci*

Main category: cs.AI

TL;DR: 论文重新审视了Conant和Ashby的定理，提出了一种更广义的“信念更新”模型概念，强调观察者在模型定义中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 探讨Conant和Ashby定理在更广泛系统中的适用性，并提出一种新的模型定义方式。

Method: 通过观察者视角，将代理的行为解释为“信念更新”，从而定义模型。

Result: 提出了一种更广义的模型概念，适用于经典控制理论和内部状态调节。

Conclusion: 模型的定义依赖于观察者，而非系统本身，解决了原有定理的局限性。

Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a
system must be a model of that system." Artificial Life has produced many
examples of systems that perform tasks with apparently no model in sight; these
suggest Conant and Ashby's theorem doesn't easily generalise beyond its
restricted setup. Nevertheless, here we show that a similar intuition can be
fleshed out in a different way: whenever an agent is able to perform a
regulation task, it is possible for an observer to interpret it as having
"beliefs" about its environment, which it "updates" in response to sensory
input. This notion of belief updating provides a notion of model that is more
sophisticated than Conant and Ashby's, as well as a theorem that is more
broadly applicable. However, it necessitates a change in perspective, in that
the observer plays an essential role in the theory: models are not a mere
property of the system but are imposed on it from outside. Our theorem holds
regardless of whether the system is regulating its environment in a classic
control theory setup, or whether it's regulating its own internal state; the
model is of its environment either way. The model might be trivial, however,
and this is how the apparent counterexamples are resolved.

</details>


### [5] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: 论文提出了一种自适应探索策略优化（AEPO）框架，用于解决多模态大语言模型（MLLMs）在图形用户界面（GUI）中语义对齐的探索问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RLVR）在空间对齐上表现良好，但语义对齐因探索效率低而受限，需要新的优化框架。

Method: 提出AEPO框架，采用多答案生成策略和自适应探索奖励（AER）函数，提升探索效率。

Result: AEPO训练的模型（InfiGUI-G1-3B和InfiGUI-G1-7B）在多个GUI基准测试中达到新SOTA，相对RLVR基线提升达9.0%。

Conclusion: AEPO有效解决了语义对齐的探索瓶颈，为MLLMs在GUI任务中的应用提供了新思路。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


### [6] [Whither symbols in the era of advanced neural networks?](https://arxiv.org/abs/2508.05776)
*Thomas L. Griffiths,Brenden M. Lake,R. Thomas McCoy,Ellie Pavlick,Taylor W. Webb*

Main category: cs.AI

TL;DR: 现代神经网络表现出类似人类思维的组合、创新和快速学习能力，挑战了人类认知基于符号系统的观点，但符号系统在抽象问题中的作用仍需研究。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络是否具备类似人类思维的组合、创新和快速学习能力，从而挑战传统符号系统对人类认知的解释。

Method: 通过分析现代神经网络的能力，并与人类认知过程对比，论证其相似性。

Result: 神经网络表现出类似人类思维的组合、创新和快速学习能力，削弱了符号系统在人类认知中的主导地位。

Conclusion: 提出新的研究方向，探讨符号系统在人类思维抽象问题中的作用，以重新理解人类认知的基础。

Abstract: Some of the strongest evidence that human minds should be thought about in
terms of symbolic systems has been the way they combine ideas, produce novelty,
and learn quickly. We argue that modern neural networks -- and the artificial
intelligence systems built upon them -- exhibit similar abilities. This
undermines the argument that the cognitive processes and representations used
by human minds are symbolic, although the fact that these neural networks are
typically trained on data generated by symbolic systems illustrates that such
systems play an important role in characterizing the abstract problems that
human minds have to solve. This argument leads us to offer a new agenda for
research on the symbolic basis of human thought.

</details>


### [7] [Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making](https://arxiv.org/abs/2508.05792)
*Kausik Lakkaraju,Siva Likitha Valluru,Biplav Srivastava*

Main category: cs.AI

TL;DR: H-XAI是一个统一的框架，结合因果评级与传统XAI方法，支持多方法交互式解释，满足不同利益相关者的需求。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法主要服务于开发者，未能满足多样化的利益相关者需求。

Method: H-XAI整合因果评级与传统XAI方法，支持交互式假设测试，并提供实例级和全局解释。

Result: 通过两个案例研究（信用风险分类和金融时间序列预测）验证了H-XAI的通用性。

Conclusion: H-XAI填补了现有XAI方法的空白，通过结合因果评级和后验解释，满足利益相关者在个体和模型层面的需求。

Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing
on justifying model outputs rather than supporting diverse stakeholder needs. A
recent shift toward Evaluative AI reframes explanation as a tool for hypothesis
testing, but still focuses primarily on operational organizations. We introduce
Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods
with traditional XAI methods to support explanation as an interactive,
multi-method process. H-XAI allows stakeholders to ask a series of questions,
test hypotheses, and compare model behavior against automatically constructed
random and biased baselines. It combines instance-level and global
explanations, adapting to each stakeholder's goals, whether understanding
individual decisions, assessing group-level bias, or evaluating robustness
under perturbations. We demonstrate the generality of our approach through two
case studies spanning six scenarios: binary credit risk classification and
financial time-series forecasting. H-XAI fills critical gaps left by existing
XAI methods by combining causal ratings and post-hoc explanations to answer
stakeholder-specific questions at both the individual decision level and the
overall model level.

</details>


### [8] [Safety of Embodied Navigation: A Survey](https://arxiv.org/abs/2508.05855)
*Zixia Wang,Jia Hu,Ronghui Mu*

Main category: cs.AI

TL;DR: 该论文综述了具身导航中的安全问题，分析了攻击策略、防御机制和评估方法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和具身AI的发展，具身导航在关键应用中的安全问题日益突出，需确保其安全性。

Method: 通过全面分析现有安全挑战、缓解技术、数据集和评估指标，探讨未解决问题和未来方向。

Result: 提供了具身导航安全的多视角分析，包括潜在攻击方法、缓解策略和更可靠的评估技术。

Conclusion: 该研究为开发更安全可靠的具身导航系统提供了指导，并对提升社会安全和工业效率有广泛意义。

Abstract: As large language models (LLMs) continue to advance and gain influence, the
development of embodied AI has accelerated, drawing significant attention,
particularly in navigation scenarios. Embodied navigation requires an agent to
perceive, interact with, and adapt to its environment while moving toward a
specified target in unfamiliar settings. However, the integration of embodied
navigation into critical applications raises substantial safety concerns. Given
their deployment in dynamic, real-world environments, ensuring the safety of
such systems is critical. This survey provides a comprehensive analysis of
safety in embodied navigation from multiple perspectives, encompassing attack
strategies, defense mechanisms, and evaluation methodologies. Beyond conducting
a comprehensive examination of existing safety challenges, mitigation
technologies, and various datasets and metrics that assess effectiveness and
robustness, we explore unresolved issues and future research directions in
embodied navigation safety. These include potential attack methods, mitigation
strategies, more reliable evaluation techniques, and the implementation of
verification frameworks. By addressing these critical gaps, this survey aims to
provide valuable insights that can guide future research toward the development
of safer and more reliable embodied navigation systems. Furthermore, the
findings of this study have broader implications for enhancing societal safety
and increasing industrial efficiency.

</details>


### [9] [Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning](https://arxiv.org/abs/2508.05888)
*Sahil Bansal,Sai Shruthi Sistla,Aarti Arikatala,Sebastian Schreiber*

Main category: cs.AI

TL;DR: 论文提出了一种基于知识图谱（KG）的工具检索框架，通过捕捉工具间的语义关系和功能依赖，显著提高了多步任务中的工具检索准确率。


<details>
  <summary>Details</summary>
Motivation: 传统工具检索方法主要依赖用户查询与工具描述的相似性，难以处理多步用户请求，因此需要一种更全面的方法。

Method: 采用知识图谱框架，利用1-hop ego工具图的集合建模工具间的直接和间接连接，实现更全面的工具选择。

Result: 在合成数据集上，该方法在Complete Recall指标上达到91.85%的工具覆盖率，优于非KG基线方法的89.26%。

Conclusion: 知识图谱的结构信息为纯相似性匹配提供了补充信号，特别适用于需要顺序工具组合的查询。

Abstract: Effective tool retrieval is essential for AI agents to select from a vast
array of tools when identifying and planning actions in the context of complex
user queries. Despite its central role in planning, this aspect remains
underexplored in the literature. Traditional approaches rely primarily on
similarities between user queries and tool descriptions, which significantly
limits retrieval accuracy, specifically when handling multi-step user requests.
To address these limitations, we propose a Knowledge Graph (KG)-based tool
retrieval framework that captures the semantic relationships between tools and
their functional dependencies. Our retrieval algorithm leverages ensembles of
1-hop ego tool graphs to model direct and indirect connections between tools,
enabling more comprehensive and contextual tool selection for multi-step tasks.
We evaluate our approach on a synthetically generated internal dataset across
six defined user classes, extending previous work on coherent dialogue
synthesis and too retrieval benchmarks. Results demonstrate that our tool
graph-based method achieves 91.85% tool coverage on the micro-average Complete
Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid
retrieval, the strongest non-KG baseline in our experiments. These findings
support our hypothesis that the structural information in the KG provides
complementary signals to pure similarity matching, particularly for queries
requiring sequential tool composition.

</details>


### [10] [Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making](https://arxiv.org/abs/2508.05996)
*Kaitao Chen,Mianxin Liu,Daoming Zong,Chaoyue Ding,Shaohao Rui,Yankai Jiang,Mu Zhou,Xiaosong Wang*

Main category: cs.AI

TL;DR: 提出MedOrch框架，通过LLM调解多模态医疗决策中的多代理协作，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有多代理研究主要针对语言任务，多模态场景扩展困难，且视觉语言模型（VLMs）在指令跟随和自反思能力上较弱，限制了协作能力。

Method: 提出MedOrch框架，利用LLM调解多个VLM专家代理的协作，交换并反思输出。采用开源通用和领域专用VLMs。

Result: 在五个医疗视觉问答基准上验证，协作性能超越单个代理，无需模型训练。

Conclusion: 调解引导的多代理协作可推动医疗多模态智能发展。

Abstract: Complex medical decision-making involves cooperative workflows operated by
different clinicians. Designing AI multi-agent systems can expedite and augment
human-level clinical decision-making. Existing multi-agent researches primarily
focus on language-only tasks, yet their extension to multimodal scenarios
remains challenging. A blind combination of diverse vision-language models
(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are
less capable in instruction following and importantly self-reflection, compared
to large language models (LLMs) of comparable sizes. This disparity largely
constrains VLMs' ability in cooperative workflows. In this study, we propose
MedOrch, a mediator-guided multi-agent collaboration framework for medical
multimodal decision-making. MedOrch employs an LLM-based mediator agent that
enables multiple VLM-based expert agents to exchange and reflect on their
outputs towards collaboration. We utilize multiple open-source general-purpose
and domain-specific VLMs instead of costly GPT-series models, revealing the
strength of heterogeneous models. We show that the collaboration within
distinct VLM-based agents can surpass the capabilities of any individual agent.
We validate our approach on five medical vision question answering benchmarks,
demonstrating superior collaboration performance without model training. Our
findings underscore the value of mediator-guided multi-agent collaboration in
advancing medical multimodal intelligence. Our code will be made publicly
available.

</details>


### [11] [Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning](https://arxiv.org/abs/2508.06042)
*Daechul Ahn,San Kim,Jonghyun Choi*

Main category: cs.AI

TL;DR: 提出了一种分层多智能体框架HIMA，结合专家模仿学习和元控制器，用于动态长时任务（如《星际争霸II》），显著提升了战略清晰度、适应性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在动态长时任务（如《星际争霸II》）中表现不佳，难以适应部分可观测环境和资源约束。

Method: 采用分层多智能体框架HIMA，包括专家模仿学习智能体和元控制器Strategic Planner（SP），通过专家演示学习特定策略并生成连贯的多步动作序列。

Result: HIMA在战略清晰度、适应性和计算效率上优于现有方法，验证了结合专家模仿模块与元级协调的潜力。

Conclusion: HIMA框架展示了结合专家模仿学习和元级协调在开发更鲁棒通用AI智能体方面的潜力。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.

</details>


### [12] [LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences](https://arxiv.org/abs/2508.06060)
*Sankarshan Damle,Boi Faltings*

Main category: cs.AI

TL;DR: 论文提出了一个双用途框架，利用参与式预算（PB）来评估LLMs在资源分配和推理能力方面的表现，并测试其从非结构化输入中提取偏好的能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在结构化资源分配任务中的能力，并解决现有评估方法因数据污染和静态基准而受限的问题。

Method: 使用三种提示策略（贪婪选择、直接优化和启发式改进）让LLMs在预算约束下选择项目子集，并比较其分配结果与效用最大化基准。

Result: 结果表明提示设计对LLMs表现至关重要，且LLMs在从非结构化输入中提取偏好方面具有潜力。

Conclusion: LLMs在机制设计和非结构化输入处理方面展现出前景，提示设计是关键因素。

Abstract: Large Language Models (LLMs) are increasingly expected to handle complex
decision-making tasks, yet their ability to perform structured resource
allocation remains underexplored. Evaluating their reasoning is also difficult
due to data contamination and the static nature of existing benchmarks. We
present a dual-purpose framework leveraging Participatory Budgeting (PB) both
as (i) a practical setting for LLM-based resource allocation and (ii) an
adaptive benchmark for evaluating their reasoning capabilities. We task LLMs
with selecting project subsets under feasibility (e.g., budget) constraints via
three prompting strategies: greedy selection, direct optimization, and a
hill-climbing-inspired refinement. We benchmark LLMs' allocations against a
utility-maximizing oracle. Interestingly, we also test whether LLMs can infer
structured preferences from natural-language voter input or metadata, without
explicit votes. By comparing allocations based on inferred preferences to those
from ground-truth votes, we evaluate LLMs' ability to extract preferences from
open-ended input. Our results underscore the role of prompt design and show
that LLMs hold promise for mechanism design with unstructured inputs.

</details>


### [13] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: 论文呼吁重视认知想象力在人工智能中的关键作用，并提出语义模型作为模拟认知想象力的工具。


<details>
  <summary>Details</summary>
Motivation: 认知想象力在人类思维中扮演重要角色，但目前在AI领域被低估，导致AI能力受限。

Method: 提出语义模型，一种基于概率因果关系的新型数学模型，能够学习和确保想象上下文的连贯性。

Result: 语义模型能够模拟认知想象力，支持连贯的推理和决策。

Conclusion: 认知想象力是AI领域的重要突破方向，语义模型为此提供了可行工具。

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [14] [A Generic Complete Anytime Beam Search for Optimal Decision Tree](https://arxiv.org/abs/2508.06064)
*Harold Silvère Kiossou,Siegfried Nijssen,Pierre Schaus*

Main category: cs.AI

TL;DR: 本文提出了一种通用的、完整的、随时可用的光束搜索算法CA-DL8.5，用于优化决策树分类误差。该算法扩展了DL8.5框架，并统一了现有的随时策略，通过模块化设计整合多种启发式和松弛机制。实验表明，CA-DL8.5在随时性能上优于其他变体和Blossom算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在随时性能上表现不佳，缺乏系统比较，因此需要一种通用框架来整合和评估不同启发式策略。

Method: CA-DL8.5结合了DL8.5的高效分支定界剪枝和基于Trie的缓存，采用重启式光束搜索逐步放松剪枝标准。

Result: 实验结果显示，基于LDS启发式的CA-DL8.5在随时性能上表现最佳，优于其他变体和Blossom算法。

Conclusion: CA-DL8.5提供了一个通用框架，能够整合多种启发式策略，并在保持最优性和完整性的同时提升随时性能。

Abstract: Finding an optimal decision tree that minimizes classification error is known
to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic
programming guarantee optimality, they often suffer from poor anytime behavior
-- meaning they struggle to find high-quality decision trees quickly when the
search is stopped before completion -- due to unbalanced search space
exploration. To address this, several anytime extensions of exact methods have
been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not
been systematically compared, making it difficult to assess their relative
effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and
anytime beam search algorithm that extends the DL8.5 framework and unifies some
existing anytime strategies. In particular, CA-DL8.5 generalizes previous
approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various
heuristics and relaxation mechanisms through a modular design. The algorithm
reuses DL8.5's efficient branch-and-bound pruning and trie-based caching,
combined with a restart-based beam search that gradually relaxes pruning
criteria to improve solution quality over time. Our contributions are twofold:
(1) We introduce this new generic framework for exact and anytime decision tree
learning, enabling the incorporation of diverse heuristics and search
strategies; (2) We conduct a rigorous empirical comparison of several
instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k
heuristics -- using an anytime evaluation metric called the primal gap
integral. Experimental results on standard classification benchmarks show that
CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime
performance, outperforming both other CA-DL8.5 variants and the Blossom
algorithm while maintaining completeness and optimality guarantees.

</details>


### [15] [ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception](https://arxiv.org/abs/2508.06074)
*Siyi Lu,Run Liu,Dongsheng Yang,Lei He*

Main category: cs.AI

TL;DR: 论文提出了一种基于深度强化学习（DRL）和鸟瞰图（BEV）感知的新型自动驾驶方法，结合高效的时空特征提取网络Mamba-BEV，显著提升了实时决策性能。


<details>
  <summary>Details</summary>
Motivation: 传统模块化方法存在误差传播和协调问题，而端到端学习系统面临计算瓶颈。本文旨在通过结合BEV感知和DRL，解决这些问题。

Method: 提出Mamba-BEV模型，结合BEV感知和Mamba框架进行时空特征建模，并进一步开发ME³-BEV框架，用于端到端DRL。

Result: 在CARLA模拟器上的实验表明，ME³-BEV在碰撞率和轨迹准确性等多项指标上优于现有模型。

Conclusion: ME³-BEV为实时自动驾驶提供了一种高效且可解释的解决方案。

Abstract: Autonomous driving systems face significant challenges in perceiving complex
environments and making real-time decisions. Traditional modular approaches,
while offering interpretability, suffer from error propagation and coordination
issues, whereas end-to-end learning systems can simplify the design but face
computational bottlenecks. This paper presents a novel approach to autonomous
driving using deep reinforcement learning (DRL) that integrates bird's-eye view
(BEV) perception for enhanced real-time decision-making. We introduce the
\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction
network that combines BEV-based perception with the Mamba framework for
temporal feature modeling. This integration allows the system to encode vehicle
surroundings and road features in a unified coordinate system and accurately
model long-range dependencies. Building on this, we propose the
\texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a
feature input for end-to-end DRL, achieving superior performance in dynamic
urban driving scenarios. We further enhance the interpretability of the model
by visualizing high-dimensional features through semantic segmentation,
providing insight into the learned representations. Extensive experiments on
the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing
models across multiple metrics, including collision rate and trajectory
accuracy, offering a promising solution for real-time autonomous driving.

</details>


### [16] [Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2](https://arxiv.org/abs/2508.06091)
*Stan P Hauke,Przemysław Andrzej Wałęga*

Main category: cs.AI

TL;DR: 本文解决了图神经网络（GNNs）逻辑表达能力是否完全由C2逻辑描述的问题，证明其表达能力严格超过C2。


<details>
  <summary>Details</summary>
Motivation: 研究GNNs与逻辑语言的关系，解决Barceló等人提出的未解决问题。

Method: 通过理论分析，比较GNNs与C2逻辑的表达能力。

Result: 证明GNNs的逻辑表达能力严格超过C2逻辑，适用于无向和有向图。

Conclusion: 研究不仅解决了GNNs的开放问题，还为无穷逻辑的表达能力提供了新见解。

Abstract: In recent years, there has been growing interest in understanding the
expressive power of graph neural networks (GNNs) by relating them to logical
languages. This research has been been initialised by an influential result of
Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded
fragment of the logic C2), characterises the logical expressiveness of
aggregate-combine GNNs. As a ``challenging open problem'' they left the
question whether full C2 characterises the logical expressiveness of
aggregate-combine-readout GNNs. This question has remained unresolved despite
several attempts. In this paper, we solve the above open problem by proving
that the logical expressiveness of aggregate-combine-readout GNNs strictly
exceeds that of C2. This result holds over both undirected and directed graphs.
Beyond its implications for GNNs, our work also leads to purely logical
insights on the expressive power of infinitary logics.

</details>


### [17] [PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion](https://arxiv.org/abs/2508.06110)
*Yiran Rex Ma*

Main category: cs.AI

TL;DR: PanelTR框架通过LLM代理科学家进行结构化科学推理，无需依赖标注数据或复杂增强，在零样本场景下表现优于普通LLM，媲美全监督模型。


<details>
  <summary>Details</summary>
Motivation: 解决表格推理任务中依赖标注数据或复杂增强的局限性，以及LLM在此类任务中表现不佳的问题。

Method: 引入PanelTR框架，利用五个科学家角色进行个体调查、自我审查和协作同行评审，实现语义级迁移。

Result: 在四个基准测试中，PanelTR优于普通LLM，媲美全监督模型，且无需训练数据。

Conclusion: 结构化科学方法能有效处理复杂任务，具备零样本场景下的灵活语义理解能力。

Abstract: Table reasoning, including tabular QA and fact verification, often depends on
annotated data or complex data augmentation, limiting flexibility and
generalization. LLMs, despite their versatility, often underperform compared to
simple supervised models. To approach these issues, we introduce PanelTR, a
framework utilizing LLM agent scientists for robust table reasoning through a
structured scientific approach. PanelTR's workflow involves agent scientists
conducting individual investigations, engaging in self-review, and
participating in collaborative peer-review discussions. This process, driven by
five scientist personas, enables semantic-level transfer without relying on
data augmentation or parametric optimization. Experiments across four
benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully
supervised models, all while remaining independent of training data. Our
findings indicate that structured scientific methodology can effectively handle
complex tasks beyond table reasoning with flexible semantic understanding in a
zero-shot context.

</details>


### [18] [SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges](https://arxiv.org/abs/2508.06111)
*Dewi S. W. Gould,Bruno Mlodozeniec,Samuel F. Brown*

Main category: cs.AI

TL;DR: SKATE是一种新型评估框架，通过让大语言模型（LLMs）相互生成和解决可验证任务来评估其能力，具有自动化、可扩展和客观性等优势。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法需要大量领域专业知识，难以适应快速发展的模型需求，因此需要一种更通用且可扩展的评估框架。

Method: SKATE将评估视为游戏，模型既作为任务生成者又作为解决者，通过生成可验证任务（如代码输出预测挑战）来相互测试，并使用TrueSkill排名系统评估模型。

Result: 实验表明，较弱模型能可靠区分较强模型，LLMs会生成与自身能力相符的任务，SKATE能自动揭示模型间的细粒度能力差异。

Conclusion: SKATE为通用、可扩展的评估框架提供了重要进展，能够跟上LLM的发展步伐。

Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet
current methods demand extensive domain expertise, hindering their scalability
as these models rapidly evolve. We introduce SKATE: a novel evaluation
framework in which large language models (LLMs) compete by generating and
solving verifiable tasks for one another. Our core insight is to treat
evaluation as a game: models act as both task-setters and solvers, incentivized
to create questions which highlight their own strengths while exposing others'
weaknesses. SKATE offers several key advantages, balancing scalability,
open-endedness, and objectivity. It is fully automated, data-free, and
scalable, requiring no human input or domain expertise. By using verifiable
tasks rather than LLM judges, scoring is objective. Unlike domain-limited
programmatically-generated benchmarks (e.g. chess-playing or spatial
reasoning), having LLMs creatively pose challenges enables open-ended and
scalable evaluation. As a proof of concept, we introduce LLM-set
code-output-prediction (COP) challenges as a verifiable and extensible
framework in which to test our approach. Using a TrueSkill-based ranking
system, we evaluate six frontier LLMs and find that: (1) weaker models can
reliably differentiate and score stronger ones, (2) LLM-based systems are
capable of self-preferencing behavior, generating questions that align with
their own capabilities, and (3) SKATE automatically surfaces fine-grained
capability differences between models. Our findings are an important step
towards general, scalable evaluation frameworks which can keep pace with LLM
progress.

</details>


### [19] [Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem](https://arxiv.org/abs/2508.06129)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux*

Main category: cs.AI

TL;DR: 该研究通过可解释AI分析机器学习模型在车辆路径问题（VRP）中的决策过程，发现某些特征对解决方案质量有显著预测作用，并提出了一个统一框架来评估特征重要性。


<details>
  <summary>Details</summary>
Motivation: 传统元启发式算法依赖人工设计，而机器学习可以挖掘组合优化问题的结构特征，帮助设计更高效的算法。

Method: 使用多种分类器模型进行敏感性分析，预测VRP解决方案质量，并利用可解释AI理解模型决策。

Result: 特征重要性因模型而异，但某些特征始终是强预测因子；提出了一个统一框架来评估特征影响。

Conclusion: 特征重要性分析为开发元启发式算法的指导机制提供了基础，有助于更高效地解决VRP。

Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with
numerous real-world applications, mostly solved using metaheuristic algorithms
due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely
on human-crafted designs developed through empirical studies. However, recent
research shows that machine learning methods can be used the structural
characteristics of solutions in combinatorial optimization, thereby aiding in
designing more efficient algorithms, particularly for solving VRP. Building on
this advancement, this study extends the previous research by conducting a
sensitivity analysis using multiple classifier models that are capable of
predicting the quality of VRP solutions. Hence, by leveraging explainable AI,
this research is able to extend the understanding of how these models make
decisions. Finally, our findings indicate that while feature importance varies,
certain features consistently emerge as strong predictors. Furthermore, we
propose a unified framework able of ranking feature impact across different
scenarios to illustrate this finding. These insights highlight the potential of
feature importance analysis as a foundation for developing a guidance mechanism
of metaheuristic algorithms for solving the VRP.

</details>


### [20] [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](https://arxiv.org/abs/2508.06145)
*Byeonghun Bang,Jongsuk Yoon,Dong-Jin Chang,Seho Park,Yong Oh Lee*

Main category: cs.AI

TL;DR: 研究通过RAG框架增强LLMs在药物禁忌领域的准确性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在医疗领域的应用，特别是在药物禁忌信息准确性方面的挑战。

Method: 采用RAG框架，结合GPT-4o-mini和text-embedding-3-small模型，利用Langchain构建混合检索系统，并整合DUR数据。

Result: 模型准确率从0.49-0.57提升至0.87-0.94。

Conclusion: RAG框架能显著提升LLMs在药物禁忌信息中的可靠性，减少处方决策的不确定性。

Abstract: The versatility of large language models (LLMs) has been explored across
various sectors, but their application in healthcare poses challenges,
particularly in the domain of pharmaceutical contraindications where accurate
and reliable information is required. This study enhances the capability of
LLMs to address contraindications effectively by implementing a Retrieval
Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base
model, and the text-embedding-3-small model for embeddings, our approach
integrates Langchain to orchestrate a hybrid retrieval system with re-ranking.
This system leverages Drug Utilization Review (DUR) data from public databases,
focusing on contraindications for specific age groups, pregnancy, and
concomitant drug use. The dataset includes 300 question-answer pairs across
three categories, with baseline model accuracy ranging from 0.49 to 0.57.
Post-integration of the RAG pipeline, we observed a significant improvement in
model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications
related to age groups, pregnancy, and concomitant drug use, respectively. The
results indicate that augmenting LLMs with a RAG framework can substantially
reduce uncertainty in prescription and drug intake decisions by providing more
precise and reliable drug contraindication information.

</details>


### [21] [Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution](https://arxiv.org/abs/2508.06225)
*Zailong Tian,Zhuoheng Han,Yanzhe Chen,Haozhe Xu,Xi Yang,richeng xuan,Hongfeng Wang,Lizi Liao*

Main category: cs.AI

TL;DR: 论文提出从以准确性为中心的评估转向以置信度驱动的风险感知LLM评估系统，强调校准置信度的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为评估工具的方法主要关注准确性，忽略了校准置信度的必要性，而后者对可靠和自适应的评估至关重要。

Method: 论文提出了TH-Score度量置信度-准确性对齐，并设计了LLM-as-a-Fuser集成框架，以提升LLM评估的可靠性和风险感知能力。

Result: 实验表明，该方法显著改善了校准效果，实现了自适应、置信度驱动的评估流程，可靠性和准确性优于现有基线。

Conclusion: 通过引入置信度驱动的评估框架，论文为LLM作为可靠评估工具的实际部署提供了更优的解决方案。

Abstract: Large Language Models (LLMs) are widely used as automated judges, where
practical value depends on both accuracy and trustworthy, risk-aware judgments.
Existing approaches predominantly focus on accuracy, overlooking the necessity
of well-calibrated confidence, which is vital for adaptive and reliable
evaluation pipelines. In this work, we advocate a shift from accuracy-centric
evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing
the necessity of well-calibrated confidence for trustworthy and adaptive
evaluation. We systematically identify the **Overconfidence Phenomenon** in
current LLM-as-a-Judges, where predicted confidence significantly overstates
actual correctness, undermining reliability in practical deployment. To
quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring
confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an
ensemble framework that transforms LLMs into reliable, risk-aware evaluators.
Extensive experiments demonstrate that our approach substantially improves
calibration and enables adaptive, confidence-driven evaluation pipelines,
achieving superior reliability and accuracy compared to existing baselines.

</details>


### [22] [GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines](https://arxiv.org/abs/2508.06226)
*Yumeng Fu,Jiayin Zhu,Lingling Zhang,Bo Zhao,Shaoxuan Ma,Yushun Zhang,Yanrui Wu,Wenjun Wu*

Main category: cs.AI

TL;DR: GeoLaux 是一个新的几何问题解决基准，包含 2,186 个问题，重点关注多模态大语言模型（MLLMs）的辅助线构造和长步推理能力。通过五维评估策略，发现 MLLMs 在长步推理和辅助线构造方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估 MLLMs 的几何能力时忽略了辅助线构造和细粒度过程评估，无法全面评估长步推理能力。

Method: 构建 GeoLaux 数据集，包含计算和证明问题，平均推理步数为 6.51，41.8% 的问题需要辅助线构造。设计五维评估策略（答案正确性、过程正确性、过程质量、辅助线影响、错误原因）。

Result: 实验发现：1) 模型在长步推理中性能显著下降；2) 模型在证明问题中倾向于走捷径；3) 模型缺乏辅助线意识，提升此能力有助于整体推理改进。

Conclusion: GeoLaux 可作为评估 MLLMs 长步几何推理能力的基准，并为能力提升提供指导。

Abstract: Geometry problem solving (GPS) requires models to master diagram
comprehension, logical reasoning, knowledge application, numerical computation,
and auxiliary line construction. This presents a significant challenge for
Multimodal Large Language Models (MLLMs). However, existing benchmarks for
evaluating MLLM geometry skills overlook auxiliary line construction and lack
fine-grained process evaluation, making them insufficient for assessing MLLMs'
long-step reasoning abilities. To bridge these gaps, we present the GeoLaux
benchmark, comprising 2,186 geometry problems, incorporating both calculation
and proving questions. Notably, the problems require an average of 6.51
reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary
line construction. Building on the dataset, we design a novel five-dimensional
evaluation strategy assessing answer correctness, process correctness, process
quality, auxiliary line impact, and error causes. Extensive experiments on 13
leading MLLMs (including thinking models and non-thinking models) yield three
pivotal findings: First, models exhibit substantial performance degradation in
extended reasoning steps (nine models demonstrate over 50% performance drop).
Second, compared to calculation problems, MLLMs tend to take shortcuts when
solving proving problems. Third, models lack auxiliary line awareness, and
enhancing this capability proves particularly beneficial for overall geometry
reasoning improvement. These findings establish GeoLaux as both a benchmark for
evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a
guide for capability advancement. Our dataset and code are included in
supplementary materials and will be released.

</details>


### [23] [Learning Logical Rules using Minimum Message Length](https://arxiv.org/abs/2508.06230)
*Ruben Sharma,Sebastijan Dumančić,Ross D. King,Andrew Cropper*

Main category: cs.AI

TL;DR: 论文提出了一种贝叶斯归纳逻辑编程方法，通过平衡假设复杂性和数据拟合，从噪声数据中学习最小消息长度程序。实验表明，该方法在多个领域显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 统一概率和逻辑学习是AI领域的关键挑战。

Method: 采用贝叶斯归纳逻辑编程方法，通过先验和似然平衡假设复杂性和数据拟合。

Result: 在游戏和药物设计等领域的实验中，该方法显著优于现有方法，且数据高效、对示例平衡不敏感。

Conclusion: 该方法在统一概率和逻辑学习方面表现出色，具有广泛的应用潜力。

Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We
introduce a Bayesian inductive logic programming approach that learns minimum
message length programs from noisy data. Our approach balances hypothesis
complexity and data fit through priors, which explicitly favour more general
programs, and a likelihood that favours accurate programs. Our experiments on
several domains, including game playing and drug design, show that our method
significantly outperforms previous methods, notably those that learn minimum
description length programs. Our results also show that our approach is
data-efficient and insensitive to example balance, including the ability to
learn from exclusively positive examples.

</details>


### [24] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti Järvisalo*

Main category: cs.AI

TL;DR: 提出了一种在归纳逻辑编程中打破假设空间对称性的方法，显著提高了求解效率。


<details>
  <summary>Details</summary>
Motivation: 解决归纳逻辑编程中假设空间庞大且存在大量逻辑等价假设的挑战。

Method: 通过打破假设空间的对称性，并在答案集编程中实现该方法。

Result: 实验表明，该方法将求解时间从超过一小时缩短至仅17秒。

Conclusion: 该方法有效提高了归纳逻辑编程的效率，适用于视觉推理和游戏等多个领域。

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [25] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peigné - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: PRISM Eval开发了BET工具，通过动态对抗优化实现100%攻击成功率，并提出细粒度鲁棒性指标，揭示模型间攻击难度差异显著。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型（LLM）的鲁棒性，揭示其漏洞，并为社区提供分布式评估方法。

Method: 使用BET工具进行自动化红队测试，结合动态对抗优化和细粒度鲁棒性指标分析。

Result: 攻击成功率达100%（37/41模型），攻击难度差异超300倍，识别出最有效的越狱技术。

Conclusion: BET工具和细粒度指标为LLM鲁棒性评估提供了实用路径，社区协作有助于分布式评估。

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


### [26] [AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games](https://arxiv.org/abs/2508.06348)
*Mille Mei Zhen Loo,Gert Luzkov,Paolo Burelli*

Main category: cs.AI

TL;DR: 本文提出了一种基于Transformer的机器学习模型AntiCheatPT_256，用于检测《反恐精英2》中的作弊行为，并公开了标注数据集CS2CD。模型在未增强测试集上准确率达89.17%，AUC为93.36%。


<details>
  <summary>Details</summary>
Motivation: 在线游戏中的作弊行为破坏了游戏体验，现有反作弊系统（如VAC）难以在不侵犯用户隐私的情况下应对不断演变的作弊手段。

Method: 提出AntiCheatPT_256模型，使用标注数据集CS2CD（795场比赛）生成90,707个上下文窗口，并通过数据增强解决类别不平衡问题。

Result: 模型在未增强测试集上准确率为89.17%，AUC为93.36%。

Conclusion: 该方法强调可重复性和实际应用性，为数据驱动的作弊检测研究提供了坚实基础。

Abstract: Cheating in online video games compromises the integrity of gaming
experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face
significant challenges in keeping pace with evolving cheating methods without
imposing invasive measures on users' systems. This paper presents
AntiCheatPT\_256, a transformer-based machine learning model designed to detect
cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we
introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using
this dataset, 90,707 context windows were created and subsequently augmented to
address class imbalance. The transformer model, trained on these windows,
achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test
set. This approach emphasizes reproducibility and real-world applicability,
offering a robust baseline for future research in data-driven cheat detection.

</details>


### [27] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: 论文提出“解释性AI”作为可解释AI（XAI）的补充范式，利用生成式AI能力提供上下文推理支持，而非仅关注算法透明度。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法过于抽象且非自适应，难以支持用户理解，因此需要一种更注重人类决策支持的AI解释方式。

Method: 提出八维概念模型，强调叙事沟通、自适应个性化和渐进披露原则，并通过医疗专业人员验证。

Result: 用户更偏好上下文敏感的多模态解释，而非技术透明度。

Conclusion: 需设计以人类理解为中心的AI解释方法，并推动跨领域和文化的研究议程。

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


### [28] [Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned](https://arxiv.org/abs/2508.06368)
*Claudia dAmato,Giuseppe Rubini,Francesco Didio,Donato Francioso,Fatima Zahra Amara,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 论文提出两种构建法律知识图谱（KG）的方法，填补法律领域KG的空白，并验证其在暴力侵害妇女案件中的应用。


<details>
  <summary>Details</summary>
Motivation: 法律决策需要全面的立法背景知识和最新案例信息，但法律领域的KG较少。本文旨在填补这一空白。

Method: 采用两种互补方法：系统化的自下而上方法和大语言模型解决方案，结合结构化数据提取、本体开发和语义丰富。

Result: 构建了针对暴力侵害妇女案件的法律KG，并通过能力问题验证其有效性。

Conclusion: 开发的KG可提升法律信息的可访问性，支持复杂查询，并为预测性司法机器学习工具提供知识支持。

Abstract: Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.

</details>


### [29] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: 论文提出了一种动态机制“Fair Game”，通过结合审计员和去偏算法，利用强化学习动态调整机器学习算法的公平性目标，以应对社会环境的动态变化。


<details>
  <summary>Details</summary>
Motivation: 现有公平机器学习的定义多为观察性，且存在冲突，难以在动态社会环境中实现公平。需要一种能够适应社会变化的公平机制。

Method: 提出“Fair Game”框架，将审计员和去偏算法通过强化学习循环结合，动态调整公平性目标。

Result: “Fair Game”能够灵活适应社会伦理和法律框架的变化，为机器学习系统提供部署前后的公平性保障。

Conclusion: “Fair Game”为公平机器学习提供了一个动态、适应性强的解决方案，能够模拟社会伦理框架的演变。

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [30] [What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting](https://arxiv.org/abs/2508.06454)
*Joshua Caiata,Ben Armstrong,Kate Larson*

Main category: cs.AI

TL;DR: 本文提出了一种数据驱动框架，用于评估多赢家投票规则在不同偏好分布下违反公理的频率，并通过神经网络展示了其在减少公理违反方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 研究多赢家投票规则在实际应用中满足公理的情况，以弥补传统最坏情况分析的不足。

Method: 提出数据驱动框架，分析不同偏好分布下投票规则与公理性能的关系，并利用神经网络作为投票规则进行实验。

Result: 神经网络作为投票规则在减少公理违反方面优于传统规则。

Conclusion: 数据驱动方法可为设计新投票系统提供依据，并支持社会选择领域的数据驱动研究。

Abstract: Committee-selection problems arise in many contexts and applications, and
there has been increasing interest within the social choice research community
on identifying which properties are satisfied by different multi-winner voting
rules. In this work, we propose a data-driven framework to evaluate how
frequently voting rules violate axioms across diverse preference distributions
in practice, shifting away from the binary perspective of axiom satisfaction
given by worst-case analysis. Using this framework, we analyze the relationship
between multi-winner voting rules and their axiomatic performance under several
preference distributions. We then show that neural networks, acting as voting
rules, can outperform traditional rules in minimizing axiom violations. Our
results suggest that data-driven approaches to social choice can inform the
design of new voting systems and support the continuation of data-driven
research in social choice.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [31] [User-Intent-Driven Semantic Communication via Adaptive Deep Understanding](https://arxiv.org/abs/2508.05884)
*Peigen Ye,Jingpu Duan,Hongyang Du,Yulan Guo*

Main category: cs.IT

TL;DR: 提出了一种基于用户意图的语义通信系统，通过多模态大模型和掩码引导注意力模块，实现了对用户意图的深度理解与高效传输。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信系统虽能提取关键语义，但难以深入理解和泛化用户真实意图。

Method: 整合多模态大模型生成用户意图先验，提出掩码引导注意力模块突出关键语义区域，并引入信道状态感知模块适应不同信道条件。

Result: 在瑞利信道下（SNR为5dB），PSNR、SSIM和LPIPS分别提升8%、6%和19%。

Conclusion: 该系统显著提升了语义通信的意图理解能力与传输效率。

Abstract: Semantic communication focuses on transmitting task-relevant semantic
information, aiming for intent-oriented communication. While existing systems
improve efficiency by extracting key semantics, they still fail to deeply
understand and generalize users' real intentions. To overcome this, we propose
a user-intention-driven semantic communication system that interprets diverse
abstract intents. First, we integrate a multi-modal large model as semantic
knowledge base to generate user-intention prior. Next, a mask-guided attention
module is proposed to effectively highlight critical semantic regions. Further,
a channel state awareness module ensures adaptive, robust transmission across
varying channel conditions. Extensive experiments demonstrate that our system
achieves deep intent understanding and outperforms DeepJSCC, e.g., under a
Rayleigh channel at an SNR of 5 dB, it achieves improvements of 8%, 6%, and 19%
in PSNR, SSIM, and LPIPS, respectively.

</details>


### [32] [On MDS Convertible Codes in the Merge Regime](https://arxiv.org/abs/2508.06219)
*Vinayak Ramkumar,Xiangliang Kong,G. Yeswanth Sai,Myna Vajha,M. Nikhil Krishnan*

Main category: cs.IT

TL;DR: 论文研究了在大规模分布式存储系统中，通过可转换码（convertible codes）优化存储效率和可靠性，提出了三种访问成本最优的构造方案，并进一步优化了带宽成本。


<details>
  <summary>Details</summary>
Motivation: 在大规模分布式存储系统中，纠删码用于确保磁盘故障的可靠性。通过动态调整码参数（即代码转换），可以在不牺牲可靠性的情况下显著节省存储空间。这促使了可转换码的设计需求。

Method: 研究了将初始MDS码的多个码字合并为最终MDS码的单个码字的场景，提出了三种访问成本最优的构造方案，并优化了带宽成本。

Result: 三种构造方案分别适用于不同参数范围，均实现了访问成本最优，且第三种构造方案在几乎所有情况下匹配了MDS猜想的下界。带宽优化方案进一步降低了子分组化。

Conclusion: 论文提出的可转换码构造方案在访问成本和带宽成本上均实现了优化，为大规模存储系统的可靠性设计提供了有效工具。

Abstract: In large-scale distributed storage systems, erasure coding is employed to
ensure reliability against disk failures. Recent work by Kadekodi et al.
demonstrates that adapting code parameters to varying disk failure rates can
lead to significant storage savings without compromising reliability. Such
adaptations, known as \emph{code conversions}, motivate the design of
\emph{convertible codes}, which enable efficient transformations between codes
of different parameters.
  In this work, we study the setting in which $\lambda$ codewords of an initial
$[n^I = k^I + r^I,\, k^I]$ MDS code are merged into a single codeword of a
final $[n^F = \lambda k^I + r^F,\, k^F = \lambda k^I]$ MDS code. We begin by
presenting three constructions that achieve optimal \emph{access cost}, defined
as the total number of disks accessed during the conversion process. The first
two constructions apply when $\lambda \leq r^I$ and impose specific
divisibility conditions on $r^I$ and the field size $q$. These schemes minimize
both the per-symbol and the overall access cost. The third construction, which
builds on a prior scheme by Kong, achieves minimal access cost while supporting
arbitrary parameter regimes. All three constructions require field sizes that
are linear in the final code length, and notably, the third construction
achieves a field size that matches the lower bound implied by the MDS
conjecture in almost all cases. In addition, we propose a construction that
optimizes the \emph{bandwidth cost}, defined as the total number of symbols
transmitted during conversion. This scheme is a refinement of Maturana and
Rashmi's bandwidth-optimal construction based on the piggybacking framework,
and achieves reduced sub-packetization.

</details>


### [33] [A New Framework for the Sum of Squared $κ$-$μ$ RVs with Application to Sub-THz Systems](https://arxiv.org/abs/2508.06242)
*Gustavo Rodrigues de Lima Tejerina,Italo Atzeni*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we adopt the $\kappa$-$\mu$ model to characterize the
propagation in the sub-THz band. We develop a new exact representation of the
sum of squared independent and identically distributed $\kappa$-$\mu$ random
variables, which can be used to express the power of the received signal in
multi-antenna systems. Unlike existing ones, the proposed analytical framework
is remarkably tractable and computationally efficient, and thus can be
conveniently employed to analyze systems with massive antenna arrays. We derive
novel expressions for the probability density function and cumulative
distribution function, analyze their convergence and truncation error, and
discuss the computational complexity and the implementation aspects. Moreover,
we derive expressions for the coverage probability and bit error probability
for coherent binary modulations. Lastly, we evaluate the performance of an
uplink sub-THz system where a single-antenna user is served by a base station
employing maximum ratio combining.

</details>
