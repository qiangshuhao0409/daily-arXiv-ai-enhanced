<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 15]
- [cs.AI](#cs.AI) [Total: 65]
- [cs.IT](#cs.IT) [Total: 15]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Iris RESTful Server and IrisTileSource: An Iris implementation for existing OpenSeaDragon viewers](https://arxiv.org/abs/2508.06615)
*Ryan Erik Landvater,Navin Kathawa,Mustafa Yousif MD,Ulysses Balis MD*

Main category: cs.NI

TL;DR: 开发了Iris RESTful Server，支持高性能的WSI文件流式传输，兼容DICOMweb WADO-RS API，并与OpenSeaDragon集成。


<details>
  <summary>Details</summary>
Motivation: 解决静态HTTP文件服务器无法流式传输高分辨率WSI文件的问题，提升病理学家的图像渲染体验。

Method: 使用C++和Boost Beast HTTP及Asio库开发RESTful服务器，兼容DICOMweb WADO-RS API，并集成OpenSeaDragon TileSource。

Result: 单实例每秒可处理5000个瓦片请求，延迟中位数为21毫秒。

Conclusion: Iris RESTful Server为WSI工作流提供了高性能、安全的解决方案，并简化了集成。

Abstract: The Iris File Extension (IFE) is a low overhead performance-oriented whole
slide image (WSI) file format designed to improve the image rendering
experience for pathologists and simplify image management for system
administrators. However, static hypertext transfer protocol (HTTP) file servers
cannot natively stream subregions of high-resolution image files, such as the
IFE. The majority of contemporary WSI viewer systems are designed as
browser-based web applications and leverage OpenSeaDragon as the tile-based
rendering framework. These systems convert WSI files to Deep Zoom Images (DZI)
for compatibility with simple static HTTP file servers. In order to address
this limitation, we have developed the Iris RESTful Server, a low-overhead HTTP
server with a RESTful API that is natively compatible with the DICOMweb WADO-RS
API. Written in C++ with Boost Beast HTTP and Asio networking libraries atop
the public IFE libraries, the server offers both security and high performance.
Testing shows that a single instance can handle over 5000 tile requests per
second with a median latency of 21 ms on a private network. We also developed
and merged a new OpenSeaDragon TileSource, compatible with the Iris RESTful
API, into the next OpenSeaDragon release, enabling simple and immediate drop-in
replacement of DZI images within WSI viewer stacks. Designed as a secure
cross-origin resource sharing microservice, this architecture includes detailed
deployment instructions for new or existing WSI workflows, and the public
examples.restful.irisdigitialpathology.org subdomain is provided as a
development tool to accelerate WSI web viewer development.

</details>


### [2] [Generative AI for Intent-Driven Network Management in 6G: A Case Study on Hierarchical Learning Approach](https://arxiv.org/abs/2508.06616)
*Md Arafat Habib,Medhat Elsayed,Yigit Ozcan,Pedro Enrique Iturria-Rivera,Majid Bavand,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 本文综述了基于大型语言模型（LLM）的意图驱动网络（IDN）架构在6G异构动态网络中的应用，并提出了一种分层框架，将生成式AI（GenAI）整合到IDN的三个关键阶段，通过案例研究验证了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 随着6G的出现，移动网络变得日益异构和动态，需要高级自动化管理。意图驱动网络（IDN）通过将高层意图转化为优化策略来解决这一问题，而大型语言模型（LLM）可以增强这一过程。

Method: 提出了一种分层学习驱动的IDN架构，将GenAI整合到意图处理、意图验证和意图执行三个阶段，并通过基于最新GenAI架构Mamba的案例研究验证其有效性。

Result: 案例研究表明，提出的GenAI驱动架构通过智能自动化提升了网络性能，优于传统IDN架构。

Conclusion: 本文为6G网络中LLM-based IDN架构的全面研究提供了重要参考，并展示了GenAI在IDN全阶段应用的潜力。

Abstract: With the emergence of 6G, mobile networks are becoming increasingly
heterogeneous and dynamic, necessitating advanced automation for efficient
management. Intent-Driven Networks (IDNs) address this by translating
high-level intents into optimization policies. Large Language Models (LLMs) can
enhance this process by understanding complex human instructions to enable
adaptive, intelligent automation. Given the rapid advancements in Generative AI
(GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated
Radio Access Network (RAN) environments is both timely and critical. This
article provides such a survey, along with a case study on a hierarchical
learning-enabled IDN architecture that integrates GenAI across three key
stages: intent processing, intent validation, and intent execution. Unlike most
existing approaches that apply GenAI in the form of LLMs for intent processing
only, we propose a hierarchical framework that introduces GenAI across all
three stages of IDN. To demonstrate the effectiveness of the proposed IDN
management architecture, we present a case study based on the latest GenAI
architecture named Mamba. The case study shows how the proposed GenAI-driven
architecture enhances network performance through intelligent automation,
surpassing the performance of the conventional IDN architectures.

</details>


### [3] [THz/RF Multi-Hop Routing Throughput: Performance, Optimization, and Application](https://arxiv.org/abs/2508.06975)
*Zhengying Lou,Baha Eddine Youcef Belmekki,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 提出了一种基于随机几何的THz通信路由优化框架，通过分步优化方法提升吞吐量，性能接近理论上限。


<details>
  <summary>Details</summary>
Motivation: THz通信的高路径损耗问题需要高效路由策略，以提升其在高吞吐量无线系统中的实用性。

Method: 采用随机几何分析框架，分步优化功率分配、中继选择和跳数设计。

Result: 提出的路由策略优于现有方法，性能接近理论上限，并验证了THz与RF路由的性能差异。

Conclusion: 该框架为THz系统参数设计和无人机网络提供了实用工具。

Abstract: Terahertz (THz) communication offers a promising solution for high-throughput
wireless systems. However, the severe path loss of THz signals raises concerns
about its effectiveness compared to radio frequency (RF) communication. In this
article, we establish the first stochastic geometry (SG)-based analytical
framework for routing in THz systems. We develop a stepwise optimization
approach to maximize throughput, including power allocation, relay selection,
and number of hops design. Analytical expressions for throughput and coverage
probability are derived under the SG framework, enabling low complexity and
scalable performance evaluation. Numerical results show that the proposed
stepwise-optimal routing strategies not only outperform existing SG-based
methods but also approach the ideal upper bound. Moreover, we compare the
throughput and coverage performance of THz and RF routing and demonstrate the
applications of the proposed analytical framework and routing strategies in
system parameter design and unmanned aerial vehicle networks.

</details>


### [4] [Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization](https://arxiv.org/abs/2508.07001)
*Myeung Suk Oh,Zhiyao Zhang,FNU Hairi,Alvaro Velasquez,Jia Liu*

Main category: cs.NI

TL;DR: 提出了一种完全去中心化的多智能体强化学习（MARL）架构，通过局部奖励交换优化随机接入（RA）MAC协议性能，减少碰撞并确保公平性。


<details>
  <summary>Details</summary>
Motivation: 现有基于集中训练和分散执行（CTDE）的MARL方法在现实应用中因集中训练依赖和信息收集开销大而不切实际。

Method: 采用完全去中心化的MARL架构，基于共识的信息交换，设计基于actor-critic网络的算法，仅交换局部奖励以减少通信开销。

Result: 数值实验表明，该算法显著优于其他基线方法，提升了RA网络性能。

Conclusion: 提出的去中心化MARL方法在理论和实验上均验证了其有效性，适用于现实场景。

Abstract: With wireless devices increasingly forming a unified smart network for
seamless, user-friendly operations, random access (RA) medium access control
(MAC) design is considered a key solution for handling unpredictable data
traffic from multiple terminals. However, it remains challenging to design an
effective RA-based MAC protocol to minimize collisions and ensure transmission
fairness across the devices. While existing multi-agent reinforcement learning
(MARL) approaches with centralized training and decentralized execution (CTDE)
have been proposed to optimize RA performance, their reliance on centralized
training and the significant overhead required for information collection can
make real-world applications unrealistic. In this work, we adopt a fully
decentralized MARL architecture, where policy learning does not rely on
centralized tasks but leverages consensus-based information exchanges across
devices. We design our MARL algorithm over an actor-critic (AC) network and
propose exchanging only local rewards to minimize communication overhead.
Furthermore, we provide a theoretical proof of global convergence for our
approach. Numerical experiments show that our proposed MARL algorithm can
significantly improve RA network performance compared to other baselines.

</details>


### [5] [ProtoScan: Measuring censorship in IPv6](https://arxiv.org/abs/2508.07194)
*Jack Wampler,Hammas Bin Tanveer,Rishab Nithyanand,Eric Wustrow*

Main category: cs.NI

TL;DR: 论文研究了IPv6与IPv4在互联网审查中的差异，发现IPv6审查能力较弱且不全面，为规避审查提供了新机会。


<details>
  <summary>Details</summary>
Motivation: 随着IPv6的普及，了解其在审查中的影响变得重要，因为现有研究主要关注IPv4。

Method: 通过全球性研究，比较IPv4和IPv6在HTTP、DNS和TLS等协议上的审查差异。

Result: 发现IPv6的审查能力较弱且不全面，部分国家在IPv6上的审查技术和资源部署与IPv4不同。

Conclusion: IPv6为规避审查提供了新途径，未来需要开发利用IPv6技术和基础设施的工具。

Abstract: Internet censorship continues to impact billions of people worldwide, and
measurement of it remains an important focus of research. However, most
Internet censorship measurements have focused solely on the IPv4 Internet
infrastructure. Yet, more clients and servers are available over IPv6:
According to Google, over a third of their users now have native IPv6 access.
Given the slow-but-steady rate of IPv6 adoption, it is important to understand
its impact on censorship. In this paper, we measure and analyze how censorship
differs over IPv6 compared to the well-studied IPv4 censorship systems in use
today. We perform a comprehensive global study of censorship across an array of
commonly censored protocols, including HTTP, DNS, and TLS, on both IPv4 and
IPv6, and compare the results. We find that there are several differences in
how countries censor IPv6 traffic, both in terms of IPv6 resources, and in
where and what blocklists or technologies are deployed on IPv6 networks. Many
of these differences are not all-or-nothing: we find that most censors have
some capacity to block in IPv6, but are less comprehensive or less reliable
compared to their IPv4 censorship systems. Our results suggest that IPv6 offers
new areas for censorship circumvention researchers to explore, providing
potentially new ways to evade censors. As more users gain access to IPv6
addresses and networks, there will be a need for tools that take advantage of
IPv6 techniques and infrastructure to bypass censorship.

</details>


### [6] [Mind the IP Gap: Measuring the impact of IPv6 on DNS censorship](https://arxiv.org/abs/2508.07197)
*Ian Martiny,Hammas Bin Tanveer,Jack Wampler,Rishab Nithyanand,Eric Wustrow*

Main category: cs.NI

TL;DR: 本文首次对IPv6互联网上的DNS审查进行了全球测量，发现IPv6审查政策不一致且效果不如IPv4。


<details>
  <summary>Details</summary>
Motivation: 随着IPv6的普及，研究其审查效果是否与IPv4一致具有重要意义。

Method: 利用IPv6开放解析器发现技术，发送2000万次DNS请求，测量解析器、网络和国家层面的审查率。

Result: 几乎所有审查者都支持IPv6审查，但政策不一致且效果较差。

Conclusion: IPv6审查支持不完善，为规避审查提供了新机会。

Abstract: Internet censorship impacts large segments of the Internet, but so far, prior
work has focused almost exclusively on performing measurements using IPv4. As
the Internet grows, and more users connect, IPv6 is increasingly supported and
available to users and servers alike. But despite this steady growth, it
remains unclear if the information control systems that implement censorship
(firewalls, deep packet inspection, DNS injection, etc) are as effective with
IPv6 traffic as they are with IPv4. In this paper, we perform the first global
measurement of DNS censorship on the IPv6 Internet. Leveraging a recent
technique that allows us to discover IPv6-capable open resolvers (along with
their corresponding IPv4 address), we send over 20 million A and AAAA DNS
requests to DNS resolvers worldwide, and measure the rate at which they block,
at the resolver, network, and country level as well examine the characteristics
of blocked domains. We observe that while nearly all censors support blocking
IPv6, their policies are inconsistent with and frequently less effective than
their IPv4 censorship infrastructure. Our results suggest that supporting IPv6
censorship is not all-or-nothing: many censors support it, but poorly. As a
result, these censors may have to expend additional resources to bring IPv6
censorship up to parity with IPv4. In the meantime, this affords censorship
circumvention researchers a new opportunity to exploit these differences to
evade detection and blocking.

</details>


### [7] [The Search for Relevance: A Context-Aware Paradigm Shift in Semantic and Task-Oriented V2X Communications](https://arxiv.org/abs/2508.07394)
*Luca Lusvarghi,Javier Gozalvez,Baldomero Coll-Perales,Mohammad Irfan Khan,Miguel Sepulcre,Seyhan Ucar,Onur Altintas*

Main category: cs.NI

TL;DR: 论文提出了一种面向6G时代的语义和任务导向的通信范式，专注于传输与接收者相关的信息，以提高通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统通信系统设计注重数据的可靠和及时传输，但6G时代对可扩展性的需求要求新的通信范式，能够根据上下文精心筛选传输内容。

Method: 提出了一种联合语义和任务导向的通信范式，特别适用于车联网（V2X）环境，利用上下文信息评估信息对接收者的相关性。

Result: 数值结果表明，通过传输最相关的信息，语义和任务导向的V2X通信可将通信效率提高两倍。

Conclusion: 语义和任务导向的通信范式显著提升了V2X网络的可扩展性，为6G时代的通信系统设计提供了新思路。

Abstract: The design of communication systems has traditionally focused on the reliable
and timely delivery of data. However, the scalability challenges faced by the
evolution to a 6G-driven society demand new communication paradigms that
carefully curate the content being transmitted. This paper envisions a joint
semantic and task-oriented communication paradigm where Connected and
Autonomous Vehicles (CAVs) transmit only the information necessary to convey
the desired meaning that is relevant to the intended receivers based on the
communication context. The V2X domain offers a unique environment for the
development of the envisioned semantic and task-oriented communications
paradigm, as CAVs are native semantic devices, and the V2X domain is rich in
contextual information. This contextual information can be leveraged to
estimate the relevance that information may have for the intended receivers. We
illustrate and quantitatively evaluate the potential benefits of semantic and
task-oriented V2X communications. Numerical results show that by focusing on
the transmission of the most relevant information for the intended receivers,
semantic and task-oriented V2X communications can achieve a two-fold
improvement in communication efficiency, which will significantly benefit the
scalability of V2X networks.

</details>


### [8] [Unveiling IPv6 Scanning Dynamics: A Longitudinal Study Using Large Scale Proactive and Passive IPv6 Telescopes](https://arxiv.org/abs/2508.07506)
*Hammas Bin Tanveer,Wai Sun Chan,Ricky K. P. Mok,Sebastian Kappes,Philipp Richter,Oliver Gasser,John Ronan,Arthur Berger,kc Claffy*

Main category: cs.NI

TL;DR: 论文介绍了新工具和视角，用于吸引和分析IPv6扫描流量，通过部署大规模主动望远镜收集了6亿个未请求数据包，并分析了扫描源和目标地址策略。


<details>
  <summary>Details</summary>
Motivation: 开发主动技术以吸引和分析IPv6扫描流量，填补现有研究的空白。

Method: 部署大规模IPv6主动望远镜在生产ISP网络中，收集和分析未请求流量。

Result: 收集了6亿个数据包，分析了1.9k个自治系统的扫描行为，评估了五种网络栈特征的效率。

Conclusion: 研究提供了对IPv6扫描行为的深入理解，揭示了扫描源和目标地址策略。

Abstract: We introduce new tools and vantage points to develop and integrate proactive
techniques to attract IPv6 scan traffic, thus enabling its analysis. By
deploying the largest-ever IPv6 proactive telescope in a production ISP
network, we collected over 600M packets of unsolicited traffic from 1.9k
Autonomous Systems in 10 months. We characterized the sources of unsolicited
traffic, evaluated the effectiveness of five major features across the network
stack, and inferred scanners' sources of target addresses and their strategies.

</details>


### [9] [Achieving Fair-Effective Communications and Robustness in Underwater Acoustic Sensor Networks: A Semi-Cooperative Approach](https://arxiv.org/abs/2508.07578)
*Yu Gou,Tong Zhang,Jun Liu,Tingting Yang,Shanshan Song,Jun-Hong Cui*

Main category: cs.NI

TL;DR: 论文提出了一种半合作功率分配方法（SECOPA），用于解决水下声学传感器网络（IC-UASNs）中的公平有效通信和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 研究在能量受限和信道时变的水下网络中，节点故障对性能的影响，以及如何在满足个体QoS需求的同时实现全局公平通信。

Method: 采用分布式多智能体强化学习（MARL）方法，智能节点自主决定传输功率，并开发高级训练算法以适应不完美环境。

Result: 数值结果表明，SECOPA方法能够有效平衡个体与全局性能，并适应时变信道和节点故障。

Conclusion: SECOPA方法为IC-UASNs中的公平有效通信和鲁棒性提供了可行的解决方案。

Abstract: This paper investigates the fair-effective communication and robustness in
imperfect and energy-constrained underwater acoustic sensor networks
(IC-UASNs). Specifically, we investigate the impact of unexpected node
malfunctions on the network performance under the time-varying acoustic
channels. Each node is expected to satisfy Quality of Service (QoS)
requirements. However, achieving individual QoS requirements may interfere with
other concurrent communications. Underwater nodes rely excessively on the
rationality of other underwater nodes when guided by fully cooperative
approaches, making it difficult to seek a trade-off between individual QoS and
global fair-effective communications under imperfect conditions. Therefore,
this paper presents a SEmi-COoperative Power Allocation approach (SECOPA) that
achieves fair-effective communication and robustness in IC-UASNs. The approach
is distributed multi-agent reinforcement learning (MARL)-based, and the
objectives are twofold. On the one hand, each intelligent node individually
decides the transmission power to simultaneously optimize individual and global
performance. On the other hand, advanced training algorithms are developed to
provide imperfect environments for training robust models that can adapt to the
time-varying acoustic channels and handle unexpected node failures in the
network. Numerical results are presented to validate our proposed approach.

</details>


### [10] [Joint Scheduling and Resource Allocation in mmWave IAB Networks Using Deep RL](https://arxiv.org/abs/2508.07604)
*Maryam Abbasalizadeh,Sashank Narain*

Main category: cs.NI

TL;DR: 提出了一种基于深度强化学习（DRL）的框架，用于动态干扰严重的IAB网络中联合链路调度和资源切片，显著提升了调度准确性和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在密集5G及更高频段部署中，IAB至关重要，但光纤回传不可行，需要高效调度和资源分配方法。

Method: 结合贪婪双深度Q网络（DDQN）调度器和多智能体DDQN分配器，实现链路激活及带宽与天线分配的联合优化。

Result: 在96种动态拓扑中，调度准确率达99.84%，吞吐量提升20.90%。

Conclusion: 该框架高效适应动态资源受限环境，适用于快速链路调度和自主回传协调。

Abstract: Integrated Access and Backhaul (IAB) is critical for dense 5G and beyond
deployments, especially in mmWave bands where fiber backhaul is infeasible. We
propose a novel Deep Reinforcement Learning (DRL) framework for joint link
scheduling and resource slicing in dynamic, interference-prone IAB networks.
Our method integrates a greedy Double Deep Q-Network (DDQN) scheduler to
activate access and backhaul links based on traffic and topology, with a
multi-agent DDQN allocator for bandwidth and antenna assignment across network
slices. This decentralized approach respects strict antenna constraints and
supports concurrent scheduling across heterogeneous links. Evaluations across
96 dynamic topologies show 99.84 percent scheduling accuracy and 20.90 percent
throughput improvement over baselines. The framework's efficient operation and
adaptability make it suitable for dynamic and resource-constrained deployments,
where fast link scheduling and autonomous backhaul coordination are vital.

</details>


### [11] [Joint link scheduling and power allocation in imperfect and energy-constrained underwater wireless sensor networks](https://arxiv.org/abs/2508.07679)
*Tong Zhang,Yu Gou,Jun Liu,Shanshan Song,Tingting Yang,Jun-Hong Cui*

Main category: cs.NI

TL;DR: 论文提出了一种基于深度多智能体强化学习（MARL）的优化器ICRL-JSA，用于解决水下无线传感器网络（UWSNs）中的公平、高效和可靠通信问题。


<details>
  <summary>Details</summary>
Motivation: 水下无线传感器网络（UWSNs）因能量供应有限和节点故障问题，难以实现高效通信。

Method: 通过结合深度Q网络和先进的训练机制，ICRL-JSA实现了联合链路调度和功率分配。

Result: 仿真结果表明，ICRL-JSA在复杂水下环境中优于多种基准算法。

Conclusion: ICRL-JSA为能量受限和节点故障的UWSNs提供了一种有效的通信优化解决方案。

Abstract: Underwater wireless sensor networks (UWSNs) stand as promising technologies
facilitating diverse underwater applications. However, the major design issues
of the considered system are the severely limited energy supply and unexpected
node malfunctions. This paper aims to provide fair, efficient, and reliable
(FER) communication to the imperfect and energy-constrained UWSNs (IC-UWSNs).
Therefore, we formulate a FER-communication optimization problem (FERCOP) and
propose ICRL-JSA to solve the formulated problem. ICRL-JSA is a deep
multi-agent reinforcement learning (MARL)-based optimizer for IC-UWSNs through
joint link scheduling and power allocation, which automatically learns
scheduling algorithms without human intervention. However, conventional RL
methods are unable to address the challenges posed by underwater environments
and IC-UWSNs. To construct ICRL-JSA, we integrate deep Q-network into IC-UWSNs
and propose an advanced training mechanism to deal with complex acoustic
channels, limited energy supplies, and unexpected node malfunctions. Simulation
results demonstrate the superiority of the proposed ICRL-JSA scheme with an
advanced training mechanism compared to various benchmark algorithms.

</details>


### [12] [An Experimental Reservoir-Augmented Foundation Model: 6G O-RAN Case Study](https://arxiv.org/abs/2508.07778)
*Farhad Rezazadeh,Raymond Zhao,Jiongyu Dai,Amir Ashtari Gargari,Hatim Chergui,Lingjia Liu*

Main category: cs.NI

TL;DR: RA-MAT是一种基于储层计算和掩码自编码的Transformer模型，用于处理6G O-RAN中的高维非平稳时间序列数据，满足低延迟和高效率需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer架构在处理超高频维非平稳时间序列数据时的性能瓶颈，满足6G O-RAN对实时性和能效的严格要求。

Method: 结合储层计算（ESN）和掩码自编码技术，通过快速投影和轻量级线性操作优化计算效率，并利用自监督预训练和微调适应下游任务。

Result: 在O-RAN KPI预测任务中，RA-MAT实现了低于0.06的均方误差（MSE）。

Conclusion: RA-MAT为6G网络中的实时基础分析提供了一种实用解决方案。

Abstract: Next-generation open radio access networks (O-RAN) continuously stream tens
of key performance indicators (KPIs) together with raw in-phase/quadrature (IQ)
samples, yielding ultra-high-dimensional, non-stationary time series that
overwhelm conventional transformer architectures. We introduce a
reservoir-augmented masked autoencoding transformer (RA-MAT). This time series
foundation model employs echo state network (ESN) computing with masked
autoencoding to satisfy the stringent latency, energy efficiency, and
scalability requirements of 6G O-RAN testing. A fixed, randomly initialized ESN
rapidly projects each temporal patch into a rich dynamical embedding without
backpropagation through time, converting the quadratic self-attention
bottleneck into a lightweight linear operation. These embeddings drive a
patch-wise masked autoencoder that reconstructs 30% randomly masked patches,
compelling the encoder to capture both local dynamics and long-range structure
from unlabeled data. After self-supervised pre-training, RA-MAT is fine-tuned
with a shallow task head while keeping the reservoir and most transformer
layers frozen, enabling low-footprint adaptation to diverse downstream tasks
such as O-RAN KPI forecasting. In a comprehensive O-RAN KPI case study, RA-MAT
achieved sub-0.06 mean squared error (MSE) on several continuous and discrete
KPIs. This work positions RA-MAT as a practical pathway toward real-time,
foundation-level analytics in future 6G networks.

</details>


### [13] [Industrial Viewpoints on RAN Technologies for 6G](https://arxiv.org/abs/2508.08225)
*Mansoor Shafi,Erik G. Larsson,Xingqin Lin,Dorin Panaitopol,Stefan Parkvall,Flavien Ronteix-Jacquet,Antti Toskala*

Main category: cs.NI

TL;DR: 本文探讨了6G标准化的技术组件和性能要求，重点关注无线接入技术，如MIMO、AI、波形等，并预测其未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 为即将开始的6G标准化提供技术指导和未来研究方向。

Method: 通过分析6G无线接入技术（如MIMO、AI、波形等）及其与非地面网络的集成，提出预测性观点。

Result: 虽然尚无具体研究结果，但基于实施和部署方面的经验，提出了对6G技术发展的预测。

Conclusion: 本文的观点有望为研究人员和行业从业者提供指导，推动6G技术的发展。

Abstract: 6G standardization is to start imminently, with commercial deployments
expected before 2030. Its technical components and performance requirements are
the focus of this article. Our emphasis is on the 6G radio access, especially
MIMO, AI, waveforms, coding, signal constellations and integration with
non-terrestrial networks. Whilst standardization has not yet formally started,
the scope of the 6G study items has been defined. Our predictions in this paper
are speculative as there are no results of the study yet, but our views are
guided by implementation and deployment aspects. We expect that the views here
will guide researchers and industry practitioners.

</details>


### [14] [Scalable and Energy-Efficient Predictive Data Collection in Wireless Sensor Networks with Constructive Interference](https://arxiv.org/abs/2508.07882)
*Conor Muldoon*

Main category: cs.NI

TL;DR: STAIR是一种利用时空激活和建设性干扰的低能耗、低延迟无线传感器网络框架。


<details>
  <summary>Details</summary>
Motivation: 解决无线传感器网络中多节点同时传输数据时的资源限制问题。

Method: 使用粗粒度拓扑信息选择子网，并通过次模优化算法确定传感器激活位置和时间。

Result: 在真实测试平台上验证了框架的有效性。

Conclusion: STAIR框架在资源受限环境下表现出色。

Abstract: A new class of Wireless Sensor Network has emerged whereby multiple nodes
transmit data simultaneously, exploiting constructive interference to enable
data collection frameworks with low energy usage and latency. This paper
presents STAIR (Spatio-Temporal Activation for Intelligent Relaying), a
scalable, resilient framework for Wireless Sensor Networks that leverages
constructive interference and operates effectively under stringent resource
constraints. Using constructive interference requires all nodes to transmit the
same packet at the same time, thus, only one source node can send data per time
slot. STAIR uses coarse-grained topology information to flood a selected subset
of the network, relaying sensor readings from individual nodes during their
allocated time slots. A submodular optimisation algorithm with proven quality
bounds determines near-optimal sensor activation locations and times, aiming to
minimise the sum of mean squared prediction errors from a multiple multivariate
linear regression model, which is used to estimate values at unselected
locations and times. This framework has been extensively validated on a
real-world testbed deployment.

</details>


### [15] [Adaptive Multiple Access and Service Placement for Generative Diffusion Models](https://arxiv.org/abs/2508.07978)
*Hamidreza Mazandarani,Mohammad Farhoudi,Masoud Shokrnezhad,Tarik Taleb*

Main category: cs.NI

TL;DR: 论文提出了一种名为LEARN-GDM的优化框架，用于解决生成扩散模型在移动边缘网络中的实时部署问题，通过深度强化学习动态分配去噪块并优化资源使用。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型在复杂数据生成任务中表现出色，但其迭代和资源密集的推理过程限制了在实时和移动环境中的部署。

Method: 提出LEARN-GDM算法，结合贪婪多址接入方案和基于D3QL的服务放置策略，动态分配去噪块并减少推理步骤。

Result: 仿真显示该框架在可扩展性和延迟弹性方面优于传统方法。

Conclusion: 该工作为边缘生成AI提供了适应性强的解决方案，为未来分布式环境中的语义网络和协同推理铺平了道路。

Abstract: Generative Diffusion Models (GDMs) have emerged as key components of
Generative Artificial Intelligence (GenAI), offering unparalleled
expressiveness and controllability for complex data generation tasks. However,
their deployment in real-time and mobile environments remains challenging due
to the iterative and resource-intensive nature of the inference process.
Addressing these challenges, this paper introduces a unified optimization
framework that jointly tackles service placement and multiple access control
for GDMs in mobile edge networks. We propose LEARN-GDM, a Deep Reinforcement
Learning-based algorithm that dynamically partitions denoising blocks across
heterogeneous edge nodes, while accounting for latent transmission costs and
enabling adaptive reduction of inference steps. Our approach integrates a
greedy multiple access scheme with a Double and Dueling Deep Q-Learning
(D3QL)-based service placement, allowing for scalable, adaptable, and
resource-efficient operation under stringent quality of service requirements.
Simulations demonstrate the superior performance of the proposed framework in
terms of scalability and latency resilience compared to conventional monolithic
and fixed chain-length placement strategies. This work advances the state of
the art in edge-enabled GenAI by offering an adaptable solution for GDM
services orchestration, paving the way for future extensions toward semantic
networking and co-inference across distributed environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization](https://arxiv.org/abs/2508.06559)
*Sina Baghal*

Main category: cs.AI

TL;DR: 本文介绍了一种基于CUDA加速的计算框架，用于模拟Pasur钓鱼卡牌游戏，并通过CFR算法计算近纳什均衡。框架通过高效内存管理和游戏树分解解决复杂规则和大规模游戏树的挑战。


<details>
  <summary>Details</summary>
Motivation: Pasur游戏的复杂规则和大规模游戏树为计算近纳什均衡带来挑战，需要高效的计算和内存管理方法。

Method: 使用PyTorch CUDA张量处理规则复杂性，将游戏树分解为实际游戏状态和继承分数，采用逐轮反向训练策略减少计算复杂度。

Result: 构建了包含超过10^9个节点的完整游戏树，并通过大规模自对弈估计每副牌的公平价值。

Conclusion: 该框架可扩展至其他多轮次强化学习场景，如回合制策略游戏或金融市场中的顺序交易决策。

Abstract: Pasur is a fishing card game played over six rounds and is played similarly
to games such as Cassino and Scopa, and Bastra. This paper introduces a
CUDA-accelerated computational framework for simulating Pasur, emphasizing
efficient memory management. We use our framework to compute near-Nash
equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm
for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the
large size of its game tree. We handle rule complexity using PyTorch CUDA
tensors and to address the memory-intensive nature of the game, we decompose
the game tree into two key components: (1) actual game states, and (2)
inherited scores from previous rounds. We construct the Full Game Tree by
pairing card states with accumulated scores in the Unfolding Process. This
design reduces memory overhead by storing only essential strategy values and
node connections. To further manage computational complexity, we apply a
round-by-round backward training strategy, starting from the final round and
recursively propagating average utilities to earlier stages. Our approach
constructs the complete game tree, which on average consists of over $10^9$
nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model
to predict these strategies for use during gameplay. We then estimate the fair
value of each deck through large-scale self-play between equilibrium strategies
by simulating, for instance, 10,000 games per matchup, executed in parallel
using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms
where the action tree naturally decomposes into multiple rounds such as
turn-based strategy games or sequential trading decisions in financial markets.

</details>


### [17] [Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop](https://arxiv.org/abs/2508.06569)
*Lance Yao,Suman Samantray,Ayana Ghosh,Kevin Roccapriore,Libor Kovarik,Sarah Allec,Maxim Ziatdinov*

Main category: cs.AI

TL;DR: SciLink是一个开源多智能体AI框架，旨在通过自动化链接实验观察、新颖性评估和理论模拟，在材料研究中实现意外发现。


<details>
  <summary>Details</summary>
Motivation: 现代自主实验室虽能加速假设验证，但效率优化可能忽略意外发现，SciLink旨在填补这一空白。

Method: 采用混合AI策略，结合机器学习模型定量分析实验数据和大型语言模型进行高级推理，将原始数据转化为可验证的科学主张，并根据文献评估新颖性。

Result: SciLink在多种研究场景中展现多功能性，包括原子分辨率和超光谱数据处理，整合实时专家指导，并提出针对性后续实验。

Conclusion: SciLink不仅提升效率，还通过系统分析所有观察结果，为AI驱动的材料研究提供了一个促进意外发现的实用框架。

Abstract: The history of science is punctuated by serendipitous discoveries, where
unexpected observations, rather than targeted hypotheses, opened new fields of
inquiry. While modern autonomous laboratories excel at accelerating hypothesis
testing, their optimization for efficiency risks overlooking these crucial,
unplanned findings. To address this gap, we introduce SciLink, an open-source,
multi-agent artificial intelligence framework designed to operationalize
serendipity in materials research by creating a direct, automated link between
experimental observation, novelty assessment, and theoretical simulations. The
framework employs a hybrid AI strategy where specialized machine learning
models perform quantitative analysis of experimental data, while large language
models handle higher-level reasoning. These agents autonomously convert raw
data from materials characterization techniques into falsifiable scientific
claims, which are then quantitatively scored for novelty against the published
literature. We demonstrate the framework's versatility across diverse research
scenarios, showcasing its application to atomic-resolution and hyperspectral
data, its capacity to integrate real-time human expert guidance, and its
ability to close the research loop by proposing targeted follow-up experiments.
By systematically analyzing all observations and contextualizing them, SciLink
provides a practical framework for AI-driven materials research that not only
enhances efficiency but also actively cultivates an environment ripe for
serendipitous discoveries, thereby bridging the gap between automated
experimentation and open-ended scientific exploration.

</details>


### [18] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: 论文提出IRL-VLA框架，通过三阶段方法解决VLA模型在自动驾驶中的闭环训练问题，结合模仿学习和逆强化学习，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在开环模仿学习中表现受限，闭环训练依赖高保真模拟，存在领域差距和计算效率问题。

Method: 三阶段方法：1) 模仿学习预训练VLA策略；2) 逆强化学习构建轻量级奖励世界模型；3) PPO优化奖励模型以平衡安全、舒适和效率。

Result: 在NAVSIM v2端到端驾驶基准中达到SOTA，CVPR2025自动驾驶挑战赛第二名。

Conclusion: IRL-VLA框架有望加速闭环自动驾驶中VLA模型的研究。

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [19] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）在视觉场景理解上表现流畅，但在对象计数能力上存在严重不足。CountQA是一个新基准，用于评估和提升MLLMs的计数能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在对象计数方面表现不佳，限制了其实际应用。现有基准无法测试复杂场景下的计数能力。

Method: 引入CountQA基准，包含1,500多个问答对，覆盖高密度、杂乱和遮挡的真实图像。评估了15种主流MLLMs。

Result: 表现最佳的模型准确率仅为42.9%，且随着对象数量增加性能下降。

Conclusion: CountQA为诊断和解决MLLMs的计数弱点提供了工具，推动更全面能力的MLLMs发展。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [20] [Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis](https://arxiv.org/abs/2508.06668)
*Jessie Galasso*

Main category: cs.AI

TL;DR: 本文总结了形式概念分析（FCA）在变异性分析中的关键属性及其应用方法。


<details>
  <summary>Details</summary>
Motivation: FCA虽然适合变异性分析，但其数学基础使得如何利用其属性进行变异性任务并不直观。本文旨在填补这一空白。

Method: 通过筛选FCA框架中对变异性分析至关重要的属性，并解释如何在概念结构中解读变异性信息。

Result: 明确了FCA中可用于变异性分析的关键属性及其应用方式。

Conclusion: 本文为FCA在变异性分析中的应用提供了实用指导，帮助更好地利用其数学框架。

Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge
representation and discovery. It performs a hierarchical clustering over a set
of objects described by attributes, resulting in conceptual structures in which
objects are organized depending on the attributes they share. These conceptual
structures naturally highlight commonalities and variabilities among similar
objects by categorizing them into groups which are then arranged by similarity,
making it particularly appropriate for variability extraction and analysis.
Despite the potential of FCA, determining which of its properties can be
leveraged for variability-related tasks (and how) is not always
straightforward, partly due to the mathematical orientation of its foundational
literature. This paper attempts to bridge part of this gap by gathering a
selection of properties of the framework which are essential to variability
analysis, and how they can be used to interpret diverse variability information
within the resulting conceptual structures.

</details>


### [21] [Zero-Shot Cellular Trajectory Map Matching](https://arxiv.org/abs/2508.06674)
*Weijie Shi,Yue Cui,Hao Chen,Jiaming Li,Mengze Li,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.AI

TL;DR: 提出了一种基于像素的轨迹校准辅助方法，用于零样本CTMM，通过迁移地理空间知识校准轨迹，并结合高斯混合模型和时空感知模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有CTMM方法依赖区域特定数据，适应性差，无法处理未探索区域。零样本CTMM需要提取区域自适应特征和位置不确定性。

Method: 使用像素化轨迹校准辅助，结合高斯混合模型和VAE识别场景自适应专家，设计时空感知模块捕捉序列特征和位置不确定性，最后通过约束路径查找算法重建道路ID序列。

Result: 实验表明，该方法在零样本CTMM中性能优于现有方法16.8%。

Conclusion: 该方法通过迁移知识和优化路径查找，显著提升了零样本CTMM的准确性和适应性。

Abstract: Cellular Trajectory Map-Matching (CTMM) aims to align cellular location
sequences to road networks, which is a necessary preprocessing in
location-based services on web platforms like Google Maps, including navigation
and route optimization. Current approaches mainly rely on ID-based features and
region-specific data to learn correlations between cell towers and roads,
limiting their adaptability to unexplored areas. To enable high-accuracy CTMM
without additional training in target regions, Zero-shot CTMM requires to
extract not only region-adaptive features, but also sequential and location
uncertainty to alleviate positioning errors in cellular data. In this paper, we
propose a pixel-based trajectory calibration assistant for zero-shot CTMM,
which takes advantage of transferable geospatial knowledge to calibrate
pixelated trajectory, and then guide the path-finding process at the road
network level. To enhance knowledge sharing across similar regions, a Gaussian
mixture model is incorporated into VAE, enabling the identification of
scenario-adaptive experts through soft clustering. To mitigate high positioning
errors, a spatial-temporal awareness module is designed to capture sequential
features and location uncertainty, thereby facilitating the inference of
approximate user positions. Finally, a constrained path-finding algorithm is
employed to reconstruct the road ID sequence, ensuring topological validity
within the road network. This process is guided by the calibrated trajectory
while optimizing for the shortest feasible path, thus minimizing unnecessary
detours. Extensive experiments demonstrate that our model outperforms existing
methods in zero-shot CTMM by 16.8\%.

</details>


### [22] [Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets](https://arxiv.org/abs/2508.06706)
*Jaikrishna Manojkumar Patil,Nathaniel Lee,Al Mehdi Saadat Chowdhury,YooJung Choi,Paulo Shakarian*

Main category: cs.AI

TL;DR: 论文提出了一种基于规则上下文和概率电路的知识图谱补全方法，显著减少了规则数量，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法需要大量规则才能达到高性能，但规则过多会降低可解释性。本文旨在通过规则上下文和概率分布优化规则使用。

Method: 从训练数据中发现规则上下文，并利用概率电路学习这些上下文的分布，以减少规则数量并提升性能。

Result: 方法减少了70-96%的规则数量，性能最高提升31倍，且保留基线91%的峰值性能。

Conclusion: 该方法在8个标准数据集上验证有效，为基于规则的推理提供了新思路。

Abstract: Rule-based methods for knowledge graph completion provide explainable results
but often require a significantly large number of rules to achieve competitive
performance. This can hinder explainability due to overwhelmingly large rule
sets. We discover rule contexts (meaningful subsets of rules that work
together) from training data and use learned probability distribution (i.e.
probabilistic circuits) over these rule contexts to more rapidly achieve
performance of the full rule set. Our approach achieves a 70-96% reduction in
number of rules used while outperforming baseline by up to 31$\times$ when
using equivalent minimal number of rules and preserves 91% of peak baseline
performance even when comparing our minimal rule sets against baseline's full
rule sets. We show that our framework is grounded in well-known semantics of
probabilistic logic, does not require independence assumptions, and that our
tractable inference procedure provides both approximate lower bounds and exact
probability of a given query. The efficacy of our method is validated by
empirical studies on 8 standard benchmark datasets where we show competitive
performance by using only a fraction of the rules required by AnyBURL's
standard inference method, the current state-of-the-art for rule-based
knowledge graph completion. This work may have further implications for general
probabilistic reasoning over learned sets of rules.

</details>


### [23] [GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning](https://arxiv.org/abs/2508.06716)
*Blair Johnson,Clayton Kerce,Faramarz Fekri*

Main category: cs.AI

TL;DR: GLIDR是一种可微分的归纳逻辑编程方法，通过更灵活的规则结构和消息传递推理算法，显著提升了知识图谱任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有链式规则结构的局限性影响了性能和可解释性，需要更灵活的规则学习方法。

Method: GLIDR采用可微分消息传递推理算法，支持分支和循环等复杂规则结构，并允许从权重中提取显式逻辑规则。

Result: GLIDR在知识图谱补全任务中优于现有规则学习方法，甚至可与嵌入方法竞争，且对训练数据噪声具有高鲁棒性。

Conclusion: GLIDR是一种高效、灵活且鲁棒的规则学习方法，适用于多模态数据，并能与深度神经网络结合进行端到端优化。

Abstract: Differentiable inductive logic programming (ILP) techniques have proven
effective at finding approximate rule-based solutions to link prediction and
node classification problems on knowledge graphs; however, the common
assumption of chain-like rule structure can hamper the performance and
interpretability of existing approaches. We introduce GLIDR, a differentiable
rule learning method that models the inference of logic rules with more
expressive syntax than previous methods. GLIDR uses a differentiable message
passing inference algorithm that generalizes previous chain-like rule learning
methods to allow rules with features like branches and cycles. GLIDR has a
simple and expressive rule search space which is parameterized by a limit on
the maximum number of free variables that may be included in a rule. Explicit
logic rules can be extracted from the weights of a GLIDR model for use with
symbolic solvers. We demonstrate that GLIDR can significantly outperform
existing rule learning methods on knowledge graph completion tasks and even
compete with embedding methods despite the inherent disadvantage of being a
structure-only prediction method. We show that rules extracted from GLIDR
retain significant predictive performance, and that GLIDR is highly robust to
training data noise. Finally, we demonstrate that GLIDR can be chained with
deep neural networks and optimized end-to-end for rule learning on arbitrary
data modalities.

</details>


### [24] [ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search](https://arxiv.org/abs/2508.06736)
*Alican Yilmaz,Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: ParBalans通过并行化扩展Balans，提升混合整数规划问题的求解性能。


<details>
  <summary>Details</summary>
Motivation: 混合整数规划问题计算资源消耗大，并行化是加速求解的关键策略。

Method: 提出ParBalans，结合求解器和算法级并行化，扩展Balans的并行能力。

Result: 实验表明ParBalans在复杂问题上性能优于商业求解器Gurobi。

Conclusion: ParBalans为混合整数规划问题提供了一种高效的并行求解方案。

Abstract: Solving Mixed-Integer Programming (MIP) problems often requires substantial
computational resources due to their combinatorial nature. Parallelization has
emerged as a critical strategy to accelerate solution times and enhance
scalability to tackle large, complex instances. This paper investigates the
parallelization capabilities of Balans, a recently proposed multi-armed
bandits-based adaptive large neighborhood search for MIPs. While Balans's
modular architecture inherently supports parallel exploration of diverse
parameter configurations, this potential has not been thoroughly examined. To
address this gap, we introduce ParBalans, an extension that leverages both
solver-level and algorithmic-level parallelism to improve performance on
challenging MIP instances. Our experimental results demonstrate that ParBalans
exhibits competitive performance compared to the state-of-the-art commercial
solver Gurobi, particularly on hard optimization benchmarks.

</details>


### [25] [Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism](https://arxiv.org/abs/2508.06746)
*Xin Tang,Qian Chen,Fengshun Li,Youchun Gong,Yinqiu Liu,Wen Tian,Shaowen Qin,Xiaohuan Li*

Main category: cs.AI

TL;DR: 论文提出了一种结合图扩散策略优化（GDPO）和Stackelberg博弈（SG）激励机制的无人机网络框架，以解决动态移动性和隐蔽通信的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着无人机网络在敏感应用中的需求增长，确保可靠连接和隐蔽通信变得至关重要，但动态移动性和暴露风险带来了挑战。

Method: 采用GDPO方法生成稀疏但连接良好的拓扑结构，并结合SG激励机制引导无人机选择支持合作和隐蔽通信的行为。

Result: 实验验证了框架在模型收敛性、拓扑生成质量和隐蔽通信性能提升方面的有效性。

Conclusion: 提出的框架能够有效应对无人机网络的动态性和隐蔽通信需求。

Abstract: With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in
sensitive applications, such as urban monitoring, emergency response, and
secure sensing, ensuring reliable connectivity and covert communication has
become increasingly vital. However, dynamic mobility and exposure risks pose
significant challenges. To tackle these challenges, this paper proposes a
self-organizing UAV network framework combining Graph Diffusion-based Policy
Optimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The
GDPO method uses generative AI to dynamically generate sparse but
well-connected topologies, enabling flexible adaptation to changing node
distributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game
(SG)-based incentive mechanism guides self-interested UAVs to choose relay
behaviors and neighbor links that support cooperation and enhance covert
communication. Extensive experiments are conducted to validate the
effectiveness of the proposed framework in terms of model convergence, topology
generation quality, and enhancement of covert communication performance.

</details>


### [26] [Pushing the Envelope of LLM Inference on AI-PC](https://arxiv.org/abs/2508.06753)
*Evangelos Georganas,Dhiraj Kalamkar,Alexander Heinecke*

Main category: cs.AI

TL;DR: 论文提出了一种针对1/1.58/2位超低比特LLM模型的优化微内核设计，集成到PyTorch-TPP框架中，实现了比现有SOTA运行时bitnet.cpp快2.2倍、比16位模型快7倍的推理性能。


<details>
  <summary>Details</summary>
Motivation: 探索超低比特LLM模型在资源受限环境（如边缘设备和AI PC）中的高效部署，填补现有推理运行时计算效率研究的空白。

Method: 设计并实现针对现代CPU优化的1位和2位微内核，集成到PyTorch-TPP框架中，进行端到端推理性能测试。

Result: 优化的运行时在2位模型上比bitnet.cpp快2.2倍，比16位模型快7倍。

Conclusion: 优化后的运行时推动了AI PC和边缘设备上LLM推理的发展，为超低比特LLM模型的高效部署铺平了道路。

Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the
perplexity and end-task performance of their full-precision counterparts using
the same model size, is ushering in a new era of LLM inference for
resource-constrained environments such as edge devices and AI PCs. While these
quantization advances promise models that are more cost-effective in terms of
latency, memory, throughput, and energy consumption, the computational
efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)
used to deploy them remains underexplored. In this work, we take a bottom-up
approach: we first design and implement 1-bit and 2-bit microkernels optimized
for modern CPUs, achieving peak computational efficiency across a variety of
CPU platforms. We integrate these microkernels into a state-of-the-art LLM
inference framework, namely PyTorch-TPP, and present end-to-end inference
results with 2-bit models that outperform the current SOTA runtime bitnet.cpp
by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model
inference. Our optimized runtime advances the state of LLM inference on AI PCs
and edge devices, paving the way for efficient deployment of ultra-low-bit LLM
models.

</details>


### [27] [A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks](https://arxiv.org/abs/2508.06754)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 提出了一种模块化提示框架，支持更安全、更自适应地使用大型语言模型（LLM），适用于动态、用户中心的任务。


<details>
  <summary>Details</summary>
Motivation: 基于人类学习理论（如最近发展区ZPD），旨在提升LLM在动态任务中的适应性和安全性。

Method: 结合自然语言边界提示和控制模式，采用模糊支架逻辑和适应规则，无需微调或外部协调。

Result: 在模拟智能辅导场景中，框架显著提升了支架质量、适应性和教学一致性，优于标准提示基线。

Conclusion: 该框架不仅适用于教育领域，还可扩展至其他交互密集型领域，为不确定或动态环境中的LLM行为提供了可重用方法。

Abstract: We introduce a modular prompting framework that supports safer and more
adaptive use of large language models (LLMs) across dynamic, user-centered
tasks. Grounded in human learning theory, particularly the Zone of Proximal
Development (ZPD), our method combines a natural language boundary prompt with
a control schema encoded with fuzzy scaffolding logic and adaptation rules.
This architecture enables LLMs to modulate behavior in response to user state
without requiring fine-tuning or external orchestration. In a simulated
intelligent tutoring setting, the framework improves scaffolding quality,
adaptivity, and instructional alignment across multiple models, outperforming
standard prompting baselines. Evaluation is conducted using rubric-based LLM
graders at scale. While initially developed for education, the framework has
shown promise in other interaction-heavy domains, such as procedural content
generation for games. Designed for safe deployment, it provides a reusable
methodology for structuring interpretable, goal-aligned LLM behavior in
uncertain or evolving contexts.

</details>


### [28] [Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation](https://arxiv.org/abs/2508.06823)
*Xuan Zhao,Jun Tao*

Main category: cs.AI

TL;DR: 提出了一种基于自然语言交互的框架，通过强化学习和CLIP Score机制优化体数据探索的视点选择。


<details>
  <summary>Details</summary>
Motivation: 体数据探索对科学数据集解释至关重要，但缺乏领域专业知识或3D导航经验的用户难以选择最佳视点。

Method: 框架将体数据块编码以区分结构，结合CLIP Score提供语义信息，并通过强化学习搜索符合用户意图的视点。

Result: 方法自动化视点选择，提升体数据导航效率和复杂科学现象的可解释性。

Conclusion: 通过自然语言交互和语义引导，框架显著优化了体数据探索的用户体验。

Abstract: Exploring volumetric data is crucial for interpreting scientific datasets.
However, selecting optimal viewpoints for effective navigation can be
challenging, particularly for users without extensive domain expertise or
familiarity with 3D navigation. In this paper, we propose a novel framework
that leverages natural language interaction to enhance volumetric data
exploration. Our approach encodes volumetric blocks to capture and
differentiate underlying structures. It further incorporates a CLIP Score
mechanism, which provides semantic information to the blocks to guide
navigation. The navigation is empowered by a reinforcement learning framework
that leverage these semantic cues to efficiently search for and identify
desired viewpoints that align with the user's intent. The selected viewpoints
are evaluated using CLIP Score to ensure that they best reflect the user
queries. By automating viewpoint selection, our method improves the efficiency
of volumetric data navigation and enhances the interpretability of complex
scientific phenomena.

</details>


### [29] [Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges](https://arxiv.org/abs/2508.06832)
*Haifeng Li,Wang Guo,Haiyang Wu,Mengwei Wu,Jipeng Zhang,Qing Zhu,Yu Liu,Xin Huang,Chao Tao*

Main category: cs.AI

TL;DR: 论文提出从视觉中心转向语言中心的遥感图像解释范式，借鉴全球工作空间理论（GWT），以大型语言模型（LLMs）为核心，整合感知、任务、知识和行动空间，实现统一理解、推理和决策。


<details>
  <summary>Details</summary>
Motivation: 现有视觉中心模型在多模态推理、语义抽象和交互决策方面存在局限性，缺乏统一的理论框架解释语言在认知中的作用。

Method: 提出语言中心框架，将LLMs作为认知中枢，整合多模态表示、知识关联、推理与决策，并构建全局工作空间驱动的解释机制。

Result: 总结了语言中心解决方案如何应对核心挑战，如统一多模态表示和知识关联，并提出了未来研究方向。

Conclusion: 为下一代遥感解释系统提供概念基础，建立认知驱动的智能地理空间分析路线图。

Abstract: The mainstream paradigm of remote sensing image interpretation has long been
dominated by vision-centered models, which rely on visual features for semantic
understanding. However, these models face inherent limitations in handling
multi-modal reasoning, semantic abstraction, and interactive decision-making.
While recent advances have introduced Large Language Models (LLMs) into remote
sensing workflows, existing studies primarily focus on downstream applications,
lacking a unified theoretical framework that explains the cognitive role of
language. This review advocates a paradigm shift from vision-centered to
language-centered remote sensing interpretation. Drawing inspiration from the
Global Workspace Theory (GWT) of human cognition, We propose a
language-centered framework for remote sensing interpretation that treats LLMs
as the cognitive central hub integrating perceptual, task, knowledge and action
spaces to enable unified understanding, reasoning, and decision-making. We
first explore the potential of LLMs as the central cognitive component in
remote sensing interpretation, and then summarize core technical challenges,
including unified multimodal representation, knowledge association, and
reasoning and decision-making. Furthermore, we construct a global
workspace-driven interpretation mechanism and review how language-centered
solutions address each challenge. Finally, we outline future research
directions from four perspectives: adaptive alignment of multimodal data, task
understanding under dynamic knowledge constraints, trustworthy reasoning, and
autonomous interaction. This work aims to provide a conceptual foundation for
the next generation of remote sensing interpretation systems and establish a
roadmap toward cognition-driven intelligent geospatial analysis.

</details>


### [30] [Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.06836)
*Xutong Zhao,Yaqi Xie*

Main category: cs.AI

TL;DR: 论文提出了一种多级优势信用分配方法（MACA），用于解决多智能体强化学习中的信用分配问题，通过多级优势函数和注意力机制捕捉不同层次的智能体贡献。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的信用分配问题复杂，尤其是当任务涉及不同层次的协作时，传统方法难以准确评估每个智能体的贡献。

Method: 提出MACA方法，通过多级优势函数（个体、联合和相关动作）和注意力框架，明确推理不同层次的信用分配。

Result: 在Starcraft v1&v2任务上的实验表明，MACA在复杂信用分配场景中表现优异。

Conclusion: MACA通过多级优势推理和注意力机制，有效解决了多智能体协作中的信用分配问题，适用于复杂任务。

Abstract: Cooperative multi-agent reinforcement learning (MARL) aims to coordinate
multiple agents to achieve a common goal. A key challenge in MARL is credit
assignment, which involves assessing each agent's contribution to the shared
reward. Given the diversity of tasks, agents may perform different types of
coordination, with rewards attributed to diverse and often overlapping agent
subsets. In this work, we formalize the credit assignment level as the number
of agents cooperating to obtain a reward, and address scenarios with multiple
coexisting levels. We introduce a multi-level advantage formulation that
performs explicit counterfactual reasoning to infer credits across distinct
levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures
agent contributions at multiple levels by integrating advantage functions that
reason about individual, joint, and correlated actions. Utilizing an
attention-based framework, MACA identifies correlated agent relationships and
constructs multi-level advantages to guide policy learning. Comprehensive
experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior
performance, underscoring its efficacy in complex credit assignment scenarios.

</details>


### [31] [Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method](https://arxiv.org/abs/2508.07586)
*Wenjing Zhang,Ye Hu,Tao Luo,Zhilong Zhang,Mingzhe Chen*

Main category: cs.AI

TL;DR: 提出了一种新颖的隐蔽语义通信框架，通过友好干扰器和时间槽优化策略保护语义信息传输，避免被攻击者窃取。


<details>
  <summary>Details</summary>
Motivation: 研究如何在语义通信中保护数据隐私，防止攻击者窃取语义信息。

Method: 采用优先采样辅助的双延迟深度确定性策略梯度算法，联合优化语义信息和传输功率。

Result: 仿真结果显示，相比传统强化学习方法，隐私保护效果提升77.8%，语义信息传输质量提升14.3%。

Conclusion: 提出的算法有效提升了语义通信的隐私保护和传输质量，无需服务器与干扰器之间的通信。

Abstract: In this paper, a novel covert semantic communication framework is
investigated. Within this framework, a server extracts and transmits the
semantic information, i.e., the meaning of image data, to a user over several
time slots. An attacker seeks to detect and eavesdrop the semantic transmission
to acquire details of the original image. To avoid data meaning being
eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming
signals to interfere the attacker so as to hide the transmitted semantic
information. Meanwhile, the server will strategically select time slots for
semantic information transmission. Due to limited energy, the jammer will not
communicate with the server and hence the server does not know the transmit
power of the jammer. Therefore, the server must jointly optimize the semantic
information transmitted at each time slot and the corresponding transmit power
to maximize the privacy and the semantic information transmission quality of
the user. To solve this problem, we propose a prioritised sampling assisted
twin delayed deep deterministic policy gradient algorithm to jointly determine
the transmitted semantic information and the transmit power per time slot
without the communications between the server and the jammer. Compared to
standard reinforcement learning methods, the propose method uses an additional
Q network to estimate Q values such that the agent can select the action with a
lower Q value from the two Q networks thus avoiding local optimal action
selection and estimation bias of Q values. Simulation results show that the
proposed algorithm can improve the privacy and the semantic information
transmission quality by up to 77.8% and 14.3% compared to the traditional
reinforcement learning methods.

</details>


### [32] [MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams](https://arxiv.org/abs/2508.06851)
*Pengfei Zhou,Xiaopeng Peng,Fanrui Zhang,Zhaopan Xu,Jiaxin Ai,Yansheng Qiu,Chuanhao Li,Zhen Li,Ming Li,Yukang Feng,Jianwen Sun,Haoquan Zhang,Zizhen Li,Xiaofeng Mao,Zekai Li,Wangbo Zhao,Kai Wang,Xiaojun Chang,Wenqi Shao,Yang You,Kaipeng Zhang*

Main category: cs.AI

TL;DR: MDK12-Bench是一个多学科大规模基准测试，用于评估多模态大语言模型（MLLMs）在多个维度上的表现，包括难度、时间变化、上下文变化和知识驱动推理。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs的基准测试存在规模小、覆盖窄、知识无结构化等问题，无法全面评估模型能力。

Method: 构建了MDK12-Bench基准，包含141K实例和6,225个知识点，采用动态评估框架和知识参考增强生成（KP-RAG）方法。

Result: 发现当前MLLMs在多个方面存在局限性，为提升模型鲁棒性和可解释性提供了指导。

Conclusion: MDK12-Bench为MLLMs的全面评估提供了新工具，并揭示了改进方向，尤其在AI辅助教育领域。

Abstract: Multimodal large language models (MLLMs), which integrate language and visual
cues for problem-solving, are crucial for advancing artificial general
intelligence (AGI). However, current benchmarks for measuring the intelligence
of MLLMs suffer from limited scale, narrow coverage, and unstructured
knowledge, offering only static and undifferentiated evaluations. To bridge
this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark
built from real-world K-12 exams spanning six disciplines with 141K instances
and 6,225 knowledge points organized in a six-layer taxonomy. Covering five
question formats with difficulty and year annotations, it enables comprehensive
evaluation to capture the extent to which MLLMs perform over four dimensions:
1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,
and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation
framework that introduces unfamiliar visual, textual, and question form shifts
to challenge model generalization while improving benchmark objectivity and
longevity by mitigating data contamination. We further evaluate knowledge-point
reference-augmented generation (KP-RAG) to examine the role of knowledge in
problem-solving. Key findings reveal limitations in current MLLMs in multiple
aspects and provide guidance for enhancing model robustness, interpretability,
and AI-assisted education.

</details>


### [33] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: 论文提出了一种基于AI的端到端天气预警系统，通过构建大规模多模态数据集MP-Bench和开发气象多模态大模型（MMLM），解决了现有方法在样本稀缺、数据对齐和多模态处理上的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前天气预警系统依赖人工专家解读，存在主观性和操作负担。AI技术的发展为自动化天气预测提供了新机遇，但面临样本稀缺、数据对齐和多模态处理等核心挑战。

Method: 构建了MP-Bench数据集（421,363对气象数据与文本描述），并开发了MMLM模型，通过自适应融合模块处理4D气象数据的时空依赖性。

Result: MMLM在MP-Bench上表现优异，验证了其在天气理解和预测任务中的有效性。

Conclusion: MMLM为自动化AI天气预警系统迈出了关键一步，代码和数据集将公开。

Abstract: Timely and accurate severe weather warnings are critical for disaster
mitigation. However, current forecasting systems remain heavily reliant on
manual expert interpretation, introducing subjectivity and significant
operational burdens. With the rapid development of AI technologies, the
end-to-end "AI weather station" is gradually emerging as a new trend in
predicting severe weather events. Three core challenges impede the development
of end-to-end AI severe weather system: (1) scarcity of severe weather event
samples; (2) imperfect alignment between high-dimensional meteorological data
and textual warnings; (3) existing multimodal language models are unable to
handle high-dimensional meteorological data and struggle to fully capture the
complex dependencies across temporal sequences, vertical pressure levels, and
spatial dimensions. To address these challenges, we introduce MP-Bench, the
first large-scale temporal multimodal dataset for severe weather events
prediction, comprising 421,363 pairs of raw multi-year meteorological data and
corresponding text caption, covering a wide range of severe weather scenarios
across China. On top of this dataset, we develop a meteorology multimodal large
model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is
designed to accommodate the unique characteristics of 4D meteorological data
flow, incorporating three plug-and-play adaptive fusion modules that enable
dynamic feature extraction and integration across temporal sequences, vertical
pressure layers, and spatial dimensions. Extensive experiments on MP-Bench
demonstrate that MMLM performs exceptionally well across multiple tasks,
highlighting its effectiveness in severe weather understanding and marking a
key step toward realizing automated, AI-driven weather forecasting systems. Our
source code and dataset will be made publicly available.

</details>


### [34] [Pushdown Reward Machines for Reinforcement Learning](https://arxiv.org/abs/2508.06894)
*Giovanni Varricchione,Toryn Q. Klassen,Natasha Alechina,Mehdi Dastani,Brian Logan,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 论文提出了推下奖励机（pdRMs），扩展了奖励机（RMs）的表达能力，支持确定性上下文无关语言的行为奖励，并验证了其在实际任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 奖励机（RMs）在强化学习中能显著提高样本效率，但其表达能力有限，仅支持正则语言。为了支持更复杂的行为（如确定性上下文无关语言），需要扩展RMs。

Method: 提出了推下奖励机（pdRMs），基于确定性下推自动机，支持更复杂的行为奖励。设计了两种策略：一种可访问整个堆栈，另一种仅访问堆栈顶部k个符号。

Result: 理论证明了pdRMs的表达能力更强，并提供了空间复杂度分析。实验表明，pdRMs能有效训练代理完成确定性上下文无关语言任务。

Conclusion: pdRMs扩展了RMs的表达能力，支持更复杂的行为奖励，并在理论和实验中验证了其有效性。

Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian)
reward functions for reinforcement learning (RL). RMs can reward any behaviour
representable in regular languages and, when paired with RL algorithms that
exploit RM structure, have been shown to significantly improve sample
efficiency in many domains. In this work, we present pushdown reward machines
(pdRMs), an extension of reward machines based on deterministic pushdown
automata. pdRMs can recognize and reward temporally extended behaviours
representable in deterministic context-free languages, making them more
expressive than reward machines. We introduce two variants of pdRM-based
policies, one which has access to the entire stack of the pdRM, and one which
can only access the top $k$ symbols (for a given constant $k$) of the stack. We
propose a procedure to check when the two kinds of policies (for a given
environment, pdRM, and constant $k$) achieve the same optimal expected reward.
We then provide theoretical results establishing the expressive power of pdRMs,
and space complexity results about the proposed learning problems. Finally, we
provide experimental results showing how agents can be trained to perform tasks
representable in deterministic context-free languages using pdRMs.

</details>


### [35] [GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization](https://arxiv.org/abs/2508.06899)
*Yanchen Deng,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 论文提出了一种名为DGLS的新方法，通过改进GDBA的三个问题点，显著提升了分布式约束优化问题的求解性能。


<details>
  <summary>Details</summary>
Motivation: GDBA在解决分布式约束优化问题时容易陷入局部最优，且性能提升有限。

Method: 提出DGLS框架，包括自适应约束违反条件、惩罚蒸发机制和同步惩罚更新方案。

Result: DGLS在标准基准测试中表现优异，尤其在结构化问题上显著优于现有方法。

Conclusion: DGLS通过改进GDBA的缺陷，成为解决分布式约束优化问题的有效方法。

Abstract: Local search is an important class of incomplete algorithms for solving
Distributed Constraint Optimization Problems (DCOPs) but it often converges to
poor local optima. While GDBA provides a comprehensive rule set to escape
premature convergence, its empirical benefits remain marginal on general-valued
problems. In this work, we systematically examine GDBA and identify three
factors that potentially lead to its inferior performance, i.e.,
over-aggressive constraint violation conditions, unbounded penalty
accumulation, and uncoordinated penalty updates. To address these issues, we
propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs
that incorporates an adaptive violation condition to selectively penalize
constraints with high cost, a penalty evaporation mechanism to control the
magnitude of penalization, and a synchronization scheme for coordinated penalty
updates. We theoretically show that the penalty values are bounded, and agents
play a potential game in our DGLS. Our extensive empirical results on various
standard benchmarks demonstrate the great superiority of DGLS over
state-of-the-art baselines. Particularly, compared to Damped Max-sum with high
damping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance
on general-valued problems, and outperforms it by significant margins
(\textbf{3.77\%--66.3\%}) on structured problems in terms of anytime results.

</details>


### [36] [Automated Formalization via Conceptual Retrieval-Augmented LLMs](https://arxiv.org/abs/2508.06931)
*Wangyue Lu,Lun Du,Sirui Li,Ke Weng,Haozhe Sun,Hengyu Liu,Minghe Yu,Tiancheng Zhang,Ge Yu*

Main category: cs.AI

TL;DR: CRAMF是一个基于概念驱动的检索增强数学形式化框架，通过检索核心数学概念的形式化定义，提升基于LLM的自动形式化能力，解决了模型幻觉和语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 交互式定理证明器（ITP）需要手动形式化，耗时且依赖专家知识。自动形式化面临模型幻觉和语义鸿沟的挑战。

Method: 提出CRAMF框架，从Mathlib4构建概念定义知识库，采用上下文查询增强和双通道混合检索策略。

Result: 在多个基准测试中，CRAMF显著提升了翻译准确性，最高相对提升达62.1%，平均提升29.9%。

Conclusion: CRAMF有效解决了自动形式化中的关键问题，为基于LLM的形式化提供了实用解决方案。

Abstract: Interactive theorem provers (ITPs) require manual formalization, which is
labor-intensive and demands expert knowledge. While automated formalization
offers a potential solution, it faces two major challenges: model hallucination
(e.g., undefined predicates, symbol misuse, and version incompatibility) and
the semantic gap caused by ambiguous or missing premises in natural language
descriptions. To address these issues, we propose CRAMF, a Concept-driven
Retrieval-Augmented Mathematical Formalization framework. CRAMF enhances
LLM-based autoformalization by retrieving formal definitions of core
mathematical concepts, providing contextual grounding during code generation.
However, applying retrieval-augmented generation (RAG) in this setting is
non-trivial due to the lack of structured knowledge bases, the polymorphic
nature of mathematical concepts, and the high precision required in formal
retrieval. We introduce a framework for automatically constructing a
concept-definition knowledge base from Mathlib4, the standard mathematical
library for the Lean 4 theorem prover, indexing over 26,000 formal definitions
and 1,000+ core mathematical concepts. To address conceptual polymorphism, we
propose contextual query augmentation with domain- and application-level
signals. In addition, we design a dual-channel hybrid retrieval strategy with
reranking to ensure accurate and relevant definition retrieval. Experiments on
miniF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that
CRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding
consistent improvements in translation accuracy, achieving up to 62.1% and an
average of 29.9% relative improvement.

</details>


### [37] [Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction](https://arxiv.org/abs/2508.06939)
*Hiba Najjar,Deepak Pathak,Marlon Nuske,Andreas Dengel*

Main category: cs.AI

TL;DR: 该研究利用基于Transformer的多模态学习模型进行作物产量预测，并通过自注意力机制解释模型，提出新方法评估特征和模态贡献。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在农业中有广泛应用，但模型可解释性常被忽视，本研究旨在利用Transformer的固有可解释性解决这一问题。

Method: 使用四种输入模态数据，基于自注意力机制提出Attention Rollout和Generic Attention方法，并与Shapley Value Sampling对比。

Result: Transformer模型优于卷积和循环网络，R2分数更高；Attention Rollout在时间属性解释上更可靠。

Conclusion: 研究验证了Transformer模型在农业多模态任务中的优势，并提供了可解释性方法，有助于理解模型决策。

Abstract: Multimodal learning enables various machine learning tasks to benefit from
diverse data sources, effectively mimicking the interplay of different factors
in real-world applications, particularly in agriculture. While the
heterogeneous nature of involved data modalities may necessitate the design of
complex architectures, the model interpretability is often overlooked. In this
study, we leverage the intrinsic explainability of Transformer-based models to
explain multimodal learning networks, focusing on the task of crop yield
prediction at the subfield level. The large datasets used cover various crops,
regions, and years, and include four different input modalities: multispectral
satellite and weather time series, terrain elevation maps and soil properties.
Based on the self-attention mechanism, we estimate feature attributions using
two methods, namely the Attention Rollout (AR) and Generic Attention (GA), and
evaluate their performance against Shapley-based model-agnostic estimations,
Shapley Value Sampling (SVS). Additionally, we propose the Weighted Modality
Activation (WMA) method to assess modality attributions and compare it with SVS
attributions. Our findings indicate that Transformer-based models outperform
other architectures, specifically convolutional and recurrent networks,
achieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field
levels, respectively. AR is shown to provide more robust and reliable temporal
attributions, as confirmed through qualitative and quantitative evaluation,
compared to GA and SVS values. Information about crop phenology stages was
leveraged to interpret the explanation results in the light of established
agronomic knowledge. Furthermore, modality attributions revealed varying
patterns across the two methods compared.[...]

</details>


### [38] [Large Language Models Do Not Simulate Human Psychology](https://arxiv.org/abs/2508.06950)
*Sarah Schröder,Thekla Morgenroth,Ulrike Kuhl,Valerie Vaquet,Benjamin Paaßen*

Main category: cs.AI

TL;DR: 论文警告不要用大语言模型（LLM）替代人类参与者进行心理学研究，并提供理论和实证证据表明LLM无法模拟人类心理。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM（如ChatGPT）是否能够模拟人类心理学并替代人类参与者，以避免潜在的研究误导。

Method: 通过概念论证和实证分析（如调整措辞导致LLM与人类反应差异）来验证LLM的不可靠性。

Result: LLM（包括专为心理学调优的CENTAUR模型）在细微措辞变化或新项目上表现不一致，无法可靠模拟人类心理。

Conclusion: LLM不能模拟人类心理，心理学研究应将其视为需与人类反应验证的工具。

Abstract: Large Language Models (LLMs),such as ChatGPT, are increasingly used in
research, ranging from simple writing assistance to complex data annotation
tasks. Recently, some research has suggested that LLMs may even be able to
simulate human psychology and can, hence, replace human participants in
psychological studies. We caution against this approach. We provide conceptual
arguments against the hypothesis that LLMs simulate human psychology. We then
present empiric evidence illustrating our arguments by demonstrating that
slight changes to wording that correspond to large changes in meaning lead to
notable discrepancies between LLMs' and human responses, even for the recent
CENTAUR model that was specifically fine-tuned on psychological responses.
Additionally, different LLMs show very different responses to novel items,
further illustrating their lack of reliability. We conclude that LLMs do not
simulate human psychology and recommend that psychological researchers should
treat LLMs as useful but fundamentally unreliable tools that need to be
validated against human responses for every new application.

</details>


### [39] [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
*Keyu Li,Mohan Jiang,Dayuan Fu,Yunze Wu,Xiangkun Hu,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 论文提出了DatasetResearch基准，评估AI代理在发现和合成数据集方面的能力，揭示了当前技术与完美数据集发现之间的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，数据可用性成为AI开发的瓶颈，许多有价值的数据集分散在各处。论文旨在探索AI代理是否能超越传统搜索，自主发现满足用户需求的数据集。

Method: 引入了DatasetResearch基准，包含208个真实需求，通过三维评估框架测试AI代理在知识密集型和推理密集型任务中的表现。

Result: 结果显示，即使是先进的深度研究系统，在挑战性子集DatasetResearch-pro上仅得22分，暴露了当前能力的不足。搜索代理在知识任务中表现优异，而合成代理在推理任务中占优，但两者在“极端案例”中均失败。

Conclusion: 论文为数据集发现代理建立了首个严格基准，为下一代自改进AI系统奠定了基础，并公开了基准和分析。

Abstract: The rapid advancement of large language models has fundamentally shifted the
bottleneck in AI development from computational power to data availability-with
countless valuable datasets remaining hidden across specialized repositories,
research appendices, and domain platforms. As reasoning capabilities and deep
research methodologies continue to evolve, a critical question emerges: can AI
agents transcend conventional search to systematically discover any dataset
that meets specific user requirements, enabling truly autonomous demand-driven
data curation? We introduce DatasetResearch, the first comprehensive benchmark
evaluating AI agents' ability to discover and synthesize datasets from 208
real-world demands across knowledge-intensive and reasoning-intensive tasks.
Our tri-dimensional evaluation framework reveals a stark reality: even advanced
deep research systems achieve only 22% score on our challenging
DatasetResearch-pro subset, exposing the vast gap between current capabilities
and perfect dataset discovery. Our analysis uncovers a fundamental
dichotomy-search agents excel at knowledge tasks through retrieval breadth,
while synthesis agents dominate reasoning challenges via structured
generation-yet both catastrophically fail on "corner cases" outside existing
distributions. These findings establish the first rigorous baseline for dataset
discovery agents and illuminate the path toward AI systems capable of finding
any dataset in the digital universe. Our benchmark and comprehensive analysis
provide the foundation for the next generation of self-improving AI systems and
are publicly available at https://github.com/GAIR-NLP/DatasetResearch.

</details>


### [40] [MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair](https://arxiv.org/abs/2508.06963)
*Changqing Li,Tianlin Li,Xiaohan Zhang,Aishan Liu,Li Pan*

Main category: cs.AI

TL;DR: MASteer是一个基于表示工程的端到端框架，用于修复大型语言模型（LLMs）的可信性问题，通过自动生成样本和自适应策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有修复方法（如SFT和RLHF）成本高且速度慢，提示工程缺乏鲁棒性和可扩展性，表示工程虽轻量但依赖人工样本和固定策略。

Method: MASteer结合AutoTester（多智能体生成样本）和AutoRepairer（自适应策略选择），实现自动化修复。

Result: 在LLaMA-3.1-8B-Chat和Qwen-3-8B-Chat上分别提升15.36%和4.21%，同时保持模型通用能力。

Conclusion: MASteer展示了高效、可扩展的可信性修复能力，具有鲁棒性和实用性。

Abstract: Large Language Models (LLMs) face persistent and evolving trustworthiness
issues, motivating developers to seek automated and flexible repair methods
that enable convenient deployment across diverse scenarios. Existing repair
methods like supervised fine-tuning (SFT) and reinforcement learning with human
feedback (RLHF) are costly and slow, while prompt engineering lacks robustness
and scalability. Representation engineering, which steers model behavior by
injecting targeted concept vectors during inference, offers a lightweight,
training-free alternative. However, current approaches depend on manually
crafted samples and fixed steering strategies, limiting automation and
adaptability. To overcome these challenges, we propose MASteer, the first
end-to-end framework for trustworthiness repair in LLMs based on representation
engineering. MASteer integrates two core components: AutoTester, a multi-agent
system that generates diverse, high-quality steer samples tailored to developer
needs; and AutoRepairer, which constructs adaptive steering strategies with
anchor vectors for automated, context-aware strategy selection during
inference. Experiments on standard and customized trustworthiness tasks show
MASteer consistently outperforms baselines, improving metrics by 15.36% on
LLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model
capabilities. MASteer demonstrates strong robustness, generalization, and
practical value for scalable, efficient trustworthiness repair.

</details>


### [41] [DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning](https://arxiv.org/abs/2508.06972)
*Dan Ivanov,Tristan Freiberg,Haruna Isah*

Main category: cs.AI

TL;DR: DSperse是一个模块化框架，用于分布式机器学习推理，通过战略性的加密验证实现高效和灵活性。


<details>
  <summary>Details</summary>
Motivation: 在分布式零知识机器学习的背景下，避免全模型电路化的高成本和僵化，提供针对性的验证策略。

Method: 通过验证选定的子计算（“切片”），支持部分或全部推理管道的验证，并通过审计、复制或经济激励确保全局一致性。

Result: 在多种证明系统下评估，展示了内存使用、运行时间和电路行为在切片与非切片配置下的表现。

Conclusion: DSperse通过灵活的验证边界支持可扩展的、针对性的验证策略，适应多样化的部署需求。

Abstract: DSperse is a modular framework for distributed machine learning inference
with strategic cryptographic verification. Operating within the emerging
paradigm of distributed zero-knowledge machine learning, DSperse avoids the
high cost and rigidity of full-model circuitization by enabling targeted
verification of strategically chosen subcomputations. These verifiable
segments, or "slices", may cover part or all of the inference pipeline, with
global consistency enforced through audit, replication, or economic incentives.
This architecture supports a pragmatic form of trust minimization, localizing
zero-knowledge proofs to the components where they provide the greatest value.
We evaluate DSperse using multiple proving systems and report empirical results
on memory usage, runtime, and circuit behavior under sliced and unsliced
configurations. By allowing proof boundaries to align flexibly with the model's
logical structure, DSperse supports scalable, targeted verification strategies
suited to diverse deployment needs.

</details>


### [42] [Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model](https://arxiv.org/abs/2508.06980)
*Aswin Paul,Moein Khajehnejad,Forough Habibollahi,Brett J. Kagan,Adeel Razi*

Main category: cs.AI

TL;DR: 论文提出了一种基于主动推理的框架，用于模拟生物神经元网络中的决策过程，为可解释AI提供了一种生物学基础的方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI的快速发展，理解自主智能体的目的性行为基础对开发安全高效系统至关重要。生物神经元网络可能提供更高效率和可解释性。

Method: 使用基于实验的生成模型，在模拟游戏环境中模拟决策过程，结合主动推理理论。

Result: 结果表明代理能够学习，揭示了记忆学习和预测规划在智能决策中的作用。

Conclusion: 该研究为可解释AI领域提供了一种生物学基础且可扩展的方法，有助于理解智能体的目的性行为。

Abstract: With recent and rapid advancements in artificial intelligence (AI),
understanding the foundation of purposeful behaviour in autonomous agents is
crucial for developing safe and efficient systems. While artificial neural
networks have dominated the path to AI, recent studies are exploring the
potential of biologically based systems, such as networks of living biological
neuronal networks. Along with promises of high power and data efficiency, these
systems may also inform more explainable and biologically plausible models. In
this work, we propose a framework rooted in active inference, a general theory
of behaviour, to model decision-making in embodied agents. Using
experiment-informed generative models, we simulate decision-making processes in
a simulated game-play environment, mirroring experimental setups that use
biological neurons. Our results demonstrate learning in these agents, providing
insights into the role of memory-based learning and predictive planning in
intelligent decision-making. This work contributes to the growing field of
explainable AI by offering a biologically grounded and scalable approach to
understanding purposeful behaviour in agents.

</details>


### [43] [Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach](https://arxiv.org/abs/2508.07015)
*Hannes Ihalainen,Dieter Vandesande,André Schidler,Jeremias Berg,Bart Bogaerts,Matti Järvisalo*

Main category: cs.AI

TL;DR: 本文探讨了隐式命中集（IHS）框架中替代优化技术的可行性，比较了伪布尔推理和随机局部搜索，发现商业整数规划求解器效率高但存在数值不稳定性问题，而基于伪布尔推理的精确计算在可靠性上更优。


<details>
  <summary>Details</summary>
Motivation: 研究隐式命中集（IHS）框架中替代优化技术的可行性，以解决现有方法（如整数规划）在效率和可靠性上的不足。

Method: 采用伪布尔（PB）推理和随机局部搜索作为替代优化技术，并与商业整数规划求解器进行比较。

Result: 商业整数规划求解器效率最高但存在数值不稳定性；基于伪布尔推理的精确计算在可靠性上更优，并能提供正确性证明。

Conclusion: 伪布尔推理在隐式命中集计算中具有竞争力，尤其在需要可靠性和正确性证明的场景中表现突出。

Abstract: The implicit hitting set (IHS) approach offers a general framework for
solving computationally hard combinatorial optimization problems declaratively.
IHS iterates between a decision oracle used for extracting sources of
inconsistency and an optimizer for computing so-called hitting sets (HSs) over
the accumulated sources of inconsistency. While the decision oracle is
language-specific, the optimizers is usually instantiated through integer
programming.
  We explore alternative algorithmic techniques for hitting set optimization
based on different ways of employing pseudo-Boolean (PB) reasoning as well as
stochastic local search. We extensively evaluate the practical feasibility of
the alternatives in particular in the context of pseudo-Boolean (0-1 IP)
optimization as one of the most recent instantiations of IHS. Highlighting a
trade-off between efficiency and reliability, while a commercial IP solver
turns out to remain the most effective way to instantiate HS computations, it
can cause correctness issues due to numerical instability; in fact, we show
that exact HS computations instantiated via PB reasoning can be made
competitive with a numerically exact IP solver. Furthermore, the use of PB
reasoning as a basis for HS computations allows for obtaining certificates for
the correctness of IHS computations, generally applicable to any IHS
instantiation in which reasoning in the declarative language at hand can be
captured in the PB-based proof format we employ.

</details>


### [44] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: MultiMedEdit是首个针对临床多模态任务的知识编辑（KE）基准，揭示了当前方法在复杂临床工作流中的局限性，并为未来开发提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑研究多集中于通用领域和医学QA任务，忽视了多模态医学场景的需求，而医学KE需要结合视觉推理以支持临床决策。

Method: 提出MultiMedEdit基准，涵盖理解和推理任务类型，定义三维度量标准（可靠性、通用性和局部性），支持跨范式比较。

Result: 实验表明当前方法在泛化和长尾推理方面表现不佳，尤其在复杂临床工作流中。效率分析揭示了实际部署中的权衡。

Conclusion: MultiMedEdit不仅揭示了当前方法的不足，还为未来开发临床稳健的KE技术奠定了基础。

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [45] [K-Dense Analyst: Towards Fully Automated Scientific Analysis](https://arxiv.org/abs/2508.07043)
*Orion Li,Vinayak Agarwal,Summer Zhou,Ashwin Gopinath,Timothy Kassis*

Main category: cs.AI

TL;DR: K-Dense Analyst是一个分层多代理系统，通过双循环架构实现自主生物信息学分析，性能优于GPT-5。


<details>
  <summary>Details</summary>
Motivation: 现代生物信息学分析的复杂性导致数据生成与科学洞察之间存在巨大差距，现有语言模型难以满足实际分析需求。

Method: 采用分层多代理系统，结合规划与验证执行，将复杂目标分解为可执行任务。

Result: 在BixBench上达到29.2%准确率，比GPT-5高6.3个百分点，性能显著提升。

Conclusion: 自主科学推理需要专门构建的系统，而不仅仅是增强的语言模型。

Abstract: The complexity of modern bioinformatics analysis has created a critical gap
between data generation and developing scientific insights. While large
language models (LLMs) have shown promise in scientific reasoning, they remain
fundamentally limited when dealing with real-world analytical workflows that
demand iterative computation, tool integration and rigorous validation. We
introduce K-Dense Analyst, a hierarchical multi-agent system that achieves
autonomous bioinformatics analysis through a dual-loop architecture. K-Dense
Analyst, part of the broader K-Dense platform, couples planning with validated
execution using specialized agents to decompose complex objectives into
executable, verifiable tasks within secure computational environments. On
BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense
Analyst achieves 29.2% accuracy, surpassing the best-performing language model
(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what
is widely considered the most powerful LLM available. Remarkably, K-Dense
Analyst achieves this performance using Gemini 2.5 Pro, which attains only
18.3% accuracy when used directly, demonstrating that our architectural
innovations unlock capabilities far beyond the underlying model's baseline
performance. Our insights demonstrate that autonomous scientific reasoning
requires more than enhanced language models, it demands purpose-built systems
that can bridge the gap between high-level scientific objectives and low-level
computational execution. These results represent a significant advance toward
fully autonomous computational biologists capable of accelerating discovery
across the life sciences.

</details>


### [46] [Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach](https://arxiv.org/abs/2508.07063)
*Naseem Machlovi,Maryam Saleki,Innocent Ababio,Ruhul Amin*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在内容审核中的局限性，提出了一种实验框架和优化模型SafePhi，表现优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在生活中的广泛应用，安全可靠的内容审核需求增加，但LLMs在道德推理和偏见处理上存在不足。

Method: 开发了一个基于SOTA模型的实验框架，引入统一基准数据集，并提出了优化模型SafePhi。

Result: SafePhi在Macro F1得分上达到0.89，优于OpenAI Moderator和Llama Guard。

Conclusion: 研究强调了LLMs在关键领域的不足，需结合更多异构数据和人类参与以提高模型鲁棒性和可解释性。

Abstract: As AI systems become more integrated into daily life, the need for safer and
more reliable moderation has never been greater. Large Language Models (LLMs)
have demonstrated remarkable capabilities, surpassing earlier models in
complexity and performance. Their evaluation across diverse tasks has
consistently showcased their potential, enabling the development of adaptive
and personalized agents. However, despite these advancements, LLMs remain prone
to errors, particularly in areas requiring nuanced moral reasoning. They
struggle with detecting implicit hate, offensive language, and gender biases
due to the subjective and context-dependent nature of these issues. Moreover,
their reliance on training data can inadvertently reinforce societal biases,
leading to inconsistencies and ethical concerns in their outputs. To explore
the limitations of LLMs in this role, we developed an experimental framework
based on state-of-the-art (SOTA) models to assess human emotions and offensive
behaviors. The framework introduces a unified benchmark dataset encompassing 49
distinct categories spanning the wide spectrum of human emotions, offensive and
hateful text, and gender and racial biases. Furthermore, we introduced SafePhi,
a QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and
outperforming benchmark moderators by achieving a Macro F1 score of 0.89, where
OpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This
research also highlights the critical domains where LLM moderators consistently
underperformed, pressing the need to incorporate more heterogeneous and
representative data with human-in-the-loop, for better model robustness and
explainability.

</details>


### [47] [Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention](https://arxiv.org/abs/2508.07107)
*Timothy Oluwapelumi Adeyemi,Nadiah Fahad AlOtaibi*

Main category: cs.AI

TL;DR: 提出了一种反馈驱动的决策支持系统（DSS），通过闭环架构和增量式训练提升学生成绩预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型多为静态，无法适应新数据（如干预后结果），需动态调整以提高预测准确性。

Method: 结合LightGBM回归器和增量式训练，支持实时更新模型；提供Flask界面和SHAP解释工具。

Result: 实验显示RMSE降低10.7%，干预学生预测分数持续提升。

Conclusion: 该系统将静态预测器转化为自优化系统，推动教育分析向以人为本、数据驱动和响应式AI发展。

Abstract: Accurate prediction of student performance is essential for timely academic
intervention. However, most machine learning models in education are static and
cannot adapt when new data, such as post-intervention outcomes, become
available. To address this limitation, we propose a Feedback-Driven Decision
Support System (DSS) with a closed-loop architecture that enables continuous
model refinement. The system integrates a LightGBM-based regressor with
incremental retraining, allowing educators to input updated student results,
which automatically trigger model updates. This adaptive mechanism improves
prediction accuracy by learning from real-world academic progress. The platform
features a Flask-based web interface for real-time interaction and incorporates
SHAP for explainability, ensuring transparency. Experimental results show a
10.7\% reduction in RMSE after retraining, with consistent upward adjustments
in predicted scores for intervened students. By transforming static predictors
into self-improving systems, our approach advances educational analytics toward
human-centered, data-driven, and responsive AI. The framework is designed for
integration into LMS and institutional dashboards.

</details>


### [48] [Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables](https://arxiv.org/abs/2508.07186)
*Amit Dhanda*

Main category: cs.AI

TL;DR: 提出了一种基于大型语言模型（LLM）代理的新框架，用于多维度结构化企业数据摘要，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统表格到文本模型在跨层次结构和上下文感知差异方面能力不足，无法满足商业报告需求。

Method: 采用多代理管道，包括数据切片、差异检测、上下文构建和基于LLM的生成。

Result: 框架在数据忠实度（83%）、显著变化覆盖率和决策关键见解相关性（4.4/5）上表现优异。

Conclusion: 该框架在复杂商业场景中显著提升了摘要的忠实性、相关性和见解质量。

Abstract: We propose a novel framework for summarizing structured enterprise data
across multiple dimensions using large language model (LLM)-based agents.
Traditional table-to-text models often lack the capacity to reason across
hierarchical structures and context-aware deltas, which are essential in
business reporting tasks. Our method introduces a multi-agent pipeline that
extracts, analyzes, and summarizes multi-dimensional data using agents for
slicing, variance detection, context construction, and LLM-based generation.
Our results show that the proposed framework outperforms traditional
approaches, achieving 83\% faithfulness to underlying data, superior coverage
of significant changes, and high relevance scores (4.4/5) for decision-critical
insights. The improvements are especially pronounced in categories involving
subtle trade-offs, such as increased revenue due to price changes amid
declining unit volumes, which competing methods either overlook or address with
limited specificity. We evaluate the framework on Kaggle datasets and
demonstrate significant improvements in faithfulness, relevance, and insight
quality over baseline table summarization approaches.

</details>


### [49] [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
*Yi Tang,Kaini Wang,Yang Chen,Guangquan Zhou*

Main category: cs.AI

TL;DR: EndoAgent是一个基于双记忆设计的AI代理，用于内窥镜图像诊断，通过短期行动追踪和长期经验学习提升决策能力，优于现有通用和医学多模态模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于大规模预训练的方法缺乏任务间的统一协调，难以处理复杂临床工作流中的多步骤过程，而AI代理在内窥镜领域的潜力尚未充分探索。

Method: 提出EndoAgent，结合迭代推理与自适应工具选择和协作，采用双记忆设计（短期行动追踪和长期经验学习），并集成专家设计的工具。

Result: 在EndoAgentBench基准测试中，EndoAgent在5,709个视觉问答对上表现优于通用和医学多模态模型，展示了其灵活性和推理能力。

Conclusion: EndoAgent为内窥镜图像诊断提供了一种灵活且高效的AI解决方案，展现了其在复杂临床任务中的潜力。

Abstract: Developing general artificial intelligence (AI) systems to support endoscopic
image diagnosis is an emerging research priority. Existing methods based on
large-scale pretraining often lack unified coordination across tasks and
struggle to handle the multi-step processes required in complex clinical
workflows. While AI agents have shown promise in flexible instruction parsing
and tool integration across domains, their potential in endoscopy remains
underexplored. To address this gap, we propose EndoAgent, the first
memory-guided agent for vision-to-decision endoscopic analysis that integrates
iterative reasoning with adaptive tool selection and collaboration. Built on a
dual-memory design, it enables sophisticated decision-making by ensuring
logical coherence through short-term action tracking and progressively
enhancing reasoning acuity through long-term experiential learning. To support
diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools
within a unified reasoning loop. We further introduce EndoAgentBench, a
benchmark of 5,709 visual question-answer pairs that assess visual
understanding and language generation capabilities in realistic scenarios.
Extensive experiments show that EndoAgent consistently outperforms both general
and medical multimodal models, exhibiting its strong flexibility and reasoning
capabilities.

</details>


### [50] [Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape](https://arxiv.org/abs/2508.07334)
*Quan Shi,Wang Xi,Zenghui Ding,Jianqing Gao,Xianjun Yang*

Main category: cs.AI

TL;DR: 论文分析了大型语言模型（LLMs）的幻觉现象，提出计算必要性层次结构，证明幻觉不可避免，并提出两种解决方案：检索增强生成（RAGs）和持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs的幻觉现象，提高其可靠性。

Method: 构建计算必要性层次结构，证明幻觉的不可避免性，并提出RAGs和持续学习作为解决方案。

Result: 证明了幻觉的不可避免性，并提供了两种有效的解决方案。

Conclusion: 通过RAGs和持续学习，可以部分解决LLMs的幻觉问题。

Abstract: The illusion phenomenon of large language models (LLMs) is the core obstacle
to their reliable deployment. This article formalizes the large language model
as a probabilistic Turing machine by constructing a "computational necessity
hierarchy", and for the first time proves the illusions are inevitable on
diagonalization, incomputability, and information theory boundaries supported
by the new "learner pump lemma". However, we propose two "escape routes": one
is to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving
their absolute escape through "computational jumps", providing the first formal
theory for the effectiveness of RAGs; The second is to formalize continuous
learning as an "internalized oracle" mechanism and implement this path through
a novel neural game theory framework.Finally, this article proposes a

</details>


### [51] [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
*Rubing Chen,Jiaxin Wu,Jian Wang,Xulu Zhang,Wenqi Fan,Chenghua Lin,Xiao-Yong Wei,Qing Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于全面性-紧凑性原则的迭代基准框架Comp-Comp，用于优化领域特定大型语言模型的基准构建，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定基准主要依赖扩展法则，但语料库和问答集设计对模型精确度和召回率的影响尚未探索。

Method: 提出Comp-Comp框架，结合全面性和紧凑性原则指导语料库和问答集构建，并通过案例研究验证。

Result: 创建了XUBench，一个大规模且全面的封闭领域基准，证明了Comp-Comp框架的有效性。

Conclusion: Comp-Comp框架不仅适用于学术领域，还可扩展到其他领域，为基准构建提供了新思路。

Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities
of large language models (LLMs), highlighting the need for effective and
efficient benchmark construction. Existing domain-specific benchmarks primarily
focus on the scaling law, relying on massive corpora for supervised fine-tuning
or generating extensive question sets for broad coverage. However, the impact
of corpus and question-answer (QA) set design on the precision and recall of
domain-specific LLMs remains unexplored. In this paper, we address this gap and
demonstrate that the scaling law is not always the optimal principle for
benchmark construction in specific domains. Instead, we propose Comp-Comp, an
iterative benchmarking framework based on a comprehensiveness-compactness
principle. Here, comprehensiveness ensures semantic recall of the domain, while
compactness enhances precision, guiding both corpus and QA set construction. To
validate our framework, we conducted a case study in a well-renowned
university, resulting in the creation of XUBench, a large-scale and
comprehensive closed-domain benchmark. Although we use the academic domain as
the case in this work, our Comp-Comp framework is designed to be extensible
beyond academia, providing valuable insights for benchmark construction across
various domains.

</details>


### [52] [Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning](https://arxiv.org/abs/2508.07382)
*He Kong,Die Hu,Jingguo Ge,Liangxiong Li,Hui Li,Tong Li*

Main category: cs.AI

TL;DR: Pentest-R1是一个通过两阶段强化学习优化LLM在渗透测试中推理能力的新框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在渗透测试中存在错误处理差、推理效率低和无法自主完成复杂任务的问题，亟需改进。

Method: 采用两阶段强化学习：离线RL学习基础攻击逻辑，在线RL在CTF环境中通过环境反馈进行微调。

Result: 在AutoPenBench上达到24.2%成功率，Cybench上15.0%成功率，性能接近顶级专有模型。

Conclusion: 两阶段训练协同作用对Pentest-R1的成功至关重要，为开源LLM设定了新标杆。

Abstract: Automating penetration testing is crucial for enhancing cybersecurity, yet
current Large Language Models (LLMs) face significant limitations in this
domain, including poor error handling, inefficient reasoning, and an inability
to perform complex end-to-end tasks autonomously. To address these challenges,
we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning
capabilities for this task through a two-stage reinforcement learning pipeline.
We first construct a dataset of over 500 real-world, multi-step walkthroughs,
which Pentest-R1 leverages for offline reinforcement learning (RL) to instill
foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in
an interactive Capture The Flag (CTF) environment, where it learns directly
from environmental feedback to develop robust error self-correction and
adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench
benchmarks demonstrate the framework's effectiveness. On AutoPenBench,
Pentest-R1 achieves a 24.2\% success rate, surpassing most state-of-the-art
models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a
15.0\% success rate in unguided tasks, establishing a new state-of-the-art for
open-source LLMs and matching the performance of top proprietary models.
Ablation studies confirm that the synergy of both training stages is critical
to its success.

</details>


### [53] [Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding](https://arxiv.org/abs/2508.07388)
*Zhaoyu Chen,Hongnan Lin,Yongwei Nie,Fei Ma,Xuemiao Xu,Fei Yu,Chengjiang Long*

Main category: cs.AI

TL;DR: Invert4TVG框架通过三个反转任务增强视频片段定位和动作理解，显著提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖时间IoU指标，牺牲了语义动作理解，影响TVG的鲁棒性。

Method: 提出三个反转任务（动词补全、动作识别、视频描述），结合强化学习框架优化定位和语义。

Result: 实验显示方法优于现有技术，Charades-STA上R1@0.7提升7.1%。

Conclusion: 通过反转任务增强语义理解，显著提高了定位精度的上限。

Abstract: Temporal Video Grounding (TVG) seeks to localize video segments matching a
given textual query. Current methods, while optimizing for high temporal
Intersection-over-Union (IoU), often overfit to this metric, compromising
semantic action understanding in the video and query, a critical factor for
robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),
a novel framework that enhances both localization accuracy and action
understanding without additional data. Our approach leverages three inversion
tasks derived from existing TVG annotations: (1) Verb Completion, predicting
masked action verbs in queries from video segments; (2) Action Recognition,
identifying query-described actions; and (3) Video Description, generating
descriptions of video segments that explicitly embed query-relevant actions.
These tasks, integrated with TVG via a reinforcement learning framework with
well-designed reward functions, ensure balanced optimization of localization
and semantics. Experiments show our method outperforms state-of-the-art
approaches, achieving a 7.1\% improvement in R1@0.7 on Charades-STA for a 3B
model compared to Time-R1. By inverting TVG to derive query-related actions
from segments, our approach strengthens semantic understanding, significantly
raising the ceiling of localization accuracy.

</details>


### [54] [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
*Jesse Ponnock*

Main category: cs.AI

TL;DR: 论文提出了一种利用生成式人工智能（GAI）为大型政府组织开发战略计划的模块化模型，并评估了BERTopic和非负矩阵分解（NMF）在主题建模中的表现。结果显示，这些技术能生成与战略计划愿景元素相似的主题，其中BERTopic表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用GAI和大型语言模型（LLMs）自动化专业服务，特别是在政府战略计划开发中，以应对传统上难以自动化的任务。

Method: 使用BERTopic和NMF模型对政府问责办公室（GAO）的大量报告进行主题建模，生成的主题与已发布的战略计划愿景元素进行相似性评分比较。

Result: BERTopic和NMF能生成与100%的愿景元素相似的主题，其中BERTopic表现更优，超过一半的主题达到“中等”或“强”相关性。

Conclusion: GAI在战略计划开发中具有潜力，未来工作将聚焦于模型的实际应用和剩余模块的可行性验证。

Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and
Large Language Models (LLMs), more and more professional services are being
augmented through Artificial Intelligence (AI), which once seemed impossible to
automate. This paper presents a modular model for leveraging GAI in developing
strategic plans for large scale government organizations and evaluates leading
machine learning techniques in their application towards one of the identified
modules. Specifically, the performance of BERTopic and Non-negative Matrix
Factorization (NMF) are evaluated in their ability to use topic modeling to
generate themes representative of Vision Elements within a strategic plan. To
accomplish this, BERTopic and NMF models are trained using a large volume of
reports from the Government Accountability Office (GAO). The generated topics
from each model are then scored for similarity against the Vision Elements of a
published strategic plan and the results are compared. Our results show that
these techniques are capable of generating themes similar to 100% of the
elements being evaluated against. Further, we conclude that BERTopic performs
best in this application with more than half of its correlated topics achieving
a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan
development impacts a multi-billion dollar industry and assists the federal
government in overcoming regulatory requirements which are crucial to the
public good. Further work will focus on the operationalization of the concept
proven in this study as well as viability of the remaining modules in the
proposed model for GAI-generated strategic plans.

</details>


### [55] [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
*Jinyuan Fang,Yanwen Peng,Xi Zhang,Yingxu Wang,Xinhao Yi,Guibin Zhang,Yi Xu,Bin Wu,Siwei Liu,Zihao Li,Zhaochun Ren,Nikos Aletras,Xi Wang,Han Zhou,Zaiqiao Meng*

Main category: cs.AI

TL;DR: 本文综述了自进化AI代理系统的现有技术，提出了一个统一的概念框架，并探讨了领域特定策略及伦理考量。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理系统多为静态配置，无法适应动态环境，因此需要研究自进化技术以实现持续适应性。

Method: 提出一个包含系统输入、代理系统、环境和优化器的统一框架，并系统回顾针对不同组件的自进化技术。

Result: 综述了多种自进化技术及领域特定策略，并讨论了评估、安全和伦理问题。

Conclusion: 本文为开发更具适应性和自主性的终身代理系统奠定了基础。

Abstract: Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
agent systems rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
environments. To this end, recent research has explored agent evolution
techniques that aim to automatically enhance agent systems based on interaction
data and environmental feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
self-evolving agentic systems. Specifically, we first introduce a unified
conceptual framework that abstracts the feedback loop underlying the design of
self-evolving agentic systems. The framework highlights four key components:
System Inputs, Agent System, Environment, and Optimisers, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the agent system. We also investigate
domain-specific evolution strategies developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the evaluation, safety, and ethical considerations for
self-evolving agentic systems, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.

</details>


### [56] [Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs](https://arxiv.org/abs/2508.07466)
*Dom Huh,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体大型语言模型（LLM）的系统框架，通过整合多智能体决策算法，提升LLM在协作与推理中的能力。


<details>
  <summary>Details</summary>
Motivation: 语言是推理与协作的基础，建立共同语言有助于清晰沟通和多智能体协调。本文旨在扩展LLM在多智能体环境中的应用能力。

Method: 提出系统框架，包括高级提示工程、有效记忆架构、多模态信息处理和微调算法对齐策略。

Result: 通过经典游戏场景的消融实验验证了设计选择的有效性。

Conclusion: 该框架为多智能体LLM的设计提供了实用方法，增强了其在复杂协作任务中的表现。

Abstract: Language is a ubiquitous tool that is foundational to reasoning and
collaboration, ranging from everyday interactions to sophisticated
problem-solving tasks. The establishment of a common language can serve as a
powerful asset in ensuring clear communication and understanding amongst
agents, facilitating desired coordination and strategies. In this work, we
extend the capabilities of large language models (LLMs) by integrating them
with advancements in multi-agent decision-making algorithms. We propose a
systematic framework for the design of multi-agentic large language models
(LLMs), focusing on key integration practices. These include advanced prompt
engineering techniques, the development of effective memory architectures,
multi-modal information processing, and alignment strategies through
fine-tuning algorithms. We evaluate these design choices through extensive
ablation studies on classic game settings with significant underlying social
dilemmas and game-theoretic considerations.

</details>


### [57] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 提出了一种基于纯代理策略的新方法，通过Python编码代理和ReAct原则，成功解决了CP-Bench基准集中的所有101个问题。


<details>
  <summary>Details</summary>
Motivation: 将自然语言问题描述转化为形式化约束模型是一个基础性挑战，传统固定流程方法在多数基准问题上失败。

Method: 使用基于ReAct原则的Python编码代理，通过持久IPython内核实现状态化代码执行和迭代开发，依赖精心设计的项目提示注入领域知识。

Result: 该方法成功解决了CP-Bench基准集中的所有101个问题。

Conclusion: 约束建模任务需要通用编码工具和提示编码的领域知识，而非专用代理架构或预定义流程。

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [58] [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
*Alexander Duffy,Samuel J Paech,Ishana Shastri,Elizabeth Karpinski,Baptiste Alloui-Cros,Tyler Marques,Matthew Lyle Olson*

Main category: cs.AI

TL;DR: 提出了一个无需微调或专门训练的评估工具，使本地大型语言模型（LLM）能够玩完整版《外交》游戏。


<details>
  <summary>Details</summary>
Motivation: 由于《外交》游戏状态的高复杂性和信息密度，先前研究需要前沿LLM或微调，限制了研究的普及。

Method: 通过数据驱动的迭代优化文本游戏状态表示，开发工具支持假设测试和统计分析，并引入关键状态分析协议。

Result: 实验表明，较大模型表现最佳，但较小模型也能胜任；工具为LLM的战略推理能力提供了自然涌现的见解。

Conclusion: 该工具消除了微调需求，普及了LLM战略推理能力的评估，并提供了开源代码。

Abstract: We present the first evaluation harness that enables any out-of-the-box,
local, Large Language Models (LLMs) to play full-press Diplomacy without
fine-tuning or specialized training. Previous work required frontier LLMs, or
fine-tuning, due to the high complexity and information density of Diplomacy's
game state. Combined with the high variance of matches, these factors made
Diplomacy prohibitive for study. In this work, we used data-driven iteration to
optimize a textual game state representation such that a 24B model can reliably
complete matches without any fine tuning. We develop tooling to facilitate
hypothesis testing and statistical analysis, and we present case studies on
persuasion, aggressive playstyles, and performance across a range of models. We
conduct a variety of experiments across many popular LLMs, finding the larger
models perform the best, but the smaller models still play adequately. We also
introduce Critical State Analysis: an experimental protocol for rapidly
iterating and analyzing key moments in a game at depth. Our harness
democratizes the evaluation of strategic reasoning in LLMs by eliminating the
need for fine-tuning, and it provides insights into how these capabilities
emerge naturally from widely used LLMs. Our code is available in the supplement
and will be open sourced.

</details>


### [59] [MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark](https://arxiv.org/abs/2508.07575)
*Shiqing Fan,Xichen Ding,Liang Zhang,Linjian Mo*

Main category: cs.AI

TL;DR: 论文提出MCPToolBench++，一个用于评估LLMs调用MCP工具性能的大规模多领域基准，解决了现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面的数据集或基准来评估LLMs和AI代理在MCP工具使用上的能力，且MCP工具的多样性和上下文窗口限制增加了评估难度。

Method: 构建MCPToolBench++基准，基于4k多个MCP服务器和40多个类别的数据集，包含单步和多步工具调用。

Result: 评估了具有代理能力的SOTA LLMs，并报告了结果。

Conclusion: MCPToolBench++为评估LLMs在MCP工具调用上的性能提供了有效解决方案。

Abstract: LLMs' capabilities are enhanced by using function calls to integrate various
data sources or API results into the context window. Typical tools include
search, web crawlers, maps, financial data, file systems, and browser usage,
etc. Integrating these data sources or functions requires a standardized
method. The Model Context Protocol (MCP) provides a standardized way to supply
context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use
abilities suffer from several issues. First, there's a lack of comprehensive
datasets or benchmarks to evaluate various MCP tools. Second, the diverse
formats of response from MCP tool call execution further increase the
difficulty of evaluation. Additionally, unlike existing tool-use benchmarks
with high success rates in functions like programming and math functions, the
success rate of real-world MCP tool is not guaranteed and varies across
different MCP servers. Furthermore, the LLMs' context window also limits the
number of available tools that can be called in a single run, because the
textual descriptions of tool and the parameters have long token length for an
LLM to process all at once. To help address the challenges of evaluating LLMs'
performance on calling MCP tools, we propose MCPToolBench++, a large-scale,
multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is
build upon marketplace of over 4k MCP servers from more than 40 categories,
collected from the MCP marketplaces and GitHub communities. The datasets
consist of both single-step and multi-step tool calls across different
categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and
reported the results.

</details>


### [60] [HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol](https://arxiv.org/abs/2508.07602)
*Wenpeng Xing,Zhipeng Chen,Changting Lin,Meng Han*

Main category: cs.AI

TL;DR: HGMF是一种概率剪枝方法，通过分层高斯混合模型提升LLM在大规模工具库中选择工具的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在大型分层工具库中选择工具时因上下文窗口限制和无关选项噪声导致的低准确性和高计算成本问题。

Method: HGMF将用户查询和工具描述映射到统一语义空间，分两阶段进行高斯混合模型聚类和基于查询可能性的筛选，生成高相关性候选集。

Result: 实验表明HGMF显著提高了工具选择准确性并降低了推理延迟。

Conclusion: HGMF具有可扩展性和高效性，适用于大规模工具库。

Abstract: Invoking external tools enables Large Language Models (LLMs) to perform
complex, real-world tasks, yet selecting the correct tool from large,
hierarchically-structured libraries remains a significant challenge. The
limited context windows of LLMs and noise from irrelevant options often lead to
low selection accuracy and high computational costs. To address this, we
propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic
pruning method for scalable tool invocation. HGMF first maps the user query and
all tool descriptions into a unified semantic space. The framework then
operates in two stages: it clusters servers using a Gaussian Mixture Model
(GMM) and filters them based on the query's likelihood. Subsequently, it
applies the same GMM-based clustering and filtering to the tools associated
with the selected servers. This hierarchical process produces a compact,
high-relevance candidate set, simplifying the final selection task for the LLM.
Experiments on a public dataset show that HGMF significantly improves tool
selection accuracy while reducing inference latency, confirming the framework's
scalability and effectiveness for large-scale tool libraries.

</details>


### [61] [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
*Aswin RRV,Jacob Dineen,Divij Handa,Md Nayem Uddin,Mihir Parmar,Chitta Baral,Ben Zhou*

Main category: cs.AI

TL;DR: 论文提出ThinkTuning方法，通过教师模型对学生模型的反馈指导，提升学生模型的推理能力，实验显示在多任务基准上有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习（RL）方法仅能挖掘基础模型已有的推理行为，而无法真正培养新的推理能力。如何让不具备此类行为的模型发展出推理能力成为研究动机。

Method: 采用GRPO基础的交互式训练方法ThinkTuning，通过教师模型对学生模型的答案提供反馈，逐步引导其推理能力。

Result: 在多个基准测试中，ThinkTuning平均比零样本基线提升3.85%，在MATH-500、AIME和GPQA-Diamond上分别提升2.08%、2.23%和3.99%。

Conclusion: ThinkTuning通过教师模型的反馈指导，有效提升了学生模型的推理能力，为培养新型推理行为提供了可行方案。

Abstract: Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

</details>


### [62] [Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization](https://arxiv.org/abs/2508.07628)
*Daniel Essien,Suresh Neethirajan*

Main category: cs.AI

TL;DR: 论文提出利用多模态人工智能（AI）替代传统主观、劳动密集型的家禽福利监测方法，通过整合视觉、声学、环境和生理数据，实现更全面的蛋鸡福利评估。


<details>
  <summary>Details</summary>
Motivation: 传统家禽福利评估方法受限于人工观察和单一传感器数据，无法全面反映现代农场中蛋鸡福利的多维复杂性。

Method: 采用多模态AI，提出中间（特征级）融合策略，并引入两个新评估工具（DTS和DRI）以及模块化部署框架。

Result: 中间融合策略在鲁棒性和性能之间取得最佳平衡，且比早期或晚期融合更具可扩展性。

Conclusion: 研究为从被动、单模态监测转向主动、精准驱动的福利系统奠定了基础，将生产力与科学伦理相结合。

Abstract: The future of poultry production depends on a paradigm shift replacing
subjective, labor-intensive welfare checks with data-driven, intelligent
monitoring ecosystems. Traditional welfare assessments-limited by human
observation and single-sensor data-cannot fully capture the complex,
multidimensional nature of laying hen welfare in modern farms. Multimodal
Artificial Intelligence (AI) offers a breakthrough, integrating visual,
acoustic, environmental, and physiological data streams to reveal deeper
insights into avian welfare dynamics. This investigation highlights multimodal
As transformative potential, showing that intermediate (feature-level) fusion
strategies achieve the best balance between robustness and performance under
real-world poultry conditions, and offer greater scalability than early or late
fusion approaches. Key adoption barriers include sensor fragility in harsh farm
environments, high deployment costs, inconsistent behavioral definitions, and
limited cross-farm generalizability. To address these, we introduce two novel
evaluation tools - the Domain Transfer Score (DTS) to measure model
adaptability across diverse farm settings, and the Data Reliability Index (DRI)
to assess sensor data quality under operational constraints. We also propose a
modular, context-aware deployment framework designed for laying hen
environments, enabling scalable and practical integration of multimodal
sensing. This work lays the foundation for a transition from reactive, unimodal
monitoring to proactive, precision-driven welfare systems that unite
productivity with ethical, science based animal care.

</details>


### [63] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: SkillNav是一个模块化框架，通过将导航任务分解为可解释的原子技能，并利用VLM路由动态选择最佳技能代理，显著提升了VLN任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLN方法在复杂空间和时间推理的未见场景中泛化能力不足，需要更结构化的方法。

Method: SkillNav将导航任务分解为原子技能（如垂直移动、区域识别等），每个技能由专门代理处理，并使用VLM路由动态选择代理。

Result: 在R2R和GSA-R2R基准测试中达到新SOTA，表现出强大的泛化能力。

Conclusion: SkillNav通过模块化和技能分解显著提升了VLN任务的性能和泛化能力。

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


### [64] [Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation](https://arxiv.org/abs/2508.07649)
*Jie Li,Haoye Dong,Zhengyang Wu,Zetao Zheng,Mingrong Lin*

Main category: cs.AI

TL;DR: DiMuST是一种基于解耦表示学习的POI推荐模型，通过多时空转换图和社会关系提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法将时空转换分开建模，导致关键节点表示不一致，引入冗余信息，降低模型可解释性。

Method: 提出DiMuST模型，使用解耦变分多图自编码器（DAE）分离共享和私有分布，通过PoE机制融合共享特征，对比约束去噪私有特征。

Result: 在两个数据集上，DiMuST在多个指标上显著优于现有方法。

Conclusion: DiMuST有效捕捉POI的时空转换表示，同时保持其时空关系的固有相关性。

Abstract: Next Point-of-Interest (POI) recommendation is a research hotspot in business
intelligence, where users' spatial-temporal transitions and social
relationships play key roles. However, most existing works model spatial and
temporal transitions separately, leading to misaligned representations of the
same spatial-temporal key nodes. This misalignment introduces redundant
information during fusion, increasing model uncertainty and reducing
interpretability. To address this issue, we propose DiMuST, a socially enhanced
POI recommendation model based on disentangled representation learning over
multiplex spatial-temporal transition graphs. The model employs a novel
Disentangled variational multiplex graph Auto-Encoder (DAE), which first
disentangles shared and private distributions using a multiplex
spatial-temporal graph strategy. It then fuses the shared features via a
Product of Experts (PoE) mechanism and denoises the private features through
contrastive constraints. The model effectively captures the spatial-temporal
transition representations of POIs while preserving the intrinsic correlation
of their spatial-temporal relationships. Experiments on two challenging
datasets demonstrate that our DiMuST significantly outperforms existing methods
across multiple metrics.

</details>


### [65] [1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning](https://arxiv.org/abs/2508.07667)
*Wenkai Li,Liwen Sun,Zhenxiang Guan,Xuhui Zhou,Maarten Sap*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体框架，通过分解隐私推理任务来减少隐私信息泄露，实验结果显示该方法显著降低了泄露率。


<details>
  <summary>Details</summary>
Motivation: 解决交互式环境中大型语言模型（LLMs）处理多源信息时的隐私问题，尤其是在混合公开和私人信息的场景中。

Method: 引入多智能体框架，将隐私推理分解为提取和分类等子任务，并通过信息流拓扑分析隐私错误的传播。

Result: 在ConfAIde和PrivacyLens基准测试中，最佳多智能体配置显著减少了隐私信息泄露（GPT-4o下分别降低18%和19%），同时保持了公共内容的准确性。

Conclusion: 研究表明，多智能体系统中的信息流设计可以有效提升LLMs在上下文隐私保护中的表现。

Abstract: Addressing contextual privacy concerns remains challenging in interactive
settings where large language models (LLMs) process information from multiple
sources (e.g., summarizing meetings with private and public information). We
introduce a multi-agent framework that decomposes privacy reasoning into
specialized subtasks (extraction, classification), reducing the information
load on any single agent while enabling iterative validation and more reliable
adherence to contextual privacy norms. To understand how privacy errors emerge
and propagate, we conduct a systematic ablation over information-flow
topologies, revealing when and why upstream detection mistakes cascade into
downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with
several open-source and closed-sourced LLMs demonstrate that our best
multi-agent configuration substantially reduces private information leakage
(\textbf{18\%} on ConfAIde and \textbf{19\%} on PrivacyLens with GPT-4o) while
preserving the fidelity of public content, outperforming single-agent
baselines. These results highlight the promise of principled information-flow
design in multi-agent systems for contextual privacy with LLMs.

</details>


### [66] [EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration](https://arxiv.org/abs/2508.07671)
*Mohamed Rayan Barhdadi,Mehmet Tuncel,Erchin Serpedin,Hasan Kurban*

Main category: cs.AI

TL;DR: EMPATHIA是一个多智能体框架，旨在解决难民整合中的文化、情感和伦理问题，通过三个模块（SEED、RISE、THRIVE）实现透明和可解释的决策。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法在难民整合中过于关注就业等狭窄目标，忽视了文化、情感和伦理等长期成功的关键维度。

Method: 基于Kegan的建构发展理论，EMPATHIA分为SEED（初始安置）、RISE（早期独立）和THRIVE（持续成果）三个模块，采用选择器-验证器架构和三个专门智能体（情感、文化、伦理）。

Result: 在UN Kakuma数据集（15,026人）和6,359名适龄难民中，验证收敛率达87.4%，并在五个东道国实现可解释的评估。

Conclusion: EMPATHIA通过平衡文化、情感和伦理因素，支持人机协作，为多价值协调的AI驱动分配任务提供了通用框架。

Abstract: Current AI approaches to refugee integration optimize narrow objectives such
as employment and fail to capture the cultural, emotional, and ethical
dimensions critical for long-term success. We introduce EMPATHIA (Enriched
Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),
a multi-agent framework addressing the central Creative AI question: how do we
preserve human dignity when machines participate in life-altering decisions?
Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes
integration into three modules: SEED (Socio-cultural Entry and Embedding
Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency
Engine) for early independence, and THRIVE (Transcultural Harmony and
Resilience through Integrated Values and Engagement) for sustained outcomes.
SEED employs a selector-validator architecture with three specialized agents -
emotional, cultural, and ethical - that deliberate transparently to produce
interpretable recommendations. Experiments on the UN Kakuma dataset (15,026
individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and
implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic
variables achieved 87.4% validation convergence and explainable assessments
across five host countries. EMPATHIA's weighted integration of cultural,
emotional, and ethical factors balances competing value systems while
supporting practitioner-AI collaboration. By augmenting rather than replacing
human expertise, EMPATHIA provides a generalizable framework for AI-driven
allocation tasks where multiple values must be reconciled.

</details>


### [67] [Ethics2vec: aligning automatic agents and human preferences](https://arxiv.org/abs/2508.07673)
*Gianluca Bontempi*

Main category: cs.AI

TL;DR: 论文提出了一种名为Ethics2Vec的方法，通过将伦理价值量化为向量空间，解决AI系统与人类价值观对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以明确或隐含地嵌入人类伦理价值，尤其是在处理不可比较的价值观时（如生命价值与治疗成本）。

Method: 扩展Anything2vec方法，将伦理价值映射为多元向量表示，用于评估与人类价值观的对齐程度。首先在二元决策场景中应用，随后扩展到自动控制领域。

Result: 提出Ethics2Vec方法，能够量化伦理价值并用于评估AI决策与人类价值观的对齐。

Conclusion: Ethics2Vec为解决AI伦理对齐问题提供了一种可行的量化方法，适用于从简单决策到复杂控制的多种场景。

Abstract: Though intelligent agents are supposed to improve human experience (or make
it more efficient), it is hard from a human perspective to grasp the ethical
values which are explicitly or implicitly embedded in an agent behaviour. This
is the well-known problem of alignment, which refers to the challenge of
designing AI systems that align with human values, goals and preferences. This
problem is particularly challenging since most human ethical considerations
refer to \emph{incommensurable} (i.e. non-measurable and/or incomparable)
values and criteria. Consider, for instance, a medical agent prescribing a
treatment to a cancerous patient. How could it take into account (and/or weigh)
incommensurable aspects like the value of a human life and the cost of the
treatment? Now, the alignment between human and artificial values is possible
only if we define a common space where a metric can be defined and used. This
paper proposes to extend to ethics the conventional Anything2vec approach,
which has been successful in plenty of similar and hard-to-quantify domains
(ranging from natural language processing to recommendation systems and graph
analysis). This paper proposes a way to map an automatic agent decision-making
(or control law) strategy to a multivariate vector representation, which can be
used to compare and assess the alignment with human values. The Ethics2Vec
method is first introduced in the case of an automatic agent performing binary
decision-making. Then, a vectorisation of an automatic control law (like in the
case of a self-driving car) is discussed to show how the approach can be
extended to automatic control settings.

</details>


### [68] [Symmetry-Aware Transformer Training for Automated Planning](https://arxiv.org/abs/2508.07743)
*Markus Fritzsche,Elliot Gestrin,Jendrik Seipp*

Main category: cs.AI

TL;DR: 论文提出了一种对比学习目标，使Transformer能够感知对称性，从而解决PlanGPT在规划问题中的局限性。


<details>
  <summary>Details</summary>
Motivation: Transformer在自动规划领域应用受限，尤其是PlanGPT在处理从简单到复杂规划问题时表现不佳，主要由于问题对称性导致的组合爆炸。

Method: 提出了一种新颖的对比学习目标，结合架构改进，使Transformer能够高效训练用于规划生成或启发式预测。

Result: 在多个规划领域的实验表明，对称感知训练有效且高效地解决了PlanGPT的局限性。

Conclusion: 通过对称感知训练，Transformer在规划任务中表现出色，解决了现有方法的不足。

Abstract: While transformers excel in many settings, their application in the field of
automated planning is limited. Prior work like PlanGPT, a state-of-the-art
decoder-only transformer, struggles with extrapolation from easy to hard
planning problems. This in turn stems from problem symmetries: planning tasks
can be represented with arbitrary variable names that carry no meaning beyond
being identifiers. This causes a combinatorial explosion of equivalent
representations that pure transformers cannot efficiently learn from. We
propose a novel contrastive learning objective to make transformers
symmetry-aware and thereby compensate for their lack of inductive bias.
Combining this with architectural improvements, we show that transformers can
be efficiently trained for either plan-generation or heuristic-prediction. Our
results across multiple planning domains demonstrate that our symmetry-aware
training effectively and efficiently addresses the limitations of PlanGPT.

</details>


### [69] [Best-Effort Policies for Robust Markov Decision Processes](https://arxiv.org/abs/2508.07790)
*Alessandro Abate,Thom Badings,Giuseppe De Giacomo,Francesco Fabiano*

Main category: cs.AI

TL;DR: 该论文研究了鲁棒马尔可夫决策过程（RMDPs）中多个最优鲁棒策略的问题，提出了一种新的策略选择标准——最优鲁棒最佳努力（ORBE）策略，以在非完全对抗性概率下最大化期望回报。


<details>
  <summary>Details</summary>
Motivation: 在RMDPs中，虽然可以通过鲁棒值迭代高效计算最优鲁棒策略，但这些策略在非对抗性概率下的表现可能不同。因此，需要一种更精细的策略选择标准。

Method: 论文提出ORBE策略，结合了鲁棒性和最佳努力的概念，并设计了一种算法来计算这些策略。

Result: 证明了ORBE策略的存在性，并展示了其计算可行性。

Conclusion: ORBE策略为RMDPs中的策略选择提供了更优的解决方案，实验验证了其有效性。

Abstract: We study the common generalization of Markov decision processes (MDPs) with
sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal
in RMDPs is to compute a policy that maximizes the expected return under an
adversarial choice of the transition probabilities. If the uncertainty in the
probabilities is independent between the states, known as s-rectangularity,
such optimal robust policies can be computed efficiently using robust value
iteration. However, there might still be multiple optimal robust policies,
which, while equivalent with respect to the worst-case, reflect different
expected returns under non-adversarial choices of the transition probabilities.
Hence, we propose a refined policy selection criterion for RMDPs, drawing
inspiration from the notions of dominance and best-effort in game theory.
Instead of seeking a policy that only maximizes the worst-case expected return,
we additionally require the policy to achieve a maximal expected return under
different (i.e., not fully adversarial) transition probabilities. We call such
a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE
policies always exist, characterize their structure, and present an algorithm
to compute them with a small overhead compared to standard robust value
iteration. ORBE policies offer a principled tie-breaker among optimal robust
policies. Numerical experiments show the feasibility of our approach.

</details>


### [70] [KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations](https://arxiv.org/abs/2508.07834)
*Mubaris Nadeem,Johannes Zenkert,Lisa Bender,Christian Weber,Madjid Fathi*

Main category: cs.AI

TL;DR: 本文提出了一种基于知识图谱和人工智能的创新知识管理系统，用于辅助急救人员在紧急情况下提供个性化治疗建议。


<details>
  <summary>Details</summary>
Motivation: 全球救援需求增加，急救人员需在短时间内提供优化医疗，但缺乏实时知识支持。

Method: 使用知识图谱作为核心知识表示，结合人工智能预识别情境，生成智能治疗建议。

Result: 系统能为急救人员提供实时、智能的治疗推荐，提升急救效率。

Conclusion: 知识图谱与AI结合的知识管理系统可有效支持急救决策，改善患者救治效果。

Abstract: Over the years, the need for rescue operations throughout the world has
increased rapidly. Demographic changes and the resulting risk of injury or
health disorders form the basis for emergency calls. In such scenarios, first
responders are in a rush to reach the patient in need, provide first aid, and
save lives. In these situations, they must be able to provide personalized and
optimized healthcare in the shortest possible time and estimate the patients
condition with the help of freshly recorded vital data in an emergency
situation. However, in such a timedependent situation, first responders and
medical experts cannot fully grasp their knowledge and need assistance and
recommendation for further medical treatments. To achieve this, on the spot
calculated, evaluated, and processed knowledge must be made available to
improve treatments by first responders. The Knowledge Graph presented in this
article as a central knowledge representation provides first responders with an
innovative knowledge management that enables intelligent treatment
recommendations with an artificial intelligence-based pre-recognition of the
situation.

</details>


### [71] [\(X\)-evolve: Solution space evolution powered by large language models](https://arxiv.org/abs/2508.07932)
*Yi Zhai,Zhiqiang Wei,Ruohan Li,Keyu Pan,Shuo Liu,Lu Zhang,Jianmin Ji,Wuyang Zhang,Yu Zhang,Yanyong Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为X-evolve的新方法，通过演化解空间而非单个解，显著减少了LLM调用成本，并在多个优化问题上取得了突破性成果。


<details>
  <summary>Details</summary>
Motivation: 当前结合LLM和进化算法的方法通常演化单个解，导致LLM调用成本高昂，限制了其在复杂优化问题中的应用。

Method: X-evolve通过LLM生成可调程序，定义可调解空间，并利用基于分数的搜索算法高效探索该空间。

Result: 在三个优化问题上取得显著成果：发现更大的cap set部分可容许集、提高Shannon容量下限、生成优于标准策略的启发式算法。

Conclusion: X-evolve通过演化解空间显著提升了搜索效率，为解决高维问题提供了新思路。

Abstract: While combining large language models (LLMs) with evolutionary algorithms
(EAs) shows promise for solving complex optimization problems, current
approaches typically evolve individual solutions, often incurring high LLM call
costs. We introduce \(X\)-evolve, a paradigm-shifting method that instead
evolves solution spaces \(X\) (sets of individual solutions) - subsets of the
overall search space \(S\). In \(X\)-evolve, LLMs generate tunable programs
wherein certain code snippets, designated as parameters, define a tunable
solution space. A score-based search algorithm then efficiently explores this
parametrically defined space, guided by feedback from objective function
scores. This strategy enables broader and more efficient exploration, which can
potentially accelerate convergence at a much lower search cost, requiring up to
two orders of magnitude fewer LLM calls than prior leading methods. We
demonstrate \(X\)-evolve's efficacy across three distinct hard optimization
problems. For the cap set problem, we discover a larger partial admissible set,
establishing a new tighter asymptotic lower bound for the cap set constant (\(C
\ge 2.2203\)). In information theory, we uncover a larger independent set for
the 15-vertex cycle graph (\(\mathcal{C}_{15}^{\boxtimes 5}\), size 19,946),
thereby raising the known lower bound on its Shannon capacity. Furthermore, for
the NP-hard online bin packing problem, we generate heuristics that
consistently outperform standard strategies across established benchmarks. By
evolving solution spaces, our method considerably improves search
effectiveness, making it possible to tackle high-dimensional problems that were
previously computationally prohibitive.

</details>


### [72] [Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots](https://arxiv.org/abs/2508.07941)
*Olivier Poulet,Frédéric Guinand,François Guérin*

Main category: cs.AI

TL;DR: 提出一种基于LSTM短期预测的碰撞风险预测方法，通过动态调整DQN的奖励减少碰撞。


<details>
  <summary>Details</summary>
Motivation: 在无通信或标识的受限环境中，减少机器人碰撞并提高稳定性。

Method: 使用LSTM预测机器人位置，动态调整DQN的奖励。

Result: 在1Hz采样频率下，碰撞次数显著减少，稳定性提高。

Conclusion: 该方法计算成本低，适合嵌入式系统实现。

Abstract: This article proposes a collision risk anticipation method based on
short-term prediction of the agents position. A Long Short-Term Memory (LSTM)
model, trained on past trajectories, is used to estimate the next position of
each robot. This prediction allows us to define an anticipated collision risk
by dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent.
The approach is tested in a constrained environment, where two robots move
without communication or identifiers. Despite a limited sampling frequency (1
Hz), the results show a significant decrease of the collisions number and a
stability improvement. The proposed method, which is computationally
inexpensive, appears particularly attractive for implementation on embedded
systems.

</details>


### [73] [FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis](https://arxiv.org/abs/2508.07950)
*Chen Shen,Wanqing Zhang,Kehan Li,Erwen Huang,Haitao Bi,Aiying Fan,Yiwen Shen,Hongmei Dong,Ji Zhang,Yuming Shao,Zengjia Liu,Xinshe Liu,Tao Li,Chunxia Yan,Shuanliang Fan,Di Wu,Jianhua Ma,Bin Cong,Zhenyuan Wang,Chunfeng Lian*

Main category: cs.AI

TL;DR: FEAT是一个基于多智能体AI框架的自动化法医死因分析系统，通过领域适应的大语言模型提升效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决法医死因鉴定中的劳动力短缺和诊断差异问题，特别是在高负荷系统如中国的法医体系中。

Method: FEAT采用多智能体架构，包括任务分解的中央规划器、证据分析的专业本地求解器、迭代优化的记忆与反思模块，以及结论合成的全局求解器。

Result: FEAT在多样化的中国案例中表现优于现有AI系统，具有强大的泛化能力和专家一致性。

Conclusion: FEAT是首个基于大语言模型的法医AI系统，结合AI效率与人类监督，有望提升法医服务的公平性和可靠性。

Abstract: Forensic cause-of-death determination faces systemic challenges, including
workforce shortages and diagnostic variability, particularly in high-volume
systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic
AgenT), a multi-agent AI framework that automates and standardizes death
investigations through a domain-adapted large language model. FEAT's
application-oriented architecture integrates: (i) a central Planner for task
decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a
Memory & Reflection module for iterative refinement, and (iv) a Global Solver
for conclusion synthesis. The system employs tool-augmented reasoning,
hierarchical retrieval-augmented generation, forensic-tuned LLMs, and
human-in-the-loop feedback to ensure legal and medical validity. In evaluations
across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI
systems in both long-form autopsy analyses and concise cause-of-death
conclusions. It demonstrated robust generalization across six geographic
regions and achieved high expert concordance in blinded validations. Senior
pathologists validated FEAT's outputs as comparable to those of human experts,
with improved detection of subtle evidentiary nuances. To our knowledge, FEAT
is the first LLM-based AI agent system dedicated to forensic medicine, offering
scalable, consistent death certification while maintaining expert-level rigor.
By integrating AI efficiency with human oversight, this work could advance
equitable access to reliable medicolegal services while addressing critical
capacity constraints in forensic systems.

</details>


### [74] [Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths](https://arxiv.org/abs/2508.08001)
*Rui Yao,Qi Chai,Jinhai Yao,Siyuan Li,Junhao Chen,Qi Zhang,Hao Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM的、不确定性感知的框架，用于解析美联储的Fedspeak并分类其隐含的货币政策立场，结合领域特定推理和动态不确定性解码模块，取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: Fedspeak作为美联储的策略性语言工具，对市场预期和经济条件有重要影响，自动解析Fedspeak对金融预测和政策分析具有重要意义。

Method: 采用LLM框架，结合货币政策传导机制的领域特定推理，并引入动态不确定性解码模块以评估预测置信度。

Result: 实验表明该框架在政策立场分析任务中达到最优性能，且感知不确定性与模型错误率显著正相关。

Conclusion: 该框架不仅提升了分类准确性和模型可靠性，还验证了感知不确定性作为诊断信号的有效性。

Abstract: "Fedspeak", the stylized and often nuanced language used by the U.S. Federal
Reserve, encodes implicit policy signals and strategic stances. The Federal
Open Market Committee strategically employs Fedspeak as a communication tool to
shape market expectations and influence both domestic and global economic
conditions. As such, automatically parsing and interpreting Fedspeak presents a
high-impact challenge, with significant implications for financial forecasting,
algorithmic trading, and data-driven policy analysis. In this paper, we propose
an LLM-based, uncertainty-aware framework for deciphering Fedspeak and
classifying its underlying monetary policy stance. Technically, to enrich the
semantic and contextual representation of Fedspeak texts, we incorporate
domain-specific reasoning grounded in the monetary policy transmission
mechanism. We further introduce a dynamic uncertainty decoding module to assess
the confidence of model predictions, thereby enhancing both classification
accuracy and model reliability. Experimental results demonstrate that our
framework achieves state-of-the-art performance on the policy stance analysis
task. Moreover, statistical analysis reveals a significant positive correlation
between perceptual uncertainty and model error rates, validating the
effectiveness of perceptual uncertainty as a diagnostic signal.

</details>


### [75] [Fitting Description Logic Ontologies to ABox and Query Examples](https://arxiv.org/abs/2508.08007)
*Maurice Funk,Marvin Grosser,Carsten Lutz*

Main category: cs.AI

TL;DR: 研究基于本体介导查询的拟合问题，确定是否存在满足正负示例的本体，并分析其计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决本体介导查询中如何根据正负示例拟合出合适本体的问题。

Method: 使用描述逻辑ALC和ALCI作为本体语言，涵盖多种查询语言（AQs、CQs、UCQs），提供有效特征化并分析计算复杂度。

Result: 对于AQs和完整CQs，问题为CONP；对于CQs和UCQs，问题为2EXPTIME完全。结果适用于ALC和ALCI。

Conclusion: 研究为不同查询语言下的本体拟合问题提供了理论支持，明确了其计算复杂度。

Abstract: We study a fitting problem inspired by ontology-mediated querying: given a
collection
  of positive and negative examples of
  the form $(\mathcal{A},q)$ with
  $\mathcal{A}$ an ABox and $q$ a Boolean query, we seek
  an ontology $\mathcal{O}$ that satisfies $\mathcal{A} \cup \mathcal{O} \vDash
q$ for all positive examples and $\mathcal{A} \cup \mathcal{O}\not\vDash q$ for
all negative examples.
  We consider the description logics $\mathcal{ALC}$ and $\mathcal{ALCI}$ as
ontology languages and
  a range of query languages that
  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof
(UCQs).
  For all of the resulting fitting problems,
  we provide
  effective characterizations and determine the computational complexity
  of deciding whether a fitting ontology exists. This problem turns out to be
${\small CO}NP$ for AQs and full CQs
  and $2E{\small XP}T{\small IME}$-complete for CQs and UCQs.
  These results hold for both $\mathcal{ALC}$ and $\mathcal{ALCI}$.

</details>


### [76] [AdaptFlow: Adaptive Workflow Optimization via Meta-Learning](https://arxiv.org/abs/2508.08053)
*Runchuan Zhu,Bowen Jiang,Lingrui Mei,Fangkai Yang,Lu Wang,Haoxiang Gao,Fengshuo Bai,Pu Zhao,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: AdaptFlow是一个基于自然语言的元学习框架，通过学习通用工作流初始化实现快速子任务适应，优于现有手动和自动基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态模板或手动设计工作流，限制了适应性和可扩展性。

Method: 采用双层优化方案：内循环通过LLM反馈优化子任务工作流，外循环更新共享初始化以跨任务表现良好。

Result: 在问答、代码生成和数学推理任务中表现优异，达到最先进水平。

Conclusion: AdaptFlow通过语言引导修改实现强泛化能力，适用于多样化任务和模型。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in agentic workflows, which are structured sequences of LLM invocations
intended to solve complex tasks. However, existing approaches often rely on
static templates or manually designed workflows, which limit adaptability to
diverse tasks and hinder scalability. We propose AdaptFlow, a natural
language-based meta-learning framework inspired by model-agnostic meta-learning
(MAML). AdaptFlow learns a generalizable workflow initialization that enables
rapid subtask-level adaptation. It employs a bi-level optimization scheme: the
inner loop refines the workflow for a specific subtask using LLM-generated
feedback, while the outer loop updates the shared initialization to perform
well across tasks. This setup allows AdaptFlow to generalize effectively to
unseen tasks by adapting the initialized workflow through language-guided
modifications. Evaluated across question answering, code generation, and
mathematical reasoning benchmarks, AdaptFlow consistently outperforms both
manually crafted and automatically searched baselines, achieving
state-of-the-art results with strong generalization across tasks and models.
The source code and data are available at
https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.

</details>


### [77] [FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence](https://arxiv.org/abs/2508.08075)
*Meishen He,Wenjun Ma,Jiao Wang,Huijun Yue,Xiaoma Fan*

Main category: cs.AI

TL;DR: 本文提出了一种基于Dempster-Shafer理论的开放世界信息融合方法FNBT，解决了异构框架下的证据融合问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，数据或模型常来自不同区域或组织，导致异构框架，传统融合方法效果不佳。

Method: 引入开放世界判定准则，扩展框架以容纳异构元素，并通过全否定机制转换质量函数。

Result: FNBT在理论满足三个性质，并在实际数据分类任务中表现优异，解决了Zadeh反例。

Conclusion: FNBT为异构框架下的信息融合提供了有效解决方案。

Abstract: The Dempster-Shafer theory of evidence has been widely applied in the field
of information fusion under uncertainty. Most existing research focuses on
combining evidence within the same frame of discernment. However, in real-world
scenarios, trained algorithms or data often originate from different regions or
organizations, where data silos are prevalent. As a result, using different
data sources or models to generate basic probability assignments may lead to
heterogeneous frames, for which traditional fusion methods often yield
unsatisfactory results. To address this challenge, this study proposes an
open-world information fusion method, termed Full Negation Belief
Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a
criterion is introduced to determine whether a given fusion task belongs to the
open-world setting. Then, by extending the frames, the method can accommodate
elements from heterogeneous frames. Finally, a full negation mechanism is
employed to transform the mass functions, so that existing combination rules
can be applied to the transformed mass functions for such information fusion.
Theoretically, the proposed method satisfies three desirable properties, which
are formally proven: mass function invariance, heritability, and essential
conflict elimination. Empirically, FNBT demonstrates superior performance in
pattern classification tasks on real-world datasets and successfully resolves
Zadeh's counterexample, thereby validating its practical effectiveness.

</details>


### [78] [TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork](https://arxiv.org/abs/2508.08115)
*Pranav Pushkar Mishra,Mohammad Arvan,Mohan Zalake*

Main category: cs.AI

TL;DR: TeamMedAgents将人类团队合作的心理学模型应用于多智能体医疗决策系统，通过六大核心团队合作组件提升LLMs的性能，在多个医疗基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 将人类团队合作的理论模型（如Salas的“Big Five”）转化为计算模型，以提升多智能体在医疗决策中的协作效果。

Method: 通过模块化、可配置的机制实现六大团队合作组件（如团队领导力、共享心智模型等），并在不同医疗任务和领域中评估智能体数量和配置的影响。

Result: 在8个医疗基准测试中，7个表现显著提升，且不同任务需要不同的最优团队配置。

Conclusion: TeamMedAgents为关键决策领域的多智能体系统设计提供了基于证据的方法，推动了协作AI的发展。

Abstract: We present TeamMedAgents, a novel multi-agent approach that systematically
integrates evidence-based teamwork components from human-human collaboration
into medical decision-making with large language models (LLMs). Our approach
validates an organizational psychology teamwork model from human collaboration
to computational multi-agent medical systems by operationalizing six core
teamwork components derived from Salas et al.'s "Big Five" model: team
leadership, mutual performance monitoring, team orientation, shared mental
models, closed-loop communication, and mutual trust. We implement and evaluate
these components as modular, configurable mechanisms within an adaptive
collaboration architecture while assessing the effect of the number of agents
involved based on the task's requirements and domain. Systematic evaluation of
computational implementations of teamwork behaviors across eight medical
benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,
Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8
evaluated datasets. Controlled ablation studies conducted on 50 questions per
configuration across 3 independent runs provide mechanistic insights into
individual component contributions, revealing optimal teamwork configurations
that vary by reasoning task complexity and domain-specific requirements. Our
ablation analyses reveal dataset-specific optimal teamwork configurations,
indicating that different medical reasoning modalities benefit from distinct
collaborative patterns. TeamMedAgents represents an advancement in
collaborative AI by providing a systematic translation of established teamwork
theories from human collaboration into agentic collaboration, establishing a
foundation for evidence-based multi-agent system design in critical
decision-making domains.

</details>


### [79] [BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks](https://arxiv.org/abs/2508.08127)
*Rui Miao,Yixin Liu,Yili Wang,Xu Shen,Yue Tan,Yiwei Dai,Shirui Pan,Xin Wang*

Main category: cs.AI

TL;DR: BlindGuard是一种无监督防御方法，用于检测LLM多智能体系统中的恶意代理，无需攻击标签或先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有监督防御方法依赖标记数据，不适用于实际场景，需开发更通用的防御方案。

Method: 采用分层代理编码器捕捉个体、邻域和全局交互模式，结合噪声注入和对比学习训练检测模型。

Result: 实验表明BlindGuard能有效检测多种攻击类型，且优于监督基线方法。

Conclusion: BlindGuard为多智能体系统提供了一种实用且通用的无监督防御解决方案。

Abstract: The security of LLM-based multi-agent systems (MAS) is critically threatened
by propagation vulnerability, where malicious agents can distort collective
decision-making through inter-agent message interactions. While existing
supervised defense methods demonstrate promising performance, they may be
impractical in real-world scenarios due to their heavy reliance on labeled
malicious agents to train a supervised malicious detection model. To enable
practical and generalizable MAS defenses, in this paper, we propose BlindGuard,
an unsupervised defense method that learns without requiring any
attack-specific labels or prior knowledge of malicious behaviors. To this end,
we establish a hierarchical agent encoder to capture individual, neighborhood,
and global interaction patterns of each agent, providing a comprehensive
understanding for malicious agent detection. Meanwhile, we design a
corruption-guided detector that consists of directional noise injection and
contrastive learning, allowing effective detection model training solely on
normal agent behaviors. Extensive experiments show that BlindGuard effectively
detects diverse attack types (i.e., prompt injection, memory poisoning, and
tool attack) across MAS with various communication patterns while maintaining
superior generalizability compared to supervised baselines. The code is
available at: https://github.com/MR9812/BlindGuard.

</details>


### [80] [From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework](https://arxiv.org/abs/2508.08147)
*Yunkai Hu,Tianqiao Zhao,Meng Yue*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（LLM）的代理，将电力系统优化场景的自然语言描述自动转换为可求解的数学公式，并生成解决方案。该方法通过结合领域知识、系统验证和迭代修复，显著提高了解决方案的可靠性。


<details>
  <summary>Details</summary>
Motivation: 直接使用LLM生成解决方案往往导致不可行或次优结果，因为LLM缺乏数值精度和约束处理能力。本文旨在通过结合LLM与优化求解器，实现高效且可靠的决策支持。

Method: 提出了一种结合领域感知提示和模式的LLM管道，通过系统验证和迭代修复确保可行性，生成可求解的数学公式和用户友好的结果。

Result: 以机组组合问题为例，代理生成了最优或接近最优的调度方案及目标成本，验证了方法的有效性。

Conclusion: 结合AI与传统优化框架，能够高效地将高层问题描述转化为可执行的数学模型，提升能源系统决策效率。

Abstract: This paper introduces a novel Large Language Models (LLMs)-assisted agent
that automatically converts natural-language descriptions of power system
optimization scenarios into compact, solver-ready formulations and generates
corresponding solutions. In contrast to approaches that rely solely on LLM to
produce solutions directly, the proposed method focuses on discovering a
mathematically compatible formulation that can be efficiently solved by
off-the-shelf optimization solvers. Directly using LLMs to produce solutions
often leads to infeasible or suboptimal results, as these models lack the
numerical precision and constraint-handling capabilities of established
optimization solvers. The pipeline integrates a domain-aware prompt and schema
with an LLM, enforces feasibility through systematic validation and iterative
repair, and returns both solver-ready models and user-facing results. Using the
unit commitment problem as a representative case study, the agent produces
optimal or near-optimal schedules along with the associated objective costs.
Results demonstrate that coupling the solver with task-specific validation
significantly enhances solution reliability. This work shows that combining AI
with established optimization frameworks bridges high-level problem
descriptions and executable mathematical models, enabling more efficient
decision-making in energy systems

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [81] [AMP-based Joint Activity Detection and Channel Estimation for Massive Grant-Free Access in OFDM-based Wideband Systems](https://arxiv.org/abs/2508.06540)
*Zhiyan Li,Ying Cui,Danny H. K. Tsang*

Main category: cs.IT

TL;DR: 论文提出两种基于近似消息传递（AMP）的算法，用于解决OFDM无授权接入中的设备活动检测和信道估计问题，显著提升了性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有OFDM无授权接入方法在频率选择性衰落下的设备活动检测和信道估计精度或计算时间不足，需改进。

Method: 提出精确时域信号模型，构建新因子图，设计两种AMP算法（AMP-A-EC和AMP-A-AC）解决MAP和MMSE问题。

Result: 两种算法有效缓解AMP收敛问题，AMP-A-AC计算复杂度更低，数值结果验证其优越性能。

Conclusion: 所提算法在OFDM无授权接入中具有显著应用价值，分别适用于不同场景。

Abstract: To realize orthogonal frequency division multiplexing (OFDM)-based grant-free
access for wideband systems under frequency-selective fading, existing device
activity detection and channel estimation methods need substantial accuracy
improvement or computation time reduction. In this paper, we aim to resolve
this issue. First, we present an exact time-domain signal model for OFDM-based
grant-free access under frequency-selective fading. Then, we present a maximum
a posteriori (MAP)-based device activity detection problem and two minimum mean
square error (MMSE)-based channel estimation problems. The MAP-based device
activity detection problem and one of the MMSE-based channel estimation
problems are formulated for the first time. Next, we build a new factor graph
that captures the exact statistics of time-domain channels and device
activities. Based on it, we propose two approximate message passing (AMP)-based
algorithms, AMP-A-EC and AMP-A-AC, to approximately solve the MAP-based device
activity detection problem and two MMSE-based channel estimation problems. Both
proposed algorithms alleviate the AMP's inherent convergence problem when the
pilot length is smaller or comparable to the number of active devices. Then, we
analyze AMP-A-EC's error probability of activity detection and mean square
error (MSE) of channel estimation via state evolution and show that AMP-A-AC
has the lower computational complexity (in dominant term). Finally, numerical
results show the two proposed AMP-based algorithms' superior performance and
respective preferable regions, revealing their significant values for
OFDM-based grant-free access.

</details>


### [82] [Communication-Learning Co-Design for Differentially Private Over-the-Air Federated Distillation](https://arxiv.org/abs/2508.06557)
*Zihao Hu,Jia Yan,Ying-Jun Angela Zhang*

Main category: cs.IT

TL;DR: 提出了一种基于差分隐私的无线联邦蒸馏框架，通过多接入信道的叠加特性实现高效通信与隐私保护，优化了学习收敛速率与通信开销的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习中模型规模增大带来的通信效率与隐私保护问题。

Method: 利用差分隐私和无线信道的叠加特性，设计了一种联合通信与学习的优化框架，包括收发器设计和长期训练决策。

Result: 提出的方法在保证差分隐私的同时，显著减少了通信开销，优于传统联邦学习基准。

Conclusion: 该框架为大规模联邦学习提供了高效且隐私保护的解决方案。

Abstract: The ever-growing learning model size nowadays challenges the communication
efficiency and privacy preservation of the traditional federated learning (FL).
In this paper, we propose a novel differentially private (DP) over-the-air
federated distillation (FD) framework, where wireless devices (WDs)
periodically share noise-perturbed model outputs with the parameter server by
harnessing the superposition property of multi-access channels. Accordingly,
over-the-air FD enables the shared responsibility of the DP preservation on the
low-dimensional disclosed signals among WDs. We study the
communication-learning co-design problem in differentially private over-the-air
FD, aiming to maximize the learning convergence rate while meeting the transmit
power and DP requirements of WDs. The main challenge is rooted in the
intractable learning and privacy analysis in over-the-air FD, together with the
strong coupling among the decision variables spanning two timescales. To tackle
this problem, we first derive the analytical learning convergence rate and
privacy losses of WDs, based on which the optimal transceiver design per FD
round and long-term training rounds decision are obtained in the closed forms.
Numerical results demonstrate that the proposed differentially private
over-the-air FD approach achieves a better learning-privacy trade-off with
largely-reduced communication overhead than the conventional FL benchmarks.

</details>


### [83] [When isometry and equivalence for skew constacyclic codes coincide](https://arxiv.org/abs/2508.06695)
*Monica Nevins,Susanne Pumpluen*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We show that the notions of $(n,\sigma)$-isometry and
$(n,\sigma)$-equivalence introduced by Ou-azzou et al coincide for most skew
$(\sigma,a)$-constacyclic codes of length $n$. To prove this, we show that all
Hamming-weight-preserving homomorphisms between their ambient algebras must
have degree one when those algebras are nonassociative. We work in the general
setting of commutative base rings $S$. As a consequence, we propose new
definitions of equivalence and isometry of skew constacyclic codes that exactly
capture all Hamming-preserving isomorphisms, and lead to tighter
classifications. In the process we determine homomorphisms between
nonassociative Petit algebras, prioritizing the algebras
$S[t;\sigma]/S[t;\sigma](t^n-a)$, which give rise to skew constacyclic codes.

</details>


### [84] [Generalized Samorodnitsky noisy function inequalities, with applications to error-correcting codes](https://arxiv.org/abs/2508.06940)
*Olakunle S. Abawonse,Jan Hazla,Ryan O'Donnell*

Main category: cs.IT

TL;DR: 论文扩展了Samorodnitsky不等式，将其推广到任意乘积概率分布下的函数，并确定了最优参数λ的范围。


<details>
  <summary>Details</summary>
Motivation: Samorodnitsky不等式在纠错码理论中有重要应用，但原不等式仅适用于布尔函数和整数q。本文旨在将其推广到更一般的情况。

Method: 将不等式推广到任意乘积概率分布μ⊗n下的函数f:Ωn→R，并确定最优参数λ(q,μ,ρ)的范围。

Result: 成功推广了不等式，并确定了最优λ的范围，适用于任何实数q∈[2,∞]和ρ∈[0,1]。

Conclusion: 推广后的不等式扩展了其在纠错码理论中的应用，适用于任何有限字母表的线性码。

Abstract: An inequality by Samorodnitsky states that if $f : \mathbb{F}_2^n \to
\mathbb{R}$ is a nonnegative boolean function, and $S \subseteq [n]$ is chosen
by randomly including each coordinate with probability a certain $\lambda =
\lambda(q,\rho) < 1$, then \begin{equation}
  \log \|T_\rho f\|_q \leq \mathbb{E}_{S} \log \|\mathbb{E}(f|S)\|_q\;.
\end{equation} Samorodnitsky's inequality has several applications to the
theory of error-correcting codes. Perhaps most notably, it can be used to show
that \emph{any} binary linear code (with minimum distance $\omega(\log n)$)
that has vanishing decoding error probability on the BEC$(\lambda)$ (binary
erasure channel) also has vanishing decoding error on \emph{all} memoryless
symmetric channels with capacity above some $C = C(\lambda)$.
  Samorodnitsky determined the optimal $\lambda = \lambda(q,\rho)$ for his
inequality in the case that $q \geq 2$ is an integer. In this work, we
generalize the inequality to $f : \Omega^n \to \mathbb{R}$ under any product
probability distribution $\mu^{\otimes n}$ on $\Omega^n$; moreover, we
determine the optimal value of $\lambda = \lambda(q,\mu,\rho)$ for any real $q
\in [2,\infty]$, $\rho \in [0,1]$, and distribution~$\mu$. As one consequence,
we obtain the aforementioned coding theory result for linear codes over
\emph{any} finite alphabet.

</details>


### [85] [Neural Beam Field for Spatial Beam RSRP Prediction](https://arxiv.org/abs/2508.06956)
*Keqiang Guo,Yuheng Zhong,Xin Tong,Jiangbin Lyu,Rui Zhang*

Main category: cs.IT

TL;DR: 提出了一种混合神经物理框架NBF，用于高效且可解释的空间波束RSRP预测，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 密集多用户无线网络中，准确预测波束级RSRP对波束管理至关重要，但高测量开销和快速信道变化使其具有挑战性。

Method: 结合了Transformer DNN和物理模块，引入MCPP桥接多径传播与波束配置，采用预训练校准策略。

Result: NBF在预测精度、训练效率和泛化能力上显著优于传统方法，同时保持紧凑模型。

Conclusion: NBF为下一代密集无线网络提供了可扩展且物理基础扎实的智能波束管理方案。

Abstract: Accurately predicting beam-level reference signal received power (RSRP) is
essential for beam management in dense multi-user wireless networks, yet
challenging due to high measurement overhead and fast channel variations. This
paper proposes Neural Beam Field (NBF), a hybrid neural-physical framework for
efficient and interpretable spatial beam RSRP prediction. Central to our
approach is the introduction of the Multi-path Conditional Power Profile
(MCPP), which bridges site-specific multipath propagation with antenna/beam
configurations via closed-form analytical modeling. We adopt a decoupled
``blackbox-whitebox" design: a Transformer-based deep neural network (DNN)
learns the MCPP from sparse user measurements and positions, while a
physics-inspired module analytically infers beam RSRP statistics. To improve
convergence and adaptivity, we further introduce a Pretrain-and-Calibrate (PaC)
strategy that leverages ray-tracing priors and on-site calibration using RSRP
data. Extensive simulations results demonstrate that NBF significantly
outperforms conventional table-based channel knowledge maps (CKMs) and pure
blackbox DNNs in prediction accuracy, training efficiency, and generalization,
while maintaining a compact model size. The proposed framework offers a
scalable and physically grounded solution for intelligent beam management in
next-generation dense wireless networks.

</details>


### [86] [Neural Channel Knowledge Map Assisted Scheduling Optimization of Active IRSs in Multi-User Systems](https://arxiv.org/abs/2508.07009)
*Xintong Chen,Zhenyu Jiang,Jiangbin Lyu,Liqun Fu*

Main category: cs.IT

TL;DR: 论文提出了一种基于神经通道知识图（CKM）的新型调度框架，结合Transformer深度神经网络和低复杂度调度算法，解决了智能反射面（IRS）在多用户系统中的路径损耗和调度复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 智能反射面（IRS）在下一代无线网络中具有潜力，但面临严重的双路径损耗和复杂的多用户调度问题。现有方法如主动IRS仅部分解决路径损耗，仍需高效调度。

Method: 设计了基于Transformer的深度神经网络（LPS-Net和SE-Net）预测链路功率统计和遍历频谱效率，并提出低复杂度调度算法SM-IB。

Result: 数值评估表明，神经CKM显著提高了预测精度和计算效率，SM-IB算法以较低复杂度实现了接近最优的最大-最小吞吐量。

Conclusion: 该框架为多IRS多用户系统提供了高效的调度解决方案，显著提升了性能和复杂度平衡。

Abstract: Intelligent Reflecting Surfaces (IRSs) have potential for significant
performance gains in next-generation wireless networks but face key challenges,
notably severe double-pathloss and complex multi-user scheduling due to
hardware constraints. Active IRSs partially address pathloss but still require
efficient scheduling in cell-level multi-IRS multi-user systems, whereby the
overhead/delay of channel state acquisition and the scheduling complexity both
rise dramatically as the user density and channel dimensions increase.
Motivated by these challenges, this paper proposes a novel scheduling framework
based on neural Channel Knowledge Map (CKM), designing Transformer-based deep
neural networks (DNNs) to predict ergodic spectral efficiency (SE) from
historical channel/throughput measurements tagged with user positions.
Specifically, two cascaded networks, LPS-Net and SE-Net, are designed to
predict link power statistics (LPS) and ergodic SE accurately. We further
propose a low-complexity Stable Matching-Iterative Balancing (SM-IB) scheduling
algorithm. Numerical evaluations verify that the proposed neural CKM
significantly enhances prediction accuracy and computational efficiency, while
the SM-IB algorithm effectively achieves near-optimal max-min throughput with
greatly reduced complexity.

</details>


### [87] [Generalized Quasi-Cyclic LDPC Codes: Design and Efficient Encoding](https://arxiv.org/abs/2508.07030)
*Roxana Smarandache,David G. M. Mitchell,Anthony Gómez-Fonseca*

Main category: cs.IT

TL;DR: 本文研究了准循环广义低密度奇偶校验（QC-GLDPC）码的构造方法，提出了一种基于多项式矩阵子式的高效编码矩阵生成技术，并展示了其在性能与速率权衡中的优势。


<details>
  <summary>Details</summary>
Motivation: GLDPC码在低延迟通信中表现优异，但缺乏高效编码的多项式矩阵构造方法。本文旨在解决这一问题。

Method: 通过分析多项式矩阵的子式，构造了多种形式的生成矩阵，并应用双图提升技术优化码参数。

Result: 提出的方法能够高效实现编码矩阵，并提供了码的最小距离和维度的计算方法。

Conclusion: 该方法为QC-GLDPC码的构造和性能优化提供了实用工具，同时不影响生成矩阵的多项式特性。

Abstract: Generalized low-density parity-check (GLDPC) codes, where single parity-check
constraints on the code bits are replaced with generalized constraints (an
arbitrary linear code), are a promising class of codes for low-latency
communication. The block error rate performance of the GLDPC codes, combined
with a complementary outer code, has been shown to outperform a variety of
state-of-the-art code and decoder designs with suitable lengths and rates for
the 5G ultra-reliable low-latency communication (URLLC) regime. A major
drawback of these codes is that it is not known how to construct appropriate
polynomial matrices to encode them efficiently. In this paper, we analyze
practical constructions of quasi-cyclic GLDPC (QC-GLDPC) codes and show how to
construct polynomial generator matrices in various forms using minors of the
polynomial matrix. The approach can be applied to fully generalized matrices or
partially generalized (with mixed constraint node types) to find better
performance/rate trade-offs. The resulting encoding matrices are presented in
useful forms that facilitate efficient implementation. The rich substructure
displayed also provides us with new methods of determining low weight
codewords, providing lower and upper bounds on the minimum distance and often
giving those of weight equal to the minimum distance. Based on the minors of
the polynomial parity-check matrix, we also give a formula for the rank of any
parity-check matrix representing a QC-LDPC or QC-GLDPC code, and hence, the
dimension of the code. Finally, we show that by applying double graph-liftings,
the code parameters can be improved without affecting the ability to obtain a
polynomial generator matrix.

</details>


### [88] [Realistic Evaluation of Impedance-Based RIS Modeling: Practical Insights and Applications](https://arxiv.org/abs/2508.07098)
*Ayane Lebeta Goshu,Placido Mursia,Vincenzo Sciancalepore,Marco Di Renzo,Xavier Costa-Pérez*

Main category: cs.IT

TL;DR: 论文分析了可重构智能表面（RIS）中的结构散射问题，比较了传统模型与电磁一致性模型的性能，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RIS模型忽略了复杂的电磁现象（如互耦合和结构散射），而电磁一致性模型能更准确地描述RIS行为，因此需要研究其对性能的实际影响。

Method: 通过全波仿真分析结构散射在RIS架构中的影响，并比较传统模型与电磁一致性模型。

Result: 研究发现当前模型在缓解结构散射问题上存在局限性，需要新的优化策略。

Conclusion: 论文强调了电磁一致性模型的重要性，并指出未来RIS设计需考虑更复杂的电磁现象。

Abstract: Reconfigurable Intelligent Surfaces (RISs) have emerged as a promising
technology for next-generation wireless communications, offering
energy-efficient control of electromagnetic (EM) waves. While conventional RIS
models based on phase shifts and amplitude adjustments have been widely
studied, they overlook complex EM phenomena such as mutual coupling, which are
crucial for advanced wave manipulations. Recent efforts in EM-consistent
modelling have provided more accurate representations of RIS behavior,
highlighting challenges like structural scattering-an unwanted signal
reflection that can lead to interference. In this paper, we analyze the impact
of structural scattering in RIS architectures and compare traditional and
EM-consistent models through full-wave simulations, thus providing practical
insights on the realistic performance of current RIS designs. Our findings
reveal the limitations of current modelling approaches in mitigating this
issue, underscoring the need for new optimization strategies.

</details>


### [89] [Duality on group algebras over finite chain rings: applications to additive group codes](https://arxiv.org/abs/2508.07461)
*Maryam Bajalan,Javier de la Cruz,Alexandre Fotue Tabue,Edgar Martínez-Moro*

Main category: cs.IT

TL;DR: 论文研究了有限链环扩展下的群环结构，提出了加法群码的概念，并通过模同构分解群环，构造了对称非退化迹欧几里得内积，证明了互补对的正交性质。


<details>
  <summary>Details</summary>
Motivation: 研究有限链环扩展下的群环结构及其在编码理论中的应用，特别是加法群码的性质和互补对的关系。

Method: 通过模同构分解群环，构造对称非退化迹欧几里得内积，并分析加法互补对的正交性质。

Result: 证明了加法互补对的正交性质，揭示了其与模正交直和分解、表示理论及群代数结构的联系。

Conclusion: 论文为有限链环扩展下的群环和加法群码提供了新的结构特征，并建立了与多种数学理论的联系。

Abstract: Given a finite group $G$ and an extension of finite chain rings $S|R$, one
can consider the group rings $\mathscr{S} = S[G]$ and $\mathscr{R} = R[G]$. The
group ring $\mathscr{S}$ can be viewed as an $R$-bimodule, and any of its
$R$-submodules naturally inherits an $R$-bimodule structure; in the framework
of coding theory, these are called \emph{additive group codes}, more precisely
a (left) additive group code of is a linear code which is the image of a (left)
ideal of a group algebra via an isomorphism which maps $G$ to the standard
basis of $S^n$, where $n=|G|$. In the first part of the paper, the ring
extension $S|R$ is studied, and several $R$-module isomorphisms are established
for decomposing group rings, thereby providing a characterization of the
structure of additive group codes. In the second part, we construct a
symmetric, nondegenerate trace-Euclidean inner product on $\mathscr{S}$. Two
additive group codes $\mathcal{C}$ and $\mathcal{D}$ form an \emph{additive
complementary pair} (ACP) if $\mathcal{C} + \mathcal{D} = \mathscr{S}$ and
$\mathcal{C} \cap \mathcal{D} = \{0\}$. For two-sided ACPs, we prove that the
orthogonal complement of one code under the trace-Euclidean duality is
precisely the image of the other under an involutive anti-automorphism of
$\mathscr{S}$, linking coding-theoretical ACPs with module orthogonal
direct-sum decompositions, representation theory, and the structure of group
algebras over finite chain rings.

</details>


### [90] [Structured Superposition of Autoencoders for UEP Codes at Intermediate Blocklengths](https://arxiv.org/abs/2508.07487)
*Vukan Ninkovic,Dejan Vukobratovic*

Main category: cs.IT

TL;DR: 提出了一种基于自动编码器（AE）的结构化UEP编码方法，通过分块设计扩展到大块长度，优于传统随机叠加编码方案。


<details>
  <summary>Details</summary>
Motivation: 现代通信系统需要差异化可靠性传输，但现有AE方法在UEP中的应用尚未充分探索，尤其是在中等块长度下。

Method: 采用叠加编码和连续干扰消除（SIC）解码的结构化AE架构，将编码和解码分解为更小的AE子块。

Result: 数值结果表明，该方法优于传统随机叠加编码方案的性能界限。

Conclusion: 结构化AE-UEP编码为下一代网络提供了可扩展且高效的解决方案。

Abstract: Unequal error protection (UEP) coding that enables differentiated reliability
levels within a transmitted message is essential for modern communication
systems. Autoencoder (AE)-based code designs have shown promise in the context
of learned equal error protection (EEP) coding schemes. However, their
application to UEP remains largely unexplored, particularly at intermediate
blocklengths, due to the increasing complexity of AE-based models. Inspired by
the proven effectiveness of superposition coding and successive interference
cancellation (SIC) decoding in conventional UEP schemes, we propose a
structured AE-based architecture that extends AE-based UEP codes to
substantially larger blocklengths while maintaining efficient training. By
structuring encoding and decoding into smaller AE subblocks, our method
provides a flexible framework for fine-tuning UEP reliability levels while
adapting to diverse system parameters. Numerical results show that the proposed
approach improves over established achievability bounds of randomized
superposition coding-based UEP schemes with SIC decoding, making the proposed
structured AE-based UEP codes a scalable and efficient solution for
next-generation networks.

</details>


### [91] [Extended AB Algorithms for Bistatic Integrated Sensing and Communications Systems](https://arxiv.org/abs/2508.07567)
*Tian Jiao,Yanlin Geng,Zhiqiang Wei,Zai Yang*

Main category: cs.IT

TL;DR: 提出扩展的Arimoto-Blahut算法，用于计算双基地ISAC系统中的率失真权衡，解决了现有算法处理非凸约束的局限性。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信（ISAC）对下一代无线网络至关重要，计算其率失真权衡具有重要意义。

Method: 引入辅助变量将非凸失真约束转化为线性约束，证明其与原问题具有相同最优解，并基于AB算法框架开发了扩展算法。

Result: 数值结果验证了所提算法的有效性。

Conclusion: 扩展AB算法成功解决了非凸约束问题，为ISAC系统的率失真权衡提供了有效工具。

Abstract: Integrated sensing and communication (ISAC) is pivotal for next-generation
wireless networks, rendering the computation of rate-distortion trade-off in
ISAC systems critically important. In this paper, we propose the extended
Arimoto-Blahut (AB) algorithms to calculate the rate-distortion trade-off in
bistatic ISAC systems, which overcome the limitation of existing AB algorithms
in handling non-convex constraints. Specifically, we introduce auxiliary
variables to transform non-convex distortion constraints into linear
constraints, prove that the reformulated linearly-constrained optimization
problem maintains the same optimal solution as the original problem, and
develop extended AB algorithms for both squared error and logarithmic loss
distortion metrics based on the framework of AB algorithm. Numerical results
validate the effectiveness of the proposed algorithm.

</details>


### [92] [QoS-Aware Integrated Sensing, Communication, and Control with Movable Antenna](https://arxiv.org/abs/2508.07799)
*Yike Wang,Zhike Wu,Jiang Chen,Chunjie Wang,Xuhui Zhang,Yanyan Shen*

Main category: cs.IT

TL;DR: 提出了一种基于可移动天线的ISCC系统，通过优化天线位置和波束成形策略，解决了动态干扰和信道衰减问题，显著提升了数据速率和控制QoS。


<details>
  <summary>Details</summary>
Motivation: 动态干扰和信道衰减限制了ISCC系统的潜力，需通过资源分配和智能环境感知提升适应性。

Method: 提出可移动天线赋能的ISCC系统，通过交替优化算法解决天线位置和波束成形策略的非凸优化问题。

Result: 数值结果表明，所提算法在数据速率和控制QoS方面显著优于基准方案。

Conclusion: 可移动天线和交替优化算法为ISCC系统提供了有效的解决方案，显著提升了性能。

Abstract: Integrated sensing, communication, and control (ISCC) has emerged as a key
enabler for low-altitude wireless networks with enhanced adaptability through
resource allocation co-design and intelligent environment awareness. However,
dynamic interference and channel attenuation constrain the potential of the
ISCC system. To address this challenge, we propose a novel movable
antenna-empowered ISCC system. An achievable data rate maximization problem is
formulated while guaranteeing the sensing and control quality-of-service (QoS)
by optimizing the positions of the antennas and the beamforming strategy for
communication, sensing, and control co-design. An efficient alternating
optimization (AO)-based algorithm is proposed to solve the highly coupled
non-convex problem. Numerical results demonstrate that the proposed AO-based
algorithm achieves substantial gains in the achievable data rate and the
control QoS compared with benchmark schemes.

</details>


### [93] [Age of Information Minimization in Goal-Oriented Communication with Processing and Cost of Actuation Error Constraints](https://arxiv.org/abs/2508.07865)
*Rishabh S. Pomaje,Jayanth S.,Rajshekhar V. Bhat,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 研究了目标导向通信系统，通过优化采样和传输策略，最小化信息年龄（AoI），同时考虑传输成本和语义误差成本（CAE）。


<details>
  <summary>Details</summary>
Motivation: 传统优化AoI的方法忽略了底层过程的动态变化和语义误差的影响，需提出更全面的策略。

Method: 提出了一种静态随机策略，确保AoI在最优策略的有限倍数内，并通过数值实验验证。

Result: 数值实验揭示了系统行为、优化问题的可行性及AoI、CAE与成本之间的权衡。

Conclusion: 优化AoI需结合语义误差和动态过程，静态随机策略提供了一种有效解决方案。

Abstract: We study a goal-oriented communication system in which a source monitors an
environment that evolves as a discrete-time, two-state Markov chain. At each
time slot, a controller decides whether to sample the environment and if so
whether to transmit a raw or processed sample, to the controller. Processing
improves transmission reliability over an unreliable wireless channel, but
incurs an additional cost. The objective is to minimize the long-term average
age of information (AoI), subject to constraints on the costs incurred at the
source and the cost of actuation error (CAE), a semantic metric that assigns
different penalties to different actuation errors. Although reducing AoI can
potentially help reduce CAE, optimizing AoI alone is insufficient, as it
overlooks the evolution of the underlying process. For instance, faster source
dynamics lead to higher CAE for the same average AoI, and different AoI
trajectories can result in markedly different CAE under identical average AoI.
To address this, we propose a stationary randomized policy that achieves an
average AoI within a bounded multiplicative factor of the optimal among all
feasible policies. Extensive numerical experiments are conducted to
characterize system behavior under a range of parameters. These results offer
insights into the feasibility of the optimization problem, the structure of
near-optimal actions, and the fundamental trade-offs between AoI, CAE, and the
costs involved.

</details>


### [94] [Adaptive Source-Channel Coding for Semantic Communications](https://arxiv.org/abs/2508.07958)
*Dongxu Li,Kai Yuan,Jianhao Huang,Chuan Huang,Xiaoqi Qin,Shuguang Cui,Ping Zhang*

Main category: cs.IT

TL;DR: 提出了一种自适应源信道编码（ASCC）方案，用于语义通信（SemComs），通过分离部署和自适应设计语义源编码和数字信道编码，解决了现有联合源信道编码（JSCC）和分离源信道编码（SSCC）的不足。


<details>
  <summary>Details</summary>
Motivation: 当前语义通信中的JSCC与现有通信系统不兼容且无法适应源或信道的变化，而SSCC在有限块长度下表现不佳。

Method: 提出ASCC方案，通过逻辑回归建模端到端失真与源编码率和误码率的关系，并利用逐次凸近似优化联合源信道编码率和功率分配。

Result: 仿真结果表明，ASCC方案在单信道和并行信道场景下均优于典型的深度JSCC和SSCC方案，且与现有数字系统完全兼容。

Conclusion: ASCC方案为语义通信提供了一种高效且兼容性强的解决方案。

Abstract: Semantic communications (SemComs) have emerged as a promising paradigm for
joint data and task-oriented transmissions, combining the demands for both the
bit-accurate delivery and end-to-end (E2E) distortion minimization. However,
current joint source-channel coding (JSCC) in SemComs is not compatible with
the existing communication systems and cannot adapt to the variations of the
sources or the channels, while separate source-channel coding (SSCC) is
suboptimal in the finite blocklength regime. To address these issues, we
propose an adaptive source-channel coding (ASCC) scheme for SemComs over
parallel Gaussian channels, where the deep neural network (DNN)-based semantic
source coding and conventional digital channel coding are separately deployed
and adaptively designed. To enable efficient adaptation between the source and
channel coding, we first approximate the E2E data and semantic distortions as
functions of source coding rate and bit error ratio (BER) via logistic
regression, where BER is further modeled as functions of signal-to-noise ratio
(SNR) and channel coding rate. Then, we formulate the weighted sum E2E
distortion minimization problem for joint source-channel coding rate and power
allocation over parallel channels, which is solved by the successive convex
approximation. Finally, simulation results demonstrate that the proposed ASCC
scheme outperforms typical deep JSCC and SSCC schemes for both the single- and
parallel-channel scenarios while maintaining full compatibility with practical
digital systems.

</details>


### [95] [Random Modulation: Achieving Asymptotic Replica Optimality over Arbitrary Norm-Bounded and Spectrally Convergent Channel Matrices](https://arxiv.org/abs/2508.08099)
*Lei Liu,Yuhao Chi,Shunqi Huang*

Main category: cs.IT

TL;DR: 本文提出了一种与信道矩阵解耦的随机调制技术，适用于任意范数有界和谱收敛的信道矩阵，并通过等效密集随机信道矩阵确保信号经历足够的统计信道衰落。


<details>
  <summary>Details</summary>
Motivation: 解决现有调制技术在高复杂度信道矩阵下的性能限制问题，提升检测器的BER和BLER性能。

Method: 提出随机调制技术，构建等效密集随机信道矩阵，并设计低复杂度CD-MAMP检测器，结合时域信道稀疏性和变换域随机性。

Result: 数值结果显示，随机调制在BER和BLER上比现有技术提升2-3 dB。

Conclusion: 随机调制技术在高复杂度信道矩阵下表现优异，显著提升性能。

Abstract: This paper introduces a random modulation technique that is decoupled from
the channel matrix, allowing it to be applied to arbitrary norm-bounded and
spectrally convergent channel matrices. The proposed random modulation
constructs an equivalent dense and random channel matrix, ensuring that the
signals undergo sufficient statistical channel fading. It also guarantees the
asymptotic replica maximum a posteriori (MAP) bit-error rate (BER) optimality
of approximate message passing (AMP)-type detectors for linear systems with
arbitrary norm-bounded and spectrally convergent channel matrices when their
state evolution has a unique fixed point. Then, a low-complexity cross-domain
memory approximate message passing (CD-MAMP) detector is proposed for random
modulation, leveraging the sparsity of the time-domain channel and the
randomness of the random transform-domain channel. Furthermore, the optimal
power allocation schemes are derived to minimize the replica MAP BER and
maximize the replica constrained capacity of random-modulated linear systems,
assuming the availability of channel state information (CSI) at the
transceiver. Numerical results show that the proposed random modulation can
achieve BER and block-error rate (BLER) performance gains of up to 2 - 3 dB
compared to existing OFDM/OTFS/AFDM with 5G-NR LDPC codes, under both average
and optimized power allocation.

</details>
