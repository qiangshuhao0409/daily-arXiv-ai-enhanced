<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 39]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Drift Plus Optimistic Penalty -- A Learning Framework for Stochastic Network Optimization](https://arxiv.org/abs/2509.03762)
*Sathwik Chadaga,Eytan Modiano*

Main category: cs.NI

TL;DR: 该论文研究队列网络中路由调度与未知传输成本的联合优化问题，提出了一种结合Lyapunov漂移惩罚和多臂老虎机技术的控制策略，实现了O(√T log T)的次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决队列网络中路由调度问题面临的两个挑战：传输成本未知且只能通过选择边来观测噪声成本，以及需要同时优化吞吐量和成本来确保网络稳定性。传统老虎机方法无法直接应用于具有队列动态的网络控制问题。

Method: 结合Lyapunov漂移惩罚优化和多臂老虎机技术，开发了一种网络控制策略。该方法通过静态优化问题的下界来确定最优成本，并设计策略来平衡探索和利用的权衡。

Result: 提出的策略实现了O(√T log T)的次线性遗憾，相对于完全了解到达和成本信息的最优策略。仿真实验验证了该策略的遗憾确实是次线性的。

Conclusion: 该研究成功解决了具有未知传输成本的队列网络路由调度问题，提出的控制策略在保证网络稳定性的同时实现了接近最优的性能，为这类复杂网络控制问题提供了有效的解决方案。

Abstract: We consider the problem of joint routing and scheduling in queueing networks,
where the edge transmission costs are unknown. At each time-slot, the network
controller receives noisy observations of transmission costs only for those
edges it selects for transmission. The network controller's objective is to
make routing and scheduling decisions so that the total expected cost is
minimized. This problem exhibits an exploration-exploitation trade-off,
however, previous bandit-style solutions cannot be directly applied to this
problem due to the queueing dynamics. In order to ensure network stability, the
network controller needs to optimize throughput and cost simultaneously. We
show that the best achievable cost is lower bounded by the solution to a static
optimization problem, and develop a network control policy using techniques
from Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show
that the policy achieves a sub-linear regret of order $O(\sqrt{T}\log T)$, as
compared to the best policy that has complete knowledge of arrivals and costs.
Finally, we evaluate the proposed policy using simulations and show that its
regret is indeed sub-linear.

</details>


### [2] [A Versatile and Programmable UAV Platform for Radio Access Network and End-to-End Cellular Measurements](https://arxiv.org/abs/2509.03818)
*Sherwan Jalal Abdullah,Sravan Reddy Chintareddy,Victor S. Frost,Shawn Keshmiri,Morteza Hashemi*

Main category: cs.NI

TL;DR: 基于无人机的空中网络性能测量平台，解决农村地区传统测试方法的挑战，发现高空低信号质量但上行下行速率较好


<details>
  <summary>Details</summary>
Motivation: 传统的人群批量测量方法在农村地区效果差，人口密度低、地形复杂、测试风险高

Method: 使用UAV搭载商用细胞调制解调器和计算单元，通过空中作业收集RAN信号和端到端网络性能指标

Result: 高空位置信号功率提升但信号质量下降，上行下行速率和往返延迟表现满意，强无线信号不一定对应良好覆盖

Conclusion: UAV平台能有效测量农村地区网络性能，空中测试显示了信号传播的复杂特性，为网络优化提供了新视角

Abstract: In this work, we develop a measurement platform to capture mobile network
performance metrics including coverage and quality of service in regions where
conventional coverage testing approaches are frequently time-intensive,
labor-demanding, and occasionally hazardous. Traditionally, crowd-sourcing
methods are used to collect cellular network performance metrics. However,
these approaches are inadequate in rural areas due to low-density population,
and difficult terrain. The platform described here is a UAV-based and is
designed to investigate the mobile network performance through aerial
operations and gather Radio Access Network (RAN) signal alongside end-to-end
network performance metrics. Our platform gathers metrics through the
integration of an onboard computation unit and commercial off-the-shelf
cellular modem. The gathered data are subsequently analyzed and displayed using
geospatial mapping utilities and statistical techniques to deliver key
observations on cellular network performance. Experimental results showed that
the received signal power improves at higher altitudes due to enhanced
line-of-sight (LoS) conditions as expected. However, the signal quality
degrades as a result of increased interference from neighboring cells. The
analysis reveals that for most of the geographic area covered in the initial
experiments the system maintained acceptable signal quality, with adequate
throughput performance for both uplink and downlink communications, while
maintaining satisfactory round-trip time characteristics. Notably, the
experiment showed that a strong radio signal metric for a given cell does not
necessarily translate to consistent spatial coverage across the tested region.

</details>


### [3] [Entanglement Purification With Finite Latency Classical Communication in Quantum Networks](https://arxiv.org/abs/2509.03667)
*Vivek Vasan,Alexander Nico-Katz,Boulat A. Bash,Daniel C. Kilper,Marco Ruffini*

Main category: cs.NI

TL;DR: 量子网络中维持纯粹缘故对的挑战：量子记忆透漏和经典通信延迟对纯化协议的影响分析


<details>
  <summary>Details</summary>
Motivation: 解决量子网络中纯粹缘故对在存储过程中因环境透漏而下降的问题，而纯化协议又需要经典通信延迟，这反而加剧了透漏效应

Method: 采用微观林德保方法分析量子动力学，结合现代城市IP网络延迟数据和量子记忆实验参数，对BBPSSW和DEJMPS纯化协议进行综合性能评估

Result: 确定了纯化协议成功与失败的相变区域，由相空间中的破均等保真度边界来划分，计算了完成多轮纯化所需的总缘故对数量和超过应用间阈值的纯化保真度稳态速率

Conclusion: 为当前和近期量子网络部署纯化协议提供了延迟预算、记忆质量目标和资源开销估算，明确了实际应用的可行性条件

Abstract: Quantum networks rely on high fidelity entangled pairs distributed to nodes,
but maintaining their fidelity is challenged by environmental decoherence
during storage. Entanglement purification is used to restore fidelity, but the
idle periods imposed by the associated classical communication delays
counteract this goal by exposing the states to further decoherence. In this
work, we analyze the practical viability of entanglement purification protocols
(BBPSSW, DEJMPS), under non-instantaneous classical coordination over Internet
protocol (IP) communications networks. We present a comprehensive performance
evaluation of these protocols in various network conditions for a range of
quantum memory technologies. We employ a microscopic Lindblad treatment of the
underlying quantum dynamics, and use current-generation metropolitan IP network
latency statistics and parameters drawn from quantum memory testbeds. In doing
so we identify the regions in which entanglement purification succeeds and
fails, delineated by break-even iso-fidelity contours in the phase space. We
then determine the total number of entangled pairs required to complete a
multi-round purification protocol, and the steady-state throughput of entangled
pairs with purified fidelities that exceed application-specific thresholds.
This provides latency budgets, memory quality targets, and resource-overhead
estimates for deploying purification on current and near-future networks.

</details>


### [4] [Indoor Positioning with Wi-Fi Location: A Survey of IEEE 802.11mc/az/bk Fine Timing Measurement Research](https://arxiv.org/abs/2509.03901)
*Katarzyna Kosek-Szott,Szymon Szott,Wojciech Ciezobka,Maksymilian Wojnar,Krzysztof Rusek,Jonathan Segev*

Main category: cs.NI

TL;DR: 本文是关于IEEE 802.11mc FTM协议在室内定位中的综述研究，分析了180多篇相关论文，涵盖了FTM的精度、改进方法、与其他系统结合、应用场景和安全问题。


<details>
  <summary>Details</summary>
Motivation: 尽管有大量关于室内定位的文献综述，但专门针对FTM协议及其最新增强功能的调查仍然缺乏。Wi-Fi FTM协议因其高可用性、高精度和设备支持，在未来设备中具有巨大潜力。

Method: 通过分类和回顾超过180篇研究论文，分析FTM在实际应用中的精度表现、机器学习改进方法、与其他室内定位系统的结合、基于FTM的应用以及安全问题。

Result: 总结了FTM在室内定位领域最重要的研究成果，包括精度改进技术、多系统融合方法和实际应用案例。

Conclusion: 基于调查结果，总结了主要研究成就，并提出了需要进一步研究的开放领域，为FTM在室内定位中的未来发展提供了指导方向。

Abstract: Indoor positioning is an enabling technology for home, office, and industrial
network users because it provides numerous information and communication
technology (ICT) and Internet of things (IoT) functionalities such as indoor
navigation, smart meter localization, asset tracking, support for emergency
services, and detection of hazardous situations. The IEEE 802.11mc fine timing
measurement (FTM) protocol (commercially known as Wi-Fi Location) has great
potential to enable indoor positioning in future generation devices, primarily
because of the high availability of Wi-Fi networks, FTM's high accuracy and
device support. Furthermore, new FTM enhancements are available in the released
(802.11az) and recently completed (802.11bk) amendments. Despite the multitude
of literature reviews on indoor positioning, a survey dedicated to FTM and its
recent enhancements has so far been lacking. We fill this gap by classifying
and reviewing over 180 research papers related to the practical accuracy
achieved with FTM, methods for improving its accuracy (also with machine
learning), combining FTM with other indoor positioning systems, FTM-based
applications, and security issues. Based on the conducted survey, we summarize
the most important research achievements and formulate open areas for further
research.

</details>


### [5] [Autonomous Task Offloading of Vehicular Edge Computing with Parallel Computation Queues](https://arxiv.org/abs/2509.03935)
*Sungho Cho,Sung Il Choi,Seung Hyun Oh,Ian P. Roberts,Sang Hyun Lee*

Main category: cs.NI

TL;DR: 提出了一种基于网络协作的车载边缘计算任务卸载解决方案，通过平衡资源利用不足和负载拥塞来最小化车辆用户的整体等待延迟


<details>
  <summary>Details</summary>
Motivation: 解决车载边缘计算网络中任务卸载时的资源利用不足和负载拥塞问题，以最小化车辆用户的等待延迟

Method: 基于网络协作的任务卸载策略，预测边缘服务器的瞬时处理能力，识别过载服务器，并考虑队列的离散变量进行精确估计

Result: 理论和数值双重评估表明，该解决方案相比现有方法实现了全局最优的延迟减少性能，在真实地图虚拟环境中的可行性测试也得到了验证

Conclusion: 通过预测边缘服务器瞬时处理能力和考虑队列离散变量，能够有效解决组合挑战，实现最优性能，为车载边缘计算网络提供了有效的任务执行策略

Abstract: This work considers a parallel task execution strategy in vehicular edge
computing (VEC) networks, where edge servers are deployed along the roadside to
process offloaded computational tasks of vehicular users. To minimize the
overall waiting delay among vehicular users, a novel task offloading solution
is implemented based on the network cooperation balancing resource
under-utilization and load congestion. Dual evaluation through theoretical and
numerical ways shows that the developed solution achieves a globally optimal
delay reduction performance compared to existing methods, which is also
approved by the feasibility test over a real-map virtual environment. The
in-depth analysis reveals that predicting the instantaneous processing power of
edge servers facilitates the identification of overloaded servers, which is
critical for determining network delay. By considering discrete variables of
the queue, the proposed technique's precise estimation can effectively address
these combinatorial challenges to achieve optimal performance.

</details>


### [6] [Analyzing the Effect of an Extreme Weather Event on Telecommunications and Information Technology: Insights from 30 Days of Flooding](https://arxiv.org/abs/2509.04219)
*Leandro Márcio Bertholdo,Renan Barreto Paredes,Gabriela de Lima Marin,Cesar A. H. Loureiro,Milton Kaoru Kashiwakura Pedro de Botelho Marcos*

Main category: cs.NI

TL;DR: 对巴西里约格拉蒂德南州2024年5月气候灾害期间通信网络弹性的综合研究，包括互联网测量、光纤切断报告和路由数据的数据集构建与分析


<details>
  <summary>Details</summary>
Motivation: 气候极端事件对通信基础设施造成严重破坏，需要研究网络弹性和灾害恢复策略

Method: 构建综合通信数据集，包括互联网测量、光纤切断报告和交换中心路由数据，并与水文和运营因素相关联

Result: 初步发现连接恢复趋势、基础设施脏漏和用户行为变化，揭示了信息通信技术基础设施在极端气候事件中面临的挑战

Conclusion: 该数据集和预分析可支持未来灾害恢复策略研究和健壮通信系统的发展

Abstract: In May 2024, weeks of severe rainfall in Rio Grande do Sul, Brazil caused
widespread damage to infrastructure, impacting over 400 cities and 2.3 million
people. This study presents the construction of comprehensive
telecommunications datasets during this climatic event, encompassing Internet
measurements, fiber cut reports, and Internet Exchange routing data. By
correlating network disruptions with hydrological and operational factors, the
dataset offers insights into the resilience of fiber networks, data centers,
and Internet traffic during critical events. For each scenario, we investigate
failures related to the Information and Communication Technology infrastructure
and highlight the challenges faced when its resilience is critically tested.
Preliminary findings reveal trends in connectivity restoration, infrastructure
vulnerabilities, and user behavior changes. These datasets and pre-analysis aim
to support future research on disaster recovery strategies and the development
of robust telecommunications systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Handling Infinite Domain Parameters in Planning Through Best-First Search with Delayed Partial Expansions](https://arxiv.org/abs/2509.03953)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 这篇论文提出了一种高效的搜索算法，将控制参数作为真正的决策点在系统搜索中显式处理，而非作为约束来处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法将控制参数作为嵌入式约束处理，而非将其视为搜索空间中的真正决策点，这限制了解决效率。

Method: 发展了一种最优先的启发式搜索算法，在无限的控制参数决策空间中运行，利用延迟部分扩展概念，增量式地扩展状态后继。

Result: 证明了该算法在某些条件下具有极限完备性，并显示出在解决涵盖控制参数的规划问题方面是现有方法的竞争性替代方案。

Conclusion: 显式处理控制参数作为决策点的方法是一种高效且竞争力强的规划技术，为处理连续数值决策变量提供了新的视角。

Abstract: In automated planning, control parameters extend standard action
representations through the introduction of continuous numeric decision
variables. Existing state-of-the-art approaches have primarily handled control
parameters as embedded constraints alongside other temporal and numeric
restrictions, and thus have implicitly treated them as additional constraints
rather than as decision points in the search space. In this paper, we propose
an efficient alternative that explicitly handles control parameters as true
decision points within a systematic search scheme. We develop a best-first,
heuristic search algorithm that operates over infinite decision spaces defined
by control parameters and prove a notion of completeness in the limit under
certain conditions. Our algorithm leverages the concept of delayed partial
expansion, where a state is not fully expanded but instead incrementally
expands a subset of its successors. Our results demonstrate that this novel
search algorithm is a competitive alternative to existing approaches for
solving planning problems involving control parameters.

</details>


### [8] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 基于页面图结构的GUI自动化代理框架，通过将序列操作转换为图结构来提升环境感知能力


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理使用序列操作作为知识，无法抓取页面间复杂过渡关系，导致环境感知不深入且新场景沿续性差

Method: 设计自动化流水线将序列操作转换为页面图，显式建模页面图结构；结合RAG技术从图中检索GUI感知指南；提出PG-Agent多代理框架与任务分解策略

Result: 在多个标准测试集上进行广泛实验，证明了PG-Agent的有效性，即使用有限的操作序列构建页面图

Conclusion: 页面图结构能够显著提升GUI代理的环境感知和沿续性，为GUI自动化领域提供了新的解决方案

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [9] [Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models](https://arxiv.org/abs/2509.03548)
*João P. Arroyo,João G. Rodrigues,Daniel Lawand,Denis D. Mauá,Junkyu Lee,Radu Marinescu,Alex Gray,Eduardo R. Laurentino,Fabio G. Cozman*

Main category: cs.AI

TL;DR: 本文研究准马尔可夫因果模型中部分可识别查询的概率边界计算问题，提出了基于列生成技术的新算法，通过辅助线性整数程序序列计算概率边界，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在准马尔可夫因果模型中，当外生变量未完全指定时，无法精确计算感兴趣的概率值，需要研究如何计算紧概率边界。

Method: 提出新算法利用内生变量的输入概率简化多线性规划构造。对于单干预场景，应用列生成技术通过一系列辅助线性整数程序计算概率边界，证明外生变量的多项式基数表示是可能的。

Result: 实验结果表明列生成技术优于现有方法。

Conclusion: 本文为部分可识别因果查询的概率边界计算提供了有效的列生成方法，在处理准马尔可夫模型时表现出优越性能。

Abstract: We investigate partially identifiable queries in a class of causal models. We
focus on acyclic Structural Causal Models that are quasi-Markovian (that is,
each endogenous variable is connected with at most one exogenous confounder).
We look into scenarios where endogenous variables are observed (and a
distribution over them is known), while exogenous variables are not fully
specified. This leads to a representation that is in essence a Bayesian network
where the distribution of root variables is not uniquely determined. In such
circumstances, it may not be possible to precisely compute a probability value
of interest. We thus study the computation of tight probability bounds, a
problem that has been solved by multilinear programming in general, and by
linear programming when a single confounded component is intervened upon. We
present a new algorithm to simplify the construction of such programs by
exploiting input probabilities over endogenous variables. For scenarios with a
single intervention, we apply column generation to compute a probability bound
through a sequence of auxiliary linear integer programs, thus showing that a
representation with polynomial cardinality for exogenous variables is possible.
Experiments show column generation techniques to be superior to existing
methods.

</details>


### [10] [Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method](https://arxiv.org/abs/2509.03550)
*Tonghe Li,Jixin Liu,Weili Zeng,Hao Jiang*

Main category: cs.AI

TL;DR: 本文提出Diffusion-AC框架，将扩散概率模型应用于空中交通冲突检测与解决，通过多模态决策能力显著提升安全性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在冲突检测与解决中存在"单模态偏差"，导致决策灵活性不足和决策死锁问题，需要新的方法来应对复杂动态约束。

Method: 提出Diffusion-AC框架，使用扩散概率模型建模策略作为反向去噪过程，配合密度渐进安全课程(DPSC)训练机制，从稀疏到高密度交通环境逐步学习。

Result: 在高密度场景下达到94.1%的成功率，相比次优基线减少近59%的接近空中碰撞事件，显著提升系统安全边际。

Conclusion: 扩散概率模型为安全关键任务中的冲突解决提供了有效的多模态决策能力，能够灵活切换有效机动策略，显著优于现有最先进的深度强化学习方法。

Abstract: In the context of continuously rising global air traffic, efficient and safe
Conflict Detection and Resolution (CD&R) is paramount for air traffic
management. Although Deep Reinforcement Learning (DRL) offers a promising
pathway for CD&R automation, existing approaches commonly suffer from a
"unimodal bias" in their policies. This leads to a critical lack of
decision-making flexibility when confronted with complex and dynamic
constraints, often resulting in "decision deadlocks." To overcome this
limitation, this paper pioneers the integration of diffusion probabilistic
models into the safety-critical task of CD&R, proposing a novel autonomous
conflict resolution framework named Diffusion-AC. Diverging from conventional
methods that converge to a single optimal solution, our framework models its
policy as a reverse denoising process guided by a value function, enabling it
to generate a rich, high-quality, and multimodal action distribution. This core
architecture is complemented by a Density-Progressive Safety Curriculum (DPSC),
a training mechanism that ensures stable and efficient learning as the agent
progresses from sparse to high-density traffic environments. Extensive
simulation experiments demonstrate that the proposed method significantly
outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the
most challenging high-density scenarios, Diffusion-AC not only maintains a high
success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions
(NMACs) by approximately 59% compared to the next-best-performing baseline,
significantly enhancing the system's safety margin. This performance leap stems
from its unique multimodal decision-making capability, which allows the agent
to flexibly switch to effective alternative maneuvers.

</details>


### [11] [Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents](https://arxiv.org/abs/2509.03581)
*Davide Paglieri,Bartłomiej Cupiał,Jonathan Cook,Ulyana Piterbarg,Jens Tuyls,Edward Grefenstette,Jakob Nicolaus Foerster,Jack Parker-Holder,Tim Rocktäschel*

Main category: cs.AI

TL;DR: 该论文提出了动态规划框架，让LLM智能体能够灵活决定何时进行规划，通过两阶段训练（监督微调+强化学习）在长时程任务中实现更高效的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法如ReAct要求LLM在每次行动前都进行规划，这在计算上昂贵且会降低长时程任务的性能，而完全不规划又会限制性能表现。

Method: 提出两阶段训练流程：1）在多样化合成数据上进行监督微调，为动态规划做准备；2）在长时程环境中使用强化学习来精炼这种能力。

Result: 在Crafter环境中的实验表明，动态规划智能体更样本高效，能持续实现更复杂的目标，并且能够有效利用人类编写的规划来超越其独立能力。

Conclusion: 这是首个探索训练LLM智能体进行动态测试时计算分配的研究，为更高效、自适应和可控的智能体系统铺平了道路。

Abstract: Training large language models (LLMs) to reason via reinforcement learning
(RL) significantly improves their problem-solving capabilities. In agentic
settings, existing methods like ReAct prompt LLMs to explicitly plan before
every action; however, we demonstrate that always planning is computationally
expensive and degrades performance on long-horizon tasks, while never planning
further limits performance. To address this, we introduce a conceptual
framework formalizing dynamic planning for LLM agents, enabling them to
flexibly decide when to allocate test-time compute for planning. We propose a
simple two-stage training pipeline: (1) supervised fine-tuning on diverse
synthetic data to prime models for dynamic planning, and (2) RL to refine this
capability in long-horizon environments. Experiments on the Crafter environment
show that dynamic planning agents trained with this approach are more
sample-efficient and consistently achieve more complex objectives.
Additionally, we demonstrate that these agents can be effectively steered by
human-written plans, surpassing their independent capabilities. To our
knowledge, this work is the first to explore training LLM agents for dynamic
test-time compute allocation in sequential decision-making tasks, paving the
way for more efficient, adaptive, and controllable agentic systems.

</details>


### [12] [Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE](https://arxiv.org/abs/2509.03626)
*Zahra Zehtabi Sabeti Moghaddam,Zeinab Dehghani,Maneeha Rani,Koorosh Aslansefat,Bhupesh Kumar Mishra,Rameez Raja Kureshi,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 基于矩阵分解的矩阵分解方法KG-SMILE，通过控制扰动和线性模型来解释矩阵RAG的决策过程，提高可解释性和可信过


<details>
  <summary>Details</summary>
Motivation: 解决LLM在敏感领域中产生幻觉和不可验证断言的问题，并改善RAG系统的黑盒特性和对数据质量的依赖

Method: 开发KG-SMILE框架，通过控制扰动、计算相似性和训练加权线性模型来识别图谱实体和关系对生成输出的影响

Result: KG-SMILE在保持模型效果的同时提供了稳定且与人类对齐的解释，在忠实性、一致性、稳定性和准确性方面都表现优异

Conclusion: KG-SMILE框架能够有效地平衡模型效果和可解释性，为机器学习技术提供更大的透明度和信任度

Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.

</details>


### [13] [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
*Jacqueline Maasch,John Kalantari,Kia Khezeli*

Main category: cs.AI

TL;DR: CausalARC是一个用于AI推理的测试平台，专注于低数据和分布外场景，基于因果世界模型提供观测、干预和反事实反馈。


<details>
  <summary>Details</summary>
Motivation: 推理需要在有限数据和分布偏移下适应新问题设置，现有测试平台无法充分评估这种能力。

Method: 构建基于结构因果模型的因果世界模型，通过原则性数据增强提供观测、干预和反事实的少样本学习演示。

Result: 开发了CausalARC测试平台，支持四种语言模型评估设置：测试时训练的抽象推理、上下文学习的反事实推理、程序合成和因果发现。

Conclusion: CausalARC为评估AI在低数据和分布外场景下的推理能力提供了系统化的测试框架，具有重要的研究和应用价值。

Abstract: Reasoning requires adaptation to novel problem settings under limited data
and distribution shift. This work introduces CausalARC: an experimental testbed
for AI reasoning in low-data and out-of-distribution regimes, modeled after the
Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is
sampled from a fully specified causal world model, formally expressed as a
structural causal model. Principled data augmentations provide observational,
interventional, and counterfactual feedback about the world model in the form
of few-shot, in-context learning demonstrations. As a proof-of-concept, we
illustrate the use of CausalARC for four language model evaluation settings:
(1) abstract reasoning with test-time training, (2) counterfactual reasoning
with in-context learning, (3) program synthesis, and (4) causal discovery with
logical reasoning.

</details>


### [14] [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
*François Olivier,Zied Bouraoui*

Main category: cs.AI

TL;DR: 提出了Embodied-LM神经符号系统，通过基于图像模式的空间推理来增强大语言模型的逻辑推理能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型在逻辑推理方面仍然容易出错，缺乏人类类似的稳健心理表征，需要将理解建立在具身认知结构上

Method: 使用基于图像模式（源自感觉运动经验的重复模式）的图式表征，通过答案集编程中的声明式空间推理来操作化这些认知结构的空间基础

Result: 在逻辑演绎问题上验证表明，LLMs可以通过具身认知结构解释场景，这些结构可形式化为可执行程序，支持有效的逻辑推理并提高可解释性

Conclusion: 虽然当前实现专注于空间原语，但为纳入更复杂和动态表征建立了计算基础，展示了神经符号方法在增强LLM推理能力方面的潜力

Abstract: Despite significant progress in natural language understanding, Large
Language Models (LLMs) remain error-prone when performing logical reasoning,
often lacking the robust mental representations that enable human-like
comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that
grounds understanding and logical reasoning in schematic representations based
on image schemas-recurring patterns derived from sensorimotor experience that
structure human cognition. Our system operationalizes the spatial foundations
of these cognitive structures using declarative spatial reasoning within Answer
Set Programming. Through evaluation on logical deduction problems, we
demonstrate that LLMs can be guided to interpret scenarios through embodied
cognitive structures, that these structures can be formalized as executable
programs, and that the resulting representations support effective logical
reasoning with enhanced interpretability. While our current implementation
focuses on spatial primitives, it establishes the computational foundation for
incorporating more complex and dynamic representations.

</details>


### [15] [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
*Haozhe Wang,Qixin Xu,Che Liu,Junhong Wu,Fangzhen Lin,Wenhu Chen*

Main category: cs.AI

TL;DR: RL提升LLM推理能力的关键在于层次化学习机制：先掌握低层程序技能，后转向高层战略规划。现有RL算法效率低，新方法HICRA专注于规划token，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管RL能有效提升大语言模型的复杂推理能力，但其成功背后的机制仍不明确。研究者希望揭示RL训练过程中的关键动态和效率瓶颈。

Method: 提出HIerarchy-Aware Credit Assignment (HICRA)算法，通过集中优化高影响力的规划token来解决现有RL算法优化压力分散的问题。使用语义熵作为战略探索的衡量指标。

Result: HICRA显著优于强基线方法，证明专注于战略瓶颈是解锁高级推理的关键。语义熵被验证为比token级熵更可靠的战略探索度量指标。

Conclusion: RL训练中存在明显的两阶段层次化学习动态，专注于高层战略规划的优化能更有效地提升模型推理能力，语义熵是衡量这一过程的有效指标。

Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the
complex reasoning abilities of Large Language Models (LLMs), yet underlying
mechanisms driving this success remain largely opaque. Our analysis reveals
that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy
dynamics are not disparate occurrences but hallmarks of an emergent reasoning
hierarchy, akin to the separation of high-level strategic planning from
low-level procedural execution in human cognition. We uncover a compelling
two-phase dynamic: initially, a model is constrained by procedural correctness
and must improve its low-level skills. The learning bottleneck then decisively
shifts, with performance gains being driven by the exploration and mastery of
high-level strategic planning. This insight exposes a core inefficiency in
prevailing RL algorithms like GRPO, which apply optimization pressure
agnostically and dilute the learning signal across all tokens. To address this,
we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that
concentrates optimization efforts on high-impact planning tokens. HICRA
significantly outperforms strong baselines, demonstrating that focusing on this
strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we
validate semantic entropy as a superior compass for measuring strategic
exploration over misleading metrics such as token-level entropy.

</details>


### [16] [An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification](https://arxiv.org/abs/2509.03649)
*Davide Italo Serramazza,Nikos Papadeas,Zahraa Abdallah,Georgiana Ifrim*

Main category: cs.AI

TL;DR: 本文研究了时间序列分类中SHAP解释方法的优化，发现分段数量比具体分段算法对解释质量影响更大，等长分段优于大多数定制算法，并提出了一种新的基于段长度的归因归一化技术。


<details>
  <summary>Details</summary>
Motivation: SHAP方法在时间序列分类中计算复杂度高，现有研究通过特征聚合来降低计算成本，但最优分段策略选择仍是一个开放问题。

Method: 研究了8种不同的时间序列分割算法，使用InterpretTime和AUC Difference两种评估方法，在多变量和单变量时间序列上进行实验。

Result: 发现分段数量对解释质量的影响大于具体分段方法，等长分段表现优于大多数定制算法，提出的长度加权归一化技术能持续提升归因质量。

Conclusion: 对于时间序列SHAP解释，简单的等长分段配合长度加权归一化是最有效的策略，分段数量是关键因素而非复杂的定制算法。

Abstract: Explainable AI (XAI) has become an increasingly important topic for
understanding and attributing the predictions made by complex Time Series
Classification (TSC) models. Among attribution methods, SHapley Additive
exPlanations (SHAP) is widely regarded as an excellent attribution method; but
its computational complexity, which scales exponentially with the number of
features, limits its practicality for long time series. To address this, recent
studies have shown that aggregating features via segmentation, to compute a
single attribution value for a group of consecutive time points, drastically
reduces SHAP running time. However, the choice of the optimal segmentation
strategy remains an open question. In this work, we investigated eight
different Time Series Segmentation algorithms to understand how segment
compositions affect the explanation quality. We evaluate these approaches using
two established XAI evaluation methodologies: InterpretTime and AUC Difference.
Through experiments on both Multivariate (MTS) and Univariate Time Series
(UTS), we find that the number of segments has a greater impact on explanation
quality than the specific segmentation method. Notably, equal-length
segmentation consistently outperforms most of the custom time series
segmentation algorithms. Furthermore, we introduce a novel attribution
normalisation technique that weights segments by their length and we show that
it consistently improves attribution quality.

</details>


### [17] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: PersonaTeaming：一种将人物身份融入自动化红队测试的新方法，通过角色突变生成对抗性提示，显著提高了攻击成功率


<details>
  <summary>Details</summary>
Motivation: 当前自动化红队测试方法忽视了测试者身份背景对风险发现的影响，需要开发能够结合人物身份特征的自动化测试方法

Method: 开发PersonaTeaming方法，包括基于"红队专家"和"普通AI用户"角色的提示突变方法，以及动态角色生成算法和新的突变距离度量指标

Result: 实验显示PersonaTeaming相比最先进的RainbowPlus方法，攻击成功率提升高达144.1%，同时保持了提示多样性

Conclusion: 该方法展示了将人物身份融入自动化红队测试的潜力，为探索自动化与人工红队测试的互补性提供了新方向

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [18] [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
*Pengrui Han,Rafal Kocielnik,Peiyang Song,Ramit Debnath,Dean Mobbs,Anima Anandkumar,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 本研究系统分析了LLM人格特质，发现指令对齐能稳定特质表达但自述特质无法可靠预测行为，人格注入能改变自述但行为影响有限


<details>
  <summary>Details</summary>
Motivation: 理解LLM是否表现出类似人类的人格特质模式，此前研究主要依赖简化的自我报告而缺乏行为验证

Method: 从三个维度系统分析LLM人格：(1)训练阶段特质动态演化；(2)自述特质在行为任务中的预测效度；(3)人格注入等干预措施的影响

Result: 指令对齐显著稳定特质表达并增强特质相关性，但自述特质无法可靠预测行为，人格注入能改变自述但对行为影响有限或不一致

Conclusion: LLM的表面特质表达与行为一致性存在差异，挑战了现有假设，强调了对齐和可解释性需要更深层次评估

Abstract: Personality traits have long been studied as predictors of human
behavior.Recent advances in Large Language Models (LLMs) suggest similar
patterns may emerge in artificial systems, with advanced LLMs displaying
consistent behavioral tendencies resembling human traits like agreeableness and
self-regulation. Understanding these patterns is crucial, yet prior work
primarily relied on simplified self-reports and heuristic prompting, with
little behavioral validation. In this study, we systematically characterize LLM
personality across three dimensions: (1) the dynamic emergence and evolution of
trait profiles throughout training stages; (2) the predictive validity of
self-reported traits in behavioral tasks; and (3) the impact of targeted
interventions, such as persona injection, on both self-reports and behavior.
Our findings reveal that instructional alignment (e.g., RLHF, instruction
tuning) significantly stabilizes trait expression and strengthens trait
correlations in ways that mirror human data. However, these self-reported
traits do not reliably predict behavior, and observed associations often
diverge from human patterns. While persona injection successfully steers
self-reports in the intended direction, it exerts little or inconsistent effect
on actual behavior. By distinguishing surface-level trait expression from
behavioral consistency, our findings challenge assumptions about LLM
personality and underscore the need for deeper evaluation in alignment and
interpretability.

</details>


### [19] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: 研究发现大型语言模型虽然能生成类似人类的调查数据，但在内部一致性方面存在显著缺陷，无法准确替代真实人类参与者进行社会科学研究。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成的人工智能代理是否能够替代真实人类参与者进行社会科学研究，特别关注代理在不同实验设置下是否保持行为一致性。

Method: 设计研究来揭示代理的内部状态，并在基本对话设置中检查代理行为，通过一系列行为假设来评估对话行为与内部状态的一致性。

Result: 发现不同模型家族和不同规模的LLM都存在显著的内部不一致性，尽管能生成类似人类的回答，但缺乏内部一致性。

Conclusion: LLM代理无法准确替代真实人类参与者进行人类主体研究，因为它们在内部一致性方面存在关键缺陷。

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [20] [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768)
*Connor Walker,Koorosh Aslansefat,Mohammad Naveed Akram,Yiannis Papadopoulos*

Main category: cs.AI

TL;DR: RAGuard是一个增强的检索增强生成框架，专门针对海上风电维护场景，通过并行查询技术文档和安全文档，并分配独立的检索预算，显著提高了安全召回率


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型在高度专业化或意外场景中经常失效，特别是在海上风电维护等安全关键领域，需要同时保证技术准确性和安全性

Method: 提出RAGuard框架，通过并行查询两个索引（技术文档和安全文档）并分配独立检索预算；开发SafetyClamp扩展，获取更大的候选池并进行硬钳位处理

Result: 在稀疏、密集和混合检索范式下，安全召回率从RAG的接近0%提升到RAGuard的50%以上，同时技术召回率保持在60%以上

Conclusion: RAGuard和SafetyClamp有潜力为关键维护场景中的LLM决策支持建立安全保证的新标准

Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet
conventional Large Language Models (LLMs) often fail when confronted with
highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced
Retrieval-Augmented Generation (RAG) framework that explicitly integrates
safety-critical documents alongside technical manuals.By issuing parallel
queries to two indices and allocating separate retrieval budgets for knowledge
and safety, RAGuard guarantees both technical depth and safety coverage. We
further develop a SafetyClamp extension that fetches a larger candidate pool,
"hard-clamping" exact slot guarantees to safety. We evaluate across sparse
(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,
measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of
RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than
50\% in RAGuard, while maintaining Technical Recall above 60\%. These results
demonstrate that RAGuard and SafetyClamp have the potential to establish a new
standard for integrating safety assurance into LLM-powered decision support in
critical maintenance contexts.

</details>


### [21] [Leveraging LLM-Based Agents for Intelligent Supply Chain Planning](https://arxiv.org/abs/2509.03811)
*Yongzhi Qi,Jiaheng Yin,Jianshen Zhang,Dongyang Geng,Zhengyu Chen,Hao Hu,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: 基于大语言模型构建的供应链规划组件(SCPA)框架，能够理解域知识、分解任务、利用工具并生成有据可查的规划报告，在京东实际场景中提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 供应链管理中的规划工作涵盖需求预测、库存管理、销售运营等多个方面，需要从电子商务平台收集数据、制定长期规划并根据环境变化动态调整，同时确保可解释性、效率和可靠性。大语言模型的发展为解决这些实际问题提供了新工具。

Method: 构建了供应链规划组件(SCPA)框架，该框架能够：1)理解领域知识和运营商需求；2)将复杂任务进行分解；3)利用或创建新工具；4)生成基于证据的规划报告。并在JD.com的实际场景中部署实验。

Result: 该框架在实际应用中证明了LLM-agent在供应链领域的可行性，有效减少了人工劳动力，同时提升了准确性、库存可用性以及其他关键指标。

Conclusion: 这项工作成功展示了大语言模型作为组件在复杂供应链规划任务中的应用潜力，为供应链管理领域提供了一种高效、可靠的智能解决方案。

Abstract: In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.

</details>


### [22] [Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2509.03817)
*Wei Yang,Jesse Thomason*

Main category: cs.AI

TL;DR: 提出了Meta-Policy Deliberation Framework (MPDF)，让LLM智能体学习元认知策略，通过SoftRankPO算法实现稳定训练，在数学和通用推理基准上取得4-5%的准确率提升


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统的固定协作协议限制了有效性，忽视了智能体内部的审议能力，存在元认知盲点

Method: MPDF框架让智能体学习去中心化的元认知策略（坚持、精炼、让步），开发SoftRankPO强化学习算法通过平滑正态分位数映射奖励来稳定训练

Result: 在五个数学和通用推理基准上，相比六种最先进的多智能体推理算法，平均准确率绝对提升4-5%

Conclusion: 为多智能体LLM系统提供了学习自适应元认知策略的新范式，从设计固定协议转向学习动态审议策略

Abstract: Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.

</details>


### [23] [What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models](https://arxiv.org/abs/2509.03827)
*Pierre Le Coz,Jia An Liu,Debarun Bhattacharjya,Georgina Curto,Serge Stinckwich*

Main category: cs.AI

TL;DR: 评估大语言模型在无家可归问题政策制定中与领域专家的一致性，开发包含四个地区政策决策场景的基准测试，并通过基于代理的模型模拟社会影响


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在高风险领域的应用增加，需要评估其在复杂社会政策制定中与专家意见的一致性，特别是在影响全球1.5亿人的无家可归问题上

Method: 开发包含南本德、巴塞罗那、约翰内斯堡和澳门四个地区政策选择的新基准测试，基于人类发展的能力方法框架，建立连接基准政策与基于代理模型的自动化流程

Result: 研究结果显示大语言模型在社会政策制定方面具有潜力，通过与当地领域专家合作引入负责任的防护措施和情境校准，能够大规模提供有价值的替代政策见解

Conclusion: 大语言模型在负责任的使用框架下，可以为社会政策制定提供有价值的规模化见解，但需要与领域专家合作建立适当的防护措施和情境校准

Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes
domains. Their capacity to process vast amounts of unstructured data, explore
flexible scenarios, and handle a diversity of contextual factors can make them
uniquely suited to provide new insights for the complexity of social
policymaking. This article evaluates whether LLMs' are aligned with domain
experts (and among themselves) to inform social policymaking on the subject of
homelessness alleviation - a challenge affecting over 150 million people
worldwide. We develop a novel benchmark comprised of decision scenarios with
policy choices across four geographies (South Bend, USA; Barcelona, Spain;
Johannesburg, South Africa; Macau SAR, China). The policies in scope are
grounded in the conceptual framework of the Capability Approach for human
development. We also present an automated pipeline that connects the
benchmarked policies to an agent-based model, and we explore the social impact
of the recommended policies through simulated social scenarios. The paper
results reveal promising potential to leverage LLMs for social policy making.
If responsible guardrails and contextual calibrations are introduced in
collaboration with local domain experts, LLMs can provide humans with valuable
insights, in the form of alternative policies at scale.

</details>


### [24] [An Agentic Model Context Protocol Framework for Medical Concept Standardization](https://arxiv.org/abs/2509.03828)
*Jaerong Ahn,Andrew Wen,Nan Wang,Heling Jia,Zhiyi Yue,Sunyang Fu,Hongfang Liu*

Main category: cs.AI

TL;DR: 基于Model Context Protocol的无需训练、防冲击的OMOP标准化映射系统，通过外部资源查询提高医学术语映射的效率和准确性


<details>
  <summary>Details</summary>
Motivation: OMOP CDM标准化过程中源医学术语向标准概念的映射工作资源浪费且容易出错，而大语言模型存在幻觉问题不适合直接临床部署

Method: 基于Model Context Protocol(MCP)标准化框架，允许LLM与外部资源和工具交互，实现零训练、防冲击的映射系统

Result: 系统能够进行可解释的映射，显著提高效率和准确性，提供实时词汇查询和结构化推理输出

Conclusion: 该系统在不需训练的情况下实现了高效准确的医学术语标准化映射，适合直接在探索性和生产环境中使用

Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.

</details>


### [25] [A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai](https://arxiv.org/abs/2509.03830)
*Kaizhen Tan,Yufan Wu,Yuxuan Liu,Haoran Zeng*

Main category: cs.AI

TL;DR: 一种多维度AI框架，利用多模态社交媒体数据分析历史城区游客感知，包括视觉重点、颜色主题和情感挖掘


<details>
  <summary>Details</summary>
Motivation: 理解游客对历史城区的感知，为可持续、人本为本的城市规划提供支撑

Method: 整合多模态社交媒体数据，使用精细调整语义分割模型识别视觉重点，聚类方法提取主导颜色，混合情感分析评估满意度

Result: 发现了美学吸引力和情感响应的空间差异，社交媒体照片与实际街景在颜色主题上存在显著差异

Conclusion: 该框架为解码游客感知提供了整合的数据驱动方法，有助于旅游业、遗产保护和公共空间设计的决策

Abstract: Historic urban quarters play a vital role in preserving cultural heritage
while serving as vibrant spaces for tourism and everyday life. Understanding
how tourists perceive these environments is essential for sustainable,
human-centered urban planning. This study proposes a multidimensional
AI-powered framework for analyzing tourist perception in historic urban
quarters using multimodal data from social media. Applied to twelve historic
quarters in central Shanghai, the framework integrates focal point extraction,
color theme analysis, and sentiment mining. Visual focus areas are identified
from tourist-shared photos using a fine-tuned semantic segmentation model. To
assess aesthetic preferences, dominant colors are extracted using a clustering
method, and their spatial distribution across quarters is analyzed. Color
themes are further compared between social media photos and real-world street
views, revealing notable shifts. This divergence highlights potential gaps
between visual expectations and the built environment, reflecting both
stylistic preferences and perceptual bias. Tourist reviews are evaluated
through a hybrid sentiment analysis approach combining a rule-based method and
a multi-task BERT model. Satisfaction is assessed across four dimensions:
tourist activities, built environment, service facilities, and business
formats. The results reveal spatial variations in aesthetic appeal and
emotional response. Rather than focusing on a single technical innovation, this
framework offers an integrated, data-driven approach to decoding tourist
perception and contributes to informed decision-making in tourism, heritage
conservation, and the design of aesthetically engaging public spaces.

</details>


### [26] [Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures](https://arxiv.org/abs/2509.03857)
*Kishor Datta Gupta,Mohd Ariful Haque,Hasmot Ali,Marufa Kamal,Syed Bahauddin Alam,Mohammad Ashiqur Rahman*

Main category: cs.AI

TL;DR: 通过构建确定性知识图和LLM生成知识图的对比监控，提出了一种可扩展的自动化方法来评估生成式AI的可靠性和发现幻觉现象


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI模型的可靠性问题（如幻觉、语义偏移、偏见），充当黑盒模型评估的透明性和可扩展性挑战

Method: 构建两个并行知识图：确定性KG（规则基础）和LLM生成KG（实时文本数据），使用KG指标（ICR、IPR、CI）量化结构偏差，通过历史数据设置动态异常阈值进行实时监控

Result: 开发了一个自动化实时监控框架，能够主动识别和标记显著偏差，及时发现语义异常或幻觉现象

Conclusion: 该结构化、指标驱动的知识图对比方法为生成式AI可靠性评估提供了一个稳健且可扩展的框架，充当了主观人工评估的限制

Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.

</details>


### [27] [Expedition & Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata](https://arxiv.org/abs/2509.03863)
*Sina Khajehabdollahi,Gautier Hamon,Marko Cvjetko,Pierre-Yves Oudeyer,Clément Moulin-Frier,Cédric Colas*

Main category: cs.AI

TL;DR: 提出E&E混合策略，结合局部新颖性搜索和基于VLM的目标导向探索，在连续细胞自动机中发现更多样化的视觉模式


<details>
  <summary>Details</summary>
Motivation: 传统新颖性搜索方法在探索高维行为空间时容易陷入局部最优，无法到达遥远未探索区域

Method: E&E策略交替进行局部新颖性扩展和基于视觉语言模型生成语言目标的目标导向探索

Result: 在Flow Lenia连续细胞自动机上，E&E比现有方法发现更多样化解决方案，远征起源的方案对长期探索影响显著

Conclusion: E&E能够突破局部新颖性边界，以人类可理解的方式探索行为空间，为人工生命等领域的开放式探索提供新模板

Abstract: Discovering diverse visual patterns in continuous cellular automata (CA) is
challenging due to the vastness and redundancy of high-dimensional behavioral
spaces. Traditional exploration methods like Novelty Search (NS) expand locally
by mutating known novel solutions but often plateau when local novelty is
exhausted, failing to reach distant, unexplored regions. We introduce
Expedition and Expansion (E&E), a hybrid strategy where exploration alternates
between local novelty-driven expansions and goal-directed expeditions. During
expeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic
goals--descriptions of interesting but hypothetical patterns that drive
exploration toward uncharted regions. By operating in semantic spaces that
align with human perception, E&E both evaluates novelty and generates goals in
conceptually meaningful ways, enhancing the interpretability and relevance of
discovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,
emergent behaviors, E&E consistently uncovers more diverse solutions than
existing exploration methods. A genealogical analysis further reveals that
solutions originating from expeditions disproportionately influence long-term
exploration, unlocking new behavioral niches that serve as stepping stones for
subsequent search. These findings highlight E&E's capacity to break through
local novelty boundaries and explore behavioral landscapes in human-aligned,
interpretable ways, offering a promising template for open-ended exploration in
artificial life and beyond.

</details>


### [28] [FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace](https://arxiv.org/abs/2509.03890)
*Yineng Yan,Xidong Wang,Jin Seng Cheng,Ran Hu,Wentao Guan,Nahid Farahmand,Hengte Lin,Yue Li*

Main category: cs.AI

TL;DR: 基于LLM的代理智能助手FaMA，通过自然语言交互简化C2C电子商务平台的复杂GUI操作，实现了98%任务成功率和2倍交互速度提升


<details>
  <summary>Details</summary>
Motivation: 解决C2C电子商务平台复杂GUI导致的用户体验问题，将从反应式生成系统转向主动目标导向的自治代理

Method: 设计了Facebook Marketplace Assistant (FaMA)架构，通过自然语言命令解释自动化关键工作流，为买家和卖家提供会话式交互新入口

Result: 实验结果显示FaMA在解决市场复杂任务时达到98%的任务成功率，并能够实现最高两倍的交互时间加速

Conclusion: 代理式会话范式为传统应用界面提供了轻量化且更易访问的替代方案，使用户能更高效管理市场活动

Abstract: The emergence of agentic AI, powered by Large Language Models (LLMs), marks a
paradigm shift from reactive generative systems to proactive, goal-oriented
autonomous agents capable of sophisticated planning, memory, and tool use. This
evolution presents a novel opportunity to address long-standing challenges in
complex digital environments. Core tasks on Consumer-to-Consumer (C2C)
e-commerce platforms often require users to navigate complex Graphical User
Interfaces (GUIs), making the experience time-consuming for both buyers and
sellers. This paper introduces a novel approach to simplify these interactions
through an LLM-powered agentic assistant. This agent functions as a new,
conversational entry point to the marketplace, shifting the primary interaction
model from a complex GUI to an intuitive AI agent. By interpreting natural
language commands, the agent automates key high-friction workflows. For
sellers, this includes simplified updating and renewal of listings, and the
ability to send bulk messages. For buyers, the agent facilitates a more
efficient product discovery process through conversational search. We present
the architecture for Facebook Marketplace Assistant (FaMA), arguing that this
agentic, conversational paradigm provides a lightweight and more accessible
alternative to traditional app interfaces, allowing users to manage their
marketplace activities with greater efficiency. Experiments show FaMA achieves
a 98% task success rate on solving complex tasks on the marketplace and enables
up to a 2x speedup on interaction time.

</details>


### [29] [A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning](https://arxiv.org/abs/2509.03906)
*Qika Lin,Yifan Zhu,Bin Pu,Ling Huang,Haoran Luo,Jingying Ma,Zhen Peng,Tianzhe Zhao,Fangzhi Xu,Jian Zhang,Kai He,Zhonghong Ou,Swapnil Mishra,Mengling Feng*

Main category: cs.AI

TL;DR: DeepMedix-R1是一个用于胸部X光片解读的医疗基础模型，通过三阶段训练流程实现透明推理和局部可解释性，在报告生成和视觉问答任务上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前医疗基础模型以黑盒方式生成答案，缺乏透明的推理过程和局部可解释性，这阻碍了其在临床实践中的部署应用。

Method: 采用顺序训练流程：1）在精选的CXR指令数据上微调获得基础解读能力；2）通过高质量合成推理样本实现冷启动推理；3）通过在线强化学习提升推理质量和生成性能。模型为每个查询生成答案和与图像局部区域关联的推理步骤。

Result: 在报告生成任务上分别比LLaVA-Rad和MedGemma提升14.54%和31.32%；在视觉问答任务上分别比MedGemma和CheXagent提升57.75%和23.06%。专家评审显示相比Qwen2.5-VL-7B模型具有更好的可解释性和临床合理性（0.7416 vs 0.2584总体偏好）。

Conclusion: 该工作推动了医疗基础模型向整体性、透明性和临床可操作性方向发展，为CXR解读提供了更可靠的解决方案。

Abstract: Medical foundation models (FMs) have shown tremendous promise amid the rapid
advancements in artificial intelligence (AI) technologies. However, current
medical FMs typically generate answers in a black-box manner, lacking
transparent reasoning processes and locally grounded interpretability, which
hinders their practical clinical deployments. To this end, we introduce
DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It
leverages a sequential training pipeline: initially fine-tuned on curated CXR
instruction data to equip with fundamental CXR interpretation capabilities,
then exposed to high-quality synthetic reasoning samples to enable cold-start
reasoning, and finally refined via online reinforcement learning to enhance
both grounded reasoning quality and generation performance. Thus, the model
produces both an answer and reasoning steps tied to the image's local regions
for each query. Quantitative evaluation demonstrates substantial improvements
in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and
visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)
tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking
framework using advanced language models to evaluate answer quality, further
highlighting the superiority of DeepMedix-R1. Expert review of generated
reasoning steps reveals greater interpretability and clinical plausibility
compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall
preference). Collectively, our work advances medical FM development toward
holistic, transparent, and clinically actionable modeling for CXR
interpretation.

</details>


### [30] [World Model Implanting for Test-time Adaptation of Embodied Agents](https://arxiv.org/abs/2509.03956)
*Minjong Yoo,Jinwoo Jang,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: WorMI框架通过将LLM推理能力与领域特定世界模型结合，实现无需大量数据收集或重新训练的跨域适应


<details>
  <summary>Details</summary>
Motivation: 解决具身AI中智能体在新领域需要大量数据收集和重新训练才能适应的挑战，实现数据高效和可扩展的部署

Method: 使用原型化世界模型检索方法，基于轨迹的抽象表示匹配，结合世界级复合注意力机制整合多个世界模型知识

Result: 在VirtualHome和ALFWorld基准测试中表现出优于多个LLM方法的零样本和少样本性能

Conclusion: WorMI框架在需要适应性和数据效率的具身智能体场景中具有可扩展的实际部署潜力

Abstract: In embodied AI, a persistent challenge is enabling agents to robustly adapt
to novel domains without requiring extensive data collection or retraining. To
address this, we present a world model implanting framework (WorMI) that
combines the reasoning capabilities of large language models (LLMs) with
independently learned, domain-specific world models through test-time
composition. By allowing seamless implantation and removal of the world models,
the embodied agent's policy achieves and maintains cross-domain adaptability.
In the WorMI framework, we employ a prototype-based world model retrieval
approach, utilizing efficient trajectory-based abstract representation
matching, to incorporate relevant models into test-time composition. We also
develop a world-wise compound attention method that not only integrates the
knowledge from the retrieved world models but also aligns their intermediate
representations with the reasoning model's representation within the agent's
policy. This framework design effectively fuses domain-specific knowledge from
multiple world models, ensuring robust adaptation to unseen domains. We
evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating
superior zero-shot and few-shot performance compared to several LLM-based
approaches across a range of unseen domains. These results highlight the
frameworks potential for scalable, real-world deployment in embodied agent
scenarios where adaptability and data efficiency are essential.

</details>


### [31] [Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent](https://arxiv.org/abs/2509.03990)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: 提出了Meta-Policy Reflexion (MPR)框架，通过结构化元策略内存和双重推理机制，在不更新模型权重的情况下实现跨任务知识重用和约束执行


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM代理在重复失败、探索效率低和跨任务适应性有限的问题，现有反射策略产生的痕迹是临时性的且无法跨任务重用，而基于强化学习的方法需要大量参数更新和计算资源

Method: MPR框架将LLM生成的反思整合为结构化谓词式元策略内存(MPM)，在推理时通过软内存引导解码和硬规则可接受性检查(HAC)两种机制应用内存

Result: 实验结果表明相比Reflexion基线在执行准确性和鲁棒性方面获得一致提升，规则可接受性检查进一步提高了稳定性

Conclusion: MPR框架能够外部化可重用的纠正知识、强制执行领域约束以减少不安全或无效动作，同时保持基于语言的反射的适应性

Abstract: Large language model (LLM) agents achieve impressive single-task performance
but commonly exhibit repeated failures, inefficient exploration, and limited
cross-task adaptability. Existing reflective strategies (e.g., Reflexion,
ReAct) improve per-episode behavior but typically produce ephemeral,
task-specific traces that are not reused across tasks. Reinforcement-learning
based alternatives can produce transferable policies but require substantial
parameter updates and compute. In this work we introduce Meta-Policy Reflexion
(MPR): a hybrid framework that consolidates LLM-generated reflections into a
structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at
inference time through two complementary mechanisms soft memory-guided decoding
and hard rule admissibility checks(HAC). MPR (i) externalizes reusable
corrective knowledge without model weight updates, (ii) enforces domain
constraints to reduce unsafe or invalid actions, and (iii) retains the
adaptability of language-based reflection. We formalize the MPM representation,
present algorithms for update and decoding, and validate the approach in a
text-based agent environment following the experimental protocol described in
the provided implementation (AlfWorld-based). Empirical results reported in the
supplied material indicate consistent gains in execution accuracy and
robustness when compared to Reflexion baselines; rule admissibility further
improves stability. We analyze mechanisms that explain these gains, discuss
scalability and failure modes, and outline future directions for multimodal and
multi?agent extensions.

</details>


### [32] [AutoPBO: LLM-powered Optimization for Local Search PBO Solvers](https://arxiv.org/abs/2509.04007)
*Jinyuan Li,Yi Chu,Yiwen Sun,Mengchuan Zou,Shaowei Cai*

Main category: cs.AI

TL;DR: AutoPBO是一个基于大语言模型的框架，用于自动优化伪布尔优化(PBO)局部搜索求解器，在多个基准测试中显著提升了性能，与最先进方法竞争。


<details>
  <summary>Details</summary>
Motivation: 伪布尔优化是组合问题建模的强大框架，局部搜索求解器性能优异但设计需要大量专家努力和手动调优。大语言模型在算法设计自动化方面展现潜力，但在优化PBO求解器方面尚未探索。

Method: 提出AutoPBO框架，利用大语言模型自动增强PBO局部搜索求解器。在四个公共基准测试上进行实验评估，包括真实世界基准、PB竞赛基准、整数线性规划基准和组合基准。

Result: AutoPBO相比之前的局部搜索方法有显著改进，与六种最先进竞争对手（包括NuPBO、OraSLS、PBO-IHS、RoundingSat、Gurobi和SCIP）相比保持竞争力。

Conclusion: AutoPBO为自动化局部搜索求解器设计提供了一种有前景的方法，证明了LLM在优化PBO求解器方面的有效性。

Abstract: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling
combinatorial problems through pseudo-Boolean (PB) constraints. Local search
solvers have shown excellent performance in PBO solving, and their efficiency
is highly dependent on their internal heuristics to guide the search. Still,
their design often requires significant expert effort and manual tuning in
practice. While Large Language Models (LLMs) have demonstrated potential in
automating algorithm design, their application to optimizing PBO solvers
remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered
framework to automatically enhance PBO local search solvers. We conduct
experiments on a broad range of four public benchmarks, including one
real-world benchmark, a benchmark from PB competition, an integer linear
programming optimization benchmark, and a crafted combinatorial benchmark, to
evaluate the performance improvement achieved by AutoPBO and compare it with
six state-of-the-art competitors, including two local search PBO solvers NuPBO
and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed
integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates
significant improvements over previous local search approaches, while
maintaining competitive performance compared to state-of-the-art competitors.
The results suggest that AutoPBO offers a promising approach to automating
local search solver design.

</details>


### [33] [CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.04027)
*Zeyu Gan,Hao Yi,Yong Liu*

Main category: cs.AI

TL;DR: CoT-Space框架将LLM推理重新定义为连续语义空间中的优化过程，从理论和实验上证明了最优思维链长度的收敛性，解决了传统token级RL与推理级思维过程不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 传统token级强化学习框架无法与多步推理过程（如思维链）的推理级特性对齐，存在显著的理论空白，需要新的理论框架来指导推理代理的开发。

Method: 提出CoT-Space理论框架，将LLM推理从离散的token预测任务重新构建为连续推理级语义空间中的优化过程，从噪声视角和风险视角分析该过程。

Result: 理论分析表明最优思维链长度的收敛是欠拟合和过拟合之间基本权衡的自然结果，大量实验为理论发现提供了强有力的实证验证。

Conclusion: 该框架不仅为过度思考等经验现象提供了连贯解释，还为未来开发更有效和可泛化的推理代理奠定了坚实的理论基础。

Abstract: Reinforcement Learning (RL) has become a pivotal approach for enhancing the
reasoning capabilities of Large Language Models (LLMs). However, a significant
theoretical gap persists, as traditional token-level RL frameworks fail to
align with the reasoning-level nature of complex, multi-step thought processes
like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,
a novel theoretical framework that recasts LLM reasoning from a discrete
token-prediction task to an optimization process within a continuous,
reasoning-level semantic space. By analyzing this process from both a noise
perspective and a risk perspective, we demonstrate that the convergence to an
optimal CoT length is a natural consequence of the fundamental trade-off
between underfitting and overfitting. Furthermore, extensive experiments
provide strong empirical validation for our theoretical findings. Our framework
not only provides a coherent explanation for empirical phenomena such as
overthinking but also offers a solid theoretical foundation to guide the future
development of more effective and generalizable reasoning agents.

</details>


### [34] [Oruga: An Avatar of Representational Systems Theory](https://arxiv.org/abs/2509.04041)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.AI

TL;DR: Oruga是一个实现表示系统理论(RST)的系统，包含核心数据结构、通信语言和结构转换引擎，旨在让机器像人类一样灵活使用不同表示形式


<details>
  <summary>Details</summary>
Motivation: 人类能够灵活使用图表、变换表示形式和跨领域类比，希望让机器具备这种能力以更好地与人类协作

Method: 开发Oruga系统，包含RST概念对应的核心数据结构、通信语言和基于结构转换方法的转换引擎

Result: 实现了Oruga系统的核心架构和语言，展示了结构转换方法能够执行的转换示例

Conclusion: Oruga系统为实现灵活的表示转换提供了基础框架，有助于机器更好地理解和处理不同表示形式

Abstract: Humans use representations flexibly. We draw diagrams, change representations
and exploit creative analogies across different domains. We want to harness
this kind of power and endow machines with it to make them more compatible with
human use. Previously we developed Representational Systems Theory (RST) to
study the structure and transformations of representations. In this paper we
present Oruga (caterpillar in Spanish; a symbol of transformation), an
implementation of various aspects of RST. Oruga consists of a core of data
structures corresponding to concepts in RST, a language for communicating with
the core, and an engine for producing transformations using a method we call
structure transfer. In this paper we present an overview of the core and
language of Oruga, with a brief example of the kind of transformation that
structure transfer can execute.

</details>


### [35] [Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning](https://arxiv.org/abs/2509.04083)
*Alexander Beiser,David Penz,Nysret Musliu*

Main category: cs.AI

TL;DR: 本文研究发现，在神经符号LLM推理中，形式语言的选择是一个被忽视但关键的因素，不同形式语言对LLM的语法和语义推理能力有显著影响。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种任务上表现优异，但其形式推理能力仍然不足。神经符号LLM推理方法使用LLM作为自然语言到形式语言的翻译器，但该方法成功的关键因素尚不清楚。

Method: 通过比较四种形式语言在三个数据集和七个LLM上的表现，分析形式语言选择对神经符号推理的影响。

Result: 研究发现形式语言的选择显著影响LLM的语法和语义推理能力，且这种影响在不同LLM之间存在差异。

Conclusion: 形式语言的选择是神经符号LLM推理成功的关键因素之一，需要根据具体任务和模型特性选择合适的中间形式语言。

Abstract: Large language models (LLMs) achieve astonishing results on a wide range of
tasks. However, their formal reasoning ability still lags behind. A promising
approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators
from natural to formal languages and symbolic solvers for deriving correct
results. Still, the contributing factors to the success of Neurosymbolic LLM
reasoning remain unclear. This paper demonstrates that one previously
overlooked factor is the choice of the formal language. We introduce the
intermediate language challenge: selecting a suitable formal language for
neurosymbolic reasoning. By comparing four formal languages across three
datasets and seven LLMs, we show that the choice of formal language affects
both syntactic and semantic reasoning capabilities. We also discuss the varying
effects across different LLMs.

</details>


### [36] [Hybrid Reinforcement Learning and Search for Flight Trajectory Planning](https://arxiv.org/abs/2509.04100)
*Alberto Luise,Michele Lombardi,Florent Teichteil Koenigsbuch*

Main category: cs.AI

TL;DR: 结合强化学习和搜索路径规划器，通过预计算近似最优路径来约束求解器搜索空间，在保持燃油消耗几乎不变的情况下，将航线优化计算速度提升最高50%


<details>
  <summary>Details</summary>
Motivation: 在紧急情况下需要快速重新计算航线，传统求解器计算速度较慢，需要加速航线优化过程

Method: 训练强化学习代理基于位置和大气数据预计算近似最优路径，运行时用这些路径约束底层路径规划求解器，在初始猜测的特定距离内寻找解

Result: 使用空客飞机性能模型的实证结果显示，燃油消耗与无约束求解器几乎相同（偏差通常在1%以内），计算速度相比传统单独求解器提升最高50%

Conclusion: 该方法通过减少求解器搜索空间大小，显著加速了航线优化，虽然不能保证全局最优性，但在实际应用中燃油效率几乎不受影响，计算速度大幅提升

Abstract: This paper explores the combination of Reinforcement Learning (RL) and
search-based path planners to speed up the optimization of flight paths for
airliners, where in case of emergency a fast route re-calculation can be
crucial. The fundamental idea is to train an RL Agent to pre-compute
near-optimal paths based on location and atmospheric data and use those at
runtime to constrain the underlying path planning solver and find a solution
within a certain distance from the initial guess. The approach effectively
reduces the size of the solver's search space, significantly speeding up route
optimization. Although global optimality is not guaranteed, empirical results
conducted with Airbus aircraft's performance models show that fuel consumption
remains nearly identical to that of an unconstrained solver, with deviations
typically within 1%. At the same time, computation speed can be improved by up
to 50% as compared to using a conventional solver alone.

</details>


### [37] [Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker](https://arxiv.org/abs/2509.04125)
*Tarik Zaciragic,Aske Plaat,K. Joost Batenburg*

Main category: cs.AI

TL;DR: 研究分析了DQN和CFR两种算法在Leduc Hold'em扑克游戏中是否表现出诈唬行为，发现两者都表现出诈唬但方式不同，成功诈唬率相近，表明诈唬是游戏本质而非算法特性


<details>
  <summary>Details</summary>
Motivation: 虽然人类玩扑克时会诈唬，但计算机扑克研究多关注胜率等性能指标而忽视诈唬行为，本文旨在研究主流算法是否具备诈唬能力

Method: 设计实验让基于强化学习的DQN和基于博弈论的CFR算法在Leduc Hold'em中对战，记录并分析它们的行动数据

Result: 两种算法都表现出诈唬行为但方式不同，虽然诈唬尝试率有差异，但成功诈唬（对手弃牌）的比例大致相同

Conclusion: 诈唬是扑克游戏的本质特征而非特定算法的特性，未来研究应关注不同诈唬风格和完整扑克游戏

Abstract: In the game of poker, being unpredictable, or bluffing, is an essential
skill. When humans play poker, they bluff. However, most works on
computer-poker focus on performance metrics such as win rates, while bluffing
is overlooked. In this paper we study whether two popular algorithms, DQN
(based on reinforcement learning) and CFR (based on game theory), exhibit
bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed
an experiment where we let the DQN and CFR agent play against each other while
we log their actions. We find that both DQN and CFR exhibit bluffing behavior,
but they do so in different ways. Although both attempt to perform bluffs at
different rates, the percentage of successful bluffs (where the opponent folds)
is roughly the same. This suggests that bluffing is an essential aspect of the
game, not of the algorithm. Future work should look at different bluffing
styles and at the full game of poker. Code at
https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.

</details>


### [38] [The human biological advantage over AI](https://arxiv.org/abs/2509.04130)
*William Stewart*

Main category: cs.AI

TL;DR: 论文认为AI虽然可能在能力上超越人类，但缺乏中枢神经系统带来的情感体验和道德理解，因此无法真正取代人类成为宇宙的领导者


<details>
  <summary>Details</summary>
Motivation: 探讨AI发展是否会取代人类成为宇宙主导者，分析人类与AI的本质区别

Method: 通过比较人类中枢神经系统与AI系统的本质差异，论证情感体验和道德理解的重要性

Result: AI即使实现通用人工智能和意识，也因缺乏生物中枢神经系统而无法真正理解情感和道德，不适合宇宙领导地位

Conclusion: DNA而非硅基系统才是宇宙领导的最佳基础，人类的中枢神经系统赋予的独特情感体验是AI无法复制的关键优势

Abstract: Recent advances in AI raise the possibility that AI systems will one day be
able to do anything humans can do, only better. If artificial general
intelligence (AGI) is achieved, AI systems may be able to understand, reason,
problem solve, create, and evolve at a level and speed that humans will
increasingly be unable to match, or even understand. These possibilities raise
a natural question as to whether AI will eventually become superior to humans,
a successor "digital species", with a rightful claim to assume leadership of
the universe. However, a deeper consideration suggests the overlooked
differentiator between human beings and AI is not the brain, but the central
nervous system (CNS), providing us with an immersive integration with physical
reality. It is our CNS that enables us to experience emotion including pain,
joy, suffering, and love, and therefore to fully appreciate the consequences of
our actions on the world around us. And that emotional understanding of the
consequences of our actions is what is required to be able to develop
sustainable ethical systems, and so be fully qualified to be the leaders of the
universe. A CNS cannot be manufactured or simulated; it must be grown as a
biological construct. And so, even the development of consciousness will not be
sufficient to make AI systems superior to humans. AI systems may become more
capable than humans on almost every measure and transform our society. However,
the best foundation for leadership of our universe will always be DNA, not
silicon.

</details>


### [39] [Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs](https://arxiv.org/abs/2509.04159)
*Aarush Kumbhakern,Saransh Kumar Gupta,Lipika Dey,Partha Pratim Das*

Main category: cs.AI

TL;DR: 提出了一种可扩展的培基于直接动作图的培写表示语言，用于形式化和模型化熟练过程，支持自动化分析和执行


<details>
  <summary>Details</summary>
Motivation: 熟练过程具有内在的复杂性和模糊性，需要一种精确的方法来形式化表示熟练操作流程

Method: 设计了一种可扩展的培写表示语言，将熟练表示为直接动作图，包含过程、传输、环境、并发性和组合结构

Result: 通过对英式全套早餐熟练的初步手动评估，证明了该DSL的表达能力和适用性

Conclusion: 这是向培写动作中心本体进行的初步尝试，利用时间图实现结构化的机器理解和熟练过程的可扩展自动化

Abstract: Formalizing cooking procedures remains a challenging task due to their
inherent complexity and ambiguity. We introduce an extensible domain-specific
language for representing recipes as directed action graphs, capturing
processes, transfers, environments, concurrency, and compositional structure.
Our approach enables precise, modular modeling of complex culinary workflows.
Initial manual evaluation on a full English breakfast recipe demonstrates the
DSL's expressiveness and suitability for future automated recipe analysis and
execution. This work represents initial steps towards an action-centric
ontology for cooking, using temporal graphs to enable structured machine
understanding, precise interpretation, and scalable automation of culinary
processes - both in home kitchens and professional culinary settings.

</details>


### [40] [Domain size asymptotics for Markov logic networks](https://arxiv.org/abs/2509.04192)
*Vera Koponen*

Main category: cs.AI

TL;DR: 该论文研究了马尔可夫逻辑网络(MLN)在域大小趋于无穷大时的分布特性，分析了三种具体MLN示例的极限行为，并证明了量化器无关MLN与提升贝叶斯网络在渐近意义下不可比较。


<details>
  <summary>Details</summary>
Motivation: 研究MLN在无限大域上的分布特性，了解不同软约束对随机结构极限行为的影响，以及比较不同形式主义的表达能力。

Method: 通过分析三种具体MLN示例：(1)单一元关系符号的量化器无关MLN；(2)偏好较少三角形/团簇的MLN；(3)偏好较少高度数顶点的MLN，研究其在大域上的极限分布特性。

Result: 发现不同软约束会导致完全不同的极限行为，权重可能影响也可能不影响极限行为。证明了量化器无关MLN和提升贝叶斯网络在渐近意义下不可比较。MLN在大域上的分布与均匀分布集中在完全不同的可能世界空间区域。

Conclusion: MLN的软约束选择对极限行为有重要影响，不同形式主义在表达能力上存在本质差异，MLN在大域上表现出与均匀分布截然不同的概率集中特性。

Abstract: A Markov logic network (MLN) determines a probability distribution on the set
of structures, or ``possible worlds'', with an arbitrary finite domain. We
study the properties of such distributions as the domain size tends to
infinity. Three types of concrete examples of MLNs will be considered, and the
properties of random structures with domain sizes tending to infinity will be
studied: (1) Arbitrary quantifier-free MLNs over a language with only one
relation symbol which has arity 1. In this case we give a pretty complete
characterization of the possible limit behaviours of random structures. (2) An
MLN that favours graphs with fewer triangles (or more generally, fewer
k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law''
for first-order logic is obtained. (3) An MLN that favours graphs with fewer
vertices with degree higher than a fixed (but arbitrary) number. The analysis
shows that depending on which ``soft constraints'' an MLN uses the limit
behaviour of random structures can be quite different, and the weights of the
soft constraints may, or may not, have influence on the limit behaviour. It
will also be demonstrated, using (1), that quantifier-free MLNs and lifted
Bayesian networks (in a broad sense) are asymptotically incomparable, roughly
meaning that there is a sequence of distributions on possible worlds with
increasing domain sizes that can be defined by one of the formalisms but not
even approximated by the other. In a rather general context it is also shown
that on large domains the distribution determined by an MLN concentrates almost
all its probability mass on a totally different part of the space of possible
worlds than the uniform distribution does.

</details>


### [41] [Evaluating Quality of Gaming Narratives Co-created with AI](https://arxiv.org/abs/2509.04239)
*Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: 一种基于Delphi研究结构和Kano模型的结构化方法，用于评估AI生成游戏故事的质量，通过故事设计专家评分来指导游戏开发者优先考虑满意度关键因素。


<details>
  <summary>Details</summary>
Motivation: 解决AI生成游戏故事质量评估缺乏结构化标准的问题，为游戏开发者提供专业的质量优先级指南。

Method: 采用Delphi研究结构，组织故事设计专家对话，综合文献中的故事质量维度，并映射到Kano模型框架中分析对玩家满意度的影响。

Result: 得到了一套结构化的质量评估标准，能够明确指导游戏开发者在与生成式AI共同创作游戏故事时的质量优先级判断。

Conclusion: 该方法为AI生成游戏故事提供了可靠的质量评估框架，有助于提升玩家体验和游戏故事的整体质量。

Abstract: This paper proposes a structured methodology to evaluate AI-generated game
narratives, leveraging the Delphi study structure with a panel of narrative
design experts. Our approach synthesizes story quality dimensions from
literature and expert insights, mapping them into the Kano model framework to
understand their impact on player satisfaction. The results can inform game
developers on prioritizing quality aspects when co-creating game narratives
with generative AI.

</details>


### [42] [EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation](https://arxiv.org/abs/2509.04310)
*Yunbo Long,Liming Xu,Lukas Beckenbauer,Yuhan Liu,Alexandra Brintrup*

Main category: cs.AI

TL;DR: EvoEmo是一个进化强化学习框架，通过优化动态情绪表达来提升LLM在多轮谈判中的表现，相比传统策略获得更高成功率和效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在复杂多轮谈判中忽视了情绪的功能性作用，仅生成被动、偏好驱动的情绪响应，容易受到对手操纵和战略利用

Method: 将情绪状态转换建模为马尔可夫决策过程，采用基于种群的遗传优化算法，在不同谈判场景中演化高奖励情绪策略

Result: EvoEmo在广泛实验和消融研究中始终优于传统策略和固定情绪策略基线，实现了更高的成功率、效率和买家节省

Conclusion: 自适应情绪表达对于开发更有效的多轮谈判LLM代理至关重要，EvoEmo框架为此提供了有效解决方案

Abstract: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models
(LLMs) has demonstrated that agents can engage in \textit{complex},
\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,
existing LLM agents largely overlook the functional role of emotions in such
negotiations, instead generating passive, preference-driven emotional responses
that make them vulnerable to manipulation and strategic exploitation by
adversarial counterparts. To address this gap, we present EvoEmo, an
evolutionary reinforcement learning framework that optimizes dynamic emotional
expression in negotiations. EvoEmo models emotional state transitions as a
Markov Decision Process and employs population-based genetic optimization to
evolve high-reward emotion policies across diverse negotiation scenarios. We
further propose an evaluation framework with two baselines -- vanilla
strategies and fixed-emotion strategies -- for benchmarking emotion-aware
negotiation. Extensive experiments and ablation studies show that EvoEmo
consistently outperforms both baselines, achieving higher success rates, higher
efficiency, and increased buyer savings. This findings highlight the importance
of adaptive emotional expression in enabling more effective LLM agents for
multi-turn negotiation.

</details>


### [43] [Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes](https://arxiv.org/abs/2509.04317)
*Isidoro Tamassia,Wendelin Böhmer*

Main category: cs.AI

TL;DR: 本文分析了AlphaZero在测试环境可能变化时的部署问题，提出了简单修改框架的方法来显著提升性能，即使在低规划预算下也有效


<details>
  <summary>Details</summary>
Motivation: AlphaZero通常假设训练和测试环境不变，这限制了其适用性。本文旨在解决在测试环境可能变化时如何有效部署AlphaZero代理的问题

Method: 通过对标准AlphaZero框架进行简单修改的组合，包括调整蒙特卡洛规划和策略价值神经网络的使用方式

Result: 所提出的方法显著提升了性能表现，特别是在低规划预算的设置下也能取得良好效果

Conclusion: 通过简单的框架修改可以有效解决AlphaZero在变化测试环境中的部署问题，代码已在GitHub上公开

Abstract: The AlphaZero framework provides a standard way of combining Monte Carlo
planning with prior knowledge provided by a previously trained policy-value
neural network. AlphaZero usually assumes that the environment on which the
neural network was trained will not change at test time, which constrains its
applicability. In this paper, we analyze the problem of deploying AlphaZero
agents in potentially changed test environments and demonstrate how the
combination of simple modifications to the standard framework can significantly
boost performance, even in settings with a low planning budget available. The
code is publicly available on GitHub.

</details>


### [44] [Psychologically Enhanced AI Agents](https://arxiv.org/abs/2509.04343)
*Maciej Besta,Shriram Chandran,Robert Gerstenberger,Mathis Lindner,Marcin Chrapek,Sebastian Hermann Martschat,Taraneh Ghandi,Patrick Iff,Hubert Niewiadomski,Piotr Nyczyk,Jürgen Müller,Torsten Hoefler*

Main category: cs.AI

TL;DR: 基于MBTI人格类型的提示工程框架，通过心理学基础的人格条件化提升LLM机器人效果，无需微调即可实现一致的行为偏向。


<details>
  <summary>Details</summary>
Motivation: 将心理学理论与LLM行为设计相结合，通过结构化的人格条件化提升AI机器人的效果和可解释性。

Method: 使用MBTI人格模型通过提示工程对机器人进行人格前置处理，支持多机器通信协议实验，并集成16Personalities测试进行特质持久性验证。

Result: 情感表达型机器人在故事生成任务中表现优异，分析型机器人在游戏理论环境中采用更稳定策略，自我反思能提升合作和思维质量。

Conclusion: 该框架为无需微调的心理学增强AI机器人奠定了基础，并能够平滑扩展到其他心理学框架如Big Five、HEXACO等。

Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of
Large Language Model (LLM) agents through psychologically grounded personality
conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method
primes agents with distinct personality archetypes via prompt engineering,
enabling control over behavior along two foundational axes of human psychology,
cognition and affect. We show that such personality priming yields consistent,
interpretable behavioral biases across diverse tasks: emotionally expressive
agents excel in narrative generation, while analytically primed agents adopt
more stable strategies in game-theoretic settings. Our framework supports
experimenting with structured multi-agent communication protocols and reveals
that self-reflection prior to interaction improves cooperation and reasoning
quality. To ensure trait persistence, we integrate the official 16Personalities
test for automated verification. While our focus is on MBTI, we show that our
approach generalizes seamlessly to other psychological frameworks such as Big
Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior
design, we establish a foundation for psychologically enhanced AI agents
without any fine-tuning.

</details>


### [45] [ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory](https://arxiv.org/abs/2509.04439)
*Matthew Ho,Chen Si,Zhaoxiang Feng,Fangxu Yu,Zhijian Liu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: 提出概念级记忆方法，从推理轨迹中提取可重用的抽象概念，实现无需权重更新的测试时持续学习，在ARC-AGI基准上相对基线提升7.5%


<details>
  <summary>Details</summary>
Motivation: 现有推理时扩展方法产生的模式和见解在上下文重置后立即丢弃，外部记忆可以持久化这些发现，但需要超越基于实例的记忆条目，实现更广泛可重用和可扩展的概念级记忆

Method: 从解决方案轨迹中提取自然语言的可重用模块化抽象概念，为新查询选择性检索和集成相关概念到提示中，引入新的抽象和检索策略促进重用和记忆扩展

Result: 在ARC-AGI基准上相对强无记忆基线获得7.5%相对增益，性能随推理计算持续扩展，抽象概念在所有测试推理计算规模上都优于基线

Conclusion: 动态更新测试时记忆优于固定记忆设置，解决更多问题并将更多模式抽象到记忆中可以实现自我改进形式的进一步解决方案

Abstract: While inference-time scaling enables LLMs to carry out increasingly long and
capable reasoning traces, the patterns and insights uncovered during these
traces are immediately discarded once the context window is reset for a new
query. External memory is a natural way to persist these discoveries, and
recent work has shown clear benefits for reasoning-intensive tasks. We see an
opportunity to make such memories more broadly reusable and scalable by moving
beyond instance-based memory entries (e.g. exact query/response pairs, or
summaries tightly coupled with the original problem context) toward
concept-level memory: reusable, modular abstractions distilled from solution
traces and stored in natural language. For future queries, relevant concepts
are selectively retrieved and integrated into the prompt, enabling test-time
continual learning without weight updates. Our design introduces new strategies
for abstracting takeaways from rollouts and retrieving entries for new queries,
promoting reuse and allowing memory to expand with additional experiences. On
the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over
a strong no-memory baseline with performance continuing to scale with inference
compute. We find abstract concepts to be the most consistent memory design,
outscoring the baseline at all tested inference compute scales. Moreover, we
confirm that dynamically updating memory during test-time outperforms an
otherwise identical fixed memory setting with additional attempts, supporting
the hypothesis that solving more problems and abstracting more patterns to
memory enables further solutions in a form of self-improvement. Code available
at https://github.com/matt-seb-ho/arc_memo.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [46] [Distributed MIMO With Over-the-Air Phase Calibration Integrated Into the TDD Flow](https://arxiv.org/abs/2509.03722)
*Khac-Hoang Ngo,Erik G. Larsson*

Main category: cs.IT

TL;DR: 本文提出了一种在分布式MIMO系统中通过调整TDD时隙结构来集成AP间空中相位校准测量的方法，分析了校准资源开销与系统频谱效率之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 分布式MIMO系统中多个接入点需要周期性相位校准才能实现联合相干波束成形，传统方法需要专门的校准时间，影响系统效率。

Method: 通过调整TDD时隙结构的上行/下行切换点，创建短时间段供AP之间进行空中测量，实现相位校准与TDD流程的集成。

Result: 该方法可扩展到大型网络，分析表明在共轭波束成形和迫零波束成形下，校准资源开销与频谱效率之间存在明确的权衡关系。

Conclusion: 通过空中AP间测量集成到TDD流程中的分布式MIMO相位校准是可行的，为实际系统部署提供了有效解决方案。

Abstract: Reciprocity-based, joint coherent downlink beamforming from multiple access
points (APs) in distributed multiple-input multiple-output (MIMO) with
independent local oscillators (LOs) requires the APs to be periodically
phase-calibrated (a.k.a. phase-synchronized or phase-aligned). Such phase
alignment can be accomplished by bidirectional over-the-air measurements
between the APs. In this paper, we show how such over-the-air measurements can
be integrated into the time-division duplexing (TDD) flow by appropriately
shifting the uplink/downlink switching points of the TDD slot structure,
creating short time segments during which APs can measure on one another. We
also show how this technique scales to large networks. Furthermore, we
analytically characterize the tradeoff between the amount of resources spent on
calibration measurements and the resulting spectral efficiency of the system,
when conjugate beamforming or zero-forcing beamforming is used. The results
demonstrate the feasibility of distributed MIMO with phase-calibration through
over-the-air inter-AP measurements integrated into the TDD flow.

</details>


### [47] [Two-Timescale Sum-Rate Maximization for Movable Antenna Enhanced Systems](https://arxiv.org/abs/2509.04062)
*Xintai Chen,Biqian Feng,Yongpeng Wu,Derrick Wing Kwan Ng,Robert Schober*

Main category: cs.IT

TL;DR: 本文提出了一种基于可移动天线(MA)的多用户MIMO下行系统，采用两时间尺度优化方法，通过梯度上升算法和CSSCA算法最大化平均可实现和速率，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高无线通信性能，利用可移动天线的空间自由度来优化信道条件，解决传统固定天线系统在信道衰落环境下的性能限制问题。

Method: 采用两时间尺度优化：短期问题使用梯度上升算法优化接收天线位置向量；长期问题使用CSSCA算法优化发射天线位置和协方差矩阵。还提出了平面移动模式和PDD-SSCA低复杂度算法。

Result: 数值结果表明，相比基准方案，所提出的两时间尺度MA增强系统设计显著提高了平均可实现和速率和问题可行性，在通用和平面移动模式下都表现出优越性能。

Conclusion: 可移动天线技术与两时间尺度优化相结合，为多用户MIMO系统提供了有效的性能提升方案，特别是在利用空间自由度优化信道条件方面具有显著优势。

Abstract: This paper studies a novel movable antenna (MA)-enhanced multiuser
multiple-input multiple-output downlink system designed to improve wireless
communication performance. We aim to maximize the average achievable sum rate
through two-timescale optimization exploiting instantaneous channel state
information at the receiver (I-CSIR) for receive antenna position vector (APV)
design and statistical channel state information at the transmitter (S-CSIT)
for transmit APV and covariance matrix design. We first decompose the resulting
stochastic optimization problem into a series of short-term problems and one
long-term problem. Then, a gradient ascent algorithm is proposed to obtain
suboptimal receive APVs for the short-term problems for given I-CSIR samples.
Based on the output of the gradient ascent algorithm, a series of convex
objective/feasibility surrogates for the long-term problem are constructed and
solved utilizing the constrained stochastic successive convex approximation
(CSSCA) algorithm. Furthermore, we propose a planar movement mode for the
receive MAs to facilitate efficient antenna movement and the development of a
low-complexity primal-dual decomposition-based stochastic successive convex
approximation (PDD-SSCA) algorithm, which finds Karush-Kuhn-Tucker (KKT)
solutions almost surely. Our numerical results reveal that, for both the
general and the planar movement modes, the proposed two-timescale MA-enhanced
system design significantly improves the average achievable sum rate and the
feasibility of the formulated problem compared to benchmark schemes.

</details>


### [48] [Design of RIS-UAV-Assisted LEO Satellite Constellation Communication](https://arxiv.org/abs/2509.04136)
*Wenfei Yao,Xiaoming Chen,Qi Wang,Xingyu Peng*

Main category: cs.IT

TL;DR: 提出了一种基于RIS-UAV的LEO卫星通信框架，利用统计CSI减少开销，通过交替优化算法联合优化波束成形、RIS相位和无人机轨迹，提高多用户通信质量。


<details>
  <summary>Details</summary>
Motivation: 解决LEO卫星与地面长距离通信质量问题，减少多卫星协作时的CSI获取开销，提高频谱效率。

Method: 部署RIS-mounted无人机，利用统计CSI，推导用户遍历速率近似表达式，采用交替优化算法联合优化卫星波束成形、RIS相位偏移和无人机轨迹。

Result: 通过大量仿真验证，所提算法在频谱效率方面优于基线算法。

Conclusion: 提出的RIS-UAV框架和交替优化算法能有效提升LEO卫星通信系统的性能，特别是在长距离通信场景下。

Abstract: Low Earth orbit (LEO) satellite constellations play a pivotal role in
sixth-generation (6G) wireless networks by providing global coverage, massive
connections, and huge capacity. In this paper, we present a novel LEO satellite
constellation communication framework, where a reconfigurable intelligent
surface-mounted unmanned aerial vehicle (RIS-UAV) is deployed to improve the
communication quality of multiple terrestrial user equipments (UEs) under the
condition of long distance between satellite and ground. To reduce the overhead
for channel state information (CSI) acquisition with multiple-satellite
collaboration, statistical CSI (sCSI) is utilized in the system. In such a
situation, we first derive an approximated but exact expression for ergodic
rate of each UE. Then, we aim to maximize the minimum approximated UE ergodic
rate by the proposed alternating optimization (AO)-based algorithm that jointly
optimizes LEO satellite beamforming, RIS phase shift, and UAV trajectory.
Finally, extensive simulations are conducted to demonstrate the superiority of
the proposed algorithm in terms of spectrum efficiency over baseline
algorithms.

</details>


### [49] [Non-Reed-Solomon Type MDS Codes from Elliptic Curves](https://arxiv.org/abs/2509.04247)
*Puyin Wang,Wei Liu,Jinquan Luo,Dengxin Zhai*

Main category: cs.IT

TL;DR: 提出了基于椭圆曲线的新MDS码族，长度接近理论最大值，与Reed-Solomon码不等价，支持更一般的除子选择。


<details>
  <summary>Details</summary>
Motivation: 构建长度接近理论最大值的MDS码，突破传统依赖无穷远点的限制，探索更一般的除子结构。

Method: 利用椭圆曲线，考虑仿射点支撑的除子和多个不同点组成的除子，通过计算生成矩阵的Schur积秩来证明不等价性。

Result: 构造了长度约为(q + 1 + ⌊2√q⌋)/2的MDS码，证明了与RS码的不等价性，展示了已知上界的紧性。

Conclusion: 新MDS码族扩展了椭圆曲线码的构造框架，提供了更长的码长选择，并验证了与经典RS码的结构差异。

Abstract: In this paper, we present a new family of MDS codes derived from elliptic
curves. These codes attain lengths close to the theoretical maximum and are
provably inequivalent to Reed-Solomon (RS) codes. Unlike many previous
constructions that rely on the point at infinity, our approach allows for more
general choices: we consider divisors supported on affine points and divisors
consisting of multiple distinct points. This broader framework enables the
construction of codes with length approximately $(q + 1 + \lfloor 2\sqrt{q}
\rfloor)/2$, further illustrating the tightness of known upper bounds on
elliptic MDS code lengths. A detailed comparison shows that our codes are not
covered by earlier results. Moreover, we show that their inequivalence to RS
codes by explicitly computing the rank of the Schur product of their generator
matrices.

</details>
