<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.IT](#cs.IT) [Total: 3]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [MuMeNet: A Network Simulator for Musical Metaverse Communications](https://arxiv.org/abs/2512.05201)
*Ali Al Housseini,Jaime Llorca,Luca Turchet,Tiziano Leidi,Cristina Rottondi,Omran Ayoub*

Main category: cs.NI

TL;DR: 该论文针对音乐元宇宙的网络服务供给问题，提出了MuMeNet仿真器，用于模拟和分析5G/6G网络中音乐元宇宙会话的服务供给策略。


<details>
  <summary>Details</summary>
Motivation: 音乐元宇宙作为元宇宙的重要应用场景，其增长受到底层网络和服务基础设施要求的限制。现有模型无法充分捕捉音乐元宇宙会话的交互性、异构性和组播导向特性，需要专门的服务供给策略。

Method: 首先形式化音乐元宇宙的服务和网络图模型，以虚拟音乐会中的实时观众互动为参考场景。然后开发了MuMeNet——一个专门针对音乐元宇宙需求和流量动态的离散事件网络仿真器。最后通过线性规划编排策略在参考场景上展示其有效性。

Result: 成功开发了MuMeNet仿真器，能够在现实的音乐元宇宙工作负载下进行性能分析，验证了基于线性规划的编排策略在音乐元宇宙服务供给中的有效性。

Conclusion: 该研究为音乐元宇宙的服务供给问题提供了首个形式化建模和分析框架，MuMeNet仿真器能够帮助设计和优化5G/6G网络中音乐元宇宙会话的服务供给策略。

Abstract: The Metaverse, a shared and spatially organized digital continuum, is transforming various industries, with music emerging as a leading use case. Live concerts, collaborative composition, and interactive experiences are driving the Musical Metaverse (MM), but the requirements of the underlying network and service infrastructures hinder its growth. These challenges underscore the need for a novel modeling and simulation paradigm tailored to the unique characteristics of MM sessions, along with specialized service provisioning strategies capable of capturing their interactive, heterogeneous, and multicast-oriented nature. To this end, we make a first attempt to formally model and analyze the problem of service provisioning for MM sessions in 5G/6G networks. We first formalize service and network graph models for the MM, using "live audience interaction in a virtual concert" as a reference scenario. We then present MuMeNet, a novel discrete-event network simulator specifically tailored to the requirements and the traffic dynamics of the MM. We showcase the effectiveness of MuMeNet by running a linear programming based orchestration policy on the reference scenario and providing performance analysis under realistic MM workloads.

</details>


### [2] [Hierarchical Reinforcement Learning for the Dynamic VNE with Alternatives Problem](https://arxiv.org/abs/2512.05207)
*Ali Al Housseini,Cristina Rottondi,Omran Ayoub*

Main category: cs.NI

TL;DR: 本文提出HRL-VNEAP，一种用于动态到达的虚拟网络嵌入替代拓扑的分层强化学习方法，通过高层策略选择拓扑、低层策略进行嵌入，显著提升了接受率、总收益和收益成本比。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟网络嵌入假设每个虚拟网络请求具有固定拓扑，而实际中可能存在多种功能等效但资源需求不同的替代拓扑。虽然这种灵活性扩大了可行空间，但也增加了动态嵌入的决策复杂度，需要新的解决方案。

Method: 提出分层强化学习框架HRL-VNEAP：高层策略负责选择最合适的替代拓扑（或拒绝请求），低层策略负责将选定拓扑嵌入到底层物理网络中。该方法专门针对动态到达的虚拟网络请求场景设计。

Result: 在真实底层拓扑和多种流量负载下的实验表明，HRL-VNEAP在所有指标上均表现最佳：相比最强基线，接受率提升高达20.7%，总收益提升高达36.2%，收益成本比提升高达22.1%。

Conclusion: HRL-VNEAP能有效处理具有替代拓扑的虚拟网络嵌入问题，显著优于现有方法。通过与MILP公式在可处理实例上的对比，量化了与最优解的差距，为未来基于学习和优化的VNEAP解决方案提供了方向。

Abstract: Virtual Network Embedding (VNE) is a key enabler of network slicing, yet most formulations assume that each Virtual Network Request (VNR) has a fixed topology. Recently, VNE with Alternative topologies (VNEAP) was introduced to capture malleable VNRs, where each request can be instantiated using one of several functionally equivalent topologies that trade resources differently. While this flexibility enlarges the feasible space, it also introduces an additional decision layer, making dynamic embedding more challenging. This paper proposes HRL-VNEAP, a hierarchical reinforcement learning approach for VNEAP under dynamic arrivals. A high-level policy selects the most suitable alternative topology (or rejects the request), and a low-level policy embeds the chosen topology onto the substrate network. Experiments on realistic substrate topologies under multiple traffic loads show that naive exploitation strategies provide only modest gains, whereas HRL-VNEAP consistently achieves the best performance across all metrics. Compared to the strongest tested baselines, HRL-VNEAP improves acceptance ratio by up to \textbf{20.7\%}, total revenue by up to \textbf{36.2\%}, and revenue-over-cost by up to \textbf{22.1\%}. Finally, we benchmark against an MILP formulation on tractable instances to quantify the remaining gap to optimality and motivate future work on learning- and optimization-based VNEAP solutions.

</details>


### [3] [AIORA: An AI-Native Multi-Stakeholder Orchestration Architecture for 6G Continuum](https://arxiv.org/abs/2512.05744)
*Nuria Molner,Luis Rosa,Fulvio Risso,Konstantinos Samdanis,David Artuñedo,Rob Smets,Tarik Taleb,David Gomez-Barquero*

Main category: cs.NI

TL;DR: AIORA架构：面向6G系统的AI原生架构，通过开放API和智能编排实现边缘-云连续体的零接触管理


<details>
  <summary>Details</summary>
Motivation: 6G系统需要高灵活性、可用性、效率、可靠性和弹性的服务与应用，现有边缘计算平台、运营商平台和API标准化等产业倡议需要更集成的框架来无缝编排边缘、云和网络资源

Method: 提出AIORA架构，基于3GPP边缘使能层和连接模型，采用多段虚拟连续体概念和嵌套AI驱动闭环，集成新工具和先进技术实现零接触管理

Result: AIORA架构不仅与ETSI MEC、GSMA运营商平台、CAMARA等产业倡议保持一致，还通过多段虚拟连续体和实时优化的AI驱动闭环扩展了这些倡议

Conclusion: AIORA架构为6G系统提供了一种创新的AI原生解决方案，能够满足未来6G服务和应用对资源编排的复杂需求，代表了边缘-云连续体管理的发展方向

Abstract: This paper elaborates on a novel AI-native architecture for emerging 6G systems harnessing open APIs, along with supporting mechanisms to empower intelligent and coordinated orchestration of edge-cloud continuum resources. The AIORA architecture facilitates a seamless creation, life-cycle management, and exposure of services in multi-segment heterogeneous environments. It integrates new breeds of tools and advanced technologies to enable zero-touch management of an edge-cloud continuum, building on top of the 3GPP Edge Enablement Layer and the respective connectivity models, allowing to cater to the high flexibility, availability, efficiency, reliability, and resilience needs of the future 6G services and applications. Several ongoing industry initiatives -- such as ETSI MEC for edge computing platforms, the GSMA Operator Platform for multi-operator service federation, and CAMARA for cross-operator API standardization -- demonstrate the growing momentum towards integrated frameworks where edge, cloud, and network resources can be seamlessly orchestrated. Our proposed AIORA architecture not only aligns with these initiatives but also extends them by leveraging a multi-segment virtual continuum concept and nested AI-driven closed loops for real-time optimization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [4] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: 本文提出两种基于信息论和热力学的无监督指标（SF和SEP）来评估LLM的任务忠实度，将LLM视为二分信息引擎，通过主题转换矩阵的KL散度量化忠实度。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型对给定任务的忠实度是一个复杂挑战，需要新的无监督评估方法来量化模型输出与任务要求的匹配程度。

Method: 将LLM视为二分信息引擎，隐藏层作为麦克斯韦妖控制上下文到答案的转换。将QCA三元组建模为共享主题的概率分布，通过主题转换矩阵Q和A分别编码查询目标和实际结果。SF指标通过这两个矩阵的KL散度量化忠实度，通过凸优化同时推断两个矩阵。SEP指标基于热力学概念计算答案生成中的语义熵产生。

Result: 提出的SF和SEP指标能够有效评估LLM忠实度，高忠实度通常对应低熵产生。在SEC 10-K文件摘要任务中验证了框架的有效性。

Conclusion: SF和SEP指标为LLM忠实度评估和幻觉控制提供了新的无监督方法，可单独或联合使用，基于信息论和热力学原理为LLM评估提供了理论框架。

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [5] [Documenting SME Processes with Conversational AI: From Tacit Knowledge to BPMN](https://arxiv.org/abs/2512.05122)
*Unnikrishnan Radhakrishnan*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM的对话助手，帮助中小企业将隐性经验知识转化为标准BPMN 2.0流程图，降低流程文档化的技能和成本门槛。


<details>
  <summary>Details</summary>
Motivation: 中小企业依赖隐性经验知识，但这些知识很少被正式记录。传统流程建模需要专业技能，对中小企业来说成本高、门槛大，导致机构知识流失和运营透明度不足。

Method: 使用Gemini 2.5 Pro驱动的对话助手，通过轻量级Gradio前端和客户端bpmn-js可视化工具，进行访谈式对话：引导流程细节、支持澄清对话和按需分析，实时渲染和优化流程图。

Result: 在设备维护场景的概念验证中，聊天机器人在约12分钟内生成了准确的"现状"模型，通过图上标注标记问题，并生成了改进的"未来"变体，同时将API成本控制在中小企业友好预算内。

Conclusion: 对话式LLM能够降低严格流程文档化的技能和成本障碍，帮助中小企业保存机构知识、增强运营透明度并加速持续改进工作，未来可向代理和多模态部署发展。

Abstract: Small and medium-sized enterprises (SMEs) still depend heavily on tacit, experience-based know-how that rarely makes its way into formal documentation. This paper introduces a large-language-model (LLM)-driven conversational assistant that captures such knowledge on the shop floor and converts it incrementally and interactively into standards-compliant Business Process Model and Notation (BPMN) 2.0 diagrams. Powered by Gemini 2.5 Pro and delivered through a lightweight Gradio front-end with client-side bpmn-js visualisation, the assistant conducts an interview-style dialogue: it elicits process details, supports clarifying dialogue and on-demand analysis, and renders live diagrams that users can refine in real time. A proof-of-concept evaluation in an equipment-maintenance scenario shows that the chatbot produced an accurate "AS-IS" model, flagged issues via on-diagram annotations, and generated an improved "TO-BE" variant, all within about 12-minutes, while keeping API costs within an SME-friendly budget. The study analyses latency sources, model-selection trade-offs, and the challenges of enforcing strict XML schemas, then outlines a roadmap toward agentic and multimodal deployments. The results demonstrate that conversational LLMs can potentially be used to lower the skill and cost barriers to rigorous process documentation, helping SMEs preserve institutional knowledge, enhance operational transparency, and accelerate continuous-improvement efforts.

</details>


### [6] [Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education](https://arxiv.org/abs/2512.05167)
*Fang Li*

Main category: cs.AI

TL;DR: 该论文提出了一种创新的AI与数据科学教学方法，系统性地将传统机器学习技术与现代大语言模型相结合，通过两部分课程设计帮助学生全面理解AI发展并掌握实用技能。


<details>
  <summary>Details</summary>
Motivation: 为了帮助学生全面理解人工智能的发展历程，同时掌握传统机器学习和现代大语言模型技术，更好地适应快速发展的AI行业需求。

Method: 采用两部分课程设计：第一部分教授基础机器学习概念，第二部分专注于当代大语言模型应用。课程包括架构设计、实施策略、评估方法，在为期两个七周学期的夏季课程中实施。

Result: 这种整合教学方法增强了学生对AI领域的理解，更好地为他们应对行业需求做好准备，证明了该方法的有效性。

Conclusion: 将传统机器学习与现代LLM系统结合的课程设计是有效的，能够帮助学生全面理解AI发展并掌握实用技能，适应快速变化的AI行业需求。

Abstract: This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.

</details>


### [7] [On the Computability of Artificial General Intelligence](https://arxiv.org/abs/2512.05212)
*Georgios Mappouras,Charalambos Rossides*

Main category: cs.AI

TL;DR: 论文证明任何算法（包括AI模型）都无法产生真正创新的功能能力，只能展示已有功能或其组合，因此无法实现真正的人工通用智能（AGI）。


<details>
  <summary>Details</summary>
Motivation: 随着AI快速发展，人们关心何时能实现达到人类智能水平的AGI。本文旨在从计算理论上界定任何机器可计算过程（算法）的能力上限，探讨AGI的可能性边界。

Method: 采用先前研究中关于AGI的定义（在某个领域展现创造性和创新能力，解锁该领域新的、未知的功能能力）。基于此定义，通过形式化证明来界定计算的极限。

Result: 形式化证明表明：任何算法都无法展示初始算法本身不具备的新功能能力。因此，没有算法（包括AI模型）能在任何领域（科学、工程、艺术、体育等）实现真正的创造性。

Conclusion: AI模型只能展示现有功能能力及其组合排列，无法实现真正的创新。这一证明对AI发展的未来和人类智能的起源都具有重要启示意义。

Abstract: In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.

</details>


### [8] [Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence](https://arxiv.org/abs/2512.05257)
*Bychkov Oleksii,Bychkova Sophia,Lytvynchuk Khrystyna*

Main category: cs.AI

TL;DR: 本文论证可能性理论是解决Dempster-Shafer理论悖论的根本方案，通过可能性与必要性测度的二元框架，为不确定性处理提供逻辑一致的基础。


<details>
  <summary>Details</summary>
Motivation: 解决Dempster-Shafer理论（DST）的危机和悖论问题。现有许多尝试都集中在修正Dempster规则上，但本文认为需要从根本上建立逻辑一致的不确定性处理基础。

Method: 采用Bychkov文章中发展的公理化方法，基于可能性与必要性测度的二元框架构建理论。通过比较概率、证据和可能性三种范式，并以经典医疗诊断困境为例进行演示。

Result: 可能性理论能够正确处理矛盾数据，避免DST的逻辑陷阱，使形式推理更接近自然智能的逻辑。它不是DST的替代方案，而是提供根本性解决方案。

Conclusion: 可能性理论为解决DST悖论提供了根本性的解决方案，通过公理化方法建立了逻辑一致的不确定性处理框架，比单纯修正Dempster规则更基础、更有效。

Abstract: This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.

</details>


### [9] [AI & Human Co-Improvement for Safer Co-Superintelligence](https://arxiv.org/abs/2512.05356)
*Jason Weston,Jakob Foerster*

Main category: cs.AI

TL;DR: 论文主张将AI研究目标从自我改进转向协同改进，即人类研究者与AI系统合作实现共同超智能


<details>
  <summary>Details</summary>
Motivation: 当前AI领域的自我改进目标充满危险且难以完全实现，需要更可行、更安全的人类-AI协作路径

Method: 倡导建立人类研究者与AI系统的协同研究循环，从构思到实验全过程合作，提升AI与人类协作研究能力

Result: 提出协同改进作为更优目标，既能加速AI研究进展，又能通过人机共生实现更安全的超智能

Conclusion: 将人类研究改进纳入循环的协同改进路径，既能更快实现目标，又能确保更安全的发展方向

Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.

</details>


### [10] [MCP-AI: Protocol-Driven Intelligence Framework for Autonomous Reasoning in Healthcare](https://arxiv.org/abs/2512.05365)
*Zag ElSayed,Craig Erickson,Ernest Pedapati*

Main category: cs.AI

TL;DR: MCP-AI：基于模型上下文协议的新型医疗AI架构，支持可解释、可组合、安全导向的临床决策，实现长期状态管理和医生参与验证。


<details>
  <summary>Details</summary>
Motivation: 传统医疗AI系统难以整合上下文推理、长期状态管理和可验证工作流，而随着医疗系统日益复杂，迫切需要自主、上下文感知的临床推理框架。

Method: 提出MCP-AI架构，基于模型上下文协议（MCP），通过模块化可执行规范协调生成式和描述式AI代理。每个MCP文件包含临床目标、患者上下文、推理状态和任务逻辑，形成可重用、可审计的内存对象。

Result: 通过两个用例验证：1）脆性X综合征伴抑郁症的诊断建模；2）2型糖尿病和高血压的远程协调。系统支持医生参与验证，简化临床流程，保证AI责任在医疗提供者间的安全转移。

Conclusion: MCP-AI为即将到来的临床环境提供了可扩展、可解释、可组合且安全导向的AI基础，代表了从传统临床决策支持系统和基于提示的LLM的重大转变。

Abstract: Healthcare AI systems have historically faced challenges in merging contextual reasoning, long-term state management, and human-verifiable workflows into a cohesive framework. This paper introduces a completely innovative architecture and concept: combining the Model Context Protocol (MCP) with a specific clinical application, known as MCP-AI. This integration allows intelligent agents to reason over extended periods, collaborate securely, and adhere to authentic clinical logic, representing a significant shift away from traditional Clinical Decision Support Systems (CDSS) and prompt-based Large Language Models (LLMs). As healthcare systems become more complex, the need for autonomous, context-aware clinical reasoning frameworks has become urgent. We present MCP-AI, a novel architecture for explainable medical decision-making built upon the Model Context Protocol (MCP) a modular, executable specification for orchestrating generative and descriptive AI agents in real-time workflows. Each MCP file captures clinical objectives, patient context, reasoning state, and task logic, forming a reusable and auditable memory object. Unlike conventional CDSS or stateless prompt-based AI systems, MCP-AI supports adaptive, longitudinal, and collaborative reasoning across care settings. MCP-AI is validated through two use cases: (1) diagnostic modeling of Fragile X Syndrome with comorbid depression, and (2) remote coordination for Type 2 Diabetes and hypertension. In either scenario, the protocol facilitates physician-in-the-loop validation, streamlines clinical processes, and guarantees secure transitions of AI responsibilities between healthcare providers. The system connects with HL7/FHIR interfaces and adheres to regulatory standards, such as HIPAA and FDA SaMD guidelines. MCP-AI provides a scalable basis for interpretable, composable, and safety-oriented AI within upcoming clinical environments.

</details>


### [11] [ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications](https://arxiv.org/abs/2512.05371)
*Changwen Xing,SamZaak Wong,Xinlai Wan,Yanfeng Lu,Mengli Zhang,Zebin Ma,Lei Qi,Zhengxiong Li,Nan Guan,Zhe Jiang,Xi Wang,Jun Yang*

Main category: cs.AI

TL;DR: ChipMind是一个基于知识图谱增强的推理框架，专门用于处理超长集成电路规格文档，通过构建电路知识图谱和自适应检索机制，显著提升LLM在硬件设计中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在集成电路开发自动化方面潜力巨大，但受限于有限的上下文窗口。现有上下文扩展方法难以对复杂、冗长的电路规格进行有效的语义建模和多跳推理，阻碍了LLM在硬件设计中的实际工业部署。

Method: 提出ChipMind框架：1）通过电路语义感知知识图谱构建方法将电路规格转换为领域特定知识图谱ChipKG；2）采用ChipKG增强推理机制，结合信息论自适应检索动态追踪逻辑依赖关系，以及意图感知语义过滤去除无关噪声，平衡检索完整性和精确度。

Result: 在工业级规格推理基准测试中，ChipMind显著优于现有最佳基线方法，平均提升34.59%，最高提升达72.73%。

Conclusion: ChipMind填补了LLM辅助硬件设计在学术研究和实际工业部署之间的关键空白，为集成电路开发自动化提供了有效的解决方案。

Abstract: While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).

</details>


### [12] [BEAVER: An Efficient Deterministic LLM Verifier](https://arxiv.org/abs/2512.05439)
*Tarun Suresh,Nalin Wadhwa,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.AI

TL;DR: BEAVER是首个为LLM约束满足提供确定性、可靠概率边界的实用框架，相比基线方法能获得6-8倍更紧的概率边界，识别出3-4倍更多高风险实例。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从研究原型转向生产系统，从业者需要可靠方法来验证模型输出是否满足所需约束。基于采样的估计方法虽然能提供模型行为的直觉，但无法提供可靠保证。

Method: BEAVER使用新颖的token trie和frontier数据结构系统性地探索生成空间，对任何前缀封闭的语义约束都能在每次迭代中保持可证明可靠的概率边界。

Result: 在正确性验证、隐私验证和安全代码生成任务中，BEAVER在相同计算预算下实现了6-8倍更紧的概率边界，识别出3-4倍更多高风险实例。

Conclusion: BEAVER能够提供精确的特征描述和风险评估，这是松散边界或经验评估无法提供的，为LLM约束验证提供了首个实用的确定性概率边界计算框架。

Abstract: As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.

</details>


### [13] [The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems](https://arxiv.org/abs/2512.05449)
*Robert Yang*

Main category: cs.AI

TL;DR: 论文提出用"意志薄弱"(akrasia)概念分析AI智能体不一致性，开发Akrasia基准测试衡量模型自我控制能力，连接哲学理论与AI智能体行为研究


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在一种特殊的不一致性：它们"知道"正确答案但未能据此行动。这种全局判断与局部冲动之间的张力在人类哲学中被称为"意志薄弱"。作者认为这一概念可作为分析AI智能体系统不一致性和目标漂移的基础框架

Method: 提出Akrasia基准测试的初步版本，包含四种结构化提示条件：基线(B)、同义词(S)、时间(T)和诱惑(X)，用于测量模型局部响应与其先前承诺相矛盾的情况。该基准支持跨模型家族、解码策略和诱惑类型的"自我控制"量化比较

Result: 基准测试能够定量比较不同模型在自我控制方面的表现。此外，作者指出微观层面的意志薄弱可能在多智能体系统中累积为宏观层面的不稳定性，这可能被解释为"阴谋"或故意不对齐行为

Conclusion: 通过将不一致性重新定义为意志薄弱，这项工作将智能体行为与经典能动性理论联系起来，为哲学、心理学和新兴的AI智能体科学之间建立了实证桥梁。Akrasia概念为分析AI智能体系统提供了新的理论框架和评估工具

Abstract: Large language models display a peculiar form of inconsistency: they "know" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce a preliminary version of the Akrasia Benchmark, currently a structured set of prompting conditions (Baseline [B], Synonym [S], Temporal [T], and Temptation [X]) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of "self-control" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as "scheming" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.

</details>


### [14] [MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models](https://arxiv.org/abs/2512.05530)
*Chuang Yu,Jinmiao Zhao,Mingxuan Zhao,Yunpeng Liu,Xiujun Shu,Yuanhao Feng,Bo Wang,Xiangyu Yue*

Main category: cs.AI

TL;DR: MIND框架通过"理解->反思->纠正"的认知过程，将MLLMs从被动模仿推理转变为主动判别推理，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在推理任务中存在多理性语义建模有限、逻辑鲁棒性不足、易受复杂场景误导等问题，需要提升其认知能力。

Method: 提出MIND推理框架，包含：1)RAD范式自动生成多样理性扩展数据集；2)P2CL策略分两阶段进行多理性正向学习和主动逻辑判别纠正；3)MCA优化策略通过对比对齐缓解多理性语义空间表示纠缠。

Result: 在涵盖科学、常识和数学场景的多个公共数据集上实现了最先进的性能，为推进MLLMs向更高认知智能水平提供了新视角。

Conclusion: MIND框架通过赋予MLLMs类似人类的"理解->反思->纠正"认知能力，实现了从被动模仿推理到主动判别推理的范式演进，显著提升了多模态推理性能。

Abstract: Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -> Rethink -> Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND

</details>


### [15] [CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning](https://arxiv.org/abs/2512.05576)
*Ting-Ting Xie,Yixin Zhang*

Main category: cs.AI

TL;DR: 提出Executor-Analyst框架解决临床AI中的上下文利用失败问题，通过分离工具执行与临床推理，采用分层集成策略，无需训练即可达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于小语言模型的临床代理（如TxAgent）存在"上下文利用失败"问题：模型能成功检索生物医学证据，但无法基于这些信息进行诊断推理。

Method: 提出Executor-Analyst框架，将工具执行的语法精度与临床推理的语义鲁棒性解耦。通过专门的TxAgents（执行器）与长上下文基础模型（分析师）协同工作，采用分层集成策略保持证据多样性。

Result: 在CURE-Bench上达到最先进性能，无需昂贵的端到端微调。发现两个关键扩展见解：上下文-性能悖论（超过12k token会引入噪声）和动作空间的维度诅咒（工具集扩展需要分层检索策略）。

Conclusion: 通过免训练架构工程为下一代可信赖的AI驱动治疗提供了可扩展、敏捷的基础，展示了模块化架构在解决临床AI推理缺陷方面的潜力。

Abstract: Current clinical agent built on small LLMs, such as TxAgent suffer from a \textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.

</details>


### [16] [Ontology Learning with LLMs: A Benchmark Study on Axiom Identification](https://arxiv.org/abs/2512.05594)
*Roos M. Bakker,Daan L. Di Scala,Maaike H. T. de Boer,Stephan A. Raaijmakers*

Main category: cs.AI

TL;DR: 本文介绍了OntoAxiom基准测试，用于评估LLMs在识别本体公理方面的性能，发现Axiom-by-Axiom提示策略效果更好，但性能因公理类型和本体而异，LLMs可作为本体工程师的辅助工具。


<details>
  <summary>Details</summary>
Motivation: 本体开发需要大量建模和领域专业知识，自动化这一过程的本体学习在过去十年中随着NLP技术特别是LLMs的发展而进步。本文研究识别公理（定义类和属性之间逻辑关系的基本本体组件）的挑战。

Method: 引入Ontology Axiom Benchmark (OntoAxiom)，包含9个中等规模本体共17,118个三元组和2,771个公理。评估12个LLMs在三种shot设置和两种提示策略下的性能：Direct方法（一次性查询所有公理）和Axiom-by-Axiom方法（每个提示只查询一个公理）。

Result: Axiom-by-Axiom提示策略比Direct方法获得更高的F1分数。性能因公理类型而异，某些公理更难识别。领域也影响性能：FOAF本体在子类公理上得分为0.642，而音乐本体仅为0.218。大型LLMs优于小型模型，但小型模型在资源受限环境下仍可用。

Conclusion: 虽然整体性能不足以完全自动化公理识别，但LLMs可以提供有价值的候选公理，支持本体工程师开发和优化本体。

Abstract: Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.

</details>


### [17] [Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting](https://arxiv.org/abs/2512.05619)
*Menghua Jiang,Haokai Gao,Shuhao Chen,Yin Chen*

Main category: cs.AI

TL;DR: 提出DeepDist SLS求解器，针对PMS和WPMS问题设计新的子句权重方案，首次区分两种问题的权重更新条件，结合新的初始化方法和优先满足单元/硬子句的decimation方法，在MaxSAT评估中超越现有最佳SLS求解器。


<details>
  <summary>Details</summary>
Motivation: 现有随机局部搜索算法主要关注子句权重方案设计，但未能充分区分PMS和WPMS问题，通常采用统一的权重更新策略，忽视了两种问题类型的关键结构差异。

Method: 1) 提出新颖的子句权重方案，首次根据不同条件更新PMS和WPMS实例的子句权重；2) 引入新的初始化方法，更好适应两种实例类型的独特特征；3) 提出优先满足单元子句和硬子句的decimation方法；4) 基于这些方法开发DeepDist SLS求解器。

Result: 在最近MaxSAT评估的anytime tracks基准测试中，DeepDist优于最先进的SLS求解器。与TT-Open-WBO-Inc结合的混合求解器超越了MaxSAT评估2024的获胜者SPB-MaxSAT-c-Band和SPB-MaxSAT-c-FPS。

Conclusion: 提出的方法有效解决了PMS和WPMS问题的区分问题，DeepDist展示了优越性能，代码已开源，为(W)PMS求解提供了新的有效工具。

Abstract: Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist

</details>


### [18] [KANFormer for Predicting Fill Probabilities via Survival Analysis in Limit Order Books](https://arxiv.org/abs/2512.05734)
*Jinfeng Zhong,Emmanuel Bacry,Agathe Guilloux,Jean-François Muzy*

Main category: cs.AI

TL;DR: KANFormer：结合Dilated Causal CNN、Transformer和KANs的深度学习模型，用于预测限价单的成交时间，整合市场级和代理级信息，在预测准确性和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅依赖限价订单簿的快照序列，未能有效整合与LOB动态相关的代理行为以及订单在队列中的位置信息，限制了成交概率预测的准确性。

Method: 结合Dilated Causal Convolutional网络和Transformer编码器，并采用Kolmogorov-Arnold Networks（KANs）增强非线性逼近能力。模型整合市场级信息（LOB快照）和代理级信息（代理行为、订单队列位置）。

Result: 在CAC 40指数期货数据上评估，KANFormer在校准指标（右删失对数似然、集成Brier分数）和判别指标（C指数、时间依赖AUC）上均优于现有工作。通过SHAP分析特征重要性随时间的变化。

Conclusion: 结合丰富的市场信号和表达性强的神经架构能够实现准确且可解释的成交概率预测，KANFormer展示了这种整合方法的优势。

Abstract: This paper introduces KANFormer, a novel deep-learning-based model for predicting the time-to-fill of limit orders by leveraging both market- and agent-level information. KANFormer combines a Dilated Causal Convolutional network with a Transformer encoder, enhanced by Kolmogorov-Arnold Networks (KANs), which improve nonlinear approximation. Unlike existing models that rely solely on a series of snapshots of the limit order book, KANFormer integrates the actions of agents related to LOB dynamics and the position of the order in the queue to more effectively capture patterns related to execution likelihood. We evaluate the model using CAC 40 index futures data with labeled orders. The results show that KANFormer outperforms existing works in both calibration (Right-Censored Log-Likelihood, Integrated Brier Score) and discrimination (C-index, time-dependent AUC). We further analyze feature importance over time using SHAP (SHapley Additive exPlanations). Our results highlight the benefits of combining rich market signals with expressive neural architectures to achieve accurate and interpretabl predictions of fill probabilities.

</details>


### [19] [A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning](https://arxiv.org/abs/2512.05753)
*Wencheng Cai,Xuchao Gao,Congying Han,Mingqiang Li,Tiande Guo*

Main category: cs.AI

TL;DR: 提出FARDA框架，使用深度强化学习快速部署认知雷达对抗干扰，相比进化算法速度提升约7000倍，覆盖效果相当。


<details>
  <summary>Details</summary>
Motivation: 现代战争中快速部署认知雷达对抗干扰是关键挑战，现有进化算法方法耗时且易陷入局部最优，需要更高效的部署方案。

Method: 将雷达部署问题建模为端到端任务，设计深度强化学习算法，开发集成神经模块感知热图信息，并设计新的奖励格式。

Result: FARDA方法达到与进化算法相当的覆盖效果，同时部署速度提升约7000倍，消融实验证实各组件必要性。

Conclusion: FARDA框架通过深度强化学习有效解决了认知雷达快速部署问题，显著提升部署效率，为现代电子战提供实用解决方案。

Abstract: The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.

</details>


### [20] [Evolutionary System 2 Reasoning: An Empirical Proof](https://arxiv.org/abs/2512.05760)
*Zeyuan Ma,Wenqi Huang,Guo-Huan Song,Hongshu Guo,Sijie Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.AI

TL;DR: 提出进化推理优化（ERO）框架，通过进化算法增强大语言模型的系统2推理能力，发现GPT-5等最新模型推理能力仍有限，但通过简单进化循环可显著提升较弱模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在特定任务上表现出色，但在通用智能和系统2推理（慢思考）方面仍有不足。研究旨在探索机器智能（如LLMs）能否像人类一样进化获得推理能力，而不仅仅是特定技能。

Method: 提出进化推理优化（ERO）框架：1）初始化多个LLMs作为种群；2）采用进化策略优化种群，最大化最佳个体的量化推理分数；3）通过"适者生存"原则搜索具有强推理能力的个体。

Result: 两个重要发现：1）最新LLMs（如GPT-5）仍表现出有限的系统2推理能力；2）通过简单的ERO进化循环，相对较弱的模型（Qwen-7B）可被增强，涌现出强大的推理能力。

Conclusion: ERO框架能有效提升LLMs的推理能力，证明通过进化方法可以增强机器的系统2推理能力，为机器智能向人类智能水平进化提供了新途径。

Abstract: Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.

</details>


### [21] [The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics](https://arxiv.org/abs/2512.05765)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文反驳了"LLMs只是模式匹配器，无法实现推理"的批评，提出真正的瓶颈在于缺乏System-2协调层，而非LLMs本身。通过UCCT理论和MACI架构，展示了如何在LLMs基础上实现推理能力。


<details>
  <summary>Details</summary>
Motivation: 针对当前对大型语言模型(LLMs)的批评——认为LLMs只是"模式匹配器"，结构上无法进行推理或规划，论文旨在反驳这一观点，指出真正的瓶颈在于缺乏System-2协调层，而非LLMs本身。论文试图证明LLMs是实现AGI的正确路径。

Method: 提出了UCCT理论，将推理建模为语义锚定的相变过程，由有效支持度(rho_d)、表征不匹配(d_r)和自适应锚定预算(gamma log k)控制。基于此理论设计了MACI协调栈架构，包含诱饵机制（行为调制辩论）、过滤机制（苏格拉底式判断）和持久性机制（事务性记忆）。

Result: 论文将常见的反对意见重新解释为可测试的协调失败，展示了如何通过UCCT理论和MACI架构在LLMs基础上实现推理能力。论证了无基础的生成只是对底层最大似然先验的无诱饵检索，而"推理"则是在锚点将后验概率向目标导向约束转移时出现。

Conclusion: LLMs是实现AGI的必要System-1基础，真正的瓶颈在于缺乏System-2协调层。通过UCCT理论和MACI架构，可以在LLMs基础上实现推理能力，因此通往AGI的道路是通过LLMs而非绕过它们。

Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

</details>


### [22] [Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma](https://arxiv.org/abs/2512.05824)
*Hafsa Akebli,Adam Shephard,Vincenzo Della Mea,Nasir Rajpoot*

Main category: cs.AI

TL;DR: 提出一个多模态肿瘤智能体（MOA），结合TITAN基础模型的病理学工具和临床/基因组数据推理，用于低级别胶质瘤IDH1突变预测，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 低级别胶质瘤中的IDH1突变具有重要的临床意义，但现有预测方法有限。需要整合多模态信息（病理、临床、基因组）来提高预测准确性。

Method: 开发多模态肿瘤智能体（MOA），包含基于TITAN基础模型的病理学工具用于IDH1突变预测，同时整合PubMed、Google Search、OncoKB等外部生物医学资源进行临床和基因组数据推理。

Result: 在TCGA-LGG队列的488名患者上评估：MOA（无病理工具）F1分数0.826优于临床基线0.798；融合病理特征后MOA达到最高性能F1分数0.912，优于病理基线0.894和融合病理-临床基线0.897。

Conclusion: MOA通过整合外部生物医学资源捕获了互补的突变相关信息，能够准确预测IDH1突变，展示了多模态方法在肿瘤分子分型中的价值。

Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.

</details>


### [23] [Using Large Language Models to Create Personalized Networks From Therapy Sessions](https://arxiv.org/abs/2512.05836)
*Clarissa W. Ong,Hiba Arnaout,Kate Sheehan,Estella Fox,Eugen Owtscharow,Iryna Gurevych*

Main category: cs.AI

TL;DR: 利用LLMs从治疗记录自动生成客户心理网络，支持个性化治疗规划


<details>
  <summary>Details</summary>
Motivation: 个性化治疗需要基于客户心理网络，但传统方法需要密集纵向数据，难以规模化。LLMs为网络驱动的治疗个性化提供了可扩展的解决方案。

Method: 1) 标注77份治疗记录中的3364个心理过程及其维度；2) 使用上下文学习联合识别心理过程和维度；3) 两步法将过程聚类为临床有意义的组；4) 生成解释增强的聚类间关系

Result: 方法在少量训练样本下表现优异；专家评估显示多步方法优于直接提示，90%专家偏好该方法；网络在临床相关性、新颖性和有用性方面得分72-75%

Conclusion: 研究证明了LLMs从治疗记录创建临床相关网络的可行性，支持自下而上的案例概念化和潜在主题识别。未来需验证这些网络是否比统计估计网络更能改善治疗效果。

Abstract: Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.

</details>


### [24] [To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis](https://arxiv.org/abs/2512.05925)
*Federico Bianchi,Yongchan Kwon,Zachary Izzo,Linjun Zhang,James Zou*

Main category: cs.AI

TL;DR: 使用GPT-5开发的论文正确性检查器发现，顶级AI会议和期刊发表的论文中存在客观错误，且错误数量随时间增加，AI检查器能高精度识别错误并提供修正建议。


<details>
  <summary>Details</summary>
Motivation: 同行评审出版物是研究的基础，但其中的错误会在文献中传播，导致后续研究混乱和可重复性问题。研究加速和同行评审压力使得错误更难被发现和避免。

Method: 开发基于GPT-5的论文正确性检查器，系统识别已发表论文中的客观错误（公式、推导、计算、图表等可验证错误），排除主观评价，并由人类专家验证AI识别的错误。

Result: 发表论文包含不可忽视的客观错误，且平均错误数随时间增加（NeurIPS从2021年3.8个增至2025年5.9个）。AI检查器识别错误的精确度为83.2%，能为75.8%的错误提供正确修正。

Conclusion: 前沿大语言模型在检测和修正已发表论文客观错误方面具有潜力，有助于建立更坚实的知识基础，减少文献混乱并增强可重复性。

Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.

</details>


### [25] [PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation](https://arxiv.org/abs/2512.05930)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: PRiSM是一个用于评估视觉语言模型在科学领域（物理和数学）推理能力的动态多模态基准，包含24,750个大学级别问题，通过Python代码生成和验证，支持五种评估任务。


<details>
  <summary>Details</summary>
Motivation: 当前评估视觉语言模型在科学领域的基准存在局限性：缺乏中间推理步骤、对变化的鲁棒性不足、缺少验证科学正确性的机制。科学领域需要概念理解、符号推理和遵循形式法则，现有基准无法满足这些需求。

Method: 开发PRiSM基准，包含24,750个大学级别物理和数学问题。使用PrismAgent代理管道生成结构化问题实例，每个问题包含动态文本和视觉输入、生成图像、可执行Python代码（用于生成和验证真实值）以及详细的分步推理。提出五种评估任务：泛化能力、符号程序合成、扰动鲁棒性、推理修正和歧义解析。

Result: 通过PRiSM对现有视觉语言模型进行全面评估，揭示了它们在科学推理方面的局限性，包括失败模式、不确定性行为和科学推理能力的不足。基准的动态特性和Python驱动的自动真实值生成支持细粒度实验审计。

Conclusion: PRiSM基准能够深入洞察视觉语言模型的科学推理能力，解决了现有基准在科学领域评估中的不足，为评估模型在数学和物理等科学领域的表现提供了更全面、动态和可验证的框架。

Abstract: Evaluating vision-language models (VLMs) in scientific domains like mathematics and physics poses unique challenges that go far beyond predicting final answers. These domains demand conceptual understanding, symbolic reasoning, and adherence to formal laws, requirements that most existing benchmarks fail to address. In particular, current datasets tend to be static, lacking intermediate reasoning steps, robustness to variations, or mechanisms for verifying scientific correctness. To address these limitations, we introduce PRiSM, a synthetic, fully dynamic, and multimodal benchmark for evaluating scientific reasoning via grounded Python code. PRiSM includes over 24,750 university-level physics and math problems, and it leverages our scalable agent-based pipeline, PrismAgent, to generate well-structured problem instances. Each problem contains dynamic textual and visual input, a generated figure, alongside rich structured outputs: executable Python code for ground truth generation and verification, and detailed step-by-step reasoning. The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing of multimodal VLMs, revealing failure modes, uncertainty behaviors, and limitations in scientific reasoning. To this end, we propose five targeted evaluation tasks covering generalization, symbolic program synthesis, perturbation robustness, reasoning correction, and ambiguity resolution. Through comprehensive evaluation of existing VLMs, we highlight their limitations and showcase how PRiSM enables deeper insights into their scientific reasoning capabilities.

</details>


### [26] [TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943)
*Shima Imani,Seungwhan Moon,Lambert Mathias,Lu Zhang,Babak Damavandi*

Main category: cs.AI

TL;DR: TRACE框架通过辅助推理集诊断推理轨迹，而非仅评估最终答案，以解决大视觉语言模型在数学和科学推理中的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大视觉语言模型在数学和科学推理方面存在可靠性问题，标准的最终答案评估方法往往掩盖推理错误，导致静默失败持续存在。

Method: TRACE框架使用辅助推理集（ARS）将复杂问题分解为子问题-答案对，通过基于一致性的指标评估中间步骤，并暴露标准评估忽略的失败。

Result: 实验表明，ARS间的一致性相关于最终答案的正确性，并能精确定位推理失败的具体步骤，为模型改进提供可操作信号。TRACE还能定义置信区域区分可靠与不可靠的推理路径。

Conclusion: TRACE框架通过透明推理和一致性评估，为诊断大视觉语言模型的推理错误提供了有效工具，支持模型过滤、调试和优化。

Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.

</details>


### [27] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: 论文提出VQR-DQN，将变分量子电路与Rainbow DQN结合，用于人力资源分配问题，相比传统方法有显著性能提升


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在资源分配问题上受限于经典函数逼近器的表示能力，而量子计算中的叠加和纠缠特性有望提升DRL的表示能力

Method: 提出VQR-DQN方法，将环形拓扑变分量子电路与Rainbow DQN集成，将人力资源分配问题建模为具有组合动作空间的MDP

Result: 在四个HRAP基准测试中，VQR-DQN相比随机基线减少26.8%标准化完工时间，相比Double DQN和经典Rainbow DQN提升4.9-13.4%

Conclusion: 量子增强的深度强化学习在大规模资源分配问题上具有潜力，电路表达能力、纠缠与策略质量的理论联系支持了这一结果

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>


### [28] [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](https://arxiv.org/abs/2512.05954)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: SymPyBench是一个包含15,045个大学物理问题的合成基准，支持无限参数配置，提供结构化推理步骤和可执行代码，包含三种问题类型和三个新颖评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有基准在测试科学推理能力方面存在局限性，需要能够评估模型在不同问题变体上的一致性和可靠性的动态基准。

Method: 创建大规模参数化物理问题数据集，每个问题都配有结构化推理步骤和生成真实解的Python代码。包含三种问题格式：MC-Symbolic、MC-Numerical和free-form。引入三个新颖评估指标：一致性分数、失败率和混淆率。

Result: 实验显示最先进的指令调优语言模型在科学推理方面既有优势也有局限，SymPyBench能够量化模型在不同问题变体上的可变性和不确定性。

Conclusion: SymPyBench为开发更鲁棒和可解释的推理系统奠定了基础，能够全面评估模型在动态科学问题上的表现。

Abstract: We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [29] [Low-Complexity OFDM Deep Neural Receivers](https://arxiv.org/abs/2512.05249)
*Ankit Gupta,Onur Dizdar,Yun Chen,Fehmi Emre Kadan,Ata Sattarzadeh,Stephen Wang*

Main category: cs.IT

TL;DR: 提出一种用于OFDM信号的新型深度神经网络接收器（NeuralRx），通过改进的ResNet块设计降低计算复杂度和训练时间，同时提高解码精度。


<details>
  <summary>Details</summary>
Motivation: 现有的OFDM神经网络接收器架构忽略了训练收敛所需的epoch数和浮点运算量（FLOPs），这些指标会随着性能提升而显著增加，需要解决这些挑战。

Method: 设计新的ResNet块，采用小核尺寸和扩张率来降低FLOPs，使用统一通道大小减少内存访问成本，引入通道分割和混洗块，移除逐元素加法，使用GELU激活函数。

Result: 提出的NeuralRx减少了FLOPs数量，改善了训练收敛速度，同时提高了解码准确率。

Conclusion: 通过优化的ResNet块设计，实现了计算效率更高、训练更快且性能更好的OFDM神经网络接收器。

Abstract: Deep neural receivers (NeuralRxs) for Orthogonal Frequency Division Multiplexing (OFDM) signals are proposed for enhanced decoding performance compared to their signal-processing based counterparts. However, the existing architectures ignore the required number of epochs for training convergence and floating-point operations (FLOPs), which increase significantly with improving performance. To tackle these challenges, we propose a new residual network (ResNet) block design for OFDM NeuralRx. Specifically, we leverage small kernel sizes and dilation rates to lower the number of FLOPs (NFLOPs) and uniform channel sizes to reduce the memory access cost (MAC). The ResNet block is designed with novel channel split and shuffle blocks, element-wise additions are removed, with Gaussian error linear unit (GELU) activations. Extensive simulations show that our proposed NeuralRx reduces NFLOPs and improves training convergence while improving the decoding accuracy.

</details>


### [30] [Uncertainty-Aware Data-Efficient AI: An Information-Theoretic Perspective](https://arxiv.org/abs/2512.05267)
*Osvaldo Simeone,Yaniv Romano*

Main category: cs.IT

TL;DR: 这篇综述论文探讨了在数据有限场景下人工智能系统的两种互补方法：量化认知不确定性和通过合成数据增强缓解数据稀缺问题，从信息论角度分析数据稀缺的影响。


<details>
  <summary>Details</summary>
Motivation: 在机器人、电信、医疗等特定应用场景中，AI系统常面临训练数据有限的问题，这导致认知不确定性（可减少的不确定性），从而限制了预测性能。需要解决数据稀缺带来的挑战。

Method: 采用两种互补方法：1) 通过广义贝叶斯学习框架和"后贝叶斯"学习框架量化认知不确定性；2) 通过信息论泛化界限理论分析数据量与预测不确定性的关系；3) 使用符合预测和符合风险控制等方法提供有限样本统计保证；4) 结合有限标注数据和丰富模型预测或合成数据提高数据效率。

Result: 论文系统性地回顾了在数据有限场景下的多种方法：从理论上的广义贝叶斯框架和信息论界限，到具有有限样本保证的实用方法（如符合预测），再到结合合成数据的数据增强技术，为应对数据稀缺问题提供了全面的解决方案。

Conclusion: 通过信息论视角，本文强调了信息度量在量化数据稀缺影响中的关键作用，为在数据有限场景下开发更鲁棒、更可靠的AI系统提供了理论框架和实用方法，特别是在需要高可靠性的应用领域。

Abstract: In context-specific applications such as robotics, telecommunications, and healthcare, artificial intelligence systems often face the challenge of limited training data. This scarcity introduces epistemic uncertainty, i.e., reducible uncertainty stemming from incomplete knowledge of the underlying data distribution, which fundamentally limits predictive performance. This review paper examines formal methodologies that address data-limited regimes through two complementary approaches: quantifying epistemic uncertainty and mitigating data scarcity via synthetic data augmentation. We begin by reviewing generalized Bayesian learning frameworks that characterize epistemic uncertainty through generalized posteriors in the model parameter space, as well as ``post-Bayes'' learning frameworks. We continue by presenting information-theoretic generalization bounds that formalize the relationship between training data quantity and predictive uncertainty, providing a theoretical justification for generalized Bayesian learning. Moving beyond methods with asymptotic statistical validity, we survey uncertainty quantification methods that provide finite-sample statistical guarantees, including conformal prediction and conformal risk control. Finally, we examine recent advances in data efficiency by combining limited labeled data with abundant model predictions or synthetic data. Throughout, we take an information-theoretic perspective, highlighting the role of information measures in quantifying the impact of data scarcity.

</details>


### [31] [Foundations of information theory for coding theory](https://arxiv.org/abs/2512.05316)
*El Mahdi Mouloua,Essaid Mohamed*

Main category: cs.IT

TL;DR: 这篇讲义介绍了信息论及其与代数编码理论的联系，重点阐述了香农信息论的基本概念和噪声信道编码定理。


<details>
  <summary>Details</summary>
Motivation: 建立信息论与代数编码理论之间的联系，为学生和研究人员提供概率框架与代数技术之间的桥梁。

Method: 基于香农的信息论框架，从数学基础出发，通过二进制对称信道等具体例子，系统阐述熵、条件熵、互信息、信道容量等核心概念。

Result: 清晰阐述了信息论的基本数学基础，包括不确定性量化、信息传输模型，以及最大似然解码原理和香农噪声信道编码定理。

Conclusion: 该讲义为理解信息论与代数编码理论的联系提供了有价值的教学资源，特别适合希望将概率框架与代数技术结合的研究者。

Abstract: Information theory is introduced in this lecture note with a particular emphasis on its relevance to algebraic coding theory. The document develops the mathematical foundations for quantifying uncertainty and information transmission by building upon Shannon's pioneering formulation of information, entropy, and channel capacity. Examples, including the binary symmetric channel, illustrate key concepts such as entropy, conditional entropy, mutual information, and the noisy channel model. Furthermore, the note describes the principles of maximum likelihood decoding and Shannon's noisy channel coding theorem, which characterizes the theoretical limits of reliable communication over noisy channels. Students and researchers seeking a connection between probabilistic frameworks of information theory and structural and algebraic techniques used in modern coding theory will find this work helpful.

</details>
