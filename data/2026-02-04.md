<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.AI](#cs.AI) [Total: 69]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [NSC-SL: A Bandwidth-Aware Neural Subspace Compression for Communication-Efficient Split Learning](https://arxiv.org/abs/2602.02696)
*Zhen Fang,Miao Yang,Zehang Lin,Zheng Lin,Zihan Fang,Zongyuan Zhang,Tianyang Duan,Dong Huang,Shunzhi Zhu*

Main category: cs.NI

TL;DR: NSC-SL是一种带宽感知的自适应压缩算法，通过动态低秩近似和误差补偿张量分解来减少分割学习中的通信开销。


<details>
  <summary>Details</summary>
Motivation: 神经网络规模扩大对分布式机器学习带来挑战，特别是在通信资源有限的情况下。分割学习虽然减轻了客户端的计算负担，但中间激活和梯度的频繁传输带来了巨大的通信开销。

Method: NSC-SL首先根据奇异值分布动态确定低秩近似的最优秩，以适应实时带宽约束。然后使用带残差反馈的交替正交迭代进行误差补偿张量分解，有效最小化截断损失。

Result: 广泛的实验表明NSC-SL具有出色的性能，能够在实现高压缩比的同时保留对收敛至关重要的语义丰富信息。

Conclusion: NSC-SL通过协作机制实现了通信高效的分割学习，在保持模型性能的同时显著减少了通信开销。

Abstract: The expanding scale of neural networks poses a major challenge for distributed machine learning, particularly under limited communication resources. While split learning (SL) alleviates client computational burden by distributing model layers between clients and server, it incurs substantial communication overhead from frequent transmission of intermediate activations and gradients. To tackle this issue, we propose NSC-SL, a bandwidth-aware adaptive compression algorithm for communication-efficient SL. NSC-SL first dynamically determines the optimal rank of low-rank approximation based on the singular value distribution for adapting real-time bandwidth constraints. Then, NSC-SL performs error-compensated tensor factorization using alternating orthogonal iteration with residual feedback, effectively minimizing truncation loss. The collaborative mechanisms enable NSC-SL to achieve high compression ratios while preserving semantic-rich information essential for convergence. Extensive experiments demonstrate the superb performance of NSC-SL.

</details>


### [2] [Real-World Applications of AI in LTE and 5G-NR Network Infrastructure](https://arxiv.org/abs/2602.02787)
*Simran Saxena,Arpad Kovesdy*

Main category: cs.NI

TL;DR: 该论文提出结合AI辅助规划、强化学习RAN优化、实时遥测分析和数字孪生验证的架构，同时引入边缘托管执行模型，将应用直接运行在基站上，以提升网络性能并扩展数字服务访问。


<details>
  <summary>Details</summary>
Motivation: 当前LTE和5G-NR部署依赖静态手动配置，难以适应农村、游牧和带宽受限环境中快速变化的流量分布、传播特性和用户行为。同时，这些社区缺乏足够的回程带宽来支持基于云的AI医疗、教育和LLM应用。

Method: 提出AI辅助规划、强化学习RAN优化、实时遥测分析和数字孪生验证的架构；引入边缘托管执行模型，将应用容器化直接运行在LTE/5G-NR基站上，减少延迟和带宽消耗。

Result: 该架构能够将无线接入网从刚性规则系统转变为自适应、自优化的基础设施，同时边缘托管模型能够为带宽受限社区提供AI医疗、教育和LLM应用服务。

Conclusion: AI技术可以提升网络性能、降低运营开销、扩展先进数字服务访问，符合可持续和包容性网络发展的更广泛目标。

Abstract: Telecommunications networks generate extensive performance and environmental telemetry, yet most LTE and 5G-NR deployments still rely on static, manually engineered configurations. This limits adaptability in rural, nomadic, and bandwidth-constrained environments where traffic distributions, propagation characteristics, and user behavior fluctuate rapidly. Artificial Intelligence (AI), more specifically Machine Learning (ML) models, provide new opportunities to transition Radio Access Networks (RANs) from rigid, rule-based systems toward adaptive, self-optimizing infrastructures that can respond autonomously to these dynamics. This paper proposes a practical architecture incorporating AI-assisted planning, reinforcement-learning-based RAN optimization, real-time telemetry analytics, and digital-twin-based validation. In parallel, the paper addresses the challenge of delivering embodied-AI healthcare services, educational tools, and large language model (LLM) applications to communities with insufficient backhaul for cloud computing. We introduce an edge-hosted execution model in which applications run directly on LTE/5G-NR base stations using containers, reducing latency and bandwidth consumption while improving resilience. Together, these contributions demonstrate how AI can enhance network performance, reduce operational overhead, and expand access to advanced digital services, aligning with broader goals of sustainable and inclusive network development.

</details>


### [3] [Analyzing Zigbee Traffic: Datasets, Classification and Storage Trade-offs](https://arxiv.org/abs/2602.03140)
*Antonio Boiano,Dalin Zheng,Fabio Palmese,Andrea Pimpinella,Alessandro E. C. Redondi*

Main category: cs.NI

TL;DR: 本文提出了ZIOTP2025数据集，用于Zigbee智能家居流量分析，研究设备分类、设备识别及存储效率，发现跨网络配置时性能下降明显，并提出量化压缩可大幅减少存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有Zigbee流量分析研究通常基于有限数据集和固定网络配置，难以反映真实智能家居环境的复杂性，需要更全面的数据集和分析方法来支持物联网取证系统的开发。

Method: 1) 创建ZIOTP2025公开数据集，包含商业智能家居设备在多种网络配置下的真实交互流量；2) 研究设备类型分类和个体设备识别任务；3) 评估模型在配置内和跨配置场景下的鲁棒性；4) 研究流量特征量化压缩对存储效率和分类性能的影响。

Result: 1) 在受控条件下可获得高分类准确率；2) 跨不同网络配置时性能显著下降，特别是细粒度识别任务；3) 通过量化压缩可将存储需求减少约4-5倍，同时保持接近无损的分类性能。

Conclusion: 需要拓扑感知的Zigbee流量分析和存储高效的特征压缩方法，以构建鲁棒且可扩展的物联网取证系统。ZIOTP2025数据集为相关研究提供了重要资源。

Abstract: Zigbee is widely used in smart home environments due to its low power consumption and support for mesh networking, making it a relevant target for traffic-based IoT forensic analysis. However, existing studies often rely on limited datasets and fixed network configurations. In this paper, we analyze Zigbee network traffic from three complementary perspectives: data collection, traffic classification, and storage efficiency. We introduce ZIOTP2025, a publicly available dataset of Zigbee traffic collected from commercial smart home devices deployed under multiple network configurations and capturing realistic interaction scenarios. Using this dataset, we study two traffic classification tasks: device type classification and individual device identification, and evaluate their robustness under both intra-configuration and cross-configuration settings. Our results show that while high classification accuracy can be achieved under controlled conditions, performance degrades significantly when models are evaluated across different network configurations, particularly for fine-grained identification tasks. Finally, we investigate the trade-off between traffic storage requirements and classification accuracy. We show that lossy compression of traffic features through quantization can reduce storage requirements by approximately 4-5x compared to lossless storage of raw packet traces, while preserving near-lossless classification performance. Overall, our results highlight the need for topology-aware Zigbee traffic analysis and storage-efficient feature compression to enable robust and scalable IoT forensic systems.

</details>


### [4] [Towards Context-Aware Edge-Cloud Continuum Orchestration for Multi-user XR Services](https://arxiv.org/abs/2602.03262)
*Inhar Yeregui,Ángel Martín,Mikel Zorrilla,Roberto Viola,Jasone Astorga,Eduardo Jacob*

Main category: cs.NI

TL;DR: 提出一个用于多用户XR服务的分层数学模型，通过边缘云连续体编排优化网络、计算和服务资源，以支持低延迟沉浸式体验。


<details>
  <summary>Details</summary>
Motivation: 多用户XR应用（娱乐、教育、远程医疗）需要无缝沉浸式体验，但现有系统在编排网络、计算和服务资源方面存在局限，需要结构化方法来分析和优化这些复杂系统，5G/6G网络为大规模XR服务提供了必要基础设施。

Method: 开发一个将多用户XR服务参数化到虚拟化架构四个关键层的模型，数学形式化该模型，提出上下文感知框架，定义每层关键参数，并将其集成到全面的边缘云连续体编排策略中。

Result: 包括对现有边缘云连续体编排方法局限性和需求的分析、分层数学模型的制定，以及验证框架展示了所提解决方案的实用性和可行性。

Conclusion: 该研究通过分层数学模型和上下文感知框架，为多用户XR服务在边缘云连续体中的资源编排提供了系统化解决方案，有助于实现低延迟沉浸式体验。

Abstract: The rapid growth of multi-user eXtended Reality (XR) applications, spanning fields such as entertainment, education, and telemedicine, demands seamless, immersive experiences for users interacting within shared, distributed environments. Delivering such latency-sensitive experiences involves considerable challenges in orchestrating network, computing, and service resources, where existing limitations highlight the need for a structured approach to analyse and optimise these complex systems. This challenge is amplified by the need for high-performance, low-latency connectivity, where 5G and 6G networks provide essential infrastructure to meet the requirements of XR services at scale. This article addresses these challenges by developing a model that parametrises multi-user XR services across four critical layers of the standard virtualisation architecture. We formalise this model mathematically, proposing a context-aware framework that defines key parameters at each level and integrates them into a comprehensive Edge-Cloud Continuum orchestration strategy. Our contributions include a detailed analysis of the current limitations and needs in existing Edge-Cloud Continuum orchestration approaches, the formulation of a layered mathematical model, and a validation framework that demonstrates the utility and feasibility of the proposed solution.

</details>


### [5] [QASM: A Novel Framework for QUIC-Aware Stateful Middleboxes](https://arxiv.org/abs/2602.03354)
*Hari Hara Sudhan Selvam,Sameer G. Kulkarni*

Main category: cs.NI

TL;DR: 提出一个通用框架，使有状态中间盒能够可靠追踪QUIC连接，即使端点改变IP地址或端口号，从而解决HTTP/3/QUIC对中间盒功能的破坏问题。


<details>
  <summary>Details</summary>
Motivation: HTTP/3使用QUIC协议，其加密和连接迁移特性模糊了流语义，破坏了有状态中间盒（如NAT、速率限制器、负载均衡器等）的可见性和功能，特别是在企业网络和Kubernetes服务部署中。

Method: 提出一个新颖的通用框架，使中间盒能够可靠追踪QUIC连接，即使端点改变IP地址或端口号。通过原型实现验证方案有效性。

Result: 原型实现表明，该方案能够保持中间盒对HTTP/3的功能，吞吐量和延迟的性能开销可忽略（<5%），即使在高达100Hz的QUIC连接迁移率下也能有效工作。

Conclusion: 提出的框架成功解决了QUIC对有状态中间盒的挑战，在保持高性能的同时确保了中间盒功能的完整性，为HTTP/3在企业网络中的部署提供了可行解决方案。

Abstract: Stateful Middleboxes are integral part of enterprise and campus networks that provide essential in-network, security, and value-added services. These stateful middleboxes rely on precise network flow identification. However, the adoption of HTTP/3, which uses the QUIC protocol, poses significant challenges to the proper functioning of these devices. QUIC's encryption and connection migration features obscure flow semantics, disrupting middlebox visibility and functionality. We examine how QUIC disrupts middleboxes like Network Address Translators (NATs), Rate Limiters, Load Balancers, etc., and affects Kubernetes-based service deployments. To address these challenges, we propose a novel, generalized framework that enables stateful middleboxes to reliably track QUIC connections, even when the endpoints change their internet protocol (IP) address or port numbers. Our prototype implementation demonstrates that the proposed approach preserves middlebox functionality with HTTP/3 with negligible performance overhead (< 5%) on both throughput and latency, and works effectively even under high QUIC connection migration rates of up to 100 Hz.

</details>


### [6] [Morphe: High-Fidelity Generative Video Streaming with Vision Foundation Model](https://arxiv.org/abs/2602.03529)
*Tianyi Gong,Zijian Cao,Zixing Zhang,Jiangkai Wu,Xinggong Zhang,Shuguang Cui,Fangxin Wang*

Main category: cs.NI

TL;DR: Morphe是一种基于视觉基础模型(VFM)的端到端生成式视频流媒体系统，相比H.265节省62.5%带宽，在恶劣网络条件下实现实时、抗丢包的视频传输


<details>
  <summary>Details</summary>
Motivation: 传统像素编解码流媒体已接近压缩极限，而新兴的神经增强或生成式流媒体在延迟和视觉保真度方面存在不足。作者希望利用视觉基础模型的强大视频理解和处理能力，实现更高压缩率、高保真度和抗丢包能力的实时视频流媒体

Method: 提出首个基于VFM的端到端生成式视频流媒体范式：1)联合训练视觉分词器和变分辨率时空优化；2)在模拟网络约束下训练；3)构建鲁棒流媒体系统，利用智能丢包抵抗真实网络扰动

Result: Morphe在保持可比视觉质量的同时，相比H.265节省62.5%带宽；在挑战性网络环境中实现实时、抗丢包的视频传输，代表了VFM赋能多媒体流媒体解决方案的里程碑

Conclusion: Morphe成功利用视觉基础模型实现了革命性的视频流媒体范式，在压缩率、保真度和抗丢包能力方面取得突破，为恶劣网络条件下的实时视频传输提供了有效解决方案

Abstract: Video streaming is a fundamental Internet service, while the quality still cannot be guaranteed especially in poor network conditions such as bandwidth-constrained and remote areas. Existing works mainly work towards two directions: traditional pixel-codec streaming nearly approaches its limit and is hard to step further in compression; the emerging neural-enhanced or generative streaming usually fall short in latency and visual fidelity, hindering their practical deployment. Inspired by the recent success of vision foundation model (VFM), we strive to harness the powerful video understanding and processing capacities of VFM to achieve generalization, high fidelity and loss resilience for real-time video streaming with even higher compression rate. We present the first revolutionized paradigm that enables VFM-based end-to-end generative video streaming towards this goal. Specifically, Morphe employs joint training of visual tokenizers and variable-resolution spatiotemporal optimization under simulated network constraints. Additionally, a robust streaming system is constructed that leverages intelligent packet dropping to resist real-world network perturbations. Extensive evaluation demonstrates that Morphe achieves comparable visual quality while saving 62.5\% bandwidth compared to H.265, and accomplishes real-time, loss-resilient video delivery in challenging network environments, representing a milestone in VFM-enabled multimedia streaming solutions.

</details>


### [7] [RIPPLE: Lifecycle-aware Embedding of Service Function Chains in Multi-access Edge Computing](https://arxiv.org/abs/2602.03662)
*Federico Giarrè,Holger Karl*

Main category: cs.NI

TL;DR: RIPPLE：一种生命周期感知的SFC嵌入方法，在MEC网络中考虑VNF生命周期动态和用户连接预测，减少服务中断


<details>
  <summary>Details</summary>
Motivation: 在MEC网络中，用户移动需要频繁的SFC重配置，但VNF的生命周期操作耗时，忽略这些动态会过度简化部署并危及QoS。现有方法未充分考虑生命周期动态和预测不确定性。

Method: 提出RIPPLE方法，利用用户连接预测来主动部署VNF和重配置SFC，同时考虑生命周期动态和连接不确定性，在正确的时间和位置部署VNF。

Result: RIPPLE减少了服务中断，即使在现实生命周期约束下，也能接近假设瞬时生命周期的理想解决方案的性能。

Conclusion: 生命周期动态对SFC管理至关重要，RIPPLE通过联合考虑生命周期和连接不确定性，实现了有效的SFC嵌入，提升了MEC网络中的服务质量。

Abstract: In Multi-access Edge Computing networks, services can be deployed on nearby edge clouds (EC) as service function chains (SFCs) to meet strict quality of service (QoS) requirements. As users move, frequent SFC reconfigurations are required, but these are non-trivial: SFCs can serve users only when all required virtual network functions (VNFs) are available, and VNFs undergo time-consuming lifecycle operations before becoming operational. We show that ignoring lifecycle dynamics oversimplifies deployment, jeopardizes QoS, and must be avoided in practical SFC management. To address this, forecasts of user connectivity can be leveraged to proactively deploy VNFs and reconfigure SFCs. But forecasts are inherently imperfect, requiring lifecycle and connectivity uncertainty to be jointly considered. We present RIPPLE, a lifecycle-aware SFC embedding approach to deploy VNFs at the right time and location, reducing service interruptions. We show that RIPPLE closes the gap with solutions that unrealistically assume instantaneous lifecycle, even under realistic lifecycle constraints.

</details>


### [8] [xDevSM: An Open-Source Framework for Portable, AI-Ready xApps Across Heterogeneous O-RAN Deployments](https://arxiv.org/abs/2602.03821)
*Angelo Feraudo,Stefano Maxenti,Andrea Lacava,Leonardo Bonati,Paolo Bellavista,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: xDevSM是一个降低O-RAN中xApp开发门槛的框架，通过统一观测和控制能力，支持AI驱动的智能应用开发。


<details>
  <summary>Details</summary>
Motivation: O-RAN架构虽然支持RAN的闭环控制，但xApp开发面临三大障碍：1）E2接口的低级消息模型复杂；2）异构RAN软件栈互操作性有限；3）缺乏开发者友好框架。这些阻碍了AI驱动xApp的发展和采用。

Method: 提出xDevSM框架，统一O-RAN部署中的观测和控制能力。该框架提供丰富的KPM（关键性能测量）和细粒度无线资源管理控制，为AI驱动xApp奠定基础。在真实测试平台上验证，使用商用设备和异构RAN硬件（USRP SDR和富士康无线单元），并展示其在多个开源RAN软件栈上的无缝互操作性。

Result: 通过三个O-RAN场景验证框架能力：1）基于KPM的网络性能监控；2）跨多UE和多切片的切片级PRB分配控制；3）移动性感知的切换控制。证明xDevSM能够实现智能闭环应用，为异构RAN部署中的学习优化奠定基础。

Conclusion: xDevSM显著降低了xApp开发门槛，提供了AI驱动xApp开发的基础工具。该框架已开源，可作为研究社区的基础工具，支持异构RAN部署中的学习优化。

Abstract: Openness and programmability in the O-RAN architecture enable closed-loop control of the Radio Access Network (RAN). Artificial Intelligence (AI)-driven xApps, in the near-real-time RAN Intelligent Controller (RIC), can learn from network data, anticipate future conditions, and dynamically adapt radio configurations. However, their development and adoption are hindered by the complexity of low-level RAN control and monitoring message models exposed over the O-RAN E2 interface, limited interoperability across heterogeneous RAN software stacks, and the lack of developer-friendly frameworks. In this paper, we introduce xDevSM, a framework that significantly lowers the barrier to xApp development by unifying observability and control in O-RAN deployment. By exposing a rich set of Key Performance Measurements (KPMs) and enabling fine-grained radio resource management controls, xDevSM provides the essential foundation for practical AI-driven xApps. We validate xDevSM on real-world testbeds, leveraging Commercial Off-the-Shelf (COTS) devices together with heterogeneous RAN hardware, including Universal Software Radio Peripheral (USRP)-based Software-defined Radios (SDRs) and Foxconn radio units, and show its seamless interoperability across multiple open-source RAN software stacks. Furthermore, we discuss and evaluate the capabilities of our framework through three O-RAN-based scenarios of high interest: (i) KPM-based monitoring of network performance, (ii) slice-level Physical Resource Block (PRB) allocation control across multiple User Equipments (UEs) and slices, and (iii) mobility-aware handover control, showing that xDevSM can implement intelligent closed-loop applications, laying the groundwork for learning-based optimization in heterogeneous RAN deployments. xDevSM is open source and available as foundational tool for the research community.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [CreditAudit: 2D Auditing for LLM Evaluation and Selection](https://arxiv.org/abs/2602.02515)
*Yiliang Song,Hongjun An,Jiangong Xiao,Haofei Zhao,Jiawei Shao,Xuelong Li*

Main category: cs.AI

TL;DR: CreditAudit：一个面向部署的信用审计框架，通过评估模型在多种系统提示模板下的表现，提供平均能力和稳定性风险两个维度的评估，帮助在实际部署中做出更明智的模型选择。


<details>
  <summary>Details</summary>
Motivation: 当前公开基准测试的分数趋同且差异微小，但这些分数无法反映用户日常使用体验。系统提示、输出协议和交互模式的常规迭代变化，以及在多步代理流程中，小的协议变化可能引发不成比例的失败，导致从业者不确定应该部署哪个模型。

Method: 提出CreditAudit框架，在多个基准测试上评估模型在一系列语义对齐且非对抗性的系统提示模板下的表现。报告平均能力（跨场景的平均性能）和场景诱导波动σ（稳定性风险信号），并将波动性通过跨模型分位数映射为可解释的信用等级（从AAA到BBB），同时提供诊断以减轻模板难度漂移。

Result: 在GPQA、TruthfulQA和MMLU Pro上的受控实验表明，具有相似平均能力的模型可能表现出显著不同的波动性，稳定性风险可以在代理或高失败成本场景中颠覆优先级决策。

Conclusion: CreditAudit通过提供基于二维和信用等级的语言，支持特定场景下的分层部署和更规范的测试与监控资源分配，为现实世界使用实现更客观和可信的模型评估。

Abstract: Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.

</details>


### [10] [Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers](https://arxiv.org/abs/2602.02559)
*Pengyu Dai,Weihao Xuan,Junjue Wang,Hongruixuan Chen,Jian Song,Yafei Ou,Naoto Yokoya*

Main category: cs.AI

TL;DR: GeoEvolver：一个自进化的多智能体系统，通过结构化交互让LLM智能体在无需参数更新的情况下获取地球观测专业知识，显著提升复杂EO任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在专业化、工具密集的领域（如地球观测）中表现不佳，这些领域需要长时程执行、多模态协调和严格的工具约束。智能体缺乏从交互中学习细粒度工具级专业知识的机制，导致无法可靠配置工具参数或从执行失败中恢复。

Method: GeoEvolver采用自进化多智能体系统：1）通过检索增强的多智能体编排器将查询分解为独立子目标；2）在子目标级别探索多样化的工具参数配置；3）从成功模式和失败根因分析中提炼知识，存储在进化记忆库中，为未来查询提供上下文演示。

Result: 在三个工具集成的EO基准测试中，GeoEvolver一致提高了端到端任务成功率，在多个LLM骨干网络上平均提升12%，表明EO专业知识可以通过与环境的细粒度交互逐步涌现。

Conclusion: GeoEvolver证明了LLM智能体可以通过结构化交互在无需参数更新的情况下获取领域专业知识，为复杂工具密集型任务提供了一种有效的自进化解决方案，显著提升了地球观测等专业领域的任务执行能力。

Abstract: Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.

</details>


### [11] [Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems](https://arxiv.org/abs/2602.02582)
*Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah*

Main category: cs.AI

TL;DR: 该论文研究LLM推荐系统中的不确定性和公平性问题，提出新的评估方法，发现Gemini 1.5 Flash存在系统性不公平，并引入人格感知公平性基准。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在零样本推荐中表现出色，但其预测不确定性和内在偏见威胁着推荐系统的可靠性和公平性。需要建立系统的评估框架来衡量这些风险。

Method: 1) 构建包含电影和音乐两个领域的基准数据集，标注8个人口统计属性（31个分类值）；2) 通过熵量化预测不确定性；3) 使用提示扰动（拼写错误和多语言输入）测试鲁棒性；4) 将人格感知公平性整合到RecLLM评估流程中；5) 提出不确定性感知的评估方法。

Result: 发现Google DeepMind的Gemini 1.5 Flash对某些敏感属性存在系统性不公平（SNSR=0.1363，SNSV=0.0507），这些差异在提示扰动下仍然存在。揭示了人格特征与偏见模式之间的关联，以及个性化与群体公平性之间的权衡。

Conclusion: 该研究为更安全、可解释的RecLLM奠定了基础，提出了不确定性感知评估方法和人格特征知情公平性基准，推动了LLM推荐系统的可解释性和公平性，并激励未来在多模型基准和自适应校准方面的研究。

Abstract: Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

</details>


### [12] [PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review](https://arxiv.org/abs/2602.02589)
*Yanki Margalit,Erni Avram,Ran Taig,Oded Margalit,Nurit Cohen-Inger*

Main category: cs.AI

TL;DR: PeerRank是一个完全自主的端到端评估框架，让LLM自主生成评估任务、通过实时网络检索回答问题、评估同行回答，无需人工监督或参考答案，实现开放世界LLM评估。


<details>
  <summary>Details</summary>
Motivation: 传统LLM评估依赖人工构建的基准、参考答案和人工/单一模型判断，这些方法扩展性差、容易过时，且与依赖网络检索和综合的开放世界部署不匹配。

Method: PeerRank将评估视为多智能体过程，每个模型对称地作为任务设计者、回答者和评估者参与，通过类别限定的实时网络检索回答问题，并移除有偏见的判断。

Result: 在12个商业模型和420个自主生成问题的研究中，PeerRank产生稳定、可区分的排名，揭示可测量的身份和呈现偏见，排名稳健，平均同行评分与Elo一致，在TruthfulQA和GSM8K上与客观准确率相关。

Conclusion: 具有选择性网络基础回答的偏见感知同行评估可以扩展开放世界LLM评估，超越静态和人工策划的基准。

Abstract: Evaluating large language models typically relies on human-authored benchmarks, reference answers, and human or single-model judgments, approaches that scale poorly, become quickly outdated, and mismatch open-world deployments that depend on web retrieval and synthesis. We introduce PeerRank, a fully autonomous end-to-end evaluation framework in which models generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses and aggregate dense peer assessments into relative performance estimates, without human supervision or gold references. PeerRank treats evaluation as a multi-agent process where each model participates symmetrically as task designer, respondent, and evaluator, while removing biased judgments. In a large-scale study over 12 commercially available models and 420 autonomously generated questions, PeerRank produces stable, discriminative rankings and reveals measurable identity and presentation biases. Rankings are robust, and mean peer scores agree with Elo. We further validate PeerRank on TruthfulQA and GSM8K, where peer scores correlate with objective accuracy. Together, these results suggest that bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static and human curated benchmarks.

</details>


### [13] [A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior](https://arxiv.org/abs/2602.02639)
*Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel*

Main category: cs.AI

TL;DR: 提出NSG指标评估LLM自我解释的忠实度，发现自我解释能显著提升模型行为预测（11-37%），优于外部模型解释，但5-15%自我解释存在严重误导。


<details>
  <summary>Details</summary>
Motivation: LLM自我解释常被视为AI监督的有力工具，但其对模型真实推理过程的忠实度缺乏理解。现有忠实度评估方法存在局限，主要依赖对抗性提示或检测推理错误，忽略了解释的预测价值。

Method: 提出归一化可模拟增益（NSG）指标，基于"忠实解释应让观察者学习模型决策标准，从而更好预测相关输入行为"的理念。评估18个前沿专有和开源模型（如Gemini 3、GPT-5.2、Claude 4.5），在健康、商业、伦理等领域的7,000个反事实数据上进行测试。

Result: 自我解释显著提升模型行为预测（11-37% NSG增益）。自我解释比外部模型生成的解释提供更多预测信息，即使外部模型更强。这表明自我知识带来的优势是外部解释方法无法复制的。同时发现5-15%的自我解释存在严重误导。

Conclusion: 尽管存在缺陷，但自我解释确实编码了有助于预测模型行为的信息，为自我解释提供了积极案例。NSG为评估解释忠实度提供了通用且可扩展的度量标准。

Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.

</details>


### [14] [MARS: Modular Agent with Reflective Search for Automated AI Research](https://arxiv.org/abs/2602.02660)
*Jiefeng Chen,Bhavana Dalvi Mishra,Jaehyun Nam,Rui Meng,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: MARS是一个专为自主AI研究设计的模块化代理框架，通过预算感知规划、模块化构建和比较反思记忆三大支柱，解决了AI研究中计算成本高和性能归因不透明的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代理在AI研究中表现不佳，因为它们通常生成忽略执行成本和因果因素的单体脚本。AI研究不同于一般软件工程，具有计算评估昂贵（如模型训练）和性能归因不透明的特点。

Method: MARS框架包含三个核心组件：1）预算感知规划：使用成本约束的蒙特卡洛树搜索（MCTS）来平衡性能与执行成本；2）模块化构建：采用"设计-分解-实现"流水线管理复杂研究仓库；3）比较反思记忆：通过分析解决方案差异来提炼高信号洞察，解决信用分配问题。

Result: MARS在MLE-Bench上实现了开源框架中的最先进性能，在可比设置下与排行榜顶级方法保持竞争力。此外，系统表现出定性的"顿悟"时刻，63%的已使用经验来自跨分支转移，表明代理能有效跨搜索路径泛化洞察。

Conclusion: MARS框架通过结合预算感知规划、模块化构建和比较反思记忆，成功解决了自主AI研究中的关键挑战，在性能和成本效率方面都表现出色，并能有效跨任务转移学习。

Abstract: Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.

</details>


### [15] [ATLAS : Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters](https://arxiv.org/abs/2602.02709)
*Ujin Jeon,Jiyong Kwon,Madison Ann Sullivan,Caleb Eunho Lee,Guang Lin*

Main category: cs.AI

TL;DR: ATLAS提出了一种任务分布式框架，通过专门的辅助代理进行探索、超参数调整和参考策略管理，核心算法EvoDPO自适应更新阶段索引参考策略，在非平稳环境中提升稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM代理系统在提示优化和自动问题解决方面表现良好，但要么在微调后保持求解器不变，要么依赖静态偏好优化循环，这在长视野任务中变得难以处理。

Method: 提出ATLAS任务分布式框架，迭代开发轻量级研究代理，同时将探索、超参数调整和参考策略管理等互补角色委托给专门的辅助代理。核心算法EvoDPO自适应更新阶段索引参考策略。

Result: 在非平稳线性上下文赌博机和科学机器学习损失重加权实验中，ATLAS相比静态单代理基线提高了稳定性和性能。提供了基于概念漂移的偏好上下文赌博机的理论遗憾分析。

Conclusion: ATLAS框架通过任务分布和自适应参考策略更新，有效解决了长视野任务中的非平稳性问题，在复杂环境中展现出优于静态方法的性能。

Abstract: Recent multi-LLM agent systems perform well in prompt optimization and automated problem-solving, but many either keep the solver frozen after fine-tuning or rely on a static preference-optimization loop, which becomes intractable for long-horizon tasks. We propose ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution), a task-distributed framework that iteratively develops a lightweight research agent while delegating complementary roles to specialized supporter agents for exploration, hyperparameter tuning, and reference policy management. Our core algorithm, Evolving Direct Preference Optimization (EvoDPO), adaptively updates the phase-indexed reference policy. We provide a theoretical regret analysis for a preference-based contextual bandit under concept drift. In addition, experiments were conducted on non-stationary linear contextual bandits and scientific machine learning (SciML) loss reweighting for the 1D Burgers' equation. Both results show that ATLAS improves stability and performance over a static single-agent baseline.

</details>


### [16] [Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction](https://arxiv.org/abs/2602.02711)
*Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang*

Main category: cs.AI

TL;DR: 提出动态混合精度路由框架，在长时决策任务中自适应选择高精度或低精度LLM，以平衡推理成本与任务成功率


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长时决策任务中表现良好，但使用大型高精度LLM进行多步交互推理成本过高。虽然通常认为更高的任务成功率需要使用更大更强的LLM模型，但实际推理成本限制了这一做法的可行性。

Method: 基于观察到不同交互步骤对精度的敏感度不同，提出动态混合精度路由框架，在每个决策步骤自适应选择高精度或低精度LLM。路由器通过两阶段训练：1) 基于KL散度的监督学习识别精度敏感步骤；2) 使用Group-Relative Policy Optimization (GRPO)进一步提高任务成功率。

Result: 在ALFWorld上的实验表明，该方法在准确率-成本权衡方面相比单精度基线和启发式路由方法有显著提升。

Conclusion: 通过动态混合精度路由框架，可以在保持任务成功率的同时显著降低长时决策任务的推理成本，为LLM在资源受限环境中的应用提供了有效解决方案。

Abstract: Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

</details>


### [17] [Scaling-Aware Adapter for Structure-Grounded LLM Reasoning](https://arxiv.org/abs/2602.02780)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Yi Li,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.AI

TL;DR: Cuttlefish是一个统一的全原子大语言模型，通过自适应缩放结构补丁和几何接地适配器，在保持几何基础的同时实现多模态结构推理，减少结构幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么缺乏几何基础导致结构幻觉，要么使用固定长度的模态融合瓶颈，过度压缩结构信息且分配不优，阻碍了通用全原子推理的实现。

Method: 1. 缩放感知补丁：通过指令条件门控机制生成结构图上的可变大小补丁，根据结构复杂度自适应缩放查询token预算；2. 几何接地适配器：通过跨注意力机制细化自适应token并注入模态嵌入，向LLM暴露显式几何线索。

Result: 在多样化的全原子基准测试中，Cuttlefish在异构结构接地推理方面实现了优越性能。

Conclusion: Cuttlefish通过自适应缩放结构token和几何接地机制，成功实现了基于几何线索的语言推理，减少了结构幻觉，为通用全原子推理提供了有效解决方案。

Abstract: Large language models (LLMs) are enabling reasoning over biomolecular structures, yet existing methods remain modality-specific and typically compress structural inputs through sequence-based tokenization or fixed-length query connectors. Such architectures either omit the geometric groundings requisite for mitigating structural hallucinations or impose inflexible modality fusion bottlenecks that concurrently over-compress and suboptimally allocate structural tokens, thereby impeding the realization of generalized all-atom reasoning. We introduce Cuttlefish, a unified all-atom LLM that grounds language reasoning in geometric cues while scaling modality tokens with structural complexity. First, Scaling-Aware Patching leverages an instruction-conditioned gating mechanism to generate variable-size patches over structural graphs, adaptively scaling the query token budget with structural complexity to mitigate fixed-length connector bottlenecks. Second, Geometry Grounding Adapter refines these adaptive tokens via cross-attention to modality embeddings and injects the resulting modality tokens into the LLM, exposing explicit geometric cues to reduce structural hallucination. Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning. Code is available at the project repository.

</details>


### [18] [Chain of Simulation: A Dual-Mode Reasoning Framework for Large Language Models with Dynamic Problem Routing](https://arxiv.org/abs/2602.02842)
*Saeid Sheikhi*

Main category: cs.AI

TL;DR: CoS是一个双模式推理框架，通过动态路由问题到专门的推理策略来提升LLM性能，包括计算流、符号状态跟踪和混合事实提取三种模式。


<details>
  <summary>Details</summary>
Motivation: 现有统一的提示方法无法针对不同类型的问题进行优化，需要一种能够根据问题类型动态选择专门推理策略的框架。

Method: CoS采用三种推理模式：数学问题的计算流与自一致性、空间推理的符号状态跟踪与JSON表示、多跳推理的混合事实提取。提供模式选择、状态跟踪和答案提取的详细算法。

Result: 在GSM8K、StrategyQA和bAbI基准测试中，CoS相比最强基线分别获得1.0%、2.5%绝对提升和65.2%相对提升。计算模式正确应用于数学问题时准确率达81.2%，而错误路由导致0%准确率。

Conclusion: CoS是一种无需额外训练即可提升LLM推理的有效方法，问题特定的模式选择至关重要，相比自一致性在54%更低计算成本下获得可比性能。

Abstract: We present Chain of Simulation (CoS), a novel dual-mode reasoning framework that dynamically routes problems to specialized reasoning strategies in Large Language Models (LLMs). Unlike existing uniform prompting approaches, CoS employs three distinct reasoning modes: (1) computational flow with self-consistency for mathematical problems, (2) symbolic state tracking with JSON representations for spatial reasoning, and (3) hybrid fact-extraction for multi-hop inference. Through comprehensive evaluation on GSM8K, StrategyQA, and bAbI benchmarks using four state-of-the-art models (Gemma-3 27B, LLaMA-3.1 8B, Mistral 7B, and Qwen-2.5 14B), we demonstrate that CoS achieves 71.5% accuracy on GSM8K (1.0% absolute improvement), 90.0% on StrategyQA (2.5% improvement), and 19.0% on bAbI (65.2% relative improvement) compared to the strongest baselines. The analysis reveals that problem-specific mode selection is crucial, with computational mode achieving 81.2% accuracy when correctly applied to mathematical problems, while misrouting leads to 0% accuracy. We provide detailed algorithms for mode selection, state tracking, and answer extraction, establishing CoS as an effective approach for improving LLM reasoning without additional training. The framework provides superior trade-offs between accuracy and efficiency compared to Self-Consistency, achieving comparable performance at 54% lower computational cost.

</details>


### [19] [AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents](https://arxiv.org/abs/2602.02849)
*Xi Yu,Dmitrii Torbunov,Soumyajit Mandal,Yihui Ren*

Main category: cs.AI

TL;DR: AutoSizer：基于大语言模型的反射式元优化框架，用于模拟混合信号集成电路的晶体管尺寸优化，通过内外循环机制和自适应搜索空间构建，显著提升优化效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 模拟混合信号集成电路设计严重依赖专家知识，晶体管尺寸优化面临非线性行为、高维设计空间和严格性能约束等挑战。现有EDA方法通常将尺寸优化视为静态黑盒优化，导致效率低下且鲁棒性差。虽然大语言模型具备强大推理能力，但不适合AMS尺寸优化的精确数值优化。

Method: 提出AutoSizer框架，采用反射式LLM驱动的元优化方法，统一电路理解、自适应搜索空间构建和优化编排。采用双循环优化框架：内循环进行电路尺寸优化，外循环分析优化动态和约束，根据仿真反馈迭代精炼搜索空间。还建立了AMS-SizingBench基准测试集。

Result: AutoSizer在实验中表现出更高的解决方案质量、更快的收敛速度和更高的成功率，在不同电路难度下均优于传统优化方法和现有基于LLM的智能体。

Conclusion: AutoSizer成功地将LLM的推理能力与数值优化相结合，为AMS电路尺寸优化提供了有效的解决方案，通过自适应搜索空间构建和闭环优化机制显著提升了优化效率和鲁棒性。

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.

</details>


### [20] [STEER: Inference-Time Risk Control via Constrained Quality-Diversity Search](https://arxiv.org/abs/2602.02862)
*Eric Yang,Jong Ha Lee,Jonathan Amar,Elissa Ye,Yugang Jia*

Main category: cs.AI

TL;DR: STEER框架通过进化搜索构建多样化语言角色，在推理时通过单一可解释参数控制决策保守度，解决LLM在顺序决策任务中的模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: LLM在平均正确性训练中常出现模式崩溃，在需要权衡特异性和敏感性的顺序决策任务（如临床分诊）中，标准对齐方法无法根据上下文约束调整ROC操作点。

Method: STEER通过离线约束质量-多样性搜索构建自然语言角色群体，确保行为覆盖同时满足安全、推理和稳定性阈值。推理时通过单一可解释参数将用户指定的风险百分位数映射到选定角色，实现决策保守度的单调调整。

Result: 在两个临床分诊基准测试中，STEER相比基于温度的采样和静态角色集成实现了更广泛的行为覆盖。与代表性后训练方法相比，在明确紧急病例上保持更高准确性，同时在模糊决策上提供可比的控制能力。

Conclusion: STEER是一种保持安全性的风险控制范式，能够在不损害领域能力的情况下引导LLM行为，为顺序决策任务提供了有效的可调控解决方案。

Abstract: Large Language Models (LLMs) trained for average correctness often exhibit mode collapse, producing narrow decision behaviors on tasks where multiple responses may be reasonable. This limitation is particularly problematic in ordinal decision settings such as clinical triage, where standard alignment removes the ability to trade off specificity and sensitivity (the ROC operating point) based on contextual constraints. We propose STEER (Steerable Tuning via Evolutionary Ensemble Refinement), a training-free framework that reintroduces this tunable control. STEER constructs a population of natural-language personas through an offline, constrained quality-diversity search that promotes behavioral coverage while enforcing minimum safety, reasoning, and stability thresholds. At inference time, STEER exposes a single, interpretable control parameter that maps a user-specified risk percentile to a selected persona, yielding a monotonic adjustment of decision conservativeness. On two clinical triage benchmarks, STEER achieves broader behavioral coverage compared to temperature-based sampling and static persona ensembles. Compared to a representative post-training method, STEER maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions. These results demonstrate STEER as a safety-preserving paradigm for risk control, capable of steering behavior without compromising domain competence.

</details>


### [21] ["I May Not Have Articulated Myself Clearly": Diagnosing Dynamic Instability in LLM Reasoning at Inference Time](https://arxiv.org/abs/2602.02863)
*Jinkun Chen,Fengxiang Cheng,Sijia Han,Vlado Keselj*

Main category: cs.AI

TL;DR: LLM推理失败可通过推理时token概率分布的不稳定性信号检测，该信号结合连续步分布偏移和不确定性，能可靠预测错误答案，且早期不稳定性可能自我修正，晚期不稳定性更可能导致失败。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理失败通常只在生成结束时测量，但许多失败表现为过程级的中断：模型在推理过程中"失去线索"。研究是否可以通过标准API中可用的推理时观测值（token对数概率）检测这种中断，无需任何训练或微调。

Method: 定义简单的不稳定性信号，结合连续步分布偏移（JSD）和不确定性（熵），通过峰值不稳定性强度总结每个轨迹，展示该信号能可靠预测失败。在GSM8K和HotpotQA数据集上验证，不稳定性强度能预测错误答案，并在不同模型规模上显示单调的桶级准确率下降。

Result: 不稳定性强度能可靠预测错误答案，具有高于随机水平的AUC值，并在不同模型规模上显示单调的准确率下降。关键发现：不稳定性并非总是有害的——早期不稳定性可能反映后续稳定化和正确答案（纠正性不稳定性），而晚期不稳定性更常导致失败（破坏性不稳定性），即使峰值幅度相似，表明可恢复性不仅取决于分布变化的强度，还取决于这种变化相对于剩余解码时域的发生时间。

Conclusion: 该方法提供了一种模型无关、无需训练、可复现的诊断视角，可作为推理过程监控的工具，而不是纠正或控制机制。研究揭示了LLM推理过程中不稳定性与失败之间的复杂关系，特别是时间维度对可恢复性的重要性。

Abstract: Reasoning failures in large language models (LLMs) are typically measured only at the end of a generation, yet many failures manifest as a process-level breakdown: the model "loses the thread" mid-reasoning. We study whether such breakdowns are detectable from inference-time observables available in standard APIs (token log probabilities), without any training or fine-tuning. We define a simple instability signal that combines consecutive-step distributional shift (JSD) and uncertainty (entropy), summarize each trace by its peak instability strength, and show that this signal reliably predicts failure. Across GSM8K and HotpotQA, instability strength predicts wrong answers with above-chance AUC and yields monotonic bucket-level accuracy decline at scale across model sizes. Crucially, we show that instability is not uniformly harmful: early instability can reflect subsequent stabilization and a correct final answer (\emph{corrective instability}), whereas late instability is more often followed by failure (\emph{destructive instability}), even at comparable peak magnitudes, indicating that recoverability depends not only on how strongly the distribution changes but also on when such changes occur relative to the remaining decoding horizon. The method is model-agnostic, training-free, and reproducible, and is presented as a diagnostic lens rather than a corrective or control mechanism.

</details>


### [22] [Aligning Language Model Benchmarks with Pairwise Preferences](https://arxiv.org/abs/2602.02898)
*Marco Gutierrez,Xinyi Leng,Hannah Cyberey,Jonathan Richard Schwarz,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.AI

TL;DR: 提出BenchAlign方法，通过有限部署信息自动更新离线基准测试，使其能更准确预测模型在真实场景中的性能偏好


<details>
  <summary>Details</summary>
Motivation: 当前语言模型基准测试虽然计算高效，但往往无法准确预测模型在实际应用中的真实效用，需要建立基准测试与实际性能之间的桥梁

Method: 提出BenchAlign方法，利用模型在基准问题上的表现和部署期间收集的模型排序对，学习与偏好对齐的基准问题权重，生成新的静态基准测试

Result: 对齐后的基准测试能准确根据人类偏好模型对未见模型进行排序，在不同规模模型上均有效，且保持可解释性

Conclusion: 这项工作揭示了将基准测试与实际人类偏好对齐的局限性，有助于加速模型开发向真实效用方向发展

Abstract: Language model benchmarks are pervasive and computationally-efficient proxies for real-world performance. However, many recent works find that benchmarks often fail to predict real utility. Towards bridging this gap, we introduce benchmark alignment, where we use limited amounts of information about model performance to automatically update offline benchmarks, aiming to produce new static benchmarks that predict model pairwise preferences in given test settings. We then propose BenchAlign, the first solution to this problem, which learns preference-aligned weight- ings for benchmark questions using the question-level performance of language models alongside ranked pairs of models that could be collected during deployment, producing new benchmarks that rank previously unseen models according to these preferences. Our experiments show that our aligned benchmarks can accurately rank unseen models according to models of human preferences, even across different sizes, while remaining interpretable. Overall, our work provides insights into the limits of aligning benchmarks with practical human preferences, which stands to accelerate model development towards real utility.

</details>


### [23] [Minimal Computational Preconditions for Subjective Perspective in Artificial Agents](https://arxiv.org/abs/2602.02902)
*Hongju Pae*

Main category: cs.AI

TL;DR: 该研究通过一个缓慢演化的全局潜状态来操作化人工智能中的主观视角，该状态调节快速策略动态但不直接优化行为后果，在环境变化中表现出方向依赖的滞后现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是在人工代理中实现主观视角的操作化，基于现象学启发的内部结构，探索机器系统中主观性的可测量特征。

Method: 方法采用一个缓慢演化的全局潜状态结构，该状态调节快速策略动态但不直接针对行为后果进行优化。在无奖励环境中测试，该环境包含制度变化，观察潜状态的滞后现象。

Result: 结果显示潜状态结构表现出方向依赖的滞后现象，而策略层面的行为保持相对反应性。这种滞后被认为是机器系统中视角类主观性的可测量特征。

Conclusion: 结论认为方向依赖的滞后现象构成了机器系统中视角类主观性的可测量特征，为在人工代理中实现主观性提供了新的操作化方法。

Abstract: This study operationalizes subjective perspective in artificial agents by grounding it in a minimal, phenomenologically motivated internal structure. The perspective is implemented as a slowly evolving global latent state that modulates fast policy dynamics without being directly optimized for behavioral consequences. In a reward-free environment with regime shifts, this latent structure exhibits direction-dependent hysteresis, while policy-level behavior remains comparatively reactive. I argue that such hysteresis constitutes a measurable signature of perspective-like subjectivity in machine systems.

</details>


### [24] [FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights](https://arxiv.org/abs/2602.02905)
*Zhen Wang,Fan Bai,Zhongyan Luo,Jinyan Su,Kaiser Sun,Xinle Yu,Jieyuan Liu,Kun Zhou,Claire Cardie,Mark Dredze,Eric P. Xing,Zhiting Hu*

Main category: cs.AI

TL;DR: FIRE-Bench是一个评估AI代理完整科学发现能力的基准测试，通过让代理重新发现已发表的机器学习研究成果来测试其从问题探索到实验设计、代码实现、执行和结论推导的全流程能力。


<details>
  <summary>Details</summary>
Motivation: 当前评估AI代理科学发现能力的基准存在局限性：要么过度依赖LLM-as-judge评估自动生成的研究输出，要么使用孤立性能指标作为科学洞察力的粗糙代理。需要更严谨的评估框架来验证代理能否进行可验证的科学发现。

Method: 提出FIRE-Bench基准，让AI代理重新发现近期高影响力机器学习研究中的已确立发现。代理仅获得从已验证研究中提取的高层次研究问题，必须自主探索想法、设计实验、实现代码、执行计划，并基于经验证据得出结论。

Result: 评估了基于前沿LLM（如GPT-5）的最先进代理，结果显示：完整周期的科学研究对当前代理系统仍具挑战性，即使最强代理的重新发现成功率也有限（<50 F1），运行间方差高，在实验设计、执行和基于证据的推理方面表现出重复性失败模式。

Conclusion: FIRE-Bench为衡量代理驱动科学发现的可靠性进展提供了一个严谨且具有诊断性的框架，揭示了当前代理系统在完整科学发现流程中的局限性。

Abstract: Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery end-to-end, but rigorously evaluating their capacity for verifiable discovery remains a central challenge. Existing benchmarks face a trade-off: they either heavily rely on LLM-as-judge evaluations of automatically generated research outputs or optimize convenient yet isolated performance metrics that provide coarse proxies for scientific insight. To address this gap, we introduce FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), a benchmark that evaluates agents through the rediscovery of established findings from recent, high-impact machine learning research. Agents are given only a high-level research question extracted from a published, verified study and must autonomously explore ideas, design experiments, implement code, execute their plans, and derive conclusions supported by empirical evidence. We evaluate a range of state-of-the-art agents with frontier LLMs backbones like gpt-5 on FIRE-Bench. Our results show that full-cycle scientific research remains challenging for current agent systems: even the strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning. FIRE-Bench provides a rigorous and diagnostic framework for measuring progress toward reliable agent-driven scientific discovery.

</details>


### [25] [Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs](https://arxiv.org/abs/2602.02909)
*Kiran Tomlinson,Tobias Schnabel,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: 论文研究了思维链推理所需的推理token数量与问题规模的关系，证明了三个典型任务需要Ω(n)个推理token，并通过实验验证了理论下界。


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽然能提升LLM性能，但带来了显著的延迟和计算成本。需要从理论上理解：随着输入规模增长，解决一个问题需要多少推理token？

Method: 扩展了有界注意力前缀预言机模型来量化任务所需的信息流，为三个BAPO-hard任务（二进制多数、三元组匹配、图可达性）证明了推理token的下界，并通过显式构造提供了匹配或接近匹配的上界。

Result: 证明了三个任务都需要Ω(n)个推理token（输入规模为n时），并通过前沿推理模型的实验显示这些任务上推理token大致呈线性缩放，当约束在较小的推理预算时会失败，与理论下界一致。

Conclusion: 研究结果识别了通过思维链进行推理时计算的基本瓶颈，为分析最优推理长度提供了原则性工具，有助于理解推理token数量与问题复杂度的基本关系。

Abstract: Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $Ω(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.

</details>


### [26] [DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution](https://arxiv.org/abs/2602.02919)
*Jiachen Jiang,Tianyu Ding,Zhihui Zhu*

Main category: cs.AI

TL;DR: DeltaEvolve：一种基于语义增量的动量驱动进化框架，用结构化语义增量替代完整代码历史，在减少token消耗的同时发现更好的解决方案


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的进化系统（如AlphaEvolve）依赖完整代码历史，存在上下文效率低和进化指导弱的问题。完整代码快照包含冗余实现细节，稀释了核心算法思想，难以提供清晰的进化灵感

Method: 将进化智能体形式化为期望最大化框架：LLM采样候选程序（E步），系统基于评估反馈更新控制上下文（M步）。提出DeltaEvolve框架，用结构化语义增量替代完整代码历史，捕捉连续节点间修改如何及为何影响性能。通过多级数据库和渐进披露机制组织语义增量，进一步减少输入token

Result: 在多个科学领域的任务上进行实证评估，表明该框架能够以更少的token消耗发现比基于完整代码的进化智能体更好的解决方案

Conclusion: DeltaEvolve通过语义增量方法有效解决了现有进化系统中上下文效率低的问题，提供了更清晰的进化指导，在减少计算资源的同时提升了解决方案的质量

Abstract: LLM-driven evolutionary systems have shown promise for automated science discovery, yet existing approaches such as AlphaEvolve rely on full-code histories that are context-inefficient and potentially provide weak evolutionary guidance. In this work, we first formalize the evolutionary agents as a general Expectation-Maximization framework, where the language model samples candidate programs (E-step) and the system updates the control context based on evaluation feedback (M-step). Under this view, constructing context via full-code snapshots constitutes a suboptimal M-step, as redundant implement details dilutes core algorithmic ideas, making it difficult to provide clear inspirations for evolution. To address this, we propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance. As programs are often decomposable, semantic delta usually contains many effective components which are transferable and more informative to drive improvement. By organizing semantic delta through multi-level database and progressive disclosure mechanism, input tokens are further reduced. Empirical evaluations on tasks across diverse scientific domains show that our framework can discover better solution with less token consumption over full-code-based evolutionary agents.

</details>


### [27] [UAT-LITE: Inference-Time Uncertainty-Aware Attention for Pretrained Transformers](https://arxiv.org/abs/2602.02952)
*Elias Hossain,Shubhashis Roy Dipta,Subash Neupane,Rajib Rana,Ravid Shwartz-Ziv,Ivan Garibay,Niloofar Yousefi*

Main category: cs.AI

TL;DR: UAT-LITE：一种推理时框架，通过蒙特卡洛dropout使预训练Transformer分类器的自注意力具备不确定性感知能力，无需修改权重或训练目标，显著改善校准误差和选择性预测性能。


<details>
  <summary>Details</summary>
Motivation: 神经NLP模型经常校准不良，对错误预测赋予过高置信度，这影响了选择性预测和高风险部署。现有后处理校准方法只调整输出概率而不改变内部计算，而集成和贝叶斯方法虽然改善不确定性但训练或存储成本高昂。

Method: 提出UAT-LITE推理时框架，通过蒙特卡洛dropout进行近似贝叶斯推断，使预训练Transformer分类器的自注意力具备不确定性感知能力。从随机前向传递中估计token级认知不确定性，并用其调制上下文化过程中的自注意力，无需修改预训练权重或训练目标。还引入层间方差分解来诊断预测不确定性如何在Transformer深度中累积。

Result: 在SQuAD 2.0可回答性、MNLI和SST-2任务上，UAT-LITE相对于微调的BERT-base基线平均减少约20%的预期校准误差，同时保持任务准确性，并改善了选择性预测和分布偏移下的鲁棒性。

Conclusion: UAT-LITE提供了一种轻量级、推理时的不确定性感知方法，通过调制自注意力显著改善模型校准，为高风险NLP应用提供了实用的不确定性量化解决方案。

Abstract: Neural NLP models are often miscalibrated, assigning high confidence to incorrect predictions, which undermines selective prediction and high-stakes deployment. Post-hoc calibration methods adjust output probabilities but leave internal computation unchanged, while ensemble and Bayesian approaches improve uncertainty at substantial training or storage cost. We propose UAT-LITE, an inference-time framework that makes self-attention uncertainty-aware using approximate Bayesian inference via Monte Carlo dropout in pretrained transformer classifiers. Token-level epistemic uncertainty is estimated from stochastic forward passes and used to modulate self-attention during contextualization, without modifying pretrained weights or training objectives. We additionally introduce a layerwise variance decomposition to diagnose how predictive uncertainty accumulates across transformer depth. Across the SQuAD 2.0 answerability, MNLI, and SST-2, UAT-LITE reduces Expected Calibration Error by approximately 20% on average relative to a fine-tuned BERT-base baseline while preserving task accuracy, and improves selective prediction and robustness under distribution shift.

</details>


### [28] [Generative Engine Optimization: A VLM and Agent Framework for Pinterest Acquisition Growth](https://arxiv.org/abs/2602.02961)
*Faye Zhang,Qianyu Cheng,Jasmine Wan,Vishwakarma Singh,Jinfeng Rao,Kofi Boakye*

Main category: cs.AI

TL;DR: Pinterest开发了GEO框架，通过视觉语言模型预测用户搜索意图，构建语义连贯的收藏页面，优化视觉内容在生成式搜索中的可见性，实现20%有机流量增长。


<details>
  <summary>Details</summary>
Motivation: 生成式AI搜索系统（如ChatGPT、Gemini、Claude）正在重塑内容发现方式，从传统的关键词匹配转向意图推断和多模态答案生成。这对视觉内容平台构成严峻挑战，因为单个图像缺乏生成式搜索所需的语义深度和权威信号，可能导致用户无需访问原始网站就能满足需求，造成平台被绕过。

Method: 1. 反向搜索设计：微调视觉语言模型（VLMs）预测用户实际会搜索什么，而非生成通用图像描述
2. AI代理挖掘实时互联网趋势，捕捉新兴搜索需求
3. 使用VLM生成的查询构建语义连贯的收藏页面，通过多模态嵌入创建可索引的聚合内容
4. 采用混合VLM和双塔ANN架构构建权威感知的互连结构，在数十亿视觉资产间传播信号

Result: 在数十亿图像和数千万收藏页面上大规模部署GEO框架，实现了20%的有机流量增长，贡献了数百万月活跃用户（MAU）的增长。

Conclusion: Pinterest GEO为视觉平台在生成式搜索时代蓬勃发展提供了一条原则性路径，通过反向搜索设计、实时趋势挖掘和权威信号传播，有效解决了视觉内容在生成式搜索中的可见性问题。

Abstract: Large Language Models are fundamentally reshaping content discovery through AI-native search systems such as ChatGPT, Gemini, and Claude. Unlike traditional search engines that match keywords to documents, these systems infer user intent, synthesize multimodal evidence, and generate contextual answers directly on the search page, introducing a paradigm shift from Search Engine Optimization (SEO) to Generative Engine Optimization (GEO). For visual content platforms hosting billions of assets, this poses an acute challenge: individual images lack the semantic depth and authority signals that generative search prioritizes, risking disintermediation as user needs are satisfied in-place without site visits.
  We present Pinterest GEO, a production-scale framework that pioneers reverse search design: rather than generating generic image captions describing what content is, we fine-tune Vision-Language Models (VLMs) to predict what users would actually search for, augmented this with AI agents that mine real-time internet trends to capture emerging search demand. These VLM-generated queries then drive construction of semantically coherent Collection Pages via multimodal embeddings, creating indexable aggregations optimized for generative retrieval. Finally, we employ hybrid VLM and two-tower ANN architectures to build authority-aware interlinking structures that propagate signals across billions of visual assets. Deployed at scale across billions of images and tens of millions of collections, GEO delivers 20\% organic traffic growth contributing to multi-million monthly active user (MAU) growth, demonstrating a principled pathway for visual platforms to thrive in the generative search era.

</details>


### [29] [Structuring Value Representations via Geometric Coherence in Markov Decision Processes](https://arxiv.org/abs/2602.02978)
*Zuyuan Zhang,Zeyu Fang,Tian Lan*

Main category: cs.AI

TL;DR: 提出GCR-RL方法，通过序理论视角将价值函数估计重构为学习偏序集，利用几何一致性正则化提升强化学习的稳定性和样本效率


<details>
  <summary>Details</summary>
Motivation: 现有方法利用几何特性（如对称结构、几何感知数据增强、结构限制）来稳定和加速强化学习，但缺乏从序理论角度系统性地处理价值函数估计的几何一致性

Method: 提出GCR-RL（几何一致性正则化强化学习），将价值函数估计重构为学习偏序集，通过超偏序集精化序列逐步学习额外的序关系，确保价值函数底层偏序集的几何一致性。开发了基于Q学习和actor-critic的两种算法实现超偏序集精化

Result: 理论分析了算法的性质和收敛速率，在多种任务上的实证评估显示，相比强基线方法，GCR-RL在样本效率和稳定性能方面均有显著提升

Conclusion: 从序理论视角重新审视强化学习，通过几何一致性正则化方法能够有效提升强化学习的稳定性和样本效率，为强化学习中的几何结构利用提供了新思路

Abstract: Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.

</details>


### [30] [Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget](https://arxiv.org/abs/2602.02983)
*Hanna M. Dettki,Charley M. Wu,Bob Rehder*

Main category: cs.AI

TL;DR: LLMs在因果推理任务中表现出比人类更规则化的推理策略，较少受到人类典型的共因偏误影响，但可能在不明确情境下失效。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在因果推理领域的表现，了解其推理机制是规范性计算、人类式捷径还是脆弱的模式匹配，为安全有效部署提供依据。

Method: 使用共因结构（C1→E←C2）的11个因果判断任务，比较20多个LLMs与匹配人类基线的表现，通过可解释模型压缩LLMs判断，并测试语义抽象和提示过载下的鲁棒性。

Result: LLMs表现出比人类更规则化的推理策略，较少受到人类典型的弱解释消除和马尔可夫违规等共因偏误影响。思维链（CoT）能提高许多LLMs的鲁棒性。

Conclusion: LLMs在已知人类偏误不受欢迎时可作为人类补充，但其规则化推理在不确定性情境下可能失效，需要深入理解LLMs推理策略以确保安全有效部署。

Abstract: Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \!\rightarrow\! E\! \leftarrow \!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.

</details>


### [31] [Large Language Models Can Take False First Steps at Inference-time Planning](https://arxiv.org/abs/2602.02991)
*Haijiang Yan,Jian-Qiao Zhu,Adam Sanborn*

Main category: cs.AI

TL;DR: LLMs在训练中获得了序列规划能力，但在推理时表现出短视行为。研究者提出贝叶斯解释：自生成上下文驱动规划偏移，导致看似受损的规划行为。


<details>
  <summary>Details</summary>
Motivation: 解释LLMs在训练中获得的序列规划能力与推理时表现出的短视、不一致规划行为之间的差距，为这一现象提供理论解释。

Method: 提出贝叶斯解释模型，将规划行为建立在不断演化的生成上下文基础上。通过两个受控实验验证：随机生成任务展示人类提示下的受限规划，以及自生成上下文积累时规划强度增加；高斯采样任务展示基于自生成序列时初始偏见的减少。

Result: 实验验证了提出的模型：随机生成任务显示随着自生成上下文积累，规划强度增加；高斯采样任务显示基于自生成序列时初始偏见减少。这些结果为LLMs在推理时如何进行前瞻规划提供了理论和实证证据。

Conclusion: LLMs推理时的规划行为差异源于自生成上下文驱动的规划偏移，而非能力受损。研究为理解LLMs规划机制提供了贝叶斯理论框架和实证支持。

Abstract: Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference.

</details>


### [32] [Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents](https://arxiv.org/abs/2602.02995)
*Sizhe Tang,Rongqian Chen,Tian Lan*

Main category: cs.AI

TL;DR: Agent Alpha框架通过步级蒙特卡洛树搜索整合生成、探索和评估，实现GUI代理的回归能力和前缀重用，在OSWorld基准上达到77%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理虽然通过轨迹级采样扩展了测试时计算，但缺乏回归能力，无法重用部分成功结果或从早期错误中恢复。

Method: 提出Agent Alpha统一框架，通过步级蒙特卡洛树搜索(MCTS)整合生成、探索和评估，采用alpha-UCT引导搜索实现主动规划，结合比较驱动评估和多样性约束扩展。

Result: 在OSWorld基准上达到约77%的成功率，显著优于同等计算条件下的轨迹级基线方法。

Conclusion: Agent Alpha通过步级MCTS实现了GUI代理的回归能力和高效规划，在保持紧凑搜索空间的同时显著提升了性能。

Abstract: While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\sim 77\%$, significantly outperforming trajectory-level baselines under equivalent compute.

</details>


### [33] [Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment](https://arxiv.org/abs/2602.03003)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 本文综述了可微分社会选择这一新兴范式，它将投票规则、机制和聚合过程构建为可从数据中优化的可学习、可微分模型，连接了机器学习、经济学和民主理论。


<details>
  <summary>Details</summary>
Motivation: 社会选择已从政治理论和经济学的边缘问题转变为现代机器学习系统的核心组成部分。从拍卖、资源分配到联邦学习、参与式治理和大语言模型对齐，机器学习系统越来越多地将异质偏好、激励和判断聚合为集体决策。许多当代机器学习系统已经实现了社会选择机制，但往往是隐式的且缺乏明确的规范审查。

Method: 本文综述了可微分社会选择这一新兴范式，将投票规则、机制和聚合过程构建为可学习、可微分的模型，可以从数据中进行优化。综合了拍卖、投票、预算分配、流动民主、去中心化聚合和逆向机制学习等领域的研究。

Result: 展示了经典公理和不可能性定理如何重新表现为目标、约束和优化权衡。传统的社会选择理论概念在机器学习框架下获得了新的表现形式和实现方式。

Conclusion: 提出了36个开放性问题，定义了机器学习、经济学和民主理论交叉领域的新研究议程。可微分社会选择为设计和分析机器学习中的集体决策机制提供了系统化的框架。

Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.
  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.

</details>


### [34] [Distilling LLM Reasoning into Graph of Concept Predictors](https://arxiv.org/abs/2602.03006)
*Ziyang Yu,Liang Zhao*

Main category: cs.AI

TL;DR: GCP是一种推理感知的主动蒸馏框架，通过将教师LLM的决策过程外化为有向无环图，并用模块化概念预测器在学生模型中镜像该图，从而提高样本效率、训练稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统主动蒸馏方法仅蒸馏最终标签，丢弃了中间推理信号，且无法诊断推理缺失和错误来源，限制了在推理延迟、计算和API成本方面的优化效果。

Method: 将教师LLM的决策过程外化为有向无环图，用模块化概念预测器构建学生模型；采用图感知的采集策略针对关键推理节点的不确定性和分歧；通过目标子模块重训练将下游损失归因于特定概念预测器，仅更新最有影响的模块。

Result: 在八个NLP分类基准测试中，GCP在有限标注预算下提升了性能，同时产生了更可解释和可控的训练动态。

Conclusion: GCP框架通过推理感知的主动蒸馏，有效解决了传统方法在样本效率、训练稳定性和可解释性方面的局限性，为部署LLM提供了更高效、可控的解决方案。

Abstract: Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.

</details>


### [35] [STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models](https://arxiv.org/abs/2602.03022)
*Jiliang Ni,Jiachen Pu,Zhongyi Yang,Jingfeng Luo,Conggang Hu*

Main category: cs.AI

TL;DR: STAR框架通过相似性引导的教师辅助精炼，将大语言模型的能力蒸馏到超小型模型中，在函数调用任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在函数调用中很重要，但规模太大阻碍广泛采用，需要将能力转移到小型模型。现有方法存在过拟合、训练不稳定、二元奖励对多解任务无效、技术难以协同等问题。

Method: 提出STAR框架：1) 约束知识蒸馏(CKD)：增强top-k前向KL散度，抑制自信的错误预测，确保训练稳定性同时保留下游RL的探索能力；2) 相似性引导RL(Sim-RL)：引入细粒度、基于相似性的奖励，通过评估生成输出与真实值的相似性提供连续丰富的信号。

Result: 在挑战性基准测试中，STAR模型在其规模类别中达到SOTA，显著优于基线。0.6B STAR模型在所有1B以下开源模型中表现最佳，甚至超越了一些更大规模的知名开源模型。

Conclusion: STAR展示了一个将LLM能力蒸馏到超小型模型的训练框架，为强大、可访问且高效的AI智能体铺平了道路。

Abstract: The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.

</details>


### [36] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: 提出RC-GRPO方法，通过奖励条件令牌增强多轮工具调用中的探索多样性，解决GRPO在组内奖励变化低时的停滞问题，在BFCLv4基准上超越基线并达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多轮工具调用对LLMs具有挑战性，因为奖励稀疏且探索成本高。传统的SFT+GRPO方法在组内奖励变化低时（如组内多数rollouts获得全0或全1奖励）会停滞，导致组归一化优势信息不足，更新消失。

Method: 提出RC-GRPO（奖励条件组相对策略优化）：1）首先微调奖励条件轨迹策略（RCTP），在混合质量轨迹中注入奖励目标特殊令牌（如<|high_reward|>, <|low_reward|>），使模型能按需生成不同质量轨迹；2）在RL阶段，在每个GRPO组内采样多样奖励令牌，并将rollouts条件化于采样令牌，提高组内多样性以增强优势增益。

Result: 在Berkeley Function Calling Leaderboard v4多轮基准测试中，该方法相比基线获得持续改进的性能，Qwen-2.5-7B-Instruct模型的表现甚至超越了所有闭源API模型。

Conclusion: RC-GRPO通过将探索视为可控的转向问题，使用离散奖励令牌有效解决了多轮工具调用中GRPO的停滞问题，显著提升了模型性能并达到SOTA水平。

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [37] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: MAS4TS：基于工具驱动的多智能体系统，通过视觉推理和潜在重构解决通用时间序列任务，在多个基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分析方法在整合直观视觉推理和跨任务泛化方面存在局限，缺乏自适应工具使用能力

Method: 基于Analyzer-Reasoner-Executor范式，集成智能体通信、视觉推理和潜在重构；使用视觉语言模型对时间序列图进行视觉推理提取时序结构，在潜在空间重构预测轨迹；三个专用智能体通过共享内存和门控通信协调，路由器选择任务特定工具链

Result: 在多个基准测试中取得最先进性能，展现出强大的泛化能力和高效推理

Conclusion: MAS4TS通过工具驱动的多智能体系统成功解决了时间序列分析中视觉推理与跨任务泛化的挑战

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [38] [KANFIS A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning](https://arxiv.org/abs/2602.03034)
*Binbin Yong,Haoran Pei,Jun Shen,Haoran Li,Qingguo Zhou,Zhao Su*

Main category: cs.AI

TL;DR: 提出KANFIS，一种紧凑的神经符号架构，通过加法聚合机制解决传统ANFIS规则爆炸问题，实现线性复杂度扩展，支持T1和IT2模糊系统，具有内在可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统ANFIS架构存在结构复杂性问题，基于乘积的推理机制在高维空间中导致规则数量指数级爆炸，需要更紧凑且可扩展的神经模糊推理系统。

Method: 提出Kolmogorov-Arnold神经模糊推理系统(KANFIS)，采用加法聚合机制，将模糊推理与加法函数分解统一，模型参数和规则复杂度随输入维度线性增长而非指数增长，支持T1和IT2模糊逻辑系统，使用稀疏掩码机制生成紧凑结构化规则集。

Result: KANFIS在性能上与代表性神经网络和神经模糊基线模型具有竞争力，同时保持内在可解释性，具有清晰的规则语义和透明的推理过程。

Conclusion: KANFIS通过加法聚合机制有效解决了传统ANFIS的规则爆炸问题，实现了线性复杂度扩展，同时保持了模糊推理的透明性和可解释性，为高维空间中的神经模糊系统提供了紧凑且高效的解决方案。

Abstract: Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabilities of neural network with the reasoning transparency of fuzzy logic. However, conventional ANFIS architectures suffer from structural complexity, where the product-based inference mechanism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with additive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than exponentially. Furthermore, KANFIS is compatible with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit modeling of uncertainty and ambiguity in fuzzy representations. By using sparse masking mechanisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demonstrate that KANFIS achieves competitive performance against representative neural and neuro-fuzzy baselines.

</details>


### [39] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文系统研究了多智能体系统中的过程验证方法，发现现有验证方法效果不稳定，LLM-as-a-Judge表现相对较好，但可靠的过程验证仍是开放挑战。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体系统在推理轨迹上存在高方差，过程验证在一般推理场景中显示出潜力，但其在多智能体系统中的实际效果尚不明确。

Method: 提出MAS-ProVe框架，系统研究三种验证范式（LLM-as-a-Judge、奖励模型、过程奖励模型），在两个验证粒度（智能体级和迭代级）上评估，涵盖五个代表性验证器和四种上下文管理策略，在六个多智能体框架和多个推理基准上进行实验。

Result: 过程级验证不能持续提升性能且常表现出高方差；LLM-as-a-Judge通常优于基于奖励的方法；训练过的法官优于通用LLM；LLM作为法官与作为单智能体的性能差距较小；存在上下文长度与性能的权衡。

Conclusion: 多智能体系统的有效且鲁棒的过程验证仍是一个开放挑战，需要超越当前范式的进一步进展。

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [40] [De-conflating Preference and Qualification: Constrained Dual-Perspective Reasoning for Job Recommendation with Large Language Models](https://arxiv.org/abs/2602.03097)
*Bryce Kan,Wei Yang,Emily Nguyen,Ganghui Yi,Bowen Yi,Chenxiao Yu,Yan Liu*

Main category: cs.AI

TL;DR: JobRec是一个生成式职位推荐框架，通过约束双视角推理分离偏好与资格，解决传统方法中两者混淆的问题，提供可控的职业匹配。


<details>
  <summary>Details</summary>
Motivation: 专业职位推荐涉及复杂的二分匹配过程，需要协调候选人的主观偏好和雇主的客观资格要求。现有方法通常将这两个决策维度压缩为单一交互信号，导致在招聘漏斗审查下的混淆监督，限制了策略可控性。

Method: JobRec框架包含：1) 统一语义对齐模式，将候选人和职位属性对齐到结构化语义层；2) 两阶段协作训练策略，学习解耦的专家分别推断偏好和资格；3) 基于拉格朗日的策略对齐模块，在明确资格要求下优化推荐，实现可控权衡。为缓解数据稀缺，构建了专家精炼的合成数据集。

Result: 实验表明，JobRec持续优于强基线方法，并为策略感知的专业匹配提供了改进的可控性。

Conclusion: JobRec通过解耦偏好和资格的生成式推荐框架，解决了传统方法中的混淆监督问题，实现了更可控、更有效的专业职位匹配。

Abstract: Professional job recommendation involves a complex bipartite matching process that must reconcile a candidate's subjective preference with an employer's objective qualification. While Large Language Models (LLMs) are well-suited for modeling the rich semantics of resumes and job descriptions, existing paradigms often collapse these two decision dimensions into a single interaction signal, yielding confounded supervision under recruitment-funnel censoring and limiting policy controllability. To address these challenges, We propose JobRec, a generative job recommendation framework for de-conflating preference and qualification via constrained dual-perspective reasoning. JobRec introduces a Unified Semantic Alignment Schema that aligns candidate and job attributes into structured semantic layers, and a Two-Stage Cooperative Training Strategy that learns decoupled experts to separately infer preference and qualification. Building on these experts, a Lagrangian-based Policy Alignment module optimizes recommendations under explicit eligibility requirements, enabling controllable trade-offs. To mitigate data scarcity, we construct a synthetic dataset refined by experts. Experiments show that JobRec consistently outperforms strong baselines and provides improved controllability for strategy-aware professional matching.

</details>


### [41] [Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment](https://arxiv.org/abs/2602.03100)
*Jingnan Zheng,Yanzhen Luo,Jingjun Xu,Bingnan Liu,Yuxin Chen,Chenhang Cui,Gelei Deng,Chaochao Lu,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Risky-Bench是一个系统化的智能体安全评估框架，通过领域无关的安全原则和上下文感知的安全标准，在真实世界部署场景下评估LLM智能体的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有智能体安全评估方法存在局限性：1）依赖针对特定智能体设置的风险导向任务，安全风险空间覆盖有限；2）无法评估智能体在复杂真实世界部署中长期交互任务执行中的安全行为；3）对特定智能体设置的专门化限制了跨不同智能体配置的适应性。

Method: Risky-Bench围绕领域无关的安全原则组织评估，推导出上下文感知的安全标准来界定安全空间，并通过在不同威胁假设下的真实任务执行，系统性地评估整个安全空间的风险。

Result: 在生活辅助智能体设置中应用Risky-Bench，发现在真实执行条件下，最先进的智能体存在显著的安全风险。该框架作为一个结构化的评估流程，不仅限于生活辅助场景，可适应其他部署设置来构建环境特定的安全评估。

Conclusion: Risky-Bench提供了一个可扩展的智能体安全评估方法学，能够系统评估LLM智能体在真实世界部署中的安全风险，填补了现有评估方法的空白，并具有跨不同智能体配置的适应性。

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.

</details>


### [42] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: MAFBench是一个统一评估套件，用于系统比较多智能体LLM框架的架构设计对性能的影响，发现框架级设计选择可导致延迟增加100倍以上、规划准确率下降30%、协调成功率从90%降至30%以下。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM框架广泛使用，但不同框架的架构设计对系统性能的影响缺乏深入理解。现有基准测试仅关注单一能力，缺乏标准化的框架级评估，而架构选择本身就能导致数量级的性能差异。

Method: 提出多智能体LLM框架的架构分类法，开发MAFBench统一评估套件，将现有基准测试集成到标准化执行流程中，在多个广泛使用的框架上进行受控实证研究。

Result: 框架级设计选择可导致延迟增加100倍以上、规划准确率下降高达30%、协调成功率从90%以上降至30%以下。研究结果转化为具体的架构设计原则和框架选择指导。

Conclusion: 多智能体LLM框架的架构设计对系统性能有重大影响，MAFBench提供了系统评估工具，研究结果为框架设计和选择提供了实证依据，并指出了未来研究方向。

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [43] [General Agents Contain World Models, even under Partial Observability and Stochasticity](https://arxiv.org/abs/2602.03146)
*Santiago Cifuentes*

Main category: cs.AI

TL;DR: 本文扩展了先前关于智能体必然学习环境模型的理论，从确定性智能体扩展到随机性智能体，从完全可观测环境扩展到部分可观测环境，证明了随机性智能体也无法避免学习其环境模型。


<details>
  <summary>Details</summary>
Motivation: 先前研究证明了在特定框架下，几乎所有最优且通用的确定性智能体必然包含足够的环境知识，但这些结果依赖于智能体是确定性的且环境完全可观测的假设。本文旨在移除这两个限制性假设。

Method: 通过理论扩展，将原定理推广到在部分可观测环境中运行的随机性智能体。同时通过弱化"通用性"的概念，证明即使是能力较弱的智能体也包含其操作环境的世界模型。

Result: 证明了随机性智能体在部分可观测环境中也无法避免学习其环境模型，这从根本上表明随机化使用不能避免环境学习。同时通过弱化通用性要求，扩展了结果的适用范围。

Conclusion: 随机性智能体在部分可观测环境中也必然学习其环境模型，这一发现加深了对智能体能力与局限性的理解，为智能体模型学习提供了更普遍的理论基础。

Abstract: Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.
  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.

</details>


### [44] [Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration](https://arxiv.org/abs/2602.03151)
*Wei Dai,Haoyu Wang,Honghao Chang,Lijun He,Fan Li,Jian Sun,Haixia Bi*

Main category: cs.AI

TL;DR: 提出一种通用的缺失模态恢复策略，使用增强扩散模型作为可插拔模块，通过动态模态门控和跨模态互学习机制，在模态不完整时恢复精确语义并保持VLM泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型(VLMs)在模态不完整时性能急剧下降。基于提示的方法难以恢复缺失的关键特征并损害VLM泛化；基于插补的方法缺乏有效指导，容易生成语义无关的噪声。需要一种既能恢复精确语义又能保持VLM泛化能力的方法。

Method: 提出通用缺失模态恢复策略：1) 使用增强扩散模型作为可插拔的中期训练模块恢复缺失特征；2) 动态模态门控：自适应利用条件特征引导生成语义一致的特征；3) 跨模态互学习机制：桥接双编码器的语义空间实现双向对齐。

Result: 在基准数据集上的零样本评估显示，该方法优于现有基线方法。大量实验和消融研究证实该模型在缺失模态场景下是VLM的鲁棒且可扩展的扩展，确保在不同缺失率和环境下的可靠性。

Conclusion: 提出的通用缺失模态恢复策略通过增强扩散模型、动态模态门控和跨模态互学习机制，有效解决了VLM在模态不完整时的性能下降问题，实现了语义精确恢复和模型泛化能力的平衡。

Abstract: Vision Language Models (VLMs) typically assume complete modality input during inference. However, their effectiveness drops sharply when certain modalities are unavailable or incomplete. Current research primarily faces two dilemmas: Prompt-based methods struggle to restore missing yet indispensable features and impair generalization of VLMs. Imputation-based approaches, lacking effective guidance, are prone to generating semantically irrelevant noise. Restoring precise semantics while sustaining VLM generalization remains challenging. Therefore, we propose a general missing modality restoration strategy in this paper. We introduce an enhanced diffusion model as a pluggable mid-stage training module to effectively restore missing features. Our strategy introduces two key innovations: (I) Dynamic Modality Gating, which adaptively leverages conditional features to steer the generation of semantically consistent features; (II) Cross-Modal Mutual Learning mechanism, which bridges the semantic spaces of dual encoders to achieve bidirectional alignment. Zero-shot evaluations across benchmark datasets demonstrate that our approach outperforms existing baseline methods. Extensive experiments and ablation studies confirm our model as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability across diverse missing rates and environments. Our code and models will be publicly available.

</details>


### [45] [VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models](https://arxiv.org/abs/2602.03160)
*Woojin Kim,Sieun Hyeon,Jusang Oh,Jaeyoung Do*

Main category: cs.AI

TL;DR: VALUEFLOW：首个统一框架，通过校准强度控制实现价值提取、评估和引导，解决LLM与人类价值对齐的挑战


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法存在三个主要问题：1）价值提取忽略层次结构；2）评估只能检测存在性而非校准强度；3）LLM在受控强度下的可引导性理解不足

Method: 提出VALUEFLOW框架，包含三个组件：1）HIVES层次价值嵌入空间；2）VIDB大规模价值标注文本数据库；3）基于锚点的评估器，通过排名产生一致强度分数

Result: 在10个模型和4个价值理论上的大规模研究，发现了可引导性的不对称性以及多价值控制的组合规律

Conclusion: VALUEFLOW为评估和控制价值强度建立了可扩展的基础设施，推动了LLM的多元对齐

Abstract: Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.

</details>


### [46] [Beyond Quantity: Trajectory Diversity Scaling for Code Agents](https://arxiv.org/abs/2602.03219)
*Guhong Chen,Chenghao Sun,Cheng Fu,Qiyao Wang,Zhihong Huang,Chaopeng Wei,Guangxu Chen,Feiteng Fang,Ahmadreza Argha,Bing Zhao,Xander Xu,Qi Han,Hamid Alinejad-Rokny,Qiang Qu,Binhua Li,Shiwen Ni,Min Yang,Hu Wei,Yongbin Li*

Main category: cs.AI

TL;DR: TDScaling是一个基于轨迹多样性而非数量的代码智能体数据合成框架，通过提升轨迹多样性而非增加数据量来改善性能-成本权衡，在固定训练预算下获得更大收益。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型通过MCP演化为工具交互智能体时，其泛化能力受到低质量合成数据和数量扩展收益递减的限制，且以数量为中心的扩展存在早期瓶颈，未能充分利用轨迹数据。

Method: TDScaling包含四个创新：1）捕捉真实服务逻辑依赖的业务集群机制；2）确保轨迹一致性的蓝图驱动多智能体范式；3）使用领域熵、推理模式熵和累积动作复杂度防止模式崩溃的自适应进化机制；4）减轻内在编码能力灾难性遗忘的沙盒化代码工具。

Result: 在通用工具使用基准（BFCL、tau^2-Bench）和代码智能体任务（RebenchT、CodeCI、BIRD）上的实验表明，TDScaling实现了双赢：既提升了工具使用泛化能力，又增强了内在编码能力。

Conclusion: TDScaling通过轨迹多样性扩展而非数量扩展，为代码智能体训练提供了更好的性能-成本权衡，计划发布完整代码库和包含30,000+工具集群的合成数据集。

Abstract: As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.

</details>


### [47] [TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking](https://arxiv.org/abs/2602.03224)
*Yu Cheng,Jiuan Zhou,Yongkang Hu,Yihang Chen,Huichi Zhou,Mingang Chen,Zhizhong Zhang,Kun Shao,Yuan Xie,Zhaoxia Yin*

Main category: cs.AI

TL;DR: 论文提出TAME框架解决智能体在任务演进过程中出现的记忆错误演化问题，通过双记忆系统分别优化执行和评估能力，在保持任务性能的同时提升可信度。


<details>
  <summary>Details</summary>
Motivation: 智能体在测试时间通过记忆演化积累经验是实现AGI的关键，但即使在良性任务演进中，智能体的安全对齐仍然脆弱，这种现象被称为"智能体记忆错误演化"。需要评估和解决这一安全问题。

Method: 提出TAME双记忆演化框架：1) 执行器记忆演化，通过提炼可泛化方法提升任务性能；2) 评估器记忆演化，基于历史反馈优化安全和任务效用评估。通过记忆过滤、草稿生成、可信度精炼、执行和双轨记忆更新的闭环流程。

Result: 构建了Trust-Memevo基准评估良性任务演进中的多维可信度，发现各任务领域和评估设置中可信度普遍下降。TAME实验证明能够缓解记忆错误演化，在可信度和任务性能上实现联合提升。

Conclusion: TAME框架通过分离演化执行和评估记忆，在保持任务效用的同时保护可信度，为解决智能体记忆错误演化问题提供了有效方案，为实现安全的AGI演进提供了重要思路。

Abstract: Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.

</details>


### [48] [The Necessity of a Unified Framework for LLM-Based Agent Evaluation](https://arxiv.org/abs/2602.03238)
*Pengyu Zhu,Li Sun,Philip S. Yu,Sen Su*

Main category: cs.AI

TL;DR: 本文提出需要统一的智能体评估框架，以解决当前评估中存在的系统提示、工具配置、环境动态等混杂因素导致的标准化缺失问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，通用智能体取得了根本性进步，但评估这些智能体面临独特挑战。当前评估存在混杂因素：系统提示、工具配置、环境动态等，导致难以将性能提升归因于模型本身。缺乏标准化导致不公平、不透明和不可复现的结果。

Method: 提出标准化智能体评估的提案，旨在建立统一的评估框架，解决现有评估中的标准化缺失问题。

Result: 提出了标准化智能体评估的提案，但论文摘要中未提供具体的实验结果或框架实施细节。

Conclusion: 统一的评估框架对于智能体评估的严谨发展至关重要，标准化是解决当前评估中不公平、不透明和不可复现问题的关键。

Abstract: With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.

</details>


### [49] [Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning](https://arxiv.org/abs/2602.03249)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Wenlei Shi,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.AI

TL;DR: Accordion-Thinking框架让LLM学习通过动态总结来自我调节推理步骤粒度，实现推理上下文压缩，在保持准确性的同时提升3倍吞吐量


<details>
  <summary>Details</summary>
Motivation: 传统的长链式思维推理虽然能提升推理能力，但面临KV缓存线性增长和注意力复杂度二次方增长的实践限制，需要更高效的推理机制

Method: 提出Accordion-Thinking端到端框架，让LLM学习通过动态总结来调节推理步骤粒度，采用Fold推理模式定期总结思维过程并丢弃历史思考，应用强化学习进一步激励这种能力

Result: 模型学会将关键推理信息编码到紧凑总结中，实现推理上下文有效压缩；在48GB GPU内存配置下保持准确性的同时达到3倍吞吐量；结构化的步骤总结提供人类可读的推理过程记录

Conclusion: 通过学习的自我压缩，LLM能够以最小的依赖token开销处理复杂推理任务而不影响解决方案质量，证明了推理上下文压缩的可行性

Abstract: Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.

</details>


### [50] [LPS-Bench: Benchmarking Safety Awareness of Computer-Use Agents in Long-Horizon Planning under Benign and Adversarial Scenarios](https://arxiv.org/abs/2602.03255)
*Tianyu Chen,Chujia Hu,Ge Gao,Dongrui Liu,Xia Hu,Wenjie Wang*

Main category: cs.AI

TL;DR: LPS-Bench是一个评估基于MCP的计算机使用代理在长时程任务中规划时安全意识的基准测试，涵盖7个任务领域、9种风险类型的65个场景，揭示了现有代理在安全行为维护方面的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注短时程或GUI任务，评估执行时错误但忽视了规划时风险预测能力。计算机使用代理面临模糊指令触发有害操作和对抗性用户操纵工具执行的风险，需要专门的规划时安全评估基准。

Method: 提出了LPS-Bench基准测试，包含7个任务领域、9种风险类型的65个场景，采用多代理自动化管道进行可扩展数据生成，并使用LLM-as-a-judge评估协议通过规划轨迹评估安全意识。

Result: 实验显示现有计算机使用代理在维持安全行为方面存在严重缺陷，无法有效应对规划时安全风险。作者进一步分析了风险并提出了改进MCP-based CUA系统长时程规划安全的缓解策略。

Conclusion: LPS-Bench填补了规划时安全评估的空白，揭示了现有计算机使用代理的安全意识不足，为改进长时程任务中的安全规划提供了基准和缓解策略。

Abstract: Computer-use agents (CUAs) that interact with real computer systems can perform automated tasks but face critical safety risks. Ambiguous instructions may trigger harmful actions, and adversarial users can manipulate tool execution to achieve malicious goals. Existing benchmarks mostly focus on short-horizon or GUI-based tasks, evaluating on execution-time errors but overlooking the ability to anticipate planning-time risks. To fill this gap, we present LPS-Bench, a benchmark that evaluates the planning-time safety awareness of MCP-based CUAs under long-horizon tasks, covering both benign and adversarial interactions across 65 scenarios of 7 task domains and 9 risk types. We introduce a multi-agent automated pipeline for scalable data generation and adopt an LLM-as-a-judge evaluation protocol to assess safety awareness through the planning trajectory. Experiments reveal substantial deficiencies in existing CUAs' ability to maintain safe behavior. We further analyze the risks and propose mitigation strategies to improve long-horizon planning safety in MCP-based CUA systems. We open-source our code at https://github.com/tychenn/LPS-Bench.

</details>


### [51] [CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs](https://arxiv.org/abs/2602.03263)
*Yuxuan Liu,Yuntian Shi,Kun Wang,Haoting Shen,Kun Yang*

Main category: cs.AI

TL;DR: CSR-Bench是一个评估多模态大语言模型跨模态可靠性的基准测试，通过四种压力测试模式（安全、过度拒绝、偏见、幻觉）评估模型在需要图像-文本联合理解任务中的表现，发现现有模型存在系统性跨模态对齐差距。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然支持文本和图像交互，但其安全行为可能由单模态捷径而非真正的联合意图理解驱动。需要评估模型在需要跨模态整合理解任务中的可靠性，以诊断模态引起的行为变化。

Method: 引入CSR-Bench基准，包含四种压力测试交互模式：安全、过度拒绝、偏见、幻觉，涵盖61种细粒度类型。每个实例都需要图像-文本联合解释，并提供配对的纯文本控制组来诊断模态引起的行为变化。评估了16个最先进的多模态大语言模型。

Result: 观察到系统性跨模态对齐差距：模型表现出弱安全意识、在干扰下强烈的语言主导性，以及从纯文本控制组到多模态输入的性能一致下降。还观察到减少过度拒绝与保持安全、非歧视行为之间的明显权衡，表明某些表面安全提升可能来自拒绝导向的启发式方法而非鲁棒的意图理解。

Conclusion: 多模态大语言模型在跨模态可靠性方面存在显著缺陷，需要更鲁棒的跨模态意图理解方法，而非依赖单模态捷径或拒绝启发式。CSR-Bench为评估和改进多模态模型的跨模态对齐提供了重要工具。

Abstract: Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we additionally provide paired text-only controls to diagnose modality-induced behavior shifts. We evaluate 16 state-of-the-art MLLMs and observe systematic cross-modal alignment gaps. Models show weak safety awareness, strong language dominance under interference, and consistent performance degradation from text-only controls to multimodal inputs. We also observe a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some apparent safety gains may come from refusal-oriented heuristics rather than robust intent understanding. WARNING: This paper contains unsafe contents.

</details>


### [52] [Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis](https://arxiv.org/abs/2602.03279)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Xuan Ren,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: Agentic Proposing框架通过建模问题合成为目标驱动的序列决策过程，使用专门代理动态选择和组合模块化推理技能，生成高质量可验证的训练数据，显著提升下游求解器性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的复杂推理能力提升依赖于高质量可验证数据集，但人工标注成本高昂且难以扩展。现有合成方法面临两难：保持结构有效性会限制问题复杂度，而放松约束增加难度则导致不一致或不可解实例。

Method: 提出Agentic Proposing框架，将问题合成建模为目标驱动的序列决策过程，使用专门代理通过内部反思和工具使用的迭代工作流动态选择和组合模块化推理技能。开发Agentic-Proposer-4B模型，采用多粒度策略优化（MGPO）生成数学、编程和科学领域的高精度可验证训练轨迹。

Result: 基于代理合成数据训练的下游求解器显著优于领先基线，并展现出强大的跨领域泛化能力。仅使用11,000条合成轨迹训练的30B求解器在AIME25上达到91.6%的SOTA准确率，媲美GPT-5等前沿专有模型。

Conclusion: 少量高质量合成信号可有效替代大规模人工标注数据集，证明了Agentic Proposing框架在生成可验证训练数据方面的有效性，为复杂推理任务的模型训练提供了高效解决方案。

Abstract: Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.

</details>


### [53] [MeetBench-XL: Calibrated Multi-Dimensional Evaluation and Learned Dual-Policy Agents for Real-Time Meetings](https://arxiv.org/abs/2602.03285)
*Yuelin Hu,Jun Xu,Bingcong Lu,Zhengxue Cheng,Hongwei Hu,Ronghua Wu,Li Song*

Main category: cs.AI

TL;DR: 论文提出MeetAll数据集和MeetMaster XL智能体框架，解决企业会议AI助手在延迟、成本和隐私约束下的多任务处理问题，通过双语多模态数据集和双策略路由机制优化质量-延迟权衡。


<details>
  <summary>Details</summary>
Motivation: 现有会议基准主要关注简化的问答任务，无法反映真实企业工作流程中多利益相关者协作、长时态上下文和工具增强推理的需求。企业会议环境需要AI助手在严格延迟、成本和隐私约束下处理多样化操作任务。

Method: 1) 引入MeetAll双语多模态数据集，基于231场企业会议(140小时)，采用企业验证的协议注入问题；2) 提出MeetBench XL多维度评估协议；3) 设计MeetMaster XL双策略智能体，通过轻量级分类器优化快速/慢速推理路径的路由和工具调用。

Result: 实验显示，相比商业系统，该方法获得一致性能提升。轻量级分类器实现准确路由，最小化开销，在质量-延迟权衡上优于单模型基线。通过消融实验、鲁棒性测试和实际部署案例验证有效性。

Conclusion: 该研究填补了企业会议AI助手基准的空白，通过接地气的数据集和学习的智能体框架，解决了真实企业工作流程中的多维度需求，在严格约束下实现了优越的性能表现。

Abstract: Enterprise meeting environments require AI assistants that handle diverse operational tasks, from rapid fact checking during live discussions to cross meeting analysis for strategic planning, under strict latency, cost, and privacy constraints. Existing meeting benchmarks mainly focus on simplified question answering and fail to reflect real world enterprise workflows, where queries arise organically from multi stakeholder collaboration, span long temporal contexts, and require tool augmented reasoning.
  We address this gap through a grounded dataset and a learned agent framework. First, we introduce MeetAll, a bilingual and multimodal corpus derived from 231 enterprise meetings totaling 140 hours. Questions are injected using an enterprise informed protocol validated by domain expert review and human discriminability studies. Unlike purely synthetic benchmarks, this protocol is grounded in four enterprise critical dimensions: cognitive load, temporal context span, domain expertise, and actionable task execution, calibrated through interviews with stakeholders across finance, healthcare, and technology sectors.
  Second, we propose MeetBench XL, a multi dimensional evaluation protocol aligned with human judgment that measures factual fidelity, intent alignment, response efficiency, structural clarity, and completeness. Third, we present MeetMaster XL, a learned dual policy agent that jointly optimizes query routing between fast and slow reasoning paths and tool invocation, including retrieval, cross meeting aggregation, and web search. A lightweight classifier enables accurate routing with minimal overhead, achieving a superior quality latency tradeoff over single model baselines. Experiments against commercial systems show consistent gains, supported by ablations, robustness tests, and a real world deployment case study.Resources: https://github.com/huyuelin/MeetBench.

</details>


### [54] [Rejecting Arguments Based on Doubt in Structured Bipolar Argumentation](https://arxiv.org/abs/2602.03286)
*Michael A. Müller,Srdjan Vesic,Bruno Yun*

Main category: cs.AI

TL;DR: 提出结构化双极论证框架(SBAF)，包含攻击和支持关系，引入允许基于怀疑拒绝论证的语义，并提供语言扩展和论证扩展两种语义。


<details>
  <summary>Details</summary>
Motivation: 现有计算论证方法忽视了两个重要哲学和语言学观点：1) 智能体可以基于怀疑理性拒绝某些论证，不必接受所有可辩护的论证；2) 有时更自然的思考单位是单个句子或主张，而非整个论证。

Method: 定义结构化双极论证框架(SBAF)，其中论证由句子组成，包含攻击和支持关系。提出新的语义学，不强制接受所有被辩护的论证，同时提供论证扩展和语言扩展两种语义。

Result: 提出的语义位于抽象论证的可接受语义和完全语义之间，能够为现有方法提供新视角，如确定忽略支持关系的条件，并证明演绎支持语义是本方法的特例。

Conclusion: 该方法将哲学和语言学洞见融入计算论证，提供更符合实际推理的框架，允许基于怀疑的拒绝，并支持句子层面的分析，为论证理论提供了更丰富的工具。

Abstract: This paper develops a new approach to computational argumentation that is informed by philosophical and linguistic views. Namely, it takes into account two ideas that have received little attention in the literature on computational argumentation: First, an agent may rationally reject an argument based on mere doubt, thus not all arguments they could defend must be accepted; and, second, that it is sometimes more natural to think in terms of which individual sentences or claims an agent accepts in a debate, rather than which arguments. In order to incorporate these two ideas into a computational approach, we first define the notion of structured bipolar argumentation frameworks (SBAFs), where arguments consist of sentences and we have both an attack and a support relation between them. Then, we provide semantics for SBAFs with two features: (1) Unlike with completeness-based semantics, our semantics do not force agents to accept all defended arguments. (2) In addition to argument extensions, which give acceptable sets of arguments, we also provide semantics for language extensions that specify acceptable sets of sentences. These semantics represent reasonable positions an agent might have in a debate. Our semantics lie between the admissible and complete semantics of abstract argumentation. Further, our approach can be used to provide a new perspective on existing approaches. For instance, we can specify the conditions under which an agent can ignore support between arguments (i.e. under which the use of abstract argumentation is warranted) and we show that deductive support semantics is a special case of our approach.

</details>


### [55] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Memora是一种谐波记忆表示方法，通过抽象与具体细节的结构化平衡来扩展智能体记忆系统，在保持检索效率的同时保留推理所需的细粒度信息。


<details>
  <summary>Details</summary>
Motivation: 智能体记忆系统需要处理不断增长的信息，同时支持高效、上下文感知的检索。现有的抽象方法虽然有助于扩展，但往往以牺牲具体细节为代价，而这些细节对于有效推理至关重要。

Method: 提出Memora谐波记忆表示：1）通过主要抽象索引具体记忆值，将相关更新整合为统一记忆条目；2）使用线索锚点扩展跨不同方面的检索访问，连接相关记忆；3）采用主动利用记忆连接的检索策略，超越直接语义相似性。

Result: 理论上证明标准RAG和基于知识图谱的记忆系统是Memora的特殊情况。在LoCoMo和LongMemEval基准测试中达到新的最先进水平，在记忆扩展时表现出更好的检索相关性和推理效果。

Conclusion: Memora通过结构化平衡抽象与具体细节，解决了智能体记忆扩展中的核心挑战，在保持检索效率的同时保留了推理所需的细粒度信息，为大规模记忆系统提供了有效解决方案。

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [56] [MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis](https://arxiv.org/abs/2602.03340)
*Xiao Sun,Yuming Yang,Junnan Zhu,Jiang Zhong,Xinyu Zhou,Kaiwen Wei*

Main category: cs.AI

TL;DR: 提出了首个面向真实临床场景的精神障碍诊断基准MentalDx Bench，并开发了专门的精神科诊断模型MentalSeek-Dx，通过监督轨迹构建和课程强化学习实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在精神健康评估中存在生态效度不足和诊断监督粒度不够的问题，需要建立更贴近真实临床场景的基准来提升LLM在精神科诊断中的实用性。

Method: 1) 构建MentalDx Bench基准：包含712份去标识化电子健康记录，由精神科医生按照ICD-11标准标注76种障碍；2) 开发MentalSeek-Dx模型：通过监督轨迹构建和课程强化学习训练医学专用LLM，模拟临床假设-演绎推理过程。

Result: 评估18个LLM发现存在范式错位：粗粒度诊断表现良好但细粒度障碍诊断系统失败；MentalSeek-Dx仅用14B参数就达到了SOTA性能，建立了可靠的临床诊断框架。

Conclusion: MentalDx Bench填补了精神科诊断基准的空白，MentalSeek-Dx通过模拟临床推理过程显著提升了诊断准确性，为可靠的AI辅助精神科诊断提供了临床基础。

Abstract: Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.

</details>


### [57] [Building Interpretable Models for Moral Decision-Making](https://arxiv.org/abs/2602.03351)
*Mayank Goel,Aritra Das,Paras Chopra*

Main category: cs.AI

TL;DR: 研究者构建了一个小型Transformer模型来研究神经网络如何在电车困境中做出道德决策，该模型在道德机器数据上达到77%准确率，并通过可解释性技术揭示了道德推理在神经网络中的分布模式。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何进行道德决策，特别是在电车困境这类道德两难情境中，理解神经网络内部的道德推理机制和潜在的偏见分布。

Method: 构建了一个定制的2层Transformer模型，使用嵌入编码结构化场景信息（受影响者、人数、结果归属），在道德机器数据集上进行训练，并应用多种可解释性技术分析网络内部机制。

Result: 模型在道德机器数据上达到77%的准确率，同时保持足够小的规模以便详细分析。研究发现道德推理分布在网络的不同部分，偏见定位在特定的计算阶段。

Conclusion: 通过构建小型可分析的Transformer模型，可以深入研究神经网络中的道德决策过程，揭示道德推理在神经网络中的分布式表征和偏见定位机制。

Abstract: We build a custom transformer model to study how neural networks make moral decisions on trolley-style dilemmas. The model processes structured scenarios using embeddings that encode who is affected, how many people, and which outcome they belong to. Our 2-layer architecture achieves 77% accuracy on Moral Machine data while remaining small enough for detailed analysis. We use different interpretability techniques to uncover how moral reasoning distributes across the network, demonstrating that biases localize to distinct computational stages among other findings.

</details>


### [58] [GFlowPO: Generative Flow Network as a Language Model Prompt Optimizer](https://arxiv.org/abs/2602.03358)
*Junmo Cho,Suhan Kim,Sangjune An,Minsu Kim,Dong Bok Lee,Heejun Lee,Sung Ju Hwang,Hae Beom Lee*

Main category: cs.AI

TL;DR: GFlowPO：基于GFlowNet的样本高效提示优化框架，通过后验推断和动态记忆更新提升提示搜索效果


<details>
  <summary>Details</summary>
Motivation: 语言模型提示优化面临组合空间巨大、奖励稀疏且评估成本高的问题，现有基于RL的方法样本效率低下，需要更高效的提示搜索方法

Method: 1) 使用GFlowNet目标微调轻量级提示LM，采用基于回放的训练策略重用历史提示评估；2) 引入动态记忆更新机制，从回放缓冲区和优先级队列注入多样化和高性能提示，逐步集中搜索到高奖励区域

Result: 在少样本文本分类、指令归纳基准和问答任务中，GFlowPO持续优于最近的离散提示优化基线方法

Conclusion: GFlowPO通过将提示搜索构建为后验推断问题，结合GFlowNet和动态记忆更新，实现了样本高效的提示优化，在多个任务上表现出色

Abstract: Finding effective prompts for language models (LMs) is critical yet notoriously difficult: the prompt space is combinatorially large, rewards are sparse due to expensive target-LM evaluation. Yet, existing RL-based prompt optimizers often rely on on-policy updates and a meta-prompt sampled from a fixed distribution, leading to poor sample efficiency. We propose GFlowPO, a probabilistic prompt optimization framework that casts prompt search as a posterior inference problem over latent prompts regularized by a meta-prompted reference-LM prior. In the first step, we fine-tune a lightweight prompt-LM with an off-policy Generative Flow Network (GFlowNet) objective, using a replay-based training policy that reuses past prompt evaluations to enable sample-efficient exploration. In the second step, we introduce Dynamic Memory Update (DMU), a training-free mechanism that updates the meta-prompt by injecting both (i) diverse prompts from a replay buffer and (ii) top-performing prompts from a small priority queue, thereby progressively concentrating the search process on high-reward regions. Across few-shot text classification, instruction induction benchmarks, and question answering tasks, GFlowPO consistently outperforms recent discrete prompt optimization baselines.

</details>


### [59] [Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402)
*Mengxuan Wang,Yuxin Chen,Gang Xu,Tao He,Hongjie Jiang,Ming Li*

Main category: cs.AI

TL;DR: RAI是一个无需训练的轻量级安全校准框架，通过放大VLMs中的不安全信号来恢复LLM式的风险识别能力，有效防御多模态越狱攻击


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在多模态越狱攻击下非常脆弱，现有防御方法要么需要大量训练成本，要么会显著降低模型效用。研究发现LLMs本身能识别文本中的不安全内容，但视觉输入会稀释风险信号

Method: RAI从语言嵌入构建不安全原型子空间，对选定的高风险视觉token进行定向调制，在跨模态特征空间中显式激活安全关键信号，恢复模型的LLM式风险识别能力

Result: 在多个越狱攻击和效用基准测试中，RAI显著降低了攻击成功率，同时不损害任务性能

Conclusion: RAI提供了一种轻量级、无需训练的安全校准方法，通过恢复VLMs的LLM式风险识别能力，有效防御多模态越狱攻击，同时保持模型效用

Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model's LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.

</details>


### [60] [Feasible strategies for conflict resolution within intuitionistic fuzzy preference-based conflict situations](https://arxiv.org/abs/2602.03403)
*Guangming Lang,Mingchuan Shang,Mengjun Hu,Jie Zhou,Feng Xu*

Main category: cs.AI

TL;DR: 提出直觉模糊偏好冲突分析模型，通过更细粒度描述智能体态度，构建三支冲突分析框架，并设计调整策略


<details>
  <summary>Details</summary>
Motivation: 现有偏好冲突模型仅使用偏好、逆偏好和中性三种定性关系描述智能体对议题对的态度，表达能力有限，无法充分捕捉冲突本质

Method: 引入直觉模糊偏好冲突情境概念，构建直觉模糊偏好冲突度量，建立三支冲突分析模型（智能体对、智能体集、议题集的三分），基于损失函数计算阈值，设计调整机制可行策略

Result: 开发了更细粒度的冲突分析框架，能够更精确地描述智能体态度，构建了三支冲突分析模型，设计了同时考虑调整幅度和冲突程度的可行策略

Conclusion: 直觉模糊偏好冲突分析模型克服了传统模型的局限性，提供了更精细的冲突描述能力，并通过示例验证了模型的有效性和实用性

Abstract: In three-way conflict analysis, preference-based conflict situations characterize agents' attitudes towards issues by formally modeling their preferences over pairs of issues. However, existing preference-based conflict models rely exclusively on three qualitative relations, namely, preference, converse, and indifference, to describe agents' attitudes towards issue pairs, which significantly limits their capacity in capturing the essence of conflict. To overcome this limitation, we introduce the concept of an intuitionistic fuzzy preference-based conflict situation that captures agents' attitudes towards issue pairs with finer granularity than that afforded by classical preference-based models. Afterwards, we develop intuitionistic fuzzy preference-based conflict measures within this framework, and construct three-way conflict analysis models for trisecting the set of agent pairs, the agent set, and the issue set. Additionally, relative loss functions built on the proposed conflict functions are employed to calculate thresholds for three-way conflict analysis. Finally, we present adjustment mechanism-based feasible strategies that simultaneously account for both adjustment magnitudes and conflict degrees, together with an algorithm for constructing such feasible strategies, and provide an illustrative example to demonstrate the validity and effectiveness of the proposed model.

</details>


### [61] [DiscoverLLM: From Executing Intents to Discovering Them](https://arxiv.org/abs/2602.03429)
*Tae Soo Kim,Yoonjoo Lee,Jaesang Yu,John Joon Young Chung,Juho Kim*

Main category: cs.AI

TL;DR: DiscoverLLM：一个训练LLM帮助用户发现和形成意图的框架，通过模拟用户认知状态和渐进具体化来优化对话，在多项任务中提升性能并减少对话长度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM处理模糊和开放式请求时，通常只是询问用户意图，但当用户自己也不知道想要什么时，这种方法会失败。用户需要观察和探索结果来发现自己的意图。

Method: 提出DiscoverLLM框架，核心是新颖的用户模拟器，用层次化的意图结构建模认知状态，意图随着模型展示相关选项而渐进具体化。具体化程度作为奖励信号来训练模型优化。

Result: 在创意写作、技术写作和SVG绘图等交互基准测试中，DiscoverLLM实现了超过10%的任务性能提升，同时将对话长度减少了40%。在75人参与的用户研究中，提高了对话满意度和效率。

Conclusion: DiscoverLLM通过训练LLM帮助用户发现和形成意图，能够根据意图清晰度自适应地探索或细化，显著提升了交互性能和用户体验。

Abstract: To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking "what kind of tone do you want?" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.

</details>


### [62] [Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents](https://arxiv.org/abs/2602.03439)
*Xiaochi Zhou,Patrick Bulter,Changxuan Yang,Simon D. Rihm,Thitikarn Angkanaporn,Jethro Akroyd,Sebastian Mosbach,Markus Kraft*

Main category: cs.AI

TL;DR: 论文提出了一种将本体论编译为工具接口的方法，使大语言模型能够与形式化领域知识结合，通过语义约束指导知识图谱的生成和修改。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型与形式化领域知识结合的问题，避免后验验证，直接在生成过程中强制执行语义约束，减少手动模式设计和提示工程的工作量。

Method: 将本体论规范编译为可执行工具接口，扩展The World Avatar的语义代理组合框架，使用模型上下文协议(MCP)和代理，实现本体论到工具的转换，并通过迭代工作流程从非结构化文本中提取、验证和修复结构化知识。

Result: 以金属有机多面体合成文献为例，展示了可执行本体语义如何指导大语言模型行为，减少手动模式设计和提示工程，建立了将形式化知识嵌入生成系统的通用范式。

Conclusion: 本体论到工具的编译机制为将大语言模型与形式化领域知识耦合提供了原理验证，通过语义约束指导知识生成，建立了嵌入形式化知识到生成系统的通用方法。

Abstract: We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.

</details>


### [63] [CRL-VLA: Continual Vision-Language-Action Learning](https://arxiv.org/abs/2602.03445)
*Qixin Zeng,Shuo Zhang,Hongyin Zhang,Renjie Wang,Han Zhao,Libang Zhao,Runze Li,Donglin Wang,Chao Huang*

Main category: cs.AI

TL;DR: CRL-VLA是一个用于视觉-语言-动作模型持续强化学习的框架，通过理论推导的统一性能边界和双评论家架构，解决了稳定性与可塑性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在开放世界环境中，终身学习对具身智能体至关重要。持续强化学习是将VLA模型部署到终身机器人场景中的有前景途径，但现有方法在平衡稳定性（保留旧技能）和可塑性（学习新技能）方面面临巨大挑战。

Method: 提出CRL-VLA框架，通过理论推导将稳定性-可塑性权衡与目标条件优势幅度（按策略散度缩放）联系起来。采用非对称调节方法：约束先前任务的优势幅度，同时允许新任务上受控增长。实现方式是通过具有新颖目标条件价值公式的双评论家架构，其中冻结评论家锚定语义一致性，可训练估计器驱动适应。

Result: 在LIBERO基准测试上的实验表明，CRL-VLA有效协调了这些冲突目标，在抗遗忘和正向适应方面均优于基线方法。

Conclusion: CRL-VLA为VLA模型的持续后训练提供了一个具有严格理论边界的框架，成功解决了终身学习中的稳定性-可塑性权衡问题，为开放世界环境中的具身智能体提供了有效的持续学习解决方案。

Abstract: Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.

</details>


### [64] [The Dual Role of Abstracting over the Irrelevant in Symbolic Explanations: Cognitive Effort vs. Understanding](https://arxiv.org/abs/2602.03467)
*Zeynep G. Saribatur,Johannes Langer,Ute Schmid*

Main category: cs.AI

TL;DR: 研究形式化抽象（移除和聚类）如何影响人类推理性能和认知负荷，使用ASP框架简化解释，实验表明聚类提升理解，移除细节降低认知负荷


<details>
  <summary>Details</summary>
Motivation: AI系统输出往往难以理解，符号AI虽然提供透明基础，但原始逻辑追踪会带来高额外认知负荷，需要研究如何通过形式化抽象来改进人类对符号解释的理解

Method: 使用答案集编程（ASP）作为形式化框架，定义不相关细节进行抽象（移除和聚类），通过认知实验让参与者基于ASP程序生成的解释对跨领域刺激进行分类

Result: 实验显示：聚类细节显著提高参与者的理解能力，移除细节显著降低认知负荷，支持抽象能增强以人为中心的符号解释的假设

Conclusion: 形式化抽象（特别是移除和聚类）能有效增强人类对符号AI解释的理解，降低认知负荷，为构建更人性化的可解释AI系统提供支持

Abstract: Explanations are central to human cognition, yet AI systems often produce outputs that are difficult to understand. While symbolic AI offers a transparent foundation for interpretability, raw logical traces often impose a high extraneous cognitive load. We investigate how formal abstractions, specifically removal and clustering, impact human reasoning performance and cognitive effort. Utilizing Answer Set Programming (ASP) as a formal framework, we define a notion of irrelevant details to be abstracted over to obtain simplified explanations. Our cognitive experiments, in which participants classified stimuli across domains with explanations derived from an answer set program, show that clustering details significantly improve participants' understanding, while removal of details significantly reduce cognitive effort, supporting the hypothesis that abstraction enhances human-centered symbolic explanations.

</details>


### [65] [IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning](https://arxiv.org/abs/2602.03468)
*Haohao Luo,Zexi Li,Yuexiang Xie,Wenhao Zhang,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: IntentRL框架训练主动智能体在开始长时研究前澄清用户潜在意图，通过两阶段强化学习策略显著提升意图命中率和下游任务性能


<details>
  <summary>Details</summary>
Motivation: 深度研究(DR)智能体虽然能自主检索和综合网络信息生成长篇报告，但存在自主性-交互困境：对模糊用户查询的高自主性往往导致执行时间长且结果不令人满意

Method: 提出IntentRL框架：1) 通过浅层到深层意图细化图扩展少量种子样本生成高质量对话轮次；2) 采用两阶段强化学习：第一阶段在离线对话上学习通用用户交互行为，第二阶段使用训练好的智能体和用户模拟器进行在线推演以增强对多样化用户反馈的适应

Result: 实验表明IntentRL显著提高了意图命中率和下游任务性能，优于闭源DR智能体的内置澄清模块和主动LLM基线

Conclusion: IntentRL通过训练主动智能体在开始长时研究前澄清用户潜在意图，有效解决了深度研究智能体的自主性-交互困境，提高了系统效率和用户满意度

Abstract: Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.

</details>


### [66] [When Routing Collapses: On the Degenerate Convergence of LLM Routers](https://arxiv.org/abs/2602.03478)
*Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: LLM路由存在"路由崩溃"问题：随着成本预算增加，路由器会系统性地选择最昂贵模型，即使便宜模型已足够。本文提出EquiRouter，通过直接学习模型排名来解决此问题，在保持GPT-4级别性能的同时降低成本约17%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由器存在普遍但未被充分探索的失败模式：当用户成本预算增加时，路由器会系统性地默认选择最强大、最昂贵的模型，即使更便宜的模型已经足够。这导致小模型未被充分利用，浪费计算和金钱成本，违背了路由的核心承诺，作者称之为"路由崩溃"现象。

Method: 作者提出EquiRouter，一种决策感知的路由器。该方法直接学习模型排名，而不是像现有路由器那样预测标量性能分数。通过解决目标-决策不匹配问题（训练目标是预测分数，但路由决策依赖于模型间的离散比较），减少预测误差对相对排序的影响，从而避免次优选择。

Result: 在RouterBench基准测试中，EquiRouter在保持GPT-4级别性能的同时，相比先前最强的路由器降低了约17%的成本。该方法恢复了小模型的作用，有效缓解了路由崩溃问题。

Conclusion: 路由崩溃是LLM和多模态路由中普遍存在的问题，源于目标-决策不匹配。EquiRouter通过直接学习模型排名来解决这一问题，实现了更好的质量-成本权衡，为实际部署提供了更高效的路由解决方案。

Abstract: LLM routing aims to achieve a favorable quality--cost trade-off by dynamically assigning easy queries to smaller models and harder queries to stronger ones. However, across both unimodal and multimodal settings, we uncover a pervasive yet underexplored failure mode in existing routers: as the user's cost budget increases, routers systematically default to the most capable and most expensive model even when cheaper models already suffice. As a result, current routers under-utilize small models, wasting computation and monetary cost and undermining the core promise of routing; we term this phenomenon routing collapse. We attribute routing collapse to an objective--decision mismatch: many routers are trained to predict scalar performance scores, whereas routing decisions ultimately depend on discrete comparisons among candidate models. Consequently, small prediction errors can flip relative orderings and trigger suboptimal selections. To bridge this gap, we propose EquiRouter, a decision-aware router that directly learns model rankings, restoring the role of smaller models and mitigating routing collapse. On RouterBench, EquiRouter reduces cost by about 17\% at GPT-4-level performance compared to the strongest prior router. Our code is available at https://github.com/AIGNLAI/EquiRouter.

</details>


### [67] [Group Selection as a Safeguard Against AI Substitution](https://arxiv.org/abs/2602.03541)
*Qiankun Zhong,Thomas F. Eisenmann,Julian Garcia,Iyad Rahwan*

Main category: cs.AI

TL;DR: 研究AI使用对人类文化演化的长期影响，发现AI替代型用户（重度依赖AI生成内容）在个体层面选择中占优，但会减少文化多样性；而AI补充型用户（以AI为辅助工具）在群体选择中更优，能维持文化创新所需的多样性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的广泛使用可能降低文化多样性和创新性，导致"文化崩溃"风险。研究旨在探讨AI使用对人类文化演化的长期影响，特别是不同AI使用策略（补充型vs替代型）如何影响文化多样性和创新。

Method: 采用基于主体的建模和演化博弈论方法，比较两种AI使用策略：AI补充型用户（寻求建议但保持主要生产者角色）和AI替代型用户（提供最小输入，依赖AI生成大部分内容）。研究这些策略在演化动态中的竞争和传播。

Result: AI替代型用户在个体层面选择中占优势，尽管会显著减少文化多样性；而AI补充型用户能通过维持探索所需的多样性使群体受益，在群体边界较强时可通过文化群体选择获得优势。

Conclusion: AI使用策略的选择对文化演化有深远影响，需要政策干预来平衡个体利益与群体文化多样性。研究结果为制定缓解AI使用风险的政策和组织策略提供了依据。

Abstract: Reliance on generative AI can reduce cultural variance and diversity, especially in creative work. This reduction in variance has already led to problems in model performance, including model collapse and hallucination. In this paper, we examine the long-term consequences of AI use for human cultural evolution and the conditions under which widespread AI use may lead to "cultural collapse", a process in which reliance on AI-generated content reduces human variation and innovation and slows cumulative cultural evolution. Using an agent-based model and evolutionary game theory, we compare two types of AI use: complement and substitute. AI-complement users seek suggestions and guidance while remaining the main producers of the final output, whereas AI-substitute users provide minimal input, and rely on AI to produce most of the output. We then study how these use strategies compete and spread under evolutionary dynamics. We find that AI-substitute users prevail under individual-level selection despite the stronger reduction in cultural variance. By contrast, AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. Overall, our findings shed light on the long-term, population-level effects of AI adoption and inform policy and organizational strategies to mitigate these risks.

</details>


### [68] [Persona Generators: Generating Diverse Synthetic Personas at Scale](https://arxiv.org/abs/2602.03545)
*Davide Paglieri,Logan Cross,William A. Cunningham,Joel Z. Leibo,Alexander Sasha Vezhnevets*

Main category: cs.AI

TL;DR: 提出Persona Generators方法，通过AlphaEvolve迭代优化生成多样化合成人口，解决AI系统评估中人类数据收集困难的问题


<details>
  <summary>Details</summary>
Motivation: 评估AI系统需要多样化人类数据，但收集代表性数据昂贵且困难，特别是对于新技术或未来场景。现有生成代理方法需要详细人口数据且偏向密度匹配而非支持覆盖，导致长尾行为未被充分探索。

Method: 引入Persona Generators函数，基于AlphaEvolve迭代优化循环，使用大语言模型作为变异算子，经过数百次迭代优化生成器代码，产生轻量级生成器，能将简短描述扩展为多样化合成人口。

Result: 进化后的生成器在六个多样性指标上显著优于现有基线，能够生成覆盖罕见特征组合的多样化人口，这些组合在标准LLM输出中难以实现。

Conclusion: Persona Generators方法能够有效生成多样化合成人口，解决AI系统评估中的数据收集问题，特别擅长覆盖长尾行为和罕见特征组合。

Abstract: Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.

</details>


### [69] [EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories](https://arxiv.org/abs/2602.03569)
*Linjie Mu,Zhongzhen Huang,Yannian Gu,Shengqian Qin,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: EHRWorld：基于因果序列范式的患者中心医疗世界模型，利用真实电子健康记录数据训练，显著优于单纯基于医学知识的LLM，在长期临床模拟中表现更稳定可靠。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在静态医学推理任务上表现良好，但在动态医疗世界建模方面存在局限，难以在序列干预下保持患者状态一致性，导致长期模拟误差累积。需要开发能够可靠模拟疾病进展和治疗结果的医疗世界模型。

Method: 提出EHRWorld患者中心医疗世界模型，采用因果序列范式训练，并构建EHRWorld-110K大规模纵向临床数据集，该数据集源自真实世界电子健康记录，包含时间演化的临床数据。

Result: EHRWorld显著优于基于医学知识的LLM基线，在长期模拟中表现更稳定，对临床敏感事件的建模更准确，推理效率更高，证明了基于因果基础、时间演化临床数据训练的必要性。

Conclusion: 训练基于因果基础、时间演化临床数据的医疗世界模型对于实现可靠、稳健的医学世界建模至关重要，EHRWorld为此类模型的发展提供了有效范例。

Abstract: World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.

</details>


### [70] [Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12](https://arxiv.org/abs/2602.03630)
*Iñaki del Campo,Pablo Cuervo,Victor Rodriguez-Fernandez,Roberto Armellin,Jack Yarndley*

Main category: cs.AI

TL;DR: 评估大语言模型在复杂天体动力学任务中的自主规划能力，发现模型在战略理解上进步显著但执行层面存在严重障碍


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在物理约束的高维环境中进行自主多阶段规划的能力极限，特别是在天体动力学领域的应用

Method: 将MLE-Bench框架适配到轨道力学领域，部署基于AIDE的智能体架构，采用"LLM-as-a-Judge"方法，使用领域专家制定的评分标准评估战略可行性

Result: 过去两年平均战略可行性得分几乎翻倍（从9.3提升到17.2/26），但发现战略与执行之间存在关键能力差距，模型在物理单位一致性、边界条件和调试循环方面存在严重问题

Conclusion: 当前大语言模型具备解决空间科学任务的知识和智能，但受限于实现障碍，只能作为强大的领域辅助工具而非完全自主的工程师

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an "LLM-as-a-Judge" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.

</details>


### [71] [Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration](https://arxiv.org/abs/2602.03647)
*Bowei He,Minda Hu,Zenan Xu,Hongru Wang,Licheng Zong,Yankai Chen,Chen Ma,Xue Liu,Pluto Zhou,Irwin King*

Main category: cs.AI

TL;DR: Search-R2：通过Actor-Refiner协作框架和密集过程奖励解决语言智能体搜索推理中的信用分配问题，提升多步推理准确性


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索推理智能体面临多尺度信用分配问题，稀疏的轨迹级奖励无法区分高质量推理和偶然猜测，导致冗余或误导性搜索行为

Method: 提出Actor-Refiner协作框架：Actor生成初始推理轨迹，Meta-Refiner通过"剪切-再生"机制选择性诊断修复错误步骤；设计混合奖励（结果正确性+检索证据信息密度的密集过程奖励）

Result: 在多种通用和多跳QA数据集上，Search-R2在不同模型规模上均优于强RAG和基于RL的基线方法，以最小开销实现更优的推理准确性

Conclusion: Search-R2通过细粒度干预和密集监督有效解决了搜索推理中的信用分配问题，理论证明选择性修正能带来严格性能提升，为语言智能体搜索推理提供了新框架

Abstract: Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.

</details>


### [72] [Mitigating Conversational Inertia in Multi-Turn Agents](https://arxiv.org/abs/2602.03664)
*Yang Wan,Zheng Cao,Zhenhao Zhang,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.AI

TL;DR: 论文提出"对话惯性"概念，指LLM在多轮对话中过度模仿自身先前回答的问题，并提出基于上下文偏好学习的解决方案来平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 将少样本学习的LLM转化为多轮代理时，模型会错误地将自己之前的回答作为少样本示例进行模仿，导致"对话惯性"问题，限制了探索能力。

Method: 通过注意力分析识别对话惯性现象，提出上下文偏好学习框架，基于相同状态下长上下文比短上下文产生更强惯性的观察构建偏好对，无需环境奖励。

Result: 在八个代理环境和深度研究场景中验证，该框架能有效减少对话惯性并提升性能。

Conclusion: 对话惯性是LLM作为代理时的重要问题，上下文偏好学习能有效校准模型偏好，平衡探索与利用。

Abstract: Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.

</details>


### [73] [TodyComm: Task-Oriented Dynamic Communication for Multi-Round LLM-based Multi-Agent System](https://arxiv.org/abs/2602.03688)
*Wenzhe Fan,Tommaso Tognoli,Henry Peng Zou,Chunyu Miao,Yibo Wang,Xinhua Zhang*

Main category: cs.AI

TL;DR: TodyComm：一种任务导向的动态通信算法，用于多轮LLM多智能体系统，能够根据每轮动态变化调整通信拓扑结构


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统使用固定通信拓扑，但在实际应用中智能体角色可能因动态对抗、任务进展或时变约束（如通信带宽）而跨轮变化，固定拓扑无法适应这些动态需求

Method: 提出TodyComm算法，通过策略梯度优化生成行为驱动的协作拓扑，使通信结构能够适应每轮的动态变化，最大化任务效用

Result: 在五个基准测试上的实验表明，在动态对抗和通信预算约束下，TodyComm在任务效果上表现优异，同时保持了令牌效率和可扩展性

Conclusion: TodyComm通过动态调整通信拓扑有效解决了多轮LLM多智能体系统中的动态适应问题，为实际应用提供了更灵活的协作框架

Abstract: Multi-round LLM-based multi-agent systems rely on effective communication structures to support collaboration across rounds. However, most existing methods employ a fixed communication topology during inference, which falls short in many realistic applications where the agents' roles may change \textit{across rounds} due to dynamic adversary, task progression, or time-varying constraints such as communication bandwidth. In this paper, we propose addressing this issue through TodyComm, a \textbf{t}ask-\textbf{o}riented \textbf{dy}namic \textbf{comm}unication algorithm. It produces behavior-driven collaboration topologies that adapt to the dynamics at each round, optimizing the utility for the task through policy gradient. Experiments on five benchmarks demonstrate that under both dynamic adversary and communications budgets, TodyComm delivers superior task effectiveness while retaining token efficiency and scalability.

</details>


### [74] [AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786)
*Jianhao Ruan,Zhihao Xu,Yiran Peng,Fashen Ren,Zhaoyang Yu,Xinbing Liang,Jinyu Xiang,Bang Liu,Chenglin Wu,Yuyu Luo,Jiayi Zhang*

Main category: cs.AI

TL;DR: AOrchestra是一个基于统一智能体抽象（指令、上下文、工具、模型）的动态编排系统，能够按需创建专门执行器，在多个基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体系统缺乏动态抽象视图，限制了在复杂长时程任务中的适应能力。需要一种统一的框架无关的智能体抽象来提升系统的灵活性和可扩展性。

Method: 提出统一的智能体抽象：将任何智能体建模为四元组（指令、上下文、工具、模型）。基于此构建AOrchestra系统，中央编排器在每个步骤中具体化这个四元组：策划任务相关上下文、选择工具和模型，并通过即时自动创建智能体来委托执行。

Result: 在三个具有挑战性的基准测试（GAIA、SWE-Bench、Terminal-Bench）中，AOrchestra与Gemini-3-Flash配对使用时，相对于最强基线实现了16.28%的相对改进。

Conclusion: 统一的智能体抽象和AOrchestra系统能够减少人工工程努力，保持框架无关性，支持即插即用的多样化智能体作为任务执行器，并实现可控的性能-成本权衡，使系统能够接近帕累托效率。

Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

</details>


### [75] [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](https://arxiv.org/abs/2602.03794)
*Yingxuan Yang,Chengrui Qu,Muning Wen,Laixi Shi,Ying Wen,Weinan Zhang,Adam Wierman,Shangding Gu*

Main category: cs.AI

TL;DR: 多智能体系统性能受限于任务内在不确定性而非智能体数量，异构智能体通过提供互补信息比同构智能体扩展更有效


<details>
  <summary>Details</summary>
Motivation: 研究LLM多智能体系统性能扩展的局限性，发现单纯增加同构智能体数量收益递减，而异构智能体却能持续提升性能，需要理解这种差异的根本原因

Method: 提出信息论框架分析多智能体系统性能边界，引入K*指标量化有效通道数量，通过实验验证异构配置相对于同构扩展的优势

Result: 异构配置显著优于同构扩展：2个异构智能体性能可匹配或超过16个同构智能体，K*指标能有效量化系统多样性

Conclusion: 多智能体系统设计应注重多样性而非单纯增加数量，通过异构智能体（不同模型、提示或工具）提供互补证据来突破性能瓶颈

Abstract: LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.

</details>


### [76] [Conformal Thinking: Risk Control for Reasoning on a Compute Budget](https://arxiv.org/abs/2602.03814)
*Xi Wang,Anushri Suresh,Alvin Zhang,Rishi More,William Jurayj,Benjamin Van Durme,Mehrdad Farajtabar,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 提出基于风险控制的LLM自适应推理框架，通过上下阈值机制在保证错误率的同时最小化计算开销


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理时存在计算开销与准确率的权衡问题，需要自适应地分配计算资源，但传统方法难以设置合适的推理预算和停止阈值

Method: 提出风险控制框架，包含上阈值（模型自信时停止）和参数化下阈值（提前停止不可解实例），使用分布无关风险控制方法优化阈值设置，并引入效率损失函数选择最有效的退出机制

Result: 在多种推理任务和模型上的实验表明，该方法能有效控制用户指定的风险目标，同时通过下阈值和集成停止机制显著提升计算效率

Conclusion: 该风险控制框架为LLM自适应推理提供了实用的解决方案，能够在保证错误率的同时优化计算资源使用，平衡风险与准确率的权衡

Abstract: Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.

</details>


### [77] [AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations](https://arxiv.org/abs/2602.03828)
*Minjun Zhu,Zhen Lin,Yixuan Weng,Panzhong Lu,Qiujie Xie,Yifan Wei,Sifan Liu,Qiyao Sun,Yue Zhang*

Main category: cs.AI

TL;DR: FigureBench是首个大规模科学插图生成基准，包含3300个高质量文本-插图对；AutoFigure是首个基于长文本自动生成高质量科学插图的智能框架，通过思考、重组和验证实现结构完整且美观的插图。


<details>
  <summary>Details</summary>
Motivation: 高质量科学插图对于有效传达复杂科技概念至关重要，但手动创建插图在学术界和工业界都是公认的瓶颈，需要自动化解决方案。

Method: 提出FigureBench基准数据集（3300个高质量文本-插图对）和AutoFigure智能框架，该框架在最终渲染前进行广泛思考、重组和验证，生成结构合理且美观的布局。

Result: AutoFigure在FigureBench上经过广泛实验，始终超越所有基线方法，能够生成可直接用于发表的科学插图。

Conclusion: FigureBench为科学插图生成提供了首个大规模基准，AutoFigure框架能够自动生成高质量、可直接发表的科学插图，解决了手动创建的瓶颈问题。

Abstract: High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [78] [Precoding-Oriented CSI Feedback Design with Mutual Information Regularized VQ-VAE](https://arxiv.org/abs/2602.02508)
*Xi Chen,Homa Esfahanizadeh,Foad Sohrabi*

Main category: cs.IT

TL;DR: 提出基于向量量化变分自编码器的预编码导向CSI反馈框架，通过信息论正则化优化码本利用，在固定反馈长度下实现接近可变长度神经压缩的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统中，用户设备的CSI压缩对信道重建和预编码设计至关重要。核心挑战是在CSI反馈开销和下行速率之间取得平衡，最大化有限反馈的效用以维持高系统性能。

Method: 提出基于向量量化变分自编码器的预编码导向CSI反馈框架，引入可微互信息下界估计器作为训练正则化器，在固定反馈预算下促进学习码本的有效利用。

Result: 数值结果显示，该方法在固定长度反馈下实现了与可变长度神经压缩方案相当的速率。学习到的码字展现出更均匀的使用分布，并捕获了与底层信道状态信息强相关的可解释结构。

Conclusion: 提出的预编码导向CSI反馈框架通过信息论正则化有效优化了码本利用，在固定反馈长度下实现了高性能，同时学习到的码字具有更好的可解释性和均匀性。

Abstract: Efficient channel state information (CSI) compression at the user equipment plays a key role in enabling accurate channel reconstruction and precoder design in massive multiple-input multiple-output systems. A key challenge lies in balancing the CSI feedback overhead with the achievable downlink rate, i.e., maximizing the utility of limited feedback to maintain high system performance. In this work, we propose a precoding-oriented CSI feedback framework based on a vector quantized variational autoencoder, augmented with an information-theoretic regularization. To achieve this, we introduce a differentiable mutual information lower-bound estimator as a training regularizer to promote effective utilization of the learned codebook under a fixed feedback budget. Numerical results demonstrate that the proposed method achieves rates comparable to variable-length neural compression schemes, while operating with fixed-length feedback. Furthermore, the learned codewords exhibit significantly more uniform usage and capture interpretable structures that are strongly correlated with the underlying channel state information.

</details>


### [79] [Rate-Distortion Analysis of Optically Passive Vision Compression](https://arxiv.org/abs/2602.02768)
*Ronald Ogden,David Fridovich-Keil,Takashi Tanaka*

Main category: cs.IT

TL;DR: 提出一种基于事件相机的光学被动视觉压缩方案，通过光学生成余弦变换实现高速、无需计算压缩，性能优于独立事件相机


<details>
  <summary>Details</summary>
Motivation: 自主决策中远程视觉传感器需要在资源受限的信道上实时传输高容量视觉数据，而机器人控制等系统可能快速失稳，需要更高采样频率，加剧了传输挑战

Method: 提出光学被动视觉压缩方案，让事件相机观测视觉场景的光学生成余弦变换，实现受现代视频编解码器启发的高速、无需计算的视频压缩

Result: 模拟显示光学被动视觉压缩方案在率失真性能上优于独立事件相机，且随着事件相机空间分辨率提高，这种性能差距会进一步增大

Conclusion: 光学被动视觉压缩方案为解决资源受限信道上的实时视觉数据传输提供了一种有前景的新方法，特别适合需要高采样频率的机器人控制应用

Abstract: The use of remote vision sensors for autonomous decision-making poses the challenge of transmitting high-volume visual data over resource-constrained channels in real-time. In robotics and control applications, many systems can quickly destabilize, which can exacerbate the issue by necessitating higher sampling frequencies. This work proposes a novel sensing paradigm in which an event camera observes the optically generated cosine transform of a visual scene, enabling high-speed, computation-free video compression inspired by modern video codecs. In this study, we simulate this optically passive vision compression (OPVC) scheme and compare its rate-distortion performance to that of a standalone event camera (SAEC). We find that the rate-distortion performance of the OPVC scheme surpasses that of the SAEC and that this performance gap increases as the spatial resolution of the event camera increases.

</details>


### [80] [Straggler-Aware Coded Polynomial Aggregation](https://arxiv.org/abs/2602.03074)
*Xi Zhong,Jörg Kliewer,Mingyue Ji*

Main category: cs.IT

TL;DR: 将编码多项式聚合（CPA）扩展到具有预定义非掉队者模式的分布式计算系统，建立了精确恢复的充要条件，并识别出保证恢复的交集大小阈值。


<details>
  <summary>Details</summary>
Motivation: 现有CPA方案局限于理想化无掉队者系统，需要扩展到实际分布式计算环境中，考虑掉队者容忍问题。

Method: 将CPA扩展到具有预定义非掉队者模式的系统，建立精确恢复的充要条件，识别交集大小阈值，提供可行的CPA方案构造方法。

Result: 证明精确恢复所需的工作节点响应数量少于基于个体解码的多项式编码，可行性由非掉队者模式的交集结构决定，建立了精确恢复的充要条件。

Conclusion: 成功将CPA扩展到掉队者感知系统，建立了理论框架，提供了可行的构造方案，并通过仿真验证了理论预测的可行性转变阈值。

Abstract: Coded polynomial aggregation (CPA) in distributed computing systems enables the master to directly recover a weighted aggregation of polynomial computations without individually decoding each term, thereby reducing the number of required worker responses. However, existing CPA schemes are restricted to an idealized setting in which the system cannot tolerate stragglers. In this paper, we extend CPA to straggler-aware distributed computing systems with a pre-specified non-straggler pattern, where exact recovery is required for a given collection of admissible non-straggler sets. Our main results show that exact recovery of the desired aggregation is achievable with fewer worker responses than that required by polynomial codes based on individual decoding, and that feasibility is characterized by the intersection structure of the non-straggler patterns. In particular, we establish necessary and sufficient conditions for exact recovery in straggler-aware CPA. We identify an intersection-size threshold that is sufficient to guarantee exact recovery. When the number of admissible non-straggler sets is sufficiently large, we further show that this threshold is necessary in a generic sense. We also provide an explicit construction of feasible CPA schemes whenever the intersection size exceeds the derived threshold. Finally, simulations verify our theoretical results by demonstrating a sharp feasibility transition at the predicted intersection threshold.

</details>


### [81] [Entropy Functions on Two-Dimensional Faces of Polymatroid Region with One Extreme Ray Containing Rank-One Matroid](https://arxiv.org/abs/2602.03363)
*Kaizhe He,Qi Chen*

Main category: cs.IT

TL;DR: 本文研究了n度多拟阵区域中2维面上的熵函数特征，特别关注包含秩1拟阵的极端射线情况，并将包含另一拟阵的极端射线的2维面分为四类。


<details>
  <summary>Details</summary>
Motivation: 熵函数的表征在信息论中具有基础重要性。通过在多拟阵区域（香农外边界）上施加约束，可以得到该区域的各个面以及具有特殊结构的熵函数。本文旨在研究包含秩1拟阵的极端射线的2维面上的熵函数特征。

Method: 通过分析多拟阵区域的2维面，特别关注那些包含秩1拟阵的极端射线的情况。将包含另一拟阵的极端射线的2维面进行分类，提出了四种类型。

Result: 成功表征了n度多拟阵区域中2维面上的熵函数，这些面包含秩1拟阵的极端射线。将所有包含另一拟阵的极端射线的2维面分为四种类型。

Conclusion: 本文对多拟阵区域2维面上的熵函数进行了系统分类，特别关注包含秩1拟阵的情况，为理解熵函数的结构提供了新的见解，对信息论中的熵函数表征研究有重要意义。

Abstract: Characterization of entropy functions is of fundamental importance in information theory. By imposing constraints on their Shannon outer bound, i.e., the polymatroidal region, one obtains the faces of the region and entropy functions on them with special structures. In this paper, we characterize entropy functions on 2-dimensional faces of polymatroid region of degree n with one extreme ray containing rank-1 matroid. We classify all such 2-dimensional faces with another extreme ray containing a matroid into four types.

</details>


### [82] [Universal Costas Matrices: Towards a General Framework for Costas Array Construction](https://arxiv.org/abs/2602.03407)
*Fatih Gulec,Vahid Abolghasemi*

Main category: cs.IT

TL;DR: 提出统一框架分析Costas阵列，引入通用Costas矩阵(UCM)和通用Costas频率矩阵(UCFM)，开发基于重构的搜索方法加速Costas阵列发现


<details>
  <summary>Details</summary>
Motivation: Costas阵列具有理想自相关和低互相关特性，在雷达、无线通信和集成感知通信中具有重要价值，但需要更高效的分析和发现方法

Method: 提出统一框架，引入UCM和UCFM概念并研究其结构特性，开发基于重构的搜索方法从UCFM生成UCM，为AI辅助发现奠定基础

Result: 数值结果表明，该方法显著加速搜索过程，增强对Costas阵列生成的结构理解

Conclusion: 提出的统一框架为Costas阵列分析提供了新视角，加速了发现过程，为未来AI辅助的Costas阵列发现铺平了道路

Abstract: Costas arrays are a special type of permutation matrices with ideal autocorrelation and low cross-correlation properties, making them valuable for radar, wireless communication, and integrated sensing and communication applications. This paper presents a novel unified framework for analyzing and discovering new Costas arrays. We introduce Universal Costas Matrices (UCMs) and Universal Costas Frequency Matrices (UCFMs) and investigate their structural characteristics. A framework integrating UCMs and UCFMs is proposed to pave the way for future artificial intelligence-assisted Costas array discovery. Leveraging the structural properties of UCMs and UCFMs, a reconstruction-based search method is developed to generate UCMs from UCFMs. Numerical results demonstrate that the proposed approach significantly accelerates the search process and enhances structural insight into Costas array generation.

</details>


### [83] [On (Im)possibility of Network Oblivious Transfer via Noisy Channels and Non-Signaling Correlations](https://arxiv.org/abs/2602.03421)
*Hadi Aghaee,Christian Deppe,Holger Boche*

Main category: cs.IT

TL;DR: 该研究证明在诚实但好奇的各方之间，通过噪声多址信道和广播信道实现完美网络不经意传输是不可能的，即使使用三方非信号相关资源也无法实现。


<details>
  <summary>Details</summary>
Motivation: 研究网络不经意传输的基本限制，探索在诚实但好奇的各方之间，当各方可以访问一般三方非信号相关时，通过噪声多址信道和广播信道实现不经意传输的可能性。

Method: 将共享资源建模为任意三方非信号盒子，为信道行为和结果相关性提供统一视角，分析渐近机制下重复使用资源对消息区分能力的影响。

Result: 主要结果表明完美不经意传输是不可能的。在渐近机制下，即使可忽略的泄漏也无法实现，因为资源的重复使用会放大接收方区分非预期消息的能力。然而，接收方自身的隐私不受普遍不可能性限制。

Conclusion: 网络不经意传输存在基本限制：完美实现不可能，渐近机制下泄漏不可避免，但接收方隐私保护仍有可能性空间。

Abstract: This work investigates the fundamental limits of implementing network oblivious transfer via noisy multiple access channels and broadcast channels between honest-but-curious parties when the parties have access to general tripartite non-signaling correlations. By modeling the shared resource as an arbitrary tripartite non-signaling box, we obtain a unified perspective on both the channel behavior and the resulting correlations. Our main result demonstrates that perfect oblivious transfer is impossible. In the asymptotic regime, we further show that even negligible leakage cannot be achieved, as repeated use of the resource amplifies the receiver(s)'s ability to distinguish messages that were not intended for him/them. In contrast, the receiver(s)'s own privacy is not subject to a universal impossibility limitation.

</details>


### [84] [Generative Decompression: Optimal Lossy Decoding Against Distribution Mismatch](https://arxiv.org/abs/2602.03505)
*Saeed R. Khosravirad,Ahmed Alkhateeb,Ingrid van de Voorde*

Main category: cs.IT

TL;DR: 本文研究在源分布不匹配情况下的最优解码策略，提出生成式解压缩方法，通过解码端的贝叶斯修正显著提升性能，无需修改编码器。


<details>
  <summary>Details</summary>
Motivation: 标准化通信系统中，解码器可能获得关于源真实分布的侧信息或先验知识，而固定编码器无法获取这些信息，导致压缩器设计与实际分布不匹配。需要解决这种不匹配情况下的最优解码问题。

Method: 提出生成式解压缩方法：在解码端采用真实分布下的条件期望作为重构规则，替代传统的质心规则。将框架扩展到有噪信道传输，推导鲁棒的软解码规则，并推广到任务导向解码，从条件均值估计转向最大后验检测。

Result: 实验结果表明，生成式解压缩方法在Gaussian源和基于深度学习的语义分类任务中，能够显著缩小与理想联合优化基准的性能差距，实现自适应高保真重构。

Conclusion: 生成式解压缩为分布不匹配问题提供了有效的解码端解决方案，通过贝叶斯修正显著提升性能，无需修改编码器，对标准化通信系统具有重要实用价值。

Abstract: This paper addresses optimal decoding strategies in lossy compression where the assumed distribution for compressor design mismatches the actual (true) distribution of the source. This problem has immediate relevance in standardized communication systems where the decoder acquires side information or priors about the true distribution that are unavailable to the fixed encoder. We formally define the mismatched quantization problem, demonstrating that the optimal reconstruction rule, termed generative decompression, aligns with classical Bayesian estimation by taking the conditional expectation under the true distribution given the quantization indices and adapting it to fixed-encoder constraints. This strategy effectively performs a generative Bayesian correction on the decoder side, strictly outperforming the conventional centroid rule. We extend this framework to transmission over noisy channels, deriving a robust soft-decoding rule that quantifies the inefficiency of standard modular source--channel separation architectures under mismatch. Furthermore, we generalize the approach to task-oriented decoding, showing that the optimal strategy shifts from conditional mean estimation to maximum a posteriori (MAP) detection. Experimental results on Gaussian sources and deep-learning-based semantic classification demonstrate that generative decompression closes a vast majority of the performance gap to the ideal joint-optimization benchmark, enabling adaptive, high-fidelity reconstruction without modifying the encoder.

</details>


### [85] [Secure Decentralized Pliable Index Coding for Target Data Size](https://arxiv.org/abs/2602.03579)
*Anjali Padmanabhan,Danya Arun Bindhu,Nujoom Sageer Karat,Shanuja Sasi*

Main category: cs.IT

TL;DR: 研究去中心化可折叠索引编码（DPIC）问题，针对具有异构边信息基数的分布式系统，提出一种在安全约束下确保每个客户端恰好获得T条消息的传输方案。


<details>
  <summary>Details</summary>
Motivation: 现有DPIC研究大多假设同质化设置（相同的边信息基数和单消息需求），但这限制了在现实世界中的应用，因为客户端通常拥有不等量的先验信息。需要研究异构边信息基数下的DPIC问题。

Method: 提出一种传输方案，协调客户端广播以最大化编码效率，同时确保每个客户端达到共同目标水平T。方案施加严格的安全约束：任何客户端获取的消息数量不超过目标T，保证每个客户端最终恰好获得T条消息。

Result: 分析了在安全约束下所提出方案的通信成本。方案在异构边信息基数环境下实现了编码效率优化，同时满足严格的安全要求。

Conclusion: 该研究扩展了DPIC问题到异构边信息基数场景，提出了在安全约束下确保每个客户端恰好获得T条消息的传输方案，并分析了其通信成本，增强了DPIC在现实分布式系统中的适用性。

Abstract: Decentralized Pliable Index Coding (DPIC) problem addresses efficient information exchange in distributed systems where clients communicate among themselves without a central server. An important consideration in DPIC is the heterogeneity of side-information and demand sizes. Although many prior works assume homogeneous settings with identical side-information cardinality and single message demands, these assumptions limit real-world applicability where clients typically possess unequal amounts of prior information. In this paper, we study DPIC problem under heterogeneous side-information cardinalities. We propose a transmission scheme that coordinates client broadcasts to maximize coding efficiency while ensuring that each client achieves a common target level $T$. In addition, we impose a strict security constraint that no client acquires more than the target $T$ number of messages, guaranteeing that each client ends up with exactly $T$ messages. We analyze the communication cost incurred by the proposed scheme under this security constraint.

</details>


### [86] [Sleep or Transmit: Dual-Mode Energy-Efficient Design for NOMA-Enabled Backscatter Networks](https://arxiv.org/abs/2602.03607)
*Hajar El Hassani,Mikael Gidlund*

Main category: cs.IT

TL;DR: 提出一种结合非正交多址接入(NOMA)的上行链路反向散射通信设计，通过优化时间、功率和反射系数，最大化系统能量效率，适用于大规模物联网部署。


<details>
  <summary>Details</summary>
Motivation: 物联网设备快速增长需要频谱效率和能量效率兼备的通信系统。传统反向散射通信在密集部署中频谱效率下降，需要新的解决方案来提升大规模物联网部署的性能。

Method: 在双静态网络中，多个反向散射节点采集射频能量并在休眠和激活模式间切换。提出基于Dinkelbach的交替优化算法，解决时间、功率和反射系数的耦合分数规划问题，提供闭式更新解。

Result: 分析揭示两种工作模式（取决于功率可用性、电路需求和传播条件）。仿真显示：相比固定功率基准提升8%能量效率，相比无休眠基准提升68%，相比正交多址接入提升127%能量效率。

Conclusion: NOMA使能的反向散射通信是大规模物联网部署的可扩展、能量高效解决方案，通过自适应时间分配优化系统性能。

Abstract: The rapid growth of Internet-of-Things (IoT) devices demands communication systems that are both spectrally efficient and energy frugal. Backscatter communication (BackCom) is an attractive low-power paradigm, but its spectral efficiency declines in dense deployments. This paper presents an uplink BackCom design that integrates non-orthogonal multiple access (NOMA) and maximizes system energy efficiency (EE). In a bistatic network where multiple backscatter nodes (BNs) harvest RF energy and alternate between sleep and active modes, we formulate a fractional program with coupled time, power, and reflection variables and develop a Dinkelbach-based alternating optimization (AO) algorithm with closed-form updates. Analysis reveals two operating modes depending on power availability, circuit demands and propagation conditions. Simulations show the proposed design adapts the time allocation, achieving up to 8% higher EE than fixed-power and 68% than no-sleep baselines, and delivering up to 127% EE gains over orthogonal multiple access (OMA). These results establish NOMA-enabled BackCom as a scalable, energy efficient solution for large-scale IoT deployments.

</details>
