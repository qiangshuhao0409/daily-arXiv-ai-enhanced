<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [PRIME: Pseudo-Random Integrated Multi-Part Entropy for Adaptive Packet Spraying in AI/ML Data centers](https://arxiv.org/abs/2507.23012)
*Ashkan Sobhani,Sogand Sadrhaghighi,Xingjun Chu*

Main category: cs.NI

TL;DR: PRIME是一种针对大规模分布式训练中网络负载均衡问题的伪随机轮询分组喷洒方法，通过考虑网络拓扑和拥塞信号优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案（如ECMP和分组喷洒）在处理AI/ML工作负载时存在网络利用率低、缓冲区膨胀和尾部延迟增加的问题。

Method: PRIME采用伪随机轮询分组喷洒方法，利用拥塞信号动态调整负载分配，避免网络热点。

Result: 实验表明，PRIME在排列流量和网络退化场景中分别实现了15%和27%的性能提升。

Conclusion: PRIME通过动态负载均衡显著提升了网络性能，尤其在复杂网络条件下表现优异。

Abstract: Large-scale distributed training in production data centers place significant
demands on network infrastructure. In particular, significant load balancing
challenges arise when processing AI/ML workloads, consisting of low-entropy,
bursty and long-lived flows. Existing solutions designed for Ethernet, such as
Equal-Cost Multi-Path (ECMP) struggle to maintain high network utilization.
While major industry players (e.g., Ultra Ethernet Consortium) and parts of
academia have proposed packet spraying to enhance AI/ML workload performance,
we argue that existing packet spraying solutions lead to buffer inflation over
time, negatively affecting network performance. Specifically, when ACK
coalescing is used, these solutions lead to stale information, degrading
network performance. Additionally, in asymmetric network conditions- such as
mix of ordered an unordered traffic, or link degradation and failures- existing
packet spraying solutions often lead to increased tail latency. In this paper,
we present the design and evaluation of PRIME, a pseudo-randomized round-robin
approach to packet spraying that considers the network topology to optimize
load distribution and performance. PRIME uses congestion as an indicator to
re-balance the load. To this extent, PRIME takes into account various
congestion signals, accounting for congestion severity, and their decay times
to avoid network hotspots. We extensively evaluated PRIME using large-scale
production-level simulator. Our results indicate that, compared to existing
solutions, PRIME leads to up to 15% improvement for permutation traffic and up
to 27% improvement in network degradation scenarios

</details>


### [2] [InterfO-RAN: Real-Time In-band Cellular Uplink Interference Detection with GPU-Accelerated dApps](https://arxiv.org/abs/2507.23177)
*Neagin Neasamoni Santhi,Davide Villa,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: InterfO-RAN是一种基于CNN的实时解决方案，用于检测5G网络中上行链路的带内干扰，准确率超过91%，并在真实环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 5G及超密集网络中的上行链路干扰问题严重影响了信号质量，导致协议操作和关键功能受损，亟需高效解决方案。

Method: 利用卷积神经网络（CNN）处理gNB物理层的I/Q样本，实时检测干扰，并通过GPU加速实现高效运行。

Result: 在真实环境中测试了超过700万个NR UL时隙，干扰检测准确率超过91%，延迟低于650微秒。

Conclusion: InterfO-RAN是首个基于O-RAN的GPU加速应用，能有效解决密集部署中的干扰问题，提升网络性能。

Abstract: Ultra-dense fifth generation (5G) and beyond networks leverage spectrum
sharing and frequency reuse to enhance throughput, but face unpredictable
in-band uplink (UL) interference challenges that significantly degrade Signal
to Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases
(gNBs). This is particularly problematic at cell edges, where overlapping
regions force User Equipments (UEs) to increase transmit power, and in
directional millimeter wave systems, where beamforming sidelobes can create
unexpected interference. The resulting signal degradation disrupts protocol
operations, including scheduling and resource allocation, by distorting quality
indicators like Reference Signal Received Power (RSRP) and Received Signal
Strength Indicator (RSSI), and can compromise critical functions such as
channel state reporting and Hybrid Automatic Repeat Request (HARQ)
acknowledgments. To address this problem, this article introduces InterfO-RAN,
a real-time programmable solution that leverages a Convolutional Neural Network
(CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical
layer, detecting in-band interference with accuracy exceeding 91% in under 650
us. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics
Processing Unit (GPU), coexisting with the 5G NR physical layer processing of
NVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial
Radio Units (RUs) and smartphones, our solution was trained and tested on more
than 7 million NR UL slots collected from real-world environments,
demonstrating robust interference detection capabilities essential for
maintaining network performance in dense deployments.

</details>


### [3] [Optimal Packetization Towards Low Latency in Random Access Networks (extended version)](https://arxiv.org/abs/2507.23286)
*Zihong Li,Anshan Yuan,Xinghua Sun*

Main category: cs.NI

TL;DR: 论文研究了随机接入网络中数据包化对排队延迟的影响，提出了优化数据包大小以最小化延迟的方法，并分析了不同网络参数的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍将数据包视为原子传输单元，忽略了数据包化对排队延迟的影响，尤其是以秒为单位的平均排队延迟。

Method: 建立了数据包化与平均排队延迟之间的数学关系，通过数值方法找到最优数据包大小，并分析了网络参数的影响。

Result: 确定了最优平均排队延迟及其对应的数据包大小，并通过仿真研究了数据包化对延迟抖动的影响。

Conclusion: 研究为随机接入网络提供了新的数据包化优化视角，并应用于NTN场景的RA-SDT分析。

Abstract: As the demand for low-latency services grows, ensuring the delay performance
of random access (RA) networks has become a priority. Existing studies on the
queueing delay performance of the Aloha model universally treat packets as
atomic transmission units, focusing primarily on delay measured in time slots.
However, the impact of packetization on queueing delay has been consistently
overlooked, particularly for the mean queueing delay measured in seconds, which
serves as a more precise and practically relevant performance metric than its
slot-based counterpart. Here, packetization refers to the process of
determining the number of bits assembled into a packet. To optimize queueing
delay from the perspective of packetization, this paper establishes the
mathematical relationship between packetization and mean queueing delay in
seconds for both connection-free and connection-based Aloha schemes, and
explores the optimal packetization strategy to minimize this delay. We identify
the optimal mean queueing delay and its corresponding packet size via numerical
methods, and further analyze the influence of various network parameters. We
further use simulations to investigate the similar impact of packetization on
jitter of queueing delay. We then apply our analysis to re-evaluate the complex
trade-off between the connection-free and connection-based schemes through the
new perspective of packetization. Furthermore, recognizing that an analysis of
the queueing delay performance for RA-SDT in NTN scenarios, especially from a
packetization perspective, also remains an unexplored area, we apply the
analysis to this scenario as a case study.

</details>


### [4] [FAST-LoRa: An Efficient Simulation Framework for Evaluating LoRaWAN Networks and Transmission Parameter Strategies](https://arxiv.org/abs/2507.23342)
*Laura Acosta García,Juan Aznar Poveda,Fabian Margreiter,Antonio-Javier García Sánchez,Joan García Haro,Thomas Fahringer,José Lorente López,José-Víctor Rodríguez*

Main category: cs.NI

TL;DR: FAST-LoRa是一种新型仿真框架，旨在快速高效评估LoRaWAN网络和传输参数选择，显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有仿真框架虽能准确模拟真实场景，但计算开销和仿真时间较大，需一种轻量级且高效的替代方案。

Method: FAST-LoRa通过分析模型简化计算，避免复杂的数据包级仿真，并利用高效矩阵操作实现网关接收。

Result: FAST-LoRa在复杂场景中仍能保持高精度（PDR的MAE为0.940×10⁻²，EE为0.040 bits/mJ），计算时间减少三个数量级。

Conclusion: FAST-LoRa是一种轻量级且准确的近似工具，适用于稳定流量模式和上行通信场景的传输参数评估。

Abstract: The Internet of Things (IoT) has transformed many industries, and LoRaWAN
(Long Range Wide Area Network), built on LoRa (Long Range) technology, has
become a crucial solution for enabling scalable, low-cost, and energy-efficient
communication in wide-area networks. Simulation tools are essential for
optimizing the transmission parameters and, therefore, the energy efficiency
and performance of LoRaWAN networks. While existing simulation frameworks
accurately replicate real-world scenarios by including multiple layers of
communication protocols, they often imply significant computational overhead
and simulation times. To address this issue, this paper introduces FAST-LoRa, a
novel simulation framework designed to enable fast and efficient evaluation of
LoRaWAN networks and selection of transmission parameters. FAST-LoRa
streamlines computation by relying on analytical models without complex
packet-level simulations and implementing gateway reception using efficient
matrix operations. Rather than aiming to replace discrete-event simulators,
FAST-LoRa is intended as a lightweight and accurate approximation tool for
evaluating transmission parameter strategies in scenarios with stable traffic
patterns and uplink-focused communications. In our evaluation, we compare
FAST-LoRa with a well-established simulator using multiple network
configurations with varying numbers of end devices and gateways. The results
show that FAST-LoRa achieves similar accuracy in estimating key network
metrics, even in complex scenarios with interference and multi-gateway
reception, with a Mean Absolute Error (MAE) of 0.940 $\times 10^{-2}$ for the
Packet Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while
significantly reducing computational time by up to three orders of magnitude.

</details>


### [5] [Dual-Mode Wireless Devices for Adaptive Pull and Push-Based Communication](https://arxiv.org/abs/2507.23421)
*Sara Cavallero,Fabio Saggese,Junya Shiraishi,Israel Leyva-Mayorga,Shashi Raj Pandey,Chiara Buratti,Petar Popovski*

Main category: cs.NI

TL;DR: 提出了一种双模式通信框架，结合查询驱动（pull）和事件驱动（push）传输，通过自适应方法实现高效数据传递，能耗降低30%。


<details>
  <summary>Details</summary>
Motivation: 无线设备通常需要同时支持查询和事件驱动的通信，但传统方法难以兼顾效率和能耗。

Method: 设计了一种统一的时帧结构，结合唤醒无线电机制和定制MAC协议，支持不同通信类别的数据传输。

Result: 系统分析显示，成功概率和能耗之间存在权衡，但能耗降低30%，同时保持两种通信模式的可靠性。

Conclusion: 提出的框架在能耗和通信效率上优于传统方法，适用于多样化网络条件。

Abstract: This paper introduces a dual-mode communication framework for wireless
devices that integrates query-driven (pull) and event-driven (push)
transmissions within a unified time-frame structure. Devices typically respond
to information requests in pull mode, but if an anomaly is detected, they
preempt the regular response to report the critical condition. Additionally,
push-based communication is used to proactively send critical data without
waiting for a request. This adaptive approach ensures timely, context-aware,
and efficient data delivery across different network conditions. To achieve
high energy efficiency, we incorporate a wake-up radio mechanism and we design
a tailored medium access control (MAC) protocol that supports data traffic
belonging to the different communication classes. A comprehensive system-level
analysis is conducted, accounting for the wake-up control operation and
evaluating three key performance metrics: the success probability of anomaly
reports (push traffic), the success probability of query responses (pull
traffic) and the total energy consumption. Numerical results characterize the
system's behavior and highlight the inherent trade-off in success probabilities
between push- and pull-based traffic as a function of allocated communication
resources. Our analysis demonstrates that the proposed approach reduces energy
consumption by up to 30% compared to a traditional approach, while maintaining
reliable support for both communication paradigms.

</details>


### [6] [From Timestamps to Versions: Version AoI in Single- and Multi-Hop Networks](https://arxiv.org/abs/2507.23433)
*Erfan Delfani,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 论文分析了通信网络中信息版本年龄（VAoI）的稳态分布，提出了多种调度策略的闭式解，并验证了最优阈值调度策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注平均指标值，而忽略了多跳网络中信息版本年龄的完整分布，因此需要更全面的分析。

Method: 研究了随机稳态、均匀和基于阈值的调度策略，推导了单跳和多跳网络中VAoI的稳态分布和平均值的闭式解。

Result: 得出了最优阈值调度策略的闭式解，并通过数值验证了分析结果。

Conclusion: 研究为高效通信网络设计提供了基于VAoI的实用见解。

Abstract: Timely and informative data dissemination in communication networks is
essential for enhancing system performance and energy efficiency, as it reduces
the transmission of outdated or redundant data. Timeliness metrics, such as Age
of Information (AoI), effectively quantify data freshness; however, these
metrics fail to account for the intrinsic informativeness of the content
itself. To address this limitation, content-based metrics have been proposed
that combine both timeliness and informativeness. Nevertheless, existing
studies have predominantly focused on evaluating average metric values, leaving
the complete distribution-particularly in multi-hop network scenarios-largely
unexplored. In this paper, we provide a comprehensive analysis of the
stationary distribution of the Version Age of Information (VAoI), a
content-based metric, under various scheduling policies, including randomized
stationary, uniform, and threshold-based policies, with transmission
constraints in single-hop and multi-hop networks. We derive closed-form
expressions for the stationary distribution and average VAoI under these
scheduling approaches. Furthermore, for threshold-based scheduling, we
analytically determine the optimal threshold value that minimizes VAoI and
derive the corresponding optimal VAoI in closed form. Numerical evaluations
verify our analytical findings, providing valuable insights into leveraging
VAoI in the design of efficient communication networks.

</details>


### [7] [Networked Physical Computing: A New Paradigm for Effective Task Completion via Hypergraph Aided Trusted Task-Resource Matching](https://arxiv.org/abs/2507.23556)
*Botao Zhu,Xianbin Wang*

Main category: cs.NI

TL;DR: 本文提出了一种基于超图的信任任务-资源匹配框架（TTR-matching），用于解决复杂连接系统中任务与资源匹配的挑战，通过整合物理属性和任务信任关系，实现价值驱动的任务完成。


<details>
  <summary>Details</summary>
Motivation: 由于计算资源和任务的物理属性多样化，复杂连接系统中任务与资源的高效匹配变得更具挑战性，需要一种机制以实现价值导向的任务完成。

Method: 提出了一种超图辅助的信任任务-资源匹配框架，包括定义任务特定的信任资源超图和任务超图，并设计超图匹配算法以实现任务与资源的高效匹配。

Result: 实验结果表明，TTR-matching框架在识别任务特定的可信合作者和最大化任务完成价值方面优于对比算法。

Conclusion: 该框架通过整合物理属性和信任关系，有效解决了任务与资源匹配的挑战，为价值驱动的任务完成提供了新方法。

Abstract: Due to the diverse physical attributes of computing resources and tasks,
developing effective mechanisms to facilitate task and resource matching in
complex connected systems for value-oriented task completion has become
increasingly challenging. To address the challenge, this paper proposes a
networked physical computing system that integrates the physical attributes of
computing resources and tasks as well as task-specific trust relationships
among devices to enable value-driven task completion. Specifically, we propose
a state-of-the-art hypergraph-aided trusted task-resource matching
(TTR-matching) framework to achieve the envisioned physical computing. First, a
task-specific trusted physical resource hypergraph is defined, which integrates
task-specific trust, the physical attributes of resources, and task types. This
enables accurate modeling of device collaboration dependencies under specific
task types. Next, a task hypergraph is generated to associate the task
initiator with the physical attributes of the corresponding tasks. Based on
these two hypergraphs, a hypergraph matching algorithm is designed to
facilitate task-specific trusted collaborator selection and accurate
task-resource matching for value-maximizing task completion. Extensive
experimental results demonstrate that the proposed TTR-matching framework
outperforms comparison algorithms in identifying task-specific trustworthy
collaborators and maximizing the average value of task completion.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Unifying Post-hoc Explanations of Knowledge Graph Completions](https://arxiv.org/abs/2507.22951)
*Alessandro Lonardi,Samy Badreddine,Tarek R. Besold,Pablo Sanchez Martin*

Main category: cs.AI

TL;DR: 本文提出了一种统一的知识图谱补全（KGC）事后解释性方法，通过多目标优化框架平衡解释的有效性和简洁性，并改进了评估协议。


<details>
  <summary>Details</summary>
Motivation: 当前KGC事后解释性缺乏形式化和一致评估，阻碍了可重复性和跨研究比较。

Method: 提出多目标优化框架统一现有解释性算法，改进评估协议（如Mean Reciprocal Rank和Hits@k）。

Result: 通过统一方法和优化评估标准，提升了KGC解释性的可重复性和实用性。

Conclusion: 本文为KGC解释性研究提供了更可重复和有意义的方法与评估标准。

Abstract: Post-hoc explainability for Knowledge Graph Completion (KGC) lacks
formalization and consistent evaluations, hindering reproducibility and
cross-study comparisons. This paper argues for a unified approach to post-hoc
explainability in KGC. First, we propose a general framework to characterize
post-hoc explanations via multi-objective optimization, balancing their
effectiveness and conciseness. This unifies existing post-hoc explainability
algorithms in KGC and the explanations they produce. Next, we suggest and
empirically support improved evaluation protocols using popular metrics like
Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of
interpretability as the ability of explanations to address queries meaningful
to end-users. By unifying methods and refining evaluation standards, this work
aims to make research in KGC explainability more reproducible and impactful.

</details>


### [9] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 论文提出一个二维数据准备框架，结合数据准备级别和处理阶段，以支持科学数据在AI训练中的标准化和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究如何将数据准备原则应用于领导级科学数据集，以训练基础模型，解决科学数据在AI训练中的挑战。

Method: 分析四个代表性领域的工作流，提出二维准备框架（数据准备级别和处理阶段），并针对高性能计算环境优化。

Result: 框架揭示了科学数据转化为AI训练数据的关键挑战，并支持跨领域的标准化和可扩展性。

Conclusion: 该框架为科学数据准备提供了成熟度矩阵，指导基础设施开发，支持可扩展和可复现的科学AI。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


### [10] [FairReason: Balancing Reasoning and Social Bias in MLLMs](https://arxiv.org/abs/2507.23067)
*Zhenyu Pan,Yutong Zhang,Jianshu Zhang,Haoran Lu,Haozheng Luo,Yuwei Han,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 研究发现，通过强化学习训练的1:4混合比例样本能在减少10%偏见的同时保留88%的推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型（MLLMs）在推理能力提升与偏见缓解之间的权衡关系。

Method: 比较三种偏见缓解策略（SFT、KD、RL），并通过调整样本比例分析推理与偏见的权衡。

Result: 强化学习训练的1:4混合样本比例能显著减少偏见并保留大部分推理能力。

Conclusion: 研究为平衡MLLMs的公平性与能力提供了具体指导。

Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art
results across a wide range of tasks and modalities. To push their reasoning
ability further, recent studies explore advanced prompting schemes and
post-training fine-tuning. Although these techniques improve logical accuracy,
they frequently leave the models' outputs burdened with pronounced social
biases. Clarifying how reasoning gains interact with bias mitigation-and
whether the two objectives inherently trade off-therefore remains an open and
pressing research problem. Our study begins by benchmarking three
bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation
(KD), and rule-based reinforcement learning (RL)-under identical conditions,
establishing their baseline strengths and weaknesses. Building on these
results, we vary the proportion of debias-focused and reasoning-centric samples
within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps
reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement
learning cuts stereotype scores by 10% while retaining 88% of the model's
original reasoning accuracy, offering concrete guidance for balancing fairness
and capability in MLLMs.

</details>


### [11] [Moravec's Paradox: Towards an Auditory Turing Test](https://arxiv.org/abs/2507.23091)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 当前AI系统在人类轻松完成的听觉任务上表现极差，失败率超过93%，揭示了AI在复杂听觉场景处理中的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 受Moravec悖论启发，研究旨在量化AI与人类在听觉任务上的差距，并探究失败原因。

Method: 引入包含917个挑战的听觉图灵测试，评估包括GPT-4和Whisper在内的先进音频模型。

Result: AI模型平均准确率仅6.9%，远低于人类的52%，暴露了选择性注意力、噪声鲁棒性和上下文适应等问题。

Conclusion: 研究提出诊断框架，强调需整合选择性注意力、基于物理的音频理解和上下文感知的新方法。

Abstract: This research work demonstrates that current AI systems fail catastrophically
on auditory tasks that humans perform effortlessly. Drawing inspiration from
Moravec's paradox (i.e., tasks simple for humans often prove difficult for
machines, and vice versa), we introduce an auditory Turing test comprising 917
challenges across seven categories: overlapping speech, speech in noise,
temporal distortion, spatial audio, coffee-shop noise, phone distortion, and
perceptual illusions. Our evaluation of state-of-the-art audio models including
GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate
exceeding 93%, with even the best-performing model achieving only 6.9% accuracy
on tasks that humans solved at 7.5 times higher success (52%). These results
expose focusing failures in how AI systems process complex auditory scenes,
particularly in selective attention, noise robustness, and contextual
adaptation. Our benchmark not only quantifies the human-machine auditory gap
but also provides insights into why these failures occur, suggesting that
current architectures lack fundamental mechanisms for human-like auditory scene
analysis. The traditional design of audio CAPTCHAs highlights common filters
that humans evolved but machines fail to select in multimodal language models.
This work establishes a diagnostic framework for measuring progress toward
human-level machine listening and highlights the need for novel approaches
integrating selective attention, physics-based audio understanding, and
context-aware perception into multimodal AI systems.

</details>


### [12] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 论文提出并形式化定义了‘论证一致性’属性，评估其在人类和LLM预测中的影响，发现过滤不一致预测可提高准确性，但用户实验显示用户未普遍遵循此属性。


<details>
  <summary>Details</summary>
Motivation: 研究论证一致性在判断性预测中的作用，提升预测准确性。

Method: 提出论证一致性定义，通过人类和LLM预测实验及用户实验验证其效果。

Result: 过滤不一致预测显著提高准确性，但用户未普遍遵循一致性。

Conclusion: 需在基于论证的判断性预测中整合机制以过滤不一致意见。

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


### [13] [Tractable Responsibility Measures for Ontology-Mediated Query Answering](https://arxiv.org/abs/2507.23191)
*Meghyn Bienvenu,Diego Figueira,Pierre Lafourcade*

Main category: cs.AI

TL;DR: 本文研究了基于Shapley值的责任度量（WSMS）在ontology-mediated查询回答中的计算复杂性，发现其在某些查询类别中具有多项式数据复杂度，但在其他情况下可能变得困难。


<details>
  <summary>Details</summary>
Motivation: 量化查询答案中各事实的贡献是近年来的研究热点，但计算责任分数的复杂性尚未充分探索。

Method: 利用数据库设置的结果，分析WSMS在不同ontology-mediated查询类别中的计算复杂性。

Result: 对于可一阶重写的查询，WSMS具有多项式数据复杂度；而对于支持合取或特定查询的ontology语言，计算变得困难。DL-Lite方言中的某些查询类别仍保持可处理性。

Conclusion: 研究揭示了WSMS计算复杂性的边界，为未来优化提供了理论基础。

Abstract: Recent work on quantitative approaches to explaining query answers employs
responsibility measures to assign scores to facts in order to quantify their
respective contributions to obtaining a given answer. In this paper, we study
the complexity of computing such responsibility scores in the setting of
ontology-mediated query answering, focusing on a very recently introduced
family of Shapley-value-based responsibility measures defined in terms of
weighted sums of minimal supports (WSMS). By exploiting results from the
database setting, we can show that such measures enjoy polynomial data
complexity for classes of ontology-mediated queries that are
first-order-rewritable, whereas the problem becomes "shP"-hard when the
ontology language can encode reachability queries (via axioms like $\exists R.
A \sqsubseteq A$). To better understand the tractability frontier, we next
explore the combined complexity of WSMS computation. We prove that
intractability applies already to atomic queries if the ontology language
supports conjunction, as well as to unions of `well-behaved' conjunctive
queries, even in the absence of an ontology. By contrast, our study yields
positive results for common DL-Lite dialects: by means of careful analysis, we
identify classes of structurally restricted conjunctive queries (which
intuitively disallow undesirable interactions between query atoms) that admit
tractable WSMS computation.

</details>


### [14] [Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification](https://arxiv.org/abs/2507.23197)
*Yuke Liao,Blaise Genest,Kuldeep Meel,Shaan Aryaman*

Main category: cs.AI

TL;DR: 论文提出了一种基于分治法的混合MILP方法，通过SAS评分选择关键ReLU变量，显著减少了二进制变量数量，提升了验证效率。


<details>
  <summary>Details</summary>
Motivation: 处理复杂实例时，传统方法效率低下，需优化ReLU变量选择策略以提升验证性能。

Method: 结合分治法，提出SAS评分和全局评分（GS）选择关键ReLU变量，并采用混合MILP方法（α,β-CROWN与部分MILP结合）。

Result: SAS评分将二进制变量数量减少6倍，验证器效率提升40%，未解决实例比例降至8-15%，平均运行时间合理（46s-417s）。

Conclusion: SAS评分和混合MILP方法显著提升了验证效率和准确性，适用于大规模神经网络。

Abstract: To handle complex instances, we revisit a divide-and-conquer approach to
break down the complexity: instead of few complex BaB calls, we rely on many
small {\em partial} MILP calls. The crucial step is to select very few but very
important ReLUs to treat using (costly) binary variables. The previous attempts
were suboptimal in that respect. To select these important ReLU variables, we
propose a novel {\em solution-aware} ReLU scoring ({\sf SAS}), as well as adapt
the BaB-SR and BaB-FSB branching functions as {\em global} ReLU scoring ({\sf
GS}) functions. We compare them theoretically as well as experimentally, and
{\sf SAS} is more efficient at selecting a set of variables to open using
binary variables. Compared with previous attempts, SAS reduces the number of
binary variables by around 6 times, while maintaining the same level of
accuracy. Implemented in {\em Hybrid MILP}, calling first $\alpha,\beta$-CROWN
with a short time-out to solve easier instances, and then partial MILP,
produces a very accurate yet efficient verifier, reducing by up to $40\%$ the
number of undecided instances to low levels ($8-15\%$), while keeping a
reasonable runtime ($46s-417s$ on average per instance), even for fairly large
CNNs with 2 million parameters.

</details>


### [15] [How Far Are AI Scientists from Changing the World?](https://arxiv.org/abs/2507.23276)
*Qiujie Xie,Yixuan Weng,Minjun Zhu,Fuchen Shen,Shulin Huang,Zhen Lin,Jiahui Zhou,Zilan Mao,Zijie Yang,Linyi Yang,Jian Wu,Yue Zhang*

Main category: cs.AI

TL;DR: 本文探讨了基于大语言模型（LLM）的AI科学家系统在科学研究中的潜力，分析了当前成就、瓶颈及未来目标。


<details>
  <summary>Details</summary>
Motivation: 研究AI科学家系统如何推动科学发现，并评估其距离改变世界和重塑科研范式的距离。

Method: 通过前瞻性综述，全面分析AI科学家系统的现状、关键瓶颈及所需组件。

Result: 指出当前系统的局限性，明确了未来科学AI的终极目标。

Conclusion: 本文为理解AI科学家系统的现状和未来发展方向提供了清晰框架。

Abstract: The emergence of large language models (LLMs) is propelling automated
scientific discovery to the next level, with LLM-based Artificial Intelligence
(AI) Scientist systems now taking the lead in scientific research. Several
influential works have already appeared in the field of AI Scientist systems,
with AI-generated research papers having been accepted at the ICLR 2025
workshop, suggesting that a human-level AI Scientist capable of uncovering
phenomena previously unknown to humans, may soon become a reality. In this
survey, we focus on the central question: How far are AI scientists from
changing the world and reshaping the scientific research paradigm? To answer
this question, we provide a prospect-driven review that comprehensively
analyzes the current achievements of AI Scientist systems, identifying key
bottlenecks and the critical components required for the emergence of a
scientific agent capable of producing ground-breaking discoveries that solve
grand challenges. We hope this survey will contribute to a clearer
understanding of limitations of current AI Scientist systems, showing where we
are, what is missing, and what the ultimate goals for scientific AI should be.

</details>


### [16] [AI Must not be Fully Autonomous](https://arxiv.org/abs/2507.23330)
*Tosin Adewumi,Lama Alkhaled,Florent Imbert,Hui Han,Nudrat Habib,Karl Löwenmark*

Main category: cs.AI

TL;DR: 论文主张AI不应完全自主，提出了3级自主AI分类，强调人类监督的重要性，并通过理论和证据支持观点。


<details>
  <summary>Details</summary>
Motivation: 探讨完全自主AI的风险，尤其是超级智能（ASI）可能带来的威胁，主张人类监督的必要性。

Method: 通过分析自主性理论、AI和代理理论，提出12个论点和6个反驳论点，并提供15个AI价值错位的证据。

Result: 明确了3级自主AI分类，证明了完全自主AI的风险，强调了人类监督的关键作用。

Conclusion: 完全自主AI（第3级）需避免，人类监督是降低风险的必要手段。

Abstract: Autonomous Artificial Intelligence (AI) has many benefits. It also has many
risks. In this work, we identify the 3 levels of autonomous AI. We are of the
position that AI must not be fully autonomous because of the many risks,
especially as artificial superintelligence (ASI) is speculated to be just
decades away. Fully autonomous AI, which can develop its own objectives, is at
level 3 and without responsible human oversight. However, responsible human
oversight is crucial for mitigating the risks. To ague for our position, we
discuss theories of autonomy, AI and agents. Then, we offer 12 distinct
arguments and 6 counterarguments with rebuttals to the counterarguments. We
also present 15 pieces of recent evidence of AI misaligned values and other
risks in the appendix.

</details>


### [17] [DSBC : Data Science task Benchmarking with Context engineering](https://arxiv.org/abs/2507.23336)
*Ram Mohan Rao Kadiyala,Siddhant Gupta,Jebish Purbey,Giulio Martini,Suman Debnath,Hamza Farooq*

Main category: cs.AI

TL;DR: 论文提出了一个针对数据科学代理的全面基准测试，评估了三种LLM模型在不同方法下的表现，揭示了性能差异和关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 尽管数据科学代理在自动化分析任务中迅速普及，但缺乏系统性的基准测试来评估其效能和局限性。

Method: 通过商业应用观察用户交互，设计了反映真实场景的基准测试，评估了三种LLM模型（Claude-4.0-Sonnet、Gemini-2.5-Flash、OpenAI-o4-Mini）在三种方法（零样本上下文工程、多步上下文工程、SmolAgent）下的表现。

Result: 发现不同模型和方法之间存在显著性能差异，并探讨了提示问题和温度参数对结果的影响。

Conclusion: 提出的基准数据集和评估框架为未来研究更鲁棒和高效的数据科学代理奠定了基础。

Abstract: Recent advances in large language models (LLMs) have significantly impacted
data science workflows, giving rise to specialized data science agents designed
to automate analytical tasks. Despite rapid adoption, systematic benchmarks
evaluating the efficacy and limitations of these agents remain scarce. In this
paper, we introduce a comprehensive benchmark specifically crafted to reflect
real-world user interactions with data science agents by observing usage of our
commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,
Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with
context engineering, multi-step with context engineering, and with SmolAgent.
Our benchmark assesses performance across a diverse set of eight data science
task categories, additionally exploring the sensitivity of models to common
prompting issues, such as data leakage and slightly ambiguous instructions. We
further investigate the influence of temperature parameters on overall and
task-specific outcomes for each model and approach. Our findings reveal
distinct performance disparities among the evaluated models and methodologies,
highlighting critical factors that affect practical deployment. The benchmark
dataset and evaluation framework introduced herein aim to provide a foundation
for future research of more robust and effective data science agents.

</details>


### [18] [LLM4Rail: An LLM-Augmented Railway Service Consulting Platform](https://arxiv.org/abs/2507.23377)
*Zhuo Li,Xianghuai Deng,Chiwei Feng,Hanmeng Li,Shenjie Wang,Haichao Zhang,Teng Jia,Conlin Chen,Louis Linchun Wu,Jia Wang*

Main category: cs.AI

TL;DR: LLM4Rail是一个基于大语言模型（LLM）的铁路服务平台，通过QTAO提示框架整合语言推理与任务导向行动，提供个性化铁路服务。


<details>
  <summary>Details</summary>
Motivation: 满足日益增长的个性化铁路服务需求，提升用户体验。

Method: 提出QTAO提示框架，结合语言推理与任务导向行动；构建CRFD-25数据集，并开发零样本对话推荐系统。

Result: LLM4Rail能提供准确的个性化服务，如票务、餐饮推荐等。

Conclusion: LLM4Rail通过QTAO框架和CRFD-25数据集，有效提升了铁路服务的个性化和智能化水平。

Abstract: Large language models (LLMs) have significantly reshaped different walks of
business. To meet the increasing demands for individualized railway service, we
develop LLM4Rail - a novel LLM-augmented railway service consulting platform.
Empowered by LLM, LLM4Rail can provide custom modules for ticketing, railway
food & drink recommendations, weather information, and chitchat. In LLM4Rail,
we propose the iterative "Question-Thought-Action-Observation (QTAO)" prompting
framework. It meticulously integrates verbal reasoning with task-oriented
actions, that is, reasoning to guide action selection, to effectively retrieve
external observations relevant to railway operation and service to generate
accurate responses. To provide personalized onboard dining services, we first
construct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible
takeout dataset tailored for railway services. CRFD-25 covers a wide range of
signature dishes categorized by cities, cuisines, age groups, and spiciness
levels. We further introduce an LLM-based zero-shot conversational recommender
for railway catering. To address the unconstrained nature of open
recommendations, the feature similarity-based post-processing step is
introduced to ensure all the recommended items are aligned with CRFD-25
dataset.

</details>


### [19] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: 论文介绍了一种基于大型语言模型（LLM）的代理，能够与工业级ERP系统交互，将自然语言查询转换为可执行的SQL语句。


<details>
  <summary>Details</summary>
Motivation: 解决工业级ERP系统中自然语言查询的自动化处理问题，提高查询生成的可靠性。

Method: 提出了一种新颖的双代理架构，结合推理和批判阶段，利用开源权重LLM。

Result: 代理能够可靠地将自然语言查询转换为SQL语句。

Conclusion: 双代理架构显著提高了查询生成的可靠性，为工业级ERP系统的自然语言交互提供了可行方案。

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [20] [Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440)
*Mingzhe Li,Xin Lu,Yanyan Zhao*

Main category: cs.AI

TL;DR: Self-Foveate是一种创新的LLM驱动方法，通过多级注视技术提升指令合成的多样性和难度。


<details>
  <summary>Details</summary>
Motivation: 传统指令合成方法依赖人工标注且多样性不足，现有自动化方法仍有局限。

Method: 采用“Micro-Scatter-Macro”多级注视方法，引导LLM从无监督文本中挖掘细粒度信息。

Result: 在多个无监督语料库和模型架构上的实验验证了方法的有效性和优越性。

Conclusion: Self-Foveate显著提升了指令合成的多样性和难度，数据与代码已公开。

Abstract: Large language models (LLMs) with instruction following capabilities have
demonstrated impressive problem-solving abilities. While synthesizing
instructional data from unsupervised text has become a common approach for
training such models, conventional methods rely heavily on human effort for
data annotation. Although existing automated synthesis paradigms have
alleviated this constraint, they still exhibit significant limitations in
ensuring adequate diversity and difficulty of synthesized instructions. To
address these challenges, we propose Self-Foveate, an innovative LLM-driven
method for instruction synthesis. This approach introduces a
"Micro-Scatter-Macro" multi-level foveation methodology that effectively guides
the LLM to deeply excavate fine-grained information embedded in unsupervised
text, thereby enhancing both the diversity and difficulty of synthesized
instructions. Comprehensive experiments across multiple unsupervised corpora
and diverse model architectures validate the effectiveness and superiority of
our proposed method. We publicly release our data and codes:
https://github.com/Mubuky/Self-Foveate

</details>


### [21] [Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery](https://arxiv.org/abs/2507.23488)
*Kacper Kadziolka,Saber Salehkaleybar*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型在因果发现任务中的表现，发现推理优先架构显著优于传统方法，并提出了一种模块化上下文管道，性能提升近三倍。


<details>
  <summary>Details</summary>
Motivation: 因果推断是大型语言模型的基本挑战，传统模型在数据扰动下表现不佳，因此研究推理优先架构的因果发现能力。

Method: 使用Corr2Cause基准测试OpenAI的o系列和DeepSeek-R模型，并引入基于Tree-of-Thoughts和Chain-of-Thoughts的模块化上下文管道。

Result: 推理优先架构在因果发现任务中表现显著优于传统方法，模块化管道性能提升近三倍。

Conclusion: 高级推理模型在因果发现中具有潜力，但需结构化上下文框架以最大化其能力，为跨领域因果发现提供通用蓝图。

Abstract: Causal inference remains a fundamental challenge for large language models.
Recent advances in internal reasoning with large language models have sparked
interest in whether state-of-the-art reasoning models can robustly perform
causal discovery-a task where conventional models often suffer from severe
overfitting and near-random performance under data perturbations. We study
causal discovery on the Corr2Cause benchmark using the emergent OpenAI's
o-series and DeepSeek-R model families and find that these reasoning-first
architectures achieve significantly greater native gains than prior approaches.
To capitalize on these strengths, we introduce a modular in-context pipeline
inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding
nearly three-fold improvements over conventional baselines. We further probe
the pipeline's impact by analyzing reasoning chain length, complexity, and
conducting qualitative and quantitative comparisons between conventional and
reasoning models. Our findings suggest that while advanced reasoning models
represent a substantial leap forward, carefully structured in-context
frameworks are essential to maximize their capabilities and offer a
generalizable blueprint for causal discovery across diverse domains.

</details>


### [22] [Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification](https://arxiv.org/abs/2507.23497)
*David A Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 论文提出因果解释方法，适用于图像分类器，兼具形式严谨性和黑盒算法兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有图像分类器解释方法缺乏形式严谨性，而逻辑解释虽严谨但假设严格，不适用于图像分类器。

Method: 引入因果解释和对比因果解释，定义置信度感知和完整因果解释，并实现高效计算。

Result: 实验显示不同模型在充分性、对比性和完整性上表现不同，算法平均6秒/图像，完全黑盒。

Conclusion: 因果解释兼具形式严谨性和实用性，适用于图像分类器。

Abstract: Existing algorithms for explaining the outputs of image classifiers are based
on a variety of approaches and produce explanations that lack formal rigor. On
the other hand, logic-based explanations are formally and rigorously defined
but their computability relies on strict assumptions about the model that do
not hold on image classifiers.
  In this paper, we show that causal explanations, in addition to being
formally and rigorously defined, enjoy the same formal properties as
logic-based ones, while still lending themselves to black-box algorithms and
being a natural fit for image classifiers. We prove formal properties of causal
explanations and introduce contrastive causal explanations for image
classifiers. Moreover, we augment the definition of explanation with confidence
awareness and introduce complete causal explanations: explanations that are
classified with exactly the same confidence as the original image.
  We implement our definitions, and our experimental results demonstrate that
different models have different patterns of sufficiency, contrastiveness, and
completeness. Our algorithms are efficiently computable, taking on average 6s
per image on a ResNet50 model to compute all types of explanations, and are
totally black-box, needing no knowledge of the model, no access to model
internals, no access to gradient, nor requiring any properties, such as
monotonicity, of the model.

</details>


### [23] [DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer](https://arxiv.org/abs/2507.23554)
*Ruoyu Wang,Junda Wu,Yu Xia,Tong Yu,Ryan A. Rossi,Julian McAuley,Lina Yao*

Main category: cs.AI

TL;DR: 论文提出DICE框架，通过动态选择上下文示例提升LLM代理的性能，解决了现有方法依赖启发式或任务特定设计的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文学习（ICL）的LLM代理在复杂推理和工具使用任务中表现优异，但其性能高度依赖演示示例的选择，缺乏通用且理论支持的选择标准。

Method: 提出DICE框架，通过因果视角分解演示知识为可转移和非转移部分，并设计逐步选择标准，确保代理性能提升。

Result: 实验表明DICE在多个领域有效且通用，可作为插件模块集成到现有代理框架中。

Conclusion: DICE为LLM代理提供了一种理论支持、通用的演示选择方法，显著提升了其鲁棒性和效率。

Abstract: Large language model-based agents, empowered by in-context learning (ICL),
have demonstrated strong capabilities in complex reasoning and tool-use tasks.
However, existing works have shown that the effectiveness of ICL is highly
sensitive to the choice of demonstrations, with suboptimal examples often
leading to unstable or degraded performance. While prior work has explored
example selection, including in some agentic or multi-step settings, existing
approaches typically rely on heuristics or task-specific designs and lack a
general, theoretically grounded criterion for what constitutes an effective
demonstration across reasoning steps. Therefore, it is non-trivial to develop a
principled, general-purpose method for selecting demonstrations that
consistently benefit agent performance. In this paper, we address this
challenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a
theoretically grounded ICL framework for agentic tasks that selects the most
relevant demonstrations at each step of reasoning. Our approach decomposes
demonstration knowledge into transferable and non-transferable components
through a causal lens, showing how the latter can introduce spurious
dependencies that impair generalization. We further propose a stepwise
selection criterion with a formal guarantee of improved agent performance.
Importantly, DICE is a general, framework-agnostic solution that can be
integrated as a plug-in module into existing agentic frameworks without any
additional training cost. Extensive experiments across diverse domains
demonstrate our method's effectiveness and generality, highlighting the
importance of principled, context-aware demo selection for robust and efficient
LLM agents.

</details>


### [24] [Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI](https://arxiv.org/abs/2507.23565)
*Botao Zhu,Xianbin Wang,Dusit Niyato*

Main category: cs.AI

TL;DR: 提出了一种基于语义信任链的自主信任编排方法，利用智能代理和超图技术优化分布式协作中的信任评估，提高资源利用率。


<details>
  <summary>Details</summary>
Motivation: 分布式协作中信任评估的复杂性和资源消耗问题，导致资源利用率低，影响任务执行效率。

Method: 采用智能代理和超图技术，通过历史数据和设备空闲期进行信任评估，并结合任务需求分析资源匹配度。

Result: 实验证明该方法能高效评估信任，平衡开销与准确性，支持大规模协作。

Conclusion: 提出的方法有效解决了信任评估的资源消耗问题，提升了分布式协作效率。

Abstract: In collaborative systems, the effective completion of tasks hinges on
task-specific trust evaluations of potential devices for distributed
collaboration. However, the complexity of tasks, the spatiotemporal dynamism of
distributed device resources, and the inevitable assessment overhead
dramatically increase the complexity and resource consumption of the trust
evaluation process. As a result, ill-timed or overly frequent trust evaluations
can reduce utilization rate of constrained resources, negatively affecting
collaborative task execution. To address this challenge, this paper proposes an
autonomous trust orchestration method based on a new concept of semantic
chain-of-trust. Our technique employs agentic AI and hypergraph to establish
and maintain trust relationships among devices. By leveraging its strengths in
autonomous perception, task decomposition, and semantic reasoning, we propose
agentic AI to perceive device states and autonomously perform trust evaluations
of collaborators based on historical performance data only during device idle
periods, thereby enabling efficient utilization of distributed resources. In
addition, agentic AI performs task-specific trust evaluations on collaborator
resources by analyzing the alignment between resource capabilities and task
requirements. Moreover, by maintaining a trust hypergraph embedded with trust
semantics for each device, agentic AI enables hierarchical management of
collaborators and identifies collaborators requiring trust evaluation based on
trust semantics, thereby achieving a balance between overhead and trust
accuracy. Furthermore, local trust hypergraphs from multiple devices can be
chained together to support multi-hop collaboration, enabling efficient
coordination in large-scale systems. Experimental results demonstrate that the
proposed method achieves resource-efficient trust evaluation.

</details>


### [25] [MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying](https://arxiv.org/abs/2507.23633)
*Qian Zhao,Zhuo Sun,Bin Guo,Zhiwen Yu*

Main category: cs.AI

TL;DR: 提出了一种基于策略引导的代理辅助记忆回忆方法，通过设计回忆策略将原始查询转化为线索丰富的查询，提升了记忆回忆性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法受限于记忆模块大小，无法完整获取记忆，影响了回忆效果。受记忆理论启发，通过有效线索主动激活相关记忆。

Method: 设计了5W回忆地图分类记忆查询，定义15种回忆策略模式，结合蒙特卡洛树搜索算法优化策略选择和响应生成，并微调开源大语言模型开发MemoCue代理。

Result: 在三个数据集上，MemoCue在回忆灵感方面优于基于LLM的方法17.74%，人类评估也验证了其在记忆回忆应用中的优势。

Conclusion: 策略引导的代理辅助记忆回忆方法显著提升了记忆回忆性能，具有实际应用潜力。

Abstract: Agent-assisted memory recall is one critical research problem in the field of
human-computer interaction. In conventional methods, the agent can retrieve
information from its equipped memory module to help the person recall
incomplete or vague memories. The limited size of memory module hinders the
acquisition of complete memories and impacts the memory recall performance in
practice. Memory theories suggest that the person's relevant memory can be
proactively activated through some effective cues. Inspired by this, we propose
a novel strategy-guided agent-assisted memory recall method, allowing the agent
to transform an original query into a cue-rich one via the judiciously designed
strategy to help the person recall memories. To this end, there are two key
challenges. (1) How to choose the appropriate recall strategy for diverse
forgetting scenarios with distinct memory-recall characteristics? (2) How to
obtain the high-quality responses leveraging recall strategies, given only
abstract and sparsely annotated strategy patterns? To address the challenges,
we propose a Recall Router framework. Specifically, we design a 5W Recall Map
to classify memory queries into five typical scenarios and define fifteen
recall strategy patterns across the corresponding scenarios. We then propose a
hierarchical recall tree combined with the Monte Carlo Tree Search algorithm to
optimize the selection of strategy and the generation of strategy responses. We
construct an instruction tuning dataset and fine-tune multiple open-source
large language models (LLMs) to develop MemoCue, an agent that excels in
providing memory-inspired responses. Experiments on three representative
datasets show that MemoCue surpasses LLM-based methods by 17.74% in recall
inspiration. Further human evaluation highlights its advantages in
memory-recall applications.

</details>


### [26] [Personalized Education with Ranking Alignment Recommendation](https://arxiv.org/abs/2507.23664)
*Haipeng Liu,Yuxuan Liu,Ting Long*

Main category: cs.AI

TL;DR: 论文提出了一种名为RAR的个性化问题推荐方法，通过将协作思想融入探索机制，解决了传统强化学习方法在训练中探索效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 个性化问题推荐旨在帮助学生掌握学习目标，但现有基于强化学习的方法在探索效率上表现不佳。

Method: 提出Ranking Alignment Recommendation (RAR)，将协作思想融入探索机制，提升训练效率。

Result: 实验表明，RAR显著提高了推荐性能，且框架适用于任何基于强化学习的推荐系统。

Conclusion: RAR通过改进探索机制，有效提升了问题推荐的性能，具有广泛适用性。

Abstract: Personalized question recommendation aims to guide individual students
through questions to enhance their mastery of learning targets. Most previous
methods model this task as a Markov Decision Process and use reinforcement
learning to solve, but they struggle with efficient exploration, failing to
identify the best questions for each student during training. To address this,
we propose Ranking Alignment Recommendation (RAR), which incorporates
collaborative ideas into the exploration mechanism, enabling more efficient
exploration within limited training episodes. Experiments show that RAR
effectively improves recommendation performance, and our framework can be
applied to any RL-based question recommender. Our code is available in
https://github.com/wuming29/RAR.git.

</details>


### [27] [TextQuests: How Good are LLMs at Text-Based Video Games?](https://arxiv.org/abs/2507.23701)
*Long Phan,Mantas Mazeika,Andy Zou,Dan Hendrycks*

Main category: cs.AI

TL;DR: TextQuests是一个基于互动小说的基准测试，用于评估AI代理在长上下文推理和自主问题解决中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能全面评估AI代理在探索性环境中的自主推理能力，因此需要新的测试方法。

Method: 利用Infocom互动小说游戏作为基准测试环境，禁止使用外部工具，专注于内在推理能力。

Result: TextQuests提供了一个评估AI代理在长会话中持续推理和问题解决能力的平台。

Conclusion: TextQuests填补了现有基准测试的空白，推动了AI代理在复杂环境中的自主推理能力发展。

Abstract: Evaluating AI agents within complex, interactive environments that mirror
real-world challenges is critical for understanding their practical
capabilities. While existing agent benchmarks effectively assess skills like
tool use or performance on structured tasks, they often do not fully capture an
agent's ability to operate autonomously in exploratory environments that demand
sustained, self-directed reasoning over a long and growing context. To spur the
development of agents capable of more robust intrinsic reasoning over long
horizons, we introduce TextQuests, a benchmark based on the Infocom suite of
interactive fiction games. These text-based adventures, which can take human
players over 30 hours and require hundreds of precise actions to solve, serve
as an effective proxy for evaluating AI agents on focused, stateful tasks. The
benchmark is specifically designed to assess an LLM agent's capacity for
self-contained problem-solving by precluding the use of external tools, thereby
focusing on intrinsic long-context reasoning capabilities in an exploratory
environment characterized by the need for trial-and-error learning and
sustained problem-solving within a single interactive session. We release
TextQuests at https://textquests.ai.

</details>


### [28] [Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving](https://arxiv.org/abs/2507.23726)
*Luoxin Chen,Jinming Gu,Liankai Huang,Wenhao Huang,Zhicheng Jiang,Allan Jie,Xiaoran Jin,Xing Jin,Chenggang Li,Kaijing Ma,Cheng Ren,Jiawei Shen,Wenlei Shi,Tong Sun,He Sun,Jiahui Wang,Siran Wang,Zhihong Wang,Chenrui Wei,Shufa Wei,Yonghui Wu,Yuchen Wu,Yihang Xia,Huajian Xin,Fan Yang,Huaiyuan Ying,Hongyi Yuan,Zheng Yuan,Tianyang Zhan,Chi Zhang,Yue Zhang,Ge Zhang,Tianyun Zhao,Jianqiu Zhao,Yichi Zhou,Thomas Hanwen Zhu*

Main category: cs.AI

TL;DR: Seed-Prover是一个基于Lean反馈的定理证明模型，通过迭代优化证明过程，显著提升了IMO级问题的解决能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在定理证明中因缺乏明确监督信号而表现不佳，而Lean等专用语言能提供清晰的验证反馈。

Method: 提出Seed-Prover模型，利用Lean反馈、已证引理和自我总结迭代优化证明，并设计三种推理策略。

Result: Seed-Prover在IMO问题上达到78.1%的证明率，显著优于之前方法，并在几何领域通过Seed-Geometry取得突破。

Conclusion: Seed-Prover展示了形式化验证与长链推理结合在自动数学推理中的有效性，实现了显著进展。

Abstract: LLMs have demonstrated strong mathematical reasoning abilities by leveraging
reinforcement learning with long chain-of-thought, yet they continue to
struggle with theorem proving due to the lack of clear supervision signals when
solely using natural language. Dedicated domain-specific languages like Lean
provide clear supervision via formal verification of proofs, enabling effective
training through reinforcement learning. In this work, we propose
\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover
can iteratively refine its proof based on Lean feedback, proved lemmas, and
self-summarization. To solve IMO-level contest problems, we design three
test-time inference strategies that enable both deep and broad reasoning.
Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F,
and achieves over 50\% on PutnamBench, outperforming the previous
state-of-the-art by a large margin. To address the lack of geometry support in
Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which
outperforms previous formal geometry engines. We use these two systems to
participate in IMO 2025 and fully prove 5 out of 6 problems. This work
represents a significant advancement in automated mathematical reasoning,
demonstrating the effectiveness of formal verification with long
chain-of-thought reasoning.

</details>


### [29] [CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks](https://arxiv.org/abs/2507.23751)
*Ping Yu,Jack Lanchantin,Tianlu Wang,Weizhe Yuan,Olga Golovneva,Ilia Kulikov,Sainbayar Sukhbaatar,Jason Weston,Jing Xu*

Main category: cs.AI

TL;DR: CoT-Self-Instruct是一种合成数据生成方法，通过链式思考（CoT）引导LLMs生成高质量合成提示，显著优于现有数据集。


<details>
  <summary>Details</summary>
Motivation: 提升LLM在可验证推理和非可验证指令任务中的表现。

Method: 基于种子任务，利用CoT生成合成提示，并通过自动指标过滤高质量数据。

Result: 在MATH500等可验证推理任务及AlpacaEval 2.0等非可验证任务中表现优异。

Conclusion: CoT-Self-Instruct能有效生成高质量训练数据，提升LLM性能。

Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that
instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the
given seed tasks, and then to generate a new synthetic prompt of similar
quality and complexity for use in LLM training, followed by filtering for
high-quality data with automatic metrics. In verifiable reasoning, our
synthetic data significantly outperforms existing training datasets, such as
s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For
non-verifiable instruction-following tasks, our method surpasses the
performance of human or standard self-instruct prompts on both AlpacaEval 2.0
and Arena-Hard.

</details>


### [30] [SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model](https://arxiv.org/abs/2507.23773)
*Mingkai Deng,Jinyu Hou,Yilin Shen,Hongxia Jin,Graham Neubig,Zhiting Hu,Eric Xing*

Main category: cs.AI

TL;DR: 论文提出SimuRA架构，通过世界模型模拟实现通用AI代理，克服自回归LLM的限制，在网页浏览任务中显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理局限于单任务单代理模式，缺乏通用性和可扩展性，且受限于自回归LLM的固有缺陷。人类通过心理模拟进行推理，启发研究者设计更通用的AI代理。

Method: 基于环境最优代理的理论框架，SimuRA引入世界模型进行模拟规划，利用LLM实现通用世界模型，支持多环境灵活规划。

Result: 在网页浏览任务中，SimuRA将航班搜索成功率从0%提升至32.2%，基于世界模型的规划比自回归规划优势高达124%。

Conclusion: SimuRA展示了世界模型模拟作为推理范式的优势，为训练单一通用LLM代理模型提供了可能性，并发布了网页浏览代理研究演示。

Abstract: AI agents built on large language models (LLMs) hold enormous promise, but
current practice focuses on a one-task-one-agent approach, which not only falls
short of scalability and generality, but also suffers from the fundamental
limitations of autoregressive LLMs. On the other hand, humans are general
agents who reason by mentally simulating the outcomes of their actions and
plans. Moving towards a more general and powerful AI agent, we introduce
SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based
on a principled formulation of optimal agent in any environment, \modelname
overcomes the limitations of autoregressive reasoning by introducing a world
model for planning via simulation. The generalized world model is implemented
using LLM, which can flexibly plan in a wide range of environments using the
concept-rich latent space of natural language. Experiments on difficult web
browsing tasks show that \modelname improves the success of flight search from
0\% to 32.2\%. World-model-based planning, in particular, shows consistent
advantage of up to 124\% over autoregressive planning, demonstrating the
advantage of world model simulation as a reasoning paradigm. We are excited
about the possibility for training a single, general agent model based on LLMs
that can act superintelligently in all environments. To start, we make SimuRA,
a web-browsing agent built on \modelname with pretrained LLMs, available as a
research demo for public testing.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [31] [A CPFSK Transceiver with Hybrid CSS-DSSS Spreading for LPWAN PHY Communication](https://arxiv.org/abs/2507.23029)
*Wenkun Wen,Ruiqi Zhang,Peiran Wu,Tierui Min,Minghua Xia*

Main category: cs.IT

TL;DR: 本文提出了一种新型LPWAN收发器，通过改进前导码设计和优化接收算法，实现了高接收灵敏度和低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统LPWAN收发器在深度覆盖和数据速率之间存在权衡，本文旨在解决这一问题。

Method: 采用CSS前导码和CPFSK调制，结合DSSS技术，开发了双峰检测和非相干解调方案。

Result: 仿真和实地测试表明，该收发器在接收灵敏度和复杂度上优于传统方案。

Conclusion: 该设计为LPWAN提供了一种高性能、低成本的解决方案。

Abstract: Traditional low-power wide-area network (LPWAN) transceivers typically
compromise data rates to achieve deep coverage. This paper presents a novel
transceiver that achieves high receiver sensitivity and low computational
complexity. At the transmitter, we replace the conventional direct sequence
spread spectrum (DSSS) preamble with a chirp spread spectrum (CSS) preamble,
consisting of a pair of down-chirp and up-chirp signals that are conjugate to
each other, simplifying packet synchronization. For enhanced coverage, the
payload incorporates continuous phase frequency shift keying (CPFSK) to
maintain a constant envelope and phase continuity, in conjunction with DSSS to
achieve a high spreading gain. At the receiver, we develop a double-peak
detection method to improve synchronization and a non-coherent joint
despreading and demodulation scheme that increases receiver sensitivity while
maintaining simplicity in implementation. Furthermore, we optimize the preamble
detection threshold and spreading sequences for maximum non-coherent receiver
performance. The software-defined radio (SDR) prototype, developed using GNU
Radio and USRP, along with operational snapshots, showcases its practical
engineering applications. Extensive Monte Carlo simulations and field-test
trials demonstrate that our transceiver outperforms traditional ones in terms
of receiver sensitivity, while also being low in complexity and cost-effective
for LPWAN requirements.

</details>


### [32] [Optimal compressed sensing for mixing stochastic processes](https://arxiv.org/abs/2507.23175)
*Yonatan Gutman,Adam Śpiewak*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Jalali and Poor introduced an asymptotic framework for compressed sensing of
stochastic processes, demonstrating that any rate strictly greater than the
mean information dimension serves as an upper bound on the number of random
linear measurements required for (universal) almost lossless recovery of
$\psi^*$-mixing processes, as measured in the normalized $L^2$ norm. In this
work, we show that if the normalized number of random linear measurements is
strictly less than the mean information dimension, then almost lossless
recovery of a $\psi^*$-mixing process is impossible by any sequence of
decompressors. This establishes the mean information dimension as the
fundamental limit for compressed sensing in this setting (and, in fact, the
precise threshold for the problem). To this end, we introduce a new quantity,
related to techniques from geometric measure theory: the correlation dimension
rate, which is shown to be a lower bound for compressed sensing of arbitrary
stationary stochastic processes.

</details>


### [33] [The Construction of Near-optimal Universal Coding of Integers](https://arxiv.org/abs/2507.23180)
*Wei Yan,Yunghsiang S. Han*

Main category: cs.IT

TL;DR: 论文提出了一种称为ν码的近乎最优通用整数编码（UCI），将最小扩展因子的范围缩小到2≤C_C∗≤2.0386，并构造了新的Δδ码，证明其与ν码在最小扩展因子方面是目前最优的。


<details>
  <summary>Details</summary>
Motivation: 研究通用整数编码（UCI）的最小扩展因子，以优化压缩性能。

Method: 提出ν码和Δδ码，并通过理论证明其最优性。

Result: 将最小扩展因子的范围缩小到2≤C_C∗≤2.0386，并证明ν码和Δδ码是目前最优的UCI。

Conclusion: ν码和Δδ码在最小扩展因子方面表现最优，为UCI的压缩性能提供了新的理论支持。

Abstract: Universal Coding of Integers (UCI) is suitable for discrete memoryless
sources with unknown probability distributions and infinitely countable
alphabet sizes. The UCI is a class of prefix codes, such that the ratio of the
average codeword length to $\max\{1, H(P)\}$ is within a constant expansion
factor $K_{\mathcal{C}}$ for any decreasing probability distribution $P$, where
$H(P)$ is the entropy of $P$. For any UCI code $\mathcal{C}$, define \emph{the
minimum expansion factor} $K_{\mathcal{C}}^{*}$ to represent the infimum of the
set of extension factors of $\mathcal{C}$. Each $\mathcal{C}$ has a unique
corresponding $K_{\mathcal{C}}^{*}$, and the smaller $K_{\mathcal{C}}^{*}$ is,
the better the compression performance of $\mathcal{C}$ is. A class of UCI
$\mathcal{C}$ (or family $\{\mathcal{C}_i\}_{i=1}^{\infty}$) achieving the
smallest $K_{\mathcal{C}}^{*}$ is defined as the \emph{optimal UCI}. The best
result currently is that the range of $C_{\mathcal{C}}^{*}$ for the optimal UCI
is $2\leq C_{\mathcal{C}}^{*}\leq 2.5$. In this paper, we prove that there
exists a class of near-optimal UCIs, called $\nu$ code, to achieve
$K_\nu=2.0386$. This narrows the range of the minimum expansion factor for
optimal UCI to $2\leq C_{\mathcal{C}}^{*}\leq 2.0386$. Another new class of
UCI, called $\Delta\delta$ code, is specifically constructed. We show that the
$\Delta\delta$ code and $\nu$ code are currently optimal in terms of minimum
expansion factor. In addition, we propose a new proof that shows the minimum
expansion factor of the optimal UCI is lower bounded by $2$.

</details>


### [34] [From Link Diversity to Cross-Band Feedback Collaboration: A New Perspective on Hybrid Optical-RF Systems](https://arxiv.org/abs/2507.23686)
*Menghan Li,Yulin Shao,Runxin Zhang,Lu Lu*

Main category: cs.IT

TL;DR: 论文提出了一种新的混合光-射频（O-RF）系统架构，利用光学下行链路实现实时反馈编码，提升射频上行链路的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为混合O-RF系统主要是通过切换链路实现多样性，而本文提出了一种新的思路：利用光学下行链路作为功能性的上行链路可靠性增强手段。

Method: 提出了一种名为O-RF-CBF的新架构，通过光学下行链路反馈指导射频上行链路的自适应编码。

Result: 数值结果表明，O-RF-CBF相比传统O-RF系统显著提升了上行链路的吞吐量。

Conclusion: 跨频段协作而非冗余是释放混合无线网络潜力的关键。

Abstract: We suggest a re-examination of the conventional view that hybrid
optical-radio frequency (O-RF) systems are primarily diversity-driven networks
that switch between RF and optical links for robustness. Instead, we uncover a
new architectural opportunity: repurposing the optical downlink to enable
real-time feedback channel coding over the RF uplink, where structured decoder
feedback is delivered from the access point to guide the transmitter's coding
strategy. This insight marks a conceptual paradigm shift from passive link
diversity to active cross-band collaboration, where the wideband,
interference-free optical wireless communication (OWC) is no longer merely a
downlink backup but a functional enabler of uplink reliability. To realize this
vision, we propose a novel architecture, O-RF with Cross-Band Feedback
(O-RF-CBF), that exploits the optical downlink feedback to facilitate adaptive
RF uplink coding. Numerical results reveal that O-RF-CBF achieves significant
uplink throughput gains over traditional O-RF systems. Our findings highlight
that inter-band synergy, not redundancy, is the key to unlocking the full
potential of hybrid wireless networks.

</details>


### [35] [Efficient DFT of Zadoff-Chu Sequences using lmFH Pattern](https://arxiv.org/abs/2507.23200)
*Fanping Du*

Main category: cs.IT

TL;DR: 本文通过线性微频跳（lmFH）模式直观展示了ZC序列的DFT和IDFT计算，并引入了一种基于广义二次高斯和的累积和计算方法，进一步将ZC序列的DFT转化为lmFH符号。


<details>
  <summary>Details</summary>
Motivation: 探索ZC序列的线性微频跳特性，简化其DFT和IDFT的计算过程，并提供新的计算方法。

Method: 利用lmFH模式计算ZC序列的DFT和IDFT，引入广义二次高斯和计算累积和，并将DFT转化为lmFH符号。

Result: 发现ZC序列的DFT可通过累积频点计算，类似于普通mFH符号的计算方式。

Conclusion: ZC序列的DFT计算可通过lmFH模式简化，为相关领域提供了新的计算思路。

Abstract: Having established that Zadoff-Chu (ZC) sequences are inherently linear
micro-frequency hopping (lmFH) symbols, this paper first presents an intuitive
and visual exposition of the computation of the DFT and IDFT of ZC sequences
using the lmFH pattern. This yields interesting results. Subsequently, an
alternative form for computing the cumulative sum of ZC sequences using the
Generalized Quadratic Gauss Sum is introduced. Furthermore, building on the
micro-frequency hopping (mFH) concept, this paper shows that the DFT of ZC
sequences can be transformed into an lmFH symbol with frequency shift and phase
offset. Therefore, the DFT of ZC sequences can be computed via cumulative
frequency points, similar to the computation of normal mFH symbols.

</details>


### [36] [Secure Integrated Sensing and Communication Networks: Stochastic Performance Analysis](https://arxiv.org/abs/2507.23234)
*Marziyeh Soltani,Mahtab Mirmohseni,Rahim Tafazolli*

Main category: cs.IT

TL;DR: 本文分析了多输入多输出（MIMO）集成感知与通信（ISAC）系统在下行链路场景中的随机安全性能，考虑了通信和感知的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保证安全和隐私的同时，实现通信与感知的多功能信号传输，并应对窃听威胁。

Method: 推导了遍历保密率（ESR）和遍历克拉美罗下界（CRB），利用中心极限定理计算信噪比（SNR）和CRB的概率密度函数（PDF）。

Result: 表征了CRB-保密率区域的边界，揭示了通信与感知之间的性能权衡。

Conclusion: 在随机ISAC网络中，通过多功能信号传输，可以同时实现通信、感知和安全性，但需权衡性能。

Abstract: This paper analyzes the stochastic security performance of a multiple-input
multiple-output (MIMO) integrated sensing and communication (ISAC) system in a
downlink scenario. A base station (BS) transmits a multi-functional signal to
simultaneously communicate with a user, sense a target's angular location, and
counteract eavesdropping threats. The attack model considers a passive
single-antenna communication eavesdropper intercepting communication data, as
well as a multi-antenna sensing eavesdropper attempting to infer the target's
location. We also consider a malicious target scenario where the target plays
the role of the communication eavesdropper. The BS-user and BS-eavesdroppers
channels follow Rayleigh fading, while the target's azimuth angle is uniformly
distributed. To evaluate the performance in this random network, we derive the
ergodic secrecy rate (ESR) and the ergodic Cramer-Rao lower bound (CRB), for
target localization, at both the BS and the sensing eavesdropper. This involves
computing the probability density functions (PDFs) of the signal-to-noise ratio
(SNR) and CRB, leveraging the central limit theorem for tractability. We
characterize the boundary of the CRB-secrecy rate region, and interpret the
performance tradeoffs between communication and sensing while guaranteeing a
level of security and privacy in the random ISAC networks.

</details>


### [37] [Exploiting Movable Elements of Intelligent Reflecting Surface for Enhancement of Integrated Sensing and Communication](https://arxiv.org/abs/2507.23296)
*Xingyu Peng,Qin Tao,Yong Liang Guan,Xiaoming Chen*

Main category: cs.IT

TL;DR: 利用智能反射面（IRS）的可移动元素提升集成感知与通信（ISAC）系统的性能，包括单用户和多用户场景的优化方案。


<details>
  <summary>Details</summary>
Motivation: 通过IRS的可移动元素优化ISAC系统的通信速率和感知精度，并扩展其覆盖范围。

Method: 1. 单用户场景下分析可移动元素的功能，设计联合波束成形和元素位置优化方案；2. 扩展到多用户场景，提出基于性能表达式的元素位置优化方案。

Result: 仿真结果表明，IRS元素的移动可提高通信速率和感知精度，并显著扩展ISAC的覆盖范围。

Conclusion: IRS的可移动元素为ISAC系统性能提升提供了有效途径。

Abstract: In this paper, we propose to exploit movable elements of intelligent
reflecting surface (IRS) to enhance the overall performance of integrated
sensing and communication (ISAC) systems. Firstly, focusing on a single-user
scenario, we reveal the function of movable elements by performance analysis,
and then design a joint beamforming and element position optimization scheme.
Further, we extend it to a general multi-user scenario, and also propose an
element position optimization scheme according to the derived performance
expressions. Finally, simulation results confirm that the movement of IRS
elements can improve the communication rate and the sensing accuracy, and
especially broaden the coverage of ISAC.

</details>


### [38] [Hybrid Generative Semantic and Bit Communications in Satellite Networks: Trade-offs in Latency, Generation Quality, and Computation](https://arxiv.org/abs/2507.23528)
*Chong Huang,Gaojie Chen,Jing Zhu,Qu Luo,Pei Xiao,Wei Huang,Rahim Tafazolli*

Main category: cs.IT

TL;DR: 提出了一种多层混合比特与生成语义通信框架，结合深度强化学习优化资源分配，并引入新的语义通信效率指标（SEM）以平衡效率与性能。


<details>
  <summary>Details</summary>
Motivation: 卫星通信在未来无线网络中日益重要，但存在链路预算有限的问题，语义通信虽能解决此问题，却增加了计算资源消耗。

Method: 提出多层混合比特与生成语义通信框架，引入SEM指标，并利用GRPO算法优化资源分配。

Result: 仿真结果表明框架灵活，SEM指标有效，揭示了语义通信指标间的关系。

Conclusion: 该框架和SEM指标为动态卫星通信网络提供了高效解决方案。

Abstract: As satellite communications play an increasingly important role in future
wireless networks, the issue of limited link budget in satellite systems has
attracted significant attention in current research. Although semantic
communications emerge as a promising solution to address these constraints, it
introduces the challenge of increased computational resource consumption in
wireless communications. To address these challenges, we propose a multi-layer
hybrid bit and generative semantic communication framework which can adapt to
the dynamic satellite communication networks. Furthermore, to balance the
semantic communication efficiency and performance in satellite-to-ground
transmissions, we introduce a novel semantic communication efficiency metric
(SEM) that evaluates the trade-offs among latency, computational consumption,
and semantic reconstruction quality in the proposed framework. Moreover, we
utilize a novel deep reinforcement learning (DRL) algorithm group relative
policy optimization (GRPO) to optimize the resource allocation in the proposed
network. Simulation results demonstrate the flexibility of our proposed
transmission framework and the effectiveness of the proposed metric SEM,
illustrate the relationships among various semantic communication metrics.

</details>


### [39] [Cell-Free Massive MIMO SWIPT with Beyond Diagonal Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2507.23702)
*Duc Thien Hua,Mohammadali Mohammadi,Hien Quoc Ngo,Michail Matthaiou*

Main category: cs.IT

TL;DR: 论文研究了在无小区大规模MIMO系统中集成超对角可重构智能表面（BDRIS）以增强同时无线信息和能量传输（SWIPT），提出了优化算法并展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了在不牺牲时频资源的情况下同时支持能量接收器（ERs）和信息接收器（IRs），研究探索了BDRIS的应用潜力。

Method: 采用保护性部分零迫预编码技术，提出联合优化AP选择、功率控制和BDRIS散射矩阵设计的算法，包括启发式搜索、逐次凸近似和深度强化学习。

Result: 数值结果表明，BDRIS在能量收集方面显著优于传统对角RIS，启发式散射矩阵设计可实现平均能量收集量七倍提升。

Conclusion: BDRIS在SWIPT系统中具有显著优势，优化算法有效提升了系统性能。

Abstract: We investigate the integration of beyond diagonal reconfigurable intelligent
surfaces (BDRISs) into cell free massive multiple input multiple output
(CFmMIMO) systems to enhance simultaneous wireless information and power
transfer (SWIPT). To simultaneously support two groups of users energy
receivers (ERs) and information receivers (IRs) without sacrificing time
frequency resources, a subset of access points (APs) is dedicated to serving
ERs with the aid of a BDRIS, while the remaining APs focus on supporting IRs. A
protective partial zero forcing precoding technique is implemented at the APs
to manage the non coherent interference between the ERs and IRs. Subsequently,
closed form expressions for the spectral efficiency of the IRs and the average
sum of harvested energy at the ERs are leveraged to formulate a comprehensive
optimization problem. This problem jointly optimizes the AP selection, AP power
control, and scattering matrix design at the BDRIS, all based on long term
statistical channel state information. This challenging problem is then
effectively transformed into more tractable forms. To solve these sub problems,
efficient algorithms are proposed, including a heuristic search for the
scattering matrix design, as well as successive convex approximation and deep
reinforcement learning methods for the joint AP mode selection and power
control design. Numerical results show that a BDRIS with a group or fully
connected architecture achieves significant energy harvesting gains over the
conventional diagonal RIS, especially delivering up to a seven fold increase in
the average sum of harvested energy when a heuristic based scattering matrix
design is employed.

</details>
