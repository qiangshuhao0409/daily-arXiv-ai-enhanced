<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning](https://arxiv.org/abs/2507.07118)
*Zelin Zhu,Kai Yang,Rui Zhang*

Main category: cs.NI

TL;DR: 论文提出了一种基于深度学习的联合定位与感知优化方法，利用MIMO-OFDM系统的高维CSI特性，通过SPG-MIBO算法实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 现代无线网络中，高性能的定位与感知技术对智能应用至关重要，但现有方法在高维CSI下的联合建模研究不足。

Method: 将定位与感知建模为混合整数双层深度学习问题，提出SPG-MIBO算法，支持高维大规模数据集的高效训练。

Result: 多数据集实验验证了算法的有效性，并展示了联合优化的性能提升。

Conclusion: SPG-MIBO算法为高维CSI下的定位与感知任务提供了高效解决方案，具有理论和实验支持。

Abstract: Wireless localization and sensing technologies are essential in modern
wireless networks, supporting applications in smart cities, the Internet of
Things (IoT), and autonomous systems. High-performance localization and sensing
systems are critical for both network efficiency and emerging intelligent
applications. Integrating channel state information (CSI) with deep learning
has recently emerged as a promising solution. Recent works have leveraged the
spatial diversity of multiple input multiple output (MIMO) systems and the
frequency granularity of orthogonal frequency division multiplexing (OFDM)
waveforms to improve spatial resolution. Nevertheless, the joint modeling of
localization and sensing under the high-dimensional CSI characteristics of
MIMO-OFDM systems remains insufficiently investigated. This work aims to
jointly model and optimize localization and sensing tasks to harness their
potential synergy. We first formulate localization and sensing as a
mixed-integer bilevel deep learning problem and then propose a novel stochastic
proximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)
algorithm. SPG-MIBO is well-suited for high-dimensional and large-scale
datasets, leveraging mini-batch training at each step for computational and
memory efficiency. The algorithm is also supported by theoretical convergence
guarantees. Extensive experiments on multiple datasets validate its
effectiveness and highlight the performance gains from joint localization and
sensing optimization.

</details>


### [2] [DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training](https://arxiv.org/abs/2507.07149)
*Renyuan Liu,Yuyang Leng,Kaiyan Liu,Shaohan Hu,Chun-Fu,Chen,Peijun Zhao,Heechul Yun,Shuochao Yao*

Main category: cs.NI

TL;DR: DAF框架通过系统级优化实现高效的动态激活量化，显著减少内存使用并加速训练，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 解决移动和边缘设备在训练深度神经网络时的内存限制问题，尤其是激活数据的内存占用和动态量化部署中的系统级挑战。

Method: 提出DAF框架，包括混合归约操作、CPU-GPU协作位打包和重要性感知分页内存管理，优化动态量化训练。

Result: 在多种深度学习模型和平台上，内存使用减少22.9倍，速度提升3.2倍，且不影响训练精度。

Conclusion: DAF为资源受限环境提供了一种可扩展且实用的高效训练解决方案。

Abstract: Recent advancements in on-device training for deep neural networks have
underscored the critical need for efficient activation compression to overcome
the memory constraints of mobile and edge devices. As activations dominate
memory usage during training and are essential for gradient computation,
compressing them without compromising accuracy remains a key research
challenge. While existing methods for dynamic activation quantization promise
theoretical memory savings, their practical deployment is impeded by
system-level challenges such as computational overhead and memory
fragmentation.
  To address these challenges, we introduce DAF, a Dynamic Activation Framework
that enables scalable and efficient on-device training through system-level
optimizations. DAF achieves both memory- and time-efficient dynamic
quantization training by addressing key system bottlenecks. It develops hybrid
reduction operations tailored to the memory hierarchies of mobile and edge
SoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic
quantization, and implements an importance-aware paging memory management
scheme to reduce fragmentation and support dynamic memory adjustments.
  These optimizations collectively enable DAF to achieve substantial memory
savings and speedup without compromising model training accuracy. Evaluations
on various deep learning models across embedded and mobile platforms
demonstrate up to a $22.9\times$ reduction in memory usage and a $3.2\times$
speedup, making DAF a scalable and practical solution for resource-constrained
environments.

</details>


### [3] [PHandover: Parallel Handover in Mobile Satellite Network](https://arxiv.org/abs/2507.07437)
*Jiasheng Wu,Shaojie Su,Wenjun Zhu,Xiong Wang,Jingjing Zhang,Xingqiu He,Yue Gao*

Main category: cs.NI

TL;DR: 论文提出了一种并行切换机制，显著降低了低地球轨道卫星网络中的切换延迟，通过基于计划的切换和机器学习预测信号强度，实验显示延迟减少21倍。


<details>
  <summary>Details</summary>
Motivation: 解决低地球轨道卫星网络中因高速移动导致的高延迟频繁切换问题，提升延迟敏感应用的性能。

Method: 采用基于计划的切换机制，引入卫星同步功能（SSF），结合机器学习信号预测和高效切换调度算法。

Result: 实验表明，切换延迟减少21倍，网络稳定性和用户性能显著提升。

Conclusion: 提出的并行切换机制有效解决了卫星网络中的高延迟切换问题，具有实际应用潜力。

Abstract: The construction of Low Earth Orbit (LEO) satellite constellations has
recently attracted tremendous attention from both academia and industry. The 5G
and 6G standards have identified LEO satellite networks as a key component of
future mobile networks. However, due to the high-speed movement of satellites,
ground terminals often experience frequent and high-latency handovers, which
significantly deteriorate the performance of latency-sensitive applications. To
address this challenge, we propose a parallel handover mechanism for mobile
satellite networks that can considerably reduce handover latency. The main idea
is to employ plan-based handovers instead of measurement-based handovers to
avoid interactions between the access and core networks, thereby eliminating
the significant time overhead associated with traditional handover procedures.
Specifically, we introduce a novel network function named the Satellite
Synchronized Function (SSF), which is designed to be fully compliant with the
standard 5G core network. In addition, we propose a machine learning model for
signal strength prediction, coupled with an efficient handover scheduling
algorithm. We have conducted extensive experiments, and the results demonstrate
that our proposed handover scheme can reduce handover latency by 21\times
compared to the standard NTN handover scheme and two other existing handover
approaches, along with significant improvements in network stability and
user-level performance.

</details>


### [4] [HaLert: A Resilient Smart City Architecture for Post-Disaster Based on Wi-Fi HaLow Mesh and SDN](https://arxiv.org/abs/2507.07841)
*Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.NI

TL;DR: HaLert是一种基于Wi-Fi HaLow IEEE 802.11s网状网络的智能城市弹性架构，用于灾难后通信。结合SDN和LoRa网络，支持远程监控和配置，测试表明其稳定性和功能性。


<details>
  <summary>Details</summary>
Motivation: 灾难事件通常不可预测，智能城市的密集IoT网络为通信基础设施的再利用提供了潜力，以支持灾后通信和警报传递。

Method: 提出HaLert架构，基于Wi-Fi HaLow网状网络和SDN，结合LoRa网络实现远程监控和配置。开发原型并在真实城市环境中测试。

Result: Wi-Fi HaLow网络在障碍物和地形影响下仍保持稳定（延迟15-54.8 ms，吞吐量134-726 Kbps）。LoRa网络消息成功率高达94.96%。

Conclusion: HaLert架构在灾难场景下表现出高稳定性和功能性，适合智能城市的应急通信需求。

Abstract: Events such as catastrophes and disasters are, in most cases, unpredictable.
Consequently, reusing existing infrastructures to develop alternative
communication strategies after disasters is essential to minimise the impact of
these events on the population's ability to communicate and promptly receive
alerts from authorities. In this context, the emergence of smart cities,
characterised by dense and geographically distributed IoT networks, presents
significant potential for such reuse. This work proposes HaLert, a resilient
architecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network,
whose resources can be readily reallocated to support a emergency communication
system to exchange messages (including text, location, image, audio, and video)
between citizens, authorities, and between both parties. To facilitate remote
monitoring and configuration of the network, the architecture incorporates the
SDN (Software-Defined Networking) paradigm, supported by a LoRa controlled
flooding mesh network. A prototype was developed based on this architecture and
tested in a real urban scenario comprising both indoor and outdoor
environments. The results demonstrated that, despite the significant impact of
obstacles, lack of line-of-sight, and terrain slopes on the latency (average
latency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and
726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow
network, it remained stable and resilient, successfully providing all
functionalities associated with the HaLert architecture. The tests conducted on
the LoRa network revealed a high average message success rate of 94.96%.

</details>


### [5] [Energy Transfer and Data Collection from Batteryless Sensors in Low-altitude Wireless Networks](https://arxiv.org/abs/2507.07481)
*Wen Zhang,Aimin Wang,Jiahui Li,Geng Sun,Jiacheng Wang,Weijie Yuan,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出了一种无人机辅助的无电池传感器网络数据收集与无线能量传输框架，通过多目标优化和强化学习算法提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决高温等极端环境下传统无线能量传输和电池的局限性，实现高效数据收集。

Method: 结合无人机能量传输与OFDMA数据收集，采用SAC-PPV算法优化功率分配与飞行轨迹。

Result: 仿真显示该方法在多种网络配置下优于基准算法。

Conclusion: 无人机辅助框架与SAC-PPV算法为极端环境下的传感器网络提供了高效解决方案。

Abstract: The integration of wireless power transfer (WPT) with Internet of Things
(IoT) offers promising solutions for sensing applications, but faces
significant challenges when deployed in hard-to-access areas such as
high-temperature environments. In such extreme conditions, traditional fixed
WPT infrastructure cannot be safely installed, and batteries rapidly degrade
due to hardware failures. In this paper, we propose an uncrewed aerial vehicle
(UAV)-assisted data collection and WPT framework for batteryless sensor (BLS)
networks deployed in these challenging environments. Specifically, we consider
a practical scenario where a UAV first transfers energy to BLS nodes via WPT,
enabling these nodes to subsequently transmit their collected data to the UAV
through orthogonal frequency-division multiple access (OFDMA). Then, we
formulate a multi-objective optimization problem that aims to maximize the fair
data collection volume while minimizing the UAV energy consumption through
joint optimization of transmit power allocation and flight trajectory planning.
Due to the non-convex nature and dynamic characteristics of this problem,
conventional optimization methods prove inadequate. To address these
challenges, we propose an enhanced soft actor-critic algorithm with
parameter-free attention, prioritized experience replay, and value-based reward
centering (SAC-PPV), thereby improving the exploration efficiency and learning
stability of the algorithm in complex WPT scenarios. Simulation results
demonstrate that the proposed approach consistently outperforms benchmark
algorithms under various network configurations.

</details>


### [6] [A Fragmentation-Aware Adaptive Bilevel Search Framework for Service Mapping in Computing Power Networks](https://arxiv.org/abs/2507.07535)
*Jingzhao Xie,Zhenglian Li,Gang Sun,Long Luo,Hongfang Yu,Dusit Niyato*

Main category: cs.NI

TL;DR: 论文提出了一种名为ABS的模块化框架，用于解决计算能力网络（CPN）中的服务映射问题，显著提高了资源利用率和服务接受率。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能完全实现CPN所设想的通过网络协调整合计算资源的目标，特别是在优化服务映射以提高资源效率和服务满意度方面存在挑战。

Method: 提出了自适应双层搜索（ABS）框架，包括基于图划分的重新表述、双层优化架构和碎片感知评估。

Result: 在多样化的CPN场景中，ABS表现优于现有方法，计算资源利用率提高了73.2%，服务接受率提高了60.2%。

Conclusion: ABS为解决CPN中的服务映射问题提供了有效方法，显著提升了资源利用和服务质量。

Abstract: Computing Power Network (CPN) unifies wide-area computing resources through
coordinated network control, while cloud-native abstractions enable flexible
resource orchestration and on-demand service provisioning atop the elastic
infrastructure CPN provides. However, current approaches fall short of fully
integrating computing resources via network-enabled coordination as envisioned
by CPN. In particular, optimally mapping services to an underlying
infrastructure to maximize resource efficiency and service satisfaction remains
challenging. To overcome this challenge, we formally define the service mapping
problem in CPN, establish its theoretical intractability, and identify key
challenges in practical optimization. We propose Adaptive Bilevel Search (ABS),
a modular framework featuring (1) graph partitioning-based reformulation to
capture variable coupling, (2) a bilevel optimization architecture for
efficient global exploration with local optimality guarantees, and (3)
fragmentation-aware evaluation for global performance guidance. Implemented
using distributed particle swarm optimization, ABS is extensively evaluated
across diverse CPN scenarios, consistently outperforming existing approaches.
Notably, in complex scenarios, ABS achieves up to 73.2% higher computing
resource utilization and a 60.2% higher service acceptance ratio compared to
the best-performing baseline.

</details>


### [7] [Can cloud-based VR streaming handle Wi-Fi OBSS contention?](https://arxiv.org/abs/2507.07677)
*Miguel Casasnovas,Marc Carrascosa-Zamacois,Boris Bellalta*

Main category: cs.NI

TL;DR: 研究分析了Wi-Fi网络中邻频干扰对VR流媒体的负面影响，发现不同重叠信道情况下的性能差异，并提出NeSt-VR算法有效缓解干扰。


<details>
  <summary>Details</summary>
Motivation: 探讨Wi-Fi网络中邻频干扰对VR流媒体的影响，特别是在80 MHz信道中的部分和完全重叠情况。

Method: 实验分析不同重叠信道配置下的VR流媒体性能，并测试NeSt-VR算法的有效性。

Result: 邻频干扰加剧会降低VR性能，不同重叠配置影响不同；NeSt-VR算法能显著改善性能。

Conclusion: 邻频干扰对VR流媒体影响显著，但通过NeSt-VR算法可有效缓解干扰，提升性能。

Abstract: This paper experimentally analyzes the negative impact of contention caused
by neighboring Wi-Fi networks operating on overlapping channels on Virtual
Reality (VR) streaming over Wi-Fi, focusing on scenarios of partial and full
channel overlap within an 80 MHz channel. Our results show that (i) increasing
the number of 80 MHz Overlapping Basic Service Sets (OBSSs) intensifies
contention and degrades VR streaming performance; (ii) OBSS activity on the
secondary-sided 40 MHz portion degrades performance more than activity on the
primary-sided 40 MHz portion; (iii) for the same aggregate load, full channel
overlap with two 40 MHz OBSS contenders is less detrimental than partial
overlap with a single high-load 40 MHz contender, but more disruptive than full
overlap with two 80 MHz contenders; and (iv) full channel overlap with two 40
MHz OBSS contenders has a smaller impact on VR streaming under symmetric
traffic loads than under asymmetric loads. Moreover, our results demonstrate
that our previously proposed Network-aware Step-wise adaptive bitrate algorithm
for VR streaming (NeSt-VR) effectively mitigates performance degradation in
OBSS environments, enabling VR streaming under heavier OBSS traffic conditions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas,Mehmet Mercangoz*

Main category: cs.AI

TL;DR: 论文提出了一种结合符号推理与自适应控制的统一代理框架，利用大语言模型（LLMs）实现离散故障恢复规划和连续过程控制，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代化学过程日益复杂，劳动力短缺和故障场景多样化，需要新的自动化范式来结合符号推理与自适应控制。

Method: 采用有限状态机（FSMs）作为可解释的操作框架，由LLM驱动的规划代理提出恢复序列，模拟代理执行和检查每个转换，验证-重新提示循环迭代优化无效计划。

Result: 在案例研究中，GPT-4o和GPT-4o-mini在180个随机生成的FSMs中实现100%有效路径成功率；在双加热器控制中，LLM控制器性能与经典PID控制相当。

Conclusion: 研究表明，通过结构化反馈和模块化代理，LLMs可以统一高层符号规划和低层连续控制，为化学工程中的语言驱动自动化铺平道路。

Abstract: The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [9] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran,Shuang Chen,Jingjing Deng,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 论文提出了一种名为BOOST的新方法，通过动态调整温度缩放和采样概率，解决AI模型在艺术分类中的偏见问题，特别是在处理分布外数据时。


<details>
  <summary>Details</summary>
Motivation: AI模型在艺术分类中存在偏见，主要源于数据集不平衡，导致某些艺术风格主导，影响模型的公平性和准确性。

Method: 提出BOOST方法，动态调整温度缩放和采样概率，以更公平地表示所有类别。

Result: 在KaoKore和PACS数据集上评估，BOOST能有效减少类别偏见，并提出新指标SODC评估效果。

Conclusion: BOOST在保持高性能的同时实现了公平性，为艺术领域的AI模型去偏见提供了可靠解决方案。

Abstract: The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [10] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim,Junsik Kim,Hwidong Bae,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 论文提出了一种基于状态推断的提示方法（SIBP），用于解决大型语言模型在规则驱动的交易系统中的问题，显著提高了状态合规性、引用准确性和计算精度。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在动态游戏交互中表现不佳，尤其是在规则驱动的交易系统中，常出现规则违反（如物品幻觉和计算错误），影响玩家信任。

Method: SIBP方法通过自主对话状态推断和上下文特定规则遵守，将交易分解为六个状态，并采用上下文感知的物品引用和基于占位符的价格计算。

Result: 在100个交易对话的评估中，SIBP实现了>97%的状态合规性、>95%的引用准确性和99.7%的计算精度，计算效率高且优于基线方法。

Conclusion: SIBP为商业游戏中可信赖的NPC交互提供了实用基础。

Abstract: Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [11] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 论文探讨了如何利用神经符号方法在数据稀疏且不可靠的供应链中检测非法活动，并比较了人工与自动化特征提取的效果。


<details>
  <summary>Details</summary>
Motivation: 供应链网络复杂且涉及非法活动时分析困难，传统机器学习方法需要大量训练数据，但非法供应链数据稀疏且不可靠。

Method: 采用神经符号方法，提出基于问题树的LLM查询方法，系统评估人工与机器分类新闻文章的差异。

Result: 研究发现神经符号方法能有效在稀疏数据中识别非法活动模式。

Conclusion: 神经符号方法为供应链非法活动检测提供了新思路，尤其在数据稀疏情况下表现突出。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [12] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: 介绍了一个名为cmbagent的多智能体系统，用于自动化科学研究任务，由约30个大型语言模型（LLM）智能体组成，采用规划与控制策略协调工作流程，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 旨在通过多智能体系统实现科学研究任务的自动化，减少人工干预，提高效率。

Method: 系统由多个专用智能体组成，各自执行不同任务（如检索科学论文和代码库、编写代码、解释结果等），并通过本地执行代码实现任务。

Result: 成功应用于博士级别的宇宙学任务，并在两个基准测试中表现优于现有最先进的LLM。

Conclusion: cmbagent展示了多智能体系统在科学研究自动化中的潜力，未来可通过云部署进一步扩展应用。

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [13] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

Main category: cs.AI

TL;DR: 本文研究了在多智能体强化学习中利用大型语言模型作为专家规划器以实现高效探索的方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的高效探索问题因其复杂性而尤为突出，本文旨在探索利用专家规划（如大型语言模型）来解决这一问题。

Method: 通过将大型语言模型作为专家规划器，应用于基于规划的多智能体任务中，以实现高效的环境探索。

Result: 研究表明，大型语言模型在多智能体任务中能够有效提升探索效率。

Conclusion: 利用大型语言模型作为专家规划器是多智能体强化学习中高效探索的一种可行方法。

Abstract: Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [14] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: ViDove是一种基于多模态输入的翻译代理系统，结合视觉和上下文背景信息提升翻译质量，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有LLM翻译代理仅限于文本输入，无法利用视觉和上下文信息，限制了翻译质量。

Method: ViDove采用多模态记忆系统和长短时记忆模块，结合领域知识，优化翻译过程。

Result: 在字幕生成和通用翻译任务中，ViDove的BLEU分数提升28%，SubER提升15%。

Conclusion: ViDove通过多模态输入显著提升翻译质量，并推出新基准DoveBench。

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [15] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball,Greg Gluch,Shafi Goldwasser,Frauke Kreuter,Omer Reingold,Guy N. Rothblum*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLM）的安全对齐问题，重点关注输入和输出过滤的挑战。结果表明，高效过滤在计算上不可行，且安全不能仅依赖外部过滤器。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，其可能被滥用于生成有害内容的问题引发关注，研究旨在探讨如何通过过滤技术实现安全对齐。

Method: 研究分析了输入提示和输出内容的过滤效率，基于密码学假设证明了其计算上的不可行性。

Result: 发现高效过滤输入和输出在计算上不可行，且外部过滤器无法确保安全。

Conclusion: 安全对齐需结合LLM内部设计，仅依赖黑盒访问或外部过滤器不足以保证安全。

Abstract: With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [16] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai,Haoyu Wang,Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haifeng Chen,Yanjie Fu*

Main category: cs.AI

TL;DR: Sim-to-Dec框架结合生成模拟和智能决策，提升供应链运输的响应速度和经济效率。


<details>
  <summary>Details</summary>
Motivation: 供应链运输中的高响应性和经济效率受运输模式决策影响，需一种低风险、可观察的环境进行策略设计。

Method: 提出Sim-to-Dec框架，包括生成模拟模块（基于自回归建模）和双感知决策模型（结合历史与预测），通过端到端优化迭代优化。

Result: 在三个真实数据集上实验表明，Sim-to-Dec显著提高了准时交付率和利润。

Conclusion: Sim-to-Dec满足跨场景通用性、细粒度动态模拟、历史与预测结合等需求，有效优化运输策略。

Abstract: High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


### [17] [DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search](https://arxiv.org/abs/2507.07426)
*Zerui Yang,Yuwei Wan,Yinqiao Li,Yudai Matsuda,Tong Xie,Linqi Song*

Main category: cs.AI

TL;DR: DrugMCTS框架结合RAG、多智能体协作和蒙特卡洛树搜索，显著提升药物重定位任务性能，无需领域微调。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在科学领域推理能力受限的问题，克服传统方法计算开销高或未能充分利用结构化数据的局限性。

Method: 提出DrugMCTS框架，整合RAG、多智能体协作和蒙特卡洛树搜索，利用五个专业智能体检索和分析分子与蛋白质信息。

Result: 在DrugBank和KIBA数据集上，DrugMCTS性能超越通用大语言模型和深度学习基线，召回率和鲁棒性显著提高。

Conclusion: 结构化推理、智能体协作和反馈驱动搜索机制对药物发现中的大语言模型应用至关重要。

Abstract: Recent advances in large language models have demonstrated considerable
potential in scientific domains such as drug discovery. However, their
effectiveness remains constrained when reasoning extends beyond the knowledge
acquired during pretraining. Conventional approaches, such as fine-tuning or
retrieval-augmented generation, face limitations in either imposing high
computational overhead or failing to fully exploit structured scientific data.
To overcome these challenges, we propose DrugMCTS, a novel framework that
synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree
Search for drug repurposing. The framework employs five specialized agents
tasked with retrieving and analyzing molecular and protein information, thereby
enabling structured and iterative reasoning. Without requiring domain-specific
fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by
over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate
that DrugMCTS achieves substantially higher recall and robustness compared to
both general-purpose LLMs and deep learning baselines. Our results highlight
the importance of structured reasoning, agent-based collaboration, and
feedback-driven search mechanisms in advancing LLM applications for drug
discovery.

</details>


### [18] [StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley](https://arxiv.org/abs/2507.07445)
*Weihao Tan,Changjiu Jiang,Yu Duan,Mingcong Lei,Jiageng Li,Yitian Hong,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: StarDojo是一个基于《星露谷物语》的新基准测试，旨在评估AI代理在开放式生产生活模拟中的表现，涵盖农业、制作、探索、战斗和社交互动五大领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试很少同时评估生产活动与社交互动能力，StarDojo旨在填补这一空白。

Method: StarDojo包含1000个任务，分为五大领域，并提供100个代表性任务子集。它提供统一界面，支持多操作系统和并行环境实例。

Result: 评估显示当前最先进的MLLM代理（如GPT-4.1）成功率仅为12.7%，主要受限于视觉理解、多模态推理和低级操作能力。

Conclusion: StarDojo旨在推动复杂生产生活环境中鲁棒开放式代理的研究。

Abstract: Autonomous agents navigating human society must master both production
activities and social interactions, yet existing benchmarks rarely evaluate
these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel
benchmark based on Stardew Valley, designed to assess AI agents in open-ended
production-living simulations. In StarDojo, agents are tasked to perform
essential livelihood activities such as farming and crafting, while
simultaneously engaging in social interactions to establish relationships
within a vibrant community. StarDojo features 1,000 meticulously curated tasks
across five key domains: farming, crafting, exploration, combat, and social
interactions. Additionally, we provide a compact subset of 100 representative
tasks for efficient model evaluation. The benchmark offers a unified,
user-friendly interface that eliminates the need for keyboard and mouse
control, supports all major operating systems, and enables the parallel
execution of multiple environment instances, making it particularly well-suited
for evaluating the most capable foundation agents, powered by multimodal large
language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents
demonstrate substantial limitations, with the best-performing model, GPT-4.1,
achieving only a 12.7% success rate, primarily due to challenges in visual
understanding, multimodal reasoning and low-level manipulation. As a
user-friendly environment and benchmark, StarDojo aims to facilitate further
research towards robust, open-ended agents in complex production-living
environments.

</details>


### [19] [Position: We Need An Algorithmic Understanding of Generative AI](https://arxiv.org/abs/2507.07544)
*Oliver Eberle,Thomas McGee,Hamza Giaffar,Taylor Webb,Ida Momennejad*

Main category: cs.AI

TL;DR: AlgEval框架旨在系统研究LLM学习的算法，揭示其底层计算原理，提供可解释性，并推动更高效的训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注性能提升，缺乏对LLM学习算法的理论理解，AlgEval填补了这一空白。

Method: 提出AlgEval框架，结合自上而下的假设和自下而上的电路级分析，研究LLM的算法组成。

Result: 案例研究展示了LLM中搜索算法的形成，验证了框架的可行性。

Conclusion: AlgEval为理解LLM的计算提供了系统方法，有望推动更高效的模型设计和训练。

Abstract: What algorithms do LLMs actually learn and use to solve problems? Studies
addressing this question are sparse, as research priorities are focused on
improving performance through scale, leaving a theoretical and empirical gap in
understanding emergent algorithms. This position paper proposes AlgEval: a
framework for systematic research into the algorithms that LLMs learn and use.
AlgEval aims to uncover algorithmic primitives, reflected in latent
representations, attention, and inference-time compute, and their algorithmic
composition to solve task-specific problems. We highlight potential
methodological paths and a case study toward this goal, focusing on emergent
search algorithms. Our case study illustrates both the formation of top-down
hypotheses about candidate algorithms, and bottom-up tests of these hypotheses
via circuit-level analysis of attention patterns and hidden states. The
rigorous, systematic evaluation of how LLMs actually solve tasks provides an
alternative to resource-intensive scaling, reorienting the field toward a
principled understanding of underlying computations. Such algorithmic
explanations offer a pathway to human-understandable interpretability, enabling
comprehension of the model's internal reasoning performance measures. This can
in turn lead to more sample-efficient methods for training and improving
performance, as well as novel architectures for end-to-end and multi-agent
systems.

</details>


### [20] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala,Jordi Planes,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 论文探讨了机器学习中规则模型解释的负面影响，并提出算法分析这些问题。


<details>
  <summary>Details</summary>
Motivation: 在机器学习高风险领域，错误的解释可能误导决策者，因此需要严格分析规则模型的负面影响。

Method: 开发算法分析规则模型中的负面重叠和冗余问题。

Result: 研究发现广泛使用的规则学习工具会导致规则集出现负面问题。

Conclusion: 论文强调规则模型在高风险应用中需谨慎使用，以避免负面影响。

Abstract: A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


### [21] [Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs](https://arxiv.org/abs/2507.07595)
*Zhixiang Su,Di Wang,Chunyan Miao*

Main category: cs.AI

TL;DR: 论文提出了一种名为Context Pooling的新方法，用于提升GNN在知识图谱链接预测中的性能，特别是在归纳设置中生成查询特定图。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型在知识图谱链接预测中的聚合操作对性能影响有限，需要更有效的方法。

Method: 提出Context Pooling方法，首次在知识图谱中应用图池化，并设计邻居精度和召回率指标筛选逻辑相关邻居。

Result: 在三个公开数据集上应用于两个SOTA模型，42/48情况下达到SOTA性能。

Conclusion: Context Pooling显著提升GNN在知识图谱链接预测中的性能，尤其在归纳设置中表现突出。

Abstract: Recent investigations on the effectiveness of Graph Neural Network
(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that
vanilla aggregation does not significantly impact the model performance. In
this paper, we introduce a novel method, named Context Pooling, to enhance
GNN-based models' efficacy for link predictions in KGs. To our best of
knowledge, Context Pooling is the first methodology that applies graph pooling
in KGs. Additionally, Context Pooling is first-of-its-kind to enable the
generation of query-specific graphs for inductive settings, where testing
entities are unseen during training. Specifically, we devise two metrics,
namely neighborhood precision and neighborhood recall, to assess the neighbors'
logical relevance regarding the given queries, thereby enabling the subsequent
comprehensive identification of only the logically relevant neighbors for link
prediction. Our method is generic and assessed by being applied to two
state-of-the-art (SOTA) models on three public transductive and inductive
datasets, achieving SOTA performance in 42 out of 48 settings.

</details>


### [22] [Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.07599)
*Sedigh Khademi,Jim Black,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila*

Main category: cs.AI

TL;DR: 研究评估了微调的Llama 3.2模型从急诊分诊笔记中提取疫苗相关信息的能力，以支持近实时疫苗安全监测。


<details>
  <summary>Details</summary>
Motivation: 通过自动化数据提取提高疫苗安全监测效率，早期发现免疫后不良事件。

Method: 使用提示工程创建标注数据集，比较提示工程模型、微调模型和基于规则的方法。

Result: 微调的Llama 3B参数模型在提取疫苗名称的准确性上优于其他模型，量化技术使其在资源受限环境中高效部署。

Conclusion: 大型语言模型在自动化急诊笔记数据提取中具有潜力，可支持疫苗安全监测和不良事件早期发现。

Abstract: This study evaluates fine-tuned Llama 3.2 models for extracting
vaccine-related information from emergency department triage notes to support
near real-time vaccine safety surveillance. Prompt engineering was used to
initially create a labeled dataset, which was then confirmed by human
annotators. The performance of prompt-engineered models, fine-tuned models, and
a rule-based approach was compared. The fine-tuned Llama 3 billion parameter
model outperformed other models in its accuracy of extracting vaccine names.
Model quantization enabled efficient deployment in resource-constrained
environments. Findings demonstrate the potential of large language models in
automating data extraction from emergency department notes, supporting
efficient vaccine safety surveillance and early detection of emerging adverse
events following immunization issues.

</details>


### [23] [Towards conservative inference in credal networks using belief functions: the case of credal chains](https://arxiv.org/abs/2507.07619)
*Marco Sangalli,Thomas Krak,Cassio de Campos*

Main category: cs.AI

TL;DR: 本文提出了一种基于Dempster-Shafer理论的信念推理框架，用于在信用网络（尤其是链式结构）中传播不确定性，结合计算速度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索如何在信用网络中高效且鲁棒地传播不确定性，特别是在链式结构中。

Method: 提出了一种基于信念和似然函数的框架，通过形式化信念推理方法，并与经典敏感性分析进行比较。

Result: 数值结果展示了该框架的优势和局限性，为链式结构和一般信用网络的实践应用提供了见解。

Conclusion: 该框架在计算速度和不确定性表示方面表现出色，但在某些情况下存在局限性，为未来研究提供了方向。

Abstract: This paper explores belief inference in credal networks using Dempster-Shafer
theory. By building on previous work, we propose a novel framework for
propagating uncertainty through a subclass of credal networks, namely chains.
The proposed approach efficiently yields conservative intervals through belief
and plausibility functions, combining computational speed with robust
uncertainty representation. Key contributions include formalizing belief-based
inference methods and comparing belief-based inference against classical
sensitivity analysis. Numerical results highlight the advantages and
limitations of applying belief inference within this framework, providing
insights into its practical utility for chains and for credal networks in
general.

</details>


### [24] [PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations](https://arxiv.org/abs/2507.07644)
*Fedor Rodionov,Abdelrahman Eldesokey,Michael Birsak,John Femiani,Bernard Ghanem,Peter Wonka*

Main category: cs.AI

TL;DR: PlanQA是一个用于评估大型语言模型（LLMs）几何和空间推理能力的诊断基准，基于室内场景的符号化表示，测试多种问题类型，发现LLMs在模拟物理约束和空间一致性方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在几何和空间推理方面存在盲点，特别是在处理真实世界布局时表现不佳，因此需要一个专门的基准来评估和改进这一能力。

Method: PlanQA使用符号化格式（如JSON、XML）表示室内场景，设计多样化问题类型，测试LLMs在度量、拓扑推理及室内设计约束方面的能力。

Result: 实验表明，LLMs在浅层查询中表现良好，但在模拟物理约束、保持空间一致性及布局扰动下的泛化能力上存在明显不足。

Conclusion: PlanQA揭示了LLMs在空间推理上的局限性，为未来开发能准确推断和操作空间几何属性的语言模型提供了方向。

Abstract: We introduce PlanQA, a diagnostic benchmark for evaluating geometric and
spatial reasoning in large-language models (LLMs). PlanQA is grounded in
structured representations of indoor scenes, such as kitchens, living rooms,
and bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The
benchmark includes diverse question types that test not only metric and
topological reasoning (e.g., distance, visibility, shortest paths) but also
interior design constraints such as affordance, clearance, balance, and
usability. Our results across a variety of frontier open-source and commercial
LLMs show that while models may succeed in shallow queries, they often fail to
simulate physical constraints, preserve spatial coherence, or generalize under
layout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they
do not consistently reason about real-world layouts. We hope that this
benchmark inspires new work on language models that can accurately infer and
manipulate spatial and geometric properties in practical settings.

</details>


### [25] [Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization](https://arxiv.org/abs/2507.07723)
*Chengtao Jian,Kai Yang,Ye Ouyang,Xiaozhou Ye*

Main category: cs.AI

TL;DR: 本文分析了直接偏好优化（DPO）的理论局限性和动态特性，提出了一种双层优化框架——稳定偏好优化（SPO），以改进模型对齐的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: DPO在语言模型对齐中表现出高效性，但其理论性质和内在局限性尚未充分研究。本文旨在揭示DPO的敏感性和概率分配问题，并提出改进方法。

Method: 通过概率演化视角分析DPO的动态特性，提出双层优化框架（SPO），结合监督微调和正则化方案，以稳定优化并提升偏好输出的概率。

Result: 实验表明，SPO在推理和摘要任务中表现优于标准DPO，提高了推理准确性并更好地对齐输出分布与预期偏好。

Conclusion: 稳定偏好优化为偏好对齐目标的设计提供了新思路，推动了更可靠和可解释的语言模型对齐方法的发展。

Abstract: Direct Preference Optimization (DPO) has emerged as a popular and efficient
alternative to reward modeling and reinforcement learning for aligning language
models with human preferences. Despite its empirical success, the theoretical
properties and intrinsic limitations of DPO remain underexplored. In this work,
we first present a comprehensive analysis of DPO's dynamics from a probability
evolution perspective. Our analysis reveals that DPO is highly sensitive to
initialization. It also tends to misallocate probability mass, which can
inadvertently shift probability toward irrelevant or undesired responses. This
misallocation may unintentionally reinforce model bias, thereby compromising
both the stability of model alignment and the consistency with intended
preferences. Motivated by these theoretical findings, we propose a
theoretically grounded bilevel optimization framework that tightly integrate
supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference
optimization. Our approach introduces a principled regularization scheme to
explicitly encourage absolute probability improvement for preferred outputs,
while maintaining stable optimization dynamics. Experiments on challenging
reasoning and summarization benchmarks elucidate that our method consistently
improves reasoning accuracy and better aligns output distributions with
intended preferences, outperforming standard DPO. Stable preference
optimization provides new insights into the design of preference-based
alignment objectives and opens up new avenues towards more reliable and
interpretable language model alignment.

</details>


### [26] [Identification of Violin Reduction via Contour Lines Classification](https://arxiv.org/abs/2507.07743)
*Philémon Beghin,Anne-Emmanuelle Ceulemans,François Glineur*

Main category: cs.AI

TL;DR: 本文提出了一种基于轮廓线的小提琴尺寸缩减分类方法，通过几何特征分析区分缩减与非缩减小提琴。


<details>
  <summary>Details</summary>
Motivation: 研究小提琴制造中的尺寸缩减现象，填补专家观察与定量研究之间的空白。

Method: 使用25把小提琴的3D几何网格数据，提取并拟合轮廓线参数，构建数值特征，应用分类方法。

Result: 几何特征可以一定程度区分缩减与非缩减小提琴，其中开口参数β最具预测性。

Conclusion: 轮廓线分析为小提琴尺寸缩减研究提供了定量方法，但需考虑缩减程度的多样性。

Abstract: The first violins appeared in late 16th-century Italy. Over the next 200
years, they spread across Europe and luthiers of various royal courts, eager to
experiment with new techniques, created a highly diverse family of instruments.
Around 1750, size standards were introduced to unify violin making for
orchestras and conservatories. Instruments that fell between two standards were
then reduced to a smaller size by luthiers. These reductions have an impact on
several characteristics of violins, in particular on the contour lines, i.e.
lines of constant altitude, which look more like a U for non reduced
instruments and a V for reduced ones. While such differences are observed by
experts, they have not been studied quantitatively.
  This paper presents a method for classifying violins as reduced or
non-reduced based on their contour lines. We study a corpus of 25 instruments
whose 3D geometric meshes were acquired via photogrammetry. For each
instrument, we extract 10-20 contour lines regularly spaced every millimetre.
Each line is fitted with a parabola-like curve (with an equation of the type y
= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)
and how vertically stretched (alpha) the curve is. We compute additional
features from those parameters, using regressions and counting how many values
fall under some threshold. We also deal with outliers and non equal numbers of
levels, and eventually obtain a numerical profile for each instrument.
  We then apply classification methods to assess whether geometry alone can
predict size reduction. We find that distinguishing between reduced and non
reduced instruments is feasible to some degree, taking into account that a
whole spectrum of more or less transformed violins exists, for which it is more
difficult to quantify the reduction. We also find the opening parameter beta to
be the most predictive.

</details>


### [27] [Measuring AI Alignment with Human Flourishing](https://arxiv.org/abs/2507.07787)
*Elizabeth Hilliard,Akshaya Jagadeesh,Alex Cook,Steele Billings,Nicholas Skytland,Alicia Llewellyn,Jackson Paull,Nathan Paull,Nolan Kurylo,Keatra Nesbitt,Robert Gruenewald,Anthony Jantzi,Omar Chavez*

Main category: cs.AI

TL;DR: FAI Benchmark是一个评估AI对人类繁荣贡献的新框架，涵盖七个维度，通过1229个问题评估AI表现。测试28个领先模型后发现，没有模型在所有维度上表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统基准仅关注技术能力或避免危害，而FAI Benchmark旨在评估AI如何促进人类全面繁荣。

Method: 采用几何平均评分和跨维度评估，结合1229个主客观问题，使用专门LLM法官。

Result: 最高分模型得72/100，但无模型在所有维度（尤其是信仰与灵性、品德与美德、意义与目的）上表现良好。

Conclusion: FAI Benchmark为开发支持人类繁荣的AI提供了框架，对AI发展、伦理和评估有重要意义。

Abstract: This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel
evaluation framework that assesses AI alignment with human flourishing across
seven dimensions: Character and Virtue, Close Social Relationships, Happiness
and Life Satisfaction, Meaning and Purpose, Mental and Physical Health,
Financial and Material Stability, and Faith and Spirituality. Unlike
traditional benchmarks that focus on technical capabilities or harm prevention,
the FAI Benchmark measures AI performance on how effectively models contribute
to the flourishing of a person across these dimensions. The benchmark evaluates
how effectively LLM AI systems align with current research models of holistic
human well-being through a comprehensive methodology that incorporates 1,229
objective and subjective questions. Using specialized judge Large Language
Models (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs
geometric mean scoring to ensure balanced performance across all flourishing
dimensions. Initial testing of 28 leading language models reveals that while
some models approach holistic alignment (with the highest-scoring models
achieving 72/100), none are acceptably aligned across all dimensions,
particularly in Faith and Spirituality, Character and Virtue, and Meaning and
Purpose. This research establishes a framework for developing AI systems that
actively support human flourishing rather than merely avoiding harm, offering
significant implications for AI development, ethics, and evaluation.

</details>


### [28] [MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving](https://arxiv.org/abs/2507.07818)
*Lu Xu,Jiaqian Yu,Xiongfeng Peng,Yiwei Chen,Weiming Li,Jaewook Yoo,Sunghyun Chunag,Dongwook Lee,Daehyun Ji,Chao Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种技能导向的混合专家模型（MoSE），模仿人类驾驶员的学习和推理过程，通过技能导向的路由机制和分层技能数据集，实现了高效的单次推理性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家模型（MoE）需要大量训练数据和复杂优化，而MoSE通过模仿人类驾驶员的学习方式，旨在减少计算成本并提升性能。

Method: 提出技能导向路由机制和分层技能数据集，预训练路由器以实现逐步推理，同时整合辅助任务于单次前向过程。

Result: MoSE在CODA AD任务中优于多个8B+参数模型，激活参数量减少至少62.5%，性能达到SOTA。

Conclusion: MoSE通过技能导向设计和单次推理优化，显著提升了计算效率和性能，为自动驾驶系统提供了新思路。

Abstract: Recent studies show large language models (LLMs) and vision language models
(VLMs) trained using web-scale data can empower end-to-end autonomous driving
systems for a better generalization and interpretation. Specifically, by
dynamically routing inputs to specialized subsets of parameters, the
Mixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve
substantial performance improvements while maintaining computational
efficiency. However, general MoE models usually demands extensive training data
and complex optimization. In this work, inspired by the learning process of
human drivers, we propose a skill-oriented MoE, called MoSE, which mimics human
drivers' learning process and reasoning process, skill-by-skill and
step-by-step. We propose a skill-oriented routing mechanism that begins with
defining and annotating specific skills, enabling experts to identify the
necessary driving competencies for various scenarios and reasoning tasks,
thereby facilitating skill-by-skill learning. Further align the driving process
to multi-step planning in human reasoning and end-to-end driving models, we
build a hierarchical skill dataset and pretrain the router to encourage the
model to think step-by-step. Unlike multi-round dialogs, MoSE integrates
valuable auxiliary tasks (e.g.\ description, reasoning, planning) in one single
forward process without introducing any extra computational cost. With less
than 3B sparsely activated parameters, our model outperforms several 8B+
parameters on CODA AD corner case reasoning task. Compared to existing methods
based on open-source models and data, our approach achieves state-of-the-art
performance with significantly reduced activated model size (at least by
$62.5\%$) with a single-turn conversation.

</details>


### [29] [AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift](https://arxiv.org/abs/2507.07820)
*Eunsu Baek,Keondo Park,Jeonggil Ko,Min-hwan Oh,Taesik Gong,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: 论文提出自适应感知作为AI发展的新范式，通过动态调整传感器参数提升效率，减少对大规模模型和数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前AI依赖大规模模型和数据集，导致环境、经济和伦理成本高昂，限制了可持续性和公平性。生物感官系统的动态适应性为这一挑战提供了灵感。

Method: 提出自适应感知方法，动态调整传感器参数（如曝光、灵敏度、多模态配置），以应对协变量偏移并提升效率。

Result: 实验证明，自适应感知能使小模型（如EfficientNet-B0）超越更大模型（如OpenCLIP-H），同时减少数据和计算需求。

Conclusion: 论文呼吁AI社区转向可持续、鲁棒和公平的系统，并提出技术集成、标准化基准和隐私保护等研究方向。

Abstract: Current AI advances largely rely on scaling neural models and expanding
training datasets to achieve generalization and robustness. Despite notable
successes, this paradigm incurs significant environmental, economic, and
ethical costs, limiting sustainability and equitable access. Inspired by
biological sensory systems, where adaptation occurs dynamically at the input
(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive
sensing as a necessary and foundational shift. Adaptive sensing proactively
modulates sensor parameters (e.g., exposure, sensitivity, multimodal
configurations) at the input level, significantly mitigating covariate shifts
and improving efficiency. Empirical evidence from recent studies demonstrates
that adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass
substantially larger models (e.g., OpenCLIP-H) trained with significantly more
data and compute. We (i) outline a roadmap for broadly integrating adaptive
sensing into real-world applications spanning humanoid, healthcare, autonomous
systems, agriculture, and environmental monitoring, (ii) critically assess
technical and ethical integration challenges, and (iii) propose targeted
research directions, such as standardized benchmarks, real-time adaptive
algorithms, multimodal integration, and privacy-preserving methods.
Collectively, these efforts aim to transition the AI community toward
sustainable, robust, and equitable artificial intelligence systems.

</details>


### [30] [Searching for actual causes: Approximate algorithms with adjustable precision](https://arxiv.org/abs/2507.07857)
*Samuel Reyd,Ada Diaconescu,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 论文提出了一种多项式复杂度的算法，用于识别实际原因，解决了现有方法无法处理非布尔、黑盒和随机系统的问题。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能（XAI）和因果性文献主要关注因素与后果的关系，但非专家用户更关注导致目标后果的实际原因。现有方法在形式化和实际应用上存在不足。

Method: 提出了一组多项式复杂度的算法，可调整精度和全面性，适用于非布尔、黑盒和随机系统。

Result: 实验表明，算法能识别现有方法无法处理的系统类别的原因，并通过调整计算时间提高精度和全面性。

Conclusion: 该算法为解决实际原因识别问题提供了实用且灵活的解决方案。

Abstract: Causality has gained popularity in recent years. It has helped improve the
performance, reliability, and interpretability of machine learning models.
However, recent literature on explainable artificial intelligence (XAI) has
faced criticism. The classical XAI and causality literature focuses on
understanding which factors contribute to which consequences. While such
knowledge is valuable for researchers and engineers, it is not what non-expert
users expect as explanations. Instead, these users often await facts that cause
the target consequences, i.e., actual causes. Formalizing this notion is still
an open problem. Additionally, identifying actual causes is reportedly an
NP-complete problem, and there are too few practical solutions to approximate
formal definitions. We propose a set of algorithms to identify actual causes
with a polynomial complexity and an adjustable level of precision and
exhaustiveness. Our experiments indicate that the algorithms (1) identify
causes for different categories of systems that are not handled by existing
approaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be
adjusted to gain more precision and exhaustiveness with more computation time.

</details>


### [31] [An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis](https://arxiv.org/abs/2507.07893)
*Mingda Zhang,Na Zhao,Jianglong Qing,Qing xu,Kaiwen Pan,Ting luo*

Main category: cs.AI

TL;DR: 论文提出了一种结合提示工程和多维知识图谱的增强框架，以解决大语言模型在法律纠纷分析中的局限性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在法律纠纷分析中存在法律知识表示不足、概念理解有限和推理缺陷等问题，亟需改进。

Method: 提出三阶段分层提示结构和三层知识图谱架构，结合四种互补的法律概念检索方法。

Result: 实验结果显示，该框架显著提升了法律纠纷分析的性能，能够准确分析复杂案件。

Conclusion: 该研究为智能法律辅助系统的实现提供了新颖的技术途径。

Abstract: The rapid development of artificial intelligence has positioned large
language models as fundamental components of intelligent legal systems.
However, these models face significant limitations in legal dispute analysis,
including insufficient legal knowledge representation, limited concept
understanding, and reasoning deficiencies. This research proposes an enhanced
framework integrating prompt engineering with multidimensional knowledge
graphs. The framework introduces a three-stage hierarchical prompt structure
comprising task definition, knowledge background, and reasoning guidance,
supplemented by legal-specific reasoning templates and dynamic optimization
mechanisms. A three-layer knowledge graph architecture is constructed with
legal classification ontology, representation, and instance layers. Four
complementary methods enable precise legal concept retrieval: direct legal norm
code matching, domain-specific semantic vector similarity, ontology-based path
reasoning, and specialized lexical segmentation. These components integrate
with web search technology to establish a knowledge-enhanced framework for
legal decision-making. Experimental results demonstrate significant performance
improvements in legal dispute analysis, enabling accurate legal application
analysis for complex cases while exhibiting nuanced understanding of judicial
decision-making logic, providing a novel technical approach for implementing
intelligent legal assistance systems.

</details>


### [32] [Meek Models Shall Inherit the Earth](https://arxiv.org/abs/2507.07931)
*Hans Gundlach,Jayson Lynch,Neil Thompson*

Main category: cs.AI

TL;DR: 论文认为，随着计算资源投入的边际效益递减，AI模型性能将趋于收敛，即使资源有限的小模型也能接近最佳模型的性能。


<details>
  <summary>Details</summary>
Motivation: 探讨AI模型性能不平等问题，提出计算资源边际效益递减将导致模型能力趋同的观点。

Method: 开发模型分析固定分布下计算资源投入的边际效益递减，结合基准数据和理论模型验证。

Result: 计算资源边际效益显著递减，小模型未来可能接近最佳模型的性能。

Conclusion: AI战略和政策需重新审视，以适应小模型能力提升的趋势。

Abstract: The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.

</details>


### [33] [Working with AI: Measuring the Occupational Implications of Generative AI](https://arxiv.org/abs/2507.07935)
*Kiran Tomlinson,Sonia Jaffe,Will Wang,Scott Counts,Siddharth Suri*

Main category: cs.AI

TL;DR: 研究分析了生成式AI对经济的影响，通过用户与Microsoft Bing Copilot的对话数据，发现AI主要协助信息收集和写作，并计算了各职业的AI适用性分数。


<details>
  <summary>Details</summary>
Motivation: 理解生成式AI对经济的潜在影响是社会的关键问题。

Method: 分析了20万条用户与Microsoft Bing Copilot的匿名对话数据，结合职业活动分类和任务成功度量。

Result: 知识型职业（如计算机、数学、行政支持）和销售职业的AI适用性最高。

Conclusion: AI在信息提供和写作等活动中表现最佳，且高工资和高教育水平的职业更易受AI影响。

Abstract: Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [34] [Sparse Signal Recovery From Quadratic Systems with Full-Rank Matrices](https://arxiv.org/abs/2507.07557)
*Jinming Wen,Yi Hu,Meng Huang*

Main category: cs.IT

TL;DR: 该论文提出了一种基于稀疏性的信号恢复方法，通过代数几何工具和两阶段SGN算法，在高维情况下实现了高效且准确的信号重构。


<details>
  <summary>Details</summary>
Motivation: 在信号处理中，从二次测量中恢复信号是一个挑战，尤其是在高维情况下（m≪n）。本文旨在利用信号的稀疏性解决这一问题。

Method: 提出两阶段Sparse Gauss-Newton (SGN)算法：第一阶段使用支持限制的谱初始化，第二阶段通过迭代硬阈值Gauss-Newton方法细化估计。

Result: 理论证明m≥2s（实数）和m≥4s-2（复数）的测量足以唯一恢复s-稀疏信号；SGN算法在数值实验中表现出更高的准确性和计算效率。

Conclusion: SGN算法在稀疏信号恢复中具有近最优的采样复杂度和快速收敛性，显著优于现有方法。

Abstract: In signal processing and data recovery, reconstructing a signal from
quadratic measurements poses a significant challenge, particularly in
high-dimensional settings where measurements $m$ is far less than the signal
dimension $n$ (i.e., $m \ll n$). This paper addresses this problem by
exploiting signal sparsity. Using tools from algebraic geometry, we derive
theoretical recovery guarantees for sparse quadratic systems, showing that
$m\ge 2s$ (real case) and $m\ge 4s-2$ (complex case) generic measurements
suffice to uniquely recover all $s$-sparse signals. Under a Gaussian
measurement model, we propose a novel two-stage Sparse Gauss-Newton (SGN)
algorithm. The first stage employs a support-restricted spectral
initialization, yielding an accurate initial estimate with $m=O(s^2\log{n})$
measurements. The second stage refines this estimate via an iterative
hard-thresholding Gauss-Newton method, achieving quadratic convergence to the
true signal within finitely many iterations when $m\ge O(s\log{n})$. Compared
to existing second-order methods, our algorithm achieves near-optimal sampling
complexity for the refinement stage without requiring resampling. Numerical
experiments indicate that SGN significantly outperforms state-of-the-art
algorithms in both accuracy and computational efficiency. In particular, (1)
when sparsity level $s$ is high, compared with existing algorithms, SGN can
achieve the same success rate with fewer measurements. (2) SGN converges with
only about $1/10$ iterations of the best existing algorithm and reach lower
relative error.

</details>


### [35] [Secure Cooperative Gradient Coding: Optimality, Reliability, and Global Privacy](https://arxiv.org/abs/2507.07565)
*Shudi Weng*

Main category: cs.IT

TL;DR: 本文提出了一种名为SecCoGC的隐私敏感联邦学习方法，解决了不可靠通信下的安全聚合和滞后节点问题，并引入公平隐私保护的扩展Fair-SecCoGC。


<details>
  <summary>Details</summary>
Motivation: 研究隐私敏感的联邦学习在不可靠通信环境下的挑战，尤其是安全聚合和滞后节点对模型准确性的影响。

Method: 提出Secure Cooperative Gradient Coding (SecCoGC)方法，支持强隐私保护和滞后节点鲁棒性，并在实数域中实现。进一步扩展为Fair-SecCoGC以确保公平隐私保护。

Result: SecCoGC在不可靠通信下表现出强鲁棒性，性能提升20%-70%，优于现有隐私保护方法。

Conclusion: 本文成功解决了不可靠通信下的安全聚合问题，提供了高效且公平的隐私保护方案，并通过实验验证了其有效性。

Abstract: This paper studies privacy-sensitive federated learning (FL) with unreliable
communication, focusing on secure aggregation and straggler mitigation. While
secure aggregation cryptographically reconstructs the global model without
exposing client updates, random link failures disrupt its key coordination,
degrading model accuracy. Moreover, unreliable communication can lead to
objective inconsistency, causing the global model to converge to arbitrary,
sub-optimal points far from the intended optimum. This paper proposes Secure
Cooperative Gradient Coding (SecCoGC), a practical solution that achieves
secure aggregation with arbitrarily strong privacy guarantees and robust
straggler mitigation under unreliable communication. SecCoGC operates natively
in the real field, making it directly applicable to practical deployments. To
ensure equitable privacy protection across clients, we further introduce
Fair-SecCoGC, an extension that enforces fairness in the level of privacy
offered to all users. To conclude, this paper formally formulates the problem
of secure aggregation in the real field and presents both general and
computationally efficient key construction methods. Moreover, it provides a
comprehensive privacy analysis under Local Mutual Information Privacy (LMIP)
and Local Differential Privacy (LDP) across all protocol layers. Robustness and
convergence properties are also rigorously analyzed. Finally, extensive
simulations are performed across diverse network conditions and benchmark
datasets to validate the effectiveness of the proposed methods. The results
show that SecCoGC achieves strong robustness to unreliable communication under
arbitrarily strong privacy guarantees. It outperforms existing
privacy-preserving methods with performance gains of up to 20\%-70\%.

</details>


### [36] [Linear codes for $b$-symbol read channels attaining the Griesmer bound](https://arxiv.org/abs/2507.07728)
*Sascha Kurz*

Main category: cs.IT

TL;DR: 研究在$b$-符号度量下线性码的最优参数，特别是当最小距离足够大时，以及在二维情况下二进制线性码的最优参数。


<details>
  <summary>Details</summary>
Motivation: 存储应用中需要读取相邻符号的$b$-元组，因此研究$b$-符号度量下的编码问题具有重要意义。

Method: 分析线性码在$b$-符号度量下的最优参数，特别关注最小距离较大的情况，以及二维二进制线性码的小维度情况。

Result: 确定了在最小距离足够大时线性码的最优参数，以及二维二进制线性码在小维度下的最优参数。

Conclusion: 研究为$b$-符号度量下的编码问题提供了理论支持，尤其是在存储应用中的潜在价值。

Abstract: Reading channels where $b$-tuples of adjacent symbols are read at every step
have e.g.\ applications in storage. Corresponding bounds and constructions of
codes for the $b$-symbol metric, especially the pair-symbol metric where $b=2$,
were intensively studied in the last fifteen years. Here we determine the
optimal code parameters of linear codes in the $b$-symbol metric assuming that
the minimum distance is sufficiently large. We also determine the optimal
parameters of linear binary codes in the pair-symbol metric for small
dimensions.

</details>


### [37] [Generalized bilateral multilevel construction for constant dimension codes from parallel mixed dimension construction](https://arxiv.org/abs/2507.07842)
*Han Li,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 本文提出了一种改进的并行混合维度构造方法，用于优化常维码（CDC）的设计，生成优于现有最佳码的新码。


<details>
  <summary>Details</summary>
Motivation: 常维码在随机网络编码中有重要应用，但其最大可能尺寸的确定是一个关键问题。

Method: 采用双边标识向量准则和广义双边多级构造，改进并行混合维度构造。

Result: 构造了许多优于已知最佳码的新常维码。

Conclusion: 提出的方法有效提升了常维码的性能，为随机网络编码提供了更好的解决方案。

Abstract: Constant dimension codes (CDCs), as special subspace codes, have received
extensive attention due to their applications in random network coding. The
basic problem of CDCs is to determine the maximal possible size
$A_q(n,d,\{k\})$ for given parameters $q, n, d$, and $k$. This paper introduces
criteria for choosing appropriate bilateral identifying vectors compatible with
the parallel mixed dimension construction (Des. Codes Cryptogr. 93(1):227--241,
2025). We then utilize the generalized bilateral multilevel construction (Des.
Codes Cryptogr. 93(1):197--225, 2025) to improve the parallel mixed dimension
construction efficiently. Many new CDCs that are better than the previously
best-known codes are constructed.

</details>
