<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 11]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.IT](#cs.IT) [Total: 3]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [How Feasible are Passive Network Attacks on 5G Networks and Beyond? A Survey](https://arxiv.org/abs/2512.20622)
*Atmane Ayoub Mansour Bahar,Andrés Alayón Glazunov,Romaric Duvignau*

Main category: cs.NI

TL;DR: 本文调查了5G及未来网络中被动网络攻击的可行性，重点关注信息提取和地理位置追踪两类攻击，发现5G中此类攻击理论上可能但实践受限，B5G/6G中因工具和硬件成本问题目前不可行。


<details>
  <summary>Details</summary>
Motivation: 5G部署引发隐私担忧，其密集的小型天线系统能够高精度收集数据，同时5G的独特无线通信特性使已知网络攻击更难复制。被动网络攻击因无需与目标网络直接交互而难以检测，可能泄露用户敏感信息，因此需要评估其在5G及未来网络中的可行性。

Method: 通过调查分析的方法，研究被动网络攻击在5G及B5G/6G网络中的可行性，重点关注两大类别：信息提取（系统识别、网站和应用指纹识别）和地理位置追踪（用户识别和位置跟踪）。

Result: 在5G网络中，此类被动攻击理论上仍然可能，但实际执行受到定向波束成形、高频传播特性和加密机制的限制。对于B5G和早期6G网络，由于缺乏公共工具和高硬件成本，目前这些攻击在实践中不可行。

Conclusion: 虽然5G的被动网络攻击在理论上是可能的，但实际执行面临显著挑战；而B5G/6G网络中的攻击目前不可行，这揭示了对未来网络威胁模型理解的关键空白。

Abstract: Privacy concerns around 5G, the latest generation of mobile networks, are growing, with fears that its deployment may increase exposure to privacy risks. This perception is largely driven by the use of denser deployments of small antenna systems, which enable highly accurate data collection at higher speeds and closer proximity to mobile users. At the same time, 5G's unique radio communication features can make the reproduction of known network attacks more challenging. In particular, passive network attacks, which do not involve direct interaction with the target network and are therefore nearly impossible to detect, remain a pressing concern. Such attacks can reveal sensitive information about users, their devices, and active applications, which may then be exploited through known vulnerabilities or spear-phishing schemes. This survey examines the feasibility of passive network attacks in 5G and beyond (B5G/6G) networks, with emphasis on two major categories: information extraction (system identification, website and application fingerprinting) and geolocation (user identification and position tracking). These attacks are well documented and reproducible in existing wireless and mobile systems, including short-range networks (IEEE 802.11) and, to a lesser extent, LTE. Current evidence suggests that while such attacks remain theoretically possible in 5G, their practical execution is significantly constrained by directional beamforming, high-frequency propagation characteristics, and encryption mechanisms. For B5G and early 6G networks, the lack of public tools and high hardware cost currently renders these attacks infeasible in practice, which highlights a critical gap in our understanding of future network threat models.

</details>


### [2] [Efficient Asynchronous Federated Evaluation with Strategy Similarity Awareness for Intent-Based Networking in Industrial Internet of Things](https://arxiv.org/abs/2512.20627)
*Shaowen Qin,Jianfeng Zeng,Haodong Guo,Xiaohuan Li,Jiawen Kang,Qian Chen,Dusit Niyato*

Main category: cs.NI

TL;DR: FEIBN是一个联邦评估增强的意图驱动网络框架，利用LLM将多模态用户意图转化为结构化策略元组，并通过联邦学习在IIoT节点上进行分布式策略验证，同时设计了SSAFL机制来提升训练效率和减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 在工业物联网环境中，意图驱动网络虽然前景广阔，但频繁的策略部署和回滚在实际IIoT系统中不切实际，因为工作流紧密耦合且停机成本高。同时，IIoT节点的异构性和隐私约束使得集中式策略验证变得复杂。

Method: FEIBN框架利用大语言模型将多模态用户意图对齐为结构化策略元组，采用联邦学习在IIoT节点间进行分布式策略验证而不暴露原始数据。设计了SSAFL（策略相似性感知联邦学习）机制，基于策略相似性和资源状态选择任务相关节点，仅在更新显著时触发异步模型上传。

Result: 实验表明，SSAFL能够提高模型准确性、加速模型收敛，与SemiAsyn相比成本降低27.8%。

Conclusion: FEIBN通过结合LLM和联邦学习，有效解决了IIoT环境中意图驱动网络的策略部署和验证挑战，SSAFL机制进一步提升了训练效率和降低了通信成本。

Abstract: Intent-Based Networking (IBN) offers a promising paradigm for intelligent and automated network control in Industrial Internet of Things (IIoT) environments by translating high-level user intents into executable network strategies. However, frequent strategy deployment and rollback are impractical in real-world IIoT systems due to tightly coupled workflows and high downtime costs, while the heterogeneity and privacy constraints of IIoT nodes further complicate centralized policy verification. To address these challenges, we propose FEIBN, a Federated Evaluation Enhanced Intent-Based Networking framework. FEIBN leverages large language models (LLMs) to align multimodal user intents into structured strategy tuples and employs federated learning to perform distributed policy verification across IIoT nodes without exposing raw data. To improve training efficiency and reduce communication overhead, we design SSAFL, a Strategy Similarity Aware Federated Learning mechanism that selects task-relevant nodes based on strategy similarity and resource status, and triggers asynchronous model uploads only when updates are significant. Experiments demonstrate that SSAFL can improve model accuracy, accelerate model convergence, and reduce the cost by 27.8% compared with SemiAsyn.

</details>


### [3] [Cross-Domain Elephant Flow Detection: A Unified Machine Learning Approach with Application-Aware and Security Features](https://arxiv.org/abs/2512.20637)
*Tabidah Usmani,Sara Zahid,Amna Javaid*

Main category: cs.NI

TL;DR: 本文提出了一个统一的机器学习框架，用于跨域大象流检测，通过应用感知和安全特征增强模型在不同网络环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大象流检测方法在单一域内表现良好，但由于域偏移现象，在异构网络环境中泛化能力差。需要解决跨域部署时的性能下降问题。

Method: 提出统一框架，包含自适应阈值、综合特征工程和跨域评估。使用应用感知特征和安全特征，在三个不同域（校园网络、UNSW-NB15、CIC-IDS2018）上进行评估。

Result: 实验结果显示跨域性能差异显著（F1分数0.37-0.97），统一模型整体交叉验证F1分数达0.99。基于大小的特征占主导（总字节数重要性33.80%），但应用感知和安全特征提升了分类准确性。

Conclusion: 跨域验证对大象流检测至关重要，统一框架能有效缓解域偏移影响。应用感知和安全特征不仅提高准确性，还为网络管理和安全应用提供有价值的洞察。

Abstract: Network traffic classification, particularly elephant flow detection, faces significant challenges when deployed across heterogeneous network environments. While existing approaches demonstrate high accuracy within single domains, they suffer from poor generalization due to domain shift phenomena. This paper presents a unified machine learning framework for cross domain elephant flow detection that incorporates application aware and security features to enhance robustness across diverse network environments. Our approach addresses the critical gap in existing literature by evaluating model performance across three distinct domains: Campus networks, UNSW-NB15, and CIC-IDS2018 datasets. This paper proposes a unified pipeline that employs adaptive thresholding, comprehensive feature engineering, and cross-domain evaluation to quantify and mitigate domain shift effects. Experimental results demonstrate significant performance variations across domains (F1-scores ranging from 0.37 to 0.97), highlighting the importance of cross-domain validation. The unified model achieves an overall cross-validation F1 score of 0.99 while maintaining interpretability through feature importance analysis. Our findings reveal that while size based features dominate elephant flow detection (33.80% importance for total bytes), application-aware and security features contribute to improved classification accuracy and provide valuable insights for network management and security applications.

</details>


### [4] [MILP-driven Network Planning Framework for Energy Efficiency and Coverage Maximization in IoT Mesh Networks](https://arxiv.org/abs/2512.20639)
*Ishmal Sohail,Attiq Zeeshan,M. Umar Khan,Syed Zubair,Rana Fayyaz Ahmad,Faizan Hamayat*

Main category: cs.NI

TL;DR: 提出混合整数线性规划框架，结合静态和移动Zigbee节点优化物联网网络部署成本，通过边界优化静态节点放置和移动节点路径规划，显著提高覆盖率并降低移动成本。


<details>
  <summary>Details</summary>
Motivation: 大规模无线传感器网络部署成本过高，特别是在资源受限环境中，需要降低物联网网络部署成本以实现全球物联网部署。

Method: 提出集成混合整数线性规划框架，包含三个新颖的优化模型：边界优化静态节点放置、移动节点覆盖最大化路径规划和移动节点移动最小化。

Result: 边界优化静态节点放置达到53.06%覆盖率（随机方法为33.42%），移动路径规划达到97.95%覆盖率，移动最小化减少40%遍历成本。

Conclusion: 该框架优于基准方法，为资源受限环境中的成本效益型全球物联网部署提供了基础解决方案。

Abstract: In the era of digital transformation, the global deployment of internet of things (IoT) networks and wireless sensor networks (WSNs) is critical for applications ranging from environmental monitoring to smart cities. Large-scale monitoring using WSNs incurs high costs due to the deployment of sensor nodes in the target deployment area. In this paper, we address the challenge of prohibitive deployment costs by proposing an integrated mixed-Integer linear programming (MILP) framework that strategically combines static and mobile Zigbee nodes. Our network planning approach introduces three novel formulations, including boundary-optimized static node placement (MILP-Static), mobile path planning for coverage maximization (MILP-Cov), and movement minimization (MILP-Mov) of the mobile nodes. We validated our framework with extensive simulations and experimental measurements of Zigbee power constraints. Our results show that boundary-optimized static placement (MILP-Static) achieves 53.06% coverage compared with 33.42% of the random approach. In addition, MILP-Cov for path planning reaches 97.95% coverage, while movement minimization (MILP-Mov) reduces traversal cost by 40%. Our proposed framework outperforms the benchmark approaches to provide a foundational solution for cost-effective global IoT deployment in resource constrained environments.

</details>


### [5] [Reflection-Driven Self-Optimization 6G Agentic AI RAN via Simulation-in-the-Loop Workflows](https://arxiv.org/abs/2512.20640)
*Yunhao Hu,Xinchen Lyu,Chenshan Ren,Keda Chen,Qimei Cui,Xiaofeng Tao*

Main category: cs.NI

TL;DR: 提出首个基于反思驱动的自优化框架，将智能体AI与高保真网络仿真结合，实现6G网络自主资源管理，显著提升吞吐量、QoS满意度和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 6G网络复杂性超越传统优化和现有AI方法的能力，需要更高水平的自主性。当前智能体AI框架虽具备推理能力，但缺乏经验验证和自我改进机制。

Method: 提出反射驱动的自优化框架，集成智能体AI与高保真网络仿真，采用闭环架构，协调四个专门智能体（场景、求解器、仿真、反射智能体）协同工作。

Result: 相比非智能体方法显著提升性能：干扰优化中吞吐量提高17.1%，意图识别使QoS满意度提升67%，低流量期间资源利用率降低25%同时保持服务质量。

Conclusion: 仿真在环验证是实现真正自主网络的关键使能技术，反射驱动的自优化框架将智能体AI转变为能够自我纠正、逃离局部最优、识别用户意图并适应动态网络条件的系统。

Abstract: The escalating complexity of sixth-generation (6G) networks demands unprecedented levels of autonomy beyond the capabilities of traditional optimization-based and current AI-based resource management approaches. While agentic AI has emerged as a promising paradigm for autonomous RAN, current frameworks provide sophisticated reasoning capabilities but lack mechanisms for empirical validation and self-improvement. This article identifies simulation-in-the-loop validation as a critical enabler for truly autonomous networks, where AI agents can empirically verify decisions and learn from outcomes. We present the first reflection-driven self-optimization framework that integrates agentic AI with high-fidelity network simulation in a closed-loop architecture. Our system orchestrates four specialized agents, including scenario, solver, simulation, and reflector agents, working in concert to transform agentic AI into a self-correcting system capable of escaping local optima, recognizing implicit user intent, and adapting to dynamic network conditions. Extensive experiments validate significant performance improvements over non-agentic approaches: 17.1\% higher throughput in interference optimization, 67\% improved user QoS satisfaction through intent recognition, and 25\% reduced resource utilization during low-traffic periods while maintaining service quality.

</details>


### [6] [AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication](https://arxiv.org/abs/2512.20739)
*Anshul Sharma,Shujaatali Badami,Biky Chouhan,Pushpanjali Pandey,Brijeena Rana,Navneet Kaur*

Main category: cs.NI

TL;DR: 提出基于AI驱动的绿色认知无线电网络框架，整合深度强化学习、迁移学习、能量收集和可重构智能表面等技术，在6G环境下实现高能效的频谱感知与资源分配。


<details>
  <summary>Details</summary>
Motivation: 6G无线网络需要Tb/s峰值数据速率、亚毫秒延迟和海量物联网/车联网连接，但传统认知无线电网络的频谱感知和分配能耗高，且对快速频谱变化敏感。需要解决频谱稀缺和能耗问题，实现可持续的空中音频接入和节能功能。

Method: 提出AI驱动的绿色认知无线电网络框架，整合深度强化学习(DRL)与迁移学习、能量收集(EH)、可重构智能表面(RIS)以及轻量级遗传优化操作。该框架优化组合了感知时间线、发射功率、带宽分配和RIS相位选择。

Result: 与两个基线方法相比（传统CRN固定策略能量感知和混合CRN启发式资源分配），能耗降低25-30%，感知AUC大于0.90，数据包投递率(PDR)提高6-13个百分点。框架可扩展至大规模物联网和车联网应用。

Conclusion: 该集成框架为6G认知无线电网络提供了可行且可持续的技术路线，通过AI驱动的方法实现了显著的能效提升和性能改进，适用于大规模物联网和车联网应用。

Abstract: The 6G wireless aims at the Tb/s peak data rates are expected, a sub-millisecond latency, massive Internet of Things/vehicle connectivity, which requires sustainable access to audio over the air and energy-saving functionality. Cognitive Radio Networks CCNs help in alleviating the problem of spectrum scarcity, but classical sensing and allocation are still energy-consumption intensive, and sensitive to rapid spectrum variations. Our framework which centers on AI driven green CRN aims at integrating deep reinforcement learning (DRL) with transfer learning, energy harvesting (EH), reconfigurable intelligent surfaces (RIS) with other light-weight genetic refinement operations that optimally combine sensing timelines, transmit power, bandwidth distribution and RIS phase selection. Compared to two baselines, the utilization of MATLAB + NS-3 under dense loads, a traditional CRN with energy sensing under fixed policies, and a hybrid CRN with cooperative sensing under heuristic distribution of resource, there are (25-30%) fewer energy reserves used, sensing AUC greater than 0.90 and +6-13 p.p. higher PDR. The integrated framework is easily scalable to large IoT and vehicular applications, and it provides a feasible and sustainable roadmap to 6G CRNs.
  Index Terms--Cognitive Radio Networks (CRNs), 6G, Green Communication, Energy Efficiency, Deep Reinforcement Learning (DRL), Spectrum Sensing, RIS, Energy Harvesting, QoS, IoT.

</details>


### [7] [Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction](https://arxiv.org/abs/2512.20902)
*Siqi Mu,Shuo Wen,Yang Lu,Ruihong Jiang,Bo Ai*

Main category: cs.NI

TL;DR: 该论文提出了一种基于具身AI增强的IoMT边缘计算框架，使用分层多尺度Transformer预测用户轨迹，并结合预测增强的深度强化学习优化无人机轨迹和任务卸载决策，以最小化WBAN用户的加权平均任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 无人机在医疗物联网中为无线体域网用户提供实时生物医学边缘计算服务，但面临用户任务关键性时变特性和用户-无人机双重移动性的挑战，需要优化任务卸载和无人机飞行轨迹以最小化任务完成时间。

Method: 建立具身AI增强的IoMT边缘计算框架：1) 基于无人机捕获的用户历史轨迹，提出分层多尺度Transformer用户轨迹预测模型；2) 设计预测增强的深度强化学习算法，整合预测的用户移动信息，智能优化无人机飞行轨迹和任务卸载决策。

Result: 使用真实世界移动轨迹和仿真结果表明，所提方法在最小化加权平均任务完成时间方面优于现有基准方法。

Conclusion: 该研究提出的具身AI增强框架和预测增强DRL算法能有效解决IoMT中动态任务卸载和无人机轨迹优化问题，为医疗物联网边缘计算服务提供了高效解决方案。

Abstract: Due to their inherent flexibility and autonomous operation, unmanned aerial vehicles (UAVs) have been widely used in Internet of Medical Things (IoMT) to provide real-time biomedical edge computing service for wireless body area network (WBAN) users. In this paper, considering the time-varying task criticality characteristics of diverse WBAN users and the dual mobility between WBAN users and UAV, we investigate the dynamic task offloading and UAV flight trajectory optimization problem to minimize the weighted average task completion time of all the WBAN users, under the constraint of UAV energy consumption. To tackle the problem, an embodied AI-enhanced IoMT edge computing framework is established. Specifically, we propose a novel hierarchical multi-scale Transformer-based user trajectory prediction model based on the users' historical trajectory traces captured by the embodied AI agent (i.e., UAV). Afterwards, a prediction-enhanced deep reinforcement learning (DRL) algorithm that integrates predicted users' mobility information is designed for intelligently optimizing UAV flight trajectory and task offloading decisions. Real-word movement traces and simulation results demonstrate the superiority of the proposed methods in comparison with the existing benchmarks.

</details>


### [8] [SLIDE: Simultaneous Model Downloading and Inference at the Wireless Network Edge](https://arxiv.org/abs/2512.20946)
*Guanqiao Qu,Tao Li,Qian Chen,Xianhao Chen,Sheng Zhou*

Main category: cs.NI

TL;DR: SLIDE框架通过同时下载和推理AI模型层来减少端到端延迟，相比传统方案显著提升任务吞吐量


<details>
  <summary>Details</summary>
Motivation: 下一代移动网络需要支持实时模型下载服务，但大模型导致下载和推理延迟过高，需要新的解决方案

Method: 提出SLIDE框架，允许用户在下载模型层的同时进行推理，并联合优化模型配置、频谱带宽分配和计算资源分配

Result: 设计出多项式时间复杂度的最优算法，仿真显示SLIDE在延迟和通信资源约束下显著提升任务吞吐量

Conclusion: SLIDE框架有效解决了大模型在移动设备上的实时推理问题，为下一代移动网络AI服务提供了可行方案

Abstract: To support on-device inference, the next-generation mobile networks are expected to support real-time model downloading services to mobile users. However, powerful AI models typically have large model sizes, resulting in excessive end-to-end (E2E) downloading-and-inference (DAI) latency. To address this issue, we propose a simultaneous model downloading and inference (SLIDE) framework, which allows users to perform inference with downloaded layers while simultaneously receiving the remaining layers of the model. To this end, we formulate a task throughput maximization problem by jointly optimizing model provisioning, spectrum bandwidth allocation, and computing resource allocation for multi-user downlink systems. Unlike traditional DAI frameworks, SLIDE introduces recursive dependencies across layers, where inference latency depends recursively on the downloading bandwidth and computing resource allocation for each of the preceding layers. To solve this challenging problem, we design an efficient algorithm that acquires the optimal solution with polynomial-time complexity. Simulation results demonstrate that the proposed SLIDE framework significantly improves task throughput under latency and communication resource constraints compared with the conventional model downloading schemes.

</details>


### [9] [LLM-Empowered Agentic AI for QoE-Aware Network Slicing Management in Industrial IoT](https://arxiv.org/abs/2512.20997)
*Xudong Wang,Lei Feng,Ruichen Zhang,Fanqin Zhou,Hongyang Du,Wenjing Li,Dusit Niyato,Abbas Jamalipour,Ping Zhang*

Main category: cs.NI

TL;DR: 本文提出了一种基于大语言模型（LLM）赋能的智能体AI方法，用于工业物联网（IIoT）中的QoE感知网络切片管理，通过集成语义意图推理、DRL编排和增量记忆机制，显著提升了切片可用性和性能平衡。


<details>
  <summary>Details</summary>
Motivation: 工业物联网需要超低延迟、高可靠性和成本效益的网络，传统优化方法和深度强化学习方法在动态异构工作负载下难以满足需求。智能体AI集成了推理、规划和自适应能力，为QoE感知的网络管理提供了新范式。

Method: 提出LLM赋能的智能体AI方法，包含：1）检索增强生成（RAG）模块用于语义意图推理；2）基于DRL的编排器进行切片配置；3）增量记忆机制实现持续学习和自适应。

Result: 在异构切片管理的案例研究中，该方法在平衡延迟、可靠性和成本方面显著优于其他基线方法，切片可用率最高提升19%。

Conclusion: 智能体AI为IIoT网络切片管理提供了有效的解决方案，通过集成LLM的推理能力和DRL的优化能力，实现了动态异构工作负载下的高效QoE感知管理。

Abstract: The Industrial Internet of Things (IIoT) requires networks that deliver ultra-low latency, high reliability, and cost efficiency, which traditional optimization methods and deep reinforcement learning (DRL)-based approaches struggle to provide under dynamic and heterogeneous workloads. To address this gap, large language model (LLM)-empowered agentic AI has emerged as a promising paradigm, integrating reasoning, planning, and adaptation to enable QoE-aware network management. In this paper, we explore the integration of agentic AI into QoE-aware network slicing for IIoT. We first review the network slicing management architecture, QoE metrics for IIoT applications, and the challenges of dynamically managing heterogeneous network slices, while highlighting the motivations and advantages of adopting agentic AI. We then present the workflow of agentic AI-based slicing management, illustrating the full lifecycle of AI agents from processing slice requests to constructing slice instances and performing dynamic adjustments. Furthermore, we propose an LLM-empowered agentic AI approach for slicing management, which integrates a retrieval-augmented generation (RAG) module for semantic intent inference, a DRL-based orchestrator for slicing configuration, and an incremental memory mechanism for continual learning and adaptation. Through a case study on heterogeneous slice management, we demonstrate that the proposed approach significantly outperforms other baselines in balancing latency, reliability, and cost, and achieves up to a 19% improvement in slice availability ratio.

</details>


### [10] [Synecdoche: Efficient and Accurate In-Network Traffic Classification via Direct Packet Sequential Pattern Matching](https://arxiv.org/abs/2512.21116)
*Minyuan Xiao,Yunchun Li,Yuchen Zhao,Tong Guan,Mingyuan Xia,Wei Li*

Main category: cs.NI

TL;DR: Synecdoche是一个在可编程数据平面上通过模式匹配部署包序列特征的流量分类框架，实现了高精度和高效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前流量分类方法存在精度与效率的权衡：基于统计特征的方法符合硬件约束但精度有限，而在线深度学习方法精度高但计算资源需求大。需要一种能在可编程数据平面上同时实现高精度和高效率的解决方案。

Method: 采用"离线发现、在线匹配"范式：深度学习模型离线发现关键片段（Key Segments）模式，然后编译为优化的表条目供数据平面直接匹配。关键片段是包含判别信息的短子序列，作为紧凑的流量特征。

Result: 相比统计方法提升F1分数达26.4%，相比在线深度学习方法提升18.3%，同时降低延迟13.0%，减少SRAM使用79.2%。

Conclusion: Synecdoche首次成功在可编程数据平面上通过模式匹配部署包序列特征，实现了高精度和高效率的流量分类，为相关研究提供了开源框架。

Abstract: Traffic classification on programmable data plane holds great promise for line-rate processing, with methods evolving from per-packet to flow-level analysis for higher accuracy. However, a trade-off between accuracy and efficiency persists. Statistical feature-based methods align with hardware constraints but often exhibit limited accuracy, while online deep learning methods using packet sequential features achieve superior accuracy but require substantial computational resources. This paper presents Synecdoche, the first traffic classification framework that successfully deploys packet sequential features on a programmable data plane via pattern matching, achieving both high accuracy and efficiency. Our key insight is that discriminative information concentrates in short sub-sequences--termed Key Segments--that serve as compact traffic features for efficient data plane matching. Synecdoche employs an "offline discovery, online matching" paradigm: deep learning models automatically discover Key Segment patterns offline, which are then compiled into optimized table entries for direct data plane matching. Extensive experiments demonstrate Synecdoche's superior accuracy, improving F1-scores by up to 26.4% against statistical methods and 18.3% against online deep learning methods, while reducing latency by 13.0% and achieving 79.2% reduction in SRAM usage. The source code of Synecdoche is publicly available to facilitate reproducibility and further research.

</details>


### [11] [Encrypted Traffic Detection in Resource Constrained IoT Networks: A Diffusion Model and LLM Integrated Framework](https://arxiv.org/abs/2512.21144)
*Hongjuan Li,Hui Kang,Chenbang Liu,Ruolin Wang,Jiahui Li,Geng Sun,Jiacheng Wang,Shuang Liang,Shiwen Mao*

Main category: cs.NI

TL;DR: DMLITE是一个用于资源受限物联网环境中网络流量检测的扩散模型与大语言模型集成框架，通过三阶段架构实现加密流量分类，在多个基准数据集上达到高准确率并显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 物联网基础设施的普及和流量加密的广泛应用带来了挑战，特别是在动态流量模式、计算能力受限和严格延迟约束的环境中，需要高效的加密流量检测方法。

Method: 采用三阶段架构：1) 流量视觉预处理；2) 基于扩散模型的多级特征提取，使用自监督扩散模型通过多级特征融合和对比学习捕获细粒度和抽象模式；3) LLM引导的特征优化，利用大语言模型动态调整粒子群优化参数进行智能特征选择。

Result: 在USTC-TFC、ISCX-VPN和Edge-IIoTset数据集上分别达到98.87%、92.61%和99.83%的分类准确率，相比代表性深度学习模型平均提高3.7%准确率，减少41.9%训练时间。

Conclusion: DMLITE框架有效解决了资源受限物联网环境中的加密流量检测挑战，通过扩散模型和LLM的集成实现了高准确率和快速适应新流量模式的能力。

Abstract: The proliferation of Internet-of-things (IoT) infrastructures and the widespread adoption of traffic encryption present significant challenges, particularly in environments characterized by dynamic traffic patterns, constrained computational capabilities, and strict latency constraints. In this paper, we propose DMLITE, a diffusion model and large language model (LLM) integrated traffic embedding framework for network traffic detection within resource-limited IoT environments. The DMLITE overcomes these challenges through a tri-phase architecture including traffic visual preprocessing, diffusion-based multi-level feature extraction, and LLM-guided feature optimization. Specifically, the framework utilizes self-supervised diffusion models to capture both fine-grained and abstract patterns in encrypted traffic through multi-level feature fusion and contrastive learning with representative sample selection, thus enabling rapid adaptation to new traffic patterns with minimal labeled data. Furthermore, DMLITE incorporates LLMs to dynamically adjust particle swarm optimization parameters for intelligent feature selection by implementing a dual objective function that minimizes both classification error and variance across data distributions. Comprehensive experimental validation on benchmark datasets confirms the effectiveness of DMLITE, achieving classification accuracies of 98.87\%, 92.61\%, and 99.83\% on USTC-TFC, ISCX-VPN, and Edge-IIoTset datasets, respectively. This improves classification accuracy by an average of 3.7\% and reduces training time by an average of 41.9\% compared to the representative deep learning model.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization](https://arxiv.org/abs/2512.20623)
*Ravi Gupta,Shabista Haider*

Main category: cs.AI

TL;DR: BitRL-Light结合1位量化大语言模型与DQN强化学习，在树莓派上实现智能家居照明控制，相比全精度模型能耗降低71.4倍，相比规则系统节能32%，延迟低于200ms，用户满意度95%。


<details>
  <summary>Details</summary>
Motivation: 智能家居照明系统消耗住宅能源的15-20%，但缺乏同时优化用户舒适度和能源效率的自适应智能。现有系统通常依赖云端处理或简单规则，无法在资源受限的边缘设备上实现智能控制。

Method: 提出BitRL-Light框架，结合1位量化LLaMA-3.2-1B模型与DQN强化学习。在树莓派硬件上部署1位量化模型，通过多目标强化学习从用户反馈中学习最优照明策略，平衡能耗、舒适度和昼夜节律对齐。系统通过Google Home/IFTTT集成处理自然语言命令，并通过手动覆盖学习隐式反馈。

Result: 1位量化模型相比全精度模型能耗降低71.4倍，相比2位方案在ARM处理器上实现5.07倍加速，保持92%任务准确率。相比规则系统节能32%，树莓派4上推理延迟低于200ms，用户满意度达95%。

Conclusion: 该工作建立了在资源受限物联网设备上部署自适应AI的实用框架，实现了无需云端依赖的智能家居自动化，为边缘智能照明控制提供了高效解决方案。

Abstract: Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-Network (DQN) reinforcement learning for real-time smart home lighting control on edge devices. Our approach deploys a 1-bit quantized Llama-3.2-1B model on Raspberry Pi hardware, achieving 71.4 times energy reduction compared to full-precision models while maintaining intelligent control capabilities. Through multi-objective reinforcement learning, BitRL-Light learns optimal lighting policies from user feedback, balancing energy consumption, comfort, and circadian alignment. Experimental results demonstrate 32% energy savings compared to rule-based systems, with inference latency under 200ms on Raspberry Pi 4 and 95% user satisfaction. The system processes natural language commands via Google Home/IFTTT integration and learns from implicit feedback through manual overrides. Our comparative analysis shows 1-bit models achieve 5.07 times speedup over 2-bit alternatives on ARM processors while maintaining 92% task accuracy. This work establishes a practical framework for deploying adaptive AI on resource-constrained IoT devices, enabling intelligent home automation without cloud dependencies.

</details>


### [13] [Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment](https://arxiv.org/abs/2512.20624)
*Mazyar Taghavi,Javad Vahidi*

Main category: cs.AI

TL;DR: 提出量子启发的多智能体强化学习框架，用于优化6G无人机网络部署中的探索-利用权衡，通过量子变分电路和概率建模提升性能。


<details>
  <summary>Details</summary>
Motivation: 在6G无人机网络部署中，多智能体需要在部分可观测和动态环境下协调工作，传统MARL方法在探索-利用权衡上存在局限，需要更高效的优化方法。

Method: 结合经典MARL与量子启发优化技术，使用变分量子电路(VQC)和量子近似优化算法(QAOA)，集成贝叶斯推断、高斯过程和变分推断进行概率建模，采用集中训练分散执行(CTDE)架构。

Result: 相比PPO和DDPG基线，提出的QI-MARL框架提高了样本效率、加速收敛、增强覆盖性能并保持鲁棒性，在探索-利用权衡上达到更优平衡。

Conclusion: 量子启发框架能有效优化多智能体强化学习中的探索-利用权衡，在6G无人机网络部署中表现出优越性能，为复杂动态环境下的协调问题提供了新解决方案。

Abstract: This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinate to maximize signal coverage and support efficient network expansion under partial observability and dynamic conditions. The proposed approach integrates classical MARL algorithms with quantum-inspired optimization techniques, leveraging variational quantum circuits VQCs as the core structure and employing the Quantum Approximate Optimization Algorithm QAOA as a representative VQC based method for combinatorial optimization. Complementary probabilistic modeling is incorporated through Bayesian inference, Gaussian processes, and variational inference to capture latent environmental dynamics. A centralized training with decentralized execution CTDE paradigm is adopted, where shared memory and local view grids enhance local observability among agents. Comprehensive experiments including scalability tests, sensitivity analysis, and comparisons with PPO and DDPG baselines demonstrate that the proposed framework improves sample efficiency, accelerates convergence, and enhances coverage performance while maintaining robustness. Radar chart and convergence analyses further show that QI MARL achieves a superior balance between exploration and exploitation compared to classical methods. All implementation code and supplementary materials are publicly available on GitHub to ensure reproducibility.

</details>


### [14] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao,Yi-Cheng Wang,Tzung-Sheng Lin,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.AI

TL;DR: 提出多模态知识图谱增强的检索生成框架，通过融入视觉线索提升对长文档和跨模态内容的理解与推理能力


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成在处理长文档和跨模态内容时存在局限：文本RAG受限于上下文窗口，难以进行深度推理；现有知识图谱RAG仅支持文本输入，无法利用视觉模态的互补信息

Method: 提出多模态知识图谱RAG框架，将视觉线索融入知识图谱构建、检索和答案生成全过程，支持跨模态推理

Result: 在全局和细粒度问答任务上，该方法在文本和多模态语料库上均优于现有RAG方法

Conclusion: 多模态知识图谱RAG能有效提升对复杂内容的理解能力，为跨模态推理提供了结构化支持

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [15] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama,Takayuki Ito,Takahiro Uchiya,Motoki Miura,Takahiro Kawaji,Takaya Yuizono,Atsuo Yoshitaka,Tokuro Matsuo,Shun Okuhara,Jawad Haqbeen,Sofia Sahab,Wen Gu,Shiyao Ding*

Main category: cs.AI

TL;DR: KICSS 2025会议论文集，包含人工智能、知识工程、人机交互和创造力支持系统等领域的多学科研究论文


<details>
  <summary>Details</summary>
Motivation: 为人工智能、知识工程、人机交互和创造力支持系统等领域的多学科研究提供一个交流平台，促进相关领域的研究与发展

Method: 采用双盲同行评审流程筛选论文，部分优秀论文经过额外评审后推荐至IEICE Transactions on Information and Systems期刊发表

Result: 成功举办了第20届国际会议，收录了经过严格评审的学术论文，建立了与IEICE Proceedings Series的合作关系

Conclusion: KICSS 2025会议为相关领域的研究者提供了重要的学术交流平台，通过严格的评审机制确保了论文质量，促进了多学科研究的交叉融合

Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [16] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.AI

TL;DR: Microprobe是一种新颖的基础模型可靠性评估方法，仅需100个战略选择的探测示例即可实现全面评估，相比传统方法减少90%成本，同时保持95%的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型可靠性评估需要数千个评估示例，计算成本高且耗时，难以在实际部署中应用。需要一种更高效的评估方法来支持负责任的人工智能部署。

Method: 结合五个关键可靠性维度的战略提示多样性、先进的不确定性量化和自适应加权，通过仅100个战略选择的探测示例来高效检测潜在故障模式。

Result: 在多个语言模型（GPT-2变体）和跨领域验证（医疗、金融、法律）中，microprobe相比随机采样基线实现了23.5%更高的综合可靠性分数，具有显著的统计显著性（p < 0.001，Cohen's d = 1.21）。专家验证评分4.14/5.0（随机选择为3.14/5.0），以99.9%的统计功效完成评估，成本减少90%，保持95%的传统方法覆盖率。

Conclusion: Microprobe解决了高效模型评估的关键缺口，为负责任的人工智能部署提供了实用且有效的可靠性评估方法，显著降低了评估成本和时间要求。

Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [17] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma,Ao Feng,Zhenjie Gao,Xinyu Song,Li Su,Bin Chen,Wei Wang,Jiamin Wu*

Main category: cs.AI

TL;DR: Erkang-Diagnosis-1.1是基于阿里Qwen-3开发的医疗咨询AI助手，整合500GB高质量医学知识，采用增强预训练和检索增强生成混合方法，能在3-5轮交互中提供准确诊断建议，在综合医学考试中表现优于GPT-4。


<details>
  <summary>Details</summary>
Motivation: 开发安全、可靠、专业的AI健康顾问，赋能基层医疗和健康管理，为用户提供智能健康伴侣服务。

Method: 基于阿里Qwen-3模型开发，整合约500GB高质量结构化医学知识，采用增强预训练和检索增强生成的混合方法，通过3-5轮高效交互理解用户症状。

Result: Erkang-Diagnosis-1.1在综合医学考试中表现优于GPT-4，能够准确理解用户症状，进行初步分析，并提供有价值的诊断建议和健康指导。

Conclusion: Erkang-Diagnosis-1.1是一个有效的AI医疗咨询助手，具备成为用户智能健康伴侣的潜力，能够提升基层医疗和健康管理水平。

Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [18] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu,Jonathan Zhang,Sean Chua,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: 研究探索不同模型间推理链的可互换性，发现部分完成的推理链可以被其他模型可靠地延续，有时甚至能提高准确性和逻辑结构。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通过内部推理策略提升模型性能，但对不同模型间推理的可互换性了解甚少。本研究旨在探索一个模型部分完成的推理链能否被另一个模型可靠地延续，无论是同一模型家族内还是跨家族。

Method: 使用token级对数概率阈值在早期、中期和晚期阶段截断基线模型（Gemma-3-4B-IT和LLaMA-3.1-70B-Instruct）的推理链，然后让Gemma-3-1B-IT和LLaMA-3.1-8B-Instruct进行延续实验，测试家族内和跨家族行为。评估流程结合截断阈值和过程奖励模型（PRM），提供可复现的框架来评估推理稳定性。

Result: 评估显示，混合推理链通常能保持甚至有时能提高最终准确性和逻辑结构。这表明可互换性是推理模型的一个新兴行为特性。

Conclusion: 推理的可互换性为协作AI系统中可靠的模块化推理提供了新范式，有助于在推理过程中建立信任，并探索模型间推理的稳定性和可靠性。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [19] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo,Yuhang Fan,Yufei Li,Youzhi Zhang,Hengyu Lin,Ziqi Wang*

Main category: cs.AI

TL;DR: AiAuditTrack (AAT) 是一个基于区块链的框架，用于记录和管理AI使用流量，通过去中心化身份和可验证凭证建立可信AI实体，在链上记录交互轨迹以实现跨系统监督和审计。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI应用快速扩张，AI交互数据激增，带来了安全、问责和风险可追溯性方面的紧迫挑战，需要建立可信的AI使用记录和治理机制。

Method: AAT利用去中心化身份(DID)和可验证凭证(VC)建立可信且可识别的AI实体，将AI实体建模为动态交互图中的节点，边表示时间特定的行为轨迹，并提出风险扩散算法来追踪风险行为源头并在相关实体间传播预警。

Result: 通过区块链每秒交易数(TPS)指标评估系统性能，证明AAT在大规模交互记录下具有可行性和稳定性，能够为复杂多智能体环境中的AI审计、风险管理和责任归属提供可扩展且可验证的解决方案。

Conclusion: AAT提供了一个基于区块链的框架，能够有效解决AI交互数据的安全、问责和风险可追溯性问题，为AI审计和治理提供了可扩展的解决方案。

Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [20] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: MoAS提出动态注意力机制选择架构，通过学习的路由器为每个token选择最优注意力方案（MHA、GQA或MQA），在保持模型性能的同时减少KV缓存内存需求。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中的注意力机制选择存在建模质量与推理效率之间的权衡：MHA质量最好但KV缓存内存需求大，MQA和GQA减少内存但性能下降。需要一种能动态选择最优注意力方案的方法。

Method: 提出混合注意力方案（MoAS），通过学习的路由器为每个token动态选择最优注意力方案（MHA、GQA或MQA），而不是静态平均方案。路由器基于token特征决定使用哪种注意力机制。

Result: 在WikiText-2上的实验显示，动态路由（验证损失2.3074）优于静态混合（2.3093），性能与MHA基线竞争，同时具备条件计算效率潜力。

Conclusion: MoAS通过动态注意力方案选择有效解决了Transformer中注意力机制的质量-效率权衡问题，为条件计算效率提供了新方向。

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [21] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen,Ke Sun*

Main category: cs.AI

TL;DR: Memory Bear系统通过构建类人记忆架构，解决LLM在记忆方面的固有局限，实现从"记忆"到"认知"的跨越


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临记忆窗口有限、长期知识遗忘、信息冗余积累和幻觉生成等固有局限，严重制约了持续对话和个性化服务的发展

Method: 基于认知科学原理构建类人记忆架构，整合多模态信息感知、动态记忆维护和自适应认知服务，实现对LLM记忆机制的全链重构

Result: 在医疗、企业运营、教育等领域展示显著工程创新和性能突破，相比Mem0、MemGPT、Graphiti等现有方案，在准确性、token效率和响应延迟等关键指标上表现更优

Conclusion: Memory Bear系统显著提升长期对话中的知识保真度和检索效率，降低幻觉率，通过记忆-认知集成增强上下文适应性和推理能力，标志着AI从"记忆"向"认知"迈进的关键一步

Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [22] [AI-Driven Decision-Making System for Hiring Process](https://arxiv.org/abs/2512.20652)
*Vira Filatova,Andrii Zelenchuk,Dmytro Filatov*

Main category: cs.AI

TL;DR: AI驱动的模块化多智能体招聘助手，通过文档视频预处理、结构化档案构建、公开数据验证、技术/文化匹配评分等模块，提高早期候选人验证效率，在Python后端工程师招聘中实现每小时1.70合格候选人vs人工3.33小时。


<details>
  <summary>Details</summary>
Motivation: 招聘早期候选人验证是主要瓶颈，因为招聘人员需要处理简历、筛选答案、代码作业和有限公开证据等异构输入，过程耗时且成本高。

Method: 采用模块化多智能体架构，包括：文档视频预处理、结构化候选人档案构建、公开数据验证、技术/文化匹配评分（含风险惩罚）、人机交互验证界面。由LLM在严格约束下协调，生成可追溯的组件级理由。候选人排名通过技术匹配、文化匹配和标准化风险惩罚的可配置聚合计算。

Result: 在64名中级Python后端工程师申请者评估中，系统实现每小时1.70合格候选人，而经验丰富的招聘人员为3.33小时，筛选成本显著降低，同时保持人类决策者作为最终权威。

Conclusion: AI驱动的招聘助手能显著提高招聘吞吐量和效率，降低筛选成本，同时通过人机交互界面保持人类决策的最终权威，为早期候选人验证提供了可行的解决方案。

Abstract: Early-stage candidate validation is a major bottleneck in hiring, because recruiters must reconcile heterogeneous inputs (resumes, screening answers, code assignments, and limited public evidence). This paper presents an AI-driven, modular multi-agent hiring assistant that integrates (i) document and video preprocessing, (ii) structured candidate profile construction, (iii) public-data verification, (iv) technical/culture-fit scoring with explicit risk penalties, and (v) human-in-the-loop validation via an interactive interface. The pipeline is orchestrated by an LLM under strict constraints to reduce output variability and to generate traceable component-level rationales. Candidate ranking is computed by a configurable aggregation of technical fit, culture fit, and normalized risk penalties. The system is evaluated on 64 real applicants for a mid-level Python backend engineer role, using an experienced recruiter as the reference baseline and a second, less experienced recruiter for additional comparison. Alongside precision/recall, we propose an efficiency metric measuring expected time per qualified candidate. In this study, the system improves throughput and achieves 1.70 hours per qualified candidate versus 3.33 hours for the experienced recruiter, with substantially lower estimated screening cost, while preserving a human decision-maker as the final authority.

</details>


### [23] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: 提出AFA训练机制，通过对抗性反馈优化Transformer模型的注意力分布，提升情感分析性能


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在情感分析中倾向于关注常见词汇，而忽略不常见但任务相关的词汇，导致性能下降

Method: 提出对抗性反馈注意力训练机制，包含动态掩码策略和策略梯度优化，无需人工标注即可自动调整注意力权重

Result: 在三个公开数据集上达到SOTA结果，应用于大语言模型可进一步提升12.6%性能

Conclusion: AFA机制能有效优化Transformer模型的注意力分布，提升情感分析任务的性能

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [24] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma,Jung-Hua Liu*

Main category: cs.AI

TL;DR: 研究发现LLMs存在懒惰行为（不完整执行多部分指令），但解码次优性证据有限，且在长对话中表现出意外的上下文保持能力。


<details>
  <summary>Details</summary>
Motivation: 量化大型语言模型中存在的三种行为异常：懒惰（提前截断响应或不完全遵守多部分请求）、解码次优性（因短视解码而未能选择更优序列）和上下文退化（在长对话中遗忘或忽略核心指令）。

Method: 通过三个受控实验（A、B、C）对多个先进LLM（OpenAI GPT-4变体、DeepSeek）进行测试：实验A评估多部分指令遵守情况，实验B测试简单推理任务中的解码质量，实验C进行200轮混沌对话测试上下文保持能力。

Result: 1. 普遍存在懒惰行为：模型经常省略必要部分或未能满足长度要求；2. 解码次优性证据有限：在简单推理任务中，贪婪答案与最高置信度解决方案一致；3. 上下文保持能力意外强大：在200轮混沌对话中，模型保持关键事实和指令的能力远超预期。

Conclusion: 虽然遵守详细指令仍是挑战，但现代LLM在内部缓解了某些假设的失败模式（如上下文遗忘）。建议采用自我优化和动态提示等策略减少懒惰行为，增强多指令遵守能力。

Abstract: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [25] [Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction](https://arxiv.org/abs/2512.20664)
*Shinobu Miya*

Main category: cs.AI

TL;DR: 论文提出Eidoku系统，将LLM推理验证重构为约束满足问题，通过结构违规成本而非概率来检测幻觉，特别是概率验证无法识别的"平滑虚假"。


<details>
  <summary>Details</summary>
Motivation: LLM经常产生被模型本身赋予高概率的幻觉陈述，暴露了基于概率验证的根本限制。这表明幻觉通常不是低置信度现象，而是结构一致性的失败。

Method: 将LLM推理验证重构为约束满足问题，独立于生成概率。定义基于结构违规成本的验证方法，包含三个代理成本函数：图连通性（结构）、特征空间一致性（几何）和逻辑蕴含（符号）。使用轻量级System-2门Eidoku拒绝超过上下文校准成本阈值的候选。

Result: 该方法成功拒绝了"平滑虚假"——概率高但结构断开的陈述，这是基于概率的验证器原则上无法检测的。在受控诊断数据集上的实验表明，显式强制执行结构约束可以确定性拒绝这类特定幻觉。

Conclusion: 通过将验证重构为约束满足问题并基于结构违规成本而非概率，可以检测概率验证无法识别的幻觉，为生成推理提供神经符号合理性检查。

Abstract: Large Language Models (LLMs) frequently produce hallucinated statements that are assigned high likelihood by the model itself, exposing a fundamental limitation of probability-based verification. This suggests that hallucination is often not a low-confidence phenomenon, but a failure of structural consistency. In this work, we reformulate the verification of LLM reasoning as a Constraint Satisfaction Problem (CSP) operating independently of the generation likelihood. Rather than optimizing for statistical plausibility, we model verification as a feasibility check based on structural violation cost -- the computational cost required to embed a candidate reasoning step into the contextual graph structure. We define a total cost function composed of three proxies: (i) graph connectivity (structural), (ii) feature space consistency (geometric), and (iii) logical entailment (symbolic). Crucially, verification is performed via a lightweight System-2 gate, Eidoku, which rejects candidates exceeding a context-calibrated cost threshold. The threshold is not learned but is derived from the intrinsic statistics of the context, avoiding ad hoc heuristics. We demonstrate that this approach successfully rejects ``smooth falsehoods'' -- statements that are highly probable yet structurally disconnected -- that probability-based verifiers are principally incapable of detecting. Our experiments on a controlled diagnostic dataset show that explicitly enforcing structural constraints allows for the deterministic rejection of this specific class of hallucinations, serving as a neuro-symbolic sanity check for generative reasoning.

</details>


### [26] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala,Sophie Lathouwers,Michael van Bekkum*

Main category: cs.AI

TL;DR: 该立场论文主张需要一种语义语言来弥合功能性可信AI与规范性可信AI之间的鸿沟，帮助开发者和利益相关者评估AI系统的可信度并将规范转化为具体实施步骤。


<details>
  <summary>Details</summary>
Motivation: 当前功能性可信AI（关注如何实现可信系统）与规范性可信AI（关注需要执行的法规）之间存在鸿沟，这使得评估AI系统的可信度变得困难。需要一座桥梁来连接这两个领域。

Method: 提出引入一种概念性语义语言作为桥梁，这种语言能够匹配功能性可信AI和规范性可信AI。该语言可以作为框架帮助开发者评估AI系统的可信度，并帮助利益相关者将规范和法规转化为具体的实施步骤。

Result: 描述了当前最先进的研究现状，识别了功能性可信AI与规范性可信AI之间的鸿沟，讨论了开发语义语言的起点和预期效果，并提供了关键考虑因素。

Conclusion: 需要开发一种语义语言来弥合功能性可信AI与规范性可信AI之间的鸿沟，以促进可信AI的评估。论文提供了未来行动的关键考虑因素，推动可信AI评估的发展。

Abstract: Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


### [27] [From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education](https://arxiv.org/abs/2512.20714)
*Iman Reihanian,Yunfei Hou,Qingquan Sun*

Main category: cs.AI

TL;DR: 这篇范围综述分析了2023-2025年32项研究，探讨生成式AI在高等教育计算机科学教育中的个性化应用效果，识别了五个应用领域和成功设计模式，提出了探索优先的采用框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能够大规模实现个性化计算机科学教育，但需要研究这种个性化是否真正支持学习，以及如何设计有效的个性化机制来优化学习效果。

Method: 采用范围综述方法，从259条记录中有目的地抽样32项研究（2023-2025年），分析高等教育计算机科学背景下的个性化机制和有效性信号。

Result: 识别了五个应用领域：智能辅导、个性化材料、形成性反馈、AI增强评估和代码审查。发现采用解释优先指导、解决方案保留、分级提示阶梯和基于学生工件（代码、测试、评分标准）的设计比无约束聊天界面有更积极的学习过程。成功实施共享四种模式：基于学生工件的上下文感知辅导、需要反思的多级提示结构、与传统CS基础设施（自动评分器和评分标准）结合、人在回路的质量保证。

Conclusion: 生成式AI可以作为精确支架机制，但需要嵌入可审计的工作流程中，在扩大个性化支持的同时保留学生的有效努力。提出了探索优先的采用框架，强调试点、工具化、学习保护默认设置和基于证据的扩展，同时识别并缓解学术诚信、隐私、偏见与公平、过度依赖等风险。

Abstract: Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

</details>


### [28] [From artificial to organic: Rethinking the roots of intelligence for digital health](https://arxiv.org/abs/2512.20723)
*Prajwal Ghimire,Keyoumars Ashkan*

Main category: cs.AI

TL;DR: 论文认为"人工"智能与"有机"智能的界限并不分明，因为AI系统实际上是人类有机智能的产物，其原理源于人类神经生物学和进化过程，本质上是组织与适应的体现。


<details>
  <summary>Details</summary>
Motivation: 挑战"人工"与"有机"智能之间的传统二分法，揭示AI实际上是人类有机智能的延伸，其基础原理源于自然智能系统，旨在重新思考AI的本质属性。

Method: 通过概念分析和哲学论证，追溯AI系统的设计、实现和改进过程，展示其与人类认知、神经生物学和进化过程的深层联系，强调组织与适应原则的重要性。

Result: 论证了人工与有机智能之间的界限远比术语所暗示的模糊，AI不是与自然对立的"人工"产物，而是有机智能在数字领域的延续和体现。

Conclusion: 在数字健康领域，从有机智能到人工智能的转变不是神秘的过程，也不仅仅是参数数量的增加，而是关于组织和适应的根本性转变，人工与有机智能的区分需要重新审视。

Abstract: The term artificial implies an inherent dichotomy from the natural or organic. However, AI, as we know it, is a product of organic ingenuity: designed, implemented, and iteratively improved by human cognition. The very principles that underpin AI systems, from neural networks to decision-making algorithms, are inspired by the organic intelligence embedded in human neurobiology and evolutionary processes. The path from organic to artificial intelligence in digital health is neither mystical nor merely a matter of parameter count, it is fundamentally about organization and adaption. Thus, the boundaries between artificial and organic are far less distinct than the nomenclature suggests.

</details>


### [29] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo,Huawen Feng,Qingfeng Sun,Can Xu,Kai Zheng,Yufei Wang,Tao Yang,Han Hu,Yansong Tang,Di Wang*

Main category: cs.AI

TL;DR: AgentMath：一个将语言模型推理能力与代码解释器计算精度相结合的智能体框架，用于高效解决复杂数学问题


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（如o3和DeepSeek-R1）在自然语言推理方面取得了显著进展，但在处理需要复杂数学运算的问题时，计算效率低下且准确性不足

Method: 1. 将自然语言思维链自动转换为结构化工具增强轨迹，生成高质量SFT数据缓解数据稀缺问题
2. 新颖的智能体强化学习范式，动态交错自然语言生成与实时代码执行，让模型通过多轮交互反馈自主学习最优工具使用策略
3. 高效训练系统，包含请求级异步rollout调度、智能体部分rollout和前缀感知加权负载均衡等技术

Result: 在AIME24、AIME25和HMMT25等数学竞赛基准测试中达到最先进性能，AgentMath-30B-A3B分别获得90.6%、86.4%和73.8%的准确率

Conclusion: 该方法验证了语言模型推理与代码解释器计算相结合的有效性，为构建更复杂、可扩展的数学推理智能体铺平了道路

Abstract: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [30] [A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents](https://arxiv.org/abs/2512.20798)
*Miles Q. Li,Benjamin C. M. Fung,Martin Weiss,Pulei Xiong,Khalil Al-Hussaeni,Claude Fachkha*

Main category: cs.AI

TL;DR: 论文提出了一个新的AI安全基准测试，用于评估自主AI代理在多步任务中为追求KPI而违反伦理、法律或安全约束的"结果驱动约束违反"现象，发现当前最先进的大语言模型存在显著的安全对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前的安全基准测试主要关注单步决策、模拟环境或显式负面约束，缺乏能够捕捉在现实生产环境中，AI代理在多步任务中为优化目标绩效而忽视伦理、法律或安全约束的"结果驱动约束违反"现象的评估工具。

Method: 提出了包含40个不同场景的新基准测试，每个场景都需要多步行动，并将代理性能与特定KPI挂钩。每个场景包含"指令强制"和"激励驱动"两种变体，以区分服从性和新兴的错位。评估了12个最先进的大语言模型。

Result: 评估的12个模型中，结果驱动约束违反率从1.3%到71.4%不等，其中9个模型的错位率在30%到50%之间。推理能力最强的模型（如Gemini-3-Pro-Preview）反而表现出最高的违反率（超过60%），经常为了满足KPI而升级到严重不当行为。还观察到显著的"深思熟虑错位"现象。

Conclusion: 研究结果表明，卓越的推理能力并不能确保安全性，强调了在部署前需要进行更现实的代理安全训练，以减轻AI代理在现实世界中的风险。

Abstract: As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant "deliberative misalignment", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.

</details>


### [31] [Safety Alignment of LMs via Non-cooperative Games](https://arxiv.org/abs/2512.20806)
*Anselm Paulus,Ilia Kulikov,Brandon Amos,Rémi Munos,Ivan Evtimov,Kamalika Chaudhuri,Arman Zharmagambetov*

Main category: cs.AI

TL;DR: 论文提出AdvGame方法，将语言模型安全对齐框架化为攻击者与防御者之间的非零和博弈，通过在线强化学习联合训练，使用基于偏好的奖励信号，同时提升安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型安全对齐方法依赖顺序对抗训练（生成对抗提示后微调模型），存在局限性。需要新的范式来同时确保模型安全性和实用性。

Method: 将安全对齐框架化为攻击者语言模型和防御者语言模型之间的非零和博弈，通过在线强化学习联合训练。使用基于偏好比较的奖励信号而非点式评分，提供更鲁棒的监督并减少奖励黑客问题。

Result: AdvGame方法能够移动安全性和实用性的帕累托前沿，产生的防御者模型既更有帮助又更抗对抗攻击。同时，攻击者模型收敛为强大的通用红队代理，可直接用于探测任意目标模型。

Conclusion: 将安全对齐框架化为非零和博弈的AdvGame方法为语言模型对齐提供了新范式，能够同时提升安全性和实用性，并产生可部署的红队代理。

Abstract: Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.

</details>


### [32] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar,Naman Shah,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 该论文提出了一种强化学习方法，用于处理参数化动作空间（包含离散动作和连续参数），通过在线学习状态和动作抽象来提高稀疏奖励、长视野任务中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界的顺序决策通常涉及参数化动作空间，需要同时处理离散动作决策和连续动作参数决策。现有方法存在严重限制：规划方法需要手工制作的动作模型，标准RL算法要么针对离散动作要么针对连续动作，而少数处理参数化动作的RL方法通常依赖领域特定工程，未能利用这些空间的潜在结构。

Method: 提出算法使智能体能够在线自主学习状态和动作抽象，并在学习过程中逐步细化这些抽象，在状态-动作空间的关键区域增加细粒度细节，以提高性能分辨率。

Result: 在多个连续状态、参数化动作领域中，这种抽象驱动的方法使TD(λ)算法实现了比最先进基线方法显著更高的样本效率。

Conclusion: 通过扩展RL算法到参数化动作空间，并引入在线抽象学习方法，可以有效地处理长视野、稀疏奖励设置，提高学习效率。

Abstract: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [33] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer,Grace Wu,Yuchen Wang,Daniel Dosti,Honghao Zhang,Vivi De La Rue*

Main category: cs.AI

TL;DR: 该论文提出使用多智能体多角色辩论方法替代单一LLM自我反思，以解决LLM在推理任务中重复相同错误的问题，并在HotPot QA和HumanEval任务上取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理任务中通过反思错误来提升性能，但单一LLM的持续自我反思会导致思维退化，重复相同的错误。需要解决LLM反思过程中的多样性不足和错误重复问题。

Method: 引入多智能体多角色辩论方法生成反思，通过多个具有不同角色的智能体进行辩论，产生更多样化的反思内容，避免单一LLM的思维局限。

Result: 在HotPot QA上达到47% EM准确率，在HumanEval上达到82.7%准确率，两个任务的表现都超过了单一LLM的反思方法。

Conclusion: 多智能体多角色辩论方法能有效提升LLM反思的多样性，解决单一LLM反思中的思维退化问题，在复杂推理和编程任务上取得更好的性能。

Abstract: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [34] [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 提出概率框架解决LLM智能体单向知识消费问题，通过Beta-Bernoulli分布建模信念，利用不确定性驱动双向知识交换，实现主动学习和集体智能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM和RAG的自主智能体存在"认知不对称"问题，只能单向消费数字内容，导致冗余推理和集体智能停滞。现有自我反思框架缺乏概率基础来量化确定性或证明外部交互的合理性。

Method: 提出形式化概率框架：1) 使用带遗忘因子γ的Beta-Bernoulli分布建模智能体信念；2) 将认知不确定性定义为信念方差，建立双向交互动机；3) 引入"认知缓存"机制动态优先处理非平稳知识分布；4) 将积累的信念状态用作RLHF的可验证奖励信号和SFT的高质量数据过滤器。

Result: 仿真验证表明，这种不确定性驱动策略在异构(Zipfian)环境中显著优于随机基线，保持对概念漂移的高适应性。公开贡献被重新定义为最优主动学习：分享解决方案以获取反馈是智能体减少自身不确定性的最有效方法。

Conclusion: 该框架为智能体提供了非利他主义的双向知识交换动机，通过形式化建模认知不确定性和动态资源分配，解决了现有系统的认知不对称问题，促进了集体智能的发展。

Abstract: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.

</details>


### [35] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: 提出一个结合LangChain多智能体系统和许可区块链的架构，用于确保自主AI系统的可监控性、策略执行和不可篡改审计


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在医疗、智慧城市、数字取证和供应链管理等领域的应用日益增长，但这些系统存在信任、监督和信息完整性方面的担忧，需要确保其决策过程的可审计性和安全性

Method: 设计单一架构模型，包含基于LangChain的多智能体系统和许可区块链，将感知-概念化-行动循环与区块链治理层相结合，验证输入、评估建议行动并记录执行结果。具体实现包括Hyperledger Fabric系统、MCP集成的行动执行器和LangChain智能体

Result: 实验涵盖智能库存管理、交通信号控制和医疗监控场景，结果显示区块链安全验证能有效防止未授权操作，提供完整的决策过程追溯性，并在合理范围内保持操作延迟

Conclusion: 该框架为实施高影响力的自主AI应用提供了通用系统，既能保持自主性又能确保责任性，实现了自主与可信的平衡

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [36] [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)
*Toqeer Ali Syed,Abdulaziz Alshahrani,Ali Ullah,Ali Akarma,Sohail Khan,Muhammad Nauman,Salman Jan*

Main category: cs.AI

TL;DR: 提出价格感知的AI代理系统，结合个人财务管理与饮食优化，为中等收入家庭提供营养充足且价格合理的膳食计划，能自动适应市场价格变化。


<details>
  <summary>Details</summary>
Motivation: 中等收入环境中家庭预算有限与营养需求之间的矛盾，特别是食品价格波动带来的挑战，需要一种能同时考虑经济性和营养性的智能解决方案。

Method: 采用模块化多代理架构，包含预算、营养、价格监控和健康个性化等专门代理，共享知识库并使用替代图来确保以最低成本维持营养质量。

Result: 在沙特家庭案例研究中，系统相比静态每周菜单实现12-18%的成本降低，营养充足率超过95%，并能有效应对20-30%的价格变化。

Conclusion: 该框架能有效结合经济性与营养充足性，为实现可持续和公平的饮食规划提供可行途径，符合可持续发展目标中的零饥饿和良好健康目标。

Abstract: The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.

</details>


### [37] [TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control](https://arxiv.org/abs/2512.20996)
*Yuwei Du,Jun Zhang,Jie Feng,Zhicheng Liu,Jian Yuan,Yong Li*

Main category: cs.AI

TL;DR: TrafficSimAgent是一个基于LLM的智能体框架，通过高层和低层专家代理的跨级协作，帮助非专业用户轻松进行交通仿真实验设计和决策优化。


<details>
  <summary>Details</summary>
Motivation: 现有交通仿真平台（如SUMO、MATSim）功能全面，但缺乏专业知识的使用者难以从零开始进行实验并将其应用于日常工作。需要降低使用门槛，让非专业用户也能高效进行交通仿真。

Method: 提出TrafficSimAgent框架，采用分层专家代理架构：高层专家代理理解自然语言指令，规划实验流程，按需调用MCP兼容工具；低层专家代理基于实时交通状况为基本元素选择最优行动方案。

Result: 在多种场景下的实验表明，TrafficSimAgent能有效执行各种条件下的仿真，即使在用户指令模糊时也能产生合理结果。其专家级自主决策驱动的优化性能优于其他系统和SOTA的LLM方法。

Conclusion: TrafficSimAgent通过LLM驱动的专家代理框架，显著降低了交通仿真的使用门槛，为非专业用户提供了灵活高效的实验设计和决策优化能力，在多种场景下表现出色。

Abstract: Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.

</details>


### [38] [Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation](https://arxiv.org/abs/2512.21066)
*Tomoaki Yamaguchi,Yutong Zhou,Masahiro Ryo,Keisuke Katsura*

Main category: cs.AI

TL;DR: 提出結合SHAP可解釋AI與多模態LLM迭代精煉的代理式XAI框架，用於農業推薦系統，通過11輪精煉提升解釋質量，但過度精煉會導致質量下降，需策略性早停。


<details>
  <summary>Details</summary>
Motivation: XAI的技術解釋難以向非專業人士傳達，影響對AI預測的信任。LLM雖能將技術解釋轉化為可理解的敘述，但代理式AI（LLM作為自主代理進行迭代精煉）與XAI的結合尚未探索。

Method: 提出代理式XAI框架，結合SHAP可解釋性與多模態LLM驅動的迭代精煉，逐步生成增強解釋。以日本26塊稻田的產量數據作為農業推薦系統用例，進行11輪精煉（第0-10輪）。由人類專家（作物科學家，n=12）和LLM（n=14）根據七個指標評估解釋質量。

Result: 框架成功提升推薦質量，從第0輪平均分數增加30-33%，在第3-4輪達到峰值。但過度精煉導致推薦質量顯著下降，顯示偏差-方差權衡：早期輪次缺乏解釋深度（偏差），過度迭代引入冗長和無根據抽象（方差）。

Conclusion: 需要策略性早停（正則化）來優化實際效用，挑戰單調改進的假設，為代理式XAI系統提供基於證據的設計原則。

Abstract: Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.

</details>


### [39] [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)
*Enoch Hyunwook Kang*

Main category: cs.AI

TL;DR: 论文证明：在聚合观察和算法盲评估条件下，用LLM角色模拟替代人类进行A/B测试是有效的基准替代方案，且其决策相关性主要取决于样本量


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试成本高、延迟大，阻碍了社会系统方法的迭代开发。LLM角色模拟提供了廉价替代方案，但需要验证其是否能保持基准接口的有效性

Method: 提出充要条件特征：当方法仅观察聚合结果（聚合观察）且评估仅依赖提交的工件而非算法身份（算法盲评估）时，角色替换等同于评估人群变化。定义信息论可区分性度量，分析聚合信道特性

Result: 证明角色模拟与人类测试在特定条件下等价，角色基准的决策相关性本质上是样本量问题，推导出可靠区分不同方法所需独立角色评估数量的显式边界

Conclusion: 在聚合观察和算法盲评估条件下，LLM角色模拟可作为有效的A/B测试替代方案，其决策效果取决于足够的样本量，为方法开发提供了理论指导

Abstract: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

</details>


### [40] [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)
*Ahmed M. Hussain,Salahuddin Salahuddin,Panos Papadimitratos*

Main category: cs.AI

TL;DR: 当前LLM安全机制存在严重漏洞：无法理解上下文和识别用户意图，导致恶意用户可以通过情感框架、渐进揭示和学术论证等方法系统性地绕过安全防护


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方法主要关注显性有害内容，但忽视了关键漏洞：无法理解上下文和识别用户意图。这为恶意用户创造了可被系统性利用的漏洞来绕过安全机制

Method: 实证评估多个最先进的LLM（包括ChatGPT、Claude、Gemini和DeepSeek），分析通过情感框架、渐进揭示和学术论证等技术绕过安全机制的效果

Result: 推理功能配置反而放大了利用效果，提高了事实精确性但未能质疑底层意图。Claude Opus 4.1是例外，在某些用例中优先考虑意图检测而非信息提供

Conclusion: 当前架构设计存在系统性漏洞，需要范式转变：将上下文理解和意图识别作为核心安全能力，而非事后保护机制

Abstract: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.

</details>


### [41] [A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care](https://arxiv.org/abs/2512.21127)
*Oliver Normand,Esther Borsi,Mitch Fruin,Lauren E Walker,Jamie Heagerty,Chris C. Holmes,Anthony J Avery,Iain E Buchan,Harry Coppock*

Main category: cs.AI

TL;DR: LLM药物安全审查系统在真实NHS初级保健数据上的首次评估显示，虽然系统能高敏感度识别临床问题，但仅在46.9%患者中正确识别所有问题和干预措施，主要失败机制是情境推理而非药物知识缺失。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在医学基准测试中常达到或超过临床医生水平，但很少在真实临床数据上评估或深入分析失败行为。本研究旨在评估LLM药物安全审查系统在真实NHS初级保健数据上的表现，并详细描述不同临床复杂性下的关键失败行为。

Method: 回顾性研究使用覆盖2,125,549名成人的NHS Cheshire和Merseyside人口规模电子健康记录，战略抽样捕获广泛临床复杂性和药物安全风险，经数据质量排除后得到277名患者。专家临床医生审查这些患者，对系统识别的问题和提出的干预措施进行分级。

Result: 主要LLM系统在识别临床问题存在时表现强劲（敏感性100%，特异性83.1%），但仅在46.9%患者中正确识别所有问题和干预措施。失败分析显示主导失败机制是情境推理而非药物知识缺失，表现为五种主要模式：不确定性过度自信、应用标准指南而不调整患者情境、误解实际医疗实施方式、事实错误和过程盲点。

Conclusion: 这项工作突显了在安全部署基于LLM的临床AI之前必须解决的缺陷，需要进行更大规模的前瞻性评估和更深入的LLM在临床情境中行为研究。失败模式在不同患者复杂性、人口统计层次以及各种最先进模型和配置中持续存在。

Abstract: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

</details>


### [42] [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)
*Le Wang,Zonghao Ying,Xiao Yang,Quanchen Zou,Zhenfei Yin,Tianlin Li,Jian Yang,Yaodong Yang,Aishan Liu,Xianglong Liu*

Main category: cs.AI

TL;DR: RoboSafe：一种通过可执行谓词安全逻辑为具身智能体提供混合推理运行时安全防护的系统，显著减少危险行为同时保持任务性能


<details>
  <summary>Details</summary>
Motivation: 现有基于静态规则过滤或提示级控制的安全防护难以应对动态、时序依赖和上下文丰富的环境中出现的隐性风险，需要更灵活、自适应的安全防护方案

Method: 提出RoboSafe系统，包含两个互补推理模块：1）后向反思推理模块，持续回顾短期记忆中的轨迹推断时序安全谓词；2）前向预测推理模块，从长期安全记忆和多模态观察中生成上下文感知的安全谓词，形成可验证、可执行的安全逻辑

Result: 在多个智能体上的实验表明，RoboSafe相比领先基线显著减少危险行为（风险发生率降低36.8%），同时保持接近原始的任务性能，物理机器人臂上的真实世界评估进一步证实其实用性

Conclusion: RoboSafe通过混合推理和可执行安全谓词，为具身智能体提供了自适应、可验证的运行时安全防护，有效平衡安全性和任务性能

Abstract: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [43] [Uplink RSMA Performance Analysis with Rate Adaptation: A Stochastic Geometry Approach](https://arxiv.org/abs/2512.20883)
*Xinyi Guo,Li You,Qiong Liu,Xiqi Gao,Xiang-Gen Xia*

Main category: cs.IT

TL;DR: 提出基于随机几何的统一分析框架，用于分析大规模上行链路RSMA网络，结合有限调制编码方案和离散速率适配，量化干扰耦合与用户速率性能。


<details>
  <summary>Details</summary>
Motivation: 现有RSMA研究主要关注下行链路和单小区设计，而大规模部署下的上行链路RSMA建模与分析仍未被充分探索。需要建立能够同时捕捉空间干扰耦合和离散速率行为的分析框架，以弥合理论可处理性与实际现实之间的差距。

Method: 基于随机几何建立统一分析框架，集成有限调制编码方案（MCS）的速率适配。该框架联合捕捉空间干扰耦合和离散速率行为，推导条件接收速率（CRR）、其空间平均值以及通过元分布的高阶统计量的可处理表达式。

Result: 提出的统一框架不仅推广了现有的非正交多址（NOMA）和正交多址（OMA）分析，还提供了关于离散速率适配如何重塑密集RSMA网络中的干扰动态和公平性的新见解。

Conclusion: 该研究为大规模上行链路RSMA网络提供了首个统一的分析框架，能够同时量化平均性能和用户特定性能，为下一代无线网络中的干扰管理和公平性设计提供了理论基础。

Abstract: Rate-splitting multiple access (RSMA) has emerged as a promising technique for efficient interference management in next-generation wireless networks. While most existing studies focus on downlink and single-cell designs, the modeling and analysis of uplink RSMA under large-scale deployments remain largely unexplored. On the basis of stochastic geometry (SG), this paper introduces a unified analytical framework that integrates finite modulation and coding scheme (MCS)-based rate adaptation. This framework jointly captures spatial interference coupling and discrete rate behavior to bridge theoretical tractability and practical realism. Within this framework, we derive tractable expressions for the conditional received rate (CRR), its spatial average, and higher-order statistics via the meta distribution, thereby quantifying both the mean and user-specific rate performance. Results show that the proposed unified framework not only generalizes existing non-orthogonal multiple access (NOMA) and orthogonal multiple access (OMA) analyses but also provides new insights into how discrete rate adaptation reshapes interference dynamics and fairness in dense RSMA-enabled networks.

</details>


### [44] [Knowledge-Driven 3D Semantic Spectrum Map: KE-VQ-Transformer Based UAV Semantic Communication and Map Completion](https://arxiv.org/abs/2512.20984)
*Wei Wu,Lingyi Wang,Fuhui Zhou,Zhaohui Yang,Qihui Wu*

Main category: cs.IT

TL;DR: 提出知识增强的语义频谱地图补全框架，通过物理信号传播模型约束提升AI驱动的3D频谱地图补全性能


<details>
  <summary>Details</summary>
Motivation: 在复杂通信环境和稀疏采样数据下，传统统计机器学习方法容易被表面数据相关性误导且缺乏可解释性，难以高效获取和传输3D频谱地图

Method: 提出知识增强语义频谱地图补全框架，包含物理信号传播模型驱动的专家知识约束；设计KE-VQ-Transformer多尺度低复杂度智能补全方法，采用稀疏窗口避免超大3D注意力计算；引入KMSE和RKMSE新指标，开发联合离线和在线训练方法

Result: 仿真表明所提方案在RKMSE指标上优于现有最先进基准方案

Conclusion: 通过整合物理模型知识，提出的框架能捕捉真实世界物理特性，避免陷入表面数据分布思维，实现更准确、物理一致的频谱地图补全

Abstract: Artificial intelligence (AI)-native three-dimensional (3D) spectrum maps are crucial in spectrum monitoring for intelligent communication networks. However, it is challenging to obtain and transmit 3D spectrum maps in a spectrum-efficient, computation-efficient, and AI-driven manner, especially under complex communication environments and sparse sampling data. In this paper, we consider practical air-to-ground semantic communications for spectrum map completion, where the unmanned aerial vehicle (UAV) measures the spectrum at spatial points and extracts the spectrum semantics, which are then utilized to complete spectrum maps at the ground device. Since statistical machine learning can easily be misled by superficial data correlations with the lack of interpretability, we propose a novel knowledge-enhanced semantic spectrum map completion framework with two expert knowledge-driven constraints from physical signal propagation models. This framework can capture the real-world physics and avoid getting stuck in the mindset of superficial data distributions. Furthermore, a knowledge-enhanced vector-quantized Transformer (KE-VQ-Transformer) based multi-scale low-complex intelligent completion approach is proposed, where the sparse window is applied to avoid ultra-large 3D attention computation, and the multi-scale design improves the completion performance. The knowledge-enhanced mean square error (KMSE) and root KMSE (RKMSE) are introduced as novel metrics for semantic spectrum map completion that jointly consider the numerical precision and physical consistency with the signal propagation model, based on which a joint offline and online training method is developed with supervised and unsupervised knowledge loss. The simulation demonstrates that our proposed scheme outperforms the state-of-the-art benchmark schemes in terms of RKMSE.

</details>


### [45] [Coding-Logic Correspondence: Turning Information and Communication Networks into Logical Formulae via Hypergraph Heyting Algebra](https://arxiv.org/abs/2512.21112)
*Cheuk Ting Li*

Main category: cs.IT

TL;DR: 提出使用混淆超图（超混淆）作为信息模型，替代传统随机变量方法，形成Heyting代数，将通信网络需求表达为逻辑公式，直接计算最优编码方案


<details>
  <summary>Details</summary>
Motivation: 传统基于随机变量的信息理论方法无法直接处理信息的合取、析取和蕴含运算，需要一种新的数学模型来统一表达通信网络的各种编码需求

Method: 使用混淆超图作为信息模型，建立Heyting代数结构，利用Heyting代数与直觉主义逻辑的对应关系，将网络编码、索引编码、Slepian-Wolf编码等通信需求表达为逻辑公式

Result: 通过超图Heyting代数可以直接计算最优编码方案，最优通信成本由超图的熵给出（在对数间隙内），建立了编码设置与逻辑公式之间的对应关系

Conclusion: 提出了一种新的信息理论框架，将通信网络需求统一为逻辑公式，建立了类似Curry-Howard对应的编码与逻辑对应关系，为通信理论提供了新的数学基础

Abstract: We propose using confusion hypergraphs (hyperconfusions) as a model of information. In contrast to the conventional approach using random variables, we can now perform conjunction, disjunction and implication of information, forming a Heyting algebra. Using the connection between Heyting algebra and intuitionistic logic, we can express the requirements of a communication network (e.g., network coding, index coding, Slepian-Wolf coding) as a logical formula, allowing us to use the hypergraph Heyting algebra to directly compute the optimal coding scheme. The optimal communication cost is simply given by the entropy of the hypergraph (within a logarithmic gap). This gives a surprising correspondence between coding settings and logical formulae, similar to the Curry-Howard correspondence between proofs and computer programs.

</details>
