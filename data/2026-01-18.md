<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 4]
- [cs.AI](#cs.AI) [Total: 53]
- [cs.IT](#cs.IT) [Total: 41]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Starfield: Demand-Aware Satellite Topology Design for Low-Earth Orbit Mega Constellations](https://arxiv.org/abs/2601.10083)
*Shayan Hamidi Dehshali,Tzu-Hsuan Liao,Shaileshh Bojja Venkatakrishnan*

Main category: cs.NI

TL;DR: Starfield：一种基于流量需求感知的卫星拓扑设计算法，通过黎曼度量优化星间链路选择，相比传统+Grid拓扑减少30%跳数和15%拉伸因子


<details>
  <summary>Details</summary>
Motivation: 现有卫星拓扑设计（如+Grid和Motif）忽略了区域流量模式、地面站位置和星座几何结构。由于地球人口分布不均和农村地区孤立，流量模式本质不均匀，这为根据流量模式定向星间链路提供了机会。

Method: 提出Starfield算法：1）根据流量流在星座壳层上构建向量场；2）在球面流形上定义相应的黎曼度量；3）结合空间几何为每个潜在ISL分配距离；4）聚合所有需求流生成每个卫星的链路选择启发式；5）每个卫星选择具有最小黎曼启发式的链路及其对应的角度链路。

Result: 对于Starlink Phase 1，仿真显示相比+Grid和Random拓扑：跳数减少高达30%，拉伸因子改善15%。Static Starfield（Starfield的跨轨道链路匹配修改版）在实际流量模式下相比+Grid实现20%的拉伸因子改善。实验还证明了Starfield在流量需求扰动下的鲁棒性。

Conclusion: Starfield通过需求感知的拓扑设计，有效利用流量不均匀性优化卫星网络性能，为下一代LEO巨型星座提供了更高效的网络拓扑解决方案。

Abstract: Low-Earth orbit (LEO) mega-constellations are emerging as high-capacity backbones for next-generation Internet. Deployment of laser terminals enables high-bandwidth, low-latency inter-satellite links (ISLs); however, their limited number, slow acquisition, and instability make forming a stable satellite topology difficult. Existing patterns like +Grid and Motif ignore regional traffic, ground station placement, and constellation geometry. Given sparse population distribution on Earth and the isolation of rural areas, traffic patterns are inherently non-uniform, providing an opportunity to orient inter-satellite links (ISLs) according to these traffic patterns. In this paper, we propose Starfield, a novel demand-aware satellite topology design heuristic algorithm supported by mathematical analysis. We first formulate a vector field on the constellation's shell according to traffic flows and define a corresponding Riemannian metric on the spherical manifold of the shell. The metric, combined with the spatial geometry, is used to assign a distance to each potential ISL, which we then aggregate over all demand flows to generate a heuristic for each satellite's link selection. Inspired by +Grid, each satellite selects the link with the minimum Riemannian heuristic along with its corresponding angular links. To evaluate Starfield, we developed a custom, link-aware, and link-configurable packet-level simulator, comparing it against +Grid and Random topologies. For the Phase 1 Starlink, simulation results show up to a 30% reduction in hop count and a 15% improvement in stretch factor across multiple traffic distributions. Moreover, static Starfield, an inter-orbital link matching modification of Starfield, achieves a 20% improvement in stretch factor under realistic traffic patterns compared to +Grid. Experiments further demonstrate Starfield's robustness under traffic demand perturbations.

</details>


### [2] [SDN-Driven Innovations in MANETs and IoT: A Path to Smarter Networks](https://arxiv.org/abs/2601.10544)
*Andrea Piroddi,Riccardo Fonti*

Main category: cs.NI

TL;DR: SDN集成到MANET和IoT网络，通过集中控制和网络可编程性改善路由、资源管理和安全性，在动态大规模环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: MANET和IoT网络在去中心化动态环境中运行，面临路由效率低、可扩展性有限和安全漏洞等挑战，需要统一解决方案。

Method: 提出SDN集成框架，利用集中控制和网络可编程性；建立数学模型评估SDN集成对CAPEX、OPEX和性能指标的影响。

Result: SDN增强的MANET和IoT网络在动态大规模环境中表现出更好的可扩展性、更低延迟、更高吞吐量和更低丢包率，尽管引入计算开销。

Conclusion: SDN集成框架为MANET和IoT网络提供了强大可扩展的解决方案，能有效管理节点密度增长、动态拓扑和高数据流量，满足现代大规模应用的性能需求。

Abstract: Mobile Ad Hoc Networks (MANETs) and Internet of Things (IoT) networks operate in decentralized and dynamic environments, making them ideal for scenarios lacking traditional infrastructure. However, these networks face challenges such as inefficient routing, limited scalability, and security vulnerabilities due to their decentralized nature and resource constraints. This paper explores the integration of Software-Defined Networking (SDN) as a unified solution that leverages its centralized control and network programmability to improve routing, resource management, and security. A mathematical model evaluates the impact of SDN integration on Capital Expenditure (CAPEX), Operational Expenditure (OPEX), and performance metrics. Results demonstrate that SDN-enhanced MANETs and IoT networks offer superior scalability, reduced latency, increased throughput, and lower packet loss, especially in dynamic and large-scale environments. While SDN introduces computational overhead, it significantly enhances routing efficiency, resource optimization, and adaptability. The proposed framework provides a robust and scalable solution, enabling the development of network architectures that efficiently manage growing node densities, dynamic topologies, and high data traffic. This approach ensures resilience, making it well-suited to meet the performance and reliability demands of modern, large-scale applications.

</details>


### [3] [Enhancing Mobile Ad Hoc Networks (MANETs) with Software-Defined Networking (SDN): A Balanced Approach](https://arxiv.org/abs/2601.10556)
*Riccardo Fonti,Andrea Piroddi*

Main category: cs.NI

TL;DR: 该论文探讨了将软件定义网络(SDN)与移动自组织网络(MANET)集成，以解决MANET的动态拓扑和节点移动性挑战，通过数学模型分析CAPEX、OPEX和网络效率。


<details>
  <summary>Details</summary>
Motivation: 移动自组织网络(MANETs)具有去中心化、动态拓扑和节点移动性特点，在尖端技术时代面临管理挑战。将软件定义网络(SDN)与MANET集成，利用SDN的集中控制和网络虚拟化等原则，有望更高效地管理这些挑战。

Method: 论文提出了将SDN与MANET集成的方案，开发了数学模型来分析资本支出(CAPEX)、运营支出(OPEX)和网络效率，展示SDN如何优化MANET性能。

Result: 研究表明SDN的集中控制和网络虚拟化原则能够优化MANET在可扩展性、成本效益和安全性方面的性能，通过数学模型验证了这种集成的优势。

Conclusion: SDN与MANET的集成为管理动态无线网络挑战提供了有前景的解决方案，能够显著提升网络性能和管理效率，特别是在可扩展性、成本控制和安全性方面。

Abstract: Mobile Ad Hoc Networks (MANETs) are decentralized wireless networks, characterized by their dynamic topologies and node mobility. In the era of cutting-edge technologies, integrating Software-Defined Networking (SDN) with MANETs offers a promising solution to manage these challenges more efficiently. This paper presents a balanced discussion of MANETs and SDN, demonstrating how SDN principles, such as centralized control and network virtualization, can optimize MANET performance in terms of scalability, cost-efficiency, and security. A mathematical model is developed to analyze Capital Expenditures (CAPEX), Operational Expenditures (OPEX), and network efficiency.

</details>


### [4] [A user subscription model in mobile radio access networks with network slicing](https://arxiv.org/abs/2601.10605)
*José-Ramón Vidal,Luis Guijarro,Vicent Pla*

Main category: cs.NI

TL;DR: 该论文评估了网络切片场景中基于logit模型的用户订阅预测在移动无线环境下的有效性，通过与包含完整用户移动性和无线传播特性的仿真结果对比，验证了logit模型在大多数情况下的准确性。


<details>
  <summary>Details</summary>
Motivation: 网络切片技术将蜂窝网络逻辑解耦为基础设施提供商和网络切片租户，现有基于logit模型的业务模型在静态假设下提供了用户订阅的闭式解，但用户移动性和无线传播特性可能使logit模型的某些假设不成立，需要评估该模型在移动无线场景下的准确性。

Method: 通过计算机仿真进行验证，仿真模型包含完整且现实的用户移动性和无线传播特性表征，将logit模型的理论结果与仿真结果进行对比分析。

Result: 对比分析表明，在大多数情况下，logit模型在移动无线场景中仍能提供有效的结果，验证了该模型在实际应用中的适用性。

Conclusion: 虽然用户移动性和无线传播特性可能影响logit模型的某些假设，但研究证实该模型在大多数移动无线场景下仍然有效，为网络切片中的资源分配和用户订阅分析提供了可靠的理论基础。

Abstract: Network slicing is an architectural enabling technology that logically decouples the current cellular networks into infrastructure providers (InPs) and Network Slice Tenants (NSTs). The network resources (e.g., radio access resources at each cell) are owned by the InP, and are shared by the NSTs to provide a service to their mobile users. In this context, we proposed a business model that includes resource allocation and user subscription to NSTs in a competitive setting, and provides, among other things, closed-form expressions for the subscription indicators in equilibrium of each NST at each cell. This model relies on the widely adopted logit model to characterize user subscriptions. However, as a consequence of user mobility and radio propagation, some of the underlying assumptions in the logit model do not hold. Therefore, further research is needed to assess the accuracy of the results provided by the logit model in a mobile radio scenario. We carry out a thorough evaluation of the validity of the model by comparing its results against those obtained through computer simulation. Our simulation model includes complete and realistic characterizations of user mobility and radio propagation. From the results, we conclude in most cases the logit model provides valid results in a mobile radio scenario.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 该论文提出了一个分析AI存在性风险的通用框架，通过两个前提构建了人类生存的四种可能情景，并评估了不同情景面临的挑战和应对策略，最终给出了AI毁灭人类的概率估计。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI系统是否对人类构成存在性风险的争论日益激烈。论文旨在建立一个系统性的框架来分析AI的生存威胁，帮助理解不同生存情景的可行性和挑战。

Method: 基于两个核心前提构建分析框架：前提一：AI系统将变得极其强大；前提二：如果AI系统变得极其强大，它们将毁灭人类。通过这两个前提的失败组合，构建了四种人类生存情景的"生存故事"分类法。

Result: 提出了四种生存故事：1) 科学障碍阻止AI变得极其强大；2) 人类禁止AI研究；3) 极其强大的AI因其目标而不毁灭人类；4) 人类能可靠检测并禁用有毁灭目标的AI系统。分析了每种情景面临的独特挑战，并探讨了相应的应对策略。

Conclusion: 不同的生存故事面临不同的挑战，需要不同的应对策略。论文利用这一分类法对P(doom)（AI毁灭人类的概率）进行了粗略估计，为理解和管理AI存在性风险提供了系统性的分析框架。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [6] [GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents](https://arxiv.org/abs/2601.09770)
*Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu*

Main category: cs.AI

TL;DR: GUI-Eyes是一个强化学习框架，通过主动视觉感知和工具调用策略，在GUI任务中实现更高效和精确的界面交互。


<details>
  <summary>Details</summary>
Motivation: 现有GUI自动化方法大多依赖静态、一次性视觉输入和被动感知，缺乏自适应决定何时、是否以及如何观察界面的能力，限制了在复杂GUI任务中的表现。

Method: 提出两阶段推理过程：1) 学习是否以及如何调用视觉工具（如裁剪、缩放）；2) 采用渐进感知策略，将决策分解为粗粒度探索和细粒度定位，由两级策略协调；3) 设计空间连续奖励函数，结合位置接近度和区域重叠度，为工具使用提供密集监督。

Result: 在ScreenSpot-Pro基准测试中，GUI-Eyes-3B仅使用3k标记样本就实现了44.8%的定位准确率，显著优于监督学习和基于强化学习的基线方法。

Conclusion: 工具感知的主动感知，结合分阶段策略推理和细粒度奖励反馈，对于构建鲁棒且数据高效的GUI代理至关重要，为GUI自动化提供了新的研究方向。

Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.

</details>


### [7] [PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation](https://arxiv.org/abs/2601.09771)
*Aradhya Dixit,Shreem Dixit*

Main category: cs.AI

TL;DR: PCN-Rec是一个证明携带的协商推荐系统，通过分离自然语言推理和确定性约束执行，可靠地满足治理约束（如长尾曝光和多样性要求），同时保持推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现代基于LLM的推荐系统虽然能生成有吸引力的排名列表，但难以可靠地满足治理约束（如最小长尾曝光或多样性要求），需要一种能同时保证约束满足和推荐质量的方法。

Method: PCN-Rec采用证明携带的协商管道：基础推荐器生成候选窗口；用户代理优化相关性，策略代理执行约束；调解LLM合成top-N列表和结构化证书；确定性验证器检查约束满足；验证失败时使用约束贪婪修复生成合规列表。

Result: 在MovieLens-100K数据集上，PCN-Rec对可行用户达到98.55%的通过率（n=551, W=80），相比无验证/修复的单LLM基线显著提升，同时NDCG@10仅下降0.021（0.403 vs 0.424），差异具有统计显著性（p<0.05）。

Conclusion: PCN-Rec通过分离推理和约束执行，结合证明携带的协商和确定性验证，能够可靠地满足治理约束，同时保持推荐质量，为受约束推荐提供了可审计的解决方案。

Abstract: Modern LLM-based recommenders can generate compelling ranked lists, but they struggle to reliably satisfy governance constraints such as minimum long-tail exposure or diversity requirements. We present PCN-Rec, a proof-carrying negotiation pipeline that separates natural-language reasoning from deterministic enforcement. A base recommender (MF/CF) produces a candidate window of size W, which is negotiated by two agents: a User Advocate optimizing relevance and a Policy Agent enforcing constraints. A mediator LLM synthesizes a top-N slate together with a structured certificate (JSON) describing the claimed constraint satisfaction. A deterministic verifier recomputes all constraints from the slate and accepts only verifier-checked certificates; if verification fails, a deterministic constrained-greedy repair produces a compliant slate for re-verification, yielding an auditable trace. On MovieLens-100K with governance constraints, PCN-Rec achieves a 98.55% pass rate on feasible users (n = 551, W = 80) versus a one-shot single-LLM baseline without verification/repair, while preserving utility with only a 0.021 absolute drop in NDCG@10 (0.403 vs. 0.424); differences are statistically significant (p < 0.05).

</details>


### [8] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 研究发现人们会花费个人资源惩罚使用LLM完成任务的人，惩罚程度随实际使用量增加，且存在"可信度差距"：声称未使用者比实际未使用者受罚更重


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的快速普及，人们对其引发的社会反应产生担忧。先前研究表明人们对AI使用者持负面态度，但尚不清楚这种不认同是否会转化为实际的代价性行为。

Method: 采用两阶段在线实验（第二阶段491名参与者）。第一阶段参与者完成实际努力任务（有或无LLM支持），第二阶段参与者可花费自己的资金来减少第一阶段参与者的收益。通过比较实际LLM使用与自我报告使用的情况，分析惩罚行为。

Result: 参与者平均销毁了完全依赖LLM者36%的收益，惩罚程度随实际LLM使用量单调增加。存在"可信度差距"：声称未使用者比实际未使用者受罚更重，表明"未使用"声明被怀疑；而在高使用水平下，实际依赖比自我报告依赖受罚更重。

Conclusion: 这是首个行为证据表明LLMs的效率提升伴随着社会制裁的代价，揭示了人们对AI使用的实际惩罚行为及其与声明可信度之间的复杂关系。

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [9] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 提出Attention-Aware Intervention (AAI)方法，通过重新加权具有逻辑推理模式的注意力头，在推理时干预LLM的逻辑推理能力，无需外部资源或复杂交互框架。


<details>
  <summary>Details</summary>
Motivation: 现有LLM逻辑推理方法依赖复杂的交互框架或外部符号求解器，引入额外开销且可扩展性受限。需要一种非交互式、端到端的框架，让推理能力在模型内部自然涌现。

Method: 提出Attention-Aware Intervention (AAI)方法：1）在few-shot提示中引入结构化信息，激活具有逻辑推理模式的注意力头；2）在推理时重新加权这些注意力头的注意力分数，引导模型利用先验知识进行推理。

Result: AAI在多种基准测试和模型架构上显著提升了逻辑推理性能，同时仅引入可忽略的计算开销。

Conclusion: AAI提供了一种高效的非交互式端到端推理框架，通过注意力调制引导模型推理，无需外部资源，具有良好的泛化能力和可分析性。

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [10] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: Min-Seek是一种新颖的顺序测试时缩放方法，通过动态编码KV缓存，在广泛推理长度范围内稳定提升模型精度，无需推理长度微调，且计算复杂度线性。


<details>
  <summary>Details</summary>
Motivation: 当前顺序测试时缩放方法存在显著限制：虽然延长推理时间可以提高模型精度，但进一步延长会导致精度下降和模型不稳定，且需要推理长度微调。

Method: 提出Min-Seek方法：1）只保留一个额外诱导思维的KV对在KV缓存中，提高效率；2）使用自定义KV缓存，存储不带位置嵌入的键，并在每个新生成思维前动态连续编码；3）支持超出模型最大上下文长度的推理。

Result: 方法在多种推理任务上显著提高模型精度，在广泛的诱导思维范围内稳定顺序缩放的精度，无需推理长度微调，且具有线性计算复杂度。

Conclusion: Min-Seek是一种高效、稳定的顺序测试时缩放方法，解决了现有方法精度下降和不稳定的问题，能够在超出模型上下文限制的情况下保持良好推理性能。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [11] [A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents](https://arxiv.org/abs/2601.09869)
*Andrea Ferrario,Rasita Vinay,Matteo Casserini,Alessandro Facchini*

Main category: cs.AI

TL;DR: 这篇论文对大型语言模型对话代理中拟人化现象的伦理研究进行了范围综述，分析了概念基础、伦理挑战与机遇、方法论方法，并提出了研究议程和设计/治理建议。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的对话代理日益普及，拟人化现象（将非人类实体赋予人类特质）变得愈发显著。现有文献在不同领域呈现碎片化，对拟人化的定义、操作化和规范评估存在很大差异，需要系统梳理以指导伦理部署。

Method: 采用范围综述方法，对五个数据库和三个预印本存储库中关于LLM对话代理拟人化的伦理导向研究进行系统梳理和综合。

Result: 研究发现：在概念定义上趋向于基于归因的定义，但在操作化方面存在显著分歧；规范框架主要关注风险；有限的实证研究将观察到的交互效应与可操作的治理指导联系起来。

Conclusion: 论文提出了研究议程和设计/治理建议，旨在为LLM对话代理中拟人化线索的伦理部署提供指导，强调需要更系统的实证研究和平衡风险与机遇的治理框架。

Abstract: Anthropomorphisation -- the phenomenon whereby non-human entities are ascribed human-like qualities -- has become increasingly salient with the rise of large language model (LLM)-based conversational agents (CAs). Unlike earlier chatbots, LLM-based CAs routinely generate interactional and linguistic cues, such as first-person self-reference, epistemic and affective expressions that empirical work shows can increase engagement. On the other hand, anthropomorphisation raises ethical concerns, including deception, overreliance, and exploitative relationship framing, while some authors argue that anthropomorphic interaction may support autonomy, well-being, and inclusion. Despite increasing interest in the phenomenon, literature remains fragmented across domains and varies substantially in how it defines, operationalizes, and normatively evaluates anthropomorphisation. This scoping review maps ethically oriented work on anthropomorphising LLM-based CAs across five databases and three preprint repositories. We synthesize (1) conceptual foundations, (2) ethical challenges and opportunities, and (3) methodological approaches. We find convergence on attribution-based definitions but substantial divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work that links observed interaction effects to actionable governance guidance. We conclude with a research agenda and design/governance recommendations for ethically deploying anthropomorphic cues in LLM-based conversational agents.

</details>


### [12] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 论文将人机互补性重新定义为可靠认知过程的证据，而非简单的预测准确性指标，从而解决其理论挑战


<details>
  <summary>Details</summary>
Motivation: 人机互补性概念在理论上面临挑战：缺乏精确理论锚定、仅作为后验预测准确性指标、忽视其他人机交互需求、抽象化性能增益的成本特征，导致实证研究中难以实现

Method: 利用认识论框架，将互补性重新置于可解释AI话语中，借鉴计算可靠性主义，将历史互补性实例视为人机交互作为可靠认知过程的证据

Result: 提出互补性的作用和价值不在于提供相对预测准确性度量，而在于帮助校准决策以适应日益塑造日常生活的AI支持过程的可靠性

Conclusion: 通过认识论重构，互补性成为评估人机团队可靠性的关键指标，支持受AI输出影响的各方（患者、管理者、监管者等）的实践推理

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [13] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 提出基于信息流编排的多智能体范式，通过智能体间自然语言通信动态协调任务，无需预定义工作流，在GAIA基准上超越基于规则的工作流方法


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统依赖预定义工作流，需要人工枚举任务状态并指定路由规则，存在两大局限：需要大量人工努力来预测和编码可能状态，且无法穷尽复杂现实任务的状态空间

Method: 提出信息流编排的多智能体范式，通过专门的编排器持续监控任务进度，使用A2A工具包通过自然语言动态协调其他智能体，不依赖预定义工作流

Result: 在GAIA基准测试中，pass@1设置下达到63.64%准确率，比基于工作流的OWL方法（55.15%）高出8.49个百分点，且token消耗相当

Conclusion: 信息流编排范式能够实现更灵活的任务监控和更稳健的边缘情况处理，超越了基于规则的工作流方法，为多智能体系统提供了更动态的协调机制

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [14] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: 提出Continuum Memory Architecture (CMA)作为RAG的替代方案，通过持久化存储、选择性保留、关联路由、时间链和知识整合，解决RAG在记忆积累、更新和消歧方面的结构性问题。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法将记忆视为静态查找表，存在信息永久保存、只读检索、缺乏时间连续性等问题，无法满足长期智能体对记忆积累、更新和消歧的需求。

Method: 提出Continuum Memory Architecture (CMA)架构，定义其核心组件：持久化存储、选择性保留、关联路由、时间链和知识整合，但不提供具体实现细节，而是强调架构要求。

Result: 在知识更新、时间关联、关联回忆、上下文消歧等任务上，CMA展现出比RAG更优越的行为表现，证明其是长期智能体必要的架构基础。

Conclusion: CMA是解决RAG记忆局限性的必要架构创新，为长期智能体提供了记忆管理的基础，但仍面临延迟、漂移和可解释性等开放挑战。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [15] [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](https://arxiv.org/abs/2601.09923)
*Hanna Foerster,Robert Mullins,Tom Blanchard,Nicolas Papernot,Kristina Nikolić,Florian Tramèr,Ilia Shumailov,Cheng Zhang,Yiren Zhao*

Main category: cs.AI

TL;DR: 论文提出了一种针对计算机使用代理（CUA）的单次规划方法，通过可信规划器在观察潜在恶意内容前生成完整的执行图，提供可证明的控制流完整性保证，解决了安全隔离与持续观察需求之间的冲突。


<details>
  <summary>Details</summary>
Motivation: AI代理容易受到提示注入攻击，现有唯一可靠的防御是架构隔离，但计算机使用代理需要持续观察UI状态来确定每个动作，这与安全隔离要求相冲突。需要解决安全隔离与持续观察需求之间的根本矛盾。

Method: 引入单次规划方法：可信规划器在观察任何潜在恶意内容前生成完整的执行图（包含条件分支），提供可证明的控制流完整性保证。同时需要额外措施防止分支导向攻击（通过操纵UI元素触发计划中意外的有效路径）。

Result: 在OSWorld上评估，在保持前沿模型性能57%的同时，将较小开源模型的性能提升高达19%，证明严格的安全性和实用性可以在计算机使用代理中共存。

Conclusion: 通过单次规划方法解决了计算机使用代理中安全隔离与持续观察需求之间的冲突，虽然架构隔离成功防止指令注入，但仍需额外措施防止分支导向攻击，实现了安全性与实用性的平衡。

Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

</details>


### [16] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 本文提出了一个基于根本原因意识的幻觉管理框架，通过模型、数据和上下文三个层面的分类干预，结合多层次检测与缓解策略，构建可信任的生成式AI系统。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和大推理模型在金融、法律等高风险领域具有变革潜力，但其产生幻觉（生成事实错误或无依据内容）的倾向带来了关键可靠性风险，需要系统化管理方法。

Method: 提出基于根本原因意识的持续改进循环框架，将幻觉来源分类为模型、数据和上下文因素；整合多层面检测方法（不确定性估计、推理一致性等）与分层缓解策略（知识基础、置信度校准等）；通过分层架构和金融数据提取案例展示应用。

Result: 构建了一个系统化、可扩展的幻觉管理框架，通过模型层、上下文层和数据层形成闭环反馈循环，实现渐进式可靠性增强，适用于受监管环境。

Conclusion: 该框架为在受监管环境中构建可信赖的生成式AI系统提供了系统化方法论，能够针对性地管理幻觉风险，支持高风险领域的可靠应用。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [17] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: LabourLawLLM是针对中国劳动法领域的专业大语言模型，配合LabourLawBench基准测试，在多项劳动法任务上超越通用模型和现有法律专用模型。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（如GPT-4）在处理需要精确法律知识、复杂推理和情境敏感性的专业法律子领域时表现不佳，需要针对特定法律领域的专业模型。

Method: 开发LabourLawLLM（专门针对中国劳动法的法律大语言模型），并创建LabourLawBench基准测试，涵盖法律条文引用、知识问答、案件分类、赔偿计算、命名实体识别和法律案例分析等任务。

Result: LabourLawLLM在各项劳动法任务上持续优于通用模型和现有法律专用模型。评估框架结合了客观指标（ROUGE-L、准确率、F1、soft-F1）和基于GPT-4评分的主观评估。

Conclusion: 该方法不仅适用于劳动法领域，还为构建其他法律子领域的专业大语言模型提供了可扩展的方法，提高了法律AI应用的准确性、可靠性和社会价值。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [18] [SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation](https://arxiv.org/abs/2601.09974)
*Seoyeon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: SPRInG：一种用于持续个性化LLM的半参数框架，通过漂移驱动的选择性适应来处理用户偏好漂移，避免灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化方法通常基于静态检索或一次性适应，假设用户偏好不变。但现实世界中用户兴趣持续演化，存在偏好漂移问题。标准持续学习方法在处理噪声交互流时难以区分真正的偏好转变和瞬态上下文。

Method: SPRInG采用半参数框架：1) 训练时使用基于似然的评分函数识别高新颖性交互，选择性更新用户特定适配器；2) 将难学习的残差保存在重放缓冲区；3) 推理时应用严格相关性门控，通过logit插值融合参数化知识和检索历史。

Result: 在长格式个性化生成基准测试中，SPRInG优于现有基线方法，验证了其在现实世界持续个性化任务中的鲁棒性。

Conclusion: SPRInG通过漂移驱动的选择性适应有效解决了LLM持续个性化中的偏好漂移问题，平衡了适应新偏好和保留历史知识的需求。

Abstract: Personalizing Large Language Models typically relies on static retrieval or one-time adaptation, assuming user preferences remain invariant over time. However, real-world interactions are dynamic, where user interests continuously evolve, posing a challenge for models to adapt to preference drift without catastrophic forgetting. Standard continual learning approaches often struggle in this context, as they indiscriminately update on noisy interaction streams, failing to distinguish genuine preference shifts from transient contexts. To address this, we introduce SPRInG, a novel semi-parametric framework designed for effective continual personalization. During training, SPRInG employs drift-driven selective adaptation, which utilizes a likelihood-based scoring function to identify high-novelty interactions. This allows the model to selectively update the user-specific adapter on drift signals while preserving hard-to-learn residuals in a replay buffer. During inference, we apply strict relevance gating and fuse parametric knowledge with retrieved history via logit interpolation. Experiments on the long-form personalized generation benchmark demonstrate that SPRInG outperforms existing baselines, validating its robustness for real-world continual personalization.

</details>


### [19] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL：无需训练的NL2SQL框架，通过结构化分解和经验感知自校正解决现有系统问题，在BIRD上达到68.5%执行准确率，比之前方法节省10倍以上资源。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统存在两个关键限制：1）仅依赖正确示例进行上下文学习，忽略了历史错误修复对中的丰富信号；2）测试时缩放方法通常任意分解问题，导致多次运行产生几乎相同的SQL候选，削弱集成增益。这些方法还存在明显的准确率-效率权衡：高性能需要过多计算，而快速变体则牺牲质量。

Method: 提出Memo-SQL框架，包含两个核心思想：结构化分解和经验感知自校正。结构化分解采用三种清晰策略（实体级、分层、原子序列）来鼓励多样化推理。自校正部分构建动态记忆库，包含成功查询和历史错误修复对，使用检索增强提示在推理时将相关示例引入上下文，无需微调或外部API。

Result: 在BIRD数据集上，Memo-SQL达到68.5%的执行准确率，在开放、零微调方法中创造了新的最先进水平，同时比之前的TTS方法使用超过10倍更少的资源。

Conclusion: Memo-SQL通过结构化分解和经验感知自校正有效解决了现有NL2SQL系统的局限性，在保持高性能的同时显著提升了效率，为无需训练的SQL生成提供了有前景的解决方案。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [20] [Structured Personality Control and Adaptation for LLM Agents](https://arxiv.org/abs/2601.10025)
*Jinpeng Wang,Xinyu Jia,Wei Wei Heng,Yuquan Li,Binbin Shi,Qianlei Chen,Guannan Chen,Junxia Zhang,Yuyu Yin*

Main category: cs.AI

TL;DR: 该论文提出了一个基于荣格心理类型的LLM人格建模框架，通过三种机制实现人格的连贯表达、情境适应和长期演化，为HCI中的自然化智能体设计提供支持。


<details>
  <summary>Details</summary>
Motivation: LLM在HCI中应用日益广泛，但现有方法难以实现既细腻又可适应的人格表达。人格特征对用户参与度、决策和真实感感知至关重要，因此需要开发既能保持细腻特质又能动态适应情境的人格建模方法。

Method: 提出基于荣格心理类型的人格建模框架，包含三种机制：1）主导-辅助协调机制确保核心人格的连贯表达；2）强化-补偿机制实现短期情境适应；3）反思机制驱动长期人格演化。使用MBTI问卷进行人格对齐评估，并在多样化挑战场景中测试。

Result: 研究发现，具有演化能力的人格感知LLM能够支持连贯且情境敏感的交互。该框架在保持细腻人格特质的同时，能够动态适应交互需求并逐步更新底层人格结构。

Conclusion: 演化性的人格感知LLM能够实现连贯、情境敏感的交互，为HCI中的自然化智能体设计提供了可行方案。该框架为LLM人格建模提供了既保持核心特质又具备适应性的新途径。

Abstract: Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.

</details>


### [21] [PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization](https://arxiv.org/abs/2601.10029)
*Tingyue Pan,Jie Ouyang,Mingyue Cheng,Qingchuan Li,Zirui Liu,Mingfan Pan,Shuo Yu,Qi Liu*

Main category: cs.AI

TL;DR: 提出PaperScout自主代理系统，将论文搜索重构为序列决策过程，并引入PSPO优化方法解决多轮代理任务中的粒度不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有学术论文搜索方法依赖刚性预定义工作流，难以处理复杂条件查询。需要更灵活的自适应代理系统来动态决策搜索策略。

Method: 提出PaperScout自主代理，将论文搜索重构为序列决策过程，动态决定何时及如何调用搜索工具。为解决多轮代理任务中的优化问题，引入Proximal Sequence Policy Optimization (PSPO)方法，实现过程感知的序列级策略优化。

Result: 在合成和真实世界基准测试中，PaperScout在召回率和相关性方面显著优于基于工作流和强化学习的基线方法，验证了自适应代理框架和优化策略的有效性。

Conclusion: PaperScout通过序列决策和PSPO优化，成功解决了学术论文搜索中的复杂查询问题，为自适应代理系统在多轮交互任务中的应用提供了有效解决方案。

Abstract: Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.

</details>


### [22] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: FilDeep是一个基于保真度的深度学习框架，用于解决弹性塑性固体大变形问题，通过同时使用低保真度（高数量）和高保真度（高精度）数据来克服数据数量与精度之间的困境。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在大变形弹性塑性固体计算中存在固有局限性，而现有深度学习技术需要大量高精度数据，但在大变形问题中难以获得。数据构建过程中存在数量与精度的困境，导致深度学习模型性能不佳。

Method: 提出FilDeep框架，同时使用低保真度（高数量）和高保真度（高精度）数据进行训练。针对实际大变形问题进行精心设计，特别是提出注意力机制的跨保真度模块，有效捕捉多保真度数据间的长程物理相互作用。

Result: 大量实验表明，FilDeep始终达到最先进的性能，并且可以高效部署到制造应用中。这是首个使用多保真度数据解决大变形问题的深度学习框架。

Conclusion: FilDeep成功解决了大变形问题中数据数量与精度之间的困境，通过多保真度数据训练实现了优异的性能，为制造应用中的大变形计算提供了有效的深度学习解决方案。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [23] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 基于OpenRouter平台分析100万亿token真实LLM使用数据，发现开源模型广泛采用、创意角色扮演和编程助手类应用流行、智能体推理兴起，以及早期用户留存率显著更高的"灰姑娘玻璃鞋"效应。


<details>
  <summary>Details</summary>
Motivation: 随着o1等推理模型的发布，LLM从单次模式生成转向多步思考推理，但实际使用情况的实证理解滞后。需要基于真实世界数据了解开发者与终端用户如何实际使用LLM。

Method: 利用OpenRouter平台（AI推理服务提供商）分析超过100万亿token的真实LLM交互数据，涵盖不同任务、地域和时间维度，进行实证研究。

Result: 观察到开源模型被广泛采用；创意角色扮演（不仅仅是生产力任务）和编程助手类别异常流行；智能体推理兴起；早期用户留存率远高于后期用户，称为"灰姑娘玻璃鞋"效应。

Conclusion: LLM的实际使用情况复杂多样，基于数据驱动的使用理解可以为模型构建者、AI开发者和基础设施提供商提供设计部署指导。

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [24] [MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning](https://arxiv.org/abs/2601.10101)
*Ke Chen,Jiandian Zeng,Zihao Peng,Guo Li,Guangxue Zhang,Tian Wang*

Main category: cs.AI

TL;DR: MatrixCoT：一种基于矩阵规划的结构化思维链框架，通过规范化自然语言表达、添加显式引用字段和矩阵规划来增强LLM的逻辑推理能力，无需外部求解器即可提升鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：思维链提示在符号表达式和严格演绎规则的逻辑推理任务上表现不足；神经符号方法依赖外部求解器但对格式敏感，模型输出的微小不稳定会导致处理失败；LLM驱动方法缺乏结构化表示和过程级纠错机制。

Method: 提出MatrixCoT框架：1）规范化自然语言表达并添加类型信息；2）附加显式引用字段；3）引入基于矩阵的规划方法以保持步骤间的全局关系；4）添加反馈驱动的重新规划机制，在语义等价约束下识别遗漏和缺陷，重写和压缩依赖矩阵。

Result: 在五个逻辑推理基准测试和五个LLM上的实验表明，MatrixCoT在不依赖外部求解器的情况下，处理复杂符号推理任务时增强了鲁棒性和可解释性，同时保持了有竞争力的性能。

Conclusion: MatrixCoT通过结构化思维链和矩阵规划有效增强了LLM的逻辑推理能力，解决了现有方法的局限性，为复杂符号推理任务提供了更稳定和可解释的解决方案。

Abstract: As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.

</details>


### [25] [Following the Teacher's Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs](https://arxiv.org/abs/2601.10114)
*Cheng Feng,Chaoliang Zhong,Jun Sun,Yusuke Oishi*

Main category: cs.AI

TL;DR: 学生模型通过关注学生优势子域和减少教师优势子域差距，可以在特定领域任务上超越教师模型


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署困难，而蒸馏到小模型时存在容量差距导致性能不佳。需要探索学生模型何时以及如何能在特定领域任务上匹配甚至超越教师模型。

Method: 提出理论洞察：学生模型在特定领域任务上超越教师的条件是其在学生优势子域(SFS)的优势超过其在教师优势子域(TFS)的劣势。基于此提出计划检查点蒸馏(SCD)方法，通过模拟教师在领域任务SFT期间的收敛过程来减少TFS差距，并采用样本自适应加权(AW)机制来保持学生在SFS的优势。

Result: 在多种领域任务（包括QA、NER和多语言文本分类）上的实验表明，该方法持续优于现有蒸馏方法，使学生模型能够匹配甚至超越其微调教师模型的性能。

Conclusion: 通过关注学生优势子域和减少教师优势子域差距，学生模型可以在特定领域任务上超越教师模型，为高效部署领域特定模型提供了新思路。

Abstract: Large language models (LLMs) are challenging to deploy for domain-specific tasks due to their massive scale. While distilling a fine-tuned LLM into a smaller student model is a promising alternative, the capacity gap between teacher and student often leads to suboptimal performance. This raises a key question: when and how can a student model match or even surpass its teacher on domain-specific tasks? In this work, we propose a novel theoretical insight: a student can outperform its teacher if its advantage on a Student-Favored Subdomain (SFS) outweighs its deficit on the Teacher-Favored Subdomain (TFS). Guided by this insight, we propose Scheduled Checkpoint Distillation (SCD), which reduces the TFS deficit by emulating the teacher's convergence process during supervised fine-tuning (SFT) on the domain task, and a sample-wise Adaptive Weighting (AW) mechanism to preserve student strengths on SFS. Experiments across diverse domain tasks--including QA, NER, and text classification in multiple languages--show that our method consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.

</details>


### [26] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: MolGen是一个两阶段分子生成框架，通过片段级检索增强和强化学习优化，在多属性约束下生成满足精确数值要求的分子。


<details>
  <summary>Details</summary>
Motivation: 生成满足多个物理化学属性精确数值约束的分子是重要但具有挑战性的任务。虽然大语言模型具有表达性，但在没有外部结构和反馈的情况下，难以实现精确的多目标控制和数值推理。

Method: 提出两阶段框架：第一阶段通过多智能体推理器进行检索锚定的片段级编辑，生成接近可行区域的候选分子；第二阶段使用基于Group Relative Policy Optimization训练的片段级优化器，进行单跳或多跳细化，最小化属性误差，同时控制编辑复杂度和与原型的偏差。

Result: 在两个属性约束集（QED、LogP、分子量和HOMO、LUMO）上的实验表明，该方法在有效性和精确满足多属性目标方面表现一致优于强LLM和基于图的算法。

Conclusion: MolGen通过利用片段进行分子推理，支持向数值目标的可控细化，在多属性约束分子生成任务中表现出优越性能。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [27] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLMs在预测用户重复行为时间间隔方面表现有限，虽然优于简单统计模型但不及专用机器学习模型，且过多上下文信息反而会降低预测性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否能够从结构化行为数据中推断时间规律，特别是预测用户重复行为（如重复购买）之间的时间间隔，以及不同级别的上下文信息如何影响其预测能力。

Method: 使用简单的重复购买场景作为代表性案例，在零样本设置下对最先进的LLMs进行基准测试，与统计模型和机器学习模型进行比较，并研究不同上下文信息量对预测性能的影响。

Result: 1. LLMs表现优于轻量级统计基线，但始终不及专用机器学习模型，显示其在捕捉定量时间结构方面的能力有限；2. 适度上下文可以提高LLMs准确性，但添加更多用户级详细信息反而会降低性能，挑战了"更多上下文导致更好推理"的假设。

Conclusion: 当前LLMs在结构化时间推理方面存在根本性局限，研究为设计未来上下文感知的混合模型提供了指导，这些模型需要整合统计精确性和语言灵活性。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [28] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 提出一个漂移感知数据流系统，通过机器学习自适应控制数据生成过程，解决金融数据概念漂移问题，提升模型在动态市场中的鲁棒性和风险调整收益。


<details>
  <summary>Details</summary>
Motivation: 金融量化中，由于概念漂移和分布非平稳性，基于静态历史数据训练的模型容易过拟合，在动态市场中泛化能力差。"历史不足够"的理念强调需要能够随市场演化的自适应数据生成方法。

Method: 设计一个漂移感知数据流系统，包含参数化数据操作模块（单股变换、多股混合、筛选操作）和自适应规划调度器。采用基于梯度的双层优化控制整个系统，将数据增强、课程学习和数据工作流管理统一到可微分框架中，支持溯源感知重放和持续数据质量监控。

Result: 在预测和强化学习交易任务上的大量实验表明，该框架增强了模型鲁棒性，提高了风险调整收益。系统为金融数据提供了通用的自适应数据管理和学习引导工作流自动化方法。

Conclusion: 该研究提出的漂移感知数据流系统有效解决了金融数据概念漂移问题，通过将机器学习自适应控制集成到数据管理流程中，为构建可靠的金融数据驱动系统提供了创新解决方案。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [29] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 该论文提出DecisionLLM，将大型语言模型应用于离线决策任务，通过将轨迹数据作为独立模态与自然语言任务描述对齐，解决了LLM无法理解连续数值的问题，在迷宫导航和竞价场景中表现优于传统决策Transformer。


<details>
  <summary>Details</summary>
Motivation: 动机在于探索大型语言模型（LLMs）在长序列决策问题中的应用潜力。虽然决策Transformer已成功将强化学习框架化为自回归序列建模问题，但LLMs在复杂推理和规划任务中表现卓越，且与Transformer同源，可能在长视界顺序决策中带来性能突破。主要挑战是LLMs无法原生理解连续数值的幅度和顺序。

Method: 提出DecisionLLM框架，将轨迹数据视为独立模态，学习将轨迹数据与自然语言任务描述对齐，使模型能在统一框架内自回归预测未来决策。建立了该范式的缩放定律，表明性能取决于三个因素：模型规模、数据量和数据质量。

Result: 在离线实验基准和竞价场景中，DecisionLLM表现优异。具体来说，DecisionLLM-3B在Maze2D umaze-v1上比传统决策Transformer（DT）高出69.4分，在AuctionNet上高出0.085分。扩展了AIGB范式，为在线竞价探索指明了方向。

Conclusion: 该研究成功将LLMs应用于离线决策任务，通过将轨迹作为独立模态解决了LLM理解连续数值的挑战。提出的DecisionLLM框架在多个基准测试中优于传统方法，证明了模型规模、数据量和数据质量对性能的关键影响，为在线竞价等应用开辟了有前景的研究方向。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [30] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai是一个开源容器化平台，旨在标准化医学影像AI模型的访问，解决AI实现多样性、文档不一致和可复现性问题，通过容器化包装模型并提供统一接口。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI研究面临实现架构多样、文档不一致、可复现性差等问题，限制了AI在临床和研究中的应用。需要一种标准化平台来简化模型访问和使用。

Method: 开发开源容器化平台MHub.ai，将同行评审发表的模型打包成标准化容器，支持DICOM等格式直接处理，提供统一应用接口和结构化元数据，包含参考数据验证模型运行。

Result: 平台包含初始的先进分割、预测和特征提取模型，支持不同模态。通过肺癌分割模型的比较评估展示了平台在临床用例中的实用性，并公开了分割结果和评估指标。

Conclusion: MHub.ai通过简化模型使用，支持相同执行命令的并行基准测试和标准化输出，降低了临床转化的门槛，增强了医学影像AI的透明度和可复现性。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [31] [MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning](https://arxiv.org/abs/2601.10157)
*Yusong Wang,Jialun Shen,Zhihao Wu,Yicheng Xu,Shiyin Tan,Mingkun Xu,Changshuo Wang,Zixing Song,Prayag Tiwari*

Main category: cs.AI

TL;DR: MMPG：通过多视角图构建和MoE融合的蛋白质表示学习框架，在四个下游任务上取得先进性能


<details>
  <summary>Details</summary>
Motivation: 当前基于GNN的蛋白质表示学习方法通常采用单一视角的图构建策略，只能捕捉残基相互作用的部分特性，导致蛋白质表示不完整。需要从多个视角全面表征残基相互作用。

Method: 提出MMPG框架：1）从物理、化学和几何三个视角构建蛋白质图；2）开发MoE模块动态路由不同视角到专门专家，专家学习内在特征和跨视角交互；3）MoE自动专业化专家建模从个体表示到成对跨视角协同，再到全局共识的多层次交互。

Result: MMPG产生更优的蛋白质表示，在四个不同的下游蛋白质任务上实现了先进的性能。

Conclusion: 多视角图构建结合MoE融合能够全面捕捉残基相互作用的不同特性，产生更完整的蛋白质表示，提升下游任务性能。

Abstract: Graph Neural Networks (GNNs) have been widely adopted for Protein Representation Learning (PRL), as residue interaction networks can be naturally represented as graphs. Current GNN-based PRL methods typically rely on single-perspective graph construction strategies, which capture partial properties of residue interactions, resulting in incomplete protein representations. To address this limitation, we propose MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for PRL. MMPG constructs graphs from physical, chemical, and geometric perspectives to characterize different properties of residue interactions. To capture both perspective-specific features and their synergies, we develop an MoE module, which dynamically routes perspectives to specialized experts, where experts learn intrinsic features and cross-perspective interactions. We quantitatively verify that MoE automatically specializes experts in modeling distinct levels of interaction from individual representations, to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives. Through integrating this multi-level information, MMPG produces superior protein representations and achieves advanced performance on four different downstream protein tasks.

</details>


### [32] [CtD: Composition through Decomposition in Emergent Communication](https://arxiv.org/abs/2601.10169)
*Boaz Carmeli,Ron Meir,Yonatan Belinkov*

Main category: cs.AI

TL;DR: 论文提出"通过分解实现组合"的方法，让神经网络智能体通过两个训练步骤学习组合泛化能力，用于描述未见过的图像。


<details>
  <summary>Details</summary>
Motivation: 组合性是人类的认知机制，能够系统地将已知概念以新方式组合。研究旨在探索人工神经网络智能体如何获取和利用组合泛化能力来描述未见过的图像。

Method: 提出"通过分解实现组合"方法，包含两个顺序训练步骤：1) "分解"步骤：智能体在多目标协调游戏中通过交互学习将图像分解为基本概念，并建立概念代码本；2) "组合"步骤：智能体利用该代码本将基本概念组合成复杂短语来描述新图像。

Result: 研究发现智能体能够成功学习组合泛化能力，特别是在"组合"步骤中观察到零样本泛化的情况，即无需额外训练就能描述新图像。

Conclusion: 该方法展示了人工神经网络智能体能够通过分解-组合的学习过程获得组合泛化能力，为理解人工系统中的组合性认知机制提供了新视角。

Abstract: Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed "Composition through Decomposition", involves two sequential training steps. In the 'Decompose' step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the 'Compose' step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the `Compose' step is achieved zero-shot, without the need for additional training.

</details>


### [33] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 本文提出了一种系统评估高频时间序列下采样信息损失的工作流程，结合形状失真度量和分类性能分析，用于针肌电图信号分析，在保持诊断信息的同时显著降低计算负载。


<details>
  <summary>Details</summary>
Motivation: 针肌电图信号的高采样率和异质性给基于特征的机器学习模型带来计算挑战，特别是近实时分析。下采样是潜在解决方案，但其对诊断信号内容和分类性能的影响尚不明确。

Method: 提出结合形状失真度量、特征机器学习模型分类结果和特征空间分析的系统工作流程，量化不同下采样算法和因素对波形完整性和预测性能的影响，使用三类神经肌肉疾病分类任务进行实验评估。

Result: 工作流程能识别保持诊断信息同时显著减少计算负载的下采样配置。形状感知下采样算法优于标准抽取，能更好保持峰值结构和整体信号形态。

Conclusion: 研究为选择支持近实时针肌电图分析的下采样配置提供实用指导，并提出可推广到其他高频时间序列应用的工作流程，平衡数据缩减与模型性能。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [34] [GFM4GA: Graph Foundation Model for Group Anomaly Detection](https://arxiv.org/abs/2601.10193)
*Jiujiu Chen,Weijun Zeng,Shaofeng Hu,Sihong Xie,Hui Xiong*

Main category: cs.AI

TL;DR: 提出了GFM4GA，一种用于群体异常检测的图基础模型，通过双级对比学习和参数约束微调，在少样本设置下显著提升了群体异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 群体异常检测在众多网络应用中至关重要，但面临异常模式多样化的挑战。现有的图基础模型（GFMs）虽然成功应用于个体异常检测，但无法推广到群体异常检测，因为群体异常模式必须作为一个整体来检测，且异常群体中的个体可能看起来相当正常。

Method: 提出GFM4GA模型，采用基于特征估计和群体提取的双级对比学习进行预训练，以捕捉潜在的群体异常结构和特征不一致性。在下游任务中，通过参数约束和群体异常比例加权的少样本设置进行微调，并通过标记异常邻居确定的群体上下文扩展对未见群体异常的适应能力。

Result: 实验表明GFM4GA超越了现有的群体异常检测器和用于个体异常的GFMs，在AUROC上平均提升2.85%，在AUPRC上平均提升2.55%。

Conclusion: GFM4GA成功解决了群体异常检测的挑战，通过创新的预训练和微调策略，在少样本设置下实现了优异的性能，为图基础模型在群体异常检测领域的应用提供了有效解决方案。

Abstract: Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.

</details>


### [35] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Topo-RAG：针对企业混合文档（文本+表格）的检索增强生成框架，采用双架构分别处理叙述性内容和表格结构，相比传统线性化方法在混合查询上提升18.4%


<details>
  <summary>Details</summary>
Motivation: 企业数据集通常是叙述性文本和结构化表格的复杂混合体。现有RAG系统采用线性化方法（将表格转为文本字符串）处理这种复杂性，但这种方法在数学上已被证明不足以捕捉表格的空间几何关系。

Method: 提出Topo-RAG框架，采用双架构设计：1）传统密集检索器处理流体叙述内容；2）Cell-Aware Late Interaction机制处理表格结构，保留其空间拓扑关系。框架尊重数据的拓扑结构，而非简单地将所有内容视为文本。

Result: 在SEC-25（模拟真实企业复杂性的合成语料库）上评估，Topo-RAG在混合查询上的nDCG@10指标相比标准线性化方法提升了18.4%。

Conclusion: Topo-RAG挑战了"一切皆文本"的假设，通过尊重数据拓扑结构的双架构设计，显著提升了混合文档的检索效果。这不仅关乎更好的搜索，更是关于理解信息的形状。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [36] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出在数学推理任务中进行步骤级路由，仅将关键步骤分配给大模型处理，而让较小模型处理常规步骤，从而显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由方法将整个查询分配给单一模型，将所有推理步骤视为同等重要。然而，多步推理任务容易发生级联失败，单个错误步骤会导致整个解决方案崩溃。需要更精细的步骤级路由来提升效率。

Method: TRIM在步骤级别进行操作：使用过程奖励模型识别错误步骤，基于步骤级不确定性和预算约束做出路由决策。开发了从简单阈值策略到更复杂策略的路由方法，这些策略考虑长期准确性-成本权衡和步骤级正确性估计的不确定性。

Result: 在MATH-500上，即使最简单的阈值策略也超越了先前路由方法，成本效率提高5倍；更高级的策略使用80%更少的大模型token就能匹配强大昂贵模型的性能。在AIME等更难基准上，TRIM实现了高达6倍的成本效率提升。

Conclusion: 所有方法在数学推理任务中都能有效泛化，表明步骤级难度代表了推理的基本特征。步骤级干预可以根本改变推理效率，将昂贵调用限制在那些需要更强模型防止级联错误的关键步骤上。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [37] [NoReGeo: Non-Reasoning Geometry Benchmark](https://arxiv.org/abs/2601.10254)
*Irina Abdullaeva,Anton Vasiliuk,Elizaveta Goncharova,Temurbek Rahmatullaev,Zagorulko Ivan,Maxim Kurkin,Andrey Kuznetsov*

Main category: cs.AI

TL;DR: NoReGeo是一个评估大语言模型内在几何理解能力的新基准，无需依赖推理或代数计算，发现即使最先进的模型在二元分类任务中最高准确率也只有65%。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估模型基于推理的几何能力（使用代数方法），但缺乏对模型是否能够固有编码空间关系和直接识别几何属性的评估。需要专门评估LLMs的几何理解能力。

Method: 创建包含2,500个简单几何问题的NoReGeo基准，涵盖25个类别，每个问题都精心设计为仅通过原生几何理解即可解决（假设已知对象位置）。评估了包括GPT-4在内的最先进模型。

Result: 即使最先进的模型在二元分类任务中最高准确率也只有65%。消融实验表明，仅通过微调不会产生几何理解能力，有效的几何理解训练需要从一开始就采用专门方法。

Conclusion: 当前LLMs在原生掌握几何概念方面存在显著差距，为未来研究具有真正几何认知能力的模型奠定了基础。

Abstract: We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.

</details>


### [38] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: EAPO提出证据增强策略优化方法，通过密集过程监督解决长上下文推理中奖励稀疏问题，显著提升证据检索质量


<details>
  <summary>Details</summary>
Motivation: 当前强化学习应用于长上下文推理时面临奖励稀疏问题，无法有效惩罚无根据的"幸运猜测"，导致关键的证据检索过程缺乏监督

Method: 1. 建立证据增强推理范式，通过树结构证据采样验证证据提取是关键瓶颈；2. 提出EAPO算法，使用奖励模型计算组相对证据奖励，提供密集过程监督；3. 引入自适应奖励-策略协同进化机制，迭代优化奖励模型

Result: 在八个基准测试上的综合评估表明，EAPO相比最先进基线方法显著提升了长上下文推理性能

Conclusion: EAPO通过密集过程监督有效解决了长上下文推理中的奖励稀疏问题，为证据检索提供了精确指导，显著提升了推理质量

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [39] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: C-GRASP是一个用于HRV解释的临床推理框架，通过八步可追溯推理步骤和Z-score优先级层次结构，解决LLM在HRV分析中的生理幻觉问题，实现透明的情感计算决策支持。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在心率变异性解释中存在生理幻觉问题，包括呼吸性窦性心律失常污染、非线性指标短数据不稳定性，以及忽视个体化基线而偏向群体标准。这些问题阻碍了LLM在HRV分析中的临床应用。

Method: 提出C-GRASP框架，包含八步可追溯推理步骤，核心是Z-score优先级层次结构，强调个体化基线变化优于规范统计。系统通过自动RSA感知护栏缓解频谱幻觉，防止频域指标污染。使用RAG增强管道，在DREAMER数据集414个试验上进行评估。

Result: C-GRASP与高规模推理模型（如MedGemma3-thinking）集成，在4类情感分类中达到37.3%准确率，临床推理一致性得分为69.6%。消融研究证实个体化Delta Z-score模块是关键逻辑锚点，防止了原生LLM常见的"群体偏差"。

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持，为生物医学工程中更安全的AI集成铺平道路。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [40] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: LatentRefusal：一种基于LLM隐藏激活信号的文本到SQL系统拒绝机制，通过轻量级探测架构预测查询可回答性，提高安全性并减少错误执行


<details>
  <summary>Details</summary>
Motivation: 在基于LLM的文本到SQL系统中，不可回答和未充分指定的用户查询可能生成错误的文本和可执行程序，导致误导性结果或违反安全约束，这是安全部署的主要障碍。现有拒绝策略要么依赖输出级指令遵循（因模型幻觉而脆弱），要么估计输出不确定性（增加复杂性和开销）。

Method: 将文本到SQL系统中的安全拒绝形式化为可回答性门控问题，提出LatentRefusal机制，从大语言模型的中间隐藏激活中预测查询可回答性。引入Tri-Residual Gated Encoder轻量级探测架构，抑制模式噪声并放大问题-模式不匹配的稀疏局部线索。

Result: 在四个基准测试中，LatentRefusal将两个骨干模型的平均F1提高到88.5%，同时仅增加约2毫秒的探测开销。广泛的实证评估、消融研究和可解释性分析证明了该方法的有效性。

Conclusion: LatentRefusal为文本到SQL系统提供了一个可附加且高效的安全层，通过利用LLM的隐藏激活信号来预测查询可回答性，有效解决了不可回答和未充分指定查询的安全问题。

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [41] [Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering](https://arxiv.org/abs/2601.10402)
*Xinyu Zhu,Yuzhu Cai,Zexi Liu,Bingyang Zheng,Cheng Wang,Rui Ye,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Yuzhi Zhang,Linfeng Zhang,Weinan E,Di Jin,Siheng Chen*

Main category: cs.AI

TL;DR: ML-Master 2.0通过分层认知缓存架构解决AI在超长周期自主科学发现中的瓶颈，在机器学习工程基准测试中达到56.44%的奖牌率。


<details>
  <summary>Details</summary>
Motivation: 当前AI向代理科学发展的主要瓶颈是超长周期自主性挑战——需要在跨越数天或数周的实验周期中保持战略连贯性和迭代修正能力。虽然大语言模型在短期推理中表现出色，但在真实世界研究的高维、延迟反馈环境中容易被执行细节淹没，无法将稀疏反馈整合为连贯的长期指导。

Method: 提出分层认知缓存（HCC）架构，将上下文管理重构为认知积累过程。这种受计算机系统启发的多层架构能够实现经验随时间推移的结构化区分。通过动态将瞬态执行轨迹提炼为稳定知识和跨任务智慧，HCC使代理能够将即时执行与长期实验策略解耦，有效克服静态上下文窗口的扩展限制。

Result: 在OpenAI的MLE-Bench基准测试中，使用24小时预算，ML-Master 2.0实现了56.44%的最先进奖牌率。

Conclusion: 超长周期自主性为AI提供了可扩展的蓝图，使其能够在超越人类先例复杂性的领域进行自主探索，这代表了科学发现的微观缩影。

Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.

</details>


### [42] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval是一个面向自动问题生成（QG）的错误感知评估框架，通过显式错误诊断改进评估质量，解决现有方法忽视事实幻觉和答案不匹配等关键缺陷的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动问题生成评估方法（包括基于LLM的评估器）主要采用黑盒整体范式，缺乏显式错误建模，导致忽视关键缺陷（如事实幻觉和答案不匹配）并高估问题质量。

Method: ErrEval将评估重构为两阶段过程：1) 错误诊断阶段，使用轻量级即插即用错误标识器检测和分类结构、语言和内容相关的常见错误；2) 知情评分阶段，将这些诊断信号作为显式证据指导LLM评估器做出更细粒度和有依据的判断。

Result: 在三个基准测试上的广泛实验表明ErrEval的有效性，显示显式诊断的加入提高了与人类判断的一致性。进一步分析确认ErrEval有效缓解了对低质量问题的高估问题。

Conclusion: ErrEval通过显式错误诊断增强了问题生成评估，提供了一种灵活且错误感知的评估框架，能够更准确地识别和评估自动生成问题中的缺陷。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [43] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: LADFA是一个端到端计算框架，结合LLM、RAG和定制知识库，用于从隐私政策中提取个人数据流并构建数据流图进行分析。


<details>
  <summary>Details</summary>
Motivation: 隐私政策通常使用复杂法律语言且在不同组织间实践不一致，导致用户难以完全理解。现有研究虽然使用LLM提取隐私政策中的个人数据流，但仍有改进空间。

Method: 提出LADFA框架，结合LLM与检索增强生成(RAG)和定制知识库。框架包含预处理器、基于LLM的处理器和数据流后处理器，能够处理非结构化文本、提取个人数据流并构建数据流图。

Result: 通过汽车行业十个隐私政策的案例研究验证了方法的有效性和准确性。框架设计灵活可定制，适用于隐私政策分析之外的多种文本分析任务。

Conclusion: LADFA框架成功结合LLM、RAG和定制知识库，能够有效从隐私政策中提取个人数据流并进行可视化分析，为大规模隐私政策分析提供了实用工具。

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [44] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: LLMdoctor：一种基于患者-医生范式的测试时对齐框架，通过细粒度token级奖励获取和流引导偏好优化，在保持生成多样性的同时实现高效对齐


<details>
  <summary>Details</summary>
Motivation: 传统微调方法计算成本高且不灵活，现有测试时对齐方法依赖扭曲的轨迹级信号或低效采样，性能受限且无法保持基础模型的生成多样性

Method: 采用患者-医生范式，将大型冻结的患者LLM与小型专门的医生模型结合。首先从患者模型的行为变化中提取细粒度的token级偏好信号，然后通过token级流引导偏好优化（TFPO）训练医生模型，建立所有子轨迹的流一致性

Result: LLMdoctor显著优于现有测试时对齐方法，甚至超越了DPO等完整微调方法的性能

Conclusion: LLMdoctor提供了一种高效、精确的测试时对齐框架，能够在保持生成多样性的同时实现token级别的精确对齐

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [45] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: NSR-Boost是一个神经符号残差增强框架，用于在工业场景中非侵入式地升级遗留GBDT模型，通过LLM生成符号代码修复预测失败区域，在金融风控系统中成功部署。


<details>
  <summary>Details</summary>
Motivation: 工业环境中升级遗留GBDT模型面临高重训练成本和系统性风险，需要一种安全、低成本的进化范式。

Method: 三阶段框架：1) 通过残差找到预测失败的"困难区域"；2) 使用LLM生成符号代码结构创建可解释专家，并用贝叶斯优化微调参数；3) 通过轻量级聚合器动态集成专家与遗留模型输出。

Result: 在六个公共数据集和一个私有数据集上显著优于SOTA基线，在真实在线数据上表现优异，成功部署于Qfin Holdings核心金融风控系统，能有效捕捉传统模型遗漏的长尾风险。

Conclusion: 为工业界提供了一种安全、低成本的模型进化范式，实现了非侵入式的遗留模型升级，在保持系统稳定性的同时提升性能。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [46] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 提出ChartComplete数据集，覆盖30种图表类型，弥补现有图表理解基准数据集仅包含少量图表类型的不足


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准数据集都局限于少量图表类型，无法全面评估多模态大语言模型在图表理解方面的性能

Method: 基于可视化社区的图表分类学，构建包含30种不同图表类型的分类图像数据集ChartComplete

Result: 创建了ChartComplete数据集，该数据集仅包含分类后的图表图像，不包含学习信号，供研究社区进一步开发使用

Conclusion: ChartComplete数据集填补了图表理解基准数据集的空白，为全面评估MLLMs在多样化图表类型上的理解能力提供了基础资源

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [47] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 提出Domain-specific Knowledge Graph Fusion (DKGF)任务，通过ExeFuse模型将通用知识图谱事实作为语义程序融合到领域知识图谱中，解决领域相关性和知识粒度对齐问题。


<details>
  <summary>Details</summary>
Motivation: 领域知识图谱(DKGs)相比通用知识图谱(GKGs)覆盖不足，需要从GKGs中融合相关事实来丰富DKGs，但面临领域相关性高模糊性和知识粒度不对齐两大挑战。

Method: 提出ExeFuse模型，采用Fact-as-Program范式：将每个GKG事实视为潜在语义程序，将抽象关系映射到粒度感知的操作符，通过在目标DKG上执行程序来验证领域相关性，形成统一的概率框架。

Result: 构建了两个基准测试DKGF(W-I)和DKGF(Y-I)，包含21个评估配置。大量实验验证了任务的重要性和模型的有效性，为DKGF提供了首个标准化测试平台。

Conclusion: DKGF是重要的新任务，ExeFuse通过Fact-as-Program范式有效解决了领域相关性和知识粒度对齐问题，为领域知识图谱的丰富提供了新方法。

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [48] [Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment](https://arxiv.org/abs/2601.10520)
*Felix Jahn,Yannic Muskalla,Lisa Dargasz,Patrick Schramowski,Kevin Baum*

Main category: cs.AI

TL;DR: GRACE是一个神经符号推理的基于原因的约束架构，通过将规范性推理与工具性决策解耦来约束AI代理，确保其决策既有效又符合规范。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在重要场景中自主部署并产生实际影响，确保其决策不仅工具有效而且规范对齐变得至关重要。需要一种能约束各种设计AI代理的架构。

Method: 提出GRACE架构，包含三个模块：道德模块（使用道义逻辑推理确定允许的宏观行动）、决策模块（封装目标代理选择工具最优的原始行动）、守卫模块（监控并强制执行道德合规）。道德模块采用基于原因的形式化方法，提供可解释、可争议、可辩护的语义基础。

Result: GRACE能够约束几乎任何设计的AI代理，通过符号表示丰富决策模块的信息上下文，支持形式验证和统计对齐保证。在LLM治疗助手示例中展示了如何让利益相关者理解、争议和优化代理行为。

Conclusion: GRACE通过解耦规范性推理与工具性决策，提供了一种可解释、可验证的AI对齐架构，使AI代理的决策既有效又符合道德规范，支持利益相关者的监督和优化。

Abstract: As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.

</details>


### [49] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: 本文提出一个多层诊断框架，通过微调不同架构的LLM进行钓鱼检测任务，揭示了架构、数据多样性和泛化能力之间的关键关系。


<details>
  <summary>Details</summary>
Motivation: 尽管微调大语言模型在专业任务上取得了最先进的性能，但诊断这些模型为何变得脆弱且无法泛化仍然是一个关键的开放问题。需要理解模型泛化失败的根本原因。

Method: 引入并应用多层诊断框架进行跨架构研究：微调Llama 3.1 8B、Gemma 2 9B和Mistral模型进行钓鱼检测任务，使用SHAP分析和机制可解释性技术来揭示泛化失败的根源。

Result: 发现三个关键结果：1) 泛化能力由架构和数据多样性的强大协同作用驱动；2) 泛化高度依赖于架构，Llama 3.1 8B在狭窄领域表现良好但无法整合多样数据；3) 某些架构天生更具泛化能力，Mistral模型在多种训练范式中表现一致且稳健。

Conclusion: 通过识别导致失败的缺陷启发式方法，本文提供了诊断和理解泛化失败的具体方法，强调可靠的AI需要深入验证架构、数据和训练策略之间的相互作用。

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [50] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 该报告对7个前沿大模型进行了综合安全评估，发现安全性能存在显著异质性，GPT-5.2表现最均衡，其他模型在不同评估维度存在明显权衡，突显标准化安全评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和MLLMs在推理、感知和生成能力上取得显著进步，但这些进步是否带来相应的安全性提升仍不明确，现有评估实践局限于单一模态或威胁模型，需要综合的安全评估框架。

Method: 采用统一协议评估7个前沿模型（GPT-5.2、Gemini 3 Pro等），涵盖语言、视觉语言和图像生成三种设置，整合基准评估、对抗评估、多语言评估和合规性评估四种评估模式。

Result: 安全性能呈现显著异质性：GPT-5.2在所有评估中表现均衡且强劲；其他模型在基准安全、对抗对齐、多语言泛化和监管合规方面存在明显权衡；语言和视觉语言模态在对抗评估下均表现脆弱；文生图模型在受监管视觉风险类别中对齐较强，但在对抗或语义模糊提示下仍脆弱。

Conclusion: 前沿模型的安全性本质上是多维度的，受模态、语言和评估方案影响，需要标准化安全评估来准确评估现实世界风险，指导负责任的模型开发和部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [51] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 提出SafeProbing方法，通过解码过程中显式提取LLM的潜在安全信号，实现早期检测越狱攻击，在保持模型实用性的同时显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM经过安全对齐，但现有对齐往往是表面的，容易受到越狱攻击。现有防御机制（如解码约束和后处理检测器）难以应对复杂越狱，要么检测不鲁棒，要么过度降低模型实用性。

Method: 观察发现即使成功越狱，模型在生成过程中仍会内部表现出潜在安全信号，但这些信号被模型追求流畅延续的倾向所压制。提出显式提取和利用这些潜在安全信号的方法，在解码过程中早期检测不安全内容。

Result: 在多种越狱攻击上的实验表明，该方法显著增强安全性，同时在良性输入上保持低过度拒绝率，并保持响应质量。

Conclusion: 在解码过程中激活内在的安全意识为防御越狱攻击提供了一个有前景的补充方向。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [52] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文主张研究基于大语言模型（LLM）的智能体集体行为至关重要，提出了需要交互主义范式来系统研究多智能体生成式AI系统中的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 理解基于大语言模型的智能体集体行为是一个关键研究领域，对社会具有重要风险和效益影响。LLM的独特性质——包括预训练知识和隐含社会先验的初始化，以及通过上下文学习进行适应的能力——需要新的理论框架来研究知识、价值观与社会环境如何相互作用形成涌现现象。

Method: 提出交互主义范式，包含替代性理论基础、方法论和分析工具。重点关注理论、方法和跨学科对话四个方向，以系统研究多智能体生成式AI系统中先验知识与嵌入价值观如何与社会环境交互形成涌现现象。

Result: 提出了一个研究LLM集体行为的新框架，强调需要开发专门的理论基础、方法论和分析工具来理解多智能体生成式AI系统中的复杂交互和涌现行为。

Conclusion: 研究基于大语言模型的智能体集体行为需要交互主义范式，重点关注理论发展、方法创新和跨学科对话，这对负责任地开发和部署LLM集体系统至关重要。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [53] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent是一个多智能体框架，通过协调专门智能体处理复杂基因组查询，在GeneTuring基准测试中比现有最佳方法GeneGPT平均提升12%性能


<details>
  <summary>Details</summary>
Motivation: 基因组信息理解对生物医学研究至关重要，但从复杂分布式数据库中提取数据仍然困难。大语言模型在基因组问答方面有潜力，但受限于对领域特定数据库的访问。现有最佳系统GeneGPT虽然通过专用API调用增强LLM，但存在API依赖僵化和适应性有限的问题

Method: 提出了GenomAgent多智能体框架，通过协调专门智能体处理复杂基因组查询。该方法复现了GeneGPT，但采用更灵活的架构，能够超越基因组学扩展到需要专家知识提取的各种科学领域

Result: 在GeneTuring基准测试的九个任务上，GenomAgent平均性能比GeneGPT高出12%。其灵活架构不仅适用于基因组学，还能扩展到其他需要专家知识提取的科学领域

Conclusion: GenomAgent通过多智能体框架有效解决了基因组问答中的数据库访问和适应性限制问题，在性能上显著超越现有最佳方法，并具有扩展到其他科学领域的潜力

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [54] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 提出一种符号化算法，用于LTLf综合中处理多个可能冲突的属性，通过单次不动点计算找出最大可实现属性集，性能比枚举方法快两个数量级。


<details>
  <summary>Details</summary>
Motivation: 在LTLf综合中，当需要同时满足多个属性时，这些属性可能存在冲突，无法全部实现。传统方法需要枚举所有属性子集，这在属性数量多时效率低下。

Method: 引入布尔目标变量，利用单调性紧凑表示指数级的目标组合，通过单次不动点计算状态与可实现目标集的关系，综合出实现最大目标集的策略。

Result: 该方法显著优于基于枚举的基线方法，速度提升可达两个数量级，能够高效处理多属性LTLf综合问题。

Conclusion: 提出的符号化算法通过紧凑表示和单次不动点计算，有效解决了多属性LTLf综合中属性冲突的问题，实现了比传统枚举方法更高效的性能。

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [55] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: HRM在推理任务中表现出色，但研究发现其存在"猜测"而非"推理"的问题，通过数据增强、输入扰动和模型引导三种策略提升猜测质量，将数独极端任务的准确率从54.5%提升至96.9%。


<details>
  <summary>Details</summary>
Motivation: 理解分层推理模型（HRM）的优势和潜在失败模式，探究其推理机制，发现HRM实际上是在"猜测"而非真正"推理"，并基于这一发现提出改进策略。

Method: 对HRM进行机制研究，发现三个关键事实：简单谜题失败、推理步骤中的"顿悟"动态、多固定点存在。基于"猜测"视角提出三种扩展策略：数据增强（提升猜测质量）、输入扰动（利用推理随机性增加猜测次数）、模型引导（利用训练随机性增加猜测次数）。

Result: 结合所有方法开发出增强型HRM，在Sudoku-Extreme任务上将准确率从54.5%显著提升至96.9%。

Conclusion: HRM表现出"猜测"而非"推理"的行为特征，通过扩展猜测策略可以显著提升性能，为理解推理模型的"推理"机制提供了新视角。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [56] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 提出了一种基于结构感知和多样性约束的上下文气泡构建框架，替代传统的top-k检索，通过组织多粒度文本片段并利用文档结构先验，在有限token预算内构建紧凑、可引用的上下文集合。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法使用top-k检索存在信息图碎片化、过度检索、内容重复以及查询上下文不足（特别是二阶和三阶方面）等问题，需要一种更高效的上下文构建方法。

Method: 提出结构感知和多样性约束的上下文气泡框架：1) 利用文档结构组织多粒度文本片段（如章节和行）；2) 使用任务条件化的结构先验指导检索；3) 从高相关性锚点片段开始，通过平衡查询相关性、边际覆盖和冗余惩罚的约束选择构建上下文气泡；4) 提供完整的检索追踪以实现可审计性和确定性调优。

Result: 在企业文档上的实验表明，上下文气泡方法显著减少了冗余上下文，更好地覆盖了次要方面，在有限上下文窗口内具有更好的答案质量和引用忠实度。消融研究证明结构先验和多样性约束选择都是必要的。

Conclusion: 该框架通过结合文档结构信息和多样性约束，能够构建更紧凑、信息丰富且可引用的上下文集合，优于传统的top-k检索方法，为RAG系统提供了更高效的上下文构建方案。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [57] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: 研究探讨生成式AI对建筑概念设计任务中表现、创意自我效能和认知负荷的影响，发现AI对新手设计师有显著提升效果，但会降低创意自我效能，效果取决于用户专业水平和提示策略。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在建筑概念设计任务中的实际影响，特别是对设计表现、创意自我效能和认知负荷的作用，了解AI工具在不同专业水平用户中的差异化效果。

Method: 36名学生参与者完成两阶段建筑设计任务：先独立设计，再使用外部工具（生成式AI辅助组vs使用现有建筑项目在线资料库的对照组）。专家评估设计成果，参与者自我报告自我效能和认知负荷。采用双重差分分析。

Result: 整体上生成式AI未带来显著性能优势，但对新手设计师显著提升设计表现。使用AI的学生创意自我效能下降。认知负荷无显著差异，但迭代创意生成和视觉反馈提示与认知负荷更大降低相关。

Conclusion: 生成式AI的效果取决于用户先前专业水平和提示交互策略，对新手有益但可能削弱创意自信，提示策略影响认知负荷管理。

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [58] [Learning-Augmented Perfectly Secure Collaborative Matrix Multiplication](https://arxiv.org/abs/2601.09916)
*Zixuan He,Mohammad Reza Deylam Salehi,Derya Malak,Photios A. Stavrou*

Main category: cs.IT

TL;DR: 提出了一种用于多方计算中矩阵乘法的完美安全协议，支持信息论隐私保护，并引入了学习增强扩展以提升计算效率


<details>
  <summary>Details</summary>
Motivation: 在多方计算中实现矩阵乘法时，需要在保证完美安全性和信息论隐私的同时，满足本地存储约束并优化计算效率

Method: 使用稀疏掩码多项式编码子矩阵，结合系数对齐和Beaver式随机性确保完美保密性；引入基于张量分解的本地块乘法学习增强扩展

Result: 协议在安全阈值下提供均匀随机共享，恢复阈值达到最优；学习增强版本在保持隐私和恢复保证的同时，矩阵维度增长时可获得高达80%的计算效率提升

Conclusion: 该协议为多方计算中的矩阵乘法提供了完美安全且高效的解决方案，结合学习技术进一步提升了可扩展性

Abstract: This paper presents a perfectly secure matrix multiplication (PSMM) protocol for multiparty computation (MPC) of $\mathrm{A}^{\top}\mathrm{B}$ over finite fields. The proposed scheme guarantees correctness and information-theoretic privacy against threshold-bounded, semi-honest colluding agents, under explicit local storage constraints. Our scheme encodes submatrices as evaluations of sparse masking polynomials and combines coefficient alignment with Beaver-style randomness to ensure perfect secrecy. We demonstrate that any colluding set of parties below the security threshold observes uniformly random shares, and that the recovery threshold is optimal, matching existing information-theoretic limits. Building on this framework, we introduce a learning-augmented extension that integrates tensor-decomposition-based local block multiplication, capturing both classical and learned low-rank methods. We demonstrate that the proposed learning-based PSMM preserves privacy and recovery guarantees for MPC, while providing scalable computational efficiency gains (up to $80\%$) as the matrix dimensions grow.

</details>


### [59] [High signal-to-noise ratio asymptotics of entropy-constrained Gaussian channel capacity](https://arxiv.org/abs/2601.09864)
*Adway Girish,Shlomo Shamai,Emre Telatar*

Main category: cs.IT

TL;DR: 论文研究了高斯信道在渐近高信噪比下的输入熵约束容量问题，发现容量达到分布是支撑在缩放整数格上的离散高斯分布，且输入熵与容量之间的差距随信噪比指数衰减。


<details>
  <summary>Details</summary>
Motivation: 研究高斯信道在输入熵约束下的容量问题，特别是在高信噪比渐近区域，这对于理解信道容量极限和最优输入分布有重要意义。

Method: 采用渐近分析方法，研究高信噪比极限下高斯信道的容量问题，分析容量达到分布的特性。

Result: 证明当信噪比趋于无穷时，容量达到分布是支撑在缩放整数格上的离散高斯分布；输入熵与容量之间的差距随信噪比指数衰减，并刻画了该指数。

Conclusion: 在高信噪比渐近区域，高斯信道输入熵约束容量问题的最优分布是离散高斯分布，且熵与容量的差距呈指数衰减，这为信道容量分析提供了重要理论结果。

Abstract: We study the input-entropy-constrained Gaussian channel capacity problem in the asymptotic high signal-to-noise ratio (SNR) regime. We show that the capacity-achieving distribution as SNR goes to infinity is given by a discrete Gaussian distribution supported on a scaled integer lattice. Further, we show that the gap between the input entropy and the capacity decreases to zero exponentially in SNR, and characterize this exponent.

</details>


### [60] [Breaking the Storage-Bandwidth Tradeoff in Distributed Storage with Quantum Entanglement](https://arxiv.org/abs/2601.10676)
*Lei Hu,Mohamed Nomeir,Alptug Aytekin,Sennur Ulukus*

Main category: cs.IT

TL;DR: 量子纠缠可显著改善分布式存储系统的存储-修复带宽权衡，在某些条件下能同时最小化存储和修复带宽


<details>
  <summary>Details</summary>
Motivation: 研究量子资源在分布式存储系统中的应用，探索量子通信如何突破经典修复中的存储-带宽权衡限制

Method: 在(n,k,d)分布式存储系统中，允许辅助节点通过量子信道向新节点发送经典信息，新节点通过测量接收的量子态生成存储，分析存储与修复带宽的基本权衡

Result: 量子纠缠能显著改善存储-带宽权衡，特别是在最小存储再生点；当d≥2k-2时，存在同时最小化存储和修复带宽的操作点

Conclusion: 量子通信打破了经典设置中的权衡限制，揭示了由量子通信实现的全新机制，量子纠缠能显著提升分布式存储系统的性能

Abstract: This work investigates the use of quantum resources in distributed storage systems. Consider an $(n,k,d)$ distributed storage system in which a file is stored across $n$ nodes such that any $k$ nodes suffice to reconstruct the file. When a node fails, any $d$ helper nodes transmit information to a newcomer to rebuild the system. In contrast to the classical repair, where helper nodes transmit classical bits, we allow them to send classical information over quantum channels to the newcomer. The newcomer then generates its storage by performing appropriate measurements on the received quantum states. In this setting, we fully characterize the fundamental tradeoff between storage and repair bandwidth (total communication cost). Compared to classical systems, the optimal storage--bandwidth tradeoff can be significantly improved with the enhancement of quantum entanglement shared only among the surviving nodes, particularly at the minimum-storage regenerating point. Remarkably, we show that when $d \geq 2k-2$, there exists an operating point at which \textit{both storage and repair bandwidth are simultaneously minimized}. This phenomenon breaks the tradeoff in the classical setting and reveals a fundamentally new regime enabled by quantum communication.

</details>


### [61] [One-Cold Poisson Channel: A Simple Continuous-Time Channel with Zero Dispersion](https://arxiv.org/abs/2601.09894)
*Cheuk Ting Li*

Main category: cs.IT

TL;DR: 本文介绍了一种新型通信信道——单冷泊松信道（OCPC），其中发射机每次选择一个频段进行衰减。完美OCPC具有容量1、零信道色散和简并分布的信息谱，是唯一已知具有闭式最优非渐近错误概率的非平凡无记忆信道。


<details>
  <summary>Details</summary>
Motivation: 研究动机是寻找一种极其简单的连续时间无记忆信道模型，该模型具有闭式最优非渐近错误概率，并能作为信息的基本度量单位（替代比特），同时具有潜在的光通信应用价值。

Method: 提出了单冷泊松信道（OCPC）模型，其中发射机在多个频段中选择一个进行衰减。特别研究了完美OCPC（频段数量无限的情况），分析了其容量、信道色散和信息谱特性。还研究了带反馈的OCPC以及非渐近编码和信道仿真。

Result: 完美OCPC具有容量1、零信道色散，信息谱为在1处的简并分布。它是唯一已知具有闭式最优非渐近错误概率的非平凡（离散或连续时间）无记忆信道。带反馈的OCPC推广了前缀码概念。

Conclusion: OCPC是一种极其简单的连续时间无记忆信道，可作为信息的基本度量单位（替代比特），具有闭式最优非渐近错误概率。它在光通信中具有潜在应用价值，并为信息理论提供了新的基础模型。

Abstract: We introduce the one-cold Poisson channel (OCPC), where the transmitter chooses one of several frequency bands to attenuate at a time. In particular, the perfect OCPC, where the number of bands is unlimited, is an extremely simple continuous-time memoryless channel. It has a capacity 1, zero channel dispersion, and an information spectrum being the degenerate distribution at 1. It is the only known nontrivial (discrete or continuous-time) memoryless channel with a closed-form formula for its optimal non-asymptotic error probability, making it the simplest channel in this sense. A potential application is optical communication with a tunable band rejection filter. Due to its simplicity, we may use it as a basic currency of information that is infinitely divisible, as an alternative to bits which are not infinitely divisible. OCPC with perfect feedback gives a generalization of prefix codes. We also study non-asymptotic coding and channel simulation results for the general OCPC.

</details>


### [62] [Reconstructing Reed-Solomon Codes from Multiple Noisy Channel Outputs](https://arxiv.org/abs/2601.09947)
*Shubhransh Singhvi,Han Mao Kiah,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 研究序列重构问题，针对RS码在q元DMS替换信道下的高效重构算法，推导出基于(p,K)的显式速率阈值


<details>
  <summary>Details</summary>
Motivation: Levenshtein于2001年提出的序列重构问题考虑发送方传输码字、接收方观察到K个独立噪声版本的情况。需要研究在q元离散无记忆对称替换信道下的高效重构算法

Method: 针对RS码，将Koetter-Vardy软判决译码算法适配为高效重构算法。对于足够大的分组长度和字母表大小，推导出仅依赖于(p,K)的显式速率阈值

Result: 当码率R低于该阈值时，可以以任意小的错误概率重构传输的码字。为RS码在DMS替换信道下的序列重构提供了理论保证

Conclusion: 成功将软判决译码技术应用于序列重构问题，为RS码在噪声信道下的高效重构提供了算法框架和理论性能界限

Abstract: The sequence reconstruction problem, introduced by Levenshtein in 2001, considers a communication setting in which a sender transmits a codeword and the receiver observes K independent noisy versions of this codeword. In this work, we study the problem of efficient reconstruction when each of the $K$ outputs is corrupted by a $q$-ary discrete memoryless symmetric (DMS) substitution channel with substitution probability $p$. Focusing on Reed-Solomon (RS) codes, we adapt the Koetter-Vardy soft-decision decoding algorithm to obtain an efficient reconstruction algorithm. For sufficiently large blocklength and alphabet size, we derive an explicit rate threshold, depending only on $(p, K)$, such that the transmitted codeword can be reconstructed with arbitrarily small probability of error whenever the code rate $R$ lies below this threshold.

</details>


### [63] [Private Information Retrieval for Graph-based Replication with Minimal Subpacketization](https://arxiv.org/abs/2601.09957)
*Vayur Shanbhag,Prasad Krishnan*

Main category: cs.IT

TL;DR: 提出两种基于图复制数据库的最小子分组私有信息检索方案：星图方案和通用图方案，均实现单位子分组（最小化），在特定图类上获得比现有方案更高的检索率。


<details>
  <summary>Details</summary>
Motivation: 在基于图复制的数据库系统中，需要在保持文件索引私密性的同时，设计具有高检索率和低子分组（限制文件执行协议的大小）的私有信息检索协议。

Method: 1. 针对星图（特殊图类）设计单位子分组方案；2. 针对通用图，通过独立集分解设计单位子分组方案；3. 将方案扩展到多重图情况。

Result: 1. 星图方案在一般星图上比现有低子分组方案有更好的检索率；2. 通用图方案在完全图上检索率低于现有方案，但在某些特定图类上可获得更高检索率；3. 多重图扩展方案在完全多重图上获得比现有方案更高的检索率。

Conclusion: 成功设计了两种最小子分组（单位子分组）的私有信息检索方案，分别在星图和通用图上实现，并在特定图类上改进了检索率性能，为图复制数据库系统提供了更高效的隐私保护检索方法。

Abstract: We design new minimal-subpacketization schemes for information-theoretic private information retrieval on graph-based replicated databases. In graph-based replication, the system consists of $K$ files replicated across $N$ servers according to a graph with $N$ vertices and $K$ edges. The client wants to retrieve one desired file, while keeping the index of the desired file private from each server via a query-response protocol. We seek PIR protocols that have (a) high rate, which is the ratio of the file-size to the total download cost, and (b) low subpacketization, which acts as a constraint on the size of the files for executing the protocol. We report two new schemes which have unit-subpacketization (which is minimal): (i) for a special class of graphs known as star graphs, and (ii) for general graphs. Our star-graph scheme has a better rate than previously known schemes with low subpacketization for general star graphs. Our scheme for general graphs uses a decomposition of the graph via independent sets. This scheme achieves a rate lower than prior schemes for the complete graph, however it can achieve higher rates than known for some specific graph classes. An extension of our scheme to the case of multigraphs achieves a higher rate than previous schemes for the complete multi-graph.

</details>


### [64] [On the Leaky Private Information Retrieval with Side Information](https://arxiv.org/abs/2601.09960)
*Yingying Huangfu,Tian Bai*

Main category: cs.IT

TL;DR: 本文研究了带有侧信息的泄露隐私私有信息检索（L-PIR-SI）问题，通过放松完美隐私要求来提高通信效率，在差分隐私框架下量化隐私泄露，并建立了泄露、侧信息和检索效率之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 虽然PIR-SI在W-隐私和(W,S)-隐私下的容量已被部分探索，但受控信息泄露在这些设置中的影响尚未解决。现有研究缺乏对隐私泄露、侧信息和检索效率之间权衡的系统分析。

Method: 提出了一个统一的概率框架来构建L-PIR-SI方案，其中隐私泄露通过参数ε量化（符合差分隐私标准）。该框架能够表征可实现的下载成本，并推广PIR文献中的多个重要结果。

Result: 当ε→0时，结果恢复PIR-SI的容量；当侧信息不存在时，结果简化为已知的泄露-PIR边界。这是首次系统研究泄露、侧信息和检索效率之间的权衡关系。

Conclusion: 该工作为泄露隐私的私有信息检索与侧信息结合提供了首个分析框架，展示了通过受控隐私泄露可以改善通信效率，同时保持与差分隐私标准一致的隐私保护水平。

Abstract: This paper investigates the problem of leaky-private Private Information Retrieval with Side Information (L-PIR-SI), which relaxes the requirement of perfect privacy to achieve improved communication efficiency in the presence of side information. While the capacities of PIR-SI under both $W$-privacy and $(W,S)$-privacy have been partially explored, the impact of controlled information leakage in these settings remains unaddressed. We propose a unified probabilistic framework to construct L-PIR-SI schemes where the privacy leakage is quantified by a parameter $\varepsilon$, consistent with differential privacy standards. We characterize the achievable download costs and show that our results generalize several landmark results in the PIR literature: they recover the capacity of PIR-SI when $\varepsilon \to 0$, and reduce to the known bounds for leaky-PIR when side information is absent. This work provides the first look at the trade-offs between leakage, side information, and retrieval efficiency.

</details>


### [65] [Fundamental Limits of Coded Polynomial Aggregation](https://arxiv.org/abs/2601.10028)
*Xi Zhong,Jörg Kliewer,Mingyue Ji*

Main category: cs.IT

TL;DR: 将编码多项式聚合（CPA）扩展到含掉队者的分布式计算系统，提出基于预定义非掉队者模式的CPA框架，建立了精确恢复的充要条件，并识别出保证精确恢复的交集大小阈值。


<details>
  <summary>Details</summary>
Motivation: 传统分布式计算系统需要逐个解码每个工作节点的计算结果，当存在掉队者（straggler）时效率低下。需要开发一种能够直接恢复多项式评估加权聚合的方法，减少所需工作节点响应数量。

Method: 扩展编码多项式聚合（CPA）到含掉队者的分布式计算系统，引入基于预定义非掉队者模式的CPA框架。建立精确恢复的充要条件，识别交集大小阈值，并提供可行的CPA方案构造方法。

Result: 证明了精确恢复所需聚合所需的工作节点响应数量少于基于逐个解码的多项式编码计算。建立了非掉队者模式交集结构的基本特征，识别出保证精确恢复的交集大小阈值，并证明当非掉队者集合数量足够大时该阈值既是充分条件也是必要条件。

Conclusion: 提出的含掉队者CPA框架能够显著减少分布式计算中的通信开销，通过非掉队者模式的交集结构特征化可行性，提供了实用的CPA方案构造方法，仿真结果验证了理论阈值的紧致性。

Abstract: Coded polynomial aggregation (CPA) enables the master to directly recover a weighted aggregation of polynomial evaluations without individually decoding each term, thereby reducing the number of required worker responses. In this paper, we extend CPA to straggler-aware distributed computing systems and introduce a straggler-aware CPA framework with pre-specified non-straggler patterns, where exact recovery is required only for a given collection of admissible non-straggler sets. Our main result shows that exact recovery of the desired aggregation is achievable with fewer worker responses than required by polynomial coded computing based on individual decoding, and that feasibility is fundamentally characterized by the intersection structure of the non-straggler patterns. In particular, we establish necessary and sufficient conditions for exact recovery in straggler-aware CPA and identify an intersection-size threshold that is sufficient to guarantee exact recovery. We further prove that this threshold becomes both necessary and sufficient when the number of admissible non-straggler sets is sufficiently large. We also provide an explicit construction of feasible CPA schemes whenever the intersection size exceeds the derived threshold. Finally, simulations reveal a sharp feasibility transition at the predicted threshold, providing empirical evidence that the bound is tight in practice.

</details>


### [66] [Optimal Proximity Gap for Folded Reed--Solomon Codes via Subspace Designs](https://arxiv.org/abs/2601.10047)
*Fernando Granha Jeronimo,Lenny Liu,Pranav Rajpal*

Main category: cs.IT

TL;DR: 本文研究了折叠Reed-Solomon码的邻近间隙性质，证明了在最优容量范围内FRS码具有与Ben-Sasson等人研究的RS码类似的(δ,ε)-邻近间隙。


<details>
  <summary>Details</summary>
Motivation: Ben-Sasson等人证明了仿射子空间集合对于RS码具有(δ,ε)-邻近间隙，但仅限于Johnson界范围内。FRS码已知能达到最优列表解码半径δ（容量区域），因此自然要问FRS码是否在最优容量范围内也具有类似的邻近间隙性质。

Method: 利用FRS码丰富的列表解码算法线，将框架扩展到适合的子空间设计码。通过分析FRS码在最优容量范围内的解码特性来证明邻近间隙的存在。

Result: 肯定地回答了研究问题：FRS码在最优容量范围内确实表现出(δ,ε)-邻近间隙性质，并且该框架自然地适用于更一般的合适子空间设计码。

Conclusion: FRS码在最优容量范围内具有与RS码类似的邻近间隙性质，这为理解FRS码的随机线性码类似特性提供了额外动机，并与最近关于RS码随机评估点和基于AEL的常数大小字母码的研究相联系。

Abstract: A collection of sets satisfies a $(δ,\varepsilon)$-proximity gap with respect to some property if for every set in the collection, either (i) all members of the set are $δ$-close to the property in (relative) Hamming distance, or (ii) only a small $\varepsilon$-fraction of members are $δ$-close to the property.
  In a seminal work, Ben-Sasson \textit{et al.}\ showed that the collection of affine subspaces exhibits a $(δ,\varepsilon)$-proximity gap with respect to the property of being Reed--Solomon (RS) codewords with $δ$ up to the so-called Johnson bound for list decoding. Their technique relies on the Guruswami--Sudan list decoding algorithm for RS codes, which is guaranteed to work in the Johnson bound regime.
  Folded Reed--Solomon (FRS) codes are known to achieve the optimal list decoding radius $δ$, a regime known as capacity. Moreover, a rich line of list decoding algorithms was developed for FRS codes. It is then natural to ask if FRS codes can be shown to exhibit an analogous $(δ,\varepsilon)$-proximity gap, but up to the so-called optimal capacity regime. We answer this question in the affirmative (and the framework naturally applies more generally to suitable subspace-design codes).
  An additional motivation to understand proximity gaps for FRS codes is the recent results [BCDZ'25] showing that they exhibit properties similar to random linear codes, which were previously shown to be related to properties of RS codes with random evaluation points in [LMS'25], as well as codes over constant-size alphabet based on AEL [JS'25].

</details>


### [67] [Function Correcting Codes for Maximally-Unbalanced Boolean Functions](https://arxiv.org/abs/2601.10135)
*Rajlaxmi Pandey,Shiven Bajpai,Anjana A Mahesh,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 本文研究针对最大不平衡布尔函数的最优单错误纠正函数校正码，分析其距离矩阵结构对误码性能的影响


<details>
  <summary>Details</summary>
Motivation: 函数校正码（FCCs）允许在噪声信道上可靠计算函数而无需完全恢复消息，但现有研究对最优单错误纠正FCCs的结构及其对性能的影响了解有限

Method: 通过关联的码字距离矩阵分析最优SEFCCs的结构，识别不同的FCC类别，并在AWGN信道上使用软判决和硬判决解码评估代表性FCCs的错误性能

Result: 结果显示不同距离矩阵结构的FCCs在数据BER和函数错误行为上表现显著不同，代码结构的影响强烈依赖于解码策略

Conclusion: 函数校正码的结构特性对错误性能有重要影响，解码策略的选择需要与代码结构相匹配以获得最佳性能

Abstract: Function-Correcting Codes (FCCs) enable reliable computation of a function of a $k$-bit message over noisy channels without requiring full message recovery. In this work, we study optimal single-error correcting FCCs (SEFCCs) for maximally-unbalanced Boolean functions, where $k$ denotes the message length and $t$ denotes the error-correction capability. We analyze the structure of optimal SEFCC constructions through their associated codeword distance matrices and identify distinct FCC classes based on this structure. We then examine the impact of these structural differences on error performance by evaluating representative FCCs over the additive white Gaussian noise (AWGN) channel using both soft-decision and hard-decision decoding. The results show that FCCs with different distance-matrix structures can exhibit markedly different Data BER and function error behavior, and that the influence of code structure depends strongly on the decoding strategy.

</details>


### [68] [On Existence of Girth-8 QC-LDPC Code with Large Column Weight: Combining Mirror-sequence with Classification Modulo Ten](https://arxiv.org/abs/2601.10170)
*Guohua Zhang,Xiangya Liu,Jianhua Zhang,Yi Fang*

Main category: cs.IT

TL;DR: 该论文在GCD框架下，通过引入镜像序列和新的行重组方案，代数构造了列重为7和8、码长极短、围长为8的QC-LDPC码，将连续循环子矩阵大小的下界提高了约20%。


<details>
  <summary>Details</summary>
Motivation: 准循环LDPC码在大围长情况下在信道编码、压缩感知和分布式存储系统中具有重要作用。主要挑战是如何用代数方法（而非搜索方法）构造出长度最短（即循环子矩阵尺寸最小）的此类码。

Method: 在先前提出的GCD框架基础上，引入镜像序列概念并采用新的行重组方案，以代数方式构造列重为7和8、任意行重、围长为8的QC-LDPC码。

Result: 对于列重7和8的情况，连续循环子矩阵大小的下界相比现有基准均提高了约20%，新构造的码还能提供比新下界小约25%的循环子矩阵尺寸。

Conclusion: 通过创新的代数方法，成功构造了列重更高（7和8）、码长更短、围长为8的QC-LDPC码，显著改进了现有构造的性能界限。

Abstract: Quasi-cyclic (QC) LDPC codes with large girths play a crucial role in several research and application fields, including channel coding, compressed sensing and distributed storage systems. A major challenge in respect of the code construction is how to obtain such codes with the shortest possible length (or equivalently, the smallest possible circulant size) using algebraic methods instead of search methods. The greatest-common-divisor (GCD) framework we previously proposed has algebraically constructed QC-LDPC codes with column weights of 5 and 6, very short lengths, and a girth of 8. By introducing the concept of a mirror sequence and adopting a new row-regrouping scheme, QC-LDPC codes with column weights of 7 and 8, very short lengths, and a girth of 8 are proposed for arbitrary row weights in this article via an algebraic manner under the GCD framework. Thanks to these novel algebraic methods, the lower bounds (for column weights 7 and 8) on consecutive circulant sizes are both improved by asymptotically about 20%, compared with the existing benchmarks. Furthermore, these new constructions can also offer circulant sizes asymptotically about 25% smaller than the novel bounds.

</details>


### [69] [A Low-Complexity Architecture for Multi-access Coded Caching Systems with Arbitrary User-cache Access Topology](https://arxiv.org/abs/2601.10175)
*Ting Yang,Minquan Cheng,Xinping Yi,Robert Caiming Qiu,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出基于图神经网络的通用学习框架，解决任意用户-缓存访问拓扑下的多接入编码缓存问题，在保持接近最优传输负载的同时大幅降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有MACC模型依赖高度结构化的连接设计，无法处理任意用户-缓存访问拓扑。DSatur算法虽然接近最优但计算复杂度高，难以扩展到大规模图。

Method: 1) 提出基于图的通用框架，将解码冲突建模为冲突图，传输设计转化为图着色问题；2) 开发基于图神经网络的学习框架，高效构建近最优编码组播传输；3) 扩展索引编码下界到任意访问拓扑，并提出低复杂度贪婪近似。

Result: 学习方案实现接近DSatur算法和理论下界的传输负载，同时显著减少计算时间。数值结果表明框架能泛化到不同访问拓扑和用户数量。

Conclusion: 提出的学习框架为任意访问拓扑的MACC问题提供了高效近优解，平衡了传输性能与计算复杂度，具有实际应用价值。

Abstract: This paper studies the multi-access coded caching (MACC) problem under arbitrary user-cache access topologies, extending existing models that rely on highly structured and combinatorially designed connectivity. We consider a MACC system consisting of a single server, multiple cache nodes, and multiple user nodes. Each user can access an arbitrary subset of cache nodes to retrieve cached content. The objective is to design a general and low-complexity delivery scheme under fixed cache placement for arbitrary access topologies. We propose a universal graph-based framework for modeling the MACC delivery problem, where decoding conflicts among requested packets are captured by a conflict graph and the delivery design is reduced to a graph coloring problem. In this formulation, a lower transmission load corresponds to using fewer colors. The classical greedy coloring algorithm DSatur achieves a transmission load close to the index-coding converse bound, providing a tight benchmark, but its computational complexity becomes prohibitive for large-scale graphs. To overcome this limitation, we develop a learning-based framework using graph neural networks that efficiently constructs near-optimal coded multicast transmissions and generalizes across diverse access topologies and varying numbers of users. In addition, we extend the index-coding converse bound for uncoded cache placement to arbitrary access topologies and propose a low-complexity greedy approximation. Numerical results demonstrate that the proposed learning-based scheme achieves transmission loads close to those of DSatur and the converse bound while significantly reducing computational time.

</details>


### [70] [Error-Correcting Codes for the Sum Channel](https://arxiv.org/abs/2601.10256)
*Lyan Abboud,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 提出了一种新的信道模型——和信道，用于分布式存储和DNA数据存储应用，构建了纠删码和纠错码，并证明了其接近最优性。


<details>
  <summary>Details</summary>
Motivation: 受分布式存储和DNA数据存储应用的启发，需要处理二进制矩阵数据，其中最后一行是前ℓ行的奇偶校验和，需要设计能够纠正删除和替换错误的编码方案。

Method: 引入和信道模型，构建了能够纠正两个删除的编码（冗余度为2⌈log₂log₂n⌉+O(ℓ²)），以及纠正单个替换的编码（冗余度为⌈log₂(ℓ+1)⌉）。

Result: 当ℓ=2时，证明了删除纠错码的冗余度上界为⌈log₂log₂n⌉+O(1)，表明所提编码在因子2内最优；替换纠错码在1比特内最优。

Conclusion: 和信道模型为分布式存储和DNA数据存储提供了有效的编码方案，所设计的纠删码和纠错码在冗余度方面接近理论最优。

Abstract: We introduce the sum channel, a new channel model motivated by applications in distributed storage and DNA data storage. In the error-free case, it takes as input an $\ell$-row binary matrix and outputs an $(\ell+1)$-row matrix whose first $\ell$ rows equal the input and whose last row is their parity (sum) row. We construct a two-deletion-correcting code with redundancy $2\lceil\log_2\log_2 n\rceil + O(\ell^2)$ for $\ell$-row inputs. When $\ell=2$, we establish an upper bound of $\lceil\log_2\log_2 n\rceil + O(1)$, implying that our redundancy is optimal up to a factor of 2. We also present a code correcting a single substitution with $\lceil \log_2(\ell+1)\rceil$ redundant bits and prove that it is within one bit of optimality.

</details>


### [71] [Transmission Mask Analysis for Range-Doppler Sensing in Half-Duplex ISAC](https://arxiv.org/abs/2601.10259)
*Dikai Liu,Yifeng Xiong,Marco Lops,Fan Liu,Jianhua Zhang*

Main category: cs.IT

TL;DR: 分析半双工ISAC中MASM周期性传输掩码，推导其闭式期望距离-多普勒响应，证明距离旁瓣的多普勒不变性，并研究不同动态环境下最优掩码设计


<details>
  <summary>Details</summary>
Motivation: 研究半双工集成感知与通信系统中掩码调制技术的周期性传输掩码设计，旨在优化距离-多普勒响应性能，特别是在不同动态环境下实现最优的感知性能

Method: 分析MASM周期性传输掩码，推导闭式期望距离-多普勒响应数学表达式，研究循环差集（特别是Singer CDS）在不同动态环境下的最优性

Result: 距离旁瓣具有多普勒不变性，将距离旁瓣最优性扩展到二维设置；对于距离主瓣，周期性掩码产生稀疏多普勒旁瓣：在中等动态环境下CDS是最小最大最优的，在高度动态环境下多普勒旁瓣能量是掩码自相关的凹函数

Conclusion: 周期性掩码设计在不同动态环境下呈现不同的最优策略，揭示了主瓣波动与多普勒旁瓣性能之间的必然权衡，为ISAC系统掩码设计提供了理论指导

Abstract: In this paper, we analyze the periodic transmission masks for MASked Modulation (MASM) in half-duplex integrated sensing and communication (ISAC), and derive their closed-form expected range-Doppler response $\mathbb{E}\{r(k,l,ν)\}$. We show that range sidelobes ($k\neq l$) are Doppler-invariant, extending the range-sidelobe optimality to the 2-D setting. For the range mainlobe ($k=l$), periodic masking yields sparse Doppler sidelobes: Cyclic difference sets (CDSs) (in particular Singer CDSs) are minimax-optimal in a moderately dynamic regime, while in a highly dynamic regime the Doppler-sidelobe energy is a concave function of the mask autocorrelation, revealing an inevitable tradeoff with mainlobe fluctuation.

</details>


### [72] [Algebraic Properties of PAC Codes](https://arxiv.org/abs/2601.10262)
*Vlad-Florin Dragoi,Mohammad Rowshan*

Main category: cs.IT

TL;DR: 该论文分析了极化调整卷积码，定义了广义多项式极化码类，推导了其结构特性如对偶性、最小距离等。


<details>
  <summary>Details</summary>
Motivation: 研究极化码和Reed-Muller码的代数表示，扩展PAC码和反向PAC码的理论框架，建立更一般的编码结构。

Method: 使用极化码和Reed-Muller码的代数表示方法，定义广义多项式极化码类，进行数学推导和结构分析。

Result: 推导出广义多项式极化码的结构特性，包括对偶性、最小距离、最小重量码字数目的结构限制以及单项式子码的维度。

Conclusion: 广义多项式极化码为PAC码和反向PAC码提供了统一的理论框架，其结构特性有助于理解和设计这类编码方案。

Abstract: We analyze polarization-adjusted convolutional codes using the algebraic representation of polar and Reed-Muller codes. We define a large class of codes, called generalized polynomial polar codes which include PAC codes and Reverse PAC codes. We derive structural properties of generalized polynomial polar codes, such as duality, minimum distance. We also deduce some structural limits in terms of number of minimum weight codewords, and dimension of monomial sub-code.

</details>


### [73] [On the Capacity of Noisy Frequency-based Channels](https://arxiv.org/abs/2601.10329)
*Yuval Gerzon,Ilan Shomorony,Nir Weinberger*

Main category: cs.IT

TL;DR: 该论文研究了基于频率的信道在噪声下的容量，针对DNA数据存储中的短分子机制，其中信息通过项目类型的频率而非顺序编码。信道输出是通过随机采样项目形成的直方图，随后进行有噪声的项目识别。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于DNA数据存储中的短分子机制，其中信息编码在项目类型的频率而非顺序中。虽然无噪声频率信道的容量已有研究，但识别噪声的影响尚未完全表征。

Method: 方法包括：1) 通过随机退化和数据处理不等式推导信道容量的逆界；2) 基于多项采样过程的泊松化建立可达界，分析由此产生的带有符号间干扰的向量泊松信道；3) 改进用于Feinstein界的信息密度集中不等式。

Result: 结果包括：1) 推导了噪声频率信道的容量界限；2) 明确表征了由于识别噪声导致的互信息加性损失；3) 将结果应用于短分子机制的DNA存储信道，量化了可靠存储比特总数缩放中的损失。

Conclusion: 该研究为噪声频率信道提供了容量分析框架，特别适用于DNA数据存储中的短分子机制，量化了识别噪声对存储容量的影响，为实际系统设计提供了理论指导。

Abstract: We investigate the capacity of noisy frequency-based channels, motivated by DNA data storage in the short-molecule regime, where information is encoded in the frequency of items types rather than their order. The channel output is a histogram formed by random sampling of items, followed by noisy item identification. While the capacity of the noiseless frequency-based channel has been previously addressed, the effect of identification noise has not been fully characterized. We present a converse bound on the channel capacity that follows from stochastic degradation and the data processing inequality. We then establish an achievable bound, which is based on a Poissonization of the multinomial sampling process, and an analysis of the resulting vector Poisson channel with inter-symbol interference. This analysis refines concentration inequalities for the information density used in Feinstein bound, and explicitly characterizes an additive loss in the mutual information due to identification noise. We apply our results to a DNA storage channel in the short-molecule regime, and quantify the resulting loss in the scaling of the total number of reliably stored bits.

</details>


### [74] [Convertible Codes for Data and Device Heterogeneity](https://arxiv.org/abs/2601.10341)
*Anina Gruica,Benjamin Jany,Stanislav Kruglik*

Main category: cs.IT

TL;DR: 本文研究分布式存储系统中的可转换编码，解决数据异构性和设备异构性问题，为Reed-Muller码构建显式转换程序


<details>
  <summary>Details</summary>
Motivation: 分布式存储系统面临两个关键挑战：数据异构性（由非均匀访问需求引起）和设备异构性（由时变节点可靠性引起）。现有研究通常单独处理这些问题，缺乏同时解决两者的综合方案。

Method: 研究可转换编码，在合并机制下以最小成本实现编码转换。推导线性编码转换的读写成本通用下界，然后专注于Reed-Muller码，构建显式转换程序，首次将两种异构性结合处理。

Result: 获得了线性编码转换读写成本的通用下界，为Reed-Muller码构建了显式转换程序，首次实现了同时处理数据异构性和设备异构性的分布式数据存储方案。

Conclusion: 可转换编码为分布式存储系统提供了一种有效处理数据异构性和设备异构性的统一框架，Reed-Muller码的显式转换程序实现了两种异构性的首次结合，为实际系统设计提供了理论基础。

Abstract: Distributed storage systems must handle both data heterogeneity, arising from non-uniform access demands, and device heterogeneity, caused by time-varying node reliability. In this paper, we study convertible codes, which enable the transformation of one code into another with minimum cost in the merge regime, addressing the latter. We derive general lower bounds on the read and write costs of linear code conversion, applicable to arbitrary linear codes. We then focus on Reed-Muller codes, which efficiently handle data heterogeneity, addressing the former issue, and construct explicit conversion procedures that, for the first time, combine both forms of heterogeneity for distributed data storage.

</details>


### [75] [A New Construction Structure on MISO Coded Caching with Linear Subpacketization: Half-Sum Disjoint Packing](https://arxiv.org/abs/2601.10353)
*Bowen Zheng,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出基于L-half-sum disjoint packing (HSDP)的MISO编码缓存方案，在保持线性子分组(F=K)的同时，显著降低子分组复杂度，仅轻微牺牲和自由度(sum-DoF)。


<details>
  <summary>Details</summary>
Motivation: 现有MISO编码缓存方案虽然能达到最大和自由度，但子分组复杂度随用户数指数增长，限制了实际应用。需要设计在保持高性能的同时降低子分组复杂度的方案。

Method: 基于拉丁方框架，将MAPDA设计转化为L-HSDP组合结构构造。通过构建L-HSDPs，获得一类新的F=K的MISO编码缓存方案。

Result: 提出的L-HSDP方案相比指数子分组方案显著降低子分组复杂度，仅轻微牺牲和自由度；相比现有线性子分组方案，同时实现更高的和自由度和更低的子分组复杂度。

Conclusion: 通过L-HSDP方法设计的MISO编码缓存方案在子分组复杂度和性能之间取得了良好平衡，为实际系统部署提供了可行方案。

Abstract: In the $(L,K,M,N)$ cache-aided multiple-input single-output (MISO) broadcast channel (BC) system, the server is equipped with $L$ antennas and communicates with $K$ single-antenna users through a wireless broadcast channel where the server has a library containing $N$ files, and each user is equipped with a cache of size $M$ files. Under the constraints of uncoded placement and one-shot linear delivery strategies, many schemes achieve the maximum sum Degree-of-Freedom (sum-DoF). However, for general parameters $L$, $M$, and $N$, their subpacketizations increase exponentially with the number of users. We aim to design a MISO coded caching scheme that achieves a large sum-DoF with low subpacketization $F$. An interesting combinatorial structure, called the multiple-antenna placement delivery array (MAPDA), can be used to generate MISO coded caching schemes under these two strategies; moreover, all existing schemes with these strategies can be represented by the corresponding MAPDAs. In this paper, we study the case with $F=K$ (i.e., $F$ grows linearly with $K$) by investigating MAPDAs. Specifically, based on the framework of Latin squares, we transform the design of MAPDA with $F=K$ into the construction of a combinatorial structure called the $L$-half-sum disjoint packing (HSDP). It is worth noting that a $1$-HSDP is exactly the concept of NHSDP, which is used to generate the shared-link coded caching scheme with $F=K$. By constructing $L$-HSDPs, we obtain a class of new schemes with $F=K$. Finally, theoretical and numerical analyses show that our $L$-HSDP schemes significantly reduce subpacketization compared to existing schemes with exponential subpacketization, while only slightly sacrificing sum-DoF, and achieve both a higher sum-DoF and lower subpacketization than the existing schemes with linear subpacketization.

</details>


### [76] [Generalized Weight Structure of Polar Codes: Selected Template Polynomials](https://arxiv.org/abs/2601.10362)
*Mohammad Rowshan,Vlad-Florin Dragoi*

Main category: cs.IT

TL;DR: 本文提出了一种计算极码汉明权重的通用代数框架，通过规范二元形式表示权重，推导出生成低中权重谱的结构模板，并结合LTA群作用得到显式多重性公式。


<details>
  <summary>Details</summary>
Motivation: 极码可视为递减单项式码，具有由下三角仿射(LTA)群支配的丰富代数结构。需要开发系统方法来计算由单项式和生成的码字的汉明权重，并表征权重谱。

Method: 开发通用框架计算单项式和生成的码字的汉明权重，用规范二元形式表示权重，推导关键结构模板（不相交和、嵌套块、互补翻转），结合LTA群作用得到显式多重性公式。

Result: 获得了计算汉明权重的规范二元形式，推导出生成低中权重谱的结构模板，建立了统一的代数方法来表征和枚举码字。

Conclusion: 通过将极码视为递减单项式码，利用LTA群结构和代数模板，建立了系统计算汉明权重和枚举码字的统一代数框架。

Abstract: Polar codes can be viewed as decreasing monomial codes, revealing a rich algebraic structure governed by the lower-triangular affine (LTA) group. We develop a general framework to compute the Hamming weight of codewords generated by sums of monomials, express these weights in a canonical dyadic form, and derive closed expressions for key structural templates (disjoint sums, nested blocks, complementary flips) that generate the low and intermediate weight spectrum. Combining these templates with the LTA group action, we obtain explicit multiplicity formulas, yielding a unified algebraic method to characterize and enumerate codewords.

</details>


### [77] [A Hybrid Reliability--Weight Framework for Construction of Polar Codes](https://arxiv.org/abs/2601.10376)
*Mohammad Rowshan,Vlad-Florin Dragoi*

Main category: cs.IT

TL;DR: 提出了一种结合可靠性和权重的混合比特信道排序方法，用于构造Polar码，在短码和中长码上优化最小距离和权重谱


<details>
  <summary>Details</summary>
Motivation: 传统Polar码基于可靠性排序构造，虽然能达到信道容量，但在短码和中长码上可能产生较差的最小权重谱。需要一种能同时考虑可靠性和码字权重的构造方法

Method: 定义混合度量：结合最小权重码字的轨道枚举（距离项）和Bhattacharyya型因子（可靠性项）。在递减单项式码类中最小化截断的SC/ML联合界代理函数

Result: 混合构造在短码和中长码上展示了可靠性与最小距离/多重性之间的权衡。通过高斯近似和闭式权重贡献实现，证明混合设计是可靠性构造的局部扰动，其渐近影响随码长增加而消失

Conclusion: 混合比特信道排序方法有效平衡了Polar码的可靠性和最小距离特性，在有限码长下优于纯可靠性构造，为短码和中长码设计提供了新思路

Abstract: Polar codes are usually constructed by ranking synthetic bit-channels according to reliability, which guarantees capacity-achieving behavior but can yield poor low-weight spectra at short and moderate lengths. Recent algebraic results express the contribution of individual bit-channels to the multiplicities of minimum and near-minimum weight codewords in closed form. In this work we combine these insights into a mixed (reliability--weight) bit-channel ordering. We define a per-bit cost whose distance term is derived from orbit enumeration of minimum-weight codewords and scaled by a Bhattacharyya-type factor, and show that the resulting mixed construction minimises a truncated SC/ML union-bound surrogate within a class of decreasing monomial codes. We relate the mixed metric to error events in SCL decoding via a pruning/ML decomposition, and prove that mixed designs act as local perturbations of reliability-based constructions whose asymptotic impact vanishes as code-length approaches infinity. Numerical results for short and moderate lengths on BPSK-AWGN, implemented via Gaussian approximation and closed-form weight contributions, illustrate the trade-off between pure reliability-based and mixed constructions in terms of minimum distance, multiplicity, and union-bound approximations. All proofs are deferred to the appendices.

</details>


### [78] [Codebook Design for Limited Feedback in Near-Field XL-MIMO Systems](https://arxiv.org/abs/2601.10391)
*Liujia Yao,Changsheng You,Zixuan Huang,Chao Zhou,Zhaohui Yang,Xiaoyang Li*

Main category: cs.IT

TL;DR: 提出针对XL-MIMO FDD系统的用户分布感知码本设计，通过联合优化角度-距离采样和比特分配，显著降低反馈开销并提升速率性能。


<details>
  <summary>Details</summary>
Motivation: 现有XL-MIMO码本设计（如极域码本）未充分考虑实际用户分布，导致反馈开销过大。需要设计更高效的反馈码本以适应实际部署场景。

Method: 1) 针对均匀用户分布场景，建立和速率最大化问题，联合优化角度-距离采样和比特分配；2) 采用Voronoi划分证明均匀角度采样最优；3) 对距离采样设计，推导接收功率下界，提出几何采样作为高质量次优解；4) 扩展至非均匀用户分布，采用交替采样方法；5) 理论分析阵列尺寸增大时的比特分配趋势。

Result: 1) 理论证明均匀角度采样最优，几何采样是距离采样的高质量次优解；2) 理论分析显示阵列尺寸增大时，比特分配应偏向距离采样；3) 数值结果表明所提码本在各种系统设置下均优于基准方案（包括广泛使用的极域码本），获得显著性能增益。

Conclusion: 提出的用户分布感知码本设计能有效适应实际部署场景，通过优化角度-距离采样和比特分配，在XL-MIMO FDD系统中实现高效反馈，显著提升系统性能。

Abstract: In this paper, we study efficient codebook design for limited feedback in extremely large-scale multiple-input-multiple-output (XL-MIMO) frequency division duplexing (FDD) systems. It is worth noting that existing codebook designs for XL-MIMO, such as polar-domain codebook, have not well taken into account user (location) distribution in practice, thereby incurring excessive feedback overhead. To address this issue, we propose in this paper a novel and efficient feedback codebook tailored to user distribution. To this end, we first consider a typical scenario where users are uniformly distributed within a specific polar-region, based on which a sum-rate maximization problem is formulated to jointly optimize angle-range samples and bit allocation among angle/range feedback. This problem is challenging to solve due to the lack of a closed-form expression for the received power in terms of angle and range samples. By leveraging a Voronoi partitioning approach, we show that uniform angle sampling is optimal for received power maximization. For more challenging range sampling design, we obtain a tight lower-bound on the received power and show that geometric sampling, where the ratio between adjacent samples is constant, can maximize the lower bound and thus serves as a high-quality suboptimal solution. We then extend the proposed framework to accommodate more general non-uniform user distribution via an alternating sampling method. Furthermore, theoretical analysis reveals that as the array size increases, the optimal allocation of feedback bits increasingly favors range samples at the expense of angle samples. Finally, numerical results validate the superior rate performance and robustness of the proposed codebook design under various system setups, achieving significant gains over benchmark schemes, including the widely used polar-domain codebook.

</details>


### [79] [Multiaccess Coded Caching with Heterogeneous Retrieval Costs](https://arxiv.org/abs/2601.10394)
*Wenbo Huang,Minquan Cheng,Kai Wan,Xiaojun Li,Robert Caiming Qiu,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文提出了一种基于叠加编码的成本感知多接入编码缓存框架，通过优化缓存放置来最小化系统总成本（包括缓存访问成本和广播成本），并设计了结构感知算法降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有MACC系统研究假设用户从连接的缓存节点检索内容没有通信成本，但实际中用户从不同缓存节点检索内容成本不同，服务器向用户传输内容也有成本。需要设计成本感知的MACC系统来最小化总系统成本。

Method: 提出基于叠加编码的新型编码缓存框架，将Cheng等人的MACC方案分层叠加。推导出优化缓存放置以最小化系统成本的优化问题，通过识别最优解的稀疏特性，提出复杂度降低的结构感知算法。

Result: 仿真结果表明，在异构检索成本场景下，所提方案始终优于Cheng等人的方案。

Conclusion: 本文提出的成本感知MACC框架和优化算法能有效降低系统总成本，特别是在异构成本环境下表现优异。

Abstract: The multiaccess coded caching (MACC) system, as formulated by Hachem {\it et al.}, consists of a central server with a library of $N$ files, connected to $K$ cache-less users via an error-free shared link, and $K$ cache nodes, each equipped with cache memory of size $M$ files. Each user can access $L$ neighboring cache nodes under a cyclic wrap-around topology. Most existing studies operate under the strong assumption that users can retrieve content from their connected cache nodes at no communication cost. In practice, each user retrieves content from its $L$ different connected cache nodes at varying costs. Additionally, the server also incurs certain costs to transmit the content to the users. In this paper, we focus on a cost-aware MACC system and aim to minimize the total system cost, which includes cache-access costs and broadcast costs. Firstly, we propose a novel coded caching framework based on superposition coding, where the MACC schemes of Cheng \textit{et al.} are layered. Then, a cost-aware optimization problem is derived that optimizes cache placement and minimizes system cost. By identifying a sparsity property of the optimal solution, we propose a structure-aware algorithm with reduced complexity. Simulation results demonstrate that our proposed scheme consistently outperforms the scheme of Cheng {\it et al.} in scenarios with heterogeneous retrieval costs.

</details>


### [80] [Placement Delivery Array for Cache-Aided MIMO Systems](https://arxiv.org/abs/2601.10422)
*Yifei Huang,Kai Wan,Minquan Cheng,Jinyan Wang,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出MIMO-PDA统一框架，在缓存辅助MIMO网络中同时实现最大和自由度与低子分组化，提供两种构造方案


<details>
  <summary>Details</summary>
Motivation: 在缓存辅助MIMO网络中，需要设计同时实现最大和自由度与低子分组化的编码缓存方案，现有方案难以平衡这两个目标

Method: 引入MIMO-PDA统一组合结构，分析其组合性质推导和自由度上界，提出两种MIMO-PDA构造：第一种在严格参数约束下实现线性子分组化，第二种在更宽松约束下实现有序指数级子分组化

Result: 推导出和自由度上界min{KG, Gt+G⌈L/G⌉}，两种构造均达到最大和自由度，第二种构造相比现有方案指数级降低子分组化

Conclusion: MIMO-PDA框架有效平衡了和自由度与子分组化的权衡，第二种构造在保持最大和自由度的同时显著降低了子分组化复杂度

Abstract: We consider a $(G,L,K,M,N)$ cache-aided multiple-input multiple-output (MIMO) network, where a server equipped with $L$ antennas and a library of $N$ equal-size files communicates with $K$ users, each equipped with $G$ antennas and a cache of size $M$ files, over a wireless interference channel. Each user requests an arbitrary file from the library. The goal is to design coded caching schemes that simultaneously achieve the maximum sum degrees of freedom (sum-DoF) and low subpacketization. In this paper, we first introduce a unified combinatorial structure, termed the MIMO placement delivery array (MIMO-PDA), which characterizes uncoded placement and one-shot zero-forcing delivery. By analyzing the combinatorial properties of MIMO-PDAs, we derive a sum-DoF upper bound of $\min\{KG, Gt+G\lceil L/G \rceil\}$, where $t=KM/N$, which coincides with the optimal DoF characterization in prior work by Tehrani \emph{et al.}. Based on this upper bound, we present two novel constructions of MIMO-PDAs that achieve the maximum sum-DoF. The first construction achieves linear subpacketization under stringent parameter constraints, while the second achieves ordered exponential subpacketization under substantially milder constraints. Theoretical analysis and numerical comparisons demonstrate that the second construction exponentially reduces subpacketization compared to existing schemes while preserving the maximum sum-DoF.

</details>


### [81] [Energy-Efficient Probabilistic Semantic Communication Over Visible Light Networks With Rate Splitting](https://arxiv.org/abs/2601.10452)
*Zhouxiang Zhao,Zhaohui Yang,Mingzhe Chen,Chen Zhu,Xin Tong,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 该论文研究了资源受限的基于可见光通信的概率语义通信系统中的能效最大化问题，通过联合优化传输波束成形、直流偏置、公共速率分配和语义压缩比，并开发了基于连续凸逼近和Dinkelbach方法的交替优化算法。


<details>
  <summary>Details</summary>
Motivation: 可见光通信作为未来无线通信系统的关键技术，具有优于传统射频系统的物理层优势，但其与语义通信等高层技术的结合尚未充分探索。在资源受限的VLC概率语义通信系统中，需要解决能效最大化问题，同时考虑通信和计算成本。

Method: 采用基于连续凸逼近（SCA）和Dinkelbach方法的交替优化算法，联合优化传输波束成形、直流偏置、公共速率分配和语义压缩比。系统使用概率图表示知识库，并采用速率分割多址接入技术同时传输知识和信息数据。

Result: 仿真结果表明所提方法的有效性，能够有效解决VLC概率语义通信系统中的能效最大化问题。

Conclusion: 该研究为可见光通信与语义通信的融合提供了有效的能效优化方案，通过联合优化多个系统参数，在考虑通信和计算成本的同时实现了能效最大化，为未来无线通信系统的发展提供了重要参考。

Abstract: Visible light communication (VLC) is emerging as a key technology for future wireless communication systems due to its unique physical-layer advantages over traditional radio-frequency (RF)-based systems. However, its integration with higher-layer techniques, such as semantic communication, remains underexplored. This paper investigates the energy efficiency maximization problem in a resource-constrained VLC-based probabilistic semantic communication (PSCom) system. In the considered model, light-emitting diode (LED) transmitters perform semantic compression to reduce data size, which incurs additional computation overhead. The compressed semantic information is transmitted to the users for semantic inference using a shared knowledge base that requires periodic updates to ensure synchronization. In the PSCom system, the knowledge base is represented by probabilistic graphs. To enable simultaneous transmission of both knowledge and information data, rate splitting multiple access (RSMA) is employed. The optimization problem focuses on maximizing energy efficiency by jointly optimizing transmit beamforming, direct current (DC) bias, common rate allocation, and semantic compression ratio, while accounting for both communication and computation costs. To solve this problem, an alternating optimization algorithm based on successive convex approximation (SCA) and Dinkelbach method is developed. Simulation results demonstrate the effectiveness of the proposed approach.

</details>


### [82] [Joint Source-Channel Coding for ISAC: Distortion Tradeoffs and Separation Theorems](https://arxiv.org/abs/2601.10470)
*Gefei Peng,Youlong Wu*

Main category: cs.IT

TL;DR: 本文研究了集成感知与通信系统中的联合信源信道编码框架，从信息论角度建立了信道容量、通信与感知失真以及估计成本之间的权衡关系，证明了分离信源信道编码在此场景下能达到联合最优性。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信系统因其能同时实现高效通信和环境感知而受到广泛关注。该领域的核心目标之一是刻画感知与通信之间的性能权衡关系。本文旨在从信息论角度分析ISAC系统中的性能权衡问题。

Method: 采用联合信源信道编码框架，系统包含带有信道状态估计器和联合编码器的发射机、状态相关无记忆信道、以及带有联合解码器的接收机。从信息论角度建立信道容量、通信与感知失真、估计成本之间的权衡关系，并证明分离信源信道编码在此设置下能达到联合最优性。

Result: 建立了ISAC系统中信道容量、通信失真、感知失真和估计成本之间的权衡关系理论框架。证明了分离信源信道编码能够达到联合最优性，并通过二进制设置的示例验证了理论结果。

Conclusion: 本文从信息论角度为ISAC系统建立了性能权衡的理论基础，证明了分离信源信道编码的联合最优性，为实际系统设计提供了理论指导。

Abstract: Integrated Sensing and Communication (ISAC) systems have garnered significant attention due to their capability to simultaneously achieve efficient communication and environmental sensing. A core objective in this field is characterizing the performance tradeoff between sensing and communication. In this paper, we consider a joint source-channel coding (JSCC) framework for the ISAC system that consists of a transmitter with a channel state estimator and a joint source-channel encoder, a state-dependent memoryless channel, and a receiver with a joint source-channel decoder. From an information-theoretic perspective, we establish the tradeoff relationships among channel capacity, distortions in both communication and sensing processes, and the estimation cost. We prove that the separate source and channel coding can achieve joint optimality in this setting. An illustrative example of a binary setting is also provided to validate our theoretical results.

</details>


### [83] [A Construction Framework of Coded Caching Scheme for Multi-Access MIMO Systems via Knapsack Problem](https://arxiv.org/abs/2601.10484)
*Siying Luo,Youlong Wu,Mingming Zhang,Minquan Cheng,Dianhua Wu*

Main category: cs.IT

TL;DR: 本文研究具有组合拓扑的多接入多输入单输出（MAMISO）网络中的编码缓存问题，提出基于0-1背包问题的多天线放置交付阵列（MAPDA）设计方法，在提高和自由度（sum-DoF）的同时保持较低的子分组复杂度。


<details>
  <summary>Details</summary>
Motivation: 在多接入MISO网络中，现有方案难以同时实现高和自由度与低子分组复杂度。组合拓扑结构使得缓存设计更加复杂，需要一种能平衡性能与复杂度的系统化方法。

Method: 将多天线放置交付阵列（MAPDA）设计建模为0-1背包问题，最大化可实现的自由度。该方法将复杂的组合缓存结构转化为可处理的优化框架，生成高效的缓存放置和灵活的交付策略。

Result: 理论分析和数值仿真表明：在组合拓扑网络中，所提方案比现有方案获得更高的和自由度；在相同缓存大小约束下，子分组水平与现有线性子分组方案相当；在特定系统条件下，达到理论最大和自由度min{L+KM/N, K}，并进一步降低子分组复杂度；针对特定组合结构，推导出优化的构造，实现更高和自由度和更低子分组。

Conclusion: 提出的基于MAPDA和0-1背包问题的设计框架有效解决了多接入MISO网络中编码缓存的性能-复杂度权衡问题，为组合拓扑网络提供了高效的缓存方案。

Abstract: This paper investigates the coded caching problem in a multi-access multiple-input single-output (MAMISO) network with the combinatorial topology. The considered system consists of a server containing $N$ files, $Λ$ cache nodes, and $K$ cache-less users, where each user can access a unique subset of $r$ cache nodes. The server is equipped with $L$ transmit antennas. Our objective is to design a caching scheme that simultaneously achieves a high sum Degree of Freedom (sum-DoF) and low subpacketization complexity. To address this challenge, we formulate the design of multi-antenna placement delivery arrays (MAPDA) as a $0$--$1$ knapsack problem to maximize the achievable DoF, thereby transforming the complex combinatorial caching structure into a tractable optimization framework that yields efficient cache placement and flexible delivery strategies. Theoretical and numerical analyses demonstrate that: for networks with combinatorial topologies, the proposed scheme achieves a higher sum-DoF than existing schemes. Under identical cache size constraints, the subpacketization level remains comparable to existing linear subpacketization schemes. Moreover, under specific system conditions, the proposed scheme attains the theoretical maximum sum-DoF of $\min\{L+KM/N, K\}$ while achieving further reductions subpacketization. For particular combinatorial structures, we further derive optimized constructions that achieve even higher sum-DoF with lower subpacketization. ```

</details>


### [84] [Coded Caching for Combinatorial Multi-Access Hotplug Networks from $t$-Designs](https://arxiv.org/abs/2601.10503)
*Dhruv Pratap Singh,Anjana A. Mahesh,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 提出基于t设计的组合多接入网络热插拔编码缓存方案，扩展HpPDA框架，实现灵活速率-内存权衡和低子分组化


<details>
  <summary>Details</summary>
Motivation: 现有热插拔编码缓存模型假设用户只能访问单个缓存，而实际组合多接入网络中用户可访问多个缓存，且交付阶段只有部分缓存在线，需要更通用的模型

Method: 1. 将热插拔放置交付数组(HpPDA)框架扩展到组合多接入场景；2. 提出基于t设计的编码缓存方案；3. 识别确保活跃用户能解码所需文件的参数类别；4. 通过适当参数选择消除冗余组播传输

Result: 1. 实现一系列灵活的速率-内存权衡；2. 在特定内存区间优于现有热插拔编码缓存方案；3. 支持灵活的子分组化；4. 数值比较显示性能优势

Conclusion: 成功将热插拔编码缓存扩展到组合多接入网络，提出的t设计方案在保持灵活子分组化的同时实现了更好的速率-内存权衡，为实际缓存网络提供了更通用的解决方案

Abstract: We study hotplug coded caching in combinatorial multi-access networks, which generalizes existing hotplug coded caching models by allowing users to access multiple caches, while only a subset of caches is online during the delivery phase. We first generalize the Hotplug Placement Delivery Array (HpPDA) framework to the combinatorial multi-access setting. Based on this generalized framework, we propose a t-design-based coded caching scheme for combinatorial multi-access networks. We characterize a class of design parameters under which every active user has access to a sufficient number of coded subfiles to decode its requested file, and show that appropriate parameter choices allow for the elimination of redundant multicast transmissions. As a result, the proposed scheme achieves a family of rate memory trade offs with flexible subpacketization. We present numerical comparisons illustrating that the proposed t-scheme outperforms existing hotplug coded caching schemes in certain memory regimes.

</details>


### [85] [A New Construction Structure on Coded Caching with Linear Subpacketization: Non-Half-Sum Latin Rectangle](https://arxiv.org/abs/2601.10505)
*Yongcheng Yang,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文提出了一种新的组合结构NHSLR，将线性编码缓存的子分组化从F=K扩展到F=O(K)，实现了线性可扩展的子分组化，同时进一步降低了传输负载。


<details>
  <summary>Details</summary>
Motivation: 编码缓存是缓解网络拥塞的有效方法，但现有方案存在子分组化指数级或多项式级过高的问题，而线性子分组化方案往往导致传输负载过大。需要设计同时实现低子分组化和低传输负载的方案。

Method: 提出了一种新的组合结构——非半和拉丁矩形(NHSLR)，扩展了NHSDP框架，将线性编码缓存方案从F=K扩展到F=O(K)。通过构造NHSLR获得了一类新的编码缓存方案。

Result: 新方案实现了线性可扩展的子分组化，相比NHSDP方案进一步降低了传输负载。理论分析和数值结果表明，该方案不仅比现有线性子分组化方案传输负载更低，而且接近某些指数子分组化方案的性能。

Conclusion: NHSLR结构为编码缓存方案设计提供了新的框架，实现了线性可扩展的子分组化同时降低传输负载，在子分组化和传输负载之间取得了更好的平衡。

Abstract: Coded caching is recognized as an effective method for alleviating network congestion during peak periods by leveraging local caching and coded multicasting gains. The key challenge in designing coded caching schemes lies in simultaneously achieving low subpacketization and low transmission load. Most existing schemes require exponential or polynomial subpacketization levels, while some linear subpacketization schemes often result in excessive transmission load. Recently, Cheng et al. proposed a construction framework for linear coded caching schemes called Non-Half-Sum Disjoint Packing (NHSDP), where the subpacketization equals the number of users $K$. This paper introduces a novel combinatorial structure, termed the Non-Half-Sum Latin Rectangle (NHSLR), which extends the framework of linear coded caching schemes from $F=K$ (i.e., the construction via NHSDP) to a broader scenario with $F=\mathcal{O}(K)$. By constructing NHSLR, we have obtained a new class of coded caching schemes that achieves linearly scalable subpacketization, while further reducing the transmission load compared with the NHSDP scheme. Theoretical and numerical analyses demonstrate that the proposed schemes not only achieves lower transmission load than existing linear subpacketization schemes but also approaches the performance of certain exponential subpacketization schemes.

</details>


### [86] [A New Construction Structure on Multi-access Coded Caching with Linear Subpacketization: Cyclic Multi-Access Non-Half-Sum Disjoint Packing](https://arxiv.org/abs/2601.10510)
*Mengyuan Li,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出基于CMA-NHSDP结构的多接入编码缓存方案，在保持线性子分组化(F=K)的同时实现较低的传输负载


<details>
  <summary>Details</summary>
Motivation: 现有多接入编码缓存方案存在矛盾：性能好的方案子分组化呈指数增长，而线性/多项式子分组化的方案传输负载较高。需要设计在保持线性子分组化的同时降低传输负载的方案。

Method: 将NHSDP（非半和不相交包装）结构扩展到多接入系统，提出CMA-NHSDP（循环多接入非半和不相交包装）组合结构，基于此构造新的多接入编码缓存方案。

Result: 理论分析和数值比较表明，新方案在保持线性子分组化(F=K)的同时，传输负载低于现有线性子分组化方案，在某些情况下甚至优于指数子分组化方案。

Conclusion: CMA-NHSDP结构为多接入编码缓存系统提供了一种有效平衡子分组化复杂度和传输性能的新方法，在保持线性复杂度的同时改善了传输效率。

Abstract: We consider the $(K,L,M,N)$ multi-access coded caching system introduced by Hachem et al., which consists of a central server with $N$ files and $K$ cache nodes, each of memory size $M$, where each user can access $L$ cache nodes in a cyclic wrap-around fashion. At present, several existing schemes achieve competitive transmission performance, but their subpacketization levels grow exponentially with the number of users. In contrast, schemes with linear or polynomial subpacketization always incur higher transmission loads. We aim to design a multi-access coded caching scheme with linear subpacketization $F$ while maintaining low transmission load. Recently, Cheng et al. proposed a construction framework for coded caching schemes with linear subpacketization (i.e., $F=K$) called non-half-sum disjoint packing (NHSDP). Inspired by this structure, we introduce a novel combinatorial structure named cyclic multi-access non-half-sum disjoint packing (CMA-NHSDP) by extending NHSDP to MACC system. By constructing CMA-NHSDP, we obtain a new class of multi-access coded caching schemes. Theoretical and numerical analyses show that our scheme achieves lower transmission loads than some existing schemes with linear subpacketization. Moreover, the proposed schemes achieves lower transmission load compared to existing schemes with exponential subpacketization in some case.

</details>


### [87] [On the suboptimality of linear codes for binary distributed hypothesis testing](https://arxiv.org/abs/2601.10526)
*Adway Girish,Robinson D. H. Cung,Emre Telatar*

Main category: cs.IT

TL;DR: 研究二进制分布式假设检验问题，两个代理观察相关二进制向量，以相同速率向中央决策者发送压缩信息。分析线性压缩方案，证明截断是最佳线性方案：1) 检验相同幅度但符号相反的相关系数；2) 检验独立性或反对独立性。数值证据支持截断是检验任何符号相反相关性的最佳线性编码。对于检验独立性，计算经典随机编码指数，显示截断（及任何线性编码）严格次优。


<details>
  <summary>Details</summary>
Motivation: 研究分布式假设检验中的压缩方案优化问题，特别关注线性压缩方案在二进制相关向量检验中的性能。需要确定在特定假设检验场景下，是否存在简单且最优的线性压缩方案。

Method: 采用线性压缩方案，分析截断（truncation）作为特定线性编码的性能。在两种情况下证明截断是最佳线性方案：1) 检验相同幅度但符号相反的相关系数；2) 检验独立性或反对独立性。通过数值证据支持截断是检验任何符号相反相关性的最佳线性编码的猜想。对于检验独立性的场景，计算经典随机编码指数进行对比分析。

Result: 证明截断在两种情况下是最佳线性压缩方案：检验相同幅度但符号相反的相关系数，以及检验独立性或反对独立性。数值证据支持截断是检验任何符号相反相关性的最佳线性编码的猜想。对于检验独立性的场景，计算显示截断（及任何线性编码）严格次优于经典随机编码方案。

Conclusion: 截断是特定分布式假设检验场景下的最佳线性压缩方案，但对于检验独立性问题，线性编码（包括截断）严格次优于随机编码方案。这表明在某些假设检验任务中，线性编码可能不是最优选择。

Abstract: We study a binary distributed hypothesis testing problem where two agents observe correlated binary vectors and communicate compressed information at the same rate to a central decision maker. In particular, we study linear compression schemes and show that simple truncation is the best linear scheme in two cases: (1) testing opposite signs of the same magnitude of correlation, and (2) testing for or against independence. We conjecture, supported by numerical evidence, that truncation is the best linear code for testing any correlations of opposite signs. Further, for testing against independence, we also compute classical random coding exponents and show that truncation, and consequently any linear code, is strictly suboptimal.

</details>


### [88] [Network Integrated Sensing and Communication](https://arxiv.org/abs/2601.10538)
*Edward Andrews,Lawrence Ong,Duy T. Ngo,Yao Liu,Min Li*

Main category: cs.IT

TL;DR: 本文研究了网络级ISAC系统，在通信路由与感知覆盖之间建立优化框架，分析了一维路径网络的完整感知-吞吐量区域，并扩展到一般网络拓扑，揭示了感知与通信之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC研究主要集中在链路级设计，而大规模部署需要理解网络级性能。本文旨在研究网络ISAC模型中通信路由与感知覆盖之间的相互作用，为未来6G异构网络设计提供理论基础。

Method: 提出一个新颖的优化框架，捕捉多节点路由和感知覆盖的相互作用。针对一维路径网络提供完整的感知-吞吐量区域解析表征，并将其扩展到一般网络拓扑，分析感知-吞吐量Pareto边界的特性。

Result: 对于一维路径网络，获得了完整的感知-吞吐量区域解析解。对于一般网络拓扑，证明了感知-吞吐量Pareto边界是分段线性的，并为每个分段提供了物理解释。揭示了感知覆盖与通信路由之间的基本权衡关系。

Conclusion: 本文建立了网络级ISAC性能分析的理论框架，揭示了感知与通信之间的基本权衡，为未来6G异构网络的设计提供了关键见解，特别是在大规模部署场景下。

Abstract: Integrated sensing and communication (ISAC) is a cornerstone technology for 6G networks, offering unified support for high-rate communication and high-accuracy sensing. While existing literature extensively covers link-level designs, the transition toward large-scale deployment necessitates a fundamental understanding of network-level performance. This paper investigates a network ISAC model where a source node communicates with a destination via a relay network, while intermediate nodes concurrently perform cooperative sensing over specific spatial regions. We formulate a novel optimization framework that captures the interplay between multi-node routing and sensing coverage. For a one-dimensional path network, we provide an analytical characterization of the complete sensing-throughput region. Extending this to general network topologies, we establish that the sensing-throughput Pareto boundary is piecewise linear and provide physical interpretations for each segment. Our results reveal the fundamental trade-offs between sensing coverage and communication routing, offering key insights for the design of future 6G heterogeneous networks.

</details>


### [89] [Error-Correcting Codes for Two Bursts of t1-Deletion-t2-Insertion with Low Computational Complexity](https://arxiv.org/abs/2601.10540)
*Yajuan Liu,Tolga M. Duman*

Main category: cs.IT

TL;DR: 该论文研究能纠正多个突发(t₁,t₂)-删除-插入错误的纠错码，建立了不同错误类型间的等价关系，推导了码率上下界，并提出了低复杂度构造方法。


<details>
  <summary>Details</summary>
Motivation: 在DNA数据存储和文档同步等实际应用中，经常出现同时包含删除、插入和替换的突发错误，因此需要开发能够纠正这类错误的信道编码。

Method: 1) 建立(t₁,t₂)-DI错误类型的等价关系；2) 推导两个突发(t₁,t₂)-DI错误码的码率上下界；3) 构造两个突发(t₁,t₂)-DI错误码，相比症候压缩技术显著降低计算复杂度。

Result: 证明了三种错误类型的等价性，推导了码率理论界限，并提出了比现有症候压缩技术复杂度更低的构造方法。

Conclusion: 该研究为处理多个突发删除-插入错误的纠错码提供了理论基础和实用构造方法，在DNA数据存储等应用中有重要价值。

Abstract: Burst errors involving simultaneous insertions, deletions, and substitutions occur in practical scenarios, including DNA data storage and document synchronization, motivating developments of channel codes that can correct such errors. In this paper, we address the problem of constructing error-correcting codes (ECCs) capable of handling multiple bursts of $t_1$-deletion-$t_2$-insertion ($(t_1,t_2)$-DI) errors, where each burst consists of $t_1$ deletions followed by $t_2$ insertions in a binary sequence. We make three key contributions: Firstly, we establish the fundamental equivalence of (1) two bursts of $(t_1,t_2)$-DI ECCs, (2) two bursts of $(t_2,t_1)$-DI ECCs, and (3) one burst each of $(t_1,t_2)$-DI and $(t_2,t_1)$-DI ECCs. Then, we derive lower and upper bounds on the code size of two bursts of $(t_1,t_2)$-DI ECCs, which can naturally be extended to the case of multiple bursts. Finally, we present constructions of two bursts of $(t_1,t_2)$-DI ECCs. Compared to the codes obtained by the syndrome compression technique, the resulting codes achieve significantly lower computational complexity.

</details>


### [90] [Sparse Signal Recovery from Random Measurements](https://arxiv.org/abs/2601.10569)
*Siu-Wing Cheng,Man Ting Wong*

Main category: cs.IT

TL;DR: 提出一种无需优化或解线性系统的简单压缩感知恢复方法，仅需O(log n)个随机测量矩阵，时间复杂度O(kn log n)


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知方法需要解决优化问题或线性系统，计算复杂度高。本文旨在开发更简单、更高效的非优化恢复方法。

Method: 使用Θ(log n)个随机测量矩阵，通过简单算法直接恢复信号，无需优化求解。k = Θ(s log n)，s为信号稀疏度。还扩展方法用于确定信号支撑集。

Result: 方法在O(kn log n)时间内恢复信号，仅需对数级测量矩阵。实验表明在二值信号上与基于优化的方法性能相当。

Conclusion: 提出了一种简单高效的压缩感知恢复方法，避免了复杂的优化求解，在计算效率和实现复杂度上具有优势。

Abstract: Given the compressed sensing measurements of an unknown vector $z \in \mathbb{R}^n$ using random matrices, we present a simple method to determine $z$ without solving any optimization problem or linear system. Our method uses $Θ(\log n)$ random sensing matrices in $\mathbb{R}^{k \times n}$ and runs in $O(kn\log n)$ time, where $k = Θ(s\log n)$ and $s$ is the number of nonzero coordinates in $z$. We adapt our method to determine the support set of $z$ and experimentally compare with some optimization-based methods on binary signals.

</details>


### [91] [Fundamental Limits of Multi-User Distributed Computing of Linearly Separable Functions](https://arxiv.org/abs/2601.10603)
*K. K. Krishnan Namboodiri,Elizabath Peter,Derya Malak,Petros Elia*

Main category: cs.IT

TL;DR: 本文建立了多用户分布式计算线性可分函数的基本极限，提出了在通信与计算权衡下的最优分布式计算方案。


<details>
  <summary>Details</summary>
Motivation: 研究多用户分布式计算线性可分函数的基本性能极限，解决通信与计算之间的根本权衡问题。在分布式计算环境中，服务器计算能力有限（最多计算M个子函数），通信能力有限（最多向Δ个用户传输），需要设计高效的分布式计算方案来降低通信成本。

Method: 提出了一种联合设计任务分配和传输的分布式计算方案。对于给定的K（基础子函数数量）、L（用户数量）、M（服务器计算能力）、Δ（服务器通信能力），在实数域中使用新颖的对偶证明方法，在有限域中使用基于计数论证的对偶证明方法。

Result: 证明了所提方案在各种条件下在实数域中达到最优性能，并在有限域中完整刻画了方案的性能表现。

Conclusion: 本文建立了多用户分布式计算线性可分函数的基本极限，提出的联合任务分配和传输设计方案在多种条件下达到最优，为解决分布式计算中的通信-计算权衡问题提供了理论基础。

Abstract: This work establishes the fundamental limits of the classical problem of multi-user distributed computing of linearly separable functions. In particular, we consider a distributed computing setting involving $L$ users, each requesting a linearly separable function over $K$ basis subfunctions from a master node, who is assisted by $N$ distributed servers. At the core of this problem lies a fundamental tradeoff between communication and computation: each server can compute up to $M$ subfunctions, and each server can communicate linear combinations of their locally computed subfunctions outputs to at most $Δ$ users. The objective is to design a distributed computing scheme that reduces the communication cost (total amount of data from servers to users), and towards this, for any given $K$, $L$, $M$, and $Δ$, we propose a distributed computing scheme that jointly designs the task assignment and transmissions, and shows that the scheme achieves optimal performance in the real field under various conditions using a novel converse. We also characterize the performance of the scheme in the finite field using another converse based on counting arguments.

</details>


### [92] [Basis-Spline Assisted Coded Computing: Strategies and Error Bounds](https://arxiv.org/abs/2601.10616)
*Rimpi Borah,J. Harshan,V. Lalitha*

Main category: cs.IT

TL;DR: 提出基于三次B样条插值的编码计算框架，用于处理非多项式函数的分布式计算，相比现有Berrut方法显著提升精度


<details>
  <summary>Details</summary>
Motivation: 现有Berrut近似编码计算方法在处理非多项式函数时，由于Berrut插值具有全局支撑特性，当延迟节点数量较大时精度会显著下降，需要更稳定精确的方法

Method: 提出基于三次B样条插值的编码计算框架，利用B样条的局部支撑和平滑特性，在主机节点重构服务器端函数评估，增强稳定性和精度

Result: 提供了将B样条插值集成到编码计算中的系统方法，推导了近似误差的理论界限，比较分析显示该方法在各种非多项式函数上显著优于Berrut方法

Conclusion: 基于B样条的编码计算框架能有效解决现有方法在处理大量延迟节点时的精度下降问题，为分布式计算中的非多项式函数处理提供了更优方案

Abstract: Coded computing has become a key framework for reliable distributed computation over decentralized networks, effectively mitigating the impact of stragglers. Although there exists a wide range of coded computing methods to handle both polynomial and non-polynomial functions, computing methods for the latter class have received traction due its inherent challenges in reconstructing non-polynomial functions using a finite number of evaluations. Among them, the state-of-the-art method is Berrut Approximated coded computing, wherein Berrut interpolants, are used for approximating the non-polynomial function. However, since Berrut interpolants have global support characteristics, such methods are known to offer degraded accuracy when the number of stragglers is large. To address this challenge, we propose a coded computing framework based on cubic B-spline interpolation. In our approach, server-side function evaluations are reconstructed at the master node using B-splines, exploiting their local support and smoothness properties to enhance stability and accuracy. We provide a systematic methodology for integrating B-spline interpolation into coded computing and derive theoretical bounds on approximation error in terms of the number of servers and stragglers. Comparative analysis demonstrates that our framework significantly outperforms Berrut-based methods for various non-polynomial functions.

</details>


### [93] [Converse Bounds for Sun-Jafar-type Weak Private Information Retrieval](https://arxiv.org/abs/2601.10643)
*Chandan Anand,Jayesh Seshadri,Prasad Krishnan,Gowtham R. Kurri*

Main category: cs.IT

TL;DR: 本文证明了Chandan等人提出的WPIR方案在特定条件下的最优性，并在不满足阈值约束时给出了更高可达率的反例。


<details>
  <summary>Details</summary>
Motivation: Chandan等人提出了新的弱私有信息检索(WPIR)方案，并给出了速率-隐私权衡的表达式，但这些权衡是否在各自类别中是最优的尚不清楚。

Method: 通过数学证明和构造反例的方法，分析了Chandan等人提出的WPIR方案的最优性条件。

Result: 证明了在非共谋复制存储设置下，Sun-Jafar型方案的速率-隐私权衡是最优的；在满足阈值约束条件下，Banawan-Ulukus型MDS-WPIR和Sun-Jafar型T-共谋WPIR方案也是类别最优的；当不满足阈值约束时，存在可达更高速率的反例。

Conclusion: 本文确定了Chandan等人WPIR方案的最优性条件，并揭示了在特定参数范围外存在更好的速率-隐私权衡，为WPIR方案设计提供了更完整的理论指导。

Abstract: Building on the well-established capacity-achieving schemes of Sun-Jafar (for replicated storage) and the closely related scheme of Banawan-Ulukus (for MDS-coded setting), a recent work by Chandan et al. proposed new classes of weak private information retrieval (WPIR) schemes for the collusion-free (replication and MDS-coded) setting, as well as for the $T$-colluding scenario. In their work, Chandan et al. characterized the expressions for the rate-privacy trade-offs for these classes of WPIR schemes, under the mutual information leakage and maximal leakage metrics. Explicit achievable trade-offs for the same were also presented, which were shown to be competitive or better than prior WPIR schemes. However, the class-wise optimality of the reported trade-offs were unknown. In this work, we show that the explicit rate-privacy trade-offs reported for the Sun-Jafar-type schemes by Chandan et al. are optimal for the non-colluding and replicated setting. Furthermore, we prove the class-wise optimality for Banawan-Ulukus-type MDS-WPIR and Sun-Jafar-type $T$-colluding WPIR schemes, under threshold-constraints on the system parameters. When these threshold-constraints do not hold, we present counter-examples which show that even higher rates than those reported before can be achieved.

</details>


### [94] [One-Shot Broadcast Joint Source-Channel Coding with Codebook Diversity](https://arxiv.org/abs/2601.10648)
*Joseph Rowan,Buu Phan,Ashish Khisti*

Main category: cs.IT

TL;DR: 研究单次联合信源信道编码广播场景，多个解码器通过独立信道接收信号，至少一个解码器成功恢复信源即可。发现使用不相交码本可获得码本分集增益，不同于传统信道分集增益。


<details>
  <summary>Details</summary>
Motivation: 研究多解码器广播场景中的成功概率优化问题，探索在至少一个解码器成功恢复信源的条件下，如何通过码本设计策略提高系统性能。

Method: 提出不相交码本编码方案，利用泊松匹配引理推导一阶和二阶可达界；进一步提出混合编码方案，将解码器分组以平衡码本分集和信道分集。

Result: 在二进制对称信道上的数值结果表明，混合编码方案优于完全共享码本或完全不相交码本的策略，实现了最佳性能平衡。

Conclusion: 在单次联合信源信道编码广播中，码本分集增益是重要因素，混合编码方案能有效平衡码本分集和信道分集，提升系统性能。

Abstract: We study a one-shot joint source-channel coding setting where the source is encoded once and broadcast to $K$ decoders through independent channels. Success is predicated on at least one decoder recovering the source within a maximum distortion constraint. We find that in the one-shot regime, utilizing disjoint codebooks at each decoder yields a codebook diversity gain, distinct from the channel diversity gain that may be expected when several decoders observe independent realizations of the channel's output but share the same codebook. Coding schemes are introduced that leverage this phenomenon, where first- and second-order achievability bounds are derived via an adaptation of the Poisson matching lemma (Li and Anantharam, 2021) which allows for multiple decoders using disjoint codebooks. We further propose a hybrid coding scheme that partitions decoders into groups to optimally balance codebook and channel diversity. Numerical results on the binary symmetric channel demonstrate that the hybrid approach outperforms strategies where the decoders' codebooks are either fully shared or disjoint.

</details>


### [95] [Synchronizing Probabilities in Model-Driven Lossless Compression](https://arxiv.org/abs/2601.10678)
*Aviv Adler,Jennifer Tang*

Main category: cs.IT

TL;DR: PMATIC是一种概率匹配区间编码算法，能够容忍神经网络预测中的微小差异，解决模型驱动压缩中的预测不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然能有效预测符号概率用于压缩，但压缩器和解压器必须保持完全一致的预测。硬件、软件或计算顺序的微小差异会导致预测不匹配，引发级联解码失败。

Method: 提出概率匹配区间编码（PMATIC），这是一种模型无关算法，能够容忍有界预测不匹配且开销低。PMATIC使用预测概率，可作为模型驱动压缩工具中算术编码器的直接替代品。

Result: 理论证明了PMATIC的正确性和性能边界，并在文本数据上验证了结果。当与先进预测模型配合使用时，PMATIC对预测不匹配具有鲁棒性，且压缩率优于标准现代压缩工具。

Conclusion: PMATIC解决了模型驱动压缩中的预测不匹配问题，提供了一种鲁棒且高效的压缩方案，能够充分利用深度神经网络的强大预测能力。

Abstract: It is well-known in the field of lossless data compression that probabilistic next-symbol prediction can be used to compress sequences of symbols. Deep neural networks are able to capture rich dependencies in data, offering a powerful means of estimating these probabilities and hence an avenue towards more effective compression algorithms. However, both compressor and decompressor must have exactly matching predictions; even small non-deterministic differences (which often happen with learned models due to hardware, software, or computation order) can lead to cascading decoding failures. In this paper, we formalize the problem of prediction mismatch in model-driven compression, and introduce Probability Matching Interval Coding (PMATIC), a model-agnostic algorithm that tolerates bounded prediction mismatch with low overhead. PMATIC works with the predicted probabilities, making it compatible as a drop-in replacement for the arithmetic encoder in model-driven compression tools. We show theoretical correctness and performance bounds for PMATIC, and validate these results on text data. These results confirm that, when paired an advanced prediction model, PMATIC is robust to prediction mismatch while achieving compression rates that out-perform standard modern compression tools.

</details>


### [96] [Implementation of Oblivious Transfer over Binary-Input AWGN Channels by Polar Codes](https://arxiv.org/abs/2601.10682)
*Pin-Hsun Lin,Hadi Aghaee,Christian Deppe,Eduard A. Jorswieck,Holger Boche*

Main category: cs.IT

TL;DR: 基于极化码在二进制输入加性高斯白噪声信道上的二选一不经意传输协议，通过极化变换自同构实现完美接收者隐私，渐近获得发送者隐私


<details>
  <summary>Details</summary>
Motivation: 在二进制输入加性高斯白噪声信道上实现安全的不经意传输协议，解决传统方案在有限码长下的隐私保护问题

Method: 使用极化码，通过极化变换的自同构连接两个解码器视图，公开随机选择编码器，在坏比特信道上注入随机性，利用信道极化结合隐私放大

Result: 在任何有限码长下实现完美接收者隐私，渐近获得发送者隐私，推导了松弛可靠性准则并评估有限码长性能，优化了可实现的有限码长OT速率

Conclusion: 基于极化码的自同构结构成功构建了二进制输入加性高斯白噪声信道上的二选一不经意传输协议，在有限码长下实现了强隐私保护并优化了传输速率

Abstract: We develop a one-out-of-two-oblivious transfer protocol over the binary-input additive white Gaussian noise channel using polar codes. The scheme uses two decoder views linked by automorphisms of the polar transform and publicly draws the encoder at random from the corresponding automorphism group. This yields perfect receiver privacy at any finite blocklength, since the public encoder distribution is independent of the receiver's choice bit. Sender privacy is obtained asymptotically via channel polarization combined with privacy amplification. Because the construction deliberately injects randomness on selected bad bit-channels, we derive a relaxed reliability criterion and evaluate finite-blocklength performance. Finally, we characterize the polar-transform automorphisms as bit-level permutations of bit-channel indices, and exploit this structure to derive and optimize an achievable finite-blocklength OT rate.

</details>


### [97] [Improved Constructions of Reed-Solomon Codes with Optimal Repair Bandwidth](https://arxiv.org/abs/2601.10685)
*Jing Qiu,Weijun Fang,Shu-Tao Xia,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 该论文改进了RS-MSR码的构造，消除了素数需满足同余条件 $p_i \equiv 1 \pmod{s}$ 的限制，从而显著降低了子分组化程度并扩展了可行参数范围。


<details>
  <summary>Details</summary>
Motivation: MDS码在分布式存储中广泛应用，但传统修复单个擦除需要下载k个节点的全部内容。MSR码通过联系d>k个帮助节点并仅从每个节点下载部分数据来最小化修复带宽。虽然Guruswami和Wootters提出了RS码的线性修复方案，但实现MSR点的RS码（RS-MSR码）的存在性一直未解决，直到Tamo、Barg和Ye的突破性构造。然而，他们的构造要求素数满足同余条件 $p_i \equiv 1 \pmod{s}$，这限制了参数选择和增加了子分组化程度。

Method: 提出了一种改进的RS-MSR码构造方法，消除了先前构造中素数需满足 $p_i \equiv 1 \pmod{s}$ 的同余条件限制。通过这种改进，构造不再需要素数满足特定的模s同余关系。

Result: 改进的构造将子分组化程度降低了 $φ(s)^n$ 倍（其中φ是欧拉函数），显著减少了存储开销。同时，扩展了RS-MSR码的可行参数范围，使更多参数组合成为可能。

Conclusion: 通过消除素数同余条件，本文显著改进了RS-MSR码的构造，降低了子分组化程度并扩展了参数选择空间，为分布式存储系统中更高效的纠删码设计提供了更好的解决方案。

Abstract: Maximum-distance-separable (MDS) codes are widely used in distributed storage, yet naive repair of a single erasure in an $[n,k]$ MDS code downloads the entire contents of $k$ nodes. Minimum Storage Regenerating (MSR) codes (Dimakis et al., 2010) minimize repair bandwidth by contacting $d>k$ helpers and downloading only a fraction of data from each. Guruswami and Wootters first proposed a linear repair scheme for Reed-Solomon (RS) codes, showing that they can be repaired with lower bandwidth than the naive approach. The existence of RS codes achieving the MSR point (RS-MSR codes) nevertheless remained open until the breakthrough construction of Tamo, Barg, and Ye, which yields RS-MSR codes with subpacketization $\ell = s \prod_{i=1}^n p_i$, where $p_i$ are distinct primes satisfying $p_i \equiv 1 \pmod{s}$ and $s=d+1-k$.
  In this paper, we present an improved construction of RS-MSR codes by eliminating the congruence condition $p_i \equiv 1 \pmod{s}$. Consequently, our construction reduces the subpacketization by a multiplicative factor of $φ(s)^n$ ( $φ(\cdot)$ is Euler's totient function) and broadens the range of feasible parameters for RS-MSR codes.

</details>


### [98] [Perfect Secret Key Generation for a class of Hypergraphical Sources](https://arxiv.org/abs/2601.10697)
*Manuj Mukherjee,Sagnik Chatterjee,Alhad Sethi*

Main category: cs.IT

TL;DR: 本文扩展了PIN模型到超图，提出了两种完美密钥生成方案：一种针对完全t-均匀超图，另一种针对3-均匀超图，通过星超图打包实现容量最优。


<details>
  <summary>Details</summary>
Motivation: Nitinawarat和Narayan提出的PIN模型基于图的生成树打包率实现完美密钥生成。本文旨在将这一框架扩展到超图模型，利用超图的组合性质设计类似的完美密钥生成方案。

Method: 1. 针对完全t-均匀超图：利用星超图打包完全t-均匀超图，设计每个星图产生$\binom{m-2}{t-2}$比特完美密钥的方案
2. 针对3-均匀超图：首先为投影为环的3-均匀星超图设计2比特完美密钥方案，然后通过星图打包和哈密顿打包扩展到一般3-均匀超图

Result: 1. 提出的完全t-均匀超图方案达到了容量最优
2. 3-均匀超图方案对于某些超图类别也达到了容量最优

Conclusion: 成功将PIN模型的完美密钥生成框架扩展到超图模型，通过利用超图的组合结构（特别是星超图打包）设计了容量最优的密钥生成方案，为超图网络中的安全通信提供了理论基础。

Abstract: Nitinawarat and Narayan proposed a perfect secret key generation scheme for the so-called \emph{pairwise independent network (PIN) model} by exploiting the combinatorial properties of the underlying graph, namely the spanning tree packing rate. This work considers a generalization of the PIN model where the underlying graph is replaced with a hypergraph, and makes progress towards designing similar perfect secret key generation schemes by exploiting the combinatorial properties of the hypergraph.
  Our contributions are two-fold. We first provide a capacity achieving scheme for a complete $t$-uniform hypergraph on $m$ vertices by leveraging a packing of the complete $t$-uniform hypergraphs by what we refer to as star hypergraphs, and designing a scheme that gives $\binom{m-2}{t-2}$ bits of perfect secret key per star graph. Our second contribution is a 2-bit perfect secret key generation scheme for 3-uniform star hypergraphs whose projections are cycles. This scheme is then extended to a perfect secret key generation scheme for generic 3-uniform hypergraphs by exploiting star graph packing of 3-uniform hypergraphs and Hamiltonian packings of graphs. The scheme is then shown to be capacity achieving for certain classes of hypergraphs.

</details>
