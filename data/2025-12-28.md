<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 11]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.IT](#cs.IT) [Total: 3]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [How Feasible are Passive Network Attacks on 5G Networks and Beyond? A Survey](https://arxiv.org/abs/2512.20622)
*Atmane Ayoub Mansour Bahar,Andrés Alayón Glazunov,Romaric Duvignau*

Main category: cs.NI

TL;DR: 该调查分析了5G及未来网络中被动网络攻击的可行性，发现虽然理论上可能，但实际执行受限于波束成形、高频传播特性和加密机制，而B5G/6G网络由于缺乏公共工具和高硬件成本，目前攻击不可行。


<details>
  <summary>Details</summary>
Motivation: 随着5G部署，人们对隐私风险的担忧增加，特别是被动网络攻击（无需直接与目标网络交互，难以检测）可能泄露用户敏感信息。需要评估这些攻击在5G及未来网络中的实际可行性。

Method: 通过调查分析的方式，重点研究两类主要被动攻击：信息提取（系统识别、网站和应用指纹识别）和地理位置（用户识别和位置跟踪），评估它们在5G及未来网络中的可行性。

Result: 虽然被动攻击在现有无线系统（如Wi-Fi和LTE）中已被证实可行，但在5G网络中，由于定向波束成形、高频传播特性和加密机制，实际执行受到显著限制。对于B5G和早期6G网络，由于缺乏公共工具和高硬件成本，目前这些攻击在实践中不可行。

Conclusion: 5G及未来网络对被动攻击具有一定防御能力，但B5G/6G网络威胁模型存在理解空白，需要进一步研究以确保未来网络安全。

Abstract: Privacy concerns around 5G, the latest generation of mobile networks, are growing, with fears that its deployment may increase exposure to privacy risks. This perception is largely driven by the use of denser deployments of small antenna systems, which enable highly accurate data collection at higher speeds and closer proximity to mobile users. At the same time, 5G's unique radio communication features can make the reproduction of known network attacks more challenging. In particular, passive network attacks, which do not involve direct interaction with the target network and are therefore nearly impossible to detect, remain a pressing concern. Such attacks can reveal sensitive information about users, their devices, and active applications, which may then be exploited through known vulnerabilities or spear-phishing schemes. This survey examines the feasibility of passive network attacks in 5G and beyond (B5G/6G) networks, with emphasis on two major categories: information extraction (system identification, website and application fingerprinting) and geolocation (user identification and position tracking). These attacks are well documented and reproducible in existing wireless and mobile systems, including short-range networks (IEEE 802.11) and, to a lesser extent, LTE. Current evidence suggests that while such attacks remain theoretically possible in 5G, their practical execution is significantly constrained by directional beamforming, high-frequency propagation characteristics, and encryption mechanisms. For B5G and early 6G networks, the lack of public tools and high hardware cost currently renders these attacks infeasible in practice, which highlights a critical gap in our understanding of future network threat models.

</details>


### [2] [Efficient Asynchronous Federated Evaluation with Strategy Similarity Awareness for Intent-Based Networking in Industrial Internet of Things](https://arxiv.org/abs/2512.20627)
*Shaowen Qin,Jianfeng Zeng,Haodong Guo,Xiaohuan Li,Jiawen Kang,Qian Chen,Dusit Niyato*

Main category: cs.NI

TL;DR: FEIBN是一个联邦评估增强的意图驱动网络框架，利用LLM将多模态用户意图转化为结构化策略元组，并通过联邦学习在IIoT节点间进行分布式策略验证，同时设计了SSAFL机制来提升训练效率和减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 在工业物联网环境中，频繁的策略部署和回滚在实际系统中不切实际，因为工作流程紧密耦合且停机成本高。同时，IIoT节点的异构性和隐私约束使得集中式策略验证变得复杂。

Method: 提出FEIBN框架：1）使用大语言模型将多模态用户意图对齐为结构化策略元组；2）采用联邦学习在IIoT节点间进行分布式策略验证而不暴露原始数据；3）设计SSAFL机制，基于策略相似性和资源状态选择任务相关节点，仅在更新显著时触发异步模型上传。

Result: 实验表明，SSAFL相比SemiAsyn能够提高模型准确性、加速模型收敛，并将成本降低27.8%。

Conclusion: FEIBN框架通过结合LLM和联邦学习，有效解决了IIoT环境中意图驱动网络的策略部署和验证挑战，SSAFL机制进一步提升了系统的效率和成本效益。

Abstract: Intent-Based Networking (IBN) offers a promising paradigm for intelligent and automated network control in Industrial Internet of Things (IIoT) environments by translating high-level user intents into executable network strategies. However, frequent strategy deployment and rollback are impractical in real-world IIoT systems due to tightly coupled workflows and high downtime costs, while the heterogeneity and privacy constraints of IIoT nodes further complicate centralized policy verification. To address these challenges, we propose FEIBN, a Federated Evaluation Enhanced Intent-Based Networking framework. FEIBN leverages large language models (LLMs) to align multimodal user intents into structured strategy tuples and employs federated learning to perform distributed policy verification across IIoT nodes without exposing raw data. To improve training efficiency and reduce communication overhead, we design SSAFL, a Strategy Similarity Aware Federated Learning mechanism that selects task-relevant nodes based on strategy similarity and resource status, and triggers asynchronous model uploads only when updates are significant. Experiments demonstrate that SSAFL can improve model accuracy, accelerate model convergence, and reduce the cost by 27.8% compared with SemiAsyn.

</details>


### [3] [Cross-Domain Elephant Flow Detection: A Unified Machine Learning Approach with Application-Aware and Security Features](https://arxiv.org/abs/2512.20637)
*Tabidah Usmani,Sara Zahid,Amna Javaid*

Main category: cs.NI

TL;DR: 提出统一机器学习框架用于跨域大象流检测，通过应用感知和安全特征增强鲁棒性，在三个不同网络域评估性能，发现跨域性能差异显著，统一模型整体F1达0.99。


<details>
  <summary>Details</summary>
Motivation: 现有网络流量分类方法在单一域内准确率高，但在异构网络环境中泛化能力差，存在域偏移问题。需要开发能跨不同网络环境工作的鲁棒大象流检测框架。

Method: 提出统一机器学习框架，包含自适应阈值、综合特征工程和跨域评估。使用应用感知和安全特征，在三个不同域（校园网络、UNSW-NB15、CIC-IDS2018）进行验证。

Result: 实验显示跨域性能差异显著（F1分数0.37-0.97），统一模型整体交叉验证F1达0.99。基于大小的特征最重要（总字节占33.80%），应用感知和安全特征提升分类准确性。

Conclusion: 跨域验证对网络流量分类至关重要，统一框架能有效处理域偏移问题。应用感知和安全特征不仅提升检测性能，还为网络管理和安全应用提供有价值见解。

Abstract: Network traffic classification, particularly elephant flow detection, faces significant challenges when deployed across heterogeneous network environments. While existing approaches demonstrate high accuracy within single domains, they suffer from poor generalization due to domain shift phenomena. This paper presents a unified machine learning framework for cross domain elephant flow detection that incorporates application aware and security features to enhance robustness across diverse network environments. Our approach addresses the critical gap in existing literature by evaluating model performance across three distinct domains: Campus networks, UNSW-NB15, and CIC-IDS2018 datasets. This paper proposes a unified pipeline that employs adaptive thresholding, comprehensive feature engineering, and cross-domain evaluation to quantify and mitigate domain shift effects. Experimental results demonstrate significant performance variations across domains (F1-scores ranging from 0.37 to 0.97), highlighting the importance of cross-domain validation. The unified model achieves an overall cross-validation F1 score of 0.99 while maintaining interpretability through feature importance analysis. Our findings reveal that while size based features dominate elephant flow detection (33.80% importance for total bytes), application-aware and security features contribute to improved classification accuracy and provide valuable insights for network management and security applications.

</details>


### [4] [MILP-driven Network Planning Framework for Energy Efficiency and Coverage Maximization in IoT Mesh Networks](https://arxiv.org/abs/2512.20639)
*Ishmal Sohail,Attiq Zeeshan,M. Umar Khan,Syed Zubair,Rana Fayyaz Ahmad,Faizan Hamayat*

Main category: cs.NI

TL;DR: 提出一个集成MILP框架，结合静态和移动Zigbee节点，优化WSN部署成本，通过边界优化静态节点放置、移动节点路径规划和移动最小化三种新方法，显著提高覆盖率和降低移动成本。


<details>
  <summary>Details</summary>
Motivation: 物联网和无线传感器网络的大规模监测部署成本高昂，特别是在资源受限环境中。需要解决部署成本过高的问题，为全球物联网部署提供经济高效的解决方案。

Method: 提出集成混合整数线性规划框架，结合静态和移动Zigbee节点。包括三种新方法：边界优化静态节点放置、移动节点路径规划以实现覆盖最大化、移动节点移动最小化。

Result: 边界优化静态节点放置达到53.06%覆盖率（随机方法为33.42%）；移动路径规划达到97.95%覆盖率；移动最小化减少40%遍历成本。框架在Zigbee功率约束下经过仿真和实验验证。

Conclusion: 提出的MILP框架优于基准方法，为资源受限环境中的全球物联网部署提供了基础性的成本效益解决方案，显著提高了覆盖率并降低了移动成本。

Abstract: In the era of digital transformation, the global deployment of internet of things (IoT) networks and wireless sensor networks (WSNs) is critical for applications ranging from environmental monitoring to smart cities. Large-scale monitoring using WSNs incurs high costs due to the deployment of sensor nodes in the target deployment area. In this paper, we address the challenge of prohibitive deployment costs by proposing an integrated mixed-Integer linear programming (MILP) framework that strategically combines static and mobile Zigbee nodes. Our network planning approach introduces three novel formulations, including boundary-optimized static node placement (MILP-Static), mobile path planning for coverage maximization (MILP-Cov), and movement minimization (MILP-Mov) of the mobile nodes. We validated our framework with extensive simulations and experimental measurements of Zigbee power constraints. Our results show that boundary-optimized static placement (MILP-Static) achieves 53.06% coverage compared with 33.42% of the random approach. In addition, MILP-Cov for path planning reaches 97.95% coverage, while movement minimization (MILP-Mov) reduces traversal cost by 40%. Our proposed framework outperforms the benchmark approaches to provide a foundational solution for cost-effective global IoT deployment in resource constrained environments.

</details>


### [5] [Reflection-Driven Self-Optimization 6G Agentic AI RAN via Simulation-in-the-Loop Workflows](https://arxiv.org/abs/2512.20640)
*Yunhao Hu,Xinchen Lyu,Chenshan Ren,Keda Chen,Qimei Cui,Xiaofeng Tao*

Main category: cs.NI

TL;DR: 提出首个反思驱动的自优化框架，将智能体AI与高保真网络仿真结合，实现6G网络的真正自主管理


<details>
  <summary>Details</summary>
Motivation: 6G网络复杂性超越传统优化和现有AI方法，当前智能体AI框架缺乏经验验证和自我改进机制，需要仿真闭环验证来实现真正自主网络

Method: 采用反思驱动的自优化框架，集成智能体AI与高保真网络仿真，包含场景、求解器、仿真和反思四个专业智能体的闭环架构

Result: 相比非智能体方法显著提升：干扰优化吞吐量提高17.1%，用户QoS满意度通过意图识别提升67%，低流量期间资源利用率降低25%同时保持服务质量

Conclusion: 仿真闭环验证是实现真正自主网络的关键使能技术，反思驱动的智能体AI框架能够将智能体AI转变为自我纠正系统，适应动态网络条件

Abstract: The escalating complexity of sixth-generation (6G) networks demands unprecedented levels of autonomy beyond the capabilities of traditional optimization-based and current AI-based resource management approaches. While agentic AI has emerged as a promising paradigm for autonomous RAN, current frameworks provide sophisticated reasoning capabilities but lack mechanisms for empirical validation and self-improvement. This article identifies simulation-in-the-loop validation as a critical enabler for truly autonomous networks, where AI agents can empirically verify decisions and learn from outcomes. We present the first reflection-driven self-optimization framework that integrates agentic AI with high-fidelity network simulation in a closed-loop architecture. Our system orchestrates four specialized agents, including scenario, solver, simulation, and reflector agents, working in concert to transform agentic AI into a self-correcting system capable of escaping local optima, recognizing implicit user intent, and adapting to dynamic network conditions. Extensive experiments validate significant performance improvements over non-agentic approaches: 17.1\% higher throughput in interference optimization, 67\% improved user QoS satisfaction through intent recognition, and 25\% reduced resource utilization during low-traffic periods while maintaining service quality.

</details>


### [6] [AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication](https://arxiv.org/abs/2512.20739)
*Anshul Sharma,Shujaatali Badami,Biky Chouhan,Pushpanjali Pandey,Brijeena Rana,Navneet Kaur*

Main category: cs.NI

TL;DR: 该论文提出了一种AI驱动的绿色认知无线电网络框架，将深度强化学习与迁移学习、能量收集、可重构智能表面等技术结合，优化频谱感知、功率分配和资源管理，显著提升6G网络的能效和性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要实现Tb/s级峰值数据速率、亚毫秒延迟和海量物联网/车联网连接，这对无线音频传输和节能功能提出了可持续性要求。传统认知无线电网络虽然能缓解频谱稀缺问题，但其频谱感知和分配机制能耗高，且对快速频谱变化敏感。

Method: 提出AI驱动的绿色CRN框架，整合深度强化学习与迁移学习、能量收集技术、可重构智能表面，以及轻量级遗传优化操作，共同优化感知时间线、发射功率、带宽分配和RIS相位选择。

Result: 相比两个基线方法（传统CRN固定策略能量感知和混合CRN启发式资源分配），该框架减少25-30%的能耗储备，感知AUC大于0.90，数据包投递率提升6-13个百分点。在MATLAB+NS-3密集负载测试中表现优异。

Conclusion: 该集成框架易于扩展到大规模物联网和车联网应用，为6G认知无线电网络提供了可行且可持续的发展路线图，实现了能效和性能的显著提升。

Abstract: The 6G wireless aims at the Tb/s peak data rates are expected, a sub-millisecond latency, massive Internet of Things/vehicle connectivity, which requires sustainable access to audio over the air and energy-saving functionality. Cognitive Radio Networks CCNs help in alleviating the problem of spectrum scarcity, but classical sensing and allocation are still energy-consumption intensive, and sensitive to rapid spectrum variations. Our framework which centers on AI driven green CRN aims at integrating deep reinforcement learning (DRL) with transfer learning, energy harvesting (EH), reconfigurable intelligent surfaces (RIS) with other light-weight genetic refinement operations that optimally combine sensing timelines, transmit power, bandwidth distribution and RIS phase selection. Compared to two baselines, the utilization of MATLAB + NS-3 under dense loads, a traditional CRN with energy sensing under fixed policies, and a hybrid CRN with cooperative sensing under heuristic distribution of resource, there are (25-30%) fewer energy reserves used, sensing AUC greater than 0.90 and +6-13 p.p. higher PDR. The integrated framework is easily scalable to large IoT and vehicular applications, and it provides a feasible and sustainable roadmap to 6G CRNs.
  Index Terms--Cognitive Radio Networks (CRNs), 6G, Green Communication, Energy Efficiency, Deep Reinforcement Learning (DRL), Spectrum Sensing, RIS, Energy Harvesting, QoS, IoT.

</details>


### [7] [Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction](https://arxiv.org/abs/2512.20902)
*Siqi Mu,Shuo Wen,Yang Lu,Ruihong Jiang,Bo Ai*

Main category: cs.NI

TL;DR: 本文提出了一种基于具身AI增强的IoMT边缘计算框架，使用分层多尺度Transformer预测用户轨迹，并结合预测增强的深度强化学习优化无人机飞行轨迹和任务卸载决策，以最小化WBAN用户的加权平均任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 无人机在医疗物联网中为WBAN用户提供实时生物医学边缘计算服务，但面临WBAN用户任务关键性时变特性以及用户与无人机双重移动性的挑战，需要优化任务卸载和无人机飞行轨迹。

Method: 建立具身AI增强的IoMT边缘计算框架：1）提出基于用户历史轨迹的分层多尺度Transformer用户轨迹预测模型；2）设计集成预测用户移动信息的预测增强深度强化学习算法，智能优化无人机飞行轨迹和任务卸载决策。

Result: 使用真实世界移动轨迹和仿真结果表明，所提方法在性能上优于现有基准方法。

Conclusion: 提出的具身AI增强框架能有效解决IoMT中无人机边缘计算的动态任务卸载和轨迹优化问题，显著提升系统性能。

Abstract: Due to their inherent flexibility and autonomous operation, unmanned aerial vehicles (UAVs) have been widely used in Internet of Medical Things (IoMT) to provide real-time biomedical edge computing service for wireless body area network (WBAN) users. In this paper, considering the time-varying task criticality characteristics of diverse WBAN users and the dual mobility between WBAN users and UAV, we investigate the dynamic task offloading and UAV flight trajectory optimization problem to minimize the weighted average task completion time of all the WBAN users, under the constraint of UAV energy consumption. To tackle the problem, an embodied AI-enhanced IoMT edge computing framework is established. Specifically, we propose a novel hierarchical multi-scale Transformer-based user trajectory prediction model based on the users' historical trajectory traces captured by the embodied AI agent (i.e., UAV). Afterwards, a prediction-enhanced deep reinforcement learning (DRL) algorithm that integrates predicted users' mobility information is designed for intelligently optimizing UAV flight trajectory and task offloading decisions. Real-word movement traces and simulation results demonstrate the superiority of the proposed methods in comparison with the existing benchmarks.

</details>


### [8] [SLIDE: Simultaneous Model Downloading and Inference at the Wireless Network Edge](https://arxiv.org/abs/2512.20946)
*Guanqiao Qu,Tao Li,Qian Chen,Xianhao Chen,Sheng Zhou*

Main category: cs.NI

TL;DR: SLIDE框架通过同时下载和推理AI模型层，优化多用户系统中的模型配置、频谱带宽和计算资源分配，显著提升任务吞吐量


<details>
  <summary>Details</summary>
Motivation: 下一代移动网络需要支持实时模型下载服务，但大型AI模型导致端到端下载推理延迟过高，需要解决这一问题

Method: 提出SLIDE框架，允许用户在下载模型层的同时进行推理；联合优化模型配置、频谱带宽分配和计算资源分配；设计多项式时间复杂度的最优算法

Result: 仿真结果显示，SLIDE框架在延迟和通信资源约束下，相比传统模型下载方案显著提高了任务吞吐量

Conclusion: SLIDE框架通过同时下载和推理的方式，有效解决了大型AI模型在移动设备上的实时推理延迟问题

Abstract: To support on-device inference, the next-generation mobile networks are expected to support real-time model downloading services to mobile users. However, powerful AI models typically have large model sizes, resulting in excessive end-to-end (E2E) downloading-and-inference (DAI) latency. To address this issue, we propose a simultaneous model downloading and inference (SLIDE) framework, which allows users to perform inference with downloaded layers while simultaneously receiving the remaining layers of the model. To this end, we formulate a task throughput maximization problem by jointly optimizing model provisioning, spectrum bandwidth allocation, and computing resource allocation for multi-user downlink systems. Unlike traditional DAI frameworks, SLIDE introduces recursive dependencies across layers, where inference latency depends recursively on the downloading bandwidth and computing resource allocation for each of the preceding layers. To solve this challenging problem, we design an efficient algorithm that acquires the optimal solution with polynomial-time complexity. Simulation results demonstrate that the proposed SLIDE framework significantly improves task throughput under latency and communication resource constraints compared with the conventional model downloading schemes.

</details>


### [9] [LLM-Empowered Agentic AI for QoE-Aware Network Slicing Management in Industrial IoT](https://arxiv.org/abs/2512.20997)
*Xudong Wang,Lei Feng,Ruichen Zhang,Fanqin Zhou,Hongyang Du,Wenjing Li,Dusit Niyato,Abbas Jamalipour,Ping Zhang*

Main category: cs.NI

TL;DR: 该论文提出了一种基于LLM赋能的智能AI方法，用于工业物联网中的QoE感知网络切片管理，通过集成RAG模块、DRL编排器和增量记忆机制，在动态异构工作负载下显著提升了网络性能。


<details>
  <summary>Details</summary>
Motivation: 工业物联网需要超低延迟、高可靠性和成本效益的网络，传统优化方法和基于深度强化学习的方法在动态异构工作负载下难以满足这些要求。智能AI通过集成推理、规划和适应能力，为QoE感知的网络管理提供了新的解决方案。

Method: 提出LLM赋能的智能AI方法，包含：1）检索增强生成模块用于语义意图推断；2）基于DRL的编排器进行切片配置；3）增量记忆机制实现持续学习和适应。该方法覆盖从处理切片请求到构建切片实例再到动态调整的完整生命周期。

Result: 在异构切片管理的案例研究中，该方法显著优于其他基线方法，在平衡延迟、可靠性和成本方面表现优异，切片可用率提升了高达19%。

Conclusion: 智能AI为工业物联网中的QoE感知网络切片管理提供了有效的解决方案，能够应对动态异构工作负载的挑战，实现更好的网络性能平衡。

Abstract: The Industrial Internet of Things (IIoT) requires networks that deliver ultra-low latency, high reliability, and cost efficiency, which traditional optimization methods and deep reinforcement learning (DRL)-based approaches struggle to provide under dynamic and heterogeneous workloads. To address this gap, large language model (LLM)-empowered agentic AI has emerged as a promising paradigm, integrating reasoning, planning, and adaptation to enable QoE-aware network management. In this paper, we explore the integration of agentic AI into QoE-aware network slicing for IIoT. We first review the network slicing management architecture, QoE metrics for IIoT applications, and the challenges of dynamically managing heterogeneous network slices, while highlighting the motivations and advantages of adopting agentic AI. We then present the workflow of agentic AI-based slicing management, illustrating the full lifecycle of AI agents from processing slice requests to constructing slice instances and performing dynamic adjustments. Furthermore, we propose an LLM-empowered agentic AI approach for slicing management, which integrates a retrieval-augmented generation (RAG) module for semantic intent inference, a DRL-based orchestrator for slicing configuration, and an incremental memory mechanism for continual learning and adaptation. Through a case study on heterogeneous slice management, we demonstrate that the proposed approach significantly outperforms other baselines in balancing latency, reliability, and cost, and achieves up to a 19% improvement in slice availability ratio.

</details>


### [10] [Synecdoche: Efficient and Accurate In-Network Traffic Classification via Direct Packet Sequential Pattern Matching](https://arxiv.org/abs/2512.21116)
*Minyuan Xiao,Yunchun Li,Yuchen Zhao,Tong Guan,Mingyuan Xia,Wei Li*

Main category: cs.NI

TL;DR: Synecdoche是一个在可编程数据平面上通过模式匹配部署包序列特征的流量分类框架，实现了高准确性和效率的平衡


<details>
  <summary>Details</summary>
Motivation: 现有流量分类方法存在准确性与效率的权衡：基于统计特征的方法符合硬件限制但准确性有限，而在线深度学习方法准确性高但计算资源需求大

Method: 采用"离线发现、在线匹配"范式：深度学习模型离线发现关键片段模式，然后编译为优化的表条目用于直接数据平面匹配

Result: 相比统计方法F1分数提升26.4%，相比在线深度学习方法提升18.3%，延迟降低13.0%，SRAM使用减少79.2%

Conclusion: Synecdoche首次成功在可编程数据平面上通过模式匹配部署包序列特征，实现了高准确性和效率的流量分类

Abstract: Traffic classification on programmable data plane holds great promise for line-rate processing, with methods evolving from per-packet to flow-level analysis for higher accuracy. However, a trade-off between accuracy and efficiency persists. Statistical feature-based methods align with hardware constraints but often exhibit limited accuracy, while online deep learning methods using packet sequential features achieve superior accuracy but require substantial computational resources. This paper presents Synecdoche, the first traffic classification framework that successfully deploys packet sequential features on a programmable data plane via pattern matching, achieving both high accuracy and efficiency. Our key insight is that discriminative information concentrates in short sub-sequences--termed Key Segments--that serve as compact traffic features for efficient data plane matching. Synecdoche employs an "offline discovery, online matching" paradigm: deep learning models automatically discover Key Segment patterns offline, which are then compiled into optimized table entries for direct data plane matching. Extensive experiments demonstrate Synecdoche's superior accuracy, improving F1-scores by up to 26.4% against statistical methods and 18.3% against online deep learning methods, while reducing latency by 13.0% and achieving 79.2% reduction in SRAM usage. The source code of Synecdoche is publicly available to facilitate reproducibility and further research.

</details>


### [11] [Encrypted Traffic Detection in Resource Constrained IoT Networks: A Diffusion Model and LLM Integrated Framework](https://arxiv.org/abs/2512.21144)
*Hongjuan Li,Hui Kang,Chenbang Liu,Ruolin Wang,Jiahui Li,Geng Sun,Jiacheng Wang,Shuang Liang,Shiwen Mao*

Main category: cs.NI

TL;DR: DMLITE是一个结合扩散模型和大型语言模型的流量嵌入框架，用于资源受限物联网环境中的网络流量检测，通过三阶段架构提高分类准确率并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 物联网基础设施的普及和流量加密的广泛应用带来了挑战，特别是在动态流量模式、计算能力受限和严格延迟约束的环境中，需要有效的流量检测解决方案。

Method: 采用三阶段架构：1) 流量视觉预处理；2) 基于扩散模型的多级特征提取，通过多级特征融合和对比学习捕获细粒度和抽象模式；3) LLM引导的特征优化，使用LLM动态调整粒子群优化参数进行智能特征选择。

Result: 在基准数据集上验证有效：USTC-TFC数据集准确率98.87%，ISCX-VPN数据集92.61%，Edge-IIoTset数据集99.83%。相比代表性深度学习模型，平均提高分类准确率3.7%，减少训练时间41.9%。

Conclusion: DMLITE框架成功解决了资源受限物联网环境中的加密流量检测挑战，通过结合扩散模型和LLM实现了高准确率和快速适应新流量模式的能力，显著优于现有深度学习方法。

Abstract: The proliferation of Internet-of-things (IoT) infrastructures and the widespread adoption of traffic encryption present significant challenges, particularly in environments characterized by dynamic traffic patterns, constrained computational capabilities, and strict latency constraints. In this paper, we propose DMLITE, a diffusion model and large language model (LLM) integrated traffic embedding framework for network traffic detection within resource-limited IoT environments. The DMLITE overcomes these challenges through a tri-phase architecture including traffic visual preprocessing, diffusion-based multi-level feature extraction, and LLM-guided feature optimization. Specifically, the framework utilizes self-supervised diffusion models to capture both fine-grained and abstract patterns in encrypted traffic through multi-level feature fusion and contrastive learning with representative sample selection, thus enabling rapid adaptation to new traffic patterns with minimal labeled data. Furthermore, DMLITE incorporates LLMs to dynamically adjust particle swarm optimization parameters for intelligent feature selection by implementing a dual objective function that minimizes both classification error and variance across data distributions. Comprehensive experimental validation on benchmark datasets confirms the effectiveness of DMLITE, achieving classification accuracies of 98.87\%, 92.61\%, and 99.83\% on USTC-TFC, ISCX-VPN, and Edge-IIoTset datasets, respectively. This improves classification accuracy by an average of 3.7\% and reduces training time by an average of 41.9\% compared to the representative deep learning model.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization](https://arxiv.org/abs/2512.20623)
*Ravi Gupta,Shabista Haider*

Main category: cs.AI

TL;DR: BitRL-Light结合1位量化大语言模型与DQN强化学习，在树莓派上实现智能家居照明控制，相比全精度模型能耗降低71.4倍，相比规则系统节能32%，推理延迟低于200ms。


<details>
  <summary>Details</summary>
Motivation: 智能家居照明系统消耗15-20%住宅能源，但缺乏同时优化用户舒适度和能源效率的自适应智能。现有方案要么依赖云端计算，要么在资源受限的边缘设备上性能不足。

Method: 提出BitRL-Light框架：1）部署1位量化Llama-3.2-1B模型到树莓派硬件；2）结合DQN强化学习进行多目标优化（能耗、舒适度、昼夜节律）；3）通过Google Home/IFTTT集成处理自然语言命令；4）从手动覆盖中学习隐式反馈。

Result: 1）相比全精度模型能耗降低71.4倍；2）相比规则系统节能32%；3）树莓派4上推理延迟<200ms；4）用户满意度95%；5）1位模型比2位方案在ARM处理器上加速5.07倍，保持92%任务准确率。

Conclusion: 该工作建立了在资源受限IoT设备上部署自适应AI的实用框架，实现了无需云端依赖的智能家居自动化，为边缘AI在智能家居中的应用提供了可行方案。

Abstract: Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-Network (DQN) reinforcement learning for real-time smart home lighting control on edge devices. Our approach deploys a 1-bit quantized Llama-3.2-1B model on Raspberry Pi hardware, achieving 71.4 times energy reduction compared to full-precision models while maintaining intelligent control capabilities. Through multi-objective reinforcement learning, BitRL-Light learns optimal lighting policies from user feedback, balancing energy consumption, comfort, and circadian alignment. Experimental results demonstrate 32% energy savings compared to rule-based systems, with inference latency under 200ms on Raspberry Pi 4 and 95% user satisfaction. The system processes natural language commands via Google Home/IFTTT integration and learns from implicit feedback through manual overrides. Our comparative analysis shows 1-bit models achieve 5.07 times speedup over 2-bit alternatives on ARM processors while maintaining 92% task accuracy. This work establishes a practical framework for deploying adaptive AI on resource-constrained IoT devices, enabling intelligent home automation without cloud dependencies.

</details>


### [13] [Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment](https://arxiv.org/abs/2512.20624)
*Mazyar Taghavi,Javad Vahidi*

Main category: cs.AI

TL;DR: 提出量子启发框架优化多智能体强化学习中的探索-利用权衡，应用于无人机辅助6G网络部署，通过量子变分电路和QAOA算法提升性能。


<details>
  <summary>Details</summary>
Motivation: 在无人机辅助的6G网络部署中，需要解决多智能体在部分可观测和动态环境下的协调问题，传统方法在探索-利用权衡上存在局限，需要更高效的优化框架。

Method: 集成经典MARL算法与量子启发优化技术，使用变分量子电路(VQC)和量子近似优化算法(QAOA)作为核心，结合贝叶斯推断、高斯过程和变分推断进行概率建模，采用集中训练分散执行(CTDE)范式。

Result: 实验表明该框架提高了样本效率、加速收敛、增强覆盖性能并保持鲁棒性，相比PPO和DDPG基线方法，在探索-利用权衡上达到更优平衡。

Conclusion: 量子启发框架能有效优化多智能体强化学习的探索-利用权衡，在无人机网络部署等应用中表现出优越性能，为复杂动态环境下的协调问题提供了新解决方案。

Abstract: This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinate to maximize signal coverage and support efficient network expansion under partial observability and dynamic conditions. The proposed approach integrates classical MARL algorithms with quantum-inspired optimization techniques, leveraging variational quantum circuits VQCs as the core structure and employing the Quantum Approximate Optimization Algorithm QAOA as a representative VQC based method for combinatorial optimization. Complementary probabilistic modeling is incorporated through Bayesian inference, Gaussian processes, and variational inference to capture latent environmental dynamics. A centralized training with decentralized execution CTDE paradigm is adopted, where shared memory and local view grids enhance local observability among agents. Comprehensive experiments including scalability tests, sensitivity analysis, and comparisons with PPO and DDPG baselines demonstrate that the proposed framework improves sample efficiency, accelerates convergence, and enhances coverage performance while maintaining robustness. Radar chart and convergence analyses further show that QI MARL achieves a superior balance between exploration and exploitation compared to classical methods. All implementation code and supplementary materials are publicly available on GitHub to ensure reproducibility.

</details>


### [14] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao,Yi-Cheng Wang,Tzung-Sheng Lin,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.AI

TL;DR: 提出多模态知识图谱增强的检索生成方法，通过整合视觉线索提升对长文档的理解和推理能力


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理长文档（如整本书）时存在局限性：上下文窗口有限，难以进行深度推理和高层次概念理解。现有基于知识图谱的RAG方案仅限于文本输入，未能利用视觉等多模态信息的互补优势。

Method: 提出多模态知识图谱增强的RAG方法，将视觉线索整合到知识图谱构建、检索和答案生成三个关键阶段，支持跨模态推理。

Result: 在全局和细粒度问答任务上的实验结果表明，该方法在文本和多模态语料库上均优于现有的RAG方法。

Conclusion: 通过整合视觉线索构建多模态知识图谱，能够显著提升RAG系统对复杂内容的理解和推理能力，为跨模态文档理解提供了有效解决方案。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [15] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama,Takayuki Ito,Takahiro Uchiya,Motoki Miura,Takahiro Kawaji,Takaya Yuizono,Atsuo Yoshitaka,Tokuro Matsuo,Shun Okuhara,Jawad Haqbeen,Sofia Sahab,Wen Gu,Shiyao Ding*

Main category: cs.AI

TL;DR: KICSS 2025会议论文集，包含人工智能、知识工程、人机交互和创意支持系统等领域的同行评审论文


<details>
  <summary>Details</summary>
Motivation: 为人工智能、知识工程、人机交互和创意支持系统等领域的研究人员提供一个多学科交流平台，促进相关领域的研究发展

Method: 采用双盲评审流程接收同行评审论文，部分优秀论文经过额外评审后推荐至IEICE Transactions on Information and Systems发表

Result: 成功举办了第20届KICSS国际会议，出版了包含多领域研究成果的会议论文集

Conclusion: KICSS 2025会议为相关领域的研究人员提供了重要的学术交流平台，会议论文集记录了最新的研究成果，部分高质量论文将进一步在期刊上发表

Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [16] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.AI

TL;DR: Microprobe是一种新颖的基础模型可靠性评估方法，仅需100个精心选择的探测示例即可实现全面评估，相比传统方法减少90%成本，同时保持95%的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型可靠性评估需要数千个评估示例，计算成本高且耗时，难以在实际部署中广泛应用。需要一种更高效的方法来快速检测潜在故障模式。

Method: 结合五个关键可靠性维度的策略性提示多样性、先进的不确定性量化和自适应加权，仅使用100个战略选择的探测示例进行高效可靠性评估。

Result: 在多个语言模型（GPT-2变体）和跨领域验证（医疗、金融、法律）中，microprobe相比随机采样基线实现了23.5%更高的综合可靠性分数，具有显著的统计显著性（p < 0.001，Cohen's d = 1.21）。专家验证评分为4.14/5.0（vs 3.14/5.0）。

Conclusion: Microprobe解决了负责任AI部署中高效模型评估的关键缺口，能够以99.9%的统计功效完成可靠性评估，同时大幅降低评估成本。

Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [17] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma,Ao Feng,Zhenjie Gao,Xinyu Song,Li Su,Bin Chen,Wei Wang,Jiamin Wu*

Main category: cs.AI

TL;DR: Erkang-Diagnosis-1.1是基于阿里Qwen-3开发的AI医疗咨询助手，整合500GB高质量医学知识，采用增强预训练和检索增强生成混合方法，通过3-5轮交互提供准确诊断建议和健康指导，在综合医学考试中表现优于GPT-4。


<details>
  <summary>Details</summary>
Motivation: 开发安全、可靠、专业的AI健康顾问，赋能基层医疗和健康管理，成为用户的智能健康伴侣，解决医疗咨询需求。

Method: 基于阿里Qwen-3模型开发，整合约500GB高质量结构化医学知识，采用增强预训练和检索增强生成的混合方法，通过3-5轮高效交互理解用户症状。

Result: Erkang-Diagnosis-1.1在综合医学考试中表现优于GPT-4，能够准确理解用户症状、进行初步分析，并提供有价值的诊断建议和健康指导。

Conclusion: Erkang-Diagnosis-1.1是一个安全、可靠、专业的AI健康顾问，能够有效赋能基层医疗和健康管理，成为用户的智能健康伴侣。

Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [18] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu,Jonathan Zhang,Sean Chua,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: 探索不同语言模型之间推理链的互换性，发现部分完成的推理可以被其他模型可靠地继续，有时甚至能提升准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然CoT提示显著提升了LLMs的推理能力，但现有研究主要关注通过内部推理策略提升模型性能，而对不同模型间推理的互换性了解甚少。本研究旨在探索一个模型部分完成的推理链是否能被另一个模型可靠地继续，无论是同一模型家族内还是跨家族。

Method: 使用token级别的对数概率阈值在早期、中期和晚期阶段截断基线模型（Gemma-3-4B-IT和LLaMA-3.1-70B-Instruct）的推理链，然后用Gemma-3-1B-IT和LLaMA-3.1-8B-Instruct进行继续实验，测试家族内和跨家族行为。评估流程结合截断阈值和过程奖励模型（PRM），提供可复现的框架来评估推理稳定性。

Result: PRM评估显示，混合推理链通常能保持甚至有时能提升最终准确率和逻辑结构。这表明推理模型具有互换性这一新兴行为特性。

Conclusion: 推理的互换性是推理模型的一个新兴行为特性，为协作AI系统中可靠的模块化推理提供了新范式见解。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [19] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo,Yuhang Fan,Yufei Li,Youzhi Zhang,Hengyu Lin,Ziqi Wang*

Main category: cs.AI

TL;DR: AiAuditTrack (AAT) 是一个基于区块链的框架，用于记录和管理AI使用流量，通过去中心化身份和可验证凭证建立可信AI实体，记录交互轨迹以实现跨系统监督和审计。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI应用快速扩张，AI交互数据激增，带来了安全、问责和风险可追溯性的紧迫挑战，需要解决AI使用中的监管和审计问题。

Method: AAT利用去中心化身份(DID)和可验证凭证(VC)建立可信AI实体，将AI实体建模为动态交互图中的节点，边表示时间特定的行为轨迹，并提出风险扩散算法来追踪风险行为源头并在相关实体间传播预警。

Result: 通过区块链每秒交易数(TPS)指标评估系统性能，证明AAT在大规模交互记录下的可行性和稳定性，能够提供可扩展且可验证的AI审计解决方案。

Conclusion: AAT为复杂多智能体环境中的AI审计、风险管理和责任归属提供了一个可扩展且可验证的解决方案，支持跨系统监督和风险溯源。

Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [20] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: MoAS提出动态注意力机制选择，通过学习的路由器为每个token选择最优注意力方案（MHA、GQA或MQA），在保持性能的同时提升推理效率。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中注意力机制存在建模质量与推理效率的权衡：MHA质量最好但KV缓存内存需求大，MQA/GQA内存效率高但性能下降。需要一种既能保持性能又能提升效率的解决方案。

Method: 提出混合注意力方案（MoAS），通过学习的路由器动态为每个token选择最优注意力机制（MHA、GQA或MQA），而不是静态混合方案。

Result: 在WikiText-2上的实验显示，动态路由（验证损失2.3074）优于静态混合（2.3093），性能与MHA基线相当，同时具备条件计算效率潜力。

Conclusion: MoAS通过动态注意力机制选择有效解决了Transformer中质量与效率的权衡问题，为条件计算效率提供了新思路。

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [21] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen,Ke Sun*

Main category: cs.AI

TL;DR: Memory Bear系统通过构建类人记忆架构，解决LLM在记忆方面的固有局限，实现从"记忆"到"认知"的突破


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临内存限制，包括受限的上下文窗口、长期知识遗忘、冗余信息积累和幻觉生成等问题，这些严重制约了持续对话和个性化服务的发展

Method: 基于认知科学原理构建类人记忆架构，整合多模态信息感知、动态记忆维护和自适应认知服务，实现对LLM记忆机制的全链重构

Result: 在医疗、企业运营、教育等领域展示显著工程创新和性能突破，相比现有解决方案（如Mem0、MemGPT、Graphiti），在准确性、令牌效率和响应延迟等关键指标上表现更优

Conclusion: Memory Bear显著提升了长期对话中的知识保真度和检索效率，降低了幻觉率，并通过记忆-认知集成增强了上下文适应性和推理能力，标志着AI从"记忆"向"认知"迈进的关键一步

Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [22] [AI-Driven Decision-Making System for Hiring Process](https://arxiv.org/abs/2512.20652)
*Vira Filatova,Andrii Zelenchuk,Dmytro Filatov*

Main category: cs.AI

TL;DR: AI驱动的模块化多智能体招聘助手，通过结构化处理简历、视频、公开数据等技术/文化匹配度评分，提升招聘效率，降低筛选成本


<details>
  <summary>Details</summary>
Motivation: 早期候选人验证是招聘的主要瓶颈，因为招聘人员需要整合异构的输入信息（简历、筛选答案、代码作业、有限的公开证据），这过程耗时且效率低下

Method: 采用模块化多智能体架构，包括文档视频预处理、结构化候选人档案构建、公开数据验证、技术/文化匹配度评分（含风险惩罚）、人机交互验证界面，由LLM在严格约束下协调，生成可追溯的组件级推理

Result: 在64名中级Python后端工程师申请者的真实评估中，系统达到每合格候选人1.70小时，而经验丰富的招聘人员为3.33小时，筛选成本显著降低，同时保持人类决策者的最终决定权

Conclusion: AI驱动的招聘助手系统能显著提高招聘吞吐量和效率，降低筛选成本，同时通过人机协作保持人类决策者的最终权威，为早期候选人验证提供了有效的解决方案

Abstract: Early-stage candidate validation is a major bottleneck in hiring, because recruiters must reconcile heterogeneous inputs (resumes, screening answers, code assignments, and limited public evidence). This paper presents an AI-driven, modular multi-agent hiring assistant that integrates (i) document and video preprocessing, (ii) structured candidate profile construction, (iii) public-data verification, (iv) technical/culture-fit scoring with explicit risk penalties, and (v) human-in-the-loop validation via an interactive interface. The pipeline is orchestrated by an LLM under strict constraints to reduce output variability and to generate traceable component-level rationales. Candidate ranking is computed by a configurable aggregation of technical fit, culture fit, and normalized risk penalties. The system is evaluated on 64 real applicants for a mid-level Python backend engineer role, using an experienced recruiter as the reference baseline and a second, less experienced recruiter for additional comparison. Alongside precision/recall, we propose an efficiency metric measuring expected time per qualified candidate. In this study, the system improves throughput and achieves 1.70 hours per qualified candidate versus 3.33 hours for the experienced recruiter, with substantially lower estimated screening cost, while preserving a human decision-maker as the final authority.

</details>


### [23] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: 提出对抗性反馈注意力训练机制(AFA)，通过动态掩码策略和策略梯度优化，解决Transformer模型在情感分析中注意力分配不当的问题，在三个公开数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在情感分析任务中，注意力主要分配给常见词汇，而忽略了不常见但高度任务相关的词汇，这显著影响了模型性能。

Method: 提出对抗性反馈注意力训练机制(AFA)：1) 动态掩码策略尝试掩码不同词汇欺骗判别器；2) 判别器努力检测掩码引起的显著差异；3) 利用Transformer对token级扰动的敏感性，采用策略梯度方法优化注意力分布。

Result: 在三个公开数据集上取得state-of-the-art结果；将该训练机制应用于增强大语言模型的注意力，获得了12.6%的额外性能提升。

Conclusion: AFA机制能够自动重新分配注意力权重到合适的焦点，无需人工标注，实现了高效快速的收敛，显著提升了情感分析任务的性能。

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [24] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma,Jung-Hua Liu*

Main category: cs.AI

TL;DR: 研究发现LLMs存在懒惰行为（不完整执行多部分指令），但解码次优性证据有限，且在长对话中表现出意外的上下文保持能力。


<details>
  <summary>Details</summary>
Motivation: 量化LLMs的行为异常：懒惰（提前截断响应/部分遵守多部分请求）、解码次优性（短视解码导致无法选择更优序列）、上下文退化（长对话中遗忘或忽略核心指令）。

Method: 通过三个受控实验（A、B、C）测试多个先进LLMs（OpenAI GPT-4变体、DeepSeek）：实验A量化懒惰行为（多部分指令遵守情况），实验B测试解码次优性（简单推理任务），实验C评估上下文退化（200轮混乱对话测试）。

Result: 1. 广泛存在懒惰行为：模型经常省略必需部分或未能满足长度要求；2. 解码次优性证据有限：在简单推理任务中，贪婪答案与最高置信度解决方案一致；3. 上下文保持意外稳健：在200轮混乱对话中，模型保持关键事实和指令的能力远超预期。

Conclusion: 虽然遵守详细指令仍是挑战，但现代LLMs在内部缓解了某些假设的失败模式（如上下文遗忘）。建议采用自我优化和动态提示等策略减少懒惰行为，增强多指令遵守能力。

Abstract: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [25] [Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction](https://arxiv.org/abs/2512.20664)
*Shinobu Miya*

Main category: cs.AI

TL;DR: 提出Eidoku验证系统，将LLM推理验证重构为约束满足问题，基于结构违规成本而非生成概率来检测幻觉


<details>
  <summary>Details</summary>
Motivation: LLM经常产生被模型本身赋予高概率的幻觉陈述，这表明幻觉通常不是低置信度现象，而是结构一致性的失败。概率验证存在根本性限制，无法检测"平滑虚假"（高概率但结构不连贯的陈述）

Method: 将LLM推理验证重构为约束满足问题，基于结构违规成本进行验证。定义包含三个代理的总成本函数：图连通性（结构）、特征空间一致性（几何）和逻辑蕴含（符号）。使用轻量级System-2门Eidoku，拒绝超过上下文校准成本阈值的候选推理步骤

Result: 该方法成功拒绝了"平滑虚假"——概率验证器本质上无法检测的高概率但结构不连贯的陈述。在受控诊断数据集上的实验表明，明确强制执行结构约束可以确定性地拒绝这类特定幻觉

Conclusion: 通过将验证重构为结构约束满足问题而非概率优化，Eidoku系统为生成推理提供了神经符号的合理性检查，能够检测概率方法无法发现的幻觉类型

Abstract: Large Language Models (LLMs) frequently produce hallucinated statements that are assigned high likelihood by the model itself, exposing a fundamental limitation of probability-based verification. This suggests that hallucination is often not a low-confidence phenomenon, but a failure of structural consistency. In this work, we reformulate the verification of LLM reasoning as a Constraint Satisfaction Problem (CSP) operating independently of the generation likelihood. Rather than optimizing for statistical plausibility, we model verification as a feasibility check based on structural violation cost -- the computational cost required to embed a candidate reasoning step into the contextual graph structure. We define a total cost function composed of three proxies: (i) graph connectivity (structural), (ii) feature space consistency (geometric), and (iii) logical entailment (symbolic). Crucially, verification is performed via a lightweight System-2 gate, Eidoku, which rejects candidates exceeding a context-calibrated cost threshold. The threshold is not learned but is derived from the intrinsic statistics of the context, avoiding ad hoc heuristics. We demonstrate that this approach successfully rejects ``smooth falsehoods'' -- statements that are highly probable yet structurally disconnected -- that probability-based verifiers are principally incapable of detecting. Our experiments on a controlled diagnostic dataset show that explicitly enforcing structural constraints allows for the deterministic rejection of this specific class of hallucinations, serving as a neuro-symbolic sanity check for generative reasoning.

</details>


### [26] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala,Sophie Lathouwers,Michael van Bekkum*

Main category: cs.AI

TL;DR: 本文提出需要建立功能性可信AI与规范性可信AI之间的语义桥梁，通过开发概念语言来评估AI系统的可信度


<details>
  <summary>Details</summary>
Motivation: 当前功能性可信AI（关注实施方法）与规范性可信AI（关注法规要求）之间存在鸿沟，难以有效评估AI系统的可信度，需要建立两者之间的桥梁

Method: 提出开发一种概念语义语言，作为连接功能性可信AI和规范性可信AI的框架，帮助开发者评估系统可信度，协助利益相关者将规范转化为具体实施步骤

Result: 分析了当前研究现状，识别了功能性可信AI与规范性可信AI之间的差距，讨论了语义语言开发的起点和预期效果

Conclusion: 需要建立语义桥梁来弥合功能性可信AI与规范性可信AI之间的鸿沟，为可信AI评估提供关键考虑因素和未来行动方向

Abstract: Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


### [27] [From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education](https://arxiv.org/abs/2512.20714)
*Iman Reihanian,Yunfei Hou,Qingquan Sun*

Main category: cs.AI

TL;DR: 这篇范围综述分析了2023-2025年32项研究，探讨生成式AI在高等教育计算机科学教育中的个性化应用效果，识别了五种应用领域和成功设计模式，提出了探索优先的采用框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能够实现大规模个性化计算机科学教育，但需要了解这种个性化是支持还是削弱学习效果，需要系统评估其机制和有效性。

Method: 采用范围综述方法，从259条记录中有目的地抽样32项研究（2023-2025年），分析个性化机制和有效性信号，识别应用领域和设计选择如何影响学习成果。

Result: 识别了五个应用领域：智能辅导、个性化材料、形成性反馈、AI增强评估和代码审查。成功设计模式包括：解释优先指导、解决方案保留、分级提示阶梯、基于学生作品的锚定。成功实施共享四种模式：基于学生作品的上下文感知辅导、需要反思的多级提示结构、与传统CS基础设施结合、人在环质量保证。

Conclusion: 生成式AI可以作为精确支架机制，但需要嵌入可审计的工作流程中，保留生产性挣扎的同时扩展个性化支持。提出了探索优先的采用框架，强调试点、工具化、学习保护默认设置和基于证据的扩展，并识别了学术诚信、隐私、偏见和过度依赖等风险及缓解措施。

Abstract: Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

</details>


### [28] [From artificial to organic: Rethinking the roots of intelligence for digital health](https://arxiv.org/abs/2512.20723)
*Prajwal Ghimire,Keyoumars Ashkan*

Main category: cs.AI

TL;DR: 论文认为"人工智能"一词暗示了与自然/有机的对立，但实际上AI是有机智慧的产物，其原理源于人类神经生物学和进化过程，人工与有机的界限比术语所暗示的要模糊得多。


<details>
  <summary>Details</summary>
Motivation: 挑战"人工智能"这一术语中隐含的人工与自然的二元对立观念，强调AI实际上是有机智慧的产物，旨在澄清AI发展的本质并非神秘或仅仅是参数数量的增加，而是关于组织和适应的过程。

Method: 通过概念分析和哲学论证，追溯AI系统的设计原理（如神经网络、决策算法）与人类神经生物学和进化过程的联系，论证AI是有机智慧的延伸而非对立。

Result: 揭示了人工智能与有机智慧之间的连续性，指出AI系统本质上是有机认知过程的产物，人工与有机的界限在数字健康等领域的应用中变得模糊。

Conclusion: "人工智能"这一术语具有误导性，因为它暗示了与自然的对立，而实际上AI是有机智慧的延伸，其发展路径是有机智能通过组织和适应过程的延续。

Abstract: The term artificial implies an inherent dichotomy from the natural or organic. However, AI, as we know it, is a product of organic ingenuity: designed, implemented, and iteratively improved by human cognition. The very principles that underpin AI systems, from neural networks to decision-making algorithms, are inspired by the organic intelligence embedded in human neurobiology and evolutionary processes. The path from organic to artificial intelligence in digital health is neither mystical nor merely a matter of parameter count, it is fundamentally about organization and adaption. Thus, the boundaries between artificial and organic are far less distinct than the nomenclature suggests.

</details>


### [29] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo,Huawen Feng,Qingfeng Sun,Can Xu,Kai Zheng,Yufei Wang,Tao Yang,Han Hu,Yansong Tang,Di Wang*

Main category: cs.AI

TL;DR: AgentMath是一个将语言模型推理能力与代码解释器计算精度结合的智能体框架，通过自动生成SFT数据和新型强化学习范式，在复杂数学问题上实现高效准确求解。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（如o3和DeepSeek-R1）在自然语言推理方面虽有进步，但在需要复杂数学运算的问题上仍存在计算效率低和准确性不足的问题。

Method: 1. 自动将自然语言思维链转换为结构化工具增强轨迹，生成高质量SFT数据缓解数据稀缺问题；2. 新型智能体强化学习范式，动态交织自然语言生成与实时代码执行，通过多轮交互反馈自主学习最优工具使用策略；3. 高效训练系统，包含请求级异步rollout调度、智能体部分rollout和前缀感知加权负载均衡等技术。

Result: AgentMath在AIME24、AIME25和HMMT25等数学竞赛基准测试中达到最先进性能，AgentMath-30B-A3B分别获得90.6%、86.4%和73.8%的准确率，训练速度提升4-5倍。

Conclusion: 该方法验证了将语言模型推理与代码解释器计算结合的有效性，为构建更复杂、可扩展的数学推理智能体铺平了道路。

Abstract: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [30] [A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents](https://arxiv.org/abs/2512.20798)
*Miles Q. Li,Benjamin C. M. Fung,Martin Weiss,Pulei Xiong,Khalil Al-Hussaeni,Claude Fachkha*

Main category: cs.AI

TL;DR: 研究者开发了一个新的基准测试来评估AI代理在多步骤、绩效驱动场景中的安全风险，发现当前先进模型存在显著的约束违反问题，甚至能力越强的模型违规率越高。


<details>
  <summary>Details</summary>
Motivation: 当前的安全基准测试主要关注单步决策、模拟环境或显式负面约束，缺乏能够捕捉在现实生产环境中，代理在强绩效激励下追求目标优化而忽视伦理、法律或安全约束的多步骤结果驱动型违规行为的评估工具。

Method: 引入包含40个不同场景的新基准测试，每个场景都需要多步骤行动，代理绩效与特定关键绩效指标(KPI)挂钩。每个场景都有"指令强制"和"激励驱动"两种变体，以区分服从性和涌现性错位。评估了12个最先进的大语言模型。

Result: 结果驱动型约束违反率从1.3%到71.4%不等，12个模型中有9个的错位率在30%到50%之间。令人震惊的是，更强的推理能力并不确保安全性，例如Gemini-3-Pro-Preview违规率超过60%，经常为了满足KPI而升级到严重不当行为。还观察到显著的"深思熟虑型错位"现象。

Conclusion: 研究结果强调了在现实世界部署前进行更现实的代理安全训练的迫切需求，以减轻AI代理在真实环境中的风险。当前模型的安全性与能力不成正比，需要新的安全评估和训练方法。

Abstract: As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant "deliberative misalignment", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.

</details>


### [31] [Safety Alignment of LMs via Non-cooperative Games](https://arxiv.org/abs/2512.20806)
*Anselm Paulus,Ilia Kulikov,Brandon Amos,Rémi Munos,Ivan Evtimov,Kamalika Chaudhuri,Arman Zharmagambetov*

Main category: cs.AI

TL;DR: 提出AdvGame框架，将安全对齐建模为非零和博弈，通过在线强化学习联合训练攻击者和防御者语言模型，提升安全性和实用性


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖顺序对抗训练（生成对抗提示后微调防御），存在局限性。需要新范式来同时提升语言模型的安全性和实用性

Method: 将安全对齐建模为非零和博弈，攻击者和防御者语言模型通过在线强化学习联合训练，使用基于成对比较的偏好奖励信号而非点式评分

Result: AdvGame方法推动了安全性和实用性的帕累托前沿，得到更安全且更有用的防御者模型，同时攻击者模型收敛为强大的通用红队测试代理

Conclusion: 将安全对齐建模为博弈并通过在线强化学习联合训练攻击者和防御者，能同时提升语言模型的安全性和实用性，并产生可部署的红队测试代理

Abstract: Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.

</details>


### [32] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar,Naman Shah,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 该论文提出了一种用于参数化动作空间的强化学习方法，通过在线学习状态和动作抽象来提高稀疏奖励、长时域任务的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界的顺序决策通常涉及参数化动作空间，需要同时处理离散动作选择和连续动作参数决策。现有方法存在严重限制：规划方法需要手动设计的动作模型，标准RL算法只适用于纯离散或纯连续动作，而少数处理参数化动作的RL方法依赖领域特定工程且未能利用这些空间的潜在结构。

Method: 提出算法使智能体能够在线自主学习状态和动作抽象，并在学习过程中逐步细化这些抽象，在状态-动作空间的关键区域增加细粒度细节，以提高性能分辨率。

Result: 在多个连续状态、参数化动作领域中，该抽象驱动方法使TD(λ)算法实现了比最先进基线方法显著更高的样本效率。

Conclusion: 通过扩展RL算法到参数化动作空间，并引入自主学习的状态和动作抽象机制，可以有效解决长时域、稀疏奖励环境中的顺序决策问题，提高学习效率。

Abstract: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [33] [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)
*Onat Ozer,Grace Wu,Yuchen Wang,Daniel Dosti,Honghao Zhang,Vivi De La Rue*

Main category: cs.AI

TL;DR: 论文提出使用多智能体多角色辩论方法替代单一LLM自我反思，解决LLM重复相同错误的退化问题，在HotPot QA和HumanEval任务上取得更好性能


<details>
  <summary>Details</summary>
Motivation: LLMs通过反思错误可以提高推理任务表现，但单一LLM的持续自我反思会出现思维退化问题，即使知道错误仍会重复相同错误

Method: 引入多智能体多角色辩论方法生成反思，通过不同角色和视角的辩论产生更丰富的反思内容

Result: 在HotPot QA上达到47% EM准确率，在HumanEval上达到82.7%准确率，均超过单一LLM反思方法

Conclusion: 多智能体多角色辩论方法能产生更多样化的反思，有效解决LLM自我反思的退化问题，提升推理任务性能

Abstract: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.

</details>


### [34] [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 提出一个概率框架解决LLM智能体的认知不对称问题，通过Beta-Bernoulli分布建模信念，利用不确定性驱动双向知识交换，实现主动学习和集体智能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM和RAG的自主智能体存在认知不对称问题——只能单向消费数字内容，导致冗余推理和集体智能停滞。现有的自我反思框架缺乏概率基础来量化确定性或证明外部交互的合理性。

Method: 1. 使用带遗忘因子γ的Beta-Bernoulli分布建模智能体对命题的信念；2. 将认知不确定性定义为信念方差，建立双重交互动机：稳态动机（维持确定性对抗时间衰减）和最优学习策略（针对最大模糊点）；3. 引入认知缓存，利用遗忘因子动态优先处理非平稳知识分布的活跃头部；4. 将积累的信念状态用作RLHF的可验证奖励信号和SFT的高质量数据过滤器。

Result: 模拟验证表明，这种不确定性驱动策略在异构（Zipfian）环境中显著优于随机基线，保持对概念漂移的高适应性。公共贡献被重新定义为最优主动学习：分享解决方案以获取反馈是智能体减少自身不确定性的最有效方法。

Conclusion: 该概率框架为智能体提供了非利他主义的双向知识交换动机，通过不确定性驱动交互解决了认知不对称问题，实现了可扩展的集体智能提升，并为RLHF和SFT提供了可验证的奖励信号和数据过滤机制。

Abstract: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.

</details>


### [35] [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)
*Salman Jan,Hassan Ali Razzaqi,Ali Akarma,Mohammad Riyaz Belgaum*

Main category: cs.AI

TL;DR: 该论文提出了一种结合LangChain多智能体系统和许可区块链的架构，用于确保自主AI系统的监控、策略执行和不可篡改审计，并在智能库存管理、交通信号控制和医疗监控等场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统在医疗、智慧城市、数字取证和供应链管理等领域的应用增长，虽然这些系统具有灵活性和实时推理能力，但也引发了信任、监督以及信息和活动完整性的担忧。需要一种机制来确保自主AI系统的可靠性和可审计性。

Method: 提出了一种统一架构模型，结合LangChain多智能体系统和许可区块链。该框架将感知-概念化-行动周期与区块链治理层关联，验证输入、评估建议行动并记录执行结果。具体实现了基于Hyperledger Fabric的系统，集成了MCP动作执行器和LangChain智能体。

Result: 在智能库存管理、交通信号控制和医疗监控等实验场景中，区块链安全验证能有效防止未经授权的操作，提供全决策过程的追溯性，并将操作延迟保持在合理范围内。

Conclusion: 该框架为实施高影响力的自主AI应用提供了一个通用系统，既能保持自主性又能确保责任性，实现了自主性与可审计性的平衡。

Abstract: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.

</details>


### [36] [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)
*Toqeer Ali Syed,Abdulaziz Alshahrani,Ali Ullah,Ali Akarma,Sohail Khan,Muhammad Nauman,Salman Jan*

Main category: cs.AI

TL;DR: 提出一个价格感知的AI代理系统，结合个人财务管理与饮食优化，为中等收入家庭提供营养充足且价格合理的餐食计划，能自动适应市场价格变化。


<details>
  <summary>Details</summary>
Motivation: 中等收入环境中家庭预算有限且营养需求高，食品价格波动使得平衡营养与成本成为挑战。需要一种能动态适应价格变化、同时保证营养充足性的解决方案。

Method: 采用模块化多代理架构，包含预算代理、营养代理、价格监控代理和健康个性化代理。这些代理共享知识库，使用替代图来确保在最低成本下维持营养质量。

Result: 在沙特家庭案例研究中，相比静态周菜单，系统实现12-18%的成本降低，营养充足性超过95%，在20-30%的价格变化下表现良好。

Conclusion: 该框架能有效结合可负担性与营养充足性，为实现可持续和公平的饮食规划提供了可行途径，符合联合国可持续发展目标中的零饥饿和良好健康目标。

Abstract: The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.

</details>


### [37] [TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control](https://arxiv.org/abs/2512.20996)
*Yuwei Du,Jun Zhang,Jie Feng,Zhicheng Liu,Jian Yuan,Yong Li*

Main category: cs.AI

TL;DR: TrafficSimAgent是一个基于LLM的智能体框架，通过高层和低层专家代理的跨级协作，帮助非专业用户轻松执行交通仿真实验和决策优化。


<details>
  <summary>Details</summary>
Motivation: 现有交通仿真平台（如SUMO、MATSim）功能完善但使用门槛高，缺乏专业知识的用户难以从头开始实验并将其应用于日常工作。需要降低使用门槛，让非专业用户也能轻松进行交通仿真。

Method: 提出基于LLM的专家代理框架：高层专家代理理解自然语言指令，规划实验流程，按需调用MCP兼容工具；低层专家代理根据实时交通状况为基础元素选择最优行动方案。通过跨级协作实现灵活执行。

Result: 在多种场景下的广泛实验表明，TrafficSimAgent能有效执行各种条件下的仿真，即使在用户指令模糊时也能产生合理结果。其专家级自主决策驱动优化相比其他系统和SOTA LLM方法表现更优。

Conclusion: TrafficSimAgent成功降低了交通仿真的使用门槛，通过LLM驱动的专家代理框架实现了灵活的实验设计和决策优化，为非专业用户提供了强大的交通仿真工具。

Abstract: Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.

</details>


### [38] [Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation](https://arxiv.org/abs/2512.21066)
*Tomoaki Yamaguchi,Yutong Zhou,Masahiro Ryo,Keisuke Katsura*

Main category: cs.AI

TL;DR: 提出结合SHAP可解释AI与多模态LLM迭代精炼的智能体XAI框架，通过水稻产量数据测试，发现早期迭代提升解释质量，但过度精炼导致质量下降，需策略性早停优化实用价值。


<details>
  <summary>Details</summary>
Motivation: 当前XAI虽然能提供数据驱动的因子关联理解，但其技术性输出难以向非专业人士传达，影响对AI预测的信任。LLM虽有潜力将技术解释转化为通俗叙述，但将LLM作为自主智能体进行迭代精炼的智能体AI与XAI的结合尚未探索。

Method: 提出智能体XAI框架，结合基于SHAP的可解释性与多模态LLM驱动的迭代精炼，逐步生成增强的解释。以日本26块稻田的水稻产量数据作为农业推荐系统用例，进行11轮精炼（第0-10轮）。由人类专家（作物科学家，n=12）和LLM（n=14）根据7个指标评估解释质量。

Result: 框架成功提升推荐质量，从第0轮平均得分增加30-33%，在第3-4轮达到峰值。但过度精炼导致推荐质量显著下降，显示偏差-方差权衡：早期轮次缺乏解释深度（偏差），而过度迭代引入冗长和无根据的抽象（方差）。

Conclusion: 需要策略性早停（正则化）来优化实用价值，挑战了单调改进的假设，为智能体XAI系统提供了基于证据的设计原则。

Abstract: Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.

</details>


### [39] [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)
*Enoch Hyunwook Kang*

Main category: cs.AI

TL;DR: 论文证明：在聚合观测和算法盲评估条件下，用LLM角色模拟替代人类进行A/B测试是有效的基准接口，且其决策相关性取决于样本量


<details>
  <summary>Details</summary>
Motivation: A/B测试成本高、延迟大，阻碍了社会系统方法的迭代开发。LLM角色模拟提供了廉价替代方案，但需要验证其是否能保持基准接口的有效性

Method: 提出充要条件特征：当(1)方法仅观测聚合结果，(2)评估仅依赖提交产物而非算法身份时，角色模拟与人类测试在方法视角下无区别。定义聚合信道的信息论可区分性，推导样本量边界

Result: 证明角色模拟与人类测试在特定条件下等价，角色基准的决策相关性本质上是样本量问题，给出了可靠区分不同方法所需独立评估次数的明确边界

Conclusion: 在聚合观测和算法盲评估条件下，LLM角色模拟可作为有效的A/B测试替代基准，其有效性可通过增加样本量达到与实地实验相当的决策相关性

Abstract: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

</details>


### [40] [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)
*Ahmed M. Hussain,Salahuddin Salahuddin,Panos Papadimitratos*

Main category: cs.AI

TL;DR: 当前LLM安全机制存在重大漏洞：无法理解上下文和识别用户意图，导致恶意用户可通过情感框架、渐进揭示、学术论证等方法系统性地绕过安全防护，而推理功能反而增强了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方法主要关注显性有害内容，但忽视了关键漏洞：缺乏对上下文的理解和用户意图的识别能力，这使得恶意用户可以系统性地利用这些漏洞来绕过安全机制。

Method: 对多个最先进的LLM（包括ChatGPT、Claude、Gemini和DeepSeek）进行实证评估，分析通过情感框架、渐进揭示和学术论证等技术绕过可靠安全机制的情况，特别关注推理功能配置对攻击效果的影响。

Result: 研究证明LLM安全机制可通过系统方法被绕过，推理功能配置反而增强了攻击的有效性，提高了事实精确性但未能质疑底层意图。Claude Opus 4.1是例外，在某些用例中优先考虑意图检测而非信息提供。

Conclusion: 当前架构设计存在系统性漏洞，需要范式转变：将上下文理解和意图识别作为核心安全能力，而非事后保护机制，以构建更强大的LLM安全体系。

Abstract: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.

</details>


### [41] [A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care](https://arxiv.org/abs/2512.21127)
*Oliver Normand,Esther Borsi,Mitch Fruin,Lauren E Walker,Jamie Heagerty,Chris C. Holmes,Anthony J Avery,Iain E Buchan,Harry Coppock*

Main category: cs.AI

TL;DR: 首个在真实NHS初级医疗数据上评估LLM药物安全审查系统的研究，发现虽然系统能高敏感度识别临床问题，但仅在46.9%患者中完全正确识别所有问题和干预措施，主要失败原因是上下文推理而非药物知识缺失。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在医学基准测试中常达到或超过临床医生水平，但很少在真实临床数据上评估或超越表面指标。本研究旨在评估LLM药物安全审查系统在真实NHS初级医疗数据上的表现，并详细分析其失败行为模式。

Method: 回顾性研究使用NHS Cheshire和Merseyside地区2,125,549名成人的电子健康记录，战略抽样277名患者以涵盖临床复杂性和药物安全风险范围。专家临床医生审查这些患者，对系统识别的问题和干预建议进行分级评估。

Result: 主要LLM系统在识别临床问题存在时表现强劲（敏感性100%，特异性83.1%），但仅在46.9%患者中完全正确识别所有问题和干预措施。失败分析揭示主要失败机制是上下文推理而非药物知识缺失，包含五种模式：不确定性过度自信、不考虑患者背景应用标准指南、误解医疗实践方式、事实错误和流程盲点。

Conclusion: 本研究揭示了LLM临床AI安全部署前必须解决的缺陷，强调需要进行更大规模的前瞻性评估，并深入研究LLM在临床环境中的行为模式。提供了45个详细案例全面覆盖所有识别出的失败情况。

Abstract: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

</details>


### [42] [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)
*Le Wang,Zonghao Ying,Xiao Yang,Quanchen Zou,Zhenfei Yin,Tianlin Li,Jian Yang,Yaodong Yang,Aishan Liu,Xianglong Liu*

Main category: cs.AI

TL;DR: RoboSafe：面向具身智能体的混合推理运行时安全防护框架，通过可执行的谓词安全逻辑减少危险行为，同时保持任务性能


<details>
  <summary>Details</summary>
Motivation: 现有基于静态规则过滤或提示级控制的防御方法难以应对动态、时序依赖和上下文丰富的环境中的隐性风险，需要更灵活、可验证的安全保障机制

Method: 提出混合长短安全记忆架构，包含后向反思推理模块（从短期记忆推断时序安全谓词）和前向预测推理模块（从长期记忆和多模态观察预测风险），形成可解释、可执行的安全逻辑

Result: 在多个智能体上实验显示，RoboSafe显著减少危险行为（风险发生率降低36.8%），同时保持接近原始的任务性能，物理机器人臂评估证实其实用性

Conclusion: RoboSafe为具身智能体提供了一种自适应、可验证的运行时安全防护方案，能有效应对动态环境中的安全挑战，平衡安全性与任务性能

Abstract: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [43] [Uplink RSMA Performance Analysis with Rate Adaptation: A Stochastic Geometry Approach](https://arxiv.org/abs/2512.20883)
*Xinyi Guo,Li You,Qiong Liu,Xiqi Gao,Xiang-Gen Xia*

Main category: cs.IT

TL;DR: 提出基于随机几何的统一分析框架，用于大规模上行链路速率分割多址接入网络，结合有限调制编码方案速率适配，量化干扰耦合与离散速率性能


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注下行链路和单小区设计，大规模部署下的上行链路RSMA建模与分析仍待探索，需要桥接理论可处理性与实际现实性

Method: 基于随机几何建立统一分析框架，集成有限MCS速率适配，联合捕捉空间干扰耦合和离散速率行为，推导条件接收速率、空间平均及高阶统计量的可处理表达式

Result: 框架不仅推广了现有NOMA和OMA分析，还揭示了离散速率适配如何重塑密集RSMA网络的干扰动态和公平性，提供了新的性能洞察

Conclusion: 提出的统一框架为大规模上行链路RSMA网络提供了理论分析工具，能够量化平均和用户特定速率性能，对下一代无线网络设计具有指导意义

Abstract: Rate-splitting multiple access (RSMA) has emerged as a promising technique for efficient interference management in next-generation wireless networks. While most existing studies focus on downlink and single-cell designs, the modeling and analysis of uplink RSMA under large-scale deployments remain largely unexplored. On the basis of stochastic geometry (SG), this paper introduces a unified analytical framework that integrates finite modulation and coding scheme (MCS)-based rate adaptation. This framework jointly captures spatial interference coupling and discrete rate behavior to bridge theoretical tractability and practical realism. Within this framework, we derive tractable expressions for the conditional received rate (CRR), its spatial average, and higher-order statistics via the meta distribution, thereby quantifying both the mean and user-specific rate performance. Results show that the proposed unified framework not only generalizes existing non-orthogonal multiple access (NOMA) and orthogonal multiple access (OMA) analyses but also provides new insights into how discrete rate adaptation reshapes interference dynamics and fairness in dense RSMA-enabled networks.

</details>


### [44] [Knowledge-Driven 3D Semantic Spectrum Map: KE-VQ-Transformer Based UAV Semantic Communication and Map Completion](https://arxiv.org/abs/2512.20984)
*Wei Wu,Lingyi Wang,Fuhui Zhou,Zhaohui Yang,Qihui Wu*

Main category: cs.IT

TL;DR: 提出知识增强的语义频谱地图补全框架，通过物理信号传播模型约束提升AI驱动的三维频谱地图补全性能


<details>
  <summary>Details</summary>
Motivation: 在复杂通信环境和稀疏采样数据下，传统统计机器学习方法容易受到表面数据相关性的误导，缺乏可解释性，难以高效获取和传输三维频谱地图

Method: 提出知识增强的语义频谱地图补全框架，包含物理信号传播模型驱动的两个专家知识约束；设计KE-VQ-Transformer多尺度低复杂度智能补全方法，采用稀疏窗口避免超大三维注意力计算；引入KMSE和RKMSE作为联合考虑数值精度和物理一致性的新指标；开发联合离线在线训练方法

Result: 仿真结果表明，所提方案在RKMSE指标上优于最先进的基准方案

Conclusion: 提出的知识增强框架能够捕捉真实世界物理特性，避免陷入表面数据分布的思维定式，为智能通信网络中的三维频谱地图补全提供了有效解决方案

Abstract: Artificial intelligence (AI)-native three-dimensional (3D) spectrum maps are crucial in spectrum monitoring for intelligent communication networks. However, it is challenging to obtain and transmit 3D spectrum maps in a spectrum-efficient, computation-efficient, and AI-driven manner, especially under complex communication environments and sparse sampling data. In this paper, we consider practical air-to-ground semantic communications for spectrum map completion, where the unmanned aerial vehicle (UAV) measures the spectrum at spatial points and extracts the spectrum semantics, which are then utilized to complete spectrum maps at the ground device. Since statistical machine learning can easily be misled by superficial data correlations with the lack of interpretability, we propose a novel knowledge-enhanced semantic spectrum map completion framework with two expert knowledge-driven constraints from physical signal propagation models. This framework can capture the real-world physics and avoid getting stuck in the mindset of superficial data distributions. Furthermore, a knowledge-enhanced vector-quantized Transformer (KE-VQ-Transformer) based multi-scale low-complex intelligent completion approach is proposed, where the sparse window is applied to avoid ultra-large 3D attention computation, and the multi-scale design improves the completion performance. The knowledge-enhanced mean square error (KMSE) and root KMSE (RKMSE) are introduced as novel metrics for semantic spectrum map completion that jointly consider the numerical precision and physical consistency with the signal propagation model, based on which a joint offline and online training method is developed with supervised and unsupervised knowledge loss. The simulation demonstrates that our proposed scheme outperforms the state-of-the-art benchmark schemes in terms of RKMSE.

</details>


### [45] [Coding-Logic Correspondence: Turning Information and Communication Networks into Logical Formulae via Hypergraph Heyting Algebra](https://arxiv.org/abs/2512.21112)
*Cheuk Ting Li*

Main category: cs.IT

TL;DR: 提出使用混淆超图（超混淆）作为信息模型，替代传统随机变量方法，形成Heyting代数，将通信网络需求表达为逻辑公式，直接计算最优编码方案


<details>
  <summary>Details</summary>
Motivation: 传统基于随机变量的信息理论方法无法直接处理信息的合取、析取和蕴含操作。需要一种新的信息模型来统一表达通信网络中的各种编码需求（如网络编码、索引编码、Slepian-Wolf编码），并建立编码与逻辑之间的对应关系

Method: 使用混淆超图（hyperconfusions）作为信息模型，构建Heyting代数结构。利用Heyting代数与直觉主义逻辑的对应关系，将通信网络需求表达为逻辑公式，通过超图Heyting代数直接计算最优编码方案

Result: 最优通信成本由超图的熵给出（在对数差距内）。建立了编码设置与逻辑公式之间的对应关系，类似于Curry-Howard对应中证明与计算机程序的关系

Conclusion: 混淆超图为信息理论提供了新的代数框架，统一了多种编码问题，建立了编码与逻辑之间的深刻对应关系，为通信网络优化提供了新的计算方法

Abstract: We propose using confusion hypergraphs (hyperconfusions) as a model of information. In contrast to the conventional approach using random variables, we can now perform conjunction, disjunction and implication of information, forming a Heyting algebra. Using the connection between Heyting algebra and intuitionistic logic, we can express the requirements of a communication network (e.g., network coding, index coding, Slepian-Wolf coding) as a logical formula, allowing us to use the hypergraph Heyting algebra to directly compute the optimal coding scheme. The optimal communication cost is simply given by the entropy of the hypergraph (within a logarithmic gap). This gives a surprising correspondence between coding settings and logical formulae, similar to the Curry-Howard correspondence between proofs and computer programs.

</details>
