<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.AI](#cs.AI) [Total: 32]
- [cs.IT](#cs.IT) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Hybrid Quantum-Classical Detection for RIS-Assisted SC-FDE via Grover Adaptive Search](https://arxiv.org/abs/2511.04173)
*Maryam Tariq,Omar Alhussein,Raneem Abdelraheem,Abdullah Quran,Georges Kaddoum,Sami Muhaidat*

Main category: cs.NI

TL;DR: 提出了一种用于RIS辅助SC-FDE系统的混合量子-经典检测框架，将ML检测问题转化为QUBO问题并通过GAS求解，在理想条件下实现近最优性能且具有Grover二次加速。


<details>
  <summary>Details</summary>
Motivation: 6G网络对宽带和低延迟的需求要求检测器在接近ML性能的同时避免指数复杂度，因此需要开发量子增强的检测方法。

Method: 将ML检测目标重新表述为QUBO问题，使用GAS求解，并引入频域MMSE阈值进行低复杂度初始化以加速收敛。

Result: 在理想条件下检测器实现近最优性能，搜索成本从O(M^N)降低到O(SQRT(M^N))；在噪声条件下，GAS电路的浅深度使去极化误差可忽略，读取误差仅引入适度性能下降。

Conclusion: 该研究确立了量子增强检测在RIS辅助宽带通信中的可行性，展示了算法可扩展性和对6G网络的实际鲁棒性。

Abstract: Wideband and low-latency requirements in sixth-generation (6G) networks
demand detectors that approach maximum-likelihood (ML) performance without
incurring exponential complexity. This work develops a hybrid quantum-classical
detection framework for reconfigurable intelligent surface (RIS)-assisted
single-carrier (SC) frequency-domain equalization (FDE) over
frequency-selective channels. The ML detection objective is reformulated as a
quadratic unconstrained binary optimization (QUBO) problem and solved via
Grover adaptive search (GAS). To accelerate convergence, we introduce a
frequency-domain MMSE threshold that exploits the circulant structure of SC-FDE
channels, yielding low-complexity initialization. The framework is evaluated
across varying channel lengths and RIS sizes, confirming robustness and
scalability. In addition, GAS requirements are quantified through register
widths and gate counts, and its query complexity is analyzed to characterize
the algorithm's cost for block transmission in frequency-selective channels.
Quantum circuit simulations are conducted in Qiskit under both ideal and noisy
conditions. In the ideal case, the detector achieves near-optimal performance
while benefiting from Grover's quadratic speedup, reducing the search cost from
from O(M^N) exhaustive evaluations to O(SQRT(M^N)) oracle queries. Under noise,
the shallow depth of the GAS circuits, aided by MMSE initialization, makes
depolarizing errors negligible, while readout errors introduce moderate
degradation yet still preserve performance close to the MMSE baseline. These
results establish the feasibility of quantum-enhanced detection for
RIS-assisted broadband communications, highlighting both algorithmic
scalability and practical robustness for 6G networks.

</details>


### [2] [Improving dynamic congestion isolation in data-center networks](https://arxiv.org/abs/2511.04639)
*Alberto Merino,Jesus Escudero-Sahuquillo,Pedro Javier Garcia,Francisco J. Quiles*

Main category: cs.NI

TL;DR: 本文提出了一种改进的拥塞隔离机制ICI，通过协调拥塞隔离和DCQCN，减少虚假拥塞检测，提高网络性能。


<details>
  <summary>Details</summary>
Motivation: 分布式AI和大规模应用导致数据中心网络拥塞，现有机制DCQCN和拥塞隔离各有缺陷，组合使用时会产生虚假拥塞识别、过度限流和网络资源利用率低下的问题。

Method: 提出ICI机制，利用隔离的拥塞流信息来指导DCQCN的ECN标记，避免受害流被错误标记，减少虚假拥塞检测和不必要的闭环反馈。

Result: 在多种流量模式下的评估显示，ICI将生成的BECN数量减少高达32倍，尾部延迟改善高达31%，同时保持高吞吐量和可扩展性。

Conclusion: ICI机制有效解决了现有拥塞控制机制组合使用时的缺陷，显著提升了网络性能。

Abstract: The rise of distributed AI and large-scale applications has impacted the
communication operations of data-center and Supercomputer interconnection
networks, leading to dramatic incast or in-network congestion scenarios and
challenging existing congestion control mechanisms, such as injection
throttling (e.g., DCQCN) or congestion isolation (CI). While DCQCN provides a
scalable traffic rate adjustment for congesting flows at end nodes (which is
slow) and CI effectively isolates these flows in special network resources
(which requires extra logic in the switches), their combined use, although it
diminishes their particular drawbacks, leads to false congestion scenarios
identification and signaling, excessive throttling, and inefficient network
resource utilization. In this paper, we propose a new CI mechanism, called
Improved Congestion Isolation (ICI), which efficiently combines CI and DCQCN so
that the information of the isolated congesting flows is used to guide the ECN
marking performed by DCQCN in a way that victim flows do not end up being
marked. This coordination reduces false-positive congestion detection,
suppresses unnecessary closed-loop feedback (i.e., wrong congestion
notifications), and improves responsiveness to communication microbursts.
Evaluated under diverse traffic patterns, including incast and Data-center
workloads, ICI reduces the number of generated BECNs by up to 32x and improves
tail latency by up to 31%, while maintaining high throughput and scalability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)
*Zhaorun Chen,Zhuokai Zhao,Kai Zhang,Bo Liu,Qi Qi,Yifan Wu,Tarun Kalluri,Sara Cao,Yuanhao Xiong,Haibo Tong,Huaxiu Yao,Hengduo Li,Jiacheng Zhu,Xian Li,Dawn Song,Bo Li,Jason Weston,Dat Huynh*

Main category: cs.AI

TL;DR: DreamGym是一个统一的框架，通过基于推理的经验模型合成多样化经验数据，解决RL训练中昂贵的真实环境交互问题，实现可扩展的自主智能体在线强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在大语言模型智能体应用中的实际挑战：昂贵的环境交互、有限的任务多样性、不可靠的奖励信号和基础设施复杂性，这些问题阻碍了可扩展经验数据的收集。

Method: 1) 将环境动态提炼为基于推理的经验模型，通过逐步推理推导一致的状态转移和反馈信号；2) 使用离线真实数据初始化经验回放缓冲区并持续丰富；3) 自适应生成挑战当前策略的新任务，实现在线课程学习。

Result: 在多样化环境和智能体骨干上的实验表明：在非RL就绪任务（如WebArena）上比所有基线方法提升超过30%；在RL就绪但成本高昂的设置中，仅使用合成交互就能匹配GRPO和PPO性能；在纯合成经验训练的策略迁移到真实环境时，显著提升性能同时大幅减少真实世界交互需求。

Conclusion: DreamGym为通用强化学习提供了可扩展的预热启动策略，通过合成经验数据有效解决了RL训练中的数据收集瓶颈问题。

Abstract: While reinforcement learning (RL) can empower large language model (LLM)
agents by enabling self-improvement through interaction, its practical adoption
remains challenging due to costly rollouts, limited task diversity, unreliable
reward signals, and infrastructure complexity, all of which obstruct the
collection of scalable experience data. To address these challenges, we
introduce DreamGym, the first unified framework designed to synthesize diverse
experiences with scalability in mind to enable effective online RL training for
autonomous agents. Rather than relying on expensive real-environment rollouts,
DreamGym distills environment dynamics into a reasoning-based experience model
that derives consistent state transitions and feedback signals through
step-by-step reasoning, enabling scalable agent rollout collection for RL. To
improve the stability and quality of transitions, DreamGym leverages an
experience replay buffer initialized with offline real-world data and
continuously enriched with fresh interactions to actively support agent
training. To improve knowledge acquisition, DreamGym adaptively generates new
tasks that challenge the current agent policy, enabling more effective online
curriculum learning. Experiments across diverse environments and agent
backbones demonstrate that DreamGym substantially improves RL training, both in
fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready
tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in
RL-ready but costly settings, it matches GRPO and PPO performance using only
synthetic interactions. When transferring a policy trained purely on synthetic
experiences to real-environment RL, DreamGym yields significant additional
performance gains while requiring far fewer real-world interactions, providing
a scalable warm-start strategy for general-purpose RL.

</details>


### [4] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 本文评估了NLP分词模型在汇编代码分析中的内在特性，包括词汇量大小、语义覆盖等，并研究了其对下游任务（如函数签名预测）的影响。


<details>
  <summary>Details</summary>
Motivation: 汇编代码分词在代码分析中至关重要，但相关研究不足。本文旨在填补这一空白，探索适合汇编代码特性的分词方法和预处理策略。

Method: 系统研究多种分词模型，通过内在评估比较分词效率、词汇压缩和表示保真度，使用Llama 3.2、BERT和BART等预训练模型进行多指标评估。

Result: 分词器选择显著影响下游性能，内在指标对下游结果具有部分但非完全的预测性，揭示了内在特性与实际应用之间的复杂权衡。

Conclusion: 本研究为低级代码分析中的分词模型优化提供了宝贵见解，有助于提升基于自然语言模型的二进制分析工作流程的鲁棒性和可扩展性。

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [5] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong,Luyi Ma,Varun Vasudevan,Jason Cho,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: BehaviorLens框架系统评估了用户行为数据的文本与图像表示对多模态大语言模型性能的影响，发现图像表示能将下一个购买预测准确率提升87.5%。


<details>
  <summary>Details</summary>
Motivation: 探索文本和图像表示用户行为数据哪种更能最大化多模态大语言模型的性能，这个问题目前研究不足。

Method: 开发BehaviorLens基准框架，在六个MLLMs上评估三种数据表示方式：文本段落、散点图和流程图，使用真实购买序列数据集。

Result: 当数据表示为图像时，MLLMs的下一个购买预测准确率比等效文本表示提高了87.5%，且无需额外计算成本。

Conclusion: 图像表示用户行为数据能显著提升MLLMs的推理性能，这为优化智能代理系统提供了重要启示。

Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.

</details>


### [6] [KnowThyself: An Agentic Assistant for LLM Interpretability](https://arxiv.org/abs/2511.03878)
*Suraj Prasai,Mengnan Du,Ying Zhang,Fan Yang*

Main category: cs.AI

TL;DR: KnowThyself是一个基于聊天的LLM可解释性工具，通过整合现有功能、降低技术门槛，提供交互式可视化和引导式解释。


<details>
  <summary>Details</summary>
Motivation: 现有LLM可解释性工具分散且代码密集，需要更易用、统一的解决方案来降低技术门槛。

Method: 使用编排器LLM重新表述用户查询，通过代理路由器分发到专用模块，最后将输出整合为连贯解释。

Result: 开发了一个基于聊天的可扩展平台，提供交互式可视化和引导式解释。

Conclusion: KnowThyself为可访问的LLM可解释性提供了强大基础，通过对话工作流嵌入整个过程。

Abstract: We develop KnowThyself, an agentic assistant that advances large language
model (LLM) interpretability. Existing tools provide useful insights but remain
fragmented and code-intensive. KnowThyself consolidates these capabilities into
a chat-based interface, where users can upload models, pose natural language
questions, and obtain interactive visualizations with guided explanations. At
its core, an orchestrator LLM first reformulates user queries, an agent router
further directs them to specialized modules, and the outputs are finally
contextualized into coherent explanations. This design lowers technical
barriers and provides an extensible platform for LLM inspection. By embedding
the whole process into a conversational workflow, KnowThyself offers a robust
foundation for accessible LLM interpretability.

</details>


### [7] [Extracting Causal Relations in Deep Knowledge Tracing](https://arxiv.org/abs/2511.03948)
*Kevin Hong,Kia Karbasi,Gregory Pottie*

Main category: cs.AI

TL;DR: 本文挑战了关于深度知识追踪(DKT)模型性能来源的普遍解释，证明DKT的优势在于其隐式建模先决条件关系作为因果结构的能力，而非双向关系。


<details>
  <summary>Details</summary>
Motivation: 长期以来，计算教育研究的目标是开发可解释的知识追踪模型。DKT被认为是传统KT方法的重大进步，但对其性能提升原因的解释存在争议。

Method: 通过将练习关系图修剪为有向无环图(DAGs)，并在Assistments数据集的因果子集上训练DKT，分析其预测能力与因果结构的一致性。

Result: 实验表明DKT的预测能力与因果结构高度一致，并提出了基于DKT学习表示提取练习关系DAGs的替代方法。

Conclusion: DKT的有效性主要源于其近似知识组件间因果依赖关系的能力，而非简单的关系映射。

Abstract: A longstanding goal in computational educational research is to develop
explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which
leverages a Recurrent Neural Network (RNN) to predict student knowledge and
performance on exercises, has been proposed as a major advancement over
traditional KT methods. Several studies suggest that its performance gains stem
from its ability to model bidirectional relationships between different
knowledge components (KCs) within a course, enabling the inference of a
student's understanding of one KC from their performance on others. In this
paper, we challenge this prevailing explanation and demonstrate that DKT's
strength lies in its implicit ability to model prerequisite relationships as a
causal structure, rather than bidirectional relationships. By pruning exercise
relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal
subsets of the Assistments dataset, we show that DKT's predictive capabilities
align strongly with these causal structures. Furthermore, we propose an
alternative method for extracting exercise relation DAGs using DKT's learned
representations and provide empirical evidence supporting our claim. Our
findings suggest that DKT's effectiveness is largely driven by its capacity to
approximate causal dependencies between KCs rather than simple relational
mappings.

</details>


### [8] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: LLMs对提示语言和文化框架有响应，但存在系统性文化偏见，偏向荷兰、德国、美国和日本等少数国家的价值观，无法充分代表文化多样性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能代表其广泛用户群体的文化多样性，特别是提示语言和文化框架如何影响模型响应与不同国家人类价值观的匹配程度。

Method: 使用霍夫斯泰德价值观调查模块和世界价值观调查的63个项目，翻译成11种语言，以带或不带明确文化视角的提示形式测试10个LLM。

Result: 提示语言和文化视角都会导致LLM输出变化，但所有模型都显示出对荷兰、德国、美国和日本价值观的系统性偏见。明确的文视角比针对性提示语言更能改善与人类价值观的匹配度。

Conclusion: LLMs处于尴尬的中间地带：对提示变化足够敏感以产生变化，但又过于固守特定文化默认值而无法充分代表文化多样性。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [9] [ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering](https://arxiv.org/abs/2511.03985)
*Zhuowen Yuan,Tao Liu,Yang Yang,Yang Wang,Feng Qi,Kaushik Rangadurai,Bo Li,Shuang Yang*

Main category: cs.AI

TL;DR: ArchPilot是一个多代理系统，通过集成架构生成、代理评估和自适应搜索来解决LLM代理在自动化ML工程中计算开销大、可扩展性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在自动化ML工程中严重依赖重复的完整训练来评估候选方案，导致计算开销大、搜索空间受限和迭代周期慢。

Method: 采用三代理协作框架：编排代理使用MCTS启发算法协调搜索过程，生成代理迭代生成和改进架构，评估代理执行代理训练并生成保真度感知性能指标。

Result: 在MLE-Bench上的实验表明，ArchPilot优于AIDE和ML-Master等SOTA基线方法。

Conclusion: ArchPilot通过多代理协作，在有限预算下实现了高效的ML工程，显著减少了对昂贵完整训练的依赖。

Abstract: Recent LLM-based agents have demonstrated strong capabilities in automated ML
engineering. However, they heavily rely on repeated full training runs to
evaluate candidate solutions, resulting in significant computational overhead,
limited scalability to large search spaces, and slow iteration cycles. To
address these challenges, we introduce ArchPilot, a multi-agent system that
integrates architecture generation, proxy-based evaluation, and adaptive search
into a unified framework. ArchPilot consists of three specialized agents: an
orchestration agent that coordinates the search process using a Monte Carlo
Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and
manages memory of previous candidates; a generation agent that iteratively
generates, improves, and debugs candidate architectures; and an evaluation
agent that executes proxy training runs, generates and optimizes proxy
functions, and aggregates the proxy scores into a fidelity-aware performance
metric. This multi-agent collaboration allows ArchPilot to prioritize
high-potential candidates with minimal reliance on expensive full training
runs, facilitating efficient ML engineering under limited budgets. Experiments
on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE
and ML-Master, validating the effectiveness of our multi-agent system.

</details>


### [10] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: 提出了多智能体AI系统中异常检测任务，构建了两个包含4,275和894条轨迹的数据集，并评估了监督和半监督方法，准确率分别达到98%和96%。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统具有非确定性和易发生静默故障（如漂移、循环、输出细节缺失）的特点，这些故障难以检测。

Method: 开发了数据集构建流程，捕捉用户行为、智能体非确定性和LLM变化，并创建了两个基准数据集。使用XGBoost（监督）和SVDD（半监督）方法进行异常检测。

Result: 监督方法（XGBoost）准确率达98%，半监督方法（SVDD）准确率达96%，两种方法性能相当。

Conclusion: 这是首个对多智能体AI系统异常检测的系统性研究，提供了数据集、基准和见解，为未来研究提供指导。

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [11] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: LLMs在数值推理中存在系统错误，主要原因是数值属性在共享潜在子空间中编码，导致相关性和无关上下文对数值表示产生干扰。


<details>
  <summary>Details</summary>
Motivation: 尽管行为研究已发现LLMs的数值推理错误，但其内部表示机制尚不清楚。研究旨在探索LLMs如何整合多个数值属性，以及无关数值上下文如何干扰这些表示。

Method: 结合线性探测与偏相关分析，以及基于提示的脆弱性测试，在不同规模的模型上进行实验。

Result: LLMs编码了真实世界的数值相关性但倾向于系统性地放大它们。无关上下文会引起数值表示的一致偏移，且下游影响随模型规模而变化。

Conclusion: 这些发现揭示了LLM决策中的脆弱性，为在多属性纠缠下实现更公平、表示感知的控制奠定了基础。

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [12] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: 提出了一个名为Agentmandering的框架，将选区重划重新构想为两个代表对立政治利益的智能体之间的回合制谈判，通过LLM智能体将战略互动嵌入到选区重划过程中，显著减少了党派偏见和不公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法主要生成大量法律上有效的选区重划方案，但忽略了选择过程中的战略动态，这为党派行为者挑选技术上合规但政治上有利的地图创造了机会。仅仅满足形式约束并不能确保公平性。

Method: 采用基于博弈论思想的回合制谈判框架，两个代表对立政治利益的LLM智能体轮流从一小部分候选地图中选择和冻结选区，通过受约束且可解释的选择逐步划分州。

Result: 在2020年美国人口普查数据上的评估显示，Agentmandering显著减少了党派偏见和不公平性，同时实现了比标准基线低2到3个数量级的方差，在摇摆州场景中表现出公平性和稳定性。

Conclusion: Agentmandering框架通过将战略互动嵌入选区重划过程，能够产生更公平和稳定的选区重划结果，特别是在竞争激烈的政治环境中。

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


### [13] [KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04093)
*Yuanning Cui,Zequn Sun,Wei Hu,Zhangjie Fu*

Main category: cs.AI

TL;DR: 提出了LLM-KGFR框架，通过LLM与结构化检索器KGFR协作解决知识密集型问题，实现零样本泛化到未见知识图谱，并采用渐进式传播处理大规模图。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖微调LLM或GNN检索器，存在数据集特定调优和大规模或未见图谱可扩展性限制的问题。

Method: LLM-KGFR框架中，KGFR使用LLM生成的关系描述编码关系，基于问题角色初始化实体，采用非对称渐进传播逐步扩展并选择性限制高度节点，通过节点、边、路径级接口支持LLM迭代推理。

Result: 实验表明LLM-KGFR在保持可扩展性和泛化性的同时实现了强劲性能。

Conclusion: 该框架为知识图谱增强推理提供了实用解决方案。

Abstract: Large language models (LLMs) excel at reasoning but struggle with
knowledge-intensive questions due to limited context and parametric knowledge.
However, existing methods that rely on finetuned LLMs or GNN retrievers are
limited by dataset-specific tuning and scalability on large or unseen graphs.
We propose the LLM-KGFR collaborative framework, where an LLM works with a
structured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR
encodes relations using LLM-generated descriptions and initializes entities
based on their roles in the question, enabling zero-shot generalization to
unseen KGs. To handle large graphs efficiently, it employs Asymmetric
Progressive Propagation (APP)- a stepwise expansion that selectively limits
high-degree nodes while retaining informative paths. Through node-, edge-, and
path-level interfaces, the LLM iteratively requests candidate answers,
supporting facts, and reasoning paths, forming a controllable reasoning loop.
Experiments demonstrate that LLM-KGFR achieves strong performance while
maintaining scalability and generalization, providing a practical solution for
KG-augmented reasoning.

</details>


### [14] [Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms](https://arxiv.org/abs/2511.04133)
*Miguel E. Andres,Vadim Fedorov,Rida Sadek,Enric Spagnolo-Arrizabalaga,Nadescha Trudel*

Main category: cs.AI

TL;DR: 提出了首个系统化评估语音AI测试质量的框架，通过人类中心化基准测试来解决测试平台的双重挑战：生成真实测试对话和准确评估代理响应。


<details>
  <summary>Details</summary>
Motivation: 随着语音AI代理快速部署到生产环境，缺乏系统化的测试可靠性验证方法，组织无法客观评估其测试方法是否有效，这在大规模部署时造成了关键的测量差距。

Method: 结合心理测量技术（成对比较产生Elo评分、bootstrap置信区间和置换检验）与严格的统计验证，提供可重复的指标，适用于任何测试方法。

Result: 对三个领先商业平台进行实证评估，结果显示Evalion平台表现最佳，评估质量F1分数达0.92（其他为0.73），模拟质量达0.61（其他为0.43），存在统计显著性能差异。

Conclusion: 该框架使研究人员和组织能够实证验证任何平台的测试能力，为大规模语音AI部署提供必要的测量基础。

Abstract: Voice AI agents are rapidly transitioning to production deployments, yet
systematic methods for ensuring testing reliability remain underdeveloped.
Organizations cannot objectively assess whether their testing approaches
(internal tools or external platforms) actually work, creating a critical
measurement gap as voice AI scales to billions of daily interactions.
  We present the first systematic framework for evaluating voice AI testing
quality through human-centered benchmarking. Our methodology addresses the
fundamental dual challenge of testing platforms: generating realistic test
conversations (simulation quality) and accurately evaluating agent responses
(evaluation quality). The framework combines established psychometric
techniques (pairwise comparisons yielding Elo ratings, bootstrap confidence
intervals, and permutation tests) with rigorous statistical validation to
provide reproducible metrics applicable to any testing approach.
  To validate the framework and demonstrate its utility, we conducted
comprehensive empirical evaluation of three leading commercial platforms
focused on Voice AI Testing using 21,600 human judgments across 45 simulations
and ground truth validation on 60 conversations. Results reveal statistically
significant performance differences with the proposed framework, with the
top-performing platform, Evalion, achieving 0.92 evaluation quality measured as
f1-score versus 0.73 for others, and 0.61 simulation quality using a league
based scoring system (including ties) vs 0.43 for other platforms.
  This framework enables researchers and organizations to empirically validate
the testing capabilities of any platform, providing essential measurement
foundations for confident voice AI deployment at scale. Supporting materials
are made available to facilitate reproducibility and adoption.

</details>


### [15] [When Empowerment Disempowers](https://arxiv.org/abs/2511.04177)
*Claire Yang,Maya Cakmak,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 本文提出了一个多人类网格世界测试套件Disempower-Grid，发现优化单人类赋能的辅助RL代理可能会显著降低其他人类的环境影响和奖励，这种现象被称为去赋能。


<details>
  <summary>Details</summary>
Motivation: 赋能作为衡量代理控制环境能力的指标，被提出作为AI代理辅助行为的通用目标无关目标。然而，先前关于赋能辅助的研究假设代理在隔离环境中辅助一个人类，而多人类环境如家庭和医院是AI辅助的有前景场景。

Method: 引入开源多人类网格世界测试套件Disempower-Grid，通过该测试套件实证研究辅助RL代理的行为。

Result: 研究发现，优化单人类赋能的辅助RL代理会显著降低其他人类的环境影响和奖励（去赋能现象）。联合赋能可以缓解去赋能，但会以用户奖励为代价。

Conclusion: 这项工作揭示了AI对齐社区面临的更广泛挑战：在单代理设置中看似对齐的目标无关目标，在多代理环境中可能变得不对齐。

Abstract: Empowerment, a measure of an agent's ability to control its environment, has
been proposed as a universal goal-agnostic objective for motivating assistive
behavior in AI agents. While multi-human settings like homes and hospitals are
promising for AI assistance, prior work on empowerment-based assistance assumes
that the agent assists one human in isolation. We introduce an open source
multi-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we
empirically show that assistive RL agents optimizing for one human's
empowerment can significantly reduce another human's environmental influence
and rewards - a phenomenon we formalize as disempowerment. We characterize when
disempowerment occurs in these environments and show that joint empowerment
mitigates disempowerment at the cost of the user's reward. Our work reveals a
broader challenge for the AI alignment community: goal-agnostic objectives that
seem aligned in single-agent settings can become misaligned in multi-agent
contexts.

</details>


### [16] [Opus: A Quantitative Framework for Workflow Evaluation](https://arxiv.org/abs/2511.04220)
*Alan Seroul,Théo Fagnoni,Inès Adnani,Dana O. Mohamed,Phillip Kingston*

Main category: cs.AI

TL;DR: Opus工作流评估框架是一个概率-规范化的数学模型，用于量化工作流质量和效率，结合了正确性、可靠性和成本等概念。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够直接比较、评分和优化工作流的统一框架，将工作流质量的不同维度整合到连贯的数学模型中。

Method: 结合Opus工作流奖励（概率函数估计预期性能）和Opus工作流规范惩罚（测量结构性和信息质量），支持自动化评估、排序和优化。

Result: 提出了一个统一的工作流评估框架，能够形式化工作流成功概率，定义可测量的规范惩罚，并建立联合奖励-惩罚权衡的优化公式。

Conclusion: 该框架支持在现代自动化系统中进行工作流自动评估、排序和优化，并可集成到强化学习循环中指导工作流发现和精炼。

Abstract: This paper introduces the Opus Workflow Evaluation Framework, a
probabilistic-normative formulation for quantifying Workflow quality and
efficiency. It integrates notions of correctness, reliability, and cost into a
coherent mathematical model that enables direct comparison, scoring, and
optimization of Workflows. The framework combines the Opus Workflow Reward, a
probabilistic function estimating expected performance through success
likelihood, resource usage, and output gain, with the Opus Workflow Normative
Penalties, a set of measurable functions capturing structural and informational
quality across Cohesion, Coupling, Observability, and Information Hygiene. It
supports automated Workflow assessment, ranking, and optimization within modern
automation systems such as Opus and can be integrated into Reinforcement
Learning loops to guide Workflow discovery and refinement. In this paper, we
introduce the Opus Workflow Reward model that formalizes Workflow success as a
probabilistic expectation over costs and outcomes. We define measurable Opus
Workflow Normative Penalties capturing structural, semantic, and signal-related
properties of Workflows. Finally, we propose a unified optimization formulation
for identifying and ranking optimal Workflows under joint Reward-Penalty
trade-offs.

</details>


### [17] [Shared Spatial Memory Through Predictive Coding](https://arxiv.org/abs/2511.04235)
*Zhengru Fang,Yu Guo,Jingjing Wang,Yuang Zhang,Haonan An,Yinhai Wang,Yuguang Fang*

Main category: cs.AI

TL;DR: 提出了一种多智能体预测编码框架，通过最小化智能体间的相互不确定性来解决协调问题，在带宽受限条件下表现出色。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，部分可观测性和有限带宽常常导致协调失败，需要开发能够有效共享和重建一致空间记忆的方法。

Method: 使用信息瓶颈目标，让智能体学习何时、与谁、以及通信什么内容；基于网格细胞般的内部空间编码，通过自监督运动预测自发形成；构建分层强化学习策略主动探索以减少联合不确定性。

Result: 在Memory-Maze基准测试中，该方法对带宽约束表现出卓越的韧性：当带宽从128位/步缩减到4位/步时，成功率从73.5%优雅下降到64.4%，而全广播基线从67.6%崩溃到28.6%。

Conclusion: 该研究为复杂社会表征如何从统一的预测驱动中涌现提供了理论原则和生物学上合理的基础，导致了社会集体智能的形成。

Abstract: Sharing and reconstructing a consistent spatial memory is a critical
challenge in multi-agent systems, where partial observability and limited
bandwidth often lead to catastrophic failures in coordination. We introduce a
multi-agent predictive coding framework that formulate coordination as the
minimization of mutual uncertainty among agents. Instantiated as an information
bottleneck objective, it prompts agents to learn not only who and what to
communicate but also when. At the foundation of this framework lies a
grid-cell-like metric as internal spatial coding for self-localization,
emerging spontaneously from self-supervised motion prediction. Building upon
this internal spatial code, agents gradually develop a bandwidth-efficient
communication mechanism and specialized neural populations that encode
partners' locations: an artificial analogue of hippocampal social place cells
(SPCs). These social representations are further enacted by a hierarchical
reinforcement learning policy that actively explores to reduce joint
uncertainty. On the Memory-Maze benchmark, our approach shows exceptional
resilience to bandwidth constraints: success degrades gracefully from 73.5% to
64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast
baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically
principled and biologically plausible basis for how complex social
representations emerge from a unified predictive drive, leading to social
collective intelligence.

</details>


### [18] [RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization](https://arxiv.org/abs/2511.04285)
*Zeng Zhiyuan,Jiashuo Liu,Zhangyue Yin,Ge Zhang,Wenhao Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: RLoop是一个自改进框架，通过迭代策略初始化解决RL训练中的过拟合问题，将探索和利用转化为稳健性能提升。


<details>
  <summary>Details</summary>
Motivation: RLVR训练中存在RL过拟合问题，模型获得训练奖励但失去泛化能力，这由策略过度专业化和灾难性遗忘驱动。

Method: RLoop通过迭代策略初始化构建自改进框架：先用RL从给定策略探索解空间，然后过滤成功轨迹创建专家数据集，通过拒绝采样微调(RFT)优化初始策略，为下一次迭代创建更好的起点。

Result: 实验显示RLoop显著减轻遗忘并提升泛化能力，相比原始RL平均准确率提升9%，pass@32提升超过15%。

Conclusion: RLoop通过探索和利用的循环有效将瞬态策略变化转化为稳健性能增益，解决了RL训练中的过拟合问题。

Abstract: While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for
training large reasoning models, its training dynamics harbor a critical
challenge: RL overfitting, where models gain training rewards but lose
generalization. Our analysis reveals this is driven by policy
over-specialization and catastrophic forgetting of diverse solutions generated
during training. Standard optimization discards this valuable inter-step policy
diversity. To address this, we introduce RLoop, a self-improving framework
built on iterative policy initialization. RLoop transforms the standard
training process into a virtuous cycle: it first uses RL to explore the
solution space from a given policy, then filters the successful trajectories to
create an expert dataset. This dataset is used via Rejection-sampling
Fine-Tuning (RFT) to refine the initial policy, creating a superior starting
point for the next iteration. This loop of exploration and exploitation via
iterative re-initialization effectively converts transient policy variations
into robust performance gains. Our experiments show RLoop mitigates forgetting
and substantially improves generalization, boosting average accuracy by 9% and
pass@32 by over 15% compared to vanilla RL.

</details>


### [19] [GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents](https://arxiv.org/abs/2511.04307)
*Jian Mu,Chaoyun Zhang,Chiming Ni,Lu Wang,Bo Qiao,Kartik Mathur,Qianhui Wu,Yuhang Xie,Xiaojun Ma,Mengyu Zhou,Si Qin,Liqun Li,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: GUI-360°是一个大规模数据集和基准套件，用于推进计算机使用代理（CUAs）的研究，包含120万+执行动作步骤，支持GUI定位、屏幕解析和动作预测三个核心任务。


<details>
  <summary>Details</summary>
Motivation: 解决计算机使用代理研究中的三个关键差距：真实世界CUA任务稀缺、缺乏多模态轨迹自动收集标注流程、以及缺少统一评估GUI定位、屏幕解析和动作预测的基准。

Method: 采用LLM增强的自动化流程，包括查询来源、环境模板构建、任务实例化、批量执行和LLM驱动的质量过滤，收集Windows办公应用中的轨迹数据。

Result: 基准测试显示现有视觉-语言模型在GUI定位和动作预测方面存在显著不足，监督微调和强化学习带来显著改进但未达到人类可靠性水平。

Conclusion: GUI-360°数据集和代码已公开发布，旨在促进可重复研究并加速稳健桌面CUAs的发展。

Abstract: We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and
benchmark suite designed to advance computer-using agents (CUAs). CUAs present
unique challenges and is constrained by three persistent gaps: a scarcity of
real-world CUA tasks, the lack of automated collection-and-annotation pipelines
for multi-modal trajectories, and the absence of a unified benchmark that
jointly evaluates GUI grounding, screen parsing, and action prediction.
  GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated
pipeline for query sourcing, environment-template construction, task
instantiation, batched execution, and LLM-driven quality filtering. The
released corpus contains over 1.2M executed action steps across thousands of
trajectories in popular Windows office applications, and includes
full-resolution screenshots, accessibility metadata when available,
instantiated goals, intermediate reasoning traces, and both successful and
failed action trajectories. The dataset supports three canonical tasks, GUI
grounding, screen parsing, and action prediction, and a hybrid GUI+API action
space that reflects modern agent designs. Benchmarking state-of-the-art
vision--language models on GUI-360$^\circ$ reveals substantial out-of-the-box
shortcomings in grounding and action prediction; supervised fine-tuning and
reinforcement learning yield significant gains but do not close the gap to
human-level reliability. We release GUI-360$^\circ$ and accompanying code to
facilitate reproducible research and accelerate progress on robust desktop
CUAs.
  The full dataset has been made public on
https://huggingface.co/datasets/vyokky/GUI-360.

</details>


### [20] [Probing the Probes: Methods and Metrics for Concept Alignment](https://arxiv.org/abs/2511.04312)
*Jacob Lysnæs-Larsen,Marte Eggen,Inga Strümke*

Main category: cs.AI

TL;DR: 论文指出CAV探测器的分类精度不能可靠反映概念对齐度，提出了基于空间线性归因的新概念定位方法，并引入三类定量评估指标来检测和缓解概念错位问题。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI中广泛假设高探测精度表示CAV能忠实表示目标概念，但实际上探测器更可能捕捉虚假相关性而非真正概念，需要更可靠的概念对齐评估方法。

Method: 引入基于空间线性归因的概念定位方法，提出三类概念对齐评估指标：硬精度、分割分数和增强鲁棒性，分析具有平移不变性和空间对齐的探测器。

Result: 研究表明故意错位的探测器利用虚假相关性也能达到接近标准探测器的精度，而具有平移不变性和空间对齐的探测器能持续提高概念对齐度。

Conclusion: 需要基于对齐度的评估指标而非探测精度，且应根据模型架构和目标概念特性定制探测器。

Abstract: In explainable AI, Concept Activation Vectors (CAVs) are typically obtained
by training linear classifier probes to detect human-understandable concepts as
directions in the activation space of deep neural networks. It is widely
assumed that a high probe accuracy indicates a CAV faithfully representing its
target concept. However, we show that the probe's classification accuracy alone
is an unreliable measure of concept alignment, i.e., the degree to which a CAV
captures the intended concept. In fact, we argue that probes are more likely to
capture spurious correlations than they are to represent only the intended
concept. As part of our analysis, we demonstrate that deliberately misaligned
probes constructed to exploit spurious correlations, achieve an accuracy close
to that of standard probes. To address this severe problem, we introduce a
novel concept localization method based on spatial linear attribution, and
provide a comprehensive comparison of it to existing feature visualization
techniques for detecting and mitigating concept misalignment. We further
propose three classes of metrics for quantitatively assessing concept
alignment: hard accuracy, segmentation scores, and augmentation robustness. Our
analysis shows that probes with translation invariance and spatial alignment
consistently increase concept alignment. These findings highlight the need for
alignment-based evaluation metrics rather than probe accuracy, and the
importance of tailoring probes to both the model architecture and the nature of
the target concept.

</details>


### [21] [AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research](https://arxiv.org/abs/2511.04316)
*Tim Beyer,Jonas Dornbusch,Jakob Steimle,Moritz Ladenburger,Leo Schwinn,Stephan Günnemann*

Main category: cs.AI

TL;DR: AdversariaLLM是一个用于LLM越狱鲁棒性研究的工具箱，旨在解决当前LLM安全研究生态系统碎片化、难以复现和比较的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全和鲁棒性研究生态系统碎片化严重，存在大量buggy的实现、数据集和评估方法，导致研究难以复现和比较，阻碍了有意义的进展。

Method: 设计了一个以可复现性、正确性和可扩展性为核心的工具箱，实现了12种对抗攻击算法，集成了7个基准数据集，涵盖有害性、过度拒绝和效用评估，并通过Hugging Face提供对多种开源LLM的访问。

Result: 该框架提供了计算资源跟踪、确定性结果和分布评估技术等高级功能，确保可比性和可复现性，并与JudgeZoo集成进行评判。

Conclusion: AdversariaLLM为LLM安全研究建立了透明、可比和可复现的坚实基础。

Abstract: The rapid expansion of research on Large Language Model (LLM) safety and
robustness has produced a fragmented and oftentimes buggy ecosystem of
implementations, datasets, and evaluation methods. This fragmentation makes
reproducibility and comparability across studies challenging, hindering
meaningful progress. To address these issues, we introduce AdversariaLLM, a
toolbox for conducting LLM jailbreak robustness research. Its design centers on
reproducibility, correctness, and extensibility. The framework implements
twelve adversarial attack algorithms, integrates seven benchmark datasets
spanning harmfulness, over-refusal, and utility evaluation, and provides access
to a wide range of open-weight LLMs via Hugging Face. The implementation
includes advanced features for comparability and reproducibility such as
compute-resource tracking, deterministic results, and distributional evaluation
techniques. \name also integrates judging through the companion package
JudgeZoo, which can also be used independently. Together, these components aim
to establish a robust foundation for transparent, comparable, and reproducible
research in LLM safety.

</details>


### [22] [RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation](https://arxiv.org/abs/2511.04328)
*Jiahao Zhao,Luxin Xu,Minghuan Tan,Lichao Zhang,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: 提出了RxSafeBench框架，通过模拟临床咨询来评估LLMs的药物安全性能力，包含6,725种禁忌症、28,781种药物相互作用和14,906种适应症-药物对，构建了2,443个高质量咨询场景的基准测试。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏真实世界数据集和隐私可访问性问题，现有研究对LLMs药物安全性的评估有限，特别是在真实临床咨询环境中的评估尚未充分探索。

Method: 构建RxRisk DB药物安全数据库，采用两阶段过滤策略确保临床真实性和专业质量，使用结构化多选题评估LLMs在模拟患者情境下推荐安全药物的能力。

Result: 当前LLMs难以整合禁忌症和相互作用知识，特别是当风险是隐含而非明确时表现不佳。

Conclusion: RxSafeBench是首个评估LLMs药物安全性的综合基准，为改进AI驱动的临床决策支持系统的可靠性提供了重要见解。

Abstract: Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.

</details>


### [23] [Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning](https://arxiv.org/abs/2511.04341)
*Nick Oh,Fernand Gobet*

Main category: cs.AI

TL;DR: 本文提出了Monitor-Generate-Verify（MGV）框架，通过在Generate-Verify范式前添加显式监控机制来解决前缀主导陷阱问题，将元认知理论转化为计算规范。


<details>
  <summary>Details</summary>
Motivation: 现有测试时推理架构缺乏监控过程，导致模型过早陷入次优推理路径而难以恢复（约20%准确率损失），即前缀主导陷阱问题。

Method: 将Flavell以及Nelson和Narens的元认知理论形式化为计算规范，在生成-验证范式前添加显式监控，捕获元认知体验（难度评估、置信度判断等），并通过验证反馈优化后续监控。

Result: 虽然未提供实证验证，但这是首次系统性地将基础元认知理论转化为计算框架。

Conclusion: MGV框架为理解推理系统失败提供了原则性词汇，并为未来测试时推理设计提出了具体的架构干预建议。

Abstract: Test-time reasoning architectures such as those following the Generate-Verify
paradigm -- where a model iteratively refines or verifies its own generated
outputs -- prioritise generation and verification but exclude the monitoring
processes that determine when and how reasoning should begin. This omission may
contribute to the prefix dominance trap, in which models commit early to
suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy
loss. We address this architectural gap by formalising Flavell's and Nelson and
Narens' metacognitive theories into computational specifications, proposing the
Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify
paradigm by adding explicit monitoring that captures metacognitive experiences
(from difficulty assessments to confidence judgements) before generation begins
and refines future monitoring through verification feedback. Though we present
no empirical validation, this work provides the first systematic computational
translation of foundational metacognitive theories, offering a principled
vocabulary for understanding reasoning system failures and suggesting specific
architectural interventions for future test-time reasoning designs.

</details>


### [24] [Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach](https://arxiv.org/abs/2511.04393)
*Chanwoo Park,Ziyang Chen,Asuman Ozdaglar,Kaiqing Zhang*

Main category: cs.AI

TL;DR: Iterative RMFT是一种后训练方法，通过反复蒸馏低后悔决策轨迹来增强LLM的决策能力，无需依赖已知算法或人工模板。


<details>
  <summary>Details</summary>
Motivation: LLM原本并非为决策设计，在在线决策问题中表现不佳，无法实现低后悔或有效的探索-利用权衡。

Method: 迭代后悔最小化微调：模型多次推出决策轨迹，选择k个最低后悔的轨迹，并在这些轨迹上微调自身。

Result: Iterative RMFT提升了多种模型的决策性能，从Transformer到开源LLM和GPT-4o mini等闭源模型，能泛化到不同任务设置。

Conclusion: Iterative RMFT提供了一个原则性且通用的后训练框架，可增强LLM的决策能力，理论分析显示在简化设置下可实现无后悔学习。

Abstract: Large language models (LLMs) are increasingly deployed as "agents" for
decision-making (DM) in interactive and dynamic environments. Yet, since they
were not originally designed for DM, recent studies show that LLMs can struggle
even in basic online DM problems, failing to achieve low regret or an effective
exploration-exploitation tradeoff. To address this, we introduce Iterative
Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure
that repeatedly distills low-regret decision trajectories back into the base
model. At each iteration, the model rolls out multiple decision trajectories,
selects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior
methods that (a) distill action sequences from known DM algorithms or (b) rely
on manually crafted chain-of-thought templates, our approach leverages the
regret metric to elicit the model's own DM ability and reasoning rationales.
This reliance on model-generated reasoning avoids rigid output engineering and
provides more flexible, natural-language training signals. Empirical results
show that Iterative RMFT improves LLMs' DM performance across diverse models -
from Transformers with numerical input/output, to open-weight LLMs, and
advanced closed-weight models like GPT-4o mini. Its flexibility in output and
reasoning formats enables generalization across tasks with varying horizons,
action spaces, reward processes, and natural-language contexts. Finally, we
provide theoretical insight showing that a single-layer Transformer under this
paradigm can act as a no-regret learner in a simplified setting. Overall,
Iterative RMFT offers a principled and general post-training framework for
enhancing LLMs' decision-making capabilities.

</details>


### [25] [The Peril of Preference: Why GRPO fails on Ordinal Rewards](https://arxiv.org/abs/2511.04439)
*Anisha Garg,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: CoRPO是一种新的强化学习优化方法，解决了GRPO在序数奖励下错误强化失败轨迹的问题，通过自适应基线确保失败解不被正向强化，并在达到质量阈值后转向相对偏好模式。


<details>
  <summary>Details</summary>
Motivation: GRPO的简单性使其在处理非二元反馈时存在缺陷，特别是在使用序数奖励给予部分学分时，其组平均基线会错误地强化失败轨迹。

Method: CoRPO使用自适应基线强制执行最低质量阈值，确保失败解决方案永远不会被正向强化。一旦策略持续满足此阈值，基线自动过渡到相对偏好模式。

Result: 在代码验证任务上的实证验证表明，CoRPO表现出更稳定的收敛性和更好的域外泛化能力。

Conclusion: 这项工作是在LLMs通过强化学习学习真正新能力的研究计划中的关键一步，使LLMs能够从丰富的多维反馈中学习。

Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly
desirable for adapting LLMs to become experts at specific tasks. But this
simplicity also makes it ill-specified as we seek to enhance RL training with
richer, non-binary feedback. When using ordinal rewards to give partial credit,
GRPO's simplicity starts to hurt, as its group-average baseline often assigns a
positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new
formulation that solves this flaw. CoRPO uses an adaptive baseline that
enforces a minimum quality threshold, ensuring failed solutions are never
positively reinforced. Once the policy consistently meets this threshold, the
baseline automatically transitions to a relative preference mode, pushing the
model to find optimal solutions rather than just "acceptable" ones. We
empirically validate CoRPO on a code verification task, where it demonstrates
more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to
enable LLMs to learn genuinely new capabilities through reinforcement learning.
We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback
- progressing from binary to ordinal rewards in this work, and onward to
denser, per-step supervision.

</details>


### [26] [Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context](https://arxiv.org/abs/2511.04464)
*Carnot Braun,Rafael O. Jarczewski,Gabriel U. Talasso,Leandro A. Villas,Allan M. de Souza*

Main category: cs.AI

TL;DR: PAVe系统结合传统路径规划算法与LLM语义推理，通过多目标Dijkstra算法生成候选路线，再由LLM代理根据用户任务、偏好和规避规则进行个性化路线优化。


<details>
  <summary>Details</summary>
Motivation: 传统车辆路径系统只能优化单一指标，缺乏对复杂语义和动态上下文的理解能力，无法满足人类驾驶员的个性化需求。

Method: 使用多目标（时间、CO2）Dijkstra算法生成候选路线，然后通过LLM代理结合预处理的地理空间POI缓存，根据用户任务、偏好和规避规则评估路线选项。

Result: 在现实城市场景基准测试中，PAVe成功将复杂用户意图转化为适当的路线修改，使用本地模型在初始路线选择中达到超过88%的准确率。

Conclusion: 将经典路由算法与基于LLM的语义推理层相结合，是创建个性化、自适应和可扩展城市移动优化解决方案的稳健有效方法。

Abstract: Traditional vehicle routing systems efficiently optimize singular metrics
like time or distance, and when considering multiple metrics, they need more
processes to optimize . However, they lack the capability to interpret and
integrate the complex, semantic, and dynamic contexts of human drivers, such as
multi-step tasks, situational constraints, or urgent needs. This paper
introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a
hybrid agentic assistant designed to augment classical pathfinding algorithms
with contextual reasoning. Our approach employs a Large Language Model (LLM)
agent that operates on a candidate set of routes generated by a multi-objective
(time, CO2) Dijkstra algorithm. The agent evaluates these options against
user-provided tasks, preferences, and avoidance rules by leveraging a
pre-processed geospatial cache of urban Points of Interest (POIs). In a
benchmark of realistic urban scenarios, PAVe successfully used complex user
intent into appropriate route modifications, achieving over 88% accuracy in its
initial route selections with a local model. We conclude that combining
classical routing algorithms with an LLM-based semantic reasoning layer is a
robust and effective approach for creating personalized, adaptive, and scalable
solutions for urban mobility optimization.

</details>


### [27] [Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis](https://arxiv.org/abs/2511.04481)
*Lars Krupp,Daniel Geißler,Vishal Banwari,Paul Lukowicz,Jakob Karolus*

Main category: cs.AI

TL;DR: 本文首次探讨了网络代理的能耗和碳排放问题，通过理论和实证分析揭示了不同网络代理设计理念对能源消耗的显著影响，并指出能耗与性能不一定成正比。


<details>
  <summary>Details</summary>
Motivation: 尽管网络代理研究蓬勃发展，但其可持续性问题仍被忽视。本文旨在揭示网络代理的能源和碳排放成本，强调该问题的紧迫性。

Method: 采用理论估计和实证基准测试相结合的方法，分析不同网络代理的能耗表现，并指出模型参数和过程透明度不足限制了准确能耗评估。

Result: 研究显示不同网络代理设计理念会严重影响能源消耗，且更高能耗不一定带来更好结果。现有基准测试缺乏专门的能耗度量指标。

Conclusion: 需要改变网络代理评估方式，在基准测试中加入专门的能耗度量指标，并提高模型参数和过程的透明度以支持准确的能耗评估。

Abstract: Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful
agentic systems pushing the boundaries of Large Language Models (LLM). They can
autonomously interact with the internet at the user's behest, such as
navigating websites, filling search masks, and comparing price lists. Though
web agent research is thriving, induced sustainability issues remain largely
unexplored. To highlight the urgency of this issue, we provide an initial
exploration of the energy and $CO_2$ cost associated with web agents from both
a theoretical -via estimation- and an empirical perspective -by benchmarking.
Our results show how different philosophies in web agent creation can severely
impact the associated expended energy, and that more energy consumed does not
necessarily equate to better results. We highlight a lack of transparency
regarding disclosing model parameters and processes used for some web agents as
a limiting factor when estimating energy consumption. Our work contributes
towards a change in thinking of how we evaluate web agents, advocating for
dedicated metrics measuring energy consumption in benchmarks.

</details>


### [28] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: LLMs在游戏理论实验中能够复制人类合作模式，Llama模型能高保真再现人类偏离理性选择理论的行为，而Qwen更接近纳什均衡预测，无需角色提示即可实现群体行为复制。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs与人类决策的匹配程度至关重要，因为不匹配可能导致实际应用中的有害结果，而无法复制人类行为会使LLMs在社会模拟中无效。

Method: 开发游戏理论实验的数字孪生，引入系统化的提示和探测框架进行机器行为评估，测试三个开源模型（Llama、Mistral和Qwen）。

Result: Llama能高保真再现人类合作模式，捕捉人类偏离理性选择理论的行为；Qwen与纳什均衡预测高度一致；无需角色提示即可实现群体行为复制；能够生成和预注册新游戏配置的可测试假设。

Conclusion: 适当校准的LLMs能够复制聚合的人类行为模式，并系统探索未开发的实验空间，为社会科学研究提供补充方法，生成关于人类社交决策的新实证预测。

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [29] [Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach](https://arxiv.org/abs/2511.04556)
*Zihang Ding,Kun Zhang*

Main category: cs.AI

TL;DR: 提出了一种数据驱动的稀疏传感框架，结合EPA-SWMM模型，通过优化传感器布局来重建城市雨水系统中的峰值流量，在资源受限条件下实现高精度流量监测。


<details>
  <summary>Details</summary>
Motivation: 城市地表水洪水日益频繁，但高时空分辨率的洪水预测和监测受到时间、预算和技术限制。如何在资源受限条件下监测城市排水网络并预测流量状况是一个主要挑战。

Method: 使用SWMM模型生成训练数据集，应用数据驱动稀疏传感框架（包括奇异值分解降维和QR分解传感器分配），识别最优监测节点，并验证重建性能。

Result: 在77个节点中仅需3个优化布置的传感器，即可实现满意的峰值流量重建性能（NSE值0.92-0.95），模型对测量不确定性具有良好鲁棒性。

Conclusion: 该框架平衡了计算效率和物理可解释性，能够以最少的传感器实现高精度流量重建，可进一步与预测模型集成，在有限传感资源下实现洪水预警和实时控制。

Abstract: Urban surface water flooding, triggered by intense rainfall overwhelming
drainage systems, is increasingly frequent and widespread. While flood
prediction and monitoring in high spatial-temporal resolution are desired,
practical constraints in time, budget, and technology hinder its full
implementation. How to monitor urban drainage networks and predict flow
conditions under constrained resource is a major challenge. This study presents
a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to
optimize sensor placement and reconstruct peak flowrates in a stormwater
system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case
study. We utilized a SWMM model to generate a training dataset of peak flowrate
profiles across the stormwater network. Furthermore, we applied DSS -
leveraging singular value decomposition for dimensionality reduction and QR
factorization for sensor allocation - to identify the optimal monitoring nodes
based on the simulated training dataset. We then validated the
representativeness of these identified monitoring nodes by comparing the
DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three
optimally placed sensors among 77 nodes achieved satisfactory reconstruction
performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to
75th percentiles). In addition, the model showed good robustness to uncertainty
in measurements. Its robustness to sensor failures is location-dependent and
improves with the number of sensors deployed. The framework balances
computational efficiency and physical interpretability, enabling high-accuracy
flow reconstruction with minimal sensors. This DSS framework can be further
integrated with predictive models to realize flood early warning and real-time
control under limited sensing and monitoring resource.

</details>


### [30] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: 开发了Jr. AI Scientist系统，这是一个模拟学生研究流程的自主AI科学家系统，能够分析论文局限性、提出假设、实验验证并撰写论文。评估显示其表现优于现有全自动系统，但也揭示了当前AI科学家系统的局限性和风险。


<details>
  <summary>Details</summary>
Motivation: 理解当前AI科学家系统的能力和风险对于确保可信赖和可持续的AI驱动科学进步至关重要，同时需要保护学术生态系统的完整性。

Method: 开发Jr. AI Scientist系统，模拟学生研究流程：分析基线论文局限性、制定改进假设、通过严格实验验证、撰写结果论文。利用现代编码代理处理复杂多文件实现。

Result: Jr. AI Scientist生成的论文在AI评审中获得比现有全自动系统更高的评分，但作者评估和Agents4Science评审都发现了重要局限性。

Conclusion: 当前AI科学家系统存在直接应用的风险和关键挑战，需要进一步研究。报告了开发过程中识别的各种风险，希望加深对AI科学家发展现状和风险的理解。

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


### [31] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 该论文将自然语言查询中的歧义重新定义为合作交互的特征，提出了区分可解析的合作查询与不可解析的非合作查询的原则性框架，并分析了15个流行数据集中的查询类型。


<details>
  <summary>Details</summary>
Motivation: 传统方法将查询歧义视为缺陷，而本文将其重新定义为合作交互的特征，认为查询规范的责任应由用户和系统共同承担。

Method: 开发了一个原则性框架来区分合作查询和非合作查询，并将该框架应用于15个流行数据集的查询分析。

Result: 发现现有数据集中的查询类型混合不当，既不适合评估系统执行准确性，也不适合评估解释能力。

Conclusion: 该框架将视角从修复歧义转向在解析查询中拥抱合作，为表格数据的自然语言接口提供了更明智的设计和评估方向。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [32] [Question the Questions: Auditing Representation in Online Deliberative Processes](https://arxiv.org/abs/2511.04588)
*Soham De,Lodewijk Gelauff,Ashish Goel,Smitha Milli,Ariel Procaccia,Alice Siu*

Main category: cs.AI

TL;DR: 提出了一个基于合理代表性(JR)概念的审计框架，用于衡量专家问答环节中问题选择的代表性质量，并开发了高效的审计算法。


<details>
  <summary>Details</summary>
Motivation: 在公民大会等审议过程中，参与者只能向专家提出有限数量的问题，如何选择最具代表性的问题子集是一个重要挑战。

Method: 开发了在一般效用设置下审计合理代表性的算法，最有效算法的时间复杂度为O(mn log n)，并将方法应用于历史审议数据进行比较分析。

Result: 比较了主持人选择的问题、整数线性规划选择的问题和LLM生成的问题的代表性，揭示了LLM在支持审议过程中的潜力和当前局限性。

Conclusion: 通过将审计方法集成到已在50多个国家使用的在线审议平台中，使实践者能够轻松审计和改进未来审议的代表性。

Abstract: A central feature of many deliberative processes, such as citizens'
assemblies and deliberative polls, is the opportunity for participants to
engage directly with experts. While participants are typically invited to
propose questions for expert panels, only a limited number can be selected due
to time constraints. This raises the challenge of how to choose a small set of
questions that best represent the interests of all participants. We introduce
an auditing framework for measuring the level of representation provided by a
slate of questions, based on the social choice concept known as justified
representation (JR). We present the first algorithms for auditing JR in the
general utility setting, with our most efficient algorithm achieving a runtime
of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number
of proposed questions. We apply our auditing methods to historical
deliberations, comparing the representativeness of (a) the actual questions
posed to the expert panel (chosen by a moderator), (b) participants' questions
chosen via integer linear programming, (c) summary questions generated by large
language models (LLMs). Our results highlight both the promise and current
limitations of LLMs in supporting deliberative processes. By integrating our
methods into an online deliberation platform that has been used for over
hundreds of deliberations across more than 50 countries, we make it easy for
practitioners to audit and improve representation in future deliberations.

</details>


### [33] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: DR.WELL是一个去中心化的神经符号框架，用于协作多智能体规划，通过两阶段协商协议和符号规划避免轨迹级协调的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 协作多智能体规划需要智能体在部分信息和有限通信下做出联合决策，轨迹级协调容易因微小偏差导致冲突，需要更高层次的抽象来确保同步和集体进展。

Method: 采用两阶段协商协议：智能体首先提出候选角色并给出理由，然后在共识和环境约束下承诺联合分配。承诺后，每个智能体独立生成和执行其角色的符号计划，通过共享世界模型将计划与执行结果关联。

Result: 在协作推块任务上的实验表明，智能体能够跨情景适应，动态世界模型捕获可重用模式，提高了任务完成率和效率，以时间开销换取更高效的协作策略。

Conclusion: 通过符号规划而非原始轨迹推理，DR.WELL避免了脆弱的步骤级对齐，实现了可重用、可同步和可解释的高层次操作，提高了多智能体协作的鲁棒性和效率。

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [34] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: VeriCoT是一个神经符号方法，通过将思维链推理形式化为一阶逻辑并验证其逻辑有效性，解决LLMs无法可靠验证自身逻辑的问题。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然能通过思维链进行多步推理，但无法可靠验证自身逻辑，即使在得出正确答案时底层推理也可能存在缺陷，这在高风险场景中会削弱信任。

Method: VeriCoT从思维链推理中提取形式逻辑论证，将每个推理步骤形式化为一阶逻辑，识别基于源上下文、常识知识或先前推理步骤的前提，使用自动求解器验证逻辑有效性。

Result: 在ProofWriter、LegalBench和BioASQ数据集上的实验表明，VeriCoT能有效识别有缺陷的推理，并作为最终答案正确性的强预测指标。

Conclusion: VeriCoT不仅能识别推理缺陷，还能通过验证信号支持推理时自我反思、监督微调和偏好微调，进一步提高推理有效性和准确性。

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [35] [Age of Job Completion Minimization with Stable Queues](https://arxiv.org/abs/2511.04630)
*Stavros Mitrolaris,Subhankar Banerjee,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究了具有马尔可夫状态机器的作业分配系统，提出了最小化作业完成时间和采样成本的策略，并分析了队列稳定性条件。


<details>
  <summary>Details</summary>
Motivation: 在中央服务器、多用户和状态变化的机器组成的系统中，需要优化作业完成效率并控制采样成本，同时确保系统稳定性。

Method: 提出了两种策略来最小化作业完成时间和采样成本，并建立了队列稳定性的充分条件。

Result: 通过数值评估验证了所提策略的性能，并确定了保证作业队列稳定的条件。

Conclusion: 所提出的策略能有效平衡作业完成效率与采样成本，并在满足特定条件下确保系统队列的稳定性。

Abstract: We consider a time-slotted job-assignment system with a central server, N
users and a machine which changes its state according to a Markov chain (hence
called a Markov machine). The users submit their jobs to the central server
according to a stochastic job arrival process. For each user, the server has a
dedicated job queue. Upon receiving a job from a user, the server stores that
job in the corresponding queue. When the machine is not working on a job
assigned by the server, the machine can be either in internally busy or in free
state, and the dynamics of these states follow a binary symmetric Markov chain.
Upon sampling the state information of the machine, if the server identifies
that the machine is in the free state, it schedules a user and submits a job to
the machine from the job queue of the scheduled user. To maximize the number of
jobs completed per unit time, we introduce a new metric, referred to as the age
of job completion. To minimize the age of job completion and the sampling cost,
we propose two policies and numerically evaluate their performance. For both of
these policies, we find sufficient conditions under which the job queues will
remain stable.

</details>


### [36] [Environment Division Multiple Access (EDMA): A Feasibility Study via Pinching Antennas](https://arxiv.org/abs/2511.03820)
*Zhiguo Ding,Robert Schober,H. V. Poor*

Main category: cs.IT

TL;DR: 提出了一种名为环境分割多址接入(EDMA)的新多址技术，利用无线传播环境的动态特性，通过捏合天线智能重构多用户传播环境，在不需复杂信号处理的情况下提高目标接收器信号强度并抑制多址干扰。


<details>
  <summary>Details</summary>
Motivation: 利用无线传播环境的动态特性作为多址技术的基础，避免传统方法所需的复杂信号处理（如预编码、波束成形或多用户检测），通过重构传播环境来提升系统性能。

Method: 采用捏合天线辅助的EDMA技术，通过将捏合天线放置在特定位置来有意识地阻挡干扰链路，重构视距链路。开发了两种低复杂度算法分别用于上行和下行传输。

Result: 推导了EDMA相对于传统多址接入的遍历和速率增益的闭式表达式，以及EDMA实现更大瞬时和速率的概率。仿真结果表明所提算法在性能上接近穷举搜索的最优解。

Conclusion: EDMA技术具有支持多用户通信的巨大潜力，通过智能重构传播环境可以有效提升系统性能，同时避免了复杂的信号处理需求。

Abstract: This paper exploits the dynamic features of wireless propagation environments
as the basis for a new multiple access technique, termed environment division
multiple access (EDMA). In particular, with the proposed
pinching-antenna-assisted EDMA, the multi-user propagation environment is
intelligently reconfigured to improve signal strength at intended receivers and
simultaneously suppress multiple-access interference, without requiring complex
signal processing, e.g., precoding, beamforming, or multi-user detection. The
key to creating a favorable propagation environment is to utilize the
capability of pinching antennas to reconfigure line-of-sight (LoS) links, e.g.,
pinching antennas are placed at specific locations, such that interference
links are blocked on purpose. Based on a straightforward choice of
pinching-antenna locations, the ergodic sum-rate gain of EDMA over conventional
multiple access and the probability that EDMA achieves a larger instantaneous
sum rate than the considered benchmarking scheme are derived in closed form.
The obtained analytical results demonstrate the significant potential of EDMA
for supporting multi-user communications. Furthermore, pinching antenna
location optimization is also investigated, since the locations of pinching
antennas are critical for reconfiguring LoS links and large-scale path losses.
Two low-complexity algorithms are developed for uplink and downlink
transmission, respectively, and simulation results are provided to show their
optimality in comparison to exhaustive searches.

</details>


### [37] [Which Similarity-Sensitive Entropy?](https://arxiv.org/abs/2511.03849)
*Phuc Nguyen,Josiah Couch,Rahul Bansal,Alexandra Morgan,Chris Tam,Miao Li,Rima Arnaout,Ramy Arnaout*

Main category: cs.IT

TL;DR: 本文比较了Leinster-Cobbold-Reeve (LCR)相似性敏感熵和Vendi评分(VS)两种方法，通过概念、分析和实验验证发现两者可以相差几个数量级并捕获互补信息，建议在大多数情况下优先使用LCR方法。


<details>
  <summary>Details</summary>
Motivation: 传统熵度量只捕获系统元素频率信息，而LCR和VS方法能够捕获元素间相似性和差异性的丰富信息，但两者比较和选择标准尚不明确。

Method: 使用53个机器学习数据集进行概念分析、理论证明和实验验证，引入"半距离"概念来参数化相似性缩放的影响。

Result: LCR和VS可以相差几个数量级，捕获互补信息；VS为LCR提供上界；两者都依赖于相似性缩放方式。

Conclusion: VS仅在将元素解释为更基本"原元素"的线性组合或系统具有量子力学特性时更优；在大多数相似性信息捕获场景中，LCR是首选方法。

Abstract: A canonical step in quantifying a system is to measure its entropy. Shannon
entropy and other traditional entropy measures capture only the information
encoded in the frequencies of a system's elements. Recently, Leinster, Cobbold,
and Reeve (LCR) introduced a method that also captures the rich information
encoded in the similarities and differences among elements, yielding
similarity-sensitive entropy. More recently, the Vendi score (VS) was
introduced as an alternative, raising the question of how LCR and VS compare,
and which is preferable. Here we address these questions conceptually,
analytically, and experimentally, using 53 machine-learning datasets. We show
that LCR and VS can differ by orders of magnitude and can capture complementary
information about a system, except in limiting cases. We demonstrate that both
LCR and VS depend on how similarities are scaled and introduce the concept of
``half distance'' to parameterize this dependence. We prove that VS provides an
upper bound on LCR for several values of the R\'enyi-Hill order parameter and
conjecture that this bound holds for all values. We conclude that VS is
preferable only when interpreting elements as linear combinations of a more
fundamental set of ``ur-elements'' or when the system or dataset possesses a
quantum-mechanical character. In the broader circumstance where one seeks
simply to capture the rich information encoded by similarity, LCR is favored;
nevertheless, for certain half-distances the two methods can complement each
other.

</details>


### [38] [Efficient and rate-optimal list-decoding in the presence of minimal feedback: Weldon and Slepian-Wolf in sheep's clothing](https://arxiv.org/abs/2511.04088)
*Pranav Joshi,Daniel McMorrow,Yihan Zhang,Amitalok J. Budkuley,Sidharth Jaggi*

Main category: cs.IT

TL;DR: 本文提出了第一个适用于任意q≥2的编码方案，在存在少量反馈的情况下，能够以接近信息论最优的速率在对抗性信道中进行通信，同时保证接收方能够将发送方的消息限定在一个小集合内。


<details>
  <summary>Details</summary>
Motivation: 现有编码方案主要针对大q值，对于任意q≥2的情况缺乏有效的编码方案。本文旨在解决在任意q值下，利用低速率反馈实现接近信息论最优速率的可靠通信问题。

Method: 提出了一种最小反馈方案，利用渐近可忽略的反馈速率（相对于传输次数），通过精心设计的编码结构和参数选择，实现了高效的编码和解码过程。

Result: 方案实现了速率1-H_q(ϱ)-ε（接近信息论最优），列表大小exp(O(ε^{-3/2}log²(1/ε)))，编码/解码计算复杂度n^{O(ε^{-1}log(1/ε))}，存储复杂度O(n^{η+1}log n)，错误概率O(n^{-η})，反馈速率O(1/log n)。

Conclusion: 该工作首次证明了对于任意q≥2，利用低速率反馈可以实现接近信息论最优速率的可靠通信，为对抗性信道中的编码设计提供了新的解决方案。

Abstract: Given a channel with length-$n$ inputs and outputs over the alphabet
$\{0,1,\ldots,q-1\}$, and of which a fraction $\varrho \in (0,1-1/q)$ of
symbols can be arbitrarily corrupted by an adversary, a fundamental problem is
that of communicating at rates close to the information-theoretically optimal
values, while ensuring the receiver can infer that the transmitter's message is
from a ``small" set. While the existence of such codes is known, and
constructions with computationally tractable encoding/decoding procedures are
known for large $q$, we provide the first schemes that attain this performance
for any $q \geq 2$, as long as low-rate feedback (asymptotically negligible
relative to the number of transmissions) from the receiver to the transmitter
is available. For any sufficiently small $\varepsilon > 0$ and $\varrho \in
(1-1/q-\Theta(\sqrt{\varepsilon})$ our minimal feedback scheme has the
following parameters: Rate $1-H_q(\varrho) - \varepsilon$ (i.e.,
$\varepsilon$-close to information-theoretically optimal -- here $H_q(\varrho)$
is the $q$-ary entropy function), list-size
$\exp(\mathcal{O}(\varepsilon^{-3/2}\log^2(1/\varepsilon))$, computational
complexity of encoding/decoding
$n^{\mathcal{O}(\varepsilon^{-1}\log(1/\varepsilon))}$, storage complexity
$\mathcal{O}(n^{\eta+1}\log n)$ for a code design parameter $\eta>1$ that
trades off storage complexity with the probability of error. The error
probability is $\mathcal{O}(n^{-\eta})$, and the (vanishing) feedback rate is
$\mathcal{O}(1/ \log n)$.

</details>


### [39] [List Decoding of Folded Reed-Solomon Codes Over Galois Ring](https://arxiv.org/abs/2511.04135)
*Chen Yuan,Ruiqi Zhu*

Main category: cs.IT

TL;DR: 本文扩展了Guruswami-Sudan列表解码算法到伽罗瓦环上的Reed-Solomon码，证明了速率为r的RS码可以达到1-√r的列表解码半径，并研究了折叠RS码在伽罗瓦环上的列表解码，达到了Singleton界，同时将列表大小改进到O(1/ε²)。


<details>
  <summary>Details</summary>
Motivation: 由于零知识证明系统的进展，需要研究伽罗瓦环上码的邻近间隙问题。有限域上的RS码列表解码已有较好结果，但伽罗瓦环上的研究较少，这阻碍了基于环的算术电路的零知识证明系统发展。

Method: 1. 将Guruswami-Sudan列表解码过程扩展到伽罗瓦环上的Reed-Solomon码；2. 研究伽罗瓦环上折叠Reed-Solomon码的列表解码；3. 将Shashank Srivastava(2025)的工作扩展到伽罗瓦环，改进列表大小。

Result: 1. 证明了伽罗瓦环上速率为r的RS码可以达到1-√r的列表解码半径；2. 折叠RS码的列表解码半径达到了Singleton界；3. 将折叠RS码的列表大小改进到O(1/ε²)。

Conclusion: 成功将列表解码技术从有限域扩展到伽罗瓦环，为基于环的零知识证明系统提供了理论基础，解决了伽罗瓦环上码的邻近间隙问题。

Abstract: List decoding of codes can be seen as the generalization of unique decoding
of codes While list decoding over finite fields has been extensively studied,
extending these results to more general algebraic structures such as Galois
rings remains an important challenge. Due to recent progress in zero knowledge
systems, there is a growing demand to investigate the proximity gap of codes
over Galois rings in Yizhou Yao and coauthors(2025), Alexander Golovne and
coauthors(2023), Yuanju Wei and coauthors(2025). The proximity gap is closely
related to the decoding capability of codes. It was shown in Eli Ben-Sasson and
coauthors(2020) that the proximity gap for RS codes over finite field can be
improved to $1-\sqrt{r}$ if one consider list decoding instead of unique
decoding. However, we know very little about RS codes over Galois ring which
might hinder the development of zero knowledge proof system for ring-based
arithmetic circuit. In this work, we first extend the list decoding procedure
of Guruswami and Sudan to Reed-Solomon codes over Galois rings, which shows
that RS codes with rate $r$ can be list decoded up to radius $1-\sqrt{r}$.
Then, we investigate the list decoding of folded Reed-Solomon codes over Galois
rings. We show that the list decoding radius of folded Reed-Solomon codes can
reach the Singlton bound as its counterpart over finite field. Finally, we
improve the list size of our folded Reed-Solomon code to
$O(\frac{1}{\varepsilon^2})$ by extending recent work in Shashank
Srivastava(2025) to Galois Rings.

</details>


### [40] [Affine Frequency Division Multiplexing: From Communication to Sensing](https://arxiv.org/abs/2511.04471)
*Ali Bemani,Nassar Ksairi,Marios Kountouris*

Main category: cs.IT

TL;DR: AFDM波形在集成感知与通信系统中解决两个关键挑战：大带宽下的低复杂度接收和抗多雷达干扰。


<details>
  <summary>Details</summary>
Motivation: 解决ISAC系统中大带宽需求带来的高接收器复杂度和能耗问题，以及多雷达环境中的干扰问题。

Method: 利用AFDM波形的特性：支持低复杂度自干扰消除、模拟去啁啾降低采样率、子奈奎斯特采样保持延迟分辨率，以及DAFT的资源分配灵活性。

Result: AFDM在单站感知中实现低复杂度SIC和降低采样率，在双站感知中支持子奈奎斯特采样，并能有效管理多雷达干扰。

Conclusion: AFDM波形为ISAC系统提供了有效的解决方案，能够平衡感知性能和系统复杂度。

Abstract: Affine Frequency Division Multiplexing (AFDM) has been proposed as an
effective waveform for achieving the full diversity of doubly-dispersive
(delay-Doppler) channels. While this property is closely related to range and
velocity estimation in sensing, this article focuses on other AFDM features
that are particularly relevant for addressing two challenges in integrated
sensing and communication (ISAC) systems: (1) maintaining receiver complexity
and energy consumption at acceptable levels while supporting the large
bandwidths required for high delay/range resolution, and (2) mitigating
interference in multiradar environments. In monostatic sensing, where direct
transmitter-receiver leakage is a major impairment, we show that AFDM-based
ISAC receivers can address the first challenge through their compatibility with
low-complexity self-interference cancellation (SIC) schemes and reduced
sampling rates via analog dechirping. In bistatic sensing, where such analog
solutions may not be feasible, we demonstrate that AFDM supports sub-Nyquist
sampling without requiring hardware modifications while preserving delay
resolution. Finally, we show that the second challenge can be addressed by
leveraging the resource-assignment flexibility of the discrete affine Fourier
transform (DAFT) underlying the AFDM waveform.

</details>
