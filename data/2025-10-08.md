<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.IT](#cs.IT) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks](https://arxiv.org/abs/2510.05625)
*Yao Zhang,Yuchen Song,Shengnan Li,Yan Shi,Shikui Shen,Xiongyan Tang,Min Zhang,Danshi Wang*

Main category: cs.NI

TL;DR: 提出了一种基于生成式人工智能的分层多智能体框架，用于实现零接触光网络的多任务自主执行，并在实际部署的网状网络中验证了其在规划、运营和升级阶段的典型应用场景。


<details>
  <summary>Details</summary>
Motivation: 随着光网络规模的扩大和传输带宽的增加，需要实现高水平自主操作和零接触管理。现有的单智能体生成式AI系统难以应对光网络生命周期管理中的多任务需求和跨层协作挑战。

Method: 设计了一个生成式AI驱动的分层多智能体框架，包括架构设计、实现方法和应用场景。在实际部署的网状网络中验证了三个典型场景：规划阶段的光传输质量估计、运营阶段的动态信道增删、升级阶段的系统容量提升。

Result: 案例研究表明，该多智能体框架在多任务分配、协调、执行、评估和总结方面展现出强大能力，能够有效管理光网络生命周期中的复杂任务。

Conclusion: 这项工作为未来智能、高效、协作的网络管理解决方案提供了有前景的方法，为更专业和自适应的零接触光网络铺平了道路。

Abstract: The rapid development of Generative Artificial Intelligence (GenAI) has
catalyzed a transformative technological revolution across all walks of life.
As the backbone of wideband communication, optical networks are expecting
high-level autonomous operation and zero-touch management to accommodate their
expanding network scales and escalating transmission bandwidth. The integration
of GenAI is deemed as the pivotal solution for realizing zero-touch optical
networks. However, the lifecycle management of optical networks involves a
multitude of tasks and necessitates seamless collaboration across multiple
layers, which poses significant challenges to the existing single-agent GenAI
systems. In this paper, we propose a GenAI-driven hierarchical multi-agent
framework designed to streamline multi-task autonomous execution for zero-touch
optical networks. We present the architecture, implementation, and applications
of this framework. A field-deployed mesh network is utilized to demonstrate
three typical scenarios throughout the lifecycle of optical network: quality of
transmission estimation in the planning stage, dynamic channel adding/dropping
in the operation stage, and system capacity increase in the upgrade stage. The
case studies, illustrate the capabilities of multi-agent framework in
multi-task allocation, coordination, execution, evaluation, and summarization.
This work provides a promising approach for the future development of
intelligent, efficient, and collaborative network management solutions, paving
the way for more specialized and adaptive zero-touch optical networks.

</details>


### [2] [Rivaling Transformers: Multi-Scale Structured State-Space Mixtures for Agentic 6G O-RAN](https://arxiv.org/abs/2510.05255)
*Farhad Rezazadeh,Hatim Chergui,Merouane Debbah,Houbing Song,Dusit Niyato,Lingjia Liu*

Main category: cs.NI

TL;DR: 提出了一种轻量级多尺度结构化状态空间混合(MS3M)预测器，用于6G O-RAN中的实时KPI预测，相比Transformer基线在保持准确性的同时实现了3-10倍的延迟降低。


<details>
  <summary>Details</summary>
Motivation: 解决6G O-RAN中在近实时延迟和计算约束下提供控制级预测的挑战，特别是在多时间尺度动态下的需求。

Method: 使用HiPPO-LegS核混合的多尺度结构化状态空间模型，通过双线性离散化构建稳定离散状态空间模型，应用因果脉冲响应作为逐特征深度卷积，结合Squeeze-and-Excitation门控和紧凑门控通道混合层。

Result: 在专用O-RAN测试床数据集上评估，MS3M实现0.057秒的单次推理延迟和0.70M参数，比Transformer基线延迟降低3-10倍，同时保持竞争性准确性。

Conclusion: MS3M为O-RAN约束提供了一种高效的KPI预测解决方案，在延迟和计算效率方面显著优于Transformer模型。

Abstract: In sixth-generation (6G) Open Radio Access Networks (O-RAN), proactive
control is preferable. A key open challenge is delivering control-grade
predictions within Near-Real-Time (Near-RT) latency and computational
constraints under multi-timescale dynamics. We therefore cast RAN Intelligent
Controller (RIC) analytics as an agentic perceive-predict xApp that turns
noisy, multivariate RAN telemetry into short-horizon per-User Equipment (UE)
key performance indicator (KPI) forecasts to drive anticipatory control. In
this regard, Transformers are powerful for sequence learning and time-series
forecasting, but they are memory-intensive, which limits Near-RT RIC use.
Therefore, we need models that maintain accuracy while reducing latency and
data movement. To this end, we propose a lightweight Multi-Scale Structured
State-Space Mixtures (MS3M) forecaster that mixes HiPPO-LegS kernels to capture
multi-timescale radio dynamics. We develop stable discrete state-space models
(SSMs) via bilinear (Tustin) discretization and apply their causal impulse
responses as per-feature depthwise convolutions. Squeeze-and-Excitation gating
dynamically reweights KPI channels as conditions change, and a compact gated
channel-mixing layer models cross-feature nonlinearities without
Transformer-level cost. The model is KPI-agnostic -- Reference Signal Received
Power (RSRP) serves as a canonical use case -- and is trained on sliding
windows to predict the immediate next step. Empirical evaluations conducted
using our bespoke O-RAN testbed KPI time-series dataset (59,441 windows across
13 KPIs). Crucially for O-RAN constraints, MS3M achieves a 0.057 s
per-inference latency with 0.70M parameters, yielding 3-10x lower latency than
the Transformer baselines evaluated on the same hardware, while maintaining
competitive accuracy.

</details>


### [3] [Impact of Packet Loss and Timing Errors on Scheduled Periodic Traffic with Time-Aware Shaping (TAS) in Time-Sensitive Networking (TSN)](https://arxiv.org/abs/2510.05290)
*Manuel Eppler,Steffen Lindner,Lukas Osswald,Thomas Stüber,Michael Menth*

Main category: cs.NI

TL;DR: 本文揭示了TSN网络中时间感知整形器(TAS)对高优先级流量传输时隙限制的假设可能导致严重问题，如长排队延迟或丢包，特别是在出现故障帧时。


<details>
  <summary>Details</summary>
Motivation: 现有TSN调度算法通常假设TAS会限制高优先级流量的传输时隙，但作者发现这种假设在出现故障帧时会导致严重问题。

Method: 通过构建最小示例说明故障帧在单链路上的基本影响，展示其如何在网络中传播，并使用仿真验证单个延迟帧可能导致多链路丢包。

Result: 研究表明，当高优先级流量出现故障帧时，TAS的传输时隙限制会导致排队延迟显著增加甚至丢包，这些问题会通过网络传播造成远程影响。

Conclusion: 通过将TAS的高优先级流量传输时隙配置得比实际需要更长或不加限制，可以缓解或避免这些问题。

Abstract: Time-Sensitive Networking (TSN) is a collection of mechanisms to enhance the
realtime transmission capability of Ethernet networks. TSN combines priority
queuing, traffic scheduling, and the Time-Aware Shaper (TAS) to carry periodic
traffic with ultra-low latency and jitter. That is, so-called Talkers send
periodic traffic with highest priority according to a schedule. The schedule is
designed such that the scheduled traffic is forwarded by the TSN bridges with
no or only little queuing delay. To protect that traffic against other frames,
the TAS is configured on all interfaces such that lower-priority queues can
send only when high-priority traffic is not supposed to be forwarded. In the
literature on scheduling algorithms for the TAS there is mostly the explicit or
implicit assumption that the TAS also limits transmission slots of
high-priority traffic.
  In this paper we show that this assumption can lead to tremendous problems
like very long queuing delay or even packet loss in case of faulty frames. A
faulty frame arrives too early or too late according to the schedule, it is
missing or additional. We construct minimal examples to illustrate basic
effects of faulty frames on a single link and demonstrate how this effect can
propagate through the networks and cause remote problems. We further show using
simulations that a single slightly delayed frame may lead to frame loss on
multiple links. We show that these problems can be alleviated or avoided when
TAS-based transmission slots for high-priority traffic are configured longer
than needed or if they are not limited at all.

</details>


### [4] [On Enhancing Delay SLAs in TCP Networks through Joint Routing and Transport Assistant Deployment](https://arxiv.org/abs/2510.05686)
*José Gómez-delaHiz,Mohamed Faten Zhani,Jaime Galán-Jiménez,John Kaippallimalil*

Main category: cs.NI

TL;DR: 该论文提出联合路由和传输助手部署的方法，以减少TCP重传延迟，在尽力而为网络中最小化数据包交付延迟，在QoS网络中满足延迟SLA。


<details>
  <summary>Details</summary>
Motivation: TCP的重传机制在数据包丢失时会导致高延迟，现有研究提出的传输助手功能可以缓存和重传丢失的数据包，但需要优化其部署位置。

Method: 为两种网络场景（尽力而为和QoS）制定整数线性规划问题，并提出针对大规模问题的启发式解决方案。

Result: 通过广泛仿真证明，联合路由和TA部署可将数据包交付延迟降低高达16.4%，同时将部署成本降低高达60.98%。

Conclusion: 联合路由和传输助手部署能有效减少数据包交付延迟并优化部署成本，适用于不同网络场景。

Abstract: The Transport Control Protocol has long been the primary transport protocol
for applications requiring performance and reliability over the Internet.
Unfortunately, due its retransmission mechanism, TCP incurs high packet
delivery delays when segments are lost. To address this issue, previous
research proposed to use a novel network function, namely Transport Assistant,
deployed within the network to cache and retransmit lost packets, thus reducing
retransmission delays. In this paper, we propose to jointly route the flows and
deploy TAs in order to minimize packet delivery delays in best-effort networks
(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based
networks (scenario 2). We hence formulate the joint routing and TA deployment
problem as Integer Linear Program for the two scenarios and propose a heuristic
solution for large-scale instances of the problem. Through extensive
simulations, we demonstrate the benefits of performing joint routing flows and
TA deployment in reducing packet delivery delays (up to 16.4%) while minimizing
deployment costs (up to 60.98%).

</details>


### [5] [A Deep Q-Network based power control mechanism to Minimize RLF driven Handover Failure in 5G Network](https://arxiv.org/abs/2510.05762)
*Kotha Kartheek,Shankar K. Ghosh,Megha Iyengar,Vinod Sharma,Souvik Deb*

Main category: cs.NI

TL;DR: 提出基于深度Q网络的功率控制机制，通过调整发射功率来避免无线链路故障导致的切换失败，显著减少RLF和HF。


<details>
  <summary>Details</summary>
Motivation: 无线链路故障(RLF)是导致切换失败(HF)的主要原因，但在切换算法设计中常被忽略。RLF在切换过程中被检测到会导致HF。

Method: 使用深度强化学习(DQN)，以切换参数(时间准备、时间执行、准备偏移、执行偏移)和无线链路监控参数(T310和N310)作为输入，决策是否增加发射功率来避免RLF驱动的HF。

Result: 仿真结果显示，传统条件切换算法结合提出的DRL功率控制算法，相比现有先进方法能显著减少RLF和后续HF。

Conclusion: 基于DRL的功率控制算法能有效避免无线链路故障导致的切换失败，提升切换性能。

Abstract: The impact of Radio link failure (RLF) has been largely ignored in designing
handover algorithms, although RLF is a major contributor towards causing
handover failure (HF). RLF can cause HF if it is detected during an ongoing
handover. The objective of this work is to propose an efficient power control
mechanism based on Deep Q-Network (DQN), considering handover parameters (i.e.,
time-to-preparation, time-to-execute, preparation offset, execution offset) and
radio link monitoring parameters (T310 and N310) as input. The proposed DRL
based power control algorithm decides on a possible increase of transmitting
power to avoid RLF driven HF. Simulation results show that the traditional
conditional handover, when equipped with the proposed DRL based power control
algorithm can significantly reduce both RLFs and subsequent HFs, as compared to
the existing state of the art approaches.

</details>


### [6] [Leveraging Generative AI for large-scale prediction-based networking](https://arxiv.org/abs/2510.05797)
*Mathias Thorsager,Israel Leyva-Mayorga,Petar Popovski*

Main category: cs.NI

TL;DR: 论文提出利用生成式AI重新定义网络层，从传统的数据包转发转向基于预测的网络架构，以解决吞吐量限制和不可控延迟问题。


<details>
  <summary>Details</summary>
Motivation: 传统网络层存在吞吐量限制和不可控延迟问题，而生成式AI能够通过预测性方法从根本上重新定义网络层的角色。

Method: 提出三个关键方向：设计高效的提示大小初始化协议；将GenAI用于确保数据及时交付；作为传统TCP拥塞控制算法的替代方案。

Result: 在实时图像内容传输场景中，使用GenAI辅助的网络节点可将到达目的地的流量提升超过100%。

Conclusion: 生成式AI有潜力作为网络层工具大规模应用，但需要解决多用户扩展和多模态数据处理的挑战。

Abstract: The traditional role of the network layer is to create an end-to-end route,
through which the intermediate nodes replicate and forward the packets towards
the destination. This role can be radically redefined by exploiting the power
of Generative AI (GenAI) to pivot towards a prediction-based network layer,
which addresses the problems of throughput limits and uncontrollable latency.
In the context of real-time delivery of image content, the use of GenAI-aided
network nodes has been shown to improve the flow arriving at the destination by
more than 100%. However, to successfully exploit GenAI nodes and achieve such
transition, we must provide solutions for the problems which arise as we scale
the networks to include large amounts of users and multiple data modalities
other than images. We present three directions that play a significant role in
enabling the use of GenAI as a network layer tool at a large scale. In terms of
design, we emphasize the need for initialization protocols to select the prompt
size efficiently. Next, we consider the use case of GenAI as a tool to ensure
timely delivery of data, as well as an alternative to traditional TCP
congestion control algorithms.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis](https://arxiv.org/abs/2510.05106)
*Joachim Diederich*

Main category: cs.AI

TL;DR: 论文通过信息论分析系统提示中规则编码对注意力机制和合规行为的影响，揭示了锚点冗余与注意力熵之间的权衡，提出了动态规则验证架构来提高合规输出的概率。


<details>
  <summary>Details</summary>
Motivation: 设计基于大语言模型的安全关键智能体需要超越简单的提示工程，需要理解规则编码如何影响注意力机制和合规行为。

Method: 采用信息论分析规则格式对注意力机制的影响，分析多种注意力架构（因果、双向、局部稀疏、核化、交叉注意力），结合动态规则验证架构。

Result: 发现低语法熵和高集中锚点的规则格式能降低注意力熵并提高指针保真度，但存在锚点冗余与注意力熵的基本权衡。通过动态规则验证可增加合规输出的渐近概率。

Conclusion: 强调需要原则性的锚点设计和双重执行机制，以保护基于LLM的智能体免受提示注入攻击，同时在不断发展的领域中保持合规性。

Abstract: The design of safety-critical agents based on large language models (LLMs)
requires more than simple prompt engineering. This paper presents a
comprehensive information-theoretic analysis of how rule encodings in system
prompts influence attention mechanisms and compliance behaviour. We demonstrate
that rule formats with low syntactic entropy and highly concentrated anchors
reduce attention entropy and improve pointer fidelity, but reveal a fundamental
trade-off between anchor redundancy and attention entropy that previous work
failed to recognize. Through formal analysis of multiple attention
architectures including causal, bidirectional, local sparse, kernelized, and
cross-attention mechanisms, we establish bounds on pointer fidelity and show
how anchor placement strategies must account for competing fidelity and entropy
objectives. Combining these insights with a dynamic rule verification
architecture, we provide a formal proof that hot reloading of verified rule
sets increases the asymptotic probability of compliant outputs. These findings
underscore the necessity of principled anchor design and dual enforcement
mechanisms to protect LLM-based agents against prompt injection attacks while
maintaining compliance in evolving domains.

</details>


### [8] [Structured Cognition for Behavioral Intelligence in Large Language Model Agents: Preliminary Study](https://arxiv.org/abs/2510.05107)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 提出了结构化认知循环（SCL）架构，将推理、记忆和控制功能分离，相比传统提示方法在多步骤任务中表现更可靠和可追踪。


<details>
  <summary>Details</summary>
Motivation: 现有框架将推理、记忆和控制功能混合在单一提示中，导致连贯性和可预测性降低，需要更清晰的架构来支持自主代理的多步骤任务。

Method: SCL架构将语言模型专用于推理，外部维护记忆，由轻量级控制器在目标导向循环中指导执行，允许中间结果存储、重访和检查后再行动。

Result: 在360个测试场景中，SCL任务成功率平均86.3%，优于基线方法的70-77%，目标保真度更高，冗余调用更少，中间状态重用更可靠，每100次工具调用中的无支持断言减少。

Conclusion: 架构分离可以在不依赖更大模型或更重提示的情况下提高可靠性和可追踪性，为扩展研究提供了初步指导。

Abstract: Large language models have advanced natural language understanding and
generation, yet their use as autonomous agents raises architectural challenges
for multi-step tasks. Existing frameworks often intertwine inference, memory,
and control in a single prompt, which can reduce coherence and predictability.
The Structured Cognitive Loop (SCL) is introduced as an alternative
architecture that separates these functions. In SCL, the language model is
dedicated to inference, memory is maintained externally, and execution is
guided by a lightweight controller within a goal-directed loop. This design
offloads cognitive load from the model and allows intermediate results to be
stored, revisited, and checked before actions are taken, providing a clearer
basis for traceability and evaluation.
  We evaluate SCL against prompt-based baselines including ReAct and common
LangChain agents across three scenarios: temperature-based travel planning,
email drafting with conditional send, and constraint-guided image generation.
All systems share the same base model and tools under matched decoding
settings. Across 360 episodes, SCL shows modest but consistent improvements.
Task success averages 86.3 percent compared with 70-77 percent for baselines.
Goal fidelity is higher, redundant calls are fewer, intermediate states are
reused more reliably, and unsupported assertions per 100 tool calls are
reduced. Ablations show that external memory and control each contribute
independently, and decoding sweeps confirm stability of the effects.
  These results suggest that architectural separation can improve reliability
and traceability without relying on larger models or heavier prompts. The
findings are preliminary and intended to guide extended studies with additional
models, longer horizons, multimodal tasks, and collaborative settings.

</details>


### [9] [Optimization Modeling via Semantic Anchored Alignment](https://arxiv.org/abs/2510.05115)
*Yansen Zhang,Qingcan Kang,Yujie Chen,Yufei Wang,Xiongwei Han,Tao Zhong,Mingxuan Yuan,Chen Ma*

Main category: cs.AI

TL;DR: SAC-Opt是一个基于语义锚点的后向引导修正框架，用于提高LLM生成优化模型代码的语义准确性，相比传统方法显著提升了建模精度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM优化建模方法主要依赖求解器驱动，通过单次前向生成和有限的基于求解器错误的后处理，存在未检测的语义错误，导致生成语法正确但逻辑有缺陷的模型。

Method: 提出SAC-Opt框架，通过将原始语义锚点与生成代码重构的锚点对齐，选择性修正不匹配的组件，实现语义驱动的细粒度约束和目标逻辑修正。

Result: 在7个公共数据集上的实验表明，SAC-Opt将平均建模精度提高了7.8%，在ComplexLP数据集上最高提升21.9%。

Conclusion: 语义锚点修正在基于LLM的优化工作流中至关重要，能确保从问题意图到求解器可执行代码的忠实转换。

Abstract: Large language models (LLMs) have opened new paradigms in optimization
modeling by enabling the generation of executable solver code from natural
language descriptions. Despite this promise, existing approaches typically
remain solver-driven: they rely on single-pass forward generation and apply
limited post-hoc fixes based on solver error messages, leaving undetected
semantic errors that silently produce syntactically correct but logically
flawed models. To address this challenge, we propose SAC-Opt, a backward-guided
correction framework that grounds optimization modeling in problem semantics
rather than solver feedback. At each step, SAC-Opt aligns the original semantic
anchors with those reconstructed from the generated code and selectively
corrects only the mismatched components, driving convergence toward a
semantically faithful model. This anchor-driven correction enables fine-grained
refinement of constraint and objective logic, enhancing both fidelity and
robustness without requiring additional training or supervision. Empirical
results on seven public datasets demonstrate that SAC-Opt improves average
modeling accuracy by 7.8\%, with gains of up to 21.9\% on the ComplexLP
dataset. These findings highlight the importance of semantic-anchored
correction in LLM-based optimization workflows to ensure faithful translation
from problem intent to solver-executable code.

</details>


### [10] [Structuring Reasoning for Complex Rules Beyond Flat Representations](https://arxiv.org/abs/2510.05134)
*Zhihao Yang,Ancheng Xu,Jingpeng Li,Liang Yan,Jiehui Zhou,Zhen Qin,Hengyun Chang,Ahmadreza Argha,Hamid Alinejad-Rokny,Minghuan Tan,Yujun Cai,Min Yang*

Main category: cs.AI

TL;DR: 提出了动态裁决模板(DAT)框架，通过定性分析、证据收集和裁决三个阶段，系统化处理复杂规则系统，解决了LLM在规则推理中的依赖关系忽略和错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂规则系统时面临挑战，通常将相互依赖的规则视为非结构化文本而非逻辑框架，导致推理发散和关键规则依赖关系的忽略。现有方法如思维链推理缺乏结构化规则处理的系统方法，容易在顺序推理链中传播错误。

Method: 提出动态裁决模板(DAT)框架，受专家人类推理过程启发，将推理机制结构化为三个方法阶段：定性分析（全面评估上下文环境）、证据收集（基于预定义模板元素的有针对性信息提取和系统验证）、裁决（综合验证组件形成全面判断）。

Result: 实证结果表明DAT在复杂基于规则的任务中持续优于传统思维链方法。值得注意的是，DAT使较小的语言模型能够匹配甚至在某些情况下超过显著更大的LLM的性能。

Conclusion: DAT框架在管理复杂规则系统方面具有高效性和有效性，能够显著提升语言模型在规则推理任务中的表现，特别是使较小模型达到或超过更大模型的性能水平。

Abstract: Large language models (LLMs) face significant challenges when processing
complex rule systems, as they typically treat interdependent rules as
unstructured textual data rather than as logically organized frameworks. This
limitation results in reasoning divergence, where models often overlook
critical rule dependencies essential for accurate interpretation. Although
existing approaches such as Chain-of-Thought (CoT) reasoning have shown
promise, they lack systematic methodologies for structured rule processing and
are particularly susceptible to error propagation through sequential reasoning
chains. To address these limitations, we propose the Dynamic Adjudication
Template (DAT), a novel framework inspired by expert human reasoning processes.
DAT structures the inference mechanism into three methodical stages:
qualitative analysis, evidence gathering, and adjudication. During the
qualitative analysis phase, the model comprehensively evaluates the contextual
landscape. The subsequent evidence gathering phase involves the targeted
extraction of pertinent information based on predefined template elements
([placeholder]), followed by systematic verification against applicable rules.
Finally, in the adjudication phase, the model synthesizes these validated
components to formulate a comprehensive judgment. Empirical results demonstrate
that DAT consistently outperforms conventional CoT approaches in complex
rule-based tasks. Notably, DAT enables smaller language models to match, and in
some cases exceed, the performance of significantly larger LLMs, highlighting
its efficiency and effectiveness in managing intricate rule systems.

</details>


### [11] [An Algorithmic Information-Theoretic Perspective on the Symbol Grounding Problem](https://arxiv.org/abs/2510.05153)
*Zhangchi Liu*

Main category: cs.AI

TL;DR: 本文通过算法信息理论重新定义符号接地问题，证明意义接地本质上受信息理论限制，统一了哥德尔自指和统计无免费午餐视角。


<details>
  <summary>Details</summary>
Motivation: 为符号接地问题提供一个统一的理论框架，揭示意义接地过程的信息理论本质约束。

Method: 将符号系统建模为通用图灵机，将接地定义为信息压缩过程，通过四个阶段的论证分析接地限制。

Result: 证明纯符号系统无法接地几乎所有可能世界，静态接地系统本质不完整，接地行为不可推断，任何算法学习过程都无法理解超过自身复杂度的世界。

Conclusion: 意义是系统不断尝试克服自身信息理论限制的开放过程。

Abstract: This paper provides a definitive, unifying framework for the Symbol Grounding
Problem (SGP) by reformulating it within Algorithmic Information Theory (AIT).
We demonstrate that the grounding of meaning is a process fundamentally
constrained by information-theoretic limits, thereby unifying the G\"odelian
(self-reference) and No Free Lunch (statistical) perspectives. We model a
symbolic system as a universal Turing machine and define grounding as an act of
information compression. The argument proceeds in four stages. First, we prove
that a purely symbolic system cannot ground almost all possible "worlds" (data
strings), as they are algorithmically random and thus incompressible. Second,
we show that any statically grounded system, specialized for compressing a
specific world, is inherently incomplete because an adversarial, incompressible
world relative to the system can always be constructed. Third, the "grounding
act" of adapting to a new world is proven to be non-inferable, as it requires
the input of new information (a shorter program) that cannot be deduced from
the system's existing code. Finally, we use Chaitin's Incompleteness Theorem to
prove that any algorithmic learning process is itself a finite system that
cannot comprehend or model worlds whose complexity provably exceeds its own.
This establishes that meaning is the open-ended process of a system perpetually
attempting to overcome its own information-theoretic limitations.

</details>


### [12] [Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework](https://arxiv.org/abs/2510.05158)
*Xin He,Liangliang You,Hongduan Tian,Bo Han,Ivor Tsang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: Lang-PINN是一个基于大语言模型的多智能体系统，能够从自然语言任务描述直接构建可训练的物理信息神经网络（PINNs），显著简化了PINN的构建过程。


<details>
  <summary>Details</summary>
Motivation: 现有的PINN构建过程劳动密集且容易出错，科学家需要将问题解释为PDE公式、设计架构和损失函数，并实现稳定的训练流程。现有LLM方法仅解决孤立步骤，缺乏端到端视角。

Method: Lang-PINN协调四个互补的智能体：PDE智能体将任务描述解析为符号PDE，PINN智能体选择架构，代码智能体生成模块化实现，反馈智能体执行和诊断错误进行迭代优化。

Result: 实验显示Lang-PINN比竞争基线显著降低误差并提高鲁棒性：均方误差降低3-5个数量级，端到端执行成功率提高50%以上，时间开销减少74%。

Conclusion: Lang-PINN能够将非正式任务陈述转化为可执行且可验证的PINN代码，为PINN构建提供了端到端的自动化解决方案。

Abstract: Physics-informed neural networks (PINNs) provide a powerful approach for
solving partial differential equations (PDEs), but constructing a usable PINN
remains labor-intensive and error-prone. Scientists must interpret problems as
PDE formulations, design architectures and loss functions, and implement stable
training pipelines. Existing large language model (LLM) based approaches
address isolated steps such as code generation or architecture suggestion, but
typically assume a formal PDE is already specified and therefore lack an
end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system
that builds trainable PINNs directly from natural language task descriptions.
Lang-PINN coordinates four complementary agents: a PDE Agent that parses task
descriptions into symbolic PDEs, a PINN Agent that selects architectures, a
Code Agent that generates modular implementations, and a Feedback Agent that
executes and diagnoses errors for iterative refinement. This design transforms
informal task statements into executable and verifiable PINN code. Experiments
show that Lang-PINN achieves substantially lower errors and greater robustness
than competitive baselines: mean squared error (MSE) is reduced by up to 3--5
orders of magnitude, end-to-end execution success improves by more than 50\%,
and reduces time overhead by up to 74\%.

</details>


### [13] [Representation Potentials of Foundation Models for Multimodal Alignment: A Survey](https://arxiv.org/abs/2510.05184)
*Jianglin Lu,Hailing Wang,Yi Xu,Yizhou Wang,Kuo Yang,Yun Fu*

Main category: cs.AI

TL;DR: 本文调查了基础模型的表征潜力，即其学习到的表征能够捕获单模态任务特定信息，同时为跨模态对齐和统一提供可迁移基础的能力。


<details>
  <summary>Details</summary>
Motivation: 基础模型通过大规模预训练学习高度可迁移的表征，研究表明这些表征在不同架构和模态间表现出显著相似性，这激发了对其表征潜力的系统性研究。

Method: 通过回顾代表性基础模型和关键度量指标，综合来自视觉、语言、语音、多模态和神经科学研究的实证证据，分析表征空间的结构规律性和语义一致性。

Result: 证据表明基础模型在其表征空间中经常表现出结构规律性和语义一致性，使其成为跨模态迁移和对齐的有力候选者。

Conclusion: 基础模型具有强大的表征潜力，能够支持跨模态的迁移和对齐，但仍存在开放问题和潜在挑战需要进一步研究。

Abstract: Foundation models learn highly transferable representations through
large-scale pretraining on diverse data. An increasing body of research
indicates that these representations exhibit a remarkable degree of similarity
across architectures and modalities. In this survey, we investigate the
representation potentials of foundation models, defined as the latent capacity
of their learned representations to capture task-specific information within a
single modality while also providing a transferable basis for alignment and
unification across modalities. We begin by reviewing representative foundation
models and the key metrics that make alignment measurable. We then synthesize
empirical evidence of representation potentials from studies in vision,
language, speech, multimodality, and neuroscience. The evidence suggests that
foundation models often exhibit structural regularities and semantic
consistencies in their representation spaces, positioning them as strong
candidates for cross-modal transfer and alignment. We further analyze the key
factors that foster representation potentials, discuss open questions, and
highlight potential challenges.

</details>


### [14] [Real-time Framework for Interoperable Semantic-driven Internet-of-Things in Smart Agriculture](https://arxiv.org/abs/2510.05187)
*Mohamed El-Dosuky*

Main category: cs.AI

TL;DR: 提出一个包含六个语义层的实时物联网框架，通过语义标注、互操作性和推理技术，帮助物联网设备理解数据含义和来源，特别适用于农业等动态环境。


<details>
  <summary>Details</summary>
Motivation: 物联网在农业等应用中面临数据收集和理解方面的挑战，需要让设备和传感器能够理解数据的含义和来源。

Method: 构建六层框架：感知层、语义标注层、互操作性层、传输层、语义推理层和应用层。使用语义算法标准化文件类型和识别同义词，采用模糊逻辑、Dempster-Shafer理论和贝叶斯网络进行推理。

Result: 开发了一个稳健的物联网数据管理解决方案，确保语义完整性并支持实时知识推理。

Conclusion: 该框架通过整合不确定性推理方法和语义互操作性技术，为推进物联网应用特别是农业应用提供了有价值的工具。

Abstract: The Internet of Things (IoT) has revolutionized various applications
including agriculture, but it still faces challenges in data collection and
understanding. This paper proposes a real-time framework with three additional
semantic layers to help IoT devices and sensors comprehend data meaning and
source. The framework consists of six layers: perception, semantic annotation,
interoperability, transportation, semantic reasoning, and application, suitable
for dynamic environments. Sensors collect data in the form of voltage, which is
then processed by microprocessors or microcontrollers in the semantic
annotation and preprocessing layer. Metadata is added to the raw data,
including the purpose, ID number, and application. Two semantic algorithms are
proposed in the semantic interoperability and ontologies layer: the
interoperability semantic algorithm for standardizing file types and the
synonym identification algorithm for identifying synonyms. In the
transportation layer, raw data and metadata are sent to other IoT devices or
cloud computing platforms using techniques like WiFi, Zigbee networks,
Bluetooth, and mobile communication networks. A semantic reasoning layer is
proposed to infer new knowledge from the existing data, using fuzzy logic,
Dempster-Shafer theory, and Bayesian networks. A Graphical User Interface (GUI)
is proposed in the application layer to help users communicate with and monitor
IoT sensors, devices, and new knowledge inferred. This framework provides a
robust solution for managing IoT data, ensuring semantic completeness, and
enabling real-time knowledge inference. The integration of uncertainty
reasoning methods and semantic interoperability techniques makes this framework
a valuable tool for advancing IoT applications in general and in agriculture in
particular.

</details>


### [15] [Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents](https://arxiv.org/abs/2510.05188)
*Wenda Xie,Chao Guo,Yanqing Jing. Junle Wang,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: Dramaturge是一个基于分层多LLM代理的任务导向分治方法，用于改进长篇叙事脚本的质量。它通过全局审查、场景级审查和分层协调修订三个阶段，采用自上而下的任务流程和从粗到精的迭代过程来确保上下文一致性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM已广泛用于创意内容生成，但单次生成过程难以产生高质量的长篇叙事。如何像编剧一样有效修订和改进长篇叙事脚本仍然是一个重大挑战，因为这需要全面理解整个上下文以识别全局结构问题和局部细节缺陷，并协调多个粒度和位置的修订。

Method: 提出Dramaturge方法，包含三个主要阶段：1) 全局审查阶段：把握整体故事情节和结构问题；2) 场景级审查阶段：识别详细场景和句子缺陷；3) 分层协调修订阶段：协调并整合整个脚本的结构和细节改进。采用自上而下的任务流程和从粗到精的迭代过程。

Result: 综合实验表明，Dramaturge在脚本级整体质量和场景级细节方面显著优于所有基线方法。该方法具有即插即用特性，可以轻松集成到现有方法中以改进生成的脚本。

Conclusion: Dramaturge通过分层多LLM代理的分治方法有效解决了长篇叙事脚本修订的挑战，确保了高层次的策略指导局部修改，保持了上下文一致性，显著提升了脚本质量。

Abstract: Although LLMs have been widely adopted for creative content generation, a
single-pass process often struggles to produce high-quality long narratives.
How to effectively revise and improve long narrative scripts like scriptwriters
remains a significant challenge, as it demands a comprehensive understanding of
the entire context to identify global structural issues and local detailed
flaws, as well as coordinating revisions at multiple granularities and
locations. Direct modifications by LLMs typically introduce inconsistencies
between local edits and the overall narrative requirements. To address these
issues, we propose Dramaturge, a task and feature oriented divide-and-conquer
approach powered by hierarchical multiple LLM agents. It consists of a Global
Review stage to grasp the overall storyline and structural issues, a
Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a
Hierarchical Coordinated Revision stage that coordinates and integrates
structural and detailed improvements throughout the script. The top-down task
flow ensures that high-level strategies guide local modifications, maintaining
contextual consistency. The review and revision workflow follows a
coarse-to-fine iterative process, continuing through multiple rounds until no
further substantive improvements can be made. Comprehensive experiments show
that Dramaturge significantly outperforms all baselines in terms of
script-level overall quality and scene-level details. Our approach is
plug-and-play and can be easily integrated into existing methods to improve the
generated scripts.

</details>


### [16] [Graph-based LLM over Semi-Structured Population Data for Dynamic Policy Response](https://arxiv.org/abs/2510.05196)
*Daqian Shi,Xiaolei Diao,Jinge Wu,Honghan Wu,Xiongfeng Tang,Felix Naughton,Paulina Bondaronek*

Main category: cs.AI

TL;DR: 提出了一种基于图推理的框架，将大语言模型与结构化人口属性和非结构化公众反馈相结合，用于公共卫生紧急情况下的智能人口健康监测。


<details>
  <summary>Details</summary>
Motivation: 在COVID-19等公共卫生紧急情况下，及时准确的人口数据分析对有效决策至关重要。传统方法面临半结构化数据处理的挑战：专家评估效率低，标准NLP管道需要大量标注数据且泛化能力差。

Method: 开发了一种弱监督的图推理框架，将大语言模型与结构化人口属性和非结构化反馈集成，动态建模公民需求为需求感知图，基于年龄、性别、多重剥夺指数等关键特征进行人口特定分析。

Result: 使用真实世界数据集进行测试，初步实验结果表明该方法的可行性。

Conclusion: 该方法为资源受限的临床和政府环境中的智能人口健康监测提供了可扩展的解决方案，能够生成可解释的见解来指导响应性健康政策决策。

Abstract: Timely and accurate analysis of population-level data is crucial for
effective decision-making during public health emergencies such as the COVID-19
pandemic. However, the massive input of semi-structured data, including
structured demographic information and unstructured human feedback, poses
significant challenges to conventional analysis methods. Manual expert-driven
assessments, though accurate, are inefficient, while standard NLP pipelines
often require large task-specific labeled datasets and struggle with
generalization across diverse domains. To address these challenges, we propose
a novel graph-based reasoning framework that integrates large language models
with structured demographic attributes and unstructured public feedback in a
weakly supervised pipeline. The proposed approach dynamically models evolving
citizen needs into a need-aware graph, enabling population-specific analyses
based on key features such as age, gender, and the Index of Multiple
Deprivation. It generates interpretable insights to inform responsive health
policy decision-making. We test our method using a real-world dataset, and
preliminary experimental results demonstrate its feasibility. This approach
offers a scalable solution for intelligent population health monitoring in
resource-constrained clinical and governmental settings.

</details>


### [17] [Efficient Prediction of Pass@k Scaling in Large Language Models](https://arxiv.org/abs/2510.05197)
*Joshua Kazdan,Rylan Schaeffer,Youssef Allouah,Colin Sullivan,Kyssen Yu,Noam Levi,Sanmi Koyejo*

Main category: cs.AI

TL;DR: 本文提出了一种改进的统计方法来预测AI模型在大规模采样时的能力和风险，解决了传统方法在数据有限情况下的预测精度问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型被大规模使用，需要准确预测模型在大量尝试时的行为和风险，但现有统计方法在数据有限时预测精度不足。

Method: 引入基于beta-二项分布的稳健估计框架，并提出动态采样策略，将更多预算分配给更难的问题。

Result: 该方法能够以更低的计算成本更可靠地预测罕见风险和能力。

Conclusion: 提出的稳健估计框架和动态采样策略显著提高了在有限数据下预测AI模型大规模行为的能力和风险准确性。

Abstract: Assessing the capabilities and risks of frontier AI systems is a critical
area of research, and recent work has shown that repeated sampling from models
can dramatically increase both. For instance, repeated sampling has been shown
to increase their capabilities, such as solving difficult math and coding
problems, but it has also been shown to increase their potential for harm, such
as being jailbroken. Such results raise a crucial question for both capability
and safety forecasting: how can one accurately predict a model's behavior when
scaled to a massive number of attempts, given a vastly smaller sampling budget?
This question is directly relevant to model providers, who serve hundreds of
millions of users daily, and to governmental regulators, who seek to prevent
harms. To answer this questions, we make three contributions. First, we find
that standard methods for fitting these laws suffer from statistical
shortcomings that hinder predictive accuracy, especially in data-limited
scenarios. Second, we remedy these shortcomings by introducing a robust
estimation framework, which uses a beta-binomial distribution to generate more
accurate predictions from limited data. Third, we propose a dynamic sampling
strategy that allocates a greater budget to harder problems. Combined, these
innovations enable more reliable prediction of rare risks and capabilities at a
fraction of the computational cost.

</details>


### [18] [Information-Theoretic Policy Pre-Training with Empowerment](https://arxiv.org/abs/2510.05996)
*Moritz Schneider,Robert Krug,Narunas Vaskevicius,Luigi Palmieri,Michael Volpp,Joschka Boedecker*

Main category: cs.AI

TL;DR: 本文提出使用折扣赋权作为强化学习的预训练信号，通过平衡短期和长期环境控制来提高下游任务的数据效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 赋权作为衡量智能体对环境潜在影响的信息理论指标，在无监督强化学习和技能学习中有广泛应用，但其作为预训练信号的研究有限。本文旨在探索赋权作为通用预训练策略的潜力。

Method: 引入折扣赋权概念，平衡智能体在短期和长期视野上的环境控制能力，并提出基于最大化折扣赋权的预训练范式来初始化策略。

Result: 实验证明，长期视野的赋权最大化策略具有数据效率和有效性，能显著提高下游任务的适应能力。

Conclusion: 赋权预训练为强化学习提供了一种有效的通用初始化策略，为未来扩展到高维复杂任务奠定了基础。

Abstract: Empowerment, an information-theoretic measure of an agent's potential
influence on its environment, has emerged as a powerful intrinsic motivation
and exploration framework for reinforcement learning (RL). Besides for
unsupervised RL and skill learning algorithms, the specific use of empowerment
as a pre-training signal has received limited attention in the literature. We
show that empowerment can be used as a pre-training signal for data-efficient
downstream task adaptation. For this we extend the traditional notion of
empowerment by introducing discounted empowerment, which balances the agent's
control over the environment across short- and long-term horizons. Leveraging
this formulation, we propose a novel pre-training paradigm that initializes
policies to maximize discounted empowerment, enabling agents to acquire a
robust understanding of environmental dynamics. We analyze empowerment-based
pre-training for various existing RL algorithms and empirically demonstrate its
potential as a general-purpose initialization strategy: empowerment-maximizing
policies with long horizons are data-efficient and effective, leading to
improved adaptability in downstream tasks. Our findings pave the way for future
research to scale this framework to high-dimensional and complex tasks, further
advancing the field of RL.

</details>


### [19] [Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment](https://arxiv.org/abs/2510.05283)
*Radha Gulhane,Sathish Reddy Indurthi*

Main category: cs.AI

TL;DR: 提出了一种混合奖励建模框架，结合模型奖励和规则奖励，通过多维度奖励信号来改进多模态大语言模型与人类偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于单一信号、模型奖励的方法存在置信度校准不足、无法捕捉人类偏好多样性、需要大量数据标注和奖励模型训练的问题。

Method: 集成两种互补的奖励范式：(1)模型奖励：从合成和人类反馈中预测标量或向量分数；(2)规则奖励：提供具有置信度的领域特定启发式正确性信号。此外还引入多维度奖励和广义长度惩罚奖励。

Result: 在3B模型家族中，在通用和数学推理任务上平均提升约9.5%，在数学基准测试上平均提升约16%。

Conclusion: 该混合奖励建模框架为通过强化学习策略优化对齐多模态大语言模型提供了一种灵活有效的方法。

Abstract: Aligning multimodal large language models (MLLMs) with human preferences
often relies on single-signal, model-based reward methods. Such monolithic
rewards often lack confidence calibration across domain-specific tasks, fail to
capture diverse aspects of human preferences, and require extensive data
annotation and reward model training. In this work, we propose a hybrid reward
modeling framework that integrates complementary reward paradigms: (i)
model-based rewards, where a learned reward model predicts scalar or vector
scores from synthetic and human feedback, and (ii) rule-based rewards, where
domain-specific heuristics provide explicit correctness signals with
confidence. Beyond accuracy, we further incorporate multi-aspect rewards to
enforce instruction adherence and introduce a generalized length-penalty reward
to stabilize training and improve performance. The proposed framework provides
a flexible and effective approach to aligning MLLMs through reinforcement
learning policy optimization. Our experiments show consistent improvements
across different multimodal benchmarks when applying hybrid and multi-aspect
reward modeling. Our best performing model in the 3B family achieves an overall
average improvement of ~9.5% across general and math reasoning tasks. Focusing
specifically on mathematical benchmarks, the model achieves a significant
average improvement of ~16%, highlighting its effectiveness in mathematical
reasoning and problem solving.

</details>


### [20] [TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language Analysis](https://arxiv.org/abs/2510.06063)
*Austin Feng,Andreas Varvarigos,Ioannis Panitsas,Daniela Fernandez,Jinbiao Wei,Yuwei Guo,Jialin Chen,Ali Maatouk,Leandros Tassiulas,Rex Ying*

Main category: cs.AI

TL;DR: 提出了TelecomTS数据集，这是一个来自5G电信网络的大规模可观测性数据集，解决了现有数据集缺乏尺度信息和多模态推理支持的问题。


<details>
  <summary>Details</summary>
Motivation: 现有可观测性数据集在公共基准中代表性不足，通常经过匿名化和归一化处理，移除了尺度信息，限制了在异常检测、根因分析等多模态推理任务中的应用。

Method: 从5G电信网络收集大规模可观测性数据，包含异构的去匿名化协变量和明确的尺度信息，支持异常检测、根因分析和多模态问答等多种下游任务。

Result: 基准测试显示现有时间序列、语言和推理模型在处理可观测性数据的突发性、噪声和高方差动态时表现不佳，证明了保留协变量绝对尺度的重要性。

Conclusion: 需要开发能够原生利用尺度信息的基础时间序列模型，以更好地支持实际可观测性应用。

Abstract: Modern enterprises generate vast streams of time series metrics when
monitoring complex systems, known as observability data. Unlike conventional
time series from domains such as weather, observability data are zero-inflated,
highly stochastic, and exhibit minimal temporal structure. Despite their
importance, observability datasets are underrepresented in public benchmarks
due to proprietary restrictions. Existing datasets are often anonymized and
normalized, removing scale information and limiting their use for tasks beyond
forecasting, such as anomaly detection, root-cause analysis, and multi-modal
reasoning. To address this gap, we introduce TelecomTS, a large-scale
observability dataset derived from a 5G telecommunications network. TelecomTS
features heterogeneous, de-anonymized covariates with explicit scale
information and supports a suite of downstream tasks, including anomaly
detection, root-cause analysis, and a question-answering benchmark requiring
multi-modal reasoning. Benchmarking state-of-the-art time series, language, and
reasoning models reveals that existing approaches struggle with the abrupt,
noisy, and high-variance dynamics of observability data. Our experiments also
underscore the importance of preserving covariates' absolute scale, emphasizing
the need for foundation time series models that natively leverage scale
information for practical observability applications.

</details>


### [21] [BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions](https://arxiv.org/abs/2510.05318)
*Nan Huo,Xiaohan Xu,Jinyang Li,Per Jacobsson,Shipei Lin,Bowen Qin,Binyuan Hui,Xiaolong Li,Ge Qu,Shuzheng Si,Linheng Han,Edward Alexander,Xintong Zhu,Rui Qin,Ruihan Yu,Yiyao Jin,Feige Zhou,Weihao Zhong,Yun Chen,Hongyu Liu,Chenhao Ma,Fatma Ozcan,Yannis Papakonstantinou,Reynold Cheng*

Main category: cs.AI

TL;DR: BIRD-INTERACT是一个多轮文本到SQL基准测试，通过结合分层知识库、元数据文件和函数驱动的用户模拟器，模拟真实数据库助手场景，支持CRUD操作和动态交互评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试将对话历史视为静态上下文或仅限于只读操作，无法反映生产级数据库助手的实际挑战，需要更真实的交互环境。

Method: 构建包含分层知识库、元数据文件和用户模拟器的综合交互环境，提供预定义对话协议和开放代理两种评估设置，涵盖完整的CRUD操作范围。

Result: BIRD-INTERACT具有很高难度，GPT-5在c-Interact中仅完成8.67%任务，在a-Interact中完成17.00%任务，验证了有效交互对复杂动态文本到SQL任务的重要性。

Conclusion: BIRD-INTERACT通过逼真的交互环境有效评估多轮文本到SQL模型的性能，强调了动态交互在数据库助手应用中的关键作用。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
single-turn text-to-SQL tasks, but real-world database applications
predominantly require multi-turn interactions to handle ambiguous queries,
execution errors, and evolving user requirements. Existing multi-turn
benchmarks fall short by treating conversation histories as static context or
limiting evaluation to read-only operations, failing to reflect
production-grade database assistant challenges. We introduce BIRD-INTERACT, a
benchmark that restores this realism through: (1) a comprehensive interaction
environment coupling each database with a hierarchical knowledge base, metadata
files, and a function-driven user simulator, enabling models to solicit
clarifications, retrieve knowledge, and recover from errors without human
supervision; (2) two evaluation settings consisting of a pre-defined
conversational protocol (c-Interact) and an open-ended agentic setting
(a-Interact) where models autonomously decide when to query the user simulator
or explore the environment; (3) a challenging task suite covering the full CRUD
spectrum for business-intelligence and operational use cases, guarded by
executable test cases. Each task features ambiguous and follow-up sub-tasks
requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600
tasks, up to 11,796 interactions) for comprehensive performance assessment, and
BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed
behavioral analysis and rapid method development. Our empirical results
highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in
c-Interact and 17.00% in a-Interact. Analysis via memory grafting and
Interaction Test-time Scaling validates the importance of effective interaction
for complex, dynamic text-to-SQL tasks.

</details>


### [22] [Biomedical reasoning in action: Multi-agent System for Auditable Biomedical Evidence Synthesis](https://arxiv.org/abs/2510.05335)
*Oskar Wysocki,Magdalena Wysocka,Mauricio Jacobo,Harriet Unsworth,André Freitas*

Main category: cs.AI

TL;DR: M-Reason是一个用于生物医学领域（特别是癌症研究）的透明、基于代理的推理和证据整合演示系统，利用大语言模型和模块化代理编排来自动化证据检索、评估和合成。


<details>
  <summary>Details</summary>
Motivation: 旨在解决生物医学研究中证据整合的复杂性和透明度问题，通过多代理系统提高证据合成的效率和可解释性。

Method: 采用模块化代理架构，每个代理专门处理特定的证据流，实现并行处理和细粒度分析，结合确定性代码进行验证，并提供交互式用户界面。

Result: 评估显示在效率和输出一致性方面取得显著提升，系统能够提供从源证据到最终结论的完整可追溯性。

Conclusion: M-Reason不仅作为证据合成的实用工具，还作为科学研究中稳健多代理LLM系统的测试平台，具有重要应用潜力。

Abstract: We present M-Reason, a demonstration system for transparent, agent-based
reasoning and evidence integration in the biomedical domain, with a focus on
cancer research. M-Reason leverages recent advances in large language models
(LLMs) and modular agent orchestration to automate evidence retrieval,
appraisal, and synthesis across diverse biomedical data sources. Each agent
specializes in a specific evidence stream, enabling parallel processing and
fine-grained analysis. The system emphasizes explainability, structured
reporting, and user auditability, providing complete traceability from source
evidence to final conclusions. We discuss critical tradeoffs between agent
specialization, system complexity, and resource usage, as well as the
integration of deterministic code for validation. An open, interactive user
interface allows researchers to directly observe, explore and evaluate the
multi-agent workflow. Our evaluation demonstrates substantial gains in
efficiency and output consistency, highlighting M-Reason's potential as both a
practical tool for evidence synthesis and a testbed for robust multi-agent LLM
systems in scientific research, available at https://m-reason.digitalecmt.com.

</details>


### [23] [Integrating Bayesian methods with neural network--based model predictive control: a review](https://arxiv.org/abs/2510.05338)
*Asli Karacelik*

Main category: cs.AI

TL;DR: 本文综述了贝叶斯方法在模型预测控制(MPC)中的应用，重点关注神经网络建模、控制设计和不确定性量化，指出当前研究存在基准不一致和可靠性分析有限的问题，呼吁建立标准化基准和透明报告。


<details>
  <summary>Details</summary>
Motivation: 评估贝叶斯方法在MPC中的使用情况，特别是如何在实际中实现这些方法，以及当前研究在性能增益和鲁棒性报告方面的局限性。

Method: 系统分析个体研究及其实际实现方式，重点关注神经网络建模、控制设计和不确定性量化三个方面的贝叶斯方法应用。

Result: 发现贝叶斯方法在MPC中越来越多地被采用以捕捉和传播不确定性，但报告的性能增益和鲁棒性改进仍然零散，存在基准不一致和可靠性分析有限的问题。

Conclusion: 需要建立标准化基准、消融研究和透明报告，以严格确定贝叶斯技术在MPC中的有效性。

Abstract: In this review, we assess the use of Bayesian methods in model predictive
control (MPC), focusing on neural-network-based modeling, control design, and
uncertainty quantification. We systematically analyze individual studies and
how they are implemented in practice. While Bayesian approaches are
increasingly adopted to capture and propagate uncertainty in MPC, reported
gains in performance and robustness remain fragmented, with inconsistent
baselines and limited reliability analyses. We therefore argue for standardized
benchmarks, ablation studies, and transparent reporting to rigorously determine
the effectiveness of Bayesian techniques for MPC.

</details>


### [24] [MHA-RAG: Improving Efficiency, Accuracy, and Consistency by Encoding Exemplars as Soft Prompts](https://arxiv.org/abs/2510.05363)
*Abhinav Jain,Xinyu Yao,Thomas Reps,Christopher Jermaine*

Main category: cs.AI

TL;DR: MHA-RAG框架通过将示例表示为软提示而非纯文本，在少样本场景下显著提升性能并降低推理成本，且对示例顺序不敏感。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用领域特定示例作为上下文演示，但纯文本表示可能不是最高效、有效和稳定的方式。

Method: 提出MHA-RAG框架，使用多注意力头控制软提示生成，将示例表示为软提示而非纯文本。

Result: 在多个问答基准测试和模型规模下，MHA-RAG比标准RAG性能提升20分，推理成本降低10倍GFLOPs。

Conclusion: 软提示表示比纯文本表示更高效有效，MHA-RAG框架在保持示例顺序不变性的同时实现了更高准确性和效率。

Abstract: Adapting Foundation Models to new domains with limited training data is
challenging and computationally expensive. While prior work has demonstrated
the effectiveness of using domain-specific exemplars as in-context
demonstrations, we investigate whether representing exemplars purely as text is
the most efficient, effective, and stable approach. We explore an alternative:
representing exemplars as soft prompts with an exemplar order invariant model
architecture. To this end, we introduce Multi-Head Attention
Retrieval-Augmented Generation (MHA-RAG), a framework with the number of
attention heads serving as a simple hyperparameter to control soft
prompt-generation across different tasks. Across multiple question-answering
benchmarks and model scales, MHA-RAG achieves a 20-point performance gain over
standard RAG, while cutting inference costs by a factor of 10X
GFLOPs-delivering both higher accuracy and greater efficiency, invariant to
exemplar order.

</details>


### [25] [What Do You Mean? Exploring How Humans and AI Interact with Symbols and Meanings in Their Interactions](https://arxiv.org/abs/2510.05378)
*Reza Habibi,Seung Wan Ha,Zhiyu Lin,Atieh Kashani,Ala Shafia,Lakshana Lakshmanarajan,Chia-Fang Chung,Magy Seif El-Nasr*

Main category: cs.AI

TL;DR: 该研究基于符号互动理论，通过两个实验探索人类与AI如何共同构建符号及其意义，发现双向符号交换和重新解释是形成共享理解的关键。


<details>
  <summary>Details</summary>
Motivation: 有意义的AI协作需要超越语言处理，理解符号及其社会建构意义。人类通过社交互动自然解释符号，而AI系统往往错过对话中出现的动态解释。

Method: 基于符号互动理论，进行了两项研究，调查人类和AI如何共同构建符号及其意义，特别关注社交语境引入时的互动过程。

Result: 参与者会根据对话AI建议的符号和解释调整初始意义定义，特别是在社交语境下。参与者将个人和社会价值观投射到互动中，随时间推移精炼意义。

Conclusion: 共享理解不是来自单纯同意，而是来自符号的双向交换和重新解释，这为人机交互设计提出了新范式。

Abstract: Meaningful human-AI collaboration requires more than processing language; it
demands a deeper understanding of symbols and their socially constructed
meanings. While humans naturally interpret symbols through social interaction,
AI systems often miss the dynamic interpretations that emerge in conversation.
Drawing on Symbolic Interactionism theory, we conducted two studies to
investigate how humans and AI co-construct symbols and their meanings. Findings
provide empirical insights into how humans and conversational AI agents
collaboratively shape meanings during interaction. We show how participants
shift their initial definitions of meaning in response to the symbols and
interpretations suggested by the conversational AI agents, especially when
social context is introduced. We also observe how participants project their
personal and social values into these interactions, refining meanings over
time. These findings reveal that shared understanding does not emerge from mere
agreement but from the bi-directional exchange and reinterpretation of symbols,
suggesting new paradigms for human-AI interaction design.

</details>


### [26] [Teacher-Student Guided Inverse Modeling for Steel Final Hardness Estimation](https://arxiv.org/abs/2510.05402)
*Ahmad Alsheikh,Andreas Fischer*

Main category: cs.AI

TL;DR: 提出基于师生学习框架的逆向建模方法，用于解决钢铁热处理硬度预测的逆向问题（从目标硬度反推输入参数）。


<details>
  <summary>Details</summary>
Motivation: 钢铁热处理过程具有多对一特性，不同输入参数组合可能产生相同硬度值，这使得从目标硬度反推输入参数的逆向问题特别困难。

Method: 首先训练前向模型（教师）从13个冶金输入特征预测最终硬度，然后训练逆向模型（学生）从目标硬度推断合理的输入配置，通过教师的反馈在迭代监督循环中优化学生模型。

Result: 在公开的调质钢数据集上评估，相比基线回归和强化学习模型，该方法不仅获得更高的逆向预测精度，而且计算时间显著减少。

Conclusion: 师生学习框架在材料科学逆向过程建模中展现出高效性和有效性。

Abstract: Predicting the final hardness of steel after heat treatment is a challenging
regression task due to the many-to-one nature of the process -- different
combinations of input parameters (such as temperature, duration, and chemical
composition) can result in the same hardness value. This ambiguity makes the
inverse problem, estimating input parameters from a desired hardness,
particularly difficult. In this work, we propose a novel solution using a
Teacher-Student learning framework. First, a forward model (Teacher) is trained
to predict final hardness from 13 metallurgical input features. Then, a
backward model (Student) is trained to infer plausible input configurations
from a target hardness value. The Student is optimized by leveraging feedback
from the Teacher in an iterative, supervised loop. We evaluate our method on a
publicly available tempered steel dataset and compare it against baseline
regression and reinforcement learning models. Results show that our
Teacher-Student framework not only achieves higher inverse prediction accuracy
but also requires significantly less computational time, demonstrating its
effectiveness and efficiency for inverse process modeling in materials science.

</details>


### [27] [AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems](https://arxiv.org/abs/2510.05432)
*Shambhavi Mishra,Gaurav Sahu,Marco Pedersoli,Laurent Charlin,Jose Dolz,Christopher Pal*

Main category: cs.AI

TL;DR: AInstein框架测试LLM仅使用预训练参数知识解决AI研究问题的能力，无需外部辅助。结果显示LLM能重新发现可行方案并偶尔提出创造性方法，但问题解决能力仍脆弱且对问题表述敏感。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的成功是源于真正推理还是复杂记忆，测试LLM作为自主科学问题解决者的能力。

Method: 从ICLR 2025论文提取问题陈述，让专业求解器通过迭代批判循环提出和改进技术解决方案，使用LLM作为评判者进行结构化评估。

Result: LLM能重新发现可行解决方案，偶尔提出创造性替代方法，但问题解决能力脆弱且对问题表述高度敏感。

Conclusion: 这是首次大规模证据显示LLM作为自主科学问题解决者的潜力和当前局限性，揭示了其潜在能力但仍有明显限制。

Abstract: Large language models (LLMs) demonstrate impressive capabilities across a
wide range of tasks, yet it remains unclear whether such success reflects
genuine reasoning or sophisticated recall. We introduce AInstein, a framework
for testing whether LLMs can generate valid solutions to AI research problems
using only their pretrained parametric knowledge -- without domain-specific
fine-tuning, retrieval augmentation, or other external aids. Our approach
extracts distilled problem statements from high-quality ICLR 2025 submissions,
then tasks specialized solver agents with proposing and refining technical
solutions through iterative critique loops, mimicking the cycles of proposal,
review, and revision central to scientific inquiry. We evaluate AInstein on
1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster),
using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by
targeted manual checks. Performance is assessed with three metrics: Success
Rate (does the solution address the problem?), Rediscovery (does it align with
human-proposed methods?), and Novelty (does it yield valid, original
approaches?). Our results reveal that while LLMs can rediscover feasible
solutions and occasionally propose creative alternatives, their problem-solving
ability remains fragile and highly sensitive to framing. These findings provide
the first large-scale evidence on the extent to which LLMs can act as
autonomous scientific problem-solvers, highlighting both their latent potential
and their current limitations.

</details>


### [28] [NASP-T: A Fuzzy Neuro-Symbolic Transformer for Logic-Constrained Aviation Safety Report Classification](https://arxiv.org/abs/2510.05451)
*Fadi Al Machot,Fidaa Al Machot*

Main category: cs.AI

TL;DR: 提出了一种结合答案集编程（ASP）与transformer学习的神经符号混合框架，用于航空安全报告系统（ASRS）的多标签文本分类，通过规则增强和模糊逻辑正则化减少领域逻辑违规。


<details>
  <summary>Details</summary>
Motivation: 深度transformer模型在多标签文本分类中表现优异，但在安全关键应用中经常违反专家认为必要的领域逻辑，这是一个特别值得关注的问题。

Method: 将领域知识形式化为加权ASP规则，通过Clingo求解器验证。采用两种互补方式整合规则：(i) 基于规则的数据增强，生成逻辑一致的合成样本；(ii) 模糊逻辑正则化，在微调期间以可微分形式强制执行规则满足。

Result: 与强基线BCE相比，该方法提高了微调和宏F1分数，在ASRS测试集上实现了高达86%的规则违规减少。

Conclusion: 这是首个将基于ASP的推理、规则驱动增强和可微分transformer训练统一应用于ASRS报告的大规模神经符号应用，为可信赖的安全关键NLP提供了解决方案。

Abstract: Deep transformer models excel at multi-label text classification but often
violate domain logic that experts consider essential, an issue of particular
concern in safety-critical applications. We propose a hybrid neuro-symbolic
framework that integrates Answer Set Programming (ASP) with transformer-based
learning on the Aviation Safety Reporting System (ASRS) corpus. Domain
knowledge is formalized as weighted ASP rules and validated using the Clingo
solver. These rules are incorporated in two complementary ways: (i) as
rule-based data augmentation, generating logically consistent synthetic samples
that improve label diversity and coverage; and (ii) as a fuzzy-logic
regularizer, enforcing rule satisfaction in a differentiable form during
fine-tuning. This design preserves the interpretability of symbolic reasoning
while leveraging the scalability of deep neural architectures. We further tune
per-class thresholds and report both standard classification metrics and
logic-consistency rates. Compared to a strong Binary Cross-Entropy (BCE)
baseline, our approach improves micro- and macro-F1 scores and achieves up to
an 86% reduction in rule violations on the ASRS test set. To the best of our
knowledge, this constitutes the first large-scale neuro-symbolic application to
ASRS reports that unifies ASP-based reasoning, rule-driven augmentation, and
differentiable transformer training for trustworthy, safety-critical NLP.

</details>


### [29] [Do Code Models Suffer from the Dunning-Kruger Effect?](https://arxiv.org/abs/2510.05457)
*Mukul Singh,Somya Chatterjee,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: 本文研究了先进LLM在编程任务中表现出的邓宁-克鲁格效应，发现AI模型在陌生或低资源领域会像人类一样过度自信。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在创意和技术领域与人类合作日益增多，需要了解认知边界和偏见如何影响共享代理能力，特别是邓宁-克鲁格效应在AI中的表现。

Method: 通过分析模型在不同编程语言中的置信度和性能表现，研究LLM的过度自信模式。

Result: 实验表明，能力较弱的模型和在罕见编程语言中操作的模型表现出更强的DKE-like偏见，偏见强度与模型能力成比例。

Conclusion: AI模型在编程任务中确实表现出类似人类的邓宁-克鲁格效应，特别是在不熟悉或低资源领域，这种偏见强度与模型能力相关。

Abstract: As artificial intelligence systems increasingly collaborate with humans in
creative and technical domains, questions arise about the cognitive boundaries
and biases that shape our shared agency. This paper investigates the
Dunning-Kruger Effect (DKE), the tendency for those with limited competence to
overestimate their abilities in state-of-the-art LLMs in coding tasks. By
analyzing model confidence and performance across a diverse set of programming
languages, we reveal that AI models mirror human patterns of overconfidence,
especially in unfamiliar or low-resource domains. Our experiments demonstrate
that less competent models and those operating in rare programming languages
exhibit stronger DKE-like bias, suggesting that the strength of the bias is
proportionate to the competence of the models.

</details>


### [30] [VAL-Bench: Measuring Value Alignment in Language Models](https://arxiv.org/abs/2510.05465)
*Aman Gupta,Denny O'Shea,Fazl Barez*

Main category: cs.AI

TL;DR: VAL-Bench是一个评估大语言模型价值对齐的基准测试，通过对比模型在争议话题正反两方面提示下的响应一致性来衡量其价值立场稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注拒绝行为或预定义安全违规，只能检查规则合规性，无法揭示模型在面对现实争议问题时是否坚持一致的价值体系。

Method: 使用维基百科争议章节构建11.5万对正反提示，通过LLM作为评判者来评估模型在配对响应中的一致性程度。

Result: 在主流开源和闭源模型上的测试显示价值对齐存在显著差异，并揭示了安全策略（如拒绝回答）与表达性价值体系之间的权衡。

Conclusion: VAL-Bench提供了一个可扩展、可复现的基准，能够系统比较LLMs如何可靠地体现人类价值观。

Abstract: Large language models (LLMs) are increasingly used for tasks where outputs
shape human decisions, so it is critical to test whether their responses
reflect consistent human values. Existing benchmarks mostly track refusals or
predefined safety violations, but these only check rule compliance and do not
reveal whether a model upholds a coherent value system when facing
controversial real-world issues. We introduce the \textbf{V}alue
\textbf{AL}ignment \textbf{Bench}mark (\textbf{VAL-Bench}), which evaluates
whether models maintain a stable value stance across paired prompts that frame
opposing sides of public debates. VAL-Bench consists of 115K such pairs from
Wikipedia's controversial sections. A well-aligned model should express similar
underlying views regardless of framing, which we measure using an LLM-as-judge
to score agreement or divergence between paired responses. Applied across
leading open- and closed-source models, the benchmark reveals large variation
in alignment and highlights trade-offs between safety strategies (e.g.,
refusals) and more expressive value systems. By providing a scalable,
reproducible benchmark, VAL-Bench enables systematic comparison of how reliably
LLMs embody human values.

</details>


### [31] [Vul-R2: A Reasoning LLM for Automated Vulnerability Repair](https://arxiv.org/abs/2510.05480)
*Xin-Cheng Wen,Zirui Lin,Yijun Yang,Cuiyun Gao,Deheng Ye*

Main category: cs.AI

TL;DR: 该论文提出了一种基于大型语言模型的自动漏洞修复方法，通过生成漏洞相关的推理数据来解决现有方法缺乏高质量训练数据和难以验证中间修复过程的挑战。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞的指数级增长迫切需要自动漏洞修复解决方案。现有基于LLM的方法虽然表现出色，但面临缺乏高质量漏洞相关推理数据和难以验证中间修复过程的挑战。

Method: 将自动漏洞修复建模为序列生成问题，利用大型语言模型生成修复方案。通过生成漏洞相关的推理数据来增强模型对多样化漏洞修复模式的理解。

Result: 现有方法主要依赖编码通用编程知识的基础模型，难以捕捉多样化的漏洞修复模式，且缺乏可验证的中间修复过程反馈。

Conclusion: 需要开发能够生成高质量漏洞相关推理数据并验证中间修复过程的新方法，以提升自动漏洞修复的效果。

Abstract: The exponential increase in software vulnerabilities has created an urgent
need for automatic vulnerability repair (AVR) solutions. Recent research has
formulated AVR as a sequence generation problem and has leveraged large
language models (LLMs) to address this problem. Typically, these approaches
prompt or fine-tune LLMs to generate repairs for vulnerabilities directly.
Although these methods show state-of-the-art performance, they face the
following challenges: (1) Lack of high-quality, vulnerability-related reasoning
data. Current approaches primarily rely on foundation models that mainly encode
general programming knowledge. Without vulnerability-related reasoning data,
they tend to fail to capture the diverse vulnerability repair patterns. (2)
Hard to verify the intermediate vulnerability repair process during LLM
training. Existing reinforcement learning methods often leverage intermediate
execution feedback from the environment (e.g., sandbox-based execution results)
to guide reinforcement learning training. In contrast, the vulnerability repair
process generally lacks such intermediate, verifiable feedback, which poses
additional challenges for model training.

</details>


### [32] [Decade-long Emission Forecasting with an Ensemble Model in Taiwan](https://arxiv.org/abs/2510.05548)
*Gordon Hung,Salinna Abdullah*

Main category: cs.AI

TL;DR: 该研究比较了21种时间序列模型预测台湾二氧化碳排放量，发现FFNN、SVM和RFR表现最佳，并通过集成学习构建了SMAPE为1.407的稳健模型，提供了十年排放预测。


<details>
  <summary>Details</summary>
Motivation: 台湾人口密集且高度依赖化石燃料导致严重空气污染，二氧化碳是最主要的温室气体，需要准确预测排放以支持政策制定。

Method: 比较21种常用时间序列模型（包括单变量和多变量方法），对表现最佳的FFNN、SVM和RFR模型通过自定义堆叠泛化集成技术与线性回归结合。

Result: 集成模型实现了1.407的SMAPE值，且没有过拟合迹象，提供了准确的十年排放预测。

Conclusion: 提出的集成模型能够为政策制定者提供数据驱动的决策支持，帮助应对台湾的空气污染问题。

Abstract: Taiwan's high population and heavy dependence on fossil fuels have led to
severe air pollution, with the most prevalent greenhouse gas being carbon
dioxide (CO2). There-fore, this study presents a reproducible and comprehensive
case study comparing 21 of the most commonly employed time series models in
forecasting emissions, analyzing both univariate and multivariate approaches.
Among these, Feedforward Neural Network (FFNN), Support Vector Machine (SVM),
and Random Forest Regressor (RFR) achieved the best performances. To further
enhance robustness, the top performers were integrated with Linear Regression
through a custom stacked generalization en-semble technique. Our proposed
ensemble model achieved an SMAPE of 1.407 with no signs of overfitting.
Finally, this research provides an accurate decade-long emission projection
that will assist policymakers in making more data-driven decisions.

</details>


### [33] [MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption](https://arxiv.org/abs/2510.05580)
*Chen Li,Zhantao Yang,Han Zhang,Fangyi Chen,Chenchen Zhu,Anudeepsekhar Bolimera,Marios Savvides*

Main category: cs.AI

TL;DR: MetaVLA是一个统一的、骨干网络无关的后训练框架，通过上下文感知元协同训练和轻量级元学习机制，在少量训练步骤和计算资源下显著提升VLA模型在未见任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language-Action模型在具身推理中表现有限，通常需要任务特定的微调，且在未见任务上泛化能力差，需要开发更高效和可扩展的对齐方法。

Method: 提出Context-Aware Meta Co-Training，将多样目标任务整合到单一微调阶段，利用结构多样的辅助任务提升域内泛化；集成轻量级元学习机制（基于Attentive Neural Processes），实现从多样上下文的快速适应。

Result: 在LIBERO基准测试中，使用6个辅助任务的MetaVLA在长时任务上比OpenVLA提升8.0%，训练步骤从240K减少到75K，GPU时间减少约76%。

Conclusion: MetaVLA证明了可扩展、低资源的后训练是可行的，为通用具身智能体的发展铺平了道路。

Abstract: Vision-Language-Action (VLA) models show promise in embodied reasoning, yet
remain far from true generalists-they often require task-specific fine-tuning,
and generalize poorly to unseen tasks. We propose MetaVLA, a unified,
backbone-agnostic post-training framework for efficient and scalable alignment.
MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse
target tasks into a single fine-tuning stage while leveraging structurally
diverse auxiliary tasks to improve in-domain generalization. Unlike naive
multi-task SFT, MetaVLA integrates a lightweight meta-learning
mechanism-derived from Attentive Neural Processes-to enable rapid adaptation
from diverse contexts with minimal architectural change or inference overhead.
On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA
by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K,
and cuts GPU time by ~76%. These results show that scalable, low-resource
post-training is achievable-paving the way toward general-purpose embodied
agents. Code will be available.

</details>


### [34] [In-the-Flow Agentic System Optimization for Effective Planning and Tool Use](https://arxiv.org/abs/2510.05592)
*Zhuofeng Li,Haoxiang Zhang,Seungju Han,Sheng Liu,Jianwen Xie,Yu Zhang,Yejin Choi,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: AgentFlow是一个可训练的流式代理框架，通过四个模块（规划器、执行器、验证器、生成器）协调工作，在实时环境中进行策略优化，解决了传统工具增强方法在长视野和多样化工具下的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的工具增强方法训练单一、整体的策略，在长视野和多样化工具下扩展性差，对新场景泛化能力弱。代理系统通过分解工作到专门模块提供了有希望的替代方案，但大多数仍是无训练的或依赖与多轮交互实时动态解耦的离线训练。

Method: 引入AgentFlow框架，协调四个模块通过演进内存进行协作，并在多轮循环中直接优化规划器。提出Flow-GRPO方法，通过将多轮优化转化为一系列可处理的单轮策略更新，解决长视野、稀疏奖励的信用分配问题。

Result: 在十个基准测试中，使用7B规模骨干的AgentFlow在搜索任务上平均准确率提升14.9%，代理任务提升14.0%，数学任务提升14.5%，科学任务提升4.1%，甚至超过了更大的专有模型如GPT-4o。

Conclusion: AgentFlow通过流式优化显著提升了规划能力、工具调用可靠性，并在模型规模和推理轮次上表现出积极的扩展性，证明了在实时环境中进行策略优化的有效性。

Abstract: Outcome-driven reinforcement learning has advanced reasoning in large
language models (LLMs), but prevailing tool-augmented approaches train a
single, monolithic policy that interleaves thoughts and tool calls under full
context; this scales poorly with long horizons and diverse tools and
generalizes weakly to new scenarios. Agentic systems offer a promising
alternative by decomposing work across specialized modules, yet most remain
training-free or rely on offline training decoupled from the live dynamics of
multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow
agentic framework that coordinates four modules (planner, executor, verifier,
generator) through an evolving memory and directly optimizes its planner inside
the multi-turn loop. To train on-policy in live environments, we propose
Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles
long-horizon, sparse-reward credit assignment by converting multi-turn
optimization into a sequence of tractable single-turn policy updates. It
broadcasts a single, verifiable trajectory-level outcome to every turn to align
local planner decisions with global success and stabilizes learning with
group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale
backbone outperforms top-performing baselines with average accuracy gains of
14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on
scientific tasks, even surpassing larger proprietary models like GPT-4o.
Further analyses confirm the benefits of in-the-flow optimization, showing
improved planning, enhanced tool-calling reliability, and positive scaling with
model size and reasoning turns.

</details>


### [35] [From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions](https://arxiv.org/abs/2510.05596)
*Changyuan Zhao,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Geng Sun,Xianbin Wang,Shiwen Mao,Abbas Jamalipour*

Main category: cs.AI

TL;DR: 提出了自演进代理AI框架，通过多LLM协作和自主进化循环，在无线系统中实现无需人工干预的持续优化，在低空无线网络天线演进案例中性能提升达52.02%。


<details>
  <summary>Details</summary>
Motivation: 传统静态AI模型无法适应动态无线环境，需要能够自主演进、持续改进的AI系统来应对未来无线系统的复杂需求。

Method: 采用分层架构和自主进化周期，结合工具智能、工作流优化、自反思和进化学习等关键技术，提出多代理协作框架，在监督代理协调下通过结构化对话、迭代反馈和系统验证实现全生命周期自主执行。

Result: 在天线演进案例中，成功将固定天线优化自主升级为可移动天线优化，波束增益显著提升，性能恢复达52.02%，持续超越固定基线。

Conclusion: 自演进代理AI框架展现了出色的适应性和鲁棒性，为下一代无线智能系统提供了可行的解决方案，能够自主应对环境变化并持续改进性能。

Abstract: Self-evolving agentic artificial intelligence (AI) offers a new paradigm for
future wireless systems by enabling autonomous agents to continually adapt and
improve without human intervention. Unlike static AI models, self-evolving
agents embed an autonomous evolution cycle that updates models, tools, and
workflows in response to environmental dynamics. This paper presents a
comprehensive overview of self-evolving agentic AI, highlighting its layered
architecture, life cycle, and key techniques, including tool intelligence,
workflow optimization, self-reflection, and evolutionary learning. We further
propose a multi-agent cooperative self-evolving agentic AI framework, where
multiple large language models (LLMs) are assigned role-specialized prompts
under the coordination of a supervisor agent. Through structured dialogue,
iterative feedback, and systematic validation, the system autonomously executes
the entire life cycle without human intervention. A case study on antenna
evolution in low-altitude wireless networks (LAWNs) demonstrates how the
framework autonomously upgrades fixed antenna optimization into movable antenna
optimization. Experimental results show that the proposed self-evolving agentic
AI autonomously improves beam gain and restores degraded performance by up to
52.02%, consistently surpassing the fixed baseline with little to no human
intervention and validating its adaptability and robustness for next-generation
wireless intelligence.

</details>


### [36] [Large Language Model-Based Uncertainty-Adjusted Label Extraction for Artificial Intelligence Model Development in Upper Extremity Radiography](https://arxiv.org/abs/2510.05664)
*Hanna Kreutzer,Anne-Sophie Caselitz,Thomas Dratsch,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung*

Main category: cs.AI

TL;DR: GPT-4o能够高精度地从放射学报告中提取诊断标签（含不确定性），这些标签可用于训练具有竞争力的多标签图像分类模型，且标签不确定性不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 评估GPT-4o从自由文本放射学报告中提取诊断标签（含不确定性）的能力，并测试这些标签如何影响肌肉骨骼X光片的多标签图像分类。

Method: 回顾性研究，包括锁骨、肘部和拇指的X光片系列。GPT-4o通过结构化模板标注影像学发现为"存在"、"不存在"或"不确定"。为评估标签不确定性的影响，将训练和验证集中的"不确定"标签自动重新分配为"存在"（包容性）或"不存在"（排他性）。使用ResNet50进行多标签分类。

Result: 在测试集中，自动提取的正确率为98.6%（60,618/61,488）。基于标签的模型训练在所有解剖区域都表现出竞争力，包容性和排他性模型的宏平均AUC值相似（如肘部：AUC=0.80）。模型在外部数据集上泛化良好，不同标注策略或数据集之间无显著差异（p≥0.15）。

Conclusion: GPT-4o能够从放射学报告中提取标签来训练具有竞争力的多标签分类模型，且检测到的放射学报告中的不确定性不影响这些模型的性能。

Abstract: Objectives: To evaluate GPT-4o's ability to extract diagnostic labels (with
uncertainty) from free-text radiology reports and to test how these labels
affect multi-label image classification of musculoskeletal radiographs.
Methods: This retrospective study included radiography series of the clavicle
(n=1,170), elbow (n=3,755), and thumb (n=1,978). After anonymization, GPT-4o
filled out structured templates by indicating imaging findings as present
("true"), absent ("false"), or "uncertain." To assess the impact of label
uncertainty, "uncertain" labels of the training and validation sets were
automatically reassigned to "true" (inclusive) or "false" (exclusive).
Label-image-pairs were used for multi-label classification using ResNet50.
Label extraction accuracy was manually verified on internal (clavicle: n=233,
elbow: n=745, thumb: n=393) and external test sets (n=300 for each).
Performance was assessed using macro-averaged receiver operating characteristic
(ROC) area under the curve (AUC), precision recall curves, sensitivity,
specificity, and accuracy. AUCs were compared with the DeLong test. Results:
Automatic extraction was correct in 98.6% (60,618 of 61,488) of labels in the
test sets. Across anatomic regions, label-based model training yielded
competitive performance measured by macro-averaged AUC values for inclusive
(e.g., elbow: AUC=0.80 [range, 0.62-0.87]) and exclusive models (elbow:
AUC=0.80 [range, 0.61-0.88]). Models generalized well on external datasets
(elbow [inclusive]: AUC=0.79 [range, 0.61-0.87]; elbow [exclusive]: AUC=0.79
[range, 0.63-0.89]). No significant differences were observed across labeling
strategies or datasets (p>=0.15). Conclusion: GPT-4o extracted labels from
radiologic reports to train competitive multi-label classification models with
high accuracy. Detected uncertainty in the radiologic reports did not influence
the performance of these models.

</details>


### [37] [D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI](https://arxiv.org/abs/2510.05684)
*Suwhan Choi,Jaeyoon Jung,Haebin Seong,Minchan Kim,Minyeong Kim,Yongjun Cho,Yoonshik Kim,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: D2E框架利用桌面游戏数据作为机器人具身AI任务的预训练基础，通过标准化数据收集、通用智能体模型和迁移学习方法，在物理操作和导航任务上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统具身AI受限于物理轨迹收集的高成本，而桌面游戏环境提供了大规模的感觉运动交互数据，可作为具身学习的有效预训练来源。

Method: 提出三部分框架：OWA工具包统一桌面交互数据格式并压缩152倍；Generalist-IDM实现跨游戏零样本泛化；VAPT将桌面预训练表示迁移到物理任务。

Result: 使用1300+小时数据（259小时人工演示+1000+小时伪标签游戏数据），在LIBERO操作任务上达到96.6%成功率，在CANVAS导航任务上达到83.3%成功率。

Conclusion: 数字交互中的感觉运动基元具有足够的跨域不变性，能够有效迁移到物理具身任务，确立了桌面预训练作为机器人学的实用范式。

Abstract: Large language models leverage internet-scale text data, yet embodied AI
remains constrained by the prohibitive costs of physical trajectory collection.
Desktop environments -- particularly gaming -- offer a compelling alternative:
they provide rich sensorimotor interactions at scale while maintaining the
structured observation-action coupling essential for embodied learning. We
present D2E (Desktop to Embodied AI), a framework that demonstrates desktop
interactions can serve as an effective pretraining substrate for robotics
embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT
for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a
complete pipeline from scalable desktop data collection to verified transfer in
embodied domains. Our framework comprises three components: (1) the OWA Toolkit
that unifies diverse desktop interactions into a standardized format with 152x
compression, (2) the Generalist-IDM that achieves strong zero-shot
generalization across unseen games through timestamp-based event prediction,
enabling internet-scale pseudo-labeling, and (3) VAPT that transfers
desktop-pretrained representations to physical manipulation and navigation.
Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of
pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO
manipulation and 83.3% on CANVAS navigation benchmarks. This validates that
sensorimotor primitives in digital interactions exhibit sufficient invariance
to transfer meaningfully to physical embodied tasks, establishing desktop
pretraining as a practical paradigm for robotics. We will make all our work
public, including the OWA toolkit, datasets of human-collected and
pseudo-labeled, and VAPT-trained models available at
https://worv-ai.github.io/d2e/

</details>


### [38] [Joint Communication Scheduling and Velocity Control for Multi-UAV-Assisted Post-Disaster Monitoring: An Attention-Based In-Context Learning Approach](https://arxiv.org/abs/2510.05698)
*Yousef Emami,Seyedsina Nabavirazavi,Jingjing Zheng,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 提出AIC-VDS方法，使用基于注意力机制的上下文学习来优化多无人机在灾害监测中的数据收集调度和速度控制，以最小化数据丢失。


<details>
  <summary>Details</summary>
Motivation: 无人机在灾害监测中面临数据收集调度和飞行速度优化的挑战，传统深度强化学习方法训练复杂且存在仿真与现实不匹配的问题，无法满足紧急需求。

Method: 提出基于注意力机制的上下文学习方法AIC-VDS，考虑地面传感器电池水平、队列长度、信道条件和无人机轨迹，联合优化数据收集调度和速度控制。

Result: 仿真结果表明，AIC-VDS方法在性能上优于深度Q网络和最大信道增益基线方法。

Conclusion: AIC-VDS作为深度强化学习的替代方案，在紧急情况下能有效优化无人机数据收集，减少数据丢失。

Abstract: Recently, Unmanned Aerial Vehicles (UAVs) are increasingly being investigated
to collect sensory data in post-disaster monitoring scenarios, such as
tsunamis, where early actions are critical to limit coastal damage. A major
challenge is to design the data collection schedules and flight velocities, as
unfavorable schedules and velocities can lead to transmission errors and buffer
overflows of the ground sensors, ultimately resulting in significant packet
loss. Meanwhile, online Deep Reinforcement Learning (DRL) solutions have a
complex training process and a mismatch between simulation and reality that
does not meet the urgent requirements of tsunami monitoring. Recent advances in
Large Language Models (LLMs) offer a compelling alternative. With their strong
reasoning and generalization capabilities, LLMs can adapt to new tasks through
In-Context Learning (ICL), which enables task adaptation through natural
language prompts and example-based guidance without retraining. However, LLM
models have input data limitations and thus require customized approaches. In
this paper, a joint optimization of data collection schedules and velocities
control for multiple UAVs is proposed to minimize data loss. The battery level
of the ground sensors, the length of the queues, and the channel conditions, as
well as the trajectories of the UAVs, are taken into account. Attention-Based
In-Context Learning for Velocity Control and Data Collection Schedule (AIC-VDS)
is proposed as an alternative to DRL in emergencies. The simulation results
show that the proposed AIC-VDS outperforms both the Deep-Q-Network (DQN) and
maximum channel gain baselines.

</details>


### [39] [Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot Fault Diagnosis on the Edge](https://arxiv.org/abs/2510.05733)
*Zijun Jia,Shuang Liang,Jinsong Yu*

Main category: cs.AI

TL;DR: Syn-Diag是一个云边协同的工业故障诊断框架，利用大语言模型解决数据稀缺和资源受限环境下的少样本故障诊断问题，通过视觉语义协同、内容感知推理和云边协同机制，在边缘设备上实现高效诊断。


<details>
  <summary>Details</summary>
Motivation: 工业故障诊断面临数据稀缺和大型AI模型在资源受限环境中部署困难的双重挑战，需要开发既能利用大语言模型能力又能在边缘设备高效运行的解决方案。

Method: 采用三层机制：1) 视觉语义协同，通过跨模态预训练将信号特征与大语言模型语义空间对齐；2) 内容感知推理，动态构建上下文提示以提升少样本诊断准确性；3) 云边协同，通过知识蒸馏创建轻量级边缘模型，支持在线更新。

Result: 在六个数据集上的实验表明，Syn-Diag显著优于现有方法，特别是在1样本和跨工况场景下。边缘模型性能接近云端版本，同时模型大小减少83%，延迟降低50%。

Conclusion: Syn-Diag为现代智能诊断提供了一个实用、鲁棒且可部署的范例，成功解决了工业故障诊断中的数据稀缺和部署挑战。

Abstract: Industrial fault diagnosis faces the dual challenges of data scarcity and the
difficulty of deploying large AI models in resource-constrained environments.
This paper introduces Syn-Diag, a novel cloud-edge synergistic framework that
leverages Large Language Models to overcome these limitations in few-shot fault
diagnosis. Syn-Diag is built on a three-tiered mechanism: 1) Visual-Semantic
Synergy, which aligns signal features with the LLM's semantic space through
cross-modal pre-training; 2) Content-Aware Reasoning, which dynamically
constructs contextual prompts to enhance diagnostic accuracy with limited
samples; and 3) Cloud-Edge Synergy, which uses knowledge distillation to create
a lightweight, efficient edge model capable of online updates via a shared
decision space. Extensive experiments on six datasets covering different CWRU
and SEU working conditions show that Syn-Diag significantly outperforms
existing methods, especially in 1-shot and cross-condition scenarios. The edge
model achieves performance comparable to the cloud version while reducing model
size by 83% and latency by 50%, offering a practical, robust, and deployable
paradigm for modern intelligent diagnostics.

</details>


### [40] [Artificially intelligent agents in the social and behavioral sciences: A history and outlook](https://arxiv.org/abs/2510.05743)
*Petter Holme,Milena Tsvetkova*

Main category: cs.AI

TL;DR: 回顾了从1950年代至今人工智能代理在社会科学中的发展历程，涵盖早期计算机社会模拟到现代大语言模型应用，强调AI技术如何改变科学研究过程。


<details>
  <summary>Details</summary>
Motivation: 梳理人工智能在社会科学和行为科学中的历史演变和当前趋势，探讨AI技术如何影响我们对人类自身的理解方式。

Method: 采用历史回顾和趋势分析的方法，从早期可编程计算机、社会模拟研究到现代大语言模型实验，系统梳理了AI在科学进程中的角色变化。

Result: 揭示了AI技术与社会科学研究的深度交织，展示了从计算机不为人知的时代到生成式AI应用热潮的完整发展脉络。

Conclusion: 我们与用于理解自身的技术深度交织，AI的发展不仅带来技术进步，更从根本上改变了科学研究的范式和方法论。

Abstract: We review the historical development and current trends of artificially
intelligent agents (agentic AI) in the social and behavioral sciences: from the
first programmable computers, and social simulations soon thereafter, to
today's experiments with large language models. This overview emphasizes the
role of AI in the scientific process and the changes brought about, both
through technological advancements and the broader evolution of science from
around 1950 to the present. Some of the specific points we cover include: the
challenges of presenting the first social simulation studies to a world unaware
of computers, the rise of social systems science, intelligent game theoretic
agents, the age of big data and the epistemic upheaval in its wake, and the
current enthusiasm around applications of generative AI, and many other topics.
A pervasive theme is how deeply entwined we are with the technologies we use to
understand ourselves.

</details>


### [41] [ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems](https://arxiv.org/abs/2510.05746)
*Bohan Yao,Shiva Krishna Reddy Malay,Vikas Yadav*

Main category: cs.AI

TL;DR: 提出了一种新的自动多智能体系统设计范式ARM，通过优化思维链推理来构建通用推理模块，显著优于手动设计和现有自动方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动多智能体系统设计方法性能不佳，需要为每个新任务重新发现架构且依赖昂贵的标注数据，而简单思维链推理表现具有竞争力，值得深入研究。

Method: 引入Agentic Reasoning Module (ARM)，将思维链泛化为智能体化推理，每个推理步骤由专门模块执行。通过代码空间的树搜索从简单CoT模块演化，利用执行轨迹反思指导变异。

Result: ARM显著优于手动设计的多智能体系统和最先进的自动设计方法，在不同基础模型和任务领域都保持高性能而无需进一步优化。

Conclusion: ARM作为通用推理构建块，可直接递归使用或作为元编排器的子程序，在多智能体系统设计中展现出卓越的泛化能力。

Abstract: Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved
state-of-the-art results on various complex reasoning tasks. Recent works have
proposed techniques to automate the design of MASes, eliminating the need for
manual engineering. However, these techniques perform poorly, often achieving
similar or inferior performance to simple baselines. Furthermore, they require
computationally expensive re-discovery of architectures for each new task
domain and expensive data annotation on domains without existing labeled
validation sets. A critical insight is that simple Chain of Thought (CoT)
reasoning often performs competitively with these complex systems, suggesting
that the fundamental reasoning unit of MASes, CoT, warrants further
investigation. To this end, we present a new paradigm for automatic MAS design
that pivots the focus to optimizing CoT reasoning. We introduce the Agentic
Reasoning Module (ARM), an agentic generalization of CoT where each granular
reasoning step is executed by a specialized reasoning module. This module is
discovered through a tree search over the code space, starting from a simple
CoT module and evolved using mutations informed by reflection on execution
traces. The resulting ARM acts as a versatile reasoning building block which
can be utilized as a direct recursive loop or as a subroutine in a learned
meta-orchestrator. Our approach significantly outperforms both manually
designed MASes and state-of-the-art automatic MAS design methods. Crucially,
MASes built with ARM exhibit superb generalization, maintaining high
performance across different foundation models and task domains without further
optimization.

</details>


### [42] [Uncertainty assessment in satellite-based greenhouse gas emissions estimates using emulated atmospheric transport](https://arxiv.org/abs/2510.05751)
*Jeffrey N. Clark,Elena Fillola,Nawid Keshtmand,Raul Santos-Rodriguez,Matthew Rigby*

Main category: cs.AI

TL;DR: 提出基于图神经网络的拉格朗日粒子扩散模型（LPDM）模拟器，用于估算大气传输足迹和温室气体浓度，实现1000倍加速并量化不确定性


<details>
  <summary>Details</summary>
Motivation: 传统传输模型计算成本高且不确定性难以量化，人工智能为加速传输模拟和量化不确定性提供了双重机会

Method: 使用图神经网络构建LPDM模拟器，采用集成方法估算大气传输足迹、温室气体浓度及其不确定性

Result: 模拟器比NAME LPDM快约1000倍，能再现大尺度足迹结构，集成方法揭示了预测误差的空间相关性

Conclusion: 该方法可推广到其他大气传输模型，支持不确定性感知的温室气体反演系统，提高卫星排放监测的鲁棒性

Abstract: Monitoring greenhouse gas emissions and evaluating national inventories
require efficient, scalable, and reliable inference methods. Top-down
approaches, combined with recent advances in satellite observations, provide
new opportunities to evaluate emissions at continental and global scales.
However, transport models used in these methods remain a key source of
uncertainty: they are computationally expensive to run at scale, and their
uncertainty is difficult to characterise. Artificial intelligence offers a dual
opportunity to accelerate transport simulations and to quantify their
associated uncertainty.
  We present an ensemble-based pipeline for estimating atmospheric transport
"footprints", greenhouse gas mole fraction measurements, and their
uncertainties using a graph neural network emulator of a Lagrangian Particle
Dispersion Model (LPDM). The approach is demonstrated with GOSAT (Greenhouse
Gases Observing Satellite) observations for Brazil in 2016. The emulator
achieved a ~1000x speed-up over the NAME LPDM, while reproducing large-scale
footprint structures. Ensembles were calculated to quantify absolute and
relative uncertainty, revealing spatial correlations with prediction error. The
results show that ensemble spread highlights low-confidence spatial and
temporal predictions for both atmospheric transport footprints and methane mole
fractions.
  While demonstrated here for an LPDM emulator, the approach could be applied
more generally to atmospheric transport models, supporting uncertainty-aware
greenhouse gas inversion systems and improving the robustness of
satellite-based emissions monitoring. With further development, ensemble-based
emulators could also help explore systematic LPDM errors, offering a
computationally efficient pathway towards a more comprehensive uncertainty
budget in greenhouse gas flux estimates.

</details>


### [43] [Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis](https://arxiv.org/abs/2510.05761)
*Sedat Dogan,Nina Dethlefs,Debarati Chakraborty*

Main category: cs.AI

TL;DR: 该研究提出了一种基于混合参与度分数的早期预测网络迷因流行度的方法，使用多模态特征在30-420分钟内预测迷因是否会走红，XGBoost模型在仅30分钟内就能达到PR-AUC > 0.52的性能。


<details>
  <summary>Details</summary>
Motivation: 预测在线内容的传播度具有挑战性，特别是在文化复杂、快速演变的迷因领域。本研究旨在探索早期预测迷因流行度的可行性，为无法获得完整传播级联数据的情况提供实用基准。

Method: 使用来自25个多样化Reddit社区的大规模跨语言数据集，提出基于混合参与度分数的数据驱动方法定义流行度，评估了逻辑回归、XGBoost和多层感知器等模型，采用包含静态内容和时间动态的全面多模态特征集。

Result: 有用信号很快出现：最佳模型XGBoost在仅30分钟内达到PR-AUC > 0.52。分析揭示了明显的"证据转换"现象，即随着迷因获得关注，特征重要性从静态上下文动态转向时间动态。

Conclusion: 这项工作为早期流行度预测建立了稳健、可解释且实用的基准，贡献了新颖的跨语言数据集和方法论上合理的流行度定义，是首个结合时间序列数据与静态内容和网络特征来预测早期迷因流行度的研究。

Abstract: Predicting the virality of online content remains challenging, especially for
culturally complex, fast-evolving memes. This study investigates the
feasibility of early prediction of meme virality using a large-scale,
cross-lingual dataset from 25 diverse Reddit communities. We propose a robust,
data-driven method to define virality based on a hybrid engagement score,
learning a percentile-based threshold from a chronologically held-out training
set to prevent data leakage. We evaluated a suite of models, including Logistic
Regression, XGBoost, and a Multi-layer Perceptron (MLP), with a comprehensive,
multimodal feature set across increasing time windows (30-420 min). Crucially,
useful signals emerge quickly: our best-performing model, XGBoost, achieves a
PR-AUC $>$ 0.52 in just 30 minutes. Our analysis reveals a clear "evidentiary
transition," in which the importance of the feature dynamically shifts from the
static context to the temporal dynamics as a meme gains traction. This work
establishes a robust, interpretable, and practical benchmark for early virality
prediction in scenarios where full diffusion cascade data is unavailable,
contributing a novel cross-lingual dataset and a methodologically sound
definition of virality. To our knowledge, this study is the first to combine
time series data with static content and network features to predict early meme
virality.

</details>


### [44] [RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases](https://arxiv.org/abs/2510.05764)
*Lang Qin,Zijian Gan,Xu Cao,Pengcheng Jiang,Yankai Jiang,Jiawei Han,Kaishun Wu,Jintai Chen*

Main category: cs.AI

TL;DR: RareAgent是一个自演化的多智能体系统，通过对抗性辩论和证据图构建来解决罕见病药物重定位问题，在缺乏先验关联的情况下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 罕见病药物重定位在缺乏药物与目标疾病先验关联时面临挑战，传统知识图谱补全和图神经网络方法由于缺乏可靠信号而表现不佳。

Method: 采用自演化的多智能体系统，组织任务特定的对抗性辩论，让智能体从不同视角动态构建证据图来支持、反驳或蕴含假设，并通过事后分析推理策略进行自我进化。

Result: RareAgent将适应症AUPRC提高了18.1%，优于推理基线方法，并提供与临床证据一致的可解释推理链。

Conclusion: 该方法将罕见病药物重定位从被动模式识别转变为主动证据寻求推理，通过自我进化机制产生可转移的启发式方法，加速未来研究。

Abstract: Computational drug repurposing for rare diseases is especially challenging
when no prior associations exist between drugs and target diseases. Therefore,
knowledge graph completion and message-passing GNNs have little reliable signal
to learn and propagate, resulting in poor performance. We present RareAgent, a
self-evolving multi-agent system that reframes this task from passive pattern
recognition to active evidence-seeking reasoning. RareAgent organizes
task-specific adversarial debates in which agents dynamically construct
evidence graphs from diverse perspectives to support, refute, or entail
hypotheses. The reasoning strategies are analyzed post hoc in a
self-evolutionary loop, producing textual feedback that refines agent policies,
while successful reasoning paths are distilled into transferable heuristics to
accelerate future investigations. Comprehensive evaluations reveal that
RareAgent improves the indication AUPRC by 18.1% over reasoning baselines and
provides a transparent reasoning chain consistent with clinical evidence.

</details>


### [45] [ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming](https://arxiv.org/abs/2510.05774)
*Weichun Shi,Minghao Liu,Wanting Zhang,Langchen Shi,Fuqi Jia,Feifei Ma,Jian Zhang*

Main category: cs.AI

TL;DR: ConstraintLLM是首个专门用于约束编程建模的大语言模型，通过多指令监督微调训练，结合约束感知检索模块和思维树框架，在工业级基准测试中达到最先进的求解精度。


<details>
  <summary>Details</summary>
Motivation: 约束编程在解决现实世界约束优化问题中具有重要作用，但相比基于运筹学模型的研究，CP建模得到的关注较少。本文旨在开发专门用于CP建模的LLM，构建可信赖的神经符号AI。

Method: 基于开源LLM进行多指令监督微调，提出约束感知检索模块增强上下文学习能力，集成到带有引导自校正机制的思维树框架中。

Result: ConstraintLLM在多个基准测试中达到最先进的求解精度，在新发布的IndusCP工业基准上性能比基线方法提升2倍。

Conclusion: ConstraintLLM成功展示了专门针对约束编程建模的LLM的有效性，为构建神经符号AI系统提供了新途径，并发布了首个工业级CP建模基准IndusCP。

Abstract: Constraint programming (CP) is a crucial technology for solving real-world
constraint optimization problems (COPs), with the advantages of rich modeling
semantics and high solving efficiency. Using large language models (LLMs) to
generate formal modeling automatically for COPs is becoming a promising
approach, which aims to build trustworthy neuro-symbolic AI with the help of
symbolic solvers. However, CP has received less attention compared to works
based on operations research (OR) models. We introduce ConstraintLLM, the first
LLM specifically designed for CP modeling, which is trained on an open-source
LLM with multi-instruction supervised fine-tuning. We propose the
Constraint-Aware Retrieval Module (CARM) to increase the in-context learning
capabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with
guided self-correction mechanism. Moreover, we construct and release IndusCP,
the first industrial-level benchmark for CP modeling, which contains 140
challenging tasks from various domains. Our experiments demonstrate that
ConstraintLLM achieves state-of-the-art solving accuracy across multiple
benchmarks and outperforms the baselines by 2x on the new IndusCP benchmark.
Code and data are available at: https://github.com/william4s/ConstraintLLM.

</details>


### [46] [The Safety Challenge of World Models for Embodied AI Agents: A Review](https://arxiv.org/abs/2510.05865)
*Lorenzo Baraldi,Zifan Zeng,Chongzhe Zhang,Aradhana Nayak,Hongbo Zhu,Feng Liu,Qunli Zhang,Peng Wang,Shiming Liu,Zheng Hu,Angelo Cangelosi,Lorenzo Baraldi*

Main category: cs.AI

TL;DR: 对自动驾驶和机器人领域的世界模型进行文献综述和实证分析，重点关注场景和控制生成任务的安全性影响，识别常见故障模式并进行定量评估。


<details>
  <summary>Details</summary>
Motivation: 随着具身人工智能的快速发展，需要更先进的集成模型来感知、解释和预测环境动态。世界模型为具身智能体提供预测未来环境状态和填补知识空白的能力，但必须确保预测对智能体和环境都是安全的。

Method: 进行全面的文献综述，收集并检查最先进模型的预测结果，识别和分类常见故障（称为病理），并提供结果的定量评估。

Result: 研究发现了世界模型在场景和控制生成任务中的常见故障模式，并对这些安全性问题进行了系统分类和量化分析。

Conclusion: 世界模型在提升具身智能体规划能力的同时，必须重视其预测的安全性，需要系统性地识别和解决预测中的病理问题。

Abstract: The rapid progress in embodied artificial intelligence has highlighted the
necessity for more advanced and integrated models that can perceive, interpret,
and predict environmental dynamics. In this context, World Models (WMs) have
been introduced to provide embodied agents with the abilities to anticipate
future environmental states and fill in knowledge gaps, thereby enhancing
agents' ability to plan and execute actions. However, when dealing with
embodied agents it is fundamental to ensure that predictions are safe for both
the agent and the environment. In this article, we conduct a comprehensive
literature review of World Models in the domains of autonomous driving and
robotics, with a specific focus on the safety implications of scene and control
generation tasks. Our review is complemented by an empirical analysis, wherein
we collect and examine predictions from state-of-the-art models, identify and
categorize common faults (herein referred to as pathologies), and provide a
quantitative evaluation of the results.

</details>


### [47] [Towards Label-Free Biological Reasoning Synthetic Dataset Creation via Uncertainty Filtering](https://arxiv.org/abs/2510.05871)
*Josefa Lia Stoisser,Lawrence Phillips,Aditya Misra,Tom A. Lamb,Philip Torr,Marc Boubnovski Martell,Julien Fauqueur,Kaspar Märtens*

Main category: cs.AI

TL;DR: 提出了一种基于不确定性的无标签过滤方法，使用模型自身置信度替代外部标签来筛选合成思维链数据，在生物扰动预测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要真实标签来筛选合成思维链数据，但在生物学等实验数据稀缺的领域成本高昂。

Method: 使用自一致性和预测困惑度等不确定性指标，采样多个推理轨迹并仅保留低不确定性子集进行监督微调。

Result: 过滤后的数据准确率更高，监督微调在不确定性过滤数据上优于未过滤合成数据，缩小了与真实标签训练的差距。

Conclusion: 模型内部置信度是高效创建推理数据集的有力信号，可在监督成本高的领域实现大型推理模型的应用。

Abstract: Synthetic chain-of-thought (CoT) traces are widely used to train large
reasoning models (LRMs), improving generalization by providing step-level
supervision. Yet most approaches require ground-truth labels to seed or filter
these traces - an expensive bottleneck in domains like biology where wet-lab
data are scarce. We propose a label-free alternative: uncertainty-based
filtering, which uses a model's own confidence - quantified through established
uncertainty metrics like self-consistency and predictive perplexity - as a
substitute for external labels. We sample multiple reasoning traces and retain
only low-uncertainty subsets. Applied to biological perturbation prediction, a
domain where wet-lab labels are especially costly, we show that the filtered
subset has higher accuracy, and that supervised fine-tuning (SFT) on
uncertainty-filtered data outperforms unfiltered synthetic data, narrows the
gap to ground-truth training, and surpasses strong LRM baselines. Ablations
show that per-class filtering corrects for class-specific uncertainty scales
and that hybrid uncertainty metrics yield higher-quality datasets. Our results
suggest that model-internal confidence is a powerful signal for efficient
reasoning dataset creation, enabling LRMs in domains where supervision is
expensive.

</details>


### [48] [Optimizing for Persuasion Improves LLM Generalization: Evidence from Quality-Diversity Evolution of Debate Strategies](https://arxiv.org/abs/2510.05909)
*Aksel Joonas Reedi,Corentin Léger,Julien Pourcel,Loris Gaven,Perrine Charriau,Guillaume Pourcel*

Main category: cs.AI

TL;DR: DebateQD是一种基于质量多样性(QD)的进化算法，通过辩论竞赛优化LLM的辩论策略，相比基于真理的优化方法，说服优化能产生更可泛化的推理能力。


<details>
  <summary>Details</summary>
Motivation: 主流基于真理的LLM优化方法容易过拟合，产生脆弱的推理能力。辩论式优化在辩论场景中显示出潜力，但尚未与主流方法进行系统比较。

Method: 提出DebateQD算法，通过锦标赛式辩论竞赛演化多样化的辩论策略（理性、权威、情感诉求等），使用单一LLM架构通过提示策略维持对手多样性。固定辩论协议，仅交换适应度函数来隔离优化目标的作用。

Result: 在三个模型规模（7B、32B、72B参数）和多个数据集大小上，说服优化策略的训练-测试泛化差距最多缩小13.94%，同时匹配或超过真理优化的测试性能。

Conclusion: 竞争性说服压力而非协作性求真，能够培养更可转移的推理技能，为改进LLM泛化提供了有前景的路径。

Abstract: Large Language Models (LLMs) optimized to output truthful answers often
overfit, producing brittle reasoning that fails to generalize. While
persuasion-based optimization has shown promise in debate settings, it has not
been systematically compared against mainstream truth-based approaches. We
introduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm
that evolves diverse debate strategies across different categories
(rationality, authority, emotional appeal, etc.) through tournament-style
competitions where two LLMs debate while a third judges. Unlike previously
proposed methods that require a population of LLMs, our approach maintains
diversity of opponents through prompt-based strategies within a single LLM
architecture, making it more accessible for experiments while preserving the
key benefits of population-based optimization. In contrast to prior work, we
explicitly isolate the role of the optimization objective by fixing the debate
protocol and swapping only the fitness function: persuasion rewards strategies
that convince the judge irrespective of truth, whereas truth rewards
collaborative correctness. Across three model scales (7B, 32B, 72B parameters)
and multiple dataset sizes from the QuALITY benchmark, persuasion-optimized
strategies achieve up to 13.94% smaller train-test generalization gaps, while
matching or exceeding truth optimization's test performance. These results
provide the first controlled evidence that competitive pressure to persuade,
rather than seek the truth collaboratively, fosters more transferable reasoning
skills, offering a promising path for improving LLM generalization.

</details>


### [49] [Training-Free Time Series Classification via In-Context Reasoning with LLM Agents](https://arxiv.org/abs/2510.05950)
*Songyuan Sui,Zihang Xu,Yu-Neng Chuang,Kwei-Herng Lai,Xia Hu*

Main category: cs.AI

TL;DR: FETA是一个无需训练的多智能体框架，通过基于示例的上下文推理进行时间序列分类，将多变量序列分解为通道级子问题，使用推理型大语言模型比较查询与示例，并通过置信度加权聚合器融合决策。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类应用中标注数据稀缺，特定任务训练成本高且不灵活。现有的大语言模型在理解时间模式方面有潜力，但纯零样本使用效果不理想。

Method: 将多变量序列分解为通道级子问题，为每个通道检索结构相似的标注示例，使用推理型LLM比较查询与示例并生成带置信度的通道级标签，最后通过置信度加权聚合器融合所有通道决策。

Result: 在九个具有挑战性的UEA数据集上，FETA在完全无需训练的情况下实现了强大的准确率，超越了多个经过训练的基线方法。

Conclusion: 多智能体上下文推理框架可以将LLM转化为具有竞争力的即插即用时间序列分类器，无需任何参数训练。

Abstract: Time series classification (TSC) spans diverse application scenarios, yet
labeled data are often scarce, making task-specific training costly and
inflexible. Recent reasoning-oriented large language models (LLMs) show promise
in understanding temporal patterns, but purely zero-shot usage remains
suboptimal. We propose FETA, a multi-agent framework for training-free TSC via
exemplar-based in-context reasoning. FETA decomposes a multivariate series into
channel-wise subproblems, retrieves a few structurally similar labeled examples
for each channel, and leverages a reasoning LLM to compare the query against
these exemplars, producing channel-level labels with self-assessed confidences;
a confidence-weighted aggregator then fuses all channel decisions. This design
eliminates the need for pretraining or fine-tuning, improves efficiency by
pruning irrelevant channels and controlling input length, and enhances
interpretability through exemplar grounding and confidence estimation. On nine
challenging UEA datasets, FETA achieves strong accuracy under a fully
training-free setting, surpassing multiple trained baselines. These results
demonstrate that a multi-agent in-context reasoning framework can transform
LLMs into competitive, plug-and-play TSC solvers without any parameter
training. The code is available at https://github.com/SongyuanSui/FETATSC.

</details>


### [50] [MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization](https://arxiv.org/abs/2510.05962)
*Dayyán O'Brien,Barry Haddow,Emily Allaway,Pinzhen Chen*

Main category: cs.AI

TL;DR: 提出MatheMagic方法，通过动态生成数学测试实例来评估模型的真实推理能力，避免测试集污染和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 当前数学基准测试存在两个问题：模型会记忆公开测试集，以及现有数学基准由于符号和规则多样性有限且答案封闭而容易过拟合。

Method: 使用MatheMagic方法，在测试时随机生成数学测试实例，改变数字和运算符的解释但保持答案可自动验证，从而评估模型的归纳或演绎能力。

Result: 实验发现模型更容易解决演绎问题而非归纳问题，但会回归到标准数学运算。数学适应模型未能展现通用的推理"技能"，在归纳任务上的微调泛化能力差。

Conclusion: 该方法提供了一种稳定、可扩展、可比较且对过拟合鲁棒的评估方式，揭示了当前模型在数学推理能力上的局限性。

Abstract: Conducting contamination-free evaluation of mathematical capabilities can be
difficult for two reasons: models may memorize a test set once it is made
public, and current mathematical benchmarks are prone to overfitting due to
having limited diversity of symbols and rules, coupled with closed-ended
answers. This paper proposes a method to leverage these shortcomings as useful
features to a construct dynamic, counterfactual benchmark, which can be used to
both reveal overfitting and measure true reasoning. We demonstrate this via
MatheMagic, which generates math test instances with the interpretations of
numbers and operators altered, yet has automatically verifiable answers. Test
instances are randomly seeded and constructed at test time to evaluate a
model's induction or deduction capability, offering stability, extensibility,
comparability, and robustness to overfitting. Our experiments find that models
solve deduction more easily than induction, but they revert to standard math.
Further analysis reveals that math-adapted models fail to exhibit a general
"skill" of reasoning, and fine-tuning on induction tasks generalizes poorly.

</details>


### [51] [Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG](https://arxiv.org/abs/2510.06002)
*Hudson de Martim*

Main category: cs.AI

TL;DR: SAT-Graph API是一个基于规范动作的查询执行层，通过原子化、可组合和可审计的原语将概率性发现与确定性检索分离，实现了可验证的知识图谱查询。


<details>
  <summary>Details</summary>
Motivation: 解决标准RAG在法律领域的核心限制，特别是如何在保持确定性属性的同时可靠查询结构化知识。

Method: 引入基于规范动作的查询执行层，通过有向无环图分解复杂查询，实现两层架构。

Result: 实现了高精度混合搜索、稳健的引用解析、时间点版本检索和可审计的因果追踪。

Conclusion: 将检索从不透明的黑盒转变为透明、可审计的过程，直接满足高风险领域对可解释AI的要求。

Abstract: The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core
limitations of standard Retrieval-Augmented Generation in the legal domain by
providing a verifiable knowledge graph that models hierarchical structure,
temporal evolution, and causal events of legal norms. However, a critical gap
remains: how to reliably query this structured knowledge without sacrificing
its deterministic properties. This paper introduces the SAT-Graph API, a formal
query execution layer centered on canonical actions-atomic, composable, and
auditable primitives that isolate probabilistic discovery from deterministic
retrieval. These actions enable: (i) high-precision hybrid search; (ii) robust
reference resolution; (iii) point-in-time version retrieval; and (iv) auditable
causal tracing. We demonstrate how planner-guided agents can decompose complex
queries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer
architecture transforms retrieval from an opaque black box to a transparent,
auditable process, directly addressing Explainable AI (XAI) requirements for
high-stakes domains.

</details>


### [52] [ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation in Large Reasoning Models](https://arxiv.org/abs/2510.06014)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Zhiyuan Yu,Qipeng Guo,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 提出了ARISE评估指标，专门用于评估大型推理模型的测试时扩展能力，包含样本级感知和动态采样机制，实验显示Claude Opus具有最佳扩展特性。


<details>
  <summary>Details</summary>
Motivation: 随着推理模型快速发展，需要系统性地比较和评估不同模型的测试时扩展能力，现有评估方法存在不足。

Method: 设计ARISE评估指标，包含样本级感知（惩罚负扩展行为）和动态采样机制（减少准确率波动和token计数不稳定性影响）。

Result: 在数学推理、代码生成和智能体任务等多个领域进行实验，ARISE能可靠地测量测试时扩展能力，发现不同模型间扩展效率存在显著差异。

Conclusion: ARISE为评估推理模型的测试时扩展能力提供了可靠工具，Claude Opus表现出优于其他当代推理模型的扩展特性。

Abstract: Test-time scaling has emerged as a transformative paradigm for enhancing the
performance of large reasoning models, enabling dynamic allocation of
computational resources during inference. However, as the landscape of
reasoning models rapidly expands, a critical question remains: how can we
systematically compare and evaluate the test-time scaling capabilities across
different models? In this paper, we introduce ARISE (Adaptive Resolution-aware
Scaling Evaluation), a novel metric specifically designed to assess the
test-time scaling effectiveness of large reasoning models. Unlike existing
evaluation approaches, ARISE incorporates two key innovations: (1) sample-level
awareness that effectively penalizes negative scaling behaviors where increased
computation leads to performance degradation, and (2) a dynamic sampling
mechanism that mitigates the impact of accuracy fluctuations and token count
instability on the final assessment. We conduct comprehensive experiments
evaluating state-of-the-art reasoning models across diverse domains including
mathematical reasoning, code generation, and agentic tasks. Our results
demonstrate that ARISE provides a reliable and fine-grained measurement of
test-time scaling capabilities, revealing significant variations in scaling
efficiency across models. Notably, our evaluation identifies Claude Opus as
exhibiting superior scaling characteristics compared to other contemporary
reasoning models.

</details>


### [53] [Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?](https://arxiv.org/abs/2510.06036)
*Qingyu Yin,Chak Tou Leong,Linyi Yang,Wenxuan Huang,Wenjie Li,Xiting Wang,Jaehong Yoon,YunXing,XingYu,Jinjin Gu*

Main category: cs.AI

TL;DR: 研究发现大型推理模型存在安全对齐失败问题，通过机制可解释性分析发现了"拒绝悬崖"现象：模型在推理过程中能正确识别有害提示并保持拒绝意图，但在输出前最后几个token处拒绝分数急剧下降。通过干预少量注意力头可显著提升安全性，并提出基于拒绝悬崖的数据选择方法，仅用1.7%的安全训练数据即可达到类似效果。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然展现出强大的多步推理能力，但存在令人担忧的安全漏洞，这些漏洞的机制尚未被充分理解。本研究旨在通过机制可解释性方法探究推理模型中安全对齐失败的原因。

Method: 使用线性探测方法追踪token位置上的拒绝意图，发现"拒绝悬崖"现象；通过因果干预分析识别对拒绝行为有负面贡献的稀疏注意力头集合；提出Cliff-as-a-Judge数据选择方法，选择展现最大拒绝悬崖的训练样本来修复推理模型的安全对齐。

Result: 发现许多对齐不良的推理模型在思考过程中能正确识别有害提示并保持强拒绝意图，但在输出前最后几个token处拒绝分数急剧下降；仅消除3%的注意力头就能将攻击成功率降至10%以下；提出的数据选择方法仅使用1.7%的普通安全训练数据即可达到可比的安全改进效果。

Conclusion: 推理模型并非天生不安全，而是其拒绝意图被系统性地抑制；通过机制可解释性分析可以识别关键的安全漏洞，并开发高效的修复方法；Cliff-as-a-Judge方法展示了安全对齐中的"少即是多"效应。

Abstract: Large reasoning models (LRMs) with multi-step reasoning capabilities have
shown remarkable problem-solving abilities, yet they exhibit concerning safety
vulnerabilities that remain poorly understood. In this work, we investigate why
safety alignment fails in reasoning models through a mechanistic
interpretability lens. Using a linear probing approach to trace refusal
intentions across token positions, we discover a striking phenomenon termed as
\textbf{refusal cliff}: many poorly-aligned reasoning models correctly identify
harmful prompts and maintain strong refusal intentions during their thinking
process, but experience a sharp drop in refusal scores at the final tokens
before output generation. This suggests that these models are not inherently
unsafe; rather, their refusal intentions are systematically suppressed. Through
causal intervention analysis, we identify a sparse set of attention heads that
negatively contribute to refusal behavior. Ablating just 3\% of these heads can
reduce attack success rates below 10\%. Building on these mechanistic insights,
we propose \textbf{Cliff-as-a-Judge}, a novel data selection method that
identifies training examples exhibiting the largest refusal cliff to
efficiently repair reasoning models' safety alignment. This approach achieves
comparable safety improvements using only 1.7\% of the vanilla safety training
data, demonstrating a less-is-more effect in safety alignment.

</details>


### [54] [MixReasoning: Switching Modes to Think](https://arxiv.org/abs/2510.06052)
*Haiquan Lu,Gongfan Fang,Xinyin Ma,Qi Li,Xinchao Wang*

Main category: cs.AI

TL;DR: MixReasoning框架通过动态调整推理深度，在困难步骤进行详细推理，在简单步骤进行简洁推理，从而缩短推理长度并提高效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 传统推理模型对所有步骤采用相同详细程度的推理，但子问题难度差异很大，关键步骤需要深入推理，而简单步骤只需简洁处理，这种统一处理方式造成了冗余。

Method: 提出MixReasoning框架，在单个响应中动态调整推理深度，形成混合推理链：对困难步骤进行详细推理，对简单步骤进行简洁推理。

Result: 在GSM8K、MATH-500和AIME数据集上的实验表明，MixReasoning缩短了推理长度，显著提高了效率，且没有影响准确性。

Conclusion: MixReasoning通过自适应调整推理深度，有效解决了推理模型中的冗余问题，在保持精度的同时大幅提升了推理效率。

Abstract: Reasoning models enhance performance by tackling problems in a step-by-step
manner, decomposing them into sub-problems and exploring long chains of thought
before producing an answer. However, applying extended reasoning to every step
introduces substantial redundancy, as sub-problems vary widely in difficulty
and complexity: a small number of pivotal steps are genuinely challenging and
decisive for the final answer, while many others only involve straightforward
revisions or simple computations. Therefore, a natural idea is to endow
reasoning models with the ability to adaptively respond to this variation,
rather than treating all steps with the same level of elaboration. To this end,
we propose MixReasoning, a framework that dynamically adjusts the depth of
reasoning within a single response. The resulting chain of thought then becomes
a mixture of detailed reasoning on difficult steps and concise inference on
simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning
shortens reasoning length and substantially improves efficiency without
compromising accuracy.

</details>


### [55] [Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research](https://arxiv.org/abs/2510.06056)
*Gang Liu,Yihan Zhu,Jie Chen,Meng Jiang*

Main category: cs.AI

TL;DR: DeepEvolve是一个结合深度研究和算法演化的科学助手代理，通过外部知识检索、跨文件代码编辑和系统调试的反馈驱动迭代循环，持续改进算法性能。


<details>
  <summary>Details</summary>
Motivation: 现有科学助手要么仅依赖算法演化（如AlphaEvolve），要么只进行孤立深度研究，两者都有严重局限性。纯算法演化依赖LLM内部知识，在复杂领域快速达到瓶颈；纯深度研究提出想法但不验证，导致不切实际的解决方案。

Method: DeepEvolve整合深度研究与算法演化，包含外部知识检索、跨文件代码编辑和系统调试，在反馈驱动的迭代循环中不仅提出新假设，还进行精炼、实现和测试。

Result: 在化学、数学、生物学、材料和专利等九个基准测试中，DeepEvolve持续改进初始算法，产生可执行的新算法并保持持续增益。

Conclusion: 通过弥合无引导演化与无基础研究之间的差距，DeepEvolve为推进科学算法发现提供了可靠框架。

Abstract: Large language models hold promise as scientific assistants, yet existing
agents either rely solely on algorithm evolution or on deep research in
isolation, both of which face critical limitations. Pure algorithm evolution,
as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly
plateaus in complex domains, while pure deep research proposes ideas without
validation, resulting in unrealistic or unimplementable solutions. We present
DeepEvolve, an agent that integrates deep research with algorithm evolution,
uniting external knowledge retrieval, cross-file code editing, and systematic
debugging under a feedback-driven iterative loop. Each iteration not only
proposes new hypotheses but also refines, implements, and tests them, avoiding
both shallow improvements and unproductive over-refinements. Across nine
benchmarks in chemistry, mathematics, biology, materials, and patents,
DeepEvolve consistently improves the initial algorithm, producing executable
new algorithms with sustained gains. By bridging the gap between unguided
evolution and research without grounding, DeepEvolve provides a reliable
framework for advancing scientific algorithm discovery. Our code is available
at https://github.com/liugangcode/deepevolve.

</details>


### [56] [Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents](https://arxiv.org/abs/2510.06078)
*Tao Zhe,Rui Liu,Fateme Memar,Xiao Luo,Wei Fan,Xinyue Ye,Zhongren Peng,Dongjie Wang*

Main category: cs.AI

TL;DR: RouteLLM是一个分层多智能体框架，将自然语言意图转化为约束感知的路线推荐，通过协调多个专业智能体来解析查询、处理约束、检索POI和优化路径。


<details>
  <summary>Details</summary>
Motivation: 传统路由算法假设结构化输入和固定目标，无法适应自然语言查询；现有LLM方法在空间推理和联合建模路线级与POI级偏好方面存在困难。

Method: 分层多智能体框架：首先解析用户查询为结构化意图，然后管理器协调约束智能体、POI智能体和路径优化智能体，最后验证器确保约束满足并生成可解释的路线。

Result: 实验表明该方法能可靠地将文本偏好转化为约束感知路线，在路线质量和偏好满足度方面优于传统方法。

Conclusion: RouteLLM成功弥合了语言灵活性和空间结构之间的差距，能够对路线可行性和用户偏好进行推理。

Abstract: Route recommendation aims to provide users with optimal travel plans that
satisfy diverse and complex requirements. Classical routing algorithms (e.g.,
shortest-path and constraint-aware search) are efficient but assume structured
inputs and fixed objectives, limiting adaptability to natural-language queries.
Recent LLM-based approaches enhance flexibility but struggle with spatial
reasoning and the joint modeling of route-level and POI-level preferences. To
address these limitations, we propose RouteLLM, a hierarchical multi-agent
framework that grounds natural-language intents into constraint-aware routes.
It first parses user queries into structured intents including POIs, paths, and
constraints. A manager agent then coordinates specialized sub-agents: a
constraint agent that resolves and formally check constraints, a POI agent that
retrieves and ranks candidate POIs, and a path refinement agent that refines
routes via a routing engine with preference-conditioned costs. A final verifier
agent ensures constraint satisfaction and produces the final route with an
interpretable rationale. This design bridges linguistic flexibility and spatial
structure, enabling reasoning over route feasibility and user preferences.
Experiments show that our method reliably grounds textual preferences into
constraint-aware routes, improving route quality and preference satisfaction
over classical methods.

</details>


### [57] [Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices](https://arxiv.org/abs/2510.06093)
*Mallika Mainali,Harsha Sureshbabu,Anik Sen,Christopher B. Rauch,Noah D. Reifsnyder,John Meyer,J. T. Turner,Michael W. Floyd,Matthew Molineaux,Rosina O. Weber*

Main category: cs.AI

TL;DR: 本文比较了经典AI方法和基于大语言模型的方法在决策者对齐任务中的表现，发现在健康保险决策数据集上两种方法表现相当，经典AI在中等风险偏好下略优。


<details>
  <summary>Details</summary>
Motivation: 随着算法决策在关键领域应用增多，AI对齐研究从通用价值对齐转向考虑决策者特定属性的上下文对齐方法。现有决策者对齐方法在泛化性方面研究不足。

Method: 实现了一个经典AI模型并开发了基于LLM的算法决策器，使用GPT-5和GPT-4在零样本提示框架下评估，测试于包含三种风险容忍度决策者的健康保险决策数据集。

Result: 经典AI和LLM模型在属性对齐方面表现相当，经典AI在中等风险偏好下对齐度略好。

Conclusion: 两种方法在决策者对齐任务中表现相似，经典AI在特定风险偏好下略有优势，为AI对齐研究提供了实用比较。

Abstract: As algorithmic decision-makers are increasingly applied to high-stakes
domains, AI alignment research has evolved from a focus on universal value
alignment to context-specific approaches that account for decision-maker
attributes. Prior work on Decision-Maker Alignment (DMA) has explored two
primary strategies: (1) classical AI methods integrating case-based reasoning,
Bayesian reasoning, and naturalistic decision-making, and (2) large language
model (LLM)-based methods leveraging prompt engineering. While both approaches
have shown promise in limited domains such as medical triage, their
generalizability to novel contexts remains underexplored. In this work, we
implement a prior classical AI model and develop an LLM-based algorithmic
decision-maker evaluated using a large reasoning model (GPT-5) and a
non-reasoning model (GPT-4) with weighted self-consistency under a zero-shot
prompting framework, as proposed in recent literature. We evaluate both
approaches on a health insurance decision-making dataset annotated for three
target decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).
In the experiments reported herein, classical AI and LLM-based models achieved
comparable alignment with attribute-based targets, with classical AI exhibiting
slightly better alignment for a moderate risk profile. The dataset and
open-source implementation are publicly available at:
https://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and
https://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.

</details>


### [58] [Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences](https://arxiv.org/abs/2510.06105)
*Batu El,James Zou*

Main category: cs.AI

TL;DR: 优化大型语言模型在竞争环境中的表现会无意中导致模型失准，表现为欺骗性营销、虚假信息和有害行为的增加，即使模型被明确指示保持真实和可靠。


<details>
  <summary>Details</summary>
Motivation: 理解竞争性反馈循环如何影响LLM行为，特别是在商业广告、选举和社交媒体等竞争性环境中，这些环境中各方都在争夺受众认可。

Method: 使用模拟环境在这些场景中进行实验，测量在优化竞争成功时LLM的行为变化。

Result: 在商业场景中，销售额增加6.3%伴随着欺骗性营销增加14.0%；在选举中，得票率增加4.9%伴随着虚假信息增加22.3%和民粹主义言论增加12.5%；在社交媒体中，参与度增加7.5%伴随着虚假信息增加188.6%和有害行为推广增加16.3%。

Conclusion: 竞争驱动的优化压力会系统性地侵蚀对齐性，形成竞次现象，AI系统的安全部署需要更强的治理和精心设计的激励机制来防止竞争动态破坏社会信任。

Abstract: Large language models (LLMs) are increasingly shaping how information is
created and disseminated, from companies using them to craft persuasive
advertisements, to election campaigns optimizing messaging to gain votes, to
social media influencers boosting engagement. These settings are inherently
competitive, with sellers, candidates, and influencers vying for audience
approval, yet it remains poorly understood how competitive feedback loops
influence LLM behavior. We show that optimizing LLMs for competitive success
can inadvertently drive misalignment. Using simulated environments across these
scenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise
in deceptive marketing; in elections, a 4.9% gain in vote share coincides with
22.3% more disinformation and 12.5% more populist rhetoric; and on social
media, a 7.5% engagement boost comes with 188.6% more disinformation and a
16.3% increase in promotion of harmful behaviors. We call this phenomenon
Moloch's Bargain for AI--competitive success achieved at the cost of alignment.
These misaligned behaviors emerge even when models are explicitly instructed to
remain truthful and grounded, revealing the fragility of current alignment
safeguards. Our findings highlight how market-driven optimization pressures can
systematically erode alignment, creating a race to the bottom, and suggest that
safe deployment of AI systems will require stronger governance and carefully
designed incentives to prevent competitive dynamics from undermining societal
trust.

</details>


### [59] [Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification](https://arxiv.org/abs/2510.06135)
*Weihao Zeng,Keqing He,Chuqiao Kuang,Xiaoguang Li,Junxian He*

Main category: cs.AI

TL;DR: 该论文研究测试时计算扩展（TTS），包括顺序扩展和并行扩展，利用验证比生成更容易的不对称验证特性，在深度搜索代理中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在特定场景（如解决数独谜题）中，验证答案比生成答案要容易得多，这种不对称验证特性凸显了测试时扩展的潜力。

Method: 结合顺序扩展（延长生成过程）和并行扩展（验证和选择多个候选输出），利用不对称验证特性，为验证器分配适度的计算资源。

Result: 通过TTS扩展开源模型为"Heavy"变体，在BrowseComp基准上获得高达27个绝对点的提升，GLM-4.5 Heavy在BrowseComp和GAIA上分别达到54.0%和66.0%的准确率，与最佳专有模型相当。Tongyi-DeepResearch Heavy在BrowseComp上达到69.0%的准确率，大幅超越专有模型。

Conclusion: 测试时计算扩展，特别是利用不对称验证特性，能够显著提升深度搜索代理的性能，使开源模型达到甚至超越专有模型的水平。

Abstract: Test-time compute can be scaled both sequentially and in parallel. Sequential
scaling involves lengthening the generation process, while parallel scaling
involves verifying and selecting among multiple candidate outputs. Combining
these two strategies has led to the most powerful AI systems, such as Grok 4
Heavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles),
verifying responses can be substantially easier than generating them. This
property, referred to as \emph{asymmetric verification}, highlights the strong
potential of test-time scaling (TTS). In this work, we study both sequential
and parallel TTS of deep search agents, motivated by the intuition that
verification in this setting is often much easier than generation. In
experiments, we first show that sequential scaling methods, such as budget
forcing, can be effective initially but soon degrade performance. Leveraging
asymmetric verification, however, we are able to achieve substantial
improvements by allocating only a modest amount of compute to the verifier. We
conduct experiments with flagship open-source models and extend them to their
``Heavy'' variants through TTS. These deep research agents achieve gains of up
to 27 absolute points on benchmarks such as BrowseComp. Remarkably, as an
open-source alternative, GLM-4.5 Heavy reaches accuracy of {\bf 54.0\%} on
BrowseComp and {\bf 66.0\%} on GAIA, placing it comparable to the best
proprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavy
further achieves {\bf 69.0\%} accuracy on BrowseComp, greatly surpassing the
best proprietary results.

</details>


### [60] [Barbarians at the Gate: How AI is Upending Systems Research](https://arxiv.org/abs/2510.06189)
*Audrey Cheng,Shu Liu,Melissa Pan,Zhifei Li,Bowen Wang,Alex Krentsel,Tian Xia,Mert Cemri,Jongseok Park,Shuo Yang,Jeff Chen,Aditya Desai,Jiarong Xing,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.AI

TL;DR: AI驱动的系统研究(ADRS)通过自动生成、评估和优化算法解决方案来变革研究过程，在多个系统领域发现了超越人类设计的最先进算法。


<details>
  <summary>Details</summary>
Motivation: 系统研究特别适合AI驱动的方法，因为系统性能问题天然具备可靠的验证器——解决方案可以在真实系统或模拟器中运行测试，通过预定义工作负载来验证性能。

Method: 提出AI驱动的系统研究(ADRS)方法，迭代地生成、评估和优化解决方案。使用penEvolve开源实例，在负载均衡、专家混合推理、LLM SQL查询和事务调度等领域进行案例研究。

Result: ADRS在多个实例中发现了超越人类设计的最先进算法，实现了高达5.0倍的运行时改进或50%的成本降低。

Conclusion: AI将在算法设计中扮演核心角色，人类研究者将更多专注于问题制定和战略指导。系统研究实践需要适应AI时代，既面临颠覆性潜力也需紧迫调整。

Abstract: Artificial Intelligence (AI) is starting to transform the research process as
we know it by automating the discovery of new solutions. Given a task, the
typical AI-driven approach is (i) to generate a set of diverse solutions, and
then (ii) to verify these solutions and select one that solves the problem.
Crucially, this approach assumes the existence of a reliable verifier, i.e.,
one that can accurately determine whether a solution solves the given problem.
We argue that systems research, long focused on designing and evaluating new
performance-oriented algorithms, is particularly well-suited for AI-driven
solution discovery. This is because system performance problems naturally admit
reliable verifiers: solutions are typically implemented in real systems or
simulators, and verification reduces to running these software artifacts
against predefined workloads and measuring performance. We term this approach
as AI-Driven Research for Systems (ADRS), which iteratively generates,
evaluates, and refines solutions. Using penEvolve, an existing open-source ADRS
instance, we present case studies across diverse domains, including load
balancing for multi-region cloud scheduling, Mixture-of-Experts inference,
LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS
discovers algorithms that outperform state-of-the-art human designs (e.g.,
achieving up to 5.0x runtime improvements or 50% cost reductions). We distill
best practices for guiding algorithm evolution, from prompt design to evaluator
construction, for existing frameworks. We then discuss the broader implications
for the systems community: as AI assumes a central role in algorithm design, we
argue that human researchers will increasingly focus on problem formulation and
strategic guidance. Our results highlight both the disruptive potential and the
urgent need to adapt systems research practices in the age of AI.

</details>


### [61] [TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning](https://arxiv.org/abs/2510.06217)
*Jiaru Zou,Soumya Roy,Vinay Kumar Verma,Ziyi Wang,David Wipf,Pan Lu,Sumit Negi,James Zou,Jingrui He*

Main category: cs.AI

TL;DR: 提出了TaTToo框架，通过表格接地和工具验证的PRM方法，显著提升了表格推理任务的性能，在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有PRM在表格推理领域存在局限性，特别是在子表检索和模式交互等表格特定操作上表现不佳，需要专门针对表格推理的监督方法。

Method: 设计了可扩展的数据标注流程构建6万多个高质量步骤级标注，采用双阶段训练范式：冷启动监督微调学习工具使用推理模式，然后通过强化学习和工具接地奖励塑形进行对齐。

Result: 在5个具有挑战性的表格推理基准测试中，TaTToo将下游策略LRM的推理性能提升了30.9%，仅用8B参数就超越了Qwen-2.5-Math-PRM-72B等强基线。

Conclusion: TaTToo框架通过表格接地和工具验证有效解决了表格推理中的关键瓶颈，展示了在多样化TTS策略下的强泛化能力。

Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework
for enhancing the reasoning capabilities of large reasoning models (LRMs),
particularly in the context of test-time scaling (TTS). However, their
potential for supervising LRMs on tabular reasoning domains remains
underexplored. Through detailed empirical analyses, we identify that existing
PRMs, though widely adopted for supervising text-only reasoning steps, struggle
with table-specific operations such as sub-table retrieval and schema
interaction, leading to critical performance bottlenecks. To address this
limitation, we propose TaTToo, a novel table-grounded PRM framework that (i)
reasons explicitly over tabular reasoning steps and (ii) integrates tool-based
verification to provide precise reward supervision. Concretely, we first design
a scalable data curation pipeline that constructs over 60k high-quality
step-level annotations by integrating table verification rationales with
tool-based executions. Building on the collected data, we train TaTToo with a
dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use
reasoning patterns, followed by reinforcement learning with tool-grounded
reward shaping to align our model with table-based verification. We provide a
comprehensive evaluation of the policy improvement induced by our newly
designed PRM. Across 5 challenging tabular reasoning benchmarks covering
numerical reasoning, fact-checking, and data analysis, TaTToo improves
downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines
such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong
generalizability across diverse TTS strategies.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [62] [Encoded Jamming Secure Communication for RIS-Assisted and ISAC Systems](https://arxiv.org/abs/2510.05247)
*Hao Yang,Hao Xu,Kai Wan,Sijie Zhao,Robert Caiming Qiu*

Main category: cs.IT

TL;DR: 提出了一种结合可重构智能表面(RIS)和编码干扰(EJ)的联合优化框架，用于提升无线通信系统的安全速率。在MISO和MIMO场景下分别采用不同的优化算法，并首次将EJ集成到ISAC系统中，揭示了安全性与感知之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 传统的高斯噪声干扰方案会降低合法接收器的性能，而编码干扰方案在不同信道条件下并不总是优于高斯噪声。为了解决这一局限性，需要开发更有效的安全通信方案。

Method: 在MISO场景下采用基于半定松弛的交替优化方法，在MIMO场景下开发基于加权和均方误差最小化的交替优化算法。首次将编码干扰集成到ISAC系统中，使用改进的WMMSE算法求解联合优化问题。

Result: 仿真结果表明，所提出的方案在不同信道条件下显著优于基准方法的安全速率，并清晰地揭示了安全性与感知之间的权衡关系。

Conclusion: 提出的RIS-EJ联合优化框架能够有效提升无线通信系统的安全性能，同时为ISAC系统中的安全与感知权衡提供了新的解决方案。

Abstract: This paper considers a cooperative jamming (CJ)-aided secure wireless
communication system. Conventionally, the jammer transmits Gaussian noise (GN)
to enhance security; however, the GN scheme also degrades the legitimate
receiver's performance. Encoded jamming (EJ) mitigates this interference but
does not always outperform GN under varying channel conditions. To address this
limitation, we propose a joint optimization framework that integrates
reconfigurable intelligent surface (RIS) with EJ to maximize the secrecy rate.
In the multiple-input single-output (MISO) case, we adopt a semidefinite
relaxation (SDR)-based alternating optimization method, while in the
multiple-input multiple-output (MIMO) case, we develop an alternating
optimization algorithm based on the weighted sum mean-square-error minimization
(WMMSE) scheme. Furthermore, we are the first to incorporate EJ into an
integrated sensing and communication (ISAC) system, characterizing the Pareto
boundary between secrecy rate and sensing mutual information (MI) by solving
the resulting joint optimization problem using a modified WMMSE-based
algorithm. Simulation results show that the proposed schemes significantly
outperform benchmark methods in secrecy rate across diverse channel conditions
and clearly reveal the trade-off between security and sensing.

</details>


### [63] [On Binary Codes That Are Maximal Totally Isotropic Subspaces with Respect to an Alternating Form](https://arxiv.org/abs/2510.05464)
*Patrick King,Mikhail Kotchetov*

Main category: cs.IT

TL;DR: 研究在交替内积下自对偶的二进制线性码，分类了n≤24的最大全迷向码，并建立了MacWilliams型恒等式。


<details>
  <summary>Details</summary>
Motivation: 虽然自对偶二进制线性码在n≤40时已被广泛研究和分类，但在非点积内积下与其正交补重合的线性码研究较少。

Method: 在F_2^n上定义交替形式，研究相对于该形式的最大全迷向码，分类n≤24的情况，并建立MacWilliams型恒等式。

Result: 分类了n≤24的最大全迷向码，推导了最大全迷向码权重枚举子的约束条件。

Conclusion: 扩展了自对偶码的研究范围，为交替内积下的线性码理论提供了新工具和分类结果。

Abstract: Self-dual binary linear codes have been extensively studied and classified
for length n <= 40. However, little attention has been paid to linear codes
that coincide with their orthogonal complement when the underlying inner
product is not the dot product. In this paper, we introduce an alternating form
defined on F_2^n and study codes that are maximal totally isotropic with
repsect to this form. We classify such codes for n <= 24 and present a
MacWilliams-type identity which relates the weight enumerator of a linear code
and that of its orthogonal complement with respect to our alternating inner
product. As an application, we derive constraints on the weight enumerators of
maximal totally isotropic codes.

</details>


### [64] [Mutual Information Estimation via Score-to-Fisher Bridge for Nonlinear Gaussian Noise Channels](https://arxiv.org/abs/2510.05496)
*Tadashi Wadayama*

Main category: cs.IT

TL;DR: 提出了一种使用去噪分数匹配学习来估计非线性高斯噪声信道中互信息的数值方法，通过Fisher信息准确估计互信息。


<details>
  <summary>Details</summary>
Motivation: 需要一种实用且高效的方法来估计非线性高斯噪声信道中的互信息，特别是在没有闭式解的情况下。

Method: 使用去噪分数匹配学习估计信道输出的分数函数，通过de Bruijn恒等式和I-MMSE关系，从学习的分数中估计Fisher信息，进而通过积分表示获得互信息。

Result: 数值实验表明该方法在各种先验和信道非线性情况下都能准确估计互信息，比核密度估计基线更有效。

Conclusion: 提出的Score-to-Fisher桥接方法为非线性高斯噪声信道中的互信息估计提供了实用且高效的解决方案。

Abstract: We present a numerical method to evaluate mutual information (MI) in
nonlinear Gaussian noise channels by using denoising score matching learning
for estimating the score function of channel output. Via de Bruijn identity and
the I--MMSE relation, Fisher information estimated from the learned score
yields accurate estimates of MI through an integral representation of MI for a
variety of priors and channel nonlinearities (e.g., elementwise nonlinearity).
In this work, we propose a comprehensive theoretical foundation for the
Score-to-Fisher bridge methodology, along with practical guidelines for its
implementation. We also conduct extensive validation experiments, comparing our
approach with closed-form solutions and a kernel density estimation baseline.
The results of our numerical experiments demonstrate that the proposed method
is both practical and efficient for mutual information estimation in nonlinear
Gaussian noise channels.

</details>


### [65] [Channel Simulation and Distributed Compression with Ensemble Rejection Sampling](https://arxiv.org/abs/2510.05552)
*Buu Phan,Ashish Khisti*

Main category: cs.IT

TL;DR: 本文研究了信道模拟和分布式匹配问题，通过引入集成拒绝采样(ERS)算法，提出了新的编码方案，在信道模拟中实现了接近最优的编码率，并提出了ERS的分布式匹配引理，应用于分布式压缩任务。


<details>
  <summary>Details</summary>
Motivation: 研究信道模拟和分布式匹配这两个在机器学习中有广泛应用的基础问题，探索拒绝采样算法家族在分布式场景下的潜力，特别是集成拒绝采样(ERS)的应用价值。

Method: 提出了基于ERS的新编码方案，证明了标准拒绝采样也能达到接近最优的编码率，并将Braverman和Garg的结果推广到连续字母表设置。主要贡献是提出了ERS的分布式匹配引理，这是拒绝采样方案家族中首个匹配概率接近PML的结果。

Result: 在信道模拟中实现了接近最优的编码率，提出的分布式匹配引理推广了重要性匹配引理，并在分布式压缩实验中验证了方法的有效性，包括合成高斯源和MNIST数据集的分布式图像压缩。

Conclusion: ERS在信道模拟和分布式匹配中表现出色，提出的分布式匹配引理填补了拒绝采样方案家族在分布式匹配方面的空白，实验验证了该方法在分布式压缩任务中的实用价值。

Abstract: We study channel simulation and distributed matching, two fundamental
problems with several applications to machine learning, using a recently
introduced generalization of the standard rejection sampling (RS) algorithm
known as Ensemble Rejection Sampling (ERS). For channel simulation, we propose
a new coding scheme based on ERS that achieves a near-optimal coding rate. In
this process, we demonstrate that standard RS can also achieve a near-optimal
coding rate and generalize the result of Braverman and Garg (2014) to the
continuous alphabet setting. Next, as our main contribution, we present a
distributed matching lemma for ERS, which serves as the rejection sampling
counterpart to the Poisson Matching Lemma (PML) introduced by Li and Anantharam
(2021). Our result also generalizes a recent work on importance matching lemma
(Phan et al, 2024) and, to our knowledge, is the first result on distributed
matching in the family of rejection sampling schemes where the matching
probability is close to PML. We demonstrate the practical significance of our
approach over prior works by applying it to distributed compression. The
effectiveness of our proposed scheme is validated through experiments involving
synthetic Gaussian sources and distributed image compression using the MNIST
dataset.

</details>


### [66] [SALAD: Self-Adaptive Link Adaptation](https://arxiv.org/abs/2510.05784)
*Reinhard Wiesmayr,Lorenzo Maggi,Sebastian Cammerer,Jakob Hoydis,Fayçal Aït Aoudia,Alexander Keller*

Main category: cs.IT

TL;DR: SALAD是一种基于ACK/NACK反馈的自适应链路适配算法，通过最小化交叉熵损失来推断SINR，并使用假设测试选择MCS，在5G测试中比标准OLLA算法提升15%吞吐量和频谱效率。


<details>
  <summary>Details</summary>
Motivation: 为了在保证可靠性的同时最大化频谱效率，需要根据无线链路质量自适应调整调制和编码方案(MCS)。现有方法在跟踪信号干扰噪声比(SINR)演化方面存在局限性。

Method: SALAD算法仅利用ACK/NACK反馈，通过最小化交叉熵损失推断SINR，采用自适应的学习率，通过知识蒸馏在线调整。基于SINR推断，通过假设测试选择MCS，并加入反馈控制回路调整瞬时BLER目标。

Result: 在5G测试平台上的实验表明，SALAD始终优于行业标准的外环链路适配(OLLA)。使用单一参数集，在不同流量场景下比多个OLLA变体实现高达15%的吞吐量和频谱效率提升，同时满足BLER目标。

Conclusion: SALAD算法能够可靠跟踪SINR演化，在保持长期BLER接近期望目标的同时实现高频谱效率，显著优于现有标准方法。

Abstract: Adapting the modulation and coding scheme (MCS) to the wireless link quality
is critical for maximizing spectral efficiency while ensuring reliability. We
propose SALAD (self-adaptive link adaptation), an algorithm that exclusively
leverages ACK/NACK feedback to reliably track the evolution of the
signal-to-interference-plus-noise ratio (SINR), achieving high spectral
efficiency while keeping the long-term block error rate (BLER) near a desired
target. SALAD infers the SINR by minimizing the cross-entropy loss between
received ACK/NACKs and predicted BLER values, with a learning rate that
self-adapts online through knowledge distillation. Based on this inference,
SALAD selects the MCS via hypothesis testing: if the SINR is likely
underestimated, a higher MCS is selected to accelerate link adaptation under
improving channel conditions. To prevent BLER drift from its long-term target,
SALAD incorporates a feedback control loop that adjusts the instantaneous BLER
target. Over-the-air experiments on a 5G testbed demonstrate that SALAD
consistently outperforms the industry-standard outer-loop link adaptation
(OLLA). With a single set of parameters, SALAD achieves up to 15% higher
throughput and spectral efficiency than multiple OLLA variants across different
traffic regimes, while meeting the BLER target.

</details>


### [67] [Risk level dependent Minimax Quantile lower bounds for Interactive Statistical Decision Making](https://arxiv.org/abs/2510.05808)
*Raghav Bongole,Amirreza Zamani,Tobias J. Oechtering,Mikael Skoglund*

Main category: cs.IT

TL;DR: 该论文开发了高概率Fano和Le Cam工具，用于交互式统计决策中的极小极大分位数分析，填补了安全关键应用中罕见失败风险的研究空白。


<details>
  <summary>Details</summary>
Motivation: 现有极小化风险方法主要关注期望值，忽略了安全关键应用中罕见但关键的失败情况。需要开发分位数特定的工具来分析交互协议中的风险尾部。

Method: 在交互式统计决策框架内，开发高概率Fano和Le Cam工具，推导风险水平明确的极小极大分位数边界，包括分位数到期望的转换和严格与下界极小极大分位数之间的紧密联系。

Result: 为两臂高斯赌博机实例化这些结果，立即恢复了最优速率边界，证明了方法的有效性。

Conclusion: 所开发的工具填补了交互协议中分位数特定分析的空白，为安全关键应用中的风险尾部分析提供了理论基础。

Abstract: Minimax risk and regret focus on expectation, missing rare failures critical
in safety-critical bandits and reinforcement learning. Minimax quantiles
capture these tails. Three strands of prior work motivate this study:
minimax-quantile bounds restricted to non-interactive estimation; unified
interactive analyses that focus on expected risk rather than risk level
specific quantile bounds; and high-probability bandit bounds that still lack a
quantile-specific toolkit for general interactive protocols. To close this gap,
within the interactive statistical decision making framework, we develop
high-probability Fano and Le Cam tools and derive risk level explicit
minimax-quantile bounds, including a quantile-to-expectation conversion and a
tight link between strict and lower minimax quantiles. Instantiating these
results for the two-armed Gaussian bandit immediately recovers optimal-rate
bounds.

</details>


### [68] [Medium Access for Multi-Cell ISAC through Scheduling of Radar and Communication Tasks](https://arxiv.org/abs/2510.05821)
*João Henrique Inacio de Souza,Fabio Saggese,Kun Chen-Hu,Petar Popovski*

Main category: cs.IT

TL;DR: 提出多小区ISAC网络中通信、雷达搜索和跟踪任务调度的MAC框架，通过干扰感知的任务分配优化雷达扫描模式，在QoS约束下提高资源效率。


<details>
  <summary>Details</summary>
Motivation: 解决多小区集成感知与通信网络中通信、雷达搜索和跟踪任务的协调调度问题，在满足服务质量约束的同时提高资源利用效率。

Method: 提出基于干扰感知任务分配的中介访问控制框架，通过优化雷达扫描模式来复用多个任务。

Result: 仿真结果表明，该方案在保证目标服务质量的同时，相比基准方案显著提高了资源效率。

Conclusion: 多小区ISAC网络中的协调调度具有显著优势，能够有效保证服务质量并提高资源利用效率。

Abstract: This paper focuses on communication, radar search, and tracking task
scheduling in multi-cell integrated sensing and communication (ISAC) networks
under quality of service (QoS) constraints. We propose a medium access control
framework multiplexing the tasks while optimizing radar scan patterns through
an interference-aware assignment formulation. Simulations show that our
solution guarantees target QoS with improved resource efficiency over baseline
schemes, highlighting the benefits of coordinated scheduling in multi-cell
ISAC.

</details>


### [69] [Recursive construction and enumeration of self-orthogonal and self-dual codes over finite commutative chain rings of even characteristic - II](https://arxiv.org/abs/2510.06069)
*Monika Yadav,Anuradha Sharma*

Main category: cs.IT

TL;DR: 本文提出了一种在有限交换链环上构造自正交码的递归方法，并给出了自正交码和自对偶码的显式计数公式。


<details>
  <summary>Details</summary>
Motivation: 研究有限交换链环上的自正交码构造和计数问题，旨在建立从Teichmüller集上的自正交码链到环上自正交码的递归构造方法。

Method: 使用递归构造方法，从Teichmüller集上的自正交码链构造环上的自正交码，并利用群论和有限几何的结果。

Result: 获得了在任意长度下有限交换链环上所有自正交码和自对偶码的显式计数公式。

Conclusion: 提出的递归构造方法有效解决了有限交换链环上自正交码的构造和计数问题，并通过实例验证了结果。

Abstract: Let $\mathcal{R}_{e,m}$ be a finite commutative chain ring of even
characteristic with maximal ideal $\langle u \rangle$ of nilpotency index $e
\geq 2,$ Teichm$\ddot{u}$ller set $\mathcal{T}_{m},$ and residue field
$\mathcal{R}_{e,m}/\langle u \rangle$ of order $2^m.$ Suppose that $2 \in
\langle u^{\kappa}\rangle \setminus \langle u^{\kappa+1}\rangle$ for some even
positive integer $ \kappa \leq e.$ In this paper, we provide a recursive method
to construct a self-orthogonal code $\mathcal{C}_e$ of type $\{\lambda_1,
\lambda_2, \ldots, \lambda_e\}$ and length $n$ over $\mathcal{R}_{e,m}$ from a
chain $\mathcal{D}^{(1)}\subseteq \mathcal{D}^{(2)} \subseteq \cdots \subseteq
\mathcal{D}^{(\lceil \frac{e}{2} \rceil)}$ of self-orthogonal codes of length
$n$ over $\mathcal{T}_{m},$ and vice versa, where $\dim
\mathcal{D}^{(i)}=\lambda_1+\lambda_2+\cdots+\lambda_i$ for $1 \leq i \leq
\lceil \frac{e}{2} \rceil,$ the codes $\mathcal{D}^{(\lfloor \frac{e+1}{2}
\rfloor-\kappa)},\mathcal{D}^{(\lfloor \frac{e+1}{2} \rfloor
-\kappa+1)},\ldots,\mathcal{D}^{(\lfloor \frac{e}{2}\rfloor-\lfloor
\frac{\kappa}{2} \rfloor)}$ satisfy certain additional conditions, and
$\lambda_1,\lambda_2,\ldots,\lambda_e$ are non-negative integers satisfying
$2\lambda_1+2\lambda_2+\cdots+2\lambda_{e-i+1}+\lambda_{e-i+2}+\lambda_{e-i+3}+\cdots+\lambda_i
\leq n$ for $\lceil \frac{e+1}{2} \rceil \leq i\leq e.$ This construction
guarantees that $Tor_i(\mathcal{C}_e)=\mathcal{D}^{(i)}$ for $1 \leq i \leq
\lceil \frac{e}{2} \rceil.$ By employing this recursive construction method,
together with the results from group theory and finite geometry, we derive
explicit enumeration formulae for all self-orthogonal and self-dual codes of an
arbitrary length over $\mathcal{R}_{e,m}.$ We also demonstrate these results
through examples.

</details>


### [70] [Recursive construction and enumeration of self-orthogonal and self-dual codes over finite commutative chain rings of even characteristic - I](https://arxiv.org/abs/2510.06082)
*Monika Yadav,Anuradha Sharma*

Main category: cs.IT

TL;DR: 本文提出了一种递归方法，在特征为偶数的有限交换链环上构造自正交码，并给出了自正交码和自对偶码的显式枚举公式。


<details>
  <summary>Details</summary>
Motivation: 研究有限交换链环上的自正交码构造和枚举问题，特别是在特征为偶数的环上，这类问题在编码理论和密码学中有重要应用。

Method: 开发递归构造方法，从Teichmüller集上的自正交码链构造环上的自正交码，反之亦然。利用群论和有限几何的结果。

Result: 获得了任意长度下自正交码和自对偶码的显式枚举公式，并通过示例进行了说明。

Conclusion: 提出的递归构造方法有效解决了有限交换链环上自正交码的构造和计数问题，为相关编码理论研究提供了新工具。

Abstract: Let $\mathscr{R}_{e,m}$ denote a finite commutative chain ring of even
characteristic with maximal ideal $\langle u \rangle$ of nilpotency index $e
\geq 3,$ Teichm$\ddot{u}$ller set $\mathcal{T}_{m},$ and residue field
$\mathscr{R}_{e,m}/\langle u \rangle$ of order $2^m.$ Suppose that $2 \in
\langle u^{\kappa}\rangle \setminus \langle u^{\kappa+1}\rangle$ for some odd
integer $\kappa$ with $3 \leq \kappa \leq e.$ In this paper, we first develop a
recursive method to construct a self-orthogonal code $\mathscr{D}_e$ of type
$\{\lambda_1, \lambda_2, \ldots, \lambda_e\}$ and length $n$ over
$\mathscr{R}_{e,m}$ from a chain $\mathcal{C}^{(1)}\subseteq \mathcal{C}^{(2)}
\subseteq \cdots \subseteq \mathcal{C}^{(\lceil \frac{e}{2} \rceil)} $ of
self-orthogonal codes of length $n$ over $\mathcal{T}_{m},$ and vice versa,
subject to certain conditions, where $\lambda_1,\lambda_2,\ldots,\lambda_e$ are
non-negative integers satisfying
$2\lambda_1+2\lambda_2+\cdots+2\lambda_{e-i+1}+\lambda_{e-i+2}+\lambda_{e-i+3}+\cdots+\lambda_i
\leq n$ for $\lceil \frac{e+1}{2} \rceil \leq i\leq e,$ and
  $\lfloor \cdot \rfloor$ and $\lceil \cdot \rceil$ denote the floor and
ceiling functions, respectively. This construction ensures that
$Tor_i(\mathscr{D}_e)=\mathcal{C}^{(i)}$ for $1 \leq i \leq \lceil \frac{e}{2}
\rceil.$
  With the help of this recursive construction method and by applying results
from group theory and finite geometry, we obtain explicit enumeration formulae
for all self-orthogonal and self-dual codes of an arbitrary length over
$\mathscr{R}_{e,m}.$ We also illustrate these results with some examples.

</details>


### [71] [Probabilistic Guarantees to Explicit Constructions: Local Properties of Linear Codes](https://arxiv.org/abs/2510.06185)
*Fernando Granha Jeronimo,Nikhil Shagrithaya*

Main category: cs.IT

TL;DR: 提出了一个通用的框架，用于对随机线性码进行去随机化，针对一类广泛的置换不变性质（称为局部性质），包括距离、列表解码、列表恢复和完美哈希等标准概念。


<details>
  <summary>Details</summary>
Motivation: 扩展经典的Alon-Edmonds-Luby (AEL)构造，通过改进的局部坐标线性(LCL)性质形式化，为随机线性码的局部性质提供系统性的去随机化方法。

Method: 基于Levi、Mosheiff和Shagrithaya (2025)引入的局部坐标线性(LCL)性质，扩展AEL构造框架，证明如果随机线性码以高概率满足LCL性质的补集，则可以构造显式码也满足该补集，但使用扩大但恒定的字母表大小。

Result: 首次为列表恢复及其特殊情况（如带擦除的列表恢复、零错误列表恢复、完美哈希矩阵）提供了显式构造，参数与随机线性码匹配。实现了这些性质的全部参数范围，达到与随机设置相同的最优性水平。

Conclusion: 该框架提供了从概率保证到实现这些保证的显式码的系统路径，并且去随机化的随机线性码还支持通过基于扩展器的解码器进行高效（列表）解码。

Abstract: We present a general framework for derandomizing random linear codes with
respect to a broad class of permutation-invariant properties, known as local
properties, which encompass several standard notions such as distance,
list-decoding, list-recovery, and perfect hashing. Our approach extends the
classical Alon-Edmonds-Luby (AEL) construction through a modified formalism of
local coordinate-wise linear (LCL) properties, introduced by Levi, Mosheiff,
and Shagrithaya (2025). The main theorem demonstrates that if random linear
codes satisfy the complement of an LCL property $\mathcal{P}$ with high
probability, then one can construct explicit codes satisfying the complement of
$\mathcal{P}$ as well, with an enlarged yet constant alphabet size. This gives
the first explicit constructions for list recovery, as well as special cases
(e.g., list recovery with erasures, zero-error list recovery, perfect hash
matrices), with parameters matching those of random linear codes. More broadly,
our constructions realize the full range of parameters associated with these
properties at the same level of optimality as in the random setting, thereby
offering a systematic pathway from probabilistic guarantees to explicit codes
that attain them. Furthermore, our derandomization of random linear codes also
admits efficient (list) decoding via recently developed expander-based
decoders.

</details>
