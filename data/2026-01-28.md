<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 9]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.IT](#cs.IT) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [FTA-NTN: Fairness and Throughput Assurance in Non-Terrestrial Networks](https://arxiv.org/abs/2601.19078)
*Sachin Ravikant Trankatwar,Heiko Straulino,Petar Djukic,Burak Kantarci*

Main category: cs.NI

TL;DR: FTA-NTN是一个多目标优化框架，联合最大化非地面网络的吞吐量和公平性，在500用户仿真中实现了9.88Gbps总吞吐量和0.42平均公平性。


<details>
  <summary>Details</summary>
Motivation: 现有非地面网络设计主要关注吞吐量最大化，缺乏对公平性的考虑。需要开发一个框架来联合优化吞吐量和公平性，确保资源公平分配，支持高效且公平的全球连接。

Method: 提出FTA-NTN框架，整合多层Walker Delta星座、加拿大陆地区域用户分布的参数化移动模型、用于波束成形和用户关联的自适应K-Means聚类，以及用于参数调优的贝叶斯优化。

Result: 仿真结果显示，在500用户场景下，FTA-NTN实现了超过9.88Gbps的总吞吐量和0.42的平均公平性，最优配置为LEO层9个轨道面（每面15颗卫星）和MEO层7个轨道面（每面3颗卫星）。

Conclusion: FTA-NTN证明在现实约束下可以联合优化吞吐量和公平性，超越了文献中仅关注吞吐量的设计，为下一代非地面网络部署提供了可扩展的方法论，支持高效且公平的全球连接。

Abstract: Designing optimal non-terrestrial network (NTN) constellations is essential for maximizing throughput and ensuring fair resource distribution. This paper presents FTA-NTN (Fairness and Throughput Assurance in Non-Terrestrial Networks), a multi-objective optimization framework that jointly maximizes throughput and fairness under realistic system constraints. The framework integrates multi-layer Walker Delta constellations, a parametric mobility model for user distributions across Canadian land regions, adaptive K-Means clustering for beamforming and user association, and Bayesian optimization for parameter tuning. Simulation results with 500 users show that FTA-NTN achieves over 9.88 Gbps of aggregate throughput with an average fairness of 0.42, corresponding to an optimal configuration of 9 planes with 15 satellites per plane in LEO and 7 planes with 3 satellites per plane in MEO. These values align with 3GPP NTN evaluation scenarios and representative system assumptions, confirming their relevance for realistic deployments. Overall, FTA-NTN demonstrates that throughput and fairness can be jointly optimized under practical constraints, advancing beyond throughput-centric designs in the literature and offering a scalable methodology for next-generation NTN deployments that supports efficient and equitable global connectivity.

</details>


### [2] [In-Network Collective Operations: Game Changer or Challenge for AI Workloads?](https://arxiv.org/abs/2601.19132)
*Torsten Hoefler,Mikhail Khalilov,Josiah Clark,Surendra Anubolu,Mohan Kalkunte,Karen Schramm,Eric Spada,Duncan Roweth,Keith Underwood,Adrian Caulfield,Abdul Kabbani,Amirreza Rastegari*

Main category: cs.NI

TL;DR: 本文总结了网内集合操作（INC）在AI工作负载加速中的机遇，介绍了边缘INC和核心INC两种类型，分析了性能优势和六大障碍，并预测了未来发展。


<details>
  <summary>Details</summary>
Motivation: 连接AI和网络两个社区，为非专家提供网内集合操作（INC）的全面介绍，促进这一重要领域的发展和应用。

Method: 将INC分为两种类型：边缘INC（在节点层面实现）和核心INC（在网络交换机内嵌入），分别分析其性能潜力和采用障碍。

Result: 识别了INC在AI工作负载加速中的显著性能优势，同时指出了六项关键障碍可能阻碍其采用，包括技术实现、兼容性、成本等方面的问题。

Conclusion: INC在加速AI集合操作方面具有巨大潜力，但需要克服技术和采用障碍。文章提供了未来发展的预测，为相关研究和应用提供指导。

Abstract: This paper summarizes the opportunities of in-network collective operations (INC) for accelerated collective operations in AI workloads. We provide sufficient detail to make this important field accessible to non-experts in AI or networking, fostering a connection between these communities. Consider two types of INC: Edge-INC, where the system is implemented at the node level, and Core-INC, where the system is embedded within network switches. We outline the potential performance benefits as well as six key obstacles in the context of both Edge-INC and Core-INC that may hinder their adoption. Finally, we present a set of predictions for the future development and application of INC.

</details>


### [3] [Intent2QoS: Language Model-Driven Automation of Traffic Shaping Configurations](https://arxiv.org/abs/2601.18974)
*Sudipta Acharya,Burak Kantarci*

Main category: cs.NI

TL;DR: 提出首个端到端自动化框架，将自然语言或声明式的高层流量整形意图转换为可部署的Linux流量控制配置规则


<details>
  <summary>Details</summary>
Motivation: 流量整形和QoS执行对网络带宽、延迟和公平性管理至关重要，但传统方法依赖需要手动设置和技术专长的底层流量控制配置，过程复杂且易出错

Method: 三步骤框架：1) 基于优先级调度和主动队列管理(AQM)的排队仿真构建语义模型；2) 语言模型使用语义模型和流量配置文件生成子意图和配置规则；3) 基于规则的批评器检查和调整规则以确保正确性和策略合规性

Result: 在100个意图上的实验结果显示，LLaMA3达到0.88语义相似度和0.87语义覆盖率，比其他模型高出30%以上。敏感性研究表明，AQM引导的提示方法将变异性降低到零样本基线的三分之一

Conclusion: 该框架成功实现了从高层业务意图到可部署流量控制配置的自动化转换，显著提高了配置准确性和一致性，为网络管理提供了更易用和可靠的解决方案

Abstract: Traffic shaping and Quality of Service (QoS) enforcement are critical for managing bandwidth, latency, and fairness in networks. These tasks often rely on low-level traffic control settings, which require manual setup and technical expertise. This paper presents an automated framework that converts high-level traffic shaping intents in natural or declarative language into valid and correct traffic control rules. To the best of our knowledge, we present the first end-to-end pipeline that ties intent translation in a queuing-theoretic semantic model and, with a rule-based critic, yields deployable Linux traffic control configuration sets. The framework has three steps: (1) a queuing simulation with priority scheduling and Active Queue Management (AQM) builds a semantic model; (2) a language model, using this semantic model and a traffic profile, generates sub-intents and configuration rules; and (3) a rule-based critic checks and adjusts the rules for correctness and policy compliance. We evaluate multiple language models by generating traffic control commands from business intents that comply with relevant standards for traffic control protocols. Experimental results on 100 intents show significant gains, with LLaMA3 reaching 0.88 semantic similarity and 0.87 semantic coverage, outperforming other models by over 30\. A thorough sensitivity study demonstrates that AQM-guided prompting reduces variability threefold compared to zero-shot baselines.

</details>


### [4] [Optimizing Network Topology Efficiency: A Resource-Centric Analysis of Non-Blocking Architectures](https://arxiv.org/abs/2601.19008)
*Jia Xu Wei,Wei Wei*

Main category: cs.NI

TL;DR: 论文提出基于硬件成本的网络效率定义，分析拓扑优化取决于链路接口成本、交换成本和网络集中度，指出高基数直接网络适合中小规模，大规模需要间接网络，冗余应通过并行网络而非拓扑路径多样性实现。


<details>
  <summary>Details</summary>
Motivation: 当前网络设计中"效率"常与延迟或吞吐量等性能指标混淆，缺乏对硬件成本的考虑。本文旨在建立资源中心的效率定义，分析不同网络拓扑在满足非阻塞吞吐约束下的硬件成本效率。

Method: 将网络成本建模为流量乘数（跳数）和路由器复杂度（基数）的函数，引入链路接口成本($α$)、交叉开关成本($β$)和网络集中度比等关键技术参数，通过数学模型分析不同拓扑结构的效率优化。

Result: 研究表明：1) 最优拓扑取决于$α$、$β$和网络集中度的技术比率；2) 高基数直接网络在中小规模下效率最优；3) 大规模网络需要间接网络（如胖树）来限制路由器复杂度；4) 冗余处理最有效的方式是通过并行网络实例而非拓扑路径多样性。

Conclusion: 网络效率应基于硬件成本而非单纯性能指标，拓扑选择需考虑规模和技术参数。中小规模适合直接网络，大规模需要间接网络，冗余应通过多平面星型网络等并行实例实现，而非依赖复杂拓扑的路径多样性。

Abstract: In modern network design, "efficiency" is often conflated with raw performance metrics like latency or aggregate throughput. This paper proposes a resource-centric definition of efficiency, isolating the hardware cost required to maintain a non-blocking throughput constraint. By modeling network cost as a function of the Traffic Multiplier (Hop Count) and Router Complexity (Radix), we demonstrate that the optimal topology is determined by the technological ratio between link interface costs ($α$), crossbar switching costs ($β$), and the network concentration ratio. We conclude that while high-radix direct networks optimize efficiency at small to medium scales, indirect networks (e.g., Fat Trees) are required to cap router complexity at massive scales. Furthermore, we posit that redundancy is most efficiently handled via parallel network instances (e.g., multi-plane Star networks) rather than intrinsic topological path diversity.

</details>


### [5] [Design and Evaluation of Next-Generation Cellular Networks through Digital and Physical Open and Programmable Platforms](https://arxiv.org/abs/2601.19027)
*Davide Villa*

Main category: cs.NI

TL;DR: 该论文开发了互补的实验平台Colosseum和X5G，用于Open RAN数字孪生和物理测试，提供端到端的方法论来桥接数字和物理实验。


<details>
  <summary>Details</summary>
Motivation: 5G/6G中RAN向开放、可编程、软件化架构演进，但面临互操作性设计、AI/ML算法训练数据获取、测试平台开发等挑战，需要实验平台来验证解决方案。

Method: 开发了两个互补实验平台：Colosseum（最大的Open RAN数字孪生）和X5G（开放可编程多供应商私有5G O-RAN测试平台），包括CaST自动场景创建、GPU加速PHY处理、实时RAN推理框架等方法。

Result: 验证了Colosseum数字孪生能准确复现真实环境，实现了GPU加速的实时RAN推理框架（亚毫秒控制循环），开发了涵盖频谱共享、干扰检测、网络切片、安全、CSI感知等智能RAN应用。

Conclusion: 该论文提供了桥接数字和物理实验的端到端方法论，为下一代蜂窝网络的Open RAN开发、测试和验证提供了完整的实验框架。

Abstract: The evolution of the Radio Access Network (RAN) in 5G and 6G technologies marks a shift toward open, programmable, and softwarized architectures, driven by the Open RAN paradigm. This approach emphasizes open interfaces for telemetry sharing, intelligent data-driven control loops for network optimization, and virtualization and disaggregation of multi-vendor RAN components. While promising, this transition introduces significant challenges, including the need to design interoperable solutions, acquire datasets to train and test AI/ML algorithms for inference and control, and develop testbeds to benchmark these solutions. Experimental wireless platforms and private 5G deployments play a key role, providing architectures comparable to real-world systems and enabling prototyping and testing in realistic environments. This dissertation focuses on the development and evaluation of complementary experimental platforms: Colosseum, the world's largest Open RAN digital twin, and X5G, an open, programmable, multi-vendor private 5G O-RAN testbed with GPU acceleration. The main contributions include: (i) CaST, enabling automated creation and validation of digital twin wireless scenarios through 3D modeling, ray-tracing, and channel sounding; (ii) validation of Colosseum digital twins at scale, demonstrating that emulated environments closely reproduce real-world setups; (iii) X5G, integrating NVIDIA Aerial GPU-accelerated PHY processing with OpenAirInterface higher layers; (iv) a GPU-accelerated dApp framework for real-time RAN inference, enabling sub-millisecond control loops for AI-native applications including ISAC; and (v) intelligent RAN applications spanning spectrum sharing, interference detection, network slicing, security, and CSI-based sensing. Overall, this dissertation provides an end-to-end methodology bridging digital and physical experimentation for next-generation cellular networks.

</details>


### [6] [Enabling SLO-Aware 5G Multi-Access Edge Computing with SMEC](https://arxiv.org/abs/2601.19162)
*Xiao Zhang,Daehyeok Kim*

Main category: cs.NI

TL;DR: SMEC是一个实用的SLO感知资源管理框架，通过解耦RAN和边缘服务器的操作实现截止时间感知调度，在5G MEC测试中显著提升SLO满足率并降低尾延迟。


<details>
  <summary>Details</summary>
Motivation: 商用MEC部署中经常出现SLO违规问题，主要原因是RAN和边缘服务器的资源争用，以及现有SLO感知方法需要RAN-边缘协调，导致部署困难、性能不佳。

Method: 提出SMEC框架，利用标准5G协议和应用行为提供的信息，实现完全解耦的RAN和边缘服务器操作，支持截止时间感知调度，无需大规模基础设施或应用变更。

Result: 在5G MEC测试平台上，SMEC实现了90-96%的SLO满足率（现有方法低于6%），尾延迟降低高达122倍。

Conclusion: SMEC通过利用现有5G协议和应用行为信息，提供了一种实用、高效的SLO感知资源管理解决方案，显著改善了MEC系统的性能表现。

Abstract: Multi-access edge computing (MEC) promises to enable latency-critical applications by bringing computational power closer to mobile devices, but our measurements on commercial MEC deployments reveal frequent SLO violations due to high tail latencies. We identify resource contention at the RAN and the edge server as the root cause, compounded by SLO-unaware schedulers. Existing SLO-aware approaches require RAN--edge coordination, making them impractical for deployment and prone to poor performance due to coordination delays, limited heterogeneous application support, and ignoring edge resource contention. This paper introduces SMEC, a practical, SLO-aware resource management framework that facilitates deadline-aware scheduling through fully decoupled operations at the RAN and edge servers. Our key insight is that standard 5G protocols and application behaviors naturally provide information exploitable for SLO-aware management without extensive infrastructure or application changes. Evaluation on our 5G MEC testbed shows that SMEC achieves 90-96% SLO satisfaction versus under 6% for existing approaches, while reducing tail latency by up to 122$\times$. We have open-sourced SMEC at https://github.com/smec-project.

</details>


### [7] [Bridging Visual and Wireless Sensing: A Unified Radiation Field for 3D Radio Map Construction](https://arxiv.org/abs/2601.19216)
*Chaozheng Wen,Jingwen Tong,Zehong Lin,Chenghong Bian,Jun Zhang*

Main category: cs.NI

TL;DR: URF-GS：基于3D高斯泼溅的统一光-无线辐射场表示框架，用于构建准确且可泛化的3D无线电地图，相比NeRF方法在频谱预测精度上提升24.7%，采样效率提高10倍。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络应用（如沉浸式3D通信、低空网络、集成感知与通信）需要高保真环境智能。现有方法将光学和无线知识视为不同模态，未能利用光和电磁传播的基本物理原理，限制了3D无线电地图的准确构建。

Method: 提出URF-GS框架，基于3D高斯泼溅（3D-GS）和逆渲染技术，融合视觉和无线传感观测，恢复场景几何和材料属性，准确预测任意发射-接收配置下的无线电信号行为。

Result: 实验结果显示，URF-GS在空间频谱预测精度上相比基于神经辐射场（NeRF）的方法提升达24.7%，在3D无线电地图构建的采样效率上提高10倍。

Conclusion: 该工作通过整体辐射场重建，整合感知、交互和通信，为下一代无线网络奠定了基础，实现了更准确和高效的3D无线电地图构建。

Abstract: The emerging applications of next-generation wireless networks (e.g., immersive 3D communication, low-altitude networks, and integrated sensing and communication) necessitate high-fidelity environmental intelligence. 3D radio maps have emerged as a critical tool for this purpose, enabling spectrum-aware planning and environment-aware sensing by bridging the gap between physical environments and electromagnetic signal propagation. However, constructing accurate 3D radio maps requires fine-grained 3D geometric information and a profound understanding of electromagnetic wave propagation. Existing approaches typically treat optical and wireless knowledge as distinct modalities, failing to exploit the fundamental physical principles governing both light and electromagnetic propagation. To bridge this gap, we propose URF-GS, a unified radio-optical radiation field representation framework for accurate and generalizable 3D radio map construction based on 3D Gaussian splatting (3D-GS) and inverse rendering. By fusing visual and wireless sensing observations, URF-GS recovers scene geometry and material properties while accurately predicting radio signal behavior at arbitrary transmitter-receiver (Tx-Rx) configurations. Experimental results demonstrate that URF-GS achieves up to a 24.7% improvement in spatial spectrum prediction accuracy and a 10x increase in sample efficiency for 3D radio map construction compared with neural radiance field (NeRF)-based methods. This work establishes a foundation for next-generation wireless networks by integrating perception, interaction, and communication through holistic radiation field reconstruction.

</details>


### [8] [NET4EXA: Pioneering the Future of Interconnects for Supercomputing and AI](https://arxiv.org/abs/2601.19413)
*Michele Martinelli,Roberto Ammendola,Andrea Biagioni,Carlotta Chiarini,Ottorino Frezza,Francesca Lo Cicero,Alessandro Lonardo,Pier Stanislao Paolucci,Elena Pastorelli,Pierpaolo Perticaroli,Luca Pontisso,Cristian Rossi,Francesco Simula,Piero Vicini,David Colin,Grégoire Pichon,Alexandre Louvet,John Gliksberg,Claire Chen,Matteo Turisini,Andrea Monterubbiano,Jean-Philippe Nominé,Denis Dutoit,Hugo Taboada,Lilia Zaourar,Mohamed Benazouz,Angelos Bilas,Fabien Chaix,Manolis Katevenis,Nikolaos Chrysos,Evangelos Mageiropoulos,Christos Kozanitis,Thomas Moen,Steffen Persvold,Einar Rustad,Sandro Fiore,Fabrizio Granelli,Simone Pezzuto,Raffaello Potestio,Luca Tubiana,Philippe Velha,Flavio Vella,Daniele De Sensi,Salvatore Pontarelli*

Main category: cs.NI

TL;DR: NET4EXA项目旨在开发下一代高性能互连技术BXIv3，用于HPC和AI系统，计划2025年后部署于百亿亿次及后百亿亿次计算系统


<details>
  <summary>Details</summary>
Motivation: 应对大规模基础设施（如训练大语言模型）日益增长的需求，为即将到来的百亿亿次及后百亿亿次计算系统提供高性能互连解决方案

Method: 采用混合开发和协同设计方法，结合商用交换技术、定制IP和基于FPGA的NIC；基于欧洲BXI技术构建BXIv3完整硬件软件互连方案

Result: 将交付TRL 8级别的全功能试点系统，为BXIv4奠定基础；通过广泛的基准测试、科学可扩展应用和AI工作负载评估性能

Conclusion: NET4EXA项目将开发出适用于未来百亿亿次计算系统的高性能互连技术BXIv3，为欧洲HPC和AI基础设施提供关键互连解决方案

Abstract: NET4EXA aims to develop a next-generation high-performance interconnect for HPC and AI systems, addressing the increasing demands of large-scale infrastructures, such as those required for training Large Language Models. Building upon the proven BXI (Bull eXascale Interconnect) European technology used in TOP15 supercomputers, NET4EXA will deliver the new BXI release, BXIv3, a complete hardware and software interconnect solution, including switch and network interface components. The project will integrate a fully functional pilot system at TRL 8, ready for deployment into upcoming exascale and post-exascale systems from 2025 onward. Leveraging prior research from European initiatives like RED-SEA, the previous achievements of consortium partners and over 20 years of expertise from BULL, NET4EXA also lays the groundwork for the future generation of BXI, BXIv4, providing analysis and preliminary design. The project will use a hybrid development and co-design approach, combining commercial switch technology with custom IP and FPGA-based NICs. Performances of NET4EXA BXIv3 interconnect will be evaluated using a broad portfolio of benchmarks, scientific scalable applications, and AI workloads.

</details>


### [9] [Quantum Takes Flight: Two-Stage Resilient Topology Optimization for UAV Networks](https://arxiv.org/abs/2601.19724)
*Huixiang Zhang,Mahzabeen Emu,Octavia A. Dobre*

Main category: cs.NI

TL;DR: 提出一个两阶段量子辅助框架，用于动态无人机网络中的高效弹性拓扑控制，利用量子并行性生成高质量且结构多样的候选拓扑，显著提升动态环境下的性能保持能力。


<details>
  <summary>Details</summary>
Motivation: 下一代无人机通信网络需要在快速拓扑变化、链路质量波动和时间关键数据交换下保持可靠连接。现有拓扑控制方法依赖全局优化产生单一最优拓扑或涉及高计算复杂度，限制了在动态环境中的适应性。

Method: 提出两阶段量子辅助框架：离线阶段将问题建模为QUBO模型，利用量子退火并行采样多个高质量且结构不同的拓扑；在线阶段采用轻量级经典选择机制，根据实时链路稳定性和信道条件快速选择最合适拓扑。

Result: 相比单一静态最优拓扑，该框架在30秒动态窗口中将性能保持提升6.6%；相比经典方法，量子退火使目标值额外降低5.15%，解决方案多样性增加28.3%。

Conclusion: 量子退火能够为下一代无人机通信网络实现快速且鲁棒的拓扑控制，通过量子并行性生成多样化的候选拓扑，结合轻量级在线选择机制，在动态环境中显著提升适应性和性能保持能力。

Abstract: Next-generation Unmanned Aerial Vehicle (UAV) communication networks must maintain reliable connectivity under rapid topology changes, fluctuating link quality, and time-critical data exchange. Existing topology control methods rely on global optimization to produce a single optimal topology or involve high computational complexity, which limits adaptability in dynamic environments. This paper presents a two-stage quantum-assisted framework for efficient and resilient topology control in dynamic UAV networks by exploiting quantum parallelism to generate a set of high-quality and structurally diverse candidate topologies. In the offline stage, we formulate the problem as a Quadratic Unconstrained Binary Optimization (QUBO) model and leverage quantum annealing (QA) to parallelly sample multiple high-quality and structurally distinct topologies, providing a rich solution space for adaptive decision-making. In the online stage, a lightweight classical selection mechanism rapidly identifies the most suitable topology based on real-time link stability and channel conditions, substantially reducing the computation delay. The simulation results show that, compared to a single static optimal topology, the proposed framework improves performance retention by 6.6% in a 30-second dynamic window. Moreover, relative to the classic method, QA achieves an additional 5.15% reduction in objective value and a 28.3% increase in solution diversity. These findings demonstrate the potential of QA to enable fast and robust topology control for next-generation UAV communication networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Agentic Business Process Management Systems](https://arxiv.org/abs/2601.18833)
*Marlon Dumas,Fredrik Milani,David Chapela-Campa*

Main category: cs.AI

TL;DR: 本文提出Agentic Business Process Management Systems (A-BPMS)架构愿景，将生成式AI和代理AI与流程挖掘结合，实现从自动化到自主性、从设计驱动到数据驱动的业务流程管理范式转变。


<details>
  <summary>Details</summary>
Motivation: 业务流程管理(BPM)经历了多次自动化技术浪潮，但当前生成式AI和代理AI的出现带来了新的机遇。传统BPM侧重于任务自动化和流程编排，而新AI技术能够实现从自动化到自主性的转变，从设计驱动管理转向数据驱动管理，这需要重新思考BPM系统的架构。

Method: 基于流程挖掘技术建立基础，使代理能够感知流程状态、推理改进机会并采取行动优化性能。提出Agentic Business Process Management Systems (A-BPMS)架构愿景，整合自主性、推理和学习能力，支持从人工驱动到完全自主的连续流程谱系。

Result: 提出了一个新型平台架构，能够集成自主性、推理和学习能力到流程管理和执行中。这种系统支持连续流程谱系，重新定义了流程自动化和治理的边界。

Conclusion: 生成式AI和代理AI为BPM带来了新的变革浪潮，通过流程挖掘技术与AI代理的结合，可以实现从自动化到自主性的根本转变。A-BPMS系统将重新定义业务流程管理的边界和治理方式。

Abstract: Since the early 90s, the evolution of the Business Process Management (BPM) discipline has been punctuated by successive waves of automation technologies. Some of these technologies enable the automation of individual tasks, while others focus on orchestrating the execution of end-to-end processes. The rise of Generative and Agentic Artificial Intelligence (AI) is opening the way for another such wave. However, this wave is poised to be different because it shifts the focus from automation to autonomy and from design-driven management of business processes to data-driven management, leveraging process mining techniques. This position paper, based on a keynote talk at the 2025 Workshop on AI for BPM, outlines how process mining has laid the foundations on top of which agents can sense process states, reason about improvement opportunities, and act to maintain and optimize performance. The paper proposes an architectural vision for Agentic Business Process Management Systems (A-BPMS): a new class of platforms that integrate autonomy, reasoning, and learning into process management and execution. The paper contends that such systems must support a continuum of processes, spanning from human-driven to fully autonomous, thus redefining the boundaries of process automation and governance.

</details>


### [11] [LLM Driven Design of Continuous Optimization Problems with Controllable High-level Properties](https://arxiv.org/abs/2601.18846)
*Urban Skvorc,Niki van Stein,Moritz Seiler,Britta Grimme,Thomas Bäck,Heike Trautmann*

Main category: cs.AI

TL;DR: 利用LLaMEA框架引导LLM从自然语言描述生成具有特定景观特征的优化问题，通过ELA属性预测器评分，引入ELA空间适应度共享机制增加多样性，生成的问题扩展了BBOB实例空间


<details>
  <summary>Details</summary>
Motivation: 现有基准测试套件（如BBOB）的结构多样性有限，阻碍了连续黑盒优化的基准测试。需要能够设计具有明确高层景观特征的优化问题的方法。

Method: 使用LLaMEA框架，在进化循环中引导LLM从目标属性的自然语言描述生成问题代码。通过基于ELA的属性预测器对候选问题进行评分，引入ELA空间适应度共享机制增加种群多样性并避免冗余景观。

Result: 生成的许多函数确实表现出预期的结构特征。t-SNE嵌入显示它们扩展了BBOB实例空间而不是形成不相关的聚类。通过盆地吸引分析、统计测试和视觉检查验证了结果。

Conclusion: 生成的库为景观分析和下游任务（如自动算法选择）提供了广泛、可解释且可复现的基准问题集。

Abstract: Benchmarking in continuous black-box optimisation is hindered by the limited structural diversity of existing test suites such as BBOB. We explore whether large language models embedded in an evolutionary loop can be used to design optimisation problems with clearly defined high-level landscape characteristics. Using the LLaMEA framework, we guide an LLM to generate problem code from natural-language descriptions of target properties, including multimodality, separability, basin-size homogeneity, search-space homogeneity and globallocal optima contrast. Inside the loop we score candidates through ELA-based property predictors. We introduce an ELA-space fitness-sharing mechanism that increases population diversity and steers the generator away from redundant landscapes. A complementary basin-of-attraction analysis, statistical testing and visual inspection, verifies that many of the generated functions indeed exhibit the intended structural traits. In addition, a t-SNE embedding shows that they expand the BBOB instance space rather than forming an unrelated cluster. The resulting library provides a broad, interpretable, and reproducible set of benchmark problems for landscape analysis and downstream tasks such as automated algorithm selection.

</details>


### [12] [Explainable Uncertainty Quantification for Wastewater Treatment Energy Prediction via Interval Type-2 Neuro-Fuzzy System](https://arxiv.org/abs/2601.18897)
*Qusai Khaled,Bahjat Mallak,Uzay Kaymak,Laura Genga*

Main category: cs.AI

TL;DR: 开发IT2-ANFIS模型用于污水处理厂能耗预测，提供可解释的不确定性量化，通过模糊规则结构分解三个层次的不确定性。


<details>
  <summary>Details</summary>
Motivation: 污水处理厂消耗全球1-3%电力，需要准确的能耗预测进行运营优化。现有机器学习模型缺乏可解释的不确定性量化，这在安全关键基础设施的风险感知决策中至关重要。

Method: 开发区间二型自适应神经模糊推理系统（IT2-ANFIS），通过模糊规则结构生成可解释的预测区间。将不确定性分解为三个层次：特征级（识别引入模糊性的变量）、规则级（分析局部模型置信度）和实例级（量化整体预测不确定性）。

Result: 在墨尔本水务东部处理厂数据集上验证，IT2-ANFIS达到与一阶ANFIS相当的预测性能，同时显著减少训练运行的方差，并提供可解释的不确定性估计，将预测置信度直接与运营条件和输入变量关联。

Conclusion: IT2-ANFIS框架为污水处理厂能耗预测提供了可解释的不确定性量化方法，支持风险感知决策，有助于提高运营优化和可持续性。

Abstract: Wastewater treatment plants consume 1-3% of global electricity, making accurate energy forecasting critical for operational optimization and sustainability. While machine learning models provide point predictions, they lack explainable uncertainty quantification essential for risk-aware decision-making in safety-critical infrastructure. This study develops an Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS) that generates interpretable prediction intervals through fuzzy rule structures. Unlike black-box probabilistic methods, the proposed framework decomposes uncertainty across three levels: feature-level, footprint of uncertainty identify which variables introduce ambiguity, rule-level analysis reveals confidence in local models, and instance-level intervals quantify overall prediction uncertainty. Validated on Melbourne Water's Eastern Treatment Plant dataset, IT2-ANFIS achieves comparable predictive performance to first order ANFIS with substantially reduced variance across training runs, while providing explainable uncertainty estimates that link prediction confidence directly to operational conditions and input variables.

</details>


### [13] [RIFT: Reordered Instruction Following Testbed To Evaluate Instruction Following in Singular Multistep Prompt Structures](https://arxiv.org/abs/2601.18924)
*Andrew Jaffe,Noah Reicin,Jinho D. Choi*

Main category: cs.AI

TL;DR: RIFT测试平台发现LLMs在非顺序指令结构下性能大幅下降，揭示当前架构对位置连续性的强依赖，而非真正的推理能力


<details>
  <summary>Details</summary>
Motivation: 现有基准测试将任务复杂性与结构顺序混为一谈，难以分离提示拓扑结构对性能的影响，需要专门工具评估LLMs在不同指令结构下的表现

Method: 引入RIFT测试平台，使用重新表述的Jeopardy!问答对，测试LLMs在线性提示（顺序）和跳跃提示（非顺序但内容相同）两种结构下的表现

Result: 在10,000次评估中，六个最先进的开源LLMs在跳跃条件下准确率下降高达72%，约50%的失败源于指令顺序违反和语义漂移

Conclusion: 当前架构将指令跟随内化为顺序模式而非推理技能，结构敏感性是基本限制，对需要非顺序控制流的应用有直接影响

Abstract: Large Language Models (LLMs) are increasingly relied upon for complex workflows, yet their ability to maintain flow of instructions remains underexplored. Existing benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on performance. We introduce RIFT, Reordered Instruction Following Testbed, to assess instruction following by disentangling structure from content. Using rephrased Jeopardy! question-answer pairs, we test LLMs across two prompt structures: linear prompts, which progress sequentially, and jumping prompts, which preserve identical content but require non-sequential traversal. Across 10,000 evaluations spanning six state-of-the-art open-source LLMs, accuracy dropped by up to 72% under jumping conditions (compared to baseline), revealing a strong dependence on positional continuity. Error analysis shows that approximately 50% of failures stem from instruction-order violations and semantic drift, indicating that current architectures internalize instruction following as a sequential pattern rather than a reasoning skill. These results reveal structural sensitivity as a fundamental limitation in current architectures, with direct implications for applications requiring non-sequential control flow such as workflow automation and multi-agent systems.

</details>


### [14] [Neural Theorem Proving for Verification Conditions: A Real-World Benchmark](https://arxiv.org/abs/2601.18944)
*Qiyuan Xu,Xiaokun Luan,Renxi Wang,Joshua Ong Jun Leang,Peixin Wang,Haonan Li,Wenda Li,Conrad Watt*

Main category: cs.AI

TL;DR: NTP4VC是首个针对程序验证中验证条件自动证明的基准测试，从真实项目生成多语言测试用例，评估LLMs在VC证明中的表现，显示仍有显著挑战。


<details>
  <summary>Details</summary>
Motivation: 程序验证中验证条件的自动证明是主要瓶颈，现有自动定理证明器无法处理困难VC，需要大量手动证明。虽然神经定理证明在数学竞赛中成功，但在程序验证中的应用仍待探索。

Method: 从Linux和Contiki-OS等真实项目收集数据，使用Why3和Frama-C工业流水线生成语义等价的测试用例，涵盖Isabelle、Lean和Rocq三种形式语言。评估通用LLMs和针对定理证明微调的LLMs。

Result: LLMs在VC证明中显示出潜力，但程序验证仍面临重大挑战，存在较大差距和未来研究机会。

Conclusion: NTP4VC填补了程序验证中VC自动证明基准的空白，为未来研究提供了重要基础，表明LLMs有潜力但需要进一步改进以应对程序验证的独特挑战。

Abstract: Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research.

</details>


### [15] [More at Stake: How Payoff and Language Shape LLM Agent Strategies in Cooperation Dilemmas](https://arxiv.org/abs/2601.19082)
*Trung-Kiet Huynh,Dao-Sy Duy-Minh,Thanh-Bang Cao,Phong-Hao Le,Hong-Dan Nguyen,Nguyen Lam Phu Quy,Minh-Luan Nguyen-Vo,Hong-Phat Pham,Pham Phu Hoa,Thien-Kim Than,Chi-Nguyen Tran,Huy Tran,Gia-Thoai Tran-Le,Alessio Buscemi,Le Hong Trang,The Anh Han*

Main category: cs.AI

TL;DR: 研究LLM在重复社会困境中的战略行为，分析收益大小和语言背景如何影响其策略选择，发现模型和语言依赖的行为模式


<details>
  <summary>Details</summary>
Motivation: 随着LLM在交互和多智能体环境中作为自主智能体使用，理解其战略行为对安全、协调以及AI驱动的社会和经济系统至关重要

Method: 使用收益缩放的囚徒困境来隔离对激励强度的敏感性，通过监督分类器在典型重复博弈策略上训练，并将其应用于LLM决策分析

Result: 观察到一致的行为模式，包括激励敏感的conditional策略和跨语言差异，语言框架有时匹配或超过架构效应，揭示了系统和模型/语言依赖的行为意图

Conclusion: 为审计LLM作为战略智能体提供了统一框架，强调了合作偏见对AI治理和多智能体系统设计的直接意义

Abstract: As LLMs increasingly act as autonomous agents in interactive and multi-agent settings, understanding their strategic behavior is critical for safety, coordination, and AI-driven social and economic systems. We investigate how payoff magnitude and linguistic context shape LLM strategies in repeated social dilemmas, using a payoff-scaled Prisoner's Dilemma to isolate sensitivity to incentive strength. Across models and languages, we observe consistent behavioral patterns, including incentive-sensitive conditional strategies and cross-linguistic divergence. To interpret these dynamics, we train supervised classifiers on canonical repeated-game strategies and apply them to LLM decisions, revealing systematic, model- and language-dependent behavioral intentions, with linguistic framing sometimes matching or exceeding architectural effects. Our results provide a unified framework for auditing LLMs as strategic agents and highlight cooperation biases with direct implications for AI governance and multi-agent system design.

</details>


### [16] [Uncertainty-Aware 3D Emotional Talking Face Synthesis with Emotion Prior Distillation](https://arxiv.org/abs/2601.19112)
*Nanhan Shen,Zhilei Liu*

Main category: cs.AI

TL;DR: UA-3DTalk提出了一种不确定性感知的3D情感说话人脸合成方法，通过情感先验蒸馏和自适应多视角融合解决现有方法的音频-视觉情感对齐不佳和统一融合策略问题


<details>
  <summary>Details</summary>
Motivation: 现有3D情感说话人脸合成方法存在两个关键挑战：1) 音频-视觉情感对齐不佳，表现为音频情感提取困难和对情感微表情控制不足；2) 采用一刀切的多视角融合策略，忽略了不确定性和特征质量差异，影响了渲染质量

Method: 提出UA-3DTalk框架，包含三个核心模块：1) 先验提取模块将音频解耦为内容同步特征和个性化特征；2) 情感蒸馏模块采用多模态注意力加权融合机制和4D高斯编码，实现细粒度音频情感提取和情感微表情精确控制；3) 基于不确定性的变形模块使用不确定性块估计视角特定的不确定性，实现自适应多视角融合，并采用多头解码器优化高斯基元

Result: 在常规和情感数据集上的实验表明，UA-3DTalk在情感对齐方面比DEGSTalk和EDTalk等SOTA方法提升5.2%的E-FID，在唇部同步方面提升3.1%的SyncC，在渲染质量方面提升0.015的LPIPS

Conclusion: UA-3DTalk通过不确定性感知的自适应多视角融合和情感先验蒸馏，显著提升了3D情感说话人脸合成的音频-视觉情感对齐和渲染质量，为解决现有方法的局限性提供了有效方案

Abstract: Emotional Talking Face synthesis is pivotal in multimedia and signal processing, yet existing 3D methods suffer from two critical challenges: poor audio-vision emotion alignment, manifested as difficult audio emotion extraction and inadequate control over emotional micro-expressions; and a one-size-fits-all multi-view fusion strategy that overlooks uncertainty and feature quality differences, undermining rendering quality. We propose UA-3DTalk, Uncertainty-Aware 3D Emotional Talking Face Synthesis with emotion prior distillation, which has three core modules: the Prior Extraction module disentangles audio into content-synchronized features for alignment and person-specific complementary features for individualization; the Emotion Distillation module introduces a multi-modal attention-weighted fusion mechanism and 4D Gaussian encoding with multi-resolution code-books, enabling fine-grained audio emotion extraction and precise control of emotional micro-expressions; the Uncertainty-based Deformation deploys uncertainty blocks to estimate view-specific aleatoric (input noise) and epistemic (model parameters) uncertainty, realizing adaptive multi-view fusion and incorporating a multi-head decoder for Gaussian primitive optimization to mitigate the limitations of uniform-weight fusion. Extensive experiments on regular and emotional datasets show UA-3DTalk outperforms state-of-the-art methods like DEGSTalk and EDTalk by 5.2% in E-FID for emotion alignment, 3.1% in SyncC for lip synchronization, and 0.015 in LPIPS for rendering quality. Project page: https://mrask999.github.io/UA-3DTalk

</details>


### [17] [Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach](https://arxiv.org/abs/2601.19122)
*Weiran Guo,Bing Bo,Shaoxiang Wu,Jingsheng Yang*

Main category: cs.AI

TL;DR: 提出基于强化学习的对抗性数据增强方法，通过训练查询模型生成对抗性查询来挑战函数调用模型，提升LLM函数调用能力的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有提升LLM函数调用能力的方法依赖于人工标注或模型自动生成的数据进行微调，但这些方法缺乏针对性设计，受限于固定模式和数据分布，限制了函数调用LLM的泛化性和鲁棒性提升。

Method: 提出对抗性数据增强方法，使用强化学习训练查询模型来生成专门挑战函数调用模型的对抗性查询。采用零和博弈框架，让查询模型和函数调用模型进行迭代交替训练。

Result: 该方法能够系统性地识别和针对函数调用LLM的弱点，推动开发更鲁棒的函数调用模型，为识别和纠正LLM与外部工具交互能力的弱点提供系统性方法。

Conclusion: 基于强化学习的对抗性数据增强方法能有效提升LLM函数调用能力的泛化性和鲁棒性，通过系统性识别和针对模型弱点，为LLM与外部工具交互能力的改进提供了新途径。

Abstract: Function call capabilities have become crucial for Large Language Models (LLMs), enabling them to interact more effectively with external tools and APIs. Existing methods for improving the function call capabilities of LLMs rely on data obtained either through manual annotation or automated generation by models, and use this data to finetune the LLMs. However, these methods often lack targeted design and are constrained by fixed patterns and data distributions, which limits their effectiveness in enhancing the generalization and robustness of function call LLMs. To address this limitation, we propose a novel adversarial data augmentation method that employs reinforcement learning to systematically identify and target the weaknesses of function call LLMs. Our training framework introduces a query model trained with reinforcement learning (RL) to generate adversarial queries that are specifically designed to challenge function call (FC) models. This approach adopts a zero sum game formulation, where the query model and the FC model engage in iterative alternating training. Overall, our method advances the development of more robust FC models and provides a systematic way to identify and correct weaknesses in the ability of LLMs to interact with external tools.

</details>


### [18] [Length-Adaptive Interest Network for Balancing Long and Short Sequence Modeling in CTR Prediction](https://arxiv.org/abs/2601.19142)
*Zhicheng Zhang,Zhaocheng Du,Jieming Zhu,Jiwei Tang,Fengyuan Lu,Wang Jiaheng,Song-Li Wu,Qianhui Zhu,Jingyu Li,Hai-Tao Zheng,Zhenhua Dong*

Main category: cs.AI

TL;DR: LAIN是一个长度自适应兴趣网络，通过将序列长度作为条件信号来平衡长短序列建模，解决推荐系统中序列长度异质性带来的注意力极化问题。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统中用户行为序列长度存在显著异质性，从稀疏的短期交互到丰富的长期历史。现有CTR模型增加最大输入序列长度反而会损害短序列用户的性能，这是由于注意力极化和训练数据长度不平衡造成的。

Method: 提出LAIN框架，包含三个轻量级组件：1) 谱长度编码器将长度映射为连续表示；2) 长度条件提示将全局上下文线索注入长短行为分支；3) 长度调制注意力根据序列长度自适应调整注意力锐度。

Result: 在三个真实世界基准测试和五个强CTR骨干网络上，LAIN持续提升整体性能，最高实现1.15% AUC增益和2.25%对数损失减少。显著提高短序列用户准确性而不牺牲长序列效果。

Conclusion: LAIN提供了一个通用、高效且可部署的解决方案，用于缓解顺序推荐中由长度引起的偏差，平衡长短序列建模。

Abstract: User behavior sequences in modern recommendation systems exhibit significant length heterogeneity, ranging from sparse short-term interactions to rich long-term histories. While longer sequences provide more context, we observe that increasing the maximum input sequence length in existing CTR models paradoxically degrades performance for short-sequence users due to attention polarization and length imbalance in training data. To address this, we propose LAIN(Length-Adaptive Interest Network), a plug-and-play framework that explicitly incorporates sequence length as a conditioning signal to balance long- and short-sequence modeling. LAIN consists of three lightweight components: a Spectral Length Encoder that maps length into continuous representations, Length-Conditioned Prompting that injects global contextual cues into both long- and short-term behavior branches, and Length-Modulated Attention that adaptively adjusts attention sharpness based on sequence length. Extensive experiments on three real-world benchmarks across five strong CTR backbones show that LAIN consistently improves overall performance, achieving up to 1.15% AUC gain and 2.25% log loss reduction. Notably, our method significantly improves accuracy for short-sequence users without sacrificing longsequence effectiveness. Our work offers a general, efficient, and deployable solution to mitigate length-induced bias in sequential recommendation.

</details>


### [19] [TS-Debate: Multimodal Collaborative Debate for Zero-Shot Time Series Reasoning](https://arxiv.org/abs/2601.19151)
*Patara Trirat,Jin Myung Kwak,Jay Heo,Heejun Lee,Sung Ju Hwang*

Main category: cs.AI

TL;DR: TS-Debate：一种用于零样本时间序列推理的模态专业化多智能体辩论框架，通过专用专家智能体处理文本、视觉和数值信号，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列分析中面临数值保真度、模态干扰和跨模态集成等挑战，需要更稳健的推理框架

Method: 采用模态专业化多智能体辩论框架，分配专用专家智能体处理文本上下文、视觉模式和数值信号，通过结构化辩论协议协调交互，使用验证-冲突-校准机制评估智能体主张

Result: 在三个公共基准测试的20个任务中，TS-Debate相比强基线（包括标准多模态辩论）实现了持续且显著的性能提升

Conclusion: TS-Debate框架能够保持模态保真度、暴露冲突证据并减轻数值幻觉，无需任务特定微调即可提升时间序列推理能力

Abstract: Recent progress at the intersection of large language models (LLMs) and time series (TS) analysis has revealed both promise and fragility. While LLMs can reason over temporal structure given carefully engineered context, they often struggle with numeric fidelity, modality interference, and principled cross-modal integration. We present TS-Debate, a modality-specialized, collaborative multi-agent debate framework for zero-shot time series reasoning. TS-Debate assigns dedicated expert agents to textual context, visual patterns, and numerical signals, preceded by explicit domain knowledge elicitation, and coordinates their interaction via a structured debate protocol. Reviewer agents evaluate agent claims using a verification-conflict-calibration mechanism, supported by lightweight code execution and numerical lookup for programmatic verification. This architecture preserves modality fidelity, exposes conflicting evidence, and mitigates numeric hallucinations without task-specific fine-tuning. Across 20 tasks spanning three public benchmarks, TS-Debate achieves consistent and significant performance improvements over strong baselines, including standard multimodal debate in which all agents observe all inputs.

</details>


### [20] [LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge](https://arxiv.org/abs/2601.19155)
*Qiujun Li,Zijin Xiao,Xulin Wang,Zhidan Ma,Cheng Yang,Haifeng Li*

Main category: cs.AI

TL;DR: LocationAgent是一个用于图像地理定位的分层智能体，采用RER架构（推理器-执行器-记录器）将地理证据验证外包给外部工具，在零样本设置下性能提升30%以上


<details>
  <summary>Details</summary>
Motivation: 现有方法通过监督训练或基于轨迹的强化微调将位置知识和推理模式内化为静态记忆，容易在开放世界或需要动态知识的场景中出现事实幻觉和泛化瓶颈

Method: 提出分层定位智能体LocationAgent，核心思想是在模型内保留分层推理逻辑，同时将地理证据验证外包给外部工具。采用RER架构实现分层推理，通过角色分离和上下文压缩防止多步推理中的漂移问题。构建线索探索工具套件提供多样化证据支持位置推理

Result: 在零样本设置下，LocationAgent显著优于现有方法，性能提升至少30%。同时构建了CCL-Bench（中国城市定位基准），涵盖不同场景粒度和难度级别

Conclusion: 通过将推理逻辑保留在模型内而将证据验证外包给外部工具，LocationAgent有效解决了现有方法的事实幻觉和泛化瓶颈问题，在图像地理定位任务上取得了显著性能提升

Abstract: Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.

</details>


### [21] [Multi-Agent Procedural Graph Extraction with Structural and Logical Refinement](https://arxiv.org/abs/2601.19170)
*Wangyang Ying,Yanchi Liu,Xujiang Zhao,Wei Cheng,Zhengzhang Chen,Wenchao Yu,Yanjie Fu,Haifeng Chen*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，将自然语言中的工作流程提取为程序图，通过结构化和逻辑反馈迭代优化


<details>
  <summary>Details</summary>
Motivation: 从自然语言自动提取工作流程作为程序图是一个有前景但尚未充分探索的领域，需要同时保证结构有效性和逻辑一致性。虽然大型语言模型在程序图提取方面显示出潜力，但常常产生结构不良或逻辑误解的结果。

Method: 提出一个多智能体框架，将程序图提取建模为多轮推理过程，包含三个迭代阶段：1) 图构建智能体进行图提取；2) 模拟智能体诊断和解释结构缺陷；3) 语义智能体对齐流程逻辑与源文本语言线索。重要反馈以自然语言形式注入后续提示，实现可解释和可控的优化。

Result: 实验表明，该框架在结构正确性和逻辑一致性方面相比强基线方法取得了显著改进。

Conclusion: 该多智能体框架通过模块化设计，使智能体能够针对不同类型的错误进行优化，无需监督或参数更新，实现了可解释和可控的程序图提取。

Abstract: Automatically extracting workflows as procedural graphs from natural language is promising yet underexplored, demanding both structural validity and logical alignment. While recent large language models (LLMs) show potential for procedural graph extraction, they often produce ill-formed structures or misinterpret logical flows. We present \model{}, a multi-agent framework that formulates procedural graph extraction as a multi-round reasoning process with dedicated structural and logical refinement. The framework iterates through three stages: (1) a graph extraction phase with the graph builder agent, (2) a structural feedback phase in which a simulation agent diagnoses and explains structural defects, and (3) a logical feedback phase in which a semantic agent aligns semantics between flow logic and linguistic cues in the source text. Important feedback is prioritized and expressed in natural language, which is injected into subsequent prompts, enabling interpretable and controllable refinement. This modular design allows agents to target distinct error types without supervision or parameter updates. Experiments demonstrate that \model{} achieves substantial improvements in both structural correctness and logical consistency over strong baselines.

</details>


### [22] [CollectiveKV: Decoupling and Sharing Collaborative Information in Sequential Recommendation](https://arxiv.org/abs/2601.19178)
*Jingyu Li,Zhaocheng Du,Qianhui Zhu,kaiyuan Li,Zhicheng Zhang,Song-Li Wu,Chaolang Li,Pengwen Dai*

Main category: cs.AI

TL;DR: 提出CollectiveKV方法，通过跨用户共享KV缓存来大幅压缩存储开销，同时保持或提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 序列推荐系统中Transformer注意力机制的计算复杂度随序列长度增长，KV缓存技术虽然能降低推理延迟，但在大规模用户和长历史序列场景下会带来巨大的存储开销。研究发现不同用户的KV序列存在显著相似性，表明KV中存在可共享的协作信号。

Method: 提出CollectiveKV跨用户KV共享机制：1）通过奇异值分解分析发现KV信息可分为可跨用户共享的大部分信息和用户特定的少部分信息；2）使用可学习的全局KV池捕获跨用户共享信息；3）推理时每个用户从池中检索高维共享KV，与低维用户特定KV拼接得到最终KV。

Result: 在5个序列推荐模型和3个数据集上的实验表明，该方法能将KV缓存压缩到原始大小的0.8%，同时保持甚至提升模型性能。

Conclusion: CollectiveKV通过利用KV中的协作信号，实现了高效的跨用户KV共享，显著降低了存储开销，为大规模序列推荐系统的实际部署提供了可行的解决方案。

Abstract: Sequential recommendation models are widely used in applications, yet they face stringent latency requirements. Mainstream models leverage the Transformer attention mechanism to improve performance, but its computational complexity grows with the sequence length, leading to a latency challenge for long sequences. Consequently, KV cache technology has recently been explored in sequential recommendation systems to reduce inference latency. However, KV cache introduces substantial storage overhead in sequential recommendation systems, which often have a large user base with potentially very long user history sequences. In this work, we observe that KV sequences across different users exhibit significant similarities, indicating the existence of collaborative signals in KV. Furthermore, we analyze the KV using singular value decomposition (SVD) and find that the information in KV can be divided into two parts: the majority of the information is shareable across users, while a small portion is user-specific. Motivated by this, we propose CollectiveKV, a cross-user KV sharing mechanism. It captures the information shared across users through a learnable global KV pool. During inference, each user retrieves high-dimensional shared KV from the pool and concatenates them with low-dimensional user-specific KV to obtain the final KV. Experiments on five sequential recommendation models and three datasets show that our method can compress the KV cache to only 0.8% of its original size, while maintaining or even enhancing model performance.

</details>


### [23] [CoReTab: Improving Multimodal Table Understanding with Code-driven Reasoning](https://arxiv.org/abs/2601.19193)
*Van-Quang Nguyen,Takayuki Okatani*

Main category: cs.AI

TL;DR: CoReTab：一个代码驱动的推理框架，通过将多步推理与可执行Python代码结合，为多模态表格理解提供可扩展、可解释且可自动验证的标注，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态表格理解数据集（如MMTab）主要提供简短的事实性答案，缺乏明确的多步推理监督。在这些数据集上训练的模型通常生成简短回答，准确率不足且难以解释推理过程。

Method: 提出CoReTab代码驱动推理框架，通过多步推理与可执行Python代码结合生成可验证标注。构建包含115K验证样本的数据集（平均每个响应529个token），通过三阶段管道微调开源MLLMs。

Result: 在17个MMTab基准测试（表格问答、事实验证、表格结构理解）中，CoReTab训练的模型相比MMTab基线分别获得+6.2%、+5.7%和+25.6%的显著提升，同时产生透明可验证的推理轨迹。

Conclusion: CoReTab作为一个稳健且可泛化的监督框架，有效提升了多模态表格理解中的多步推理能力，实现了更好的准确性、可解释性和可验证性。

Abstract: Existing datasets for multimodal table understanding, such as MMTab, primarily provide short factual answers without explicit multi-step reasoning supervision. Models trained on these datasets often generate brief responses that offers insufficient accuracy and limited interpretability into how these models arrive at the final answer. We introduce CoReTab, a code-driven reasoning framework that produces scalable, interpretable, and automatically verifiable annotations by coupling multi-step reasoning with executable Python code. Using the CoReTab framework, we curate a dataset of 115K verified samples averaging 529 tokens per response and fine-tune open-source MLLMs through a three-stage pipeline. We evaluate the resulting model trained on CoReTab across 17 MMTab benchmarks spanning table question answering, fact verification, and table structure understanding. Our model achieves significant gains of +6.2%, +5.7%, and +25.6%, respectively, over MMTab-trained baselines, while producing transparent and verifiable reasoning traces. These results establish CoReTab as a robust and generalizable supervision framework for improving multi-step reasoning in multimodal table understanding.

</details>


### [24] [MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution](https://arxiv.org/abs/2601.19199)
*Libo Sun,Jiwen Zhang,Siyuan Wang,Zhongyu Wei*

Main category: cs.AI

TL;DR: MAGNET：基于双级记忆的自适应移动GUI代理框架，通过连接视觉特征与稳定功能语义来应对界面变化，提升任务执行鲁棒性


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理面临界面频繁更新导致训练数据过时的问题，但界面变化背后功能语义和任务意图保持稳定，需要利用这种稳定性来提升代理的适应能力

Method: 提出MAGNET框架，包含双级记忆：静态记忆连接多样视觉特征与稳定功能语义以进行鲁棒动作定位；过程记忆捕获不同工作流中的稳定任务意图。采用动态记忆进化机制，通过优先访问频率高的知识持续优化两种记忆

Result: 在AndroidWorld在线基准测试中显著优于基线方法，离线基准测试在分布偏移下也显示出一致的性能提升

Conclusion: 利用界面变化中的稳定结构能够有效提升代理在演化软件环境中的性能和泛化能力，验证了功能语义和任务意图稳定性的重要性

Abstract: Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments.

</details>


### [25] [MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning](https://arxiv.org/abs/2601.19204)
*Zhixi Cai,Fucai Ke,Kevin Leo,Sukai Huang,Maria Garcia de la Banda,Peter J. Stuckey,Hamid Rezatofighi*

Main category: cs.AI

TL;DR: MATA是一个基于分层有限状态自动机的多智能体系统，用于视觉推理任务，通过可训练的超级智能体选择顶层状态转移，实现透明且高效的协作推理。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型虽然感知能力强，但隐含推理难以解释，容易在复杂查询上产生幻觉。组合方法虽然提高了可解释性，但大多依赖单一智能体或手工设计的流程，无法决定何时在互补智能体之间协作或在重叠智能体之间竞争。

Method: 提出MATA（Multi-Agent hierarchical Trainable Automaton），这是一个基于分层有限状态自动机的多智能体系统。每个智能体对应超自动机中的一个状态，运行基于规则的小型子自动机进行可靠的微控制。所有智能体读写共享内存，产生透明的执行历史。通过构建转移轨迹树并转换为内存到下一状态对，创建MATA-SFT-90K数据集用于监督微调，训练LLM作为转移策略。

Result: 在多个视觉推理基准测试中，MATA相比单一模型和组合基线方法取得了最先进的结果。

Conclusion: MATA通过可训练的超级智能体选择最优智能体来解决任务，实现了透明、高效的多智能体协作推理，在视觉推理任务中表现出色。

Abstract: Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as a hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent's transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines. The code and dataset are available at https://github.com/ControlNet/MATA.

</details>


### [26] [Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection](https://arxiv.org/abs/2601.19245)
*Yongxin Deng,Zhen Fang,Yixuan Li,Ling Chen*

Main category: cs.AI

TL;DR: 该论文提出SpikeScore方法，通过量化多轮对话中的不确定性突变来检测大语言模型的幻觉，在跨域泛化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在同域数据上表现良好，但跨域泛化能力差。论文研究通用幻觉检测问题，旨在在单域数据上训练检测器，同时确保在相关域上的鲁棒性能。

Method: 通过模拟LLM初始响应后的多轮对话，发现幻觉引发的对话比事实性对话表现出更大的不确定性波动。基于此提出SpikeScore，量化多轮对话中的突变波动。

Result: 理论和实证验证表明，SpikeScore在幻觉和非幻觉响应之间具有强大的跨域可分性。在多个LLM和基准测试上的实验显示，基于SpikeScore的方法在跨域泛化方面优于代表性基线方法。

Conclusion: SpikeScore方法在跨域幻觉检测中表现出色，为解决大语言模型幻觉检测的跨域泛化问题提供了有效方案。

Abstract: Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this paper, we study an important yet overlooked problem, termed generalizable hallucination detection (GHD), which aims to train hallucination detectors on data from a single domain while ensuring robust performance across diverse related domains. In studying GHD, we simulate multi-turn dialogues following LLMs initial response and observe an interesting phenomenon: hallucination-initiated multi-turn dialogues universally exhibit larger uncertainty fluctuations than factual ones across different domains. Based on the phenomenon, we propose a new score SpikeScore, which quantifies abrupt fluctuations in multi-turn dialogues. Through both theoretical analysis and empirical validation, we demonstrate that SpikeScore achieves strong cross-domain separability between hallucinated and non-hallucinated responses. Experiments across multiple LLMs and benchmarks demonstrate that the SpikeScore-based detection method outperforms representative baselines in cross-domain generalization and surpasses advanced generalization-oriented methods, verifying the effectiveness of our method in cross-domain hallucination detection.

</details>


### [27] [GLOVE: Global Verifier for LLM Memory-Environment Realignment](https://arxiv.org/abs/2601.19249)
*Xingkun Yin,Hongyang Du*

Main category: cs.AI

TL;DR: GLOVE框架通过主动探测记忆与观察间的不一致性，建立相对真理概念，实现无监督记忆更新，提升LLM在动态环境中的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆增强方法依赖外部评估器或模型内省来验证记忆有效性，但这些假设在动态漂移的实际环境中往往失效，需要更鲁棒的验证机制。

Method: 提出GLOVE框架，通过主动探测检索记忆与新鲜观察之间的不一致性，建立相对真理概念，实现无监督记忆-环境对齐，无需真实监督或强依赖模型内省。

Result: 在网页导航、规划和控制的多样化基准测试中，GLOVE显著提高了智能体成功率，特别是在引入超出原始设置的非平稳性环境漂移时表现优异。

Conclusion: GLOVE为LLM记忆系统提供了新的设计维度，通过相对真理验证机制实现记忆自我演化，为构建能够自我进化的认知智能体提供了稳健路径。

Abstract: Most existing memory-enhanced Large Language Model (LLM) approaches implicitly assume that memory validity can be established either through external evaluators that provide task-specific success signals or through internal model cognition, such as reflection, for editing memory entries. However, these assumptions often break down in practical environments with dynamic drifts. We propose the Global Verifier (GLOVE), a framework that introduces a new design dimension for LLM memory systems by establishing a relative notion of truth. Through active probing to detect inconsistencies between retrieved memories and fresh observations, GLOVE enables memory-environment realignment by verifying and updating memory without access to ground-truth supervision or strong reliance on model introspection. We evaluate GLOVE on diverse benchmarks spanning web navigation, planning, and control, augmented with controlled environmental drifts that introduce non-stationarity beyond the original benchmark settings. Our results show that GLOVE substantially improves agent success rates, suggesting a robust pathway to cognitive agents capable of self-evolving.

</details>


### [28] [Curiosity Driven Knowledge Retrieval for Mobile Agents](https://arxiv.org/abs/2601.19306)
*Sijia Li,Xiaoyu Tan,Shahir Ali,Niels Schmidt,Gengchen Ma,Xihe Qiu*

Main category: cs.AI

TL;DR: 提出好奇心驱动的知识检索框架，通过AppCards编码应用信息来增强移动代理在复杂任务中的表现


<details>
  <summary>Details</summary>
Motivation: 移动代理在智能手机自动化方面虽有进展，但在复杂应用中仍受限于知识不完整和对未见环境的泛化能力弱

Method: 引入好奇心驱动的知识检索框架，将执行中的不确定性量化为好奇心分数，超过阈值时从文档、代码库和历史轨迹中检索外部信息，组织成结构化的AppCards（编码功能语义、参数约定、界面映射和交互模式），增强代理在推理过程中选择性集成相关信息

Result: 在AndroidWorld基准测试中，所有骨干模型均获得一致改进，平均提升6个百分点，与GPT-5结合时达到88.8%的最新SOTA成功率；AppCards对多步骤和跨应用任务特别有效，能减少歧义、缩短探索时间并支持稳定的执行轨迹

Conclusion: 好奇心驱动的知识检索框架通过AppCards有效弥补移动代理的知识盲点，提高规划可靠性，为复杂智能手机自动化任务提供了可扩展的解决方案

Abstract: Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at https://lisalsj.github.io/Droidrun-appcard/.

</details>


### [29] [Balancing Sustainability And Performance: The Role Of Small-Scale Llms In Agentic Artificial Intelligence Systems](https://arxiv.org/abs/2601.19311)
*Anh Khoa Ngo Ho,Martin Chauvin,Simon Gosset,Philippe Cordier,Boris Gamazaychikov*

Main category: cs.AI

TL;DR: 研究发现小型开源语言模型在保持任务质量的同时能显著降低能耗，为可持续AI系统设计提供实用指南


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型成为智能代理系统的核心，其推理阶段的高能耗可能带来可持续性挑战。研究旨在探索是否可以通过部署较小规模的语言模型来降低能耗，同时不影响多智能体真实环境中的响应速度和输出质量。

Method: 对多种不同规模的语言模型进行对比分析，量化效率与性能之间的权衡关系。基于研究结果，提出了可持续AI设计的实用指南，包括最佳批处理大小配置和计算资源分配策略。

Result: 结果显示，较小的开源权重模型能够降低能源使用，同时保持任务质量。这些发现为开发可扩展、环境友好的AI系统提供了可行的策略。

Conclusion: 小型语言模型在降低能耗方面具有显著优势，通过合理的配置和资源分配，可以在保持性能的同时实现可持续的AI系统设计。研究为开发环境友好的智能系统提供了实用指导。

Abstract: As large language models become integral to agentic artificial intelligence systems, their energy demands during inference may pose significant sustainability challenges. This study investigates whether deploying smaller-scale language models can reduce energy consumption without compromising responsiveness and output quality in a multi-agent, real-world environments. We conduct a comparative analysis across language models of varying scales to quantify trade-offs between efficiency and performance. Results show that smaller open-weights models can lower energy usage while preserving task quality. Building on these findings, we propose practical guidelines for sustainable artificial intelligence design, including optimal batch size configuration and computation resource allocation. These insights offer actionable strategies for developing scalable, environmentally responsible artificial intelligence systems.

</details>


### [30] [SETA: Statistical Fault Attribution for Compound AI Systems](https://arxiv.org/abs/2601.19337)
*Sayak Chowdhury,Meenakshi D'Souza*

Main category: cs.AI

TL;DR: 提出模块化鲁棒性测试框架，用于多网络AI系统的组件级分析和错误传播推理


<details>
  <summary>Details</summary>
Motivation: 现代AI系统通常由多个互联神经网络组成，现有鲁棒性测试技术主要针对单网络模型，难以扩展到多网络管道系统

Method: 提出模块化鲁棒性测试框架，支持组件级系统分析和跨神经网络模块的错误传播推理，框架与架构和模态无关

Result: 将框架应用于真实世界的自主铁路检测系统，成功展示了比传统端到端指标更细粒度的鲁棒性分析能力

Conclusion: 该模块化测试框架为多网络AI系统提供了有效的鲁棒性测试方法，支持组件级错误隔离和错误传播分析

Abstract: Modern AI systems increasingly comprise multiple interconnected neural networks to tackle complex inference tasks. Testing such systems for robustness and safety entails significant challenges. Current state-of-the-art robustness testing techniques, whether black-box or white-box, have been proposed and implemented for single-network models and do not scale well to multi-network pipelines. We propose a modular robustness testing framework that applies a given set of perturbations to test data. Our testing framework supports (1) a component-wise system analysis to isolate errors and (2) reasoning about error propagation across the neural network modules. The testing framework is architecture and modality agnostic and can be applied across domains. We apply the framework to a real-world autonomous rail inspection system composed of multiple deep networks and successfully demonstrate how our approach enables fine-grained robustness analysis beyond conventional end-to-end metrics.

</details>


### [31] [PROTEUS: SLA-Aware Routing via Lagrangian RL for Multi-LLM Serving Systems](https://arxiv.org/abs/2601.19402)
*Amit Singh Bhatti,Vishal Vaddina,Dagnachew Birru*

Main category: cs.AI

TL;DR: PROTEUS是一个接受准确率目标τ作为运行时输入的LLM路由器，使用拉格朗日对偶控制实现单一模型满足全准确率谱系，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由器需要离线调参并猜测准确率结果，参数与结果关系间接、非单调且依赖数据集。操作员需要指定准确率目标，而不是从模糊设置中推断。

Method: PROTEUS采用拉格朗日对偶控制方法，学习对偶变量λ跟踪训练期间的约束违反情况，并调节策略网络，将指定的τ值转换为满足目标的路由决策。

Result: 在RouterBench和SPROUT数据集上，PROTEUS实现一致的准确率下限合规（准确率≥τ），目标-响应相关性达0.97-0.98。相比最佳基线OmniRouter仅22%时间满足下限，PROTEUS在[0.85,0.95]范围内从单一模型运行，RouterBench准确率90.1%（与oracle相差1.3%），SPROUT准确率94.0%（与oracle相差4.6%），成本节省达89.8%。

Conclusion: PROTEUS通过接受准确率目标作为直接输入，解决了LLM路由中参数设置不透明的问题，实现了操作员友好的目标驱动路由，在保持高准确率的同时显著降低成本。

Abstract: Production LLM deployments serve diverse workloads where cost and quality requirements vary by customer tier, time of day, and query criticality. Model serving systems accept latency SLOs directly. LLM routers do not. They force operators to tune parameters offline and guess what accuracy might result. The relationship between parameters and outcomes is indirect, non-monotonic, and dataset-dependent. Operators need to specify accuracy targets, not infer them from opaque settings. We present PROTEUS (Polymorphic Router for Operational Target Enforcement with Unified SLA), a router that accepts accuracy targets tau as runtime input. PROTEUS uses Lagrangian dual control. A learned dual variable lambda tracks constraint violations during training and conditions the policy network. This lets the router translate specified tau values into routing decisions that satisfy them. A single trained model serves the full accuracy spectrum without retraining.We evaluate on RouterBench (11 models, 405K queries) and SPROUT (14 models, 45K queries). PROTEUS achieves consistent floor compliance where accuracy meets or exceeds tau. The target-response correlation reaches 0.97 to 0.98. The closest baseline, OmniRouter, meets floors only 22% of the time despite also using Lagrangian optimization. PROTEUS operates across tau in [0.85, 0.95] from a single model. On RouterBench it achieves 90.1% accuracy, within 1.3% of oracle. On SPROUT it achieves 94.0% accuracy, within 4.6% of oracle. Cost savings reach 89.8% versus the best fixed model.

</details>


### [32] [RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization](https://arxiv.org/abs/2601.19404)
*Hongzhu Yi,Xinming Wang,Zhenghao zhang,Tianyu Zong,Yuanxiang Wang,Jun Xie,Tao Yu,Haopeng Jin,Zhepeng Wang,Kaixin Xu,Feng Chen,Jiahuan Chen,Yujia Yang,Zhenyu Guan,Bingkang Shi,Jungang Xu*

Main category: cs.AI

TL;DR: RPO是一种创新的强化微调算法，通过仅生成推理路径的后缀来训练模型，相比传统方法减少约95%的token生成，大幅降低训练时间开销。


<details>
  <summary>Details</summary>
Motivation: 传统强化微调算法需要从输入查询开始生成完整的推理轨迹，这在训练rollout阶段会产生巨大的计算开销。为了解决这个问题，研究者分析了推理路径不同部分对最终结果正确性的影响。

Method: 提出RPO（Partial Reasoning Optimization）算法，这是一种即插即用的强化微调方法。与传统生成完整推理路径的方法不同，RPO通过经验缓存生成推理路径的后缀来训练模型，在rollout阶段大幅减少token生成。

Result: RPO在rollout阶段减少约95%的token生成，将1.5B模型的训练时间减少90%，7B模型减少72%。同时能与GRPO、DAPO等典型算法集成，在保持性能相当的情况下实现训练加速。

Conclusion: RPO是一种高效的强化微调算法，通过部分推理优化显著降低了训练计算开销，同时保持模型性能，为大语言模型的高效训练提供了实用解决方案。

Abstract: Within the domain of large language models, reinforcement fine-tuning algorithms necessitate the generation of a complete reasoning trajectory beginning from the input query, which incurs significant computational overhead during the rollout phase of training. To address this issue, we analyze the impact of different segments of the reasoning path on the correctness of the final result and, based on these insights, propose Reinforcement Fine-Tuning with Partial Reasoning Optimization (RPO), a plug-and-play reinforcement fine-tuning algorithm. Unlike traditional reinforcement fine-tuning algorithms that generate full reasoning paths, RPO trains the model by generating suffixes of the reasoning path using experience cache. During the rollout phase of training, RPO reduces token generation in this phase by approximately 95%, greatly lowering the theoretical time overhead. Compared with full-path reinforcement fine-tuning algorithms, RPO reduces the training time of the 1.5B model by 90% and the 7B model by 72%. At the same time, it can be integrated with typical algorithms such as GRPO and DAPO, enabling them to achieve training acceleration while maintaining performance comparable to the original algorithms. Our code is open-sourced at https://github.com/yhz5613813/RPO.

</details>


### [33] [Fuzzy expert system for the process of collecting and purifying acidic water: a digital twin approach](https://arxiv.org/abs/2601.19527)
*Temirbolat Maratuly,Pakizar Shamoi,Timur Samigulin*

Main category: cs.AI

TL;DR: 开发了一个结合模糊专家系统和数字孪生的控制系统，用于净化酸性水处理过程，通过模拟人类推理来维持关键参数，并允许非专家人员有效操作。


<details>
  <summary>Details</summary>
Motivation: 酸性水净化对于减少排放、降低腐蚀风险、实现处理水回用以及降低运营成本至关重要。自动化处理过程可以减少工人参与，降低工伤风险。酸性水中的硫化氢、二氧化碳等酸性成分如果不妥善处理，会造成严重环境威胁和设备腐蚀。

Method: 结合模糊专家系统和自定义数字孪生，数字孪生使用Honeywell UniSim Design R492开发，阀门动态通过MATLAB系统识别建模，实时数据交换使用OPC DA。模糊控制器采用分程控制两个阀门，在21种不同初始压力条件下测试了5种去模糊化策略，共105个测试场景。

Result: 系统性能使用误差指标（MSE、RMSE、MAE、IAE、ISE、ITAE）和动态响应指标（超调、欠调、上升时间、下降时间、稳定时间、稳态误差）进行评估。开发了基于Python Streamlit框架的Web仿真界面。

Conclusion: 虽然本文以酸性水处理为例进行演示，但所提出的模糊专家系统具有通用性，可用于其他工业过程控制。

Abstract: Purifying sour water is essential for reducing emissions, minimizing corrosion risks, enabling the reuse of treated water in industrial or domestic applications, and ultimately lowering operational costs. Moreover, automating the purification process helps reduce the risk of worker harm by limiting human involvement. Crude oil contains acidic components such as hydrogen sulfide, carbon dioxide, and other chemical compounds. During processing, these substances are partially released into sour water. If not properly treated, sour water poses serious environmental threats and accelerates the corrosion of pipelines and equipment. This paper presents a fuzzy expert system, combined with a custom-generated digital twin, developed from a documented industrial process to maintain key parameters at desired levels by mimicking human reasoning. The control strategy is designed to be simple and intuitive, allowing junior or non-expert personnel to interact with the system effectively. The digital twin was developed using Honeywell UniSim Design R492 to simulate real industrial behavior accurately. Valve dynamics were modeled through system identification in MATLAB, and real-time data exchange between the simulator and controller was established using OPC DA. The fuzzy controller applies split-range control to two valves and was tested under 21 different initial pressure conditions using five distinct defuzzification strategies, resulting in a total of 105 unique test scenarios. System performance was evaluated using both error-based metrics (MSE, RMSE, MAE, IAE, ISE, ITAE) and dynamic response metrics, including overshoot, undershoot, rise time, fall time, settling time, and steady-state error. A web-based simulation interface was developed in Python using the Streamlit framework. Although demonstrated here for sour water treatment, the proposed fuzzy expert system is general-purpose.

</details>


### [34] [Benchmarks Saturate When The Model Gets Smarter Than The Judge](https://arxiv.org/abs/2601.19532)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.AI

TL;DR: Omni-MATH-2是Omni-MATH数据集的修订版本，包含4181个精确答案问题和247个标记的非标准问题，通过人工审核确保可编译性、可解性和可验证性，显著减少数据集噪声，提供更精确的模型性能评估。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型基准测试存在数据集和评估方法的不准确问题，这些不准确性持续削弱基准测试的有效性。需要更干净的数据集和可靠的评估方法来准确衡量模型性能。

Method: 创建Omni-MATH-2数据集：1）手动审核每个问题，确保LaTeX可编译性、可解性和可验证性；2）添加缺失的图形或信息；3）标记需要证明、估计或图像的问题；4）移除杂乱内容；5）使用专家标注评估法官引起的噪声，比较GPT-5 mini与原始Omni-Judge的差异。

Result: 1）数据集修订显著减少了数据集引起的噪声；2）法官比较显示，在法官分歧中，Omni-Judge在96.4%的情况下是错误的；3）随着问题难度增加，需要更胜任的法官来防止法官错误掩盖模型间的真实差异；4）对于标记的非标准问题子集，两种法官都未能识别当前的失败模式。

Conclusion: 数据集质量和法官可靠性对于开发准确的模型性能基准测试都至关重要。仅靠改进数据集不足以确保可靠的评估，需要同时关注数据集质量和评估方法的可靠性。

Abstract: Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset ($n{=}4181$) and a tagged, non-standard subset ($n{=}247$). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in $96.4\%$ of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.

</details>


### [35] [Learning Adaptive Parallel Execution for Efficient Code Localization](https://arxiv.org/abs/2601.19568)
*Ke Xu,Siyang Xiao,Ming Liang,Yichen Yu,Zhixiang Wang,Jingxuan Xu,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.AI

TL;DR: FuseSearch通过联合质量-效率优化方法，动态调整并行搜索广度，显著提升代码定位效率，减少冗余调用，在保持SOTA性能的同时实现93.6%的速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前自动化软件开发流程中，代码定位是关键瓶颈。虽然并行工具执行可以提高发现速度，但现有代理存在34.9%的冗余调用率，抵消了并行化的优势。

Method: 提出FuseSearch，将并行代码定位重新定义为联合质量-效率优化任务。定义工具效率（独特信息增益与调用次数的比率），采用两阶段SFT和RL训练方法学习自适应并行策略，动态根据任务上下文调整搜索广度。

Result: 在SWE-bench Verified上评估，FuseSearch-4B达到SOTA性能（文件级F1分数84.7%，函数级56.4%），同时实现93.6%的速度提升，减少67.7%的轮次和68.9%的令牌使用。

Conclusion: 效率感知训练通过消除噪声冗余信号自然提高质量，实现了高性能且成本效益高的代码定位代理。动态并行策略比固定广度方法更有效。

Abstract: Code localization constitutes a key bottleneck in automated software development pipelines. While concurrent tool execution can enhance discovery speed, current agents demonstrate a 34.9\% redundant invocation rate, which negates parallelism benefits. We propose \textbf{FuseSearch}, reformulating parallel code localization as a \textbf{joint quality-efficiency optimization} task. Through defining \textbf{tool efficiency} -- the ratio of unique information gain to invocation count -- we utilize a two-phase SFT and RL training approach for learning adaptive parallel strategies. Different from fixed-breadth approaches, FuseSearch dynamically modulates search breadth according to task context, evolving from exploration phases to refinement stages. Evaluated on SWE-bench Verified, FuseSearch-4B achieves SOTA-level performance (84.7\% file-level and 56.4\% function-level $F_1$ scores) with 93.6\% speedup, utilizing 67.7\% fewer turns and 68.9\% fewer tokens. Results indicate that efficiency-aware training naturally improves quality through eliminating noisy redundant signals, enabling high-performance cost-effective localization agents.

</details>


### [36] [ComAgent: Multi-LLM based Agentic AI Empowered Intelligent Wireless Networks](https://arxiv.org/abs/2601.19607)
*Haoyun Li,Ming Xiao,Kezhi Wang,Robert Schober,Dong In Kim,Yong Liang Guan*

Main category: cs.AI

TL;DR: ComAgent是一个多LLM智能体框架，通过感知-规划-行动-反思循环，自动将高层网络意图转化为数学公式和可执行代码，解决6G网络跨层优化中的手动建模瓶颈。


<details>
  <summary>Details</summary>
Motivation: 6G网络依赖复杂的跨层优化，但将高层意图手动转化为数学公式存在瓶颈。现有LLM方法缺乏足够的领域知识、约束意识和验证能力，需要更智能的自动化解决方案。

Method: 提出ComAgent多LLM智能体框架，采用闭环的感知-规划-行动-反思循环，协调专门的智能体进行文献搜索、编码和评分，通过迭代分解问题和自我纠错，自动生成求解器就绪的公式和可复现的仿真。

Result: 评估显示ComAgent在复杂波束成形优化中达到专家可比的性能，在多种无线任务中优于单体LLM，展示了在新型无线网络中自动化设计的潜力。

Conclusion: ComAgent框架通过多智能体协作和闭环学习，有效弥合用户意图与执行之间的差距，为新兴无线网络的自动化设计提供了有前景的解决方案。

Abstract: Emerging 6G networks rely on complex cross-layer optimization, yet manually translating high-level intents into mathematical formulations remains a bottleneck. While Large Language Models (LLMs) offer promise, monolithic approaches often lack sufficient domain grounding, constraint awareness, and verification capabilities. To address this, we present ComAgent, a multi-LLM agentic AI framework. ComAgent employs a closed-loop Perception-Planning-Action-Reflection cycle, coordinating specialized agents for literature search, coding, and scoring to autonomously generate solver-ready formulations and reproducible simulations. By iteratively decomposing problems and self-correcting errors, the framework effectively bridges the gap between user intent and execution. Evaluations demonstrate that ComAgent achieves expert-comparable performance in complex beamforming optimization and outperforms monolithic LLMs across diverse wireless tasks, highlighting its potential for automating design in emerging wireless networks.

</details>


### [37] [Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A* Search](https://arxiv.org/abs/2601.19622)
*Thomas Bömer,Nico Koltermann,Max Disselnmeyer,Bastian Amberg,Anne Meyer*

Main category: cs.AI

TL;DR: 本文提出了一种名为A-CEoH的新框架，通过将A*算法代码融入提示词中，利用上下文学习自动生成A*搜索的启发式函数，在单元装载预整理问题和滑块拼图问题上取得了优于专家设计启发式的效果。


<details>
  <summary>Details</summary>
Motivation: 传统启发式函数需要手工设计，依赖专家知识且耗时。随着大语言模型和进化框架的发展，自动化启发式设计成为可能。本文旨在扩展启发式进化框架，探索A*搜索引导启发式的自动生成方法。

Method: 提出了算法-上下文启发式进化框架（A-CEoH），这是一种领域无关的提示增强策略，将A*算法代码融入提示词中以利用上下文学习。该方法扩展了原有的启发式进化框架，通过LLM生成和进化启发式函数。

Result: 在单元装载预整理问题和滑块拼图问题上的计算实验表明，A-CEoH能显著提高生成启发式的质量，甚至在某些情况下超越了专家设计的启发式函数。

Conclusion: A-CEoH框架成功实现了A*搜索启发式函数的自动化生成，通过算法代码的上下文集成有效提升了生成质量，为自动化启发式设计提供了新途径。

Abstract: Heuristic functions are essential to the performance of tree search algorithms such as A*, where their accuracy and efficiency directly impact search outcomes. Traditionally, such heuristics are handcrafted, requiring significant expertise. Recent advances in large language models (LLMs) and evolutionary frameworks have opened the door to automating heuristic design. In this paper, we extend the Evolution of Heuristics (EoH) framework to investigate the automated generation of guiding heuristics for A* search. We introduce a novel domain-agnostic prompt augmentation strategy that includes the A* code into the prompt to leverage in-context learning, named Algorithmic - Contextual EoH (A-CEoH). To evaluate the effectiveness of A-CeoH, we study two problem domains: the Unit-Load Pre-Marshalling Problem (UPMP), a niche problem from warehouse logistics, and the classical sliding puzzle problem (SPP). Our computational experiments show that A-CEoH can significantly improve the quality of the generated heuristics and even outperform expert-designed heuristics.

</details>


### [38] [Agentic Design Patterns: A System-Theoretic Framework](https://arxiv.org/abs/2601.19752)
*Minh-Dung Dao,Quy Minh Le,Hoang Thanh Lam,Duc-Trong Le,Quoc-Viet Pham,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 提出一个基于系统理论的AI智能体工程化框架，包含5个核心功能子系统和12种设计模式，用于解决当前智能体系统设计缺乏理论基础、不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于基础模型的智能体AI系统存在幻觉、推理能力差等问题，且系统设计往往临时拼凑，导致应用不可靠。现有对智能体设计模式的描述缺乏严格的系统理论基础，多为高层次或基于便利性的分类，难以实际实施。

Method: 提出两个主要贡献：1) 一个系统理论框架，将智能体AI系统解构为5个核心交互功能子系统：推理与世界模型、感知与接地、动作执行、学习与适应、智能体间通信；2) 基于此架构，映射到全面的智能体挑战分类，提出12种智能体设计模式，分为基础类、认知与决策类、执行与交互类、适应与学习类。

Result: 通过ReAct框架的案例研究展示了该框架的实用性，表明所提出的模式可以纠正系统性架构缺陷。该工作为研究人员和工程师提供了标准化的智能体设计交流语言和结构化方法。

Conclusion: 这项工作为智能体AI系统提供了基础语言和结构化方法论，能够标准化研究人员和工程师之间的智能体设计交流，从而构建更模块化、可理解和可靠的自适应系统。

Abstract: With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.

</details>


### [39] [GAVEL: Towards rule-based safety through activation monitoring](https://arxiv.org/abs/2601.19768)
*Shir Rozenfeld,Rahul Pankajakshan,Itay Zloczower,Eyal Lenga,Gilad Gressel,Yisroel Mirsky*

Main category: cs.AI

TL;DR: 论文提出基于规则的激活安全新范式，将LLM激活建模为可组合的认知元素，通过谓词规则实时检测违规行为，提高精度并支持领域定制。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活的安全监控方法存在精度低、灵活性差、缺乏可解释性等问题，需要一种更精确、可定制且可解释的安全框架。

Method: 提出规则化激活安全范式：1) 将激活建模为细粒度认知元素；2) 定义认知元素上的谓词规则；3) 实时检测规则违规；4) 提供开源框架GAVEL和自动规则创建工具。

Result: 基于规则的激活安全方法提高了检测精度，支持领域定制，为可扩展、可解释、可审计的AI治理奠定了基础。

Conclusion: 规则化激活安全通过认知元素和谓词规则的组合，实现了更精确、灵活、可解释的安全监控，推动了AI治理的发展。

Abstract: Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.

</details>


### [40] [CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing](https://arxiv.org/abs/2601.19793)
*Shanyv Liu,Xuyang Yuan,Tao Chen,Zijun Zhan,Zhu Han,Danyang Zheng,Weishan Zhang,Shaohua Cao*

Main category: cs.AI

TL;DR: CASTER是一个用于图基多智能体系统的轻量级路由器，通过上下文感知的动态模型选择，在保持任务成功率的同时大幅降低推理成本


<details>
  <summary>Details</summary>
Motivation: 图基多智能体系统虽然支持复杂循环工作流，但存在静态模型分配效率低下的问题，统一部署强大模型会在简单子任务上浪费计算资源

Method: CASTER采用双信号路由器，结合语义嵌入和结构元特征来估计任务难度，通过"冷启动到迭代演化"范式进行自我优化，从路由失败中学习

Result: 在软件工程、数据分析、科学发现和网络安全四个领域的实验中，CASTER相比强模型基线减少高达72.4%的推理成本，同时保持相同的成功率，在所有领域都优于启发式路由和FrugalGPT

Conclusion: CASTER为图基多智能体系统提供了一种高效、自适应的模型选择方案，能够在保持性能的同时显著降低计算成本

Abstract: Graph-based Multi-Agent Systems (MAS) enable complex cyclic workflows but suffer from inefficient static model allocation, where deploying strong models uniformly wastes computation on trivial sub-tasks. We propose CASTER (Context-Aware Strategy for Task Efficient Routing), a lightweight router for dynamic model selection in graph-based MAS. CASTER employs a Dual-Signal Router that combines semantic embeddings with structural meta-features to estimate task difficulty. During training, the router self-optimizes through a Cold Start to Iterative Evolution paradigm, learning from its own routing failures via on-policy negative feedback. Experiments using LLM-as-a-Judge evaluation across Software Engineering, Data Analysis, Scientific Discovery, and Cybersecurity demonstrate that CASTER reduces inference cost by up to 72.4% compared to strong-model baselines while matching their success rates, and consistently outperforms both heuristic routing and FrugalGPT across all domains.

</details>


### [41] [An Interpretable Recommendation Model for Psychometric Data, With an Application to Gerontological Primary Care](https://arxiv.org/abs/2601.19824)
*Andre Paulino de Lima,Paula Castro,Suzana Carvalho Vaz de Andrade,Rosa Maria Marcucci,Ruth Caldeira de Melo,Marcelo Garcia Manzato*

Main category: cs.AI

TL;DR: 提出一个用于老年初级医疗的推荐系统，利用心理测量数据结构提供可视化解释，帮助护理专业人员制定个性化护理计划


<details>
  <summary>Details</summary>
Motivation: 医疗推荐系统面临多重挑战：缺乏公开临床数据、用户难以理解推荐原因、遵循推荐存在风险、效果不确定性。特别是在老年初级医疗领域，需要可解释且可靠的推荐系统来辅助护理专业人员

Method: 开发了一个推荐模型，利用心理测量数据结构生成可视化解释。模型专注于老年初级医疗领域，通过巴西研究合作伙伴收集的医疗数据集进行离线性能评估，并进行用户研究评估可视化解释的可解释性

Result: 模型在医疗数据集上表现出良好的离线性能，用户研究显示生成的可视化解释对护理专业人员具有可解释性。结果表明该模型能够推进推荐系统在老年初级医疗领域的应用

Conclusion: 该推荐模型能够解决医疗推荐系统的关键挑战，特别是在老年初级医疗领域。随着人口老龄化加剧，该领域的需求、机会和信息技术需求将不断增长，该模型为此提供了可行的解决方案

Abstract: There are challenges that must be overcome to make recommender systems useful in healthcare settings. The reasons are varied: the lack of publicly available clinical data, the difficulty that users may have in understanding the reasons why a recommendation was made, the risks that may be involved in following that recommendation, and the uncertainty about its effectiveness. In this work, we address these challenges with a recommendation model that leverages the structure of psychometric data to provide visual explanations that are faithful to the model and interpretable by care professionals. We focus on a narrow healthcare niche, gerontological primary care, to show that the proposed recommendation model can assist the attending professional in the creation of personalised care plans. We report results of a comparative offline performance evaluation of the proposed model on healthcare datasets that were collected by research partners in Brazil, as well as the results of a user study that evaluates the interpretability of the visual explanations the model generates. The results suggest that the proposed model can advance the application of recommender systems in this healthcare niche, which is expected to grow in demand , opportunities, and information technology needs as demographic changes become more pronounced.

</details>


### [42] [Routing End User Queries to Enterprise Databases](https://arxiv.org/abs/2601.19825)
*Saikrishna Sudarshan,Tanay Kulkarni,Manasi Patwardhan,Lovekesh Vig,Ashwin Srinivasan,Tanmay Tulsidas Verlekar*

Main category: cs.AI

TL;DR: 该论文提出了一种用于企业多数据库环境中自然语言查询路由的模块化推理驱动重排序策略，通过显式建模模式覆盖、结构连接性和细粒度语义对齐，在扩展的NL-to-SQL基准测试中优于嵌入方法和直接LLM提示基线。


<details>
  <summary>Details</summary>
Motivation: 企业多数据库环境中自然语言查询路由面临挑战：随着数据库规模增大、领域重叠以及查询歧义性增加，现有方法难以有效路由查询到正确的数据库。需要更结构化、鲁棒的基于推理的解决方案。

Method: 提出模块化推理驱动重排序策略，显式建模三个关键因素：1) 模式覆盖（schema coverage）评估查询与数据库模式的匹配程度；2) 结构连接性（structural connectivity）分析数据库内部结构关系；3) 细粒度语义对齐（fine-grained semantic alignment）进行深层语义匹配。

Result: 通过扩展现有NL-to-SQL数据集构建现实基准测试，该方法在所有指标上一致优于嵌入方法和直接LLM提示基线，特别是在大规模、领域重叠的数据库仓库和歧义查询场景中表现更优。

Conclusion: 显式建模模式覆盖、结构连接性和语义对齐的推理驱动方法能有效解决企业多数据库查询路由问题，为复杂企业环境中的自然语言查询处理提供了更鲁棒的解决方案。

Abstract: We address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly modelling schema coverage, structural connectivity, and fine-grained semantic alignment, the proposed modular, reasoning-driven reranking strategy consistently outperforms embedding-only and direct LLM-prompting baselines across all the metrics.

</details>


### [43] [Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models](https://arxiv.org/abs/2601.19834)
*Jialong Wu,Xiaoying Zhang,Hongyi Yuan,Xiangcheng Zhang,Tianhao Huang,Changjing He,Chaoyi Deng,Renrui Zhang,Youbin Wu,Mingsheng Long*

Main category: cs.AI

TL;DR: 研究首次系统探讨视觉生成何时及如何提升推理能力，提出视觉优势假说：在物理世界相关任务中，视觉生成能更自然地作为世界模型，而纯语言模型会遭遇表征限制或先验知识不足的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在数学和编程等抽象领域已达到专家水平，但在物理和空间智能等需要丰富表征和先验知识的领域仍远落后于人类。统一多模态模型的出现引发了人们对基于互补多模态路径进行类人推理的兴趣，但其优势尚不明确。

Method: 1) 理论层面：将内部世界建模形式化为CoT推理的核心组件，分析不同形式世界模型的区别；2) 实证层面：识别需要交错视觉-语言CoT推理的任务，构建新的评估套件VisWorld-Eval；3) 在先进统一多模态模型上进行对照实验。

Result: 交错CoT在倾向于视觉世界建模的任务上显著优于纯语言CoT，但在其他任务上没有明显优势。这验证了视觉优势假说，阐明了多模态世界建模的潜力。

Conclusion: 本研究首次原则性地阐明了视觉生成何时及如何有益于推理，为构建更强大、更类人的多模态AI提供了理论依据和实践指导，明确了多模态世界建模的潜力边界。

Abstract: Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [44] [Belief-Combining Framework for Multi-Trace Reconstruction over Channels with Insertions, Deletions, and Substitutions](https://arxiv.org/abs/2601.18920)
*Aria Nouri*

Main category: cs.IT

TL;DR: 提出一种迭代信念组合方法，通过消息传递计算符号后验概率，在收敛时达到与联合最大后验估计相同的重建性能，同时将复杂度从指数级降低到二次方级。


<details>
  <summary>Details</summary>
Motivation: 从多个受随机插入、删除和替换噪声污染的痕迹中重建源序列通常需要联合处理所有痕迹，导致计算复杂度随痕迹数量指数增长，需要更高效的方法。

Method: 提出迭代信念组合过程，通过消息传递传播痕迹间推理，计算符号后验概率，避免联合处理所有痕迹。

Result: 方法在收敛时达到与联合最大后验估计相同的重建性能，同时将复杂度从指数级降低到痕迹数量的二次方级，在真实世界短链DNA读取数据集上得到验证。

Conclusion: 该方法在保持最优重建性能的同时显著降低了计算复杂度，为多痕迹序列重建提供了实用高效的解决方案。

Abstract: Optimal reconstruction of a source sequence from multiple noisy traces corrupted by random insertions, deletions, and substitutions typically requires joint processing of all traces, leading to computational complexity that grows exponentially with the number of traces. In this work, we propose an iterative belief-combining procedure that computes symbol-wise a posteriori probabilities by propagating trace-wise inferences via message passing. We prove that, upon convergence, our method achieves the same reconstruction performance as joint maximum a posteriori estimation, while reducing the complexity to quadratic in the number of traces. This performance equivalence is validated using a real-world dataset of clustered short-strand DNA reads.

</details>


### [45] [How Entanglement Reshapes the Geometry of Quantum Differential Privacy](https://arxiv.org/abs/2601.19126)
*Xi Wang,Parastoo Sadeghi,Guodong Shi*

Main category: cs.IT

TL;DR: 量子纠缠在量子本地差分隐私中引发相变现象：低于熵阈值时隐私泄露与无纠缠输入相同，超过阈值后隐私泄露随熵减少，严格增强隐私保护


<details>
  <summary>Details</summary>
Motivation: 研究量子纠缠如何从根本上塑造量子本地差分隐私（QLDP）。经典相关性通常被视为隐私的对抗因素，但其量子类比——纠缠的作用尚未被充分理解

Method: 考虑具有规定纠缠水平的二分量子系统，每个子系统由本地量子机制处理并通过本地操作测量，确保过程中不产生额外纠缠。使用黎曼优化分析纠缠约束量子态集合的非凸几何结构

Result: 发现纠缠与QLDP之间存在尖锐的相变现象：低于机制依赖的熵阈值时，最优隐私泄露水平与无纠缠输入相同；超过阈值后，隐私泄露水平随熵减少，严格增强隐私保证，甚至可将某些非私有机制变为私有机制

Conclusion: 纠缠是真正的隐私增强资源，为设计鲁棒的隐私保护量子协议提供了几何和操作基础。相变现象源于纠缠约束量子态集合的内在非凸几何结构

Abstract: Quantum differential privacy provides a rigorous framework for quantifying privacy guarantees in quantum information processing. While classical correlations are typically regarded as adversarial to privacy, the role of their quantum analogue, entanglement, is not well understood. In this work, we investigate how quantum entanglement fundamentally shapes quantum local differential privacy (QLDP). We consider a bipartite quantum system whose input state has a prescribed level of entanglement, characterized by a lower bound on the entanglement entropy. Each subsystem is then processed by a local quantum mechanism and measured using local operations only, ensuring that no additional entanglement is generated during the process. Our main result reveals a sharp phase-transition phenomenon in the relation between entanglement and QLDP: below a mechanism-dependent entropy threshold, the optimal privacy leakage level mirrors that of unentangled inputs; beyond this threshold, the privacy leakage level decreases with the entropy, which strictly improves privacy guarantees and can even turn some non-private mechanisms into private ones. The phase-transition phenomenon gives rise to a nonlinear dependence of the privacy leakage level on the entanglement entropy, even though the underlying quantum mechanisms and measurements are linear. We show that the transition is governed by the intrinsic non-convex geometry of the set of entanglement-constrained quantum states, which we parametrize as a smooth manifold and analyze via Riemannian optimization. Our findings demonstrate that entanglement serves as a genuine privacy-enhancing resource, offering a geometric and operational foundation for designing robust privacy-preserving quantum protocols.

</details>


### [46] [Information-Theoretic Secure Aggregation over Regular Graphs](https://arxiv.org/abs/2601.19183)
*Xiang Zhang,Zhou Li,Han Yu,Kai Wan,Hua Sun,Mingyue Ji,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文提出拓扑安全聚合(TSA)，研究在任意网络拓扑结构下邻居用户输入的单次信息论安全聚合，建立了基于通信图谱谱特性的统一线性设计框架，揭示了去中心化网络中安全聚合的基本限制。


<details>
  <summary>Details</summary>
Motivation: 大规模去中心化学习框架（如联邦学习）需要通信效率和强数据安全性，但现有信息论安全聚合主要针对集中式和全连接网络，对于具有有限本地连接的去中心化网络扩展研究不足。

Method: 提出拓扑安全聚合(TSA)框架，开发统一的线性设计方法，通过通信图谱的谱特性（特别是对角调制邻接矩阵的核）来表征TSA的可实现性，针对d-正则图（环、棱柱、完全拓扑等）进行分析。

Result: 建立了最优通信和密钥速率区域：每个用户需要存储至少1个密钥符号，广播至少1个消息符号，所有用户总共需要至少d个独立同分布密钥符号。总密钥需求仅取决于邻域大小d，与网络规模无关。

Conclusion: 揭示了去中心化网络中具有有限本地连接的安全聚合的基本限制：总密钥需求仅由邻域大小决定，与网络规模无关，为去中心化网络中的安全聚合提供了理论框架和设计指导。

Abstract: Large-scale decentralized learning frameworks such as federated learning (FL), require both communication efficiency and strong data security, motivating the study of secure aggregation (SA). While information-theoretic SA is well understood in centralized and fully connected networks, its extension to decentralized networks with limited local connectivity remains largely unexplored. This paper introduces \emph{topological secure aggregation} (TSA), which studies one-shot, information-theoretically secure aggregation of neighboring users' inputs over arbitrary network topologies. We develop a unified linear design framework that characterizes TSA achievability through the spectral properties of the communication graph, specifically the kernel of a diagonally modulated adjacency matrix. For several representative classes of $d$-regular graphs including ring, prism and complete topologies, we establish the optimal communication and secret key rate region. In particular, to securely compute one symbol of the neighborhood sum, each user must (i) store at least one key symbol, (ii) broadcast at least one message symbol, and (iii) collectively, all users must hold at least $d$ i.i.d. key symbols. Notably, this total key requirement depends only on the \emph{neighborhood size} $d$, independent of the network size, revealing a fundamental limit of SA in decentralized networks with limited local connectivity.

</details>


### [47] [Movable-Antenna Empowered Backscatter ISAC: Toward Geometry-Adaptive, Low-Power Networks](https://arxiv.org/abs/2601.19224)
*Haohao Zhang,Bowen Gu,Xianhua Yu,Hao Xie,Liejun Wang,Yongjun Xu,Xiaoming Tao,Haijun Zhang*

Main category: cs.IT

TL;DR: 论文提出利用可移动天线系统(MAS)解决反向散射集成感知与通信(B-ISAC)中的几何瓶颈问题，通过实时天线重定位主动重构级联信道，提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统B-ISAC系统面临级联反向散射链路的严重双衰落问题和几何失配敏感性，这从根本上限制了其超低功耗双功能无线系统的潜力。

Method: 在收发端集成可移动天线系统(MAS)，通过亚波长级天线重定位提供实时可控的空间自由度，在不修改被动标签或消耗额外频谱的情况下主动重构级联信道。

Result: 通过比较分析和数值结果展示了MAS辅助B-ISAC架构的系统级增益，并在关键物联网应用场景中展示了这种几何自适应范式的潜力。

Conclusion: MAS辅助B-ISAC为解决几何瓶颈提供了有效方案，指向未来运动感知无线网络的发展方向，为超低功耗双功能系统开辟了新路径。

Abstract: Backscatter-based integrated sensing and communication (B-ISAC) elevates passive tags into information-bearing scatterers, offering an ultra-low-power path toward dual-function wireless systems. However, this promise is fundamentally undermined by a cascaded backscattering link that suffers from severe double fading and is exquisitely sensitive to geometric misalignment. This article tackles this geometric bottleneck by integrating movable antenna systems (MAS) at the transceiver side. MAS provides real-time, controllable spatial degrees of freedom through sub-wavelength antenna repositioning, enabling active reconfiguration of the cascaded channel without modifying passive tags or consuming additional spectrum. We position this solution within a unified ISAC-backscatter communication-B-ISAC evolution, describe the resulting MAS-assisted B-ISAC architecture and operating principles, and demonstrate its system-level gains through comparative analysis and numerical results. Finally, we showcase the potential of this geometry-adaptive paradigm across key IoT application scenarios, pointing toward future motion-aware wireless networks.

</details>


### [48] [On the Analysis of Platooned Vehicular Networks on Highways](https://arxiv.org/abs/2601.19370)
*Kaushlendra Pandey,Harpreet S. Dhillon,Abhishek K. Gupta*

Main category: cs.IT

TL;DR: 该论文分析了高速公路车辆编队对V2V和V2I通信性能的影响，使用随机几何模型比较编队与非编队交通场景下的网络性能指标。


<details>
  <summary>Details</summary>
Motivation: 高速公路为车辆编队提供了天然环境，但编队对通信连接性的影响尚未充分探索。有效的编队依赖于可靠的V2V和V2I链路，因此理解高速公路上的连接动态至关重要。

Method: 使用随机几何建模：将RSU建模为一维泊松点过程，非编队交通场景下VU建模为一维泊松点过程，编队交通场景下建模为一维Matern聚类过程。分析每RSU负载分布、覆盖概率、速率覆盖及其元分布。

Result: 通过理论推导和仿真验证，提供了不同交通模式对RSU负载分布、覆盖概率和速率覆盖性能影响的数值分析。

Conclusion: 车辆编队对高速公路通信性能有显著影响，该研究为理解编队交通场景下的网络性能提供了理论框架和实用见解。

Abstract: Vehicular platooning refers to coordinated and close movement of vehicular users (VUs) traveling together along a common route segment, offering strategic benefits such as reduced fuel costs, lower emissions, and improved traffic flow. {Highways offer a natural setting for platooning due to extended travel distances, yet their potential remains underexplored, particularly in terms of communication and connectivity. Given that effective platooning relies on robust vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) links, understanding connectivity dynamics on highways is essential.} In this paper, we analyze the dynamics of vehicular platooning on a highway and its impact on the performance of two forms of vehicular communications -- namely V2V and V2I communication -- compared to independent vehicle movement on a highway. The vehicular networks consists of road-side units (RSUs), modeled as a 1D Poisson point process (PPP), to provide the vehicular connectivity to the VUs. VUs are modeled as 1D PPP under the non-platooned traffic scenario (N-PTS) and as a 1D Matern cluster process (MCP) under the platooned traffic scenario (PTS). We evaluate the distribution on the per-RSU load, representing the number of VUs served, for the typical and tagged RSU. Additionally, we derive coverage probability (CP) and rate coverage (RC), which measures the probability of the signal-to-interference-plus-noise ratio (SINR) and achievable rate above a specified threshold at the typical VU along with their meta distribution (MD), providing a deeper understanding of the reliability and variability of these metrics across different spatial distributions of VUs and RSUs. Finally, we validate our theoretical findings through simulations and provide numerical insights into the impact of different traffic patterns on RSU load distribution, CP, and RC performance.

</details>


### [49] [Joint Power Allocation and Antenna Placement for Pinching-Antenna Systems under User Location Uncertainty](https://arxiv.org/abs/2601.19704)
*Hao Feng,Ming Zeng,Xingwang Li,Wenwu Xie,Nian Xia,Octavia A. Dobre*

Main category: cs.IT

TL;DR: 提出一种针对多用户下行链路pinching天线系统的鲁棒资源分配框架，在用户定位存在高斯分布不确定性的条件下，通过功率分配和天线位置优化来最大化能量效率。


<details>
  <summary>Details</summary>
Motivation: 现有pinching天线系统研究大多假设完美信道状态信息或理想用户定位，而实际应用中存在定位误差。先前工作考虑均匀分布定位误差过于简化，需要更准确建模现实世界的高斯分布定位不确定性。

Method: 1) 在给定天线位置下推导解析功率分配策略；2) 采用启发式粒子群优化算法寻找实现全局能量效率最优的天线位置配置；3) 整个框架在概率中断约束下最大化能量效率。

Result: 仿真结果表明，相比固定天线基准方案，所提方案显著提升了能量效率和系统可靠性，验证了其在实际高频无线部署中的有效性。

Conclusion: 该鲁棒资源分配框架能够有效处理高斯分布定位不确定性，为实际pinching天线系统部署提供了实用的解决方案，特别适用于需要动态信号传播控制的高频通信场景。

Abstract: Pinching antenna systems have attracted much attention recently owing to its capability to maintain reliable line-of-sight (LoS) communication in high-frequency bands. By guiding signals through a waveguide and emitting them via a movable pinching antenna, these systems enable dynamic control of signal propagation and spatial adaptability. However, their performance heavily depends on effective resource allocation-encompassing power, bandwidth, and antenna positioning-which becomes challenging under imperfect channel state information (CSI) and user localization uncertainty. Existing studies largely assume perfect CSI or ideal user positioning, while our prior work considered uniform localization errors, an oversimplified assumption. In this paper, we develop a robust resource allocation framework for multiuser downlink pinching antenna systems under Gaussian-distributed localization uncertainty, which more accurately models real-world positioning errors. An energy efficiency (EE) maximization problem is formulated subject to probabilistic outage constraints, and an analytical power allocation strategy is derived under given antenna positions. On this basis, the heuristic particle swarm optimization (PSO) algorithm is employed to identify the antenna position that achieves the global EE configuration. Simulation results illustrate that the proposed scheme greatly enhances both EE and system reliability compared with fixed-antenna benchmark, validating its effectiveness for practical high-frequency wireless deployments.

</details>
