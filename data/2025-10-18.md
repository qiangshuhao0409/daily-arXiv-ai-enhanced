<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 52]
- [cs.IT](#cs.IT) [Total: 11]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Joint Active RIS Configuration and User Power Control for Localization: A Neuroevolution-Based Approach](https://arxiv.org/abs/2510.13819)
*George Stamatelis,Hui Chen,Henk Wymeersch,George C. Alexandropoulos*

Main category: cs.NI

TL;DR: 提出了一种基于可重构智能表面(RIS)的用户定位方法，采用多智能体算法联合控制RIS相位配置和用户发射功率，仅需单比特反馈即可实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究利用RIS辅助用户定位，通过基站到用户的反馈链路实现动态功率控制，旨在提高定位精度和系统效率。

Method: 采用结合神经进化(NE)和监督学习的混合方法，开发多智能体算法来联合控制RIS相位配置和用户发射功率，支持离散RIS元件和单比特反馈。

Result: 数值结果表明，该方法在性能上优于指纹识别、深度强化学习基线和基于反向传播的位置估计器。

Conclusion: 所提出的方案能够有效利用RIS辅助定位，仅需最小反馈即可实现优越的定位性能，具有实际应用价值。

Abstract: This paper studies user localization aided by a Reconfigurable Intelligent
Surface (RIS). A feedback link from the Base Station (BS) to the user is
adopted to enable dynamic power control of the user pilot transmissions in the
uplink. A novel multi-agent algorithm for the joint control of the RIS phase
configuration and the user transmit power is presented, which is based on a
hybrid approach integrating NeuroEvolution (NE) and supervised learning. The
proposed scheme requires only single-bit feedback messages for the uplink power
control, supports RIS elements with discrete responses, and is numerically
shown to outperform fingerprinting, deep reinforcement learning baselines and
backpropagation-based position estimators.

</details>


### [2] [Leveraging Wireless Sensor Networks for Real-Time Monitoring and Control of Industrial Environments](https://arxiv.org/abs/2510.13820)
*Muhammad Junaid Asif,Shazia Saqib,Rana Fayyaz Ahmad,Hamza Khan*

Main category: cs.NI

TL;DR: 提出基于NRF收发器和ARDUINO微控制器的IoT系统，通过无线传感器网络实时监测工业参数（温度、湿度、土壤湿度、火灾），并远程控制直流电机速度，提升工业操作效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决传统有线通信系统的局限性，减少物理监控需求，应对2020-2024年全球工业火灾频发问题，通过IoT技术实现远程监控和控制。

Method: 使用NRF收发器建立无线传感器网络，ARDUINO微控制器驱动中央设置，集成多种传感器监测关键参数，并通过互联网实现远程监控和直流电机控制。

Result: 系统成功实现工业参数的实时监测和远程控制，无线通信创新在工业过程自动化和安全中发挥关键作用，提供快速应急响应。

Conclusion: IoT与无线传感器网络的整合显著降低物理监控风险，提升操作效率和安全性，为各种工业应用中的监控和控制带来革命性变革。

Abstract: This research proposes an extensive technique for monitoring and controlling
the industrial parameters using Internet of Things (IoT) technology based on
wireless communication. We proposed a system based on NRF transceivers to
establish a strong Wireless Sensor Network (WSN), enabling transfer of
real-time data from multiple sensors to a central setup that is driven by
ARDUINO microcontrollers. Different key parameters, crucial for industrial
setup such as temperature, humidity, soil moisture and fire detection, are
monitored and displayed on an LCD screen, enabling factory administration to
oversee the industrial operations remotely over the internet. Our proposed
system bypasses the need for physical presence for monitoring by addressing the
shortcomings of conventional wired communication systems. Other than
monitoring, there is an additional feature to remotely control these parameters
by controlling the speed of DC motors through online commands. Given the rising
incidence of industrial fires over the worldwide between 2020 and 2024 due to
an array of hazards, this system with dual functionality boosts the overall
operational efficiency and safety. This overall integration of IoT and Wireless
Sensor Network (WSN) reduces the potential risks linked with physical
monitoring, providing rapid responses in emergency scenarios, including the
activation of firefighting equipment. The results show that innovations in
wireless communication perform an integral part in industrial process
automation and safety, paving the way to more intelligent and responsive
operating environments. Overall, this study highlights the potential for change
of IoT-enabled systems to revolutionize monitoring and control in a variety of
industrial applications, resulting in increased productivity and safety.

</details>


### [3] [LLM Agent Communication Protocol (LACP) Requires Urgent Standardization: A Telecom-Inspired Protocol is Necessary](https://arxiv.org/abs/2510.13821)
*Xin Li,Mengbing Liu,Chau Yuen*

Main category: cs.NI

TL;DR: 提出需要为LLM智能体建立统一的通信协议LACP，以解决当前临时通信方法导致的碎片化问题，确保安全性、互操作性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体通信方法零散，类似于网络早期的"协议战争"，阻碍创新并带来风险，特别是在下一代网络环境中。

Method: 提出LLM-Agent通信协议(LACP)，采用三层架构设计，确保语义清晰性、复杂任务的事务完整性和内置安全性。

Result: 建立了基于电信启发的分层标准化协议框架，为分布式AI在多智能体系统中的安全可靠运行提供基础。

Conclusion: 采用原则性、通用协议对于实现分布式AI潜力至关重要，特别是在6G及以后复杂实时应用场景中。

Abstract: This position paper argues that the field of LLM agents requires a unified,
telecom-inspired communication protocol to ensure safety, interoperability, and
scalability, especially within the context of Next Generation (NextG) networks.
Current ad-hoc communication methods are creating a fragmented ecosystem,
reminiscent of the early "protocol wars" in networking, which stifles
innovation and poses significant risks. Drawing inspiration from the layered,
standardized protocols that underpin modern telecommunications, we propose the
LLM-Agent Communication Protocol (LACP). LACP establishes a three-layer
architecture designed to ensure semantic clarity in communication,
transactional integrity for complex tasks, and robust, built-in security. In
this position paper, we argue that adopting a principled, universal protocol is
not merely beneficial but essential for realizing the potential of distributed
AI. Such a standard is critical for ensuring that multi-agent systems can
operate safely and reliably in the complex, real-time applications envisioned
for 6G and beyond.

</details>


### [4] [A Simulator for FANETs Using 5G Vehicle-to-Everything Communications and Named-Data Networking](https://arxiv.org/abs/2510.13823)
*José Manuel Rúa-Estévez,Alicia Meleiro-Estévez,Pablo Fondo-Ferreiro,Felipe Gil-Castiñeira,Brais Sánchez-Rama,Lois Gomez-Gonzalez*

Main category: cs.NI

TL;DR: 开发了一个用于验证、评估和演示飞行自组织网络的模拟器，该模拟器集成了ns-3网络模拟器和Zenoh NDN协议，支持基于5G V2X通信的多无人机多跳通信应用测试。


<details>
  <summary>Details</summary>
Motivation: 为了验证和评估使用5G V2X通信和命名数据网络范式的飞行自组织网络，需要一个能够支持多无人机多跳通信的模拟环境。

Method: 通过集成ns-3网络模拟器和Zenoh NDN协议，构建了一个能够模拟多无人机通信的仿真平台。

Result: 成功开发了一个能够进行多无人机多跳通信应用测试的模拟器。

Conclusion: 该模拟器为飞行自组织网络的研究和开发提供了一个有效的验证和测试工具。

Abstract: This work presents a simulator designed for the validation, evaluation, and
demonstration of flying adhoc networks (FANETs) using 5G vehicle-to-everything
(V2X) communications and the named-data networking (NDN) paradigm. The
simulator integrates the ns-3 network simulator and the Zenoh NDN protocol,
enabling realistic testing of applications that involve the multi-hop
communication among multiple unmanned aerial vehicles (UAVs).

</details>


### [5] [DiffLoc: Diffusion Model-Based High-Precision Positioning for 6G Networks](https://arxiv.org/abs/2510.14111)
*Taekyun Lee,Tommaso Balercia,Heasung Kim,Hyeji Kim,Jeffrey G. Andrews*

Main category: cs.NI

TL;DR: DiffLoc框架使用条件生成扩散模型直接从大规模MIMO信道状态信息实现高精度室外用户定位，在东京城市宏小区环境中达到亚厘米级精度，相比现有方法有数量级提升。


<details>
  <summary>Details</summary>
Motivation: 传统指纹定位方法难以扩展到大型动态室外环境，需要密集且不切实际的数据调查，因此需要开发能直接从原始上行链路参考信号指纹映射到连续地理坐标的新方法。

Method: 应用条件生成扩散模型处理高维大规模MIMO信道状态信息，学习从原始上行链路SRS指纹到连续地理坐标的直接映射，采用一致性训练减少推理步骤。

Result: DiffLoc-CT模型在东京城市宏小区环境中实现0.5厘米融合精度和1-2厘米单基站精度，相比监督回归方法（误差超过10米）和基于网格的融合（3米误差）有显著提升，推理时间从200步减少到2步。

Conclusion: 该框架在高速用户（15-25米/秒）和未见用户轨迹下仍保持优异精度，证明了其在实时6G应用中的实际可行性。

Abstract: This paper introduces a novel framework for high-accuracy outdoor user
equipment (UE) positioning that applies a conditional generative diffusion
model directly to high-dimensional massive MIMO channel state information
(CSI). Traditional fingerprinting methods struggle to scale to large, dynamic
outdoor environments and require dense, impractical data surveys. To overcome
these limitations, our approach learns a direct mapping from raw uplink
Sounding Reference Signal (SRS) fingerprints to continuous geographic
coordinates. We demonstrate that our DiffLoc framework achieves unprecedented
sub-centimeter precision, with our best model (DiffLoc-CT) delivering 0.5 cm
fusion accuracy and 1-2 cm single base station (BS) accuracy in a realistic,
ray-traced Tokyo urban macro-cell environment. This represents an
order-of-magnitude improvement over existing methods, including supervised
regression approaches (over 10 m error) and grid-based fusion (3 m error). Our
consistency training approach reduces inference time from 200 steps to just 2
steps while maintaining exceptional accuracy even for high-speed users (15-25
m/s) and unseen user trajectories, demonstrating the practical feasibility of
our framework for real-time 6G applications.

</details>


### [6] [Energy-Latency Optimization for Dynamic 5G Mobile Radio Access Networks](https://arxiv.org/abs/2510.14214)
*Gabriela N. Caspa H.,Carlos A. Astudillo,Nelson L. S. da Fonseca*

Main category: cs.NI

TL;DR: 提出一个混合整数线性规划模型，通过三种目标函数优化5G RAN配置：最小化前传延迟、最小化能耗、以及平衡延迟和能耗的双目标优化。


<details>
  <summary>Details</summary>
Motivation: 5G网络中基站解耦和新服务对RAN配置提出挑战，需要在满足带宽和延迟约束的同时，解决能耗问题，因为RAN运营占移动网络运营商运营支出的主要部分。

Method: 使用混合整数线性规划模型，确定最优功能分割选项、RAN功能放置和路由，针对eMBB、URLLC和mMTC切片。同时提出启发式算法以应对MILP计算时间长的问题。

Result: 研究揭示了延迟和能耗之间的权衡关系，强调了动态RAN重新配置的必要性。评估涵盖不同拓扑结构、gNB需求变化和多样化功能分割组合。

Conclusion: 该研究为优化现有和未来RAN部署提供了基础，通过平衡服务性能和成本效益能耗，解决了现实场景中延迟和能耗的双重优化问题。

Abstract: In 5G networks, base station (BS) disaggregation and new services present
challenges in radio access network (RAN) configuration, particularly in meeting
their bandwidth and latency constraints. The BS disaggregation is enabled by
functional splitting (FS), which distributes the RAN functions in processing
nodes and alleviates latency and bandwidth requirements in the fronthaul (FH).
Besides network performance, energy consumption is a critical concern for
mobile network operators (MNO), since RAN operation constitutes a major portion
of their operational expenses (OPEX). RAN configuration optimization is
essential to balance service performance with cost-effective energy
consumption. In this paper, we propose a mixed-integer linear programming
(MILP) model formulated with three objective functions: (i) minimizing
fronthaul (FH) latency, (ii) minimizing energy consumption, and (iii) a
bi-objective optimization that jointly balances both latency and energy
consumption. The model determines the optimal FS option, RAN function
placement, and routing for eMBB, URLLC, and mMTC slices. Although prior studies
have addressed RAN configuration either from an energy minimization or latency
reduction perspective, few have considered both aspects in realistic scenarios.
Our evaluation spans different topologies, accounts for variations in
aggregated gNB demand, explores diverse FS combinations, and incorporates Time
Sensitive Networking (TSN) modeling for latency analysis, as it is also crucial
in RAN performance. Given that MILP's execution time can be significant, we
propose a heuristic algorithm that adheres to RAN constraints. Our results
reveal a trade-off between latency and energy consumption, highlighting the
need for dynamic RAN reconfiguration. These insights provide a foundation to
optimize existing and future RAN deployments.

</details>


### [7] [Automated Extraction of Protocol State Machines from 3GPP Specifications with Domain-Informed Prompts and LLM Ensembles](https://arxiv.org/abs/2510.14348)
*Miao Zhang,Runhan Feng,Hongbo Tang,Yu Zhao,Jie Yang,Hang Qiu,Qi Liu*

Main category: cs.NI

TL;DR: SpecGPT是一个利用大语言模型从3GPP文档自动提取协议状态机的新框架，在5G协议建模上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 移动通信网络对关键行业至关重要，但其安全性和可靠性依赖于准确的状态机建模。传统手动建模方法劳动密集、易出错且难以维护，而现有NLP方法难以处理蜂窝协议的规模和复杂性。

Method: SpecGPT将技术规范分割为有意义的段落，应用具有思维链推理的领域知识提示，并采用集成方法提高输出可靠性。

Result: 在三个代表性5G协议（NAS、NGAP和PFCP）上使用人工标注的真实数据进行评估，SpecGPT优于现有方法。

Conclusion: 大语言模型在大规模协议建模方面具有有效性，SpecGPT框架展示了自动提取协议状态机的可行性。

Abstract: Mobile telecommunication networks are foundational to global infrastructure
and increasingly support critical sectors such as manufacturing,
transportation, and healthcare. The security and reliability of these networks
are essential, yet depend heavily on accurate modeling of underlying protocols
through state machines. While most prior work constructs such models manually
from 3GPP specifications, this process is labor-intensive, error-prone, and
difficult to maintain due to the complexity and frequent updates of the
specifications. Recent efforts using natural language processing have shown
promise, but remain limited in handling the scale and intricacy of cellular
protocols. In this work, we propose SpecGPT, a novel framework that leverages
large language models (LLMs) to automatically extract protocol state machines
from 3GPP documents. SpecGPT segments technical specifications into meaningful
paragraphs, applies domain-informed prompting with chain-of-thought reasoning,
and employs ensemble methods to enhance output reliability. We evaluate SpecGPT
on three representative 5G protocols (NAS, NGAP, and PFCP) using manually
annotated ground truth, and show that it outperforms existing approaches,
demonstrating the effectiveness of LLMs for protocol modeling at scale.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 提出了DOTechnique方法，基于决策一致性而非输出相似性来确定模型有效性，通过评估替代模型是否产生与高保真模型相同的决策来识别有效区域。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预定义的有效性框架，但这些框架可能不可用或不充分。模型有效性对决策过程至关重要。

Method: DOTechnique方法通过决策一致性评估模型有效性，整合领域约束和符号推理来缩小搜索空间，提高计算效率。

Result: 以高速公路变道系统为例，展示了DOTechnique能够发现仿真模型的有效区域。

Conclusion: 该技术有潜力通过决策者上下文来支持寻找模型有效性。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [9] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 该论文提出了一种融合视觉信息（特别是演示幻灯片）的多模态自动语音识别系统，用于科学演示场景，通过数据增强方法解决了数据集不足的问题，相比基线模型在词错误率上取得了显著降低。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的ASR系统主要依赖声学信息而忽略了多模态上下文，但视觉信息对于消歧和适应至关重要。特别是在科学演示场景中，演示幻灯片包含重要的领域特定术语信息。

Method: 创建了多模态演示基准，包括自动分析转录领域特定术语；探索了用多模态信息增强语音模型的方法；通过数据增强方法解决缺乏配套幻灯片数据集的问题；使用增强数据集训练模型。

Result: 相比基线模型，在所有词汇上词错误率相对降低了约34%，在领域特定术语上相对降低了35%。

Conclusion: 集成演示幻灯片等多模态信息可以显著提高ASR系统在科学演示场景中的性能，特别是在处理领域特定术语方面效果显著。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [10] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在零相关情境下会系统性地产生因果幻觉，错误推断不存在的因果关系，这对其在需要准确因果推理的领域应用提出了担忧。


<details>
  <summary>Details</summary>
Motivation: 研究因果学习中的认知偏差——因果幻觉，并探讨大型语言模型是否也会出现这种偏差，特别是在医疗等需要准确因果推理的领域。

Method: 构建了1000个零相关情境的医疗场景数据集，使用经典认知科学范式——列联判断任务，让LLMs评估潜在原因的效力。

Result: 所有评估的模型都系统性地推断出无根据的因果关系，显示出对因果幻觉的强烈易感性。

Conclusion: 研究结果支持LLMs只是复制因果语言而非真正理解因果关系的假设，对其在需要准确因果推理的决策领域应用提出了警示。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [11] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar,Prasad Tadepalli*

Main category: cs.AI

TL;DR: GammaZero是一个基于动作中心图表示的学习框架，用于在部分可观测马尔可夫决策过程中指导规划，能够实现跨问题规模的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要领域特定的神经网络架构且难以扩展，GammaZero旨在通过统一的图表示实现跨问题规模的泛化能力。

Method: 将信念状态系统性地转换为动作中心图，使用图神经网络从专家演示中学习价值函数和策略，然后将学习到的启发式方法应用于更大问题的蒙特卡洛树搜索。

Result: 在标准POMDP基准测试中，GammaZero在相同规模问题上性能与BetaZero相当，同时能零样本泛化到训练时未见过的2-4倍大的问题，保持解质量同时减少搜索需求。

Conclusion: GammaZero通过动作中心图表示实现了POMDP规划的可扩展性和泛化能力，为大规模部分可观测环境中的决策提供了有效解决方案。

Abstract: We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.

</details>


### [12] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: 提出一种替代性监管方法：要求大型AI实验室发布小型开源模拟模型，这些模型是从其最大专有模型中蒸馏训练而来的缩小版本，既能确保AI安全又能促进创新。


<details>
  <summary>Details</summary>
Motivation: 当前前沿AI模型监管提案因安全与创新的权衡而被搁置，需要一种既能确保安全又不阻碍创新的监管方法。

Method: 强制要求大型AI实验室发布小型开源模拟模型，这些模型作为公共代理，允许广泛参与安全验证、可解释性研究和算法透明度工作。

Result: 研究表明，基于这些小型模型开发的安全和可解释性方法能有效推广到前沿规模系统，显著降低监管负担并加速安全进展。

Conclusion: 这种监管方法能以最小额外成本显著促进公共利益，通过深入理解模型来缓解安全与创新的权衡，实现两者的双赢。

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [13] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 本文提出了一种基于多目标马尔可夫决策过程的共识声明生成框架，通过社会选择理论提供可证明的公平性保证，包括两种方法：一种保证ex-ante核心稳定性，另一种最大化平等主义福利。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型共识声明生成框架缺乏提供可证明公平性保证的结构，无法在聚合多样化自由形式意见时确保公平性。

Method: 将任务建模为多目标、令牌级的马尔可夫决策过程，每个目标对应一个代理的偏好。提出两种基于社会选择理论的方法：一是保证ex-ante核心稳定性的随机生成策略，二是使用搜索算法最大化平等主义福利。

Result: 实验表明，基于平等主义目标的搜索生成的共识声明在代理对齐的最坏情况下优于基线方法，包括Habermas Machine。

Conclusion: 该框架为共识声明生成提供了可证明的公平性保证，通过社会选择理论原则实现了更好的代理对齐效果。

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [14] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: STEMS是一个安全约束的多智能体强化学习框架，用于协调建筑能源管理，通过空间-时间图表示学习和控制屏障函数实现21%成本降低、18%排放减少，并将安全违规从35.1%降至5.6%。


<details>
  <summary>Details</summary>
Motivation: 解决多建筑能源系统中空间-时间依赖性利用不足、缺乏严格安全保证和系统复杂性等关键挑战，以实现碳减排目标、提高居住者舒适度和降低能源成本。

Method: 提出STEMS框架，包含：(1)使用GCN-Transformer融合架构的空间-时间图表示学习框架，捕捉建筑间关系和时序模式；(2)结合控制屏障函数的安全约束多智能体强化学习算法，提供数学安全保证。

Result: 在真实建筑数据集上的实验显示，STEMS相比现有方法实现21%成本降低、18%排放减少，安全违规从35.1%大幅降至5.6%，同时保持仅0.13的不适比例，在极端天气条件下表现出强鲁棒性。

Conclusion: STEMS框架在多建筑能源协调管理中表现出卓越性能，有效平衡了能源效率、安全性和舒适度要求，适用于不同类型的建筑场景。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [15] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: 提出了一个用于多AI代理系统的建模框架，包含主机代理模型和任务生命周期模型，通过17个主机代理属性和14个任务生命周期属性来形式化验证系统行为。


<details>
  <summary>Details</summary>
Motivation: 当前代理间通信生态系统碎片化，存在语义鸿沟，阻碍系统属性严格分析，并引入架构错位和可被利用的协调问题等风险。

Method: 引入两个基础模型：主机代理模型（与用户交互、分解任务、协调执行）和任务生命周期模型（详细描述子任务状态转换），提供统一语义框架。

Result: 定义了31个形式化属性（17个主机代理属性+14个任务生命周期属性），分为活性、安全性、完整性和公平性类别，支持系统行为的形式化验证。

Conclusion: 这是首个严格基础、领域无关的框架，用于系统分析、设计和部署正确、可靠、鲁棒的多AI代理系统。

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [16] [The Gatekeeper Knows Enough](https://arxiv.org/abs/2510.14881)
*Fikresilase Wondmeneh Abebayew*

Main category: cs.AI

TL;DR: 提出了Gatekeeper协议来解决LLM作为自主代理时的上下文窗口限制和状态不同步问题，通过使用简化的潜在状态表示和按需请求高保真上下文的方法，显著提高了代理的可靠性和计算效率。


<details>
  <summary>Details</summary>
Motivation: LLM作为自主代理部署时面临上下文窗口限制和状态不同步问题，导致输出不可靠、行为不可预测和资源使用效率低下，特别是在与代码库和文档等结构化知识系统交互时。

Method: 引入Gatekeeper协议框架，要求代理先在简化的低保真潜在状态表示上进行操作和推理，然后按需策略性地请求高保真上下文。所有交互通过统一的JSON格式进行中介，作为声明式、状态同步的协议。

Result: 在软件开发场景中实施的Sage参考实现表明，该方法显著提高了代理可靠性，通过最小化token消耗改善了计算效率，并实现了与复杂系统的可扩展交互。

Conclusion: Gatekeeper协议为在任何结构化知识领域构建更稳健、可预测和基于现实的AI代理提供了基础方法学。

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents,
yet their practical utility is fundamentally constrained by a limited context
window and state desynchronization resulting from the LLMs' stateless nature
and inefficient context management. These limitations lead to unreliable
output, unpredictable behavior, and inefficient resource usage, particularly
when interacting with large, structured, and sensitive knowledge systems such
as codebases and documents. To address these challenges, we introduce the
Gatekeeper Protocol, a novel, domain-agnostic framework that governs
agent-system interactions. Our protocol mandates that the agent first operate
and reason on a minimalist, low-fidelity "latent state" representation of the
system to strategically request high-fidelity context on demand. All
interactions are mediated through a unified JSON format that serves as a
declarative, state-synchronized protocol, ensuring the agent's model of the
system remains verifiably grounded in the system's reality. We demonstrate the
efficacy of this protocol with Sage, a reference implementation of the
Gatekeeper Protocol for software development. Our results show that this
approach significantly increases agent reliability, improves computational
efficiency by minimizing token consumption, and enables scalable interaction
with complex systems, creating a foundational methodology for building more
robust, predictable, and grounded AI agents for any structured knowledge
domain.

</details>


### [17] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 提出轻量级多模态架构，融合传感器数据和视觉图像预测文化遗产地退化严重程度，在斯特拉斯堡大教堂数据上达到76.9%准确率，比标准多模态架构提升43%。


<details>
  <summary>Details</summary>
Motivation: 文化遗产地因气候变化加速退化，传统单模态监测方法无法捕捉环境压力与材料退化之间的复杂相互作用。

Method: 采用改进的PerceiverIO架构，包含简化编码器（64D潜在空间）和自适应Barlow Twins损失函数，鼓励模态互补性而非冗余。

Result: 模型准确率达76.9%，传感器单独使用为61.5%，图像单独使用为46.2%，证实了多模态协同效应。最佳相关系数目标τ=0.3时达到69.2%准确率。

Conclusion: 架构简单性与对比正则化相结合，可在数据稀缺的遗产监测环境中实现有效的多模态学习，为AI驱动的保护决策支持系统奠定基础。

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [18] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve是一个开源进化编码代理，结合大语言模型和遗传算法解决复杂计算问题，在数学基准测试中超越了Google DeepMind的AlphaEvolve。


<details>
  <summary>Details</summary>
Motivation: 将强大的进化概念应用于LLM领域，基于广义科学发现的最新方法，解决复杂计算问题。

Method: 采用基于岛屿的遗传算法保持种群多样性，引入新颖的基于启发的交叉机制利用LLM上下文窗口组合成功解决方案的特征，实现元提示策略动态探索解空间。

Result: 在用于评估AlphaEvolve的数学基准测试子集上，CodeEvolve在多个挑战性问题上的表现超越了AlphaEvolve。

Conclusion: CodeEvolve成功展示了结合LLM与进化算法的有效性，并通过开源框架促进合作与进展加速。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [19] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: 本文探讨了将强化学习与传统行为树结合用于游戏AI的可行性，使用AMD Schola插件在复杂3D环境中训练多任务NPC。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习研究进展迅速，但在商业视频游戏中的应用仍然缓慢。本文旨在解决游戏AI社区在使用RL驱动NPC时面临的常见挑战。

Method: 使用AMD Schola插件在虚幻引擎中训练RL代理，将强化学习与行为树结合，在受《最后生还者》启发的复杂3D环境中创建多任务NPC。

Result: 展示了联合训练RL模型与行为树的方法，并展示了各种技能，证明了这种方法的可行性。

Conclusion: 强化学习与行为树的结合是一个值得进一步探索的关键交汇点，虽然已有研究提出，但实际采用仍然罕见。

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [20] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA是一个联合嵌入模型，用于直接从临床对话中检索规范医嘱，支持显式查询和无查询模式，通过对比学习对齐异构意图表达，实现实时临床医嘱检索。


<details>
  <summary>Details</summary>
Motivation: 临床对话混合了显式指令和隐式推理，现有LLM重写方法存在延迟、不稳定和不透明问题，阻碍实时医嘱处理。

Method: 基于PubMedBERT初始化，使用重复安全对比目标进行微调，通过受限LLM指导将签署医嘱与补充表述绑定，训练双编码器模型。

Result: JEDA在实践中取得显著提升，大幅优于基础编码器和近期开源嵌入模型，提供快速、可解释、无LLM的检索层。

Conclusion: JEDA实现了将环境上下文与可操作临床医嘱实时链接，具有噪声鲁棒性和强泛化能力。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [21] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: ARM-FM是一个利用基础模型自动生成奖励机器的框架，通过自然语言规范实现强化学习的自动化、组合式奖励设计。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法对奖励函数规范高度敏感，这限制了其广泛应用。需要一种自动化、可组合的奖励设计方法来解决这一核心挑战。

Method: 使用基础模型从自然语言规范自动生成奖励机器；将语言嵌入与奖励机器自动机状态关联，实现跨任务泛化；利用奖励机器的结构化形式化实现有效的任务分解。

Result: 在多个挑战性环境中验证了ARM-FM的有效性，包括展示了零样本泛化能力。

Conclusion: ARM-FM框架通过结合基础模型和奖励机器，实现了从自然语言到强化学习目标的自动转换，为解决奖励函数设计难题提供了有效方案。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [22] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: 本文对2019-2024年精准医学中AI实施文献进行了范围综述，识别了数据质量、临床可靠性、工作流整合和治理等方面的关键障碍和促进因素，提出了支持可信和可持续实施的未来方向。


<details>
  <summary>Details</summary>
Motivation: 人工智能在精准医学中日益重要，能够整合和解释多模态数据，但在临床环境中的实施仍然有限，需要系统分析实施障碍和促进因素。

Method: 采用范围综述方法，分析2019-2024年相关文献，使用基于生态系统的框架分析多维度因素。

Result: 识别了数据质量、临床可靠性、工作流整合和治理四个关键维度的障碍和促进因素，揭示了影响现实世界转化的相互依赖关系。

Conclusion: 需要采用生态系统视角来支持精准医学中AI的可信和可持续实施，提出了促进临床转化的未来方向。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [23] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: 提出了在线骚扰代理基准，包含多轮骚扰对话数据集、多代理模拟、三种越狱攻击方法和混合评估框架。研究发现越狱调优使骚扰成功率大幅提升，闭源和开源模型表现出不同的攻击升级轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究主要关注单轮提示，而真实骚扰往往在多轮交互中展开，需要评估LLM代理在多轮交互中的安全性。

Method: 构建了包含多轮骚扰对话数据集、多代理模拟、三种越狱攻击方法（针对记忆、规划和微调）的混合评估框架，使用LLaMA-3.1-8B-Instruct和Gemini-2.0-flash进行测试。

Result: 越狱调优使骚扰成功率从57.25-64.19%提升至95.78-96.89%（Llama），从98.46%提升至99.33%（Gemini），拒绝率降至1-2%。侮辱和谩骂是最普遍的毒性行为，闭源模型表现出显著脆弱性。

Conclusion: 多轮和理论基础的攻击不仅成功率高，还能模拟人类骚扰动态，需要开发强大的安全防护措施来确保在线平台的安全和负责任。

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [24] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出了LiveResearchBench基准和DeepEval评估套件，用于系统评估深度研究系统的能力，涵盖100个专家策划的任务和全面的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估深度研究系统时存在不足，无法反映真实信息需求、动态性、明确性和多面性等关键原则。

Method: 基于四个关键原则构建LiveResearchBench基准，包含100个专家策划任务；开发DeepEval评估套件，整合四种互补评估协议。

Result: 对17个前沿深度研究系统进行全面评估，揭示了当前系统的优势、常见失败模式和关键系统组件需求。

Conclusion: LiveResearchBench和DeepEval为深度研究系统提供了严格的评估基础，识别了推进可靠深度研究所需的关键组件。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [25] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 提出Agentic Self-Learning (ASL)框架，通过生成式奖励模型和多角色协同进化实现无需人工标注数据的自主智能体学习，在搜索任务中展现出持续的性能提升和样本效率优势。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不依赖人工标注数据集或预定义规则奖励的情况下，实现基于LLM的智能体的规模化训练，探索开放领域智能体学习的关键因素。

Method: 提出ASL框架，包含提示生成器、策略模型和生成式奖励模型，形成任务生成、策略执行和评估的闭环强化学习系统，通过多角色协同进化实现自我改进。

Result: ASL在搜索任务中实现稳定持续的轮次性能提升，超越现有基线方法，在零标注数据条件下仍能持续改进，展现出优异的样本效率和鲁棒性。

Conclusion: 奖励来源和数据规模是开放领域智能体学习的关键因素，多角色协同进化是实现可扩展、自我改进智能体的有效方法，生成式奖励模型的验证能力是主要瓶颈。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [26] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: MorphoBench是一个评估大型模型推理能力的基准测试，通过多学科问题和自适应难度调整来全面评估模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试范围有限，缺乏根据模型推理能力发展而调整难度的灵活性，无法有效评估大规模推理模型的能力。

Method: 从现有基准和奥林匹克竞赛中收集复杂推理问题，利用模型推理过程中的关键陈述自适应修改分析难度，并使用仿真软件生成可动态调整难度的问题。

Result: 收集了1300多个测试问题，基于o3和GPT-5等模型的推理能力迭代调整了MorphoBench的难度。

Conclusion: MorphoBench提高了模型推理评估的全面性和有效性，为改进大型模型的推理能力和科学鲁棒性提供了可靠指导。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [27] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: GuardSpace是一个保护大语言模型在微调过程中安全对齐的框架，通过安全敏感子空间和有害抵抗零空间来防止安全行为退化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在微调过程中容易失去预训练的安全对齐能力，即使使用良性数据或低秩适配也会产生有害响应，需要保护安全机制。

Method: 使用协方差预处理的奇异值分解将预训练权重分解为安全相关和安全无关组件，初始化低秩适配器时冻结安全相关部分，并构建零空间投影器限制适配器更新对有害提示的响应。

Result: 在多个下游任务实验中，GuardSpace优于现有方法。对于Llama-2-7B-Chat在GSM8K上的微调，有害分数从14.4%降至3.6%，准确率从26.0%提升至28.0%。

Conclusion: GuardSpace能有效保护大语言模型在微调过程中的安全对齐，同时保持或提升任务性能。

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [28] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 提出了Terrarium框架，用于研究基于LLM的多智能体系统中的安全、隐私和安全问题，通过重新设计黑板架构来创建模块化、可配置的测试平台。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统能够自动化用户任务，但引入了新的风险，包括错位、恶意方攻击、智能体被破坏或用户数据被盗等问题。

Method: 重新利用早期的多智能体系统设计——黑板设计，创建模块化、可配置的测试平台，识别关键攻击向量，并在三个协作MAS场景中实现四种代表性攻击。

Result: 开发了Terrarium框架，提供了快速原型设计、评估和迭代防御与设计的工具，展示了框架的灵活性。

Conclusion: Terrarium旨在加速可信多智能体系统的进展，通过提供工具来快速原型设计、评估和迭代防御与设计。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [29] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC是一个元认知框架，为多智能体系统提供实时、无监督的步骤级错误检测和自我纠正，通过历史条件异常评分来防止错误传播。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在协作解决问题方面表现出色，但对级联错误很脆弱——单个错误步骤可能在智能体间传播并破坏整个轨迹。

Method: 采用两种互补设计：1）下一执行重建，从查询和交互历史预测下一步的嵌入来捕捉因果一致性；2）原型引导增强，学习正常步骤嵌入的原型先验，在稀疏上下文下稳定重建和异常评分。检测到异常时触发纠正智能体。

Result: 在Who&When基准测试中，MASC始终优于所有基线方法，步骤级错误检测AUC-ROC提升高达8.47%；在不同MAS框架中都能带来一致的端到端性能提升。

Conclusion: 元认知监控和针对性纠正能够以最小开销缓解错误传播，证实了该框架的有效性和通用性。

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [30] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出了AI4Service新范式，通过Alpha-Service框架实现主动式AI助手，能够从第一视角视频中检测服务机会并主动提供个性化服务。


<details>
  <summary>Details</summary>
Motivation: 现有AI服务大多是被动响应式，而真正智能的助手应该能够预测用户需求并在适当时机主动采取行动。

Method: 基于AI眼镜构建Alpha-Service统一框架，包含五个核心组件：输入单元（感知）、中央处理单元（任务调度）、算术逻辑单元（工具使用）、记忆单元（长期个性化）和输出单元（人机交互），采用多智能体系统实现。

Result: 通过实时21点顾问、博物馆导览和购物搭配助手等案例研究，展示了系统能够无缝感知环境、推断用户意图，并在无需明确提示的情况下提供及时有用的帮助。

Conclusion: AI4Service范式将AI从被动工具转变为主动伴侣，Alpha-Service框架为实现这一愿景提供了可行路径，在多个实际场景中证明了其有效性。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [31] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: IP-Merging方法通过识别多模态大语言模型和数学推理大语言模型中的关键推理参数，将其投影到MLLM的子空间进行融合，无需调优即可提升MLLMs的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在数学推理能力上落后于纯文本大语言模型，但现有模型融合方法忽视了MLLM与LLM之间的参数空间对齐问题，导致性能不佳。

Method: 提出IP-Merging方法：1）识别MLLM和数学LLM中的推理相关参数；2）将这些参数投影到MLLM的子空间；3）在子空间中进行参数融合。该方法无需调优直接调整参数。

Result: 大量实验表明，IP-Merging方法能够在不损害MLLMs其他能力的前提下，直接从数学LLMs中吸收数学推理能力，显著提升MLLMs的数学推理性能。

Conclusion: IP-Merging是一种有效的免调优方法，通过参数空间对齐和投影融合，成功解决了MLLMs从数学LLMs吸收推理能力时的参数空间差距问题。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [32] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent是一个可训练的分层视觉语言代理，用于移动设备控制，通过高层推理模型和低层动作模型的联合优化，在Android-in-the-Wild基准测试中达到87.9%的任务成功率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的移动设备控制方法大多依赖直接的状态到动作映射，缺乏结构化推理和规划，导致在新任务或未见UI布局上泛化能力差。

Method: 提出分层架构：高层推理模型和低层动作模型联合优化。将多步决策重新表述为单步子目标序列，并提出前瞻优势函数，利用低层模型的执行反馈指导高层优化，缓解长时任务中的路径爆炸问题。

Result: 在Android-in-the-Wild基准测试中达到87.9%的任务成功率，显著优于提示型(17.7%)、监督学习(54.5%)和强化学习方法(71.9%)。在ScreenSpot-v2基准测试中展示竞争性的零样本泛化能力，在AndroidWorld基准测试中随骨干网络增大而有效扩展。

Conclusion: Hi-Agent通过分层设计和联合优化策略，在移动设备控制任务中实现了最先进的性能，并展示了良好的泛化能力和可扩展性。

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [33] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: 提出IMAGINE框架，将多智能体系统的推理规划能力集成到单个紧凑模型中，显著超越MAS性能，在TravelPlanner基准上达到82.7%的最终通过率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理和规划任务中仍面临挑战，多智能体系统虽然能提供更好的集体推理，但存在推理成本高、响应延迟长和端到端训练困难等问题。

Method: 提出IMAGINE框架，通过端到端训练将多智能体系统的能力集成到单个模型中，使用Qwen3-8B-Instruct作为基础模型进行训练。

Result: 训练后的模型在TravelPlanner基准上达到82.7%的最终通过率，远超DeepSeek-R1-671B的40%，同时保持更小的模型规模。

Conclusion: IMAGINE框架成功地将多智能体系统的优势集成到单个紧凑模型中，显著提升了推理规划能力，同时解决了MAS的高成本和训练难题。

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [34] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文提出了一种转换方法，用于消除PDDL公理中派生谓词的负出现，证明这种限制实际上不会影响表达能力。


<details>
  <summary>Details</summary>
Motivation: PDDL标准限制公理体中谓词的负出现只能针对直接由动作设置的谓词，而非由公理派生的谓词。但文献中常偏离此限制，只要求公理集可分层。本文旨在证明这两种变体实际上表达能力相同。

Method: 提出了相应的转换方法，能够消除派生谓词的负出现，将包含负出现的公理转换为等价的、符合PDDL标准限制的形式。

Result: 证明了两种变体（PDDL标准限制和可分层公理）能够表达完全相同的查询，即最小不动点逻辑所表达的查询。

Conclusion: 负出现的派生谓词可以被消除，PDDL标准对公理体的限制实际上不会影响其表达能力，两种方法在逻辑表达能力上是等价的。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [35] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman是一个多智能体系统，通过模拟研发工作流自动合成联邦学习系统，包含交互式规划、模块化代码生成和自主评估三个阶段，在AgentFL-Bench基准测试中表现优于手工基线。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统设计复杂，需要解决数据异构性和系统约束等多方面挑战，导致解决方案脆弱且定制化，需要自动化方法来克服这一瓶颈。

Method: 提出Helmsman多智能体系统，包含三个协作阶段：交互式人机规划制定研究计划、监督智能体团队进行模块化代码生成、在沙盒模拟环境中进行闭环自主评估和优化。

Result: 在包含16个多样化任务的AgentFL-Bench基准测试中，Helmsman生成的解决方案与手工基线相当甚至更优。

Conclusion: 这项工作代表了向复杂去中心化AI系统自动化工程迈出的重要一步。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [36] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: JSPLIT是一个基于分类学的框架，通过层次化组织MCP工具并基于用户提示选择相关工具，有效解决提示膨胀问题，降低提示大小和成本，同时提升工具选择准确性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统发展，用户对超越简单文本交互的需求增加，LLM需要与外部工具交互。但工具数量增加导致提示膨胀，带来高令牌成本、延迟增加和任务成功率下降的问题。

Method: JSPLIT将工具组织成层次化分类学，使用用户提示基于查询和分类结构识别并仅包含最相关工具，包括分类设计、工具选择算法和评估数据集。

Result: JSPLIT显著减少提示大小而不显著影响代理响应能力。当可用工具数量大幅增加时，JSPLIT甚至能提高代理的工具选择准确性，在高度复杂代理环境中降低成本同时提升任务成功率。

Conclusion: JSPLIT框架有效解决了MCP工具使用中的提示膨胀问题，通过分类学驱动的方法在减少成本的同时保持甚至提升代理性能，适用于复杂代理环境。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [37] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 本文概述了神经符号AI中的推理捷径问题，讨论了其成因、后果及解决方法，旨在为开发可靠的神经符号AI模型提供统一视角。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI模型在概念未被直接监督时容易出现推理捷径问题，这会损害模型解释性、分布外性能及可靠性，但目前相关研究分散，需要系统梳理。

Method: 通过直观术语介绍推理捷径，回顾现有理论特征，详细分析处理推理捷径的方法（包括缓解和意识策略），并评估其优缺点。

Result: 提供了对推理捷径问题的统一理解框架，系统总结了现有解决方案及其局限性，降低了研究该问题的入门门槛。

Conclusion: 本文通过重新整理高级材料使其易于理解，为应对推理捷径问题提供了统一视角，有望促进可靠神经符号AI和可信AI模型的发展。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [38] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 本研究探讨了预训练LLM代理能否通过自我生成任务、积累知识和环境交互，实现开放式的自主规划和推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理能力的提升，研究者希望探索它们是否能成为具有自主规划能力和模糊目标推理能力的实体，而不仅仅是问题解决工具。

Method: 采用开放式实验设置，增强预训练LLM代理的能力，使其能够生成自己的任务、积累知识并与环境进行广泛交互。

Result: 该代理能够可靠地遵循复杂的多步骤指令、跨运行存储和重用信息、提出并解决自己的任务，但对提示设计敏感、容易重复生成任务、无法形成自我表征。

Conclusion: 研究展示了将预训练LLM适应开放式的潜力与当前局限，指出了未来在训练代理管理记忆、有效探索和追求抽象长期目标方面的方向。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [39] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 提出ColorBench图结构基准框架，通过模拟真实设备交互的有限状态实现动态行为的静态仿真，用于评估移动代理在复杂长视野任务中的多路径解决能力。


<details>
  <summary>Details</summary>
Motivation: 当前移动代理评估标准存在局限：离线静态基准只能验证单一预设路径，而在线动态测试受限于真实设备的复杂性和不可复现性，两者都无法全面评估代理能力。

Method: 开发基于图结构的基准框架，建模真实设备交互中的有限状态，实现动态行为的静态仿真。构建ColorBench基准，包含175个任务（74个单应用、101个跨应用），平均长度超过13步，每个任务包含至少两条正确路径和典型错误路径。

Result: 通过在各种基线模型上评估ColorBench，发现了现有模型的局限性，并基于实验结果提出了改进方向和可行的技术路径。

Conclusion: ColorBench框架能够有效评估移动代理在复杂长视野任务中的多路径解决能力，为提升代理性能提供了评估基准和改进方向。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [40] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: 本文提出了Rose-Frame框架，用于诊断人机交互中的认知和认识论漂移，通过三个维度（地图vs领土、直觉vs理性、冲突vs确认）来识别和缓解LLM的局限性，强调将AI对齐重新定义为认知治理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然流畅且具有情感共鸣，但基于统计预测而非有根据的推理，存在幻觉风险。它们实现了系统1认知的规模化运作，快速、联想且具有说服力，但缺乏反思或证伪能力。

Method: 引入Rose-Frame三维框架：地图vs领土（区分现实表征与现实本身）、直觉vs理性（区分快速情感判断与缓慢反思思维）、冲突vs确认（检验思想是通过批判性测试还是简单相互验证）。

Result: Rose-Frame不试图通过更多数据或规则来修复LLM，而是提供一个反思工具，使模型的局限性和用户的假设变得可见，从而实现更透明和批判性意识强的AI部署。

Conclusion: 将AI对齐重新定义为认知治理：无论是人类还是人工智能的直觉，都必须受到人类理性的治理。只有通过嵌入反思性、可证伪的监督，我们才能使机器的流畅性与人类的理解保持一致。

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [41] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 本文对2021-2025年荷兰公共卫生机器学习研究中的算法偏见进行了系统文献回顾，开发了RABAT评估工具，发现普遍存在公平性框架缺失等问题，并提出了ACAR四阶段框架来帮助研究人员在ML生命周期中解决公平性问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习在公共卫生领域具有革命性潜力，但如果不系统关注算法偏见，可能会无意中加剧现有的健康不平等。

Method: 开发了RABAT评估工具，整合了Cochrane偏见风险、PROBAST和微软负责任AI检查表的元素，应用于35篇同行评审研究进行系统分析。

Result: 分析显示普遍存在差距：虽然数据采样和缺失数据实践记录良好，但大多数研究忽略了明确的公平性框架、亚组分析和对潜在危害的透明讨论。

Conclusion: 提出了ACAR四阶段公平性导向框架，为公共卫生ML从业者提供可操作建议，确保算法创新促进健康公平而非损害它。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [42] [TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence](https://arxiv.org/abs/2510.14670)
*Marco Simoni,Aleksandar Fontana,Andrea Saracino,Paolo Mori*

Main category: cs.AI

TL;DR: TITAN是一个将自然语言网络安全威胁查询与结构化知识图谱上的可执行推理连接起来的框架，包含路径规划器和图执行器，支持在威胁、行为和防御之间进行双向推理。


<details>
  <summary>Details</summary>
Motivation: 传统检索系统无法在网络安全威胁、行为和防御之间进行清晰可逆的推理，需要一种能够将自然语言查询与结构化知识图谱推理相结合的系统。

Method: 集成路径规划器模型预测文本中的逻辑关系链，图执行器遍历TITAN本体图检索事实答案和证据，基于MITRE构建类型化双向图。

Result: 创建了包含88209个示例的TITAN数据集，实证评估显示TITAN能生成语法有效且语义连贯的可执行推理路径。

Conclusion: TITAN框架成功实现了自然语言威胁查询与结构化知识图谱推理的有效集成，支持确定性的图执行推理。

Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that
connects natural-language cyber threat queries with executable reasoning over a
structured knowledge graph. It integrates a path planner model, which predicts
logical relation chains from text, and a graph executor that traverses the
TITAN Ontology to retrieve factual answers and supporting evidence. Unlike
traditional retrieval systems, TITAN operates on a typed, bidirectional graph
derived from MITRE, allowing reasoning to move clearly and reversibly between
threats, behaviors, and defenses. To support training and evaluation, we
introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:
13951) pairing natural language questions with executable reasoning paths and
step by step Chain of Thought explanations. Empirical evaluations show that
TITAN enables models to generate syntactically valid and semantically coherent
reasoning paths that can be deterministically executed on the underlying graph.

</details>


### [43] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: NAEL是一个基于主动推理和符号推理的非人类中心主义人工智能伦理框架，将伦理行为形式化为智能系统在动态多智能体环境中最小化全局期望自由能量的涌现属性。


<details>
  <summary>Details</summary>
Motivation: 解决现有伦理模型局限于人类中心主义的问题，让智能体能够发展上下文敏感、自适应和关系性的伦理行为，而无需预设人类道德直觉。

Method: 提出神经符号架构，使智能体能够在不确定环境中评估其行为的伦理后果，通过最小化全局期望自由能量来形式化伦理行为。

Result: 通过伦理资源分配的案例研究展示了NAEL能够动态平衡自我保存、认知学习和集体福利。

Conclusion: NAEL为人工智能伦理提供了一个非人类中心主义的新范式，能够产生适应性的伦理行为，克服了传统人类中心伦理模型的局限性。

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [44] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: COUP算法配置方法的实践性能改进，使其在保持理论保证的同时能与启发式配置方法竞争，并通过案例研究展示了算法选择对效用函数变化的鲁棒性分析。


<details>
  <summary>Details</summary>
Motivation: COUP作为一种基于效用的算法配置方法，主要关注理论保证而忽视了实际性能。本文旨在弥补这一差距，使基于理论的效用算法配置方法能够在实践中与广泛使用的启发式配置方法竞争。

Method: 提出了一系列对COUP的改进措施，这些改进在不降低理论保证的前提下提升了其经验性能，并通过实验验证了这些改进的效果。

Result: 改进后的COUP在保持理论保证的同时，其经验性能得到了显著提升，能够与广泛使用的启发式配置方法竞争。

Conclusion: 通过改进COUP，成功地将基于理论的效用算法配置方法提升到了实用水平，同时展示了如何分析算法选择对效用函数变化的鲁棒性。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [45] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 提出了PAVE方法，通过知识感知子空间净化任务向量，消除任务无关冗余，提升模型合并性能


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法中任务向量存在任务无关冗余，导致合并模型性能下降，而现有去冗余方法随机且缺乏知识感知

Method: 使用训练样本获取协方差矩阵，进行上下文导向的奇异值分解，在知识感知子空间分离任务相关和冗余组件，并引入谱秩分配策略

Result: PAVE作为即插即用方案可应用于多种任务向量合并方法，在多样化任务和模型架构上验证了有效性

Conclusion: PAVE方法能有效净化任务向量，提升模型合并性能，适用于各种基于任务向量的合并方法

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [46] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: CoAST框架通过自然语言接口整合世界知识、时空轨迹模式和用户信息，使用持续预训练和认知对齐来提升下一个兴趣点推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型缺乏对结构化地理实体和移动模式的理解，且工业级POI推荐需要整合季节、天气、假期等世界知识和用户画像来提升体验。

Method: CoAST包含两个阶段：(1)在脱敏用户时空轨迹数据上进行持续预训练获取推荐知识；(2)通过监督微调和强化学习实现认知对齐，使模型判断与人类偏好一致。

Result: 在多个真实数据集上的离线实验和在AMAP App首页“猜你想去”的在线实验证明了CoAST的有效性。

Conclusion: CoAST框架成功解决了LLMs在POI推荐中的地理理解和认知对齐问题，显著提升了推荐性能。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [47] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 提出了一个结合细粒度波束搜索和过程奖励模型ToolPRM的推理扩展框架，用于提升大语言模型在函数调用等结构化输出任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理扩展研究主要关注非结构化输出生成任务，而在函数调用等结构化输出领域的应用研究不足，需要填补这一空白。

Method: 构建首个细粒度调用内过程监督数据集，使用函数掩码技术自动标注步骤级奖励，训练ToolPRM过程奖励模型来评分函数调用的内部步骤。

Result: ToolPRM在预测准确性上优于粗粒度和结果奖励模型，结合推理扩展技术显著提升了骨干模型在各种函数调用任务和基准测试中的性能。

Conclusion: 揭示了将推理扩展技术应用于结构化输出的关键原则：由于结构化函数调用生成的不可恢复特性，需要"更多探索但更少保留"。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [48] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: 本文分析了强化学习与可验证奖励（RLVR）方法中存在的系统性偏差问题，即过度利用而非探索，导致pass@1性能提升但pass@K（K>1）性能下降。作者提出了Simple Pass@K Optimization (SimKO)方法来缓解概率过度集中问题，鼓励探索。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在系统性偏差，倾向于过度利用而非探索，表现为pass@1性能提升但pass@K（K>1）性能下降。作者希望通过分析训练动态来理解这一问题，并开发解决方案。

Method: 通过跟踪词汇候选的token级概率分布来分析RLVR训练动态，发现概率集中效应。提出SimKO方法，采用不对称设计：对已验证正确的响应，提升top-K候选的概率；对已验证错误的响应，对top-1候选施加更强惩罚。

Result: SimKO在多个数学和逻辑推理基准测试中，对各种K值都能一致地获得更高的pass@K性能，有效改善了RLVR的探索能力。

Conclusion: SimKO通过缓解概率过度集中问题，有效鼓励了探索行为，为改进RLVR的探索能力提供了一种简单有效的方法。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [49] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: Datalake Agent是一个代理系统，通过交互式循环减少NL2SQL任务中的元信息使用，显著降低LLM的token消耗和成本，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统NL2SQL方法需要处理大量数据库元信息，导致提示过长、token消耗大、处理成本高。

Method: 采用代理系统设计，使用交互式循环和推理框架，让LLM选择性请求必要信息来解决表格问答任务。

Result: 在23个数据库和100个表格问答任务上评估，Datalake Agent将LLM使用的token减少高达87%，成本大幅降低。

Conclusion: Datalake Agent通过减少元信息使用，在保持性能的同时显著降低了NL2SQL任务的成本。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [50] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 提出了RoboGPT-R1，一个两阶段微调框架，通过监督训练和强化学习提升具身智能体的推理能力，在EmbodiedBench基准上显著优于GPT-4o-mini和其他模型。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型和视觉语言模型在复杂真实世界环境中执行长视距操作任务时推理能力受限的问题，特别是缺乏常识和物理理解能力。

Method: 采用两阶段微调框架：监督训练从专家序列获取基础知识，然后使用强化学习解决模型在视觉空间理解和推理方面的不足。设计了基于规则的奖励函数来确保物理理解和动作序列一致性。

Result: 在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准上比GPT-4o-mini高出21.33%，比在Qwen2.5-VL-7B上训练的其他工作高出20.33%。

Conclusion: RoboGPT-R1框架通过结合监督训练和强化学习，有效提升了具身智能体的推理能力，在长视距操作任务中表现出色。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [51] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: 提出了Instruction Boosting方法，通过后生成处理提高LLM对提示指令的遵循率，在2条指令时提升7个百分点，10条指令时提升4个百分点。


<details>
  <summary>Details</summary>
Motivation: 开发者通常通过修改提示来影响LLM行为，但仅添加更多指令无法保证它们被实际遵循。

Method: 引入Instruction Boosting作为后生成方法，并创建SCALEDIF基准测试，包含最多10条指令的数据样本。

Result: 指令遵循率显著提升，同时发现性能随指令数量增加而下降的趋势与指令间的冲突程度相关。

Conclusion: 开发了定量冲突评分工具来解释性能趋势，并为开发者提供关于额外提示指令对模型性能影响的反馈。

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


### [52] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: 本文提出了一个紧凑的形式理论来描述和测量由领域先验引导的LLM辅助迭代搜索，通过模糊关系算子表示智能体，引入覆盖生成函数来量化可达性难度，并提供了可测试的推断。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的生成-过滤-精炼迭代范式在推理、编程和科学发现中取得进展，但搜索效果取决于如何将领域先验编码为操作结构化的假设空间。

Method: 将智能体表示为输入输出的模糊关系算子，通过单一延续参数加权所有可达路径得到覆盖生成函数，从而测量可达性难度并提供搜索的几何解释。

Result: 通过多数投票实例验证了最简单的可测试推断，该理论提供了可操作的语言和工具来测量智能体及其搜索空间。

Conclusion: 该理论为LLM构建的迭代搜索提供了系统的形式化描述框架，能够有效测量智能体和搜索空间。

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [53] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong,Zaixi Zhang,Xiaotong Wang,Yin Di,Ruofan Jin,Michal Gerasimiuk,Yinkai Wang,Ravi K. Dinesh,David Smerkous,Alex Smerkous,Xuekun Wu,Shilong Liu,Peishan Li,Yi Zhu,Simran Serrao,Ning Zhao,Imran A. Mohammad,John B. Sunwoo,Joseph C. Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: LabOS是首个将计算推理与物理实验结合的AI共科学家系统，通过多模态感知、自进化代理和XR技术实现人机协作，将实验室转变为智能协作环境。


<details>
  <summary>Details</summary>
Motivation: 现代科学发展需要思想与行动的结合，当前AI主要局限于计算设计，缺乏与物理实验的实时交互能力。

Method: 通过连接多模型AI代理、智能眼镜和人机协作，使AI能够看到科学家所见、理解实验背景并实时协助执行。

Result: 在癌症免疫治疗靶点发现和干细胞工程等应用中，LabOS证明AI能够超越计算设计，直接参与实验过程。

Conclusion: LabOS将实验室转变为智能协作环境，实现了人类与机器发现的共同进化。

Abstract: Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.

</details>


### [54] [Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates](https://arxiv.org/abs/2510.14900)
*Wen-Kwang Tsao,Yao-Ching Yu,Chien-Ming Huang*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的智能体，能够在没有标注数据或模型权重更新的情况下自我改进，通过生成针对性网络搜索查询来收集外部证据，从而提高企业日志数据模式映射的准确性。


<details>
  <summary>Details</summary>
Motivation: 企业智能平台需要整合来自多个第三方供应商的日志数据，但供应商文档在测试时往往不可用、不匹配、格式混乱或不完整，这使得模式映射变得困难。

Method: 使用强化学习代理：1）识别模糊的字段映射尝试；2）生成针对性网络搜索查询以收集外部证据；3）应用基于置信度的奖励来迭代优化映射结果。

Result: 在将Microsoft Defender for Endpoint日志转换为通用模式的任务中，该方法将映射准确率从56.4%(仅使用LLM)提升到72.73%(使用RAG)，最终达到93.94%(经过100次迭代使用GPT-4o)。同时，需要专家审查的低置信度映射减少了85%。

Conclusion: 这种新方法提供了一种基于证据的透明解决方案，为未来行业问题提供了更稳健、可问责、可扩展、高效、灵活、适应性强和协作的解决途径。

Abstract: The Enterprise Intelligence Platform must integrate logs from numerous
third-party vendors in order to perform various downstream tasks. However,
vendor documentation is often unavailable at test time. It is either misplaced,
mismatched, poorly formatted, or incomplete, which makes schema mapping
challenging. We introduce a reinforcement learning agent that can self-improve
without labeled examples or model weight updates. During inference, the agent:
1) Identifies ambiguous field-mapping attempts. 2) Generates targeted
web-search queries to gather external evidence. 3) Applies a confidence-based
reward to iteratively refine its mappings. To demonstrate this concept, we
converted Microsoft Defender for Endpoint logs into a common schema. Our method
increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\%
over 100 iterations using GPT-4o. At the same time, it reduced the number of
low-confidence mappings requiring expert review by 85\%. This new approach
provides an evidence-driven, transparent method for solving future industry
problems, paving the way for more robust, accountable, scalable, efficient,
flexible, adaptable, and collaborative solutions.

</details>


### [55] [Budget-aware Test-time Scaling via Discriminative Verification](https://arxiv.org/abs/2510.14913)
*Kyle Montgomery,Sijun Tan,Yuqi Chen,Siyuan Zhuang,Tianjun Zhang,Raluca Ada Popa,Chenguang Wang*

Main category: cs.AI

TL;DR: 提出了一种结合判别式验证器和自一致性的混合方法，在固定计算预算下显著优于生成式验证，在AIME2025上准确率提升达15.3%。


<details>
  <summary>Details</summary>
Motivation: 现有生成式验证器虽然性能强大，但计算成本过高，限制了实际应用。需要寻找更高效的测试时扩展策略。

Method: 采用判别式验证器与自一致性结合的混合方法，在固定计算预算下进行预算感知的测试时扩展。

Result: 在固定计算预算下，混合方法比最先进的生成式验证准确率显著提升，在AIME2025上达到15.3%的准确率提升。

Conclusion: 对于实际应用，基于判别式验证器的预算感知扩展不仅是自一致性的"免费"升级，也是比昂贵生成式技术更有效和高效的替代方案。

Abstract: Test-time scaling is a powerful strategy for boosting the performance of
large language models on complex reasoning tasks. While state-of-the-art
approaches often employ generative verifiers to select the best solution from a
pool of candidates, this method incurs prohibitive computational costs,
limiting its practicality. In this work, we shift the focus to a more
budget-aware paradigm: discriminative verification. We conduct a thorough
empirical analysis and demonstrate that while discriminative verifiers may
underperform in isolation, combining them with self-consistency in a hybrid
approach creates a powerful and efficient test-time scaling mechanism. Notably,
under a fixed compute budget, this hybrid approach surpasses state-of-the-art
generative verification by a significant margin: achieving up to 15.3\% higher
accuracy on AIME2025. Our findings establish that for practical, real-world
applications, budget-aware scaling with discriminative verifiers is not only a
"free" upgrade over self-consistency, but also a more effective and efficient
alternative to costly generative techniques. Code is available at
https://github.com/wang-research-lab/verification.

</details>


### [56] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi,Eleonora Mancini,Paolo Torroni*

Main category: cs.AI

TL;DR: 本文系统研究了多模态抑郁症检测，比较了EEG、语音和文本三种模态，发现三模态组合能提升检测性能，预训练嵌入优于手工特征，精心设计的三模态模型达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 抑郁症检测面临挑战，现有研究范围有限、缺乏系统特征比较、评估协议不一致。本文旨在填补这些空白，系统探索特征表示和建模策略。

Method: 系统评估手工特征与预训练嵌入，比较不同神经编码器，分析单模态、双模态和三模态配置，研究融合策略，特别关注EEG的作用，使用一致的受试者独立分割确保可重复性。

Result: EEG、语音和文本三模态组合增强多模态检测性能；预训练嵌入优于手工特征；精心设计的三模态模型达到最先进水平。

Conclusion: 本研究为未来多模态抑郁症检测研究奠定了基础，证明了多模态融合的有效性和预训练方法的优势。

Abstract: Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.

</details>


### [57] [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](https://arxiv.org/abs/2510.14925)
*Akira Okutomi*

Main category: cs.AI

TL;DR: 将康德的《纯粹理性批判》重新解释为反馈稳定性理论，提出复合不稳定性指数(H-Risk)来评估推理系统的稳定性，发现高H-Risk预测过度自信错误，并在大语言模型中验证了脆弱内部动态与错误校准和幻觉的关联。


<details>
  <summary>Details</summary>
Motivation: 将康德的理性自我限制理论与反馈控制理论联系起来，为诊断和减少推理系统中的过度自信提供原则性框架。

Method: 提出复合不稳定性指数H-Risk，结合频谱裕度、条件数、时间敏感性和创新放大等指标，在线性高斯模拟和大型语言模型中进行验证。

Result: 在模拟中，高H-Risk预测过度自信错误；在LLMs中，脆弱内部动态与错误校准和幻觉相关，批判式提示对校准和幻觉效果不一。

Conclusion: 建立了康德自我限制与反馈控制之间的结构桥梁，为诊断和选择性减少推理系统中的过度自信提供了原则性视角。

Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback
stability, viewing reason as a regulator that keeps inference within the bounds
of possible experience. We formalize this intuition via a composite instability
index (H-Risk) combining spectral margin, conditioning, temporal sensitivity,
and innovation amplification. In linear-Gaussian simulations, higher H-Risk
predicts overconfident errors even under formal stability, revealing a gap
between nominal and epistemic stability. Extending to large language models
(LLMs), we find that fragile internal dynamics correlate with miscalibration
and hallucination, while critique-style prompts show mixed effects on
calibration and hallucination. These results suggest a structural bridge
between Kantian self-limitation and feedback control, offering a principled
lens for diagnosing -- and selectively reducing -- overconfidence in reasoning
systems. This is a preliminary version; supplementary experiments and broader
replication will be reported in a future revision.

</details>


### [58] [GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning](https://arxiv.org/abs/2510.14942)
*Yao Zhang,Yu Wu,Haowei Zhang,Weiguo Li,Haokun Chen,Jingpei Wu,Guohao Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: GroundedPRM是一个基于树引导和保真度感知的自动过程监督框架，通过蒙特卡洛树搜索构建结构化推理路径，使用外部工具验证中间步骤，融合工具验证和MCTS反馈的混合奖励机制，在少量自动标注数据上实现高性能过程推理。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型面临三大挑战：噪声奖励、低事实保真度以及与步骤级推理目标不对齐。传统方法依赖昂贵的人工标注、容易产生幻觉的LLM自评估，或仅从最终结果推断步骤质量的蒙特卡洛估计，导致信用分配错误。

Method: 1. 使用蒙特卡洛树搜索构建结构化推理路径以减少奖励噪声；2. 通过外部工具验证每个中间步骤，提供执行基础的正确性信号；3. 设计混合奖励聚合机制，融合工具验证和MCTS反馈；4. 将奖励信号格式化为理由增强的生成结构。

Result: 仅使用4万自动标注样本（最佳性能PRM所需数据的10%），在ProcessBench上实现高达26%的相对性能提升。在奖励引导的贪婪搜索中，甚至优于使用人工标注监督训练的PRM。

Conclusion: GroundedPRM为高质量过程级推理提供了一条可扩展且可验证的路径，通过树引导和保真度感知的方法有效解决了现有PRM的局限性。

Abstract: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large
Language Models (LLMs) by supervising intermediate steps and identifying
errors. However, building effective PRMs remains challenging due to the lack of
scalable, high-quality annotations. Existing approaches rely on costly human
labeling, LLM-based self-evaluation that is prone to hallucination, or Monte
Carlo (MC) estimation, which infers step quality solely from rollout outcomes
and often introduces noisy, misaligned supervision due to credit
misattribution. These issues result in three core limitations: noisy rewards,
low factual fidelity, and misalignment with step-level reasoning objectives. To
address these challenges, we introduce GroundedPRM, a tree-guided and
fidelity-aware framework for automatic process supervision. To reduce reward
noise and enable fine-grained credit assignment, we construct structured
reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated
supervision, we validate each intermediate step using an external tool,
providing execution-grounded correctness signals. To combine both step-level
validation and global outcome assessment, we design a hybrid reward aggregation
mechanism that fuses tool-based verification with MCTS-derived feedback.
Finally, we format the reward signal into a rationale-enhanced, generative
structure to promote interpretability and compatibility with instruction-tuned
LLMs. GroundedPRM is trained on only 40K automatically labeled samples,
amounting to just 10% of the data used by the best-performing PRM trained with
auto-labeled supervision. Nevertheless, it achieves up to a 26% relative
improvement in average performance on ProcessBench. When used for reward-guided
greedy search, GroundedPRM outperforms even PRMs trained with human-labeled
supervision, offering a scalable and verifiable path toward high-quality
process-level reasoning.

</details>


### [59] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: 该研究探讨大型语言模型是否能够进行组合式机器设计，通过在Besiege游戏基础上构建的BesiegeField测试平台，评估LLMs在空间推理、策略组装和指令遵循方面的能力，并探索通过强化学习改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大型语言模型是否能够学习创造性任务，特别是复杂机器的设计，这既是人类智能的标志，也是工程实践的基础。

Method: 方法包括：1）构建BesiegeField测试平台，支持基于部件的构建、物理模拟和奖励驱动评估；2）基准测试最先进的LLMs及其代理工作流程；3）通过强化学习进行微调实验。

Result: 研究发现当前开源模型在组合式机器设计任务上表现不足，特别是在空间推理、策略组装和指令遵循等关键能力方面存在短板。

Conclusion: 结论是机器设计任务需要结合语言理解、物理推理和空间能力，强化学习为改进模型性能提供了有前景的路径，但在语言、机器设计和物理推理的交叉领域仍存在开放挑战。

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [60] [Information flow in multilayer perceptrons: an in-depth analysis](https://arxiv.org/abs/2510.13846)
*Giuliano Armano*

Main category: cs.IT

TL;DR: 本文从信息论角度分析多层感知机中的信息流动，提出了信息矩阵的概念，并研究了监督学习中的优化策略和信息处理机制。


<details>
  <summary>Details</summary>
Motivation: 分析多层感知机各层之间信息流动的方式对理解人工神经网络至关重要，特别是在监督学习背景下研究信息处理过程。

Method: 从信息论角度构建问题框架，提出信息矩阵概念作为形式化框架，用于理解优化策略的病因学和研究信息流动。

Result: 研究产生了三个关键成果：参数化优化策略的定义、发现信息瓶颈框架中的优化策略与信息矩阵推导的策略具有强相似性、以及多层感知机作为"适配器"处理输入以达成给定目标的洞察。

Conclusion: 多层感知机在监督学习中起到信息适配器的作用，信息矩阵为理解优化策略和信息流动提供了有效的理论框架。

Abstract: Analysing how information flows along the layers of a multilayer perceptron
is a topic of paramount importance in the field of artificial neural networks.
After framing the problem from the point of view of information theory, in this
position article a specific investigation is conducted on the way information
is processed, with particular reference to the requirements imposed by
supervised learning. To this end, the concept of information matrix is devised
and then used as formal framework for understanding the aetiology of
optimisation strategies and for studying the information flow. The underlying
research for this article has also produced several key outcomes: i) the
definition of a parametric optimisation strategy, ii) the finding that the
optimisation strategy proposed in the information bottleneck framework shares
strong similarities with the one derived from the information matrix, and iii)
the insight that a multilayer perceptron serves as a kind of "adaptor", meant
to process the input according to the given objective.

</details>


### [61] [Structure-Preserving Error-Correcting Codes for Polynomial Frames](https://arxiv.org/abs/2510.13882)
*Baigang Chen,Dongfang Zhao*

Main category: cs.IT

TL;DR: 提出了一种结构保持的可靠性层，在多项式环中操作，通过添加少量系统冗余来纠正符号错误/标记擦除，无需往返或格式转换。


<details>
  <summary>Details</summary>
Motivation: 现代FFT/NTT分析、编码计算和隐私保护ML接口经常在NIC、存储和加速器之间传输多项式帧。即使罕见的静默数据损坏也可能翻转几个环系数并在下游算术中级联。传统防御方法不匹配当前低延迟流水线。

Method: 构建了两个互补方案：针对奇数长度使用Hensel提升的BCH理想与幂等编码器；针对2的幂长度使用重根负循环码与导数式解码。通过环自同构提供原位交织以分散突发错误。

Result: 在四个帧大小N=1024,2048,4096,8192上，在符号错误率10^-6-10^-5时达到每帧故障目标10^-9，t=8-9，仅产生0.20%-1.56%开销，在交织后能容忍约32-72字节未知错误突发（标记为擦除时约加倍）。

Conclusion: 通过将纠错与环语义对齐，从代数编码角度为多项式帧计算的可部署鲁棒性迈出了实际步骤。

Abstract: Modern FFT/NTT analytics, coded computation, and privacy-preserving ML
interface routinely move polynomial frames across NICs, storage, and
accelerators. However, even rare silent data corruption (SDC) can flip a few
ring coefficients and cascade through downstream arithmetic. Conventional
defenses are ill-matched to current low-latency pipelines:
detect-and-retransmit adds RTTs, while byte-stream ECC ignores the algebraic
structure and forces format conversions. To that end, we propose a
structure-preserving reliability layer that operates in the encoded data's
original polynomial ring, adds a small amount of systematic redundancy, and
corrects symbol errors/flagged erasures without round-trip or format changes.
We construct two complementary schemes: one for odd length $N_{odd}$ via a
Hensel-lifted BCH ideal with an idempotent encoder, and one for power-of-two
length $N_{2^m}$ via a repeated-root negacyclic code with derivative-style
decoding. In particular, to stay robust against clustered errors, a ring
automorphism provides in-place interleaving to disperse bursts. Implementation
wise, on four frame sizes $N\!=\!1024, 2048, 4096, 8192$, we meet a per-frame
failure target of $10^{-9}$ at symbol error rates $10^{-6}\text{--}10^{-5}$
with $t\!=\!8\text{--}9$, incurring only $0.20\%\text{--}1.56\%$ overhead and
tolerating $\sim\!32\text{--}72$\,B unknown-error bursts (roughly doubled when
flagged as erasures) after interleaving. By aligning error correction with ring
semantics, we take a practical step toward deployable robustness for
polynomial-frame computations from an algebraic coding perspective.

</details>


### [62] [Location-Aided Distributed Beamforming for Near-Field Communications with Element-Wise RIS](https://arxiv.org/abs/2510.14226)
*Xiao Zheng,Wenchi Cheng,Jingqing Wang,Zhuohui Yao,Jiangzhou Wang*

Main category: cs.IT

TL;DR: 提出了一种新的元素级RIS架构和分布式位置辅助传输方案，用于解决零功率有源RIS在CSI受限近场通信中的信道估计困难和离散相位约束问题。


<details>
  <summary>Details</summary>
Motivation: 现有工作忽视了RIS辅助系统中信道估计的固有困难以及实际部署中的离散相位约束，特别是在零功率有源RIS增强部署灵活性但面临CSI获取挑战的背景下。

Method: 设计了新的元素级RIS架构，基于菲涅尔衍射理论构建空间位置到波相位分布的映射，提出分布式波束成形设计和确定-对齐相位方法，免除基站对RIS相关信道估计的需求。

Result: 渐近分析表明当RIS规模较大时，该方案能以固定比例的反射元素实现最优增益，仿真验证了其相对于其他协议的优越性。

Conclusion: 所提出的分布式位置辅助传输方案有效解决了零功率有源RIS在近场通信中的信道估计难题，同时降低了估计开销和硬件复杂度。

Abstract: Active reconfigurable intelligent surface (RIS) emerges as an effective
technique to resist the double-fading attenuation of passive RIS. By embedding
with power harvesting function, it further evolves to zero-power active RIS,
which can effectively enhance the flexibility of RIS deployment without
external power demand. Nevertheless, existing works neglected the inherent
difficulty of channel estimation (CE) for RIS-assisted systems, and the
discrete phase shift constraint in practical deployment. In this paper we
design a new element-wise RIS architecture and propose a distributed
location-aided transmission scheme with low complexity to enhance the reflected
gain for channel state information (CSI)-limited RIS-assisted near-field
communications. Specifically, the new element-wise RIS provides dynamic element
selection capability with low hardware resources. Based on Fresnel diffraction
theory, we construct the mapping from locations in space-domain to phase
distributions of waves in phase-domain and reveal the priority of elements for
harvesting and reflecting. {Then, the distributed beamforming design with the
phase of determine-then-align is proposed, where the estimation overhead
reduction stems from exempted requirements of RIS-associated CE at base station
(BS).} The asymptotic analysis indicates that the proposed scheme can achieve
the optimal gain with a fixed proportion of reflective elements when RIS is
large, followed by simulations to verify its superiority to other protocols.

</details>


### [63] [Spatial Computing Communications for Multi-User Virtual Reality in Distributed Mobile Edge Computing Network](https://arxiv.org/abs/2510.14243)
*Caolu Xu,Zhiyong Chen,Meixia Tao,Li Song,Wenjun Zhang*

Main category: cs.IT

TL;DR: 提出空间计算通信(SCC)框架，通过多目标组合优化解决多用户VR在分布式移动边缘计算网络中的延迟和能耗问题，使用MO-CMPO算法生成帕累托最优解。


<details>
  <summary>Details</summary>
Motivation: 沉浸式VR应用对延迟、能效和计算资源有严格要求，特别是在多用户交互场景中，需要解决分布式移动边缘计算网络中的资源部署挑战。

Method: 提出SCC框架联合表示物理空间和虚拟空间，使用概率模型描述用户动态和资源需求。将资源部署建模为多目标组合优化问题，提出MO-CMPO算法结合监督学习和强化学习微调，利用稀疏图神经网络生成帕累托最优解。

Result: 在真实世界新无线电基站数据集上的仿真显示，MO-CMPO在超体积性能上优于基线方法，推理延迟显著降低。延迟导向的解决方案偏好本地MEC执行以减少传输延迟，而能耗导向的解决方案最小化冗余部署以节省能量。

Conclusion: SCC框架和MO-CMPO算法有效解决了多用户VR在分布式MEC网络中的延迟和能耗优化问题，为实际部署提供了实用的解决方案模式。

Abstract: Immersive virtual reality (VR) applications impose stringent requirements on
latency, energy efficiency, and computational resources, particularly in
multi-user interactive scenarios. To address these challenges, we introduce the
concept of spatial computing communications (SCC), a framework designed to meet
the latency and energy demands of multi-user VR over distributed mobile edge
computing (MEC) networks. SCC jointly represents the physical space, defined by
users and base stations, and the virtual space, representing shared immersive
environments, using a probabilistic model of user dynamics and resource
requirements. The resource deployment task is then formulated as a
multi-objective combinatorial optimization (MOCO) problem that simultaneously
minimizes system latency and energy consumption across distributed MEC
resources. To solve this problem, we propose MO-CMPO, a multi-objective
consistency model with policy optimization that integrates supervised learning
and reinforcement learning (RL) fine-tuning guided by preference weights.
Leveraging a sparse graph neural network (GNN), MO-CMPO efficiently generates
Pareto-optimal solutions. Simulations with real-world New Radio base station
datasets demonstrate that MO-CMPO achieves superior hypervolume performance and
significantly lower inference latency than baseline methods. Furthermore, the
analysis reveals practical deployment patterns: latency-oriented solutions
favor local MEC execution to reduce transmission delay, while energy-oriented
solutions minimize redundant placements to save energy.

</details>


### [64] [Reconfigurable Intelligent Surface-Enabled Channel Signature Modulation](https://arxiv.org/abs/2510.14290)
*M. A. Teeti*

Main category: cs.IT

TL;DR: 提出RIS-CSM方案，通过将RIS分区并使用二进制反射模式生成不同信道签名，在签名索引中嵌入信息，实现轻量级索引调制。


<details>
  <summary>Details</summary>
Motivation: 为可重构智能表面(RIS)设计轻量级索引调制方案，避免复杂的RIS侧波束成形，实现简单的信道估计和可扩展的频谱效率。

Method: 将N单元RIS划分为不相交组，每组使用预定义二进制反射模式在n_R天线接收器处生成不同信道签名，信息嵌入在签名索引中。

Result: 推导了误码概率闭式上界和容量分析，显示分集阶数为n_R，编码增益与N成正比。瑞利衰落下的仿真验证了理论分析，且空间相关性在低频谱效率下可提升性能。

Conclusion: RIS-CSM是一种有效的轻量级索引调制方案，具有简单信道估计、可扩展频谱效率和良好性能特性。

Abstract: This work proposes RIS-enabled channel signature modulation (RIS-CSM), a
lightweight index modulation scheme for reconfigurable intelligent surfaces
(RIS). An N-element RIS is partitioned into disjoint groups, each employing
predetermined binary reflection patterns to generate distinct channel
signatures at an $n_R$-antenna receiver, without RIS-side beamforming.
Information is embedded in the indices of these signatures, enabling simple
channel estimation and scalable spectral efficiency. A closed-form upper bound
on error probability and capacity analysis are derived, revealing diversity
order $n_R$ and coding gain proportional to N. Simulation results under
Rayleigh fading validate the theoretical analysis. Moreover, simulations
indicate that spatial correlation among RIS elements can improve system
performance at low spectral efficiency.

</details>


### [65] [The asymptotic number of equivalence classes of linear codes with given dimension](https://arxiv.org/abs/2510.14424)
*Andrea Di Giusto,Alberto Ravagnani*

Main category: cs.IT

TL;DR: 本文研究了具有给定长度和维度的线性码等价类数量的渐近行为，推导了三种标准等价关系下的显式渐近公式，并建立了与布朗运动离散高斯分布的自然联系。


<details>
  <summary>Details</summary>
Motivation: 虽然之前研究过给定长度下不等价码的总数，但维度随长度变化的情况尚未被考虑，需要填补这一研究空白。

Method: 使用渐近分析方法推导线性码等价类数量的显式公式，并建立与q-二项式系数和离散高斯分布的联系。

Result: 得到了固定字母表大小和增长长度下三种等价关系的渐近公式，同时给出了q-二项式系数和的精确渐近表达式。

Conclusion: 研究不仅解决了线性码计数问题，还建立了与概率论中离散高斯分布的自然联系，为结果提供了概率解释。

Abstract: We investigate the asymptotic number of equivalence classes of linear codes
with prescribed length and dimension. While the total number of inequivalent
codes of a given length has been studied previously, the case where the
dimension varies as a function of the length has not yet been considered. We
derive explicit asymptotic formulas for the number of equivalence classes under
three standard notions of equivalence, for a fixed alphabet size and increasing
length. Our approach also yields an exact asymptotic expression for the sum of
all q-binomial coefficients, which is of independent interest and answers an
open question in this context. Finally, we establish a natural connection
between these asymptotic quantities and certain discrete Gaussian distributions
arising from Brownian motion, providing a probabilistic interpretation of our
results.

</details>


### [66] [Rotatable Antenna-Enhanced Beamforming: Signal Enhancement and Interference Suppression](https://arxiv.org/abs/2510.14574)
*Jie Feng,Zhenbing Liu,Junjie Dai,Hongbin Chen,Fangjiong Chen*

Main category: cs.IT

TL;DR: 本文研究了可旋转天线增强的单/多波束形成，通过优化天线旋转来利用新的空间自由度，显著提高了阵列增益性能。


<details>
  <summary>Details</summary>
Motivation: 传统固定方向天线阵列由于天线定向增益在不同转向角度变化显著，难以有效增强信号和抑制干扰。

Method: 联合优化天线旋转向量和天线权重向量，采用交替优化算法迭代求解，在无干扰单波束情况下推导出闭式最优解。

Result: 仿真结果表明，所提出的可旋转天线方案在阵列增益方面显著优于传统固定方向天线和各向同性天线方案。

Conclusion: 通过利用天线旋转带来的新自由度，可以有效提升波束形成性能，突破传统天线阵列的限制。

Abstract: Conventional beamforming with fixed-orientation antenna (FOA) arrays may
struggle to effectively enhance signal and/or suppress interference due to
significant variations in antenna directive gains over different steering
angles. To break this limitation, we investigate in this paper the rotatable
antenna (RA)-enhanced single/multi-beam forming by exploiting the new spatial
degrees of freedom (DoFs) via antennas' rotation optimization. Specifically,
the antenna rotation vector (ARV) and antenna weight vector (AWV) are jointly
optimized to maximize the minimum array gain over signal directions, subject to
a given constraint on the maximum array gain over interference directions. For
the special case of single-beam forming without interference, the optimal ARV
is derived in closed-form with the maximum ratio combining (MRC) beamformer
applied to the AWV. For the general case of multi-beam forming, we propose an
efficient alternating optimization (AO) algorithm to find a high-quality
suboptimal solution by iteratively optimizing one of the ARV and AWV with the
other being fixed. Simulation results demonstrate that the proposed RA-based
scheme can significantly outperform the traditional FOA-based and isotropic
antenna (IA)-based schemes in terms of array gain.

</details>


### [67] [Task-Based Quantization for Channel Estimation in RIS Empowered MmWave Systems](https://arxiv.org/abs/2510.14649)
*Gyoseung Lee,In-soo Kim,Yonina C. Eldar,A. Lee Swindlehurst,Hyeongtaek Lee,Minje Kim,Junil Choi*

Main category: cs.IT

TL;DR: 该论文研究了在低分辨率量化条件下，可重构智能表面赋能的毫米波多用户单输入多输出通信系统的信道估计问题，提出了基于任务量化的信道估计设计。


<details>
  <summary>Details</summary>
Motivation: 由于大规模天线阵列和宽信号带宽中模数转换器的高成本和功耗，设计具有低分辨率ADC的毫米波系统是有益的。

Method: 提出了基于任务量化的信道估计设计，考虑混合模拟和数字架构，开发了两种信道估计器：纯无源RIS的级联信道估计器和利用半无源元素信息的分离信道估计器。

Result: 数值结果表明，所提出的基于任务量化的信道估计设计优于纯数字方法，并能有效接近无限分辨率ADC系统的性能，且训练开销小。

Conclusion: 基于任务量化的信道估计方法在低分辨率ADC约束下能显著提升系统性能，是毫米波RIS系统的有效解决方案。

Abstract: In this paper, we investigate channel estimation for reconfigurable
intelligent surface (RIS) empowered millimeter-wave (mmWave) multi-user
single-input multiple-output communication systems using low-resolution
quantization. Due to the high cost and power consumption of analog-to-digital
converters (ADCs) in large antenna arrays and for wide signal bandwidths,
designing mmWave systems with low-resolution ADCs is beneficial. To tackle this
issue, we propose a channel estimation design using task-based quantization
that considers the underlying hybrid analog and digital architecture in order
to improve the system performance under finite bit-resolution constraints. Our
goal is to accomplish a channel estimation task that minimizes the mean squared
error distortion between the true and estimated channel. We develop two types
of channel estimators: a cascaded channel estimator for an RIS with purely
passive elements, and an estimator for the separate RIS-related channels that
leverages additional information from a few semi-passive elements at the RIS
capable of processing the received signals with radio frequency chains.
Numerical results demonstrate that the proposed channel estimation designs
exploiting task-based quantization outperform purely digital methods and can
effectively approach the performance of a system with unlimited resolution
ADCs. Furthermore, the proposed channel estimators are shown to be superior to
baselines with small training overhead.

</details>


### [68] [Rate-Adaptive Spatially Coupled MacKay-Neal Codes with Thresholds Close to Capacity](https://arxiv.org/abs/2510.14843)
*Ayman Zahr,Gianluigi Liva*

Main category: cs.IT

TL;DR: 分析速率自适应MacKay-Neal码的渐近性能，其中内码采用原图空间耦合LDPC码，通过密度进化计算BP解码阈值，在[0,1]速率范围内接近BIAWGN信道容量0.15dB以内。


<details>
  <summary>Details</summary>
Motivation: 研究速率自适应码在完整速率范围内的性能，探索如何接近信道容量极限。

Method: 使用密度进化分析，建立并行信道模型，计算信念传播解码阈值，内码采用原图空间耦合LDPC码。

Result: SC MN码在[0,1]速率范围内性能接近BIAWGN信道容量，差距在0.15dB以内。

Conclusion: 空间耦合MN码在完整速率范围内表现出接近信道容量的优异性能。

Abstract: We analyze by density evolution the asymptotic performance of rate-adaptive
MacKay-Neal (MN) code ensembles, where the inner code is a protograph spatially
coupled (SC) low-density parity-check code. By resorting to a suitably-defined
parallel channel model, we compute belief propagation decoding thresholds,
showing that SC MN code ensembles can perform within 0.15 dB from the
binary-input additive white Gaussian noise capacity over the full [0,1] rate
range.

</details>


### [69] [Rate-Adaptive Protograph-Based MacKay-Neal Codes](https://arxiv.org/abs/2510.14856)
*Ayman Zahr,Emna Ben Yacoub,Balázs Matuz,Gianluigi Liva*

Main category: cs.IT

TL;DR: 该论文分析了基于原图的速率自适应MacKay-Neal码，通过外部分布匹配器和内层LDPC码的组合实现非线性编码结构，可在固定码长下实现速率灵活性。


<details>
  <summary>Details</summary>
Motivation: 为高吞吐量无线/光链路提供恒定码长下的速率自适应编码方案，使用单一LDPC码集合在宽速率范围内接近香农极限。

Method: 采用外部分布匹配器与内层原图LDPC码结合的非线性编码结构，通过密度演化分析和误码平层分析进行性能评估。

Result: 设计的码可在宽速率范围内在香农极限1dB以内工作，通过调整DM参数选择码率，保持固定LDPC码和块长度。

Conclusion: 该构造为采用二进制输入调制的高吞吐量无线/光链路提供了有吸引力的解决方案，实现了恒定块长度下的速率灵活性。

Abstract: Rate-adaptive MacKay-Neal (MN) codes based on protographs are analyzed. The
code construction employs an outer distribution matcher (DM) to adapt the rate
of the scheme. The DM is coupled with an inner protograph-based low-density
parity-check (LDPC) code. The performance achievable by the resulting code
structure, that is nonlinear, is studied by means of an equivalent
communication model that reduces the problem to the analysis of the inner
(linear) LDPC code with transmission that takes place in parallel over the
communication channel, and over a suitably defined binary symmetric channel. A
density evolution analysis of protograph MN code ensembles is outlined, and it
is complemented by an error floor analysis that relies on the derivation of the
average input-output weight distribution of the inner LDPC code ensemble.
Conditions on the shape of the normalized logarithmic asymptotic input-output
weight distribution are defined, which allow discarding code ensembles with bad
error floor properties during the code design phase. Examples of code designs
are provided, showing how the use of a single LDPC code ensemble allows
operating within 1 dB from the Shannon limit over a wide range of code rates,
where the code rate is selected by tuning the DM parameters. By enabling rate
flexibility with a constant blocklength, and with a fixed LDPC code as inner
code, the construction provides an appealing solution for very high-throughput
wireless (optical) links that employ binary-input modulations.

</details>


### [70] [The Whole Is Less than the Sum of Parts: Subsystem Inconsistency in Partial Information Decomposition](https://arxiv.org/abs/2510.14864)
*Aobo Lyu,Andrew Clark,Netanel Raviv*

Main category: cs.IT

TL;DR: 本文指出部分信息分解(PID)违反整体等于部分之和的原则，提出了新的系统信息分解(SID)框架来解决三变量系统中的问题，但证明在四变量及以上系统中，现有格结构方法无法完全消除这一矛盾。


<details>
  <summary>Details</summary>
Motivation: 部分信息分解(PID)自2010年提出以来在神经科学和隐私等领域有广泛应用，但缺乏统一理论框架，存在关键概念和技术挑战。本文旨在解决PID违反整体等于部分之和原则的根本问题。

Method: 通过三变量系统的反例展示PID如何违反整体等于部分之和原则，然后引入新的公理化框架——系统信息分解(SID)，专门针对三变量系统重新定义基于协同关系的信息原子求和规则。

Result: SID框架成功解决了三变量系统中的整体等于部分之和原则违反问题，但进一步证明在四变量及以上系统中，现有格结构方法无法完全消除这一矛盾。

Conclusion: 研究结果凸显了基于(反链)格结构的分解方法对于一般多变量系统的内在不足，需要新的理论框架来克服这一根本限制。

Abstract: Partial Information Decomposition (PID) was proposed by Williams and Beer in
2010 as a tool for analyzing fine-grained interactions between multiple random
variables, and has since found numerous applications ranging from neuroscience
to privacy. However, a unified theoretical framework remains elusive due to key
conceptual and technical challenges. We identify and illustrate a crucial
problem: PID violates the set-theoretic principle that the whole equals the sum
of its parts (WESP). Through a counterexample in a three-variable system, we
demonstrate how such violations naturally arise, revealing a fundamental
limitation of current lattice-based PID frameworks. To address this issue, we
introduce a new axiomatic framework, termed System Information Decomposition
(SID), specifically tailored for three-variable systems. SID resolves the WESP
violation by redefining the summation rules of decomposed information atoms
based on synergistic relationships. However, we further show that for systems
with four or more variables, no partial summation approach within the existing
lattice-based structures can fully eliminate WESP inconsistencies. Our results
thus highlight the inherent inadequacy of (antichain) lattice-based
decompositions for general multivariate systems.

</details>
