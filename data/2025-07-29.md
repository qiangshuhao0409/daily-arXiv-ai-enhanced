<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 16]
- [cs.AI](#cs.AI) [Total: 51]
- [cs.IT](#cs.IT) [Total: 18]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [On the Limitations of Ray-Tracing for Learning-Based RF Tasks in Urban Environments](https://arxiv.org/abs/2507.19653)
*Armen Manukyan,Hrant Khachatrian,Edvard Ghukasyan,Theofanis P. Raptis*

Main category: cs.NI

TL;DR: 论文研究了Sionna v1.0.2射线追踪在罗马市中心室外蜂窝链路的真实性，通过真实测量数据和参数优化评估模拟器的保真度。


<details>
  <summary>Details</summary>
Motivation: 评估射线追踪模拟器在复杂城市环境中的真实性，以提升无线通信仿真的准确性。

Method: 使用1,664个用户设备和6个基站位置的实测数据，系统调整模拟参数（如路径深度、天线属性等），并通过Spearman相关性和kNN定位算法评分。

Result: 天线位置和方向对模拟结果影响显著，优化后Spearman相关性提升5%-130%，kNN定位误差减少三分之一。

Conclusion: 精确的几何和天线模型是必要的，但模拟仍需解决城市噪声问题以实现高保真度。

Abstract: We study the realism of Sionna v1.0.2 ray-tracing for outdoor cellular links
in central Rome. We use a real measurement set of 1,664 user-equipments (UEs)
and six nominal base-station (BS) sites. Using these fixed positions we
systematically vary the main simulation parameters, including path depth,
diffuse/specular/refraction flags, carrier frequency, as well as antenna's
properties like its altitude, radiation pattern, and orientation. Simulator
fidelity is scored for each base station via Spearman correlation between
measured and simulated powers, and by a fingerprint-based k-nearest-neighbor
localization algorithm using RSSI-based fingerprints. Across all experiments,
solver hyper-parameters are having immaterial effect on the chosen metrics. On
the contrary, antenna locations and orientations prove decisive. By simple
greedy optimization we improve the Spearman correlation by 5% to 130% for
various base stations, while kNN-based localization error using only simulated
data as reference points is decreased by one-third on real-world samples, while
staying twice higher than the error with purely real data. Precise geometry and
credible antenna models are therefore necessary but not sufficient; faithfully
capturing the residual urban noise remains an open challenge for transferable,
high-fidelity outdoor RF simulation.

</details>


### [2] ["X of Information'' Continuum: A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems](https://arxiv.org/abs/2507.19657)
*Beining Wu,Jun Huang,Shui Yu*

Main category: cs.NI

TL;DR: 论文探讨了下一代网络系统从吞吐量导向转向信息质量导向的设计趋势，提出了一个四维分类框架来量化信息质量，并分析了AI技术在多领域应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统网络指标（如延迟、丢包）无法满足现代智能应用（如自动驾驶、元宇宙）对信息质量的需求，因此需要新的评估框架。

Method: 提出了一个系统性的四维分类框架（时间、质量/效用、可靠性/鲁棒性、网络/通信），并利用AI技术（如深度强化学习）优化信息质量目标。

Result: 在多领域应用（如自动驾驶、工业物联网）中展示了多维信息指标的潜力，并揭示了各维度间的相互依赖性。

Conclusion: 多维信息指标对满足多样化需求具有革命性意义，但实际应用中仍面临挑战。

Abstract: The development of next-generation networking systems has inherently shifted
from throughput-based paradigms towards intelligent, information-aware designs
that emphasize the quality, relevance, and utility of transmitted information,
rather than sheer data volume. While classical network metrics, such as latency
and packet loss, remain significant, they are insufficient to quantify the
nuanced information quality requirements of modern intelligent applications,
including autonomous vehicles, digital twins, and metaverse environments. In
this survey, we present the first comprehensive study of the ``X of
Information'' continuum by introducing a systematic four-dimensional taxonomic
framework that structures information metrics along temporal, quality/utility,
reliability/robustness, and network/communication dimensions. We uncover the
increasing interdependencies among these dimensions, whereby temporal freshness
triggers quality evaluation, which in turn helps with reliability appraisal,
ultimately enabling effective network delivery. Our analysis reveals that
artificial intelligence technologies, such as deep reinforcement learning,
multi-agent systems, and neural optimization models, enable adaptive,
context-aware optimization of competing information quality objectives. In our
extensive study of six critical application domains, covering autonomous
transportation, industrial IoT, healthcare digital twins, UAV communications,
LLM ecosystems, and metaverse settings, we illustrate the revolutionary promise
of multi-dimensional information metrics for meeting diverse operational needs.
Our survey identifies prominent implementation challenges, including ...

</details>


### [3] [Predicting Locations of Cell Towers for Network Capacity Expansion](https://arxiv.org/abs/2507.19925)
*Sowmiyan Morri,Joy Bose,L Raghunatha Reddy,Sai Hareesh Anamandra*

Main category: cs.NI

TL;DR: 提出了一种基于机器学习的框架，结合深度神经网络和空间聚类，用于推荐新基站位置，优化网络容量扩展。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如手动测试和静态优化）未能充分考虑用户密度、地形特征和财务约束等现实因素。

Method: 结合深度神经网络预测信号覆盖，利用空间聚类推荐新基站位置，整合地理空间、人口和基础设施数据，并考虑预算约束。

Result: 框架具有模块化、鲁棒性和通用性，适用于多样化部署场景，提供了一种可扩展的数据驱动方法。

Conclusion: 该方法为无线电网络规划提供了一种自适应、经济高效的扩展方案，优于传统手动方法。

Abstract: Network capacity expansion is a critical challenge for telecom operators,
requiring strategic placement of new cell sites to ensure optimal coverage and
performance. Traditional approaches, such as manual drive tests and static
optimization, often fail to consider key real-world factors including user
density, terrain features, and financial constraints. In this paper, we propose
a machine learning-based framework that combines deep neural networks for
signal coverage prediction with spatial clustering to recommend new tower
locations in underserved areas. The system integrates geospatial, demographic,
and infrastructural data, and incorporates budget-aware constraints to
prioritize deployments. Operating within an iterative planning loop, the
framework refines coverage estimates after each proposed installation, enabling
adaptive and cost-effective expansion. While full-scale simulation was limited
by data availability, the architecture is modular, robust to missing inputs,
and generalizable across diverse deployment scenarios. This approach advances
radio network planning by offering a scalable, data-driven alternative to
manual methods.

</details>


### [4] [Optimizing Spreading Factor Selection for Mobile LoRa Gateways Using Single-Channel Hardware](https://arxiv.org/abs/2507.19938)
*W. A. Sasindu Wijesuriya*

Main category: cs.NI

TL;DR: 论文提出了一种静态选择最优扩频因子（SF）的两阶段算法，用于低成本单通道LoRa网关，解决了动态配置支持的不足。


<details>
  <summary>Details</summary>
Motivation: 低成本单通道LoRa网关缺乏动态配置支持，导致通信可靠性问题。传统LoRaWAN网络中的自适应数据速率（ADR）机制仅适用于昂贵的多通道网关。

Method: 采用两阶段算法：首先基于规则排除不满足距离、数据速率、链路裕度和法规限制的SF；然后通过加权评分模型评估剩余候选SF，考虑空中时间、能耗、数据速率和链路鲁棒性。

Result: 通过实地测试和NS-3仿真验证，算法在672个模拟场景中92%的情况下匹配最优SF。

Conclusion: 该算法为成本敏感环境（如农业和农村传感应用）提供了一种可扩展的替代方案，无需依赖动态协议。

Abstract: The deployment of mobile LoRa gateways using low-cost single-channel hardware
presents a significant challenge in maintaining reliable communication due to
the lack of dynamic configuration support. In traditional LoRaWAN networks,
Adaptive Data Rate (ADR) mechanisms optimize communication parameters in real
time. However, such features are typically supported only by expensive
multi-channel gateways. This study proposes a cost-effective and
energy-efficient solution by statically selecting the optimal Spreading Factor
(SF) using a two-phase algorithm. The method first applies rule-based exclusion
to eliminate SFs that violate constraints related to distance, data rate, link
margin, and regulatory limits. Remaining candidates are then evaluated using a
weighted scoring model incorporating Time-on-Air, energy consumption, data
rate, and link robustness. The proposed algorithm was validated through
extensive field tests and NS-3 simulations under line-of-sight conditions.
Results demonstrate that the selected SF matched the optimal SF in over 92% of
cases across 672 simulated scenarios, confirming the algorithm's effectiveness.
This approach offers a scalable alternative to dynamic protocols, enabling
reliable mobile LoRa deployments in cost-sensitive environments such as
agriculture and rural sensing applications.

</details>


### [5] [Towards Next Generation Immersive Applications in 5G Environments](https://arxiv.org/abs/2507.20050)
*Rohail Asim,Ankit Bhardwaj,Lakshmi Suramanian,Yasir Zaki*

Main category: cs.NI

TL;DR: Hera框架通过结合应用感知流逻辑和QoE中心速率控制，优化了动态无线环境下的多用户沉浸式体验，显著降低延迟并提升视觉质量。


<details>
  <summary>Details</summary>
Motivation: 无线网络限制成为多用户沉浸式现实（MIR）发展的瓶颈，无法满足高带宽和超低延迟需求。

Method: Hera框架包含高层流同步层和低层延迟感知QoE速率控制协议，适应动态无线环境。

Result: Hera在延迟、视觉质量和公平性上优于现有算法，延迟降低66%，比特率提升50%。

Conclusion: Hera为更可扩展、稳健和高保真的多用户沉浸式体验奠定了基础。

Abstract: The Multi-user Immersive Reality (MIR) landscape is evolving rapidly, with
applications spanning virtual collaboration, entertainment, and training.
However, wireless network limitations create a critical bottleneck, struggling
to meet the high-bandwidth and ultra-low latency demands essential for
next-generation MIR experiences. This paper presents Hera, a modular framework
for next-generation immersive applications, comprising a high-level streaming
and synchronization layer for AR/VR systems and a low-level delay-based
QoE-aware rate control protocol optimized for dynamic wireless environments.
The Hera framework integrates application-aware streaming logic with a
QoE-centric rate control core, enabling adaptive video quality, multi-user
fairness, and low-latency communication across challenging 5G network
conditions. We demonstrate that Hera outperforms existing state-of-the-art rate
control algorithms by maintaining up to 66% lower latencies with comparable
throughput performance, higher visual quality with 50% average bitrate
improvements in our analysis, and improved fairness. By bridging the gap
between application-level responsiveness and network-level adaptability, Hera
lays the foundation for more scalable, robust, and high-fidelity multi-user
immersive experiences.

</details>


### [6] [A Scalable Resource Management Layer for FPGA SoCs in 6G Radio Units](https://arxiv.org/abs/2507.19963)
*Nikolaos Bartzoudis,José Rubio Fernández,David López-Bueno,Antonio Román Villarroel*

Main category: cs.NI

TL;DR: 提出了一种解决FPGA SoC设备在5G和边缘计算中资源利用率不足的方法，通过动态迁移和扩展功能的资源管理层，设计了一个数据驱动的微协调器。


<details>
  <summary>Details</summary>
Motivation: 解决FPGA SoC设备在5G和边缘计算基础设施中资源利用率不足的问题。

Method: 开发一个资源管理层，支持动态迁移和扩展功能，并设计一个数据驱动的微协调器来管理功能生命周期。

Result: 资源管理层成功用于基于计算机视觉边缘应用的事件重新配置功能。

Conclusion: 该方法有效提升了FPGA SoC设备的资源利用率，并通过实际应用验证了其可行性。

Abstract: This work presents a perspective on addressing the underutilization of
computing resources in FPGA SoC devices deployed in 5G radio and edge computing
infrastructure. The initial step in this approach involves developing a
resource management layer capable of dynamically migrating and scaling
functions within these devices in response to contextual events. This layer
serves as the foundation for designing a hierarchical, data-driven
micro-orchestrator responsible for managing the lifecycle of functions in FPGA
SoC devices. In this paper, the proposed resource management layer is utilized
to reconfigure a function based on events identified by a computer vision edge
application.

</details>


### [7] [Packet-Level DDoS Data Augmentation Using Dual-Stream Temporal-Field Diffusion](https://arxiv.org/abs/2507.20115)
*Gongli Xi,Ye Tian,Yannan Hu,Yuchao Zhang,Yapeng Niu,Xiangyang Gong*

Main category: cs.NI

TL;DR: 提出了一种基于扩散模型的双流时空场扩散（DSTF-Diffusion）方法，用于生成高质量的网络流量数据，以解决DDoS攻击检测中数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有合成网络流量数据方法难以捕捉复杂时空模式，导致生成的训练数据与真实数据相似度不足，影响机器学习检测效果。

Method: 采用双流网络结构：场流通过空间映射将网络数据特征与预训练扩散模型结合，时空流则动态建模网络流量的时间模式。

Result: 实验表明，生成的数据在统计上与真实数据更相似，并能显著提升下游任务的性能。

Conclusion: DSTF-Diffusion能有效生成高质量网络流量数据，提升DDoS攻击检测的准确性。

Abstract: In response to Distributed Denial of Service (DDoS) attacks, recent research
efforts increasingly rely on Machine Learning (ML)-based solutions, whose
effectiveness largely depends on the quality of labeled training datasets. To
address the scarcity of such datasets, data augmentation with synthetic traces
is often employed. However, current synthetic trace generation methods struggle
to capture the complex temporal patterns and spatial distributions exhibited in
emerging DDoS attacks. This results in insufficient resemblance to real traces
and unsatisfied detection accuracy when applied to ML tasks. In this paper, we
propose Dual-Stream Temporal-Field Diffusion (DSTF-Diffusion), a multi-view,
multi-stream network traffic generative model based on diffusion models,
featuring two main streams: The field stream utilizes spatial mapping to bridge
network data characteristics with pre-trained realms of stable diffusion
models, effectively translating complex network interactions into formats that
stable diffusion can process, while the spatial stream adopts a dynamic
temporal modeling approach, meticulously capturing the intrinsic temporal
patterns of network traffic. Extensive experiments demonstrate that data
generated by our model exhibits higher statistical similarity to originals
compared to current state-of-the-art solutions, and enhance performances on a
wide range of downstream tasks.

</details>


### [8] [Accelerating Containerized Service Delivery at the Network Edge](https://arxiv.org/abs/2507.20116)
*Yinuo Deng,Hailiang Zhao,Dongjing Wang,Peng Chen,Wenzhuo Qian,Jianwei Yin,Schahram Dustdar,Shuiguang Deng*

Main category: cs.NI

TL;DR: PeerSync是一个去中心化的P2P系统，用于优化边缘环境中的容器镜像分发，显著提升速度并减少网络流量。


<details>
  <summary>Details</summary>
Motivation: 边缘环境中的资源限制和动态网络条件对容器镜像分发提出了挑战，需要高效解决方案。

Method: PeerSync采用基于流行度和网络感知的下载引擎，结合滑动窗口机制、自动跟踪器选举和动态缓存管理。

Result: 实验显示，PeerSync比基线、Dragonfly和Kraken分别快2.72倍、1.79倍和1.28倍，并减少90.72%的网络流量。

Conclusion: PeerSync在边缘环境中高效优化了容器镜像分发，显著提升了性能和资源利用率。

Abstract: Efficient container image distribution is crucial for enabling machine
learning inference at the network edge, where resource limitations and dynamic
network conditions create significant challenges. In this paper, we present
PeerSync, a decentralized P2P-based system designed to optimize image
distribution in edge environments. PeerSync employs a popularity- and
network-aware download engine that dynamically adapts to content popularity and
real-time network conditions using a sliding window mechanism. PeerSync further
integrates automated tracker election for rapid peer discovery and dynamic
cache management for efficient storage utilization. We implement PeerSync with
8000+ lines of Rust code and test its performance extensively on both physical
edge devices and Docker-based emulations. Experimental results show that
PeerSync delivers a remarkable speed increase of 2.72$\times$, 1.79$\times$,
and 1.28$\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,
while significantly reducing peak cross-network traffic by 90.72\% under
congested and varying network conditions.

</details>


### [9] [Democracy for DAOs: An Empirical Study of Decentralized Governance and Dynamic (Case Study Internet Computer SNS Ecosystem)](https://arxiv.org/abs/2507.20234)
*Burak Arda Okutan,Stefan Schmid,Yvonne-Anne Pignolet*

Main category: cs.NI

TL;DR: 本文通过实证研究分析了基于互联网计算机协议（ICP）的SNS DAO框架中用户治理行为，比较了14个SNS DAO与其他区块链平台DAO的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索去中心化自治组织（DAO）的治理机制，特别是SNS DAO框架在用户参与、决策速度和成本效率方面的表现。

Method: 方法包括测量提案提交频率、投票通过率、决策时长等指标，并分析时间动态变化。研究覆盖了14个SNS DAO的3,000多个提案，时间跨度为20个月。

Result: 结果显示SNS DAO的通过率较高，治理机制更高效，用户参与度随时间持续或增加，决策速度更快且成本更低。

Conclusion: 结论是SNS DAO框架在治理效率和用户参与度方面优于其他平台，显示出更强的可持续性。

Abstract: Decentralized autonomous organizations (DAOs) rely on governance mechanism
without centralized leadership. This paper presents an empirical study of user
behavior in governance for a variety of DAOs, ranging from DeFi to gaming,
using the Internet Computer Protocol DAO framework called SNS (Service Nervous
System). To analyse user engagement, we measure participation rates and
frequency of proposals submission and voter approval rates. We evaluate
decision duration times to determine DAO agility. To investigate dynamic
aspects, we also measure metric shifts in time. We evaluate over 3,000
proposals submitted in a time frame of 20 months from 14 SNS DAOs. The selected
DAO have been existing between 6 and 20 months and cover a wide spectrum of use
cases, treasury sizes, and number of participants. We also compare our results
for SNS DAOs with DAOs from other blockchain platforms. While approval rates
are generally high for all DAOs studied, SNS DAOs show slightly more alignment.
We observe that the SNS governance mechanisms and processes in ICP lead to
higher activity, lower costs and faster decisions. Most importantly, in
contrast to studies which report a decline in participation over time for other
frameworks, SNS DAOs exhibit sustained or increasing engagement levels over
time.

</details>


### [10] [Joint Fiber and Free Space Optical Infrastructure Planning for Hybrid Integrated Access and Backhaul Networks](https://arxiv.org/abs/2507.20367)
*Charitha Madapatha,Piotr Lechowicz,Carlos Natalino,Paolo Monti,Tommy Svensson*

Main category: cs.NI

TL;DR: 研究了集成接入与回传（IAB）网络中基础设施规划和优化对覆盖性能的影响，探讨了在光纤连接受限时采用自由空间光通信（FSO）的性能增益和能效。


<details>
  <summary>Details</summary>
Motivation: 由于回传链路对高速率和高可靠性的需求敏感，且光纤连接成本高，需要优化网络规划以确保IAB网络的性能。

Method: 研究了在光纤连接受限的情况下，采用FSO链路的混合部署方案，分析了其对覆盖概率、能效和成本的影响。

Result: 结果表明，混合光纤/FSO部署相比全光纤网络可显著节省成本，同时提高服务覆盖概率。

Conclusion: 通过合理的网络规划，可以同时提升服务覆盖、能效和成本效率。

Abstract: Integrated access and backhaul (IAB) is one of the promising techniques for
5G networks and beyond (6G), in which the same node/hardware is used to provide
both backhaul and cellular services in a multi-hop architecture. Due to the
sensitivity of the backhaul links with high rate/reliability demands, proper
network planning is needed to ensure the IAB network performs with the desired
performance levels. In this paper, we study the effect of infrastructure
planning and optimization on the coverage of IAB networks. We concentrate on
the cases where the fiber connectivity to the nodes is constrained due to cost.
Thereby, we study the performance gains and energy efficiency in the presence
of free-space optical (FSO) communication links. Our results indicate hybrid
fiber/FSO deployments offer substantial cost savings compared to fully fibered
networks, suggesting a beneficial trade-off for strategic link deployment while
improving the service coverage probability. As we show, with proper network
planning, the service coverage, energy efficiency, and cost efficiency can be
improved.

</details>


### [11] [Teleoperating Autonomous Vehicles over Commercial 5G Networks: Are We There Yet?](https://arxiv.org/abs/2507.20438)
*Rostand A. K. Fezeu,Jason Carpenter,Rushikesh Zende,Sree Ganesh Lalitaditya Divakarla,Nitin Varyani,Faaiq Bilal,Steven Sleder,Nanditha Naik,Duncan Joly,Eman Ramadan,Ajay Kumar Gurumadaiah,Zhi-Li Zhang*

Main category: cs.NI

TL;DR: 本文研究了在商用5G网络上远程驾驶自动驾驶车辆（AVs）的可行性，重点关注传感器数据的上行传输性能，分析了5G网络对端到端延迟和上层协议的影响。


<details>
  <summary>Details</summary>
Motivation: 远程驾驶是5G网络支持的关键应用，但现有5G网络和传感器数据传输机制存在挑战，需系统性研究以指导未来网络和应用的协同设计。

Method: 从跨层和端到端角度分析5G网络的物理层因素（如信道条件、资源分配、切换）对延迟的影响，并评估上层协议（如RTSP、WebRTC）的性能。

Result: 研究发现当前5G网络和传感器数据流机制存在局限性，揭示了低延迟传输的挑战。

Conclusion: 研究结果为未来无线网络、边缘云系统和应用的协同设计提供了重要见解，以克服AV远程操作中的低延迟障碍。

Abstract: Remote driving, or teleoperating Autonomous Vehicles (AVs), is a key
application that emerging 5G networks aim to support. In this paper, we conduct
a systematic feasibility study of AV teleoperation over commercial 5G networks
from both cross-layer and end-to-end (E2E) perspectives. Given the critical
importance of timely delivery of sensor data, such as camera and LiDAR data,
for AV teleoperation, we focus in particular on the performance of uplink
sensor data delivery. We analyze the impacts of Physical Layer (PHY layer) 5G
radio network factors, including channel conditions, radio resource allocation,
and Handovers (HOs), on E2E latency performance. We also examine the impacts of
5G networks on the performance of upper-layer protocols and E2E application
Quality-of-Experience (QoE) adaptation mechanisms used for real-time sensor
data delivery, such as Real-Time Streaming Protocol (RTSP) and Web Real Time
Communication (WebRTC). Our study reveals the challenges posed by today's 5G
networks and the limitations of existing sensor data streaming mechanisms. The
insights gained will help inform the co-design of future-generation wireless
networks, edge cloud systems, and applications to overcome the low-latency
barriers in AV teleoperation.

</details>


### [12] [DD-JSCC: Dynamic Deep Joint Source-Channel Coding for Semantic Communications](https://arxiv.org/abs/2507.20467)
*Avi Deb Raha,Apurba Adhikary,Mrityunjoy Gain,Yumin Park,Walid Saad,Choong Seon Hong*

Main category: cs.NI

TL;DR: DD-JSCC是一种动态深度联合源信道编码方法，通过实时调整编码器-解码器结构，适应不同设备能力和信道条件，提升语义通信中的图像重建性能。


<details>
  <summary>Details</summary>
Motivation: 传统Deep-JSCC采用固定编码器-解码器结构，无法适应变化的设备能力、实时性能优化、功率限制和信道条件。

Method: 提出DD-JSCC，通过分层激活机制和顺序随机化训练，动态调整层结构以适应不同需求。

Result: DD-JSCC在PSNR上比传统Deep-JSCC提升2 dB，训练成本降低40%。

Conclusion: DD-JSCC通过统一框架减少训练复杂性和部署开销，提升语义通信性能。

Abstract: Deep Joint Source-Channel Coding (Deep-JSCC) has emerged as a promising
semantic communication approach for wireless image transmission by jointly
optimizing source and channel coding using deep learning techniques. However,
traditional Deep-JSCC architectures employ fixed encoder-decoder structures,
limiting their adaptability to varying device capabilities, real-time
performance optimization, power constraints and channel conditions. To address
these limitations, we propose DD-JSCC: Dynamic Deep Joint Source-Channel Coding
for Semantic Communications, a novel encoder-decoder architecture designed for
semantic communication systems. Unlike traditional Deep-JSCC models, DD-JSCC is
flexible for dynamically adjusting its layer structures in real-time based on
transmitter and receiver capabilities, power constraints, compression ratios,
and current channel conditions. This adaptability is achieved through a
hierarchical layer activation mechanism combined with implicit regularization
via sequential randomized training, effectively reducing combinatorial
complexity, preventing overfitting, and ensuring consistent feature
representations across varying configurations. Simulation results demonstrate
that DD-JSCC enhances the performance of image reconstruction in semantic
communications, achieving up to 2 dB improvement in Peak Signal-to-Noise Ratio
(PSNR) over fixed Deep-JSCC architectures, while reducing training costs by
over 40%. The proposed unified framework eliminates the need for multiple
specialized models, significantly reducing training complexity and deployment
overhead.

</details>


### [13] [A Lyapunov-Guided Diffusion-Based Reinforcement Learning Approach for UAV-Assisted Vehicular Networks with Delayed CSI Feedback](https://arxiv.org/abs/2507.20524)
*Zhang Liu,Lianfen Huang,Zhibin Gao,Xianbin Wang,Dusit Niyato,Xuemin,Shen*

Main category: cs.NI

TL;DR: 论文提出了一种基于扩散模型的深度确定性策略梯度（D3PG）算法，用于优化无人机辅助车联网中的信道分配、功率控制和飞行高度调整，以最大化通信总速率并满足无人机长期能量约束。


<details>
  <summary>Details</summary>
Motivation: 低空无人机（UAV）在智能交通系统和低空经济中潜力巨大，但面临网络资源动态优化、无人机续航限制和不完善的信道状态信息（CSI）等挑战。

Method: 利用Lyapunov优化将长期问题分解为时隙子问题，并提出D3PG算法，结合扩散模型优化信道分配、功率控制和飞行高度调整。

Result: 通过真实车辆移动轨迹的仿真验证，D3PG算法在性能上优于现有基准方案。

Conclusion: D3PG算法为低空经济网络中的无人机辅助车联网通信提供了高效解决方案。

Abstract: Low altitude uncrewed aerial vehicles (UAVs) are expected to facilitate the
development of aerial-ground integrated intelligent transportation systems and
unlocking the potential of the emerging low-altitude economy. However, several
critical challenges persist, including the dynamic optimization of network
resources and UAV trajectories, limited UAV endurance, and imperfect channel
state information (CSI). In this paper, we offer new insights into low-altitude
economy networking by exploring intelligent UAV-assisted vehicle-to-everything
communication strategies aligned with UAV energy efficiency. Particularly, we
formulate an optimization problem of joint channel allocation, power control,
and flight altitude adjustment in UAV-assisted vehicular networks. Taking CSI
feedback delay into account, our objective is to maximize the vehicle-to-UAV
communication sum rate while satisfying the UAV's long-term energy constraint.
To this end, we first leverage Lyapunov optimization to decompose the original
long-term problem into a series of per-slot deterministic subproblems. We then
propose a diffusion-based deep deterministic policy gradient (D3PG) algorithm,
which innovatively integrates diffusion models to determine optimal channel
allocation, power control, and flight altitude adjustment decisions. Through
extensive simulations using real-world vehicle mobility traces, we demonstrate
the superior performance of the proposed D3PG algorithm compared to existing
benchmark solutions.

</details>


### [14] [Collusion Resistant DNS With Private Information Retrieval](https://arxiv.org/abs/2507.20806)
*Yunming Xiao,Peizhi Liu,Ruijie Yu,Chenkai Weng,Matteo Varvello,Aleksandar Kuzmanovic*

Main category: cs.NI

TL;DR: 论文探讨了DNS隐私问题，提出了PDNS，一种基于单服务器PIR的DNS扩展，以增强隐私保护，性能表现可接受但牺牲了可扩展性。


<details>
  <summary>Details</summary>
Motivation: DNS隐私长期被忽视，现有解决方案（如DoH和ODoH）依赖非共谋假设，难以实际保证。

Method: 提出PDNS，利用单服务器PIR技术处理加密查询，避免信任假设，并解决DNS层次结构带来的信息泄露问题。

Result: PDNS原型性能测试显示其比DoH over Tor快2倍，提供类似隐私保障，但可扩展性受限。

Conclusion: PDNS在隐私和性能间取得平衡，未来可通过专用硬件提升可扩展性。

Abstract: There has been a growing interest in Internet user privacy, demonstrated by
the popularity of privacy-preserving products such as Telegram and Brave, and
the widespread adoption of HTTPS. The Domain Name System (DNS) is a key
component of Internet-based communication and its privacy has been neglected
for years. Recently, DNS over HTTPS (DoH) has improved the situation by fixing
the issue of in-path middleboxes. Further progress has been made with
proxy-based solutions such as Oblivious DoH (ODoH), which separate a user's
identity from their DNS queries. However, these solutions rely on non-collusion
assumptions between DNS resolvers and proxies -- an assumption difficult to
guarantee in practice. To address this, we explore integrating single-server
Private Information Retrieval (PIR) into DNS to enable encrypted query
processing without relying on trust assumptions. However, applying PIR to DNS
is challenging due to its hierarchical nature -- particularly, interactions
with recursive resolvers can still leak information. Navigating performance and
privacy trade-offs, we propose PDNS, a DNS extension leveraging single-server
PIR to strengthen privacy guarantees. We have implemented a prototype of PDNS
and compared its performance against state-of-the-art solutions via
trace-driven experiments. The results show that PDNS achieves acceptable
performance (2x faster than DoH over Tor with similar privacy guarantees) and
strong privacy guarantees today, mainly at the cost of its scalability, which
specialized hardware for PIR can address in the near future.

</details>


### [15] [\textit{FedABC}: Attention-Based Client Selection for Federated Learning with Long-Term View](https://arxiv.org/abs/2507.20871)
*Wenxuan Ye,Xueli An,Junfan Wang,Xueqiang Yan,Georg Carle*

Main category: cs.NI

TL;DR: 论文提出了一种名为FedABC的创新客户端选择算法，旨在解决联邦学习中数据异构性和客户端参与效率的问题，显著提升了模型精度和参与效率。


<details>
  <summary>Details</summary>
Motivation: 6G网络中的原生AI支持是关键目标，联邦学习（FL）作为一种隐私保护范式，面临数据异构性和客户端参与效率的挑战。

Method: 提出FedABC算法，基于注意力机制评估客户端模型相似性和贡献，动态调整选择阈值，优化参与效率。

Result: 在CIFAR-10上的实验显示，FedABC比FedAvg减少32%的客户端参与，同时比现有最优方法提高3.5%的精度。

Conclusion: FedABC为异构资源受限环境中的联邦学习部署提供了有效解决方案，支持6G网络的原生AI能力。

Abstract: Native AI support is a key objective in the evolution of 6G networks, with
Federated Learning (FL) emerging as a promising paradigm. FL allows
decentralized clients to collaboratively train an AI model without directly
sharing their data, preserving privacy. Clients train local models on private
data and share model updates, which a central server aggregates to refine the
global model and redistribute it for the next iteration. However, client data
heterogeneity slows convergence and reduces model accuracy, and frequent client
participation imposes communication and computational burdens. To address these
challenges, we propose \textit{FedABC}, an innovative client selection
algorithm designed to take a long-term view in managing data heterogeneity and
optimizing client participation. Inspired by attention mechanisms,
\textit{FedABC} prioritizes informative clients by evaluating both model
similarity and each model's unique contributions to the global model. Moreover,
considering the evolving demands of the global model, we formulate an
optimization problem to guide \textit{FedABC} throughout the training process.
Following the ``later-is-better" principle, \textit{FedABC} adaptively adjusts
the client selection threshold, encouraging greater participation in later
training stages. Extensive simulations on CIFAR-10 demonstrate that
\textit{FedABC} significantly outperforms existing approaches in model accuracy
and client participation efficiency, achieving comparable performance with 32\%
fewer clients than the classical FL algorithm \textit{FedAvg}, and 3.5\% higher
accuracy with 2\% fewer clients than the state-of-the-art. This work marks a
step toward deploying FL in heterogeneous, resource-constrained environments,
thereby supporting native AI capabilities in 6G networks.

</details>


### [16] [Towards a Robust Transport Network With Self-adaptive Network Digital Twin](https://arxiv.org/abs/2507.20971)
*Cláudio Modesto,João Borges,Cleverson Nahum,Lucas Matni,Cristiano Bonato Both,Kleber Cardoso,Glauco Gonçalves,Ilan Correa,Silvia Lins,Andrey Silva,Aldebaro Klautau*

Main category: cs.NI

TL;DR: 本文提出了一种自适应的网络数字孪生（NDT）架构，旨在动态适应流量变化，提高延迟预测准确性，并增强虚拟孪生（VTwin）与物理孪生（PTwin）的同步性。


<details>
  <summary>Details</summary>
Motivation: 网络数字孪生需要实时感知物理孪生的变化以实现同步，但现有研究对流量变化的适应性和同步性提升关注不足。

Method: 采用自适应的NDT架构，结合遥测模块监测流量变化，利用概念漂移检测技术指导VTwin的更新和重新部署。

Result: 在多种网络拓扑和流量模式下，该架构在流量变化后延迟预测性能提升至少56.7%。

Conclusion: 所提架构显著提高了NDT在流量波动下的预测准确性和同步性，为网络管理提供了有效解决方案。

Abstract: The ability of the network digital twin (NDT) to remain aware of changes in
its physical counterpart, known as the physical twin (PTwin), is a fundamental
condition to enable timely synchronization, also referred to as twinning. In
this way, considering a transport network, a key requirement is to handle
unexpected traffic variability and dynamically adapt to maintain optimal
performance in the associated virtual model, known as the virtual twin (VTwin).
In this context, we propose a self-adaptive implementation of a novel NDT
architecture designed to provide accurate delay predictions, even under
fluctuating traffic conditions. This architecture addresses an essential
challenge, underexplored in the literature: improving the resilience of
data-driven NDT platforms against traffic variability and improving
synchronization between the VTwin and its physical counterpart. Therefore, the
contributions of this article rely on NDT lifecycle by focusing on the
operational phase, where telemetry modules are used to monitor incoming
traffic, and concept drift detection techniques guide retraining decisions
aimed at updating and redeploying the VTwin when necessary. We validate our
architecture with a network management use case, across various emulated
network topologies, and diverse traffic patterns to demonstrate its
effectiveness in preserving acceptable performance and predicting per-flow
delay under unexpected traffic variation. The results in all tested topologies,
using the normalized mean square error as the evaluation metric, demonstrate
that our proposed architecture, after a traffic concept drift, achieves a
performance improvement in prediction of at least 56.7% compared to a
configuration without NDT synchronization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA是一个开源平台，旨在促进临床、研究和AI开发者的跨学科合作，加速AI在医疗中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决AI技术与实际医疗应用之间的差距，促进协作和互操作性。

Method: 基于Kubernete构建的模块化、可扩展平台，提供数据管理、模型开发、部署和临床反馈工具。

Result: 成功应用于医学影像AI项目，并在学术和临床环境中部署。

Conclusion: MAIA通过促进协作和透明度，加速了AI研究向临床解决方案的转化。

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [18] [Agent WARPP: Workflow Adherence via Runtime Parallel Personalization](https://arxiv.org/abs/2507.19543)
*Maria Emilia Mazzolenis,Ruirui Zhang*

Main category: cs.AI

TL;DR: WARPP是一个无需训练的模块化框架，通过多智能体编排和运行时个性化，提升LLM在任务导向对话中的工作流依从性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在长条件工作流和外部工具调用中的不足，特别是依赖用户特定信息时的性能问题。

Method: 结合多智能体编排和运行时个性化，动态修剪条件分支，减少推理开销并优化工具选择。

Result: 在五个用户意图的评估中，WARPP在参数保真度和工具准确性上优于非个性化方法和ReAct基线，同时减少令牌使用。

Conclusion: WARPP通过运行时个性化显著提升了LLM在复杂工作流中的性能，且无需额外训练。

Abstract: Large language models (LLMs) are increasingly applied in task-oriented
dialogue (TOD) systems but often struggle with long, conditional workflows that
involve external tool calls and depend on user-specific information. We present
Workflow Adherence via Runtime Parallel Personalization, or WARPP, a
training-free, modular framework that combines multi-agent orchestration with
runtime personalization to improve workflow adherence in LLM-based systems. By
dynamically pruning conditional branches based on user attributes, the
framework reduces reasoning overhead and narrows tool selection at runtime.
WARPP deploys a parallelized architecture where a dedicated Personalizer agent
operates alongside modular, domain-specific agents to dynamically tailor
execution paths in real time. The framework is evaluated across five
representative user intents of varying complexity within three domains:
banking, flights, and healthcare. Our evaluation leverages synthetic datasets
and LLM-powered simulated users to test scenarios with conditional
dependencies. Our results demonstrate that WARPP outperforms both the
non-personalized method and the ReAct baseline, achieving increasingly larger
gains in parameter fidelity and tool accuracy as intent complexity grows, while
also reducing average token usage, without any additional training.

</details>


### [19] [Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems](https://arxiv.org/abs/2507.19593)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 本文系统回顾了超博弈理论在动态多智能体系统中的应用，分析了44项研究，提出了智能体兼容性标准和分类框架，并指出了当前研究的不足与未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统博弈论假设理性、完全信息和共同知识，而现实多智能体系统存在不确定性、认知偏差和嵌套信念。超博弈理论通过建模主观感知解决了这些问题。

Method: 通过分析44项研究，提出智能体兼容性标准和分类框架，评估超博弈理论在动态多智能体系统中的集成模式和应用潜力。

Result: 研究发现分层和图模型在欺骗性推理中占主导，实际应用中简化了理论框架，但HNF模型采用较少，缺乏形式化语言。

Conclusion: 本文总结了超博弈理论的趋势、挑战和未来方向，为动态多智能体环境中的战略建模提供了新路线图。

Abstract: Classical game-theoretic models typically assume rational agents, complete
information, and common knowledge of payoffs - assumptions that are often
violated in real-world MAS characterized by uncertainty, misaligned
perceptions, and nested beliefs. To overcome these limitations, researchers
have proposed extensions that incorporate models of cognitive constraints,
subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory
extends the classical paradigm by explicitly modeling agents' subjective
perceptions of the strategic scenario, known as perceptual games, in which
agents may hold divergent beliefs about the structure, payoffs, or available
actions. We present a systematic review of agent-compatible applications of
hypergame theory, examining how its descriptive capabilities have been adapted
to dynamic and interactive MAS contexts. We analyze 44 selected studies from
cybersecurity, robotics, social simulation, communications, and general
game-theoretic modeling. Building on a formal introduction to hypergame theory
and its two major extensions - hierarchical hypergames and HNF - we develop
agent-compatibility criteria and an agent-based classification framework to
assess integration patterns and practical applicability. Our analysis reveals
prevailing tendencies, including the prevalence of hierarchical and graph-based
models in deceptive reasoning and the simplification of extensive theoretical
frameworks in practical applications. We identify structural gaps, including
the limited adoption of HNF-based models, the lack of formal hypergame
languages, and unexplored opportunities for modeling human-agent and
agent-agent misalignment. By synthesizing trends, challenges, and open research
directions, this review provides a new roadmap for applying hypergame theory to
enhance the realism and effectiveness of strategic modeling in dynamic
multi-agent environments.

</details>


### [20] [DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference](https://arxiv.org/abs/2507.19608)
*Jiawen Qi,Chang Gao,Zhaochun Ren,Qinyu Chen*

Main category: cs.AI

TL;DR: DeltaLLM是一个无需训练的框架，通过利用注意力模式的时间稀疏性，在资源受限的边缘设备上实现高效的大型语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在边缘设备上的部署因计算量随序列长度二次增长而具有挑战性，现有动态注意力修剪方法不适合边缘场景，DeltaLLM旨在解决这一问题。

Method: DeltaLLM采用准确性和内存感知的delta矩阵构建策略引入时间稀疏性，并结合上下文感知的混合注意力机制，局部窗口内使用完整注意力，外部使用delta近似以提高准确性。

Result: 在BitNet和Llama模型上，DeltaLLM在预填充和解码阶段分别实现了高达60%和57%的注意力稀疏性，同时保持或略微提高准确性。

Conclusion: DeltaLLM为边缘设备上的高效部署提供了无需微调的解决方案，并能无缝集成到现有推理流程中。

Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging
due to their quadratically increasing computations with the sequence length.
Existing studies for dynamic attention pruning are designed for hardware with
massively parallel computation capabilities, such as GPUs or TPUs, and aim at
long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We
present DeltaLLM, a training-free framework that exploits temporal sparsity in
attention patterns to enable efficient LLM inference across both the prefilling
and decoding stages, on resource-constrained edge devices. DeltaLLM introduces
an accuracy- and memory-aware delta matrix construction strategy that
introduces temporal sparsity, and a context-aware hybrid attention mechanism
that combines full attention in a local context window with delta approximation
outside it to increase accuracy. We evaluate our framework on the
edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model
across diverse language tasks. The results show that on BitNet, our framework
increases the attention sparsity from 0% to 60% during the prefilling stage
with slight accuracy improvement on the WG task, and 0% to 57% across both the
prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97
on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity
during the prefilling stage and around 57% across both stages with negligible
accuracy drop. These results demonstrate that DeltaLLM offers a promising
solution for efficient edge deployment, requiring no fine-tuning and seamlessly
integrating with existing inference pipelines.

</details>


### [21] [Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges](https://arxiv.org/abs/2507.19672)
*Haoran Lu,Luyang Fang,Ruidong Zhang,Xinliang Li,Jiazhang Cai,Huimin Cheng,Lin Tang,Ziyu Liu,Zeliang Sun,Tao Wang,Yingchuan Zhang,Arif Hassan Zidan,Jinwen Xu,Jincheng Yu,Meizhi Yu,Hanqi Jiang,Xilin Gong,Weidi Luo,Bolun Sun,Yongkai Chen,Terry Ma,Shushan Wu,Yifan Zhou,Junhao Chen,Haotian Xiang,Jing Zhang,Afrar Jahin,Wei Ruan,Ke Deng,Yi Pan,Peilong Wang,Jiahui Li,Zhengliang Liu,Lu Zhang,Lin Zhao,Wei Liu,Dajiang Zhu,Xin Xing,Fei Dou,Wei Zhang,Chao Huang,Rongjie Liu,Mengrui Zhang,Yiwen Liu,Xiaoxiao Sun,Qin Lu,Zhen Xiang,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.AI

TL;DR: 本文综述了大语言模型（LLM）对齐人类价值观和意图的技术、训练协议及实证研究，分析了不同范式下的对齐方法及其核心目标间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的显著提升及其对社会影响的增加，确保其与人类价值观和意图的对齐成为关键挑战。

Method: 综述了监督微调、偏好对齐方法（如DPO、Constitutional AI等）、脑启发方法及对齐不确定性量化（AUQ）等技术，并分析了现有评估框架和基准数据集的局限性。

Result: 研究表明，监督微调可实现基本指令跟随，而偏好对齐方法更适用于复杂人类意图的对齐。

Conclusion: 总结了当前实践中的策略，并提出了监督、价值多元性、鲁棒性和持续对齐等开放性问题，为研究者和从业者提供参考。

Abstract: Due to the remarkable capabilities and growing impact of large language
models (LLMs), they have been deeply integrated into many aspects of society.
Thus, ensuring their alignment with human values and intentions has emerged as
a critical challenge. This survey provides a comprehensive overview of
practical alignment techniques, training protocols, and empirical findings in
LLM alignment. We analyze the development of alignment methods across diverse
paradigms, characterizing the fundamental trade-offs between core alignment
objectives. Our analysis shows that while supervised fine-tuning enables basic
instruction-following, preference-based methods offer more flexibility for
aligning with nuanced human intent. We discuss state-of-the-art techniques,
including Direct Preference Optimization (DPO), Constitutional AI,
brain-inspired methods, and alignment uncertainty quantification (AUQ),
highlighting their approaches to balancing quality and efficiency. We review
existing evaluation frameworks and benchmarking datasets, emphasizing
limitations such as reward misspecification, distributional robustness, and
scalable oversight. We summarize strategies adopted by leading AI labs to
illustrate the current state of practice. We conclude by outlining open
problems in oversight, value pluralism, robustness, and continuous alignment.
This survey aims to inform both researchers and practitioners navigating the
evolving landscape of LLM alignment.

</details>


### [22] [The wall confronting large language models](https://arxiv.org/abs/2507.19703)
*Peter V. Coveney,Sauro Succi*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）的扩展法则限制了其预测不确定性的改进能力，导致其难以满足科学研究的可靠性标准。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在提升预测可靠性方面的局限性，以及其生成非高斯输出分布可能导致的错误累积和信息灾难。

Method: 分析LLM的扩展法则及其与预测不确定性的关系，探讨非高斯输出分布的影响。

Result: 发现LLM在扩展过程中存在学习与准确性之间的张力，且虚假相关性随数据规模增加而加剧。

Conclusion: 为避免LLM的退化路径，需更重视对问题结构特征的洞察与理解。

Abstract: We show that the scaling laws which determine the performance of large
language models (LLMs) severely limit their ability to improve the uncertainty
of their predictions. As a result, raising their reliability to meet the
standards of scientific inquiry is intractable by any reasonable measure. We
argue that the very mechanism which fuels much of the learning power of LLMs,
namely the ability to generate non-Gaussian output distributions from Gaussian
input ones, might well be at the roots of their propensity to produce error
pileup, ensuing information catastrophes and degenerative AI behaviour. This
tension between learning and accuracy is a likely candidate mechanism
underlying the observed low values of the scaling components. It is
substantially compounded by the deluge of spurious correlations pointed out by
Calude and Longo which rapidly increase in any data set merely as a function of
its size, regardless of its nature. The fact that a degenerative AI pathway is
a very probable feature of the LLM landscape does not mean that it must
inevitably arise in all future AI research. Its avoidance, which we also
discuss in this paper, necessitates putting a much higher premium on insight
and understanding of the structural characteristics of the problems being
investigated.

</details>


### [23] [Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors](https://arxiv.org/abs/2507.19725)
*Leonardo Villalobos-Arias,Grant Forbes,Jianxun Wang,David L Roberts,Arnav Jhala*

Main category: cs.AI

TL;DR: 论文研究了内在动机（IM）方法在强化学习（RL）中可能导致的行为变化和奖励滥用问题，并通过实验验证了广义奖励匹配（GRM）的缓解效果。


<details>
  <summary>Details</summary>
Motivation: 游戏环境中的奖励稀疏性使得RL代理难以学习，IM方法虽能缓解这一问题，但可能引发奖励滥用行为。目前对IM如何影响代理行为的研究不足。

Method: 在MiniGrid环境中，比较了三种IM技术与GRM方法对RL代理行为的影响。

Result: IM显著改变了代理的行为，增加了初始奖励，但也导致奖励滥用；GRM在某些场景下能有效缓解这一问题。

Conclusion: IM确实会改变RL代理的行为，而GRM是一种有潜力的解决方案，但需进一步研究其适用性。

Abstract: Games are challenging for Reinforcement Learning~(RL) agents due to their
reward-sparsity, as rewards are only obtainable after long sequences of
deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce
exploration rewards -- are an effective solution to reward-sparsity. However,
IM also causes an issue known as `reward hacking' where the agent optimizes for
the new reward at the expense of properly playing the game. The larger problem
is that reward hacking itself is largely unknown; there is no answer to
whether, and to what extent, IM rewards change the behavior of RL agents. This
study takes a first step by empirically evaluating the impact on behavior of
three IM techniques on the MiniGrid game-like environment. We compare these IM
models with Generalized Reward Matching~(GRM), a method that can be used with
any intrinsic reward function to guarantee optimality. Our results suggest that
IM causes noticeable change by increasing the initial rewards, but also
altering the way the agent plays; and that GRM mitigated reward hacking in some
scenarios.

</details>


### [24] [HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare](https://arxiv.org/abs/2507.19726)
*Yuzhang Xie,Xu Han,Ran Xu,Xiao Hu,Jiaying Lu,Carl Yang*

Main category: cs.AI

TL;DR: HypKG框架通过整合电子健康记录（EHRs）和知识图谱（KGs），生成情境化知识表示，提升医疗预测准确性。


<details>
  <summary>Details</summary>
Motivation: 通用知识图谱缺乏对患者具体情境的考虑，而电子健康记录提供了丰富的个人数据，两者结合可提升精准医疗的效果。

Method: 使用实体链接技术将通用KGs与EHRs数据连接，通过超图模型和超图变换器学习情境化表示。

Result: 实验表明，HypKG在多项医疗预测任务中显著优于基线方法，并能调整KGs中的实体和关系表示。

Conclusion: HypKG通过整合外部情境，提升了知识图谱的实用性和医疗预测的准确性。

Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are
widely used in various application domains. Healthcare is one of such domains
where KGs are intensively used, due to the high requirement for knowledge
accuracy and interconnected nature of healthcare data. However, KGs storing
general factual information often lack the ability to account for important
contexts of the knowledge such as the status of specific patients, which are
crucial in precision healthcare. Meanwhile, electronic health records (EHRs)
provide rich personal data, including various diagnoses and medications, which
provide natural contexts for general KGs. In this paper, we propose HypKG, a
framework that integrates patient information from EHRs into KGs to generate
contextualized knowledge representations for accurate healthcare predictions.
Using advanced entity-linking techniques, we connect relevant knowledge from
general KGs with patient information from EHRs, and then utilize a hypergraph
model to "contextualize" the knowledge with the patient information. Finally,
we employ hypergraph transformers guided by downstream prediction tasks to
jointly learn proper contextualized representations for both KGs and patients,
fully leveraging existing knowledge in KGs and patient contexts in EHRs. In
experiments using a large biomedical KG and two real-world EHR datasets, HypKG
demonstrates significant improvements in healthcare prediction tasks across
multiple evaluation metrics. Additionally, by integrating external contexts,
HypKG can learn to adjust the representations of entities and relations in KG,
potentially improving the quality and real-world utility of knowledge.

</details>


### [25] [Integrating Activity Predictions in Knowledge Graphs](https://arxiv.org/abs/2507.19733)
*Alec Scully,Cameron Stockton,Forrest Hare*

Main category: cs.AI

TL;DR: 论文提出利用本体结构的知识图谱预测未来事件，通过BFO和CCO组织数据并生成马尔可夫链模型，提出“时空实例”概念，改进概率模型，并将结果反馈至知识图谱。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用本体结构的知识图谱改进未来事件的预测能力，解决现有概率模型的局限性。

Method: 结合BFO和CCO构建知识图谱，引入“时空实例”概念，使用马尔可夫链模型预测未来状态，并提出新的概率处理方式。

Result: 成功将数据组织为知识图谱并生成预测模型，改进了概率的表示方式，实现了预测结果与知识图谱的无缝集成。

Conclusion: 本体结构的知识图谱结合马尔可夫链模型能有效预测未来事件，提出的概率处理方式更符合现实动态。

Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
which conflates probability with likelihood and relies on the problematic
concept of modal measurements: measurements of future entities. We propose an
alternative view, where probabilities are treated as being about process
profiles, which better captures the dynamics of real world phenomena. Finally,
we demonstrate how our Markov chain based probability calculations can be
seamlessly integrated back into the knowledge graph, enabling further analysis
and decision-making. Keywords: predictive analytics, ontology, Markov chains,
probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.

</details>


### [26] [Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)](https://arxiv.org/abs/2507.19749)
*Lin Ren,Guohui Xiao,Guilin Qi,Yishuai Geng,Haohan Xue*

Main category: cs.AI

TL;DR: ASPBench是一个新的ASP基准测试，揭示了当前大型语言模型在ASP求解中的局限性，尤其是在核心任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型在ASP中能力的评估有限，缺乏专门设计的任务和复杂程序支持。

Method: 引入ASPBench，包含三个ASP特定任务：ASP蕴含、答案集验证和答案集计算。

Result: 14个先进的大型语言模型在前两个简单任务上表现较好，但在核心任务答案集计算上表现不佳。

Conclusion: 需要更有效整合符号推理能力的新方法。

Abstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic
reasoning. Recently, large language models (LLMs) have demonstrated promising
capabilities in logical reasoning. Despite this potential, current evaluations
of LLM capabilities in ASP are often limited. Existing works normally employ
overly simplified ASP programs, do not support negation, disjunction, or
multiple answer sets. Furthermore, there is a lack of benchmarks that introduce
tasks specifically designed for ASP solving. To bridge this gap, we introduce
ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:
ASP entailment, answer set verification, and answer set computation. Our
extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,
including \emph{deepseek-r1}, \emph{o4-mini}, and
\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two
simpler tasks, they struggle with answer set computation, which is the core of
ASP solving. These findings offer insights into the current limitations of LLMs
in ASP solving. This highlights the need for new approaches that integrate
symbolic reasoning capabilities more effectively. The code and dataset are
available at https://github.com/HomuraT/ASPBench.

</details>


### [27] [Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation](https://arxiv.org/abs/2507.19788)
*Rifny Rachman,Josh Tingey,Richard Allmendinger,Pradyumn Shukla,Wei Pan*

Main category: cs.AI

TL;DR: 本文提出了一种基于马尔可夫决策过程的多目标、多层级供应链优化模型，结合经济、环境和社会因素，并通过多目标强化学习方法进行评估。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决非平稳市场下供应链优化问题，同时平衡经济、环境和社会目标。

Method: 采用多目标强化学习（RL）方法，并与改进的单目标RL算法和多目标进化算法（MOEA）进行对比。

Result: 主要方法在最优性、多样性和密度方面表现最佳，复杂场景下超体积比MOEA高75%，解决方案密度是改进单目标RL的11倍。

Conclusion: 该方法在复杂供应链中实现了稳定的生产和库存水平，同时最小化需求损失，表现出更好的鲁棒性。

Abstract: This study develops a generalised multi-objective, multi-echelon supply chain
optimisation model with non-stationary markets based on a Markov decision
process, incorporating economic, environmental, and social considerations. The
model is evaluated using a multi-objective reinforcement learning (RL) method,
benchmarked against an originally single-objective RL algorithm modified with
weighted sum using predefined weights, and a multi-objective evolutionary
algorithm (MOEA)-based approach. We conduct experiments on varying network
complexities, mimicking typical real-world challenges using a customisable
simulator. The model determines production and delivery quantities across
supply chain routes to achieve near-optimal trade-offs between competing
objectives, approximating Pareto front sets. The results demonstrate that the
primary approach provides the most balanced trade-off between optimality,
diversity, and density, further enhanced with a shared experience buffer that
allows knowledge transfer among policies. In complex settings, it achieves up
to 75\% higher hypervolume than the MOEA-based method and generates solutions
that are approximately eleven times denser, signifying better robustness, than
those produced by the modified single-objective RL method. Moreover, it ensures
stable production and inventory levels while minimising demand loss.

</details>


### [28] [Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation](https://arxiv.org/abs/2507.19882)
*Xinshu Li,Ruoyu Wang,Erdun Gao,Mingming Gong,Lina Yao*

Main category: cs.AI

TL;DR: 论文提出了一种基于扩散的反事实提示学习框架DiCap，通过理论推导和对比学习生成因果不变的提示，显著提升了跨类别任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法因缺乏理论支持，难以生成因果不变的提示，导致特征提取不够鲁棒。

Method: 采用扩散过程从因果模型的边际和条件分布中迭代采样梯度，生成满足最小充分性准则的反事实，并结合对比学习优化提示。

Result: 实验表明，DiCap在图像分类、图文检索和视觉问答等任务中表现优异，尤其在未见类别上优势明显。

Conclusion: DiCap通过理论驱动的反事实生成和对比学习，显著提升了提示学习的鲁棒性和泛化能力。

Abstract: Prompt learning has garnered attention for its efficiency over traditional
model training and fine-tuning. However, existing methods, constrained by
inadequate theoretical foundations, encounter difficulties in achieving
causally invariant prompts, ultimately falling short of capturing robust
features that generalize effectively across categories. To address these
challenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoretically
grounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual
$\textbf{p}$rompt learning framework, which leverages a diffusion process to
iteratively sample gradients from the marginal and conditional distributions of
the causal model, guiding the generation of counterfactuals that satisfy the
minimal sufficiency criterion. Grounded in rigorous theoretical derivations,
this approach guarantees the identifiability of counterfactual outcomes while
imposing strict bounds on estimation errors. We further employ a contrastive
learning framework that leverages the generated counterfactuals, thereby
enabling the refined extraction of prompts that are precisely aligned with the
causal features of the data. Extensive experimental results demonstrate that
our method performs excellently across tasks such as image classification,
image-text retrieval, and visual question answering, with particularly strong
advantages in unseen categories.

</details>


### [29] [What Does 'Human-Centred AI' Mean?](https://arxiv.org/abs/2507.19960)
*Olivia Guest*

Main category: cs.AI

TL;DR: 论文探讨了以人为中心的人工智能（AI）本质上是技术与人类认知的关系，分析了AI对人类认知劳动的替代、增强或取代，并强调忽视这种关系会扭曲认知科学和AI工程。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于澄清AI与人类认知的关系，避免因忽视这种关系而导致的认知科学扭曲和AI工程中的人类中心化缺失。

Method: 通过对比技术与人认知劳动的例子（如算盘与心算、闹钟与叫醒服务），提出新的定义和分析框架，将社会技术关系分为替代（有害）、增强（有益）和取代（中性）。

Result: 研究发现所有AI都涉及人类认知，忽视这一点会阻碍批判性思考、扭曲认知科学，并限制AI系统中真正以人为中心的设计。

Conclusion: 结论强调必须正视AI中的人类认知角色，才能真正实现以人为中心的AI设计。

Abstract: While it seems sensible that human-centred artificial intelligence (AI) means
centring "human behaviour and experience," it cannot be any other way. AI, I
argue, is usefully seen as a relationship between technology and humans where
it appears that artifacts can perform, to a greater or lesser extent, human
cognitive labour. This is evinced using examples that juxtapose technology with
cognition, inter alia: abacus versus mental arithmetic; alarm clock versus
knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel
definitions and analyses, sociotechnical relationships can be analysed into
varying types of: displacement (harmful), enhancement (beneficial), and/or
replacement (neutral) of human cognitive labour. Ultimately, all AI implicates
human cognition; no matter what. Obfuscation of cognition in the AI context --
from clocks to artificial neural networks -- results in distortion, in slowing
critical engagement, perverting cognitive science, and indeed in limiting our
ability to truly centre humans and humanity in the engineering of AI systems.
To even begin to de-fetishise AI, we must look the human-in-the-loop in the
eyes.

</details>


### [30] [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
*Ebrahim Rasromani,Stella K. Kang,Yanqi Xu,Beisong Liu,Garvit Luhadia,Wan Fung Chui,Felicia L. Pasadyn,Yu Chih Hung,Julie Y. An,Edwin Mathieu,Zehui Gu,Carlos Fernandez-Granda,Ammar A. Javed,Greg D. Sacks,Tamas Gonda,Chenchan Huang,Yiqiu Shen*

Main category: cs.AI

TL;DR: 利用链式思维（CoT）提示微调开源大语言模型（LLMs），实现了从MRI/CT报告中自动提取胰腺囊性病变（PCL）特征并分类风险，性能媲美GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 手动提取PCL特征耗时且难以大规模研究，需开发自动化工具以推动PCL研究。

Method: 使用GPT-4o生成的CoT数据微调LLaMA和DeepSeek模型，基于指南映射特征至风险类别，并在285份人工标注报告上评估。

Result: 微调后模型特征提取准确率显著提升（LLaMA: 97%，DeepSeek: 98%），风险分类F1分数接近GPT-4o（0.97），与放射科医生一致性高。

Conclusion: 微调开源LLMs结合CoT监督，为大规模PCL研究提供了高效、准确的自动化解决方案，性能与GPT-4o相当。

Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

</details>


### [31] [Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application](https://arxiv.org/abs/2507.19974)
*Tongjie Li,Jianhua Zhang,Li Yu,Yuxiang Zhang,Yunlong Cai,Fan Xu,Guangyi Liu*

Main category: cs.AI

TL;DR: 论文提出了一种基于数字孪生信道（DTC）的在线优化框架，用于6G网络中低延迟、可靠的资源分配，通过环境感知预测CSI，结合轻量级博弈论算法，显著提升了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 6G网络中新兴应用（如全息通信、自动驾驶）对灵活、低延迟和可靠的资源分配提出了严格要求，传统统计建模方法在动态环境中可能表现不佳，且实时获取CSI需要高开销。

Method: 采用数字孪生信道（DTC）预测CSI，结合轻量级博弈论算法进行在线资源分配。

Result: 仿真结果显示，该方法比基于导频的理想CSI方案吞吐量提升高达11.5%。

Conclusion: 该方法为未来6G网络提供了可扩展、低开销且环境感知的通信解决方案。

Abstract: Emerging applications such as holographic communication, autonomous driving,
and the industrial Internet of Things impose stringent requirements on
flexible, low-latency, and reliable resource allocation in 6G networks.
Conventional methods, which rely on statistical modeling, have proven effective
in general contexts but may fail to achieve optimal performance in specific and
dynamic environments. Furthermore, acquiring real-time channel state
information (CSI) typically requires excessive pilot overhead. To address these
challenges, a digital twin channel (DTC)-enabled online optimization framework
is proposed, in which DTC is employed to predict CSI based on environmental
sensing. The predicted CSI is then utilized by lightweight game-theoretic
algorithms to perform online resource allocation in a timely and efficient
manner. Simulation results based on a digital replica of a realistic industrial
workshop demonstrate that the proposed method achieves throughput improvements
of up to 11.5\% compared with pilot-based ideal CSI schemes, validating its
effectiveness for scalable, low-overhead, and environment-aware communication
in future 6G networks.

</details>


### [32] [Matching Game Preferences Through Dialogical Large Language Models: A Perspective](https://arxiv.org/abs/2507.20000)
*Renaud Fabre,Daniel Egret,Patrice Bellot*

Main category: cs.AI

TL;DR: 本文探讨了如何结合大型语言模型（LLMs）与GRAPHYP网络系统，以提升对话智能的透明度和可追溯性，提出了一种名为D-LLMs的框架。


<details>
  <summary>Details</summary>
Motivation: 旨在通过透明化和可追溯的AI推理过程，增强人类对AI决策的理解和信任。

Method: 提出了D-LLM框架，包含推理过程、用户偏好分类系统和对话方法三个主要组件。

Result: 设想了一种可解释的AI系统，用户能查看和理解AI决策背后的偏好和逻辑。

Conclusion: 目标是开发更透明、可信的AI系统，帮助人类更好地理解和利用AI决策。

Abstract: This perspective paper explores the future potential of "conversational
intelligence" by examining how Large Language Models (LLMs) could be combined
with GRAPHYP's network system to better understand human conversations and
preferences. Using recent research and case studies, we propose a conceptual
framework that could make AI rea-soning transparent and traceable, allowing
humans to see and understand how AI reaches its conclusions. We present the
conceptual perspective of "Matching Game Preferences through Dialogical Large
Language Models (D-LLMs)," a proposed system that would allow multiple users to
share their different preferences through structured conversations. This
approach envisions personalizing LLMs by embedding individual user preferences
directly into how the model makes decisions. The proposed D-LLM framework would
require three main components: (1) reasoning processes that could analyze
different search experiences and guide performance, (2) classification systems
that would identify user preference patterns, and (3) dialogue approaches that
could help humans resolve conflicting information. This perspective framework
aims to create an interpretable AI system where users could examine,
understand, and combine the different human preferences that influence AI
responses, detected through GRAPHYP's search experience networks. The goal of
this perspective is to envision AI systems that would not only provide answers
but also show users how those answers were reached, making artificial
intelligence more transparent and trustworthy for human decision-making.

</details>


### [33] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*Müge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: 论文研究了稳定室友问题，提出了一种基于代理人习惯和偏好网络的个性化匹配方法，以解决无稳定解时的“足够好”匹配问题。


<details>
  <summary>Details</summary>
Motivation: 稳定室友问题在实际应用中并不总是有解，因此需要研究如何生成“足够好”的匹配。

Method: 结合代理人的习惯、偏好及其朋友网络，提出了一种生成个性化匹配的方法。

Result: 通过示例和实证评估验证了方法的有效性。

Conclusion: 该方法为解决稳定室友问题提供了一种可行的个性化解决方案。

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>


### [34] [PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](https://arxiv.org/abs/2507.20067)
*Sarat Chandra Bobbili,Ujwal Dinesha,Dheeraj Narasimha,Srinivas Shakkottai*

Main category: cs.AI

TL;DR: PITA框架通过直接整合偏好反馈到LLM的token生成中，消除了对预训练奖励模型的依赖，降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练的奖励模型，这可能是一个不稳定的过程，PITA旨在解决这一问题。

Method: PITA学习一个基于偏好的小型指导策略，在推理时修改token概率，无需LLM微调。

Result: PITA在数学推理和情感分类等任务中有效对齐LLM输出与用户偏好。

Conclusion: PITA提供了一种高效且稳定的方法，无需奖励模型即可实现LLM输出的对齐。

Abstract: Inference-time alignment enables large language models (LLMs) to generate
outputs aligned with end-user preferences without further training. Recent
post-training methods achieve this by using small guidance models to modify
token generation during inference. These methods typically optimize a reward
function KL-regularized by the original LLM taken as the reference policy. A
critical limitation, however, is their dependence on a pre-trained reward
model, which requires fitting to human preference feedback--a potentially
unstable process. In contrast, we introduce PITA, a novel framework that
integrates preference feedback directly into the LLM's token generation,
eliminating the need for a reward model. PITA learns a small preference-based
guidance policy to modify token probabilities at inference time without LLM
fine-tuning, reducing computational cost and bypassing the pre-trained reward
model dependency. The problem is framed as identifying an underlying preference
distribution, solved through stochastic search and iterative refinement of the
preference-based guidance model. We evaluate PITA across diverse tasks,
including mathematical reasoning and sentiment classification, demonstrating
its effectiveness in aligning LLM outputs with user preferences.

</details>


### [35] [Concept Learning for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.20143)
*Zhonghan Ge,Yuanyang Zhu,Chunlin Chen*

Main category: cs.AI

TL;DR: 论文提出了一种基于概念瓶颈模型的可解释值分解框架CMQ，用于多智能体强化学习，通过显式学习合作概念提升透明度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络在多智能体强化学习中缺乏透明度和互操作性，其隐含的合作机制难以理解。

Method: 提出CMQ方法，通过监督向量表示合作概念，利用全局状态嵌入的条件化动作值来增强合作表示能力。

Result: 在StarCraft II和LBF任务中，CMQ性能优于现有方法，并能捕捉有意义的合作模式。

Conclusion: CMQ突破了性能与可解释性的权衡，支持测试时概念干预，有助于检测合作偏差和伪影。

Abstract: Despite substantial progress in applying neural networks (NN) to multi-agent
reinforcement learning (MARL) areas, they still largely suffer from a lack of
transparency and interoperability. However, its implicit cooperative mechanism
is not yet fully understood due to black-box networks. In this work, we study
an interpretable value decomposition framework via concept bottleneck models,
which promote trustworthiness by conditioning credit assignment on an
intermediate level of human-like cooperation concepts. To address this problem,
we propose a novel value-based method, named Concepts learning for Multi-agent
Q-learning (CMQ), that goes beyond the current performance-vs-interpretability
trade-off by learning interpretable cooperation concepts. CMQ represents each
cooperation concept as a supervised vector, as opposed to existing models where
the information flowing through their end-to-end mechanism is concept-agnostic.
Intuitively, using individual action value conditioning on global state
embeddings to represent each concept allows for extra cooperation
representation capacity. Empirical evaluations on the StarCraft II
micromanagement challenge and level-based foraging (LBF) show that CMQ achieves
superior performance compared with the state-of-the-art counterparts. The
results also demonstrate that CMQ provides more cooperation concept
representation capturing meaningful cooperation modes, and supports test-time
concept interventions for detecting potential biases of cooperation mode and
identifying spurious artifacts that impact cooperation.

</details>


### [36] [The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models](https://arxiv.org/abs/2507.20150)
*Xingcheng Xu*

Main category: cs.AI

TL;DR: 本文提出了一个数学框架，用于分析强化学习（RL）中奖励函数到最优策略的映射稳定性，解释了策略脆弱性的根源，并提出了熵正则化作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大型语言和推理模型（LLMs/LRMs）中常导致脆弱和不稳定的策略，引发虚假推理、欺骗性对齐和指令不服从等问题，缺乏统一的理论解释。

Method: 通过数学框架分析奖励函数到最优策略的映射稳定性，探讨非唯一最优动作的影响，并扩展到多奖励RL和熵正则化的作用。

Result: 理论分析揭示了策略脆弱性的根源，验证了熵正则化能恢复稳定性但增加随机性，并通过扰动实验在多奖励RL中验证了框架的有效性。

Conclusion: 该框架为RL策略稳定性提供了统一的理论解释，为设计更安全、可信赖的AI系统提供了重要见解。

Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of
large language and reasoning models (LLMs/LRMs). However, it often produces
brittle and unstable policies, leading to critical failures such as spurious
reasoning, deceptive alignment, and instruction disobedience that undermine the
trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified
theoretical explanation and are typically addressed using ad-hoc heuristics.
This paper presents a rigorous mathematical framework for analyzing the
stability of the mapping from a reward function to the optimal policy. We show
that policy brittleness often stems from non-unique optimal actions, a common
occurrence when multiple valid traces exist in a reasoning task. This
theoretical lens provides a unified explanation for a range of seemingly
disparate failures, reframing them as rational outcomes of optimizing rewards
that may be incomplete or noisy, especially in the presence of action
degeneracy. We extend this analysis from the fundamental single-reward setting
to the more realistic multi-reward RL across diverse domains, showing how
stability is governed by an "effective reward" aggregation mechanism. We also
prove that entropy regularization restores policy stability at the cost of
increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following
trade-offs, and RLHF-induced sophistry, and is further validated through
perturbation experiments in multi-reward RL. This work advances
policy-stability analysis from empirical heuristics towards a principled
theory, offering essential insights for designing safer and more trustworthy AI
systems.

</details>


### [37] [Adaptive Fuzzy Time Series Forecasting via Partially Asymmetric Convolution and Sub-Sliding Window Fusion](https://arxiv.org/abs/2507.20641)
*Lijian Li*

Main category: cs.AI

TL;DR: 提出了一种基于自适应模糊时间序列和部分不对称卷积架构的新方法，用于时间序列预测，解决了现有模型在时空依赖性和全局信息整合上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的预测模型在学习阶段缺乏捕捉时空依赖性和整合全局信息的能力。

Method: 1. 改进传统模糊时间序列构建策略，提取短期和长期时间相关性；2. 设计双边Atrous算法以减少计算需求；3. 设计部分不对称卷积架构以灵活挖掘数据特征。

Result: 在多个流行时间序列数据集上取得了最先进的结果。

Conclusion: 该方法通过自适应模糊化和部分不对称设计，显著提升了时间序列预测的准确性。

Abstract: At present, state-of-the-art forecasting models are short of the ability to
capture spatio-temporal dependency and synthesize global information at the
stage of learning. To address this issue, in this paper, through the adaptive
fuzzified construction of temporal data, we propose a novel convolutional
architecture with partially asymmetric design based on the scheme of sliding
window to realize accurate time series forecasting. First, the construction
strategy of traditional fuzzy time series is improved to further extract short
and long term temporal interrelation, which enables every time node to
automatically possess corresponding global information and inner relationships
among them in a restricted sliding window and the process does not require
human involvement. Second, a bilateral Atrous algorithm is devised to reduce
calculation demand of the proposed model without sacrificing global
characteristics of elements. And it also allows the model to avoid processing
redundant information. Third, after the transformation of time series, a
partially asymmetric convolutional architecture is designed to more flexibly
mine data features by filters in different directions on feature maps, which
gives the convolutional neural network (CNN) the ability to construct
sub-windows within existing sliding windows to model at a more fine-grained
level. And after obtaining the time series information at different levels, the
multi-scale features from different sub-windows will be sent to the
corresponding network layer for time series information fusion. Compared with
other competitive modern models, the proposed method achieves state-of-the-art
results on most of popular time series datasets, which is fully verified by the
experimental results.

</details>


### [38] [StepFun-Prover Preview: Let's Think and Verify Step by Step](https://arxiv.org/abs/2507.20199)
*Shijie Shang,Ruosi Wan,Yue Peng,Yutong Wu,Xiong-hui Chen,Jie Yan,Xiangyu Zhang*

Main category: cs.AI

TL;DR: StepFun-Prover是一个用于形式定理证明的大语言模型，通过工具集成推理实现高效Lean 4证明生成，在miniF2F-test上达到70.0%的pass@1成功率。


<details>
  <summary>Details</summary>
Motivation: 推动自动定理证明和数学AI助手的发展，通过工具集成推理模拟人类问题解决策略。

Method: 采用强化学习管道，结合工具交互，迭代优化证明生成。

Result: 在miniF2F-test基准测试中，pass@1成功率为70.0%。

Conclusion: 提出了一种端到端的训练框架，为自动定理证明和数学AI助手提供了新方向。

Abstract: We present StepFun-Prover Preview, a large language model designed for formal
theorem proving through tool-integrated reasoning. Using a reinforcement
learning pipeline that incorporates tool-based interactions, StepFun-Prover can
achieve strong performance in generating Lean 4 proofs with minimal sampling.
Our approach enables the model to emulate human-like problem-solving strategies
by iteratively refining proofs based on real-time environment feedback. On the
miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of
$70.0\%$. Beyond advancing benchmark performance, we introduce an end-to-end
training framework for developing tool-integrated reasoning models, offering a
promising direction for automated theorem proving and Math AI assistant.

</details>


### [39] [Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks](https://arxiv.org/abs/2507.20226)
*Shuyang Guo,Wenjin Xie,Ping Lu,Ting Deng,Richong Zhang,Jianxin Li,Xiangping Huang,Zhongyi Liu*

Main category: cs.AI

TL;DR: HFrame是一个基于图神经网络的子图同态框架，结合传统算法与机器学习，性能优于标准图神经网络，并提供了泛化误差界限。


<details>
  <summary>Details</summary>
Motivation: 子图同态问题比子图同构更复杂，传统方法效率低，需要结合机器学习提升性能。

Method: 提出HFrame框架，结合图神经网络与传统算法，用于子图同态问题。

Result: HFrame比精确匹配算法快101.91倍，平均准确率达0.962，能区分更多非同态图对。

Conclusion: HFrame在子图同态问题中表现出色，兼具高效性和准确性。

Abstract: Homomorphism is a key mapping technique between graphs that preserves their
structure. Given a graph and a pattern, the subgraph homomorphism problem
involves finding a mapping from the pattern to the graph, ensuring that
adjacent vertices in the pattern are mapped to adjacent vertices in the graph.
Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism
allows multiple vertices in the pattern to map to the same vertex in the graph,
making it more complex. We propose HFrame, the first graph neural network-based
framework for subgraph homomorphism, which integrates traditional algorithms
with machine learning techniques. We demonstrate that HFrame outperforms
standard graph neural networks by being able to distinguish more graph pairs
where the pattern is not homomorphic to the graph. Additionally, we provide a
generalization error bound for HFrame. Through experiments on both real-world
and synthetic graphs, we show that HFrame is up to 101.91 times faster than
exact matching algorithms and achieves an average accuracy of 0.962.

</details>


### [40] [A Multi-Agent System for Information Extraction from the Chemical Literature](https://arxiv.org/abs/2507.20230)
*Yufan Chen,Ching Ting Leung,Bowen Yu,Jianwei Sun,Yong Huang,Linyan Li,Hao Chen,Hanyu Gao*

Main category: cs.AI

TL;DR: 开发了一种基于多模态大语言模型（MLLM）的多智能体系统，用于自动提取化学信息，显著提升了复杂化学反应图形的识别性能。


<details>
  <summary>Details</summary>
Motivation: 高质量化学数据库是AI驱动化学研究的基础，但现有方法受限于化学信息的多模态和风格多样性。

Method: 利用MLLM的强推理能力理解复杂化学图形结构，将提取任务分解为子任务，并由多个专门智能体协同完成。

Result: 在复杂化学反应图形的基准数据集上，系统F1得分为80.8%，显著优于之前的最佳模型（35.6%）。

Conclusion: 该系统是实现化学信息自动提取的关键进展，将推动AI驱动的化学研究。

Abstract: To fully expedite AI-powered chemical research, high-quality chemical
databases are the cornerstone. Automatic extraction of chemical information
from the literature is essential for constructing reaction databases, but it is
currently limited by the multimodality and style variability of chemical
information. In this work, we developed a multimodal large language model
(MLLM)-based multi-agent system for automatic chemical information extraction.
We used the MLLM's strong reasoning capability to understand the structure of
complex chemical graphics, decompose the extraction task into sub-tasks and
coordinate a set of specialized agents to solve them. Our system achieved an F1
score of 80.8% on a benchmark dataset of complex chemical reaction graphics
from the literature, surpassing the previous state-of-the-art model (F1 score:
35.6%) by a significant margin. Additionally, it demonstrated consistent
improvements in key sub-tasks, including molecular image recognition, reaction
image parsing, named entity recognition and text-based reaction extraction.
This work is a critical step toward automated chemical information extraction
into structured datasets, which will be a strong promoter of AI-driven chemical
research.

</details>


### [41] [SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration](https://arxiv.org/abs/2507.20280)
*Keyan Ding,Jing Yu,Junjie Huang,Yuchen Yang,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: SciToolAgent是一个基于大语言模型的代理，通过知识图谱和增强生成技术自动化科学工具的使用，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 科学工具的使用需要专业知识，现有大语言模型在多工具集成和复杂工作流中表现不足。

Method: 利用科学工具知识图谱进行智能选择和执行，结合安全检查模块确保伦理使用。

Result: 在多个科学领域（如生物学、化学、材料科学）的评估中表现优异，案例研究验证了其复杂工作流自动化能力。

Conclusion: SciToolAgent使高级研究工具对专家和非专家都更易用，推动了科学研究的可及性。

Abstract: Scientific research increasingly relies on specialized computational tools,
yet effectively utilizing these tools demands substantial domain expertise.
While Large Language Models (LLMs) show promise in tool automation, they
struggle to seamlessly integrate and orchestrate multiple tools for complex
scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that
automates hundreds of scientific tools across biology, chemistry, and materials
science. At its core, SciToolAgent leverages a scientific tool knowledge graph
that enables intelligent tool selection and execution through graph-based
retrieval-augmented generation. The agent also incorporates a comprehensive
safety-checking module to ensure responsible and ethical tool usage. Extensive
evaluations on a curated benchmark demonstrate that SciToolAgent significantly
outperforms existing approaches. Case studies in protein engineering, chemical
reactivity prediction, chemical synthesis, and metal-organic framework
screening further demonstrate SciToolAgent's capability to automate complex
scientific workflows, making advanced research tools accessible to both experts
and non-experts.

</details>


### [42] [Artificial Intelligence In Patent And Market Intelligence: A New Paradigm For Technology Scouting](https://arxiv.org/abs/2507.20322)
*Manish Verma,Vivek Sharma,Vishal Singh*

Main category: cs.AI

TL;DR: 开发了一个基于AI的软件平台，利用大型语言模型（LLMs）改进工业研发中的技术搜寻和解决方案发现。


<details>
  <summary>Details</summary>
Motivation: 传统方法耗时、依赖人工和领域专业知识，且信息来源分散，导致效率低下和见解不完整。

Method: 平台利用LLMs的语义理解、上下文推理和跨领域知识提取能力，分析专利文本并整合商业情报。

Result: 平台减少了人工努力，加速了创新周期，并提升了复杂研发环境中的决策质量。

Conclusion: 该AI驱动的平台为工业研发提供了高效、全面的技术搜寻和解决方案发现工具。

Abstract: This paper presents the development of an AI powered software platform that
leverages advanced large language models (LLMs) to transform technology
scouting and solution discovery in industrial R&D. Traditional approaches to
solving complex research and development challenges are often time consuming,
manually driven, and heavily dependent on domain specific expertise. These
methods typically involve navigating fragmented sources such as patent
repositories, commercial product catalogs, and competitor data, leading to
inefficiencies and incomplete insights. The proposed platform utilizes cutting
edge LLM capabilities including semantic understanding, contextual reasoning,
and cross-domain knowledge extraction to interpret problem statements and
retrieve high-quality, sustainable solutions. The system processes unstructured
patent texts, such as claims and technical descriptions, and systematically
extracts potential innovations aligned with the given problem context. These
solutions are then algorithmically organized under standardized technical
categories and subcategories to ensure clarity and relevance across
interdisciplinary domains. In addition to patent analysis, the platform
integrates commercial intelligence by identifying validated market solutions
and active organizations addressing similar challenges. This combined insight
sourced from both intellectual property and real world product data enables R&D
teams to assess not only technical novelty but also feasibility, scalability,
and sustainability. The result is a comprehensive, AI driven scouting engine
that reduces manual effort, accelerates innovation cycles, and enhances
decision making in complex R&D environments.

</details>


### [43] [The Blessing and Curse of Dimensionality in Safety Alignment](https://arxiv.org/abs/2507.20333)
*Rachel S. Y. Teo,Laziz U. Abdullaev,Tan M. Nguyen*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）高维表示对安全对齐的双重影响，提出降维方法以减少线性结构被利用的风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，其高维表示虽带来优势，但也可能被利用绕过安全对齐，需研究其潜在问题。

Method: 通过可视化不同概念（如安全性）的线性子空间，并验证降维方法在保留对齐信息的同时避免线性结构。

Result: 实验证明降维显著减少模型被越狱的脆弱性，同时保持足够的安全对齐信息。

Conclusion: 高维表示既是优势也是挑战，降维为安全对齐提供了可行的解决方案。

Abstract: The focus on safety alignment in large language models (LLMs) has increased
significantly due to their widespread adoption across different domains. The
scale of LLMs play a contributing role in their success, and the growth in
parameter count follows larger hidden dimensions. In this paper, we hypothesize
that while the increase in dimensions has been a key advantage, it may lead to
emergent problems as well. These problems emerge as the linear structures in
the activation space can be exploited, in the form of activation engineering,
to circumvent its safety alignment. Through detailed visualizations of linear
subspaces associated with different concepts, such as safety, across various
model scales, we show that the curse of high-dimensional representations
uniquely impacts LLMs. Further substantiating our claim, we demonstrate that
projecting the representations of the model onto a lower dimensional subspace
can preserve sufficient information for alignment while avoiding those linear
structures. Empirical results confirm that such dimensional reduction
significantly reduces susceptibility to jailbreaking through representation
engineering. Building on our empirical validations, we provide theoretical
insights into these linear jailbreaking methods relative to a model's hidden
dimensions. Broadly speaking, our work posits that the high dimensions of a
model's internal representations can be both a blessing and a curse in safety
alignment.

</details>


### [44] [VLMPlanner: Integrating Visual Language Models with Motion Planning](https://arxiv.org/abs/2507.20342)
*Zhipeng Tang,Sha Zhang,Jiajun Deng,Chenjie Wang,Guoliang You,Yuting Huang,Xinrui Lin,Yanyong Zhang*

Main category: cs.AI

TL;DR: VLMPlanner结合视觉语言模型（VLM）与实时规划器，通过多视角图像捕捉细节，提升自动驾驶在复杂环境中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖抽象感知或地图输入，缺乏视觉上下文，难以应对复杂驾驶环境。

Method: 提出VLMPlanner框架，利用VLM处理多视角图像，并通过CAI-Gate机制动态调整推理频率。

Result: 在nuPlan基准测试中表现优异，尤其在复杂道路条件和动态场景中。

Conclusion: VLMPlanner通过视觉上下文和动态推理机制，显著提升了自动驾驶的鲁棒性和安全性。

Abstract: Integrating large language models (LLMs) into autonomous driving motion
planning has recently emerged as a promising direction, offering enhanced
interpretability, better controllability, and improved generalization in rare
and long-tail scenarios. However, existing methods often rely on abstracted
perception or map-based inputs, missing crucial visual context, such as
fine-grained road cues, accident aftermath, or unexpected obstacles, which are
essential for robust decision-making in complex driving environments. To bridge
this gap, we propose VLMPlanner, a hybrid framework that combines a
learning-based real-time planner with a vision-language model (VLM) capable of
reasoning over raw images. The VLM processes multi-view images to capture rich,
detailed visual information and leverages its common-sense reasoning
capabilities to guide the real-time planner in generating robust and safe
trajectories. Furthermore, we develop the Context-Adaptive Inference Gate
(CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by
dynamically adjusting its inference frequency based on scene complexity,
thereby achieving an optimal balance between planning performance and
computational efficiency. We evaluate our approach on the large-scale,
challenging nuPlan benchmark, with comprehensive experimental results
demonstrating superior planning performance in scenarios with intricate road
conditions and dynamic elements. Code will be available.

</details>


### [45] [Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping](https://arxiv.org/abs/2507.20377)
*Farshid Nooshi,Suining He*

Main category: cs.AI

TL;DR: 提出了一种名为HAG-PS的多智能体强化学习方法，用于动态分配城市移动资源（如共享单车/电动滑板车），解决了策略共享和内存效率问题。


<details>
  <summary>Details</summary>
Motivation: 解决城市环境中移动资源需求与供给的动态平衡问题，特别是在多智能体强化学习中如何动态共享策略和实现内存高效参数共享。

Method: 采用分层自适应分组参数共享（HAG-PS），包括全局和局部信息的分层设计、自适应智能体分组方法以及可学习的身份嵌入。

Result: 基于纽约共享单车数据的实验表明，HAG-PS在提高单车可用性等方面优于基线方法。

Conclusion: HAG-PS为动态移动资源分配提供了一种高效且可扩展的解决方案。

Abstract: Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing
vehicles) is crucial for rebalancing the mobility demand and supply in the
urban environments. We propose in this work a novel multi-agent reinforcement
learning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS)
for dynamic mobility resource allocation. HAG-PS aims to address two important
research challenges regarding multi-agent reinforcement learning for mobility
resource allocation: (1) how to dynamically and adaptively share the mobility
resource allocation policy (i.e., how to distribute mobility resources) across
agents (i.e., representing the regional coordinators of mobility resources);
and (2) how to achieve memory-efficient parameter sharing in an urban-scale
setting. To address the above challenges, we have provided following novel
designs within HAG-PS. To enable dynamic and adaptive parameter sharing, we
have designed a hierarchical approach that consists of global and local
information of the mobility resource states (e.g., distribution of mobility
resources). We have developed an adaptive agent grouping approach in order to
split or merge the groups of agents based on their relative closeness of
encoded trajectories (i.e., states, actions, and rewards). We have designed a
learnable identity (ID) embeddings to enable agent specialization beyond simple
parameter copy. We have performed extensive experimental studies based on
real-world NYC bike sharing data (a total of more than 1.2 million trips), and
demonstrated the superior performance (e.g., improved bike availability) of
HAG-PS compared with other baseline approaches.

</details>


### [46] [MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models](https://arxiv.org/abs/2507.20395)
*Hafsteinn Einarsson*

Main category: cs.AI

TL;DR: 论文提出了MazeEval基准，用于评估大语言模型（LLMs）在无视觉线索下的纯空间推理能力，发现不同模型表现差异显著，且跨语言能力受限。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在机器人和具身AI中的应用增加，评估其空间推理能力对实际部署至关重要。当前研究缺乏对无视觉线索下空间导航能力的评估。

Method: 通过MazeEval基准，使用坐标反馈和距离信息测试LLMs在迷宫导航中的表现，排除视觉输入，评估跨语言能力。

Result: OpenAI的O3模型表现最佳，其他模型在复杂迷宫中表现崩溃，跨语言测试显示冰岛语下性能显著下降。

Conclusion: LLMs的空间推理能力受训练数据和语言模式限制，需架构创新以实现跨语言的可靠导航。

Abstract: As Large Language Models (LLMs) increasingly power autonomous agents in
robotics and embodied AI, understanding their spatial reasoning capabilities
becomes crucial for ensuring reliable real-world deployment. Despite advances
in language understanding, current research lacks evaluation of how LLMs
perform spatial navigation without visual cues, a fundamental requirement for
agents operating with limited sensory information. This paper addresses this
gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure
spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our
methodology employs a function-calling interface where models navigate mazes of
varying complexity ($5\times 5$ to $15\times 15$ grids) using only coordinate
feedback and distance-to-wall information, excluding visual input to test
fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across
identical mazes in both English and Icelandic to assess cross-linguistic
transfer of spatial abilities. Our findings reveal striking disparities: while
OpenAI's O3 achieves perfect navigation for mazes up to size $30\times 30$,
other models exhibit catastrophic failure beyond $9\times 9$ mazes, with 100%
of failures attributed to excessive looping behavior where models revisit a
cell at least 10 times. We document a significant performance degradation in
Icelandic, with models solving mazes 3-4 sizes smaller than in English,
suggesting spatial reasoning in LLMs emerges from linguistic patterns rather
than language-agnostic mechanisms. These results have important implications
for global deployment of LLM-powered autonomous systems, showing spatial
intelligence remains fundamentally constrained by training data availability
and highlighting the need for architectural innovations to achieve reliable
navigation across linguistic contexts.

</details>


### [47] [Enhancing QoS in Edge Computing through Federated Layering Techniques: A Pathway to Resilient AI Lifelong Learning Systems](https://arxiv.org/abs/2507.20444)
*Chengzhuo Han*

Main category: cs.AI

TL;DR: 本文提出了一种基于联邦分层技术（FLT）的通用人工智能终身学习系统，以提高边缘计算环境中的服务质量（QoS）。通过小型模型的协作机制和隐私保护措施，该方法显著提升了学习效率和推理准确性。


<details>
  <summary>Details</summary>
Motivation: 随着6G通信网络的发展，网络环境中的数据量和复杂性急剧增加，亟需提升边缘计算中的QoS。

Method: 采用联邦分层技术（FLT），结合小型模型的协作机制和隐私保护措施，优化AI模型的运行效率和响应时间。

Result: 实验结果表明，该方法不仅提高了学习效率和推理准确性，还保护了边缘节点的隐私。

Conclusion: 该方法为构建弹性的大型模型终身学习系统提供了可行方案，显著提升了边缘计算环境中的QoS。

Abstract: In the context of the rapidly evolving information technology landscape,
marked by the advent of 6G communication networks, we face an increased data
volume and complexity in network environments. This paper addresses these
challenges by focusing on Quality of Service (QoS) in edge computing
frameworks. We propose a novel approach to enhance QoS through the development
of General Artificial Intelligence Lifelong Learning Systems, with a special
emphasis on Federated Layering Techniques (FLT). Our work introduces a
federated layering-based small model collaborative mechanism aimed at improving
AI models' operational efficiency and response time in environments where
resources are limited. This innovative method leverages the strengths of cloud
and edge computing, incorporating a negotiation and debate mechanism among
small AI models to enhance reasoning and decision-making processes. By
integrating model layering techniques with privacy protection measures, our
approach ensures the secure transmission of model parameters while maintaining
high efficiency in learning and reasoning capabilities. The experimental
results demonstrate that our strategy not only enhances learning efficiency and
reasoning accuracy but also effectively protects the privacy of edge nodes.
This presents a viable solution for achieving resilient large model lifelong
learning systems, with a significant improvement in QoS for edge computing
environments.

</details>


### [48] [STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction](https://arxiv.org/abs/2507.20451)
*Pritom Ray Nobin,Imran Ahammad Rifat*

Main category: cs.AI

TL;DR: STARN-GAT是一种多模态时空图注意力网络，用于预测交通事故严重程度，结合了道路网络拓扑、时间模式和上下文信息，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效建模影响交通事故结果的空间、时间和上下文变量之间的复杂关系，因此需要更先进的模型。

Method: 提出STARN-GAT，利用自适应图构建和多模态注意力机制，统一建模时空和上下文变量。

Result: 在FARS和ARI-BUET数据集上分别取得85%和84%的Macro F1分数，展示了模型的高效性和泛化能力。

Conclusion: STARN-GAT填补了图神经网络技术与实际道路安全分析应用之间的空白，具有实时部署潜力。

Abstract: Accurate prediction of traffic accident severity is critical for improving
road safety, optimizing emergency response strategies, and informing the design
of safer transportation infrastructure. However, existing approaches often
struggle to effectively model the intricate interdependencies among spatial,
temporal, and contextual variables that govern accident outcomes. In this
study, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention
Network, which leverages adaptive graph construction and modality-aware
attention mechanisms to capture these complex relationships. Unlike
conventional methods, STARN-GAT integrates road network topology, temporal
traffic patterns, and environmental context within a unified attention-based
framework. The model is evaluated on the Fatality Analysis Reporting System
(FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and
recall of 81 percent for severe incidents. To ensure generalizability within
the South Asian context, STARN-GAT is further validated on the ARI-BUET traffic
accident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78,
and ROC-AUC of 0.89. These results demonstrate the model's effectiveness in
identifying high-risk cases and its potential for deployment in real-time,
safety-critical traffic management systems. Furthermore, the attention-based
architecture enhances interpretability, offering insights into contributing
factors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT
bridges the gap between advanced graph neural network techniques and practical
applications in road safety analytics.

</details>


### [49] [Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition](https://arxiv.org/abs/2507.20526)
*Andy Zou,Maxwell Lin,Eliot Jones,Micha Nowak,Mateusz Dziemian,Nick Winter,Alexander Grattan,Valent Nathanael,Ayla Croft,Xander Davies,Jai Patel,Robert Kirk,Nate Burnikell,Yarin Gal,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson*

Main category: cs.AI

TL;DR: 论文研究了LLM驱动的AI代理在现实环境中的安全性，通过大规模红队竞赛发现其存在严重漏洞，并提出了ART基准以评估和改进安全性。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理在现实环境中是否能够遵循部署策略，尤其是在受到攻击时。

Method: 通过举办大规模红队竞赛，收集和分析180万次提示注入攻击，构建ART基准并评估19个前沿模型。

Result: 几乎所有代理在10-100次查询内都会违反政策，攻击在不同模型和任务间具有高转移性，且代理鲁棒性与模型规模、能力或计算资源相关性有限。

Conclusion: 当前AI代理存在严重漏洞，需要额外防御措施。ART基准的发布旨在支持更严格的安全评估和推动更安全的代理部署。

Abstract: Recent advances have enabled LLM-powered AI agents to autonomously execute
complex tasks by combining language model reasoning with tools, memory, and web
access. But can these systems be trusted to follow deployment policies in
realistic environments, especially under attack? To investigate, we ran the
largest public red-teaming competition to date, targeting 22 frontier AI agents
across 44 realistic deployment scenarios. Participants submitted 1.8 million
prompt-injection attacks, with over 60,000 successfully eliciting policy
violations such as unauthorized data access, illicit financial actions, and
regulatory noncompliance. We use these results to build the Agent Red Teaming
(ART) benchmark - a curated set of high-impact attacks - and evaluate it across
19 state-of-the-art models. Nearly all agents exhibit policy violations for
most behaviors within 10-100 queries, with high attack transferability across
models and tasks. Importantly, we find limited correlation between agent
robustness and model size, capability, or inference-time compute, suggesting
that additional defenses are needed against adversarial misuse. Our findings
highlight critical and persistent vulnerabilities in today's AI agents. By
releasing the ART benchmark and accompanying evaluation framework, we aim to
support more rigorous security assessment and drive progress toward safer agent
deployment.

</details>


### [50] [MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design](https://arxiv.org/abs/2507.20541)
*Zishang Qiu,Xinan Chen,Long Chen,Ruibin Bai*

Main category: cs.AI

TL;DR: MeLA是一种基于元认知的LLM驱动架构，通过“提示进化”自动设计启发式方法，显著优于传统进化方法。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法直接操作启发式代码，而MeLA通过优化LLM的提示来生成启发式，旨在探索更鲁棒和可解释的自动启发式设计路径。

Method: MeLA结合问题分析器、错误诊断系统和元认知搜索引擎，通过性能反馈迭代优化提示。

Result: 在基准和实际问题中，MeLA生成的启发式方法更有效且鲁棒，显著优于现有方法。

Conclusion: 研究表明，将认知科学融入AI架构，通过元认知调节LLM的问题解决过程，为自动启发式设计提供了更鲁棒和可解释的路径。

Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that
presents a new paradigm for Automatic Heuristic Design (AHD). Traditional
evolutionary methods operate directly on heuristic code; in contrast, MeLA
evolves the instructional prompts used to guide a Large Language Model (LLM) in
generating these heuristics. This process of "prompt evolution" is driven by a
novel metacognitive framework where the system analyzes performance feedback to
systematically refine its generative strategy. MeLA's architecture integrates a
problem analyzer to construct an initial strategic prompt, an error diagnosis
system to repair faulty code, and a metacognitive search engine that
iteratively optimizes the prompt based on heuristic effectiveness. In
comprehensive experiments across both benchmark and real-world problems, MeLA
consistently generates more effective and robust heuristics, significantly
outperforming state-of-the-art methods. Ultimately, this research demonstrates
the profound potential of using cognitive science as a blueprint for AI
architecture, revealing that by enabling an LLM to metacognitively regulate its
problem-solving process, we unlock a more robust and interpretable path to AHD.

</details>


### [51] [Unlearning of Knowledge Graph Embedding via Preference Optimization](https://arxiv.org/abs/2507.20566)
*Jiajun Liu,Wenjun Ke,Peng Wang,Yao He,Ziyu Shang,Guozheng Li,Zijie Xu,Ke Ji*

Main category: cs.AI

TL;DR: GraphDPO是一种基于直接偏好优化（DPO）的知识遗忘框架，用于从知识图谱嵌入模型中有效移除特定信息，同时保留剩余知识的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱（KGs）中存在过时或错误的知识，需要从知识图谱嵌入（KGE）模型中移除。现有遗忘方法（精确遗忘和近似遗忘）存在高成本或信息移除不完全的问题。

Method: GraphDPO将遗忘问题重构为偏好优化问题，通过DPO训练模型偏好重构的替代三元组而非原始遗忘三元组。此外，引入边界外采样策略和边界回忆机制，以最小化语义重叠并保留边界知识。

Result: 在四个流行KGs上构建的八个遗忘数据集上，GraphDPO在MRR_Avg和MRR_F1上分别比现有基线高出10.1%和14.0%。

Conclusion: GraphDPO通过偏好优化和边界知识保留机制，有效解决了知识图谱遗忘中的信息移除不完全和边界知识削弱问题。

Abstract: Existing knowledge graphs (KGs) inevitably contain outdated or erroneous
knowledge that needs to be removed from knowledge graph embedding (KGE) models.
To address this challenge, knowledge unlearning can be applied to eliminate
specific information while preserving the integrity of the remaining knowledge
in KGs. Existing unlearning methods can generally be categorized into exact
unlearning and approximate unlearning. However, exact unlearning requires high
training costs while approximate unlearning faces two issues when applied to
KGs due to the inherent connectivity of triples: (1) It fails to fully remove
targeted information, as forgetting triples can still be inferred from
remaining ones. (2) It focuses on local data for specific removal, which
weakens the remaining knowledge in the forgetting boundary. To address these
issues, we propose GraphDPO, a novel approximate unlearning framework based on
direct preference optimization (DPO). Firstly, to effectively remove forgetting
triples, we reframe unlearning as a preference optimization problem, where the
model is trained by DPO to prefer reconstructed alternatives over the original
forgetting triples. This formulation penalizes reliance on forgettable
knowledge, mitigating incomplete forgetting caused by KG connectivity.
Moreover, we introduce an out-boundary sampling strategy to construct
preference pairs with minimal semantic overlap, weakening the connection
between forgetting and retained knowledge. Secondly, to preserve boundary
knowledge, we introduce a boundary recall mechanism that replays and distills
relevant information both within and across time steps. We construct eight
unlearning datasets across four popular KGs with varying unlearning rates.
Experiments show that GraphDPO outperforms state-of-the-art baselines by up to
10.1% in MRR_Avg and 14.0% in MRR_F1.

</details>


### [52] [Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression](https://arxiv.org/abs/2507.20613)
*Te Zhang,Yuheng Li,Junxiang Wang,Lujun Li*

Main category: cs.AI

TL;DR: 提出了一种自适应搜索算法，通过优化稀疏性和KV缓存压缩来提升大型多模态模型（LMM）的效率，无需额外微调即可实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 尽管LMM在视觉编码器和语言模型结合方面取得了进展，但在边缘设备上的部署仍面临压缩挑战。

Method: 采用Tree-structured Parzen Estimator动态调整不同层的剪枝比例和KV缓存量化带宽，结合剪枝与KV缓存量化技术。

Result: 在LLaVA-1.5 7B和13B等基准数据集上表现优于SparseGPT和Wanda，实现了内存效率与性能的平衡。

Conclusion: 该框架为LMM优化设定了新标准，显著提升了压缩效率且几乎不影响性能。

Abstract: Large multimodal models (LMMs) have advanced significantly by integrating
visual encoders with extensive language models, enabling robust reasoning
capabilities. However, compressing LMMs for deployment on edge devices remains
a critical challenge. In this work, we propose an adaptive search algorithm
that optimizes sparsity and KV cache compression to enhance LMM efficiency.
Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts
pruning ratios and KV cache quantization bandwidth across different LMM layers,
using model performance as the optimization objective. This approach uniquely
combines pruning with key-value cache quantization and incorporates a fast
pruning technique that eliminates the need for additional fine-tuning or weight
adjustments, achieving efficient compression without compromising accuracy.
Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and
13B, demonstrate our method superiority over state-of-the-art techniques such
as SparseGPT and Wanda across various compression levels. Notably, our
framework automatic allocation of KV cache compression resources sets a new
standard in LMM optimization, delivering memory efficiency without sacrificing
much performance.

</details>


### [53] [Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion](https://arxiv.org/abs/2507.20620)
*Lijian Li*

Main category: cs.AI

TL;DR: 提出了一种名为MoCME的新框架，通过互补性模态专家混合和熵引导负采样机制，解决了多模态知识图谱补全中的模态不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多模态知识图谱中模态分布不平衡导致实体表示不鲁棒，现有方法忽略了多模态数据的互补性。

Method: 提出互补性引导的模态知识融合模块（CMKF）和熵引导负采样机制（EGNS），分别利用模态内和模态间互补性增强实体表示，并动态选择负样本。

Result: 在五个基准数据集上的实验表明，MoCME优于现有方法，达到了最先进的性能。

Conclusion: MoCME通过有效利用多模态互补性和动态负采样，显著提升了多模态知识图谱补全的效果。

Abstract: Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world
knowledge in multimodal knowledge graphs by leveraging both multimodal and
structural entity information. However, the inherent imbalance in multimodal
knowledge graphs, where modality distributions vary across entities, poses
challenges in utilizing additional modality data for robust entity
representation. Existing MMKGC methods typically rely on attention or
gate-based fusion mechanisms but overlook complementarity contained in
multi-modal data. In this paper, we propose a novel framework named Mixture of
Complementary Modality Experts (MoCME), which consists of a
Complementarity-guided Modality Knowledge Fusion (CMKF) module and an
Entropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits
both intra-modal and inter-modal complementarity to fuse multi-view and
multi-modal embeddings, enhancing representations of entities. Additionally, we
introduce an Entropy-guided Negative Sampling mechanism to dynamically
prioritize informative and uncertain negative samples to enhance training
effectiveness and model robustness. Extensive experiments on five benchmark
datasets demonstrate that our MoCME achieves state-of-the-art performance,
surpassing existing approaches.

</details>


### [54] [A General Framework for Dynamic MAPF using Multi-Shot ASP and Tunnels](https://arxiv.org/abs/2507.20703)
*Aysu Bogatarkan,Esra Erdem*

Main category: cs.AI

TL;DR: 论文研究了动态多智能体路径规划（D-MAPF）问题，提出了一种通用定义、新框架和基于ASP的新方法，并通过实验评估验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中（如仓库环境），智能体的动态变化（如进出或障碍物移动）需要动态路径规划方法。

Method: 1）提出D-MAPF的通用定义；2）设计多阶段计算框架；3）开发基于ASP的新方法，结合重规划和修复技术，引入隧道概念。

Result: 实验评估展示了该方法在计算性能和解决方案质量方面的优劣势。

Conclusion: 新方法适用于动态环境，结合了重规划和修复的优势，并通过隧道概念提升了灵活性。

Abstract: MAPF problem aims to find plans for multiple agents in an environment within
a given time, such that the agents do not collide with each other or obstacles.
Motivated by the execution and monitoring of these plans, we study Dynamic MAPF
(D-MAPF) problem, which allows changes such as agents entering/leaving the
environment or obstacles being removed/moved. Considering the requirements of
real-world applications in warehouses with the presence of humans, we introduce
1) a general definition for D-MAPF (applicable to variations of D-MAPF), 2) a
new framework to solve D-MAPF (utilizing multi-shot computation, and allowing
different methods to solve D-MAPF), and 3) a new ASP-based method to solve
D-MAPF (combining advantages of replanning and repairing methods, with a novel
concept of tunnels to specify where agents can move). We have illustrated the
strengths and weaknesses of this method by experimental evaluations, from the
perspectives of computational performance and quality of solutions.

</details>


### [55] [Algorithmic Fairness: A Runtime Perspective](https://arxiv.org/abs/2507.20711)
*Filip Cano,Thomas A. Henzinger,Konstantin Kueffner*

Main category: cs.AI

TL;DR: 该论文提出了一种将公平性作为运行时属性的框架，通过基于硬币抛掷序列的模型研究监控和执行公平性的策略。


<details>
  <summary>Details</summary>
Motivation: 传统公平性研究是静态的，而现实AI系统是动态演化的，因此需要动态公平性分析框架。

Method: 使用基于硬币抛掷序列的模型，研究监控和执行公平性的策略，参数化环境动态、预测范围和置信阈值。

Result: 总结了监控和执行策略，提供了简单假设下的通用结果，并调查了现有解决方案。

Conclusion: 动态公平性分析需要针对不同场景定制策略，论文为未来研究提供了基础。

Abstract: Fairness in AI is traditionally studied as a static property evaluated once,
over a fixed dataset. However, real-world AI systems operate sequentially, with
outcomes and environments evolving over time. This paper proposes a framework
for analysing fairness as a runtime property. Using a minimal yet expressive
model based on sequences of coin tosses with possibly evolving biases, we study
the problems of monitoring and enforcing fairness expressed in either toss
outcomes or coin biases. Since there is no one-size-fits-all solution for
either problem, we provide a summary of monitoring and enforcement strategies,
parametrised by environment dynamics, prediction horizon, and confidence
thresholds. For both problems, we present general results under simple or
minimal assumptions. We survey existing solutions for the monitoring problem
for Markovian and additive dynamics, and existing solutions for the enforcement
problem in static settings with known dynamics.

</details>


### [56] [Learning the Value Systems of Societies from Preferences](https://arxiv.org/abs/2507.20728)
*Andrés Holgado-Sánchez,Holger Billhardt,Sascha Ossowski,Sara Degli-Esposti*

Main category: cs.AI

TL;DR: 论文提出了一种基于启发式深度聚类的方法，用于学习社会共享的价值基础和多组价值系统，以更准确地反映社会多样性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以手动校准和获取个体价值系统，且社会价值系统更适合被视为多组价值系统的集合，而非简单聚合。

Method: 采用启发式深度聚类方法，通过观察代理样本的定性价值偏好，学习社会共享价值基础和多样价值系统。

Result: 在旅行决策的真实数据用例中验证了方法的有效性。

Conclusion: 该方法为构建更符合社会多样性的价值感知AI系统提供了可行途径。

Abstract: Aligning AI systems with human values and the value-based preferences of
various stakeholders (their value systems) is key in ethical AI. In value-aware
AI systems, decision-making draws upon explicit computational representations
of individual values (groundings) and their aggregation into value systems. As
these are notoriously difficult to elicit and calibrate manually, value
learning approaches aim to automatically derive computational models of an
agent's values and value system from demonstrations of human behaviour.
Nonetheless, social science and humanities literature suggest that it is more
adequate to conceive the value system of a society as a set of value systems of
different groups, rather than as the simple aggregation of individual value
systems. Accordingly, here we formalize the problem of learning the value
systems of societies and propose a method to address it based on heuristic deep
clustering. The method learns socially shared value groundings and a set of
diverse value systems representing a given society by observing qualitative
value-based preferences from a sample of agents. We evaluate the proposal in a
use case with real data about travelling decisions.

</details>


### [57] [Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours](https://arxiv.org/abs/2507.20755)
*Arpan Dasgupta,Sarvesh Gharat,Neha Madhiwalla,Aparna Hegde,Milind Tambe,Aparna Taneja*

Main category: cs.AI

TL;DR: AI干预通过提升听众参与度，显著改善了孕产妇和婴儿健康行为及知识。


<details>
  <summary>Details</summary>
Motivation: 解决自动化语音呼叫项目中听众流失和参与度低的问题，并验证AI干预是否能转化为健康行为改善。

Method: 使用AI模型（如多臂老虎机模型）识别最需要干预的受益者，并通过实时服务呼叫提升参与度。

Result: AI干预显著提高了听众参与度，并改善了孕产妇的铁和钙补充行为及健康知识。

Conclusion: AI在孕产妇和儿童健康领域具有推动实质性改善的潜力。

Abstract: Automated voice calls with health information are a proven method for
disseminating maternal and child health information among beneficiaries and are
deployed in several programs around the world. However, these programs often
suffer from beneficiary dropoffs and poor engagement. In previous work, through
real-world trials, we showed that an AI model, specifically a restless bandit
model, could identify beneficiaries who would benefit most from live service
call interventions, preventing dropoffs and boosting engagement. However, one
key question has remained open so far: does such improved listenership via
AI-targeted interventions translate into beneficiaries' improved knowledge and
health behaviors? We present a first study that shows not only listenership
improvements due to AI interventions, but also simultaneously links these
improvements to health behavior changes. Specifically, we demonstrate that
AI-scheduled interventions, which enhance listenership, lead to statistically
significant improvements in beneficiaries' health behaviors such as taking iron
or calcium supplements in the postnatal period, as well as understanding of
critical health topics during pregnancy and infancy. This underscores the
potential of AI to drive meaningful improvements in maternal and child health.

</details>


### [58] [How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation](https://arxiv.org/abs/2507.20758)
*Hao Yang,Qinghua Zhao,Lei Li*

Main category: cs.AI

TL;DR: CoT提示通过解码空间修剪和任务依赖的神经元调节提升模型推理，但其机制尚不明确。


<details>
  <summary>Details</summary>
Motivation: 研究CoT的内部机制，以理解其如何通过信息流和神经元调节提升推理性能。

Method: 通过反向追踪解码、投影和激活阶段的信息流，定量分析CoT的作用。

Result: CoT作为解码空间修剪器，利用答案模板引导输出，且神经元调节因任务而异。

Conclusion: 研究为CoT的机制解释提供了新框架，并指导更高效提示设计。

Abstract: Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet
its internal mechanisms remain poorly understood. We analyze CoT's operational
principles by reversely tracing information flow across decoding, projection,
and activation phases. Our quantitative analysis suggests that CoT may serve as
a decoding space pruner, leveraging answer templates to guide output
generation, with higher template adherence strongly correlating with improved
performance. Furthermore, we surprisingly find that CoT modulates neuron
engagement in a task-dependent manner: reducing neuron activation in
open-domain tasks, yet increasing it in closed-domain scenarios. These findings
offer a novel mechanistic interpretability framework and critical insights for
enabling targeted CoT interventions to design more efficient and robust
prompts. We released our code and data at
https://anonymous.4open.science/r/cot-D247.

</details>


### [59] [evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments](https://arxiv.org/abs/2507.20774)
*Fatou Ndiaye Mbodji*

Main category: cs.AI

TL;DR: 提出了一个名为evalSmarT的框架，利用大语言模型（LLMs）评估智能合约注释生成的质量，解决了传统指标和人工评估的不足。


<details>
  <summary>Details</summary>
Motivation: 智能合约注释生成的质量评估面临传统指标不准确和人工评估成本高的问题。

Method: 开发了evalSmarT框架，结合约40种LLMs和10种提示策略，支持400多种评估配置。

Result: 实验表明提示设计显著影响与人类判断的一致性，LLM评估提供了可扩展且语义丰富的替代方案。

Conclusion: evalSmarT为智能合约注释生成的质量评估提供了高效且灵活的解决方案。

Abstract: Smart contract comment generation has gained traction as a means to improve
code comprehension and maintainability in blockchain systems. However,
evaluating the quality of generated comments remains a challenge. Traditional
metrics such as BLEU and ROUGE fail to capture domain-specific nuances, while
human evaluation is costly and unscalable. In this paper, we present
\texttt{evalSmarT}, a modular and extensible framework that leverages large
language models (LLMs) as evaluators. The system supports over 400 evaluator
configurations by combining approximately 40 LLMs with 10 prompting strategies.
We demonstrate its application in benchmarking comment generation tools and
selecting the most informative outputs. Our results show that prompt design
significantly impacts alignment with human judgment, and that LLM-based
evaluation offers a scalable and semantically rich alternative to existing
methods.

</details>


### [60] [MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs](https://arxiv.org/abs/2507.20804)
*Xueyao Wan,Hang Yu*

Main category: cs.AI

TL;DR: MMGraphRAG通过场景图和知识图谱改进多模态检索增强生成，解决了传统方法在知识结构和逻辑链上的不足，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法缺乏多模态信息融合能力，且无法捕捉知识结构和逻辑链，限制了泛化能力。

Method: MMGraphRAG利用场景图和知识图谱构建多模态知识图谱，通过谱聚类实现跨模态实体链接，并沿推理路径检索上下文。

Result: 在DocBench和MMLongBench数据集上达到最优性能，表现出强大的领域适应性和清晰的推理路径。

Conclusion: MMGraphRAG通过结构化和逻辑化的多模态信息处理，显著提升了生成模型的性能和泛化能力。

Abstract: Retrieval-Augmented Generation (RAG) enhances language model generation by
retrieving relevant information from external knowledge bases. However,
conventional RAG methods face the issue of missing multimodal information.
Multimodal RAG methods address this by fusing images and text through mapping
them into a shared embedding space, but they fail to capture the structure of
knowledge and logical chains between modalities. Moreover, they also require
large-scale training for specific tasks, resulting in limited generalizing
ability. To address these limitations, we propose MMGraphRAG, which refines
visual content through scene graphs and constructs a multimodal knowledge graph
(MMKG) in conjunction with text-based KG. It employs spectral clustering to
achieve cross-modal entity linking and retrieves context along reasoning paths
to guide the generative process. Experimental results show that MMGraphRAG
achieves state-of-the-art performance on the DocBench and MMLongBench datasets,
demonstrating strong domain adaptability and clear reasoning paths.

</details>


### [61] [Partially Observable Monte-Carlo Graph Search](https://arxiv.org/abs/2507.20951)
*Yang You,Vincent Thomas,Alex Schutz,Robert Skilton,Nick Hawes,Olivier Buffet*

Main category: cs.AI

TL;DR: 提出了一种新的离线算法POMCGS，用于解决大规模POMDP问题，通过动态折叠搜索树减少计算量，并支持连续POMDP。


<details>
  <summary>Details</summary>
Motivation: 现有离线算法无法扩展到大规模POMDP，而在线方法不适用于时间或能源受限的应用。

Method: POMCGS算法通过动态折叠搜索树构建策略图，结合动作渐进扩展和观测聚类方法。

Result: POMCGS能处理传统离线算法无法解决的挑战性POMDP，且策略性能与先进在线算法相当。

Conclusion: POMCGS为大规模POMDP提供了一种高效的离线解决方案，适用于实际应用。

Abstract: Currently, large partially observable Markov decision processes (POMDPs) are
often solved by sampling-based online methods which interleave planning and
execution phases. However, a pre-computed offline policy is more desirable in
POMDP applications with time or energy constraints. But previous offline
algorithms are not able to scale up to large POMDPs. In this article, we
propose a new sampling-based algorithm, the partially observable Monte-Carlo
graph search (POMCGS) to solve large POMDPs offline. Different from many online
POMDP methods, which progressively develop a tree while performing
(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to
construct a policy graph, so that computations can be drastically reduced, and
users can analyze and validate the policy prior to embedding and executing it.
Moreover, POMCGS, together with action progressive widening and observation
clustering methods provided in this article, is able to address certain
continuous POMDPs. Through experiments, we demonstrate that POMCGS can generate
policies on the most challenging POMDPs, which cannot be computed by previous
offline algorithms, and these policies' values are competitive compared with
the state-of-the-art online POMDP algorithms.

</details>


### [62] [On the Limits of Hierarchically Embedded Logic in Classical Neural Networks](https://arxiv.org/abs/2507.20960)
*Bill Cochran*

Main category: cs.AI

TL;DR: 论文提出了一种基于神经网络深度的形式化模型，用于分析大型语言模型的推理限制，证明其逻辑表达能力受限于网络深度。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大型神经网络在语言任务中的推理局限性，尤其是其在逻辑表达上的限制。

Method: 方法是将神经网络视为逻辑谓词空间上的线性算子，分析每层网络的逻辑推理能力。

Result: 结果表明，特定深度的神经网络无法准确表示更高阶逻辑（如复杂谓词的计数），并揭示了其逻辑表达能力的严格上限。

Conclusion: 结论是这种限制解释了模型中的幻觉、重复等问题，并为未来语言模型的架构扩展和可解释性策略提供了理论基础。

Abstract: We propose a formal model of reasoning limitations in large neural net models
for language, grounded in the depth of their neural architecture. By treating
neural networks as linear operators over logic predicate space we show that
each layer can encode at most one additional level of logical reasoning. We
prove that a neural network of depth a particular depth cannot faithfully
represent predicates in a one higher order logic, such as simple counting over
complex predicates, implying a strict upper bound on logical expressiveness.
This structure induces a nontrivial null space during tokenization and
embedding, excluding higher-order predicates from representability. Our
framework offers a natural explanation for phenomena such as hallucination,
repetition, and limited planning, while also providing a foundation for
understanding how approximations to higher-order logic may emerge. These
results motivate architectural extensions and interpretability strategies in
future development of language models.

</details>


### [63] [Core Safety Values for Provably Corrigible Agents](https://arxiv.org/abs/2507.20964)
*Aran Nayebi*

Main category: cs.AI

TL;DR: 提出首个可实现的修正性框架，在多步、部分可观测环境中提供可证明的保证，通过五个独立效用头实现安全性和人类利益。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如Constitutional AI或RLHF/RLAIF）将所有规范合并为一个标量的局限性，确保在激励冲突时仍能优先服从和限制影响。

Method: 使用五个结构分离的效用头（服从、开关访问保护、真实性、低影响行为和有限任务奖励），并通过严格权重间隙结合。

Result: 证明在部分可观测的开关游戏中实现单轮修正性，并扩展到多步自生成代理，安全性违反概率有界且确保人类净利益。

Conclusion: 框架将风险转移到评估质量而非隐藏激励泄漏，为当前LLM助手和未来自主系统提供更清晰的实施指导。

Abstract: We introduce the first implementable framework for corrigibility, with
provable guarantees in multi-step, partially observed environments. Our
framework replaces a single opaque reward with five *structurally separate*
utility heads -- deference, switch-access preservation, truthfulness,
low-impact behavior via a belief-based extension of Attainable Utility
Preservation, and bounded task reward -- combined lexicographically by strict
weight gaps. Theorem 1 proves exact single-round corrigibility in the partially
observable off-switch game; Theorem 3 extends the guarantee to multi-step,
self-spawning agents, showing that even if each head is \emph{learned} to
mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal,
the probability of violating \emph{any} safety property is bounded while still
ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,
which merge all norms into one learned scalar, our separation makes obedience
and impact-limits dominate even when incentives conflict. For open-ended
settings where adversaries can modify the agent, we prove that deciding whether
an arbitrary post-hack agent will ever violate corrigibility is undecidable by
reduction to the halting problem, then carve out a finite-horizon ``decidable
island'' where safety can be certified in randomized polynomial time and
verified with privacy-preserving, constant-round zero-knowledge proofs.
Consequently, the remaining challenge is the ordinary ML task of data coverage
and generalization: reward-hacking risk is pushed into evaluation quality
rather than hidden incentive leak-through, giving clearer implementation
guidance for today's LLM assistants and future autonomous systems.

</details>


### [64] [MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them](https://arxiv.org/abs/2507.21017)
*Weichen Zhang,Yiyou Sun,Pohao Huang,Jiayue Pu,Heyue Lin,Dawn Song*

Main category: cs.AI

TL;DR: MIRAGE-Bench是首个用于评估交互式LLM代理幻觉的统一基准，通过分类、系统审计和精细评估方法提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 解决现有评估的碎片化问题，为LLM代理的幻觉行为提供系统性测试平台。

Method: 引入三部分分类法，通过系统审计和快照策略合成测试用例，采用LLM-as-a-Judge范式进行精细评估。

Result: 提供了幻觉行为的详细分析，揭示了LLM代理的失败模式。

Conclusion: MIRAGE-Bench为减少交互环境中的幻觉行为奠定了基础，推动了系统性进展。

Abstract: Hallucinations pose critical risks for large language model (LLM)-based
agents, often manifesting as hallucinative actions resulting from fabricated or
misinterpreted information within the cognitive context. While recent studies
have exposed such failures, existing evaluations remain fragmented and lack a
principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions
in Risky AGEnt settings--the first unified benchmark for eliciting and
evaluating hallucinations in interactive LLM-agent scenarios. We begin by
introducing a three-part taxonomy to address agentic hallucinations: actions
that are unfaithful to (i) task instructions, (ii) execution history, or (iii)
environment observations. To analyze, we first elicit such failures by
performing a systematic audit of existing agent benchmarks, then synthesize
test cases using a snapshot strategy that isolates decision points in
deterministic and reproducible manners. To evaluate hallucination behaviors, we
adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware
prompts, enabling scalable, high-fidelity assessment of agent actions without
enumerating full action spaces. MIRAGE-Bench provides actionable insights on
failure modes of LLM agents and lays the groundwork for principled progress in
mitigating hallucinations in interactive environments.

</details>


### [65] [Smart Expansion Techniques for ASP-based Interactive Configuration](https://arxiv.org/abs/2507.21027)
*Lucia Balážová,Richard Comploi-Taupe,Susana Hahn,Nicolas Rühling,Gottfried Schenner*

Main category: cs.AI

TL;DR: 本文提出了一种基于ASP的交互式配置求解器，通过四种智能扩展功能优化部分配置的自动完成性能，减少搜索空间和成本高昂的不可满足性检查。


<details>
  <summary>Details</summary>
Motivation: 解决交互式系统中大规模工业配置问题的性能瓶颈，支持直观的用户界面。

Method: 采用增量式多轮求解方法，结合四种智能扩展功能，利用谨慎和勇敢的后果来优化部分配置。

Result: 减少了不可满足性检查的次数和搜索空间，显著提高了求解性能。

Conclusion: 提出的方法有效提升了交互式配置系统的性能，并通过API支持直观的用户界面。

Abstract: Product configuration is a successful application of Answer Set Programming
(ASP). However, challenges are still open for interactive systems to
effectively guide users through the configuration process. The aim of our work
is to provide an ASP-based solver for interactive configuration that can deal
with large-scale industrial configuration problems and that supports intuitive
user interfaces via an API. In this paper, we focus on improving the
performance of automatically completing a partial configuration. Our main
contribution enhances the classical incremental approach for multi-shot solving
by four different smart expansion functions. The core idea is to determine and
add specific objects or associations to the partial configuration by exploiting
cautious and brave consequences before checking for the existence of a complete
configuration with the current objects in each iteration. This approach limits
the number of costly unsatisfiability checks and reduces the search space,
thereby improving solving performance. In addition, we present a user interface
that uses our API and is implemented in ASP.

</details>


### [66] [GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis](https://arxiv.org/abs/2507.21035)
*Haoyang Liu,Yijiang Li,Haohan Wang*

Main category: cs.AI

TL;DR: GenoMAS是一个基于LLM的团队协作系统，通过结合结构化工作流和自主代理的灵活性，显著提升了基因表达分析的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 基因表达分析复杂且需要专业知识，现有自动化方法在灵活性和精确性上存在不足。

Method: GenoMAS采用六个专业LLM代理，通过类型化消息传递协议协作，并结合指导性规划框架动态调整任务执行。

Result: 在GenoTEX基准测试中，GenoMAS在数据预处理和基因识别任务上分别提升了10.61%和16.85%。

Conclusion: GenoMAS不仅性能优越，还能发现生物学上可信的基因-表型关联，为基因分析提供了新工具。

Abstract: Gene expression analysis holds the key to many biomedical discoveries, yet
extracting insights from raw transcriptomic data remains formidable due to the
complexity of multiple large, semi-structured files and the need for extensive
domain expertise. Current automation approaches are often limited by either
inflexible workflows that break down in edge cases or by fully autonomous
agents that lack the necessary precision for rigorous scientific inquiry.
GenoMAS charts a different course by presenting a team of LLM-based scientists
that integrates the reliability of structured workflows with the adaptability
of autonomous agents. GenoMAS orchestrates six specialized LLM agents through
typed message-passing protocols, each contributing complementary strengths to a
shared analytic canvas. At the heart of GenoMAS lies a guided-planning
framework: programming agents unfold high-level task guidelines into Action
Units and, at each juncture, elect to advance, revise, bypass, or backtrack,
thereby maintaining logical coherence while bending gracefully to the
idiosyncrasies of genomic data.
  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation
of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene
identification, surpassing the best prior art by 10.61% and 16.85%
respectively. Beyond metrics, GenoMAS surfaces biologically plausible
gene-phenotype associations corroborated by the literature, all while adjusting
for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.

</details>


### [67] [A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence](https://arxiv.org/abs/2507.21046)
*Huan-ang Gao,Jiayi Geng,Wenyue Hua,Mengkang Hu,Xinzhe Juan,Hongzhang Liu,Shilong Liu,Jiahao Qiu,Xuan Qi,Yiran Wu,Hongru Wang,Han Xiao,Yuhang Zhou,Shaokun Zhang,Jiayi Zhang,Jinyu Xiang,Yixiong Fang,Qiwen Zhao,Dongrui Liu,Qihan Ren,Cheng Qian,Zhenghailong Wang,Minda Hu,Huazheng Wang,Qingyun Wu,Heng Ji,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文综述了自进化代理的研究，围绕“进化什么”、“何时进化”和“如何进化”三个维度，系统分析了其机制、方法和应用，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的静态特性限制了其在动态环境中的适应性，因此需要开发能够实时进化的自进化代理。

Method: 围绕三个维度（进化内容、时机和方法）分析自进化代理的机制、适应方法和架构设计，并评估其应用与挑战。

Result: 提出了自进化代理的系统框架，总结了进化机制、适应方法和应用领域，并指出了安全性、可扩展性等关键挑战。

Conclusion: 自进化代理是实现人工超级智能（ASI）的关键路径，未来研究需解决安全性和协同进化等问题。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities but remain
fundamentally static, unable to adapt their internal parameters to novel tasks,
evolving knowledge domains, or dynamic interaction contexts. As LLMs are
increasingly deployed in open-ended, interactive environments, this static
nature has become a critical bottleneck, necessitating agents that can
adaptively reason, act, and evolve in real time. This paradigm shift -- from
scaling static models to developing self-evolving agents -- has sparked growing
interest in architectures and methods enabling continual learning and
adaptation from data, interactions, and experiences. This survey provides the
first systematic and comprehensive review of self-evolving agents, organized
around three foundational dimensions -- what to evolve, when to evolve, and how
to evolve. We examine evolutionary mechanisms across agent components (e.g.,
models, memory, tools, architecture), categorize adaptation methods by stages
(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and
architectural designs that guide evolutionary adaptation (e.g., scalar rewards,
textual feedback, single-agent and multi-agent systems). Additionally, we
analyze evaluation metrics and benchmarks tailored for self-evolving agents,
highlight applications in domains such as coding, education, and healthcare,
and identify critical challenges and research directions in safety,
scalability, and co-evolutionary dynamics. By providing a structured framework
for understanding and designing self-evolving agents, this survey establishes a
roadmap for advancing adaptive agentic systems in both research and real-world
deployments, ultimately shedding lights to pave the way for the realization of
Artificial Super Intelligence (ASI), where agents evolve autonomously,
performing at or beyond human-level intelligence across a wide array of tasks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [68] [Polar Coding and Linear Decoding](https://arxiv.org/abs/2507.19695)
*Geraldo A. Barbosa*

Main category: cs.IT

TL;DR: 本文探讨了Polar编码的线性解码方法，适用于小损失或少量比特翻转的情况，并通过Mathematica编程实现。


<details>
  <summary>Details</summary>
Motivation: Polar编码在通信领域具有重要意义，但传统解码方法复杂。本文旨在探索更直接的线性解码方法。

Method: 采用线性方程直接解码，适用于小损失或少量比特翻转；对于中等损失，采用重复传输策略。

Result: 通过数值示例展示了线性解码的可行性，并提供了Mathematica代码供实践。

Conclusion: 线性解码为Polar编码提供了一种简单直接的替代方案，适用于特定场景。

Abstract: Polar encoding, described by Arikan in IEEE Transactions on Information
Theory, Vol. 55, No. 7, July 2009, was a milestone for telecommunications. A
Polar code distributes information among high and low-capacity channels,
showing the possibility of achieving perfect channel capacity. The
high-capacity channels allow almost noiseless transmission of data. When these
channels are not high noise, reliability is achieved in the signal
transmission. It starts to compete against codes such a Low-Density
Parity-Check (LDPC) codes. Polar code can be also considered error correcting,
based on the redundancy inherent in its structure. This feature makes polar
encoding also applicable to digital quantum-resistant cryptography protocols.
This work explores linear decoding at a first or single trial in the case of
small losses or small number of bit-flipping, and repeated transmission for
medium level losses. This is distinct from Arikans successive probabilistic
decoding by application of probabilistic rules. Linear decoding is done
directly from solving the linear equations connecting the codewords x and the
received signals y after transmission via noisy channels. Numerical examples
will be shown. Along with this work, programming in Mathematica language was
used. Codes are available for copy-and-paste for Mathematica users to
immediately try the described formalism.

</details>


### [69] [Efficient Computation of Marton's Error Exponent via Constraint Decoupling](https://arxiv.org/abs/2507.19816)
*Jiachuan Ye,Shitong Wu,Lingyi Chen,Wenyi Zhang,Huihui Wu,Hao Wu*

Main category: cs.IT

TL;DR: 本文提出了一种复合最大化方法，用于高效计算Marton误差指数及其逆函数，通过约束解耦技术显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: Marton误差指数是损失源编码中误差概率渐近衰减率的最优理论界限，但由于其非凸优化问题的特性，现有计算方法效率低下。

Method: 采用复合最大化方法，通过约束解耦和交替最大化算法，将问题分解为可高效求解的凸子问题。

Result: 数值实验表明，该方法在简单源和Ahlswede反例中均表现出优于现有方法的效率。

Conclusion: 所提算法显著降低了计算复杂度，并保证了全局收敛性，为Marton误差指数的计算提供了高效解决方案。

Abstract: The error exponent in lossy source coding characterizes the asymptotic decay
rate of error probability with respect to blocklength. The Marton's error
exponent provides the theoretically optimal bound on this rate. However,
computation methods of the Marton's error exponent remain underdeveloped due to
its formulation as a non-convex optimization problem with limited efficient
solvers. While a recent grid search algorithm can compute its inverse function,
it incurs prohibitive computational costs from two-dimensional brute-force
parameter grid searches. This paper proposes a composite maximization approach
that effectively handles both Marton's error exponent and its inverse function.
Through a constraint decoupling technique, the resulting problem formulations
admit efficient solvers driven by an alternating maximization algorithm. By
fixing one parameter via a one-dimensional line search, the remaining
subproblem becomes convex and can be efficiently solved by alternating variable
updates, thereby significantly reducing search complexity. Therefore, the
global convergence of the algorithm can be guaranteed. Numerical experiments
for simple sources and the Ahlswede's counterexample, demonstrates the superior
efficiency of our algorithm in contrast to existing methods.

</details>


### [70] [Neural Estimation of the Information Bottleneck Based on a Mapping Approach](https://arxiv.org/abs/2507.19832)
*Lingyi Chen,Shitong Wu,Sicheng Xu,Huihui Wu,Wenyi Zhang*

Main category: cs.IT

TL;DR: 本文研究了基于神经网络的IB问题解决方案，提出了一种新方法，通过优化单一变量实现高效估计，并在理论和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 信息瓶颈（IB）方法在机器学习中广泛应用，但现有方法复杂，难以直接应用神经网络。本文旨在简化IB问题，使其更适合数据驱动的神经网络估计。

Method: 通过利用IB功能的内在结构和映射方法，将IB问题重新表述为仅需优化单一变量的形式，从而便于神经网络估计。

Result: 理论分析证明神经估计器渐近解决IB问题，数值实验在合成和MNIST数据集上验证了其有效性。

Conclusion: 提出的神经估计器简化了IB问题，为基于神经网络的解决方案提供了理论和实践支持。

Abstract: The information bottleneck (IB) method is a technique designed to extract
meaningful information related to one random variable from another random
variable, and has found extensive applications in machine learning problems. In
this paper, neural network based estimation of the IB problem solution is
studied, through the lens of a novel formulation of the IB problem. Via
exploiting the inherent structure of the IB functional and leveraging the
mapping approach, the proposed formulation of the IB problem involves only a
single variable to be optimized, and subsequently is readily amenable to
data-driven estimators based on neural networks. A theoretical analysis is
conducted to guarantee that the neural estimator asymptotically solves the IB
problem, and the numerical experiments on both synthetic and MNIST datasets
demonstrate the effectiveness of the neural estimator.

</details>


### [71] [An Efficient Alternating Minimization Algorithm for Computing Quantum Rate-Distortion Function](https://arxiv.org/abs/2507.19920)
*Lingyi Chen,Deheng Yuan,Wenyi Zhang,Hao Wu,Huihui Wu*

Main category: cs.IT

TL;DR: 本文提出了一种基于拉格朗日分析的交替最小化算法，用于计算纠缠辅助量子率失真函数，提高了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 纠缠辅助量子率失真函数在量子信息理论中具有核心作用，但现有方法效率不足。

Method: 通过交替最小化算法动态更新拉格朗日乘子，避免多维非线性方程求解。

Result: 数值实验表明该算法准确且效率优于现有方法。

Conclusion: 所提算法解决了原始问题而非其拉格朗日松弛，显著提升了计算性能。

Abstract: We consider the computation of the entanglement-assisted quantum
rate-distortion function, which plays a central role in quantum information
theory. We propose an efficient alternating minimization algorithm based on the
Lagrangian analysis. Instead of fixing the multiplier corresponding to the
distortion constraint, we update the multiplier in each iteration. Hence the
algorithm solves the original problem itself, rather than the Lagrangian
relaxation of it. Moreover, all the other variables are iterated in closed form
without solving multi-dimensional nonlinear equations or multivariate
optimization problems. Numerical experiments show the accuracy of our proposed
algorithm and its improved efficiency over existing methods.

</details>


### [72] [Adaptive Learned Belief Propagation for Decoding Error-Correcting Codes](https://arxiv.org/abs/2507.19941)
*Alireza Tasdighi,Mansoor Yousefi*

Main category: cs.IT

TL;DR: 该论文提出了一种自适应加权置信传播（WBP）解码器，通过动态调整权重提升解码性能。


<details>
  <summary>Details</summary>
Motivation: 静态WBP解码器权重固定，无法适应不同接收信号，限制了性能提升。

Method: 提出两种自适应WBP解码器：并行WBP（离散权重搜索）和两阶段解码器（神经网络动态调整权重）。

Result: 在BCH、极化和QC-LDPC码中，自适应WBP比静态WBP降低一个数量级的误码率；在非线性光纤信道中，提供0.8 dB的编码增益。

Conclusion: 自适应WBP显著提升解码性能，且计算复杂度与静态方法相当。

Abstract: Weighted belief propagation (WBP) for the decoding of linear block codes is
considered. In WBP, the Tanner graph of the code is unrolled with respect to
the iterations of the belief propagation decoder. Then, weights are assigned to
the edges of the resulting recurrent network and optimized offline using a
training dataset. The main contribution of this paper is an adaptive WBP where
the weights of the decoder are determined for each received word. Two variants
of this decoder are investigated. In the parallel WBP decoders, the weights
take values in a discrete set. A number of WBP decoders are run in parallel to
search for the best sequence of weights in real time. In the two-stage decoder,
a small neural network is used to dynamically determine the weights of the WBP
decoder for each received word. The proposed adaptive decoders demonstrate
significant improvements over the static counterparts in two applications. In
the first application, Bose-Chaudhuri-Hocquenghem, polar and quasi-cyclic
low-density parity-check (QC-LDPC) codes are used over an additive white
Gaussian noise channel. The results indicate that the adaptive WBP achieves bit
error rates (BERs) up to an order of magnitude less than the BERs of the static
WBP at about the same decoding complexity, depending on the code, its rate, and
the signal-to-noise ratio. The second application is a concatenated code
designed for a long-haul nonlinear optical fiber channel where the inner code
is a QC-LDPC code and the outer code is a spatially coupled LDPC code. In this
case, the inner code is decoded using an adaptive WBP, while the outer code is
decoded using the sliding window decoder and static belief propagation. The
results show that the adaptive WBP provides a coding gain of 0.8 dB compared to
the neural normalized min-sum decoder, with about the same computational
complexity and decoding latency.

</details>


### [73] [Performance Analysis of Spatiotemporal 2-D Polar Codes for Massive MIMO with MMSE Receivers](https://arxiv.org/abs/2507.19986)
*Yaqi Li,Xiaohu You,Jiamin Li,Chen Ji,Bin Sheng*

Main category: cs.IT

TL;DR: 论文提出了一种时空二维极编码方案，用于大规模MIMO系统，以降低延迟并提高可靠性，适用于6G URLLC场景。


<details>
  <summary>Details</summary>
Motivation: 5G向6G演进中，URLLC对性能要求更严格，传统极编码因解码延迟高难以满足需求，需利用大规模MIMO的空间自由度。

Method: 提出联合时空维度的极编码方案，利用MMSE接收机的SINR特性，将空间域建模为并行高斯子信道，并进行二维极化分析。

Result: 仿真表明，与传统时域极编码相比，该方案显著降低延迟或提高可靠性，在有限块长和大空间自由度下实现容量逼近。

Conclusion: 该方案为6G URLLC提供了高效的低延迟信道编码解决方案。

Abstract: With the evolution from 5G to 6G, ultra-reliable low-latency communication
(URLLC) faces increasingly stringent performance requirements. Lower latency
constraints demand shorter channel coding lengths, which can severely degrade
decoding performance. The massive multiple-input multiple-output (MIMO) system
is considered a crucial technology to address this challenge due to its
abundant spatial degrees of freedom (DoF). While polar codes are theoretically
capacity-achieving in the limit of infinite code length, their practical
applicability is limited by significant decoding latency. In this paper, we
establish a unified theoretical framework and propose a novel spatiotemporal
two-dimensional (2-D) polar coding scheme for massive MIMO systems employing
minimum mean square error (MMSE) receivers. The polar transform is jointly
applied over both spatial and temporal dimensions to fully exploit the large
spatial DoF. By leveraging the near-deterministic
signal-to-interference-plus-noise ratio (SINR) property of MMSE detection, the
spatial domain is modeled as a set of parallel Gaussian sub-channels. Within
this framework, we perform a theoretical analysis of the 2-D polarization
behavior using the Gaussian approximation method, and the capacity-achieving
property of the proposed scheme is proved under finite blocklength constraints
and large spatial DoF. Simulation results further demonstrate that, compared to
traditional time-domain polar codes, the proposed 2-D scheme can significantly
reduce latency while guaranteeing reliability, or alternatively improve
reliability under the same latency constraint -- offering a capacity-achieving
and latency-efficient channel coding solution for massive MIMO systems in
future 6G URLLC scenarios.

</details>


### [74] [Rotatable RIS Assisted Physical Layer Multicasting](https://arxiv.org/abs/2507.20113)
*Ji Wang,Jiayu Tian,Lijuan Qin,Kunrui Cao,Hongbo Xu,Xingwang Li,Tony. Q. S. Quek*

Main category: cs.IT

TL;DR: 本文提出了一种可旋转RIS辅助的物理层多播系统框架，通过联合优化基站波束成形、RIS相位偏移和方向，最大化最小多播速率之和。


<details>
  <summary>Details</summary>
Motivation: 提升无线通信中多播系统的公平性和速率，特别是为弱用户提供更好的服务。

Method: 采用交替优化方法，结合凸优化、穷举搜索和粒子群优化（PSO）来优化波束成形、相位偏移和RIS方向。

Result: 仿真结果显示，该框架比非旋转RIS基线提升了24.1%（穷举搜索）和20.0%（PSO）的速率，PSO性能接近穷举搜索上限。

Conclusion: 物理层多播和方向优化显著提升了系统性能，PSO是一种高效的替代方案。

Abstract: Reconfigurable Intelligent Surfaces (RIS) dynamically control signal
propagation to enhance wireless communications. This paper presents a novel
framework for rotatable RIS assisted physical-layer multicast systems, aiming
to maximize the sum of minimum multicast rates via joint optimization of base
station beamforming, RIS phase shifts, and orientation. Unlike unicast or
non-rotatable setups, the rotatable RIS adapts orientation to align signals
with user groups, improving fairness and rates for weak users. An alternating
optimization approach combines convex optimization for beamforming/phase shifts
with exhaustive search and particle swarm optimization (PSO) for orientation.
Majorization-Minimization-based algorithms solve subproblems iteratively.
Simulation results show the framework achieves 24.1% rate improvement via
exhaustive search and 20.0% via PSO over the non-rotatable RIS baseline, with
PSO performance close to the exhaustive search upper bound, highlighting the
benefits of physical-layer multicast and orientation optimization.

</details>


### [75] [An Optimal Transport-Based Method for Computing LM Rate and Its Convergence Analysis](https://arxiv.org/abs/2507.20129)
*Shitong Wu,Wenhao Ye,Xinwei Li,Lingyi Chen,Wenyi Zhang,Huihui Wu,Hao Wu*

Main category: cs.IT

TL;DR: 论文提出了一种基于Sinkhorn算法的新方法，用于高效计算LM率（失配容量的下界），解决了传统数值方法在高输入字母表规模下的计算难题。


<details>
  <summary>Details</summary>
Motivation: LM率作为失配容量的下界，比GMI更紧，但计算复杂度高，传统方法难以应对大规模问题。

Method: 将LM率计算重新表述为带约束的最优传输问题，并基于Sinkhorn算法设计新算法。

Result: 算法具有次线性收敛速度，数值实验验证了其高效性和可行性。

Conclusion: 新方法为LM率的高效计算提供了可行方案，适用于大规模通信场景。

Abstract: The mismatch capacity characterizes the highest information rate of the
channel under a prescribed decoding metric and serves as a critical performance
indicator in numerous practical communication scenarios. Compared to the
commonly used Generalized Mutual Information (GMI), the Lower bound on the
Mismatch capacity (LM rate) generally provides a tighter lower bound on the
mismatch capacity. However, the efficient computation of the LM rate is
significantly more challenging than that of the GMI, particularly as the size
of the channel input alphabet increases. This growth in complexity renders
standard numerical methods (e.g., interior point methods) computationally
intensive and, in some cases, impractical. In this work, we reformulate the
computation of the LM rate as a special instance of the optimal transport (OT)
problem with an additional constraint. Building on this formulation, we develop
a novel numerical algorithm based on the Sinkhorn algorithm, which is well
known for its efficiency in solving entropy regularized optimization problems.
We further provide the convergence analysis of the proposed algorithm,
revealing that the algorithm has a sub-linear convergence rate. Numerical
experiments demonstrate the feasibility and efficiency of the proposed
algorithm for the computation of the LM rate.

</details>


### [76] [Sparse Regression Codes for Secret Key Agreement: Achieving Strong Secrecy and Near-Optimal Rates for Gaussian Sources](https://arxiv.org/abs/2507.20157)
*Emmanouil M. Athanasakos,Hariprasad Manjunath*

Main category: cs.IT

TL;DR: 本文提出了一种基于稀疏回归码（SPARCs）的高斯源秘密密钥协议方案，展示了其近最优的密钥率和强保密性。


<details>
  <summary>Details</summary>
Motivation: 利用SPARCs在率失真和Wyner-Ziv编码中的最优性，构建一种低复杂度且安全的密钥生成协议。

Method: 采用SPARCs的嵌套结构，结合量化参数优化，实现密钥生成。

Result: 方案实现了接近最优的密钥率，并揭示了量化参数与密钥率之间的权衡关系。

Conclusion: SPARCs为安全密钥生成提供了理论支持，是现有方案的低复杂度替代方案。

Abstract: Secret key agreement from correlated physical layer observations is a
cornerstone of information-theoretic security. This paper proposes and
rigorously analyzes a complete, constructive protocol for secret key agreement
from Gaussian sources using Sparse Regression Codes (SPARCs). Our protocol
systematically leverages the known optimality of SPARCs for both
rate-distortion and Wyner-Ziv (WZ) coding, facilitated by their inherent nested
structure. The primary contribution of this work is a comprehensive end-to-end
analysis demonstrating that the proposed scheme achieves near-optimal secret
key rates with strong secrecy guarantees, as quantified by a vanishing
variational distance. We explicitly characterize the gap to the optimal rate,
revealing a fundamental trade-off between the key rate and the required public
communication overhead, which is governed by a tunable quantization parameter.
Furthermore, we uncover a non-trivial constrained optimization for this
parameter, showing that practical constraints on the SPARC code parameters
induce a peak in the achievable secret key rate. This work establishes SPARCs
as a viable and theoretically sound framework for secure key generation,
providing a compelling low-complexity alternative to existing schemes and
offering new insights into the practical design of such protocols.

</details>


### [77] [Stochastic Channel Models for Satellite Mega-Constellations](https://arxiv.org/abs/2507.20255)
*Brendon McBain,Yi Hong,Emanuele Viterbo*

Main category: cs.IT

TL;DR: 提出了一种用于LEO卫星与地面用户通信的通用卫星信道模型，基于非齐次二项点过程（NBPP）建模卫星位置和方向，推导了功率增益、传播延迟和多普勒频移的概率分布，并分析了散射函数和全局信道参数。


<details>
  <summary>Details</summary>
Motivation: 为快速移动的LEO卫星与地面用户之间的通信提供一种通用的信道模型，以支持大规模星座系统的设计和分析。

Method: 使用非齐次二项点过程（NBPP）建模卫星位置和方向，推导功率增益、传播延迟和多普勒频移的概率分布，并分析散射函数和全局信道参数。

Result: 模型推导的信道统计和全局参数与Starlink星座的实际轨道模拟结果高度吻合。

Conclusion: 提出的信道模型为大规模LEO星座通信系统的设计和分析提供了理论基础，且具有实际应用的潜力。

Abstract: A general satellite channel model is proposed for communications between a
rapidly moving low Earth orbit (LEO) satellite in a mega-constellation and a
stationary user on Earth. The channel uses a non-homogeneous binomial point
process (NBPP) for modelling the satellite positions, marked with an
ascending/descending binary random variable for modelling the satellite
directions. Using the marked NBPP, we derive the probability distributions of
power gain, propagation delay, and Doppler shift, resulting in a stochastic
signal propagation model for the mega-constellation geometry in isolation of
other effects. This forms the basis for our proposed channel model as a
randomly time-varying channel. The scattering function of this channel is
derived to characterise how the received power is spread in the delay-Doppler
domain. Global channel parameters such as path loss and channel spread are
analysed in terms of the scattering function. The channel statistics and the
global channel parameters closely match realistic orbit simulations of the
Starlink constellation.

</details>


### [78] [Ensemble Average Analysis of Non-Adaptive Group Testing with Sparse Pooling Graphs](https://arxiv.org/abs/2507.20281)
*Emna Ben Yacoub,Gianluigi Liva,Enrico Paolini,Marco Chiani*

Main category: cs.IT

TL;DR: 论文通过组合分析研究了稀疏池化图中非自适应群测试的假警报（FA）和漏检（MD）概率，针对无噪声非定量场景下的组合正交匹配追踪和确定缺陷检测算法。


<details>
  <summary>Details</summary>
Motivation: 旨在分析稀疏池化图中非自适应群测试的性能，为相关算法提供理论支持。

Method: 采用集合平均视角，计算具有规定度分布的池化图集合的平均FA/MD概率。

Result: 数值实验验证了分析的准确性，表明该方法能有效表征基于稀疏池化图的非自适应群测试方案性能。

Conclusion: 提出的技术可用于评估稀疏池化图在非自适应群测试中的表现。

Abstract: A combinatorial analysis of the false alarm (FA) and misdetection (MD)
probabilities of non-adaptive group testing with sparse pooling graphs is
developed. The analysis targets the combinatorial orthogonal matching pursuit
and definite defective detection algorithms in the noiseless, non-quantitative
setting. The approach follows an ensemble average perspective, where average
FA/MD probabilities are computed for pooling graph ensembles with prescribed
degree distributions. The accuracy of the analysis is demonstrated through
numerical examples, showing that the proposed technique can be used to
characterize the performance of non-adaptive group testing schemes based on
sparse pooling graphs.

</details>


### [79] [Rethinking Multi-User Communication in Semantic Domain: Enhanced OMDMA by Shuffle-Based Orthogonalization and Diffusion Denoising](https://arxiv.org/abs/2507.20477)
*Maojun Zhang,Guangxu Zhu,Xiaoming Chen,Kaibin Huang,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 提出了一种基于随机置换JSCC特征向量的新型框架，通过将用户间干扰转化为类高斯噪声，显著提升了语义通信的性能。


<details>
  <summary>Details</summary>
Motivation: 解决语义通信中用户间干扰严重降低关键语义信息的问题，避免传统方法需要用户特定JSCC模型的复杂性。

Method: 采用随机置换JSCC特征向量的方法，为每个用户分配独特的置换模式，将干扰转化为噪声，并利用扩散模型进行缓解。

Result: 在模拟中表现出优于现有技术的性能，提升了语义保真度、抗干扰能力和可扩展性。

Conclusion: 该框架简化了系统设计，增强了隐私性，并在语义相关数据场景中进一步优化了性能。

Abstract: Inter-user interference remains a critical bottleneck in wireless
communication systems, particularly in the emerging paradigm of semantic
communication (SemCom). Compared to traditional systems, inter-user
interference in SemCom severely degrades key semantic information, often
causing worse performance than Gaussian noise under the same power level. To
address this challenge, inspired by the recently proposed concept of Orthogonal
Model Division Multiple Access (OMDMA) that leverages semantic orthogonality
rooted in the personalized joint source and channel (JSCC) models to
distinguish users, we propose a novel, scalable framework that eliminates the
need for user-specific JSCC models as did in original OMDMA. Our key innovation
lies in shuffle-based orthogonalization, where randomly permuting the positions
of JSCC feature vectors transforms inter-user interference into Gaussian-like
noise. By assigning each user a unique shuffling pattern, the interference is
treated as channel noise, enabling effective mitigation using diffusion models
(DMs). This approach not only simplifies system design by requiring a single
universal JSCC model but also enhances privacy, as shuffling patterns act as
implicit private keys. Additionally, we extend the framework to scenarios
involving semantically correlated data. By grouping users based on semantic
similarity, a cooperative beamforming strategy is introduced to exploit
redundancy in correlated data, further improving system performance. Extensive
simulations demonstrate that the proposed method outperforms state-of-the-art
multi-user SemCom frameworks, achieving superior semantic fidelity, robustness
to interference, and scalability-all without requiring additional training
overhead.

</details>


### [80] [Cooperative Jamming Detection Using Low-Rank Structure of Received Signal Matrix](https://arxiv.org/abs/2507.20504)
*Amir Mehrabian,Georges Kaddoum*

Main category: cs.IT

TL;DR: 本文提出了一种基于接收信号矩阵低秩结构的协作干扰检测方法，利用似然比测试设计了多种场景下的检测器，并通过仿真验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 无线通信因其开放性和共享媒介特性容易受到恶意干扰，检测干扰是采取抗干扰策略的第一步。

Method: 利用接收信号矩阵的低秩结构，采用似然比测试设计检测器，涵盖不同友好节点和干扰节点数量及噪声统计信息水平的场景。

Result: 仿真表明，所提检测器优于现有方法，在协作感知网络中实现了鲁棒且准确的干扰检测。

Conclusion: 提出的方法在多种场景下表现优异，为协作网络中的干扰检测提供了有效解决方案。

Abstract: Wireless communication can be simply subjected to malicious attacks due to
its open nature and shared medium. Detecting jamming attacks is the first and
necessary step to adopt the anti-jamming strategies. This paper presents novel
cooperative jamming detection methods that use the low-rank structure of the
received signal matrix. We employed the likelihood ratio test to propose
detectors for various scenarios. We regarded several scenarios with different
numbers of friendly and jamming nodes and different levels of available
statistical information on noise. We also provided an analytical examination of
the false alarm performance of one of the proposed detectors, which can be used
to adjust the detection threshold. We discussed the synthetic signal generation
and the Monte Carlo (MC)-based threshold setting method, where knowledge of the
distribution of the jamming-free signal, as well as several parameters such as
noise variance and channel state information (CSI), is required to accurately
generate synthetic signals for threshold estimation. Extensive simulations
reveal that the proposed detectors outperform several existing methods,
offering robust and accurate jamming detection in a collaborative network of
sensing nodes.

</details>


### [81] [Handoff Design in User-Centric Cell-Free Massive MIMO Networks Using DRL](https://arxiv.org/abs/2507.20966)
*Hussein A. Ammar,Raviraj Adve,Shahram Shahbazpanahi,Gary Boudreau,Israfil Bahceci*

Main category: cs.IT

TL;DR: 本文提出了一种基于深度强化学习（DRL）的方法，用于预测和管理用户为中心的蜂窝自由大规模MIMO（UC-mMIMO）网络中移动用户的切换（HO）操作，以减少频繁切换带来的资源开销。


<details>
  <summary>Details</summary>
Motivation: 在UC-mMIMO网络中，用户移动性导致频繁的切换操作，增加了资源分配和释放的开销。本文旨在通过DRL技术优化切换策略，平衡用户速率和切换开销。

Method: 采用Soft Actor-Critic算法（连续动作空间）训练深度神经网络作为切换策略，并提出了一种结合切换惩罚的奖励函数。系统设计了两种变体：基于用户移动模式的DA观测和基于大尺度衰落历史的HA观测。

Result: 仿真结果表明，基于连续动作空间的DRL方法比离散空间更具扩展性，且切换策略能够自动学习在特定时隙集中切换以减少开销。系统响应时间小于0.4毫秒。

Conclusion: 本文提出的DRL方法有效减少了UC-mMIMO网络中切换操作的开销，同时保持了用户速率，适用于实时操作。

Abstract: In the user-centric cell-free massive MIMO (UC-mMIMO) network scheme, user
mobility necessitates updating the set of serving access points to maintain the
user-centric clustering. Such updates are typically performed through handoff
(HO) operations; however, frequent HOs lead to overheads associated with the
allocation and release of resources. This paper presents a deep reinforcement
learning (DRL)-based solution to predict and manage these connections for
mobile users. Our solution employs the Soft Actor-Critic algorithm, with
continuous action space representation, to train a deep neural network to serve
as the HO policy. We present a novel proposition for a reward function that
integrates a HO penalty in order to balance the attainable rate and the
associated overhead related to HOs. We develop two variants of our system; the
first one uses mobility direction-assisted (DA) observations that are based on
the user movement pattern, while the second one uses history-assisted (HA)
observations that are based on the history of the large-scale fading (LSF).
Simulation results show that our DRL-based continuous action space approach is
more scalable than discrete space counterpart, and that our derived HO policy
automatically learns to gather HOs in specific time slots to minimize the
overhead of initiating HOs. Our solution can also operate in real time with a
response time less than 0.4 ms.

</details>


### [82] [Construction of non-generalized Reed-Solomon MDS codes based on systematic generator matrix](https://arxiv.org/abs/2507.20559)
*Shengwei Liu,Hongwei Liu,Bocong Chen*

Main category: cs.IT

TL;DR: 论文证明了两类广义扭曲Reed-Solomon（GTRS）码是非GRS码，并提供了GTRS码的系统生成矩阵，还构造了非GRS的MDS码。


<details>
  <summary>Details</summary>
Motivation: 研究非GRS的MDS码，扩展MDS码的构造方法。

Method: 通过广义扭曲Reed-Solomon（GTRS）码，证明其非GRS性质，并构造系统生成矩阵。

Result: 证明了两类GTRS码是非GRS码，并提供了系统生成矩阵。

Conclusion: GTRS码为非GRS的MDS码提供了新的构造方法。

Abstract: Maximum distance separable (MDS) codes are considered optimal because the
minimum distance cannot be improved for a given length and code size. The most
prominent MDS codes are likely the generalized Reed-Solomon (GRS) codes. In
1989, Roth and Lempel constructed a type of MDS code that is not a GRS code
(referred to as non-GRS). In 2017, Beelen et al. introduced twisted
Reed-Solomon (TRS) codes and demonstrated that many MDS TRS codes are indeed
non-GRS. Following this, the definition of TRS codes was generalized to the
most comprehensive form, which we refer to as generalized twisted Reed-Solomon
(GTRS) codes. In this paper, we prove that two families of GTRS codes are
non-GRS and provide a systematic generator matrix for a class of GTRS codes.
Inspired by the form of the systematic generator matrix for GTRS codes,we also
present a construction of non-GRS MDS codes.

</details>


### [83] [A note on the Artstein-Avidan-Milman's generalized Legendre transforms](https://arxiv.org/abs/2507.20577)
*Frank Nielsen*

Main category: cs.IT

TL;DR: 论文证明了广义勒让德变换对应于仿射变形函数的普通勒让德变换，并从信息几何的角度进行了初步解释。


<details>
  <summary>Details</summary>
Motivation: 研究广义勒让德变换与普通勒让德变换之间的关系，揭示其数学本质。

Method: 通过分析仿射变形函数，证明广义勒让德变换与普通勒让德变换的对应性。

Result: 广义勒让德变换实际上是仿射变形函数的普通勒让德变换。

Conclusion: 广义勒让德变换的本质可以通过信息几何的视角进一步理解。

Abstract: Artstein-Avidan and Milman [Annals of mathematics (2009), (169):661-674]
characterized invertible reverse-ordering transforms on the space of
lower-semi-continuous extended real-valued convex functions as affine
deformations of the ordinary Legendre transform. In this note, we prove that
all those generalized Legendre transforms on functions correspond to the
ordinary Legendre transform on dually corresponding affine-deformed functions.
That is, generalized convex conjugates are convex conjugates of affine-deformed
functions. We conclude this note by sketching how this result can be
interpreted from the lens of information geometry.

</details>


### [84] [The Coverage Depth Problem in DNA Storage Over Small Alphabets](https://arxiv.org/abs/2507.20639)
*Matteo Bertuzzo,Alberto Ravagnani,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 研究小有限域上编码在DNA数据存储中的覆盖深度问题，提供闭式解并与理论界限比较。


<details>
  <summary>Details</summary>
Motivation: 解决在基础域不够大时无法使用MDS编码的问题，探索小有限域编码的性能。

Method: 应用概率论、对偶理论和组合数学技术，分析不同编码家族的期望读取次数。

Result: 提供了小有限域编码的闭式解，并与渐近理论界限进行了比较。

Conclusion: 小有限域编码在覆盖深度问题中表现良好，为实际应用提供了理论支持。

Abstract: The coverage depth problem in DNA data storage is about minimizing the
expected number of reads until all data is recovered. When they exist, MDS
codes offer the best performance in this context. This paper focuses on the
scenario where the base field is not large enough to allow the existence of MDS
codes. We investigate the performance for the coverage depth problem of codes
defined over a small finite field, providing closed formulas for the expected
number of reads for various code families. We also compare the results with the
theoretical bounds in asymptotic regimes. The techniques we apply range from
probability, to duality theory and combinatorics.

</details>


### [85] [The Random Variables of the DNA Coverage Depth Problem](https://arxiv.org/abs/2507.20645)
*Şeyma Bodur,Stefano Lia,Hiram H. López,Rati Ludhani,Alberto Ravagnani,Lisa Seccia*

Main category: cs.IT

TL;DR: 研究DNA数据存储中的随机访问覆盖深度问题，优化线性编码的读取次数，验证并改进现有猜想，分析几何编码构造，并研究覆盖深度问题的完整分布。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储系统需要高效的随机访问机制，覆盖深度是关键性能指标，但目前研究主要集中在期望值上，缺乏对完整分布的分析。

Method: 通过计算渐近性能验证和改进现有编码构造的猜想，分析基于平衡准弧的几何编码构造，并优化其参数，同时研究覆盖深度问题的完整分布。

Result: 验证并改进了现有猜想，优化了几何编码构造的参数，发现完整分布能区分看似行为相同的编码构造。

Conclusion: 完整分布分析为DNA数据存储系统的编码设计提供了更全面的性能评估方法，有助于优化随机访问效率。

Abstract: DNA data storage systems encode digital data into DNA strands, enabling dense
and durable storage. Efficient data retrieval depends on coverage depth, a key
performance metric. We study the random access coverage depth problem and focus
on minimizing the expected number of reads needed to recover information
strands encoded via a linear code. We compute the asymptotic performance of a
recently proposed code construction, establishing and refining a conjecture in
the field by giving two independent proofs. We also analyze a geometric code
construction based on balanced quasi-arcs and optimize its parameters. Finally,
we investigate the full distribution of the random variables that arise in the
coverage depth problem, of which the traditionally studied expectation is just
the first moment. This allows us to distinguish between code constructions
that, at first glance, may appear to behave identically.

</details>
