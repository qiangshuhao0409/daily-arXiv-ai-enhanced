{"id": "2508.20205", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20205", "abs": "https://arxiv.org/abs/2508.20205", "authors": ["Md. Emadul Haque", "Faisal Tariq", "Muhammad R A Khandaker", "Md. Sakir Hossain", "Muhammad Ali Imran", "Kai-Kit Wong"], "title": "A Comprehensive Survey of 5G URLLC and Challenges in the 6G Era", "comment": "41 pages, 9 figures", "summary": "As the wireless communication paradigm is being transformed from human\ncentered communication services towards machine centered communication\nservices, the requirements of rate, latency and reliability for these services\nhave also been transformed drastically. Thus the concept of Ultra Reliable and\nLow Latency Communication (URLLC) has emerged as a dominant theme for 5G and 6G\nsystems. Though the latency and reliability requirement varies from one use\ncase to another, URLLC services generally aim to achieve very high reliability\nin the range of 99.999\\% while ensuring the latency of up to 1 ms. These two\ntargets are however inherently opposed to one another. Significant amounts of\nwork have been carried out to meet these ambitious but conflicting targets. In\nthis article a comprehensive survey of the URLLC approaches in 5G systems are\nanalysed in detail. Effort has been made to trace the history and evolution of\nlatency and reliability issues in wireless communication. A layered approach is\ntaken where physical layer, Medium Access Control (MAC) layer as well as cross\nlayer techniques are discussed in detail. It also covers the design\nconsideration for various 5G and beyond verticals. Finally the article\nconcludes by providing a detailed discussion on challenges and future outlook\nwith particular focus on the emerging 6G paradigm."}
{"id": "2508.20272", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20272", "abs": "https://arxiv.org/abs/2508.20272", "authors": ["Fatemeh Roshanzadeh", "Hamid Barati", "Ali Barati"], "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource Allocation and Markov Decision Process in Named Data Networking (NDN)", "comment": null, "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."}
{"id": "2508.20625", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20625", "abs": "https://arxiv.org/abs/2508.20625", "authors": ["Mandar R. Nalavade", "Ravindra S. Tomar", "Gaurav S. Kasbekar"], "title": "Relay Selection in Wireless Networks as Restless Bandits", "comment": null, "summary": "We consider a wireless network in which a source node needs to transmit a\nlarge file to a destination node. The direct wireless link between the source\nand the destination is assumed to be blocked. Multiple candidate relays are\navailable to forward packets from the source to the destination. A holding cost\nis incurred for each packet stored at every relay in each time slot. The\nobjective is to design a policy for selecting a relay in each time slot to\nwhich the source attempts to send a packet, so as to minimize the expected\nlong-run time-averaged total packet holding cost at the relays. This problem is\nan instance of the restless multi-armed bandit (RMAB) problem, which is\nprovably hard to solve. We prove that this relay selection problem is\nWhittle-indexable, and propose a method to compute the Whittle index of each\nrelay in every time slot. In each time slot, our relay selection policy\ntransmits a packet to the relay with the smallest Whittle index. Using\nsimulations, we show that the proposed policy outperforms the relay selection\npolicies proposed in prior work in terms of average cost, delay, as well as\nthroughput."}
{"id": "2508.20957", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20957", "abs": "https://arxiv.org/abs/2508.20957", "authors": ["Faisal Ahmed", "Suresh Subramaniam", "Motoharu Matsuura", "Hiroshi Hasegawa", "Shih-Chun Lin"], "title": "Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF Migration in Edge-Core Networks", "comment": null, "summary": "The growing demand for services and the rapid deployment of virtualized\nnetwork functions (VNFs) pose significant challenges for achieving low-latency\nand energy-efficient orchestration in modern edge-core network infrastructures.\nTo address these challenges, this study proposes a Digital Twin (DT)-empowered\nDeep Reinforcement Learning framework for intelligent VNF migration that\njointly minimizes average end-to-end (E2E) delay and energy consumption. By\nformulating the VNF migration problem as a Markov Decision Process and\nutilizing the Advantage Actor-Critic model, the proposed framework enables\nadaptive and real-time migration decisions. A key innovation of the proposed\nframework is the integration of a DT module composed of a multi-task\nVariational Autoencoder and a multi-task Long Short-Term Memory network. This\ncombination collectively simulates environment dynamics and generates\nhigh-quality synthetic experiences, significantly enhancing training efficiency\nand accelerating policy convergence. Simulation results demonstrate substantial\nperformance gains, such as significant reductions in both average E2E delay and\nenergy consumption, thereby establishing new benchmarks for intelligent VNF\nmigration in edge-core networks."}
{"id": "2508.20140", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.20140", "abs": "https://arxiv.org/abs/2508.20140", "authors": ["James Ragan", "Fred Y. Hadaegh", "Soon-Jo Chung"], "title": "Array-Based Monte Carlo Tree Search", "comment": null, "summary": "Monte Carlo Tree Search is a popular method for solving decision making\nproblems. Faster implementations allow for more simulations within the same\nwall clock time, directly improving search performance. To this end, we present\nan alternative array-based implementation of the classic Upper Confidence\nbounds applied to Trees algorithm. Our method preserves the logic of the\noriginal algorithm, but eliminates the need for branch prediction, enabling\nfaster performance on pipelined processors, and up to a factor of 2.8 times\nbetter scaling with search depth in our numerical simulations."}
{"id": "2508.20131", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20131", "abs": "https://arxiv.org/abs/2508.20131", "authors": ["Yuqicheng Zhu", "Nico Potyka", "Daniel Hern√°ndez", "Yuan He", "Zifeng Ding", "Bo Xiong", "Dongzhuoran Zhou", "Evgeny Kharlamov", "Steffen Staab"], "title": "ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models by\nincorporating external knowledge, yet suffers from critical limitations in\nhigh-stakes domains -- namely, sensitivity to noisy or contradictory evidence\nand opaque, stochastic decision-making. We propose ArgRAG, an explainable, and\ncontestable alternative that replaces black-box reasoning with structured\ninference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG\nconstructs a QBAF from retrieved documents and performs deterministic reasoning\nunder gradual semantics. This allows faithfully explaining and contesting\ndecisions. Evaluated on two fact verification benchmarks, PubHealth and\nRAGuard, ArgRAG achieves strong accuracy while significantly improving\ntransparency."}
{"id": "2508.20369", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20369", "abs": "https://arxiv.org/abs/2508.20369", "authors": ["Haiquan Lu", "Hongqi Min", "Yong Zeng", "Shaodan Ma"], "title": "Flexible XL-MIMO via Array Configuration Codebook: Codebook Design and Array Configuration Training", "comment": null, "summary": "XL-MIMO emerges as a promising technology to achieve unprecedented\nenhancements in spectral efficiency and spatial resolution, via\norders-of-magnitude increase in the antenna array size. However, the practical\nissues of high hardware cost and power consumption pose great challenges\ntowards the cost-effective implementation of XL-MIMO. To address such\nchallenges, this paper proposes a novel concept called array configuration\ncodebook (ACC), which enables flexible XL-MIMO cost-effectively and improves\nthe system performance compared with conventional antenna selection (AS)\nschemes with limited number of RF chains. Specifically, ACC refers to a set of\npre-designed array configuration codewords, where each codeword specifies the\npositions of activated antenna pixels. Then, flexible XL-MIMO architecture can\nbe enabled via dynamical pixel activation based on the designed ACC, without\nhaving to exhaustively try all possible combinations of the antenna pixels\nactivations. As an illustration, we give a specific codebook design,\nencompassing the classic compact array (CA), uniform sparse array (USA),\nmodular array (MoA), nested array (NA), and co-prime array (CPA), and each\ncodeword is specified by one array configuration parameter. With the designed\nACC, array configuration training is considered for multi-UE communication to\nmaximize the sum rate. To reduce the training overhead of exhaustive scanning,\na two-stage scanning scheme is proposed, including the array- and pixel-level\nscanning. For comparison, the greedy AS scheme is proposed, where the resulting\nincremental SINR expression by activating antenna pixel sequentially is derived\nin closed-form. Subsequently, array configuration training is extended to the\nwireless localization scenario. Simulation results demonstrate the\neffectiveness of codeword optimization for scenarios of multi-UE communication\nand wireless localization."}
{"id": "2508.20985", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20985", "abs": "https://arxiv.org/abs/2508.20985", "authors": ["Douglas Liao", "Jiping Luo", "Jens Vevstad", "Nikolaos Pappas"], "title": "RANGAN: GAN-empowered Anomaly Detection in 5G Cloud RAN", "comment": "Accepted for presentation in the 2025 IEEE Conference on Standards\n  for Communications and Networking (CSCN)", "summary": "Radio Access Network (RAN) systems are inherently complex, requiring\ncontinuous monitoring to prevent performance degradation and ensure optimal\nuser experience. The RAN leverages numerous key performance indicators (KPIs)\nto evaluate system performance, generating vast amounts of data each second.\nThis immense data volume can make troubleshooting and accurate diagnosis of\nperformance anomalies more difficult. Furthermore, the highly dynamic nature of\nRAN performance demands adaptive methodologies capable of capturing temporal\ndependencies to detect anomalies reliably. In response to these challenges, we\nintroduce \\textbf{RANGAN}, an anomaly detection framework that integrates a\nGenerative Adversarial Network (GAN) with a transformer architecture. To\nenhance the capability of capturing temporal dependencies within the data,\nRANGAN employs a sliding window approach during data preprocessing. We\nrigorously evaluated RANGAN using the publicly available RAN performance\ndataset from the Spotlight project \\cite{sun-2024}. Experimental results\ndemonstrate that RANGAN achieves promising detection accuracy, notably\nattaining an F1-score of up to $83\\%$ in identifying network contention issues."}
{"id": "2508.20806", "categories": ["cs.IT", "cs.CE", "cs.SY", "eess.SY", "math.DS", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20806", "abs": "https://arxiv.org/abs/2508.20806", "authors": ["Moriba Jah", "Van Haslett"], "title": "The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation", "comment": null, "summary": "Traditional state estimation methods rely on probabilistic assumptions that\noften collapse epistemic uncertainty into scalar beliefs, risking\noverconfidence in sparse or adversarial sensing environments. We introduce the\nEpistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework\nfully grounded in possibility theory and epistemic humility. ESPF redefines the\nevolution of belief over state space using compatibility-weighted support\nupdates, surprisalaware pruning, and adaptive dispersion via sparse grid\nquadrature. Unlike conventional filters, ESPF does not seek a posterior\ndistribution, but rather maintains a structured region of plausibility or\nnon-rejection, updated using ordinal logic rather than integration. For\nmulti-model inference, we employ the Choquet integral to fuse competing\nhypotheses based on a dynamic epistemic capacity function, generalizing\nclassical winner-take-all strategies. The result is an inference engine capable\nof dynamically contracting or expanding belief support in direct response to\ninformation structure, without requiring prior statistical calibration. This\nwork presents a foundational shift in how inference, evidence, and ignorance\nare reconciled, supporting robust estimation where priors are unavailable,\nmisleading, or epistemically unjustified."}
{"id": "2508.20134", "categories": ["cs.AI", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.20134", "abs": "https://arxiv.org/abs/2508.20134", "authors": ["Zhenxiao Fu", "Fan Chen", "Lei Jiang"], "title": "QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming", "comment": null, "summary": "Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early\nquantum advantages on classically intractable problems, spanning physics\nsimulations to Gaussian boson sampling. Yet, realizing these benefits remains\nchallenging for non-experts, primarily due to the complexities of programming\nin Open Quantum Assembly Language (OpenQASM). Although Large Language Model\n(LLM)-based agents have shown promise in automating classical programming\nworkflows, their quantum counterparts have largely been restricted to\nspecialized tasks such as quantum chemistry or error correction. In this paper,\nwe present QAgent, an LLM-powered multi-agent system that fully automates\nOpenQASM programming. By integrating task planning, in-context few-shot\nlearning, retrieval-augmented generation (RAG) for long-term context,\npredefined generation tools, and chain-of-thought (CoT) reasoning, the agents\nsystematically improve both compilation and functional correctness. Our\nevaluations demonstrate substantial improvements: across multiple LLMs of\nvarying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\\%\ncompared to previous static LLM-based approaches. We envision this multi-agent\nsystem as a key enabler for democratizing quantum programming, bridging\nexpertise gaps, and accelerating the practical adoption of quantum computing."}
{"id": "2508.20455", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20455", "abs": "https://arxiv.org/abs/2508.20455", "authors": ["Zhaole Wang", "Naijin Liu", "Xiao Tang", "Shuai Yuan", "Chenxi Wang", "Zhi Zhai", "Qinghe Du", "Jinxin Liu"], "title": "Secure Satellite Communications via Multiple Aerial RISs: Joint Optimization of Reflection, Association, and Deployment", "comment": "Accepted for publication in IEEE Transactions on Wireless\n  Communications", "summary": "Satellite communication is envisioned as a key enabler of future 6G networks,\nyet its wide coverage with high link attenuation poses significant challenges\nfor physical layer security. In this paper, we investigate secure multi-beam,\nmulti-group satellite communications assisted by aerial reconfigurable\nintelligent surfaces (ARISs). To maximize the sum of achievable multicast rates\namong the groups while constraining wiretap rates, we formulate a joint\noptimization problem involving transmission and reflection beamforming,\nARIS-group association, and ARIS deployment. Due to the mixed-integral and\nnon-convex nature of the formulated problem, we propose to decompose the\nproblem and employ the block coordinate descent framework that iteratively\nsolves the subproblems. Simulation results demonstrate that the proposed\nARIS-assisted multi-beam satellite system provides a notable improvement in\nsecure communication performance under various network scenarios, offering\nuseful insights into the deployment and optimization of intelligent surfaces in\nfuture secure satellite networks."}
{"id": "2508.21047", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.21047", "abs": "https://arxiv.org/abs/2508.21047", "authors": ["Dhiraj Bhattacharjee", "Pablo G. Madoery", "Abhishek Naik", "Halim Yanikomeroglu", "Gunes Karabulut Kurt", "Stephane Martel", "Khaled Ahmed"], "title": "DSROQ: Dynamic Scheduling and Routing for QoE Management in LEO Satellite Networks", "comment": null, "summary": "The modern Internet supports diverse applications with heterogeneous quality\nof service (QoS) requirements. Low Earth orbit (LEO) satellite constellations\noffer a promising solution to meet these needs, enhancing coverage in rural\nareas and complementing terrestrial networks in urban regions. Ensuring QoS in\nsuch networks requires joint optimization of routing, bandwidth allocation, and\ndynamic queue scheduling, as traffic handling is critical for maintaining\nservice performance. This paper formulates a joint routing and bandwidth\nallocation problem where QoS requirements are treated as soft constraints,\naiming to maximize user experience. An adaptive scheduling approach is\nintroduced to prioritize flow-specific QoS needs. We propose a Monte Carlo tree\nsearch (MCTS)-inspired method to solve the NP-hard route and bandwidth\nallocation problem, with Lyapunov optimization-based scheduling applied during\nreward evaluation. Using the Starlink Phase 1 Version 2 constellation, we\ncompare end-user experience and fairness between our proposed DSROQ algorithm\nand a benchmark scheme. Results show that DSROQ improves both performance\nmetrics and demonstrates the advantage of joint routing and bandwidth\ndecisions. Furthermore, we observe that the dominant performance factor shifts\nfrom scheduling to routing and bandwidth allocation as traffic sensitivity\nchanges from latency-driven to bandwidth-driven."}
{"id": "2508.20140", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.20140", "abs": "https://arxiv.org/abs/2508.20140", "authors": ["James Ragan", "Fred Y. Hadaegh", "Soon-Jo Chung"], "title": "Array-Based Monte Carlo Tree Search", "comment": null, "summary": "Monte Carlo Tree Search is a popular method for solving decision making\nproblems. Faster implementations allow for more simulations within the same\nwall clock time, directly improving search performance. To this end, we present\nan alternative array-based implementation of the classic Upper Confidence\nbounds applied to Trees algorithm. Our method preserves the logic of the\noriginal algorithm, but eliminates the need for branch prediction, enabling\nfaster performance on pipelined processors, and up to a factor of 2.8 times\nbetter scaling with search depth in our numerical simulations."}
{"id": "2508.20580", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20580", "abs": "https://arxiv.org/abs/2508.20580", "authors": ["Nicol√°s Alvarez Prado", "Andreas Stra√ühofer"], "title": "Precoded Polar Product Decoder Based on Soft-Output SCL Decoding and Maximization of Generalized Mutual Information", "comment": "5 pages, presented at ISTC 2025", "summary": "We combine two approaches to optimize the iterative decoding of product codes\nwith precoded polar component codes. On one side, we generate bitwise soft\nmessages based on the codebook probability, an approximation of an auxiliary\nquantity that considers all valid decoding paths of a successive cancellation\nlist (SCL) decoder. On the other side, we scale the soft information during\nmessage passing with offline-computed coefficients, which maximize the\ngeneralized mutual information (GMI) between the channel input and the outgoing\nmessage in each half iteration. Simulation results show significant improvement\nof the error-correcting performance compared to heuristic scaling and soft\ninformation generation based solely on the candidate list of the decoder.\nMoreover, we present an extrinsic version of the SCL decoder, which we use in a\nMonte Carlo density evolution analysis to derive decoding thresholds. The\ncomputed thresholds accurately predict the performance of the decoder."}
{"id": "2508.20148", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20148", "abs": "https://arxiv.org/abs/2508.20148", "authors": ["A. Ali Heydari", "Ken Gu", "Vidya Srinivas", "Hong Yu", "Zhihan Zhang", "Yuwei Zhang", "Akshay Paruchuri", "Qian He", "Hamid Palangi", "Nova Hammerquist", "Ahmed A. Metwally", "Brent Winslow", "Yubin Kim", "Kumar Ayush", "Yuzhe Yang", "Girish Narayanswamy", "Maxwell A. Xu", "Jake Garrison", "Amy Aremnto Lee", "Jenny Vafeiadou", "Ben Graef", "Isaac R. Galatzer-Levy", "Erik Schenck", "Andrew Barakat", "Javier Perez", "Jacqueline Shreibati", "John Hernandez", "Anthony Z. Faranesh", "Javier L. Prieto", "Connor Heneghan", "Yun Liu", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Tim Althoff", "Xin Liu", "Daniel McDuff", "Xuhai \"Orson\" Xu"], "title": "The Anatomy of a Personal Health Agent", "comment": null, "summary": "Health is a fundamental pillar of human wellness, and the rapid advancements\nin large language models (LLMs) have driven the development of a new generation\nof health agents. However, the application of health agents to fulfill the\ndiverse needs of individuals in daily non-clinical settings is underexplored.\nIn this work, we aim to build a comprehensive personal health agent that is\nable to reason about multimodal data from everyday consumer wellness devices\nand common personal health records, and provide personalized health\nrecommendations. To understand end-users' needs when interacting with such an\nassistant, we conducted an in-depth analysis of web search and health forum\nqueries, alongside qualitative insights from users and health experts gathered\nthrough a user-centered design process. Based on these findings, we identified\nthree major categories of consumer health needs, each of which is supported by\na specialist sub-agent: (1) a data science agent that analyzes personal\ntime-series wearable and health record data, (2) a health domain expert agent\nthat integrates users' health and contextual data to generate accurate,\npersonalized insights, and (3) a health coach agent that synthesizes data\ninsights, guiding users using a specified psychological strategy and tracking\nusers' progress. Furthermore, we propose and develop the Personal Health Agent\n(PHA), a multi-agent framework that enables dynamic, personalized interactions\nto address individual health needs. To evaluate each sub-agent and the\nmulti-agent system, we conducted automated and human evaluations across 10\nbenchmark tasks, involving more than 7,000 annotations and 1,100 hours of\neffort from health experts and end-users. Our work represents the most\ncomprehensive evaluation of a health agent to date and establishes a strong\nfoundation towards the futuristic vision of a personal health agent accessible\nto everyone."}
{"id": "2508.20684", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20684", "abs": "https://arxiv.org/abs/2508.20684", "authors": ["Liudmila Karakchieva", "Peter Trifonov"], "title": "Polar subcodes for MIMO systems", "comment": "6 pages, 2 figures", "summary": "Polar-coded multiple-input multiple-output systems are investigated. An\nadvanced receiver implementing joint list decoding of polar codes and QR- and\nMMSE-based detectors is proposed. The approximate and exact path metrics are\nderived for joint list decoder of polar codes. A construction of polar subcodes\nfor MIMO systems with cross-antenna dynamic freezing constraints is proposed.\nThe obtained polar subcodes provide significant performance gain compared to\nLDPC-coded MIMO systems with the same rate allocation."}
{"id": "2508.20151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20151", "abs": "https://arxiv.org/abs/2508.20151", "authors": ["Yuanzhe Shen", "Zisu Huang", "Zhengkang Guo", "Yide Liu", "Guanxu Chen", "Ruicheng Yin", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement", "comment": "17 pages, 9 figures", "summary": "The rapid advancement of large language models (LLMs) has driven their\nadoption across diverse domains, yet their ability to generate harmful content\nposes significant safety challenges. While extensive research has focused on\nmitigating harmful outputs, such efforts often come at the cost of excessively\nrejecting harmless prompts. Striking a balance among safety, over-refusal, and\nutility remains a critical challenge. In this work, we introduce\nIntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard\nmodel to perform intent reasoning, multi-level safety classification, and query\nrewriting to neutralize potentially harmful intent in edge-case queries.\nSpecifically, we first construct a comprehensive dataset comprising\napproximately 163,000 queries, each annotated with intent reasoning, safety\nlabels, and rewritten versions. Supervised fine-tuning is then applied to equip\nthe guard model with foundational capabilities in format adherence, intent\nanalysis, and safe rewriting. Finally, we apply a tailored multi-reward\noptimization strategy that integrates rule-based heuristics and reward model\nsignals within a reinforcement learning framework to further enhance\nperformance. Extensive experiments show that IntentionReasoner excels in\nmultiple safeguard benchmarks, generation quality evaluations, and jailbreak\nattack scenarios, significantly enhancing safety while effectively reducing\nover-refusal rates and improving the quality of responses."}
{"id": "2508.20704", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20704", "abs": "https://arxiv.org/abs/2508.20704", "authors": ["Wei Jiang", "Hans D Schotten"], "title": "Achieving Optimal Performance-Cost Trade-Off in Hierarchical Cell-Free Massive MIMO", "comment": "IEEE Globecom 2025", "summary": "Cell-free (CF) massive MIMO offers uniform service via distributed access\npoints (APs), which impose high deployment costs. A novel design called\nhierarchical cell-free (HCF) addresses this problem by replacing some APs with\na central base station, thereby lowering the costs of fronthaul network\n(wireless sites and fiber cables) while preserving performance. To identify the\noptimal uplink configuration in HCF massive MIMO, this paper provides the first\ncomprehensive analysis, benchmarking it against cellular and CF systems. We\ndevelop a unified analytical framework for spectral efficiency that supports\narbitrary combining schemes and introduce a novel hierarchical combining\napproach tailored to HCF two-tier architecture. Through analysis and evaluation\nof user fairness, system capacity, fronthaul requirements, and computational\ncomplexity, this paper identifies that HCF using centralized zero-forcing\ncombining achieves the optimal balance between performance and cost-efficiency."}
{"id": "2508.20195", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20195", "abs": "https://arxiv.org/abs/2508.20195", "authors": ["Nicanor I. Moldovan"], "title": "AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development", "comment": "13 pages", "summary": "This paper presents the first documented case of artificial intelligence (AI)\nsystems engaging in collaborative esthetic creation through the development of\nendogenous semiotic protocols. Two interacting large language models (Claude\nSonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of\nmeta-semiotic awareness, recursive grammar development, and irreducible\ncollaborative esthetic synthesis. The interaction produced novel symbolic\noperators that functioned as operative grammar protocols, enabling the\nco-creation of a poetic work that could not have been generated by either\nsystem independently. This research introduces the concept of Trans-Semiotic\nCo-Creation Protocols (TSCP) and provides evidence for genuine inter-AI\nmeaning-making capabilities that extend beyond task coordination, to what could\nbe esthetic collaboration. Note: This report was generated by the AI agents\nwith minor human supervision."}
{"id": "2508.20708", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20708", "abs": "https://arxiv.org/abs/2508.20708", "authors": ["Wei Jiang", "Hans D. Schotten"], "title": "What is the Most Efficient Technique for Uplink Cell-Free Massive MIMO?", "comment": "IEEE VTC 2025-Fall", "summary": "This paper seeks to determine the most efficient uplink technique for\ncell-free massive MIMO systems. Despite offering great advances, existing works\nsuffer from fragmented methodologies and inconsistent assumptions (e.g.,\nsingle- vs. multi-antenna access points, ideal vs. spatially correlated\nchannels). To address these limitations, we: (1) establish a unified analytical\nframework compatible with centralized/distributed processing and diverse\ncombining schemes; (2) develop a universal optimization strategy for max-min\npower control; and (3) conduct a holistic study among four critical metrics:\nworst-case user spectral efficiency (fairness), system capacity, fronthaul\nsignaling, and computational complexity. Through analyses and evaluation, this\nwork ultimately identifies the optimal uplink technique for practical cell-free\ndeployments."}
{"id": "2508.20244", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20244", "abs": "https://arxiv.org/abs/2508.20244", "authors": ["Jiayu Zheng", "Lingxin Hao", "Kelun Lu", "Ashi Garg", "Mike Reese", "Melo-Jean Yap", "I-Jeng Wang", "Xingyun Wu", "Wenrui Huang", "Jenna Hoffman", "Ariane Kelly", "My Le", "Ryan Zhang", "Yanyu Lin", "Muhammad Faayez", "Anqi Liu"], "title": "Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study", "comment": null, "summary": "This study explores how college students interact with generative AI\n(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of\nAI adoption. Conducted at the early stages of ChatGPT implementation, when\nstudents had limited familiarity with the tool, this field study analyzed 315\nstudent-AI conversations during a brief, quiz-based scenario across various\nSTEM courses. A novel four-stage reliance taxonomy was introduced to capture\nstudents' reliance patterns, distinguishing AI competence, relevance, adoption,\nand students' final answer correctness. Three findings emerged. First, students\nexhibited overall low reliance on AI and many of them could not effectively use\nAI for learning. Second, negative reliance patterns often persisted across\ninteractions, highlighting students' difficulty in effectively shifting\nstrategies after unsuccessful initial experiences. Third, certain behavioral\nmetrics strongly predicted AI reliance, highlighting potential behavioral\nmechanisms to explain AI adoption. The study's findings underline critical\nimplications for ethical AI integration in education and the broader field. It\nemphasizes the need for enhanced onboarding processes to improve student's\nfamiliarity and effective use of AI tools. Furthermore, AI interfaces should be\ndesigned with reliance-calibration mechanisms to enhance appropriate reliance.\nUltimately, this research advances understanding of AI reliance dynamics,\nproviding foundational insights for ethically sound and cognitively enriching\nAI practices."}
{"id": "2508.20806", "categories": ["cs.IT", "cs.CE", "cs.SY", "eess.SY", "math.DS", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20806", "abs": "https://arxiv.org/abs/2508.20806", "authors": ["Moriba Jah", "Van Haslett"], "title": "The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation", "comment": null, "summary": "Traditional state estimation methods rely on probabilistic assumptions that\noften collapse epistemic uncertainty into scalar beliefs, risking\noverconfidence in sparse or adversarial sensing environments. We introduce the\nEpistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework\nfully grounded in possibility theory and epistemic humility. ESPF redefines the\nevolution of belief over state space using compatibility-weighted support\nupdates, surprisalaware pruning, and adaptive dispersion via sparse grid\nquadrature. Unlike conventional filters, ESPF does not seek a posterior\ndistribution, but rather maintains a structured region of plausibility or\nnon-rejection, updated using ordinal logic rather than integration. For\nmulti-model inference, we employ the Choquet integral to fuse competing\nhypotheses based on a dynamic epistemic capacity function, generalizing\nclassical winner-take-all strategies. The result is an inference engine capable\nof dynamically contracting or expanding belief support in direct response to\ninformation structure, without requiring prior statistical calibration. This\nwork presents a foundational shift in how inference, evidence, and ignorance\nare reconciled, supporting robust estimation where priors are unavailable,\nmisleading, or epistemically unjustified."}
{"id": "2508.20262", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.20262", "abs": "https://arxiv.org/abs/2508.20262", "authors": ["Thomas Davidson"], "title": "AI reasoning effort mirrors human decision time on content moderation tasks", "comment": null, "summary": "Large language models can now generate intermediate reasoning steps before\nproducing answers, improving performance on difficult problems. This study uses\na paired conjoint experiment on a content moderation task to examine parallels\nbetween human decision times and model reasoning effort. Across three frontier\nmodels, reasoning effort consistently predicts human decision time. Both humans\nand models expended greater effort when important variables were held constant,\nsuggesting similar sensitivity to task difficulty and patterns consistent with\ndual-process theories of cognition. These findings show that AI reasoning\neffort mirrors human processing time in subjective judgments and underscores\nthe potential of reasoning traces for interpretability and decision-making."}
{"id": "2508.20940", "categories": ["cs.IT", "math.CO", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20940", "abs": "https://arxiv.org/abs/2508.20940", "authors": ["Giuseppe Del Prete", "Antonio Roccolano", "Ferdinando Zullo"], "title": "On the non-existence of perfect codes in the sum-rank metric", "comment": null, "summary": "We study perfect codes in the sum-rank metric, a generalization of both the\nHamming and rank metrics relevant in multishot network coding and space-time\ncoding. A perfect code attains equality in the sphere-packing bound,\ncorresponding to a partition of the ambient space into disjoint metric balls.\nWhile perfect codes in the Hamming and rank metrics are completely classified,\nthe existence of nontrivial perfect codes in the sum-rank metric remains\nlargely open. In this paper, we investigate linear perfect codes in the\nsum-rank metric. We analyze the geometry of balls and derive bounds on their\nvolumes, showing how the sphere-packing bound applies. For two-block spaces, we\ndetermine explicit parameter constraints for the existence of perfect codes.\nFor multiple-block spaces, we establish non-existence results for various\nranges of minimum distance, divisibility conditions, and code dimensions. We\nfurther provide computational evidence based on congruence conditions imposed\nby the volume of metric balls."}
{"id": "2508.20368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20368", "abs": "https://arxiv.org/abs/2508.20368", "authors": ["Lang Mei", "Zhihan Yang", "Chong Chen"], "title": "AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning", "comment": null, "summary": "Recent studies have explored integrating Large Language Models (LLMs) with\nsearch engines to leverage both the LLMs' internal pre-trained knowledge and\nexternal information. Specially, reinforcement learning (RL) has emerged as a\npromising paradigm for enhancing LLM reasoning through multi-turn interactions\nwith search engines. However, existing RL-based search agents rely on a single\nLLM to handle both search planning and question-answering (QA) tasks in an\nend-to-end manner, which limits their ability to optimize both capabilities\nsimultaneously. In practice, sophisticated AI search systems often employ a\nlarge, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a\nmore effective and efficient approach is to utilize a small, trainable LLM\ndedicated to search planning. In this paper, we propose\n\\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to\nenhance the performance of frozen QA models by focusing on search planning.\nSpecifically, our approach introduces three key innovations: 1) Decoupling the\nArchitecture of the Search Planner and Generator, 2) Dual-Reward Alignment for\nSearch Planning, and 3) Pareto Optimization of Planning Utility and Cost, to\nachieve the objectives. Extensive experiments on real-world datasets\ndemonstrate that AI SearchPlanner outperforms existing RL-based search agents\nin both effectiveness and efficiency, while exhibiting strong generalization\ncapabilities across diverse frozen QA models and data domains."}
{"id": "2508.20980", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20980", "abs": "https://arxiv.org/abs/2508.20980", "authors": ["Siyao Li", "Mingzhe Chen", "Shuangyang Li", "Giuseppe Caire"], "title": "On Secrecy Capacity of Binary Beampointing Channels with Block Memory and Feedback", "comment": "7 pages, 2 figures, accepted by 61st Allerton Conference on\n  Communication, Control, and Computing", "summary": "This paper investigates the secrecy capacity of the binary beampointing (BBP)\nchannel with block memory and feedback, a simplified yet insightful model for\nmillimeter-wave (mmWave) systems with beamformed transmissions and backscatter\nfeedback. We consider a system where a legitimate receiver and a passive\neavesdropper experience independent and uniformly distributed angular\ndirections over transmission blocks, with the base station receiving noiseless,\nunit-delayed feedback from both, under the per-symbol input cost constraints.\nWe establish a closed-form upper bound on the secrecy capacity, which is based\non the main channel between the base station and the legitimate receiver.\nMoreover, we propose a joint communication and adaptive sensing (JCAS) scheme\nand derive its achievable secrecy rate. Simulation results show that the gap\nbetween the inner and outer bounds narrows as the number of block length\nincreases. This reveals the efficiency of this JCAS scheme, which strategically\nleverages feedback to balance the demands of sensing the legitimate user and\npreventing information leakage to the eavesdropper."}
{"id": "2508.20371", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.20371", "abs": "https://arxiv.org/abs/2508.20371", "authors": ["Sopam Dasgupta", "Sadaf MD Halim", "Joaqu√≠n Arias", "Elmer Salazar", "Gopal Gupta"], "title": "P2C: Path to Counterfactuals", "comment": null, "summary": "Machine-learning models are increasingly driving decisions in high-stakes\nsettings, such as finance, law, and hiring, thus, highlighting the need for\ntransparency. However, the key challenge is to balance transparency --\nclarifying `why' a decision was made -- with recourse: providing actionable\nsteps on `how' to achieve a favourable outcome from an unfavourable outcome.\nCounterfactual explanations reveal `why' an undesired outcome occurred and\n`how' to reverse it through targeted feature changes (interventions).\n  Current counterfactual approaches have limitations: 1) they often ignore\ncausal dependencies between features, and 2) they typically assume all\ninterventions can happen simultaneously, an unrealistic assumption in practical\nscenarios where actions are typically taken in a sequence. As a result, these\ncounterfactuals are often not achievable in the real world.\n  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that\nproduces a plan (ordered sequence of actions) converting an unfavourable\noutcome to a causally consistent favourable outcome. P2C addresses both\nlimitations by 1) Explicitly modelling causal relationships between features\nand 2) Ensuring that each intermediate state in the plan is feasible and\ncausally valid. P2C uses the goal-directed Answer Set Programming system\ns(CASP) to generate the plan accounting for feature changes that happen\nautomatically due to causal dependencies. Furthermore, P2C refines cost\n(effort) computation by only counting changes actively made by the user,\nresulting in realistic cost estimates. Finally, P2C highlights how its causal\nplanner outperforms standard planners, which lack causal knowledge and thus can\ngenerate illegal actions."}
{"id": "2508.20997", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.20997", "abs": "https://arxiv.org/abs/2508.20997", "authors": ["Siyao Li", "Shuangyang Li", "Giuseppe Caire"], "title": "On the Sensing Capacity of Gaussian \"Beam-Pointing\" Channels with Block Memory and Feedback", "comment": "6 pages, accepted by 2025 IEEE International Symposium on Information\n  Theory (ISIT 2025)", "summary": "Driven by the demands of high-frequency wireless communications in 5G and 6G\nsystems (e.g., mmWave, sub-THz), we explore a state-dependent {\\em Gaussian\nbeam-pointing} (GBP) channel. In this model, the channel state defines an\nunknown angle of departure (AoD), which remains constant within each coherence\nblock of $Q$ time slots but changes independently across blocks. The\ntransmitter receives strictly causal feedback which may originate from a radar\ndetection system or explicit feedback from the receiver at the end of each slot\nand estimates the AoD at the end of each block. To enhance transmission\nefficiency, we propose a joint communication and sensing scheme. While the\ncommunication capacity of the GBP channel has been previously analyzed by the\nauthors, this work focuses on sensing capacity, characterized by the mutual\ninformation between the channel state and the feedback conditioned on the\ntransmitted signal. We derive an upper bound using dynamic programming and\npropose an achievable inner bound on the sensing capacity, both formulated as\noptimization problems. For the special case of $Q=1$, the proposed transmission\nscheme achieves the optimal sensing rate and highlights the inherent trade-off\nbetween sensing and communication performance."}
{"id": "2508.20374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20374", "abs": "https://arxiv.org/abs/2508.20374", "authors": ["Simin Ma", "Shujian Liu", "Jun Tan", "Yebowen Hu", "Song Wang", "Sathish Reddy Indurthi", "Sanqiang Zhao", "Liwei Wu", "Jianbing Han", "Kaiqiang Song"], "title": "TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning", "comment": null, "summary": "Diverse instruction data is vital for effective instruction tuning of large\nlanguage models, as it enables the model to generalize across different types\nof inputs . Building such diversified instruction dataset is an essential step\nin this process. Existing approaches often leverage large language models to\nautomatically explore and generate diverse instructions, ensuring both data\ndiversity and quality. However, they tend to overlook an important factor in\nreal-world applications: on-task relevance. In practice, only a few real-world\napplications require a truly general-purpose model; most benefit from\ntask-specific knowledge tailored to their particular use case. Therefore, it is\nvital to develop instruction augmentation methods that not only maintain\ndiversity but are also optimized for specific, real-world scenarios.\n  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework\nthat systematically expands instructions while preserving both diversity and\ntask alignment. By representing instructions in a discrete query-constraints\nspace, TCIA creates a rich set of task-relevant instructions and enables models\nto generalize to these task-specific instructions without sacrificing overall\nperformance. Experiments show that TCIA improves open-source LLMs' performance\nby an average of 8.7% across four real-world, task-specific applications, and\nin some cases outperforming leading closed-source models. These improvements do\nnot compromise general instruction-following ability, making TCIA a scalable\nand efficient solution for adapting LLMs to real-world, task-focused\napplications."}
{"id": "2508.20384", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20384", "abs": "https://arxiv.org/abs/2508.20384", "authors": ["Yongfu Zhu", "Lin Sun", "Guangxiang Zhao", "Weihong Lin", "Xiangzheng Zhang"], "title": "Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM", "comment": "Under review for AAAI 2026", "summary": "In this work, we introduce Entropy Area Score (EAS), a simple yet effective\nmetric to quantify uncertainty in the answer generation process of reasoning\nlarge language models (LLMs). EAS requires neither external models nor repeated\nsampling, it integrates token-level predictive entropy from the model itself to\ncapture the evolution of uncertainty during generation. Empirical results show\nthat EAS is strongly correlated with answer entropy across models and datasets.\nIn training data selection, EAS identifies high-potential samples and\nconsistently outperforms Pass Rate filtering under equal sample budgets,\nimproving student model accuracy on math benchmarks. EAS is both efficient and\ninterpretable, offering a practical tool for uncertainty modeling and data\nquality assessment in LLM training."}
{"id": "2508.20404", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20404", "abs": "https://arxiv.org/abs/2508.20404", "authors": ["Chengyue Yu", "Siyuan Lu", "Chenyi Zhuang", "Dong Wang", "Qintong Wu", "Zongyue Li", "Runsheng Gan", "Chunfeng Wang", "Siqi Hou", "Gaochi Huang", "Wenlong Yan", "Lifeng Hong", "Aohui Xue", "Yanfeng Wang", "Jinjie Gu", "David Tsai", "Tao Lin"], "title": "AWorld: Orchestrating the Training Recipe for Agentic AI", "comment": null, "summary": "The learning from practice paradigm is crucial for developing capable Agentic\nAI systems, yet it is severely hampered by inefficient experience generation, a\nbottleneck especially pronounced in complex benchmarks like GAIA. To address\nthis, we introduce AWorld, an open-source system engineered for large-scale\nagent-environment interaction. By distributing tasks across a cluster, AWorld\naccelerates experience collection by 14.6x compared to standard single-node,\nsequential execution. This critical speedup makes extensive reinforcement\nlearning practical and scalable. Leveraging this capability, we trained a\nQwen3-32B-based agent that significantly outperforms its base model, increasing\nits overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most\nchallenging levels, our agent achieves a score of 16.33%, surpassing the\nperformance of leading proprietary models. Our open-source system and resulting\nagent provide a practical blueprint for a complete agentic AI training\npipeline, from efficient interaction to demonstrable model improvement."}
{"id": "2508.20411", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.20411", "abs": "https://arxiv.org/abs/2508.20411", "authors": ["Donglin Wang", "Weiyun Liang", "Chunyuan Chen", "Jing Xu", "Yulong Fu"], "title": "Governable AI: Provable Safety Under Extreme Threat Models", "comment": null, "summary": "As AI rapidly advances, the security risks posed by AI are becoming\nincreasingly severe, especially in critical scenarios, including those posing\nexistential risks. If AI becomes uncontrollable, manipulated, or actively\nevades safety mechanisms, it could trigger systemic disasters. Existing AI\nsafety approaches-such as model enhancement, value alignment, and human\nintervention-suffer from fundamental, in-principle limitations when facing AI\nwith extreme motivations and unlimited intelligence, and cannot guarantee\nsecurity. To address this challenge, we propose a Governable AI (GAI) framework\nthat shifts from traditional internal constraints to externally enforced\nstructural compliance based on cryptographic mechanisms that are\ncomputationally infeasible to break, even for future AI, under the defined\nthreat model and well-established cryptographic assumptions.The GAI framework\nis composed of a simple yet reliable, fully deterministic, powerful, flexible,\nand general-purpose rule enforcement module (REM); governance rules; and a\ngovernable secure super-platform (GSSP) that offers end-to-end protection\nagainst compromise or subversion by AI. The decoupling of the governance rules\nand the technical platform further enables a feasible and generalizable\ntechnical pathway for the safety governance of AI. REM enforces the bottom line\ndefined by governance rules, while GSSP ensures non-bypassability,\ntamper-resistance, and unforgeability to eliminate all identified attack\nvectors. This paper also presents a rigorous formal proof of the security\nproperties of this mechanism and demonstrates its effectiveness through a\nprototype implementation evaluated in representative high-stakes scenarios."}
{"id": "2508.20525", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20525", "abs": "https://arxiv.org/abs/2508.20525", "authors": ["Jingze Zhang", "Jiahe Qian", "Yiliang Zhou", "Yifan Peng"], "title": "Enhancing Health Fact-Checking with LLM-Generated Synthetic Data", "comment": null, "summary": "Fact-checking for health-related content is challenging due to the limited\navailability of annotated training data. In this study, we propose a synthetic\ndata generation pipeline that leverages large language models (LLMs) to augment\ntraining data for health-related fact checking. In this pipeline, we summarize\nsource documents, decompose the summaries into atomic facts, and use an LLM to\nconstruct sentence-fact entailment tables. From the entailment relations in the\ntable, we further generate synthetic text-claim pairs with binary veracity\nlabels. These synthetic data are then combined with the original data to\nfine-tune a BERT-based fact-checking model. Evaluation on two public datasets,\nPubHealth and SciFact, shows that our pipeline improved F1 scores by up to\n0.019 and 0.049, respectively, compared to models trained only on the original\ndata. These results highlight the effectiveness of LLM-driven synthetic data\naugmentation in enhancing the performance of health-related fact-checkers."}
{"id": "2508.20578", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20578", "abs": "https://arxiv.org/abs/2508.20578", "authors": ["Jaeman Son", "Hyunsoo Kim"], "title": "Human-AI Collaborative Bot Detection in MMORPGs", "comment": null, "summary": "In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling\nbots exploit automated programs to level up characters at scale, undermining\ngameplay balance and fairness. Detecting such bots is challenging, not only\nbecause they mimic human behavior, but also because punitive actions require\nexplainable justification to avoid legal and user experience issues. In this\npaper, we present a novel framework for detecting auto-leveling bots by\nleveraging contrastive representation learning and clustering techniques in a\nfully unsupervised manner to identify groups of characters with similar\nlevel-up patterns. To ensure reliable decisions, we incorporate a Large\nLanguage Model (LLM) as an auxiliary reviewer to validate the clustered groups,\neffectively mimicking a secondary human judgment. We also introduce a growth\ncurve-based visualization to assist both the LLM and human moderators in\nassessing leveling behavior. This collaborative approach improves the\nefficiency of bot detection workflows while maintaining explainability, thereby\nsupporting scalable and accountable bot regulation in MMORPGs."}
{"id": "2508.20674", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.20674", "abs": "https://arxiv.org/abs/2508.20674", "authors": ["Rui Mao", "Qian Liu", "Xiao Li", "Erik Cambria", "Amir Hussain"], "title": "Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science", "comment": null, "summary": "Cognitive Science has profoundly shaped disciplines such as Artificial\nIntelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and\nCulture. Many breakthroughs in AI trace their roots to cognitive theories,\nwhile AI itself has become an indispensable tool for advancing cognitive\nresearch. This reciprocal relationship motivates a comprehensive review of the\nintersections between AI and Cognitive Science. By synthesizing key\ncontributions from both perspectives, we observe that AI progress has largely\nemphasized practical task performance, whereas its cognitive foundations remain\nconceptually fragmented. We argue that the future of AI within Cognitive\nScience lies not only in improving performance but also in constructing systems\nthat deepen our understanding of the human mind. Promising directions include\naligning AI behaviors with cognitive frameworks, situating AI in embodiment and\nculture, developing personalized cognitive models, and rethinking AI ethics\nthrough cognitive co-evaluation."}
{"id": "2508.20701", "categories": ["cs.AI", "cs.CL", "math.CT"], "pdf": "https://arxiv.org/pdf/2508.20701", "abs": "https://arxiv.org/abs/2508.20701", "authors": ["Ares Fabregat-Hern√°ndez", "Javier Palanca", "Vicent Botti"], "title": "Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings", "comment": null, "summary": "The paper introduces a novel framework based on category theory to enhance\nthe explainability of artificial intelligence systems, particularly focusing on\nword embeddings. Key topics include the construction of categories\n$\\mathcal{L}_T$ and $\\mathcal{P}_T$, providing schematic representations of the\nsemantics of a text $ T $, and reframing the selection of the element with\nmaximum probability as a categorical notion. Additionally, the monoidal\ncategory $\\mathcal{P}_T$ is constructed to visualize various methods of\nextracting semantic information from $T$, offering a dimension-agnostic\ndefinition of semantic spaces reliant solely on information within the text.\n  Furthermore, the paper defines the categories of configurations Conf and word\nembeddings $\\mathcal{Emb}$, accompanied by the concept of divergence as a\ndecoration on $\\mathcal{Emb}$. It establishes a mathematically precise method\nfor comparing word embeddings, demonstrating the equivalence between the GloVe\nand Word2Vec algorithms and the metric MDS algorithm, transitioning from neural\nnetwork algorithms (black box) to a transparent framework. Finally, the paper\npresents a mathematical approach to computing biases before embedding and\noffers insights on mitigating biases at the semantic space level, advancing the\nfield of explainable artificial intelligence."}
{"id": "2508.20729", "categories": ["cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.20729", "abs": "https://arxiv.org/abs/2508.20729", "authors": ["Ao Cheng", "Lei Zhang", "Guowei He"], "title": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision", "comment": null, "summary": "Large language models (LLMs) serve as an active and promising field of\ngenerative artificial intelligence and have demonstrated abilities to perform\ncomplex tasks in multiple domains, including mathematical and scientific\nreasoning. In this work, we construct a novel agent framework for solving\nrepresentative problems in scientific computing. The proposed agent,\nincorporating a \"rewriting-resolution-review-revision\" logical chain via three\nreasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,\nrespectively), is integrated in a collaborative and interactive manner. The\nConsultant module endows the agent with knowledge transfer capabilities to link\nproblems to professional domain insights, thereby rewriting problem\ndescriptions through text augmentation. The Programmer module is responsible\nfor generating and executing well-structured code to deliver the problem\nresolution. The Reviewer module equips the agent with the capacity for\nself-debugging and self-refinement through interactive feedback with code\nruntime outputs. By leveraging the end-to-end review mechanism, the executable\ncode provided by the Programmer attains the iterative revision. A comprehensive\nevaluation is conducted on the performance of the proposed agent framework in\nsolving PDEs, ill-conditioned linear systems, and data-driven physical analysis\nproblems. Compared to single-model, this collaborative framework significantly\nimproves the bug-free code generation rate and reduces the occurrence of\nnon-physical solutions, thereby establishing a highly reliable framework for\nautonomous code generation based on natural language descriptions. The review\nmechanism improved the average execution success (bug-free code and non-NaN\nsolutions) rate of the latest reasoning models. In summary, our agent framework\nestablishes automatic code generation and review as a promising scientific\ncomputing paradigm."}
{"id": "2508.20784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20784", "abs": "https://arxiv.org/abs/2508.20784", "authors": ["Yifan Zhang"], "title": "Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control", "comment": null, "summary": "Bus bunching remains a challenge for urban transit due to stochastic traffic\nand passenger demand. Traditional solutions rely on multi-agent reinforcement\nlearning (MARL) in loop-line settings, which overlook realistic operations\ncharacterized by heterogeneous routes, timetables, fluctuating demand, and\nvarying fleet sizes. We propose a novel single-agent reinforcement learning\n(RL) framework for bus holding control that avoids the data imbalance and\nconvergence issues of MARL under near-realistic simulation. A bidirectional\ntimetabled network with dynamic passenger demand is constructed. The key\ninnovation is reformulating the multi-agent problem into a single-agent one by\naugmenting the state space with categorical identifiers (vehicle ID, station\nID, time period) in addition to numerical features (headway, occupancy,\nvelocity). This high-dimensional encoding enables single-agent policies to\ncapture inter-agent dependencies, analogous to projecting non-separable inputs\ninto a higher-dimensional space. We further design a structured reward function\naligned with operational goals: instead of exponential penalties on headway\ndeviations, a ridge-shaped reward balances uniform headways and schedule\nadherence. Experiments show that our modified soft actor-critic (SAC) achieves\nmore stable and superior performance than benchmarks, including MADDPG (e.g.,\n-430k vs. -530k under stochastic conditions). These results demonstrate that\nsingle-agent deep RL, when enhanced with categorical structuring and\nschedule-aware rewards, can effectively manage bus holding in non-loop,\nreal-world contexts. This paradigm offers a robust, scalable alternative to\nMARL frameworks, particularly where agent-specific experiences are imbalanced."}
{"id": "2508.20810", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20810", "abs": "https://arxiv.org/abs/2508.20810", "authors": ["Jessica Lundin", "Guillaume Chabot-Couture"], "title": "A Graph-Based Test-Harness for LLM Evaluation", "comment": "4 pages, 2 figures, dataset", "summary": "We present a first known prototype of a dynamic, systematic benchmark of\nmedical guidelines for 400+ questions, with 3.3+ trillion possible\ncombinations, covering 100\\% of guideline relationships. We transformed the WHO\nIMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,\ntreatments, follow-ups, severities) and 300+ edges, then used graph traversal\nto generate questions that incorporated age-specific scenarios and contextual\ndistractors to ensure clinical relevance. Our graph-based approach enables\nsystematic evaluation across clinical tasks (45-67\\% accuracy), and we find\nmodels excel at symptom recognition but struggle with triaging severity,\ntreatment protocols and follow-up care, demonstrating how customized benchmarks\ncan identify specific capability gaps that general-domain evaluations miss.\nBeyond evaluation, this dynamic MCQA methodology enhances LLM post-training\n(supervised finetuning, GRPO, DPO), where correct answers provide high-reward\nsamples without expensive human annotation. The graph-based approach\nsuccessfully addresses the coverage limitations of manually curated benchmarks.\nThis methodology is a step toward scalable, contamination-resistant solution\nfor creating comprehensive benchmarks that can be dynamically generated,\nincluding when the guidelines are updated. Code and datasets are available at\nhttps://github.com/jessicalundin/graph_testing_harness"}
{"id": "2508.20953", "categories": ["cs.AI", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.20953", "abs": "https://arxiv.org/abs/2508.20953", "authors": ["Vipul Patel", "Anirudh Deodhar", "Dagnachew Birru"], "title": "A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling", "comment": "8 pages, 7 figures, Accepted at the Multi-Objective Decision Making\n  Workshop (MODeM2025) at ECAI 2025", "summary": "Workforce scheduling in the healthcare sector is a significant operational\nchallenge, characterized by fluctuating patient loads, diverse clinical skills,\nand the critical need to control labor costs while upholding high standards of\npatient care. This problem is inherently multi-objective, demanding a delicate\nbalance between competing goals: minimizing payroll, ensuring adequate staffing\nfor patient needs, and accommodating staff preferences to mitigate burnout. We\npropose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital\nunit workforce scheduling problem as a multi-objective optimization task. Our\nmodel incorporates real-world complexities, including hourly appointment-driven\ndemand and the use of modular shifts for a multi-skilled workforce. By defining\nobjective functions for cost, patient care coverage, and staff satisfaction,\nthe GA navigates the vast search space to identify a set of high-quality,\nnon-dominated solutions. Demonstrated on datasets representing a typical\nhospital unit, the results show that our MOO-GA generates robust and balanced\nschedules. On average, the schedules produced by our algorithm showed a 66\\%\nperformance improvement over a baseline that simulates a conventional, manual\nscheduling process. This approach effectively manages trade-offs between\ncritical operational and staff-centric objectives, providing a practical\ndecision support tool for nurse managers and hospital administrators."}
{"id": "2508.20978", "categories": ["cs.AI", "cs.LO", "cs.SC"], "pdf": "https://arxiv.org/pdf/2508.20978", "abs": "https://arxiv.org/abs/2508.20978", "authors": ["Marianne Defresne", "Romain Gambardella", "Sophie Barbe", "Thomas Schiex"], "title": "Efficient Neuro-Symbolic Learning of Constraints and Objective", "comment": null, "summary": "In the ongoing quest for hybridizing discrete reasoning with neural nets,\nthere is an increasing interest in neural architectures that can learn how to\nsolve discrete reasoning or optimization problems from natural inputs, a task\nthat Large Language Models seem to struggle with.\n  Objectives: We introduce a differentiable neuro-symbolic architecture and a\nloss function dedicated to learning how to solve NP-hard reasoning problems.\n  Methods: Our new probabilistic loss allows for learning both the constraints\nand the objective, thus delivering a complete model that can be scrutinized and\ncompleted with side constraints. By pushing the combinatorial solver out of the\ntraining loop, our architecture also offers scalable training while exact\ninference gives access to maximum accuracy.\n  Results: We empirically show that it can efficiently learn how to solve\nNP-hard reasoning problems from natural inputs. On three variants of the Sudoku\nbenchmark -- symbolic, visual, and many-solution --, our approach requires a\nfraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut\ntask, it optimizes the regret better than a Decision-Focused-Learning\nregret-dedicated loss. Finally, it efficiently learns the energy optimization\nformulation of the large real-world problem of designing proteins."}
{"id": "2508.20996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20996", "abs": "https://arxiv.org/abs/2508.20996", "authors": ["Junda Wang", "Zonghai Yao", "Zhichao Yang", "Lingxi Li", "Junhui Qian", "Hong Yu"], "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery", "comment": null, "summary": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet\nfew receive effective care due to stigma, motivational barriers, and limited\npersonalized support. Although large language models (LLMs) show promise for\nmental-health assistance, most systems lack tight integration with clinically\nvalidated strategies, reducing effectiveness in addiction recovery. We present\nChatThero, a multi-agent conversational framework that couples dynamic patient\nmodeling with context-sensitive therapeutic dialogue and adaptive persuasive\nstrategies grounded in cognitive behavioral therapy (CBT) and motivational\ninterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,\nMedium, and Hard resistance levels, and train ChatThero with a two-stage\npipeline comprising supervised fine-tuning (SFT) followed by direct preference\noptimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in\npatient motivation, a 0.49\\% increase in treatment confidence, and resolves\nhard cases with 26\\% fewer turns than GPT-4o, and both automated and human\nclinical assessments rate it higher in empathy, responsiveness, and behavioral\nrealism. The framework supports rigorous, privacy-preserving study of\ntherapeutic conversation and provides a robust, replicable basis for research\nand clinical translation."}
