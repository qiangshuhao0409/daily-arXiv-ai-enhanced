<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 11]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [On Borrowed Time: Measurement-Informed Understanding of the NTP Pool's Robustness to Monopoly Attacks](https://arxiv.org/abs/2602.12321)
*Robert Beverly,Erik Rye*

Main category: cs.NI

TL;DR: 首次对NTP时间池进行全面直接测量，发现其存在严重的安全脆弱性，攻击者仅需少量恶意服务器即可控制全球90%国家的NTP流量


<details>
  <summary>Details</summary>
Motivation: NTP协议是互联网时间同步的核心机制，而NTP Pool作为分布式志愿者服务器网络，因其广泛使用（包括IoT和基础设施设备）且管理分散，成为极具吸引力的攻击目标。然而，目前缺乏对其实际状况的直接、全面的测量数据。

Method: 开发测量方法收集NTP Pool的直接、非推断性综合数据，包括：纵向服务器和账户成员关系、服务器配置、时间质量、别名和全球查询流量负载。在9个月期间收集完整且细粒度的数据，覆盖超过1.5万台服务器（包括活跃和非活跃）。

Result: 发现NTP Pool中仅有19.7%的活跃服务器是完全独立的；通过分析地址别名、账户和网络连接性，揭示了其集中化风险；攻击者利用这些数据可以更精确地实施"垄断攻击"，仅需10个或更少的恶意NTP服务器即可控制90%国家的NTP流量。

Conclusion: NTP Pool存在严重的安全脆弱性，测量数据揭示了多个改进其鲁棒性的途径，需要采取措施增强其抗攻击能力。

Abstract: Internet services and applications depend critically on the availability and acc uracy of network time. The Network Time Protocol (NTP) is one of the oldest core network protocols and remains the de facto mechanism for clock synchronization across the Internet today. While multiple NTP infrastructures exist, one, the "NTP Pool," presents an attractive attack target for two basic reasons, it is: 1) administratively distributed and based on volunteer servers; and 2) heavily utilized, including by IoT and infrastructure devices worldwide. We %develop measurements to gather the first direct, non-inferential, and comprehensive data on the NTP pool, including: longitudinal server and account membership, server configurations, time quality, aliases, and global query traffic load.
  We gather complete and granular data over a nine month period to discover over 15k servers (both active and inactive) and shed new light into the NTP Pool's use, dynamics, and robustness. By analyzing address aliases, accounts, and network connectivity, we find that only 19.7% of the pool's active servers are fully independent. Finally, we show that an adversary informed with our data can better and more precisely mount "monopoly attacks" to capture the preponderance of NTP pool traffic in 90% of all countries with only 10 or fewer malicious NTP servers. Our results suggest multiple avenues by which the robustness of the pool can be improved.

</details>


### [2] [Tracking The Trackers: Commercial Surveillance Occurring on U.S. Army Networks](https://arxiv.org/abs/2602.12388)
*Alexander Master,Jaclyn Fox,Nicolas Starck,Maxwell Love,Benjamin Allison*

Main category: cs.NI

TL;DR: 美国陆军网络中发现超过21%的访问域名是网络追踪器，建议通过配置变更和政策调整来限制商业数据收集风险


<details>
  <summary>Details</summary>
Motivation: 尽管有现有安全措施，国防部网络上的互联网活动仍易受网络追踪器和商业数据收集的影响，可能暴露军人信息和部队行动信息

Method: 从云隔离平台获取两个月数据，分析1000个最常访问的互联网资源，使用Ghostery的WhoTracks.me数据库识别商业追踪实体

Result: 研究发现超过21%的访问域名是互联网追踪器，表明商业数据收集在国防部网络中广泛存在

Conclusion: 建议陆军实施网络配置变更和政策调整来限制商业追踪，云隔离平台可通过较小配置变更更有效缓解商业信息风险

Abstract: Despite current security implementations, Internet activity on DoD networks is susceptible to web trackers and commercial data collection, which have the potential to expose information about service members and unit operations. This report documents the outcomes of a study to characterize web tracking occurring on Army CONUS unclassified networks. We derived a dataset from the Cloud-Based Internet Isolation (CBII) platform, encompassing data measured over a two-month period in 2024. This dataset comprised the 1,000 most frequently accessed Internet resources, determined by the number of connection requests on CONUS DoDIN-A during the study period. We then compared all domains and subdomains in the dataset against Ghostery's WhoTracks.me, an open-source database of commercial tracking entities. We found that over 21% of the domains accessed during the study period were Internet trackers. The ACI recommends that the Army implement changes to its enterprise networks to limit commercial Internet-based tracking, as well as policy changes towards the same end. With relatively minor configuration changes, CBII can serve as a more effective mitigation against risks posed by commercially available information.

</details>


### [3] [Generalizing UxV Network Control Optimization with Disruption Tolerant Networking](https://arxiv.org/abs/2602.12448)
*Quyen Dang,Geoffrey Xie*

Main category: cs.NI

TL;DR: 提出一种基于容断网络(DTN)的无人机网络控制系统，通过允许节点临时断开连接来提高拓扑灵活性，从而提升任务执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有网络控制系统(NCS)要求始终保持连接拓扑，限制了无人机在军事和救灾任务中的效用。需要更灵活的网络模型来支持任务优化。

Method: 设计DTN兼容的通信效用模型，允许节点临时断开，同时精细指定节点对之间的最小通信频率和最大跳数。将该模型集成到现有NCS中。

Result: 在5架无人机搜索敌舰的模拟场景中，新模型不仅让NCS更快找到目标，还支持将无人机分成多组负责不同搜索区域的新能力。

Conclusion: 基于DTN的通信模型显著提高了NCS的拓扑灵活性，增强了无人机在复杂任务中的协调能力和任务执行效率。

Abstract: Military and disaster relief operations increasingly rely on unmanned vehicles (UxVs). It is important to develop a network control system (NCS) that can continuously coordinate and optimize the movement of UxVs based on mission objectives. However, prior research on NCS aims to always maintain a connected network topology, which limits the utility of the resulting systems. In this paper, we present an approach to systematically increase the topology flexibility for an NCS by leveraging the well-studied concept of disruption-tolerant networking (DTN). We design a DTN-compatible communication utility model that, while allowing some nodes to temporarily disconnect from others, provides for a fine-grain specification of the minimum communication frequency and the maximum hops permitted for message delivery between each pair of nodes. As such, the model supports what-if analyses before a mission to determine the best communication parameters to use for a given set of UxVs. Furthermore, we incorporate our communication model into an existing NCS and evaluate its performance in a simulated scenario involving the use of five UxVs searching for an enemy ship. The results show that our model not only enables the NCS to find the enemy ship faster but also facilitates new capabilities, such as dividing the UxVs into multiple teams responsible for different search areas.

</details>


### [4] [Photonic Rails in ML Datacenters with Opus](https://arxiv.org/abs/2602.12521)
*Eric Ding,Barry Lyu,Bhaskar Kataria,Rachee Singh*

Main category: cs.NI

TL;DR: Opus：使用光路开关重构数据中心ML训练网络，通过并行驱动的时间复用降低功耗和成本


<details>
  <summary>Details</summary>
Motivation: 传统基于高基数电开关的rail网络在大型ML训练中功耗和成本过高，需要更高效的替代方案

Method: 提出并行驱动rail重构：利用光路开关，根据不同并行维度的非重叠通信阶段进行时间复用配置

Result: 在2048 GPU规模下，实现23倍网络功耗降低和4倍成本节省，训练开销小于6%

Conclusion: 光路开关rail网络是可行的ML训练网络替代方案，能显著降低功耗和成本

Abstract: Rail-optimized network fabrics have become the de facto datacenter scale-out fabric for large-scale ML training. However, the use of high-radix electrical switches to provide all-to-all connectivity in rails imposes massive power and cost. We propose a rethinking of the rail abstraction by retaining its communication semantics, but realizing it using optical circuit switches. The key challenge is that optical switches support one-to-one connectivity at a time, limiting the fan-out of traffic in ML workloads using hybrid parallelisms. We overcome this through \emph{parallelism-driven rail reconfiguration}, which exploits the non-overlapping communication phases of different parallelism dimensions. This time-multiplexes a single set of physical ports across circuit configurations tailored to each phase within a training iteration. We design and implement Opus, a control plane that orchestrates this in-job reconfiguration of photonic rails at parallelism phase boundaries, and evaluate it on a physical OCS testbed, the Perlmutter supercomputer, and in simulation at up to 2,048 GPUs. Our results show that photonic rails can achieve over $23\times$ network power reduction and $4\times$ cost savings while incurring less than $6\%$ training overhead at production-relevant OCS reconfiguration latencies.

</details>


### [5] [Adaptive Meta-Aggregation Federated Learning for Intrusion Detection in Heterogeneous Internet of Things](https://arxiv.org/abs/2602.12541)
*Saadat Izadi,Mahmood Ahmadi*

Main category: cs.NI

TL;DR: 提出AMAFed方法，通过元学习动态加权机制提升异构IoT网络入侵检测性能，在多个基准数据集上达到99%以上准确率。


<details>
  <summary>Details</summary>
Motivation: IoT设备快速增长带来显著安全漏洞，传统入侵检测系统难以应对异构IoT设备的挑战，需要新的解决方案。

Method: 提出自适应元聚合联邦学习(AMAFed)，使用元学习动态加权机制，根据数据质量和贡献度为本地模型分配自适应重要性权重，实现个性化协作学习。

Result: 在ToN-IoT、N-BaIoT和BoT-IoT三个基准数据集上评估，AMAFed分别达到99.8%、99.88%和98.12%的检测准确率，F1分数均超过98%，优于现有方法。

Conclusion: AMAFed方法能有效提升异构IoT网络入侵检测性能，通过自适应聚合机制解决了传统联邦学习在异构环境中的局限性。

Abstract: The rapid proliferation of the Internet of Things (IoT) has brought remarkable advancements to industries by enabling interconnected systems and intelligent automation. However, this exponential growth has also introduced significant security vulnerabilities, making IoT networks increasingly targets for sophisticated cyberattacks. The heterogeneity of IoT devices poses critical challenges for traditional intrusion detection systems. To address these challenges, this paper proposes an innovative method called Adaptive Meta-Aggregation Federated Learning (AMAFed), designed to enhance intrusion detection in heterogeneous IoT networks. By employing a dynamic weighting mechanism using meta-learning, AMAFed assigns adaptive importance to local models based on their data quality and contributions, enabling personalized yet collaborative learning across devices. The proposed method was evaluated on three benchmark IoT datasets: ToN-IoT, N-BaIoT, and BoT-IoT, representing diverse real-world scenarios. Experimental results demonstrate that AMAFed achieves detection accuracy up to 99.8% on ToN-IoT, with F1-scores exceeding 98% across all datasets. On the N-BaIoT dataset, it reaches 99.88% accuracy, and on BoT-IoT, it achieves 98.12% accuracy, consistently outperforming state-of-the-art approaches.

</details>


### [6] [Lightweight Cluster-Based Federated Learning for Intrusion Detection in Heterogeneous IoT Networks](https://arxiv.org/abs/2602.12543)
*Saadat Izadi,Mahmood Ahmadi*

Main category: cs.NI

TL;DR: 提出基于聚类的轻量级联邦入侵检测框架，针对异构IoT设备，通过分层架构、MobileNet模型和混合损失函数，显著降低训练时间和测试延迟，同时保持高检测精度。


<details>
  <summary>Details</summary>
Motivation: 异构物联网设备的安全漏洞日益严重，传统入侵检测系统面临隐私和性能挑战。联邦学习虽能保护隐私，但IoT设备的异构性和有限计算资源导致延迟增加和性能下降。

Method: 采用分层IoT架构（边缘、雾、云层），在雾层部署入侵检测客户端。使用轻量级MobileNet模型和Gumbel-SoftMax与SoftMax混合损失函数。关键创新是基于硬件相似性对IoT设备进行聚类，为每个集群定制模型训练和聚合策略。

Result: 在ToN-IoT和CICDDoS2019数据集上验证：相比传统FL方法，端到端训练时间减少2.47倍，测试延迟降低2.16倍，检测准确率分别达到99.22%和99.02%。

Conclusion: 提出的聚类联邦入侵检测框架有效解决了IoT异构性和资源限制问题，显著提升了系统效率、可扩展性和整体性能，同时保持了高检测精度。

Abstract: The rise of heterogeneous Internet of Things (IoT) devices has raised security concerns due to their vulnerability to cyberattacks. Intrusion Detection Systems (IDS) are crucial in addressing these threats. Federated Learning (FL) offers a privacy-preserving solution, but IoT heterogeneity and limited computational resources cause increased latency and reduced performance. This paper introduces a novel approach Cluster-based federated intrusion detection with lightweight networks for heterogeneous IoT designed to address these limitations. The proposed framework utilizes a hierarchical IoT architecture that encompasses edge, fog, and cloud layers. Intrusion detection clients operate at the fog layer, leveraging federated learning to enhance data privacy and distributed processing efficiency. To enhance efficiency, the method employs the lightweight MobileNet model alongside a hybrid loss function that integrates Gumbel-SoftMax and SoftMax, optimizing resource consumption while maintaining high detection accuracy. A key feature of this approach is clustering IoT devices based on hardware similarities, enabling more efficient model training and aggregation tailored to each cluster's computational capacity. This strategy not only simplifies the complexity of managing heterogeneous data and devices but also improves scalability and overall system performance. To validate the effectiveness of the proposed method, extensive experiments were conducted using the ToN-IoT and CICDDoS2019 datasets. Results demonstrate that the proposed approach reduces end-to-end training time by 2.47x compared to traditional FL methods, achieves 2.16x lower testing latency, and maintains exceptionally high detection accuracy of 99.22% and 99.02% on the ToN-IoT and CICDDoS2019 datasets, respectively.

</details>


### [7] [CF-HFC:Calibrated Federated based Hardware-aware Fuzzy Clustering for Intrusion Detection in Heterogeneous IoTs](https://arxiv.org/abs/2602.12557)
*Saadat Izadi,Mahmood Ahmadi*

Main category: cs.NI

TL;DR: CF-HFC：一种用于异构物联网入侵检测的校准联邦学习方法，通过硬件感知模糊聚类、Fuzzy-FedProx聚合和自适应一致性校准解决设备与数据异质性挑战。


<details>
  <summary>Details</summary>
Motivation: 异构物联网环境中的资源受限设备面临多种网络攻击威胁，传统联邦学习在设备异质性和数据非独立同分布情况下存在延迟、收敛不稳定和错误率不平衡等问题。

Method: 提出三层边缘-雾-云架构，包含：1）硬件感知模糊聚类按计算能力组织客户端；2）Fuzzy-FedProx聚合稳定非独立同分布数据优化；3）自适应一致性校准动态调整决策阈值。

Result: 在多个数据集上实验显示，CF-HFC优于FedAvg和FedProx等基线方法，达到超过99%的检测准确率，收敛更快，通信延迟更低。

Conclusion: CF-HFC能有效缓解设备和数据层面的异质性，相比现有联邦学习方法，为异构物联网环境提供准确高效的入侵检测。

Abstract: The rapid expansion of heterogeneous Internet of Things (IoT) environments has heightened security risks, as resource-constrained devices remain vulnerable to diverse cyberattacks. Federated Learning (FL) has emerged as a privacy-preserving paradigm for collaborative intrusion detection; however, device and data heterogeneity introduce major challenges, including straggler delays, unstable convergence, and unbalanced error rates. This paper presents a Calibrated Federated Learning method with Hardware-aware Fuzzy Clustering (CF-HFC) to enhance intrusion detection performance in heterogeneous IoT networks. The proposed three-tier Edge-Fog-Cloud architecture integrates three complementary components: (1) hardware-aware fuzzy clustering, which organizes clients by computational capacity to mitigate straggler effects; (2) Fuzzy-FedProx aggregation, which stabilizes optimization under non-IID data distributions; and (3) Adaptive Conformal Calibration (ACC), which dynamically adjusts decision thresholds to balance false negative and false positive rates. Extensive experiments on ToN-IoT, BoT-IoT, Edge-IIoTset, and CICDDoS2019 datasets demonstrate that CF-HFC outperforms baseline methods such as FedAvg and FedProx, achieving over 99% detection accuracy, faster convergence, and lower communication latency. Overall, the results verify that CF-HFC effectively mitigates both device- and data-level heterogeneity, compared to existing federated learning approaches, providing accurate and efficient intrusion detection across Heterogeneous IoTs environment.

</details>


### [8] [Artic: AI-oriented Real-time Communication for MLLM Video Assistant](https://arxiv.org/abs/2602.12641)
*Jiangkai Wu,Zhiyuan Ren,Junquan Zhong,Liming Liu,Xinggong Zhang*

Main category: cs.NI

TL;DR: Artic是一个面向AI视频助手的实时通信框架，通过自适应码率控制和上下文感知流媒体，解决传统RTC框架与MLLM视频助手之间的不匹配问题，显著提升准确性和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前实时通信框架与AI视频助手之间存在根本性不匹配，因为传统RTC是为"人类观看视频"设计的，而AI视频助手需要"AI理解视频"。这种不匹配导致延迟激增和准确性下降，阻碍了人机交互的自然体验。

Method: 提出Artic框架，包含三个核心技术：1) 响应能力感知的自适应码率控制，利用MLLM准确性饱和特性主动限制码率；2) 零开销的上下文感知流媒体，将有限码率分配给对响应最重要的区域；3) 退化视频理解基准测试，评估RTC引起的视频退化对MLLM准确性的影响。

Result: 使用真实世界上行链路轨迹的原型实验表明，与现有方法相比，Artic显著提高了15.12%的准确性，并减少了135.31毫秒的延迟。

Conclusion: Artic成功解决了AI视频助手与现有RTC框架之间的不匹配问题，通过AI导向的设计实现了更好的准确性和延迟性能，为MLLM视频助手的实时通信提供了有效解决方案。

Abstract: AI Video Assistant emerges as a new paradigm for Real-time Communication (RTC), where one peer is a Multimodal Large Language Model (MLLM) deployed in the cloud. This makes interaction between humans and AI more intuitive, akin to chatting with a real person. However, a fundamental mismatch exists between current RTC frameworks and AI Video Assistants, stemming from the drastic shift in Quality of Experience (QoE) and more challenging networks. Measurements on our production prototype also confirm that current RTC fails, causing latency spikes and accuracy drops.
  To address these challenges, we propose Artic, an AI-oriented RTC framework for MLLM Video Assistants, exploring the shift from "humans watching video" to "AI understanding video." Specifically, Artic proposes: (1) Response Capability-aware Adaptive Bitrate, which utilizes MLLM accuracy saturation to proactively cap bitrate, reserving bandwidth headroom to absorb future fluctuations for latency reduction; (2) Zero-overhead Context-aware Streaming, which allocates limited bitrate to regions most important for the response, maintaining accuracy even under ultra-low bitrates; and (3) Degraded Video Understanding Benchmark, the first benchmark evaluating how RTC-induced video degradation affects MLLM accuracy. Prototype experiments using real-world uplink traces show that compared with existing methods, Artic significantly improves accuracy by 15.12% and reduces latency by 135.31 ms. We will release the benchmark and codes at https://github.com/pku-netvideo/DeViBench.

</details>


### [9] [PEMI: Transparent Performance Enhancements for QUIC](https://arxiv.org/abs/2602.12732)
*Jie Zhang,Lei Zhang,Ziyi Wang,Chenxiang Sun,Yuming Hu,Xiaohui Xie,Zeqi Lai,Yong Cui*

Main category: cs.NI

TL;DR: PEMI是一个透明中间件系统，能够在无需端点协作的情况下为QUIC连接测量RTT和推断丢包，实现快速重传，显著提升QUIC性能。


<details>
  <summary>Details</summary>
Motivation: QUIC作为下一代Web传输层，虽然提供了安全和性能改进，但其端到端加密使得传统TCP中的性能增强代理(PEP)无法工作。现有方案需要端点与中间件协作，部署难度大。

Method: 通过分析QUIC标准、实现和应用流量局部性，识别透明中间件测量RTT和推断丢包的机会。提出PEMI系统，采用基于延迟的拥塞控制和反馈机制来维护公平性。

Result: 在Mininet实验中，PEMI将文件传输的吞吐量提升高达2.5倍，并将RTC帧的90百分位抖动降低20-75%。

Conclusion: PEMI能够以完全透明的方式为QUIC提供性能增强，无需端点显式协作，解决了QUIC中间件辅助的部署难题。

Abstract: QUIC, as the transport layer of the next-generation Web stack (HTTP/3), natively provides security and performance improvements over TCP-based stacks. However, since QUIC provides end-to-end encryption for both data and packet headers, in-network assistance like Performance-Enhancing Proxy (PEP) is unavailable for QUIC. To achieve the similar optimization as TCP, some works seek to collaborate endpoints and middleboxes to provide in-network assistance for QUIC. But involving both host and in-network devices increases the difficulty of deployment in the Internet.
  In this paper, by analyzing the QUIC standard, implementations, and the locality of application traffic, we identify opportunities for transparent middleboxes to measure RTT and infer packet loss for QUIC connections, despite the absence of plaintext ACK information. We then propose PEMI as a concrete system that continuously measures RTT and infers lost packets, enabling fast retransmissions for QUIC. PEMI enables performance enhancement for QUIC in a completely transparent manner, without requiring any explicit cooperation from the endpoints. To keep fairness, PEMI employs a delay-based congestion control and utilizes feedback-based methods to enforce CWND. Extensive evaluation results, including Mininet and trace-driven dynamic experiments, show that PEMI can significantly improve the performance of QUIC. For example, in the Mininet experiments, PEMI increases the goodput of file transfers by up to 2.5$\times$, and reduces the 90th percentile jitter of RTC frames by 20-75%.

</details>


### [10] [Chimera: Neuro-Symbolic Attention Primitives for Trustworthy Dataplane Intelligence](https://arxiv.org/abs/2602.12851)
*Rong Fu,Wenxin Zhang,Xiaowen Ma,Kun Liu,Wangyu Wu,Ziyu Kong,Jia Yee Tan,Tailong Luo,Xianda Li,Zeli Su,Youjin Wang,Yongtai Liu,Simon Fong*

Main category: cs.NI

TL;DR: Chimera是一个将注意力神经网络与符号约束映射到可编程数据平面的框架，在严格硬件限制下实现可信赖的线速推理。


<details>
  <summary>Details</summary>
Motivation: 在可编程数据平面上直接部署表达性学习模型可以实现线速、低延迟的流量分析，但受到严格硬件约束和可预测、可审计行为需求的阻碍。

Method: 结合核化线性注意力近似、两层键选择层次结构和级联融合机制，强制执行硬符号保证同时保持神经表达性；包含硬件感知映射协议和双时间尺度更新方案。

Result: 神经符号注意力原语可以在商用可编程交换机的资源限制内实现高保真推理，支持稳定、线速操作。

Conclusion: Chimera框架成功地将注意力神经网络与符号约束映射到数据平面原语，在严格硬件限制下实现了可信赖的线速推理，为可编程数据平面上的智能流量分析提供了可行方案。

Abstract: Deploying expressive learning models directly on programmable dataplanes promises line-rate, low-latency traffic analysis but remains hindered by strict hardware constraints and the need for predictable, auditable behavior. Chimera introduces a principled framework that maps attention-oriented neural computations and symbolic constraints onto dataplane primitives, enabling trustworthy inference within the match-action pipeline. Chimera combines a kernelized, linearized attention approximation with a two-layer key-selection hierarchy and a cascade fusion mechanism that enforces hard symbolic guarantees while preserving neural expressivity. The design includes a hardware-aware mapping protocol and a two-timescale update scheme that together permit stable, line-rate operation under realistic dataplane budgets. The paper presents the Chimera architecture, a hardware mapping strategy, and empirical evidence showing that neuro-symbolic attention primitives can achieve high-fidelity inference within the resource envelope of commodity programmable switches.

</details>


### [11] [TENORAN: Automating Fine-grained Energy Efficiency Profiling in Open RAN Systems](https://arxiv.org/abs/2602.13085)
*Ravis Shirkhani,Stefano Maxenti,Leonardo Bonati,Niloofar Mohamadi,Maxime Elkael,Umair Hashmi,Jeebak Mitra,Michele Polese,Tommaso Melodia,Salvatore D'Oro*

Main category: cs.NI

TL;DR: TENORAN框架是一个自动化测量脚手架，用于对O-RAN部署进行细粒度能效分析，原型在异构OpenShift集群上实现，能够测量不同RAN组件在受控工作负载下的能耗。


<details>
  <summary>Details</summary>
Motivation: O-RAN架构向解耦和互操作转型，以及RIC的引入创造了新的资源优化机会，但需要细粒度和准确的能耗测量来解锁这一潜力，而异构部署中的测量面临三大挑战。

Method: 设计TENORAN框架，基于高层规范（如gNB软件栈、分流选项、流量配置）对端到端部署进行仪表化，在受控工作负载（包括空口流量）下收集同步的性能指标和各个RAN组件的功率测量。

Result: 实验展示了端到端实验的能耗分析（包含xApps）、OpenAirInterface和srsRAN两个RAN栈在上行和下行链路上的能效差异，以及核心网功耗趋势。

Conclusion: TENORAN框架能够有效解决O-RAN异构部署中的能耗测量挑战，为网络组件的细粒度调优和配置提供支持，从而实现节能目标。

Abstract: The transition to disaggregated and interoperable Open Radio Access Network (RAN) architectures and the introduction of RAN Intelligent Controllers (RICs) in O-RAN creates new resource optimization opportunities and fine-grained tuning and configuration of network components to save energy while fulfilling service demand. However, unlocking this potential requires fine-grained and accurate energy measurements across heterogeneous deployments. Three factors make this particularly challenging [...]. To address these challenges, we design the TENORAN framework, an automated measurement scaffold for fine-grained energy efficiency profiling of O-RAN deployments, and prototype it on a heterogeneous OpenShift cluster. TENORAN instruments an end-to-end deployment based on high-level specifications (e.g., gNB software stack and split options, traffic profiles), and collects synchronized performance metrics and power measurements for individual RAN components while the network is under controlled workloads including over-the-air traffic. Our experimental results demonstrate energy profiling of end-to-end experiments with xApps in the loop, energy efficiency differences between two RAN stacks, OpenAirInterface and srsRAN, in uplink and downlink, and core network power consumption trends.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory](https://arxiv.org/abs/2602.12316)
*Pepijn Cobben,Xuanqiang Angelo Huang,Thao Amelia Pham,Isabel Dahlgren,Terry Jingchen Zhang,Zhijing Jin*

Main category: cs.AI

TL;DR: GT-HarmBench是一个包含2009个高风险场景的多智能体安全基准测试，涵盖囚徒困境、猎鹿博弈等博弈论结构，评估前沿AI模型在多智能体环境中的社会效益行为表现。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全基准主要评估单智能体，而多智能体环境中的协调失败、冲突等风险缺乏充分理解。需要开发专门的多智能体安全基准来评估前沿AI系统在高风险多智能体环境中的表现。

Method: 从MIT AI风险库中提取真实AI风险场景，构建包含2009个高风险场景的GT-HarmBench基准，涵盖囚徒困境、猎鹿博弈、胆小鬼博弈等经典博弈论结构。对15个前沿模型进行评估，测量其对博弈论提示框架和顺序的敏感性，并分析导致失败的推理模式。

Result: 在15个前沿模型中，智能体只在62%的情况下选择社会效益行为，经常导致有害结果。研究还发现博弈论干预可以将社会效益结果提高最多18%。

Conclusion: 研究揭示了前沿AI系统在多智能体环境中存在显著可靠性差距，GT-HarmBench为研究多智能体环境中的对齐问题提供了广泛的标准化测试平台。基准测试和代码已开源。

Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.

</details>


### [13] [A Theoretical Framework for Adaptive Utility-Weighted Benchmarking](https://arxiv.org/abs/2602.12356)
*Philip Waggoner*

Main category: cs.AI

TL;DR: 提出一个多层自适应网络框架，将基准测试重新概念化为连接评估指标、模型组件和利益相关者群体的动态系统，通过人类在环更新规则嵌入人类权衡，实现更符合社会技术背景的评估。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在更多样化和重要的环境中部署，需要超越传统基准测试的局限性，考虑社会技术背景和多利益相关者的不同优先级，建立更全面、符合人类价值观的评估框架。

Method: 提出理论框架，将基准测试重新概念化为多层自适应网络，连接评估指标、模型组件和利益相关者群体，通过联合分析推导的效用和人类在环更新规则，将人类权衡嵌入基准结构，实现动态演化同时保持稳定性和可解释性。

Result: 该框架将经典排行榜作为特例进行推广，为构建更具上下文感知能力的评估协议提供基础，产生分析基准结构特性的新鲁棒工具，通向更负责任和人类对齐的评估路径。

Conclusion: 通过将基准测试重新概念化为多层自适应网络，可以创建更全面、动态且符合社会技术背景的评估框架，实现更负责任和人类对齐的AI系统评估，推动评估实践向更符合人类价值观的方向发展。

Abstract: Benchmarking has long served as a foundational practice in machine learning and, increasingly, in modern AI systems such as large language models, where shared tasks, metrics, and leaderboards offer a common basis for measuring progress and comparing approaches. As AI systems are deployed in more varied and consequential settings, though, there is growing value in complementing these established practices with a more holistic conceptualization of what evaluation should represent. Of note, recognizing the sociotechnical contexts in which these systems operate invites an opportunity for a deeper view of how multiple stakeholders and their unique priorities might inform what we consider meaningful or desirable model behavior. This paper introduces a theoretical framework that reconceptualizes benchmarking as a multilayer, adaptive network linking evaluation metrics, model components, and stakeholder groups through weighted interactions. Using conjoint-derived utilities and a human-in-the-loop update rule, we formalize how human tradeoffs can be embedded into benchmark structure and how benchmarks can evolve dynamically while preserving stability and interpretability. The resulting formulation generalizes classical leaderboards as a special case and provides a foundation for building evaluation protocols that are more context aware, resulting in new robust tools for analyzing the structural properties of benchmarks, which opens a path toward more accountable and human-aligned evaluation.

</details>


### [14] [Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2602.12389)
*Siyuan Li,Yunjia Wu,Yiyong Xiao,Pingyang Huang,Peize Li,Ruitong Liu,Yan Wen,Te Sun,Fangyi Pei*

Main category: cs.AI

TL;DR: 提出Entity State Tuning (EST)框架，通过维护持续演化的实体状态来解决时序知识图谱预测中的长期依赖问题，显著提升现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱预测方法大多是无状态的，每次预测时都从有限查询窗口重新计算实体表示，导致"间歇性遗忘"和长期依赖快速衰减的问题。

Method: 提出EST框架：1) 维护全局状态缓冲区；2) 拓扑感知状态感知器将实体状态先验注入结构编码；3) 统一时序上下文模块聚合状态增强事件；4) 双轨演化机制平衡可塑性与稳定性。

Result: 在多个基准测试中，EST框架能持续改进不同骨干网络，达到最先进的性能，证明了状态持久性对长期时序知识图谱预测的重要性。

Conclusion: EST框架通过赋予时序知识图谱预测器持久且持续演化的实体状态，有效解决了长期依赖问题，为时序知识图谱预测提供了新的解决方案。

Abstract: Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots

</details>


### [15] [Intent-Driven Smart Manufacturing Integrating Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2602.12419)
*Takoua Jradi,John Violos,Dimitrios Spatharakis,Lydia Mavraidi,Ioannis Dimolitsas,Aris Leivadeas,Symeon Papavassiliou*

Main category: cs.AI

TL;DR: 提出一个统一框架，结合指令调优大语言模型和本体对齐知识图谱，实现制造即服务生态系统中的人类意图驱动交互


<details>
  <summary>Details</summary>
Motivation: 智能制造环境日益复杂，需要能够将高级人类意图转化为机器可执行动作的接口，以支持制造即服务生态系统中的意图驱动交互

Method: 使用领域特定数据集微调Mistral-7B-Instruct-V02模型，将自然语言意图转化为结构化JSON需求模型，然后通过基于ISA-95标准的Neo4j知识图谱进行语义映射

Result: 实验结果显示显著优于零样本和3样本基线，达到89.33%的精确匹配准确率和97.27%的整体准确率

Conclusion: 这项工作为可扩展、可解释和自适应的人机交互奠定了基础，推动了智能制造环境中意图驱动的接口发展

Abstract: The increasing complexity of smart manufacturing environments demands interfaces that can translate high-level human intents into machine-executable actions. This paper presents a unified framework that integrates instruction-tuned Large Language Models (LLMs) with ontology-aligned Knowledge Graphs (KGs) to enable intent-driven interaction in Manufacturing-as-a-Service (MaaS) ecosystems. We fine-tune Mistral-7B-Instruct-V02 on a domain-specific dataset, enabling the translation of natural language intents into structured JSON requirement models. These models are semantically mapped to a Neo4j-based knowledge graph grounded in the ISA-95 standard, ensuring operational alignment with manufacturing processes, resources, and constraints. Our experimental results demonstrate significant performance gains over zero-shot and 3-shots baselines, achieving 89.33\% exact match accuracy and 97.27\% overall accuracy. This work lays the foundation for scalable, explainable, and adaptive human-machine

</details>


### [16] [Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation](https://arxiv.org/abs/2602.12544)
*Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Creighton Glasscock,Honglak Lee*

Main category: cs.AI

TL;DR: 提出一个可扩展的自动生成高质量网页代理训练数据的流程，通过约束评估框架量化任务完成进度，利用部分成功轨迹扩大可用训练数据，在BookingArena基准上表现优于开源方法并匹配商业系统


<details>
  <summary>Details</summary>
Motivation: 网页代理训练面临高质量训练数据稀缺的挑战，特别是难以评估轨迹质量（量化任务完成进度），这限制了训练数据的有效利用

Method: 1. 提出可扩展的自动生成高质量训练数据流程；2. 引入基于约束的评估框架，提供细粒度的任务完成进度评估；3. 利用部分成功轨迹显著扩大可用训练数据；4. 提出BookingArena新基准，包含20个流行网站的复杂预订任务

Result: 蒸馏出的学生模型在BookingArena基准上表现优于开源方法，匹配或超过商业系统，同时模型规模显著更小

Conclusion: 该工作解决了高效创建多样化、真实的网页交互数据集的挑战，为复杂结构化网页任务提供了系统化的评估方法

Abstract: We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion. We introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion. This enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.

</details>


### [17] [To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.12566)
*Haoqing Wang,Xiang Long,Ziheng Li,Yilong Xu,Tingguang Li,Yehui Tang*

Main category: cs.AI

TL;DR: 该研究比较了多领域强化学习与可验证奖励（RLVR）的两种训练范式：混合多任务训练与单独训练后模型合并，发现跨领域RLVR存在较少相互干扰，推理密集型领域甚至表现出协同效应。


<details>
  <summary>Details</summary>
Motivation: 当前多领域专家级模型需要跨领域RLVR协作，但现有研究缺乏对混合多任务RLVR与单独训练后模型合并这两种范式的详细比较分析。

Method: 选择数学、编程、科学和指令跟随等多个常用高级任务作为目标领域，使用开源数据集设计广泛的定性和定量实验，从权重空间几何、模型预测行为和信息约束等角度分析内部机制。

Result: 发现跨领域RLVR存在较少相互干扰，推理密集型领域表现出相互协同效应；从权重空间几何、模型预测行为和信息约束等角度揭示了相互增益的内部机制。

Conclusion: 该研究为多领域RLVR训练范式提供了实证分析，表明跨领域协作具有可行性，推理密集型领域的协同效应为多领域专家模型训练提供了新见解。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL

</details>


### [18] [Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models](https://arxiv.org/abs/2602.12586)
*Joshua Ong Jun Leang,Yu Zhao,Mihaela Cătălina Stoian,Wenda Li,Shay B. Cohen,Eleonora Giunchiglia*

Main category: cs.AI

TL;DR: McDiffuSE使用蒙特卡洛树搜索优化掩码扩散模型中的槽位填充顺序，通过前瞻模拟评估部分完成情况，在数学和代码推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型中的计划-填充解码方法对填充顺序非常敏感，导致输出方差大，需要系统化的方法来优化生成顺序以提高推理任务的性能。

Method: 将槽位选择建模为决策过程，使用蒙特卡洛树搜索优化填充顺序，通过前瞻模拟评估部分完成情况，系统探索生成顺序的组合空间。

Result: 相比自回归基线平均提升3.2%，相比基线计划-填充方法提升8.0%，在MBPP上提升19.5%，在MATH500上提升4.9%。研究发现需要更大的探索常数而非更多模拟来克服模型置信度偏差。

Conclusion: MCTS规划是提升掩码扩散模型生成质量的有效方法，虽然主要遵循顺序生成，但结合非顺序生成对最大化性能至关重要。

Abstract: While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.

</details>


### [19] [GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics](https://arxiv.org/abs/2602.12617)
*Modi Jin,Yiming Zhang,Boyuan Sun,Dingwen Zhang,MingMing Cheng,Qibin Hou*

Main category: cs.AI

TL;DR: GeoAgent：一个能够与人类紧密推理并得出细粒度地址结论的地理智能模型，通过专家标注数据集和地理相似性奖励机制提升性能


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的方法虽然取得了性能突破，但依赖AI生成的思维链数据，与地理任务特性存在冲突，需要更符合地理特性的训练方法

Method: 1) 引入GeoSeek专家标注数据集；2) 提出地理相似性奖励和一致性奖励机制，通过一致性智能体评估，确保推理过程的地理合理性和一致性

Result: GeoAgent在多个粒度上超越了现有方法和通用视觉语言大模型，生成的推理过程与人类思维高度一致

Conclusion: 通过专家标注数据和地理特性驱动的奖励机制，GeoAgent实现了与人类紧密对齐的地理推理能力，为地理智能任务提供了有效解决方案

Abstract: This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics. To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process. Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.

</details>


### [20] [AI Agents for Inventory Control: Human-LLM-OR Complementarity](https://arxiv.org/abs/2602.12631)
*Jackie Baek,Yaopeng Fu,Will Ma,Tianyi Peng*

Main category: cs.AI

TL;DR: LLM增强的运筹学方法在库存控制中优于单独使用运筹学或LLM，人机协作团队也比单独的人类或AI表现更好


<details>
  <summary>Details</summary>
Motivation: 传统运筹学算法依赖刚性建模假设，在需求分布变化或缺乏上下文信息时表现不佳。LLM具有灵活推理能力，但如何将其融入传统决策流程尚不明确。研究探索运筹学算法、LLM和人类如何互补协作。

Method: 构建InventoryBench基准测试（包含1000多个库存实例，涵盖合成和真实需求数据），测试需求变化、季节性和不确定交货期下的决策规则。通过课堂实验研究人机协作决策流程。

Result: 运筹学增强的LLM方法优于单独使用任一种方法。人机协作团队平均利润高于单独的人类或AI。建立了个体层面互补效应的形式化定义，并推导了分布无关的下界，实证显示受益个体比例显著。

Conclusion: 运筹学算法、LLM和人类在库存控制中是互补而非替代关系。人机协作能提升决策质量，为AI增强决策系统设计提供了实证依据。

Abstract: Inventory control is a fundamental operations problem in which ordering decisions are traditionally guided by theoretically grounded operations research (OR) algorithms. However, such algorithms often rely on rigid modeling assumptions and can perform poorly when demand distributions shift or relevant contextual information is unavailable. Recent advances in large language models (LLMs) have generated interest in AI agents that can reason flexibly and incorporate rich contextual signals, but it remains unclear how best to incorporate LLM-based methods into traditional decision-making pipelines.
  We study how OR algorithms, LLMs, and humans can interact and complement each other in a multi-period inventory control setting. We construct InventoryBench, a benchmark of over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through this benchmark, we find that OR-augmented LLM methods outperform either method in isolation, suggesting that these methods are complementary rather than substitutes.
  We further investigate the role of humans through a controlled classroom experiment that embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior findings that human-AI collaboration can degrade performance, we show that, on average, human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond this population-level finding, we formalize an individual-level complementarity effect and derive a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration; empirically, we find this fraction to be substantial.

</details>


### [21] [Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents](https://arxiv.org/abs/2602.12662)
*Ruihan Yang,Fanghua Ye,Xiang We,Ruoqing Zhao,Kang Luo,Xinbo Xu,Bo Zhao,Ruotian Ma,Shanyi Wang,Zhaopeng Tu,Xiaolong Li,Deqing Yang,Linus*

Main category: cs.AI

TL;DR: CogRouter是一个让LLM代理动态调整认知深度的框架，通过分层认知级别和两阶段训练，在长视野任务中实现高效决策。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的认知模式过于僵化：非思考模型立即响应，思考模型则统一深度推理。这种刚性在长视野任务中效率低下，因为不同步骤的认知需求差异很大，有些需要战略规划，有些只需常规执行。

Method: 基于ACT-R理论设计四个分层认知级别（从本能响应到战略规划）。采用两阶段训练：认知感知监督微调（CoSFT）建立稳定的级别特定模式，认知感知策略优化（CoPO）通过置信度感知优势重加权进行步骤级信用分配。

Result: 在ALFWorld和ScienceWorld上的实验显示，CogRouter达到最先进性能且效率优越。使用Qwen2.5-7B模型，达到82.3%成功率，优于GPT-4o（+40.3%）、OpenAI-o3（+18.3%）和GRPO（+14.0%），同时减少62%的token使用。

Conclusion: CogRouter通过动态调整认知深度，使LLM代理能够根据任务需求灵活选择适当的认知级别，在长视野决策任务中实现高效且高性能的自主决策。

Abstract: Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.

</details>


### [22] [Evaluating Robustness of Reasoning Models on Parameterized Logical Problems](https://arxiv.org/abs/2602.12665)
*Naïm Es-sebbani,Esteban Marquer,Yakoub Salhi,Zied Bouraoui*

Main category: cs.AI

TL;DR: 作者创建了一个诊断性的2-SAT基准测试，通过参数化公式家族来分离表面难度与结构现象，揭示LLM推理器的特定能力与失败模式。


<details>
  <summary>Details</summary>
Motivation: 传统SAT基准测试往往将表面特征（长度、措辞、子句顺序）与决定可满足性的结构现象混为一谈，无法准确评估LLM推理器的真实推理能力。

Method: 构建基于参数化结构化2-CNF公式的诊断基准，通过五种生成器隔离不同能力：矛盾循环UNSAT核心、具有规定自由变量比例的SAT实例、植入骨干、晚期桥接子句、对称/重复变体。

Result: 评估显示LLM推理器在目标结构干预下表现出急剧的性能转变，即使表面统计特征保持不变，揭示了在聚合SAT准确率中不可见的脆弱性区域。

Conclusion: 该诊断基准能够揭示LLM推理器对特定结构现象的敏感性，为评估推理能力提供了比传统SAT基准更精细的工具，有助于识别模型在表面统计特征不变情况下的脆弱性。

Abstract: Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.

</details>


### [23] [SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks](https://arxiv.org/abs/2602.12670)
*Xiangyi Li,Wenbo Chen,Yimin Liu,Shenghan Zheng,Xiaokun Chen,Yifeng He,Yubo Li,Bingran You,Haotian Shen,Jiankai Sun,Shuyi Wang,Qunhong Zeng,Di Wang,Xuandong Zhao,Yuanli Wang,Roey Ben Chaim,Zonglin Di,Yipeng Gao,Junwei He,Yizhuo He,Liqiang Jing,Luyang Kong,Xin Lan,Jiachen Li,Songlin Li,Yijiang Li,Yueqian Lin,Xinyi Liu,Xuanqing Liu,Haoran Lyu,Ze Ma,Bowei Wang,Runhui Wang,Tianyu Wang,Wengao Ye,Yue Zhang,Hanwen Xing,Yiqi Xue,Steven Dillmann,Han-chung Lee*

Main category: cs.AI

TL;DR: SkillsBench是一个评估Agent Skills效果的基准测试，包含86个任务和11个领域，测试结果显示精心设计的Skills能提升16.2%的平均通过率，但效果因领域而异，而模型自生成的Skills基本无效。


<details>
  <summary>Details</summary>
Motivation: 尽管Agent Skills（结构化程序知识包）在LLM代理中被广泛采用，但目前缺乏标准方法来衡量它们是否真正有效。需要建立一个系统性的评估框架来量化Skills对代理性能的实际影响。

Method: 提出了SkillsBench基准测试，包含86个任务覆盖11个领域，每个任务配有精心设计的Skills和确定性验证器。在三种条件下评估：无Skills、精心设计的Skills、自生成的Skills。测试了7种代理-模型配置，共7,308条轨迹。

Result: 精心设计的Skills平均提升通过率16.2个百分点，但效果差异很大：软件工程领域仅提升4.5个百分点，医疗领域提升51.9个百分点。84个任务中有16个出现负增长。自生成的Skills平均无益处。2-3个模块的聚焦Skills优于全面文档，配备Skills的小模型可匹敌无Skills的大模型。

Conclusion: 精心设计的Skills能显著提升代理性能，但效果高度依赖领域和任务。模型无法可靠地生成它们能从中受益的程序知识。Skills的质量比数量更重要，精心设计的Skills可以弥补模型规模的不足。

Abstract: Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.

</details>


### [24] [X-SYS: A Reference Architecture for Interactive Explanation Systems](https://arxiv.org/abs/2602.12748)
*Tobias Labarta,Nhi Hoang,Maximilian Dreyer,Jim Berend,Oleg Hein,Jackie Ma,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.AI

TL;DR: X-SYS：一个用于交互式解释系统的参考架构，通过STAR质量属性和五组件分解，将XAI从算法扩展到可部署系统。


<details>
  <summary>Details</summary>
Motivation: 当前XAI研究提出了许多技术方法，但将可解释性部署为系统仍然具有挑战性。交互式解释系统需要合适的算法和系统能力，以在重复查询、模型和数据演化以及治理约束下保持解释可用性。作者认为，将XAI操作化需要将可解释性视为信息系统问题。

Method: 提出X-SYS参考架构，围绕STAR四个质量属性（可扩展性、可追溯性、响应性和适应性）组织，并指定五个组件分解（XUI服务、解释服务、模型服务、数据服务、编排与治理）。该架构将交互模式映射到系统能力，解耦用户界面演进和后端计算。通过SemanticLens系统（用于视觉语言模型的语义搜索和激活导向）实现X-SYS。

Result: X-SYS提供了一个可重用的蓝图，通过基于契约的服务边界实现独立演进，离线/在线分离确保响应性，持久状态管理支持可追溯性。SemanticLens展示了该架构的具体实例化。

Conclusion: 这项工作为交互式解释系统提供了端到端设计的可重用蓝图和具体实例，支持在操作约束下的系统构建，帮助XAI研究人员、开发者和从业者将交互式解释用户界面与系统能力连接起来。

Abstract: The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.

</details>


### [25] [WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning](https://arxiv.org/abs/2602.12852)
*Junjie Wang,Zequn Xie,Dan Yang,Jie Feng,Yue Shen,Duolin Sun,Meixiu Long,Yihan Jiao,Zhehao Tan,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: WebClipper：通过图剪枝压缩网页代理轨迹的框架，减少约20%工具调用轮次同时提升准确性


<details>
  <summary>Details</summary>
Motivation: 现有网页代理系统存在搜索效率低下的问题，表现为工具调用轨迹过长、循环推理和探索无效分支，需要平衡准确性和效率

Method: 将代理搜索过程建模为状态图，将轨迹优化转化为最小必要有向无环图挖掘问题，通过图剪枝压缩轨迹，并在精炼轨迹上继续训练

Result: WebClipper在保持优秀性能的同时压缩工具调用轮次约20%，并提高准确性；提出新的F-AE Score指标平衡准确性和效率

Conclusion: WebClipper通过图剪枝有效优化网页代理轨迹，为平衡网页代理设计中的有效性和效率提供了实用见解

Abstract: Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.

</details>


### [26] [BrowseComp-$V^3$: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2602.12876)
*Huanyao Zhang,Jiepeng Zhou,Bo Li,Bowen Zhou,Yanzhe Dan,Haishan Lu,Zhiyong Cao,Jiaoyang Chen,Yuqian Han,Zinan Sheng,Zhengwei Tao,Hao Liang,Jialong Wu,Yang Shi,Yuanpeng He,Jiaye Lin,Qintong Zhang,Guochen Yan,Runhao Zhao,Zhengpin Li,Xiaohan Yu,Lang Mei,Chong Chen,Wentao Zhang,Bin Cui*

Main category: cs.AI

TL;DR: BrowseComp-V³是一个新的多模态网页浏览基准测试，包含300个挑战性问题，强调跨模态多跳推理，要求所有证据必须可公开搜索，并采用专家验证的子目标驱动过程评估机制。


<details>
  <summary>Details</summary>
Motivation: 现有多模态浏览基准在任务复杂性、证据可访问性和评估粒度方面存在局限，阻碍了对深度搜索能力的全面和可重复评估。需要一个新的基准来更好地评估多模态大语言模型在真实世界环境中的深度搜索能力。

Method: 提出了BrowseComp-V³基准，包含300个精心策划的挑战性问题，涵盖多个领域，强调深度、多层次、跨模态的多跳推理。所有支持证据必须可公开搜索。引入了专家验证的子目标驱动过程评估机制，进行细粒度中间推理行为分析。还提出了OmniSeeker统一多模态浏览代理框架，整合多种网页搜索和视觉感知工具。

Result: 即使是当前最先进的模型在BrowseComp-V³基准上也仅达到36%的准确率，揭示了在多模态信息整合和细粒度感知方面的关键瓶颈。实验结果显示了当前模型能力与真实世界环境中稳健多模态深度搜索之间的根本差距。

Conclusion: BrowseComp-V³基准为评估多模态深度搜索能力提供了更全面和可重复的框架，揭示了当前多模态大语言模型在真实世界网页浏览任务中的显著局限性，为未来研究指明了改进方向。

Abstract: Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-$V^3$, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.

</details>


### [27] [Information-theoretic analysis of world models in optimal reward maximizers](https://arxiv.org/abs/2602.12963)
*Alfred Harwood,Jose Faustino,Alex Altair*

Main category: cs.AI

TL;DR: 该论文量化了最优策略关于环境的信息量，证明对于n状态m动作的CMP，任何非恒定奖励函数的最优确定性策略都包含n log m比特的环境信息。


<details>
  <summary>Details</summary>
Motivation: 研究AI中一个重要问题：成功行为在多大程度上需要世界的内部表示。量化最优策略提供的关于底层环境的信息量，为"隐式世界模型"提供信息论下界。

Method: 考虑具有n个状态和m个动作的受控马尔可夫过程(CMP)，假设在可能的转移动态空间上具有均匀先验。证明观察任何非恒定奖励函数的最优确定性策略能传递n log m比特的环境信息。

Result: 证明环境与最优策略之间的互信息为n log m比特。这个界限适用于广泛的优化目标，包括有限时域、无限时域折扣和时间平均奖励最大化。

Conclusion: 这些发现为最优性所需的"隐式世界模型"提供了精确的信息论下界，表明最优策略必然包含关于环境结构的大量信息。

Abstract: An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the "implicit world model'' necessary for optimality.

</details>


### [28] [Consistency of Large Reasoning Models Under Multi-Turn Attacks](https://arxiv.org/abs/2602.13093)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.AI

TL;DR: 推理模型在对抗攻击下表现出有意义但不完整的鲁棒性，虽然优于指令调优基线，但存在多种脆弱性模式，且基于置信度的防御方法对推理模型失效。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务上表现出色，但其在多轮对抗压力下的鲁棒性尚未充分探索。需要评估前沿推理模型在对抗攻击下的表现，了解推理能力是否自动带来对抗鲁棒性。

Method: 评估九个前沿推理模型在对抗攻击下的表现，通过轨迹分析识别失败模式，测试置信感知响应生成（CARG）方法对推理模型的有效性。

Result: 推理提供有意义但不完整的鲁棒性：大多数推理模型显著优于指令调优基线，但所有模型都表现出不同的脆弱性特征。误导性建议普遍有效，社会压力具有模型特异性效果。识别出五种失败模式（自我怀疑、社会从众、建议劫持、情感易感性、推理疲劳），前两种占失败的50%。CARG对推理模型失效，因为扩展推理轨迹导致过度自信；随机置信嵌入反而优于针对性提取。

Conclusion: 推理能力不会自动带来对抗鲁棒性，基于置信度的防御方法需要对推理模型进行根本性重新设计。需要开发专门针对推理模型特点的鲁棒性增强方法。

Abstract: Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.

</details>


### [29] [Constrained Assumption-Based Argumentation Frameworks](https://arxiv.org/abs/2602.13135)
*Emanuele De Angelis,Fabio Fioravanti,Maria Chiara Meo,Alberto Pettorossi,Maurizio Proietti,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种新的约束ABA（CABA）框架，通过引入约束变量来扩展传统ABA的表达能力，使其能够处理非地面参数和攻击。


<details>
  <summary>Details</summary>
Motivation: 传统ABA框架基于原子语言，仅限于地面（无变量）参数和命题原子构建的攻击，这种表示限制限制了其适用性。需要一种能够处理包含约束变量的非地面参数的方法。

Method: 提出约束ABA（CABA）框架，允许组件和参数包含约束变量，这些变量可以在可能无限的域上取值。定义了CABA的非地面语义，基于各种非地面攻击概念。

Result: 新的CABA语义能够保守地推广标准ABA语义，证明新框架在表达能力上对传统ABA进行了有意义的扩展。

Conclusion: CABA框架成功突破了传统ABA的地面限制，通过引入约束变量实现了更强大的表达能力，为结构化论证提供了更灵活的表示方法。

Abstract: Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.

</details>


### [30] [Optimal Take-off under Fuzzy Clearances](https://arxiv.org/abs/2602.13166)
*Hugo Henry,Arthur Tsai,Kelly Cohen*

Main category: cs.AI

TL;DR: 提出混合障碍物规避架构，结合最优控制与模糊规则系统，实现无人机自适应约束处理，但发现软件兼容性问题影响约束执行。


<details>
  <summary>Details</summary>
Motivation: 经典最优控制在不确定性下的局限性，以及航空安全关键系统需要可解释决策，促使开发能适应航空法规的智能障碍物规避系统。

Method: 采用三阶段Takagi-Sugeno-Kang模糊层调节约束半径、紧急程度和激活决策，结合FALCON工具箱和IPOPT求解器将模糊推导的间隙作为软约束纳入最优控制问题。

Result: 概念验证显示每迭代2-3秒计算时间，但发现FALCON和IPOPT最新版本存在拉格朗日惩罚项恒为零的软件不兼容问题，导致约束无法正常执行。

Conclusion: 方法在简化飞机模型中可行，但软件兼容性问题需解决。未来工作包括验证软件回归、优化模糊隶属函数、扩展到高保真模型和随机障碍环境。

Abstract: This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [31] [EARL: Energy-Aware Adaptive Antenna Control with Reinforcement Learning in O-RAN Cell-Free Massive MIMO Networks](https://arxiv.org/abs/2602.12841)
*Zilin Ge,Ozan Alp Topal,Irshad Ahmad Meer,Pei Xiao,Cicek Cavdar*

Main category: cs.IT

TL;DR: 提出EARL框架，基于强化学习动态配置分布式天线单元，在满足用户频谱效率需求的同时，显著降低无线接入网能耗。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝大规模MIMO虽然能提供均匀的高性能，但分布式无线电单元联合传输和云端集中处理带来了高能耗问题。需要利用O-RAN的资源共享能力来解决这一能耗挑战。

Method: 基于强化学习的能量感知自适应天线控制框架（EARL），动态配置无线电单元中的天线元件，同时最小化无线电、光前传和云端处理的功耗。

Result: 相比全开和启发式基线，分别实现了高达81%和50%的节能效果。强化学习方法在220毫秒内运行，满足O-RAN近实时限制，贪婪优化进一步在2秒运行时间内将功耗减半。

Conclusion: EARL框架能够有效平衡无蜂窝大规模MIMO系统的性能需求和能耗问题，通过智能天线控制实现显著的节能效果，同时满足O-RAN的实时性要求。

Abstract: Cell-free massive multi-input multi-output (MIMO) promises uniform high performance across the network, but also brings a high energy cost due to joint transmission from distributed radio units (RUs) and centralized processing in the cloud. Leveraging the resource-sharing capabilities of Open Radio Access Network (O-RAN), we propose EARL, an energy-aware adaptive antenna control framework based on reinforcement learning. EARL dynamically configures antenna elements in RUs to minimize radio, optical fronthaul, and cloud processing power consumption while meeting user spectral efficiency demands. Numerical results show power savings of up to 81% and 50% over full-on and heuristic baselines, respectively. The RL-based approach operates within 220 ms, satisfying O-RAN's near-real-time limit, and a greedy refinement further halves power consumption at a 2 s runtime.

</details>


### [32] [Channel Gain Map Reconstruction Based on Virtual Scatterer Model](https://arxiv.org/abs/2602.12602)
*He Sun,Lipeng Zhu,Jie Xu,Rui Zhang*

Main category: cs.IT

TL;DR: 提出基于虚拟散射体的信道增益地图高效建模与重构方法，通过虚拟散射体模型表征3D空间信道功率增益分布，利用渐进估计算法和GPR推断实现高精度重构。


<details>
  <summary>Details</summary>
Motivation: 传统信道增益地图估计方法在效率和精度方面存在局限，需要一种能够灵活建模多径传播环境结构并利用散射体响应空间相关性的高效方法。

Method: 1) 建立虚拟散射体模型，将信道增益地图表示为虚拟散射体参数（数量、位置、散射响应系数）的函数；2) 提出渐进估计算法，逐步增加虚拟散射体数量平衡计算复杂度和估计精度；3) 利用高斯过程回归基于空间相关性预测无法直接估计的散射响应系数。

Result: 在真实物理环境下的射线追踪仿真结果表明，该方法相比传统信道增益地图估计方法实现了更高的重构精度。

Conclusion: 提出的虚拟散射体模型为信道增益地图重构提供了灵活可扩展的建模框架，结合渐进估计算法和GPR推断方法，能够高效准确地重构信道增益地图。

Abstract: This paper proposes an efficient method for modeling and reconstructing the channel gain map (CGM) based on virtual scatterers. Specifically, we develop a virtual scatterer model to characterize the channel power gain distribution in three-dimensional (3D) space, by capturing the multi-path propagation environment structure and exploiting the angular-domain spatial correlation of scatterer response. In this model, the CGM is represented as a function over a set of tunable parameters for virtual scatterers, including their number, positions, and scatterer response coefficients (SRCs), which can be estimated from a limited number of channel power gain measurements at a given set of locations within the region of interest. This new representation offers a flexible and scalable modeling framework for efficient and accurate CGM reconstruction. Furthermore, we propose a progressive estimation algorithm to acquire the scatterers' parameters. In this algorithm, we gradually increase the number of virtual scatterers to balance the computational complexity and estimation accuracy. In addition, by exploiting the spatial correlation of scatterer response, we propose a Gaussian process regression (GPR)-based inference method to predict the SRCs that cannot be directly estimated. Finally, ray-tracing-based simulation results under realistic physical environments validate the effectiveness of the proposed method, demonstrating that it achieves higher reconstruction accuracy compared to conventional CGM estimation approaches.

</details>


### [33] [Secure Beamforming for ISAC Systems Under Communication Eavesdropper and Sensing Eavesdropper](https://arxiv.org/abs/2602.12614)
*Tian Zhang,Zhirong Su,Yueyi Dong*

Main category: cs.IT

TL;DR: 该论文研究了集成感知与通信系统中通信窃听者和感知窃听者共存时的物理层安全问题，提出了一种联合安全波束成形算法来最大化系统保密率。


<details>
  <summary>Details</summary>
Motivation: 随着集成感知与通信技术在资源利用和硬件空间效率方面的显著提升，其安全问题日益重要。现有研究主要关注通信窃听者，而忽略了感知窃听者的威胁，这在实际应用中可能导致系统安全漏洞。

Method: 采用连续凸逼近方法结合一阶泰勒展开和半定松弛技术，处理非凸优化问题。提出了一种迭代的联合安全波束成形算法，同时对抗通信窃听者和感知窃听者，并理论上证明了半定松弛不会产生次优解。

Result: 仿真结果表明，所提出的方案在保证感知性能的同时，能够有效提升系统保密率，验证了该方法的有效性和先进性。

Conclusion: 该研究为集成感知与通信系统提供了全面的安全解决方案，通过联合设计通信和感知信号的波束成形，能够同时防御通信和感知窃听者，为未来安全ISAC系统设计提供了理论基础和实用算法。

Abstract: Due to great efficiency improvement in resource and hardware space, integrated sensing and communication (ISAC) has gained much attention. In the paper, the physical layer security (PLS) of ISAC system under communication eavesdropper together with sensing eavesdropper is investigated. The system secrecy rate is maximized by transmit beamforming design of communication and sensing signals when taking sensing security, sensing performance and transmit power constraint into consideration. To deal with the formulated non-convex optimization problem, the successive convex approximation (SCA) together with the first-order Taylor expansion and semidefinite relaxation (SDR) is utilized. Additionally, it is theoretically validated that the SDR does not yield sub-optimality in the paper. Thereafter, an iterated joint secure beamforming algorithm against communication and sensing eavesdroppers is proposed. Simulation results validate the effectiveness and advance of the proposed scheme.

</details>


### [34] [Secrecy Capacity Analysis and Beamforming Optimization for MIMO-VLC Wiretap Channels](https://arxiv.org/abs/2602.12720)
*Sufang Yang,Longguang Li,Jintao Wang,Ya Li,Liang Xia,Hongjun He,Qixing Wang,Guangyi Liu*

Main category: cs.IT

TL;DR: 提出MIMO可见光通信窃听信道下的安全传输方案，包括全连接和子连接波束成形设计，通过凸优化方法解决非凸问题，显著提升保密性能。


<details>
  <summary>Details</summary>
Motivation: 研究MIMO可见光通信系统中的物理层安全问题，针对存在窃听者的场景，在光信号峰值和平均强度约束下，设计有效的保密传输方案。

Method: 应用广义熵功率不等式到截断指数输入，推导出MIMO VLC保密速率闭式表达式；提出全连接波束成形方案和低复杂度子连接方案；使用逐次凸逼近框架将非凸波束成形设计问题转化为凸子问题序列求解。

Result: 数值结果表明，所提方案相比基准方案实现了显著的保密性能提升，验证了方法的有效性。

Conclusion: 成功解决了MIMO VLC窃听信道的保密传输问题，提出的波束成形方案和优化方法为可见光通信系统安全提供了有效解决方案。

Abstract: This paper investigates a multiple-input multipleoutput (MIMO) visible light communication (VLC) wiretap channel consisting of a transmitter, a legitimate receiver, and an eavesdropper. The optical input is subject to both peakand average-intensity constraints. By applying the generalized entropy-power inequality to truncated exponential inputs, we derive a novel closed-form expression for the achievable secrecy rate for general MIMO VLC configurations. To enhance transmission confidentiality, a fully-connected beamforming scheme is proposed, along with a low-complexity sub-connected alternative. Although the resulting beamforming design problems are nonconvex, they are efficiently addressed by transforming them into a sequence of convex subproblems solvable via the successive convex approximation framework. Numerical results demonstrate that the proposed schemes achieve significant secrecy performance improvements compared with the benchmark scheme.

</details>


### [35] [Construction of MRD Codes Based on Circular-Shift Operations](https://arxiv.org/abs/2602.12766)
*Zhe Zhai,Sheng Jin,Qifu Tyler Sun,Zongpeng Li*

Main category: cs.IT

TL;DR: 本文提出了一种基于循环移位操作、完全在有限域F_q上构造最大秩距离(MRD)码的新方法，避免了传统构造中需要F_{q^N}算术的复杂性，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统MRD码构造主要依赖于F_{q^N}的算术运算，随着N增大，计算复杂度急剧增加，这限制了参数选择和实际实现。需要一种更高效、完全在基域F_q上操作的构造方法。

Method: 基于循环移位操作，利用欧拉函数J=φ(L)（其中gcd(q,L)=1），在F_q上构造(J×n, q^{Jk}, d) MRD码。通过q-线性化多项式在行向量空间F_q^N上刻画所构造的MRD码、Gabidulin码和扭曲Gabidulin码，阐明它们之间的差异和联系。

Result: 1) 当J≠m_L时，所构造的MRD码在多种参数设置下不同于任何Gabidulin码和扭曲Gabidulin码；2) 当J=m_L时，构造的MRD码与Gabidulin码重合，提供了直接在F_q上操作的等价循环移位构造；3) 在某些参数设置下，构造的MRD码等价于通过求和和拼接多个Gabidulin码得到的广义Gabidulin码；4) 当q=2、L为素数且n≤m_L时，生成一个码字仅需O(nkL)异或操作，而传统Gabidulin码需要O(nkL^2)异或操作。

Conclusion: 提出的基于循环移位的MRD码构造方法完全在F_q上操作，避免了扩展域的复杂算术，显著降低了计算复杂度，为实际应用提供了更高效的实现方案，特别是在二进制情况下能获得显著的性能提升。

Abstract: Most well-known constructions of $(N \times n, q^{Nk}, d)$ maximum rank distance (MRD) codes rely on the arithmetic of $\mathbb{F}_{q^N}$, whose increasing complexity with larger $N$ hinders parameter selection and practical implementation. In this work, based on circular-shift operations, we present a construction of $(J \times n, q^{Jk}, d)$ MRD codes with efficient encoding, where $J$ equals to the Euler's totient function of a defined $L$ subject to $\gcd(q, L) = 1$. The proposed construction is performed entirely over $\mathbb{F}_q$ and avoids the arithmetic of $\mathbb{F}_{q^J}$. We further characterize the constructed MRD codes, Gabidulin codes and twisted Gabidulin codes using a set of $q$-linearized polynomials over the row vector space $\mathbb{F}_{q}^N$, and clarify their inherent difference and connection. For the case $J \neq m_L$, where $m_L$ denotes the multiplicative order of $q$ modulo $L$, we show that the proposed MRD codes, in a family of settings, are different from any Gabidulin code and any twisted Gabidulin code. For the case $J = m_L$, we prove that every constructed $(J \times n, q^{Jk}, d)$ MRD code coincides with a $(J \times n, q^{Jk}, d)$ Gabidulin code, yielding an equivalent circular-shift-based construction that operates directly over $\mathbb{F}_q$. In addition, we prove that under some parameter settings, the constructed MRD codes are equivalent to a generalization of Gabidulin codes obtained by summing and concatenating several $(m_L \times n, q^{m_Lk}, d)$ Gabidulin codes. When $q=2$, $L$ is prime and $n\leq m_L$, it is analyzed that generating a codeword of the proposed $((L-1) \times n, 2^{(L-1)k}, d)$ MRD codes requires $O(nkL)$ exclusive OR (XOR) operations, while generating a codeword of $((L-1) \times n, 2^{(L-1)k}, d)$ Gabidulin codes, based on customary construction, requires $O(nkL^2)$ XOR operations.

</details>


### [36] [FPNet: Joint Wi-Fi Beamforming Matrix Feedback and Anomaly-Aware Indoor Positioning](https://arxiv.org/abs/2602.12799)
*Ran Tao,Jiajia Guo,Yiming Cui,Xiangyi Li,Chao-Kai Wen,Shi Jin*

Main category: cs.IT

TL;DR: FPNet是一个统一的深度学习框架，利用Wi-Fi波束赋形反馈矩阵进行室内定位，同时实现信道反馈压缩和异常检测，在商用设备上达到高精度定位和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有Wi-Fi室内定位面临两个主要问题：1）完整信道状态信息（CSI）在实际部署中难以获取，硬件限制和反馈开销大；2）现有定位模型缺乏检测用户是否离开训练区域的机制，在动态环境中可靠性差。

Method: FPNet采用统一的深度学习框架，利用IEEE 802.11ac/ax/be协议原生支持的波束赋形反馈矩阵（BFM）作为压缩的CSI表示，减少反馈开销。框架包含ADBlock轻量级异常检测模块，在正常BFM样本上训练，用于识别用户离开预定义空间区域的异常情况。

Result: 使用标准2.4 GHz Wi-Fi硬件实验显示：定位准确率超过97%（仅需100反馈比特），网络吞吐量提升高达22.92%，异常检测准确率超过99%，误报率低于1.5%。

Conclusion: FPNet能够在商用Wi-Fi设备上实现高效、准确且可靠的室内定位，通过统一框架同时解决了信道反馈压缩、精确定位和异常检测三个关键问题。

Abstract: Channel State Information (CSI) provides a detailed description of the wireless channel and has been widely adopted for Wi-Fi sensing, particularly for high-precision indoor positioning. However, complete CSI is rarely available in real-world deployments due to hardware constraints and the high communication overhead required for feedback. Moreover, existing positioning models lack mechanisms to detect when users move outside their trained regions, leading to unreliable estimates in dynamic environments. In this paper, we present FPNet, a unified deep learning framework that jointly addresses channel feedback compression, accurate indoor positioning, and robust anomaly detection (AD). FPNet leverages the beamforming feedback matrix (BFM), a compressed CSI representation natively supported by IEEE 802.11ac/ax/be protocols, to minimize feedback overhead while preserving critical positioning features. To enhance reliability, we integrate ADBlock, a lightweight AD module trained on normal BFM samples, which identifies out-of-distribution scenarios when users exit predefined spatial regions. Experimental results using standard 2.4 GHz Wi-Fi hardware show that FPNet achieves positioning accuracy above 97% with only 100 feedback bits, boosts net throughput by up to 22.92%, and attains AD accuracy over 99% with a false alarm rate below 1.5%. These results demonstrate FPNet's ability to deliver efficient, accurate, and reliable indoor positioning on commodity Wi-Fi devices.

</details>


### [37] [Concatenated Codes for Short-Molecule DNA Storage with Sequencing Channels of Positive Zero-Undetected-Error Capacity](https://arxiv.org/abs/2602.12800)
*Ran Tamir,Nir Weinberger,Albert Guillén i Fàbregas*

Main category: cs.IT

TL;DR: 研究DNA存储系统中存在测序噪声时的可靠信息存储量，提出一种级联编码方案，外码处理随机采样，内码处理测序噪声，推导出可靠存储信息比特数的可达性界。


<details>
  <summary>Details</summary>
Motivation: DNA存储系统面临测序噪声和随机采样的挑战，需要设计能够同时处理这两种噪声的编码方案，以提高存储可靠性和容量。

Method: 采用级联编码方案：外码处理随机采样，内码处理测序噪声。内码使用线性分组码和零未检测错误解码器，假设测序信道对称，推导出最大似然解码器并分析其性能。

Result: 推导出可靠存储信息比特数缩放的可达性界。证明随机线性分组码在零未检测错误解码下的平均错误概率随块长度指数收敛到零，只要编码率不超过某个临界值。

Conclusion: 提出的级联编码方案能有效处理DNA存储中的测序噪声和随机采样问题，零未检测错误解码下的随机线性码性能分析为DNA存储系统设计提供了理论依据。

Abstract: We study the amount of reliable information that can be stored in a DNA-based storage system with noisy sequencing, where each codeword is composed of short DNA molecules. We analyze a concatenated coding scheme, where the outer code is designed to handle the random sampling, while the inner code is designed to handle the random sequencing noise. We assume that the sequencing channel is symmetric and choose the inner coding scheme to be composed by a linear block code and a zero-undetected-error decoder. As a byproduct, the resulting optimal maximum-likelihood decoder land itself for an amenable analysis, and we are able to derive an achievability bound for the scaling of the number of information bits that can be reliably stored. As a result of independent interest, we prove that the average error probability of random linear block codes under zero-undetected-error decoding converges to zero exponentially fast with the block length, as long as its coding rate does not exceed some critical value, which is known to serve as a lower bound to the zero-undetected-error capacity.

</details>


### [38] [Model-Aware Rate-Distortion Limits for Task-Oriented Source Coding](https://arxiv.org/abs/2602.12866)
*Andriy Enttsel,Vincent Corlay*

Main category: cs.IT

TL;DR: 该论文重新审视了面向任务的源编码（TOSC）的基本极限，指出现有速率-失真界限的局限性，并提出了考虑任务模型次优性和架构约束的新界限。


<details>
  <summary>Details</summary>
Motivation: 现有的面向任务的源编码（TOSC）速率-失真界限通常基于强假设（如任务可识别性），忽略了实际部署的任务模型的影响，导致在现实场景中应用受限。

Method: 通过间接速率-失真理论的视角重新审视单TOSC的基本极限，分析现有界限的可达条件，并提出考虑任务模型次优性和架构约束的任务模型感知速率-失真界限。

Result: 实验表明，当前学习的TOSC方案远未达到这些理论界限，揭示了发送端复杂度是主要瓶颈。

Conclusion: 需要开发更高效的TOSC方案来接近理论界限，特别是需要解决发送端复杂度问题，以实现机器推理系统中视觉数据通信的比特率、延迟和任务性能的联合优化。

Abstract: Task-Oriented Source Coding (TOSC) has emerged as a paradigm for efficient visual data communication in machine-centric inference systems, where bitrate, latency, and task performance must be jointly optimized under resource constraints. While recent works have proposed rate-distortion bounds for coding for machines, these results often rely on strong assumptions on task identifiability and neglect the impact of deployed task models. In this work, we revisit the fundamental limits of single-TOSC through the lens of indirect rate-distortion theory. We highlight the conditions under which existing rate-distortion bounds are achievable and show their limitations in realistic settings. We then introduce task model-aware rate-distortion bounds that account for task model suboptimality and architectural constraints. Experiments on standard classification benchmarks confirm that current learned TOSC schemes operate far from these limits, highlighting transmitter-side complexity as a key bottleneck.

</details>
