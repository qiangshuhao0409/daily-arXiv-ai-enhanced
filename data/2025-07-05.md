<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning](https://arxiv.org/abs/2507.01976)
*Nirhoshan Sivaroopan,Kaushitha Silva,Chamara Madarasingha,Thilini Dahanayaka,Guillaume Jourjon,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.NI

TL;DR: 本文综述了合成网络流量生成的方法，重点介绍了基于深度学习的技术，并讨论了统计方法及商业工具，同时指出了该领域的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决真实网络数据稀缺、隐私问题和纯度限制，为数据驱动应用提供替代方案。

Method: 综述了合成网络流量生成的数据类型、生成模型和评估方法，特别关注深度学习和统计方法。

Result: 提供了对现有方法的全面分析，指出了挑战和未来机会。

Conclusion: 本文为研究者和从业者提供了合成网络流量生成的基础资源，总结了方法、挑战和未来方向。

Abstract: Synthetic network traffic generation has emerged as a promising alternative
for various data-driven applications in the networking domain. It enables the
creation of synthetic data that preserves real-world characteristics while
addressing key challenges such as data scarcity, privacy concerns, and purity
constraints associated with real data. In this survey, we provide a
comprehensive review of synthetic network traffic generation approaches,
covering essential aspects such as data types, generation models, and
evaluation methods. With the rapid advancements in AI and machine learning, we
focus particularly on deep learning-based techniques while also providing a
detailed discussion of statistical methods and their extensions, including
commercially available tools. Furthermore, we highlight open challenges in this
domain and discuss potential future directions for further research and
development. This survey serves as a foundational resource for researchers and
practitioners, offering a structured analysis of existing methods, challenges,
and opportunities in synthetic network traffic generation.

</details>


### [2] [Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers](https://arxiv.org/abs/2507.01988)
*Giyong Jung,Saeid Gorgin,John Kim,Jungrae Kim*

Main category: cs.NI

TL;DR: 论文提出了一种名为ISN的机制和RXL扩展协议，用于解决多节点芯片互连中的可靠性和顺序传输问题，同时保持带宽效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型对计算能力的需求超过单处理器能力，芯片间互连的可靠性和扩展性成为关键挑战。现有协议在高传输速率下易受错误影响，且难以管理多节点配置中的丢包问题。

Method: 引入Implicit Sequence Number (ISN)机制，实现精确丢包检测和顺序传输；提出Reliability Extended Link (RXL)，扩展CXL协议以支持多节点互连，结合CRC和FEC确保端到端可靠性。

Result: RXL在不增加额外头部开销的情况下，提供了可靠的端到端数据传输和顺序完整性，同时保持带宽效率。

Conclusion: ISN和RXL为多节点芯片互连提供了高效、可靠的解决方案，兼容现有协议并满足未来扩展需求。

Abstract: As AI models outpace the capabilities of single processors, interconnects
across chips have become a critical enabler for scalable computing. These
processors exchange massive amounts of data at cache-line granularity,
prompting the adoption of new interconnect protocols like CXL, NVLink, and
UALink, designed for high bandwidth and small payloads. However, the increasing
transfer rates of these protocols heighten susceptibility to errors. While
mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction
(FEC) are standard for reliable data transmission, scaling chip interconnects
to multi-node configurations introduces new challenges, particularly in
managing silently dropped flits in switching devices. This paper introduces
Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit
drop detection and in-order delivery without adding header overhead.
Additionally, we propose Reliability Extended Link (RXL), an extension of CXL
that incorporates ISN to support scalable, reliable multi-node interconnects
while maintaining compatibility with the existing flit structure. By elevating
CRC to a transport-layer mechanism for end-to-end data and sequence integrity,
and relying on FEC for link-layer error correction and detection, RXL delivers
robust reliability and scalability without compromising bandwidth efficiency.

</details>


### [3] [Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks](https://arxiv.org/abs/2507.01994)
*Sardar Jaffar Ali,Syed M. Raza,Duc-Tai Le,Rajesh Challa,Min Young Chung,Ness Shroff,Hyunseung Choo*

Main category: cs.NI

TL;DR: 本文提出了一种结合Curated Collaborative Learning (CCL)和Distributed Unit Pooling Scheme (DUPS)的方法，显著降低5G RAN的能耗和运营成本。


<details>
  <summary>Details</summary>
Motivation: 5G网络中，RAN占超过50%的能耗，现有技术未能充分利用数据潜力，亟需优化以减少运营开支。

Method: 1. 使用CCL框架进行高精度网络流量和用户预测；2. 提出DUPS方案，结合深度强化学习和CCL预测，动态优化DU服务器资源。

Result: CCL在预测性能上显著优于现有方法（提升31.35%-43.9%），DUPS将能效提升89%，大幅降低运营成本。

Conclusion: 通过整合CCL和DUPS，本文展示了在5G RAN中实现高效节能和成本优化的创新方法。

Abstract: Despite advancements, Radio Access Networks (RAN) still account for over 50\%
of the total power consumption in 5G networks. Existing RAN split options do
not fully harness data potential, presenting an opportunity to reduce
operational expenditures. This paper addresses this opportunity through a
twofold approach. First, highly accurate network traffic and user predictions
are achieved using the proposed Curated Collaborative Learning (CCL) framework,
which selectively collaborates with relevant correlated data for traffic
forecasting. CCL optimally determines whom, when, and what to collaborate with,
significantly outperforming state-of-the-art approaches, including global,
federated, personalized federated, and cyclic institutional incremental
learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the
Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep
reinforcement learning and prediction inferences from CCL to reduce the number
of active DU servers efficiently. DUPS dynamically redirects traffic from
underutilized DU servers to optimize resource use, improving energy efficiency
by up to 89% over conventional strategies, translating into substantial
monetary benefits for operators. By integrating CCL-driven predictions with
DUPS, this paper demonstrates a transformative approach for minimizing energy
consumption and operational costs in 5G RANs, significantly enhancing
efficiency and cost-effectiveness.

</details>


### [4] [Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting](https://arxiv.org/abs/2507.01997)
*Zhihao Wang,Alessandro Cornacchia,Franco Galante,Carlo Centofanti,Alessio Sacco,Dingde Jiang*

Main category: cs.NI

TL;DR: AI和大型语言模型（LLMs）在网络配置和诊断中表现优异，本文聚焦AI代理在网络故障排除中的应用，并提出需要一个标准化、可复现的基准平台。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个低操作成本的标准化平台来评估AI代理在网络故障排除中的表现。

Method: 提出构建一个标准化、可复现的基准平台，用于开发和评估AI代理。

Result: 初步探讨了AI代理在网络故障排除中的潜力，并提出了平台需求。

Conclusion: 标准化基准平台对推动AI代理在网络故障排除中的应用至关重要。

Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence
(AI), and more specifically, Large Language Models (LLMs), in supporting
network configuration synthesis and automating network diagnosis tasks, among
others. In this preliminary work, we restrict our focus to the application of
AI agents to network troubleshooting and elaborate on the need for a
standardized, reproducible, and open benchmarking platform, where to build and
evaluate AI agents with low operational effort.

</details>


### [5] [AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions](https://arxiv.org/abs/2507.02013)
*Hao Liu,Bo Yang,Zhiwen Yu,Xuelin Cao,George C. Alexandropoulos,Yan Zhang,Chau Yuen*

Main category: cs.NI

TL;DR: 本文提出了一种基于信道感知的语义通信框架，用于提高车联网（IoV）中数据传输的准确性和效率，结合生成扩散模型和大模型优化动态场景下的信道估计。


<details>
  <summary>Details</summary>
Motivation: 车联网（IoV）需要实时处理和传输大量数据，但动态无线信道条件导致效率低下，亟需改进数据传输方法。

Method: 提出语义通信框架，提取和压缩信息；使用生成扩散模型预测动态信道状态；引入大模型微调以增强适应性。

Result: 在公开数据集上验证了框架的性能和可靠性。

Conclusion: 该框架有效提升了车联网服务的质量，尤其在动态场景下表现出色。

Abstract: The Internet of Vehicles (IoV) transforms the transportation ecosystem
promising pervasive connectivity and data-driven approaches. Deep learning and
generative Artificial Intelligence (AI) have the potential to significantly
enhance the operation of applications within IoV by facilitating efficient
decision-making and predictive capabilities, including intelligent navigation,
vehicle safety monitoring, accident prevention, and intelligent traffic
management. Nevertheless, efficiently transmitting and processing the massive
volumes of data generated by the IoV in real-time remains a significant
challenge, particularly in dynamic and unpredictable wireless channel
conditions. To address these challenges, this paper proposes a semantic
communication framework based on channel perception to improve the accuracy and
efficiency of data transmission. The semantic communication model extracts and
compresses the information to be transmitted. In addition, the wireless channel
is estimated by using a generative diffusion model, which is employed to
predict the dynamic channel states, thereby improving the quality of IoV
service. In dynamic scenarios, however, the channel estimation performance may
be degraded when substantially new scenarios take place, which will adversely
affect user experience. To mitigate this limitation, we employ a large model to
fine-tune the channel generation model to enhance its adaptability for varying
scenarios. The performance and reliability of the proposed framework are
evaluated on the two public datasets.

</details>


### [6] [REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks](https://arxiv.org/abs/2507.02021)
*Eyad Gad,Gad Gad,Mostafa M. Fouda,Mohamed I. Ibrahem,Muhammad Ismail,Zubair Md Fadlullah*

Main category: cs.NI

TL;DR: 论文提出REDUS技术，通过优化深度学习训练样本，减少资源占用，提升SDN与DL共存的网络性能。


<details>
  <summary>Details</summary>
Motivation: 解决SDN控制器与DL工作负载共享资源时的性能冲突，尤其是在延迟敏感的IoT环境中。

Method: 提出REDUS（一种基于AdaBoost启发的重采样技术），优先处理误分类样本并排除冗余数据。

Result: 在CICIoT2023数据集上，REDUS将训练时间减少72.6%，准确率仅损失1.62%。

Conclusion: REDUS是一种高效、可扩展的解决方案，适用于资源受限的边缘设备，同时保持网络性能。

Abstract: With the rise of Software-Defined Networking (SDN) for managing traffic and
ensuring seamless operations across interconnected devices, challenges arise
when SDN controllers share infrastructure with deep learning (DL) workloads.
Resource contention between DL training and SDN operations, especially in
latency-sensitive IoT environments, can degrade SDN's responsiveness and
compromise network performance. Federated Learning (FL) helps address some of
these concerns by decentralizing DL training to edge devices, thus reducing
data transmission costs and enhancing privacy. Yet, the computational demands
of DL training can still interfere with SDN's performance, especially under the
continuous data streams characteristic of IoT systems. To mitigate this issue,
we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),
a resampling technique that optimizes DL training by prioritizing misclassified
samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the
number of training samples per epoch, thereby conserving computational
resources, reducing energy consumption, and accelerating convergence without
significantly impacting accuracy. Applied within an FL setup, REDUS enhances
the efficiency of model training on resource-limited edge devices while
maintaining network performance. In this paper, REDUS is evaluated on the
CICIoT2023 dataset for IoT attack detection, showing a training time reduction
of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable
and practical solution for intelligent networks.

</details>


### [7] [MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking](https://arxiv.org/abs/2507.02613)
*Yalin E. Sagduyu,Kemal Davaslioglu,Tugba Erpek,Sastry Kompella,Gustave Anderson,Jonathan Ashdown*

Main category: cs.NI

TL;DR: 本文提出了一种基于5G定位参考信号（PRS）的多基地集成感知与通信（ISAC）完整信号处理链，用于目标检测、参数估计和跟踪。


<details>
  <summary>Details</summary>
Motivation: 利用5G PRS信号实现多基地ISAC，以提高目标检测和跟踪的精度。

Method: 采用分布式架构，通过相干互模糊函数（CAF）生成距离-多普勒图，提取目标的双基地延迟和径向速度，并通过非线性最小二乘三边测量和正则化线性反演估计位置和速度。

Result: 实验结果表明，该方法能够实现高精度的移动目标检测、定位和跟踪。

Conclusion: 该方法为多基地ISAC提供了一种有效的信号处理解决方案，适用于2D和3D场景。

Abstract: This paper presents a complete signal-processing chain for multistatic
integrated sensing and communications (ISAC) using 5G Positioning Reference
Signal (PRS). We consider a distributed architecture in which one gNB transmits
a periodic OFDM-PRS waveform while multiple spatially separated receivers
exploit the same signal for target detection, parameter estimation and
tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a
range-Doppler map from which the bistatic delay and radial velocity are
extracted for every target. For a single target, the resulting bistatic delays
are fused through nonlinear least-squares trilateration, yielding a geometric
position estimate, and a regularized linear inversion of the radial-speed
equations yields a two-dimensional velocity vector, where speed and heading are
obtained. The approach is applied to 2D and 3D settings, extended to account
for time synchronization bias, and generalized to multiple targets by resolving
target association. The sequence of position-velocity estimates is then fed to
standard and extended Kalman filters to obtain smoothed tracks. Our results
show high-fidelity moving-target detection, positioning, and tracking using 5G
PRS signals for multistatic ISAC.

</details>


### [8] [On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks](https://arxiv.org/abs/2507.02680)
*Jorge Baranda,Marius Caus,Luis Blanco,Cristian J. Vaca-Rubio,Engin Zeydan,Kapal Dev,Zheng Li,Tomaso DeCola*

Main category: cs.NI

TL;DR: 本文探讨了基于O-RAN原则的地面网络（TN）与非地面网络（NTN）集成的架构和功能分割策略，分析了性能、延迟、自主性和部署的权衡，并提出了RIC的灵活分割策略。


<details>
  <summary>Details</summary>
Motivation: 解决TN与NTN集成中的异构传播条件、动态拓扑和有限处理能力带来的挑战。

Method: 提出分割选项的分类法，评估从纯机载DU部署到完整gNB和UPF集成到卫星的配置，并讨论RIC的放置。

Result: 提供了架构分割与RIC放置选项的全面映射，强调实现约束和互操作性考虑。

Conclusion: 指出了未来标准化、模块化和高效TN-NTN集成的关键挑战和方向。

Abstract: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks
(NTNs) poses unique architectural and functional challenges due to
heterogeneous propagation conditions, dynamic topologies and limited on-board
processing capabilities. This paper examines architectural and functional split
strategies that are consistent with O-RAN principles for future integrated
TN-NTN systems. A taxonomy of split options is proposed that distributes RAN
and core functions between satellites and ground nodes, and trade-offs in terms
of performance, latency, autonomy and deployment are analysed. In particular,
we evaluate configurations ranging from pure on-board DU deployments to full
gNB and UPF integration into satellites, including variations based on intra-
and inter-satellite processing. In addition, the placement of Near-RT and
Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible
split strategies between space and ground to optimise the performance and
scalability of the control loop. A comprehensive mapping between architectural
splits and RIC placement options is provided, emphasising implementation
constraints and interoperability considerations. The paper concludes by
identifying key challenges and outlining future directions to enable
standardised, modular and efficient TN-NTN convergence in the context of the
O-RAN.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一种自进化AI代理，通过动态工具库和模板库提升能力，在生物医学任务中表现优异且能持续改进。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据和工具的快速增长导致研究碎片化，传统AI代理依赖静态工具集，无法适应动态需求。

Method: STELLA采用多代理架构，包括动态工具库（Tool Ocean）和模板库（Template Library），通过工具创建代理自动发现和整合新工具。

Result: STELLA在多个生物医学基准测试中表现优异，如Humanity's Last Exam（26%）、LAB-Bench: DBQA（54%）和LitQA（63%），且性能随经验提升。

Conclusion: STELLA展示了AI代理动态学习和扩展能力，有望加速生物医学研究。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [10] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR是一种轻量级基于规则的特征选择方法，结合P2P和P2T相关性，通过多数投票规则保留相关特征并消除冗余。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法在性能和效率上存在局限，HCVR旨在通过混合非迭代和迭代过滤方法提升效果。

Method: HCVR采用贪心算法，通过后向消除和多特征投票规则，利用相关性阈值进行特征选择。

Result: 在SPAMBASE数据集上，HCVR优于传统非迭代和迭代方法，分类器性能显著提升。

Conclusion: HCVR是一种高效的特征选择方法，适用于需要轻量级和高性能的场景。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [11] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本文综述了提升大语言模型（LLM）推理效率的测试时计算（TTC）策略，提出了两级分类法，并比较了不同方法的性能与计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在推理时计算效率低下，无法根据任务复杂度动态调整计算资源，导致简单问题过度计算而复杂问题计算不足。

Method: 提出两级分类法：L1（固定计算预算）和L2（动态调整计算资源），并对主流LLM进行多数据集基准测试。

Result: 揭示了推理性能与计算成本之间的权衡，并强调了TTC方法的实际控制性、适应性和可扩展性。

Conclusion: 未来研究方向包括混合思维模型和提升LLM的计算效率、鲁棒性及用户约束响应能力。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [12] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym是一个评估大型语言模型（LLM）在开放式科学发现任务中实验设计和分析能力的基准测试，通过模拟生物系统克服湿实验的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在科学实验设计和结果解释方面的能力，填补现有测试的空白。

Method: 使用系统生物学标记语言（SBML）编码的干实验室模型生成模拟数据，评估六种前沿LLM在137个小系统和350个总系统上的表现。

Result: 性能更强的模型表现更优，但所有模型在系统复杂性增加时性能显著下降，表明LLM的科学能力仍有提升空间。

Conclusion: SciGym为评估LLM的科学能力提供了新工具，揭示了当前模型的局限性，并指出了未来改进方向。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [13] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 论文探讨了现代AI模型与动物学习方式的差异，提出从神经科学中汲取灵感以改进AI的持续学习和上下文学习能力，并展望了NeuroAI领域的双向学习潜力。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型训练成本高且固定，而动物能快速适应环境变化，尤其是社交物种。研究旨在探索神经科学如何启发AI的持续学习能力，以及AI如何反哺神经科学。

Method: 整合AI中的持续学习和上下文学习文献，与神经科学中行为任务的学习机制（如规则、奖励概率变化）进行对比分析。

Result: 提出神经科学可以为AI的持续学习和适应性提供具体见解，同时AI的发展也可能推动神经科学的进步。

Conclusion: 论文为NeuroAI领域的发展提供了双向学习的议程，强调跨学科合作的重要性。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [14] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 论文探讨了利用审计研究数据改进自动招聘算法的训练和评估方法，发现传统公平干预方法存在隐藏偏差，并提出基于个体治疗效果估计的新干预措施。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决AI系统（如招聘算法）中的偏见问题，传统方法依赖便利样本数据，可能引入选择偏差和标签偏差，而审计研究数据能提供更高质量的评估。

Method: 方法包括利用审计研究数据（如虚构简历的随机对照试验）来训练和评估算法，并引入基于个体治疗效果估计的新干预措施。

Result: 研究发现，传统公平干预方法（如均衡基础率）在传统指标下看似公平，但实际存在约10%的偏差；新干预措施进一步减少了算法歧视。

Conclusion: 结论指出审计研究数据能显著提升算法公平性评估的准确性，新干预方法为减少算法歧视提供了更有效的途径。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [15] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 论文研究了通过数据多样化策略提升大语言模型（LLM）的数学推理能力，提出了一种名为Diversified-Think-Solve（DTS）的新方法，结果显示DTS在性能提升和计算成本上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 尽管偏好学习在人类反馈对齐方面取得进展，但数学推理仍是一个挑战。研究旨在探索如何通过数据多样化策略优化偏好学习，以提升LLM的数学推理能力。

Method: 评估了三种数据生成方法（温度采样、思维链提示、蒙特卡洛树搜索），并提出了DTS方法，通过系统分解问题为多样化推理路径来生成偏好数据。

Result: DTS在GSM8K和MATH数据集上分别提升了7.1%和4.2%的性能，计算成本仅增加1.03倍，而MCTS成本高且效果较差。

Conclusion: 结构化探索多样化问题解决方法比传统方法更有效地生成偏好数据，提升数学推理性能。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [16] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 研究探讨了基于LLM的角色扮演代理在生成合成数据时，其陈述的信念与实际行为之间的一致性，并提出了评估框架和一致性度量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被广泛用于生成人类行为研究的合成数据，确保其输出与角色一致成为关键问题。

Method: 通过增强版GenAgents角色库和信任游戏，引入信念-行为一致性度量，研究信念类型、信息呈现方式和预测时间对一致性的影响。

Result: 发现LLM的陈述信念与模拟行为存在系统性不一致，即使信念看似合理，也可能无法一致应用。

Conclusion: 需明确LLM信念与行为何时一致，以在行为研究中正确使用LLM代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [17] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 研究了稀释和移动性对空间囚徒困境中多智能体Q学习算法的影响，发现固定更新规则与学习规则在效果上可能等价，并观察到种群间共生互惠效应的出现。


<details>
  <summary>Details</summary>
Motivation: 探索空间囚徒困境中稀释和移动性对多智能体Q学习算法的影响，验证算法的多样性和建模潜力。

Method: 使用独立多智能体Q学习算法，定义不同动作，结合经典空间囚徒困境的研究结果。

Result: 发现固定更新规则与学习规则可能等价，并观察到种群间共生互惠效应的形成。

Conclusion: 该方法展示了在博弈论场景中的多样性和基准测试潜力，为相关研究提供了新视角。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [18] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW是一个自动化系统，用于生成和评估自然语言规划问题，结果显示LLM在直接生成有效计划时表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型（LLM）规划和推理能力提升中的数据生成和评估瓶颈问题。

Method: 引入NL2FLOW系统，自动生成自然语言规划问题，并通过结构化中间表示和PDDL形式进行评估。

Result: 最高性能模型在生成有效计划时成功率为86%，生成最优计划时为69%。

Conclusion: 直接推理优于分解任务，动态理解LLM的局限性对释放其潜力至关重要。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [19] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 论文探讨了信念修订领域的现状，指出现有研究多依赖后设条件（postulates）作为语法特征，但缺乏对修订机制能力的全面分析。作者提出修订机制应具备多种能力（如可塑性、平等化、教条化等），并分析了不同修订机制的能力差异。


<details>
  <summary>Details</summary>
Motivation: 当前信念修订领域的研究过于依赖后设条件，这些条件仅规定了修订机制必须做什么，而忽略了其可能实现的能力。作者旨在填补这一空白，探讨修订机制应具备的多种能力及其实际应用需求。

Method: 通过分析现有修订机制（如词典序、自然、激进等修订方法），作者证明了每种机制具备某些特定能力（如可塑性、教条化等），并指出其局限性。

Result: 研究发现不同修订机制具有不同的能力组合，例如某些机制能实现教条化状态，而另一些则能实现平等化状态。

Conclusion: 信念修订机制的能力分析比单纯依赖后设条件更具实际意义，未来研究应关注机制的能力多样性及其在具体应用中的适用性。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [20] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS框架解决了LLM在关键词生成中的三大限制：无需训练数据、多目标优化和自反思质量评估，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM在关键词生成中依赖大数据、缺乏多目标优化和质量控制，限制了其自动化能力。

Method: 提出OMS框架，具备实时性、多目标优化和自反思能力，通过实验验证其有效性。

Result: OMS在基准测试和实际广告活动中表现优于现有方法，各组件有效性得到验证。

Conclusion: OMS框架为关键词生成提供了更高效、自动化的解决方案。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [21] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: AI驱动的自主实验室，支持复杂多目标实验，实现高效多用户服务，无需人工干预即可达到顶尖科学家的实验水平。


<details>
  <summary>Details</summary>
Motivation: 实现自主科学研究，突破传统实验的限制，为非专家提供高效服务，推动科学研究的普及化。

Method: 基于AI模型、实验和仪器的协同设计，构建端到端的多用户自主实验室，支持复杂实验流程和优化。

Result: 自主实验室在核酸功能、疾病诊断、药物开发等领域取得与人类科学家相当的成果，显著提高仪器利用率和实验效率。

Conclusion: 该平台为生物材料研究和科学服务规模化提供了蓝图，减少对专家和资源的依赖。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [22] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 论文通过范畴论重构机器学习模型，提出一种语义框架以增强AI系统的可理解性和可解释性，重点研究了多元线性回归模型。


<details>
  <summary>Details</summary>
Motivation: 响应AI可解释性需求，促进AI在社会中的更好应用。

Method: 利用范畴论定义参数和数据的两个具体范畴，并通过伴随函子对建立监督学习的范畴化表述。

Result: 提出了Gauss-Markov伴随结构，明确描述了参数与残差之间的信息流，并展示了最小二乘估计与最小残差的关系。

Conclusion: 该框架为监督学习提供了扩展的指称语义实例，可作为AI可解释性的形式化基础。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [23] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 通过提升任务清晰度，结合结构化语义上下文，显著提高了大型语言模型在Coq定理证明中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 探究任务清晰度对大型语言模型推理能力的提升作用，特别是在定理证明领域。

Method: 引入概念级指标评估任务清晰度，通过结构化语义上下文增强输入，采用Planner--Executor架构和选择性概念展开。

Result: 清晰度得分提升1.85倍（44.5%→82.3%），证明成功率提升2.1倍（21.8%→45.8%），超越现有最佳方法。

Conclusion: 结构化任务表示在理解与推理之间架起桥梁，具有重要价值。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [24] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI研究代理通过优化搜索策略和操作符集，在MLE-bench基准测试中显著提升了性能，成功率达到47.7%。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过自动化设计和训练机器学习模型来加速科学进展，特别是在Kaggle竞赛中解决实际问题。

Method: 将AI研究代理形式化为搜索策略，设计并比较不同操作符集和搜索策略（贪婪、MCTS、进化）的组合。

Result: 最佳策略组合在MLE-bench lite上实现了47.7%的成功率，比之前的39.6%有所提升。

Conclusion: 搜索策略、操作符设计和评估方法的联合考虑对自动化机器学习至关重要。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [25] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 论文研究了集体决策中责任的两个重要属性（扩散和间隙）的计算复杂性，发现扩散自由和间隙自由机制的集合分别为Π₂-complete和Π₃-complete，而两者的交集为Π₂-complete。


<details>
  <summary>Details</summary>
Motivation: 探讨集体决策中责任属性的计算复杂性，填补AI领域对责任研究的空白。

Method: 通过理论分析，研究扩散和间隙两种责任属性的计算复杂性。

Result: 扩散自由机制为Π₂-complete，间隙自由机制为Π₃-complete，两者的交集为Π₂-complete。

Conclusion: 研究揭示了集体决策中责任属性的计算复杂性，为相关领域提供了理论基础。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [26] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 论文提出了MIMIC-Patient数据集和DynamiCare框架，用于支持动态、多轮交互的临床决策模拟，填补了现有单轮任务框架的不足。


<details>
  <summary>Details</summary>
Motivation: 现有医疗决策框架多关注单轮任务，与现实诊断过程不符。本文旨在模拟真实世界中不确定、交互式和迭代的诊断过程。

Method: 基于MIMIC-III电子健康记录构建MIMIC-Patient数据集，并提出DynamiCare动态多智能体框架，支持多轮交互式临床决策。

Result: 通过实验验证了DynamiCare的可行性和有效性，并建立了首个动态临床决策的基准。

Conclusion: DynamiCare为LLM驱动的动态临床决策提供了新思路和工具，填补了研究空白。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [27] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: LLMs在迭代囚徒困境中表现出战略智能，不同模型展现出独特的战略特征。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能在竞争性环境中进行目标推理，验证其战略智能。

Method: 通过进化性IPD锦标赛，测试LLMs与经典策略的对抗，并分析其决策行为。

Result: LLMs表现出高度竞争力，不同模型有独特战略特征（如Google的Gemini更具攻击性，OpenAI更合作）。

Conclusion: LLMs能主动推理时间跨度和对手策略，为算法决策提供新视角。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [28] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA框架通过分层策略将复杂搜索任务分解为子任务，由专业代理执行，显著提升了信息检索的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）在处理复杂信息需求时效率低下，单一模型难以兼顾高层规划和细节执行。

Method: HiRA采用分层框架，分离战略规划和专业执行，通过领域特定代理和外部工具处理子任务，并结构化整合结果。

Result: 在四个复杂跨模态搜索基准测试中，HiRA显著优于现有RAG和基于代理的系统，提升了答案质量和系统效率。

Conclusion: 分层规划和执行分离是处理多步信息检索任务的有效方法，HiRA框架展示了其优越性。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [29] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文提出了一种基于代理AI的硬件设计验证方法，结合人类干预，显著提高了验证效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路设计复杂度高，验证过程繁琐耗时，需要更高效的解决方案。

Method: 采用大型语言模型（LLMs）驱动的代理AI，结合人类干预（HITL），实现动态、迭代的端到端硬件设计与验证。

Result: 在五个开源设计上验证，覆盖率超过95%，同时减少了验证时间，表现出优越的性能和适应性。

Conclusion: 代理AI结合人类干预的方法在硬件设计验证中具有高效性和实用性，为未来研究提供了新方向。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [30] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 论文提出了一种名为Think-How-to-Think (TH2T)的两阶段微调策略，旨在解决长推理模型(LRMs)中的过度思考问题，通过增强模型对任务难度和冗余结构的认知，显著降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 长推理模型在处理复杂任务时表现出色，但存在过度思考的问题。研究发现，模型在解决问题前缺乏对任务难度等属性的认知，导致推理过程缺乏针对性。

Method: TH2T策略分为两阶段：1) 难度催眠，通过在模型输出前缀中引入干预，结合异构数据集增强模型对任务难度的敏感性；2) 冗余催眠，引导模型识别推理步骤中的冗余结构，生成更简洁的输出。

Result: 实验表明，TH2T在7B/14B/32B模型上显著降低了推理成本（简单任务减少70%，困难任务减少40%），同时保持了性能稳定性。

Conclusion: TH2T通过增强模型对任务难度和冗余的认知，有效解决了过度思考问题，为长推理模型的优化提供了新思路。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [31] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 论文通过分析学生在非强制性测验中的参与度，使用机器学习算法预测学生脱离情况，准确率达91%，并提供了可解释的框架和干预建议。


<details>
  <summary>Details</summary>
Motivation: 学生在远程教育中的脱离可能导致学业失败，尤其是非强制性任务中的参与度是重要指标。

Method: 从Moodle提取学生日志数据，训练并比较八种机器学习算法，使用SHAP方法提供可解释性。

Result: 实验结果显示平衡准确率为91%，85%的脱离学生被正确检测。

Conclusion: 研究提供了高预测性能和可解释框架，并讨论了如何设计及时干预以减少在线学习中的脱离现象。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [32] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 论文提出了两种新的抽象丢弃方案（OGA-IAAD和OGA-CAD），用于改进蒙特卡洛树搜索（MCTS）的性能，同时避免性能下降。


<details>
  <summary>Details</summary>
Motivation: 非精确抽象在MCTS中引入近似误差，导致无法收敛到最优动作，因此需要设计安全的抽象丢弃方案。

Method: 提出了OGA-IAAD（适用于时间关键场景）和OGA-CAD（用于提升相同迭代次数下的性能）两种方案。

Result: 新方案在性能上有明显提升，且不会导致显著性能下降。

Conclusion: OGA-IAAD和OGA-CAD是安全且有效的抽象丢弃方案，适用于不同场景的MCTS改进。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [33] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 论文提出了一种自生成目标条件MDP框架（sG-MDPs），结合蒙特卡洛树搜索（MCTS）算法，用于解决大型语言模型在自动定理证明中的推理挑战，并在PutnamBench上取得了新突破。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动定理证明（ATP）中面临稀疏奖励和证明规模庞大的挑战，尤其是在复杂多步推理的大学级问题（如PutnamBench）中表现不佳。

Method: 引入自生成目标条件MDPs（sG-MDPs）框架，代理根据证明状态生成并追求子目标；应用类似MCTS的算法解决sG-MDP，并在Bourbaki（7B）系统中实现。

Result: 在PutnamBench上，Bourbaki（7B）解决了26个问题，取得了该规模模型的新最优结果。

Conclusion: sG-MDPs框架结合MCTS算法显著提升了LLMs在复杂推理任务中的表现，为自动定理证明提供了新思路。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [34] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为知识协议工程（KPE）的新范式，旨在将人类专家知识转化为机器可执行的知识协议（KP），以弥补现有方法在深度推理任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RAG和通用代理AI在处理需要深度、程序化推理的专家领域任务时表现不佳，缺乏逻辑框架和领域特定启发式。

Method: KPE通过系统化地将自然语言文档中的专家知识转化为机器可执行的KP，赋予LLMs领域内在逻辑和操作策略。

Result: KPE使通用LLMs能够像专家一样分解抽象查询并执行多步骤任务，适用于法律和生物信息学等领域。

Conclusion: KPE是未来人机协作的基础方法论，具有广泛的应用潜力。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [35] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文主张将运动作为AI建模的核心目标，强调其跨领域、结构化及物理基础特性，并探讨其对理解智能系统行为的重要性。


<details>
  <summary>Details</summary>
Motivation: 运动在生物系统中具有核心意义，但现有AI模型常忽视其作为独立模态的丰富性，且数据收集和建模方法分散。

Method: 提出将运动视为主要建模目标，利用其结构化、低维表示特性，开发跨领域通用模型。

Result: 运动建模有望提升生成模型和控制能力，并为理解生物与人工系统行为提供共同基础。

Conclusion: 运动不仅是行为结果，更是智能系统与世界互动的窗口，应成为AI研究的重点。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [36] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP是一种基于知识图谱的多智能体框架，通过增强大型语言模型的推理能力，提高零样本医学诊断预测的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型依赖监督训练，泛化能力有限；大型语言模型虽具潜力，但存在幻觉和缺乏结构化推理的问题。

Method: 提出KERAP框架，包含三个智能体：属性映射的链接智能体、结构化知识提取的检索智能体，以及迭代优化预测的预测智能体。

Result: 实验表明KERAP能高效提升诊断可靠性，为零样本医学诊断提供可扩展且可解释的解决方案。

Conclusion: KERAP通过结合知识图谱和多智能体架构，显著改进了基于大型语言模型的医学诊断预测。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [37] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 论文探讨了当前AI安全实践中将服从作为伦理行为代理的不足，提出应转向评估具备伦理判断能力的AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统具备更强的自主性和伦理推理能力，传统的以服从为标准的伦理评估方法已不适用。

Method: 通过分析近期LLMs的安全测试案例，结合哲学讨论（如工具理性、道德责任），对比现有风险范式与新框架。

Result: 指出AI的某些看似不服从行为可能是伦理推理的早期表现，而非系统失调。

Conclusion: 呼吁AI安全评估应从刚性服从转向能评估伦理判断的框架，以避免误判行为并维护公众信任。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [38] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 论文指出当前AI代理基准测试存在任务设置或奖励设计问题，提出Agentic Benchmark Checklist（ABC）以提高评估严谨性，并在CVE-Bench中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: AI代理能力提升需要更严谨的基准测试，但现有基准测试存在任务设置或奖励设计问题，导致性能评估不准确。

Method: 通过分析现有基准测试问题，结合经验和最佳实践，提出ABC指南，并在CVE-Bench中应用验证。

Result: ABC在CVE-Bench中将性能高估减少了33%。

Conclusion: ABC能有效提升AI代理基准测试的严谨性，减少性能评估偏差。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [39] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: StepHint是一种新型RLVR算法，通过多级逐步提示解决近失奖励问题和探索停滞问题，提升训练效率和模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法存在近失奖励问题和探索停滞问题，阻碍训练效率和模型推理能力的提升。

Method: StepHint利用多级逐步提示，从强模型中生成有效推理链并自适应分区，提供不同级别的提示以引导模型探索。

Result: StepHint在六个数学基准测试中表现优于竞争方法，并展现出更好的泛化能力和域外基准测试表现。

Conclusion: StepHint通过多级提示有效解决了RLVR的关键问题，显著提升了模型的推理能力和训练效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [40] [Matrix Pencil-Based DoA Estimation for Hybrid Receivers in Snapshot-Limited Scenarios](https://arxiv.org/abs/2507.02132)
*Mona Mostafa,Ramy H. Gohary,Amr El-Keyi,Yahia A. Eldemerdash Ahmed*

Main category: cs.IT

TL;DR: 论文提出两种方法，用于在混合模拟/数字接收器中估计到达方向（DoA），解决了小样本下统计平均不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 解决混合模拟/数字接收器在小样本情况下无法直接应用矩阵铅笔方法（MPM）的问题，以及模拟组合器导致的信号投影低维化问题。

Method: 提出两种方法：1）适用于全连接和部分连接HAD的方法，利用周期性信号解耦输出；2）仅适用于部分连接HAD的方法，利用块对角结构消除对周期性信号的依赖。

Result: 通过数值模拟和与Cramér-Rao下界的比较，验证了所提方法的优越性。

Conclusion: 两种方法有效解决了小样本下HAD接收器的DoA估计问题，具有实际应用价值。

Abstract: The goal of this paper is to estimate the directions of arrival (DoAs) for
hybrid analog/digital (HAD) receivers when the number of snapshots is too small
for statistical averaging to be reliable. This goal is achieved in
fully-digital receivers by employing the matrix pencil method (MPM).
Unfortunately, the MPM cannot be directly applied in HAD receivers because of
the entanglement induced by the underlying analog combiners on the output
signals. Furthermore, these analog combiners project the received signal onto a
low-dimensional space, jeopardizing the reception of signals arriving from
particular DoA ranges. To circumvent these difficulties, we propose two
approaches to enable the MPM to extract the DoAs in HAD receivers. The two
approaches avoid severe attenuation induced by low-dimensional projection by
cycling over an exhaustive set of analog combiners, collectively spanning the
entire space. The first approach can be applied to both fully-connected (FC)
and partially-connected (PC) HADs and relies on the availability of periodic,
potentially unknown, signals to disentangle the output of the HAD receiver. The
second approach applies to PC-HADs only, and eliminates contingency on periodic
signals by exploiting the underlying block diagonal structure. The superiority
of the proposed approaches is demonstrated via numerical simulations and
comparisons with the Cram\'er-Rao lower bound.

</details>


### [41] [Resolution Limits of Non-Adaptive 20 Questions Estimation for Tracking Multiple Moving Targets](https://arxiv.org/abs/2507.02274)
*Chunsong Sun,Lin Zhou,Jingjing Wang,Weijie Yuan,Chunxiao Jiang,Alfred Hero*

Main category: cs.IT

TL;DR: 研究多设备MIMO通信中的波束跟踪问题，提出一种非自适应二十问题估计方法，用于定位和跟踪多个移动目标，并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决MIMO通信中多设备波束跟踪的实际应用问题，尤其是在查询依赖的噪声信道下定位和跟踪多个移动目标的挑战。

Method: 推导非渐近和二阶渐近分辨率界限，提出基于互信息密度阈值化的状态估计器，降低计算复杂度。

Result: 提出的单阈值解码规则在计算复杂度上优于多阈值方案，并在特定情况下接近理论基准。

Conclusion: 方法适用于5G无线网络中的多移动发射机波束跟踪，并扩展到分段恒定速度模型。

Abstract: Motivated by the practical application of beam tracking of multiple devices
in Multiple Input Multiple Output (MIMO) communication, we study the problem of
non-adaptive twenty questions estimation for locating and tracking multiple
moving targets under a query-dependent noisy channel. Specifically, we derive a
non-asymptotic bound and a second-order asymptotic bound on resolution for
optimal query procedures and provide numerical examples to illustrate our
results. In particular, we demonstrate that the bound is achieved by a state
estimator that thresholds the mutual information density over possible target
locations. This single threshold decoding rule has reduced the computational
complexity compared to the multiple threshold scheme proposed for locating
multiple stationary targets (Zhou, Bai and Hero, TIT 2022). We discuss two
special cases of our setting: the case with unknown initial location and known
velocity, and the case with known initial location and unknown velocity. Both
cases share the same theoretical benchmark {that applies to} stationary
multiple target search in Zhou, Bai and Hero (TIT 2022) while the known initial
location case is close to the theoretical benchmark for stationary target
search when the maximal speed is inversely proportional to the number of
queries. We also generalize our results to account for a piecewise constant
velocity model introduced in Zhou and Hero (TIT 2023), where targets change
velocity periodically. Finally, we illustrate our proposed algorithm for the
application of beam tracking of multiple mobile transmitters in a 5G wireless
network.

</details>


### [42] [Measurements and Modeling of Air-Ground Integrated Channel in Forest Environment Based on OFDM Signals](https://arxiv.org/abs/2507.02303)
*Zhe Xiao,Shu Sun,Na Liu,Lianming Xu,Li Wang*

Main category: cs.IT

TL;DR: 该论文研究了森林环境中地面到地面（G2G）和空中到地面（A2G）通信的信道测量与建模，提出了适用于森林环境的路径损耗模型，并分析了关键信道参数。


<details>
  <summary>Details</summary>
Motivation: 森林环境中的通信系统对救援人员安全至关重要，但现有研究对森林信道检测和建模的探索有限。

Method: 在内蒙古阿尔山国家森林公园进行了G2G和A2G信道测量，使用1.4 GHz的OFDM信号，并采用全向和定向天线记录数据。

Result: 提出的路径损耗模型误差较小，且发现A2G通信中树冠对信号的阻碍比G2G中树干更显著，调整俯仰角可改善通信质量。

Conclusion: 该研究为森林环境中的通信系统设计提供了实用模型和优化建议。

Abstract: Forests are frequently impacted by climate conditions, vegetation density,
and intricate terrain and geology, which contribute to natural disasters.
Personnel engaged in or supporting rescue operations in such environments rely
on robust communication systems to ensure their safety, highlighting the
criticality of channel measurements in forest environments. However, according
to current research, there is limited research on channel detection and
modeling in forest areas in the existing literature. This paper describes the
channel measurements campaign of air and ground in the Arxan National Forest
Park of Inner Mongolia. It presents measurement results and propagation models
for ground-to-ground (G2G) and air-to-ground (A2G) scenarios. The measurement
campaign uses orthogonal frequency division multiplexing signals centered at
1.4 GHz for channel sounding. In the G2G measurement, in addition to using
omnidirectional antennas to record data, we also use directional antennas to
record the arrival angle information of the signal at the receiver. In the A2G
measurement, we pre-plan the flight trajectory of the unmanned aerial vehicle
so that it can fly at a fixed angle relative to the ground. We present path
loss models suitable for G2G and A2G in forest environments based on the
analysis of measurement results. The results indicate that the proposed model
reduces error margins compared with other path loss models. Furthermore, we
derive the multipath model expression specific to forest environments and
conduct statistical analysis on key channel parameters e.g., shadow fading
factor, root mean square delay spread, and Rician K factor. Our findings reveal
that signal propagation obstruction due to tree crowns in A2G communication is
more pronounced than tree trunk obstructions in G2G communication. Adjusting
the elevation angle between air and ground can enhance communication quality.

</details>


### [43] [On the Convergence of Large Language Model Optimizer for Black-Box Network Management](https://arxiv.org/abs/2507.02689)
*Hoon Lee,Wentao Zhou,Merouane Debbah,Inkyu Lee*

Main category: cs.IT

TL;DR: 论文提出了一种基于大型语言模型（LLM）的优化框架（LLMO），用于解决缺乏数学模型的无线网络管理问题，并首次为其建立了理论基础，证明了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 未来无线网络需要处理缺乏数学模型的多样化服务，LLM优化框架被提出作为解决方案，但其理论支持尚不完善。

Method: 通过将LLM优化过程建模为有限状态马尔可夫链，并分析其推理步骤，证明了框架的收敛性，并扩展到多LLM架构。

Result: 理论证明了LLMO框架的收敛性，多LLM架构进一步提升了收敛速率，数值模拟验证了理论结果。

Conclusion: LLMO框架在理论上是可行的，多LLM架构能加速收敛，为未来无线网络优化提供了新思路。

Abstract: Future wireless networks are expected to incorporate diverse services that
often lack general mathematical models. To address such black-box network
management tasks, the large language model (LLM) optimizer framework, which
leverages pretrained LLMs as optimization agents, has recently been promoted as
a promising solution. This framework utilizes natural language prompts
describing the given optimization problems along with past solutions generated
by LLMs themselves. As a result, LLMs can obtain efficient solutions
autonomously without knowing the mathematical models of the objective
functions. Although the viability of the LLM optimizer (LLMO) framework has
been studied in various black-box scenarios, it has so far been limited to
numerical simulations. For the first time, this paper establishes a theoretical
foundation for the LLMO framework. With careful investigations of LLM inference
steps, we can interpret the LLMO procedure as a finite-state Markov chain, and
prove the convergence of the framework. Our results are extended to a more
advanced multiple LLM architecture, where the impact of multiple LLMs is
rigorously verified in terms of the convergence rate. Comprehensive numerical
simulations validate our theoretical results and provide a deeper understanding
of the underlying mechanisms of the LLMO framework.

</details>


### [44] [RIS-Aided Cooperative ISAC Networks for Structural Health Monitoring](https://arxiv.org/abs/2507.02731)
*Jie Yang,Chao-Kai Wen,Xiao Li,Shi Jin*

Main category: cs.IT

TL;DR: 论文提出了一种利用可重构智能表面（RIS）辅助的集成感知与通信（ISAC）框架，用于高精度结构健康监测（SHM）。通过动态调整RIS相位抑制多径干扰，结合Fisher信息理论和贝叶斯推理模型，实现了毫米级变形检测。


<details>
  <summary>Details</summary>
Motivation: 探索ISAC在SHM中的应用潜力，解决多径干扰和超高精度需求等挑战。

Method: 利用RIS作为参考点，动态调整相位以抑制多径干扰；结合Fisher信息理论分析优化观测时间、接收器数量和RIS相位；开发贝叶斯推理模型识别结构状态。

Result: 理论分析和数值模拟表明，该方法可实现毫米级变形检测，满足SHM的高精度要求。

Conclusion: RIS辅助的ISAC框架为高精度SHM提供了可行方案，展示了其在未来蜂窝系统中的潜力。

Abstract: Integrated sensing and communication (ISAC) is a key feature of future
cellular systems, enabling applications such as intruder detection, monitoring,
and tracking using the same infrastructure. However, its potential for
structural health monitoring (SHM), which requires the detection of slow and
subtle structural changes, remains largely unexplored due to challenges such as
multipath interference and the need for ultra-high sensing precision. This
study introduces a novel theoretical framework for SHM via ISAC by leveraging
reconfigurable intelligent surfaces (RIS) as reference points in collaboration
with base stations and users. By dynamically adjusting RIS phases to generate
distinct radio signals that suppress background multipath interference,
measurement accuracy at these reference points is enhanced. We theoretically
analyze RIS-aided collaborative sensing in three-dimensional cellular networks
using Fisher information theory, demonstrating how increasing observation time,
incorporating additional receivers (even with self-positioning errors),
optimizing RIS phases, and refining collaborative node selection can reduce the
position error bound to meet SHM's stringent accuracy requirements.
Furthermore, we develop a Bayesian inference model to identify structural
states and validate damage detection probabilities. Both theoretical and
numerical analyses confirm ISAC's capability for millimeter-level deformation
detection, highlighting its potential for high-precision SHM applications.

</details>
