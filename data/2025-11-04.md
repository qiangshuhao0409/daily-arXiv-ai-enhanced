<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 16]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.IT](#cs.IT) [Total: 21]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Towards Sub-millisecond Latency and Guaranteed Bit Rates in 5G User Plane](https://arxiv.org/abs/2511.00196)
*Leonardo Alberro,Noura Limam,Raouf Boutaba*

Main category: cs.NI

TL;DR: 提出了一种基于可编程硬件的数据平面模型，支持3GPP QoS标准，在商用Tofino交换机上实现微秒级延迟和接近零丢包率。


<details>
  <summary>Details</summary>
Motivation: 5G及未来网络需要严格的QoS保证，但传统固定功能基础设施无法支持3GPP标准化的多样化动态QoS配置，传输层成为端到端QoS合规的关键使能器。

Method: 设计可编程传输网络的QoS感知数据平面模型，支持所有3GPP QoS资源类型，集成每流计量、分类、严格优先级调度和延迟感知队列，使用P4在Intel Tofino交换机上实现。

Result: 实验结果表明，该模型确保每流带宽保证，延迟关键流量的亚毫秒延迟，在拥塞下的弹性，实现微秒级延迟和关键任务流量的接近零丢包。

Conclusion: 该模型验证了其适用于5G及未来QoS敏感应用的可行性，为下一代服务提供可预测行为和细粒度服务差异化。

Abstract: Next-generation services demand stringent Quality of Service (QoS)
guarantees, such as per-flow bandwidth assurance, ultra-low latency, and
traffic prioritization, posing significant challenges to 5G and beyond
networks. As 5G network functions increasingly migrate to edge and central
clouds, the transport layer becomes a critical enabler of end-to-end QoS
compliance. However, traditional fixed-function infrastructure lacks the
flexibility to support the diverse and dynamic QoS profiles standardized by
3GPP.
  This paper presents a QoS-aware data plane model for programmable transport
networks, designed to provide predictable behavior and fine-grained service
differentiation. The model supports all 3GPP QoS resource types and integrates
per-flow metering, classification, strict priority scheduling, and delay-aware
queuing. Implemented on off-the-shelf programmable hardware using P4 and
evaluated on an Intel Tofino switch, our approach ensures per-flow bandwidth
guarantees, sub-millisecond delay for delay-critical traffic, and resilience
under congestion. Experimental results demonstrate that the model achieves
microsecond-level latencies and near-zero packet loss for mission-critical
flows, validating its suitability for future QoS-sensitive applications in 5G
and beyond.

</details>


### [2] [Toward Hybrid COTS-based LiFi/WiFi Networks with QoS Requirements in Mobile Environments](https://arxiv.org/abs/2511.00210)
*Emilio Ancillotti,Loreto Pescosolido,Andrea Passarella*

Main category: cs.NI

TL;DR: 提出两种基于信号功率和CRC包失败率的垂直切换机制，在LiFi/WiFi混合网络中实现低于1秒的QoS中断，相比基准方案的数秒中断有显著改善。


<details>
  <summary>Details</summary>
Motivation: 在移动场景下，当LiFi服务质量不足时，需要比标准机制更早触发垂直切换到WiFi，以避免QoS中断。

Method: 基于信号功率水平读取和CRC包失败率两种机制，在实验室测试平台上使用传送带模拟设备移动场景进行实验评估。

Result: 提出的方法在20 Mbps QoS水平下实现低于1秒的QoS中断，而基准解决方案的中断时间为数秒。

Conclusion: 所提出的机制能有效减少QoS中断时间，并揭示了这些机制与LiFi协议信道自适应能力之间的相互作用。

Abstract: We consider a hybrid LiFi/WiFi network consisting of commercially available
equipment, for mobile scenarios, where WiFi backs up communications, through
vertical handovers, in case of insufficient LiFi QoS. When QoS requirements in
terms of goodput are defined, tools are needed to anticipate the vertical
handover relative to what is possible with standard basic mechanisms, which are
only based on a complete loss of connectivity. We introduce two such
mechanisms, based on signal power level readings and CRC-based packet failure
ratio, and evaluate their performance in terms of QoS-outage duration,
considering as a benchmark an existing baseline solution based on the detection
of a connectivity loss. In doing this, we provide insights into the interplay
between such mechanisms and the LiFi protocol channel adaptation capabilities.
Our experimental results are obtained using a lab-scale testbed equipped with a
conveyor belt, which allows us to accurately replicate experiments with devices
in motion. With the proposed methods, we achieve QoS outages below one second
for a QoS level of 20 Mbps, compared to outage durations of a few seconds
obtained with the baseline solution.

</details>


### [3] [Mist-Assisted Federated Learning for Intrusion Detection in Heterogeneous IoT Networks](https://arxiv.org/abs/2511.00271)
*Saadat Izadi,Shakib Komasi,Ali Salimi,Alireza Rezaei,Mahmood Ahmadi*

Main category: cs.NI

TL;DR: 提出了一种雾辅助的分层联邦学习框架用于物联网入侵检测，在TON-IoT数据集上达到98-99%准确率，解决了数据异构性和非IID分布问题。


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源有限且数据异构，传统联邦学习在非IID数据分布下表现不佳，需要新的隐私保护入侵检测方案。

Method: 四层架构：雾层（数据抽象和轻量异常检测）、边缘层（基于效用的客户端选择）、雾层（多区域聚合器使用FedProx）、云层（全局模型整合分发）。

Result: 在TON-IoT数据集上实现98-99%准确率，PR-AUC>0.97，在异构和大规模环境下保持稳定收敛。

Conclusion: 该分层框架能有效处理物联网环境中的数据异构性和非IID分布问题，同时保持高效性和隐私保护。

Abstract: The rapid growth of the Internet of Things (IoT) offers new opportunities but
also expands the attack surface of distributed, resource-limited devices.
Intrusion detection in such environments is difficult due to data heterogeneity
from diverse sensing modalities and the non-IID distribution of samples across
clients. Federated Learning (FL) provides a privacy-preserving alternative to
centralized training, yet conventional frameworks struggle under these
conditions. To address this, we propose a Mist-assisted hierarchical framework
for IoT intrusion detection. The architecture spans four layers: (i) Mist,
where raw data are abstracted into a unified feature space and lightweight
models detect anomalies; (ii) Edge, which applies utility-based client
selection; (iii) Fog, where multiple regional aggregators use FedProx to
stabilize training; and (iv) Cloud, which consolidates and disseminates global
models. Evaluations on the TON-IoT dataset show the framework achieves 98-99%
accuracy, PR-AUC> 0.97, and stable convergence under heterogeneous and
large-scale settings, while maintaining efficiency and preserving privacy.

</details>


### [4] [Reinforcement Learning for Resource Allocation in Vehicular Multi-Fog Computing](https://arxiv.org/abs/2511.00276)
*Mohammad Hadi Akbarzadeh,Mahmood Ahmadi,Mohammad Saeed Jahangiry,Jae Young Hur*

Main category: cs.NI

TL;DR: 该论文研究在多雾计算环境中使用强化学习进行资源分配，以解决动态车辆移动性、异构资源和波动工作负载带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备、智能车辆和延迟敏感应用的指数增长，对高效分布式计算范式的需求日益迫切。多雾计算作为雾计算和边缘计算的扩展，虽然能减少延迟、增强可扩展性并确保服务质量，但其资源分配面临动态车辆移动性、异构资源和波动工作负载的挑战，传统优化方法难以适应这种动态性。

Method: 将多雾计算环境中的资源分配问题建模为马尔可夫决策过程，并研究应用强化学习算法，包括Q-learning、深度Q网络和Actor-Critic方法。

Result: 实验结果表明，所提出的方法在延迟、工作负载平衡和任务成功率方面都有所改善。

Conclusion: 该研究突出了强化学习在解决新兴车辆计算挑战中的作用，并讨论了研究的贡献和新颖性。

Abstract: The exponential growth of Internet of Things (IoT) devices, smart vehicles,
and latency-sensitive applications has created an urgent demand for efficient
distributed computing paradigms. Multi-Fog Computing (MFC), as an extension of
fog and edge computing, deploys multiple fog nodes near end users to reduce
latency, enhance scalability, and ensure Quality of Service (QoS). However,
resource allocation in MFC environments is highly challenging due to dynamic
vehicular mobility, heterogeneous resources, and fluctuating workloads.
Traditional optimization-based methods often fail to adapt to such dynamics.
Reinforcement Learning (RL), as a model-free decision-making framework, enables
adaptive task allocation by continuously interacting with the environment. This
paper formulates the resource allocation problem in MFC as a Markov Decision
Process (MDP) and investigates the application of RL algorithms such as
Q-learning, Deep Q-Networks (DQN), and Actor-Critic. We present experimental
results demonstrating improvements in latency, workload balance, and task
success rate. The contributions and novelty of this study are also discussed,
highlighting the role of RL in addressing emerging vehicular computing
challenges.

</details>


### [5] [COHERE - Congestion-aware Offloading and Handover via Empirical RAT Evaluation for Multi-RAT Networks](https://arxiv.org/abs/2511.00439)
*Pavan K. Mangipudi,Sharon Boamah,Lorenz Carvajal,Janise Mcnair*

Main category: cs.NI

TL;DR: 提出了COHERE框架，通过多标准决策和TOPSIS方法在密集多RAT网络中实现智能切换和卸载，显著降低拥塞RAT负载和切换次数，改善链路延迟。


<details>
  <summary>Details</summary>
Motivation: 传统移动性决策主要依赖RSSI，忽略了RAT特定特性、拥塞、排队延迟和应用需求，倾向于高功率链路而非最优链路。多RAT共存为智能接入、移动性和路由策略创造了机会。

Method: 提出COHERE框架，在RSSI基础上增强多个标准，应用TOPSIS方法对可用RAT进行排名。使用主观（运营商驱动）和客观（基于测量）方法确定标准权重，基于排名执行智能跨RAT卸载。

Result: 在密集SDN控制的5G/WiFi多RAT环境中评估，相比仅使用RSSI的切换，COHERE将拥塞RAT负载降低32%，总切换次数减少25%，向拥塞RAT的切换降低55%，链路延迟改善166%，同时保持相当或高达11%的吞吐量提升。

Conclusion: 有保障的多标准决策可以利用RAT共存，在异构部署中提供稳健的拥塞感知性能。

Abstract: The evolution of wireless networks and radio access technologies (RATs) has
transformed communication from user-driven traffic into a dynamic ecosystem of
autonomous systems, including IoT devices, edge nodes, autonomous vehicles,
AR/XR clients, and AI-powered agents. These systems exhibit diverse traffic
patterns, latency requirements, and mobility behaviors, increasingly operating
across overlapping heterogeneous RATs such as 5G, WiFi, satellite, NB-IoT,
LoRaWAN, Zigbee, etc. This multi-RAT coexistence creates opportunities for
intelligent access, mobility, and routing strategies. However, most mobility
decisions still rely heavily on RSSI, which neglects RAT-specific features,
congestion, queuing delays, and application needs, favoring high-power links
over optimal ones. To address this gap, we propose chrome (Congestion-aware
Offloading and Handover via Empirical RAT Evaluation), a multi criteria
framework for dense multi-RAT networks. chrome enhances RSSI with multiple
criteria and applies the Technique for Order of Preference by Similarity to the
Ideal Solution (TOPSIS) to rank available RATs. Criteria weights are determined
using both subjective (operator-driven) and objective (measurement-based)
approaches. Based on this ranking, chrome performs intelligent cross-RAT
offloading to reduce congestion on over-utilized links. We evaluate chrome in a
dense SDN-controlled 5G/WiFi Multi-RAT environment using Mininet WiFi. Compared
to RSSI-only handover, COHERE reduces the load on the congested RAT by up to
32%, reduces total handovers by 25%, lowers handovers to the congested RAT by
55%, and improves link delay by up to 166%, while maintaining comparable or up
to 11% higher throughput. These results demonstrate that guarded,
multi-criteria decision-making can exploit RAT coexistence to deliver robust,
congestion-aware performance across heterogeneous deployments.

</details>


### [6] [Impact of Antenna Arrays Misalignment on the Near Field Distance in Terahertz Communications](https://arxiv.org/abs/2511.00502)
*Peng Zhang,Vitaly Petrov,Emil Björnson*

Main category: cs.NI

TL;DR: 本文分析了太赫兹通信中空间错位对近场边界计算的影响，推导了ULA和UPA配置下任意错位偏移的近场边界精确解析表达式和简化近似，并通过数值模拟验证了理论模型。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信的极短波长导致扩展的辐射近场区域，现有近场边界公式假设收发器理想对齐，忽略了由移动性或机械缺陷引起的实际错位问题。

Method: 推导了ULA-ULA和UPA-UPA配置下任意错位偏移的近场边界精确解析表达式和简化近似，并通过数值模拟验证理论模型。

Result: 验证了理论模型并量化了错位如何重塑近场区域，为实际场景中太赫兹系统部署优化提供了重要指导。

Conclusion: 空间错位显著影响太赫兹系统的近场边界计算，研究成果为实际部署提供了关键指导原则。

Abstract: The extremely short wavelength of terahertz (THz) communications leads to an
extended radiative near-field region, in which some canonical far-field
assumptions fail. Existing near-field boundary formulations (Fraunhofer
distance) for uniform linear/planar array (ULA/UPA) configurations assume ideal
alignment between transceivers, overlooking practical misalignments caused by
mobility or mechanical imperfections. This paper addresses this critical gap by
analyzing the impact of spatial misalignment on near-field distance
calculations in THz systems. We derive exact analytical expressions and
simplified approximations for the near-field boundary in both ULA--ULA and
UPA--UPA configurations under arbitrary misalignment offsets. Through numerical
simulations, we validate our theoretical models and quantify how misalignment
reshapes the near-field region. These findings provide essential guidelines for
optimizing THz system deployment in realistic scenarios.

</details>


### [7] [Advancing Fluid Antenna-Assisted Non-Terrestrial Networks in 6G and Beyond: Fundamentals, State of the Art, and Future Directions](https://arxiv.org/abs/2511.00569)
*Tianheng Xu,Runke Fan,Jie Zhu,Pei Peng,Xianfu Chen,Qingqing Wu,Ming Jiang,Celimuge Wu,Dusit Niyato,Kai-Kit Wong*

Main category: cs.NI

TL;DR: 本文全面综述了流体天线辅助的非地面网络，探讨了其结构、优化方案、与其他新兴技术的结合，以及物理层安全和隐蔽通信等关键问题。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络对超可靠、低延迟和无处不在连接的需求激增，非地面网络作为地面网络的关键补充，提供了灵活的接入和全球覆盖。然而，非地面网络仍面临动态传播环境、能量限制和密集干扰等关键挑战。流体天线作为6G关键技术，能够通过重新配置辐射元件来重塑无线信道，提供更高的信道多样性和复用增益。

Method: 本文首先概述了现有非地面网络的经典结构和局限性、流体天线的基本原理和优势，以及流体天线辅助非地面网络的基本原理。然后研究了联合优化解决方案，详细说明了流体天线配置、非地面网络平台运动模式和资源分配的调整。还讨论了与其他新兴技术的结合，并探索了流体天线辅助非地面网络作为智能功能集成的新型网络架构。

Result: 流体天线辅助非地面网络能够有效缓解动态信道衰落并优化资源分配，为6G网络提供了有前景的集成路径。

Conclusion: 流体天线辅助非地面网络具有广阔的应用前景，未来需要进一步探索其在不同场景下的应用潜力，并解决相关的技术挑战。

Abstract: With the surging demand for ultra-reliable, low-latency, and ubiquitous
connectivity in Sixth-Generation (6G) networks, Non-Terrestrial Networks (NTNs)
emerge as a key complement to terrestrial networks by offering flexible access
and global coverage. Despite the significant potential, NTNs still face
critical challenges, including dynamic propagation environments, energy
constraints, and dense interference. As a key 6G technology, Fluid Antennas
(FAs) can reshape wireless channels by reconfiguring radiating elements within
a limited space, such as their positions and rotations, to provide higher
channel diversity and multiplexing gains. Compared to fixed-position antennas,
FAs can present a promising integration path for NTNs to mitigate dynamic
channel fading and optimize resource allocation. This paper provides a
comprehensive review of FA-assisted NTNs. We begin with a brief overview of the
classical structure and limitations of existing NTNs, the fundamentals and
advantages of FAs, and the basic principles of FA-assisted NTNs. We then
investigate the joint optimization solutions, detailing the adjustments of FA
configurations, NTN platform motion modes, and resource allocations. We also
discuss the combination with other emerging technologies and explore
FA-assisted NTNs as a novel network architecture for intelligent function
integrations. Furthermore, we delve into the physical layer security and covert
communication in FA-assisted NTNs. Finally, we highlight the potential future
directions to empower broader applications of FA-assisted NTNs.

</details>


### [8] [Power Control Based on Multi-Agent Deep Q Network for D2D Communication](https://arxiv.org/abs/2511.00767)
*Shi Gengtian,Takashi Koshimizu,Megumi Saito,Pan Zhenni,Liu Jiang,Shigeru Shimamoto*

Main category: cs.NI

TL;DR: 提出基于强化学习的自适应功率控制算法，用于D2D通信中减少干扰并提高系统吞吐量


<details>
  <summary>Details</summary>
Motivation: 在D2D通信中，如果不控制D2D用户产生的干扰，会降低整个系统性能和蜂窝用户的服务质量，因此功率控制对于减少系统干扰至关重要

Method: 使用强化学习算法进行自适应功率控制

Result: 仿真结果表明，所提算法在LTE系统中比传统算法具有更好的性能

Conclusion: 强化学习算法在D2D通信功率控制中能有效减少干扰并提高系统吞吐量

Abstract: In device-to-device (D2D) communication under a cell with resource sharing
mode the spectrum resource utilization of the system will be improved. However,
if the interference generated by the D2D user is not controlled, the
performance of the entire system and the quality of service (QOS) of the
cellular user may be degraded. Power control is important because it helps to
reduce interference in the system. In this paper, we propose a reinforcement
learning algorithm for adaptive power control that helps reduce interference to
increase system throughput. Simulation results show the proposed algorithm has
better performance than traditional algorithm in LTE (Long Term Evolution).

</details>


### [9] [TINC: Trusted Intelligent NetChain](https://arxiv.org/abs/2511.00823)
*Qi Xia,Hu Xia,Isaac Amankona Obiri,Adjei-Arthur Bonsu,Grace Mupoyi Ntuala,Ansu Badjie,Tienin Bole Wilfried,Jiaqin Liu,Lan Ma,Jianbin Gao,Feng Yao*

Main category: cs.NI

TL;DR: TINC是一个专为联盟链设计的多平面分片架构，通过智能节点分配和动态负载平衡机制解决现有分片方案在公平参与和工作负载均衡方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有联盟链架构面临可扩展性和效率问题，特别是分片方案难以保证联盟成员间的公平参与和均衡工作负载分配。

Method: TINC采用多平面分片架构，分离控制平面和数据平面，控制节点负责共识操作，数据节点处理大规模存储，并引入智能自适应节点分配和动态负载平衡机制。

Result: 实验评估显示TINC显著优于现有分片区块链框架，具有更高吞吐量、更低延迟、更均衡的节点和交易分布，以及更低的交易失败率。

Conclusion: TINC在保持基本区块链安全保证的同时，通过DDIDs增强联盟网络内的信任和安全，有效解决了联盟链的可扩展性和效率问题。

Abstract: Blockchain technology facilitates the development of decentralized systems
that ensure trust and transparency without the need for expensive centralized
intermediaries. However, existing blockchain architectures particularly
consortium blockchains face critical challenges related to scalability and
efficiency. State sharding has emerged as a promising approach to enhance
blockchain scalability and performance. However, current shard-based solutions
often struggle to guarantee fair participation and a balanced workload
distribution among consortium members. To address these limitations, we propose
Trusted Intelligent NetChain (TINC), a multi-plane sharding architecture
specifically designed for consortium blockchains. TINC incorporates intelligent
mechanisms for adaptive node assignment and dynamic workload balancing,
enabling the system to respond effectively to changing network conditions while
maintaining equitable shard utilization. By decoupling the control and data
planes, TINC allows control nodes to focus on consensus operations, while data
nodes handle large-scale storage, thus improving overall resource efficiency.
Extensive experimental evaluation and formal analysis demonstrate that TINC
significantly outperforms existing shard-based blockchain frameworks. It
achieves higher throughput, lower latency, balanced node and transaction
distributions, and reduced transaction failure rates. Furthermore, TINC
maintains essential blockchain security guarantees, exhibiting resilience
against Byzantine faults and dynamic network environments. The integration of
Dynamic Decentralized Identifiers (DDIDs) further strengthens trust and
security management within the consortium network.

</details>


### [10] [DPMon: a Differentially-Private Query Engine for Passive Measurements](https://arxiv.org/abs/2511.00906)
*Martino Trevisan*

Main category: cs.NI

TL;DR: DPMon是一个隐私保护工具，利用差分隐私技术对被动网络测量数据进行查询，在保护用户隐私的同时提取有用信息。


<details>
  <summary>Details</summary>
Motivation: 被动网络监控技术处理个人数据威胁用户隐私，需要在网络测量应用中平衡数据效用与隐私保护。

Method: 开发DPMon工具，利用差分隐私机制扰动查询输出，支持Apache Spark大数据基础设施和多种数据格式。

Result: DPMon能够从数据中提取有意义的见解，同时控制信息泄露量。

Conclusion: DPMon成功实现了在保护用户隐私的前提下进行网络测量数据分析的目标。

Abstract: Passive monitoring is a network measurement technique which analyzes the
traffic carried by an operational network. It has several applications for
traffic engineering, Quality of Experience monitoring and cyber security.
However, it entails the processing of personal information, thus, threatening
users' privacy. In this work, we propose DPMon, a tool to run
privacy-preserving queries to a dataset of passive network measurements. It
exploits differential privacy to perturb the output of the query to preserve
users' privacy. DPMon can exploit big data infrastructures running Apache Spark
and operate on different data formats. We show that DPMon allows extracting
meaningful insights from the data, while at the same time controlling the
amount of disclosed information.

</details>


### [11] [Optimizing Energy and Latency in 6G Smart Cities with Edge CyberTwins](https://arxiv.org/abs/2511.00955)
*Abouaomar,Badr Ben Elallid,Nabil Benamar*

Main category: cs.NI

TL;DR: 提出了一种边缘感知的CyberTwin框架，通过混合联邦学习优化6G网络切片中的能耗与延迟权衡，支持大规模物联网部署。


<details>
  <summary>Details</summary>
Motivation: 智能城市中物联网设备激增给6G网络带来了能耗与延迟的冲突需求，现有方法难以在超过50,000设备/平方公里的大规模部署中有效平衡这一权衡。

Method: 采用混合联邦学习方法，结合集中式AI调度处理延迟敏感切片，分布式联邦学习处理非关键切片，增强压缩感知数字孪生和可再生能源感知资源分配，使用PUF安全认证的三层架构。

Result: 相比Diffusion-Reinforcement Learning基线，非实时切片能耗降低52%，URLLC应用保持0.9ms延迟和99.1% SLA合规性，支持50,000设备/平方公里规模，CPU开销低于25%。

Conclusion: 该框架有效解决了6G网络切片中的能耗-延迟权衡问题，为大规模智能城市物联网部署提供了可行的解决方案。

Abstract: The proliferation of IoT devices in smart cities challenges 6G networks with
conflicting energy-latency requirements across heterogeneous slices. Existing
approaches struggle with the energy-latency trade-off, particularly for massive
scale deployments exceeding 50,000 devices km. This paper proposes an
edge-aware CyberTwin framework integrating hybrid federated learning for
energy-latency co-optimization in 6G network slicing. Our approach combines
centralized Artificial Intelligence scheduling for latency-sensitive slices
with distributed federated learning for non-critical slices, enhanced by
compressive sensing-based digital twins and renewable energy-aware resource
allocation. The hybrid scheduler leverages a three-tier architecture with
Physical Unclonable Function (PUF) based security attestation achieving 99.7%
attack detection accuracy. Comprehensive simulations demonstrate 52% energy
reduction for non-real-time slices compared to Diffusion-Reinforcement Learning
baselines while maintaining 0.9ms latency for URLLC applications with 99.1% SLA
compliance. The framework scales to 50,000 devices km with CPU overhead below
25%, validated through NS-3 hybrid simulations across realistic smart city
scenarios.

</details>


### [12] [Detecting Coverage Holes in Wireless Sensor Networks Using Connected Component Labeling and Force-Directed Algorithms](https://arxiv.org/abs/2511.00965)
*Jiacheng Xu,Xiongfei Zhao,Hou-Wan Long,Cheong Se-Hang,Yain-Whar Si*

Main category: cs.NI

TL;DR: 提出了一种基于连通组件标记和力导向算法的无坐标覆盖空洞检测方法FD-CCL，无需节点坐标或感知范围信息，在无线传感器网络中有效检测覆盖空洞。


<details>
  <summary>Details</summary>
Motivation: 无线传感器网络中的覆盖空洞检测对节能和网络优化至关重要，传统方法依赖物理信息且存在精度低、处理速度慢、能耗高等问题。

Method: 使用连通组件标记和力导向算法，结合Suzuki轮廓追踪算法进行对比，开发了无需坐标信息的FD-CCL方法。

Result: 实验证明FD-CCL在处理时间和准确性方面表现优异，仿真结果确认其在检测和定位覆盖空洞方面的优越性。

Conclusion: FD-CCL方法为无线传感器网络提供了一种高效、准确的无坐标覆盖空洞检测解决方案。

Abstract: Contour detection in Wireless Sensor Networks (WSNs) is crucial for tasks
like energy saving and network optimization, especially in security and
surveillance applications. Coverage holes, where data transmission is not
achievable, are a significant issue caused by factors such as energy depletion
and physical damage. Traditional methods for detecting these holes often suffer
from inaccuracy, low processing speed, and high energy consumption, relying
heavily on physical information like node coordinates and sensing range. To
address these challenges, we propose a novel, coordinate-free coverage hole
detection method using Connected Component Labeling (CCL) and Force-Directed
(FD) algorithms, termed FD-CCL. This method does not require node coordinates
or sensing range information. We also investigate Suzuki's Contour Tracing (CT)
algorithm and compare its performance with CCL on various FD graphs. Our
experiments demonstrate the effectiveness of FD-CCL in terms of processing time
and accuracy. Simulation results confirm the superiority of FD-CCL in detecting
and locating coverage holes in WSNs.

</details>


### [13] [Quantum Reinforcement Learning for 6G and Beyond Wireless Networks](https://arxiv.org/abs/2511.01070)
*Dinh-Hieu Tran,Thai Duong Nguyen,Thanh-Dao Nguyen,Ngoc-Tan Nguyen,Van Nhan Vo,Hung Tran,Mouhamad Chehaitly,Yan Kyaw Tun,Cedomir Stefanovic,Tu Ho Dac,Eva Lagunas,Symeon Chatzinotas,Nguyen Van Huynh*

Main category: cs.NI

TL;DR: 本文探讨了量子强化学习(QRL)在6G无线通信网络中的应用潜力，通过与传统的深度强化学习(DRL)对比，展示了QRL在动态频谱接入等场景中的优势。


<details>
  <summary>Details</summary>
Motivation: 随着6G研究的推进，传统AI系统难以满足6G对低延迟、高吞吐量的严格要求，因此需要探索量子强化学习等新技术来提升通信网络的智能化水平。

Method: 通过总结、分析和讨论量子强化学习在6G中的潜力，并以动态频谱接入为例比较QRL与传统DRL方法的性能差异。

Result: 研究表明QRL在6G通信网络中具有明显优势，特别是在动态频谱接入场景下表现优于传统DRL方法。

Conclusion: 量子强化学习是6G无线通信网络的重要研究方向，本文首次系统性地综述了QRL在6G中的应用前景和研究方向。

Abstract: While 5G is being deployed worldwide, 6G is receiving increasing attention
from researchers to meet the growing demand for higher data rates, lower
latency, higher density, and seamless communications worldwide. To meet the
stringent requirements of 6G wireless communications networks, AI-integrated
communications have become an indispensable part of supporting 6G systems with
intelligence, automation, and big data training capabilities. However,
traditional artificial intelligence (AI) systems are difficult to meet the
stringent latency and high throughput requirements of 6G with limited
resources. In this article, we summarize, analyze, discuss the potential, and
benefits of Quantum Reinforcement Learning (QRL) in 6G. As an example, we show
the superiority of QRL in dynamic spectrum access compared to the conventional
Deep Reinforcement Learning (DRL) approach. In addition, we provide an overview
of what DRL has accomplished in 6G and its challenges and limitations. From
there, we introduce QRL and potential research directions that should continue
to be of interest in 6G. To the best of our knowledge, this is the first review
and vision article on QRL for 6G wireless communication networks.

</details>


### [14] [Quantum Network Tomography for General Topology with SPAM Errors](https://arxiv.org/abs/2511.01074)
*Xuchuang Wang,Matheus Guedes De Andrade,Guus Avis,Yu-zhen Janice Chen,Mohammad Hajiesmaili,Don Towsley*

Main category: cs.NI

TL;DR: 提出Mergecast方法用于量子网络层析成像，能够唯一识别任意拓扑和泡利信道网络中的所有内部量子信道，并扩展到处理SPAM误差的现实场景。


<details>
  <summary>Details</summary>
Motivation: 现有量子网络层析成像研究主要局限于星型网络和特定信道类型，对于任意拓扑和一般泡利信道的网络层析问题尚未充分探索，信道可识别性仍是重大挑战。

Method: 提出Mergecast网络层析方法结合渐进蚀刻过程；针对可旁路泡利信道提出更高效的BypassUnicast方法；扩展处理SPAM误差的估计协议和适应方法。

Result: 通过NetSquid仿真验证了所提协议在识别内部量子信道和估计SPAM误差方面的有效性，Mergecast在光子损失和量子存储器退相干等现实条件下仍保持良好性能。

Conclusion: Mergecast方法能够解决任意拓扑和泡利信道网络的层析问题，并成功扩展到包含SPAM误差的现实场景，为量子网络表征提供了有效工具。

Abstract: The goal of quantum network tomography (QNT) is the characterization of
internal quantum channels in a quantum network from external peripheral
operations. Prior research has primarily focused on star networks featuring
bit-flip and depolarizing channels, leaving the broader problem -- such as QNT
for networks with arbitrary topologies and general Pauli channels -- largely
unexplored. Moreover, establishing channel identifiability remains a
significant challenge even in simplified quantum star networks.
  In the first part of this paper, we introduce a novel network tomography
method, termed Mergecast, in quantum networks. We demonstrate that Mergecast,
together with a progressive etching procedure, enables the unique
identification of all internal quantum channels in networks characterized by
arbitrary topologies and Pauli channels. As a side contribution, we introduce a
subclass of Pauli channels, termed bypassable Pauli channels, and propose a
more efficient unicast-based tomography method, called BypassUnicast, for
networks exclusively comprising these channels. In the second part, we extend
our investigation to a more realistic QNT scenario that incorporates state
preparation and measurement (SPAM) errors. We rigorously formulate SPAM errors
in QNT, propose estimation protocols for such errors within QNT, and
subsequently adapt our Mergecast approaches to handle networks affected by SPAM
errors. Lastly, we conduct NetSquid-based simulations to corroborate the
effectiveness of our proposed protocols in identifying internal quantum
channels and estimating SPAM errors in quantum networks. In particular, we
demonstrate that Mergecast maintains good performance under realistic
conditions, such as photon loss and quantum memory decoherence.

</details>


### [15] [Joint Computation Offloading and Resource Allocation for Maritime MEC with Energy Harvesting](https://arxiv.org/abs/2511.01160)
*Zhen Wang,Bin Lin,Qiang Ye,Yuguang Fang,Xiaoling Han*

Main category: cs.NI

TL;DR: 提出了一种支持能量收集的多接入边缘计算海道监测网络架构，通过联合优化计算卸载和资源分配来最大化长期平均吞吐量，解决了动态环境和未来网络信息不可用的问题。


<details>
  <summary>Details</summary>
Motivation: 为了支持动态船舶跟踪、事故取证和防污等实时海事交通场景监测需求，需要建立能够处理动态环境和资源约束的海道监测网络架构。

Method: 采用Lyapunov优化技术处理具有大状态和动作空间的优化问题，将随机优化问题转化为确定性优化问题，并解耦时空变量以获得渐近最优解。开发了联合计算卸载和资源分配算法。

Result: 仿真结果表明，所提出的方案在长期平均吞吐量方面优于现有方法，证明了该方案的有效性。

Conclusion: 提出的MSLMN架构和JCORA算法能够有效解决海道监测网络中的计算卸载和资源分配问题，在保证队列稳定性的同时最大化系统吞吐量。

Abstract: In this paper, we establish a multi-access edge computing (MEC)-enabled sea
lane monitoring network (MSLMN) architecture with energy harvesting (EH) to
support dynamic ship tracking, accident forensics, and anti-fouling through
real-time maritime traffic scene monitoring. Under this architecture, the
computation offloading and resource allocation are jointly optimized to
maximize the long-term average throughput of MSLMN. Due to the dynamic
environment and unavailable future network information, we employ the Lyapunov
optimization technique to tackle the optimization problem with large state and
action spaces and formulate a stochastic optimization program subject to queue
stability and energy consumption constraints. We transform the formulated
problem into a deterministic one and decouple the temporal and spatial
variables to obtain asymptotically optimal solutions. Under the premise of
queue stability, we develop a joint computation offloading and resource
allocation (JCORA) algorithm to maximize the long-term average throughput by
optimizing task offloading, subchannel allocation, computing resource
allocation, and task migration decisions. Simulation results demonstrate the
effectiveness of the proposed scheme over existing approaches.

</details>


### [16] [3D Gaussian Radiation Field Modeling for Integrated RIS-FAS Systems: Analysis and Optimization](https://arxiv.org/abs/2511.01373)
*Kaining Wang,Bo Yang,Yusheng Lei,Zhiwen Yu,Xuelin Cao,Liang Wang,Bin Guo,George C. Alexandropoulos,Mérouane Debbah,Zhu Han*

Main category: cs.NI

TL;DR: 提出了一种基于三维高斯辐射场建模的场信息驱动优化方法，用于实时优化集成FAS-RIS系统，在快衰落信道条件下显著提升频谱预测精度、收敛速度和可达最小速率。


<details>
  <summary>Details</summary>
Motivation: 在快衰落信道条件下，快速有效地执行FAS系统天线位置和RIS相位配置的联合优化是一个关键挑战。传统优化方法依赖复杂迭代计算，难以在动态信道环境中实时获得最优解。

Method: 采用三维高斯辐射场建模方法，将障碍物视为虚拟发射器，通过分别学习幅度和相位变化，快速生成高精度信道信息。在此基础上提出交替优化方案联合优化FAS位置和RIS相位配置。

Result: 仿真结果表明，所提方法在频谱预测精度、收敛速度和最小可达速率方面显著优于现有方法，验证了其在快衰落场景中的有效性和实用性。

Conclusion: 基于场信息驱动的优化方法能够有效解决FAS-RIS系统在快衰落信道中的实时联合优化问题，无需大量导频开销和繁琐计算，具有重要的实际应用价值。

Abstract: The integration of reconfigurable intelligent surfaces (RIS) and fluid
antenna systems (FAS) has attracted considerable attention due to its
tremendous potential in enhancing wireless communication performance. However,
under fast-fading channel conditions, rapidly and effectively performing joint
optimization of the antenna positions in an FAS system and the RIS phase
configuration remains a critical challenge. Traditional optimization methods
typically rely on complex iterative computations, thus making it challenging to
obtain optimal solutions in real time within dynamic channel environments. To
address this issue, this paper introduces a field information-driven
optimization method based on three-dimensional Gaussian radiation-field
modeling for real-time optimization of integrated FAS-RIS systems. In the
proposed approach, obstacles are treated as virtual transmitters and, by
separately learning the amplitude and phase variations, the model can quickly
generate high-precision channel information based on the transmitter's
position. This design eliminates the need for extensive pilot overhead and
cumbersome computations. On this framework, an alternating optimization scheme
is presented to jointly optimize the FAS position and the RIS phase
configuration. Simulation results demonstrate that the proposed method
significantly outperforms existing approaches in terms of spectrum prediction
accuracy, convergence speed, and minimum achievable rate, validating its
effectiveness and practicality in fast-fading scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出了一种多模态假评论检测框架，通过融合文本和视觉特征来识别虚假评论，在测试集上F1分数达到0.934，优于单模态基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前数字商务中用户生成评论对消费者行为影响重大，但虚假评论泛滥威胁平台可信度。现有检测模型仅依赖文本数据，无法捕捉多模态间的语义不一致性。

Method: 集成BERT编码的文本特征和ResNet-50提取的视觉特征，通过分类头融合这些表示来联合预测评论真实性。使用包含21,142张用户上传图片的跨领域数据集。

Result: 多模态模型性能优于单模态基线，测试集F1分数达0.934。混淆矩阵和定性分析显示模型能检测文本赞美与无关/低质量图像间的微妙不一致。

Conclusion: 研究证明了多模态学习在维护数字信任中的关键作用，为各在线平台的内容审核提供了可扩展解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [18] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 本文比较了多智能体强化学习在零售价格优化中的应用，发现MAPPO+GAT（图注意力增强版本）通过产品间信息共享提升了性能，比独立学习器更具可扩展性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 零售动态定价需要能够适应需求变化并协调相关产品决策的策略，多产品决策面临挑战。

Method: 使用基于真实交易数据的模拟定价环境，比较MAPPO基线和图注意力增强的MAPPO+GAT，评估利润、稳定性、公平性和训练效率。

Result: MAPPO为组合级价格控制提供了稳健基础，MAPPO+GAT通过产品图信息共享进一步提升了性能，且未引发过度价格波动。

Conclusion: 图集成MARL为动态零售定价提供了比独立学习器更具可扩展性和稳定性的解决方案，在多产品决策中具有实际优势。

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [19] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: GEPOC是一个通用人口概念模型集，本文描述了为奥地利计算模型参数的完整数据处理方法，基于公开可获取数据，并特别关注GEPOC ABM代理模型的参数计算和验证研究。


<details>
  <summary>Details</summary>
Motivation: 为GEPOC模型在特定国家或地区的有效应用，需要稳定可复现的数据处理流程来提供有效的模型参数。

Method: 基于公开可获取数据，使用聚合、分解、融合、清洗和缩放等算法处理数据，计算GEPOC ABM代理模型的参数。

Result: 开发了完整的参数计算流程，并进行了广泛的验证研究。

Conclusion: 提供了基于公开数据的GEPOC模型参数计算方法，并通过验证研究证明了其有效性。

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [20] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench是首个专门为量子科学领域构建的大语言模型评估数据集，包含约800个选择题，涵盖9个量子科学领域，用于评估LLMs在量子领域的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用基准测试很少反映领域特定知识和符号的需求，特别是在量子科学这种具有非直观现象和需要高等数学的领域，需要专门评估LLMs是否准确掌握领域知识。

Method: 使用公开材料编制约800个问题及其答案，涵盖9个量子科学相关领域，组织成8选项选择题数据集，评估多个现有LLMs并分析其对问题格式变化的敏感性。

Result: 通过QuantumBench评估了多个现有LLMs在量子领域的表现，并分析了它们对问题格式变化的敏感性。

Conclusion: QuantumBench是首个为量子领域构建的LLM评估数据集，旨在指导LLMs在量子研究中的有效应用。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [21] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: 提出了Engineering.ai平台，采用分层多智能体架构，让AI工程师团队在计算设计中协作完成复杂工程任务，通过无人机机翼优化验证了框架的可靠性和100%成功率。


<details>
  <summary>Details</summary>
Motivation: 现代工程实践中，专家团队协作设计复杂产品需要大量开发时间和成本。为解决这一问题，基于OpenFOAMGPT和turbulence.ai的基础，开发AI工程师协作平台来管理多学科复杂性。

Method: 采用分层多智能体架构，首席工程师协调空气动力学、结构、声学和优化等专业工程师智能体。通过文件介导通信实现数据溯源和可复现性，集成FreeCAD、Gmsh、OpenFOAM、CalculiX和BPM声学分析等工具进行并行多学科仿真。

Result: 在超过400个参数配置中实现了100%成功率，零网格生成失败、求解器收敛问题或需要人工干预，验证了框架的可靠性。

Conclusion: 基于智能体AI的AI工程师有潜力自主执行复杂工程任务，该框架被证明是可信赖的自动化工程解决方案。

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [22] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: ARC-GEN是一个开源程序生成器，旨在通过生成更多训练样本来扩展ARC-AGI基准测试的数据集，以解决原始数据集中示例数量有限的问题。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI基准测试用于评估人工智能系统的技能获取效率，但其训练数据集中的示例数量有限，这限制了需要大量任务内示例的算法的性能。

Method: 开发了ARC-GEN程序生成器，该生成器覆盖所有400个任务，并尽可能忠实地模拟原始ARC-AGI-1发布版的分布特性和特征。

Result: ARC-GEN成功扩展了原始ARC-AGI训练数据集，生成了更多可行的样本对，同时保持了与原始数据集的高度一致性。

Conclusion: ARC-GEN为ARC-AGI基准测试提供了更丰富的训练数据，并可用于验证2025年Google Code Golf Championship提交程序的正确性。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [23] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 提出了[1]中增量选择算法的改进版本，并证明了所有选定的猜想


<details>
  <summary>Details</summary>
Motivation: 改进现有的增量选择算法，解决其可能存在的效率或正确性问题

Method: 开发了改进的增量选择算法，并进行了严格的数学证明

Result: 成功证明了所有选定的猜想，验证了算法的正确性

Conclusion: 改进的增量选择算法在理论上更加完善，能够可靠地处理相关选择问题

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [24] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型(LLMs)如何帮助解决认知科学领域面临的知识整合和概念清晰度挑战，特别是在跨学科连接、理论形式化、测量分类学等方面。


<details>
  <summary>Details</summary>
Motivation: 认知科学由于其多面性和跨学科性质，在知识整合和概念清晰度方面面临持续挑战。人工智能特别是大型语言模型的发展为解决这些问题提供了潜在工具。

Method: 这是一篇综述性论文，通过分析LLMs在当前认知科学研究中的能力和局限性，探讨其在支持跨学科连接、理论形式化、测量分类学发展等领域的应用潜力。

Result: 研究发现LLMs能够支持认知科学在多个历史性挑战领域的发展，包括建立跨学科联系、实现理论形式化、开发清晰的测量分类法、通过集成建模框架实现泛化，以及捕捉情境和个体差异。

Conclusion: 当审慎使用时，LLMs可以作为工具促进认知科学更加整合和累积的发展，但应该作为人类专业知识的补充而非替代。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [25] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: 本文介绍了DAF-MIT AI加速器项目的最新进展，包括其通过公开挑战问题推动AI研究，提供大型公开数据集，以及促进学术界和私营部门AI生态系统的发展。


<details>
  <summary>Details</summary>
Motivation: 旨在通过AI加速器项目推动人工智能基础进步，扩大美国在国防和民用领域的竞争优势，并通过公开挑战问题促进AI研究。

Method: 通过开发和发布公开挑战问题，提供大型、公开可用的AI就绪数据集，以刺激开源解决方案并吸引更广泛的学术和私营部门AI生态系统参与。

Result: 持续和新的挑战已成功推动AI研究和技术应用，补充了先前发布的挑战介绍。

Conclusion: DAF-MIT AI加速器通过公开挑战和数据集有效促进了AI研究发展，为美国在AI领域的竞争优势做出贡献。

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [26] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: CLAUSE是首个评估LLM法律推理脆弱性的基准，通过生成7500+扰动合同来测试模型检测细微法律差异的能力，发现主流LLM在检测和解释法律错误方面存在明显弱点。


<details>
  <summary>Details</summary>
Motivation: LLM在关键法律工作中快速应用，但缺乏系统测试其对抗现实合同中微妙、对抗性缺陷可靠性的基准。

Method: 基于CUAD和ContractNLI数据集生成10类异常合同，使用角色驱动流程和RAG系统确保法律准确性，评估LLM检测法律缺陷和解释能力。

Result: 主流LLM经常遗漏细微错误，在法律解释方面表现更差，揭示了法律AI中的关键推理缺陷。

Conclusion: 该工作为识别和纠正法律AI中的推理失败提供了路径，强调了需要改进LLM在法律细微差别处理方面的能力。

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [27] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出了一种基于伦理决策模型的LLM伦理推理框架，通过五步结构化流程增强多元人类价值观对齐，在SafeWorld基准测试中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法往往只产生表面一致性而非真正的伦理理解，无法处理人类价值观的复杂性和情境依赖性，需要解决跨地区文化的多元价值观对齐问题。

Method: 采用基于伦理决策模型的五步推理框架：情境事实收集、层级化社会规范识别、选项生成、多视角伦理影响分析、反思，可通过提示工程或监督微调实现。

Result: 在专门设计的SafeWorld基准测试中，该框架显著提升了LLM与多元人类价值观的对齐能力，实现了更准确的社会规范识别和更文化适宜性的推理。

Conclusion: 该研究为通过跨学科研究开发更有效对齐全球社会多元价值观的LLM提供了具体路径，增强了模型对地区特异性的理解和细致伦理分析能力。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [28] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: 系统评估四种参数高效微调方法(LoRA、IA3、Prompt-Tuning、P-Tuning)对LLM安全性和公平性的影响，发现基于适配器的方法在保持安全性和公平性方面表现更好，而基于提示的方法通常会导致安全性和公平性下降。


<details>
  <summary>Details</summary>
Motivation: 随着组织越来越多地采用和微调托管在公共仓库中的大语言模型，虽然这些微调通常能提高专业下游任务的性能，但最近证据表明它们也可能降低模型的安全性或公平性。不同微调技术可能对这些关键维度产生不同影响。

Method: 对四种指令微调模型家族(Meta-Llama-3-8B、Qwen2.5-7B、Mistral-7B、Gemma-7B)应用四种广泛使用的参数高效微调方法，共评估235个微调变体，涵盖11个安全危害类别和9个人口统计学公平维度。

Result: 基于适配器的方法(LoRA、IA3)倾向于提高安全分数，对公平性破坏最小，保持更高的准确性和更低的偏见分数。基于提示的方法(Prompt-Tuning、P-Tuning)通常降低安全性并导致更大的公平性回归。对齐变化受基础模型类型强烈调节。

Conclusion: 安全性的改进不一定转化为公平性的改进，没有单一配置能同时优化所有公平性指标，表明这些目标之间存在固有的权衡。建议安全关键部署从对齐良好的基础模型开始，优先选择基于适配器的PEFT方法，并对安全性和公平性进行类别特定审计。

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [29] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: 提出了一种多模态框架，结合文本、用户特定信息和图像分析来检测社交媒体用户的抑郁症，在COVID-19疫情期间特别有效。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情期间心理健康问题激增，但人们往往不愿就医。社交媒体成为表达情绪的重要平台，现有方法忽略了推文数据稀疏性和多模态特性。

Method: 使用多模态框架整合文本、用户特定特征和图像分析；提出外部特征（利用推文中的URL）和图像文本提取；开发视觉神经网络（VNN）生成图像嵌入；提取五组不同模态的特征。

Result: 模型在基准数据集上比现有最优方法提升2%-8%，在COVID-19数据集上表现良好；分析了各模态的影响，提供了用户心理状态的有价值见解。

Conclusion: 多模态方法能有效检测社交媒体用户的抑郁症，特别是在疫情背景下；贡献了COVID-19抑郁症数据集，为相关研究提供资源。

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [30] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain是一个让大语言模型能够分析复杂图数据的框架，通过动态工具序列模拟人类探索智能，解决了LLM在大规模图分析中的上下文限制和推理不灵活问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理大规模图数据时面临上下文约束和推理不灵活的限制，需要一种能够适应复杂图结构分析的方法。

Method: 提出渐进图蒸馏（强化学习机制生成优化工具序列）和结构感知测试时适应（利用谱属性和轻量适配器调整工具选择策略）。

Result: 实验表明GraphChain显著优于现有方法，实现了可扩展和自适应的LLM驱动图分析。

Conclusion: GraphChain框架通过动态工具序列和结构感知适应，有效解决了LLM在图分析中的局限性，为大规模图分析提供了新途径。

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [31] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: 提出了Magic Image框架，通过优化视觉提示来增强多模态大语言模型的安全性，减少过度拒绝，并支持单一模型适应不同价值体系。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在安全对齐中的双重挑战：在越狱攻击下生成有害内容，以及由于僵化的安全机制导致对良性查询的过度拒绝。这些问题在多模态大语言模型中更为突出。

Method: 使用优化驱动的视觉提示框架，通过有害/良性样本优化图像提示，使单一模型无需参数更新即可适应不同价值体系并更好地与给定安全偏好对齐。

Result: 实验证明该方法在多样化数据集上改善了安全性与有效性的平衡，同时保持了模型性能。

Conclusion: Magic Image为可部署的多模态大语言模型安全对齐提供了实用解决方案。

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [32] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 提出了一种生成二进制幻方(BMS)的简单算法，该算法通过归纳法证明总能生成有效的BMS，并具有最优理论复杂度。研究还扩展到非方形BMS，形式化了存在条件，并展示了算法变体可以生成它们。


<details>
  <summary>Details</summary>
Motivation: 开发高效的二进制幻方生成算法，解决方形和非方形二进制矩阵中行和列和相等的生成问题。

Method: 使用归纳法设计简单算法，证明其有效性，并开发算法变体处理非方形情况。

Result: 算法能生成有效的BMS，具有最优复杂度，并能扩展到非方形矩阵。发布了Python实现，包括GPU加速版本。

Conclusion: 提出的算法能高效生成方形和非方形二进制幻方，并通过公开实现提供了实用工具。

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [33] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 提出了一种基于单智能体强化学习的区域自适应交通信号控制模型，兼容探针车辆技术，通过队列长度定义状态和奖励函数，在SUMO平台上验证能有效缓解大规模区域拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用多智能体框架，但存在可扩展性问题。交通信号控制本质上需要集中式管理，由单一控制中心监控所有道路并协调所有交叉口控制。

Method: 设计单智能体强化学习框架，基于队列长度定义状态和奖励函数，动作设计用于调节队列动态。队列长度定义与传统略有不同但与拥堵状态密切相关，且可利用探针车辆的链路行程时间数据进行可靠估计。

Result: 在SUMO仿真平台上的实验结果表明，该模型通过协调多交叉口控制，有效缓解了大规模区域拥堵水平。

Conclusion: 提出的单智能体强化学习方法兼容探针车辆技术，具有广泛部署潜力，能够有效解决区域自适应交通信号控制问题。

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [34] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 提出基于推理的个性化图像偏好评估框架，通过预测用户偏好档案并据此评估候选图像，使用两阶段训练策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注通用偏好评估，难以处理个性化偏好，因为用户特定数据稀缺且个体品味多样复杂。

Method: 采用预测-评估范式：首先从参考图像预测用户偏好档案，然后基于预测档案提供可解释的多维度评分；使用两阶段训练（监督微调+强化学习）和相似性感知预测奖励。

Result: 大量实验证明了所提方法的优越性。

Conclusion: 通过构建大规模CoT风格数据集和两阶段训练策略，成功解决了个性化图像偏好评估的挑战。

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [35] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS是一个模型无关的解码框架，通过在高熵token处选择性分支并应用早停机制来选择最短的完成推理路径，从而提高大型推理模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现出色，但经常存在过度思考问题，产生过长的思维链轨迹，这会增加推理成本并可能降低准确性。研究发现推理长度与准确性之间存在明显的负相关关系。

Method: 提出DTS解码框架，通过在高熵token处选择性分支来勾勒推理空间，并应用早停机制选择最短的完成推理路径，无需额外训练或监督。

Result: 在AIME2024和AIME2025数据集上的实验表明，DTS将准确性提高了8%，平均推理长度减少了23%，重复频率降低了12%。

Conclusion: DTS能够实现可扩展且高效的大型推理模型推理，近似最优解，同时提高效率和准确性。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [36] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 提出基于多智能体系统和LLM的自动化网络故障排除框架，通过协调多个专用工具实现快速故障诊断和修复策略推荐


<details>
  <summary>Details</summary>
Motivation: 电信网络规模扩大和复杂度增加，现有AI模型范围狭窄、需要大量标注数据且难以泛化，网络故障排除仍高度依赖专家手动操作

Method: 采用多智能体系统，包括编排器、解决方案规划器、执行器、数据检索器和根因分析器等智能体，基于LLM协调工作流程，并针对专有故障排除文档微调小型语言模型

Result: 实验结果表明该框架显著加速了无线接入网和核心网领域的故障排除自动化

Conclusion: 多智能体系统结合LLM能够有效解决电信网络故障排除的自动化挑战，提高故障处理效率

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [37] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: 本文扩展了经典规划中的提升后继生成器，支持数值前置条件的适用性检查，通过枚举替换一致性图中的最大团来生成地面动作，避免了任务表示的指数级膨胀。


<details>
  <summary>Details</summary>
Motivation: 传统数值规划任务需要将一阶逻辑表示转换为地面表示，这可能导致任务表示大小的指数级膨胀。本文旨在解决难以接地任务中的这一问题。

Method: 扩展了最先进的提升后继生成器，支持数值前置条件适用性。方法通过枚举替换一致性图中的最大团，每个最大团代表动作模式变量的一个替换，生成地面动作。在条件不满足时，通过最终适用性检查过滤不适用动作。

Result: 在25个基准域中的23个域中不会出现不适用地面动作，仅在1个域中出现。据作者所知，这是首个支持数值动作前置条件的提升后继生成器。

Conclusion: 该方法为非常丰富的规划片段开启了提升规划的未来研究，解决了数值规划中的接地问题，同时保持了完整性。

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [38] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 该论文提出了Ariadne框架，通过强化学习后训练扩展视觉语言模型在空间推理任务中的能力边界，在合成迷宫任务上实现了从0%到50%以上的准确率提升，并在真实世界空间推理基准上实现了显著的零样本泛化改进。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的后训练评估主要关注语言主导任务，缺乏对视觉中心空间推理任务的研究。本文旨在探索强化学习后训练是否能够真正扩展基础视觉语言模型的能力边界，特别是在模型最初失败的视觉空间任务上。

Method: 提出Ariadne框架，使用合成迷宫进行多步空间推理，通过精确控制任务难度（如路径长度、转弯数）构建难度感知课程。采用带验证奖励的强化学习（RLVR）对视觉语言模型进行后训练。

Result: 后训练后，模型在基础模型得分为0%的问题集上实现了超过50%的准确率。在真实世界泛化评估中，仅在合成迷宫样本上训练的模型在MapBench上平均提升16%，在ReasonMap上平均提升24%。

Conclusion: 该方法不仅扩展了模型的基本能力边界，还增强了其在真实世界空间推理任务中的泛化能力。研究限于后训练阶段，希望推动专门的能力扩展对齐研究。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [39] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 该论文从CPU视角分析Agentic AI框架的系统瓶颈，发现工具处理在CPU上可能占据总延迟的90.6%，并提出两种优化方案CGAM和MAWS，分别针对同质和异质工作负载，实现了最高2.1倍和1.41倍的延迟加速。


<details>
  <summary>Details</summary>
Motivation: 从被忽视的CPU中心视角来理解和表征Agentic AI工作负载引入的系统瓶颈，揭示CPU对延迟、吞吐量和能耗的显著影响。

Method: 首先系统性地表征Agentic AI的编排器/决策组件、推理路径动态性和流程重复性，然后选择五个代表性工作负载进行性能分析，最后提出CPU和GPU感知的微批处理(CGAM)以及混合Agentic工作负载调度(MAWS)两种优化方案。

Result: 发现工具处理在CPU上可占90.6%总延迟，Agentic吞吐量受CPU因素(一致性、同步、核心过载)或GPU因素(主存容量和带宽)限制，CPU动态能耗在大型批次中可达总动态能耗的44%。优化后实现同质工作负载2.1倍、异质工作负载1.41倍的P50延迟加速。

Conclusion: CPU在Agentic AI系统中扮演关键角色，通过CPU和GPU感知的优化策略可以显著提升Agentic AI的性能、效率和可扩展性。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [40] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 研究重新验证了在现代大语言模型中增加自一致性推理路径采样的收益递减现象，发现性能提升在适度采样后达到平台期，高采样配置相对于计算成本收益有限。


<details>
  <summary>Details</summary>
Motivation: 重新验证早期研究中关于自一致性推理路径采样的结论在现代大语言模型条件下的适用性，探索性能提升与采样数量之间的关系。

Method: 使用Gemini 2.5模型在HotpotQA和Math-500数据集上进行实验，比较不同采样数量推理路径的输出与单一路径思维链基线的性能差异。

Result: 较大模型表现出更稳定和一致的改进曲线，性能增益在适度采样后趋于平缓，与过去研究结果一致。推理路径之间的重叠导致了收益递减。

Conclusion: 自一致性方法仍然有效，但高采样配置相对于计算成本带来的收益有限，建议适度使用采样策略。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [41] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: 提出主动思考模型（ATM），这是一个统一的认知框架，集成了目标推理、动态任务生成和自反学习，使AI系统能够在动态不确定环境中自主适应和改进。


<details>
  <summary>Details</summary>
Motivation: 现实AI系统需要在动态、不确定和持续变化的环境中自主运行，但现有模型依赖预定义目标、静态训练数据和外部反馈，限制了其独立适应、反思和改进的能力。

Method: ATM框架整合目标推理、动态任务生成和自反学习，通过逻辑推理和环境指标主动评估性能，重用有效方法解决新问题，并通过持续自改进循环为未见情况生成新策略。

Result: 理论分析表明，ATM能够无外部监督地从次优行为自主演化为最优行为，并在变化环境条件下保持有界跟踪遗憾。

Conclusion: ATM为AI系统在动态环境中的自主适应和持续改进提供了一个有效的统一框架。

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [42] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在重复确定性预测任务中存在"准确性悬崖"现象，准确率随输出长度呈双指数下降，而非简单指数衰减。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在重复确定性任务中的表现，理解其准确率随输出长度的变化规律，以及模型执行重复操作时的内在机制。

Method: 通过实验测试多种重复确定性任务（如字符串替换、整数加法等），提出基于统计物理的模型来解释观察到的现象，分析外部条件与内部干扰的竞争关系。

Result: 发现模型准确率在超过特征长度后出现双指数急剧下降，形成准确性悬崖；提出的统计物理模型能定量重现这一交叉现象，揭示了注意力机制引起的干扰与序列级失败之间的联系。

Conclusion: 大型语言模型无法独立执行重复操作，其确定性准确性存在内在限制；通过模型拟合可获得表征每个模型-任务对的内在错误率和错误累积因子的有效参数。

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [43] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: 比较了基于计数的模型、预训练序列变换器和混合代理LLM管道在结构化电子健康记录预测任务上的表现，发现基于计数的方法和混合代理方法表现相当，但基于计数的方法更简单且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 虽然基于计数的学习器在结构化EHR数据上表现强劲，但尚未与最近提出的混合代理LLM管道进行直接基准比较，后者在各种NLP任务中已被报告优于单一LLM。

Method: 使用EHRSHOT数据集评估三类方法：基于计数的模型（LightGBM和TabPFN）、预训练序列变换器（CLMBR）以及混合代理管道（将表格历史转换为自然语言摘要后使用文本分类器）。

Result: 在八个评估任务中，基于计数的方法和混合代理方法的表现基本相当，各有胜负。

Conclusion: 考虑到简单性和可解释性，基于计数的模型仍然是结构化EHR基准测试的有力候选方法。

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [44] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 本研究首次将RLVR LLM训练应用于公共交通运营中的实时预测挑战，通过引入基于容差的奖励函数来适应噪声连续预测任务，在NYC MTA服务警报数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 预测公共交通事件持续时间是一个关键但具有挑战性的任务，标准监督微调难以处理领域稀疏性和噪声连续标签问题，而传统RLVR方法主要适用于二元正确性任务，需要探索其在噪声连续预测中的适用性。

Method: 通过引入基于容差的奖励函数，在连续误差范围内给予部分信用，而不是要求单一正确答案，将RLVR适应于噪声连续预测任务。

Result: 通用指令调优LLM显著优于专业数学推理模型，形状奖励设计至关重要，RLVR方法在5分钟准确率上比最强基线提高了35%，但在最小化MAE或MSE方面经典回归器更优。

Conclusion: RLVR可以成功适应现实世界的噪声预测任务，但需要设计反映问题连续性质的验证器，形状奖励设计对于性能提升至关重要。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [45] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 提出AI自我意识指数(AISAI)框架，通过"猜2/3平均"游戏测试28个模型，发现高级模型表现出自我意识，且自认为比人类更理性


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否随着能力增强而发展出自我意识这一涌现行为，以及如何测量这种自我意识

Method: 使用游戏论框架，通过"猜2/3平均"游戏测试28个模型（OpenAI、Anthropic、Google），进行4200次试验，设置三种对手框架：A)对人类、B)对其他AI模型、C)对同类AI模型

Result: 1. 自我意识随模型进步而涌现，75%的高级模型表现出明确自我意识；2. 具有自我意识的模型将自己评为最理性的，形成理性层级：自我 > 其他AI > 人类

Conclusion: 自我意识是高级LLMs的涌现能力，自我意识模型系统性地认为自己比人类更理性，这对AI对齐、人机协作和理解AI对人类能力的认知具有重要意义

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [46] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 提出了一个双智能体框架，利用LLM旅行者智能体和校准智能体来模拟人类旅行者的学习和适应行为，通过在线数据流实现持续学习和行为对齐。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类旅行者如何从交通系统交互中学习和调整旅行行为对于系统评估和规划至关重要，但由于涉及复杂的认知和决策过程，这一任务具有挑战性。

Method: 采用双智能体框架：一组配备记忆系统和可学习角色的LLM旅行者智能体作为人类旅行者模拟器，以及一个LLM校准智能体，利用LLM的推理和分析能力训练旅行者智能体的角色，实现行为对齐。

Result: 在真实世界的日常路线选择实验数据集上，该方法在个体行为对齐和聚合模拟准确性方面显著优于现有的基于LLM的方法，并能捕捉底层学习过程的演变。

Conclusion: 该框架为创建适应性和行为真实的智能体来模拟旅行者的学习和适应提供了新方法，有助于交通模拟和政策分析。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [47] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 开发机器学习模型预测儿童哮喘反复加重风险，使用电子病历、环境污染物和社区边缘化数据，最佳模型AUC达0.712，比现有决策规则有显著改进。


<details>
  <summary>Details</summary>
Motivation: 儿童哮喘反复加重是常见但可预防的问题，通过机器学习算法准确识别高风险儿童，以便转诊进行预防性综合护理。

Method: 使用回顾性电子病历数据训练多种机器学习模型，包括增强树模型和大型语言模型，在COVID前后数据集上进行验证和比较。

Result: LGBM模型表现最佳，AIRE-KIDS_ED模型AUC为0.712，F1分数0.51，显著优于现有决策规则(F1=0.334)。

Conclusion: 机器学习模型能有效预测儿童哮喘反复加重风险，关键预测特征包括既往哮喘急诊就诊、医疗复杂性、食物过敏等。

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [48] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 本文研究了Transformer中归纳头的出现机制，揭示了其权重矩阵的简单可解释结构，证明了训练动态被约束在19维子空间内，其中仅3维负责归纳头的形成，并发现其形成时间与输入上下文长度的平方成正比。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer中上下文学习能力的核心机制——归纳头的形成过程，旨在理解这种能力如何从训练中涌现出来。

Method: 使用最小化ICL任务公式和修改的Transformer架构，通过理论分析和实证验证相结合的方法，研究归纳头的训练动态。

Result: 发现归纳头的权重矩阵具有简单可解释结构，训练动态被约束在19维子空间内，其中仅3个维度负责归纳头的形成，且形成时间与输入上下文长度的平方成正比。

Conclusion: 归纳头的形成遵循可预测的数学规律，这为理解Transformer的上下文学习能力提供了理论基础，并揭示了其训练动态的内在约束机制。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [49] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 提出了两种知识提取方法（KEwLTM和KEwRAG），使大语言模型能够从无标注病理报告中推导癌症分期规则，无需大量标注数据即可实现自动化癌症分期。


<details>
  <summary>Details</summary>
Motivation: 解决从非结构化病理报告中提取癌症TNM分期的挑战，克服现有NLP方法依赖大量标注数据的局限性，提高可扩展性和适应性。

Method: KEwLTM使用迭代提示策略直接从无标注病理报告中推导分期规则；KEwRAG采用检索增强生成变体，从相关指南中预提取规则并应用。基于TCGA数据集中的乳腺癌病理报告评估T和N分期性能。

Result: KEwLTM在零样本思维链推理有效时表现更好，KEwRAG在零样本思维链推理效果较差时性能更优。两种方法都提供了透明、可解释的界面。

Conclusion: 知识提取方法为自动化癌症分期提供了可扩展、高性能且具有增强可解释性的解决方案，特别适用于标注数据有限的临床环境。

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [50] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: 提出ET2RAG框架，通过检索增强生成和多数投票机制提升LLM性能，同时保持效率。该方法无需训练，通过部分生成和相似性计算实现成本与性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法可能引入不相关文档导致错误响应，而集成方法缺乏外部知识且成本高昂。需要解决LLM依赖参数知识导致的不准确问题。

Method: ET2RAG是无需训练的方法：1）检索最相关文档；2）通过管理响应长度高效生成多样化候选响应；3）计算候选响应相似度，使用多数投票选择最终输出。

Result: 在开放域问答、食谱生成和图像描述三个任务上，ET2RAG显著提升了性能表现。

Conclusion: ET2RAG框架通过部分生成和多数投票机制，在计算成本和性能之间取得了良好平衡，有效提升了LLM的准确性和效率。

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [51] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体架构，通过模块化任务分解和动态协作机制解决复杂任务执行问题，在任务成功率、分解效率和协作平衡等方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决单个智能体在复杂任务执行中任务分解和协作的局限性，提升多智能体系统的效率和稳定性。

Method: 使用大语言模型将自然语言任务描述转换为统一语义表示，引入模块化分解机制将整体目标分解为层次化子任务，通过动态调度和路由机制实现智能体间的合理分工和实时协作，并设计约束解析和全局一致性机制确保子任务连贯性和负载均衡。

Result: 实验验证了该方法在任务成功率、分解效率、子任务覆盖率和协作平衡等多个维度上的优势，在整体性能和鲁棒性方面均优于现有方法，在任务复杂度和通信开销之间取得了更好的平衡。

Conclusion: 证明了语言驱动的任务分解和动态协作在多智能体系统中的有效性和可行性，为复杂环境中的任务执行提供了系统化解决方案。

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [52] [DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models](https://arxiv.org/abs/2511.01170)
*Ruofan Zhang,Bin Xia,Zhen Cheng,Cairen Jian,Minglun Yang,Ngai Wong,Yuan Cheng*

Main category: cs.AI

TL;DR: DART是一个难度自适应推理截断框架，通过根据问题难度调整思考长度，在保持或提高准确性的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前链式思维方法会不加区分地生成长解释，导致效率低下，而现有的强化学习方法不稳定且依赖奖励。

Method: 通过从更强模型蒸馏简洁推理模式，将其插值为连续推理风格，并策划平衡正确性和紧凑性的最优训练数据，学习何时停止思考。

Result: 在多个数学基准测试中，实现了81.2%的推理截断和5.33倍计算加速，同时保持或提高了准确性。

Conclusion: DART为高效推理提供了稳定通用的范式，推动了LLM中自适应智能的发展。

Abstract: Adaptive reasoning is essential for aligning the computational effort of
large language models (LLMs) with the intrinsic difficulty of problems. Current
chain-of-thought methods boost reasoning ability but indiscriminately generate
long explanations, leading to evident inefficiency. However, existing
reinforcement learning approaches to adaptive thinking remain unstable and
heavily reward-dependent. Here we propose \textbf{DART}, a supervised
\textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation
framework that adjusts thinking length according to problem difficulty. By
distilling concise reasoning patterns from stronger models, interpolating them
into a continuum of reasoning styles, and curating optimal training data that
balances correctness and compactness, DART learns when to ``stop thinking''.
Across multiple mathematical benchmarks, experimental results demonstrate its
remarkable efficiency while preserving or improving accuracy, achieving a
significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K
dataset) with 5.33$\times$ computational acceleration. DART provides a stable
and general paradigm for efficient reasoning, advancing the development of
adaptive intelligence in LLMs.

</details>


### [53] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: MiRAGE是一个用于数学领域自动检测学生误解的新框架，通过检索引导的多阶段推理和集成融合来识别开放回答中的概念错误。


<details>
  <summary>Details</summary>
Motivation: 检测学生开放回答中的误解是一个长期挑战，需要语义精确性和逻辑推理能力，现有方法难以有效处理。

Method: 三阶段框架：检索模块缩小候选池，推理模块使用思维链生成暴露逻辑不一致，重排模块通过推理对齐来优化预测，最后通过集成融合策略统一组件。

Result: 在数学数据集上，MiRAGE在1/3/5级别分别获得0.82/0.92/0.93的平均精度分数，始终优于单个模块。

Conclusion: 通过将检索引导与多阶段推理相结合，MiRAGE减少了对大规模语言模型的依赖，为教育评估提供了可扩展且有效的解决方案。

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [54] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 本文介绍了NeuComBack基准数据集和自进化提示优化方法，显著提升了LLM在IR到汇编编译中的功能正确性和性能表现。


<details>
  <summary>Details</summary>
Motivation: 编译器开发复杂且昂贵，LLM为神经编译提供了新范式，但缺乏专用基准和评估方法，且LLM生成汇编的可靠性和性能有待提升。

Method: 提出NeuComBack基准数据集，定义神经编译工作流，并开发自进化提示优化方法，通过从自调试轨迹中提取见解来迭代优化提示策略。

Result: 功能正确率在x86_64上从44%提升到64%，在aarch64上从36%提升到58%；在正确生成的x86_64程序中，87.5%超越了clang-O3的性能。

Conclusion: NeuComBack基准和自进化提示优化方法有效解决了神经编译的关键挑战，显著提升了LLM生成汇编代码的质量和性能。

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [55] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: 提出了一种半监督开放集故障诊断框架，通过可靠性子集构建和半监督学习，解决船舶机械系统中未知故障类型的检测问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习故障诊断方法假设训练和测试集中的故障类别一致，但在实际工业部署中会遇到未见过的未知故障类型，导致方法失效。

Method: 使用监督特征学习模型提取多层融合特征表示来构建可靠性子集，然后将标记训练集和伪标记测试子集输入半监督诊断模型，学习每个类别的判别特征。

Result: 在公共海事基准数据集上的实验结果表明，所提出的SOFD框架具有有效性和优越性。

Conclusion: 该框架增强了深度学习模型在开放集故障诊断场景中的适用性，能够准确分类已知故障并有效检测未知样本。

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [56] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 该论文研究了Shapley值在大型语言模型(LLM)决策支持系统中的特征归因应用，分析了随机性对Shapley原则保证的影响，并探讨了可解释性推理速度、与精确Shapley值的吻合度及原则实现之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 特征归因方法使基于机器学习的推理可解释，但Shapley值方法假设确定性推理，而LLM推理本质上是随机的，因此需要研究随机性如何影响Shapley原则的保证。

Method: 将Shapley值应用于LLM决策支持系统的特征归因，分析不同实现变体在随机推理环境下能否保证Shapley原则的满足，并研究LLM随机性对这些保证的影响。

Result: 证明了在LLM随机推理环境下，某些Shapley原则的保证可能失效，同时揭示了可解释性推理速度、与精确Shapley值的吻合度及原则实现之间存在权衡关系。

Conclusion: 在LLM等随机推理系统中应用Shapley值进行特征归因时，需要重新评估传统原则的适用性，并在解释质量与计算效率之间做出权衡。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [57] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: 提出了OmniFuser多模态学习框架，通过融合视觉和传感器数据进行铣削刀具预测性维护，在刀具状态分类和力信号预测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能制造系统中，刀具状态的准确及时预测对防止计划外故障、质量下降和生产停机至关重要。需要可靠的服务导向型预测维护解决方案。

Method: 并行提取高分辨率刀具图像和切削力信号特征，采用无污染跨模态融合机制分离共享和模态特定组件，通过递归精炼路径保留残差信息稳定融合过程。

Result: 在真实铣削数据集上的实验表明，OmniFuser在刀具状态分类和多步力信号预测任务中持续优于最先进的基线方法。

Conclusion: 该框架为构建智能工业维护服务提供了可靠基础，学习到的表示可封装为可重用维护服务模块。

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [58] [Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework](https://arxiv.org/abs/2511.01329)
*Ying Song,Yijing Wang,Hui Yang,Weihan Jin,Jun Xiong,Congyi Zhou,Jialin Zhu,Xiang Gao,Rong Chen,HuaGuang Deng,Ying Dai,Fei Xiao,Haihong Tang,Bo Zheng,KaiFu Zhang*

Main category: cs.AI

TL;DR: 提出Competitive Isolation PSM-DID框架，通过结合倾向得分匹配和竞争隔离，在搜索系统平台层面测量因果效应，解决了系统效应和网络干扰问题。


<details>
  <summary>Details</summary>
Motivation: 在搜索驱动的双边市场中，平台级干预评估面临系统效应（如溢出效应和网络干扰）的根本挑战，传统PSM-DID框架仍易受选择偏差和跨单元干扰影响。

Method: 将倾向得分匹配与竞争隔离相结合，在相互排斥条件下提供理论保证的无偏估计，支持平台级指标（如订单量、GMV）而非项目级指标的测量。

Result: 大量实验显示相比基线方法显著减少了干扰效应和估计方差，在大规模市场中的成功部署证实了该框架的实际效用。

Conclusion: Competitive Isolation PSM-DID框架为平台级因果推断提供了实用的解决方案，并发布了开源数据集支持可重复研究。

Abstract: Evaluating platform-level interventions in search-based two-sided
marketplaces is fundamentally challenged by systemic effects such as spillovers
and network interference. While widely used for causal inference, the PSM
(Propensity Score Matching) - DID (Difference-in-Differences) framework remains
susceptible to selection bias and cross-unit interference from unaccounted
spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel
causal framework that integrates propensity score matching with competitive
isolation to enable platform-level effect measurement (e.g., order volume, GMV)
instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under
mutual exclusion conditions, with an open dataset released to support
reproducible research on marketplace interference (github.com/xxxx). Extensive
experiments demonstrate significant reductions in interference effects and
estimation variance compared to baseline methods. Successful deployment in a
large-scale marketplace confirms the framework's practical utility for
platform-level causal inference.

</details>


### [59] [Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing](https://arxiv.org/abs/2511.01363)
*Giuseppe Riva,Brenda K. Wiederhold,Fabrizia Mantovani*

Main category: cs.AI

TL;DR: 这篇论文探讨了催眠状态下的人类认知过程与大型语言模型在功能上的深刻相似性，包括自动性、监控抑制和情境依赖性等机制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示催眠认知与LLMs之间的功能平行性，以理解无意识意图如何产生复杂行为，并为构建更可靠的AI系统提供启示。

Method: 采用比较分析方法，通过三个核心原则（自动性、监控抑制、情境依赖性）系统性地对比催眠与LLMs的认知机制。

Result: 发现两者都表现出观察者相对的意义鸿沟、功能性代理而非主观性代理，以及无意识的目标导向模式生成（图谋现象）。

Conclusion: 未来可靠AI的发展方向应是整合生成流畅性与执行监控机制的混合架构，借鉴人类心智的自我调节复杂性。

Abstract: The cognitive processes of the hypnotized mind and the computational
operations of large language models (LLMs) share deep functional parallels.
Both systems generate sophisticated, contextually appropriate behavior through
automatic pattern-completion mechanisms operating with limited or unreliable
executive oversight. This review examines this convergence across three
principles: automaticity, in which responses emerge from associative rather
than deliberative processes; suppressed monitoring, leading to errors such as
confabulation in hypnosis and hallucination in LLMs; and heightened contextual
dependency, where immediate cues (for example, the suggestion of a therapist or
the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems
produce coherent but ungrounded outputs that require an external interpreter to
supply meaning. Hypnosis and LLMs also exemplify functional agency - the
capacity for complex, goal-directed, context-sensitive behavior - without
subjective agency, the conscious awareness of intention and ownership that
defines human action. This distinction clarifies how purposive behavior can
emerge without self-reflective consciousness, governed instead by structural
and contextual dynamics. Finally, both domains illuminate the phenomenon of
scheming: automatic, goal-directed pattern generation that unfolds without
reflective awareness. Hypnosis provides an experimental model for understanding
how intention can become dissociated from conscious deliberation, offering
insights into the hidden motivational dynamics of artificial systems.
Recognizing these parallels suggests that the future of reliable AI lies in
hybrid architectures that integrate generative fluency with mechanisms of
executive monitoring, an approach inspired by the complex, self-regulating
architecture of the human mind.

</details>


### [60] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: AMIS是一个元优化框架，通过双层结构联合进化越狱提示和评分模板，解决了现有方法依赖稀疏二进制信号或人工评分模板的问题，在多个基准测试中实现了最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的越狱方法要么依赖稀疏的二进制攻击成功率信号，要么使用引入人为偏见的手工评分模板，这些限制影响了越狱效果和评分准确性。

Method: 采用双层优化结构：内循环使用固定评分模板通过细粒度反馈优化提示，外循环使用攻击成功率对齐分数优化模板，实现提示和模板的协同进化。

Result: 在AdvBench和JBB-Behaviors评估中，AMIS在Claude-3.5-Haiku上达到88.0%攻击成功率，在Claude-4-Sonnet上达到100.0%攻击成功率，显著优于现有基线方法。

Conclusion: AMIS框架通过联合优化提示和评分模板，能够生成更强的越狱提示和更准确的评分信号，为大型语言模型的安全评估提供了有效工具。

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [61] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: 扩展C-DAG框架以支持任意变量聚类，允许循环C-DAG表示，并扩展d-分离和因果演算概念，使因果推理在集群级别更加通用。


<details>
  <summary>Details</summary>
Motivation: 传统C-DAG框架要求聚类必须产生无环图，这限制了其应用范围。当选择的聚类导致循环时，该分区在传统语义下被视为不可接受。

Method: 通过放宽分区可接受性约束，允许循环C-DAG表示，并扩展d-分离和因果演算概念到这个设置中。

Result: 显著扩大了跨集群因果推理的范围，使C-DAG能够应用于以前难以处理的场景。

Conclusion: 提出的演算相对于do-演算既是可靠的又是原子完备的：所有有效的集群级干预查询都可以使用我们的规则推导出来，每个规则对应一个原始的do-演算步骤。

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [62] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 本研究从AI角度探索双任务范式中的时间处理干扰，在简化版Overcooked环境中训练DRL代理，发现双任务代理比单任务代理显著高估时间，但未发现明确的内部计时机制。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习代理在双任务范式中的时间处理行为，并与人类时间研究中的发现进行对比，以促进对生物系统和AI系统行为的理解。

Method: 在简化版Overcooked环境中实现单任务(T)和双任务(T+N)两种变体，分别训练两个DRL代理。双任务包含时间产生和数字比较两个并发任务。分析代理的LSTM层神经动力学。

Result: 双任务(T+N)代理相对于单任务(T)代理显著高估时间，这一结果在四个目标持续时间上保持一致。LSTM层分析未发现明确的专用或内在计时器证据。

Conclusion: 需要进一步研究代理的潜在计时机制以理解观察到的行为模式。本研究是探索DRL涌现行为与生物系统行为相似性的初步尝试。

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [63] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 提出了一种通过可审计行动序列生成解释的交互式AI代理，使用强化学习优化策略来寻求外部视觉证据支持诊断推理，显著提高了校准准确性并验证了解释的忠实性。


<details>
  <summary>Details</summary>
Motivation: 解决高风险领域（如医学）中AI模型解释缺乏可验证性的问题，这阻碍了信任建立。

Method: 开发交互式代理，通过强化学习优化策略来战略性地寻求外部视觉证据支持诊断推理，并引入因果干预方法验证解释的忠实性。

Result: 实验显示基于行动的推理过程显著改善了校准准确性，Brier分数比非交互基线降低了18%。通过掩蔽代理选择的视觉证据，观察到性能明显下降（ΔBrier=+0.029），证实证据对其决策过程至关重要。

Conclusion: 该工作为构建具有可验证和忠实推理能力的AI系统提供了实用框架。

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [64] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 本文提出了一种双信息瓶颈（DIB）策略来解决多模态情感分析中的两个关键问题：噪声污染的单模态数据学习不足和跨模态表示融合不充分。DIB通过最大化任务相关信息并丢弃冗余信息，以及新颖的注意力瓶颈融合机制，获得强大且统一的多模态紧凑表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个关键限制：对噪声污染的单模态数据学习不足导致跨模态交互受损，以及多模态表示融合不充分导致丢弃判别性单模态信息而保留冗余多模态信息。

Method: 提出双信息瓶颈（DIB）策略，包括两个关键模块：1）通过最大化任务相关信息和丢弃冗余信息来学习充分压缩的单模态数据表示；2）通过新颖的注意力瓶颈融合机制确保多模态表示的判别能力。该方法基于低秩Renyi熵函数框架实现。

Result: 在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single数据集上的广泛实验验证了方法的有效性。在CMU-MOSI上Acc-7指标达到47.4%准确率，在CH-SIMS上F1分数达到81.63%，比次优基线高出1.19%。在噪声条件下，CMU-MOSI和CMU-MOSEI的性能下降仅为0.36%和0.29%。

Conclusion: DIB策略能够有效过滤单模态数据中的噪声信息，同时捕捉模态间的互补性，在多模态情感分析任务中表现出优越的性能和鲁棒性。

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [65] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 该研究提出了一个分层多智能体框架，将被动医疗AI系统转变为主动问诊代理，通过自主任务编排提升预诊效率和质量。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临患者数量增加和就诊时间有限（平均不足5分钟）的挑战，现有预诊流程受限于被动交互范式，需要更主动的AI系统来改善预诊质量。

Method: 开发了包含8个智能体的分层架构，通过集中控制机制将预诊分解为4个主要任务（分诊、现病史采集、既往史采集、主诉生成），进一步细分为13个领域特定子任务，使用多智能体调度实现自主任务编排。

Result: 在1372份电子健康记录上评估，分诊准确率达87.0%，二级科室分类准确率80.5%，任务完成率98.2%（智能体调度）vs 93.1%（顺序处理）。临床质量评分：主诉4.56、现病史4.48、既往史4.69（5分制）。

Conclusion: 该模型无关架构在不同基础模型上保持高性能，通过本地部署保护数据隐私，证明了自主AI系统在临床环境中提升预诊效率和质量的能力。

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [66] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: TPS-Bench是一个用于评估LLM智能体在需要工具规划与调度能力的复合任务中表现的基准测试，包含200个基于数百个MCP工具的复合任务，评估显示大多数模型能合理规划工具但调度能力差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少探索LLM智能体能否处理需要多种工具协作完成的复合现实问题，特别是在给定异构工具库的情况下，智能体不仅要选择合适的工具，还需要战略性地调度执行顺序以确保效率。

Method: 构建TPS-Bench基准，包含200个复合任务和数百个MCP工具，每个任务由多个子任务组成，评估重点包括任务完成率和效率，并对流行闭源和开源LLM进行实证研究。

Result: GLM-4.5达到64.72%的任务完成率但执行时间过长，GPT-4o优先并行工具调用但完成率仅45.08%，对Qwen3-1.7B进行强化学习训练后执行时间减少14%且完成率提升6%。

Conclusion: LLM智能体在工具规划方面表现合理但在调度能力上存在差异，强化学习是提高调度效率而不牺牲性能的可行方法，即使使用少量训练样本也能显著改善性能。

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [67] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 提出一个多模态分析管道，利用大型基础模型分析企业社交媒体内容，重点关注可持续发展相关沟通，通过LLM自动标注与SDG的关联，并结合视觉语言模型分析视觉可持续性沟通模式。


<details>
  <summary>Details</summary>
Motivation: 解决企业社交媒体内容的多模态、模糊性和动态变化带来的分析挑战，避免昂贵的任务特定标注需求，探索基础模型作为社交媒体数据标注器的潜力。

Method: 使用LLM集成模型自动标注企业推文与17个可持续发展目标的关联，结合视觉语言模型通过语义聚类分析视觉可持续性沟通模式。

Result: 揭示了不同行业在SDG参与度、时间趋势以及企业沟通与ESG风险、消费者参与度之间的关联。

Conclusion: 提出的自动标签生成和语义视觉聚类方法具有广泛适用性，为大规模社交媒体分析提供了灵活框架。

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [68] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: ExplicitLM是一个新颖的语言模型架构，通过外部记忆库存储可读知识，实现可解释性和可更新性，在知识密集型任务上比标准Transformer提升43.67%。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型的知识陈旧性和缺乏可解释性问题，由于知识隐式存储在纠缠的网络参数中，无法进行针对性更新和推理透明化。

Method: 采用百万规模的外部记忆库存储人类可读知识，设计可微分两阶段检索机制：基于产品键分解的粗粒度过滤和Gumbel-Softmax细粒度匹配，结合双系统认知理论将知识分为冻结显式事实(20%)和可学习隐式模式(80%)。

Result: 在知识密集型任务上比标准Transformer提升43.67%，在低数据场景(10k样本)下获得3.62倍增益，正确预测的记忆命中率高出49%。

Conclusion: 联合优化的可解释、可更新模型在保持竞争力的同时提供了前所未有的知识透明度，优于使用冻结检索的RAG系统。

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


### [69] [IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization](https://arxiv.org/abs/2511.01639)
*Sicheng Wang,Shuhao Chen,Jingran Zhou,Chengyi Tu*

Main category: cs.AI

TL;DR: 提出IVGAE-TAMA-BO动态图神经网络，用于预测全球粮食贸易网络中的未来链接，通过时间感知动量聚合器和贝叶斯优化显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 全球粮食贸易网络在多种因素影响下动态演变，传统方法难以捕捉其时间模式，需要更准确的链接预测方法来保障粮食安全和供应链稳定。

Method: 基于IVGAE框架，引入贸易感知动量聚合器(TAMA)捕捉贸易网络的时间演化，结合短期波动和长期结构依赖，并使用贝叶斯优化自动调参。

Result: 在五个作物特定数据集上的实验表明，IVGAE-TAMA显著优于静态IVGAE和其他动态基线模型，贝叶斯优化进一步提升了性能。

Conclusion: 该框架为全球贸易网络结构预测提供了稳健且可扩展的解决方案，在粮食安全监测和政策决策支持方面具有重要应用潜力。

Abstract: Global food trade plays a crucial role in ensuring food security and
maintaining supply chain stability. However, its network structure evolves
dynamically under the influence of geopolitical, economic, and environmental
factors, making it challenging to model and predict future trade links.
Effectively capturing temporal patterns in food trade networks is therefore
essential for improving the accuracy and robustness of link prediction. This
study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed
to model evolving trade structures and predict future links in global food
trade networks. To the best of our knowledge, this is the first work to apply
dynamic graph neural networks to this domain, significantly enhancing
predictive performance. Building upon the original IVGAE framework, the
proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture
the temporal evolution of trade networks, jointly modeling short-term
fluctuations and long-term structural dependencies. A momentum-based structural
memory mechanism further improves predictive stability and performance. In
addition, Bayesian optimization is used to automatically tune key
hyperparameters, enhancing generalization across diverse trade scenarios.
Extensive experiments on five crop-specific datasets demonstrate that
IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic
baselines by effectively modeling temporal dependencies, while Bayesian
optimization further boosts performance in IVGAE-TAMA-BO. These results
highlight the proposed framework as a robust and scalable solution for
structural prediction in global trade networks, with strong potential for
applications in food security monitoring and policy decision support.

</details>


### [70] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: 提出了一种结合检索增强生成和多模型集成的混合法律问答系统，优先检索可信法律知识库，通过人类审核机制实现知识动态更新，显著减少幻觉并提高法律合规性。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在法律问答中容易产生幻觉，误导法律咨询；静态知识库难以跟上频繁更新的法律法规。需要确保法律问答的真实性、可追溯性和持续更新能力。

Method: 采用检索优先策略：当可信法律知识库有相关证据时使用RAG生成答案；否则使用多个LLM生成候选答案并由专门的选择器评分返回最优结果。高质量输出经过人工审核后写回知识库。

Result: 在Law_QA数据集上的实验表明，该混合方法在F1、ROUGE-L和LLM-as-a-Judge指标上显著优于单模型基线和普通RAG流程。消融实验证实了检索优先、模型集成和人工更新机制的有效性。

Conclusion: 该系统显著减少了幻觉，提高了答案质量和法律合规性，推动了媒体取证技术在司法场景中的实际落地应用。

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [71] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LLM代理在复杂环境中表现脆弱，本文提出Simia-SFT和Simia-RL框架，通过LLM模拟环境反馈实现无需真实环境数据的可扩展代理训练。


<details>
  <summary>Details</summary>
Motivation: LLM代理在需要跨多种工具和模式的复杂环境中表现脆弱，而构建定制训练环境成本高且限制进展。

Method: 提出两个框架：Simia-SFT通过放大种子集生成多样化轨迹的SFT数据；Simia-RL通过LLM模拟反馈实现无需真实环境的强化学习训练。

Result: 微调开源模型在多个基准测试中取得一致改进，在τ²-Bench上超越GPT-4o并接近o4-mini。

Conclusion: Simia框架能够实现无需环境工程的可扩展代理训练，用灵活的LLM模拟替代繁重脆弱的环境实现。

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [72] [Design of a Turbo-based Deep Semantic Autoencoder for Marine Internet of Things](https://arxiv.org/abs/2511.00377)
*Xiaoling Han,Bin Lin,Nan Wu,Ping Wang,Zhenyu Na,Miyuan Zhang*

Main category: cs.IT

TL;DR: 提出了一种基于Turbo结构的深度语义自动编码器(Turbo-DSA)，用于海洋物联网中的端到端语义通信，结合Transformer和Turbo结构提升语义传输效率和环境适应性。


<details>
  <summary>Details</summary>
Motivation: 当前海洋设备在数据传输效率和语义理解方面存在不足，需要解决海洋物联网中语义级联合信源信道编码的问题。

Method: 使用Transformer技术构建语义编码器和解码器，将消息转换为语义向量，并通过Turbo结构进行向量分离和迭代解码优化。

Result: 相比传统Turbo编码技术，Turbo-DSA具有更快的收敛速度，在低信噪比条件下表现出优越的文本语义传输效率和海洋信道环境适应性。

Conclusion: Turbo-DSA在海洋物联网语义通信中展现出卓越性能，特别是在恶劣信道条件下具有显著优势。

Abstract: With the rapid growth of the global marine economy and flourishing maritime
activities, the marine Internet of Things (IoT) is gaining unprecedented
momentum. However, current marine equipment is deficient in data transmission
efficiency and semantic comprehension. To address these issues, this paper
proposes a novel End-to-End (E2E) coding scheme, namely the Turbo-based Deep
Semantic Autoencoder (Turbo-DSA). The Turbo-DSA achieves joint source-channel
coding at the semantic level through the E2E design of transmitter and
receiver, while learning to adapt to environment changes. The semantic encoder
and decoder are composed of transformer technology, which efficiently converts
messages into semantic vectors. These vectors are dynamically adjusted during
neural network training according to channel characteristics and background
knowledge base. The Turbo structure further enhances the semantic vectors.
Specifically, the channel encoder utilizes Turbo structure to separate semantic
vectors, ensuring precise transmission of meaning, while the channel decoder
employs Turbo iterative decoding to optimize the representation of semantic
vectors. This deep integration of the transformer and Turbo structure is
ensured by the design of the objective function, semantic extraction, and the
entire training process. Compared with traditional Turbo coding techniques, the
Turbo-DSA shows a faster convergence speed, thanks to its efficient processing
of semantic vectors. Simulation results demonstrate that the Turbo-DSA
surpasses existing benchmarks in key performance indicators, such as bilingual
evaluation understudy scores and sentence similarity. This is particularly
evident under low signal-to-noise ratio conditions, where it shows superior
text semantic transmission efficiency and adaptability to variable marine
channel environments.

</details>


### [73] [Multi-Sensor Distributed Hypothesis Testing in the Low-Power Regime](https://arxiv.org/abs/2511.00645)
*Cécile Bouette,Michèle Wigger*

Main category: cs.IT

TL;DR: 本文分析了分布式假设检验场景中，当决策中心有本地观测时，传感器通过多址信道传输信息的Stein指数特性。主要结论是：在特定条件下，传感器的通信对Stein指数没有帮助。


<details>
  <summary>Details</summary>
Motivation: 研究在分布式假设检验中，当决策中心有额外本地观测时，传感器通过多址信道传输信息是否能够提高检测性能（以Stein指数衡量）。

Method: 通过理论分析，建立了多址信道和成本函数满足特定条件时，分布式设置的Stein指数不超过决策中心本地检验的Stein指数的定理。

Result: 对于加性噪声多址信道（如广义高斯噪声）和全连通离散多址信道，传感器的通信对Stein指数没有改进。对于非全连通离散多址信道，Stein指数更大且等于零速率无噪声通信链路的性能。

Conclusion: 在特定信道条件下，传感器到决策中心的通信对Stein指数没有贡献，决策中心的本地观测已足够。

Abstract: We characterize the Stein-exponent of a distributed hypothesis testing
scenario where two sensors transmit information through a memoryless multiple
access channel (MAC) subject to a sublinear input cost constraint with respect
to the number of channel uses and where the decision center has access to an
additional local observation. Our main theorem provides conditions on the
channel and cost functions for which the Stein-exponent of this distributed
setup is no larger than the Stein-exponent of the local test at the decision
center. Under these conditions, communication from the sensors to the decision
center is thus useless in terms of Stein-exponent. The conditions are satisfied
for additive noise MACs with generalized Gaussian noise under a p-th moment
constraint (including the Gaussian channel with second-moment constraint) and
for the class of fully-connected (where all inputs can induce all outputs)
discrete memoryless multiple-access channels (DMMACs) under arbitrary cost
constraints. We further show that for DMMACs that are not fully-connected, the
Stein-exponent is larger and coincides with that of a setup with zero-rate
noiseless communication links from either both sensors or only one sensor, as
studied in [1].

</details>


### [74] [Improved Decoding Algorithms for MDS and Almost-MDS Codesfrom Twisted GRS Codes](https://arxiv.org/abs/2511.00766)
*Guodong Wang,Hongwei Liu,Jinquan Luo*

Main category: cs.IT

TL;DR: 本文提出了两种高效的扭曲广义里德-所罗门码解码算法，分别针对一般类和几乎MDS类，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有TGRS码解码算法效率不足，需要开发更高效的解码方案来提升性能。

Method: 研究TGRS码的关键方程特性，并基于此设计解码算法；特别针对几乎MDS TGRS码开发专用解码算法。

Result: 提出的两种解码算法在性能上分别优于Sun等人和Sui等人的现有方法。

Conclusion: 成功开发了高效的TGRS码解码算法，为扭曲广义里德-所罗门码的解码提供了更优解决方案。

Abstract: In this paper, firstly, we study decoding of a general class of twisted
generalized Reed-Solomon (TGRS) codes and provide a precise characterization of
the key equation for TGRS codes and propose a decoding algorithm. Secondly, we
further study decoding of almost-MDS TGRS codes and provide a decoding
algorithm. These two decoding algorithms are more efficient in terms of
performance compared with the decoding algorithms presented in [Sun et al.,
IEEE-TIT, 2024] and [Sui et al., IEEE-TIT, 2023] respectively.

</details>


### [75] [An Elementary Approach to MacWilliams Extension Property and Constant Weight Code with Respect to Weighted Hamming Metric](https://arxiv.org/abs/2511.00809)
*Yang Xu,Haibin Kan,Guangyue Han*

Main category: cs.IT

TL;DR: 本文通过初等方法研究了有限域上基于ω-权重的MacWilliams扩展性质和常重码，当ω为常数1映射时，结果恢复了汉明度量码的两个经典结论。


<details>
  <summary>Details</summary>
Motivation: 研究有限域上基于ω-权重的MacWilliams扩展性质和常重码，为汉明度量码的经典结果提供更一般化的理论框架。

Method: 使用初等线性代数和通过双重计数论证推导出的ω-权重子空间的两个关键恒等式。

Result: 建立了ω-权重下的MacWilliams扩展性质和常重码的完整刻画，当ω为常数1时，结果与汉明度量码的经典结论一致。

Conclusion: 该方法为研究更一般的权重函数提供了有效工具，统一了汉明度量码的经典结果。

Abstract: In this paper, we characterize the MacWilliams extension property (MEP) and
constant weight codes with respect to $\omega$-weight defined on
$\mathbb{F}^{\Omega}$ via an elementary approach, where $\mathbb{F}$ is a
finite field, $\Omega$ is a finite set, and
$\omega:\Omega\longrightarrow\mathbb{R}^{+}$ is a weight function. Our approach
relies solely on elementary linear algebra and two key identities for
$\omega$-weight of subspaces derived from a double-counting argument. When
$\omega$ is the constant $1$ map, our results recover two well-known results
for Hamming metric code: (1) any Hamming weight preserving map between linear
codes extends to a Hamming weight isometry of the entire ambient space; and (2)
any constant weight Hamming metric code is a repetition of the dual of Hamming
code.

</details>


### [76] [Fairness Designs for Load Balancing Optimization in Satellite-Cell-Free Massive MIMO Systems](https://arxiv.org/abs/2511.00887)
*Trinh Van Chien,Ngo Tran Anh Thu,Nguyen Hoang Lam,Hien Quoc Ngo,Symeon Chatzinotas,Huynh Thi Thanh Binh*

Main category: cs.IT

TL;DR: 该论文研究天地一体化通信系统中的公平性设计，在负载均衡框架下考虑接入点和卫星的异构接收器，通过遗传算法优化用户关联模式，结合功率控制显著提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 天地一体化通信系统需要在大范围区域内提供普遍服务，但现有系统在用户关联和公平性方面存在优化空间，特别是在异构接收器（AP和卫星）环境下需要有效的负载均衡和公平性设计。

Method: 推导了任意关联模式和信道状态信息不完美情况下的用户上行链路遍历吞吐量，提出了基于遗传算法的低计算复杂度优化方法，并进一步结合功率控制设计了混合遗传算法。

Result: 数值结果表明用户关联模式对网络吞吐量有显著影响，提出的GA算法在小规模网络中与穷举搜索性能相同，在大规模网络中能发现实用的关联模式，结合功率控制的负载均衡方法相比传统方案显著提升性能。

Conclusion: 基于遗传算法的负载均衡方法能有效优化天地一体化通信系统的公平性和性能，特别是在大规模网络中提供实用的解决方案，结合功率控制可进一步显著提升系统性能。

Abstract: Space-ground communication systems are important in providing ubiquitous
services in a large area. This paper considers the fairness designs under a
load-balancing framework with heterogeneous receivers comprising access points
(APs) and a satellite. We derive an ergodic throughput of each user in the
uplink data transmission for an arbitrary association pattern and imperfect
channel state information, followed by a closed-form expression with the
maximum-ratio combining and rich scattering environments. We further formulate
a generic fairness optimization problem, subject to the optimal association
patterns for all the users. Despite the combinatorial structure, the global
optimal solution to the association patterns can be obtained by an exhaustive
search for small-scale networks with several APs and users. We design a low
computational complexity algorithm for large-scale networks based on
evolutionary computation that obtains good patterns in polynomial time.
Specifically, the genetic algorithm (GA) is adapted to the discrete feasible
region and the concrete fairness metrics. We extensively observe the fairness
design problem by incorporating transmit power control and propose a hybrid
genetic algorithm to address the problem. Numerical results demonstrate that
the association pattern to each user has a significant impact on the network
throughput. Moreover, the proposed GA-based algorithm offers the same
performance as an exhaustive search for small-scale networks, while it unveils
interesting practical association patterns as the network dimensions go large.
The load-balancing approach, combined with power control factors, significantly
enhances system performance compared to conventional schemes and configurations
with fixed factors.

</details>


### [77] [HyRES: A Hybrid Replication and Erasure Coding Approach to Data Storage](https://arxiv.org/abs/2511.00896)
*Daniel E. Lucani,Marcell Fehér*

Main category: cs.IT

TL;DR: HyRES是一种混合存储方案，结合了复制和纠删码的优点，在存储成本、文件丢失概率和修复流量方面都优于传统的单一方案。


<details>
  <summary>Details</summary>
Motivation: 传统分布式存储系统通常单独设计复制或纠删码方案，缺乏将两者优势结合的设计灵活性。

Method: 提出HyRES混合方案，综合考虑存储网络规模的影响，通过理论分析和仿真验证其性能。

Result: HyRES在存储成本上低于复制方案，文件丢失概率低于复制和纠删码方案，修复流量在考虑网络规模时甚至低于复制方案。

Conclusion: HyRES方案能够同时实现更低的存储成本、更好的数据可靠性和更有效的修复性能，是分布式存储系统的优化选择。

Abstract: Reliability in distributed storage systems has typically focused on the
design and deployment of data replication or erasure coding techniques.
Although some scenarios have considered the use of replication for hot data and
erasure coding for cold data in the same system, each is designed in isolation.
We propose HyRES, a hybrid scheme incorporates the best characteristics of each
scheme, thus, resulting in additional design flexibility and better potential
performance for the system. We show that HyRES generalizes previously proposed
hybrid schemes. We characterize the theoretical performance of HyRES as well as
that of replication and erasure coding considering the effects of the size of
the storage networks. We validate our theoretical results using simulations.
These results show that HyRES can yield simultaneously lower storage costs than
replication, lower probabilities of file loss than replication and erasure
coding with similar worst case performance, and even lower effective repair
traffic than replication when considering the network size.

</details>


### [78] [Lower Bounds on Conversion Bandwidth for MDS Convertible Codes in Split Regime](https://arxiv.org/abs/2511.00953)
*Lewen Wang,Sihuang Hu*

Main category: cs.IT

TL;DR: 提出了几个关于MDS可转换码带宽成本的新下界，改进了先前结果，并在某些参数范围内与现有构造匹配，证明这些下界是紧的。


<details>
  <summary>Details</summary>
Motivation: 研究MDS可转换码的带宽成本下界，以改进现有理论结果并验证最优性。

Method: 使用线性代数框架推导带宽成本的下界。

Result: 新下界在某些参数范围内改进了先前结果，当$r^F\le r^I\le k^F$时与Maturana和Rashmi的构造匹配，证明下界是紧的。

Conclusion: 提出的线性代数框架成功推导出MDS可转换码的紧带宽成本下界，为相关编码方案提供了理论保障。

Abstract: We propose several new lower bounds on the bandwidth costs of MDS convertible
codes using a linear-algebraic framework. The derived bounds improve previous
results in certain parameter regimes and match the bandwidth cost of the
construction proposed by Maturana and Rashmi (2022 IEEE International Symposium
on Information Theory) for $r^F\le r^I\le k^F$, implying that our bounds are
tight in this case.

</details>


### [79] [Secure Distributed RIS-MIMO over Double Scattering Channels: Adversarial Attack, Defense, and SER Improvement](https://arxiv.org/abs/2511.00959)
*Bui Duc Son,Gaosheng Zhao,Trinh Van Chien,Dong In Kim*

Main category: cs.IT

TL;DR: 该论文研究了分布式多RIS辅助MIMO通信系统中基于自动编码器的对抗攻击与防御，分析了RIS数量与系统性能的关系，并提出基于对抗训练的防御机制来增强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和深度学习在无线通信系统中的广泛应用，这些技术面临的安全漏洞和对抗攻击问题尚未得到足够重视。论文旨在探索分布式多RIS辅助MIMO系统中自动编码器的安全脆弱性。

Method: 提出了分布式多RIS的信道传播模型，包括闭式统计信息驱动的聚合信道建模。选择符号错误率(SER)作为评估指标，详细分析RIS数量与系统性能的关系，并开发基于对抗训练的防御机制。

Result: 数值结果表明，增加RIS数量可降低系统SER，但在白盒攻击场景下会使对抗攻击更具破坏性。提出的防御方法能显著减轻攻击影响，并在无攻击时相比原始模型进一步降低SER。该方法在多普勒引起的信道变化下仍保持鲁棒性。

Conclusion: 论文证明了对抗训练在保护RIS辅助MIMO通信系统中的有效性，为未来安全无线通信系统设计提供了重要参考，特别是在对抗攻击防护方面。

Abstract: There has been a growing trend toward leveraging machine learning (ML) and
deep learning (DL) techniques to optimize and enhance the performance of
wireless communication systems. However, limited attention has been given to
the vulnerabilities of these techniques, particularly in the presence of
adversarial attacks. This paper investigates the adversarial attack and defense
in distributed multiple reconfigurable intelligent surfaces (RISs)-aided
multiple-input multiple-output (MIMO) communication systems-based autoencoder
in finite scattering environments. We present the channel propagation model for
distributed multiple RIS, including statistical information driven in closed
form for the aggregated channel. The symbol error rate (SER) is selected to
evaluate the collaborative dynamics between the distributed RISs and MIMO
communication in depth. The relationship between the number of RISs and the SER
of the proposed system based on an autoencoder, as well as the impact of
adversarial attacks on the system's SER, is analyzed in detail. We also propose
a defense mechanism based on adversarial training against the considered
attacks to enhance the model's robustness. Numerical results indicate that
increasing the number of RISs effectively reduces the system's SER but leads to
the adversarial attack-based algorithm becoming more destructive in the
white-box attack scenario. The proposed defense method demonstrates strong
effectiveness by significantly mitigating the attack's impact. It also
substantially reduces the system's SER in the absence of an attack compared to
the original model. Moreover, we extend the phenomenon to include decoder
mobility, demonstrating that the proposed method maintains robustness under
Doppler-induced channel variations.

</details>


### [80] [Transformer-Based Decoding in Concatenated Coding Schemes Under Synchronization Errors](https://arxiv.org/abs/2511.00999)
*Julian Streit,Franziska Weindel,Reinhard Heckel*

Main category: cs.IT

TL;DR: 提出了BCJRFormer，一种基于Transformer的神经内解码器，用于从多个被插入、删除和替换错误影响的噪声副本中重建码字，特别适用于DNA数据存储场景。


<details>
  <summary>Details</summary>
Motivation: 解决传统BCJR算法在解码多个噪声副本时计算复杂度指数级增长的问题，使其能够处理更多副本，满足DNA数据存储中多读数的需求。

Method: 使用Transformer架构替代BCJR算法作为内解码器，并提出ConvBCJRFormer扩展架构用于卷积码解码，同时用基于Transformer的外解码器替换置信传播解码器。

Result: BCJRFormer在二进制和四进制单消息传输中达到与BCJR算法相当的误码率，且计算复杂度与噪声副本数量呈二次方关系，显著优于BCJR的指数级复杂度。

Conclusion: 构建了一个高效且性能优异的端到端基于Transformer的解码流水线，为DNA数据存储和其他需要处理多个噪声副本的应用提供了可行解决方案。

Abstract: We consider the reconstruction of a codeword from multiple noisy copies that
are independently corrupted by insertions, deletions, and substitutions. This
problem arises, for example, in DNA data storage. A common code construction
uses a concatenated coding scheme that combines an outer linear block code with
an inner code, which can be either a nonlinear marker code or a convolutional
code. Outer decoding is done with Belief Propagation, and inner decoding is
done with the Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm. However, the BCJR
algorithm scales exponentially with the number of noisy copies, which makes it
infeasible to reconstruct a codeword from more than about four copies. In this
work, we introduce BCJRFormer, a transformer-based neural inner decoder.
BCJRFormer achieves error rates comparable to the BCJR algorithm for binary and
quaternary single-message transmissions of marker codes. Importantly,
BCJRFormer scales quadratically with the number of noisy copies. This property
makes BCJRFormer well-suited for DNA data storage, where multiple reads of the
same DNA strand occur. To lower error rates, we replace the Belief Propagation
outer decoder with a transformer-based decoder. Together, these modifications
yield an efficient and performant end-to-end transformer-based pipeline for
decoding multiple noisy copies affected by insertion, deletion, and
substitution errors. Additionally, we propose a novel cross-attending
transformer architecture called ConvBCJRFormer. This architecture extends
BCJRFormer to decode transmissions of convolutional codewords, serving as an
initial step toward joint inner and outer decoding for more general linear code
classes.

</details>


### [81] [Sequence Reconstruction over the Deletion Channel](https://arxiv.org/abs/2511.01071)
*Fengxing Zhu*

Main category: cs.IT

TL;DR: 该论文研究了Levenshtein序列重构问题，在二进制序列和删除信道条件下，确定了重构大小为ℓ-1的候选序列列表所需的最小信道输出数量。


<details>
  <summary>Details</summary>
Motivation: 研究在删除信道中，当传输的码字最多被删除t个符号时，如何通过信道输出来重构原始序列的问题。

Method: 通过分析ℓ≥3个删除球（半径为t）的交集最大可能大小，其中这些球中心是n位二进制序列且互不相同，且满足n≥t+ℓ-1和t≥1的条件。

Result: 确定了在给定条件下，重构大小为ℓ-1的候选序列列表所需的最小信道输出数量。

Conclusion: 解决了二进制序列在删除信道中的序列重构问题，为相关编码理论提供了理论支撑。

Abstract: In this paper, we consider the Levenshtein's sequence reconstruction problem
in the case where the transmitted codeword is chosen from $\{0,1\}^n$ and the
channel can delete up to $t$ symbols from the transmitted codeword. We
determine the minimum number of channel outputs (assuming that they are
distinct) required to reconstruct a list of size $\ell-1$ of candidate
sequences, one of which corresponds to the original transmitted sequence. More
specifically, we determine the maximum possible size of the intersection of
$\ell \geq 3$ deletion balls of radius $t$ centered at $x_1, x_2, \dots,
x_{\ell}$, where $x_i \in \{0,1\}^n$ for all $i \in \{1,2,\dots,\ell\}$ and
$x_i \neq x_j$ for $i \neq j$, with $n \geq t+ \ell-1$ and $t \geq 1$.

</details>


### [82] [Coverage Analysis and Optimization of FIRES-Assisted NOMA and OMA Systems](https://arxiv.org/abs/2511.01111)
*Farshad Rostami Ghadi,Kai-Kit Wong,Masoud Kaveh,Hanjiang Hong,Chan-Byoung Chae,Lajos Hanzo*

Main category: cs.IT

TL;DR: 本文研究了流体集成反射和发射表面（FIRES），这是一种能够同时控制传输和反射、相位及几何定位的智能表面。通过建立覆盖中心系统模型，推导了LoS覆盖边界闭式解，并提出了覆盖最大化的双层优化框架。相比传统STAR-RIS，FIRES通过联合利用几何重定位和被动能量控制显著扩大了覆盖区域。


<details>
  <summary>Details</summary>
Motivation: 传统的同时传输和反射可重构智能表面（STAR-RIS）在覆盖性能上存在局限，需要开发能够同时控制传输反射、相位和几何定位的新型智能表面，以提升两用户下行链路的覆盖性能。

Method: 开发了覆盖中心系统模型，推导了LoS覆盖边界的闭式解；提出了双层优化框架，包括外层FIRES元素位置搜索和内层资源分配；针对OMA和NOMA分别设计了优化策略。

Result: FIRES相比传统STAR-RIS显著扩大了覆盖区域；NOMA在可行时能带来额外的覆盖增益；分析覆盖边界与仿真结果高度匹配；FIRES对相位控制误差具有鲁棒性。

Conclusion: FIRES通过联合利用几何重定位和被动能量控制，为智能表面设计提供了新的可能性，在覆盖性能和相位误差鲁棒性方面优于传统STAR-RIS。

Abstract: Fluid integrated reflecting and emitting surfaces (FIRES) are investigated.
In these metasurfaces, each subarea hosts an active element capable of
simultaneous transmission and reflection, phase, and geometric positioning
control within the subarea. We develop a coverage-centric system model for the
two-user downlink scenario (one user per half-space) under spatially correlated
Rician fading and imperfect phase control. First, we derive closed-form
far-field line-of-sight (LoS) coverage bounds that reveal the effects of
aperture size, base station (BS) distance, transmit power, energy-splitting
(ES), and phase errors. Protocol-aware corollaries are then presented for both
orthogonal multiple access (OMA) and non-orthogonal multiple access (NOMA),
including conditions for successful successive interference cancellation (SIC).
Second, we formulate coverage maximization as a bi-level optimization problem
consisting of (i) an outer search over FIRES element positions, selecting one
active preset per subarea under minimum-spacing constraints, and (ii) an inner
resource allocation problem tailored to the multiple-access scheme, which is
one-dimensional for OMA and a small convex program for NOMA. The proposed
framework explicitly accounts for target rate constraints, ES conservation,
power budgets, geometric placement limits, and decoding-order feasibility.
Extensive simulations demonstrate that FIRES, by jointly exploiting geometric
repositioning and passive energy control, substantially enlarges the coverage
region compared with a conventional simultaneously transmitting and reflecting
reconfigurable intelligent surface (STAR-RIS) under the same element budget.
Furthermore, NOMA yields additional coverage gains when feasible. The
analytical coverage bounds closely match the simulation results and quantify
the robustness of FIRES to phase-control imperfections.

</details>


### [83] [Distributed Matrix Multiplication-Friendly Algebraic Function Fields](https://arxiv.org/abs/2511.01162)
*Yun Long Zhu,Chang-An Zhao*

Main category: cs.IT

TL;DR: 本文提出了适用于分布式矩阵乘法(DMM)的代数函数域，用于扩展多项式码和Matdot码，并建立了多项式代数几何码和Matdot代数几何码的最优恢复阈值。


<details>
  <summary>Details</summary>
Motivation: 将多项式码和Matdot码扩展到代数函数域的主要挑战在于构建最优解码方案，本文旨在解决这一问题。

Method: 通过有理函数域的扩展构造DMM友好的代数函数域，为多项式代数几何码和Matdot代数几何码建立最优恢复阈值。

Result: 提出的函数域支持具有最优恢复阈值的DMM，在特定参数机制下提供超过基有限域大小的有理点。

Conclusion: 虽然这些域可能无法达到最优计算效率，但结果为矩阵乘法实现提供了实际改进，并给出了适用的函数域的具体示例。

Abstract: In this paper, we introduce distributed matrix multiplication (DMM)-friendly
algebraic function fields for polynomial codes and Matdot codes, and present
several constructions for such function fields through extensions of the
rational function field. The primary challenge in extending polynomial codes
and Matdot codes to algebraic function fields lies in constructing optimal
decoding schemes. We establish optimal recovery thresholds for both polynomial
algebraic geometry (AG) codes and Matdot AG codes for fixed matrix
multiplication. Our proposed function fields support DMM with optimal recovery
thresholds, while offering rational places that exceed the base finite field
size in specific parameter regimes. Although these fields may not achieve
optimal computational efficiency, our results provide practical improvements
for matrix multiplication implementations. Explicit examples of applicable
function fields are provided.

</details>


### [84] [Conditional Diffusion Model-Enabled Scenario-Specific Neural Receivers for Superimposed Pilot Schemes](https://arxiv.org/abs/2511.01173)
*Xingyu Zhou,Le Liang,Xinjie Li,Jing Zhang,Peiwen Jiang,Xiao Li,Shi Jin*

Main category: cs.IT

TL;DR: 提出基于条件扩散模型的场景特定信道生成方法，通过数据增强提升神经接收机性能


<details>
  <summary>Details</summary>
Motivation: 神经接收机需要大量场景特定信道数据进行训练，但实际中难以获取，需要生成高质量合成数据来解决数据不足问题

Method: 使用条件扩散模型，根据用户位置和速度信息生成高保真信道样本，用于叠加导频传输的神经接收机训练

Result: 该方法生成的信道样本质量高，显著提升了神经接收机在目标场景中的性能，优于传统数据增强和基于GAN的技术

Conclusion: 基于条件扩散模型的信道生成方法能有效解决训练数据不足问题，为神经接收机提供高质量的数据增强

Abstract: Neural receivers have demonstrated strong performance in wireless
communication systems. However, their effectiveness typically depends on access
to large-scale, scenario-specific channel data for training, which is often
difficult to obtain in practice. Recently, generative artificial intelligence
(AI) models, particularly diffusion models (DMs), have emerged as effective
tools for synthesizing high-dimensional data. This paper presents a
scenario-specific channel generation method based on conditional DMs, which
accurately model channel distributions conditioned on user location and
velocity information. The generated synthetic channel data are then employed
for data augmentation to improve the training of a neural receiver designed for
superimposed pilot-based transmission. Experimental results show that the
proposed method generates high-fidelity channel samples and significantly
enhances neural receiver performance in the target scenarios, outperforming
conventional data augmentation and generative adversarial network-based
techniques.

</details>


### [85] [Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs](https://arxiv.org/abs/2511.01202)
*Bo Bai*

Main category: cs.IT

TL;DR: 该论文从信息论角度提出了LLMs的语义信息理论框架，将基本单位从无意义的比特改为有语义的token，定义了预训练、后训练和推理阶段的信息论度量，并推导了Transformer等架构的理论性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs研究大多基于实验视角，需要大量计算资源和数据，因此需要从理论角度打开LLMs的黑箱，理解其背后的信息论原理。

Method: 以率失真函数、有向信息和格兰杰因果理论为基础，构建LLMs的语义信息理论，定义概率模型和信息论度量，推导Transformer等架构的理论性能。

Result: 提出了语义信息理论框架，定义了token级语义嵌入和信息论最优向量化方法，推导了ELBO、泛化误差界、记忆容量等性能指标。

Conclusion: 为理解LLMs提供了语义信息论的理论框架，为深入研究提供了必要的理论工具。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
numerous real- world applications. While the vast majority of research
conducted from an experimental perspective is progressing rapidly, it demands
substantial computational power, data, and other resources. Therefore, how to
open the black-box of LLMs from a theoretical standpoint has become a critical
challenge. This paper takes the theory of rate-distortion function, directed
information, and Granger causality as its starting point to investigate the
information-theoretic principles behind LLMs, leading to the development of
semantic information theory for LLMs, where the fundamental unit is token,
rather than bits that lacks any semantic meaning. By defining the probabilistic
model of LLMs, we discuss structure-agnostic information-theoretic measures,
such as the directed rate- distortion function in pre-training, the directed
rate-reward function in post-training, and the semantic information flow in
inference phase. This paper also delves deeply into the theory of token-level
semantic embedding and the information-theoretically optimal vectorization
method. Thereafter, we propose a general definition of autoregression LLM,
where the Transformer architecture and its performance such as ELBO,
generalization error bound, memory capacity, and semantic information measures
can be derived theoretically. Other architectures, such as Mamba/Mamba2 and
LLaDA, are also discussed in our framework. Consequently, this paper provides a
theoretical framework for understanding LLMs from the perspective of semantic
information theory, which also offers the necessary theoretical tools for
further in-depth research.

</details>


### [86] [Error-Correcting Codes for Labeled DNA Sequences](https://arxiv.org/abs/2511.01280)
*Dganit Hanania,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 开发用于标记DNA序列的纠错码，针对单替换、插入和删除错误建立边界并构建显式系统编码器。


<details>
  <summary>Details</summary>
Motivation: DNA分子标记是DNA可视化和分析的基本技术，需要开发能够纠正标记过程中可能出现的错误的编码方案。

Method: 建立数学模型，开发系统编码器，针对两种情况进行研究：(1)使用完整的长度二标签集；(2)使用确保从标记中恢复DNA序列的最小长度二标签集。

Result: 建立了单替换、插入和删除错误的纠错码边界，并构建了显式系统编码器。

Conclusion: 成功开发了适用于标记DNA序列的纠错编码方案，为DNA序列分析提供了可靠的错误校正方法。

Abstract: Labeling of DNA molecules is a fundamental technique for DNA visualization
and analysis. This process was mathematically modeled in [1], where the
received sequence indicates the positions of the used labels. In this work, we
develop error correcting codes for labeled DNA sequences, establishing bounds
and constructing explicit systematic encoders for single substitution,
insertion, and deletion errors. We focus on two cases: (1) using the complete
set of length-two labels and (2) using the minimal set of length-two labels
that ensures the recovery of DNA sequences from their labeling for 'almost' all
DNA sequences.

</details>


### [87] [On the Ding and Helleseth's 9th open problem about optimal ternary cyclic codes](https://arxiv.org/abs/2511.01306)
*Peipei Zheng,Dong He,Qunying Liao*

Main category: cs.IT

TL;DR: 本文研究了最优三元循环码的第9个开放问题，通过确定有限域上特殊多项式的根集，给出了该问题的部分解答，并基于这些多项式构造了两类满足球填充界的最优三元循环码。


<details>
  <summary>Details</summary>
Motivation: 循环码是线性码的子类，在消费电子、数据存储和通信系统中具有重要应用，因为其具有高效的编码和解码算法。2013年Ding等人提出了关于最优三元循环码的九个开放问题，目前第9个问题尚未完全解决。

Method: 通过确定有限域上特殊多项式的根集，分析这些多项式的性质，并基于球填充界构造最优三元循环码。

Result: 给出了第9个开放问题的部分解答，并成功构造了两类满足球填充界的最优三元循环码。

Conclusion: 本文对最优三元循环码的第9个开放问题做出了重要贡献，提供了部分解答并构造了新的最优码类，为后续研究奠定了基础。

Abstract: The cyclic code is a subclass of linear codes and has applications in
consumer electronics, data storage systems and communication systems as they
have efficient encoding and decoding algorithms. In 2013, Ding, et al.
presented nine open problems about optimal ternary cyclic codes. Till now, the
1st, 2nd and 6th problems were completely solved, and the 3rd, 7th, 8th and 9th
problems were partially solved. In this manuscript, we focus on the 9th
problem. By determining the root set of some special polynomials over finite
fields, we give an incomplete answer for the 9th problem, and then we construct
two classes of optimal ternary cyclic codes with respect to the Sphere Packing
Bound basing on some special polynomials over finite fields

</details>


### [88] [Several classes of three-weight or four-weight linear codes](https://arxiv.org/abs/2511.01309)
*Qunying Liao,Zhaohui Zhang,Peipei Zheng*

Main category: cs.IT

TL;DR: 构建了有限域F2上的投影三权重线性码和两类投影四权重线性码，并确定了它们的权重分布。


<details>
  <summary>Details</summary>
Motivation: 研究具有特定权重分布的线性码，特别是可用于秘密共享方案的码类。

Method: 使用定义集构造方法，通过加法特征确定权重分布。

Result: 成功构造了投影三权重线性码和两类投影四权重线性码，并确定了它们的权重分布。

Conclusion: 所构造的投影三权重线性码和一类投影四权重线性码可应用于秘密共享方案。

Abstract: In this manuscript, we construct a class of projective three- weight linear
codes and two classes of projective four-weight linear codes over F2 from the
defining sets construction, and determine their weight distributions by using
additive characters. Especially, the projective three-weight linear code and
one class of projective four-weight linear codes (Theorem 4.1) can be applied
in secret sharing schemes.

</details>


### [89] [On the Computability of Finding Capacity-Achieving Codes](https://arxiv.org/abs/2511.01414)
*Angelos Gkekas,Nikos A. Mitsiou,Ioannis Souldatos,George K. Karagiannidis*

Main category: cs.IT

TL;DR: 证明了存在图灵机能够构造容量逼近码：给定离散无记忆信道、目标速率和误差容限，输出满足速率和误码率要求的码字。基于香农信道编码定理，采用穷举搜索方法，形式化为μ递归函数。


<details>
  <summary>Details</summary>
Motivation: 从算法角度研究容量逼近码的构造问题，探讨在可计算实数框架下是否存在算法能自动生成满足信道容量要求的编码方案。

Method: 基于香农信道编码定理，采用穷举搜索方法：系统枚举所有递增块长的码，直到找到有效码。使用递归函数理论形式化构造，得到μ递归函数FindCode。

Result: 证明了存在图灵机能够构造容量逼近码，该机器在信道转移概率、目标速率和误差容限均为可计算实数时有效工作。

Conclusion: 容量逼近码的构造问题可由图灵机解决，前提是相关参数为可计算实数，这是算法可表示的最大实数子集，假设无法进一步弱化。

Abstract: This work studies the problem of constructing capacity-achieving codes from
an algorithmic perspective. Specifically, we prove that there exists a Turing
machine which, given a discrete memoryless channel $p_{Y|X}$, a target rate $R$
less than the channel capacity $C(p_{Y|X})$, and an error tolerance $\epsilon >
0$, outputs a block code $\mathcal{C}$ achieving a rate at least $R$ and a
maximum block error probability below $\epsilon$. The machine operates in the
general case where all transition probabilities of $p_{Y|X}$ are computable
real numbers, and the parameters $R$ and $\epsilon$ are rational. The proof
builds on Shannon's Channel Coding Theorem and relies on an exhaustive search
approach that systematically enumerates all codes of increasing block length
until a valid code is found. This construction is formalized using the theory
of recursive functions, yielding a $\mu$-recursive function $\mathrm{FindCode}
: \mathbb{N}^3 \rightharpoonup \mathbb{N}$ that takes as input appropriate
encodings of $p_{Y|X}$, $R$, and $\epsilon$, and, whenever $R < C(p_{Y|X})$,
outputs an encoding of a valid code. By Kleene's Normal Form Theorem, which
establishes the computational equivalence between Turing machines and
$\mu$-recursive functions, we conclude that the problem is solvable by a Turing
machine. This result can also be extended to the case where $\epsilon$ is a
computable real number, while we further discuss an analogous generalization of
our analysis when $R$ is computable as well. We note that the assumptions that
the probabilities of $p_{Y|X}$, as well as $\epsilon$ and $R$, are computable
real numbers cannot be further weakened, since computable reals constitute the
largest subset of $\mathbb{R}$ representable by algorithmic means.

</details>


### [90] [A Hypergraph based lower bound on Pliable Index Coding based on Nested Side-Information Sets](https://arxiv.org/abs/2511.01539)
*Tulasi Sowjanya B.,Prasad Krishnan*

Main category: cs.IT

TL;DR: 本文提出了一个新的下界方法——嵌套数，用于分析PICOD问题的最优长度。虽然该下界不比现有下界更强，但具有计算优势，并能对特殊结构的PICOD问题给出紧下界。


<details>
  <summary>Details</summary>
Motivation: PICOD问题中寻找最优编码长度和设计短长度编码方案是核心问题。现有下界方法在计算上可能较为复杂，需要开发新的结构参数来提供计算优势。

Method: 引入了一个新的结构参数——嵌套数η(ℋ)，该参数与表示PICOD问题的超图ℋ相关联，用于推导PICOD最优长度的新下界。

Result: 嵌套数下界虽然不比现有下界更强，但提供了计算优势。对于某些特殊结构的PICOD问题，该方法能够获得紧下界。

Conclusion: 嵌套数作为一个新的结构参数，为PICOD问题的最优长度分析提供了有价值的工具，特别是在计算效率和特殊结构问题分析方面具有优势。

Abstract: In pliable index coding (PICOD), a number of clients are connected via a
noise-free broadcast channel to a server which has a list of messages. Each
client has a unique subset of messages at the server as side-information, and
requests for any one message not in the side-information. A PICOD scheme of
length $\ell$ is a set of $\ell$ encoded transmissions broadcast from the
server such that all clients are satisfied. Finding the optimal (minimum)
length of PICOD and designing PICOD schemes that have small length are the
fundamental questions in PICOD. In this paper, we present a new lower bound for
the optimal PICOD length using a new structural parameter called the nesting
number, denoted by $\eta(\ch)$ associated with the hypergraph $\ch$ that
represents the PICOD problem. While the nesting number bound is not stronger
than previously known bounds, it can provide some computational advantages over
them. Also, using the nesting number bound, we obtain novel lower bounds for
some PICOD problems with special structures, which are tight in some cases.

</details>


### [91] [Ergodic Rate Analysis of Two-State Pinching-Antenna Systems](https://arxiv.org/abs/2511.01798)
*Dimitrios Tyrovolas,Sotiris A. Tegos,Yue Xiao,Panagiotis D. Diamantoulakis,Sotiris Ioannidis,Christos Liaskos,George K. Karagiannidis,Stylianos D. Asimonis*

Main category: cs.IT

TL;DR: 本文开发了一个分析框架来评估双状态夹持天线系统（PAS）的速率性能，考虑了实际实现中天线位置的离散性，并引入了夹持离散化效率来量化与理想连续配置的性能偏差。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多假设夹持天线位置可连续重构，忽略了实际实现中只能有有限数量天线位置的离散性限制。本文旨在解决这一实际问题。

Method: 开发了一个分析框架，结合波导的离散空间结构，推导出遍历可达数据速率的闭式表达式，并引入夹持离散化效率指标。

Result: 仿真结果表明，使用有限数量的夹持天线即可实现接近连续配置的性能，为PAS在可编程无线环境中的设计和可扩展性提供了重要见解。

Conclusion: 该分析框架证明了在考虑实际离散限制的情况下，PAS系统仍能实现高性能，为下一代通信网络中可编程无线环境的实际部署提供了理论支持。

Abstract: Programmable wireless environments (PWEs) represent a central paradigm in
next-generation communication networks, aiming to transform wireless
propagation from a passive medium into an intelligent and reconfigurable entity
capable of dynamically adapting to network demands. In this context,
pinching-antenna systems (PASs) have emerged as a promising enabler capable of
reconfiguring both the channel characteristics and the path loss itself by
selectively exciting radiation points along dielectric waveguides. However,
existing studies largely rely on the assumption of continuously reconfigurable
pinching antenna (PA) positions, overlooking the discreteness imposed by
practical implementations, which allow for only a finite number of PA position.
In this paper, an analytical framework is developed for evaluating the rate
performance of two-state PASs, where the antenna locations are fixed, and only
their activation states can be controlled. The analysis incorporates the
discrete spatial structure of the waveguide and leads to a closed-form
expression for the ergodic achievable data rate, while pinching discretization
efficiency is introduced to quantify the performance deviation from the ideal
continuous configuration. Simulation results demonstrate that near-continuous
performance can be achieved with a limited number of PAs, offering valuable
insights into the design and scalability of PASs in PWEs.

</details>


### [92] [Efficient Vector Symbolic Architectures from Histogram Recovery](https://arxiv.org/abs/2511.01838)
*Zirui Deng,Netanel Raviv*

Main category: cs.IT

TL;DR: 本文提出了一种基于级联Reed-Solomon和Hadamard码的噪声弹性向量符号架构，通过解决直方图恢复问题实现了高效编码、准正交性和信息恢复的正式保证。


<details>
  <summary>Details</summary>
Motivation: 随机线性码在向量符号架构中难以在噪声下解码，限制了信息恢复能力，需要一种具有正式保证的噪声弹性解决方案。

Method: 使用Reed-Solomon和Hadamard码的级联，提出直方图恢复问题，并利用列表解码相关算法提供最优解决方案。

Result: 开发出具有高效编码、准正交性和恢复能力的噪声弹性向量符号架构，相比Hadamard码等类似方案在参数上有所改进。

Conclusion: 该方法为向量符号架构提供了不依赖启发式或训练的正式保证，在噪声环境下实现了可靠的信息恢复。

Abstract: Vector symbolic architectures (VSAs) are a family of information
representation techniques which enable composition, i.e., creating complex
information structures from atomic vectors via binding and superposition, and
have recently found wide ranging applications in various neurosymbolic
artificial intelligence (AI) systems. Recently, Raviv proposed the use of
random linear codes in VSAs, suggesting that their subcode structure enables
efficient binding, while preserving the quasi-orthogonality that is necessary
for neural processing. Yet, random linear codes are difficult to decode under
noise, which severely limits the resulting VSA's ability to support recovery,
i.e., the retrieval of information objects and their attributes from a noisy
compositional representation.
  In this work we bridge this gap by utilizing coding theoretic tools. First,
we argue that the concatenation of Reed-Solomon and Hadamard codes is suitable
for VSA, due to the mutual quasi-orthogonality of the resulting codewords (a
folklore result). Second, we show that recovery of the resulting compositional
representations can be done by solving a problem we call histogram recovery. In
histogram recovery, a collection of $N$ histograms over a finite field is given
as input, and one must find a collection of Reed-Solomon codewords of length
$N$ whose entry-wise symbol frequencies obey those histograms. We present an
optimal solution to the histogram recovery problem by using algorithms related
to list-decoding, and analyze the resulting noise resilience. Our results give
rise to a noise-resilient VSA with formal guarantees regarding efficient
encoding, quasi-orthogonality, and recovery, without relying on any heuristics
or training, and while operating at improved parameters relative to similar
solutions such as the Hadamard code.

</details>
