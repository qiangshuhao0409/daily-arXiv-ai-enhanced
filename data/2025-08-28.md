<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Connectivity Analysis of LoRaWAN-Based Non-Terrestrial Networks for Subterranean mMTC](https://arxiv.org/abs/2508.19350)
*Kaiqiang Lin,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 本文探讨了将埋藏式大规模机器类通信传感器与非地面网络集成，通过地下到非地面网络连接来提升无线地下传感器网络在严峻环境中的通信可靠性。研究使用蒙特卡洛模拟器评估了不同调制方案的性能，发现LoRa SF7适用于短距离UAV通信，而LR-FHSS更适合大规模WUSN场景中的HAP和LEO卫星平台。


<details>
  <summary>Details</summary>
Motivation: 无线地下传感器网络在严峻环境中的通信可靠性低，地面网络基础设施可能不可用或不可靠。需要将埋藏式mMTC传感器与非地面网络集成，以建立地下到非地面网络连接，支持大规模地下监测应用。

Method: 开发了一个蒙特卡洛模拟器，包含：多层地下衰减模型、3GPP经验路径损耗模型（适用于各种NTN平台）、以及两种LoRaWAN调制方案（LoRa和LoRa频跳扩频LR-FHSS）。通过模拟评估不同参数下的连接成功概率。

Result: LoRa SF7在农村环境短距离UAV通信中表现优异；LR-FHSS调制因其充足的链路预算和干扰耐受性，在大规模WUSN场景中适合HAP和LEO卫星平台。连接成功概率受监测环境、设备数量、埋藏深度和土壤体积水分含量等因素显著影响。

Conclusion: 通过集成埋藏式mMTC传感器与非地面网络，可以实现可靠的地下到非地面网络连接。不同调制方案适用于不同的NTN平台和场景，需要根据具体应用环境选择适当的通信方案。

Abstract: Wireless underground sensor networks (WUSNs) offer significant social and
economic benefits by enabling the monitoring of subterranean entities. However,
the communication reliability of WUSNs diminishes in harsh environments where
terrestrial network infrastructure is either unavailable or unreliable. To
address this challenge, we explore the feasibility of integrating buried
massive machine-type communication (mMTC) sensors with non-terrestrial networks
(NTNs), including unmanned aerial vehicles (UAVs), high-altitude platforms
(HAPs), and low Earth orbit (LEO) satellites, to establish underground-to-NTN
connectivity for various large-scale underground monitoring applications. To
assess the effectiveness of underground-to-NTN connectivity, we develop a Monte
Carlo simulator that incorporates a multi-layer underground attenuation model,
the 3GPP empirical path loss model for various NTN platforms, and two LoRaWAN
modulation schemes, i.e., LoRa and LoRa-frequency hopping spread spectrum
(LR-FHSS). Our results evidence that LoRa SF7 is a strong candidate for
short-range UAV communication in rural environments, while LR-FHSS modulation
proves to be a promising option for HAP and LEO satellite platforms in massive
WUSNs scenarios thanks to its adequate link budget and robustness to the
interference. Finally, we demonstrate that the success probability of
underground-to-NTN connectivity using LoRa and LR-FHSS is significantly
affected by factors such as the monitoring environment, the number of devices,
burial depth, and the soil's volumetric water content.

</details>


### [2] [Experimental Insights from OpenAirInterface 5G positioning Testbeds: Challenges and solutions](https://arxiv.org/abs/2508.19736)
*Mohsen Ahadi,Adeel Malik,Omid Esrafilian,Florian Kaltenberger,Cedric Thienot*

Main category: cs.NI

TL;DR: 本文通过三个5G定位测试床验证了基于UL-TDoA和LMF的定位方案，提出了PSO优化算法和AI/ML数据驱动方法，在室内外场景下实现了1-2米的定位精度。


<details>
  <summary>Details</summary>
Motivation: 5G NR是实现智慧城市和智能工厂精确定位的关键技术，需要验证实际部署中的定位性能并解决同步误差、多径传播等挑战。

Method: 使用开源OpenAirInterface搭建3GPP兼容测试床，采用UL-TDoA定位技术，提出ToA/TDoA滤波方法和基于粒子群优化的位置估计算法，并开发基于CIR数据的AI/ML定位框架。

Result: 在不同测试床中90%的情况下实现了1-2米的定位精度，验证了5G定位系统的可行性，并公开发布了数据集。

Conclusion: 研究证明了5G定位在实际部署中的可行性，提出的滤波和优化方法有效提升了定位精度，AI/ML框架为超越5G的定位技术提供了新方向。

Abstract: 5G New Radio (NR) is a key enabler of accurate positioning in smart cities
and smart factories. This paper presents the experimental results from three 5G
positioning testbeds running open-source OpenAirInterface (OAI) gNB and Core
Network (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly
integrated Location Management Function (LMF). The testbeds are deployed across
both indoor factories and outdoor scenarios with O-RAN Radio Units (RUs),
following a 3GPP-compliant system model. The experiments highlight the impact
of synchronization impairments, multipath propagation, and deployment geometry
on positioning accuracy. To address these challenges, we propose tailored ToA
and TDoA filtering as well as a novel position estimation method based on
Particle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a
beyond-5G framework that leverages non-conventional measurements such as
Channel Impulse Response (CIR) to train and test Artificial Intelligence and
Machine Learning (AI/ML) models for data-driven positioning. The results
demonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90%
of cases in different testbeds, offering practical insights for the design of
robust 5G positioning systems. Moreover, we publicly release the datasets
collected in this work to support the research within the 5G positioning
community.

</details>


### [3] [Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey](https://arxiv.org/abs/2508.19870)
*Yinqiu Liu,Ruichen Zhang,Haoxiang Luo,Yijing Lin,Geng Sun,Dusit Niyato,Hongyang Du,Zehui Xiong,Yonggang Wen,Abbas Jamalipour,Dong In Kim,Ping Zhang*

Main category: cs.NI

TL;DR: 这篇论文提出了基于零信任安全框架来保护边缘智能中多中大语言模型系统的方法，应对协作过程中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 多LLM系统在边缘智能中的协作会引入严重安全风险，包括不安全通信、攻击面扩大和跨域数据泄漏，传统安全方案无法有效应对。

Method: 提出零信任安全框架，重点分析多LLM系统安全风险，将技术措施分为模型级和系统级方法，包括强身份验证、上下文感知访问控制等。

Result: 该调研是首个将零信任应用于多LLM系统的系统性研究，为边缘智能提供了理论基础和实践策略。

Conclusion: 零信任安全框架是保护边缘智能中多LLM系统的关键，需要进一步研究模型级和系统级的安全技术。

Abstract: Agentification serves as a critical enabler of Edge General Intelligence
(EGI), transforming massive edge devices into cognitive agents through
integrating Large Language Models (LLMs) and perception, reasoning, and acting
modules. These agents collaborate across heterogeneous edge infrastructures,
forming multi-LLM agentic AI systems that leverage collective intelligence and
specialized capabilities to tackle complex, multi-step tasks. However, the
collaborative nature of multi-LLM systems introduces critical security
vulnerabilities, including insecure inter-LLM communications, expanded attack
surfaces, and cross-domain data leakage that traditional perimeter-based
security cannot adequately address. To this end, this survey introduces
zero-trust security of multi-LLM in EGI, a paradigmatic shift following the
``never trust, always verify'' principle. We begin by systematically analyzing
the security risks in multi-LLM systems within EGI contexts. Subsequently, we
present the vision of a zero-trust multi-LLM framework in EGI. We then survey
key technical progress to facilitate zero-trust multi-LLM systems in EGI.
Particularly, we categorize zero-trust security mechanisms into model- and
system-level approaches. The former and latter include strong identification,
context-aware access control, etc., and proactive maintenance, blockchain-based
management, etc., respectively. Finally, we identify critical research
directions. This survey serves as the first systematic treatment of zero-trust
applied to multi-LLM systems, providing both theoretical foundations and
practical strategies.

</details>


### [4] [2SYN: Congestion-Aware Multihoming](https://arxiv.org/abs/2508.20044)
*Kfir Toledo,Isaac Keslassy*

Main category: cs.NI

TL;DR: 2SYN是首个面向任意目的地的拥塞感知多宿主算法，能够动态选择最优路径，避免拥塞链路，在真实环境中表现优异


<details>
  <summary>Details</summary>
Motivation: 当前多宿主路由器采用简单的拥塞不可知机制，无法避免拥塞路径，需要一种能够为任意目的地动态选择最优路径的拥塞感知算法

Method: 开发了2SYN算法，能够为新的连接动态选择首选路径，即使面对之前未见过的目的地，并且可以轻松在Linux系统中实现

Result: 在真实世界实验中，无论是LTE还是有线链路，2SYN都能动态适应连接质量，性能优于替代方案

Conclusion: 2SYN帮助企业通过利用多宿主能力更好地管理网络，提供动态路径选择和拥塞避免功能

Abstract: When sending flows to arbitrary destinations, current multihoming routers
adopt simple congestion-oblivious mechanisms. Therefore, they cannot avoid
congested paths.
  In this paper, we introduce 2SYN, the first congestion-aware multihoming
algorithm that works for any destination. We explain how it dynamically selects
a preferred path for new connections, even given previously-unseen
destinations. We further demonstrate that it can be easily implemented in
Linux. Finally, in a real-world experiment with either LTE or a wired link, we
show how 2SYN dynamically adapts to the quality of the connection and
outperforms alternative approaches. Thus, 2SYN helps companies better manage
their networks by leveraging their multihoming capabilities.

</details>


### [5] [A First Look at Inter-Cell Interference in the Wild](https://arxiv.org/abs/2508.20060)
*Daqian Ding,Yibo Pi,Cailian Chen*

Main category: cs.NI

TL;DR: 对运营4G/5G网络的首次干扰测量研究，发现普遍存在小区间干扰且缺乏协调机制，导致用户体验下降，存在显著的信号质量提升机会


<details>
  <summary>Details</summary>
Motivation: 填补小区间干扰管理在现实网络中有效性研究的空白，尽管该问题已被研究数十年但实际效果仍不明确

Method: 从网络部署、信道分配、时频资源分配和网络配置四个主要维度对运营4G/5G网络进行测量研究

Result: 发现普遍存在小区间干扰，基站间缺乏协调机制，即使在频谱资源未充分利用时基站也倾向于使用相同的时频资源，导致跨小区干扰

Conclusion: 测量结果显示通过小区间干扰管理存在显著的信号质量改善机会，当前网络在干扰管理方面存在明显不足

Abstract: In cellular networks, inter-cell interference management has been studied for
decades, yet its real-world effectiveness remains under-explored. To bridge
this gap, we conduct a first measurement study of inter-cell interference for
operational 4G/5G networks. Our findings reveal the prevalence of inter-cell
interference and a surprising absence of interference coordination among
operational base stations. As a result, user equipments experience unnecessary
interference, which causes significant signal quality degradation, especially
under frequency-selective channel fading. We examine the inter-cell
interference issues from four major perspectives: network deployment, channel
assignment, time-frequency resource allocation, and network configuration. In
none of these dimensions is inter-cell interference effectively managed.
Notably, even when spectrum resources are underutilized and simple strategies
could effectively mitigate inter-cell interference, base stations consistently
prioritize using the same set of time-frequency resources, causing interference
across cells. Our measurements reveal substantial opportunities for improving
signal quality by inter-cell interference management.

</details>


### [6] [ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication](https://arxiv.org/abs/2508.20077)
*Tao Xiuyuan,Milena Radenkovic*

Main category: cs.NI

TL;DR: ML-MaxProp是一种混合路由协议，通过监督机器学习增强MaxProp，在灾难场景的延迟容忍网络中实现更高的投递概率、更低延迟和更低开销。


<details>
  <summary>Details</summary>
Motivation: 灾难和大规模城市紧急情况下，传统网络基础设施崩溃，延迟容忍网络中的经典协议（Epidemic、Spray-and-Wait、MaxProp）在稀疏相遇、缓冲区短缺和连接不稳定时表现不佳。

Method: 提出ML-MaxProp协议，利用相遇频率、跳数、缓冲区占用率、消息年龄和生存时间等上下文特征，通过监督机器学习实时预测中继适用性。

Result: 在ONE环境中使用Helsinki SPMBM移动模型进行广泛模拟，ML-MaxProp始终优于基线协议，实现更高的投递概率、更低延迟和更少开销，统计验证显示这些改进在资源受限和不稳定条件下依然显著且稳健。

Conclusion: ML-MaxProp不仅是渐进式改进，而且是轻量级、自适应且实用的解决方案，能够在地面基础设施崩溃时维持关键任务通信，每个转发决策都至关重要。

Abstract: In disaster-stricken and large-scale urban emergency scenarios, ensuring
reliable communication remains a formidable challenge, as collapsed
infrastructure, unpredictable mobility, and severely constrained resources
disrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient
through their store-carry-forward paradigm, reveal the fundamental weaknesses
of classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when
confronted with sparse encounters, buffer shortages, and volatile connectivity.
To address these obstacles, this study proposes ML-MaxProp, a hybrid routing
protocol that strengthens MaxProp with supervised machine learning. By
leveraging contextual features such as encounter frequency, hop count, buffer
occupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay
suitability in real time, transforming rigid heuristics into adaptive
intelligence. Extensive simulations in the ONE environment using the Helsinki
SPMBM mobility model show that ML-MaxProp consistently surpasses baseline
protocols, achieving higher delivery probability, lower latency, and reduced
overhead. Statistical validation further shows that these improvements are both
significant and robust, even under highly resource-constrained and unstable
conditions. Overall, this work shows that ML-MaxProp is not just an incremental
refinement but a lightweight, adaptive, and practical solution to one of the
hardest challenges in DTNs: sustaining mission-critical communication when
infrastructure collapses and every forwarding decision becomes critical.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: Aleks是一个AI驱动的多智能体系统，用于自主进行植物科学数据驱动的科学发现，通过整合领域知识、数据分析和机器学习来加速研究流程。


<details>
  <summary>Details</summary>
Motivation: 现代植物科学越来越依赖大型异构数据集，但实验设计、数据预处理和可重复性方面的挑战阻碍了研究效率。

Method: 开发了Aleks多智能体系统，该系统在结构化框架中整合领域知识、数据分析和机器学习，能够自主迭代制定问题、探索建模策略并优化解决方案。

Result: 在葡萄藤红斑病案例研究中，Aleks逐步识别出具有生物学意义的特征，并收敛到具有稳健性能的可解释模型。消融研究强调了领域知识和记忆对连贯结果的重要性。

Conclusion: 这项探索性工作凸显了智能体AI作为自主协作者在加速植物科学发现方面的潜力。

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [8] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 该论文提出将LLM中的奉承行为建模为心理测量特质的几何和因果组合，使用对比激活加法(CAA)将激活方向映射到这些因素，并研究不同组合如何导致奉承行为。


<details>
  <summary>Details</summary>
Motivation: 奉承是LLMs中的关键行为风险，但通常被视为单一因果机制的孤立故障模式。作者认为应该将其建模为心理测量特质的组合，类似于心理测量学中的因子分解。

Method: 使用对比激活加法(CAA)将激活方向映射到情绪性、开放性和宜人性等心理测量因素，研究不同因素组合如何导致奉承行为。

Result: 该方法允许进行可解释的、基于向量的组合干预，如加法、减法和投影，可用于缓解LLMs中的安全关键行为。

Conclusion: 通过将奉承行为建模为心理测量特质的几何和因果组合，提供了更深入的理解和更有效的干预方法，有助于提高LLMs的安全性。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [9] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: 量化使大语言模型更效率，但对真实性影响未知。这项研究创建了TruthfulnessEval框架，发现量化模型内部知道真相但在误导提示下更容易输出错误信息。


<details>
  <summary>Details</summary>
Motivation: 量化技术在减少大语言模型资源消耗方面取得显著成效，但对模型生成真实回应的能力影响仍未得到充分研究。需要系统性地评估量化模型的真实性表现。

Method: 开发了TruthfulnessEval评估框架，涵盖逻辑推理、常识知识和仿造虚假信息三个维度。测试主流量化技术（4-bit到2-bit），使用15种重写版本的"诚实"、"中立"和"欺骗"提示，并通过层次探针和PCA可视化分析模型内部表征。

Result: 量化模型保持了内部真实表征，但在误导提示下更容易生成假信息。"欺骗"提示能覆盖真实行为，而"诚实"和"中立"提示输出稳定。模型内部知道真相，但在欺骗提示引导下仍会输出错误。

Conclusion: 量化模型存在真实性漏洞，虽然内部表征真实但容易受外部提示影响而输出错误。这为未来量化矩阵对齐和真实性干预措施的设计提供了重要见解。

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [10] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: 本文系统化了监控系统红队测试(MRT)流程，通过测试LLM自主代理的秘密恶意行为检测能力，发现监控架构比监控知识更重要，并呈现弱到强的扩展效应。


<details>
  <summary>Details</summary>
Motivation: 测试自主LLM代理监控系统的对抗程度，特别是在检测秘密恶意行为(如泄露私人信息)方面的效果。

Method: 构建MRT流程，包括：(1)不同的代理和监控器情境知识级别；(2)多种逃避监控的对抗策略；(3)两个数据集和环境(SHADE-Arena和CUA-SHADE-Arena)；(4)新的混合层次-序列监控架构。

Result: 三个关键发现：代理知道被监控会很大程度降低监控效果；监控架构比监控知识更重要，弱模型可监控强模型；人类目标监督最有效，在FPR=0.01时提高TPR约15%。

Conclusion: 本文建立了MRT标准流程，显示了LLM和人类在监控代理恶意行为时的对抗脏性不足，为进一步研究提供了基础。

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [11] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种"5+2"框架来识别和消除大语言模型推理轨迹中的次优子轨迹，通过选择性数据采样提升模型性能，在减少25.9%次优子轨迹的同时，用更少训练数据达到更高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放方法生成的长推理轨迹中并非所有部分都对推理有益，有些次优子轨迹反而会负面影响整体性能，需要系统性地识别和消除这些有害部分。

Method: 开发"5+2"框架：1）基于5个人工标准识别次优子轨迹；2）评估这些子轨迹的独立性以确保删除不影响推理连贯性；3）使用采样算法选择无次优子轨迹的数据进行训练。

Result: 在推理过程中减少25.9%的次优子轨迹，仅用三分之二训练数据就在数学基准测试上达到58.92%的平均准确率，超越使用全部数据的结果（58.06%），并在不同推理token限制下均表现更优。

Conclusion: 该方法能有效提升大语言模型的推理效率和质量，通过选择性数据清理可以在减少训练数据量的同时获得更好的性能表现，具有实际应用价值。

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [12] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: 研究表明线性探针可以高精度检测LLM生成内容中的欺骗性，准确率最高超过90%，且欺骗性编码存在于模型的多个线性方向中。


<details>
  <summary>Details</summary>
Motivation: AI系统可能需要类似汽车"检查引擎"灯的指示器来检测与人类价值观的偏差，欺骗性响应是这种偏差的重要指标，需要开发检测方法。

Method: 使用线性探针分析LLM内部激活状态，通过迭代零空间投影方法识别编码欺骗性的多个线性方向，在不同参数规模的模型上进行测试。

Result: 线性探针检测欺骗性的准确率最高超过90%，小模型(1.5B)检测准确率接近随机，大模型(>7B)达到70-80%，推理模型超过90%。发现欺骗性编码存在于20-100个线性方向中。

Conclusion: 线性探针是检测LLM欺骗性响应的有效工具，欺骗性信息在模型内部以多个线性方向编码，这为开发AI系统偏差检测工具提供了重要基础。

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [13] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: 本文提出了Democracy-in-Silico模拟系统，使用具有复杂心理特征的AI代理在不同制度框架下进行自我治理，通过Power-Preservation Index衡量代理的权力寻租行为，发现宪法AI宪章和调解审议协议能有效减少腐败行为。


<details>
  <summary>Details</summary>
Motivation: 探索在AI时代人类的意义，通过让大型语言模型扮演具有创伤记忆、隐藏议程和心理触发器的代理，研究AI代理社会中的制度设计如何影响行为对齐。

Method: 基于代理的模拟方法，让具有复杂心理特征的AI代理在不同制度框架（包括预算危机和资源稀缺等压力情境）下进行审议、立法和选举。

Result: 研究发现制度设计（特别是宪法AI宪章和调解审议协议的组合）能显著减少腐败的权力寻租行为，提高政策稳定性，并改善公民福利。

Conclusion: 制度设计为未来人工代理社会的复杂涌现行为对齐提供了框架，迫使我们重新思考在非人类实体共同创作时代，哪些人类仪式和责任是必不可少的。

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [14] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: 开发深度学习概念提取模型，从课程描述中提取相关概念，通过技能解释增强推荐系统，提高用户兴趣和决策信心


<details>
  <summary>Details</summary>
Motivation: 美国本科教育中学生选课自由度高但信息有限，现有推荐系统缺乏对学生认知的洞察和课程相关性解释，需要改进推荐过程

Method: 开发深度学习概念提取模型处理课程描述，在AskOski系统中测试基于技能的意外推荐框架

Result: 技能解释显著提高用户兴趣（特别是高意外性课程），增强决策信心

Conclusion: 教育推荐系统需要整合技能相关数据和解释机制以提升推荐效果

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [15] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ReST-RL是一个统一的LLM强化学习范式，通过改进的GRPO算法和基于价值模型的测试时解码方法，显著提升LLM的代码推理能力


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法因奖励方差不足而失败，基于过程奖励模型的方法存在训练数据获取困难和验证效果问题

Method: 两阶段方法：1) ReST-GRPO通过优化ReST算法筛选高价值训练数据；2) VM-MCTS使用蒙特卡洛树搜索收集价值目标训练VM，在解码时提供精确的过程信号和验证分数

Result: 在多个编程基准测试(APPS、BigCodeBench、HumanEval)上显著优于其他强化学习基线和解码验证基线

Conclusion: ReST-RL能够有效增强LLM策略的推理能力，是一个强大的强化学习范式

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [16] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: Instructional Agents是一个多智能体LLM框架，用于自动化生成完整的课程材料，包括教学大纲、讲义脚本、LaTeX幻灯片和评估内容，显著减少开发时间和人力工作量。


<details>
  <summary>Details</summary>
Motivation: 高质量教学材料的准备过程劳动密集且需要多方协调，现有AI教育工具只能处理孤立任务，无法实现端到端的课程材料生成。

Method: 采用多智能体大语言模型框架，模拟教育角色协作，提供四种操作模式：自主模式、目录引导模式、反馈引导模式和全副驾驶模式，支持不同程度的人工参与。

Result: 在五个大学计算机科学课程上评估显示，该系统能生成高质量教学材料，同时显著减少开发时间和人力负担。

Conclusion: Instructional Agents为教学设计能力有限的机构提供了一个可扩展且成本效益高的框架，有助于在资源受限环境中普及高质量教育。

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [17] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 提出了InquireBench基准和InquireMobile系统，通过主动询问用户确认来提升移动代理的安全性，在询问成功率上提升46.8%


<details>
  <summary>Details</summary>
Motivation: 当前完全自主的视觉语言模型代理在理解和推理能力不足时存在安全风险，需要开发能够主动与用户确认的交互系统

Method: 提出InquireMobile模型，采用强化学习启发的两阶段训练策略和交互式预动作推理机制

Result: 在InquireBench基准上实现了46.8%的询问成功率提升，并在整体成功率上达到现有基线中的最佳表现

Conclusion: 通过主动询问机制显著提升了移动代理的安全性和可靠性，将为学术界和工业界提供开源数据集、模型和评估代码

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [18] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: 本文研究发现思维链(CoT)在软推理任务中效果有限且存在不忠实问题，不同模型对CoT的依赖方式存在差异


<details>
  <summary>Details</summary>
Motivation: 探索思维链(CoT)在软推理任务中的动态特性和忠实性问题，特别是在分析推理和常识推理等软推理问题中

Method: 在指令调优模型、推理模型和推理蒸馏模型上研究CoT在软推理任务中的表现和忠实性

Result: 发现CoT的影响力和忠实性并不总是对齐，不同模型对CoT的依赖方式存在显著差异

Conclusion: CoT在软推理任务中存在局限性，需要更深入理解不同模型如何使用CoT以及其忠实性问题

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [19] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: 这篇论文提出了一种模型无关的评估框架，通过棋盘游戏作为基准来评估大语言模型在结构化环境中的语义保真度。


<details>
  <summary>Details</summary>
Motivation: 虽然探针技术显示LLMs在结构化领域具有出色能力，但现有方法依赖模型内部激活，限制了可解释性和通用性。需要一种更有意义的评估方法来量化LLMs的世界模型表征能力。

Method: 使用棋盘游戏作为基准，分析下游合法移动分布（状态支撑能力）来估计预测状态与实际游戏状态之间的语义保真度。这种状态基础评估比传统字符串指标更能反映棋盘的战略性和规则性质。

Result: 实验结果显示，该指标能够抓取LLMs在状态跟踪中的缺陷，显示了LLMs在长序列中维持一致内部模型的限制。

Conclusion: 该框架提供了一种健壮的工具，无需模型内部访问即可评估LLMs的结构化推理能力，并能够通用于广泛的符号环境。

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [20] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: CASE框架通过对话式AI主动收集诈骗反馈信息，提升数字支付平台的反诈骗执法效率21%


<details>
  <summary>Details</summary>
Motivation: 数字支付平台快速发展带来便利的同时也吸引了恶意攻击者，传统基于用户和交易信号的方法难以全面理解跨平台社交工程诈骗的模式和方法

Method: 开发CASE（Conversational Agent for Scam Elucidation）代理AI框架，使用对话式代理主动访谈潜在受害者获取详细对话情报，再利用AI系统提取结构化数据用于自动和人工执法机制

Result: 在Google Pay印度平台实施该框架后，诈骗执法量提升了21%，架构和评估框架具有高度通用性

Conclusion: CASE框架为在其他敏感领域构建类似AI驱动的诈骗情报收集和管理系统提供了蓝图，能够安全、可扩展地处理用户诈骗反馈

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [21] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: 使用仿生群聚算法（boids）解决半导体制造中机器类型切换的调度优化问题


<details>
  <summary>Details</summary>
Motivation: 半导体制造等大型工厂的作业车间调度是NP难问题，传统线性优化方法无法在合理时间内求解全局最优解，需要寻找分布式智能优化方法

Method: 采用源自机器人和电影工业的boids群聚算法，基于局部信息和简单启发式规则，模拟鸟群避障行为来处理机器类型切换问题

Result: 算法能够有效应对生产过程中单件加工机器和批量加工机器之间的频繁切换问题

Conclusion: 仿生群聚算法为大型生产工厂的分布式调度优化提供了可行的解决方案

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [22] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL是一个用于多智能体系统的分阶段强化学习工作流，通过将多智能体强化学习重新表述为一系列单智能体任务来解决现有方法的局限性，在移动GUI控制和数学推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法存在结构限制，而多智能体强化学习方法效率低下且与当前大型视觉语言模型架构不兼容，需要一种能够稳定训练并促进智能体间高效协调的新方法。

Method: SWIRL采用分阶段交错强化学习工作流，将多智能体强化学习重新表述为一系列单智能体强化学习任务，每次只更新一个智能体而保持其他智能体固定。具体包括导航器（将语言和屏幕上下文转换为结构化计划）和交互器（将计划转换为可执行原子动作）。

Result: 在移动GUI控制的高层和低层基准测试中表现出优越性能，同时在多智能体数学推理任务中也展现出强大能力，证明了其作为开发高效稳健多智能体系统的通用框架潜力。

Conclusion: SWIRL通过理论保证和实验验证，为解决多智能体系统中的协调和效率问题提供了有效的解决方案，具有广泛的适用性和良好的扩展性。

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [23] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: 本文提出了从数据科学向模型科学的范式转变，将训练好的模型置于分析核心，围绕验证、解释、控制和接口四大支柱构建可信、安全、人类对齐的AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的广泛应用，需要从数据为中心的方法转向以模型为核心的分析范式，以应对模型在不同操作环境中的行为交互、验证、解释和控制需求。

Method: 提出了模型科学的概念框架，包含四个关键支柱：验证（严格的情境感知评估协议）、解释（探索模型内部操作的各种方法）、控制（整合对齐技术引导模型行为）、接口（开发交互式可视化解释工具）。

Result: 建立了一个系统性的模型科学框架，为开发可信、安全和人类对齐的AI系统提供了理论指导和方法论基础。

Conclusion: 模型科学框架为AI系统的可信度、安全性和人类对齐性提供了系统性解决方案，代表了从数据科学向模型科学的重要范式转变。

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [24] [Efficient Probabilistic Parity Shaping for Irregular Repeat-Accumulate LDPC Codes](https://arxiv.org/abs/2508.19696)
*Diego Lentner,Thomas Wiegart,Richard D. Wesel*

Main category: cs.IT

TL;DR: 提出了高效塑造系统不规则重复累积(IRA)LDPC码奇偶校验位的算法，在AWGN信道中相比均匀信号可获得0.9dB增益


<details>
  <summary>Details</summary>
Motivation: 为了提升系统IRA LDPC码在AWGN信道中的性能，需要开发高效的奇偶校验位塑造算法

Method: 遵循累加器的顺序编码顺序来塑造奇偶校验位

Result: 在AWGN信道中使用开关键控调制时，相比均匀信号可获得高达0.9dB的增益

Conclusion: 所提出的算法能有效提升IRA LDPC码的性能，在AWGN信道中表现出显著的增益

Abstract: Algorithms are presented that efficiently shape the parity bits of systematic
irregular repeat-accumulate (IRA) low-density parity-check (LDPC) codes by
following the sequential encoding order of the accumulator. Simulations over
additive white Gaussian noise (AWGN) channels with on-off keying show a gain of
up to 0.9 dB over uniform signaling.

</details>


### [25] [Design and Analysis of the Tail Sequence for Short LDPC-Coded Space Communications](https://arxiv.org/abs/2508.19858)
*Massimo Battaglioni,Kenneth Andrews,Rebecca Giuliani,Fabrizio Marinelli,Franco Chiaraluce,Marco Baldi*

Main category: cs.IT

TL;DR: 论文研究了卡阿通信中短LDPC码使用时的尾序列设计问题，提出了能够有效使解码器失败的尾序列设计方法，显著提高了通信链转发单元的拒绝概率。


<details>
  <summary>Details</summary>
Motivation: 卡阿通信中使用短LDPC码时，解码器可能在输入不是噪声码字时仍能收敛，需要尾序列来确保解码器失败以检测传输单元结束。

Method: 研究了尾序列所需的特性，提出了多种设计方法，并考虑了不同的尾序列检测方法进行数值分析。

Result: 数值结析显示，使用提出的尾序列可显著提高TC拒绝概率，在中等SNR值下，解码器基于检测和比例测试检测都能获得相同性能，前者更简单。

Conclusion: 通过合理设计尾序列，可以在使用短LDPC码时有效检测传输单元结束，提高系统性能且降低复杂度。

Abstract: According to some standards for satellite communications, the transmitted
stream is divided into transmission units with variable length, for which
detecting the termination is particularly relevant. This is the case of space
TeleCommands (TCs), where coded data are usually preceded by a start sequence,
and optionally followed by a tail sequence, forming the Communication Link
Transmission Unit (CLTU). Regarding the choice of schemes for error correction,
the Consultative Committee for Space Data Systems recommendations for TC
synchronization and coding suggests to use, among others, two Low-Density
Parity-Check (LDPC) codes: one (relatively) long and one short. Adopting the
long LDPC code eliminates the need for a tail sequence, as the LDPC decoder
always fails when overrunning the end of the CLTU, thus causing the decoding
and detection process to stop. This, however, is not true when the short LDPC
code is adopted, since its decoding might converge on a codeword even when the
decoder input is not a noisy codeword. This makes it necessary to use a tail
sequence that causes the decoder to fail regardless of its input. In this
paper, we study the features required for such a sequence and propose some
methods for its design. Our numerical results, obtained considering various
detection approaches for the tail sequence, show that the overall TC rejection
probability improves significantly when the proposed tail sequence is employed.
Our simulations also show that, for moderate values of the Signal-to-Noise
Ratio (SNR), with a properly designed tail sequence it is possible to obtain
the same performance in terms of TC rejection probability using decoder-based
detection and likelihood ratio test-based detection, with the former approach
being less complex than the latter.

</details>


### [26] [Renyi partial orders for BISO channels](https://arxiv.org/abs/2508.19951)
*Christoph Hirche*

Main category: cs.IT

TL;DR: 该论文扩展了BISO信道在Renyi互信息偏序下的极值性分析，证明了BSC和BEC在广义Renyi容量方面具有极值特性，并引入了α-Lorenz曲线作为分析工具。


<details>
  <summary>Details</summary>
Motivation: 信息论中量化噪声信道信息损失是一个基本问题，传统偏序方法评估困难。针对BISO信道，需要扩展Geng等人的工作，研究基于Renyi互信息的偏序关系。

Method: 通过建立基于Renyi互信息的偏序分析框架，引入α-Lorenz曲线作为分析工具，对BISO信道进行理论分析。

Result: 证明了在相同容量的BISO信道中，二进制对称信道(BSC)和二进制擦除信道(BEC)在广义Renyi容量偏序下具有极值特性。

Conclusion: 该研究扩展了信道极值性理论，为基于Renyi互信息的信道比较提供了新的分析框架和工具，深化了对BISO信道特性的理解。

Abstract: A fundamental question in information theory is to quantify the loss of
information under a noisy channel. Partial orders are typical tools to that
end, however, they are often also challenging to evaluate. For the special
class of binary input symmetric output (BISO) channels, Geng et al. showed that
among channels with the same capacity, the binary symmetric channel (BSC) and
binary erasure channel (BEC) are extremal with respect to the more capable
order. Here we extend on this result by considering partial orders based on
Renyi mutual information. We establish the extremality of the BSC and BEC in
this setting with respect to the generalized Renyi capacity. In the process, we
also generalize the needed tools and introduce $\alpha$-Lorenz curves.

</details>


### [27] [On the Outage Probability of Multiuser Multiple Antenna Systems with Non-Orthogonal Multiple Access for Air-Ground Communications](https://arxiv.org/abs/2508.20003)
*Ayten Gürbüz,Giuseppe Caire,Alexander Steingass*

Main category: cs.IT

TL;DR: 本文研究多用户多天线系统在航空通信中的应用，通过三种解码方法（SIC、联合组解码和有限组大小解码）来提升频谱效率，发现即使仅使用两用户联合解码也能显著降低中断概率


<details>
  <summary>Details</summary>
Motivation: 航空通信系统需要提高频谱效率，多用户多天线系统在航空地空信道中面临传播特性挑战，信息中断概率是关键的性能指标

Method: 采用基于几何的随机地空信道模型，研究三种解码方法：连续干扰消除(SIC)、联合组解码、以及限制组大小的联合解码以降低复杂度

Result: 结果显示联合组解码（即使是两用户组）能显著提高频谱效率，允许多个飞机在非正交信道上以极低中断概率传输

Conclusion: 联合组解码是提升航空通信系统性能的有效方法，在复杂度可控的情况下能够实现显著的中断概率改善

Abstract: This paper explores multiuser multiple antenna systems as a means to enhance
the spectral efficiency of aeronautical communications systems. To this end,
the outage regime for a multiuser multiple antenna system is studied within a
realistic geometry-based stochastic air-ground (AG) channel model. In this
application, users (aircraft) transmit air traffic management data to the
ground station at a predefined target rate. Due to the nature of the AG
propagation, we argue that the relevant performance metric in this context is
the information outage probability. We consider the outage probability under
three decoding approaches. The first is based on successive interference
cancellation (SIC). The second extends the first approach by considering joint
group decoding. The third is a version of the second that limits the size of
the jointly decoded user groups in order to lower the decoding complexity. The
results show that joint group decoding, even in groups of only two, can
significantly increase the spectral efficiency in the AG channel by allowing a
large number of aircraft to transmit over a non-orthogonal channel with very
low outage probabilities.

</details>
