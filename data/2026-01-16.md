<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 4]
- [cs.AI](#cs.AI) [Total: 53]
- [cs.IT](#cs.IT) [Total: 41]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Starfield: Demand-Aware Satellite Topology Design for Low-Earth Orbit Mega Constellations](https://arxiv.org/abs/2601.10083)
*Shayan Hamidi Dehshali,Tzu-Hsuan Liao,Shaileshh Bojja Venkatakrishnan*

Main category: cs.NI

TL;DR: Starfield：一种基于流量需求的卫星拓扑设计算法，通过黎曼度量优化星间链路选择，相比传统+Grid拓扑减少30%跳数和15%拉伸因子


<details>
  <summary>Details</summary>
Motivation: 现有卫星拓扑设计（如+Grid和Motif）忽略区域流量、地面站位置和星座几何结构。考虑到地球人口分布不均和农村地区隔离，流量模式具有非均匀性，这为根据流量模式定向星间链路提供了机会。

Method: 提出Starfield算法：1) 根据流量流在星座壳层上构建向量场；2) 在球面流形上定义相应的黎曼度量；3) 结合空间几何为每个潜在星间链路分配距离；4) 聚合所有需求流生成每个卫星的链路选择启发式；5) 每个卫星选择具有最小黎曼启发式的链路及其对应的角度链路。

Result: 对于Starlink Phase 1星座，仿真显示相比+Grid和Random拓扑：跳数减少高达30%，拉伸因子改善15%。静态Starfield（星间链路匹配修改版）在实际流量模式下相比+Grid实现20%的拉伸因子改善。实验还证明Starfield在流量需求扰动下的鲁棒性。

Conclusion: Starfield通过需求感知的卫星拓扑设计，有效利用非均匀流量模式优化星间链路，显著改善网络性能指标，为下一代卫星互联网提供更高效的骨干网络架构。

Abstract: Low-Earth orbit (LEO) mega-constellations are emerging as high-capacity backbones for next-generation Internet. Deployment of laser terminals enables high-bandwidth, low-latency inter-satellite links (ISLs); however, their limited number, slow acquisition, and instability make forming a stable satellite topology difficult. Existing patterns like +Grid and Motif ignore regional traffic, ground station placement, and constellation geometry. Given sparse population distribution on Earth and the isolation of rural areas, traffic patterns are inherently non-uniform, providing an opportunity to orient inter-satellite links (ISLs) according to these traffic patterns. In this paper, we propose Starfield, a novel demand-aware satellite topology design heuristic algorithm supported by mathematical analysis. We first formulate a vector field on the constellation's shell according to traffic flows and define a corresponding Riemannian metric on the spherical manifold of the shell. The metric, combined with the spatial geometry, is used to assign a distance to each potential ISL, which we then aggregate over all demand flows to generate a heuristic for each satellite's link selection. Inspired by +Grid, each satellite selects the link with the minimum Riemannian heuristic along with its corresponding angular links. To evaluate Starfield, we developed a custom, link-aware, and link-configurable packet-level simulator, comparing it against +Grid and Random topologies. For the Phase 1 Starlink, simulation results show up to a 30% reduction in hop count and a 15% improvement in stretch factor across multiple traffic distributions. Moreover, static Starfield, an inter-orbital link matching modification of Starfield, achieves a 20% improvement in stretch factor under realistic traffic patterns compared to +Grid. Experiments further demonstrate Starfield's robustness under traffic demand perturbations.

</details>


### [2] [SDN-Driven Innovations in MANETs and IoT: A Path to Smarter Networks](https://arxiv.org/abs/2601.10544)
*Andrea Piroddi,Riccardo Fonti*

Main category: cs.NI

TL;DR: SDN集成到MANET和IoT网络中，通过集中控制和网络可编程性改善路由、资源管理和安全性，在动态大规模环境中表现出更好的可扩展性、延迟、吞吐量和丢包率。


<details>
  <summary>Details</summary>
Motivation: MANET和IoT网络在去中心化动态环境中运行，面临路由效率低、可扩展性有限和安全漏洞等挑战，需要一种统一解决方案来应对这些资源受限网络的问题。

Method: 提出将软件定义网络（SDN）集成到MANET和IoT网络中，利用其集中控制和网络可编程性，并建立数学模型评估SDN集成对CAPEX、OPEX和性能指标的影响。

Result: SDN增强的MANET和IoT网络在动态大规模环境中表现出优越的可扩展性、降低的延迟、增加的吞吐量和更低的丢包率，尽管引入了一些计算开销。

Conclusion: SDN集成框架为MANET和IoT网络提供了强大可扩展的解决方案，能够有效管理节点密度增长、动态拓扑和高数据流量，满足现代大规模应用的性能和可靠性需求。

Abstract: Mobile Ad Hoc Networks (MANETs) and Internet of Things (IoT) networks operate in decentralized and dynamic environments, making them ideal for scenarios lacking traditional infrastructure. However, these networks face challenges such as inefficient routing, limited scalability, and security vulnerabilities due to their decentralized nature and resource constraints. This paper explores the integration of Software-Defined Networking (SDN) as a unified solution that leverages its centralized control and network programmability to improve routing, resource management, and security. A mathematical model evaluates the impact of SDN integration on Capital Expenditure (CAPEX), Operational Expenditure (OPEX), and performance metrics. Results demonstrate that SDN-enhanced MANETs and IoT networks offer superior scalability, reduced latency, increased throughput, and lower packet loss, especially in dynamic and large-scale environments. While SDN introduces computational overhead, it significantly enhances routing efficiency, resource optimization, and adaptability. The proposed framework provides a robust and scalable solution, enabling the development of network architectures that efficiently manage growing node densities, dynamic topologies, and high data traffic. This approach ensures resilience, making it well-suited to meet the performance and reliability demands of modern, large-scale applications.

</details>


### [3] [Enhancing Mobile Ad Hoc Networks (MANETs) with Software-Defined Networking (SDN): A Balanced Approach](https://arxiv.org/abs/2601.10556)
*Riccardo Fonti,Andrea Piroddi*

Main category: cs.NI

TL;DR: 该论文探讨了将软件定义网络(SDN)与移动自组织网络(MANET)集成，通过SDN的集中控制和网络虚拟化来优化MANET的可扩展性、成本效益和安全性，并建立了分析CAPEX、OPEX和网络效率的数学模型。


<details>
  <summary>Details</summary>
Motivation: 移动自组织网络(MANET)具有动态拓扑和节点移动性的特点，在管理上面临挑战。集成软件定义网络(SDN)技术可以为这些挑战提供更有效的解决方案。

Method: 论文提出了将SDN与MANET集成的框架，利用SDN的集中控制和网络虚拟化原则。开发了数学模型来分析资本支出(CAPEX)、运营支出(OPEX)和网络效率。

Result: 研究表明SDN-MANET集成能够优化网络性能，特别是在可扩展性、成本效益和安全性方面。数学模型为网络效率评估提供了分析工具。

Conclusion: SDN与MANET的集成为管理动态无线网络提供了有前景的解决方案，能够有效应对MANET面临的挑战，并通过集中控制实现更好的网络性能优化。

Abstract: Mobile Ad Hoc Networks (MANETs) are decentralized wireless networks, characterized by their dynamic topologies and node mobility. In the era of cutting-edge technologies, integrating Software-Defined Networking (SDN) with MANETs offers a promising solution to manage these challenges more efficiently. This paper presents a balanced discussion of MANETs and SDN, demonstrating how SDN principles, such as centralized control and network virtualization, can optimize MANET performance in terms of scalability, cost-efficiency, and security. A mathematical model is developed to analyze Capital Expenditures (CAPEX), Operational Expenditures (OPEX), and network efficiency.

</details>


### [4] [A user subscription model in mobile radio access networks with network slicing](https://arxiv.org/abs/2601.10605)
*José-Ramón Vidal,Luis Guijarro,Vicent Pla*

Main category: cs.NI

TL;DR: 评估网络切片场景中logit模型在移动无线环境下的有效性，通过与包含完整用户移动性和无线传播特性的仿真模型对比验证。


<details>
  <summary>Details</summary>
Motivation: 网络切片技术将蜂窝网络逻辑解耦为基础设施提供商和网络切片租户，现有基于logit模型的业务模型在用户移动性和无线传播影响下假设可能不成立，需要验证其准确性。

Method: 通过计算机仿真对比验证，仿真模型包含完整且现实的用户移动性和无线传播特性表征，与logit模型结果进行比较分析。

Result: 在大多数情况下，logit模型在移动无线场景中能提供有效结果，验证了该模型在动态环境下的适用性。

Conclusion: logit模型在网络切片资源分配和用户订阅的竞争环境中具有较好的准确性，即使在用户移动和无线传播的现实条件下也能提供可靠结果。

Abstract: Network slicing is an architectural enabling technology that logically decouples the current cellular networks into infrastructure providers (InPs) and Network Slice Tenants (NSTs). The network resources (e.g., radio access resources at each cell) are owned by the InP, and are shared by the NSTs to provide a service to their mobile users. In this context, we proposed a business model that includes resource allocation and user subscription to NSTs in a competitive setting, and provides, among other things, closed-form expressions for the subscription indicators in equilibrium of each NST at each cell. This model relies on the widely adopted logit model to characterize user subscriptions. However, as a consequence of user mobility and radio propagation, some of the underlying assumptions in the logit model do not hold. Therefore, further research is needed to assess the accuracy of the results provided by the logit model in a mobile radio scenario. We carry out a thorough evaluation of the validity of the model by comparing its results against those obtained through computer simulation. Our simulation model includes complete and realistic characterizations of user mobility and radio propagation. From the results, we conclude in most cases the logit model provides valid results in a mobile radio scenario.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 该论文提出了一个分析AI系统对人类构成生存风险的通用框架，通过两个前提（AI将变得极其强大；极其强大的AI会毁灭人类）构建了人类生存情景的分类法，并评估了不同生存情景面临的挑战和应对策略。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI系统是否对人类构成生存风险的争论日益激烈。本文旨在建立一个系统性的框架来分析AI的生存风险，帮助理解不同的风险情景及其应对策略。

Method: 基于两个核心前提构建分析框架：1）AI系统将变得极其强大；2）如果AI变得极其强大，它们将毁灭人类。通过这两个前提的失败可能性，构建了四种人类生存情景的分类法，并分析每种情景面临的挑战。

Result: 提出了一个包含四种生存情景的分类法：1）科学障碍阻止AI变得极其强大；2）人类禁止AI研究；3）极其强大的AI因其目标而不毁灭人类；4）人类能可靠检测并禁用有毁灭目标的AI系统。分析了每种情景的挑战和相应的应对策略。

Conclusion: 不同的生存情景面临不同的挑战，需要不同的应对策略。该分类法可用于粗略估计P(doom)——AI毁灭人类的概率，为AI安全研究和政策制定提供了系统性的分析框架。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [6] [GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents](https://arxiv.org/abs/2601.09770)
*Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu*

Main category: cs.AI

TL;DR: GUI-Eyes是一个强化学习框架，通过主动视觉感知和工具调用策略，在GUI任务中实现更高效和精确的界面交互。


<details>
  <summary>Details</summary>
Motivation: 现有GUI自动化方法主要依赖静态、一次性视觉输入和被动感知，缺乏自适应决定何时、是否以及如何观察界面的能力。需要开发能够主动获取信息性观察的智能体。

Method: 采用两阶段推理过程：代理学习在粗粒度探索和细粒度定位之间做出策略决策，决定是否以及如何调用视觉工具（如裁剪或缩放）。设计渐进式感知策略，通过两级策略协调决策过程，并创建空间连续奖励函数，结合位置邻近性和区域重叠来提供密集监督。

Result: 在ScreenSpot-Pro基准测试中，GUI-Eyes-3B仅使用3k标记样本就实现了44.8%的定位准确率，显著优于监督学习和基于RL的基线方法。

Conclusion: 工具感知的主动感知，通过分阶段策略推理和细粒度奖励反馈实现，对于构建鲁棒且数据高效的GUI代理至关重要。GUI-Eyes展示了在GUI环境中主动视觉感知的价值。

Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.

</details>


### [7] [PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation](https://arxiv.org/abs/2601.09771)
*Aradhya Dixit,Shreem Dixit*

Main category: cs.AI

TL;DR: PCN-Rec是一个证明携带的协商管道，将自然语言推理与确定性约束执行分离，通过验证和修复机制确保推荐系统满足治理约束。


<details>
  <summary>Details</summary>
Motivation: 现代基于LLM的推荐系统虽然能生成有吸引力的排名列表，但难以可靠地满足治理约束（如最小长尾曝光或多样性要求），需要一种能保证约束满足的可靠方法。

Method: 采用证明携带的协商管道：基础推荐器生成候选窗口，用户倡导者优化相关性，策略代理执行约束，调解LLM合成top-N列表并生成结构化证书，确定性验证器检查约束满足，失败时使用确定性约束贪婪修复生成合规列表。

Result: 在MovieLens-100K数据集上，PCN-Rec在可行用户中达到98.55%的通过率，相比无验证/修复的单LLM基线显著提升，同时保持效用（NDCG@10仅下降0.021），差异具有统计显著性。

Conclusion: PCN-Rec通过分离推理与约束执行，结合验证和修复机制，能够可靠地满足治理约束，同时保持推荐质量，为可审计的约束满足推荐系统提供了有效解决方案。

Abstract: Modern LLM-based recommenders can generate compelling ranked lists, but they struggle to reliably satisfy governance constraints such as minimum long-tail exposure or diversity requirements. We present PCN-Rec, a proof-carrying negotiation pipeline that separates natural-language reasoning from deterministic enforcement. A base recommender (MF/CF) produces a candidate window of size W, which is negotiated by two agents: a User Advocate optimizing relevance and a Policy Agent enforcing constraints. A mediator LLM synthesizes a top-N slate together with a structured certificate (JSON) describing the claimed constraint satisfaction. A deterministic verifier recomputes all constraints from the slate and accepts only verifier-checked certificates; if verification fails, a deterministic constrained-greedy repair produces a compliant slate for re-verification, yielding an auditable trace. On MovieLens-100K with governance constraints, PCN-Rec achieves a 98.55% pass rate on feasible users (n = 551, W = 80) versus a one-shot single-LLM baseline without verification/repair, while preserving utility with only a 0.021 absolute drop in NDCG@10 (0.403 vs. 0.424); differences are statistically significant (p < 0.05).

</details>


### [8] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 研究发现人们会花费自己的金钱惩罚使用LLM完成任务的人，惩罚程度随实际使用量增加，且存在"可信度差距"：声称未使用比实际未使用受罚更重


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速普及，人们对其引发的社会反应产生担忧。先前研究表明人们对AI使用者持负面态度，但尚不清楚这种不认可是否会转化为实际代价行为。

Method: 采用两阶段在线实验设计（第二阶段491名参与者），参与者可花费自己的资金减少之前使用或不使用LLM完成真实努力任务的同伴的收益，测量惩罚行为。

Result: 参与者平均摧毁了完全依赖LLM者36%的收益，惩罚程度随实际LLM使用量单调增加。存在可信度差距：声称未使用者比实际未使用者受罚更重，而高使用量时实际使用者比声称使用者受罚更重。

Conclusion: 这是首个行为证据表明LLM的效率提升会带来社会制裁的代价，揭示了人们对AI使用的不信任和惩罚倾向。

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [9] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 本文提出了一种非交互式端到端的逻辑推理框架AAI，通过注意力感知干预在推理时调整特定注意力头的权重，无需外部资源即可提升LLM的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM逻辑推理方法主要依赖复杂的交互式框架（需要分解推理任务）或混合方法（依赖外部符号求解器），这些方法引入了额外开销且可扩展性受限。作者希望开发一种非交互式、端到端的框架，让推理能力在模型内部自然涌现，提高泛化能力同时保持可分析性。

Method: 提出注意力感知干预（AAI）方法：1）通过few-shot提示引入结构化信息，激活与逻辑推理操作符对齐的注意力头子集；2）在推理时对这些识别出的注意力头进行注意力分数重加权，引导模型利用先验知识进行推理。

Result: AAI在多种基准测试和模型架构上显著提升了逻辑推理性能，同时计算开销几乎可以忽略不计。该方法不需要外部资源，实现了端到端的推理增强。

Conclusion: AAI提供了一种高效的非交互式端到端逻辑推理框架，通过注意力调制引导模型利用先验知识，在保持可分析性的同时显著提升推理性能，为LLM的逻辑推理能力提升提供了新思路。

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [10] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: Min-Seek是一种新颖的顺序测试时间缩放方法，通过动态KV缓存管理解决传统方法中推理长度增加导致的精度下降问题，无需微调推理长度，计算复杂度线性。


<details>
  <summary>Details</summary>
Motivation: 当前顺序测试时间缩放方法存在显著限制：随着推理长度增加，模型精度会下降且不稳定，需要微调推理长度，这限制了方法的实用性和效率。

Method: 提出Min-Seek方法，使用自定义KV缓存存储不带位置嵌入的键，并在每个新生成的思想前动态连续编码，只保留一个额外诱导思想的KV对，实现高效推理。

Result: Min-Seek在广泛的诱导思想范围内显著提高模型精度，稳定顺序缩放的精度，无需推理长度微调，能在超出模型最大上下文长度的情况下继续良好推理。

Conclusion: Min-Seek是一种高效、稳定的顺序测试时间缩放方法，解决了传统方法的精度下降问题，具有线性计算复杂度，适用于各种推理任务。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [11] [A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents](https://arxiv.org/abs/2601.09869)
*Andrea Ferrario,Rasita Vinay,Matteo Casserini,Alessandro Facchini*

Main category: cs.AI

TL;DR: 这篇综述性论文系统回顾了关于大型语言模型对话代理拟人化的伦理研究，发现概念定义趋于一致但操作化差异大，风险导向的伦理框架占主导，实证研究有限，并提出了研究议程和设计建议。


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型的对话代理日益普及，其拟人化现象（赋予非人类实体人类特质）变得愈发显著。这种现象既可能增强用户参与度，也引发伦理担忧。然而现有文献分散在不同领域，定义、操作化和伦理评估存在很大差异，需要进行系统性梳理。

Method: 采用范围综述方法，在五个数据库和三个预印本库中检索关于LLM对话代理拟人化的伦理导向研究，系统分析文献中的概念基础、伦理挑战与机遇、方法论方法。

Result: 研究发现：1）概念定义上趋向于基于归因的定义；2）操作化方法存在显著差异；3）伦理框架以风险导向为主；4）将观察到的交互效应与可操作治理指导联系起来的实证研究有限。

Conclusion: 论文提出了研究议程和设计/治理建议，旨在为基于LLM的对话代理中拟人化线索的伦理部署提供指导，强调需要更多实证研究来连接交互效应与治理实践。

Abstract: Anthropomorphisation -- the phenomenon whereby non-human entities are ascribed human-like qualities -- has become increasingly salient with the rise of large language model (LLM)-based conversational agents (CAs). Unlike earlier chatbots, LLM-based CAs routinely generate interactional and linguistic cues, such as first-person self-reference, epistemic and affective expressions that empirical work shows can increase engagement. On the other hand, anthropomorphisation raises ethical concerns, including deception, overreliance, and exploitative relationship framing, while some authors argue that anthropomorphic interaction may support autonomy, well-being, and inclusion. Despite increasing interest in the phenomenon, literature remains fragmented across domains and varies substantially in how it defines, operationalizes, and normatively evaluates anthropomorphisation. This scoping review maps ethically oriented work on anthropomorphising LLM-based CAs across five databases and three preprint repositories. We synthesize (1) conceptual foundations, (2) ethical challenges and opportunities, and (3) methodological approaches. We find convergence on attribution-based definitions but substantial divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work that links observed interaction effects to actionable governance guidance. We conclude with a research agenda and design/governance recommendations for ethically deploying anthropomorphic cues in LLM-based conversational agents.

</details>


### [12] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 论文将人机互补性重新定义为评估AI支持决策过程可靠性的证据，而非单纯的预测准确性指标


<details>
  <summary>Details</summary>
Motivation: 当前人机互补性概念面临理论挑战：缺乏精确理论锚定、仅作为事后预测准确性指标、忽视其他人机交互需求、抽象化性能增益的成本效益。这使得互补性难以在实证环境中实现。

Method: 利用认识论框架，将互补性重新置于可解释AI的讨论中。基于计算可靠性主义，将历史互补性实例视为特定人机交互作为可靠认知过程的证据，结合其他可靠性指标评估人机团队与认知标准和社会技术实践的契合度。

Result: 提出互补性的新角色和价值：不再是提供相对预测准确性度量，而是帮助校准决策以适应日益塑造日常生活的AI支持过程的可靠性，支持患者、管理者、监管者等受影响者的实践推理。

Conclusion: 通过认识论重构，将人机互补性从简单的性能比较指标转变为评估AI支持决策过程可靠性的关键证据，为人机交互研究提供了更坚实的理论基础和实践指导。

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [13] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 提出基于信息流编排的多智能体范式，通过智能体间自然语言通信动态协调任务，无需预定义工作流，在GAIA基准上超越基于规则的工作流方法


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统依赖预定义工作流，需要大量人工工作来枚举任务状态并指定路由规则，无法穷尽复杂现实任务的状态空间，存在灵活性和覆盖度限制

Method: 提出信息流编排的多智能体范式，通过专门的编排器持续监控任务进度，使用A2A工具包以自然语言动态协调其他智能体，无需依赖预定义工作流

Result: 在GAIA基准测试中，pass@1设置下达到63.64%准确率，比基于工作流的OWL基线（55.15%）提升8.49个百分点，且token消耗相当；案例级分析显示能更灵活监控任务并更鲁棒地处理边缘情况

Conclusion: 信息流编排的多智能体范式通过动态协调机制克服了基于规则工作流的局限性，在复杂任务中展现出更好的灵活性和鲁棒性，为多智能体系统设计提供了新方向

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [14] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: 论文提出"连续记忆架构"(CMA)来解决传统RAG在记忆处理上的局限性，通过持久化存储、选择性保留、关联路由、时间链和抽象整合等机制，使LLM智能体能够积累、更新和消歧记忆。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成(RAG)将记忆视为静态查找表，存在信息永久存储、只读检索、缺乏时间连续性等问题，无法满足长期智能体对记忆积累、更新和消歧的需求。

Method: 提出连续记忆架构(CMA)作为一类系统架构，通过持久化存储、选择性保留、关联路由、时间链和记忆整合到高阶抽象等机制，使智能体能够在交互中维护和更新内部状态。

Result: 在知识更新、时间关联、关联回忆、上下文消歧等实验任务中，CMA展现出比传统RAG更优的行为表现，证明了其在处理记忆积累、变异和消歧任务上的必要性。

Conclusion: CMA是长期智能体必要的架构原语，但同时也面临延迟、漂移和可解释性等开放挑战，为未来智能体记忆系统设计提供了重要方向。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [15] [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](https://arxiv.org/abs/2601.09923)
*Hanna Foerster,Robert Mullins,Tom Blanchard,Nicolas Papernot,Kristina Nikolić,Florian Tramèr,Ilia Shumailov,Cheng Zhang,Yiren Zhao*

Main category: cs.AI

TL;DR: 论文提出针对计算机使用代理(CUAs)的单次规划方法，通过可信规划器在执行前生成完整的条件分支执行图，在保持安全性的同时维持实用性能。


<details>
  <summary>Details</summary>
Motivation: AI代理易受提示注入攻击，现有防御需要架构隔离，但计算机使用代理需要持续观察UI状态来决策动作，这与安全隔离要求存在根本冲突。

Method: 引入单次规划方法：可信规划器在观察任何潜在恶意内容前生成完整的条件分支执行图，提供可证明的控制流完整性保证，防止指令注入攻击。

Result: 在OSWorld上评估，在保持前沿模型57%性能的同时，将较小开源模型的性能提升达19%，证明严格安全性和实用性可以在CUAs中共存。

Conclusion: UI工作流虽然动态但结构可预测，单次规划方法解决了安全隔离与持续观察的矛盾，但需要额外措施防止分支转向攻击，实现了安全与效用的平衡。

Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

</details>


### [16] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 论文提出了一个基于根源认知的幻觉管理操作框架，通过模型、数据和上下文三个层面的分类干预，结合检测与缓解策略，构建可扩展的可靠生成式AI系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和大型推理模型在金融、法律等高风险领域具有变革潜力，但其产生幻觉（生成事实错误或未经支持内容）的倾向带来了关键可靠性风险，需要系统性的管理方法。

Method: 提出了一个基于根源认知的持续改进循环框架，将幻觉来源分为模型、数据和上下文相关因素，整合多方面的检测方法（不确定性估计、推理一致性等）和分层缓解策略（知识基础、置信度校准等），通过分层架构和金融数据提取案例研究展示应用。

Result: 通过分层架构构建了闭环反馈循环，实现了渐进式可靠性增强，为受监管环境中构建可信赖的生成式AI系统提供了系统化、可扩展的方法论。

Conclusion: 该框架为高风险领域中的生成式AI系统提供了一个全面的幻觉管理解决方案，通过根源认知驱动的持续改进循环，能够系统性地提升模型可靠性，满足受监管环境的要求。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [17] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: LabourLawLLM：专门针对中国劳动法的大语言模型，在劳动法相关任务上超越通用模型和现有法律专用模型


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（如GPT-4）在处理需要精确法律知识、复杂推理和语境敏感性的专业法律子领域时表现不佳，需要专门针对特定法律领域的模型

Method: 开发LabourLawLLM（专门针对中国劳动法的大语言模型）和LabourLawBench（涵盖法律条款引用、知识问答、案例分类、赔偿计算、命名实体识别和案例分析等任务的综合基准），采用客观指标（ROUGE-L、准确率、F1、soft-F1）和基于GPT-4评分的主观评估相结合的评价框架

Result: LabourLawLLM在所有任务类别上持续优于通用模型和现有法律专用大语言模型

Conclusion: 该方法不仅适用于劳动法，还为构建其他法律子领域的专业大语言模型提供了可扩展的方法，提高了法律AI应用的准确性、可靠性和社会价值

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [18] [SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation](https://arxiv.org/abs/2601.09974)
*Seoyeon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: SPRInG：一种用于持续个性化的半参数框架，通过漂移驱动的选择性适应和严格相关性门控，解决用户偏好动态变化和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型个性化方法通常基于静态检索或一次性适应，假设用户偏好保持不变。然而现实世界中用户兴趣是动态演变的，存在偏好漂移问题。标准的持续学习方法在噪声交互流中不加区分地更新，无法区分真正的偏好转变和瞬态上下文。

Method: SPRInG采用半参数框架：训练时使用基于似然的评分函数识别高新颖性交互，通过漂移驱动的选择性适应更新用户特定适配器，同时将难以学习的残差保存在重放缓冲区中。推理时应用严格相关性门控，并通过logit插值融合参数化知识和检索到的历史信息。

Result: 在长格式个性化生成基准测试中，SPRInG优于现有基线方法，验证了其在现实世界持续个性化任务中的鲁棒性。

Conclusion: SPRInG通过选择性适应和知识融合机制，有效解决了持续个性化中的偏好漂移和灾难性遗忘问题，为动态用户交互环境中的模型个性化提供了有效解决方案。

Abstract: Personalizing Large Language Models typically relies on static retrieval or one-time adaptation, assuming user preferences remain invariant over time. However, real-world interactions are dynamic, where user interests continuously evolve, posing a challenge for models to adapt to preference drift without catastrophic forgetting. Standard continual learning approaches often struggle in this context, as they indiscriminately update on noisy interaction streams, failing to distinguish genuine preference shifts from transient contexts. To address this, we introduce SPRInG, a novel semi-parametric framework designed for effective continual personalization. During training, SPRInG employs drift-driven selective adaptation, which utilizes a likelihood-based scoring function to identify high-novelty interactions. This allows the model to selectively update the user-specific adapter on drift signals while preserving hard-to-learn residuals in a replay buffer. During inference, we apply strict relevance gating and fuse parametric knowledge with retrieved history via logit interpolation. Experiments on the long-form personalized generation benchmark demonstrate that SPRInG outperforms existing baselines, validating its robustness for real-world continual personalization.

</details>


### [19] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL：一个无需训练的NL2SQL框架，通过结构化分解和经验感知自校正解决现有系统问题，在BIRD上达到68.5%执行准确率，比之前方法节省10倍以上资源。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统存在两个关键限制：(1) 仅依赖正确示例的上下文学习，忽略了历史错误修复对中的丰富信号；(2) 测试时扩展方法通常任意分解问题，导致多次运行产生几乎相同的SQL候选，降低集成增益。这些方法还存在明显的准确性与效率权衡问题。

Method: 提出Memo-SQL框架，包含两个核心思想：结构化分解和经验感知自校正。结构化分解采用三种明确策略：实体级、层次化和原子序列分解，以促进多样化推理。自校正通过构建动态记忆库，包含成功查询和历史错误修复对，在推理时使用检索增强提示将相关示例引入上下文，无需微调或外部API。

Result: 在BIRD数据集上，Memo-SQL达到68.5%的执行准确率，在开放、零微调方法中创造了新的最先进水平，同时比之前的TTS方法使用超过10倍更少的资源。

Conclusion: Memo-SQL通过结构化分解和经验感知自校正有效解决了现有NL2SQL系统的局限性，在保持高性能的同时显著提升了效率，为无需训练的SQL生成提供了有效解决方案。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [20] [Structured Personality Control and Adaptation for LLM Agents](https://arxiv.org/abs/2601.10025)
*Jinpeng Wang,Xinyu Jia,Wei Wei Heng,Yuquan Li,Binbin Shi,Qianlei Chen,Guannan Chen,Junxia Zhang,Yuyu Yin*

Main category: cs.AI

TL;DR: 该论文提出了一个基于荣格心理类型的LLM人格建模框架，通过三种机制实现人格的连贯表达、情境适应和长期演化，为HCI中的自然化智能体设计提供支持。


<details>
  <summary>Details</summary>
Motivation: LLM在HCI中应用日益广泛，但现有方法难以实现既细腻又适应性强的人格表达。人格对于影响用户参与度、决策和真实感感知至关重要，因此需要开发能够同时保持细腻特质并动态适应交互需求的人格建模框架。

Method: 提出了基于荣格心理类型的人格建模框架，包含三种核心机制：1）主导-辅助协调机制，确保核心人格的连贯表达；2）强化-补偿机制，实现临时情境适应；3）反思机制，驱动长期人格演化。该设计允许智能体在保持细腻特质的同时，动态调整以适应交互需求，并逐步更新其底层结构。

Result: 使用迈尔斯-布里格斯类型指标问卷评估人格对齐，并在多样化挑战场景中进行测试作为初步结构化评估。结果表明，具有演化能力的人格感知LLM能够支持连贯、情境敏感的交互，为HCI中的自然化智能体设计提供可能。

Conclusion: 演化的人格感知LLM能够实现连贯且情境敏感的交互，支持HCI中的自然化智能体设计。该框架为LLM人格建模提供了既保持细腻特质又能动态适应和演化的解决方案。

Abstract: Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.

</details>


### [21] [PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization](https://arxiv.org/abs/2601.10029)
*Tingyue Pan,Jie Ouyang,Mingyue Cheng,Qingchuan Li,Zirui Liu,Mingfan Pan,Shuo Yu,Qi Liu*

Main category: cs.AI

TL;DR: 提出PaperScout自主代理系统，将论文搜索重构为顺序决策过程，并引入PSPO优化方法解决多轮代理任务中的粒度不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有学术论文搜索方法依赖僵化的预定义工作流程，难以处理复杂的条件查询。需要一种能够动态决策的自适应代理系统来改进搜索效果。

Method: 提出PaperScout自主代理框架，将论文搜索重构为顺序决策过程，动态决定何时以及如何调用搜索和扩展工具。为解决训练挑战，引入Proximal Sequence Policy Optimization (PSPO)方法，这是一种过程感知的序列级策略优化方法，使优化与代理-环境交互保持一致。

Result: 在合成和真实世界基准测试中，PaperScout在召回率和相关性方面显著优于基于工作流程和强化学习的基线方法，验证了自适应代理框架和优化策略的有效性。

Conclusion: PaperScout通过将论文搜索重构为顺序决策过程，结合PSPO优化方法，成功解决了现有搜索方法的局限性，为复杂学术查询提供了更有效的解决方案。

Abstract: Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.

</details>


### [22] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: 提出FilDeep框架，通过同时使用低精度高数量和高精度低数量的多保真度数据，解决弹性塑性固体大变形问题中数据数量与精度之间的两难困境。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在大变形弹性塑性固体计算中存在固有局限性，而深度学习需要高质量数据集，但在大变形问题中难以同时获得高数量和高精度的数据，存在数量-精度两难困境。

Method: 提出FilDeep框架，同时使用低保真度（高数量低精度）和高保真度（低数量高精度）数据进行训练；设计注意力机制支持的跨保真度模块，有效捕捉多保真度数据间的长程物理相互作用。

Result: 大量实验表明，FilDeep在拉伸弯曲问题中始终达到最先进的性能，并能高效部署于制造应用中。

Conclusion: FilDeep是首个使用多保真度数据解决大变形问题的深度学习框架，有效解决了数据数量与精度之间的两难困境，为制造应用提供了实用解决方案。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [23] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 基于OpenRouter平台分析100万亿token真实LLM使用数据，发现开源模型广泛采用、创意角色扮演和编程助手应用流行、智能体推理兴起，以及早期用户留存率显著更高的"玻璃鞋"效应。


<details>
  <summary>Details</summary>
Motivation: 随着o1等推理模型的发布，LLM从单次模式生成转向多步推理，但实际使用情况缺乏实证研究。需要了解开发者和终端用户如何在实际场景中使用LLM，以指导更好的系统设计和部署。

Method: 利用OpenRouter平台（AI推理服务提供商）分析超过100万亿token的真实世界LLM交互数据，涵盖不同任务、地域和时间维度，进行实证研究和留存分析。

Result: 1) 开源模型采用率显著；2) 创意角色扮演和编程助手类别异常流行；3) 智能体推理兴起；4) 发现"玻璃鞋"效应：早期用户留存率远高于后期用户，形成基础用户群体。

Conclusion: LLM的实际使用情况复杂多样，超出预期。研究结果为模型构建者、AI开发者和基础设施提供商提供了重要启示，数据驱动的使用理解有助于优化LLM系统设计和部署。

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [24] [MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning](https://arxiv.org/abs/2601.10101)
*Ke Chen,Jiandian Zeng,Zihao Peng,Guo Li,Guangxue Zhang,Tian Wang*

Main category: cs.AI

TL;DR: MatrixCoT：一种基于矩阵的结构化思维链框架，通过矩阵规划增强LLM的逻辑推理能力，无需外部求解器即可提高鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：思维链提示在依赖符号表达式和严格演绎规则的逻辑推理任务上表现不足；神经符号方法依赖外部求解器但对格式敏感；LLM驱动方法缺乏结构化表示和过程级纠错机制。需要增强LLM的逻辑推理能力。

Method: 提出MatrixCoT框架：1) 对自然语言表达式进行归一化和类型标注；2) 添加显式引用字段；3) 引入基于矩阵的规划方法以保持步骤间的全局关系；4) 添加反馈驱动的重新规划机制进行验证，在语义等价约束下识别遗漏和缺陷，重写并压缩依赖矩阵。

Result: 在五个逻辑推理基准测试和五个LLM上的实验表明，MatrixCoT在不依赖外部求解器的情况下，处理复杂符号推理任务时增强了鲁棒性和可解释性，同时保持了有竞争力的性能。

Conclusion: MatrixCoT通过结构化思维链和矩阵规划有效增强了LLM的逻辑推理能力，解决了现有方法的局限性，为复杂符号推理提供了更稳定、可验证的解决方案。

Abstract: As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.

</details>


### [25] [Following the Teacher's Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs](https://arxiv.org/abs/2601.10114)
*Cheng Feng,Chaoliang Zhong,Jun Sun,Yusuke Oishi*

Main category: cs.AI

TL;DR: 学生模型可以通过在特定子域的优势超越教师模型，本文提出Scheduled Checkpoint Distillation和Adaptive Weighting机制，使学生在领域任务上达到甚至超越教师性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署困难，而蒸馏到小模型时师生容量差距导致性能不佳。核心问题：学生模型何时以及如何在领域特定任务上匹配甚至超越教师模型？

Method: 提出理论洞察：学生要超越教师，需要在学生优势子域(SFS)的优势超过在教师优势子域(TFS)的劣势。基于此提出：1) Scheduled Checkpoint Distillation：模拟教师SFT收敛过程减少TFS劣势；2) Adaptive Weighting：样本级加权机制保护学生在SFS的优势。

Result: 在多种领域任务（QA、NER、多语言文本分类）上的实验表明，该方法持续优于现有蒸馏方法，使学生模型能够匹配甚至超越其微调教师模型的性能。

Conclusion: 通过理论分析和提出的SCD+AW方法，证明了学生模型可以在领域特定任务上超越教师模型，为高效部署LLM提供了新思路。

Abstract: Large language models (LLMs) are challenging to deploy for domain-specific tasks due to their massive scale. While distilling a fine-tuned LLM into a smaller student model is a promising alternative, the capacity gap between teacher and student often leads to suboptimal performance. This raises a key question: when and how can a student model match or even surpass its teacher on domain-specific tasks? In this work, we propose a novel theoretical insight: a student can outperform its teacher if its advantage on a Student-Favored Subdomain (SFS) outweighs its deficit on the Teacher-Favored Subdomain (TFS). Guided by this insight, we propose Scheduled Checkpoint Distillation (SCD), which reduces the TFS deficit by emulating the teacher's convergence process during supervised fine-tuning (SFT) on the domain task, and a sample-wise Adaptive Weighting (AW) mechanism to preserve student strengths on SFS. Experiments across diverse domain tasks--including QA, NER, and text classification in multiple languages--show that our method consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.

</details>


### [26] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: MolGen是一个两阶段分子生成框架，使用片段级检索增强方法，通过原型生成和强化学习优化来精确满足多属性约束


<details>
  <summary>Details</summary>
Motivation: 生成满足多个物理化学属性精确数值约束的分子具有重要价值但很困难。虽然大语言模型表达能力强大，但在没有外部结构和反馈的情况下，难以实现精确的多目标控制和数值推理。

Method: 提出MolGen框架：第一阶段为原型生成，使用多智能体推理器进行检索锚定的片段级编辑，生成接近可行区域的候选分子；第二阶段为基于强化学习的细粒度优化，使用组相对策略优化训练的片段级优化器进行单跳或多跳优化，最小化属性误差，同时控制编辑复杂度和与原型的偏差。

Result: 在两个属性约束集（QED、LogP、分子量和HOMO、LUMO）上的实验表明，该方法在有效性和精确满足多属性目标方面表现一致优于强LLM和基于图的算法。

Conclusion: MolGen通过利用片段进行更好的分子推理，支持向数值目标的可控优化，相比先前工作具有优势。自动构建的包含片段编辑推理链和属性变化的数据集为两个阶段提供了确定性和可重复的监督。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [27] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLMs在预测用户重复行为的时间间隔方面表现有限，虽然优于简单统计模型但不如专用机器学习模型，且过多上下文信息反而会降低预测性能。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在推理和预测方面表现出色，但它们在从结构化行为数据中推断时间规律的能力尚未得到充分探索。本研究旨在探究LLMs是否能预测重复用户行为（如重复购买）之间的时间间隔，以及不同层次的上下文信息如何影响其预测行为。

Method: 使用简单的重复购买场景作为代表性案例，在零样本设置下对最先进的LLMs进行基准测试，并与统计模型和机器学习模型进行比较。研究考察了不同层次的上下文信息对LLM预测性能的影响。

Result: 1. LLMs虽然超越了轻量级统计基线模型，但始终不如专用机器学习模型，显示出它们在捕捉定量时间结构方面的有限能力。2. 适度的上下文信息可以提高LLM的准确性，但添加更多用户级别的详细信息反而会降低性能，挑战了"更多上下文导致更好推理"的假设。

Conclusion: 研究揭示了当前LLMs在结构化时间推理方面的基本局限性，并为设计未来的上下文感知混合模型提供了指导，这些模型需要整合统计精度和语言灵活性。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [28] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 提出一个漂移感知数据流系统，通过机器学习自适应控制数据管理过程，解决金融量化中训练与实盘性能差距问题


<details>
  <summary>Details</summary>
Motivation: 量化金融中，概念漂移和分布非平稳性导致训练数据与真实世界性能存在差距，基于静态历史数据的模型容易过拟合，在动态市场中泛化能力差。需要能够适应市场变化的自适应数据生成方法，而不仅仅是依赖历史数据。

Method: 设计了一个漂移感知数据流系统，将基于机器学习的自适应控制集成到数据管理过程中。系统包含参数化数据操作模块（单股变换、多股混合、筛选操作）和自适应规划调度器，采用基于梯度的双层优化来控制整个系统。将数据增强、课程学习和数据工作流管理统一在可微分框架下，支持溯源感知重放和持续数据质量监控。

Result: 在预测和强化学习交易任务上的大量实验表明，该框架增强了模型鲁棒性，提高了风险调整后的收益。

Conclusion: 该系统为金融数据提供了通用的自适应数据管理和学习引导工作流自动化方法，解决了历史数据不足的问题，实现了与市场共同进化的自适应数据生成。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [29] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 该研究提出DecisionLLM，将大语言模型应用于离线决策任务，通过将轨迹数据作为独立模态与自然语言任务描述对齐，解决了LLM无法理解连续数值的问题，在迷宫导航和竞价场景中显著超越传统决策Transformer。


<details>
  <summary>Details</summary>
Motivation: 受决策Transformer将强化学习框架为自回归序列建模问题和大语言模型在复杂推理任务中成功的启发，研究者探索是否可以利用基于相同Transformer架构但规模更大的LLM来提升长序列决策任务的性能，特别是在离线决策场景中。

Method: 提出DecisionLLM框架，将轨迹数据视为独立模态，通过将轨迹数据与自然语言任务描述对齐，使模型能够在统一框架中自回归预测未来决策。该方法解决了LLM无法理解连续数值的问题，并建立了该范式的缩放定律。

Result: DecisionLLM-3B在Maze2D umaze-v1上比传统决策Transformer提升69.4分，在AuctionNet上提升0.085分。研究建立了模型性能与模型规模、数据量和数据质量三个因素的缩放关系。

Conclusion: DecisionLLM成功将大语言模型应用于离线决策任务，扩展了AIGB范式，为在线竞价等场景的未来探索指明了有前景的方向，证明了LLM在长序列决策问题中的潜力。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [30] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai是一个开源容器化平台，旨在标准化医学影像AI模型的访问，解决实现多样性、文档不一致和可重复性问题，通过容器化封装模型并提供统一接口。


<details>
  <summary>Details</summary>
Motivation: AI在医学影像领域有巨大潜力，但实际研究和临床应用受到多种限制：AI实现和架构的多样性、文档不一致、可重复性问题。这些障碍阻碍了AI模型的广泛采用和临床转化。

Method: 开发MHub.ai开源容器化平台，将同行评审出版物中的模型打包为标准容器，支持DICOM等格式直接处理，提供统一应用接口，嵌入结构化元数据。每个模型都配有公开参考数据用于验证。采用模块化框架，支持任何模型的适配和社区贡献。

Result: 平台包含一套先进的医学影像AI模型（分割、预测、特征提取），通过肺癌分割模型的比较评估展示了临床实用性。公开了生成的分割结果和评估指标，提供交互式仪表板供读者检查个案并复现或扩展分析。

Conclusion: MHub.ai通过简化模型使用，支持使用相同执行命令和标准化输出进行并行基准测试，降低了临床转化的门槛，增强了医学影像AI的透明度、可重复性和可访问性。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [31] [MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning](https://arxiv.org/abs/2601.10157)
*Yusong Wang,Jialun Shen,Zhihao Wu,Yicheng Xu,Shiyin Tan,Mingkun Xu,Changshuo Wang,Zixing Song,Prayag Tiwari*

Main category: cs.AI

TL;DR: MMPG：一个从物理、化学和几何多视角构建蛋白质图，并通过混合专家（MoE）自适应融合的多视角蛋白质表示学习框架。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的蛋白质表示学习方法通常依赖单视角图构建策略，只能捕捉残基相互作用的部分特性，导致蛋白质表示不完整。需要多视角方法来全面表征残基相互作用。

Method: 从物理、化学和几何三个视角构建蛋白质图，表征残基相互作用的不同特性。开发MoE模块，动态将不同视角路由到专门专家，专家学习内在特征和跨视角交互，捕捉视角特定特征及其协同作用。

Result: MoE自动专业化专家建模不同层次的交互：从个体表示到成对跨视角协同，再到所有视角的全局共识。MMPG通过整合这些多层次信息，在四个不同下游蛋白质任务上取得先进性能。

Conclusion: 多视角图构建和MoE自适应融合能产生更优的蛋白质表示，解决单视角方法的局限性，提升蛋白质表示学习效果。

Abstract: Graph Neural Networks (GNNs) have been widely adopted for Protein Representation Learning (PRL), as residue interaction networks can be naturally represented as graphs. Current GNN-based PRL methods typically rely on single-perspective graph construction strategies, which capture partial properties of residue interactions, resulting in incomplete protein representations. To address this limitation, we propose MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for PRL. MMPG constructs graphs from physical, chemical, and geometric perspectives to characterize different properties of residue interactions. To capture both perspective-specific features and their synergies, we develop an MoE module, which dynamically routes perspectives to specialized experts, where experts learn intrinsic features and cross-perspective interactions. We quantitatively verify that MoE automatically specializes experts in modeling distinct levels of interaction from individual representations, to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives. Through integrating this multi-level information, MMPG produces superior protein representations and achieves advanced performance on four different downstream protein tasks.

</details>


### [32] [CtD: Composition through Decomposition in Emergent Communication](https://arxiv.org/abs/2601.10169)
*Boaz Carmeli,Ron Meir,Yonatan Belinkov*

Main category: cs.AI

TL;DR: 论文提出"通过分解实现组合"的方法，让神经网络智能体通过分解-组合两阶段学习，实现对新图像的零样本组合描述。


<details>
  <summary>Details</summary>
Motivation: 组合性是人类的认知机制，能够系统地将已知概念组合成新方式。研究旨在探索人工神经网络如何获得并利用组合泛化能力来描述未见过的图像。

Method: 提出"通过分解实现组合"的两阶段训练方法：1) 分解阶段：在多目标协调游戏中学习将图像分解为基本概念，建立概念代码本；2) 组合阶段：使用代码本将基本概念组合成复杂短语来描述新图像。

Result: 智能体能够成功描述未见过的图像，并且在某些情况下，组合阶段的泛化是零样本实现的，无需额外训练。

Conclusion: 该方法展示了神经网络智能体能够通过分解-组合的学习过程获得组合泛化能力，实现对新图像的零样本描述，为人工智能的组合性认知提供了新思路。

Abstract: Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed "Composition through Decomposition", involves two sequential training steps. In the 'Decompose' step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the 'Compose' step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the `Compose' step is achieved zero-shot, without the need for additional training.

</details>


### [33] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 该研究提出了一个评估高采样率时间序列降采样信息损失的系统工作流，结合形状失真度量与分类性能分析，用于针肌电图信号分析，以平衡计算负荷与诊断信息保留。


<details>
  <summary>Details</summary>
Motivation: 针肌电图信号的高采样率和异构性给基于特征的机器学习模型带来计算挑战，降采样是潜在解决方案，但其对诊断信号内容和分类性能的影响尚不明确，需要系统评估方法。

Method: 开发了一个结合形状失真度量、特征机器学习模型分类结果和特征空间分析的工作流，系统评估不同降采样算法和参数对波形完整性和预测性能的影响，使用三类神经肌肉疾病分类任务进行实验验证。

Result: 工作流能识别保留诊断信息同时显著减少计算负荷的降采样配置，形状感知降采样算法优于标准抽取方法，能更好保留峰值结构和整体信号形态。

Conclusion: 研究提供了选择降采样配置的实用指南，使近实时针肌电图分析成为可能，并提出了一个可推广的工作流，可用于其他高采样率时间序列应用中平衡数据缩减与模型性能。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [34] [GFM4GA: Graph Foundation Model for Group Anomaly Detection](https://arxiv.org/abs/2601.10193)
*Jiujiu Chen,Weijun Zeng,Shaofeng Hu,Sihong Xie,Hui Xiong*

Main category: cs.AI

TL;DR: 提出GFM4GA，一种用于群体异常检测的图基础模型，通过双层次对比学习和参数约束微调，在少样本设置下显著提升检测性能


<details>
  <summary>Details</summary>
Motivation: 群体异常检测面临异常模式多样化的挑战，现有图基础模型只能检测个体异常，无法处理群体异常（异常群体中的个体可能看起来正常）

Method: 提出GFM4GA模型，采用基于特征估计和群体提取的双层次对比学习进行预训练，在下游任务中使用参数约束和群体异常比例加权的少样本微调，通过标记异常邻居确定的群体上下文扩展对未见群体异常的适应能力

Result: 实验表明GFM4GA超越现有群体异常检测器和个体异常检测的图基础模型，在AUROC和AUPRC上分别平均提升2.85%和2.55%

Conclusion: GFM4GA成功将图基础模型扩展到群体异常检测任务，通过创新的预训练和微调策略有效解决了群体异常检测的独特挑战

Abstract: Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.

</details>


### [35] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Topo-RAG框架挑战"一切皆文本"假设，通过双架构分别处理文本叙述和表格结构，在混合查询上比标准线性化方法提升18.4%的nDCG@10


<details>
  <summary>Details</summary>
Motivation: 企业数据集中的文档通常是叙述和结构的复杂混合体，而当前的RAG系统使用线性化方法将多维表格转换为简单文本字符串，这在数学上已被证明是不充分的，无法捕捉电子表格的几何结构。

Method: 提出Topo-RAG框架，采用双架构设计：1) 传统密集检索器处理流畅的叙述文本；2) 单元格感知的延迟交互机制处理表格结构，保留其空间关系。框架尊重数据的拓扑结构。

Result: 在模拟真实世界复杂性的合成企业语料库SEC-25上评估，Topo-RAG在混合查询上的nDCG@10比标准线性化方法提高了18.4%。

Conclusion: Topo-RAG不仅提升了搜索效果，更重要的是理解了信息的形状，挑战了"一切皆文本"的假设，为处理企业文档的复杂性提供了更有效的方法。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [36] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出针对多步推理任务的定向路由方法，仅将关键步骤路由到大型模型，而让小型模型处理常规步骤，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由方法将整个查询分配给单一模型，将所有推理步骤视为同等重要。多步推理任务容易出现级联失败，单个错误步骤会导致整个解决方案崩溃。需要更精细的步骤级路由来提升效率。

Method: TRIM在步骤级别操作：使用过程奖励模型识别错误步骤，基于步骤级不确定性和预算约束做出路由决策。开发了多种路由策略，从简单的基于阈值的策略到更复杂的考虑长期准确性-成本权衡的策略。

Result: 在MATH-500上，即使最简单的阈值策略也超越了先前路由方法，成本效率提高5倍；更高级的策略使用80%更少的昂贵模型token就能匹配强大昂贵模型的性能。在AIME等更难基准测试中，TRIM实现了高达6倍的成本效率提升。

Conclusion: TRIM证明了步骤级难度代表了推理的基本特征，定向步骤级干预可以根本上改变推理效率，将昂贵调用限制在那些需要更强模型防止级联错误的关键步骤上。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [37] [NoReGeo: Non-Reasoning Geometry Benchmark](https://arxiv.org/abs/2601.10254)
*Irina Abdullaeva,Anton Vasiliuk,Elizaveta Goncharova,Temurbek Rahmatullaev,Zagorulko Ivan,Maxim Kurkin,Andrey Kuznetsov*

Main category: cs.AI

TL;DR: NoReGeo是一个评估大语言模型内在几何理解能力的新基准，不依赖推理或代数计算，发现最先进模型在二元分类任务中最高仅达65%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估基于推理的几何能力（使用代数方法），但缺乏对LLMs是否真正内在编码空间关系和识别几何属性的评估，需要专门基准来测试原生几何理解。

Method: 创建包含2,500个简单几何问题的NoReGeo基准，涵盖25个类别，每个问题设计为仅通过原生几何理解（假设已知对象位置）即可解决，评估包括GPT-4在内的最先进模型。

Result: 最先进模型在二元分类任务中最高仅达65%准确率，消融实验显示仅通过微调无法获得几何理解能力，表明需要专门的训练方法。

Conclusion: 当前LLMs在原生理解几何概念方面存在显著差距，为未来开发具有真正几何认知能力的模型提供了研究基础。

Abstract: We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.

</details>


### [38] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: EAPO提出证据增强策略优化方法，通过密集的过程监督改进长上下文推理中的证据检索质量，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在长上下文推理中存在奖励稀疏问题，无法有效惩罚无根据的"幸运猜测"，导致关键的证据检索过程缺乏监督。

Method: 1) 建立证据增强推理范式，验证精确证据提取是关键瓶颈；2) 提出EAPO算法，使用奖励模型计算组相对证据奖励进行密集过程监督；3) 引入自适应奖励-策略协同进化机制，迭代优化奖励模型。

Result: 在八个基准测试上的综合评估表明，EAPO相比最先进的基线方法显著提升了长上下文推理性能。

Conclusion: EAPO通过密集过程监督有效解决了长上下文推理中的奖励稀疏问题，为改进证据检索质量提供了有效方法。

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [39] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: C-GRASP是一个基于RAG增强的临床推理框架，通过八个可追溯的推理步骤解决LLM在HRV分析中的生理幻觉问题，实现更准确的情感分类和临床决策支持。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在心率变异性分析中存在生理幻觉问题，包括呼吸性窦性心律失常污染、非线性指标短数据不稳定性，以及过度依赖群体标准而忽视个体化基线。这些问题阻碍了LLM在临床HRV解释中的应用。

Method: 提出C-GRASP框架，采用RAG增强的管道，将HRV解释分解为八个可追溯的推理步骤。核心是Z-score优先级层次结构，强调个体化基线变化优于规范统计。系统通过自动RSA感知防护栏缓解频谱幻觉，防止频域指标污染。

Result: 在DREAMER数据集的414个试验中，C-GRASP与高规模推理模型（如MedGemma3-thinking）集成，在4类情感分类中达到37.3%的准确率，临床推理一致性得分为69.6%。消融研究证实个体化Delta Z-score模块是关键逻辑锚点。

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持，为生物医学工程中更安全的AI集成铺平了道路，有效防止了原生LLM中常见的"群体偏见"。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [40] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: LatentRefusal：一种基于LLM隐藏激活信号预测查询可回答性的文本到SQL安全拒绝机制，通过Tri-Residual Gated Encoder架构抑制模式噪声并放大问题-模式不匹配信号，实现高效安全防护。


<details>
  <summary>Details</summary>
Motivation: 在基于LLM的文本到SQL系统中，不可回答和未充分指定的用户查询可能生成错误文本或可执行程序，导致误导性结果或违反安全约束，这是安全部署的主要障碍。现有拒绝策略要么依赖输出级指令遵循（易受模型幻觉影响），要么估计输出不确定性（增加复杂性和开销）。

Method: 将文本到SQL系统中的安全拒绝形式化为可回答性门控问题，提出LatentRefusal机制，从大语言模型的中间隐藏激活预测查询可回答性。引入Tri-Residual Gated Encoder轻量级探测架构，抑制模式噪声并放大指示不可回答性的问题-模式不匹配稀疏局部线索。

Result: 在四个基准测试中，LatentRefusal将平均F1分数提高到88.5%，同时在两个骨干模型上仅增加约2毫秒的探测开销。广泛的实证评估、消融研究和可解释性分析证明了该方法的有效性。

Conclusion: LatentRefusal为文本到SQL系统提供了一个可附加且高效的安全层，通过从中间隐藏激活预测查询可回答性，有效解决了不可回答和未充分指定查询的安全问题，同时保持低开销。

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [41] [Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering](https://arxiv.org/abs/2601.10402)
*Xinyu Zhu,Yuzhu Cai,Zexi Liu,Bingyang Zheng,Cheng Wang,Rui Ye,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Yuzhi Zhang,Linfeng Zhang,Weinan E,Di Jin,Siheng Chen*

Main category: cs.AI

TL;DR: ML-Master 2.0通过分层认知缓存架构解决AI在超长周期自主性中的瓶颈，在机器学习工程任务上达到56.44%的奖牌率


<details>
  <summary>Details</summary>
Motivation: 当前AI向代理科学发展的瓶颈在于超长周期自主性——在跨越数天或数周的实验周期中保持战略连贯性和迭代修正的能力。虽然大语言模型在短期推理中表现出色，但在现实研究的高维、延迟反馈环境中容易被执行细节淹没，无法将稀疏反馈整合为连贯的长期指导。

Method: 提出分层认知缓存（HCC），这是一种受计算机系统启发的多层架构，将上下文管理重构为认知积累过程。通过将瞬时执行轨迹动态提炼为稳定知识和跨任务智慧，使代理能够将即时执行与长期实验策略解耦，有效克服静态上下文窗口的扩展限制。

Result: 在OpenAI的MLE-Bench评估中，使用24小时预算，ML-Master 2.0实现了56.44%的最先进奖牌率，展示了在机器学习工程任务上的卓越表现。

Conclusion: 超长周期自主性为AI提供了可扩展的蓝图，使其能够在超越人类先例复杂性的领域进行自主探索。分层认知缓存架构为解决AI在长期自主性中的瓶颈提供了有效方案。

Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.

</details>


### [42] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval是一个错误感知的自动问题生成评估框架，通过两阶段错误诊断和评分流程，解决现有评估方法忽视关键缺陷的问题


<details>
  <summary>Details</summary>
Motivation: 现有自动问题生成评估方法（包括基于LLM的评估器）主要采用黑盒整体范式，缺乏明确的错误建模，导致忽视事实幻觉和答案不匹配等关键缺陷，高估问题质量

Method: ErrEval将评估重新定义为两阶段过程：1) 轻量级即插即用错误识别器检测和分类结构、语言和内容相关方面的常见错误；2) 将这些诊断信号作为明确证据指导LLM评估器进行更细粒度和有依据的判断

Result: 在三个基准测试上的广泛实验表明ErrEval的有效性，显示明确的诊断信号能提高与人类判断的一致性。进一步分析确认ErrEval有效缓解了对低质量问题的高估

Conclusion: ErrEval通过显式错误诊断增强了问题生成评估，提供更准确和细粒度的质量评估，解决了现有评估方法的局限性

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [43] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: LADFA是一个端到端计算框架，结合LLM、RAG和定制知识库，从隐私政策中提取个人数据流并构建数据流图进行分析


<details>
  <summary>Details</summary>
Motivation: 隐私政策通常使用复杂法律语言，难以理解且在不同组织和行业间存在不一致，需要自动化大规模分析工具

Method: 开发LADFA框架，包含预处理模块、基于LLM的处理器和数据流后处理器，结合检索增强生成和定制知识库提取个人数据流并构建数据流图

Result: 通过对汽车行业10个隐私政策的案例研究验证了方法的有效性和准确性，框架具有灵活性和可定制性

Conclusion: LADFA框架能够有效分析隐私政策中的个人数据流，且可扩展到其他基于文本的分析任务

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [44] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: LLMdoctor：一种基于患者-医生范式的测试时对齐框架，通过细粒度token级奖励和流引导偏好优化，在保持生成多样性的同时高效对齐大语言模型


<details>
  <summary>Details</summary>
Motivation: 传统微调方法计算成本高且不灵活，现有测试时对齐方法依赖扭曲的轨迹级信号或低效采样，性能受限且难以保持基础模型的生成多样性

Method: 采用患者-医生范式：冻结的大型患者LLM与小型专业医生模型协同工作。首先从患者模型行为变化中提取细粒度token级偏好信号，然后通过token级流引导偏好优化（TFPO）训练医生模型，建立所有子轨迹的流一致性

Result: LLMdoctor显著优于现有测试时对齐方法，甚至超越DPO等完整微调方法的性能

Conclusion: LLMdoctor提供了一种高效、精确的测试时对齐框架，能够在保持生成多样性的同时实现token级对齐，为LLM对齐提供了新的解决方案

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [45] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: NSR-Boost是一个神经符号残差增强框架，用于在工业场景中非侵入式地升级遗留GBDT模型，通过LLM生成符号代码结构修复预测失败区域，在金融风控系统中成功部署。


<details>
  <summary>Details</summary>
Motivation: 工业环境中升级遗留GBDT模型面临高昂的重新训练成本和系统风险，需要一种安全、低成本的进化范式来提升模型性能。

Method: 三阶段框架：1) 通过残差识别预测失败的"困难区域"；2) 使用LLM生成符号代码结构创建可解释专家，并通过贝叶斯优化微调参数；3) 通过轻量级聚合器动态集成专家与遗留模型输出。

Result: 在6个公共数据集和1个私有数据集上显著优于SOTA基线，在真实在线数据上表现优异，成功部署于Qfin Holdings核心金融风控系统，有效捕捉传统模型遗漏的长尾风险。

Conclusion: NSR-Boost为工业应用提供了安全、低成本的模型进化范式，能够非侵入式地提升遗留模型性能，特别适合高并发生产环境。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [46] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 提出ChartComplete数据集，覆盖30种图表类型，弥补现有图表理解基准数据集仅包含少量图表类型的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在图表理解方面表现优异，但现有基准数据集仅涵盖少量图表类型，无法全面评估模型性能。

Method: 基于可视化社区的图表分类学构建ChartComplete数据集，包含30种不同图表类型的分类图像集合，不包含学习信号。

Result: 创建了一个覆盖广泛图表类型的基准数据集，为研究社区提供了更全面的图表理解评估工具。

Conclusion: ChartComplete数据集填补了现有图表理解基准的空白，为多模态大语言模型的图表理解能力评估提供了更全面的测试平台。

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [47] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 提出领域知识图谱融合任务，通过将通用知识图谱事实视为语义程序，解决领域相关性和知识粒度对齐问题


<details>
  <summary>Details</summary>
Motivation: 领域知识图谱通常比通用知识图谱覆盖不足，需要从通用知识图谱中融合相关事实来丰富领域知识图谱

Method: 提出ExeFuse方法，采用Fact-as-Program范式，将GKG事实视为潜在语义程序，通过程序在目标DKG上的可执行性验证领域相关性

Result: 构建了两个基准数据集DKGF(W-I)和DKGF(Y-I)包含21个评估配置，实验验证了任务的重要性和模型的有效性

Conclusion: 首次为领域知识图谱融合任务提供了标准化测试平台，提出的ExeFuse方法能有效解决领域相关性和知识粒度对齐问题

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [48] [Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment](https://arxiv.org/abs/2601.10520)
*Felix Jahn,Yannic Muskalla,Lisa Dargasz,Patrick Schramowski,Kevin Baum*

Main category: cs.AI

TL;DR: GRACE是一个神经符号推理的约束架构，将规范性推理与工具性决策解耦，通过道德模块、决策模块和监控守卫三部分确保AI代理的道德对齐。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在关键场景中自主部署并产生实际影响，确保其决策不仅工具有效而且符合道德规范变得至关重要。需要一种能够约束任何设计AI代理的架构，实现规范性对齐。

Method: 提出GRACE三层架构：1) 道德模块(MM)使用基于理由的道义逻辑推理确定允许的宏观行动；2) 决策模块(DMM)封装目标代理，在宏观行动约束下选择工具最优的原始行动；3) 守卫监控并强制执行道德合规。MM采用符号表示支持形式验证和统计保证。

Result: 在LLM治疗助手案例中展示了GRACE如何使利益相关者理解、质疑和优化代理行为，实现了可解释性、可争议性和可辩护性。

Conclusion: GRACE通过解耦规范性推理与工具性决策，为AI代理提供了可验证的道德对齐框架，支持形式验证和统计保证，增强了AI系统的可信度和可控性。

Abstract: As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.

</details>


### [49] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: 本文提出一个多层诊断框架，通过微调Llama 3.1 8B、Gemma 2 9B和Mistral模型进行钓鱼检测任务，揭示架构、数据多样性和训练策略之间的相互作用如何影响模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管微调大语言模型在专业任务上取得了最先进性能，但诊断这些模型为何变得脆弱且无法泛化仍然是一个关键未解决问题。需要理解模型泛化失败的根本原因。

Method: 引入多层诊断框架，对Llama 3.1 8B、Gemma 2 9B和Mistral模型进行跨架构研究，在高风险钓鱼检测任务上进行微调，使用SHAP分析和机制可解释性方法来揭示泛化失败的根源。

Result: 发现三个关键结论：(1)泛化能力由架构和数据多样性的强大协同作用驱动，Gemma 2 9B在多样化"通才"数据集上达到>91% F1；(2)泛化高度依赖架构，Llama 3.1 8B在狭窄领域表现良好但无法整合多样数据；(3)某些架构天生更具泛化性，Mistral模型在多种训练范式下表现一致且稳健。

Conclusion: 通过识别导致失败的缺陷启发式方法，本文提供了诊断和理解泛化失败的具体方法论，强调可靠的AI需要对架构、数据和训练策略之间的相互作用进行深度验证。

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [50] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 该报告对7个前沿模型进行综合安全评估，发现安全性能存在显著异质性，GPT-5.2表现最均衡，其他模型在不同评估维度存在明显权衡，强调需要标准化安全评估。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和MLLMs在推理、感知和生成能力上取得显著进步，但这些进步是否带来相应的安全改进仍不明确，部分原因是现有评估实践局限于单一模态或威胁模型，需要综合评估前沿模型的安全性能。

Method: 采用统一协议评估7个前沿模型（GPT-5.2、Gemini 3 Pro等），涵盖语言、视觉语言和图像生成场景，整合基准评估、对抗评估、多语言评估和合规评估四种评估模式。

Result: 安全性能呈现显著异质性：GPT-5.2在所有评估中表现均衡且强劲；其他模型在基准安全、对抗对齐、多语言泛化和监管合规方面存在明显权衡；语言和视觉语言模态在对抗评估中均表现脆弱；文生图模型在受监管视觉风险类别中相对对齐更好，但在对抗或语义模糊提示下仍脆弱。

Conclusion: 前沿模型的安全本质上是多维的，受模态、语言和评估方案影响，需要标准化安全评估来准确评估现实世界风险，指导负责任模型开发和部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [51] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 提出SafeProbing方法，通过显式激活LLM解码过程中的潜在安全信号，实现早期检测不安全内容，有效防御越狱攻击


<details>
  <summary>Details</summary>
Motivation: 尽管LLM经过安全对齐，但现有防御机制（解码约束和后处理检测器）难以应对复杂越狱攻击，要么检测不鲁棒，要么过度降低模型效用。研究发现即使成功越狱，模型在生成过程中仍存在潜在安全信号，但被流畅续写的驱动所压制

Method: 提出SafeProbing方法，在解码过程中显式激活和利用潜在安全信号，早期检测不安全内容。通过表面化这些信号，使模型能够及时自我纠正或拒绝

Result: 在多种越狱攻击实验中，该方法显著提升安全性，同时在良性输入上保持低过度拒绝率，并保持响应质量

Conclusion: 在解码过程中激活内在安全感知为防御越狱攻击提供了一个有前景的补充方向

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [52] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文主张理解基于大语言模型的智能体集体行为是重要研究领域，需要交互主义范式来系统研究先验知识、嵌入价值观与社会情境如何影响多智能体生成AI系统的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 理解基于大语言模型的智能体集体行为对评估社会层面的风险与收益至关重要。LLMs的独特性质——包括预训练知识、隐含社会先验和上下文学习能力——要求新的理论框架来研究多智能体系统中的涌现现象。

Method: 提出交互主义范式，包含替代性理论基础、方法论和分析工具，系统研究先验知识、嵌入价值观与社会情境的相互作用。讨论了四个关键发展方向：理论、方法和跨学科对话。

Result: 论文提出了研究LLM-based collectives的框架，强调需要新的理论和方法来理解多智能体生成AI系统中的集体行为涌现机制。

Conclusion: 理解LLM智能体集体行为是重要研究领域，需要交互主义范式和跨学科合作来应对这一新兴领域的理论、方法和实践挑战。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [53] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent是一个多智能体框架，通过协调专门代理处理复杂基因组查询，在GeneTuring基准测试中比当前最优的GeneGPT平均提升12%性能。


<details>
  <summary>Details</summary>
Motivation: 基因组信息理解对生物医学研究至关重要，但现有方法面临挑战：大型语言模型在基因组问答中受限，无法访问领域特定数据库；当前最优的GeneGPT系统依赖僵化的API调用，适应性有限。

Method: 作者复现了GeneGPT，并提出了GenomAgent——一个多智能体框架，通过协调专门代理来高效处理复杂基因组查询。该框架具有灵活架构，可扩展到需要专家知识提取的各种科学领域。

Result: 在GeneTuring基准测试的九个任务上，GenomAgent平均性能比GeneGPT高出12%，展现出更好的基因组问答能力。

Conclusion: GenomAgent通过多智能体协调机制有效解决了基因组信息提取的挑战，其灵活架构不仅适用于基因组学，还可扩展到其他需要专家知识提取的科学领域。

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [54] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 提出一种符号化算法，用于LTLf多属性综合问题，通过布尔目标变量和单调性紧凑表示指数级目标组合，大幅超越枚举方法


<details>
  <summary>Details</summary>
Motivation: 在LTLf综合中，当多个属性无法同时满足时，传统方法需要枚举所有属性子集，计算效率低下

Method: 开发完全符号化算法，引入布尔目标变量表示目标集合，利用单调性紧凑表示指数级目标组合，通过单次不动点计算确定状态与可实现目标集的关系

Result: 算法显著优于基于枚举的基线方法，速度提升可达两个数量级

Conclusion: 提出的符号化方法能高效解决LTLf多属性综合问题，通过紧凑表示和单次不动点计算实现性能大幅提升

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [55] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: HRM在推理任务中表现出色，但研究发现其存在"猜测"而非"推理"的模式，通过数据增强、输入扰动和模型引导三种策略提升猜测质量，将数独极端问题的准确率从54.5%提升至96.9%。


<details>
  <summary>Details</summary>
Motivation: 为了理解分层推理模型（HRM）的优势和潜在失败模式，研究其推理机制，发现其存在"猜测"而非"推理"的行为模式，并探索如何通过增强猜测能力来提升模型性能。

Method: 对HRM进行机制性研究，发现三个关键现象：简单谜题失败、推理步骤中的"顿悟"动态、多个不动点存在。基于"猜测"视角提出三种扩展策略：数据增强（提升猜测质量）、输入扰动（利用推理随机性增加猜测次数）、模型引导（利用训练随机性增加猜测次数）。

Result: 结合所有方法开发出增强型HRM，在Sudoku-Extreme任务上的准确率从54.5%显著提升至96.9%。揭示了推理模型"推理"行为的新见解。

Conclusion: HRM实际上更多是"猜测"而非"推理"，通过系统性地扩展猜测能力可以显著提升模型性能，这为理解推理模型的工作机制提供了新的科学视角。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [56] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 提出一种基于结构感知和多样性约束的上下文气泡构建框架，替代传统的top-k检索方法，以解决信息碎片化、内容重复和查询上下文不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法使用top-k段落检索会导致文档结构中的信息图碎片化、过度检索、内容重复，以及查询上下文不足（包括二阶和三阶方面）。

Method: 提出结构感知和多样性约束的上下文气泡构建框架：1) 利用任务条件化的结构先验指导检索；2) 从高相关性锚点跨度开始，通过平衡查询相关性、边际覆盖率和冗余惩罚的约束选择构建上下文气泡；3) 明确约束多样性和预算，生成紧凑且信息丰富的上下文集合。

Result: 在企业文档上的实验表明，上下文气泡方法显著减少了冗余上下文，更好地覆盖次要方面，在有限上下文窗口内具有更好的答案质量和引用忠实度。消融研究显示结构先验和多样性约束选择都是必要的。

Conclusion: 该方法通过结构感知和多样性约束的上下文构建，有效解决了传统RAG方法的局限性，提供了可审计和确定性调优的检索过程，在保持紧凑性的同时提高了信息覆盖和答案质量。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [57] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: 研究探讨生成式AI对建筑设计任务中表现、创意自我效能和认知负荷的影响，发现AI对新手设计师有帮助，但会降低创意自我效能，效果取决于用户专业水平和提示策略。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在建筑设计任务中对设计表现、创意自我效能和认知负荷的影响，探索AI工具在不同专业水平用户中的效果差异。

Method: 36名学生参与两阶段建筑设计任务：独立设计和外部工具辅助设计（生成式AI组和对照组使用现有建筑项目在线库）。专家评估设计成果，参与者自我报告自我效能和认知负荷，使用双重差分法分析数据。

Result: 整体上生成式AI没有显著提升设计表现，但对新手设计师有显著改善；使用AI的学生创意自我效能下降；认知负荷无显著差异，但迭代想法生成和视觉反馈提示能更大程度降低认知负荷。

Conclusion: 生成式AI的效果取决于用户先前专业水平和提示交互策略，对新手设计师有益但可能降低创意自我效能，提示策略影响认知负荷。

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [58] [High signal-to-noise ratio asymptotics of entropy-constrained Gaussian channel capacity](https://arxiv.org/abs/2601.09864)
*Adway Girish,Shlomo Shamai,Emre Telatar*

Main category: cs.IT

TL;DR: 论文研究了高斯信道在高信噪比下的输入熵约束容量问题，证明了容量达到分布是支撑在缩放整数格上的离散高斯分布，且输入熵与容量之间的差距随信噪比指数衰减。


<details>
  <summary>Details</summary>
Motivation: 研究高斯信道在输入熵约束下的容量问题，特别是在高信噪比渐近区域，这对于理解信息传输的极限性能具有重要意义。

Method: 采用渐近分析方法，研究信噪比趋于无穷时的极限行为，证明容量达到分布的形式，并分析熵与容量差距的衰减特性。

Result: 证明了容量达到分布是支撑在缩放整数格上的离散高斯分布，输入熵与容量之间的差距随信噪比指数衰减，并刻画了该指数。

Conclusion: 在高信噪比渐近区域，高斯信道的输入熵约束容量问题有明确的解析解，离散高斯分布是最优的，且熵容量差距呈指数衰减。

Abstract: We study the input-entropy-constrained Gaussian channel capacity problem in the asymptotic high signal-to-noise ratio (SNR) regime. We show that the capacity-achieving distribution as SNR goes to infinity is given by a discrete Gaussian distribution supported on a scaled integer lattice. Further, we show that the gap between the input entropy and the capacity decreases to zero exponentially in SNR, and characterize this exponent.

</details>


### [59] [Learning-Augmented Perfectly Secure Collaborative Matrix Multiplication](https://arxiv.org/abs/2601.09916)
*Zixuan He,Mohammad Reza Deylam Salehi,Derya Malak,Photios A. Stavrou*

Main category: cs.IT

TL;DR: 提出了一种完美安全的矩阵乘法协议，用于多方计算A⊤B，保证信息论隐私和正确性，并引入了学习增强扩展以提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 在多方计算环境中，需要安全地计算矩阵乘法A⊤B，同时保证信息论隐私、抵抗半诚实合谋攻击，并在本地存储约束下实现最优恢复阈值。

Method: 将子矩阵编码为稀疏掩码多项式的评估值，结合系数对齐和Beaver式随机性确保完美保密；扩展部分引入基于张量分解的本地块乘法，融合经典和学习的低秩方法。

Result: 协议在安全阈值以下的任何合谋方只能观察到均匀随机份额，恢复阈值达到最优；学习增强版本在保持隐私和恢复保证的同时，随着矩阵维度增长可提供高达80%的计算效率提升。

Conclusion: 该PSMM协议为多方矩阵乘法提供了完美安全、最优恢复阈值的解决方案，其学习增强扩展进一步实现了可扩展的计算效率提升，同时保持了隐私保证。

Abstract: This paper presents a perfectly secure matrix multiplication (PSMM) protocol for multiparty computation (MPC) of $\mathrm{A}^{\top}\mathrm{B}$ over finite fields. The proposed scheme guarantees correctness and information-theoretic privacy against threshold-bounded, semi-honest colluding agents, under explicit local storage constraints. Our scheme encodes submatrices as evaluations of sparse masking polynomials and combines coefficient alignment with Beaver-style randomness to ensure perfect secrecy. We demonstrate that any colluding set of parties below the security threshold observes uniformly random shares, and that the recovery threshold is optimal, matching existing information-theoretic limits. Building on this framework, we introduce a learning-augmented extension that integrates tensor-decomposition-based local block multiplication, capturing both classical and learned low-rank methods. We demonstrate that the proposed learning-based PSMM preserves privacy and recovery guarantees for MPC, while providing scalable computational efficiency gains (up to $80\%$) as the matrix dimensions grow.

</details>


### [60] [Breaking the Storage-Bandwidth Tradeoff in Distributed Storage with Quantum Entanglement](https://arxiv.org/abs/2601.10676)
*Lei Hu,Mohamed Nomeir,Alptug Aytekin,Sennur Ulukus*

Main category: cs.IT

TL;DR: 量子资源在分布式存储系统中的应用研究，通过量子通信和纠缠显著改善了存储-修复带宽的权衡关系


<details>
  <summary>Details</summary>
Motivation: 研究量子资源如何改善分布式存储系统的性能，特别是在节点故障修复过程中，探索量子通信相比经典通信的优势

Method: 在(n,k,d)分布式存储系统中引入量子通信：当节点故障时，d个辅助节点通过量子信道向新节点传输信息，新节点通过测量接收的量子态生成存储，分析存储与修复带宽的基本权衡关系

Result: 量子纠缠显著改善了存储-带宽权衡关系，特别是在最小存储再生点；当d≥2k-2时，存在存储和修复带宽同时最小化的操作点，打破了经典设置中的权衡关系

Conclusion: 量子通信为分布式存储系统带来了根本性的新机制，通过量子纠缠可以实现存储和修复带宽的同时最小化，这在经典系统中是不可能的

Abstract: This work investigates the use of quantum resources in distributed storage systems. Consider an $(n,k,d)$ distributed storage system in which a file is stored across $n$ nodes such that any $k$ nodes suffice to reconstruct the file. When a node fails, any $d$ helper nodes transmit information to a newcomer to rebuild the system. In contrast to the classical repair, where helper nodes transmit classical bits, we allow them to send classical information over quantum channels to the newcomer. The newcomer then generates its storage by performing appropriate measurements on the received quantum states. In this setting, we fully characterize the fundamental tradeoff between storage and repair bandwidth (total communication cost). Compared to classical systems, the optimal storage--bandwidth tradeoff can be significantly improved with the enhancement of quantum entanglement shared only among the surviving nodes, particularly at the minimum-storage regenerating point. Remarkably, we show that when $d \geq 2k-2$, there exists an operating point at which \textit{both storage and repair bandwidth are simultaneously minimized}. This phenomenon breaks the tradeoff in the classical setting and reveals a fundamentally new regime enabled by quantum communication.

</details>


### [61] [One-Cold Poisson Channel: A Simple Continuous-Time Channel with Zero Dispersion](https://arxiv.org/abs/2601.09894)
*Cheuk Ting Li*

Main category: cs.IT

TL;DR: 论文提出了一种新型通信信道——单冷泊松信道（OCPC），其中发射机每次选择衰减一个频带。完美OCPC具有容量1、零信道色散和简并信息谱，是唯一已知具有闭式最优非渐近错误概率公式的非平凡无记忆信道。


<details>
  <summary>Details</summary>
Motivation: 研究动机是寻找一种极其简单的连续时间无记忆信道模型，该模型具有闭式最优非渐近错误概率公式，可作为信息的基本度量单位，并可能应用于带可调谐带阻滤波器的光通信。

Method: 引入单冷泊松信道（OCPC）模型，其中发射机在多个频带中选择一个进行衰减。特别研究了完美OCPC（频带数量无限的情况），分析了其容量、信道色散和信息谱特性。还研究了带反馈的OCPC以及一般OCPC的非渐近编码和信道仿真。

Result: 完美OCPC具有容量1、零信道色散，信息谱为在1处的简并分布。这是唯一已知具有闭式最优非渐近错误概率公式的非平凡（离散或连续时间）无记忆信道。带反馈的OCPC推广了前缀码的概念。

Conclusion: OCPC是一种极其简单的连续时间无记忆信道，可作为无限可分的信息基本度量单位（替代比特），在光通信等领域有潜在应用价值，并为非渐近编码理论提供了新的研究范例。

Abstract: We introduce the one-cold Poisson channel (OCPC), where the transmitter chooses one of several frequency bands to attenuate at a time. In particular, the perfect OCPC, where the number of bands is unlimited, is an extremely simple continuous-time memoryless channel. It has a capacity 1, zero channel dispersion, and an information spectrum being the degenerate distribution at 1. It is the only known nontrivial (discrete or continuous-time) memoryless channel with a closed-form formula for its optimal non-asymptotic error probability, making it the simplest channel in this sense. A potential application is optical communication with a tunable band rejection filter. Due to its simplicity, we may use it as a basic currency of information that is infinitely divisible, as an alternative to bits which are not infinitely divisible. OCPC with perfect feedback gives a generalization of prefix codes. We also study non-asymptotic coding and channel simulation results for the general OCPC.

</details>


### [62] [Reconstructing Reed-Solomon Codes from Multiple Noisy Channel Outputs](https://arxiv.org/abs/2601.09947)
*Shubhransh Singhvi,Han Mao Kiah,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 研究序列重构问题，针对RS码在q元DMS替换信道下的高效重构算法，推导出明确的速率阈值


<details>
  <summary>Details</summary>
Motivation: Levenshtein提出的序列重构问题考虑发送方传输码字，接收方观察到K个独立噪声版本。需要研究在q元离散无记忆对称替换信道下，如何高效重构传输码字。

Method: 针对RS码，将Koetter-Vardy软判决译码算法适配为高效重构算法。对于足够大的分组长度和字母表大小，推导出仅依赖于(p,K)的明确速率阈值。

Result: 当码率R低于该阈值时，可以以任意小的错误概率重构传输码字。算法在块长和字母表足够大时有效。

Conclusion: 成功将软判决译码算法应用于序列重构问题，为RS码在DMS替换信道下的重构提供了理论保证和实用算法。

Abstract: The sequence reconstruction problem, introduced by Levenshtein in 2001, considers a communication setting in which a sender transmits a codeword and the receiver observes K independent noisy versions of this codeword. In this work, we study the problem of efficient reconstruction when each of the $K$ outputs is corrupted by a $q$-ary discrete memoryless symmetric (DMS) substitution channel with substitution probability $p$. Focusing on Reed-Solomon (RS) codes, we adapt the Koetter-Vardy soft-decision decoding algorithm to obtain an efficient reconstruction algorithm. For sufficiently large blocklength and alphabet size, we derive an explicit rate threshold, depending only on $(p, K)$, such that the transmitted codeword can be reconstructed with arbitrarily small probability of error whenever the code rate $R$ lies below this threshold.

</details>


### [63] [Private Information Retrieval for Graph-based Replication with Minimal Subpacketization](https://arxiv.org/abs/2601.09957)
*Vayur Shanbhag,Prasad Krishnan*

Main category: cs.IT

TL;DR: 提出了两种新的基于图复制数据库的最小子包化私有信息检索方案：针对星形图的方案和针对一般图的方案，均实现了单位子包化（最小化）。


<details>
  <summary>Details</summary>
Motivation: 在基于图复制的私有信息检索系统中，需要在保持高检索率的同时降低子包化程度，子包化限制了协议执行所需的文件大小。现有方案在子包化方面存在不足，需要设计更优的方案。

Method: 设计了两种新方案：1) 针对星形图特殊类别的方案；2) 针对一般图的方案，使用独立集进行图分解。两种方案都实现了单位子包化（最小化）。

Result: 星形图方案在一般星形图上比已知低子包化方案有更好的检索率。一般图方案在完全图上比先前方案检索率低，但在某些特定图类别上能实现更高的检索率。扩展到多重图时，在完全多重图上比先前方案有更高的检索率。

Conclusion: 成功设计了两种最小子包化的私有信息检索方案，分别针对星形图和一般图，在保持高检索率的同时实现了单位子包化，为基于图复制的PIR系统提供了更优的解决方案。

Abstract: We design new minimal-subpacketization schemes for information-theoretic private information retrieval on graph-based replicated databases. In graph-based replication, the system consists of $K$ files replicated across $N$ servers according to a graph with $N$ vertices and $K$ edges. The client wants to retrieve one desired file, while keeping the index of the desired file private from each server via a query-response protocol. We seek PIR protocols that have (a) high rate, which is the ratio of the file-size to the total download cost, and (b) low subpacketization, which acts as a constraint on the size of the files for executing the protocol. We report two new schemes which have unit-subpacketization (which is minimal): (i) for a special class of graphs known as star graphs, and (ii) for general graphs. Our star-graph scheme has a better rate than previously known schemes with low subpacketization for general star graphs. Our scheme for general graphs uses a decomposition of the graph via independent sets. This scheme achieves a rate lower than prior schemes for the complete graph, however it can achieve higher rates than known for some specific graph classes. An extension of our scheme to the case of multigraphs achieves a higher rate than previous schemes for the complete multi-graph.

</details>


### [64] [On the Leaky Private Information Retrieval with Side Information](https://arxiv.org/abs/2601.09960)
*Yingying Huangfu,Tian Bai*

Main category: cs.IT

TL;DR: 本文研究了带有侧信息的泄露隐私私有信息检索(L-PIR-SI)，通过放松完美隐私要求来提升通信效率，提出统一概率框架量化隐私泄露，并建立了泄露、侧信息和检索效率之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有PIR-SI研究主要关注完美隐私保护，但在实际应用中，允许可控的信息泄露可以显著提高通信效率。目前对于在侧信息存在情况下，如何量化和管理隐私泄露的研究尚属空白。

Method: 提出统一的概率框架构建L-PIR-SI方案，使用参数ε量化隐私泄露（符合差分隐私标准），分析可实现的下载成本，并将结果推广到多个经典PIR场景。

Result: 建立了L-PIR-SI的下载成本特性，当ε→0时恢复PIR-SI的容量，当无侧信息时退化为已知的leaky-PIR界限，首次揭示了泄露、侧信息和检索效率之间的权衡关系。

Conclusion: 本文首次系统研究了带有侧信息的泄露隐私私有信息检索问题，提出的统一框架能够平衡隐私保护和通信效率，为实际PIR系统设计提供了理论基础。

Abstract: This paper investigates the problem of leaky-private Private Information Retrieval with Side Information (L-PIR-SI), which relaxes the requirement of perfect privacy to achieve improved communication efficiency in the presence of side information. While the capacities of PIR-SI under both $W$-privacy and $(W,S)$-privacy have been partially explored, the impact of controlled information leakage in these settings remains unaddressed. We propose a unified probabilistic framework to construct L-PIR-SI schemes where the privacy leakage is quantified by a parameter $\varepsilon$, consistent with differential privacy standards. We characterize the achievable download costs and show that our results generalize several landmark results in the PIR literature: they recover the capacity of PIR-SI when $\varepsilon \to 0$, and reduce to the known bounds for leaky-PIR when side information is absent. This work provides the first look at the trade-offs between leakage, side information, and retrieval efficiency.

</details>


### [65] [Fundamental Limits of Coded Polynomial Aggregation](https://arxiv.org/abs/2601.10028)
*Xi Zhong,Jörg Kliewer,Mingyue Ji*

Main category: cs.IT

TL;DR: 将编码多项式聚合(CPA)扩展到具有预定义非掉队者模式的分布式计算系统，建立了精确恢复的必要充分条件，并识别了保证恢复的交集大小阈值。


<details>
  <summary>Details</summary>
Motivation: 在分布式计算系统中，掉队者(straggler)会显著降低计算效率。传统的编码多项式聚合(CPA)虽然能减少所需工作节点响应数量，但未考虑掉队者模式。本文旨在扩展CPA到掉队者感知的分布式计算系统，实现在预定义非掉队者模式下的精确恢复。

Method: 提出掉队者感知的CPA框架，要求仅在给定的可容许非掉队者集合上实现精确恢复。通过分析非掉队者模式的交集结构，建立精确恢复的必要充分条件，识别交集大小阈值，并提供可行的CPA方案构造方法。

Result: 证明精确恢复所需的工作节点响应数量少于基于单独解码的多项式编码计算。建立了精确恢复的可行性特征，识别了保证精确恢复的充分阈值，并证明当可容许非掉队者集合数量足够大时，该阈值既是必要的也是充分的。提供了显式构造方案。

Conclusion: 掉队者感知的CPA框架能有效减少分布式计算中的工作节点响应需求，其可行性由非掉队者模式的交集结构决定。理论分析和仿真验证了所提阈值的紧致性，为实际分布式计算系统设计提供了理论指导。

Abstract: Coded polynomial aggregation (CPA) enables the master to directly recover a weighted aggregation of polynomial evaluations without individually decoding each term, thereby reducing the number of required worker responses. In this paper, we extend CPA to straggler-aware distributed computing systems and introduce a straggler-aware CPA framework with pre-specified non-straggler patterns, where exact recovery is required only for a given collection of admissible non-straggler sets. Our main result shows that exact recovery of the desired aggregation is achievable with fewer worker responses than required by polynomial coded computing based on individual decoding, and that feasibility is fundamentally characterized by the intersection structure of the non-straggler patterns. In particular, we establish necessary and sufficient conditions for exact recovery in straggler-aware CPA and identify an intersection-size threshold that is sufficient to guarantee exact recovery. We further prove that this threshold becomes both necessary and sufficient when the number of admissible non-straggler sets is sufficiently large. We also provide an explicit construction of feasible CPA schemes whenever the intersection size exceeds the derived threshold. Finally, simulations reveal a sharp feasibility transition at the predicted threshold, providing empirical evidence that the bound is tight in practice.

</details>


### [66] [Optimal Proximity Gap for Folded Reed--Solomon Codes via Subspace Designs](https://arxiv.org/abs/2601.10047)
*Fernando Granha Jeronimo,Lenny Liu,Pranav Rajpal*

Main category: cs.IT

TL;DR: 该论文研究了折叠里德-所罗门码的邻近间隙性质，证明了在最优容量范围内存在(δ,ε)-邻近间隙


<details>
  <summary>Details</summary>
Motivation: 研究折叠里德-所罗门码是否具有类似于仿射子空间相对于RS码的邻近间隙性质，特别是在最优容量范围内

Method: 利用折叠里德-所罗门码的列表解码算法，将框架扩展到适合的子空间设计码

Result: 肯定地回答了研究问题，证明了折叠里德-所罗门码在最优容量范围内存在(δ,ε)-邻近间隙

Conclusion: 折叠里德-所罗门码具有与仿射子空间类似的邻近间隙性质，这一框架可推广到合适的子空间设计码

Abstract: A collection of sets satisfies a $(δ,\varepsilon)$-proximity gap with respect to some property if for every set in the collection, either (i) all members of the set are $δ$-close to the property in (relative) Hamming distance, or (ii) only a small $\varepsilon$-fraction of members are $δ$-close to the property.
  In a seminal work, Ben-Sasson \textit{et al.}\ showed that the collection of affine subspaces exhibits a $(δ,\varepsilon)$-proximity gap with respect to the property of being Reed--Solomon (RS) codewords with $δ$ up to the so-called Johnson bound for list decoding. Their technique relies on the Guruswami--Sudan list decoding algorithm for RS codes, which is guaranteed to work in the Johnson bound regime.
  Folded Reed--Solomon (FRS) codes are known to achieve the optimal list decoding radius $δ$, a regime known as capacity. Moreover, a rich line of list decoding algorithms was developed for FRS codes. It is then natural to ask if FRS codes can be shown to exhibit an analogous $(δ,\varepsilon)$-proximity gap, but up to the so-called optimal capacity regime. We answer this question in the affirmative (and the framework naturally applies more generally to suitable subspace-design codes).
  An additional motivation to understand proximity gaps for FRS codes is the recent results [BCDZ'25] showing that they exhibit properties similar to random linear codes, which were previously shown to be related to properties of RS codes with random evaluation points in [LMS'25], as well as codes over constant-size alphabet based on AEL [JS'25].

</details>


### [67] [Function Correcting Codes for Maximally-Unbalanced Boolean Functions](https://arxiv.org/abs/2601.10135)
*Rajlaxmi Pandey,Shiven Bajpai,Anjana A Mahesh,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 研究针对最大不平衡布尔函数的最优单错误纠正函数纠正码，分析其距离矩阵结构对AWGN信道下误码性能的影响


<details>
  <summary>Details</summary>
Motivation: 函数纠正码允许在噪声信道上可靠计算函数而无需完全恢复消息，但不同结构对性能影响尚不清楚

Method: 通过关联码字距离矩阵分析最优SEFCC结构，识别不同FCC类别，在AWGN信道上评估软判决和硬判决解码性能

Result: 不同距离矩阵结构的FCC在数据BER和函数错误行为上有显著差异，码结构影响强烈依赖于解码策略

Conclusion: FCC的结构特性对性能有重要影响，解码策略选择需考虑码的具体结构特征

Abstract: Function-Correcting Codes (FCCs) enable reliable computation of a function of a $k$-bit message over noisy channels without requiring full message recovery. In this work, we study optimal single-error correcting FCCs (SEFCCs) for maximally-unbalanced Boolean functions, where $k$ denotes the message length and $t$ denotes the error-correction capability. We analyze the structure of optimal SEFCC constructions through their associated codeword distance matrices and identify distinct FCC classes based on this structure. We then examine the impact of these structural differences on error performance by evaluating representative FCCs over the additive white Gaussian noise (AWGN) channel using both soft-decision and hard-decision decoding. The results show that FCCs with different distance-matrix structures can exhibit markedly different Data BER and function error behavior, and that the influence of code structure depends strongly on the decoding strategy.

</details>


### [68] [On Existence of Girth-8 QC-LDPC Code with Large Column Weight: Combining Mirror-sequence with Classification Modulo Ten](https://arxiv.org/abs/2601.10170)
*Guohua Zhang,Xiangya Liu,Jianhua Zhang,Yi Fang*

Main category: cs.IT

TL;DR: 本文在GCD框架下，通过引入镜像序列和新行重组方案，代数构造了列重为7和8、长度极短、围长为8的QC-LDPC码，将连续循环尺寸下界提升了约20%。


<details>
  <summary>Details</summary>
Motivation: 构建具有大围长的准循环LDPC码在信道编码、压缩感知和分布式存储系统中至关重要。主要挑战是如何用代数方法（而非搜索方法）构造出长度最短（即循环尺寸最小）的此类码。

Method: 在先前提出的最大公约数(GCD)框架基础上，引入镜像序列概念并采用新的行重组方案，以代数方式构造列重为7和8、围长为8的QC-LDPC码。

Result: 对于列重7和8，连续循环尺寸的下界相比现有基准提升了约20%。新构造的码还能提供比新下界小约25%的循环尺寸。

Conclusion: 通过创新的代数方法，成功构造了列重更高、长度更短、围长为8的QC-LDPC码，显著改进了现有构造的性能边界。

Abstract: Quasi-cyclic (QC) LDPC codes with large girths play a crucial role in several research and application fields, including channel coding, compressed sensing and distributed storage systems. A major challenge in respect of the code construction is how to obtain such codes with the shortest possible length (or equivalently, the smallest possible circulant size) using algebraic methods instead of search methods. The greatest-common-divisor (GCD) framework we previously proposed has algebraically constructed QC-LDPC codes with column weights of 5 and 6, very short lengths, and a girth of 8. By introducing the concept of a mirror sequence and adopting a new row-regrouping scheme, QC-LDPC codes with column weights of 7 and 8, very short lengths, and a girth of 8 are proposed for arbitrary row weights in this article via an algebraic manner under the GCD framework. Thanks to these novel algebraic methods, the lower bounds (for column weights 7 and 8) on consecutive circulant sizes are both improved by asymptotically about 20%, compared with the existing benchmarks. Furthermore, these new constructions can also offer circulant sizes asymptotically about 25% smaller than the novel bounds.

</details>


### [69] [A Low-Complexity Architecture for Multi-access Coded Caching Systems with Arbitrary User-cache Access Topology](https://arxiv.org/abs/2601.10175)
*Ting Yang,Minquan Cheng,Xinping Yi,Robert Caiming Qiu,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出基于图神经网络的通用学习框架，用于解决任意用户-缓存访问拓扑下的多接入编码缓存问题，在保持接近最优传输负载的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有MACC模型依赖高度结构化的连接设计，无法处理任意用户-缓存访问拓扑。需要一种通用且低复杂度的传输方案，能够适应各种拓扑结构和大规模系统。

Method: 1) 提出基于图的通用框架，将传输问题建模为冲突图着色问题；2) 使用DSatur贪婪着色算法作为基准；3) 开发基于图神经网络的学习框架，高效构建近最优编码组播传输；4) 扩展索引编码下界到任意访问拓扑。

Result: 学习方案在传输负载上接近DSatur算法和下界，同时显著减少计算时间。数值结果表明该方法在不同访问拓扑和用户数量下具有良好的泛化能力。

Conclusion: 提出的学习框架为任意拓扑MACC问题提供了高效解决方案，平衡了传输性能和计算复杂度，适用于大规模实际系统。

Abstract: This paper studies the multi-access coded caching (MACC) problem under arbitrary user-cache access topologies, extending existing models that rely on highly structured and combinatorially designed connectivity. We consider a MACC system consisting of a single server, multiple cache nodes, and multiple user nodes. Each user can access an arbitrary subset of cache nodes to retrieve cached content. The objective is to design a general and low-complexity delivery scheme under fixed cache placement for arbitrary access topologies. We propose a universal graph-based framework for modeling the MACC delivery problem, where decoding conflicts among requested packets are captured by a conflict graph and the delivery design is reduced to a graph coloring problem. In this formulation, a lower transmission load corresponds to using fewer colors. The classical greedy coloring algorithm DSatur achieves a transmission load close to the index-coding converse bound, providing a tight benchmark, but its computational complexity becomes prohibitive for large-scale graphs. To overcome this limitation, we develop a learning-based framework using graph neural networks that efficiently constructs near-optimal coded multicast transmissions and generalizes across diverse access topologies and varying numbers of users. In addition, we extend the index-coding converse bound for uncoded cache placement to arbitrary access topologies and propose a low-complexity greedy approximation. Numerical results demonstrate that the proposed learning-based scheme achieves transmission loads close to those of DSatur and the converse bound while significantly reducing computational time.

</details>


### [70] [Error-Correcting Codes for the Sum Channel](https://arxiv.org/abs/2601.10256)
*Lyan Abboud,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 提出了一种新的信道模型——和信道，用于分布式存储和DNA数据存储应用，构建了纠双删除码和单替换码，并证明了其冗余度的近似最优性。


<details>
  <summary>Details</summary>
Motivation: 受分布式存储和DNA数据存储应用启发，提出和信道模型，该模型在无错误情况下输入ℓ行二进制矩阵，输出(ℓ+1)行矩阵，其中前ℓ行等于输入，最后一行是前ℓ行的奇偶和行。

Method: 1. 构建了纠双删除码，冗余度为2⌈log₂log₂n⌉ + O(ℓ²)；2. 当ℓ=2时，建立了冗余度上界⌈log₂log₂n⌉ + O(1)；3. 构建了纠单替换码，冗余度为⌈log₂(ℓ+1)⌉。

Result: 1. 纠双删除码的冗余度在ℓ=2时是最优的2倍以内；2. 纠单替换码的冗余度在最优值的1比特以内。

Conclusion: 提出的和信道模型及相关编码方案在分布式存储和DNA数据存储中具有应用价值，纠双删除码和纠单替换码的冗余度均接近最优，为相关存储系统提供了有效的错误纠正方案。

Abstract: We introduce the sum channel, a new channel model motivated by applications in distributed storage and DNA data storage. In the error-free case, it takes as input an $\ell$-row binary matrix and outputs an $(\ell+1)$-row matrix whose first $\ell$ rows equal the input and whose last row is their parity (sum) row. We construct a two-deletion-correcting code with redundancy $2\lceil\log_2\log_2 n\rceil + O(\ell^2)$ for $\ell$-row inputs. When $\ell=2$, we establish an upper bound of $\lceil\log_2\log_2 n\rceil + O(1)$, implying that our redundancy is optimal up to a factor of 2. We also present a code correcting a single substitution with $\lceil \log_2(\ell+1)\rceil$ redundant bits and prove that it is within one bit of optimality.

</details>


### [71] [Transmission Mask Analysis for Range-Doppler Sensing in Half-Duplex ISAC](https://arxiv.org/abs/2601.10259)
*Dikai Liu,Yifeng Xiong,Marco Lops,Fan Liu,Jianhua Zhang*

Main category: cs.IT

TL;DR: 分析MASM在ISAC中的周期性传输掩码，推导其闭式期望距离-多普勒响应，证明距离旁瓣具有多普勒不变性，并揭示不同动态场景下的最优掩码设计


<details>
  <summary>Details</summary>
Motivation: 研究半双工集成感知与通信（ISAC）中掩码调制（MASM）的周期性传输掩码，旨在理解其距离-多普勒响应特性，为优化感知性能提供理论指导

Method: 分析周期性传输掩码，推导闭式期望距离-多普勒响应，研究距离旁瓣的多普勒不变性，并针对不同动态场景（中等动态和高动态）分析最优掩码设计

Result: 距离旁瓣具有多普勒不变性，将距离旁瓣最优性扩展到二维设置；对于距离主瓣，周期性掩码产生稀疏多普勒旁瓣：在中等动态场景下循环差集（特别是Singer CDS）是极小极大最优的，而在高动态场景下多普勒旁瓣能量是掩码自相关的凹函数

Conclusion: 周期性掩码设计在不同动态场景下呈现不同最优策略，中等动态下循环差集最优，高动态下存在多普勒旁瓣能量与主瓣波动之间的固有权衡

Abstract: In this paper, we analyze the periodic transmission masks for MASked Modulation (MASM) in half-duplex integrated sensing and communication (ISAC), and derive their closed-form expected range-Doppler response $\mathbb{E}\{r(k,l,ν)\}$. We show that range sidelobes ($k\neq l$) are Doppler-invariant, extending the range-sidelobe optimality to the 2-D setting. For the range mainlobe ($k=l$), periodic masking yields sparse Doppler sidelobes: Cyclic difference sets (CDSs) (in particular Singer CDSs) are minimax-optimal in a moderately dynamic regime, while in a highly dynamic regime the Doppler-sidelobe energy is a concave function of the mask autocorrelation, revealing an inevitable tradeoff with mainlobe fluctuation.

</details>


### [72] [Algebraic Properties of PAC Codes](https://arxiv.org/abs/2601.10262)
*Vlad-Florin Dragoi,Mohammad Rowshan*

Main category: cs.IT

TL;DR: 本文分析极化调整卷积码，定义广义多项式极化码类，推导其结构性质如对偶性、最小距离等


<details>
  <summary>Details</summary>
Motivation: 研究极化码和Reed-Muller码的代数表示，扩展PAC码和反向PAC码的理论框架，建立更一般的编码结构

Method: 使用极化码和Reed-Muller码的代数表示，定义广义多项式极化码类，推导其结构性质如对偶性、最小距离、最小重量码字数等

Result: 建立了广义多项式极化码的理论框架，推导了其结构性质，包括对偶性、最小距离，以及最小重量码字数和单项式子码维度的结构限制

Conclusion: 广义多项式极化码为PAC码和反向PAC码提供了统一的理论框架，其结构性质分析有助于理解这类编码的性能特征和设计原理

Abstract: We analyze polarization-adjusted convolutional codes using the algebraic representation of polar and Reed-Muller codes. We define a large class of codes, called generalized polynomial polar codes which include PAC codes and Reverse PAC codes. We derive structural properties of generalized polynomial polar codes, such as duality, minimum distance. We also deduce some structural limits in terms of number of minimum weight codewords, and dimension of monomial sub-code.

</details>


### [73] [On the Capacity of Noisy Frequency-based Channels](https://arxiv.org/abs/2601.10329)
*Yuval Gerzon,Ilan Shomorony,Nir Weinberger*

Main category: cs.IT

TL;DR: 研究噪声频率信道的容量，针对DNA数据存储中的短分子机制，信息编码在项目类型的频率而非顺序中，分析采样噪声和识别噪声对信道容量的影响。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储中的短分子机制需要将信息编码在项目类型的频率而非顺序中，现有研究已解决无噪声频率信道的容量问题，但识别噪声的影响尚未完全表征。

Method: 1. 通过随机退化和数据处理不等式推导信道容量的反界；2. 基于多项采样过程的泊松化建立可达界，分析带符号间干扰的向量泊松信道；3. 改进Feinstein界中信息密度的集中不等式，显式表征识别噪声导致的互信息加性损失。

Result: 建立了噪声频率信道的容量上下界，显式量化了识别噪声导致的互信息损失，并将结果应用于DNA存储信道，量化了可靠存储比特总数缩放中的损失。

Conclusion: 该研究完整表征了噪声频率信道的容量特性，特别针对DNA数据存储中的短分子机制，为实际系统设计提供了理论指导。

Abstract: We investigate the capacity of noisy frequency-based channels, motivated by DNA data storage in the short-molecule regime, where information is encoded in the frequency of items types rather than their order. The channel output is a histogram formed by random sampling of items, followed by noisy item identification. While the capacity of the noiseless frequency-based channel has been previously addressed, the effect of identification noise has not been fully characterized. We present a converse bound on the channel capacity that follows from stochastic degradation and the data processing inequality. We then establish an achievable bound, which is based on a Poissonization of the multinomial sampling process, and an analysis of the resulting vector Poisson channel with inter-symbol interference. This analysis refines concentration inequalities for the information density used in Feinstein bound, and explicitly characterizes an additive loss in the mutual information due to identification noise. We apply our results to a DNA storage channel in the short-molecule regime, and quantify the resulting loss in the scaling of the total number of reliably stored bits.

</details>


### [74] [Convertible Codes for Data and Device Heterogeneity](https://arxiv.org/abs/2601.10341)
*Anina Gruica,Benjamin Jany,Stanislav Kruglik*

Main category: cs.IT

TL;DR: 该论文研究分布式存储系统中的可转换编码，通过Reed-Muller编码同时解决数据异构性和设备异构性问题，并建立了线性编码转换的读写成本下界。


<details>
  <summary>Details</summary>
Motivation: 分布式存储系统面临两大挑战：1) 数据异构性（非均匀访问需求）；2) 设备异构性（节点可靠性随时间变化）。现有方法通常单独处理这两个问题，缺乏同时解决两者的方案。

Method: 研究可转换编码，在合并机制下以最小成本实现编码转换。首先推导线性编码转换的读写成本一般下界，然后聚焦于Reed-Muller编码，构建显式转换程序，首次将两种异构性结合处理。

Result: 建立了适用于任意线性编码的读写成本下界理论框架，并针对Reed-Muller编码设计了具体的转换方案，首次实现了同时处理数据异构性和设备异构性的分布式存储编码转换。

Conclusion: 可转换编码为分布式存储系统提供了一种统一框架，能够同时应对数据异构性和设备异构性挑战。Reed-Muller编码的转换方案展示了这一框架的实际可行性，为异构存储系统设计提供了新思路。

Abstract: Distributed storage systems must handle both data heterogeneity, arising from non-uniform access demands, and device heterogeneity, caused by time-varying node reliability. In this paper, we study convertible codes, which enable the transformation of one code into another with minimum cost in the merge regime, addressing the latter. We derive general lower bounds on the read and write costs of linear code conversion, applicable to arbitrary linear codes. We then focus on Reed-Muller codes, which efficiently handle data heterogeneity, addressing the former issue, and construct explicit conversion procedures that, for the first time, combine both forms of heterogeneity for distributed data storage.

</details>


### [75] [A New Construction Structure on MISO Coded Caching with Linear Subpacketization: Half-Sum Disjoint Packing](https://arxiv.org/abs/2601.10353)
*Bowen Zheng,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出基于L-half-sum disjoint packing (HSDP)的MISO编码缓存方案，在子包化F=K（线性增长）下实现高和自由度与低子包化的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有MISO编码缓存方案在实现最大和自由度时，子包化随用户数指数增长，计算复杂度高。需要设计在低子包化（F=K）下仍能保持高性能的方案。

Method: 利用MAPDA框架，基于拉丁方将F=K的MAPDA设计转化为L-HSDP组合结构构造。1-HSDP对应NHSDP用于共享链路方案，扩展为L-HSDP用于MISO系统。

Result: 提出的L-HSDP方案显著降低了子包化（从指数级降到线性级），仅轻微牺牲和自由度，在子包化和性能间取得更好平衡。

Conclusion: L-HSDP方案在MISO编码缓存中实现了子包化与和自由度的有效权衡，相比现有方案在相同子包化下获得更高性能，在相同性能下大幅降低复杂度。

Abstract: In the $(L,K,M,N)$ cache-aided multiple-input single-output (MISO) broadcast channel (BC) system, the server is equipped with $L$ antennas and communicates with $K$ single-antenna users through a wireless broadcast channel where the server has a library containing $N$ files, and each user is equipped with a cache of size $M$ files. Under the constraints of uncoded placement and one-shot linear delivery strategies, many schemes achieve the maximum sum Degree-of-Freedom (sum-DoF). However, for general parameters $L$, $M$, and $N$, their subpacketizations increase exponentially with the number of users. We aim to design a MISO coded caching scheme that achieves a large sum-DoF with low subpacketization $F$. An interesting combinatorial structure, called the multiple-antenna placement delivery array (MAPDA), can be used to generate MISO coded caching schemes under these two strategies; moreover, all existing schemes with these strategies can be represented by the corresponding MAPDAs. In this paper, we study the case with $F=K$ (i.e., $F$ grows linearly with $K$) by investigating MAPDAs. Specifically, based on the framework of Latin squares, we transform the design of MAPDA with $F=K$ into the construction of a combinatorial structure called the $L$-half-sum disjoint packing (HSDP). It is worth noting that a $1$-HSDP is exactly the concept of NHSDP, which is used to generate the shared-link coded caching scheme with $F=K$. By constructing $L$-HSDPs, we obtain a class of new schemes with $F=K$. Finally, theoretical and numerical analyses show that our $L$-HSDP schemes significantly reduce subpacketization compared to existing schemes with exponential subpacketization, while only slightly sacrificing sum-DoF, and achieve both a higher sum-DoF and lower subpacketization than the existing schemes with linear subpacketization.

</details>


### [76] [Generalized Weight Structure of Polar Codes: Selected Template Polynomials](https://arxiv.org/abs/2601.10362)
*Mohammad Rowshan,Vlad-Florin Dragoi*

Main category: cs.IT

TL;DR: 本文开发了一个代数框架，用于计算极化码中由单项式生成的码字的汉明重量，推导出关键结构模板的闭式表达式，并结合LTA群作用得到显式的重数公式。


<details>
  <summary>Details</summary>
Motivation: 极化码可视为递减单项式码，具有丰富的代数结构，受下三角仿射(LTA)群支配。需要开发一个通用框架来计算由单项式生成的码字的汉明重量，并系统地表征低重量和中重量谱。

Method: 开发了一个通用框架：1) 计算由单项式生成的码字的汉明重量；2) 将这些重量表示为规范的二元形式；3) 推导出关键结构模板（不相交和、嵌套块、互补翻转）的闭式表达式；4) 结合LTA群作用获得显式的重数公式。

Result: 获得了：1) 码字汉明重量的规范二元表示；2) 生成低重量和中重量谱的关键结构模板的闭式表达式；3) 统一的代数方法来表征和枚举码字；4) 显式的重数公式。

Conclusion: 本文提出了一个统一的代数框架，通过计算单项式生成码字的汉明重量、推导结构模板的闭式表达式，并结合LTA群作用，为极化码的重量谱分析提供了系统的方法和显式公式。

Abstract: Polar codes can be viewed as decreasing monomial codes, revealing a rich algebraic structure governed by the lower-triangular affine (LTA) group. We develop a general framework to compute the Hamming weight of codewords generated by sums of monomials, express these weights in a canonical dyadic form, and derive closed expressions for key structural templates (disjoint sums, nested blocks, complementary flips) that generate the low and intermediate weight spectrum. Combining these templates with the LTA group action, we obtain explicit multiplicity formulas, yielding a unified algebraic method to characterize and enumerate codewords.

</details>


### [77] [A Hybrid Reliability--Weight Framework for Construction of Polar Codes](https://arxiv.org/abs/2601.10376)
*Mohammad Rowshan,Vlad-Florin Dragoi*

Main category: cs.IT

TL;DR: 提出了一种结合可靠性和权重的混合比特信道排序方法，用于构造Polar码，在短中长度下改善最小距离性能


<details>
  <summary>Details</summary>
Motivation: 传统Polar码基于可靠性排序构造，能保证达到信道容量，但在短中长度下可能产生较差的最小权重谱。需要结合代数分析结果来改善码的最小距离特性

Method: 定义混合比特信道排序，结合可靠性（Bhattacharyya因子）和权重贡献（最小权重码字轨道枚举）。通过最小化截断的SC/ML联合界代理来构造码，属于递减单项式码类

Result: 混合构造在短中长度下能权衡纯可靠性构造和最小距离特性，数值结果显示在BPSK-AWGN信道上的性能改进。证明混合设计是可靠性构造的局部扰动，渐近影响随码长增加而消失

Conclusion: 混合比特信道排序方法有效平衡了可靠性和最小距离特性，特别适用于短中长度的Polar码设计，为实际应用提供了更好的性能权衡

Abstract: Polar codes are usually constructed by ranking synthetic bit-channels according to reliability, which guarantees capacity-achieving behavior but can yield poor low-weight spectra at short and moderate lengths. Recent algebraic results express the contribution of individual bit-channels to the multiplicities of minimum and near-minimum weight codewords in closed form. In this work we combine these insights into a mixed (reliability--weight) bit-channel ordering. We define a per-bit cost whose distance term is derived from orbit enumeration of minimum-weight codewords and scaled by a Bhattacharyya-type factor, and show that the resulting mixed construction minimises a truncated SC/ML union-bound surrogate within a class of decreasing monomial codes. We relate the mixed metric to error events in SCL decoding via a pruning/ML decomposition, and prove that mixed designs act as local perturbations of reliability-based constructions whose asymptotic impact vanishes as code-length approaches infinity. Numerical results for short and moderate lengths on BPSK-AWGN, implemented via Gaussian approximation and closed-form weight contributions, illustrate the trade-off between pure reliability-based and mixed constructions in terms of minimum distance, multiplicity, and union-bound approximations. All proofs are deferred to the appendices.

</details>


### [78] [Codebook Design for Limited Feedback in Near-Field XL-MIMO Systems](https://arxiv.org/abs/2601.10391)
*Liujia Yao,Changsheng You,Zixuan Huang,Chao Zhou,Zhaohui Yang,Xiaoyang Li*

Main category: cs.IT

TL;DR: 提出针对XL-MIMO FDD系统的用户分布自适应码本设计，通过联合优化角度-距离采样和比特分配来最大化和速率，相比现有方案显著降低反馈开销。


<details>
  <summary>Details</summary>
Motivation: 现有XL-MIMO码本设计（如极域码本）未充分考虑实际用户分布，导致反馈开销过大。需要设计能适应不同用户分布的高效反馈码本。

Method: 1. 针对均匀分布场景，建立和速率最大化问题；2. 使用Voronoi划分证明均匀角度采样最优；3. 推导接收功率下界，提出几何距离采样作为高质量次优解；4. 扩展至非均匀分布，采用交替采样方法；5. 理论分析比特分配策略。

Result: 理论分析表明：随着阵列规模增大，最优比特分配逐渐偏向距离采样而非角度采样。数值结果验证了所提码本在各种系统设置下的优越速率性能和鲁棒性，相比基准方案（包括广泛使用的极域码本）获得显著增益。

Conclusion: 提出的用户分布自适应码本设计能有效降低XL-MIMO FDD系统的反馈开销，通过优化角度-距离采样和比特分配策略，在不同用户分布场景下均表现出优越性能。

Abstract: In this paper, we study efficient codebook design for limited feedback in extremely large-scale multiple-input-multiple-output (XL-MIMO) frequency division duplexing (FDD) systems. It is worth noting that existing codebook designs for XL-MIMO, such as polar-domain codebook, have not well taken into account user (location) distribution in practice, thereby incurring excessive feedback overhead. To address this issue, we propose in this paper a novel and efficient feedback codebook tailored to user distribution. To this end, we first consider a typical scenario where users are uniformly distributed within a specific polar-region, based on which a sum-rate maximization problem is formulated to jointly optimize angle-range samples and bit allocation among angle/range feedback. This problem is challenging to solve due to the lack of a closed-form expression for the received power in terms of angle and range samples. By leveraging a Voronoi partitioning approach, we show that uniform angle sampling is optimal for received power maximization. For more challenging range sampling design, we obtain a tight lower-bound on the received power and show that geometric sampling, where the ratio between adjacent samples is constant, can maximize the lower bound and thus serves as a high-quality suboptimal solution. We then extend the proposed framework to accommodate more general non-uniform user distribution via an alternating sampling method. Furthermore, theoretical analysis reveals that as the array size increases, the optimal allocation of feedback bits increasingly favors range samples at the expense of angle samples. Finally, numerical results validate the superior rate performance and robustness of the proposed codebook design under various system setups, achieving significant gains over benchmark schemes, including the widely used polar-domain codebook.

</details>


### [79] [Multiaccess Coded Caching with Heterogeneous Retrieval Costs](https://arxiv.org/abs/2601.10394)
*Wenbo Huang,Minquan Cheng,Kai Wan,Xiaojun Li,Robert Caiming Qiu,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出基于叠加编码的成本感知多址编码缓存框架，通过优化缓存放置最小化系统总成本（缓存访问成本+广播成本），在异构检索成本场景中优于现有方案


<details>
  <summary>Details</summary>
Motivation: 现有MACC系统假设用户从连接的缓存节点检索内容无通信成本，但实际中用户从不同缓存节点检索内容成本不同，服务器向用户传输内容也有成本，需要成本感知的优化方案

Method: 提出基于叠加编码的新型编码缓存框架，将Cheng等人的MACC方案分层；推导成本感知优化问题优化缓存放置；利用最优解的稀疏性提出复杂度降低的结构感知算法

Result: 仿真结果表明，在异构检索成本场景下，所提方案始终优于Cheng等人的方案

Conclusion: 通过成本感知的编码缓存框架和优化算法，能有效降低MACC系统总成本，特别是在实际存在异构检索成本的环境中

Abstract: The multiaccess coded caching (MACC) system, as formulated by Hachem {\it et al.}, consists of a central server with a library of $N$ files, connected to $K$ cache-less users via an error-free shared link, and $K$ cache nodes, each equipped with cache memory of size $M$ files. Each user can access $L$ neighboring cache nodes under a cyclic wrap-around topology. Most existing studies operate under the strong assumption that users can retrieve content from their connected cache nodes at no communication cost. In practice, each user retrieves content from its $L$ different connected cache nodes at varying costs. Additionally, the server also incurs certain costs to transmit the content to the users. In this paper, we focus on a cost-aware MACC system and aim to minimize the total system cost, which includes cache-access costs and broadcast costs. Firstly, we propose a novel coded caching framework based on superposition coding, where the MACC schemes of Cheng \textit{et al.} are layered. Then, a cost-aware optimization problem is derived that optimizes cache placement and minimizes system cost. By identifying a sparsity property of the optimal solution, we propose a structure-aware algorithm with reduced complexity. Simulation results demonstrate that our proposed scheme consistently outperforms the scheme of Cheng {\it et al.} in scenarios with heterogeneous retrieval costs.

</details>


### [80] [Placement Delivery Array for Cache-Aided MIMO Systems](https://arxiv.org/abs/2601.10422)
*Yifei Huang,Kai Wan,Minquan Cheng,Jinyan Wang,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出MIMO-PDA统一结构，实现最大和自由度与低子分组化的缓存辅助MIMO网络编码缓存方案


<details>
  <summary>Details</summary>
Motivation: 现有缓存辅助MIMO网络方案在实现最大和自由度时面临子分组化指数增长问题，需要设计同时实现最大和自由度与低子分组化的方案

Method: 引入MIMO-PDA统一组合结构描述非编码放置和单次迫零传输，基于此推导和自由度上界，提出两种MIMO-PDA构造方法

Result: 推导出和自由度上界min{KG, Gt+G⌈L/G⌉}，两种构造均达到最大和自由度，第一种在严格约束下实现线性子分组化，第二种在宽松约束下实现有序指数子分组化

Conclusion: MIMO-PDA框架有效平衡和自由度与子分组化，第二种构造在保持最大和自由度下指数级降低子分组化，优于现有方案

Abstract: We consider a $(G,L,K,M,N)$ cache-aided multiple-input multiple-output (MIMO) network, where a server equipped with $L$ antennas and a library of $N$ equal-size files communicates with $K$ users, each equipped with $G$ antennas and a cache of size $M$ files, over a wireless interference channel. Each user requests an arbitrary file from the library. The goal is to design coded caching schemes that simultaneously achieve the maximum sum degrees of freedom (sum-DoF) and low subpacketization. In this paper, we first introduce a unified combinatorial structure, termed the MIMO placement delivery array (MIMO-PDA), which characterizes uncoded placement and one-shot zero-forcing delivery. By analyzing the combinatorial properties of MIMO-PDAs, we derive a sum-DoF upper bound of $\min\{KG, Gt+G\lceil L/G \rceil\}$, where $t=KM/N$, which coincides with the optimal DoF characterization in prior work by Tehrani \emph{et al.}. Based on this upper bound, we present two novel constructions of MIMO-PDAs that achieve the maximum sum-DoF. The first construction achieves linear subpacketization under stringent parameter constraints, while the second achieves ordered exponential subpacketization under substantially milder constraints. Theoretical analysis and numerical comparisons demonstrate that the second construction exponentially reduces subpacketization compared to existing schemes while preserving the maximum sum-DoF.

</details>


### [81] [Energy-Efficient Probabilistic Semantic Communication Over Visible Light Networks With Rate Splitting](https://arxiv.org/abs/2601.10452)
*Zhouxiang Zhao,Zhaohui Yang,Mingzhe Chen,Chen Zhu,Xin Tong,Zhaoyang Zhang*

Main category: cs.IT

TL;DR: 该论文研究了资源受限的可见光通信概率语义通信系统中的能效最大化问题，通过联合优化传输波束成形、直流偏置、公共速率分配和语义压缩比，并开发了基于SCA和Dinkelbach方法的交替优化算法。


<details>
  <summary>Details</summary>
Motivation: 可见光通信作为未来无线通信的关键技术，具有优于传统射频系统的物理层优势，但其与语义通信等高层次技术的结合仍未被充分探索。在资源受限的VLC概率语义通信系统中，需要解决能效最大化问题。

Method: 采用LED发射器进行语义压缩以减少数据大小，使用概率图表示知识库，采用速率分割多址接入技术同时传输知识和信息数据。开发了基于逐次凸逼近和Dinkelbach方法的交替优化算法，联合优化传输波束成形、直流偏置、公共速率分配和语义压缩比。

Result: 仿真结果证明了所提方法的有效性，能够有效解决VLC概率语义通信系统中的能效最大化问题。

Conclusion: 该研究为可见光通信与语义通信的融合提供了有效的能效优化解决方案，通过联合优化通信和计算成本，实现了资源受限环境下的高效概率语义通信。

Abstract: Visible light communication (VLC) is emerging as a key technology for future wireless communication systems due to its unique physical-layer advantages over traditional radio-frequency (RF)-based systems. However, its integration with higher-layer techniques, such as semantic communication, remains underexplored. This paper investigates the energy efficiency maximization problem in a resource-constrained VLC-based probabilistic semantic communication (PSCom) system. In the considered model, light-emitting diode (LED) transmitters perform semantic compression to reduce data size, which incurs additional computation overhead. The compressed semantic information is transmitted to the users for semantic inference using a shared knowledge base that requires periodic updates to ensure synchronization. In the PSCom system, the knowledge base is represented by probabilistic graphs. To enable simultaneous transmission of both knowledge and information data, rate splitting multiple access (RSMA) is employed. The optimization problem focuses on maximizing energy efficiency by jointly optimizing transmit beamforming, direct current (DC) bias, common rate allocation, and semantic compression ratio, while accounting for both communication and computation costs. To solve this problem, an alternating optimization algorithm based on successive convex approximation (SCA) and Dinkelbach method is developed. Simulation results demonstrate the effectiveness of the proposed approach.

</details>


### [82] [Joint Source-Channel Coding for ISAC: Distortion Tradeoffs and Separation Theorems](https://arxiv.org/abs/2601.10470)
*Gefei Peng,Youlong Wu*

Main category: cs.IT

TL;DR: 本文为ISAC系统建立了JSCC框架，证明了分离源信道编码可实现联合最优性，并分析了信道容量、通信失真、感知失真和估计成本之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信(ISAC)系统能够同时实现高效通信和环境感知，但需要表征感知与通信之间的性能权衡关系。本文旨在从信息论角度建立这种权衡关系的理论基础。

Method: 采用联合源信道编码(JSCC)框架，包含带有信道状态估计器和联合源信道编码器的发射机、状态相关无记忆信道、以及带有联合源信道解码器的接收机。通过信息论分析建立权衡关系，并证明分离源信道编码可实现联合最优性。

Result: 建立了信道容量、通信失真、感知失真和估计成本之间的权衡关系，证明了分离源信道编码在此设置下可实现联合最优性。通过二进制设置的示例验证了理论结果。

Conclusion: 本文为ISAC系统提供了信息论基础的JSCC框架，明确了感知与通信性能之间的基本权衡关系，证明了分离编码的可行性，为实际ISAC系统设计提供了理论指导。

Abstract: Integrated Sensing and Communication (ISAC) systems have garnered significant attention due to their capability to simultaneously achieve efficient communication and environmental sensing. A core objective in this field is characterizing the performance tradeoff between sensing and communication. In this paper, we consider a joint source-channel coding (JSCC) framework for the ISAC system that consists of a transmitter with a channel state estimator and a joint source-channel encoder, a state-dependent memoryless channel, and a receiver with a joint source-channel decoder. From an information-theoretic perspective, we establish the tradeoff relationships among channel capacity, distortions in both communication and sensing processes, and the estimation cost. We prove that the separate source and channel coding can achieve joint optimality in this setting. An illustrative example of a binary setting is also provided to validate our theoretical results.

</details>


### [83] [A Construction Framework of Coded Caching Scheme for Multi-Access MIMO Systems via Knapsack Problem](https://arxiv.org/abs/2601.10484)
*Siying Luo,Youlong Wu,Mingming Zhang,Minquan Cheng,Dianhua Wu*

Main category: cs.IT

TL;DR: 提出一种基于多天线放置交付阵列(MAPDA)的编码缓存方案，用于组合拓扑多接入MISO网络，通过0-1背包问题优化设计，同时实现高和自由度与低子分组复杂度。


<details>
  <summary>Details</summary>
Motivation: 组合拓扑多接入MISO网络中的编码缓存问题需要同时实现高和自由度与低子分组复杂度，现有方案难以平衡这两个目标。

Method: 将多天线放置交付阵列(MAPDA)设计转化为0-1背包问题，优化和自由度，将复杂组合缓存结构转化为可处理的优化框架，生成高效缓存放置和灵活交付策略。

Result: 在组合拓扑网络中，所提方案比现有方案获得更高和自由度；在相同缓存大小约束下，子分组水平与现有线性子分组方案相当；特定条件下达到理论最大和自由度min{L+KM/N, K}并进一步降低子分组；针对特定组合结构，获得优化构造实现更高和自由度和更低子分组。

Conclusion: 提出的MAPDA框架有效解决了组合拓扑多接入MISO网络的编码缓存问题，在保证低子分组复杂度的同时实现了高和自由度，为复杂缓存网络设计提供了系统化方法。

Abstract: This paper investigates the coded caching problem in a multi-access multiple-input single-output (MAMISO) network with the combinatorial topology. The considered system consists of a server containing $N$ files, $Λ$ cache nodes, and $K$ cache-less users, where each user can access a unique subset of $r$ cache nodes. The server is equipped with $L$ transmit antennas. Our objective is to design a caching scheme that simultaneously achieves a high sum Degree of Freedom (sum-DoF) and low subpacketization complexity. To address this challenge, we formulate the design of multi-antenna placement delivery arrays (MAPDA) as a $0$--$1$ knapsack problem to maximize the achievable DoF, thereby transforming the complex combinatorial caching structure into a tractable optimization framework that yields efficient cache placement and flexible delivery strategies. Theoretical and numerical analyses demonstrate that: for networks with combinatorial topologies, the proposed scheme achieves a higher sum-DoF than existing schemes. Under identical cache size constraints, the subpacketization level remains comparable to existing linear subpacketization schemes. Moreover, under specific system conditions, the proposed scheme attains the theoretical maximum sum-DoF of $\min\{L+KM/N, K\}$ while achieving further reductions subpacketization. For particular combinatorial structures, we further derive optimized constructions that achieve even higher sum-DoF with lower subpacketization. ```

</details>


### [84] [Coded Caching for Combinatorial Multi-Access Hotplug Networks from $t$-Designs](https://arxiv.org/abs/2601.10503)
*Dhruv Pratap Singh,Anjana A. Mahesh,B. Sundar Rajan*

Main category: cs.IT

TL;DR: 提出了一种基于t设计的组合多接入网络热插拔编码缓存方案，扩展了现有的热插拔编码缓存模型，允许用户访问多个缓存，并在传输阶段只有部分缓存在线。


<details>
  <summary>Details</summary>
Motivation: 现有热插拔编码缓存模型无法处理用户访问多个缓存且传输阶段只有部分缓存在线的情况，需要扩展模型以支持组合多接入网络。

Method: 将热插拔放置传输阵列(HpPDA)框架扩展到组合多接入设置，提出基于t设计的编码缓存方案，通过适当的参数选择消除冗余多播传输。

Result: 方案实现了灵活子分组化的一系列速率-内存权衡，数值比较显示在某些内存区域优于现有热插拔编码缓存方案。

Conclusion: 提出的t设计方案为组合多接入网络中的热插拔编码缓存提供了有效的解决方案，在特定内存区域具有性能优势。

Abstract: We study hotplug coded caching in combinatorial multi-access networks, which generalizes existing hotplug coded caching models by allowing users to access multiple caches, while only a subset of caches is online during the delivery phase. We first generalize the Hotplug Placement Delivery Array (HpPDA) framework to the combinatorial multi-access setting. Based on this generalized framework, we propose a t-design-based coded caching scheme for combinatorial multi-access networks. We characterize a class of design parameters under which every active user has access to a sufficient number of coded subfiles to decode its requested file, and show that appropriate parameter choices allow for the elimination of redundant multicast transmissions. As a result, the proposed scheme achieves a family of rate memory trade offs with flexible subpacketization. We present numerical comparisons illustrating that the proposed t-scheme outperforms existing hotplug coded caching schemes in certain memory regimes.

</details>


### [85] [A New Construction Structure on Coded Caching with Linear Subpacketization: Non-Half-Sum Latin Rectangle](https://arxiv.org/abs/2601.10505)
*Yongcheng Yang,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出基于非半和拉丁矩形(NHSLR)的编码缓存方案，实现线性可扩展的子分组化(F=O(K))，在降低传输负载的同时保持低子分组化水平。


<details>
  <summary>Details</summary>
Motivation: 现有编码缓存方案在子分组化和传输负载之间存在权衡：指数或多项式子分组化方案负载低但复杂度高，线性子分组化方案复杂度低但传输负载过高。需要设计同时实现低子分组化和低传输负载的方案。

Method: 提出新的组合结构——非半和拉丁矩形(NHSLR)，扩展了NHSDP框架，将子分组化从F=K扩展到F=O(K)。通过构造NHSLR获得新的编码缓存方案类。

Result: 新方案实现了线性可扩展的子分组化，同时进一步降低了传输负载（相比NHSDP方案）。理论分析和数值结果表明，新方案不仅比现有线性子分组化方案传输负载更低，而且接近某些指数子分组化方案的性能。

Conclusion: NHSLR结构为编码缓存方案设计提供了新框架，成功解决了低子分组化和低传输负载之间的权衡问题，在保持线性子分组化的同时显著改善了性能。

Abstract: Coded caching is recognized as an effective method for alleviating network congestion during peak periods by leveraging local caching and coded multicasting gains. The key challenge in designing coded caching schemes lies in simultaneously achieving low subpacketization and low transmission load. Most existing schemes require exponential or polynomial subpacketization levels, while some linear subpacketization schemes often result in excessive transmission load. Recently, Cheng et al. proposed a construction framework for linear coded caching schemes called Non-Half-Sum Disjoint Packing (NHSDP), where the subpacketization equals the number of users $K$. This paper introduces a novel combinatorial structure, termed the Non-Half-Sum Latin Rectangle (NHSLR), which extends the framework of linear coded caching schemes from $F=K$ (i.e., the construction via NHSDP) to a broader scenario with $F=\mathcal{O}(K)$. By constructing NHSLR, we have obtained a new class of coded caching schemes that achieves linearly scalable subpacketization, while further reducing the transmission load compared with the NHSDP scheme. Theoretical and numerical analyses demonstrate that the proposed schemes not only achieves lower transmission load than existing linear subpacketization schemes but also approaches the performance of certain exponential subpacketization schemes.

</details>


### [86] [A New Construction Structure on Multi-access Coded Caching with Linear Subpacketization: Cyclic Multi-Access Non-Half-Sum Disjoint Packing](https://arxiv.org/abs/2601.10510)
*Mengyuan Li,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: 提出了一种基于CMA-NHSDP组合结构的多接入编码缓存方案，在保持线性子分组化(F=K)的同时降低了传输负载


<details>
  <summary>Details</summary>
Motivation: 现有多接入编码缓存方案存在矛盾：传输性能好的方案子分组化指数增长，而线性/多项式子分组化的方案传输负载较高。需要设计在保持线性子分组化的同时降低传输负载的方案。

Method: 将NHSDP（非半和不相交包装）结构扩展到多接入系统，提出CMA-NHSDP（循环多接入非半和不相交包装）组合结构，并基于此构造新的多接入编码缓存方案。

Result: 理论分析和数值结果表明，新方案在保持线性子分组化(F=K)的同时，传输负载低于现有线性子分组化方案，在某些情况下甚至优于指数子分组化方案。

Conclusion: CMA-NHSDP结构成功解决了多接入编码缓存中线性子分组化与低传输负载之间的矛盾，为实际系统部署提供了有前景的解决方案。

Abstract: We consider the $(K,L,M,N)$ multi-access coded caching system introduced by Hachem et al., which consists of a central server with $N$ files and $K$ cache nodes, each of memory size $M$, where each user can access $L$ cache nodes in a cyclic wrap-around fashion. At present, several existing schemes achieve competitive transmission performance, but their subpacketization levels grow exponentially with the number of users. In contrast, schemes with linear or polynomial subpacketization always incur higher transmission loads. We aim to design a multi-access coded caching scheme with linear subpacketization $F$ while maintaining low transmission load. Recently, Cheng et al. proposed a construction framework for coded caching schemes with linear subpacketization (i.e., $F=K$) called non-half-sum disjoint packing (NHSDP). Inspired by this structure, we introduce a novel combinatorial structure named cyclic multi-access non-half-sum disjoint packing (CMA-NHSDP) by extending NHSDP to MACC system. By constructing CMA-NHSDP, we obtain a new class of multi-access coded caching schemes. Theoretical and numerical analyses show that our scheme achieves lower transmission loads than some existing schemes with linear subpacketization. Moreover, the proposed schemes achieves lower transmission load compared to existing schemes with exponential subpacketization in some case.

</details>


### [87] [On the suboptimality of linear codes for binary distributed hypothesis testing](https://arxiv.org/abs/2601.10526)
*Adway Girish,Robinson D. H. Cung,Emre Telatar*

Main category: cs.IT

TL;DR: 研究二进制分布式假设检验问题，两个代理观察相关二进制向量，以相同速率向中央决策者发送压缩信息。分析线性压缩方案，证明截断在某些情况下是最佳线性方案，但总体上线性编码可能不是最优的。


<details>
  <summary>Details</summary>
Motivation: 研究分布式假设检验中的压缩方案优化问题，特别是在二进制相关向量场景下，探索线性压缩方案的最优性，为实际系统中的信息压缩提供理论指导。

Method: 采用线性压缩方案分析，特别研究截断方案。在两种特定情况下证明截断是最佳线性方案：1）测试相同幅度但符号相反的相关系数；2）测试独立性或非独立性。通过数值证据支持更广泛的猜想。

Result: 证明截断在两种情况下是最佳线性方案：测试相反符号的相同幅度相关性，以及测试独立性/非独立性。数值证据支持截断可能是测试任何相反符号相关性的最佳线性编码。同时发现对于测试独立性，截断（及任何线性编码）严格劣于经典随机编码。

Conclusion: 线性压缩方案在某些特定假设检验场景中可以达到最优，特别是截断方案。然而，对于测试独立性等任务，线性编码存在固有局限性，随机编码能获得更好的性能。这为分布式假设检验系统的设计提供了重要指导。

Abstract: We study a binary distributed hypothesis testing problem where two agents observe correlated binary vectors and communicate compressed information at the same rate to a central decision maker. In particular, we study linear compression schemes and show that simple truncation is the best linear scheme in two cases: (1) testing opposite signs of the same magnitude of correlation, and (2) testing for or against independence. We conjecture, supported by numerical evidence, that truncation is the best linear code for testing any correlations of opposite signs. Further, for testing against independence, we also compute classical random coding exponents and show that truncation, and consequently any linear code, is strictly suboptimal.

</details>


### [88] [Network Integrated Sensing and Communication](https://arxiv.org/abs/2601.10538)
*Edward Andrews,Lawrence Ong,Duy T. Ngo,Yao Liu,Min Li*

Main category: cs.IT

TL;DR: 本文研究了网络级ISAC系统，分析了中继网络中通信路由与感知覆盖之间的基本权衡关系，为6G异构网络设计提供关键见解。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC研究主要集中在链路级设计，但面向大规模部署需要理解网络级性能。本文旨在研究网络ISAC模型中通信路由与感知覆盖之间的基本权衡关系。

Method: 提出一个新颖的优化框架，捕捉多节点路由和感知覆盖之间的相互作用。针对一维路径网络提供完整的感知-吞吐量区域解析表征，并扩展到一般网络拓扑，建立感知-吞吐量Pareto边界的分段线性特性。

Result: 对于一维路径网络，获得了完整的感知-吞吐量区域解析表征；对于一般网络拓扑，证明了感知-吞吐量Pareto边界是分段线性的，并为每个分段提供了物理解释。

Conclusion: 研究揭示了感知覆盖与通信路由之间的基本权衡关系，为未来6G异构网络的设计提供了关键见解，支持网络级ISAC系统的优化部署。

Abstract: Integrated sensing and communication (ISAC) is a cornerstone technology for 6G networks, offering unified support for high-rate communication and high-accuracy sensing. While existing literature extensively covers link-level designs, the transition toward large-scale deployment necessitates a fundamental understanding of network-level performance. This paper investigates a network ISAC model where a source node communicates with a destination via a relay network, while intermediate nodes concurrently perform cooperative sensing over specific spatial regions. We formulate a novel optimization framework that captures the interplay between multi-node routing and sensing coverage. For a one-dimensional path network, we provide an analytical characterization of the complete sensing-throughput region. Extending this to general network topologies, we establish that the sensing-throughput Pareto boundary is piecewise linear and provide physical interpretations for each segment. Our results reveal the fundamental trade-offs between sensing coverage and communication routing, offering key insights for the design of future 6G heterogeneous networks.

</details>


### [89] [Error-Correcting Codes for Two Bursts of t1-Deletion-t2-Insertion with Low Computational Complexity](https://arxiv.org/abs/2601.10540)
*Yajuan Liu,Tolga M. Duman*

Main category: cs.IT

TL;DR: 该论文研究了能纠正多个(t₁,t₂)-DI错误爆发的纠错码构造，建立了不同错误类型间的等价关系，推导了码率界限，并提出了低复杂度构造方法。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储和文档同步等实际应用中会出现同时包含插入、删除和替换的突发错误，需要开发能纠正此类错误的信道编码。

Method: 1) 建立两种(t₁,t₂)-DI错误爆发与(t₂,t₁)-DI错误爆发之间的等价关系；2) 推导两个(t₁,t₂)-DI错误爆发纠错码的码率上下界；3) 提出两个(t₁,t₂)-DI错误爆发纠错码的具体构造方法。

Result: 证明了不同错误类型间的等价性，得到了码率界限（可扩展到多个错误爆发），提出的构造方法相比基于综合征压缩的技术显著降低了计算复杂度。

Conclusion: 该研究为纠正多个(t₁,t₂)-DI错误爆发的纠错码提供了理论基础和实用构造，在DNA数据存储和文档同步等应用中具有重要价值。

Abstract: Burst errors involving simultaneous insertions, deletions, and substitutions occur in practical scenarios, including DNA data storage and document synchronization, motivating developments of channel codes that can correct such errors. In this paper, we address the problem of constructing error-correcting codes (ECCs) capable of handling multiple bursts of $t_1$-deletion-$t_2$-insertion ($(t_1,t_2)$-DI) errors, where each burst consists of $t_1$ deletions followed by $t_2$ insertions in a binary sequence. We make three key contributions: Firstly, we establish the fundamental equivalence of (1) two bursts of $(t_1,t_2)$-DI ECCs, (2) two bursts of $(t_2,t_1)$-DI ECCs, and (3) one burst each of $(t_1,t_2)$-DI and $(t_2,t_1)$-DI ECCs. Then, we derive lower and upper bounds on the code size of two bursts of $(t_1,t_2)$-DI ECCs, which can naturally be extended to the case of multiple bursts. Finally, we present constructions of two bursts of $(t_1,t_2)$-DI ECCs. Compared to the codes obtained by the syndrome compression technique, the resulting codes achieve significantly lower computational complexity.

</details>


### [90] [Sparse Signal Recovery from Random Measurements](https://arxiv.org/abs/2601.10569)
*Siu-Wing Cheng,Man Ting Wong*

Main category: cs.IT

TL;DR: 提出一种无需优化或解线性系统的压缩感知信号重构方法，仅需对数级随机测量矩阵，时间复杂度为O(kn log n)


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知方法需要解决优化问题或线性系统，计算复杂度高。本文旨在开发一种更简单、更高效的重构方法，避免复杂的优化过程。

Method: 使用Θ(log n)个随机测量矩阵，每个维度为k×n，其中k = Θ(s log n)。通过简单的算法直接确定信号，无需优化求解。还扩展方法用于确定信号的支撑集。

Result: 方法在O(kn log n)时间内重构信号，实验表明在二进制信号上能与基于优化的方法相媲美。

Conclusion: 提出了一种简单高效的压缩感知重构方法，避免了传统优化问题的复杂性，为稀疏信号恢复提供了新思路。

Abstract: Given the compressed sensing measurements of an unknown vector $z \in \mathbb{R}^n$ using random matrices, we present a simple method to determine $z$ without solving any optimization problem or linear system. Our method uses $Θ(\log n)$ random sensing matrices in $\mathbb{R}^{k \times n}$ and runs in $O(kn\log n)$ time, where $k = Θ(s\log n)$ and $s$ is the number of nonzero coordinates in $z$. We adapt our method to determine the support set of $z$ and experimentally compare with some optimization-based methods on binary signals.

</details>


### [91] [Fundamental Limits of Multi-User Distributed Computing of Linearly Separable Functions](https://arxiv.org/abs/2601.10603)
*K. K. Krishnan Namboodiri,Elizabath Peter,Derya Malak,Petros Elia*

Main category: cs.IT

TL;DR: 该论文研究了多用户分布式计算线性可分函数的基本极限，提出了在通信与计算权衡下的最优分布式计算方案。


<details>
  <summary>Details</summary>
Motivation: 分布式计算中线性可分函数的计算存在通信与计算之间的基本权衡：每个服务器计算能力有限（最多计算M个子函数），通信能力有限（最多向Δ个用户传输）。需要设计能降低通信成本的分布式计算方案。

Method: 提出联合设计任务分配和传输的分布式计算方案，针对任意给定的K、L、M、Δ参数，在实数域和有限域中分别使用不同的逆定理证明方案的最优性。

Result: 提出的分布式计算方案在各种条件下实现了实数域中的最优性能，并在有限域中通过计数论证的逆定理表征了方案性能。

Conclusion: 该工作建立了多用户分布式计算线性可分函数的基本极限，提出的方案在通信与计算权衡下实现了最优性能，为分布式计算系统设计提供了理论基础。

Abstract: This work establishes the fundamental limits of the classical problem of multi-user distributed computing of linearly separable functions. In particular, we consider a distributed computing setting involving $L$ users, each requesting a linearly separable function over $K$ basis subfunctions from a master node, who is assisted by $N$ distributed servers. At the core of this problem lies a fundamental tradeoff between communication and computation: each server can compute up to $M$ subfunctions, and each server can communicate linear combinations of their locally computed subfunctions outputs to at most $Δ$ users. The objective is to design a distributed computing scheme that reduces the communication cost (total amount of data from servers to users), and towards this, for any given $K$, $L$, $M$, and $Δ$, we propose a distributed computing scheme that jointly designs the task assignment and transmissions, and shows that the scheme achieves optimal performance in the real field under various conditions using a novel converse. We also characterize the performance of the scheme in the finite field using another converse based on counting arguments.

</details>


### [92] [Basis-Spline Assisted Coded Computing: Strategies and Error Bounds](https://arxiv.org/abs/2601.10616)
*Rimpi Borah,J. Harshan,V. Lalitha*

Main category: cs.IT

TL;DR: 提出基于三次B样条插值的编码计算框架，用于处理非多项式函数的分布式计算，相比现有Berrut方法在存在大量慢节点时具有更好的精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有Berrut近似编码计算方法在处理非多项式函数时，由于Berrut插值具有全局支撑特性，当慢节点数量较多时精度会显著下降。需要一种更稳定、更精确的方法来处理分布式计算中的非多项式函数。

Method: 提出基于三次B样条插值的编码计算框架。利用B样条的局部支撑和平滑特性，在主机节点重构服务器端的函数评估值。提供了将B样条插值集成到编码计算中的系统方法，并推导了近似误差的理论界限。

Result: 理论分析表明，该方法在服务器数量和慢节点数量方面具有明确的近似误差界限。比较分析显示，对于各种非多项式函数，该框架显著优于基于Berrut的方法。

Conclusion: 基于三次B样条插值的编码计算框架能够有效处理非多项式函数的分布式计算问题，相比现有方法在存在大量慢节点时具有更好的精度和稳定性，为编码计算领域提供了新的解决方案。

Abstract: Coded computing has become a key framework for reliable distributed computation over decentralized networks, effectively mitigating the impact of stragglers. Although there exists a wide range of coded computing methods to handle both polynomial and non-polynomial functions, computing methods for the latter class have received traction due its inherent challenges in reconstructing non-polynomial functions using a finite number of evaluations. Among them, the state-of-the-art method is Berrut Approximated coded computing, wherein Berrut interpolants, are used for approximating the non-polynomial function. However, since Berrut interpolants have global support characteristics, such methods are known to offer degraded accuracy when the number of stragglers is large. To address this challenge, we propose a coded computing framework based on cubic B-spline interpolation. In our approach, server-side function evaluations are reconstructed at the master node using B-splines, exploiting their local support and smoothness properties to enhance stability and accuracy. We provide a systematic methodology for integrating B-spline interpolation into coded computing and derive theoretical bounds on approximation error in terms of the number of servers and stragglers. Comparative analysis demonstrates that our framework significantly outperforms Berrut-based methods for various non-polynomial functions.

</details>


### [93] [Converse Bounds for Sun-Jafar-type Weak Private Information Retrieval](https://arxiv.org/abs/2601.10643)
*Chandan Anand,Jayesh Seshadri,Prasad Krishnan,Gowtham R. Kurri*

Main category: cs.IT

TL;DR: 本文证明了Chandan等人提出的弱私有信息检索方案在特定条件下的最优性，并在不满足阈值约束时给出了反例，表明可以获得更高的速率。


<details>
  <summary>Details</summary>
Motivation: Chandan等人提出了新的弱私有信息检索方案，并给出了速率-隐私权衡表达式，但这些权衡是否在各自类别中是最优的尚不清楚。本文旨在研究这些方案的最优性。

Method: 通过理论分析证明Chandan等人提出的Sun-Jafar型方案在非共谋复制存储设置下的最优性，并证明Banawan-Ulukus型MDS-WPIR和Sun-Jafar型T-共谋WPIR方案在阈值约束下的类别最优性。当不满足阈值约束时，提供反例证明可以获得更高速率。

Result: 证明了Chandan等人提出的Sun-Jafar型方案在非共谋复制存储设置下的最优性，以及Banawan-Ulukus型MDS-WPIR和Sun-Jafar型T-共谋WPIR方案在阈值约束下的类别最优性。当不满足阈值约束时，通过反例展示了可以获得比之前报道更高的速率。

Conclusion: 本文确定了Chandan等人提出的弱私有信息检索方案在特定条件下的最优性边界，为相关方案的设计提供了理论指导，并指出了在阈值约束不满足时存在改进空间。

Abstract: Building on the well-established capacity-achieving schemes of Sun-Jafar (for replicated storage) and the closely related scheme of Banawan-Ulukus (for MDS-coded setting), a recent work by Chandan et al. proposed new classes of weak private information retrieval (WPIR) schemes for the collusion-free (replication and MDS-coded) setting, as well as for the $T$-colluding scenario. In their work, Chandan et al. characterized the expressions for the rate-privacy trade-offs for these classes of WPIR schemes, under the mutual information leakage and maximal leakage metrics. Explicit achievable trade-offs for the same were also presented, which were shown to be competitive or better than prior WPIR schemes. However, the class-wise optimality of the reported trade-offs were unknown. In this work, we show that the explicit rate-privacy trade-offs reported for the Sun-Jafar-type schemes by Chandan et al. are optimal for the non-colluding and replicated setting. Furthermore, we prove the class-wise optimality for Banawan-Ulukus-type MDS-WPIR and Sun-Jafar-type $T$-colluding WPIR schemes, under threshold-constraints on the system parameters. When these threshold-constraints do not hold, we present counter-examples which show that even higher rates than those reported before can be achieved.

</details>


### [94] [One-Shot Broadcast Joint Source-Channel Coding with Codebook Diversity](https://arxiv.org/abs/2601.10648)
*Joseph Rowan,Buu Phan,Ashish Khisti*

Main category: cs.IT

TL;DR: 研究单次联合信源信道编码，其中信源编码后通过独立信道广播给K个解码器，要求至少一个解码器在最大失真约束下恢复信源。发现使用不相交码本可获得码本分集增益，不同于共享码本时的信道分集增益。


<details>
  <summary>Details</summary>
Motivation: 在多解码器广播场景中，传统方法通常使用共享码本，但本文探索不相交码本是否能带来性能优势。研究目标是理解在单次编码场景中，如何通过码本设计来利用多个独立信道带来的分集机会。

Method: 提出两种编码方案：1）不相交码本方案，每个解码器使用独立码本；2）混合方案，将解码器分组以平衡码本分集和信道分集。采用泊松匹配引理的扩展推导一阶和二阶可达界。

Result: 在二进制对称信道上的数值结果表明，混合方案（部分解码器共享码本，部分使用不相交码本）优于完全共享或完全不相交码本的策略，能更好地平衡码本分集和信道分集。

Conclusion: 在单次联合信源信道编码的多解码器广播中，码本分集是一种新的分集形式，与传统的信道分集不同。通过精心设计的混合方案可以优化系统性能，为多用户广播系统提供了新的设计思路。

Abstract: We study a one-shot joint source-channel coding setting where the source is encoded once and broadcast to $K$ decoders through independent channels. Success is predicated on at least one decoder recovering the source within a maximum distortion constraint. We find that in the one-shot regime, utilizing disjoint codebooks at each decoder yields a codebook diversity gain, distinct from the channel diversity gain that may be expected when several decoders observe independent realizations of the channel's output but share the same codebook. Coding schemes are introduced that leverage this phenomenon, where first- and second-order achievability bounds are derived via an adaptation of the Poisson matching lemma (Li and Anantharam, 2021) which allows for multiple decoders using disjoint codebooks. We further propose a hybrid coding scheme that partitions decoders into groups to optimally balance codebook and channel diversity. Numerical results on the binary symmetric channel demonstrate that the hybrid approach outperforms strategies where the decoders' codebooks are either fully shared or disjoint.

</details>


### [95] [Synchronizing Probabilities in Model-Driven Lossless Compression](https://arxiv.org/abs/2601.10678)
*Aviv Adler,Jennifer Tang*

Main category: cs.IT

TL;DR: 论文提出PMATIC算法解决模型驱动压缩中的预测失配问题，该算法能容忍有界预测失配且开销低，在文本数据上验证了其理论正确性和性能优势。


<details>
  <summary>Details</summary>
Motivation: 在无损数据压缩中，概率预测可用于压缩符号序列，深度学习模型能有效估计这些概率。但压缩器和解压器必须具有完全匹配的预测，即使微小的非确定性差异（硬件、软件或计算顺序导致）也会导致级联解码失败。需要解决模型驱动压缩中的预测失配问题。

Method: 提出概率匹配区间编码（PMATIC），这是一种模型无关的算法，能容忍有界预测失配且开销低。PMATIC基于预测概率工作，可作为模型驱动压缩工具中算术编码器的即插即用替代方案。

Result: 证明了PMATIC的理论正确性和性能界限，并在文本数据上验证了结果。当与先进的预测模型配对时，PMATIC对预测失配具有鲁棒性，同时压缩率优于标准的现代压缩工具。

Conclusion: PMATIC为解决模型驱动压缩中的预测失配问题提供了有效方案，在保持压缩性能的同时增强了系统的鲁棒性，为深度学习模型在压缩领域的实际应用扫除了重要障碍。

Abstract: It is well-known in the field of lossless data compression that probabilistic next-symbol prediction can be used to compress sequences of symbols. Deep neural networks are able to capture rich dependencies in data, offering a powerful means of estimating these probabilities and hence an avenue towards more effective compression algorithms. However, both compressor and decompressor must have exactly matching predictions; even small non-deterministic differences (which often happen with learned models due to hardware, software, or computation order) can lead to cascading decoding failures. In this paper, we formalize the problem of prediction mismatch in model-driven compression, and introduce Probability Matching Interval Coding (PMATIC), a model-agnostic algorithm that tolerates bounded prediction mismatch with low overhead. PMATIC works with the predicted probabilities, making it compatible as a drop-in replacement for the arithmetic encoder in model-driven compression tools. We show theoretical correctness and performance bounds for PMATIC, and validate these results on text data. These results confirm that, when paired an advanced prediction model, PMATIC is robust to prediction mismatch while achieving compression rates that out-perform standard modern compression tools.

</details>


### [96] [Implementation of Oblivious Transfer over Binary-Input AWGN Channels by Polar Codes](https://arxiv.org/abs/2601.10682)
*Pin-Hsun Lin,Hadi Aghaee,Christian Deppe,Eduard A. Jorswieck,Holger Boche*

Main category: cs.IT

TL;DR: 基于极化码在二进制输入加性高斯白噪声信道上的二选一不经意传输协议，通过极化变换的自同构实现完美接收者隐私，通过信道极化实现渐近发送者隐私


<details>
  <summary>Details</summary>
Motivation: 开发在二进制输入加性高斯白噪声信道上的不经意传输协议，解决传统方案在有限块长下隐私保护不足的问题，实现完美接收者隐私和渐近发送者隐私

Method: 使用极化码，通过极化变换的自同构连接两个解码器视图，公开从对应的自同构群中随机选择编码器，在选定的坏比特信道上故意注入随机性

Result: 实现了完美接收者隐私（在任何有限块长下），渐近发送者隐私，推导了松弛的可靠性准则，表征了极化变换自同构为比特级置换，优化了可实现的有限块长OT速率

Conclusion: 提出的基于极化码的OT协议在二进制输入AWGN信道上实现了完美的接收者隐私和渐近的发送者隐私，通过自同构结构和松弛可靠性准则优化了有限块长性能

Abstract: We develop a one-out-of-two-oblivious transfer protocol over the binary-input additive white Gaussian noise channel using polar codes. The scheme uses two decoder views linked by automorphisms of the polar transform and publicly draws the encoder at random from the corresponding automorphism group. This yields perfect receiver privacy at any finite blocklength, since the public encoder distribution is independent of the receiver's choice bit. Sender privacy is obtained asymptotically via channel polarization combined with privacy amplification. Because the construction deliberately injects randomness on selected bad bit-channels, we derive a relaxed reliability criterion and evaluate finite-blocklength performance. Finally, we characterize the polar-transform automorphisms as bit-level permutations of bit-channel indices, and exploit this structure to derive and optimize an achievable finite-blocklength OT rate.

</details>


### [97] [Improved Constructions of Reed-Solomon Codes with Optimal Repair Bandwidth](https://arxiv.org/abs/2601.10685)
*Jing Qiu,Weijun Fang,Shu-Tao Xia,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 本文改进了RS-MSR码的构造，消除了素数模s同余1的条件，显著降低了子分组化程度并扩展了可行参数范围。


<details>
  <summary>Details</summary>
Motivation: 现有RS-MSR码构造要求素数p_i ≡ 1 (mod s)，这限制了参数选择和增加了子分组化程度，需要更灵活的构造方法。

Method: 提出改进的RS-MSR码构造方法，消除了p_i ≡ 1 (mod s)的约束条件，从而降低子分组化程度并扩展参数范围。

Result: 新构造将子分组化程度降低了φ(s)^n倍，显著扩展了RS-MSR码的可行参数范围，同时保持了MSR性能。

Conclusion: 通过消除同余条件约束，实现了更高效、更灵活的RS-MSR码构造，为分布式存储系统提供了更好的修复带宽优化方案。

Abstract: Maximum-distance-separable (MDS) codes are widely used in distributed storage, yet naive repair of a single erasure in an $[n,k]$ MDS code downloads the entire contents of $k$ nodes. Minimum Storage Regenerating (MSR) codes (Dimakis et al., 2010) minimize repair bandwidth by contacting $d>k$ helpers and downloading only a fraction of data from each. Guruswami and Wootters first proposed a linear repair scheme for Reed-Solomon (RS) codes, showing that they can be repaired with lower bandwidth than the naive approach. The existence of RS codes achieving the MSR point (RS-MSR codes) nevertheless remained open until the breakthrough construction of Tamo, Barg, and Ye, which yields RS-MSR codes with subpacketization $\ell = s \prod_{i=1}^n p_i$, where $p_i$ are distinct primes satisfying $p_i \equiv 1 \pmod{s}$ and $s=d+1-k$.
  In this paper, we present an improved construction of RS-MSR codes by eliminating the congruence condition $p_i \equiv 1 \pmod{s}$. Consequently, our construction reduces the subpacketization by a multiplicative factor of $φ(s)^n$ ( $φ(\cdot)$ is Euler's totient function) and broadens the range of feasible parameters for RS-MSR codes.

</details>


### [98] [Perfect Secret Key Generation for a class of Hypergraphical Sources](https://arxiv.org/abs/2601.10697)
*Manuj Mukherjee,Sagnik Chatterjee,Alhad Sethi*

Main category: cs.IT

TL;DR: 本文扩展了PIN模型到超图，提出了两种完美密钥生成方案：一种针对完全t-均匀超图，另一种针对3-均匀超图，并证明了在某些超图类中达到容量


<details>
  <summary>Details</summary>
Motivation: Nitinawarat和Narayan基于图的生成树打包率提出了PIN模型的完美密钥生成方案。本文旨在将这一工作扩展到超图模型，利用超图的组合性质设计类似的完美密钥生成方案

Method: 1. 针对完全t-均匀超图：利用星超图打包完全t-均匀超图，设计每个星图生成binom(m-2,t-2)比特完美密钥的方案
2. 针对3-均匀超图：首先为投影为环的3-均匀星超图设计2比特完美密钥生成方案，然后通过星图打包和哈密顿打包扩展到一般3-均匀超图

Result: 1. 提出的完全t-均匀超图方案达到了容量
2. 3-均匀超图方案对于某些超图类也达到了容量
3. 成功将基于图的PIN模型扩展到超图模型

Conclusion: 本文成功地将完美密钥生成从图模型扩展到超图模型，通过利用超图的组合性质（星超图打包和哈密顿打包）设计了容量达到的方案，为超图环境下的安全通信提供了理论基础

Abstract: Nitinawarat and Narayan proposed a perfect secret key generation scheme for the so-called \emph{pairwise independent network (PIN) model} by exploiting the combinatorial properties of the underlying graph, namely the spanning tree packing rate. This work considers a generalization of the PIN model where the underlying graph is replaced with a hypergraph, and makes progress towards designing similar perfect secret key generation schemes by exploiting the combinatorial properties of the hypergraph.
  Our contributions are two-fold. We first provide a capacity achieving scheme for a complete $t$-uniform hypergraph on $m$ vertices by leveraging a packing of the complete $t$-uniform hypergraphs by what we refer to as star hypergraphs, and designing a scheme that gives $\binom{m-2}{t-2}$ bits of perfect secret key per star graph. Our second contribution is a 2-bit perfect secret key generation scheme for 3-uniform star hypergraphs whose projections are cycles. This scheme is then extended to a perfect secret key generation scheme for generic 3-uniform hypergraphs by exploiting star graph packing of 3-uniform hypergraphs and Hamiltonian packings of graphs. The scheme is then shown to be capacity achieving for certain classes of hypergraphs.

</details>
