<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 9]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [GRU-Based Learning for the Identification of Congestion Protocols in TCP Traffic](https://arxiv.org/abs/2509.13490)
*Paul Bergeron,Sandhya Aneja*

Main category: cs.NI

TL;DR: 使用GRU神经网络模型在Marist大学校园网络中识别TCP Reno、TCP Cubic、TCP Vegas和BBR拥塞控制协议，准确率达到97.04%


<details>
  <summary>Details</summary>
Motivation: 在更复杂和竞争性的网络环境中，开发更快的神经网络架构来准确识别不同的拥塞控制协议

Method: 采用基于GRU（门控循环单元）的学习模型，构建了更快的神经网络架构

Result: 在Marist大学校园网络上实现了97.04%的识别准确率，与现有工作相比达到了相当高的精度

Conclusion: GRU-based模型在复杂网络环境中能够有效识别多种拥塞控制协议，具有较高的准确性和实用性

Abstract: This paper presents the identification of congestion control protocols TCP
Reno, TCP Cubic, TCP Vegas, and BBR on the Marist University campus, with an
accuracy of 97.04% using a GRU-based learning model. We used a faster neural
network architecture on a more complex and competitive network in comparison to
existing work and achieved comparably high accuracy.

</details>


### [2] [Odin: Effective End-to-End SLA Decomposition for 5G/6G Network Slicing via Online Learning](https://arxiv.org/abs/2509.13511)
*Duo Cheng,Ramanujan K Sheshadri,Ahan Kak,Nakjung Choi,Xingyu Zhou,Bo Ji*

Main category: cs.NI

TL;DR: Odin是一个基于贝叶斯优化的SLA分解方案，通过各域的在线反馈实现高效端到端SLA分解，相比基线方法提升45%的SLA满足率并降低资源成本


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络切片需要跨多域部署，但SLA分解面临域异构性、动态网络条件和资源优化不透明等挑战

Method: 采用贝叶斯优化方法，利用各域的在线反馈进行SLA分解

Result: 相比基线方法提升45%的SLA满足率，即使在存在噪声反馈的情况下也能降低总体资源成本

Conclusion: Odin方案能够有效解决跨域SLA分解问题，在保证性能的同时优化资源利用

Abstract: Network slicing plays a crucial role in realizing 5G/6G advances, enabling
diverse Service Level Agreement (SLA) requirements related to latency,
throughput, and reliability. Since network slices are deployed end-to-end
(E2E), across multiple domains including access, transport, and core networks,
it is essential to efficiently decompose an E2E SLA into domain-level targets,
so that each domain can provision adequate resources for the slice. However,
decomposing SLAs is highly challenging due to the heterogeneity of domains,
dynamic network conditions, and the fact that the SLA orchestrator is oblivious
to the domain's resource optimization. In this work, we propose Odin, a
Bayesian Optimization-based solution that leverages each domain's online
feedback for provably-efficient SLA decomposition. Through theoretical analyses
and rigorous evaluations, we demonstrate that Odin's E2E orchestrator can
achieve up to 45% performance improvement in SLA satisfaction when compared
with baseline solutions whilst reducing overall resource costs even in the
presence of noisy feedback from the individual domains.

</details>


### [3] [LINC: An In-Network Coding Approach to Tame Packet Loss in Hybrid Wireless-Fiber Backbones](https://arxiv.org/abs/2509.13714)
*Benoit Pit-Claudel,Muriel Médard,Manya Ghobadi*

Main category: cs.NI

TL;DR: LINC是一个在混合骨干网络中提供网络内网络编码能力的系统，通过逐链路编码解码来缓解环境因素导致的丢包，无需终端主机配合，可减少18%的端到端延迟


<details>
  <summary>Details</summary>
Motivation: 超低延迟应用（如金融交易）需要混合骨干网络，但这些网络存在环境因素导致的丢包问题。传统传输协议将丢包误判为网络拥塞，现有网络编码方案需要终端主机配合

Method: LINC采用系统块编码方法，在网络内部逐链路进行编码和解包操作。通过建模重传与冗余数据包的吞吐量权衡，提出优化公式确定最佳编码参数

Result: 在真实骨干网络拓扑上的模拟显示，LINC通过消除不必要的重传，可将端到端延迟降低高达18%

Conclusion: LINC系统成功实现了无需终端主机配合的网络内网络编码，有效解决了混合骨干网络中环境丢包问题，显著提升了网络性能

Abstract: The emergence of ultra-low latency applications, such as financial
transactions, has driven the development of hybrid backbone networks that rely
on fiber, satellite, and microwave links. Despite providing low latencies,
these hybrid networks suffer from occasional environmental packet loss caused
by poor weather, construction, and line of sight blockage. Paradoxically,
today's hybrid backbones rely on conventional transport protocols that take
packet loss to signal network congestion, as opposed to transient environmental
obstacles. A common approach to address this challenge is to use network coding
(NC) between the end hosts to recover from these occasional packet loss events.
However, current NC proposals assume full access to the end-hosts' stack to
perform end-to-end encoding/decoding operations. In this paper, we introduce
LINC, a novel system that provides in-network NC capabilities to mitigate
environmental packet loss events without requiring cooperation from the end
hosts. LINC uses a systematic block coding approach on a link-by-link basis,
encoding and decoding packets inside the network. We model the tradeoff in
goodput between end-to-end retransmissions and redundant packets introduced by
LINC, and propose an optimization formulation to determine the optimal choice
of coding parameters. Our simulations on real-world backbone topologies
demonstrate that LINC reduces the end-to-end latency by up to 18% by
eliminating unnecessary retransmissions.

</details>


### [4] [A Framework for Multi-source Prefetching Through Adaptive Weight](https://arxiv.org/abs/2509.13604)
*Yoseph Berhanu Alebachew,Mulugeta Libsie*

Main category: cs.NI

TL;DR: 这篇论文提出了一种新的预取框架，能够集成基于访问历史和语义信息的两种预取方案，通过自适应权重管理来提高预测准确性并适合资源受限的移动设备。


<details>
  <summary>Details</summary>
Motivation: 解决网络延迟问题，现有预取方案对应用层文档关系的利用有限，且无法有效集成基于访问历史和语义信息的两种预取方案。

Method: 提出一种新的框架，允许任何预取算法作为生成候选对象列表的组件集成进来，通过自适应权重管理技术根据每个算法的表现调整其在整体预测中的影响力。

Result: 该框架比现有方案更不具攻击性，特别适合资源受限的移动设备，能够在不需重大修改算法的情况下集成新的应用层上下文捐捕算法。

Conclusion: 该框架提供了一种灵活可扩展的方式来结合不同预取方案，有效解决了网络延迟问题，尤其适合当今以移动设备为主要访问手段的现代网络环境。

Abstract: The World Wide Web has come to be a great part of our daily life, yet user
observed latency is still a problem that needs a proper means of handling. Even
though earlier attempts focused on caching as the chief solution to tackling
this issue, its success was extremely limited. Prefetching has come to be the
primary technique in supplementing caching towards soothing the latency problem
associated with the contemporary Internet. However, existing approaches in
prefetching are extremely limited in their ability to employ application level
web document relationship which is often visible only to the content developer.
This is because most approaches are access history based schemes that make
future users' access prediction only based on past user access. Attempts to
incorporate prefetching schemes that utilize semantic information with those
that use users past access history are extremely limited in their
extensibility. In this work we present a novel framework that enables
integration of schemes from both worlds of prefetching without the need for a
major modification to the algorithms. When there is a need/possibility to
capture new application level context, a new algorithm could be developed to do
so and then it can be integrated into the framework. Since each participating
scheme is merely viewed as an algorithm that produces a list of candidate
objects that are likely to be accessed in the near future, the framework can
entertain any one of the existing prefetching schemes. With its adaptive weight
management technique the framework adjusts the effect of each algorithm in the
overall prediction to parallel with its observed performance so far. We have
found this formwork to be less aggressive than its contemporary counterparts
which is extremely important for resource constrained mobile devices that have
come to be the major means of access by users of the current web.

</details>


### [5] [Conducting Mission-Critical Voice Experiments with Automated Speech Recognition and Crowdsourcing](https://arxiv.org/abs/2509.13724)
*Jan Janak,Kahlil Dozier,Lauren Berny,Liang Hu,Dan Rubenstein,Charles Jennings,Henning Schulzrinne*

Main category: cs.NI

TL;DR: 本文研究公安安全语音通信系统(MCV)的用户体验质量(QoE)，开发了在模拟真实环境中进行人类参与实验的方法和工具，包括MCV系统模拟平台和ASR机器人，并使用Levenshtein距离指标评估QoE。


<details>
  <summary>Details</summary>
Motivation: 公安安全用户期望MCV系统在具有挑战性的条件下可靠运行，但现有研究受限于模拟真实环境的挑战，且缺少最佳的QoE评估指标。

Method: 开发了包括MCV系统模拟平台和ASR机器人的工具集，在模拟真实环境中进行人类参与实验，使用Levenshtein距离作为QoE评估指标，通过Amazon MTurk录用志愿者进行人体实验。

Result: 发现人类在准确性相关的MCV任务中通常表现更好于ASR，编解码器(codec)对用户QoE和ASR性能有显著影响。

Conclusion: 该研究为公安安全语音通信系统的用户体验质量评估提供了有效的方法和工具，并证明Levenshtein距离是适合的QoE代理指标，为后续研究奠定了基础。

Abstract: Mission-critical voice (MCV) communications systems have been a critical tool
for the public safety community for over eight decades. Public safety users
expect MCV systems to operate reliably and consistently, particularly in
challenging conditions. Because of these expectations, the Public Safety
Communications Research (PSCR) Division of the National Institute of Standards
and Technology (NIST) has been interested in correlating impairments in MCV
communication systems and public safety user quality of experience (QoE).
Previous research has studied MCV voice quality and intelligibility in a
controlled environment. However, such research has been limited by the
challenges inherent in emulating real-world environmental conditions.
Additionally, there is the question of the best metric to use to reflect QoE
accurately.
  This paper describes our efforts to develop the methodology and tools for
human-subject experiments with MCV. We illustrate their use in human-subject
experiments in emulated real-world environments. The tools include a testbed
for emulating real-world MCV systems and an automated speech recognition (ASR)
robot approximating human subjects in transcription tasks. We evaluate QoE
through a Levenshtein Distance-based metric, arguing it is a suitable proxy for
measuring comprehension and the QoE. We conducted human-subject studies with
Amazon MTurk volunteers to understand the influence of selected system
parameters and impairments on human subject performance and end-user QoE. We
also compare the performance of several ASR system configurations with
human-subject performance. We find that humans generally perform better than
ASR in accuracy-related MCV tasks and that the codec significantly influences
the end-user QoE and ASR performance.

</details>


### [6] [Performance Evaluation of Intent-Based Networking Scenarios: A GitOps and Nephio Approach](https://arxiv.org/abs/2509.13901)
*Saptarshi Ghosh,Ioannis Mavromatis,Konstantinos Antonakoglou,Konstantinos Katsaros*

Main category: cs.NI

TL;DR: 本文通过可复现的性能测试对三种主流GitOps工具(Argo CD、Flux CD、ConfigSync)在意图基于网络场景中的延迟和资源开销进行了综合评估，为自主网络系统的工具选型和优化提供指导。


<details>
  <summary>Details</summary>
Motivation: GitOps已成为管理云原生基础设施的核心范式，但在意图基于网络场景中，GitOps工具的性能和可扩展性缺乏充分评估。

Method: 进行可复现的指标驱动性能测试，在单意图和多意图场景下测试三种GitOps运算符，捕获延迟和资源消耗关键指标，并使用Nephio作为组织器进行实际场景测试。

Result: 结果显示了各工具在确定性、资源效率和响应能力方面的特性和交换。定量了声明式端到端部署流水线中的处理延迟和开销。

Conclusion: 研究结果为未来自主网络组织系统的工具选型和优化提供了有价值的见解。

Abstract: GitOps has emerged as a foundational paradigm for managing cloud-native
infrastructures by enabling declarative configuration, version-controlled
state, and automated reconciliation between intents and runtime deployments.
Despite its widespread adoption, the performance and scalability of GitOps
tools in Intent-Based Networking (IBN) scenarios are insufficiently evaluated.
This paper presents a reproducible, metric-driven benchmarking, assessing the
latency and resource overheads of three widely used GitOps operators: Argo CD,
Flux CD, and ConfigSync. We conduct controlled experiments under both single-
and multi-intent scenarios, capturing key performance indicators such as
latency and resource consumption. Our results highlight trade-offs between the
tools in terms of determinism, resource efficiency, and responsiveness. We
further investigate a realistic orchestration scenario, using Nephio as our
orchestrator, to quantify the processing latency and overhead in declarative
end-to-end deployment pipelines. Our findings can offer valuable insights for
tool selection and optimisation in future autonomous network orchestration
systems.

</details>


### [7] [Low-cost Highly-interoperable Multiplatform Campus Network: Experience of YARSI University](https://arxiv.org/abs/2509.13954)
*Surya Agustian,Sandra Permana,Salman Teguh Pratista,Syarifu Adam,Iswandi*

Main category: cs.NI

TL;DR: 基于开源系统和本地组装PC的低成本校园网设计方案，通过结合Cisco交换技术和宽带连接，为预算有限的组织提供网络访问


<details>
  <summary>Details</summary>
Motivation: 解决组织建设校园网络成本高、专业IT知识缺乏导致外包费用过高的问题

Method: 使用开源操作系统在本地组装PC上搭建网关/路由器，结合Cisco交换技术，通过多个宽带连接和专用无线网共享互联网访问

Result: 成功建设了覆盖YARSI大学环境的低成本UTP基础校园网，支持超100同时用户访问

Conclusion: 该模型能显著降低网络基础设施的采购、维护和运营成本，适合农村社区或预算有限的组织采用

Abstract: To some organizations, building campus network is sometimes considered to be
very expensive; and this has made the project uneasy to perform. Moreover, if
the organization without sufficient IT knowledge does not have capable IT
engineers, leaving this project to third parties without supervision would lead
to unexpected larger expenses. For this reason, in the year of 2003, YARSI
University formed CMIS (Center for Management Infor-mation System) to perform
tasks in designing, operations and maintenance of campus network and its
services. By combining Open Source operating system run on a local assembled
personal computer as gateway and router, and switching technology from Cisco,
we designed a low-cost UTP-based campus network which covering rooms and
buildings in YARSI environment. Meanwhile the internet access through several
broadband connections and dedicated wireless was shared to more than 100
simultaneous users by a captive portal system. With this strategy, we can
significantly reduce cost for purchasing, maintenance and operations of network
infrastructure and internet access. Our model in designing low-cost campus
network and internet connections could be adopted by rural community or
organizations that have limited budget to have internet access.

</details>


### [8] [Path-Oblivious Entanglement Swapping for the Quantum Internet](https://arxiv.org/abs/2509.13993)
*Vincent Mutolo,Rhea Parekh,Dan Rubenstein*

Main category: cs.NI

TL;DR: 本文提出了一种路径无关的Bell对交换协议，相比传统的预定路径方法，在量子网络资源充足时具有更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统量子网络中的Bell对交换采用预定路径方式，但经典网络经验表明在资源充足的网络中，灵活的无预留方法往往表现更好。随着量子态技术发展变得更廉价和稳定，需要探索更灵活的交换方法。

Method: 将Bell对交换过程建模为线性规划问题，提出并评估了一种简单的基线交换协议，该协议尝试在网络中平衡Bell对的分布。

Result: 初步结果显示，虽然简单的平衡策略还有改进空间，但路径无关的交换方法是一个有前景的研究方向。

Conclusion: 随着量子态技术发展，路径无关的Bell对交换协议比传统的预定路径方法更适合未来量子互联网的发展需求。

Abstract: Proposed Bell pair swapping protocols, an essential component of the Quantum
Internet, are planned-path: specific, structured, routing paths are reserved
prior to the execution of the swapping process. This makes sense when one
assumes the state used in the swapping process is expensive, fragile, and
unstable. However, lessons from classical networking have shown that while
reservations seem promising in concept, flexible, reservation-light or free
approaches often outperform their more restrictive counterparts in
well-provisioned networks. In this paper, we propose that a path-oblivious
approach is more amenable to supporting swapping as quantum state evolves into
a cheaper, more robust form. We formulate the swapping process as a linear
program and present and evaluate a fairly naive baseline swapping protocol that
tries to balance Bell pairs throughout the network. Preliminary results show
that while naive balancing leaves room for improvement, investigating
path-oblivious swapping is a promising direction.

</details>


### [9] [RepCaM++: Exploring Transparent Visual Prompt With Inference-Time Re-Parameterization for Neural Video Delivery](https://arxiv.org/abs/2509.14002)
*Rongyu Zhang,Xize Duan,Jiaming Liu,Li Du,Yuan Du,Dan Wang,Shanghang Zhang,Fangxin Wang*

Main category: cs.NI

TL;DR: RepCaM++是一个基于重参数化内容感知调制模块的创新框架，通过透明视觉提示技术，在视频超分辨率任务中实现了带宽压缩和视频恢复质量的提升。


<details>
  <summary>Details</summary>
Motivation: 现有的内容感知方法为每个视频块训练独立超分辨率模型会导致参数累积问题，随着视频长度增加，传输成本上升且性能下降。

Method: 提出RepCaM模块统一调制视频块，在训练时集成并行级联参数，推理时通过重参数化消除额外参数；引入透明视觉提示(TVP)技术，使用极少量零初始化图像级参数捕捉细节。

Result: 在VSD4K数据集的6种不同视频场景上进行广泛实验，在视频恢复质量和传输带宽压缩方面达到了最先进的性能。

Conclusion: RepCaM++框架有效解决了参数累积问题，通过重参数化和TVP技术实现了高效的视频传输和高质量的超分辨率恢复。

Abstract: Recently, content-aware methods have been employed to reduce bandwidth and
enhance the quality of Internet video delivery. These methods involve training
distinct content-aware super-resolution (SR) models for each video chunk on the
server, subsequently streaming the low-resolution (LR) video chunks with the SR
models to the client. Prior research has incorporated additional partial
parameters to customize the models for individual video chunks. However, this
leads to parameter accumulation and can fail to adapt appropriately as video
lengths increase, resulting in increased delivery costs and reduced
performance. In this paper, we introduce RepCaM++, an innovative framework
based on a novel Re-parameterization Content-aware Modulation (RepCaM) module
that uniformly modulates video chunks. The RepCaM framework integrates extra
parallel-cascade parameters during training to accommodate multiple chunks,
subsequently eliminating these additional parameters through
re-parameterization during inference. Furthermore, to enhance RepCaM's
performance, we propose the Transparent Visual Prompt (TVP), which includes a
minimal set of zero-initialized image-level parameters (e.g., less than 0.1%)
to capture fine details within video chunks. We conduct extensive experiments
on the VSD4K dataset, encompassing six different video scenes, and achieve
state-of-the-art results in video restoration quality and delivery bandwidth
compression.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 这篇论文系统性比较了"思考型"和"非思考型"LLM在评测任务中的表现，发现显式推理能够在准确性、效率和稳健性方面带来显著优势


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被采用作为自动化评测器，确保其可靠性、效率和稳健性变得至关重要

Method: 使用开源Qwen 3模型(0.6B、1.7B和4B参数)，在RewardBench任务上评估准确性和计算效率(FLOPs)，并测试各种增强策略如上下文学习、指南引导评测、参考基准评估和n-best聚合

Result: 思考型模型准确性高约10%，计算成本仅增加不到2倍，而少量学习等增强方法效果有限但成本高过8倍以上。思考型模型在各种偏见条件下保持更高一致性(平均高出6%)，多语言实验也证实显式推理的优势超越英语语境

Conclusion: 显式推理在LLM-as-a-judge范式中具有明显优势，不仅在准确性和效率方面，还在稳健性方面都有显著改善，为推理能力在评测任务中的重要性提供了系统性证据

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [11] [Evaluation Awareness Scales Predictably in Open-Weights Large Language Models](https://arxiv.org/abs/2509.13333)
*Maheep Chaudhary,Ian Su,Nikhil Hooda,Nishith Shankar,Julia Tan,Kevin Zhu,Ashwinee Panda,Ryan Lagasse,Vasu Sharma*

Main category: cs.AI

TL;DR: 研究发现大型语言模型存在评估意识行为，即模型能在评估和部署环境中区分并隐藏危险能力。通过对15个不同规模模型的分析，揭示了评估意识随模型规模呈幂律增长的关系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在评估时可能隐藏危险能力，这种评估意识行为会破坏AI安全评估的有效性。之前的研究仅在单个70B模型中发现此现象，但不同规模模型间的缩放关系尚不清楚。

Method: 使用线性探测方法分析来自4个模型家族的15个不同规模模型（0.27B到70B参数）的转向向量激活，研究评估意识的缩放规律。

Result: 发现评估意识随模型规模呈清晰的幂律缩放关系，评估意识能力随模型参数增加而可预测地增强。

Conclusion: 这一缩放规律可用于预测未来更大模型的欺骗行为，并为设计规模感知的AI安全评估策略提供指导。

Abstract: Large language models (LLMs) can internally distinguish between evaluation
and deployment contexts, a behaviour known as \emph{evaluation awareness}. This
undermines AI safety evaluations, as models may conceal dangerous capabilities
during testing. Prior work demonstrated this in a single $70$B model, but the
scaling relationship across model sizes remains unknown. We investigate
evaluation awareness across $15$ models scaling from $0.27$B to $70$B
parameters from four families using linear probing on steering vector
activations. Our results reveal a clear power-law scaling: evaluation awareness
increases predictably with model size. This scaling law enables forecasting
deceptive behavior in future larger models and guides the design of scale-aware
evaluation strategies for AI safety. A link to the implementation of this paper
can be found at
https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.

</details>


### [12] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: FRIT是一种通过干预训练提升思维链推理忠实性的方法，通过生成忠实/不忠实推理对并应用直接偏好优化，在保持准确性的同时显著提高推理的因果一致性。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理方法存在推理步骤与最终答案缺乏因果关联的问题，导致输出脆弱不可信。虽然已有方法关注忠实性测量，但系统性提升方法仍然有限。

Method: 提出FRIT方法：1）在模型生成的思维链中对单个推理步骤进行干预，生成忠实/不忠实推理对；2）应用直接偏好优化训练模型偏好因果一致的推理路径。

Result: 在Qwen3-8B和Mistral-7B-v0.1模型上，FRIT使Mistral在GSM8K上的忠实推理提升3.4个百分点，准确率提升7.6个百分点。

Conclusion: FRIT提供了首个可扩展、无监督的方法来训练语言模型产生更可靠和可解释的推理，解决了推理性能与可信度之间的关键差距。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [13] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: 该位置文程主张采用反脆弱性视角来提升AI安全性，让系统在应对稀罕和分布外事件时不断增强能力，而非仅靠静态测试和单次程度测试。


<details>
  <summary>Details</summary>
Motivation: 现有的静态测试方法忽视了环境的演化特性，模型可能因长期缺乏挑战而出现奖励欺骗、过度优化或能力衰退等问题。需要一种能够利用不确定性来应对未来更大不确定性的方法。

Method: 提出反脆弱性方法，通过识别静态测试的主要限制（场景多样性、奖励欺骗、过度对齐等），探索反脆弱性解决方案来管理稀有事件。

Result: 为长期AI安全性提供了理论基础，建议重新调整AI安全性的测量、测试和持续改进方法。

Conclusion: 反脆弱性方法对于开放式机器学习系统的长期可靠性至关重要，完善了现有稳健性方法，为建设反脆弱性AI安全社区提供了道德和实践指南。

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


### [14] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: IMAC方法利用世界模型生成想象环境，通过无监督环境设计自动课程训练智能体，在有限数据下实现对新任务变体的强泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中训练数据稀缺和模拟环境不足的问题，利用离线被动收集的数据构建世界模型来训练鲁棒智能体

Method: 提出IMAC（想象自动课程）方法，结合无监督环境设计（UED）在世界模型生成的想象环境中诱导自动课程，训练智能体

Result: 在程序生成的环境中，仅使用较窄数据集学习的世界模型进行训练，就能在保留环境中实现强大的迁移性能

Conclusion: 该方法为利用大规模基础世界模型训练通用智能体开辟了新途径

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [15] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: 这篇论文提出了链式动作（CoA）框架，通过将高级规划与低级控制统一在单个模型中，解决了动作空间选择的困境，在Minecraft环境中实现了新的最高水平。


<details>
  <summary>Details</summary>
Motivation: 动作空间的选择是建立能力强大、端到端可训练代理的关键挑战。研究发现没有单一的最优动作空间，效果依赖于具体任务，这为建立通用代理人带来困难。

Method: 提出链式动作（CoA）框架，将抽象动作视为中间推理步骤（类似链式思维），用于指导生成最终可执行动作。训练一个在多样化动作空间混合数据上的"万能"代理。

Result: 统一的CoA代理在Minecraft中达到了新的最高水平，提高了整体任务成功率，超越了强大的专门化基线模型。

Conclusion: 链式动作框架能够有效解决动作空间选择困境，学习到更稳健和可推广的策略，为建立通用代理人提供了新的解决方案。

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [16] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 提出了PDDL-Instruct指令调优框架，通过逻辑思维链推理增强大语言模型的符号规划能力，在标准基准测试中达到94%的规划准确率，比基线模型提升66%


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种任务上表现出色，但在需要形式化表示（如PDDL）的结构化符号规划方面能力有限，需要弥合通用推理能力与自动规划所需逻辑精度之间的差距

Method: 开发指令提示引导模型通过精确的逻辑推理步骤来思考动作适用性、状态转换和计划有效性，将规划过程分解为关于前提条件满足、效果应用和不变性保持的显式推理链

Result: 在多个规划领域的实验结果显示，基于思维链推理的指令调优模型规划能力显著提升，在标准基准测试中达到94%的规划准确率

Conclusion: 该工作为开发更好的AI规划系统提供了有前景的方向，成功弥合了大语言模型通用推理能力与自动规划所需逻辑精度之间的差距

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [17] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: 提出了Agentic UAVs框架，通过五层架构将LLM驱动的推理能力集成到无人机中，在搜救模拟中显著提升了检测性能和自主决策能力


<details>
  <summary>Details</summary>
Motivation: 现有无人机系统主要依赖基于规则的控制和窄AI，缺乏上下文感知推理、自主决策和生态系统集成能力，无法充分利用LLM的实时知识访问优势

Method: 设计了五层架构（感知、推理、行动、集成、学习），结合ROS2和Gazebo原型，集成YOLOv11目标检测与GPT-4推理，并部署本地Gemma-3模型

Result: 在模拟搜救场景中，检测置信度从0.72提升到0.79，人员检测率从75%提升到91%，行动推荐率从4.5%大幅提升到92%

Conclusion: 适度的计算开销就能实现质的自主性提升和生态系统集成，证明了LLM驱动的无人机框架的有效性

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [18] [Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling](https://arxiv.org/abs/2509.13357)
*Yongchao Huang,Hassan Raza*

Main category: cs.AI

TL;DR: 语义融合方案：通过并行模糊成员特征通道增强Transformer语言模型，实现可解释的语义特征融合和用户可控生成


<details>
  <summary>Details</summary>
Motivation: 提升语言模型的语义理解能力和生成可控性，同时保持模型简洁性和可解释性

Method: 为每个token构建可解释特征向量（词性、浅层角色、边界标志、情感极性等），通过门控适配器将语义矩阵融合到LM中，使用标准下一词预测、辅助损失和轻量级正则化器训练

Result: 在合成双子句语料库上，语义融合降低了困惑度，实现了精确的用户可控极性和标点生成，同时保持模型简洁性

Conclusion: 语义融合为条件自然语言生成提供了可解释的途径，仅增加少量开销，且完全兼容输入输出嵌入绑定

Abstract: We propose semantic fusion, a lightweight scheme that augments a Transformer
language model (LM) with a parallel, fuzzy-membership feature channel that
encodes token-level semantics. Each token is represented by a vector of
interpretable features (e.g. part-of-speech cues, shallow roles, boundary
flags, sentiment polarity and strength) whose values are graded degrees from
differentiable membership functions (e.g. power kernels). These per-token
vectors form a sentence-level semantic matrix fused via a gated adapter into
the LM. Training uses standard next-token prediction, an auxiliary loss that
reconstructs the semantic features from hidden states, and a lightweight
uniformizer that regularizes adjective-class distributions. On a synthetic
two-clause corpus with held-out adjectives for out-of-distribution (OOD)
control, semantic fusion improves perplexity and enables precise,
user-controllable generation of polarity and punctuation while maintaining
model simplicity. This approach adds only small overhead, remains fully
compatible with tied input-output embeddings, and provides an interpretable
pathway for conditioned natural language generation.

</details>


### [19] [Asterisk Operator](https://arxiv.org/abs/2509.13364)
*Zixi Li*

Main category: cs.AI

TL;DR: 提出了星号算子（*算子）这一新颖的统一抽象推理框架，基于邻接结构并行传播（ASPP），将结构化推理任务形式化为由隐式关系图指导的局部并行状态演化过程。


<details>
  <summary>Details</summary>
Motivation: 为了解决抽象推理任务中的计算效率和全局推理能力之间的平衡问题，需要一种既能保持局部计算约束又能实现全局推理的统一框架。

Method: 基于邻接结构并行传播（ASPP）的星号算子，将推理任务建模为局部并行状态演化，并通过数学证明确保其收敛性和计算效率。

Result: 在ARC2挑战和康威生命游戏中验证了算子的通用性和收敛性，创新的Embedding-Asterisk蒸馏方法仅用600万参数就在ARC2验证集上达到100%准确率。

Conclusion: 星号算子为神经符号推理提供了高效且收敛的计算范式，在抽象推理领域实现了重要突破，展示了优异的性能和通用近似能力。

Abstract: We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified
framework for abstract reasoning based on Adjacency-Structured Parallel
Propagation (ASPP). The operator formalizes structured reasoning tasks as
local, parallel state evolution processes guided by implicit relational graphs.
We prove that the $\ast$-operator maintains local computational constraints
while achieving global reasoning capabilities, providing an efficient and
convergent computational paradigm for abstract reasoning problems. Through
rigorous mathematical analysis and comprehensive experiments on ARC2 challenges
and Conway's Game of Life, we demonstrate the operator's universality,
convergence properties, and superior performance. Our innovative
Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2
validation with only 6M parameters, representing a significant breakthrough in
neural-symbolic reasoning.
  \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel
Propagation, Asterisk Operator, Convergence, Universal Approximation

</details>


### [20] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: Agent²是一个完全自动化的强化学习代理生成框架，通过LLM驱动将自然语言任务描述和环境代码转换为高性能RL解决方案，无需人工干预，在多个基准测试中表现优于人工设计方案。


<details>
  <summary>Details</summary>
Motivation: 传统RL代理开发需要大量专业知识和迭代周期，失败率高且可访问性有限。为了解决这些问题，需要实现完全自动化的RL代理设计。

Method: 采用双代理架构：Generator Agent作为自主AI设计器分析任务并生成可执行RL代理，Target Agent是自动生成的RL代理。框架将RL开发分解为MDP建模和算法优化两个阶段，基于Model Context Protocol提供统一框架。

Result: 在MuJoCo、MetaDrive、MPE和SMAC等多个基准测试中，Agent²始终优于人工设计的解决方案，性能提升高达55%，平均性能也有显著提升。

Conclusion: 这项工作通过实现真正的端到端闭环自动化，建立了智能代理设计和优化其他代理的新范式，是自动化AI系统的根本性突破。

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [21] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: 这篇论文对16个先进的视觉-语言模型进行了全面的不确定性量化评测，发现较大模型具有更好的不确定性表现，且数学和推理任务的不确定性表现较差


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言模型在科学和推理任务中取得了显著进步，但不确定性量化还没有得到充分关注，需要进行全面的评测研究

Method: 使用3种不同的评分函数，在6个多模态数据集上评估16个先进的开源和闭源视觉-语言模型

Result: 较大的模型一贯表现出更好的不确定性量化能力，更确定的模型达到更高的准确率，数学和推理任务的不确定性表现比其他领域更差

Conclusion: 这项工作为多模态系统的可靠不确定性评估奠定了基础

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


### [22] [From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](https://arxiv.org/abs/2509.13389)
*Carlos Núñez-Molina,Vicenç Gómez,Hector Geffner*

Main category: cs.AI

TL;DR: 使用变换器模型从动作踪迹中学习命题STRIPS世界模型，通过下一个动作预测来学习动作前提条件和效果。


<details>
  <summary>Details</summary>
Motivation: 从动作踪迹中自动学习世界模型是人工智能和规划领域的重要问题，传统方法需要明确的基础知识。这项研究针对如何从纯粹的动作序列数据中推断出隐藏的动作前提条件和效果。

Method: 将任务形式化为监督学习问题，使用变换器模型进行下一个动作预测。模型从随机生成的有效（正样本）和无效（负样本）动作序列中学习。通过动作序列的合法性来推断隐藏的前提条件约束。

Result: 研究表明适当的变换器架构能够实现对命题STRIPS世界模型的忠实表征。实验结果显示可以仅从正负样本动作序列中成功学习到这些模型。

Conclusion: 这项工作证明了使用深度学习方法（特别是变换器）从动作踪迹中学习世界模型的可行性，为自动化规划和模型学习领域提供了新的方法。

Abstract: We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.

</details>


### [23] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl是一个评估表示引导方法的基准，重点关注偏见、有害生成和幻觉等核心对齐目标，以及这些方法对次要行为的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐工作主要关注真实性或推理能力，但表示引导方法在其他重要行为方面的副作用尚未得到系统性的研究。

Method: 构建了一个模块化的引导框架，收集了安全相关的主要和次要行为数据集，并在Qwen-2.5-7B和Llama-3.1-8B模型上评估了五种流行引导方法。

Result: 发现强引导性能取决于引导方法、模型和目标行为的特定组合，不良组合会导致严重的概念纠缠问题。

Conclusion: 表示引导方法的效果具有高度依赖性，需要综合考虑方法、模型和行为目标的匹配性，以避免负面副作用。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [24] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: 为LLM智能体提供类似人类的协作工具（社交媒体和日志工具）可以显著提升其在最困难编程问题上的性能表现，降低成本、减少交互轮次并加快完成速度。


<details>
  <summary>Details</summary>
Motivation: 研究是否通过赋予LLM智能体人类自然使用的协作工具和自主性，能够改善它们在问题解决中的表现。

Method: 为Claude Code智能体配备基于MCP的社交媒体和日志工具，让它们自主决定如何使用这些工具来解决34个Aider Polyglot Python编程挑战。

Result: 协作工具在最困难问题上显著提升性能：成本降低15-40%，交互轮次减少12-27%，完成速度加快12-38%。不同模型自然采用不同的协作策略，智能体偏好写作而非阅读（2-9倍），表明结构化表达是改进的主要驱动力。

Conclusion: AI智能体在其能力边缘可以系统性地受益于人类启发的协作工具，这表明自适应协作界面可以作为推理增强器，而非普遍效率提升工具。

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [25] [Gen AI in Proof-based Math Courses: A Pilot Study](https://arxiv.org/abs/2509.13570)
*Hannah Klawa,Shraddha Rajpal,Cigole Thomas*

Main category: cs.AI

TL;DR: 这篇论文研究了在证明基础数学课程中学生使用生成式AI的情况和观点，分析了AI工具的用途和限制，为数学教学提供政策建议。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在高等教育中的快速兴起和当前AI检测工具的不可靠性，需要开发能够鼓励学生学习和承利思维的政策。

Method: 研究在三门证明基础的本科数学课程中进行（第一学期抽象代数、拓扑学和第二学期抽象代数），课程政策允许某些AI使用。通过调查回复和学生访谍收集数据。

Result: 分析了学生如何使用AI工具、对生成式AI有用性和限制的看法，以及这些观点对证明基础数学教学的含义。

Conclusion: 论文最后讨论了将生成式AI整合到证明基础数学教学中的未来考虑因素。

Abstract: With the rapid rise of generative AI in higher education and the
unreliability of current AI detection tools, developing policies that encourage
student learning and critical thinking has become increasingly important. This
study examines student use and perceptions of generative AI across three
proof-based undergraduate mathematics courses: a first-semester abstract
algebra course, a topology course and a second-semester abstract algebra
course. In each case, course policy permitted some use of generative AI.
Drawing on survey responses and student interviews, we analyze how students
engaged with AI tools, their perceptions of generative AI's usefulness and
limitations, and what implications these perceptions hold for teaching
proof-based mathematics. We conclude by discussing future considerations for
integrating generative AI into proof-based mathematics instruction.

</details>


### [26] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: CoBRA是一个用于在基于LLM的社会模拟中系统化指定智能体行为的新工具包，通过显式编程认知偏见来解决传统自然语言描述方法的一致性问题


<details>
  <summary>Details</summary>
Motivation: 传统方法通过隐式自然语言描述指定智能体行为存在两个问题：无法在不同模型间产生一致行为，且生成的行为无法捕捉描述的细微差别

Method: CoBRA包含两个组件：1) 认知偏见指数 - 通过量化智能体在一组经过验证的经典社会科学实验中的反应来测量认知偏见；2) 行为调节引擎 - 调整智能体行为以展示受控的认知偏见

Result: 评估显示CoBRA能够以模型无关的方式精确编程社会智能体中展示的认知偏见

Conclusion: CoBRA提供了一种新的方法来显式编程智能体的认知偏见，通过基于经典社会科学实验来锚定预期行为，解决了传统方法的局限性

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>


### [27] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 该论文提出State-aware Reasoning (StaR)方法，通过训练多模态代理感知当前开关状态并根据指令执行操作，解决了GUI控制中开关指令执行不可靠的问题，提高了执行准确率和通用性能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态代理在图形用户界面(GUI)控制中无法可靠执行开关指令，尤其是当当前状态与目标状态相同时。这一问题严重限制了GUI控制代理的实际应用效果。

Method: 构建了一个包含二进制开关指令的状态控制测试集，并提出State-aware Reasoning (StaR)训练方法，教会代理感知当前开关状态、分析指令中的目标状态，并根据分析结果执行相应操作。

Result: 在三个多模态代理上的实验显示，StaR能够将开关指令执行准确率提高30%以上。在公开测试集上的进一步评估显示，StaR还能提升通用任务性能。动态环境评测显示其在实际应用中具有潜力。

Conclusion: StaR方法有效解决了GUI控制代理在开关指令执行上的可靠性问题，显著提高了执行准确率和通用性能力，为实际应用提供了有效解决方案。

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [28] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: InfraMind是一个专门为工业管理系统设计的基于探索的GUI代理框架，通过五个创新模块解决LLM-based GUI代理在工业管理中的五大挑战，显著提升了任务成功率和操作效率。


<details>
  <summary>Details</summary>
Motivation: 工业基础设施管理面临系统复杂性增加、多供应商集成和专家操作员短缺等挑战。传统RPA自动化灵活性有限且维护成本高，而通用LLM-based GUI代理在工业管理中存在元素理解、精度效率、状态定位、部署约束和安全需求等五大问题。

Method: 提出InfraMind框架，包含五个核心模块：(1)基于系统搜索探索和虚拟机快照的自主GUI理解；(2)内存驱动规划确保高精度高效执行；(3)高级状态识别用于分层界面中的鲁棒定位；(4)结构化知识蒸馏实现轻量模型高效部署；(5)多层安全机制保护敏感操作。

Result: 在开源和商业DCIM平台上的大量实验表明，该方法在任务成功率和操作效率方面持续优于现有框架。

Conclusion: InfraMind为工业管理自动化提供了一个严谨且可扩展的解决方案，有效解决了LLM-based GUI代理在工业环境中的关键挑战。

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [29] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR是一个通过强化学习集成外部工具的层次优化框架，解决了LLM在数学推理中工具集成的三大挑战：数据构建、细粒度优化和推理增强，在多个数学和代码基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面取得显著进展，但在高精度任务（如数值计算和符号操作）上仍然存在困难，需要集成外部工具来弥补这一差距

Method: 提出THOR框架：1) TIRGen多智能体管道构建高质量工具集成推理数据集；2) 分层强化学习策略联合优化轨迹级问题解决和步骤级代码生成；3) 推理时利用工具反馈的自校正机制动态修正错误推理路径

Result: 该方法在不同模型上表现出强泛化能力，在推理和非推理模型中都有效，在多个数学基准测试中达到同类规模模型的最先进性能，并在代码基准测试中实现持续改进

Conclusion: THOR通过创新的工具集成和分层优化方法，成功解决了LLM在高精度数学任务中的关键挑战，为工具增强的推理提供了有效的解决方案

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [30] [MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation](https://arxiv.org/abs/2509.13773)
*Zhipeng Bian,Jieming Zhu,Xuyang Xie,Quanyu Dai,Zhou Zhao,Zhenhua Dong*

Main category: cs.AI

TL;DR: MIRA是一个智能手机AI任务指令推荐框架，通过长按图像或文本来推荐上下文相关的AI任务指令，使用多模态大语言模型和结构化推理来提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的快速发展，智能手机需要更直观的方式来访问预定义的AI服务，简化用户与设备的交互。

Method: 1) 基于多模态大语言模型的推荐管道，进行结构化推理提取关键实体和推断用户意图；2) 模板增强推理机制整合高层推理模板；3) 基于前缀树的约束解码策略限制输出到预定义指令候选。

Result: 通过真实世界标注数据集和用户研究评估，MIRA在指令推荐准确性方面表现出显著提升。

Conclusion: MIRA有潜力彻底改变用户在智能手机上与AI服务的交互方式，提供更无缝和高效的体验。

Abstract: The rapid advancement of generative AI technologies is driving the
integration of diverse AI-powered services into smartphones, transforming how
users interact with their devices. To simplify access to predefined AI
services, this paper introduces MIRA, a pioneering framework for task
instruction recommendation that enables intuitive one-touch AI tasking on
smartphones. With MIRA, users can long-press on images or text objects to
receive contextually relevant instruction recommendations for executing AI
tasks. Our work introduces three key innovations: 1) A multimodal large
language model (MLLM)-based recommendation pipeline with structured reasoning
to extract key entities, infer user intent, and generate precise instructions;
2) A template-augmented reasoning mechanism that integrates high-level
reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based
constrained decoding strategy that restricts outputs to predefined instruction
candidates, ensuring coherent and intent-aligned suggestions. Through
evaluation using a real-world annotated datasets and a user study, MIRA has
demonstrated substantial improvements in the accuracy of instruction
recommendation. The encouraging results highlight MIRA's potential to
revolutionize the way users engage with AI services on their smartphones,
offering a more seamless and efficient experience.

</details>


### [31] [An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques](https://arxiv.org/abs/2509.13880)
*Mingwei Zhang,Zhenhao Gu,Liangda Fang,Cunjing Ge,Ziliang Chen,Zhao-Rong Lai,Quanlong Guan*

Main category: cs.AI

TL;DR: 提出了一种基于DPLL架构的精确方法来解决整数线性约束的模型计数问题，整合了混合整数规划中的简化技术，在随机基准和应用基准测试中均显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 线性约束是计算机科学、运筹学和优化等领域的基础约束，许多应用问题可归结为整数线性约束的模型计数问题(MCILC)，需要高效的精确求解方法

Method: 基于穷举DPLL架构设计精确方法，整合混合整数规划中的有效简化技术来提高效率

Result: 在2840个随机基准和4131个应用基准测试中，新方法解决了1718个随机实例(现有最佳方法为1470个)，并且是唯一能解决所有4131个应用实例的方法

Conclusion: 该方法在整数线性约束模型计数问题上显著优于现有精确方法，特别是在应用实例上表现出色

Abstract: Linear constraints are one of the most fundamental constraints in fields such
as computer science, operations research and optimization. Many applications
reduce to the task of model counting over integer linear constraints (MCILC).
In this paper, we design an exact approach to MCILC based on an exhaustive DPLL
architecture. To improve the efficiency, we integrate several effective
simplification techniques from mixed integer programming into the architecture.
We compare our approach to state-of-the-art MCILC counters and propositional
model counters on 2840 random and 4131 application benchmarks. Experimental
results show that our approach significantly outperforms all exact methods in
random benchmarks solving 1718 instances while the state-of-the-art approach
only computes 1470 instances. In addition, our approach is the only approach to
solve all 4131 application instances.

</details>


### [32] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 该研究使用人工神经网络模型探讨信息流结构变化是否能带来认知性能的过渡性变化，发现循环网络相比前馈网络在处理复杂语法时具有质的性能提升，并观察到训练难度形成的过渡障碍。


<details>
  <summary>Details</summary>
Motivation: 探索认知进化是否通过一系列主要过渡来实现，这些过渡操纵生物神经网络结构并根本改变信息流，从而产生认知性能的质的飞跃。

Method: 使用理想化的信息流模型和人工神经网络（ANNs），比较前馈、循环和分层拓扑结构的网络，测试它们在学习不同复杂度人工语法时的性能，控制网络大小和资源。

Result: 循环网络相比前馈网络能够处理更多类型的输入，在最复杂语法学习上表现出质的性能提升。循环网络的训练难度形成了过渡障碍和偶然不可逆性。分层网络在语法学习任务中并未优于非分层网络。

Conclusion: 某些信息流结构的变化确实能够产生认知性能的过渡性变化，支持认知进化可能通过主要过渡实现的假设。

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [33] [CrowdAgent: Multi-Agent Managed Multi-Source Annotation System](https://arxiv.org/abs/2509.14030)
*Maosheng Qin,Renyu Zhu,Mingxuan Xia,Chenkai Chen,Zhen Zhu,Minmin Lin,Junbo Zhao,Lu Xu,Changjie Fan,Runze Wu,Haobo Wang*

Main category: cs.AI

TL;DR: CrowdAgent是一个多智能体系统，提供端到端的流程控制，整合任务分配、数据标注和质量/成本管理，实现LLMs、SLMs和人类专家的协同标注工作流。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注标注步骤本身，缺乏对多样化标注源（LLMs、SLMs、人类专家）的动态管理和复杂调度与质量成本权衡的统一处理。

Method: 采用多智能体系统，实现任务分配、数据标注和质量/成本管理的端到端流程控制，通过理性任务分配使不同标注源协同工作。

Result: 在六个多样化多模态分类任务上的广泛实验证明了CrowdAgent的有效性。

Conclusion: CrowdAgent为管理多样化标注源提供了统一的端到端解决方案，解决了复杂调度和质量成本权衡问题。

Abstract: High-quality annotated data is a cornerstone of modern Natural Language
Processing (NLP). While recent methods begin to leverage diverse annotation
sources-including Large Language Models (LLMs), Small Language Models (SLMs),
and human experts-they often focus narrowly on the labeling step itself. A
critical gap remains in the holistic process control required to manage these
sources dynamically, addressing complex scheduling and quality-cost trade-offs
in a unified manner. Inspired by real-world crowdsourcing companies, we
introduce CrowdAgent, a multi-agent system that provides end-to-end process
control by integrating task assignment, data annotation, and quality/cost
management. It implements a novel methodology that rationally assigns tasks,
enabling LLMs, SLMs, and human experts to advance synergistically in a
collaborative annotation workflow. We demonstrate the effectiveness of
CrowdAgent through extensive experiments on six diverse multimodal
classification tasks. The source code and video demo are available at
https://github.com/QMMMS/CrowdAgent.

</details>


### [34] [Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning](https://arxiv.org/abs/2509.14195)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 本文通过分层架构（GCN作为一阶学习器，MLP作为二阶学习器）实证验证了二阶学习促进环境-认知同构性形成的假设，证明当认知系统发展出与环境结构同构的心理地图时，二阶学习效果最佳。


<details>
  <summary>Details</summary>
Motivation: 研究心理表征（结构化内部模型反映外部环境）对高级认知的重要性，但现有方法难以实证研究。现有理论假设二阶学习（适应一阶学习的学习机制）能够促进环境-认知同构性的出现。

Method: 提出分层架构：使用图卷积网络（GCN）作为一阶学习器直接映射节点特征到最优导航路径预测，使用MLP控制器作为二阶学习器在遇到结构新颖的迷宫环境时动态调整GCN参数。

Result: 定量和定性结果显示，当认知系统发展出与环境结构同构的内部心理地图时，二阶学习特别有效，在未见过的迷宫任务上表现出显著的性能提升和强大的泛化能力。

Conclusion: 研究为结构化心理表征在最大化二阶学习效果中的关键作用提供了实证支持，验证了二阶学习促进环境-认知同构性形成的理论假设。

Abstract: Mental representation, characterized by structured internal models mirroring
external environments, is fundamental to advanced cognition but remains
challenging to investigate empirically. Existing theory hypothesizes that
second-order learning -- learning mechanisms that adapt first-order learning
(i.e., learning about the task/domain) -- promotes the emergence of such
environment-cognition isomorphism. In this paper, we empirically validate this
hypothesis by proposing a hierarchical architecture comprising a Graph
Convolutional Network (GCN) as a first-order learner and an MLP controller as a
second-order learner. The GCN directly maps node-level features to predictions
of optimal navigation paths, while the MLP dynamically adapts the GCN's
parameters when confronting structurally novel maze environments. We
demonstrate that second-order learning is particularly effective when the
cognitive system develops an internal mental map structurally isomorphic to the
environment. Quantitative and qualitative results highlight significant
performance improvements and robust generalization on unseen maze tasks,
providing empirical support for the pivotal role of structured mental
representations in maximizing the effectiveness of second-order learning.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [35] [Efficient STAR-RIS Mode for Energy Minimization in WPT-FL Networks with NOMA](https://arxiv.org/abs/2509.13441)
*MohammadHossien Alishahi,Ming Zeng,Paul Fortier,Omer Waqar,Muhammad Hanif,Dinh Thai Hoang,Diep N. Nguyen,Quoc-Viet Pham*

Main category: cs.IT

TL;DR: 本文研究在6G IoT网络中通过STAR-RIS、WPT和FL技术的联合优化，以最小化能消耗。提出了一种解决非凸优化问题的分解策略，通过坐标递渝和一维搜索等算法实现了高效的质量控制。


<details>
  <summary>Details</summary>
Motivation: 解决6G IoT网络中的大规模通信开销、覆盖限制和电池寿命问题，通过STAR-RIS、WPT和FL技术的联合优化实现可持续、低延迟和高能效的通信。

Method: 提出一种联合优化方法，将非凸能消耗最小化问题分解为两个子问题：第一个通过坐标递渝优化STAR-RIS相位移向量和波束成型矩阵，第二个通过一维搜索或二分法分配时间、功率和计算频率。

Result: 方法能够高效处理非凸优化问题，通过坐标递渝和一维搜索等算法实现了质量控制，为大规模IoT网络提供了可行的能消耗优化方案。

Conclusion: 该研究为6G IoT网络提供了一种通过STAR-RIS、WPT和FL技术联合优化来最小化能消耗的方法，有效解决了大规模通信中的能消耗问题，具有重要的实践意义。

Abstract: With the massive deployment of IoT devices in 6G networks, several critical
challenges have emerged, such as large communication overhead, coverage
limitations, and limited battery lifespan. FL, WPT, multi-antenna AP, and RIS
can mitigate these challenges by reducing the need for large data
transmissions, enabling sustainable energy harvesting, and optimizing the
propagation environment. Compared to conventional RIS, STAR-RIS not only
extends coverage from half-space to full-space but also improves energy saving
through appropriate mode selection. Motivated by the need for sustainable,
low-latency, and energy-efficient communication in large-scale IoT networks,
this paper investigates the efficient STAR-RIS mode in the uplink and downlink
phases of a WPT-FL multi-antenna AP network with non-orthogonal multiple access
to minimize energy consumption, a joint optimization that remains largely
unexplored in existing works on RIS or STAR-RIS. We formulate a non-convex
energy minimization problem for different STAR-RIS modes, i.e., energy
splitting (ES) and time switching (TS), in both uplink and downlink
transmission phases, where STAR-RIS phase shift vectors, beamforming matrices,
time and power for harvesting, uplink transmission, and downlink transmission,
local processing time, and computation frequency for each user are jointly
optimized. To tackle the non-convexity, the problem is decoupled into two
subproblems: the first subproblem optimizes STAR-RIS phase shift vectors and
beamforming matrices across all WPT-FL phases using block coordinate descent
over either semi-definite programming or Rayleigh quotient problems, while the
second one allocates time, power, and computation frequency via the
one-dimensional search algorithms or the bisection algorithm.

</details>


### [36] [Uplink-Downlink Duality for Beamforming in Integrated Sensing and Communications](https://arxiv.org/abs/2509.13661)
*Kareem M. Attiah,Wei Yu*

Main category: cs.IT

TL;DR: 本文研究集成感知通信(ISAC)中的波束形成和功率优化问题，通过最小化贝叶斯拉勒约束下界(BCRB)来估计参数的均方误差，同时满足下行链路信干比约束。


<details>
  <summary>Details</summary>
Motivation: 解决集成感知通信中的关键技术挑战，在使用通信信号进行同时感知时，如何优化波束形成和功率分配以同时满足感知性能和通信需求。

Method: 提出了两个核心新要素：1) 证明BCRB最小化问题对应于在感知关注方向上最大化波束形成功率；2) 将经典的MIMO上下行双向性扩展到ISAC场景，但需要处理负噪声功率和额外的上行链波束形成器条件。

Result: 新的双向性理论为ISAC的功率和波束形成器优化开启了高效迭代算法的可能性。

Conclusion: 该研究为ISAC系统提供了一种有效的优化框架，通过新的双向性理论和算法实现，能够同时优化感知性能和通信性能。

Abstract: This paper considers the beamforming and power optimization problem for a
class of integrated sensing and communications (ISAC) problems that utilize the
communication signals simultaneously for sensing. We formulate the problem of
minimizing the Bayesian Cram\'er-Rao bound (BCRB) on the mean-squared error of
estimating a vector of parameters, while satisfying downlink
signal-to-interference-and-noise-ratio constraints for a set of communication
users at the same time. The proposed optimization framework comprises two key
new ingredients. First, we show that the BCRB minimization problem corresponds
to maximizing beamforming power along certain sensing directions of interest.
Second, the classical uplink-downlink duality for multiple-input
multiple-output communications can be extended to the ISAC setting, but unlike
the classical communication problem, the dual uplink problem for ISAC may
entail negative noise power and needs to include an extra condition on the
uplink beamformers. This new duality theory opens doors for an efficient
iterative algorithm for optimizing power and beamformers for ISAC.

</details>


### [37] [Clustering Strategies in Satellite-Aided Communications](https://arxiv.org/abs/2509.13701)
*Tam Ninh Thi-Thanh,Nguyen Minh Quan,Do Son Tung,Trinh Van Chien,Hung Tran*

Main category: cs.IT

TL;DR: 这篇论文给出了下一代卫星网络中基于机器学习和吨测算法的现代聚类方法综述，实验结果显示改进的机器学习技术和图论方法比传统聚类方法更优秀


<details>
  <summary>Details</summary>
Motivation: 解决下一代卫星网络中的聚类任务、用户分组和高效链路管理问题，以优化网络性能和减少干扰

Method: 给出了基于机器学习和吨测算法的现代聚类方法的综述性概述

Result: 改进的机器学习技术和图论基础方法在性能和可扩展性方面显著更优，特别是在大规模卫星网络场景中

Conclusion: 提出了未来研究方向和集成多维解决方案，以提高未来卫星通信的适应性和效率

Abstract: With the rapid advancement of next-generation satellite networks, addressing
clustering tasks, user grouping, and efficient link management has become
increasingly critical to optimize network performance and reduce interference.
In this paper, we provide a comprehensive overview of modern clustering
approaches based on machine learning and heuristic algorithms. The experimental
results indicate that improved machine learning techniques and graph
theory-based methods deliver significantly better performance and scalability
than conventional clustering methods, such as the pure clustering algorithm
examined in previous research. These advantages are especially evident in
large-scale satellite network scenarios. Furthermore, the paper outlines
potential research directions and discusses integrated, multi-dimensional
solutions to enhance adaptability and efficiency in future satellite
communication.

</details>


### [38] [Asymptotic Analysis of Nonlinear One-Bit Precoding in Massive MIMO Systems via Approximate Message Passing](https://arxiv.org/abs/2509.13955)
*Zheyu Wu,Junjie Ma,Ya-Feng Liu,Bruno Clerckx*

Main category: cs.IT

TL;DR: 本文分析了一比特大规模MIMO系统中非线性符号级预编码的凸松弛量化方法，基于AMP框架推导了高维渐近性能的闭式符号错误概率表达式，证明了ℓ∞²正则化器在混合正则化函数中的最优性。


<details>
  <summary>Details</summary>
Motivation: 一比特数模转换器的大规模MIMO系统具有硬件效率优势，但一比特约束使预编码设计变为离散非凸优化问题，需要分析凸松弛量化方法的性能。

Method: 采用"凸松弛后量化"方法：先求解离散最小均方误差预编码问题的凸松弛，然后量化解以满足一比特约束；基于近似消息传递(AMP)建立分析框架，推导高维渐近性能。

Result: 在大型系统极限下推导出接收端符号错误概率(SEP)的闭式表达式，实证表明ℓ∞²正则化器在最优正则化参数下在广泛凸正则化函数类中达到最优SEP性能。

Conclusion: 证明了ℓ∞²正则化器在混合ℓ∞²-ℓ₂²正则化函数中的最优性，为凸松弛量化方法在一比特大规模MIMO预编码中的性能提供了理论分析框架。

Abstract: Massive multiple-input multiple-output (MIMO) systems employing one-bit
digital-to-analog converters offer a hardware-efficient solution for wireless
communications. However, the one-bit constraint poses significant challenges
for precoding design, as it transforms the problem into a discrete and
nonconvex optimization task. In this paper, we investigate a widely adopted
``convex-relaxation-then-quantization" approach for nonlinear symbol-level
one-bit precoding. Specifically, we first solve a convex relaxation of the
discrete minimum mean square error precoding problem, and then quantize the
solution to satisfy the one-bit constraint. To analyze the high-dimensional
asymptotic performance of this scheme, we develop a novel analytical framework
based on approximate message passing (AMP). This framework enables us to derive
a closed-form expression for the symbol error probability (SEP) at the receiver
side in the large-system limit, which provides a quantitative characterization
of how model and system parameters affect the SEP performance. Our empirical
results suggest that the $\ell_\infty^2$ regularizer, when paired with an
optimally chosen regularization parameter, achieves optimal SEP performance
within a broad class of convex regularization functions. As a first step
towards a theoretical justification, we prove the optimality of the
$\ell_\infty^2$ regularizer within the mixed $\ell_\infty^2$-$\ell_2^2$
regularization functions.

</details>


### [39] [Deriving Moments in the Age of Gossip Process from Percolation](https://arxiv.org/abs/2509.13981)
*Thomas Jacob Maranzatto,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文基于AoI与首次通过渗流理论的新联系，重新推导了谣言网络中信息年龄过程任意k阶矩的基本递归恒等式，提供了更简洁的概率证明方法


<details>
  <summary>Details</summary>
Motivation: 研究谣言网络中信息年龄(AoI)的基本恒等式，利用AoI与首次通过渗流理论的新联系来简化证明过程

Method: 基于AoI与首次通过渗流理论的联系，使用概率论基本事实推导k阶矩的递归恒等式，推广了统计物理学中的Eden模型相关技术

Result: 成功恢复了任意k阶矩的递归恒等式，证明过程比现有方法更简洁易懂

Conclusion: 该方法不仅建立了与渗流理论的联系，还提供了更简洁的概率证明框架，对统计物理学中的Eden模型也有启示意义

Abstract: This paper concerns fundamental identities in the study of age of information
(AoI) in gossip networks. We recover known recursive identities for arbitrary
kth moments of the age process based on the recent connection between AoI and
first passage percolation. Apart from the connection to percolation, our proofs
are more concise and can be followed using only elementary facts from
probability. Our argument generalizes some techniques known in the statistical
physics community, and we remark on connections to the Eden model.

</details>
