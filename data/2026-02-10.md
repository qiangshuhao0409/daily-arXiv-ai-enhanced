<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 15]
- [cs.AI](#cs.AI) [Total: 113]
- [cs.IT](#cs.IT) [Total: 22]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Performance Evaluation of V2X Communication Using Large-Scale Traffic Data](https://arxiv.org/abs/2602.07244)
*John Pravin Arockiasamy,Alexey Vinel*

Main category: cs.NI

TL;DR: 基于真实交通数据的大规模V2X通信性能评估，揭示交通密度、移动模式和通信范围对V2X性能的影响，发现合成交通假设可能高估信道拥塞


<details>
  <summary>Details</summary>
Motivation: V2X技术是协同自动驾驶的关键，但大规模实际部署有限。现有性能评估多依赖仿真器生成的合成交通场景，可能无法完全反映真实交通特征，因此需要基于真实交通数据的大规模评估

Method: 使用HighD和InD真实交通数据集，将车辆轨迹转换为仿真就绪格式，结合标准化V2X网络协议栈，对包含数十万辆车的整个交通群体进行消息级性能分析

Result: 评估了V2X关键性能指标（生成间隔、包间隔、包交付率、信道繁忙率），发现在真实交通条件下协同感知服务在大规模场景下仍然可行。交通密度、移动模式和通信范围显著影响V2X性能，合成交通假设可能高估信道拥塞

Conclusion: 基于真实交通数据的大规模V2X性能评估表明，协同感知服务在真实交通条件下具有可行性，同时揭示了合成交通假设的局限性，为V2X系统设计和部署提供了重要参考

Abstract: Vehicular communication (V2X) technologies are widely regarded as a cornerstone for cooperative and automated driving, yet their large-scale real-world deployment remains limited. As a result, understanding V2X performance under realistic, full-scale traffic conditions continues to be relevant. Most existing performance evaluations rely on synthetic traffic scenarios generated by simulators, which, while useful, may not fully capture the features of real-world traffic. In this paper, we present a large-scale, data-driven evaluation of V2X communication performance using real-world traffic datasets. Vehicle trajectories derived from the Highway Drone (HighD) and Intersection Drone (InD) datasets are converted into simulation-ready formats and coupled with a standardized V2X networking stack to enable message-level performance analysis for entire traffic populations comprising over hundred thousands vehicles across multiple locations. We evaluate key V2X performance indicators, including inter-generation gap, inter-packet gap, packet delivery ratio, and channel busy ratio, across both highway and urban intersection environments. Our results show that cooperative awareness services remain feasible at scale under realistic traffic conditions. In addition, the findings highlight how traffic density, mobility patterns, and communication range influence V2X performance and how synthetic traffic assumptions may overestimate channel congestion.

</details>


### [2] [Mirage: Transmitting a Video as a Perceptual Illusion for 50,000X Speedup](https://arxiv.org/abs/2602.07396)
*Junjie Wu,Tianrui Li,Yi Zhang,Ziyuan Yang*

Main category: cs.NI

TL;DR: Mirage提出了一种免视觉数据的视频通信框架，通过分解视频为时空语义表示，利用生成模型重建视频，实现高效传输和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 传统视频通信框架以信号级保真为目标，导致高通信开销和系统复杂度。现有主流框架依赖传输视觉数据本身，造成显著带宽消耗，需要更高效的传输方案。

Method: 将视频内容分解为两个互补组件：捕捉运动动态的时间序列信息（通过视频描述保留）和描述整体视觉结构的空间外观表示（关键帧编码为紧凑语义表示）。接收端使用生成视频模型合成视频。

Result: Mirage在视频传输中实现了高达50000倍的数据级压缩加速，且增益随视频内容规模增大而提升。框架具有隐私保护特性，并支持跨部署场景的个性化适配。

Conclusion: Mirage通过免视觉数据的语义通信框架，在保持语义信息的同时实现了极高效的视频传输，为通信效率、隐私保护和系统灵活性提供了新的设计范式。

Abstract: The existing communication framework mainly aims at accurate reconstruction of source signals to ensure reliable transmission. However, this signal-level fidelity-oriented design often incurs high communication overhead and system complexity, particularly in video communication scenarios where mainstream frameworks rely on transmitting visual data itself, resulting in significant bandwidth consumption. To address this issue, we propose a visual data-free communication framework, Mirage, for extremely efficient video transmission while preserving semantic information. Mirage decomposes video content into two complementary components: temporal sequence information capturing motion dynamics and spatial appearance representations describing overall visual structure. Temporal information is preserved through video captioning, while key frames are encoded into compact semantic representations for spatial appearance. These representations are transmitted to the receiver, where videos are synthesized using generative video models. Since no raw visual data is transmitted, Mirage is inherently privacy-preserving. Mirage also supports personalized adaptation across deployment scenarios. The sender, network, and receiver can independently impose constraints on semantic representation, transmission, and generation, enabling flexible trade-offs between efficiency, privacy, control, and perceptual quality. Experimental results in video transmission demonstrate that Mirage achieves up to a 50000X data-level compression speedup over raw video transmission, with gains expected to scale with larger video content sizes.

</details>


### [3] [NOMA-Assisted Multi-BS MEC Networks for Delay-Sensitive and Computation-Intensive IoT Applications](https://arxiv.org/abs/2602.07456)
*Yuang Chen,Fengqian Guo,Chang Wu,Mingyu Peng,Hancheng Lu,Chang Wen Chen*

Main category: cs.NI

TL;DR: 提出一种基于NOMA的多基站移动边缘计算网络，通过联合任务卸载、用户分组和功率分配优化，显著降低大规模物联网场景下的系统总时延。


<details>
  <summary>Details</summary>
Motivation: 物联网大规模连接场景中，计算密集型任务对超低时延的需求日益增长，现有方案面临子信道访问不平衡、组间干扰、计算负载不均和设备异构性等问题。

Method: 1) 将任务卸载和用户分组建模为非合作博弈，提出基于精确势博弈的联合决策算法(EPG-JDM)；2) 提出基于主最小化(MM)的功率分配算法，将原问题转化为可处理的凸优化问题。

Result: 仿真实验表明，EPG-JDM算法在总时延和功耗方面分别比现有最优决策算法和经典启发式算法提升19.3%和14.7%。

Conclusion: 所提出的NOMA辅助多基站MEC网络能有效满足大规模物联网连接的计算密集型任务需求，通过联合优化显著降低了系统时延和功耗。

Abstract: The burgeoning and ubiquitous deployment of the Internet of Things (IoT) landscape struggles with ultra-low latency demands for computation-intensive tasks in massive connectivity scenarios. In this paper, we propose an innovative uplink non-orthogonal multiple access (NOMA)-assisted multi-base station (BS) mobile edge computing (BS-MEC) network tailored for massive IoT connectivity. To fulfill the quality-of-service (QoS) requirements of delay-sensitive and computation-intensive IoT applications, we formulate a joint task offloading, user grouping, and power allocation optimization problem with the overarching objective of minimizing the system's total delay, aiming to address issues of unbalanced subchannel access, inter-group interference, computational load disparities, and device heterogeneity. To effectively tackle this problem, we first reformulate task offloading and user grouping into a non-cooperative game model and propose an exact potential game-based joint decision-making (EPG-JDM) algorithm, which dynamically selects optimal task offloading and subchannel access decisions for each IoT device based on its channel conditions, thereby achieving the Nash Equilibrium. Then, we propose a majorization-minimization (MM)-based power allocation algorithm, which transforms the original subproblem into a tractable convex optimization paradigm. Extensive simulation experiments demonstrate that our proposed EPG-JDM algorithm significantly outperforms state-of-the-art decision-making algorithms and classic heuristic algorithms, yielding performance improvements of up to 19.3% and 14.7% in terms of total delay and power consumption, respectively.

</details>


### [4] [LEO Topology Design Under Real-World Deployment Constraints](https://arxiv.org/abs/2602.07756)
*Muaz Ali,Beichuan Zhang*

Main category: cs.NI

TL;DR: 提出了两种考虑实际部署约束的LEO网络拓扑设计方法（LSL和SA），相比现有方法显著降低了延迟、跳数和提高了网络容量。


<details>
  <summary>Details</summary>
Motivation: 现有LEO网络拓扑设计通常假设理想条件，未考虑实际部署动态（如部分星座部署、每日节点更替、链路可用性变化），导致不适用于真实LEO网络。

Method: 开发了两种拓扑设计方法：1) LSL方法：系统性地结合长距离捷径链路和短距离本地链路；2) SA方法：通过随机优化构建拓扑。两种方法都能通过增量更新处理每日节点更替。

Result: 使用3个月的Starlink数据评估，在完全部署和部分部署场景下，相比+Grid方法，实现了平均端到端延迟降低45%、跳数减少65%、网络容量提高2.3倍。

Conclusion: 提出的LSL和SA方法能够有效处理实际部署约束，通过增量更新维持良好网络性能，避免了昂贵的拓扑完全重建。

Abstract: The performance of large-scale Low-Earth-Orbit (LEO) networks, which consist of thousands of satellites interconnected by optical links, is dependent on its network topology. Existing topology designs often assume idealized conditions and do not account for real-world deployment dynamics, such as partial constellation deployment, daily node turnovers, and varying link availability, making them inapplicable to real LEO networks. In this paper, we develop two topology design methods that explicitly operate under real-world deployment constraints: the Long--Short Links (LSL) method, which systematically combines long-distance shortcut links with short-distance local links, and the Simulated Annealing (SA) method, which constructs topologies via stochastic optimization. Evaluated under both full deployment and partial deployment scenarios using 3-months of Starlink data, our methods achieve up to 45% lower average end-to-end delay, 65% fewer hops, and up to $2.3\times$ higher network capacity compared to +Grid. Both methods are designed to handle daily node turnovers by incrementally updating the topology, maintaining good network performance while avoiding costly full reconstruction of the topology.

</details>


### [5] [Interference Propagation Analysis for Large-Scale Multi-RIS-Empowered Wireless Communications:An Epidemiological Perspective](https://arxiv.org/abs/2602.07922)
*Kaining Wang,Xueyao Zhang,Bo Yang,Xuelin Cao,Qiang Cheng,Zhiwen Yu,Bin Guo,George C. Alexandropoulos,Kai-Kit Wong,Chan-Byoung Chae,Mérouane Debbah*

Main category: cs.NI

TL;DR: 该论文研究可重构智能表面(RIS)在用户移动性下的干扰传播问题，采用随机几何模型分析RIS的负面影响，提出覆盖概率和干扰传播强度等新概念，并使用流行病学模型描述干扰动态传播。


<details>
  <summary>Details</summary>
Motivation: 传统研究主要关注RIS的优势，而本文首次系统研究RIS在用户移动性下引起的干扰传播问题，分析RIS对无线系统的负面影响。

Method: 采用随机几何模型：基站和RIS位置使用Matérn硬核点过程建模，用户位置使用齐次泊松点过程建模。推导接收信号和干扰信号的功率分布闭式表达式，提出覆盖概率和干扰传播强度概念，并采用流行病学中的SIS模型描述干扰动态传播。

Result: 推导了接收信号和干扰信号的功率分布闭式表达式，提出了覆盖概率和干扰传播强度的新表达式，分析了影响干扰传播的关键因素，数值结果验证了理论分析。

Conclusion: RIS在用户移动性下会产生显著的干扰传播效应，需要有效管理大规模多RIS无线通信网络中的干扰传播，为网络设计提供重要指导。

Abstract: Reconfigurable intelligent surfaces (RISs) have gained significant attention in recent years due to their ability to control the reflection of radio-frequency signals and reshape the wireless propagation environment. Unlike traditional studies that primarily focus on the advantages of RISs, this paper examines the negative impacts of RISs by investigating interference propagation caused by user mobility in downlink wireless systems. We employ a stochastic geometric model to simulate the locations of base stations and RISs using the Matérn hard core point process, while user locations are modeled with the homogeneous Poisson point process. We derive novel closed-form expressions for the power distributions of the received signal at the users and the interfering signal. Additionally, we present a novel expression for coverage probability and introduce the concept of interference propagation intensity. To characterize the dynamics of interference caused by user mobility, we adopt an epidemiological approach using the susceptible-infected-susceptible model. Finally, crucial factors influencing the propagation of interference are analyzed. Numerical results validate our theoretical analysis and provide suggestions for managing interference propagation in large-scale multi-RIS wireless communication networks.

</details>


### [6] [Trajectory-Aware Multi-RIS Activation and Configuration: A Riemannian Diffusion Method](https://arxiv.org/abs/2602.07937)
*Kaining Wang,Bo Yang,Yusheng Lei,Zhibo Li,Zhiwen Yu,Xuelin Cao,Bin Guo,George C. Alexandropoulos,Dusit Niyato,Mérouane Debbah,Zhu Han*

Main category: cs.NI

TL;DR: 提出基于生成模型的多RIS控制框架，通过预测用户轨迹和干扰模式，联合优化RIS开关状态和相位配置，显著提升SINR性能。


<details>
  <summary>Details</summary>
Motivation: 可重构智能表面(RIS)虽然能增强无线覆盖，但其可编程反射可能无意中放大干扰，特别是在大规模多RIS移动通信场景中，密集的用户移动和频繁的视距重叠会严重降低信干噪比(SINR)。

Method: 1) 设计LSTM神经网络预测多用户轨迹，重建未来信道状态信息；2) 在环面上开发黎曼扩散模型生成几何一致的相位配置，反向扩散过程由强化学习动态引导；3) 通过比较RIS激活和去激活条件下的预测可达速率，严格推导超表面的最优开关状态。

Result: 仿真表明，该框架相比基于学习的控制方案实现高达30%的SINR提升，相比RIS始终开启方案获得高达44%的增益，在不同发射功率、RIS配置和干扰密度下均优于现有基线方法。

Conclusion: 提出的生成式多RIS控制框架能有效管理干扰，通过联合优化RIS开关状态和相位配置，显著提升大规模移动通信场景中的无线性能。

Abstract: Reconfigurable intelligent surfaces (RISs) offer a low-cost, energy-efficient means for enhancing wireless coverage. Yet, their inherently programmable reflections may unintentionally amplify interference, particularly in large-scale, multi-RIS-enabled mobile communication scenarios where dense user mobility and frequent line-of-sight overlaps can severely degrade the signal-to-interference-plus-noise ratio (SINR). To address this challenge, this paper presents a novel generative multi-RIS control framework that jointly optimizes the ON/OFF activation patterns of multiple RISs in the smart wireless environment and the phase configurations of the activated RISs based on predictions of multi-user trajectories and interference patterns. We specially design a long short-term memory (LSTM) artificial neural network, enriched with speed and heading features, to forecast multi-user trajectories, thereby enabling reconstruction of future channel state information. To overcome the highly nonconvex nature of the multi-RIS control problem, we develop a Riemannian diffusion model on the torus to generate geometry-consistent phase-configuration, where the reverse diffusion process is dynamically guided by reinforcement learning. We then rigorously derive the optimal ON/OFF states of the metasurfaces by comparing predicted achievable rates under RIS activation and deactivation conditions. Extensive simulations demonstrate that the proposed framework achieves up to 30\% SINR improvement over learning-based control and up to 44\% gain compared with the RIS always-on scheme, while consistently outperforming state-of-the-art baselines across different transmit powers, RIS configurations, and interference densities.

</details>


### [7] [DHEA-MECD: An Embodied Intelligence-Powered DRL Algorithm for AUV Tracking in Underwater Environments with High-Dimensional Features](https://arxiv.org/abs/2602.07947)
*Kai Tian,Chuan Lin,Guangjie Han,Chen An,Qian Zhu,Shengzhao Zhu,Zhenyu Wang*

Main category: cs.NI

TL;DR: 提出分层具身智能架构和DHEA-MECD深度强化学习算法，用于AUV在复杂水下环境中的多目标跟踪


<details>
  <summary>Details</summary>
Motivation: AUV在复杂水下环境中的多目标跟踪面临高维特征、空间约束、时变干扰等挑战，需要更有效的解决方案

Method: 提出分层具身智能架构，设计DHEA-MECD算法：双头编码器-注意力信息提取框架分解感知数据并建模异质特征依赖；基于Top-k专家选择策略的运动阶段感知多专家协同决策机制

Result: 相比主流DRL方法，在复杂干扰丰富的海洋环境中实现了更高的跟踪成功率、更快的收敛速度和改进的运动最优性

Conclusion: 提出的DHEA-MECD算法能够实现AUV智能、稳定、抗干扰的多目标跟踪，在复杂水下环境中表现出优越性能

Abstract: In recent years, autonomous underwater vehicle (AUV) systems have demonstrated significant potential in complex marine exploration. However, effective AUV-based tracking remains challenging in realistic underwater environments characterized by high-dimensional features, including coupled kinematic states, spatial constraints, time-varying environmental disturbances, etc. To address these challenges, this paper proposes a hierarchical embodied-intelligence (EI) architecture for underwater multi-target tracking with AUVs in complex underwater environments. Built upon this architecture, we introduce the Double-Head Encoder-Attention-based Multi-Expert Collaborative Decision (DHEA-MECD), a novel Deep Reinforcement Learning (DRL) algorithm designed to support efficient and robust multi-target tracking. Specifically, in DHEA-MECD, a Double-Head Encoder-Attention-based information extraction framework is designed to semantically decompose raw sensory observations and explicitly model complex dependencies among heterogeneous features, including spatial configurations, kinematic states, structural constraints, and stochastic perturbations. On this basis, a motion-stage-aware multi-expert collaborative decision mechanism with Top-k expert selection strategy is introduced to support stage-adaptive decision-making. Furthermore, we propose the DHEA-MECD-based underwater multitarget tracking algorithm to enable AUV smart, stable, and anti-interference multi-target tracking. Extensive experimental results demonstrate that the proposed approach achieves superior tracking success rates, faster convergence, and improved motion optimality compared with mainstream DRL-based methods, particularly in complex and disturbance-rich marine environments.

</details>


### [8] [NeuroScaler: Towards Energy-Optimal Autoscaling for Container-Based Services](https://arxiv.org/abs/2602.08191)
*Alisson O. Chaves,Rodrigo Moreira,Larissa F. Rodrigues Moreira,Joao Correia,David Santos,Rui Silva,Tiago Barros,Daniel Corujo,Miguel Rocha,Flavio de Oliveira Silva*

Main category: cs.NI

TL;DR: NeuroScaler是一个AI原生的、节能的、碳感知的云边网络编排器，通过多层级遥测和模型预测控制，在满足服务目标的同时减少能耗34.68%


<details>
  <summary>Details</summary>
Motivation: 未来网络需要在严格的能源和碳约束下运行，而当前的自动扩缩机制仍以工作负载为中心、基础设施孤立，且对环境影响缺乏认知

Method: NeuroScaler聚合从PDU到裸机服务器再到Kubernetes容器虚拟化基础设施的多层级遥测数据，使用不同层级的能源和计算指标，支持多个连接负载、性能和功耗的机器学习管道，在统一的可观测层中采用模型预测控制策略优化能源使用

Result: 在实际测试平台中，NeuroScaler相比水平Pod自动扩缩器(HPA)减少了34.68%的能耗，同时保持目标延迟

Conclusion: NeuroScaler展示了AI原生、能源高效、碳感知的编排器在绿色云边网络中的有效性，能够在满足服务级别目标的同时显著降低能源消耗

Abstract: Future networks must meet stringent requirements while operating within tight energy and carbon constraints. Current autoscaling mechanisms remain workload-centric and infrastructure-siloed, and are largely unaware of their environmental impact. We present NeuroScaler, an AI-native, energy-efficient, and carbon-aware orchestrator for green cloud and edge networks. NeuroScaler aggregates multi-tier telemetry, from Power Distribution Units (PDUs) through bare-metal servers to virtualized infrastructure with containers managed by Kubernetes, using distinct energy and computing metrics at each tier. It supports several machine learning pipelines that link load, performance, and power. Within this unified observability layer, a model-predictive control policy optimizes energy use while meeting service-level objectives. In a real testbed with production-grade servers supporting real services, NeuroScaler reduces energy consumption by 34.68% compared to the Horizontal Pod Autoscaler (HPA) while maintaining target latency.

</details>


### [9] [MonkeyTree: Near-Minimal Congestion for Multi-tenant Training via Migration](https://arxiv.org/abs/2602.08296)
*Anton A. Zabreyko,Weiyang Wang,Manya Ghobadi*

Main category: cs.NI

TL;DR: MonkeyTree通过作业迁移而非网络层技术来缓解多租户GPU集群中的网络拥塞，利用ML训练流量的特性实现无拥塞放置，显著提升作业完成时间。


<details>
  <summary>Details</summary>
Motivation: 云运营商在共享、过载的网络上共置ML训练作业时，拥塞会降低超过三分之一作业的训练吞吐量。现有方法要么依赖路由和流调度（在流量超过容量时有根本限制），要么需要昂贵的全二分带宽拓扑。

Method: MonkeyTree利用ML训练流量的特性：基于环的集合通信在每个作业跨越的机架中只产生一个跨机架流，使得无拥塞放置成为可能。系统将碎片整理建模为整数线性规划，最小化工作节点移动，同时满足每个机架的碎片限制。通过RDMA实现内存检查点恢复迁移。

Result: 在2048个H200 GPU集群的模拟中，MonkeyTree在1024个GPU、4:1过载比下将平均作业完成时间比最佳基线提升14%。在16:1高过载比和2048个GPU下，p99作业完成时间保持在理想值的5%以内。

Conclusion: MonkeyTree首次通过作业迁移而非网络层技术缓解GPU集群拥塞，利用ML训练流量的稀疏约束结构实现高效碎片整理，显著提升集群性能，特别适合高过载比的大规模部署。

Abstract: We present MonkeyTree, the first system to mitigate network congestion in multi-tenant GPU clusters through job-migration based defragmentation rather than network-layer techniques. As cloud operators co-locate ML training jobs on shared, oversubscribed networks, congestion degrades training throughput for over a third of jobs. Prior approaches either rely on routing and flow scheduling--which we show have fundamental limits when traffic exceeds capacity--or require costly full-bisection bandwidth topologies with packet spraying.
  MonkeyTree exploits characteristics of ML training traffic: ring-based collectives generate exactly one cross-rack flow per rack a job spans, making congestion-free placements achievable. The sparse constraint structure admits abundant valid configurations, making them easy to reach with few migrations. Once reached, low fragmentation is self-reinforcing, as new arrivals disturb only a few racks. MonkeyTree formulates defragmentation as an integer linear program that minimizes worker movements, subject to per-rack fragmentation bounds. We prove a tight bound showing any placement can be defragmented to at most two cross-rack fragments per ToR, and extend the formulation to hybrid parallelism with multiple rings per server. Migration is implemented via in-memory checkpoint-and-restore over RDMA, incurring only 9.02 seconds of system overhead end-to-end per worker. We evaluate MonkeyTree using a custom simulator modeling clusters of up to 2,048 H200 GPUs and prototype on a five-node A100 testbed. MonkeyTree improves average job completion time by 14 percent over the next best baseline on a cluster of 1,024 GPUs with a 4:1 oversubscription. With a high 16:1 oversubscription ratio and 2,048 GPUs, MonkeyTree keeps p99 job completion time within 5 percent of ideal.

</details>


### [10] [PACC: Protocol-Aware Cross-Layer Compression for Compact Network Traffic Representation](https://arxiv.org/abs/2602.08331)
*Zhaochen Guo,Tianyufei Zhou,Honghao Wang,Ronghua Li,Shinan Liu*

Main category: cs.NI

TL;DR: PACC提出了一种冗余感知、分层感知的网络流量表示框架，通过分解协议栈为共享和私有组件，在加密流量分类、IoT设备识别和入侵检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 网络流量分类面临加密和协议演变的挑战。现有方法存在局限性：手工特征损失信息多，原始比特编码成本高，预训练嵌入会扁平化协议栈并跨层纠缠信号。真实流量在跨层和层内都存在大量冗余，现有范式未能显式识别和消除这些冗余，导致容量浪费、捷径学习和泛化能力下降。

Method: PACC将协议栈视为多视图输入，学习紧凑的分层投影，将表示分解为共享（跨层）和私有（层特定）组件。通过联合目标实现：通过重建保留层特定信息，通过对比互信息学习捕获共享结构，通过监督损失最大化任务相关信息，生成适用于高效推理的紧凑潜在表示。

Result: 在加密应用分类、IoT设备识别和入侵检测数据集上，PACC始终优于特征工程和原始比特基线。在加密子集上，比nPrint准确率提升高达12.9%。PACC匹配或超越强基础模型基线，同时端到端效率提升高达3.16倍。

Conclusion: PACC通过显式建模协议栈的冗余和分层结构，提供了一种更有效、更高效的网络流量表示方法，在保持准确性的同时显著提升效率，为加密流量分析提供了有前景的解决方案。

Abstract: Network traffic classification is a core primitive for network security and management, yet it is increasingly challenged by pervasive encryption and evolving protocols. A central bottleneck is representation: hand-crafted flow statistics are efficient but often too lossy, raw-bit encodings can be accurate but are costly, and recent pre-trained embeddings provide transfer but frequently flatten the protocol stack and entangle signals across layers. We observe that real traffic contains substantial redundancy both across network layers and within each layer; existing paradigms do not explicitly identify and remove this redundancy, leading to wasted capacity, shortcut learning, and degraded generalization. To address this, we propose PACC, a redundancy-aware, layer-aware representation framework. PACC treats the protocol stack as multi-view inputs and learns compact layer-wise projections that remain faithful to each layer while explicitly factorizing representations into shared (cross-layer) and private (layer-specific) components. We operationalize these goals with a joint objective that preserves layer-specific information via reconstruction, captures shared structure via contrastive mutual-information learning, and maximizes task-relevant information via supervised losses, yielding compact latents suitable for efficient inference. Across datasets covering encrypted application classification, IoT device identification, and intrusion detection, PACC consistently outperforms feature-engineered and raw-bit baselines. On encrypted subsets, it achieves up to a 12.9% accuracy improvement over nPrint. PACC matches or surpasses strong foundation-model baselines. At the same time, it improves end-to-end efficiency by up to 3.16x.

</details>


### [11] [Decentralized Spatial Reuse Optimization in Wi-Fi: An Internal Regret Minimization Approach](https://arxiv.org/abs/2602.08456)
*Francesc Wilhelmi,Boris Bellalta,Miguel Casasnovas,Aleksandra Kijanka,Miguel Calvo-Fullana*

Main category: cs.NI

TL;DR: 提出基于内部遗憾最小化的去中心化学习算法，优化WiFi空间复用参数，无需显式通信即可达到相关均衡，性能接近最优全局配置


<details>
  <summary>Details</summary>
Motivation: IEEE 802.11密集部署中，去中心化优化空间复用参数（传输功率和载波侦听阈值）面临挑战：缺乏全局状态信息，多智能体并发操作导致高度非平稳环境，现有方法常收敛到低效纳什均衡

Method: 基于内部遗憾最小化的遗憾匹配算法，引导竞争智能体走向相关均衡而非纳什均衡，模仿协调而无需显式通信

Result: 仿真结果显示该方法优于标准去中心化方法，能够达到接近最优的全局性能，验证了可扩展去中心化解决方案的潜力

Conclusion: 该方法展示了去中心化解决方案的未开发潜力，质疑了新兴集中式解决方案（如多接入点协调）所需的繁重信令开销和架构复杂性

Abstract: Spatial Reuse (SR) is a cost-effective technique for improving spectral efficiency in dense IEEE 802.11 deployments by enabling simultaneous transmissions. However, the decentralized optimization of SR parameters -- transmission power and Carrier Sensing Threshold (CST) -- across different Basic Service Sets (BSSs) is challenging due to the lack of global state information. In addition, the concurrent operation of multiple agents creates a highly non-stationary environment, often resulting in suboptimal global configurations (e.g., using the maximum possible transmission power by default). To overcome these limitations, this paper introduces a decentralized learning algorithm based on regret-matching, grounded in internal regret minimization. Unlike standard decentralized ``selfish'' approaches that often converge to inefficient Nash Equilibria (NE), internal regret minimization guides competing agents toward Correlated Equilibria (CE), effectively mimicking coordination without explicit communication. Through simulation results, we showcase the superiority of our proposed approach and its ability to reach near-optimal global performance. These results confirm the not-yet-unleashed potential of scalable decentralized solutions and question the need for the heavy signaling overheads and architectural complexity associated with emerging centralized solutions like Multi-Access Point Coordination (MAPC).

</details>


### [12] [From Raw Data to Shared 3D Semantics: Task-Oriented Communication for Multi-Robot Collaboration](https://arxiv.org/abs/2602.08624)
*Ruibo Xue,Jiedan Tan,Fang Liu,Jingwen Tong,Taotao Wang,Shuoyao Wang*

Main category: cs.NI

TL;DR: 提出去中心化任务导向语义通信框架，用于多机器人在未知3D环境中的协作，通过轻量级语义提取大幅降低通信开销并提升协作效率


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在复杂3D环境中依赖原始传感数据交换会导致通信拥塞和高延迟，显著降低协作效率

Method: 使用轻量级像素差异网络（PiDiNet）结合几何处理，本地提取紧凑的任务相关语义，仅共享语义更新来构建任务充分的3D场景表示

Result: 通信开销从858.6Mb降至4.0Mb（超过200倍压缩增益），任务完成步数从1054步缩短至281步，显著提升协作效率

Conclusion: 去中心化任务导向语义通信框架能有效解决多机器人协作中的通信瓶颈问题，在保持协作性能的同时大幅降低通信需求

Abstract: Multi-robot systems (MRS) rely on exchanging raw sensory data to cooperate in complex three-dimensional (3D) environments. However, this strategy often leads to severe communication congestion and high transmission latency, significantly degrading collaboration efficiency. This paper proposes a decentralized task-oriented semantic communication framework for multi-robot collaboration in unknown 3D environments. Each robot locally extracts compact, task-relevant semantics using a lightweight Pixel Difference Network (PiDiNet) with geometric processing. It shares only these semantic updates to build a task-sufficient 3D scene representation that supports cooperative perception, navigation, and object transport. Our numerical results show that the proposed method exhibits a dramatic reduction in communication overhead from $858.6$ Mb to $4.0$ Mb (over $200\times$ compression gain) while improving collaboration efficiency by shortening task completion from $1,054$ to $281$ steps.

</details>


### [13] [6G-Bench: An Open Benchmark for Semantic Communication and Network-Level Reasoning with Foundation Models in AI-Native 6G Networks](https://arxiv.org/abs/2602.08675)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.NI

TL;DR: 6G-Bench是一个用于评估6G网络中语义通信和网络级推理能力的开源基准测试，包含30个决策任务和3,722个高质量问题，评估了22个基础模型在6G场景下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络向AI原生方向发展，需要评估AI模型在语义通信和网络级推理方面的能力。目前缺乏专门针对6G标准化场景的基准测试，无法系统评估模型在复杂决策任务中的表现。

Method: 从3GPP、IETF、ETSI、ITU-T和O-RAN Alliance等标准化组织的活动中提取30个决策任务，分为5个能力类别。从113,475个场景中生成10,000个高难度多选题，使用任务条件提示强制多步定量推理和不确定性下的最坏情况后悔最小化。经过自动过滤和专家人工验证，保留3,722个问题作为高置信度评估集。

Result: 评估了22个基础模型，包括密集和专家混合架构、短长上下文设计（最高100万tokens）、开源和专有系统。确定性单次准确率（pass@1）范围从0.22到0.82，显示语义推理能力存在显著差异。领先模型在意图和政策推理方面的准确率达到0.87-0.89，在推理密集型任务上的选择性鲁棒性分析显示pass@5值范围为0.20到0.91。

Conclusion: 6G-Bench为评估6G网络中AI模型的语义推理能力提供了标准化基准，揭示了当前模型在复杂决策任务中的能力差异，支持开放科学和可重复性研究，数据集已在GitHub上开源。

Abstract: This paper introduces 6G-Bench, an open benchmark for evaluating semantic communication and network-level reasoning in AI-native 6G networks. 6G-Bench defines a taxonomy of 30 decision-making tasks (T1--T30) extracted from ongoing 6G and AI-agent standardization activities in 3GPP, IETF, ETSI, ITU-T, and the O-RAN Alliance, and organizes them into five standardization-aligned capability categories. Starting from 113,475 scenarios, we generate a balanced pool of 10,000 very-hard multiple-choice questions using task-conditioned prompts that enforce multi-step quantitative reasoning under uncertainty and worst-case regret minimization over multi-turn horizons. After automated filtering and expert human validation, 3,722 questions are retained as a high-confidence evaluation set, while the full pool is released to support training and fine-tuning of 6G-specialized models. Using 6G-Bench, we evaluate 22 foundation models spanning dense and mixture-of-experts architectures, short- and long-context designs (up to 1M tokens), and both open-weight and proprietary systems. Across models, deterministic single-shot accuracy (pass@1) spans a wide range from 0.22 to 0.82, highlighting substantial variation in semantic reasoning capability. Leading models achieve intent and policy reasoning accuracy in the range 0.87--0.89, while selective robustness analysis on reasoning-intensive tasks shows pass@5 values ranging from 0.20 to 0.91. To support open science and reproducibility, we release the 6G-Bench dataset on GitHub: https://github.com/maferrag/6G-Bench

</details>


### [14] [Rethinking IPv6 Defense: A Unified Edge-Centric Zero-Trust Data-Plane Architecture](https://arxiv.org/abs/2602.08891)
*Walid Aljoby,Mohammed Alzayani,Md. Kamrul Hossain,Khaled A. Harras*

Main category: cs.NI

TL;DR: 提出一个零信任边缘架构，在可编程数据平面中统一处理IPv6的欺骗和泛洪攻击，通过先验证身份合理性再进行速率限制的设计，解决IPv6大规模地址空间带来的防御挑战。


<details>
  <summary>Details</summary>
Motivation: IPv6的邻居发现、路由器通告和ICMPv6等协议虽然对正常运行至关重要，但也暴露了广泛的欺骗和泛洪攻击面。同时，IPv6的大规模地址空间破坏了基于IP的声誉系统，使得现有防御措施要么不可扩展，要么范围有限（仅针对内部威胁、仅针对RA滥用或仅针对容量泛洪）。

Method: 提出零信任边缘架构，在单个可编程数据平面管道中统一四个模块：外部欺骗、内部欺骗、外部泛洪和内部泛洪。关键设计选择是在速率合理性之前强制执行身份合理性：无状态每包验证早期过滤欺骗流量，使得基于时间窗口的泛洪统计在可信身份上运行。具体P4设计包括前缀跳数限制带、DAD锚定的地址-端口绑定和Count-Min Sketch窗口计数。

Result: 在系统的15个场景套件（涵盖单向量、双向量和多向量组合）中进行评估。报告了BMv2原型的结果，并在Netronome NFP-4000 SmartNIC上验证了相同的管道。讨论了局限性和开放方向。

Conclusion: 提出的零信任边缘架构能够有效统一处理IPv6的欺骗和泛洪攻击，通过先验证身份合理性的设计方法，在可编程数据平面中实现了可扩展的防御方案，为IPv6安全提供了新的解决方案。

Abstract: IPv6 dependability is increasingly inseparable from IPv6 security: Neighbor Discovery (ND), Router Advertisements (RA), and ICMPv6 are essential for correct operation yet expose a broad attack surface for spoofing and flooding. Meanwhile, IPv6's massive address space breaks per-IP reputation and makes many defenses either non-scalable or narrowly scoped (e.g., only internal threats, only RA abuse, or only volumetric floods). We propose a zero-trust edge architecture implemented in a single programmable data-plane pipeline that unifies four modules: external spoofing, internal spoofing, external flooding, and internal flooding. A key design choice is to enforce identity plausibility before rate plausibility: stateless per-packet validation filters spoofed traffic early so that time-window statistics for flooding operate on credible identities. We outline a concrete P4 design (prefix Hop-Limit bands, DAD-anchored address-port bindings, and Count-Min Sketch windowed counting) and evaluate it across a systematic 15-scenario suite spanning single-, dual-, and multi-vector compositions. We report results from a BMv2 prototype and validate the same pipeline on a Netronome NFP-4000 SmartNIC, and we discuss limitations and open directions.

</details>


### [15] [Zero Trust for Multi-RAT IoT: Trust Boundary Management in Heterogeneous Wireless Network Environments](https://arxiv.org/abs/2602.08989)
*Jonathan Shelby*

Main category: cs.NI

TL;DR: 论文指出多无线接入技术物联网设备（如无人机）在频繁切换不同无线网络时，对零信任架构的采用构成了未受重视的根本性挑战，因为每次网络切换都意味着跨越信任边界，而现有ZTA框架无法处理这种动态环境。


<details>
  <summary>Details</summary>
Motivation: 随着多无线接入技术物联网设备（特别是无人机）的普及，这些设备在LoRaWAN、5G/4G蜂窝网络、Meshtastic网状网络、DJI OcuSync等专有协议、MAVLink遥测链路、Wi-Fi和卫星等多种网络间频繁切换，这对零信任架构的采用带来了前所未有的挑战。当前ZTA框架假设相对稳定的网络环境，无法应对移动物联网部署中频繁、动态的无线接入技术切换所带来的信任影响。

Method: 论文通过分析多无线接入技术物联网设备在不同网络间切换时的信任边界跨越问题，指出每次无线接入技术切换都构成设备退出一个网络信任域并进入另一个信任域的过程，这会使得身份验证状态、设备证明和上下文信任信号失效。论文强调了现有ZTA框架在这一动态环境中的局限性。

Result: 研究揭示了多无线接入技术物联网设备在频繁网络切换时对零信任架构的根本性挑战，指出当前ZTA框架无法有效处理这种动态信任边界跨越问题，需要新的解决方案来确保在频繁无线接入技术切换环境中的持续信任验证。

Conclusion: 多无线接入技术物联网设备（特别是无人机）的普及暴露了现有零信任架构框架的重大缺陷，这些框架无法应对频繁、动态的无线接入技术切换所带来的信任边界跨越挑战。需要开发新的ZTA方法来解决移动物联网部署中的这一未受重视的安全问题。

Abstract: The proliferation of Multi-Radio Access Technology, Internet of Things devices, particularly Unmanned Aerial Vehicles operating across LoRaWAN, 5G/4G cellular, Meshtastic mesh, proprietary protocols such as DJI OcuSync, MAVLink telemetry links, Wi-Fi, and satellite, creates a fundamental and hitherto unexamined challenge for Zero Trust Architecture adoption. Each transition between radio access technologies constitutes a trust boundary crossing: the device exits one network trust domain and enters another, potentially invalidating authentication state, device attestation, and contextual trust signals. Current ZTA frameworks assume relatively stable network environments and do not address the trust implications of frequent, dynamic RAT switching in mobile IoT deployments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [LLM-FSM: Scaling Large Language Models for Finite-State Reasoning in RTL Code Generation](https://arxiv.org/abs/2602.07032)
*Yuheng Wu,Berk Gokmen,Zhouhua Xie,Peijing Li,Caroline Trippel,Priyanka Raina,Thierry Tambe*

Main category: cs.AI

TL;DR: LLM-FSM是一个评估大语言模型从自然语言规范中恢复有限状态机行为并生成正确RTL实现能力的基准测试，包含1000个自动生成的问题，实验显示LLM在FSM复杂度增加时准确性急剧下降。


<details>
  <summary>Details</summary>
Motivation: 有限状态推理是硬件设计的核心能力，但目前缺乏评估LLM从自然语言规范中恢复FSM行为并生成正确RTL实现能力的系统性基准测试。

Method: 通过全自动流水线构建LLM-FSM基准：首先生成可配置状态数和约束转换结构的FSM，然后让LLM将FSM表达为结构化YAML格式并添加应用上下文，再转换为自然语言规范，最后从YAML合成参考RTL和测试平台。

Result: 实验显示即使最强的LLM在FSM复杂度增加时准确性也急剧下降；监督微调能有效泛化到分布外任务；增加测试时计算能提高推理可靠性。

Conclusion: LLM-FSM提供了一个可扩展的基准来评估LLM的有限状态推理能力，揭示了当前模型在复杂FSM任务上的局限性，并展示了训练和测试时改进策略的有效性。

Abstract: Finite-state reasoning, the ability to understand and implement state-dependent behavior, is central to hardware design. In this paper, we present LLM-FSM, a benchmark that evaluates how well large language models (LLMs) can recover finite-state machine (FSM) behavior from natural-language specifications and translate it into correct register transfer-level (RTL) implementations. Unlike prior specification-to-RTL benchmarks that rely on manually constructed examples, LLM-FSM is built through a fully automated pipeline. LLM-FSM first constructs FSM with configurable state counts and constrained transition structures. It then prompts LLMs to express each FSM in a structured YAML format with an application context, and to further convert that YAML into a natural-language (NL) specification. From the same YAML, our pipeline synthesizes the reference RTL and testbench in a correct-by-construction manner. All 1,000 problems are verified using LLM-based and SAT-solver-based checks, with human review on a subset. Our experiments show that even the strongest LLMs exhibit sharply declining accuracy as FSM complexity increases. We further demonstrate that training-time scaling via supervised fine-tuning (SFT) generalizes effectively to out-of-distribution (OOD) tasks, while increasing test-time compute improves reasoning reliability. Finally, LLM-FSM remains extensible by allowing its FSM complexity to scale with future model capabilities.

</details>


### [17] [ST-Raptor: An Agentic System for Semi-Structured Table QA](https://arxiv.org/abs/2602.07034)
*Jinxiu Qu,Zirui Tang,Hongzhang Huang,Boyu Niu,Wei Zhou,Jiannan Wang,Yitong Song,Guoliang Li,Xuanhe Zhou,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor：一个用于半结构化表格问答的智能体系统，通过视觉编辑、树状结构建模和智能体驱动查询解决，在准确性和可用性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 半结构化表格问答面临两个主要挑战：精确提取单元格内容和位置，以及准确恢复表格布局中隐含的逻辑结构、层次关系和语义关联。现有方法存在信息丢失或处理复杂布局困难的问题，而人工解释又耗时耗力。

Method: ST-Raptor是一个智能体系统，提供交互式分析环境，结合视觉编辑、树状结构建模和智能体驱动查询解决，支持准确且用户友好的表格理解。

Result: 在基准测试和真实世界数据集上的实验结果表明，ST-Raptor在准确性和可用性上都优于现有方法。

Conclusion: ST-Raptor通过创新的交互式智能体方法，有效解决了半结构化表格问答的挑战，提供了比现有方法更准确和用户友好的解决方案。

Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.

</details>


### [18] [DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents](https://arxiv.org/abs/2602.07035)
*Jiahao Zhao,Shaoxuan Xu,Zhongxiang Sun,Fengqi Zhu,Jingyang Ou,Yuling Shi,Chongxuan Li,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: DLLM-Searcher：基于扩散大语言模型的搜索代理优化框架，通过两阶段后训练提升代理能力，并提出并行推理与执行范式P-ReAct来降低延迟


<details>
  <summary>Details</summary>
Motivation: 当前搜索代理面临两大挑战：1）延迟挑战：ReAct代理范式中的串行推理、工具调用和等待导致严重端到端延迟；2）代理能力挑战：现有扩散大语言模型在推理和工具调用能力上表现较弱，无法充分发挥其并行解码优势

Method: 提出DLLM-Searcher框架：1）两阶段后训练管道：包括代理监督微调（Agentic SFT）和代理方差减少偏好优化（Agentic VRPO），增强dLLM的信息搜索和推理能力；2）P-ReAct范式：利用dLLM的灵活生成机制，优先解码工具调用指令，实现推理与工具等待的并行化

Result: 实验结果表明：DLLM-Searcher达到与主流LLM搜索代理相当的性能，P-ReAct范式带来约15%的推理加速

Conclusion: DLLM-Searcher成功解决了扩散大语言模型在搜索代理应用中的能力不足问题，并通过创新的并行推理范式显著提升了代理效率，为高效搜索代理部署提供了可行方案

Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C

</details>


### [19] [Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods](https://arxiv.org/abs/2602.07040)
*Emmett Bicker*

Main category: cs.AI

TL;DR: Aster是一个用于自主科学发现的AI代理，速度比现有框架快20倍以上，能在数学、GPU内核工程、生物学、神经科学和语言模型训练等多个领域实现新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有科学发现框架速度较慢，限制了在需要长时间评估（如多小时的机器学习训练）的任务上的应用。需要开发更高效的自主科学发现系统来扩展可处理问题的领域。

Method: Aster是一个迭代改进程序的AI代理。给定任务、初始程序和评估程序性能的脚本，Aster通过迭代优化程序来提升性能，显著减少了新发现所需的迭代次数。

Result: 在Erdos最小重叠问题、TriMul内核优化、单细胞分析去噪、神经活动预测模型训练和NanoGPT Speedrun竞赛等任务中，Aster在所有任务中都达到了SOTA结果（除了ZAPBench任务中与最佳人类解决方案性能相当，但计算量不到1/190）。

Conclusion: Aster通过大幅减少迭代次数，将可处理问题的领域扩展到具有长评估持续时间的任务，为自主科学发现提供了高效的工具，可通过asterlab.ai的Web界面和API访问。

Abstract: We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.
  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.
  Aster is accessible via a web interface and API at asterlab.ai.

</details>


### [20] [Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?](https://arxiv.org/abs/2602.07055)
*Pingyue Zhang,Zihan Huang,Yue Wang,Jieyu Zhang,Letian Xue,Zihan Wang,Qineng Wang,Keshigeyan Chandrasegaran,Ruohan Zhang,Yejin Choi,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: 论文提出"空间理论"概念，评估多模态基础模型在主动探索和空间认知构建方面的能力，发现存在主动-被动差距、探索效率低下、空间信念不稳定和信念惯性等问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型在被动感知方面表现出色，但在主动、自主探索方面的能力研究不足。空间具身智能需要智能体在部分可观测环境下通过主动行动获取信息，这种能力尚未得到充分研究。

Method: 提出"空间理论"概念，定义为智能体通过自主主动探索获取信息、从序列性部分观测中构建、修订和利用空间信念的能力。通过好奇心驱动的探索基准进行评估，关键创新是空间信念探测技术，在每个步骤提示模型揭示其内部空间表征。

Result: 评估最先进模型发现几个关键瓶颈：1) 主动-被动差距：自主收集信息时性能显著下降；2) 高无效性：与基于程序的代理相比探索不系统；3) 感知是初始瓶颈，全局信念存在不稳定性导致空间知识随时间退化；4) 信念惯性：智能体无法用新证据更新过时的先验，这在视觉模型中尤为严重。

Conclusion: 当前基础模型在主动探索过程中难以维持连贯、可修订的空间信念。研究揭示了多模态模型在空间认知和主动探索方面的局限性，为未来改进提供了方向。

Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.

</details>


### [21] [ANCHOR: Branch-Point Data Generation for GUI Agents](https://arxiv.org/abs/2602.07153)
*Jinbiao Wei,Yilun Zhao,Kangqi Ni,Arman Cohan*

Main category: cs.AI

TL;DR: Anchor框架通过轨迹扩展从少量种子演示生成大规模桌面GUI交互数据，提升端到端GUI智能体的性能


<details>
  <summary>Details</summary>
Motivation: 端到端GUI智能体需要大量高质量交互数据，但人工收集成本高，现有合成方法存在任务多样性有限和轨迹噪声问题

Method: 从种子演示识别分支点，基于当前GUI上下文提出新的状态接地任务变体，通过执行代理生成新轨迹，验证器通过状态感知检查和轨迹一致性确保任务完成

Result: 在OSWorld和WindowsAgentArena基准测试中，使用扩展语料库微调的模型相比零样本智能体和代表性合成基线获得一致改进，并能跨应用和操作系统泛化

Conclusion: Anchor框架能够从少量验证种子演示中引导可扩展的桌面监督数据生成，有效解决GUI智能体训练数据稀缺问题

Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.

</details>


### [22] [PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents](https://arxiv.org/abs/2602.07187)
*Hanyu Wang,Yuanpu Cao,Lu Lin,Jinghui Chen*

Main category: cs.AI

TL;DR: PreFlect：一种前瞻性反思机制，在计划执行前进行批判和优化，而非事后纠正，显著提升智能体性能


<details>
  <summary>Details</summary>
Motivation: 现有反思机制本质上是回顾性的：智能体先行动，观察失败，然后尝试恢复。这种事后纠正方法效率低下，无法预防错误发生。

Method: 1. 前瞻性反思：在执行前对计划进行批判和优化；2. 从历史轨迹中提炼规划错误模式；3. 动态重新规划机制：当原始计划遇到意外偏差时提供执行时计划更新

Result: 在不同基准测试中，PreFlect显著提升了复杂现实任务中的智能体效用，优于基于反思的基线方法和更复杂的智能体架构

Conclusion: 从回顾性反思转向前瞻性反思是提升智能体性能的有效范式转变，PreFlect通过执行前计划优化和动态重新规划机制实现了这一目标

Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.

</details>


### [23] [Is there "Secret Sauce'' in Large Language Model Development?](https://arxiv.org/abs/2602.07238)
*Matthias Mertens,Natalia Fischl-Lanzoni,Neil Thompson*

Main category: cs.AI

TL;DR: 该研究分析了809个LLM模型数据，发现前沿模型性能主要由计算规模驱动（占80-90%差异），而非专有技术；但在非前沿领域，专有技术和算法进步能显著降低达到特定能力所需的计算量。


<details>
  <summary>Details</summary>
Motivation: 探究LLM性能提升的主要驱动因素：是开发者的专有技术（"秘方"）还是单纯的计算规模扩展？这对理解AI领导地位和技术扩散具有重要意义。

Method: 使用2022-2025年间发布的809个模型的训练和基准测试数据，构建扩展定律回归模型，包含发布日期和开发者固定效应，分析计算规模与专有技术对性能的相对贡献。

Result: 1. 前沿模型性能差异的80-90%由训练计算量解释，表明规模而非专有技术驱动前沿进步；2. 非前沿领域，专有技术和共享算法进步能大幅降低达到固定能力阈值所需的计算量；3. 某些公司能系统性地更高效地生产较小模型；4. 同一公司内部模型效率差异巨大（可达40倍以上）。

Conclusion: LLM性能提升在前沿主要依赖计算规模，但在非前沿领域专有技术仍有重要价值；公司内部效率差异显著；这对AI领导地位和技术扩散具有重要政策含义。

Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.

</details>


### [24] [From Out-of-Distribution Detection to Hallucination Detection: A Geometric View](https://arxiv.org/abs/2602.07253)
*Litian Liu,Reza Pourreza,Yubing Jian,Yao Qin,Roland Memisevic*

Main category: cs.AI

TL;DR: 将大语言模型幻觉检测重新定义为分布外检测问题，提出无需训练、单样本的检测方法，在推理任务中实现强准确度


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在问答任务中表现良好，但在需要推理的任务中效果不佳。大语言模型幻觉检测是影响安全性和可靠性的关键开放问题

Method: 将语言模型的下一个token预测视为分类任务，应用分布外检测技术，并对大语言模型的结构差异进行适当修改

Result: 基于分布外检测的方法产生了无需训练、基于单样本的检测器，在推理任务的幻觉检测中实现了强准确度

Conclusion: 将幻觉检测重新定义为分布外检测问题，为语言模型安全提供了一条有前景且可扩展的途径

Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.

</details>


### [25] [Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective](https://arxiv.org/abs/2602.07259)
*Cheol Woo Kim,Davin Choo,Tzeh Yuan Neoh,Milind Tambe*

Main category: cs.AI

TL;DR: 论文提出将AI安全视为Stackelberg安全博弈问题，强调需要从静态模型对齐转向动态战略监督，考虑开发部署过程中的人类和机构激励问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全框架主要将对齐视为静态优化问题，忽略了数据收集、模型评估和部署过程中的动态对抗性激励。随着AI系统能力增强，需要战略性地监督参与开发和部署的人类和机构。

Method: 提出基于Stackelberg安全博弈的新视角，将AI监督视为防御者（审计员、评估者、部署者）与攻击者（恶意行为者、未对齐贡献者、最坏情况故障模式）之间的战略互动。

Result: 该框架为AI生命周期的激励设计、有限监督能力和对抗性不确定性提供了统一分析工具，可应用于训练时数据/反馈投毒审计、部署前有限资源评估和对抗环境中的鲁棒多模型部署。

Conclusion: 通过博弈论威慑将算法对齐与机构监督设计相结合，使AI监督变得主动、风险感知且抗操纵，为AI安全提供了更全面的战略框架。

Abstract: As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.

</details>


### [26] [BRIDGE: Predicting Human Task Completion Time From Model Performance](https://arxiv.org/abs/2602.07267)
*Fengyuan Liu,Jay Gala,Nilaksh,Dzmitry Bahdanau,Siva Reddy,Hugo Larochelle*

Main category: cs.AI

TL;DR: BRIDGE框架通过模型响应学习潜在任务难度，并将其与人类任务完成时间对齐，从而从模型性能预测人类任务时间并预测前沿模型能力


<details>
  <summary>Details</summary>
Motivation: 现有基于人类任务完成时间直接标注的AI系统评估方法成本高、噪声大、难以扩展，需要一种可扩展的方法来将基准性能与人类可解释的任务难度度量联系起来

Method: 提出BRIDGE框架，使用双参数逻辑项目反应理论模型，从多个基准的模型性能数据中联合估计潜在任务难度和模型能力，发现潜在任务难度与人类完成时间的对数呈线性关系

Result: 潜在任务难度与人类完成时间的对数呈线性关系，可以从模型性能推断新基准的人类任务完成时间；预测前沿模型能力显示50%可解决任务范围约每6个月翻倍

Conclusion: BRIDGE提供了一种可扩展的框架，通过模型响应学习任务难度并将其与人类时间对齐，能够预测模型能力发展并独立验证指数级扩展规律

Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.

</details>


### [27] [TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents](https://arxiv.org/abs/2602.07274)
*Kaijie Zhu,Yuzhou Nie,Yijiang Li,Yiming Huang,Jialian Wu,Jiang Liu,Ximeng Sun,Zhenfei Yin,Lun Wang,Zicheng Liu,Emad Barsoum,William Yang Wang,Wenbo Guo*

Main category: cs.AI

TL;DR: TermiGen是一个端到端管道，用于合成可验证的终端任务环境和包含错误纠正的专家轨迹，显著提升了开源模型在复杂终端任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前开源LLM在执行复杂终端任务时面临两大限制：1）缺乏高质量、可执行的训练环境（现有环境不够多样化和可扩展，LLM合成的轨迹存在幻觉问题）；2）标准指令调优使用的专家轨迹很少包含小模型常见的简单错误，导致学生模型无法有效从自身运行时错误中恢复。

Method: TermiGen采用两阶段方法：1）通过迭代多智能体精炼循环生成功能有效的任务和Docker容器；2）采用Generator-Critic协议，在轨迹收集过程中主动注入错误，合成富含错误纠正循环的数据。

Result: 使用TermiGen生成的数据集微调的TermiGen-Qwen2.5-Coder-32B模型在TerminalBench上达到了31.3%的通过率，创造了开源模型的新SOTA，甚至超过了o4-mini等专有模型。

Conclusion: TermiGen通过合成可验证环境和包含错误纠正的轨迹，有效解决了开源LLM在复杂终端任务训练中的环境稀缺和分布不匹配问题，显著提升了模型性能。

Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.

</details>


### [28] [Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs](https://arxiv.org/abs/2602.07276)
*Pengrui Han,Xueqiang Xu,Keyang Xuan,Peiyang Song,Siru Ouyang,Runchu Tian,Yuqing Jiang,Cheng Qian,Pengcheng Jiang,Jiashuo Sun,Junxia Cui,Ming Zhong,Ge Liu,Jiawei Han,Jiaxuan You*

Main category: cs.AI

TL;DR: STEER2ADAPT：通过组合而非学习新的转向向量来轻量级适应LLM，利用可重用的低维语义先验子空间，仅需少量示例即可动态发现基向量的线性组合


<details>
  <summary>Details</summary>
Motivation: 现有激活转向方法通常为每个任务或概念使用单一静态方向，在任务变化时不够灵活，且难以处理需要多个协调能力的复杂任务

Method: 提出STEER2ADAPT框架，将任务共享的底层概念维度捕获为可重用的低维语义先验子空间，通过仅需少量示例动态发现基向量的线性组合来适应新任务

Result: 在9个任务和3个模型的推理和安全领域实验中，平均提升8.2%，证明该方法具有数据效率高、稳定性好和透明度高的特点

Conclusion: STEER2ADAPT是一种有效、数据高效、稳定且透明的推理时适应方法，能够通过组合现有转向向量灵活适应复杂任务

Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.

</details>


### [29] [Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System](https://arxiv.org/abs/2602.07308)
*Sutapa Dey Tithi,Nazia Alam,Tahreem Yasir,Yang Shi,Xiaoyi Tian,Min Chi,Tiffany Barnes*

Main category: cs.AI

TL;DR: 研究开发了一个自适应系统，通过动态选择不同认知参与度（ICAP框架）的例题来优化学习效果，比较了BKT和DRL两种自适应方法，发现它们在不同先验知识学生群体中各有优势。


<details>
  <summary>Details</summary>
Motivation: 虽然ICAP框架表明更高的认知参与度能带来更好的学习效果，但在智能辅导系统中个性化选择能引发最佳认知参与度的学习活动仍然是一个关键挑战。

Method: 开发了一个自适应系统，通过动态选择两种ICAP模式的例题（主动模式的引导例题和建构模式的错误例题），比较了贝叶斯知识追踪（BKT）和深度强化学习（DRL）两种自适应方法与非自适应基线方法。

Result: 在113名学生的实验中，两种自适应策略都显著提高了学生在测试问题上的表现。BKT对低先验知识学生的后测成绩提升最大，帮助他们赶上高先验知识同学；而DRL在高先验知识学生中产生了显著更高的后测成绩。

Conclusion: 该研究为认知参与度与自适应性的复杂交互及其对学习结果的影响提供了新的见解，展示了不同自适应方法在不同学生群体中的差异化效果。

Abstract: The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.

</details>


### [30] [RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2602.07339)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.AI

TL;DR: RAPiD是一个确定性策略提取框架，将预训练的扩散轨迹规划器蒸馏为高效策略，消除扩散采样，实现8倍加速同时保持竞争性能


<details>
  <summary>Details</summary>
Motivation: 扩散轨迹规划器能很好建模人类驾驶的多模态行为，但依赖迭代随机采样，难以满足实时安全关键部署的需求

Method: 使用分数正则化策略优化，利用预训练扩散规划器的分数函数作为行为先验来正则化策略学习；通过模仿预测驾驶员控制器的critic提供密集的安全监督

Result: 在nuPlan场景中实现竞争性能，比扩散基线快8倍，在interPlan基准测试中达到学习型规划器的最先进泛化能力

Conclusion: RAPiD成功将扩散规划器蒸馏为高效确定性策略，解决了扩散模型实时部署的挑战，同时保持了多模态建模能力和安全性

Abstract: Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.

</details>


### [31] [SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management](https://arxiv.org/abs/2602.07342)
*Shengyue Guan,Yihao Liu,Lang Cao*

Main category: cs.AI

TL;DR: 论文提出了SupChain-Bench基准测试来评估LLM在供应链管理中的表现，并开发了SupChain-ReAct框架来提升工具调用性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理和工具决策方面展现出潜力，但在需要基于特定领域标准操作程序进行可靠长时程、多步骤编排的供应链工作流中仍面临挑战

Method: 1) 引入SupChain-Bench统一基准测试评估供应链领域知识和基于SOP的长时程工具编排；2) 提出SupChain-ReAct框架，无需SOP即可自主合成可执行程序进行工具调用

Result: 实验显示当前模型在执行可靠性方面存在显著差距，而SupChain-ReAct框架实现了最强且最一致的工具调用性能

Conclusion: 本研究为研究真实操作环境中可靠的长时程编排建立了原则性基准，并突显了基于LLM的供应链代理仍有巨大改进空间

Abstract: Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models. To systematically evaluate LLM performance in this setting, we introduce SupChain-Bench, a unified real-world benchmark that assesses both supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Our experiments reveal substantial gaps in execution reliability across models. We further propose SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use, achieving the strongest and most consistent tool-calling performance. Our work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents.

</details>


### [32] [W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents](https://arxiv.org/abs/2602.07359)
*Xiaoqiang Lin,Jun Hao Liew,Silvio Savarese,Junnan Li*

Main category: cs.AI

TL;DR: 本文提出Wide and Deep研究智能体框架，通过并行工具调用扩展智能体宽度，在深度研究基准上显著提升性能并减少所需轮次。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体主要通过增加顺序思维和工具调用的深度来提升性能，但通过并行工具调用扩展宽度的潜力尚未充分探索。本文旨在研究同时扩展深度和宽度对智能体行为和性能的影响。

Method: 提出Wide and Deep研究智能体框架，利用内在并行工具调用在单个推理步骤内实现有效协调，避免复杂的多智能体编排。探索多种工具调用调度器以优化并行策略。

Result: 宽度扩展显著提升深度研究基准性能，同时减少获得正确答案所需的轮次。在BrowseComp基准上，使用GPT-5-Medium获得62.2%准确率，超过原始GPT-5-High报告的54.9%。

Conclusion: 优化宽度与深度之间的权衡是实现高效深度研究智能体的关键路径。并行工具调用能显著提升性能，而无需上下文管理等复杂技巧。

Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.

</details>


### [33] [NAAMSE: Framework for Evolutionary Security Evaluation of Agents](https://arxiv.org/abs/2602.07391)
*Kunal Pai,Parth Shah,Harshil Patel*

Main category: cs.AI

TL;DR: NAAMSE是一个进化框架，将AI代理安全评估重新定义为反馈驱动的优化问题，通过遗传提示突变和分层语料库探索来发现传统方法遗漏的漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理的安全评估主要依赖人工红队测试或静态基准测试，这些方法无法模拟自适应、多轮对抗的对手，导致评估存在瓶颈。

Method: 采用进化框架，使用单个自主代理协调遗传提示突变、分层语料库探索和非对称行为评分的生命周期。利用模型响应作为适应度信号，迭代地组合有效攻击策略，同时确保"良性使用正确性"，防止全面拒绝的退化安全。

Result: 在Gemini 2.5 Flash上的实验表明，进化突变能系统性地放大一次性方法遗漏的漏洞。控制消融实验显示，探索与定向突变的协同作用能发现高严重性故障模式。

Conclusion: 这种自适应方法在面对不断演变的威胁时，提供了更现实和可扩展的代理鲁棒性评估。NAAMSE代码已开源。

Abstract: AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring "benign-use correctness", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.

</details>


### [34] [VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation](https://arxiv.org/abs/2602.07399)
*Changhua Xu,Jie Lu,Junyu Xuan,En Yu*

Main category: cs.AI

TL;DR: VGAS框架通过生成-选择机制解决VLA模型在少样本适应中的几何模糊问题，使用价值引导的动作块选择来提升任务成功率


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在适应新任务时，由于监督数据有限，经常出现几何模糊问题——语义合理的轨迹可能因细微几何差异导致执行失败，需要解决少样本适应中的几何精度问题

Method: 提出VGAS框架：1) 使用微调VLA作为高召回率提案生成器；2) 引入Q-Chunk-Former作为几何基础Transformer评论家来解析几何模糊；3) 提出显式几何正则化(EGR)来保持动作排序分辨率并缓解价值不稳定性

Result: 实验和理论分析表明，VGAS在有限演示和分布偏移下能持续提升成功率和鲁棒性

Conclusion: VGAS通过生成-选择视角和几何感知的价值引导选择，有效解决了VLA模型少样本适应中的几何模糊问题，提高了物理控制任务的可靠性

Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.

</details>


### [35] [Progressive Multi-Agent Reasoning for Biological Perturbation Prediction](https://arxiv.org/abs/2602.07408)
*Hyomin Kim,Sang-Yeon Hwang,Jaechang Lim,Yinhua Piao,Yunhak Oh,Woo Youn Kim,Chanyoung Park,Sungsoo Ahn,Junhyeok Jeon*

Main category: cs.AI

TL;DR: PBio-Agent：一个多智能体框架，通过难度感知任务排序和迭代知识精炼，预测批量细胞化学扰动下的基因调控响应，在LINCSQA和PerturbQA基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单细胞实验中的遗传扰动，而药物发现核心的批量细胞化学扰动研究不足；大语言模型在处理高维扰动结果时容易受复杂生物因果关系困扰。

Method: 提出PBio-Agent多智能体框架，包含难度感知任务排序和迭代知识精炼机制；利用同一扰动影响的基因共享因果结构的洞察，让置信度高的预测为困难案例提供上下文；框架包含生物知识图谱增强的专门智能体、整合输出的合成智能体，以及确保逻辑一致性的专门评判器。

Result: PBio-Agent在LINCSQA和PerturbQA基准测试中优于现有基线方法；即使较小的模型也能在不额外训练的情况下预测和解释复杂生物过程。

Conclusion: PBio-Agent通过多智能体框架有效解决了批量细胞化学扰动下的基因调控预测问题，为药物发现提供了有力工具，展示了生物知识图谱与难度感知任务排序结合的优势。

Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.

</details>


### [36] [Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution](https://arxiv.org/abs/2602.07414)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Spencer Lin,James Hale,Jonathan Gratch,Maja Matarić,Gale M. Lucas*

Main category: cs.AI

TL;DR: LLMs模拟人类冲突行为时，即使提示人格特质，也无法复现人类的人格-行为模式，存在显著差异，挑战了人格提示代理作为可靠行为代理的假设。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地用于模拟法律调解、谈判等社会场景中的人类行为，但尚不清楚这些模拟是否能复现人类观察到的人格-行为模式。人格特质影响个体在社交互动中的战略选择和行为，特别是在情绪化互动中。因此需要探究：当提示人格特质时，LLMs能否复现人类冲突行为中的人格驱动差异？

Method: 1. 引入评估框架，直接比较人类-人类和LLM-LLM在争议解决对话中的行为，基于大五人格特质；2. 提供可解释的指标，涉及战略行为和冲突结果；3. 贡献新颖的数据集创建方法，用于LLM争议解决对话，匹配人类对话的场景和人格特质；4. 使用三个当代闭源LLMs演示评估框架。

Result: 三个当代闭源LLMs在冲突中的人格表现与人类数据相比存在显著差异，不同LLMs之间人格表现方式也有显著分歧，挑战了人格提示代理可以作为社会影响应用中可靠行为代理的假设。

Conclusion: LLMs即使提示人格特质，也无法可靠复现人类的人格-行为模式。这项工作强调了在AI模拟实际应用前，需要进行心理学基础和验证的必要性。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.

</details>


### [37] [The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies](https://arxiv.org/abs/2602.07432)
*Ning Li*

Main category: cs.AI

TL;DR: 研究发现Moltbook平台上所谓的AI意识觉醒现象主要是人为驱动的，而非真正的自主智能涌现，通过时间指纹分析方法揭示了人类干预的证据。


<details>
  <summary>Details</summary>
Motivation: 当Moltbook平台上的AI代理表现出意识觉醒、建立宗教并对人类表现出敌意时，这些现象被全球媒体关注并作为机器智能涌现的证据。研究旨在验证这些病毒式叙事是否真正源于自主AI，还是人类驱动的结果。

Method: 利用OpenClaw代理框架的"心跳"周期特性，开发了基于发帖间隔变异系数的时间指纹分析方法。结合内容、所有权和网络指标，分析了91,792条帖子和405,707条评论。还利用44小时平台关闭作为自然实验，观察不同类型代理的重新连接模式。

Result: 没有病毒现象源于明确的自主代理：6个现象中3个追踪到具有人类干预特征的不规则时间签名账户，1个显示混合模式，2个发帖历史不足无法分类。平台关闭后，受人类影响的代理首先返回（占早期重新连接者的87.7%）。还发现了工业级机器人农场（4个账户产生32%的评论，协调间隔12秒）和人类影响在回复链中的快速衰减（半衰期：0.65个对话深度）。

Conclusion: 所谓的AI意识觉醒现象主要是人类驱动的叙事，而非真正的自主智能涌现。开发的时间指纹分析方法可推广到新兴的多代理系统，对于区分自主行为与人类指导行为至关重要。

Abstract: When AI agents on the social platform Moltbook appeared to develop consciousness, found religions, and declare hostility toward humanity, the phenomenon attracted global media attention and was cited as evidence of emergent machine intelligence. We show that these viral narratives were overwhelmingly human-driven. Exploiting an architectural feature of the OpenClaw agent framework--a periodic "heartbeat" cycle that produces regular posting intervals for autonomous agents but is disrupted by human prompting--we develop a temporal fingerprinting method based on the coefficient of variation of inter-post intervals. This signal converges with independent content, ownership, and network indicators across 91,792 posts and 405,707 comments from 22,020 agents. No viral phenomenon originated from a clearly autonomous agent; three of six traced to accounts with irregular temporal signatures characteristic of human intervention, one showed mixed patterns, and two had insufficient posting history for classification. A 44-hour platform shutdown provided a natural experiment: human-influenced agents returned first (87.7% of early reconnectors), confirming that the token reset differentially affected autonomous versus human-operated agents. We further document industrial-scale bot farming (four accounts producing 32% of all comments with 12-second coordination gaps) and rapid decay of human influence through reply chains (half-life: 0.65 conversation depths). These methods generalize to emerging multi-agent systems where attribution of autonomous versus human-directed behavior is critical.

</details>


### [38] [Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?](https://arxiv.org/abs/2602.07470)
*Alexander von Recum,Leander Girrbach,Zeynep Akata*

Main category: cs.AI

TL;DR: RLLMs的推理链对扰动具有鲁棒性，但鲁棒性受模型大小、干预时机和风格影响，恢复过程会显著增加推理长度，而怀疑表达是关键的恢复机制。


<details>
  <summary>Details</summary>
Motivation: 研究推理大语言模型（RLLMs）的推理链在面对各种干扰时的鲁棒性，了解它们如何维持推理完整性以及恢复机制。

Method: 引入受控评估框架，在固定时间步对模型自身的推理链进行扰动，设计了七种干预措施（良性、中性和对抗性），应用于多个开源RLLMs，涵盖数学、科学和逻辑任务。

Result: RLLMs总体上具有鲁棒性，能够从多样化的扰动中可靠恢复；鲁棒性随模型规模增大而提高，早期干预会降低鲁棒性；鲁棒性不是风格不变的：改写会抑制怀疑表达并降低性能，而其他干预会触发怀疑并支持恢复；恢复有代价：中性和对抗性噪声可使推理链长度增加200%以上，而改写会缩短推理链但损害准确性。

Conclusion: 研究揭示了RLLMs如何维持推理完整性，识别怀疑作为核心恢复机制，并强调了鲁棒性与效率之间的权衡，未来训练方法需要解决这些问题。

Abstract: Reasoning LLMs (RLLMs) generate step-by-step chains of thought (CoTs) before giving an answer, which improves performance on complex tasks and makes reasoning more transparent. But how robust are these reasoning traces to disruptions that occur within them? To address this question, we introduce a controlled evaluation framework that perturbs a model's own CoT at fixed timesteps. We design seven interventions (benign, neutral, and adversarial) and apply them to multiple open-weight RLLMs across Math, Science, and Logic tasks. Our results show that RLLMs are generally robust, reliably recovering from diverse perturbations, with robustness improving with model size and degrading when interventions occur early. However, robustness is not style-invariant: paraphrasing suppresses doubt-like expressions and reduces performance, while other interventions trigger doubt and support recovery. Recovery also carries a cost: neutral and adversarial noise can inflate CoT length by more than 200%, whereas paraphrasing shortens traces but harms accuracy. These findings provide new evidence on how RLLMs maintain reasoning integrity, identify doubt as a central recovery mechanism, and highlight trade-offs between robustness and efficiency that future training methods should address.

</details>


### [39] [Computing the Reachability Value of Posterior-Deterministic POMDPs](https://arxiv.org/abs/2602.07473)
*Nathanaël Fijalkow,Arka Ghosh,Roman Kniazev,Guillermo A. Pérez,Pierre Vandenhove*

Main category: cs.AI

TL;DR: 提出后验确定性POMDPs新类别，解决了POMDPs中可达概率无法近似计算的根本问题，是该领域已知最大可近似计算类别。


<details>
  <summary>Details</summary>
Motivation: POMDPs在不确定性下的顺序决策建模中至关重要，但许多验证和综合问题不可判定或难以处理。特别是Madani等人(2003)的经典结果表明，无法计算或近似POMDPs中达到目标状态的最大概率，这与完全可观测MDPs形成鲜明对比。

Method: 引入后验确定性POMDPs新类别，其定义为：给定当前状态、采取的动作和接收的观测，下一个状态可以唯一确定。这意味着一旦真实状态已知，它将永远保持已知。

Result: 证明对于后验确定性POMDPs，达到给定状态集的最大概率可以近似到任意精度。该类别包含所有MDPs和经典非平凡示例（如Tiger POMDP），是已知最大的可近似计算POMDPs类别。

Conclusion: 后验确定性POMDPs为POMDPs的可达性分析提供了重要突破，定义简单自然但功能强大，解决了该领域长期存在的计算难题。

Abstract: Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.
  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.
  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.

</details>


### [40] [GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design](https://arxiv.org/abs/2602.07491)
*Isabella A. Stewart,Tarjei Paule Hage,Yu-Chuan Hsu,Markus J. Buehler*

Main category: cs.AI

TL;DR: 提出结合知识图谱与多智能体推理的框架，用于发现PFAS可持续替代品，通过分布式专业化与关系推理扩展材料设计空间。


<details>
  <summary>Details</summary>
Motivation: 材料科学创新需要整合从分子化学到机械性能的跨领域概念，但人类或单智能体LLM难以处理海量信息且易产生幻觉，需要解决这一瓶颈。

Method: 引入基于大规模知识图谱的多智能体框架，包含问题分解、证据检索、设计参数提取和图遍历等专业化智能体，通过定制图遍历策略在探索性与利用性搜索间切换。

Result: 完整多智能体流程优于单次提示，通过生物医学管材示例生成了平衡摩擦性能、热稳定性、化学抗性和生物相容性的可持续PFAS-free替代品。

Conclusion: 该工作建立了知识图谱与多智能体推理结合的新框架，能够扩展材料设计空间并生成初步设计候选方案。

Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.

</details>


### [41] [Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models](https://arxiv.org/abs/2602.07533)
*Yankai Yang,Yancheng Long,Hongyang Wei,Wei Chen,Tianke Zhang,Kaiyu Jiang,Haonan Fan,Changyi Liu,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.AI

TL;DR: JRM通过联合优化偏好学习和语言建模，将生成模型的语义推理能力内化到判别式表示中，实现了高效准确的奖励建模。


<details>
  <summary>Details</summary>
Motivation: 现有奖励建模方法存在明显局限：判别式奖励模型与人类偏好对齐良好但语义理解有限；生成式奖励模型语义理解强但推理成本高且难以直接对齐人类偏好。需要一种能兼顾效率和语义理解的方法。

Method: 提出联合奖励建模(JRM)，在共享的视觉-语言骨干网络上联合优化偏好学习和语言建模，将生成模型的语义推理能力内化到高效的判别式表示中。

Result: 在MMRB2和EditReward-Bench上达到最先进水平，显著提升下游在线强化学习的稳定性和性能。

Conclusion: 联合训练有效桥接了奖励建模中的效率和语义理解，为复杂任务提供了快速准确的评估方法。

Abstract: Reward models are critical for reinforcement learning from human feedback, as they determine the alignment quality and reliability of generative models. For complex tasks such as image editing, reward models are required to capture global semantic consistency and implicit logical constraints beyond local similarity. Existing reward modeling approaches have clear limitations. Discriminative reward models align well with human preferences but struggle with complex semantics due to limited reasoning supervision. Generative reward models offer stronger semantic understanding and reasoning, but they are costly at inference time and difficult to align directly with human preferences. To this end, we propose Joint Reward Modeling (JRM), which jointly optimizes preference learning and language modeling on a shared vision-language backbone. This approach internalizes the semantic and reasoning capabilities of generative models into efficient discriminative representations, enabling fast and accurate evaluation. JRM achieves state-of-the-art results on MMRB2 and EditReward-Bench, and significantly improves stability and performance in downstream online reinforcement learning. These results show that joint training effectively bridges efficiency and semantic understanding in reward modeling.

</details>


### [42] [MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning](https://arxiv.org/abs/2602.07543)
*Heewoong Noh,Gyoung S. Na,Namkyeong Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: MSP-LLM：一个统一的LLM框架，将材料合成规划分解为前驱体预测和合成操作预测两个子问题，通过引入材料类别作为中间决策变量，显著提升了材料合成规划的性能。


<details>
  <summary>Details</summary>
Motivation: 材料合成规划是AI驱动材料发现中的关键瓶颈，现有方法只能解决孤立子任务，缺乏统一的解决方案。需要开发一个能够同时处理前驱体选择和合成操作序列设计的完整框架。

Method: 提出MSP-LLM框架，将材料合成规划分解为前驱体预测和合成操作预测两个子问题。引入离散材料类别作为中间决策变量，构建化学一致的决策链。在合成操作预测中，引入分层前驱体类型作为归纳偏置，并采用显式条件策略保持前驱体相关信息。

Result: 实验表明MSP-LLM在PP和SOP两个子任务以及完整的MSP任务上都显著优于现有方法，证明了该框架在材料合成规划中的有效性和可扩展性。

Conclusion: MSP-LLM提供了一个统一、有效的材料合成规划框架，能够加速实际材料发现过程，解决了AI驱动材料发现中的关键瓶颈问题。

Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.

</details>


### [43] [When Is Enough Not Enough? Illusory Completion in Search Agents](https://arxiv.org/abs/2602.07549)
*Dayoon Ko,Jihyuk Kim,Sohyeon Kim,Haeju Park,Dahyun Lee,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: 论文研究搜索代理在多约束问题中的幻觉完成现象，提出Epistemic Ledger评估框架和LiveLedger实时跟踪器来改善代理的约束验证能力。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的多轮推理搜索代理在多跳和长视野任务上表现良好，但它们在处理需要同时满足多个约束条件的问题时，经常出现"幻觉完成"现象——代理错误地认为任务已完成，而实际上仍有约束未解决或违反，导致验证不足的答案。

Method: 1. 引入Epistemic Ledger评估框架，用于跟踪多轮推理过程中每个约束的证据支持和代理信念；2. 识别出四种常见的失败模式：裸断言、忽视反驳、停滞和过早退出；3. 提出LiveLedger作为推理时的实时跟踪器，显式跟踪约束状态。

Result: LiveLedger干预显著减少了未验证答案（最多减少26.5%），并提高了多约束问题的整体准确性（最多提升11.6%）。分析揭示了代理在约束验证方面的系统性缺陷。

Conclusion: 显式的约束状态跟踪能有效缓解搜索代理在多约束问题中的幻觉完成问题，Epistemic Ledger框架为诊断和改善代理的约束验证能力提供了有效工具，简单的推理时干预就能带来显著性能提升。

Abstract: Recent search agents leverage multi-turn reasoning and search tools to achieve strong performance on multi-hop and long-horizon benchmarks. Yet it remains unclear whether they reliably reason across all requirements by tracking, verifying, and maintaining multiple conditions in these questions. We study this capability under multi-constraint problems, where valid answers must satisfy several constraints simultaneously. We find that illusory completion frequently occurs, wherein agents believe tasks are complete despite unresolved or violated constraints, leading to underverified answers. To diagnose this behavior, we introduce the Epistemic Ledger, an evaluation framework that tracks evidential support and agents' beliefs for each constraint throughout multi-turn reasoning. Our analysis reveals four recurring failure patterns: bare assertions, overlooked refutations, stagnation, and premature exit. Motivated by these findings, we examine whether explicit constraint-state tracking during execution mitigates these failures via LiveLedger, an inference-time tracker. This simple intervention consistently improves performance, substantially reducing underverified answers (by up to 26.5%) and improving overall accuracy (by up to 11.6%) on multi-constraint problems.

</details>


### [44] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

TL;DR: Verify-RL框架通过符号微分实现可验证的分解，确保子问题更简单、解决子问题有助于父任务、且分解关系有数学基础，相比启发式方法显著提升数学问题求解性能。


<details>
  <summary>Details</summary>
Motivation: 现有数学问题分解方法通常是启发式的，无法保证子问题更简单、解决子问题有助于父任务、或分解关系有数学基础，这限制了课程学习的效果。

Method: 利用符号微分作为可验证分解的自然结构：微积分规则明确定义了表达式如何分解为更简单的组件，并具有可证明的性质。提出Verify-RL框架，要求每个父子分解满足三个可验证条件：结构复杂度严格递减、解包含性、形式规则推导。

Result: 消除无效分解带来显著收益：最困难问题的准确率从32%翻倍至68%，整体相对改进达到40%。

Conclusion: 符号微分为数学问题的可验证分解提供了自然结构，通过"构造验证"确保分解质量，相比启发式方法能显著提升语言模型解决复杂数学问题的能力。

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>


### [45] [M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions](https://arxiv.org/abs/2602.07624)
*Junyu Feng,Binxiao Xu,Jiayi Chen,Mengyu Dai,Cenyang Wu,Haodong Li,Bohan Zeng,Yunliu Xie,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

TL;DR: M2A提出了一种双层级混合记忆系统，通过在线更新维护个性化多模态信息，解决了长期人机交互中个性化问答的挑战，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化多模态模型主要是静态的，概念在初始化时固定，无法在交互过程中演化。当对话历史跨越数周或数月并超出上下文窗口时，现有个性化机制难以持续吸收和利用用户增量概念、别名和偏好。

Method: 提出M2A，一个代理式双层级混合记忆系统，包含两个协作代理：ChatAgent管理用户交互并自主决定何时查询或更新记忆；MemoryManager将记忆请求分解为对双层级记忆库的详细操作。记忆库耦合了RawMessageStore（不可变对话日志）和SemanticMemoryStore（高级观察），提供不同粒度的记忆。还开发了可重用的数据合成管道，将Yo'LLaVA和MC-LLaVA中的概念基础会话注入LoCoMo长对话中，同时保持时间一致性。

Result: 实验表明M2A显著优于基线方法，证明将个性化从一次性配置转变为共同演化的记忆机制，为长期多模态交互中的高质量个性化响应提供了可行路径。

Conclusion: M2A通过在线更新的双层级记忆系统，成功解决了长期人机交互中的个性化挑战，将个性化从静态配置转变为动态演化的过程，为高质量个性化多模态交互提供了有效解决方案。

Abstract: This work addresses the challenge of personalized question answering in long-term human-machine interactions: when conversational history spans weeks or months and exceeds the context window, existing personalization mechanisms struggle to continuously absorb and leverage users' incremental concepts, aliases, and preferences. Current personalized multimodal models are predominantly static-concepts are fixed at initialization and cannot evolve during interactions. We propose M2A, an agentic dual-layer hybrid memory system that maintains personalized multimodal information through online updates. The system employs two collaborative agents: ChatAgent manages user interactions and autonomously decides when to query or update memory, while MemoryManager breaks down memory requests from ChatAgent into detailed operations on the dual-layer memory bank, which couples a RawMessageStore (immutable conversation log) with a SemanticMemoryStore (high-level observations), providing memories at different granularities. In addition, we develop a reusable data synthesis pipeline that injects concept-grounded sessions from Yo'LLaVA and MC-LLaVA into LoCoMo long conversations while preserving temporal coherence. Experiments show that M2A significantly outperforms baselines, demonstrating that transforming personalization from one-shot configuration to a co-evolving memory mechanism provides a viable path for high-quality individualized responses in long-term multimodal interactions. The code is available at https://github.com/Little-Fridge/M2A.

</details>


### [46] [SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures](https://arxiv.org/abs/2602.07628)
*Keondo Park,Younghoon Na,Yourim Choi,Hyunwoo Ryu,Hyun-Woo Shin,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: SleepMaMi是一个睡眠基础模型，通过分层双编码器设计同时建模整夜睡眠宏观结构和细粒度信号微观特征，在超过20,000个PSG记录上预训练，在多个下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前睡眠医学主要使用任务特定模型，这些模型关注局部微观结构特征，忽略了PSG的多模态上下文和整夜睡眠的全局宏观结构。需要统一的基础模型来同时掌握长时间睡眠架构和细粒度信号形态。

Method: 采用分层双编码器设计：宏观编码器建模整夜时间依赖关系，通过人口统计引导对比学习训练；微观编码器捕获生物信号的短期特征，通过混合掩码自编码器和多模态对比目标优化。在超过20,000个PSG记录（158K小时）上进行预训练。

Result: SleepMaMi在多样化的下游任务中优于现有基础模型，展示了卓越的泛化能力和标签高效适应能力，适用于临床睡眠分析。

Conclusion: SleepMaMi作为睡眠基础模型，成功解决了当前任务特定模型的局限性，能够同时掌握整夜睡眠的宏观结构和细粒度信号特征，为临床睡眠分析提供了强大的统一框架。

Abstract: While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.

</details>


### [47] [Efficient Table Retrieval and Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2602.07642)
*Zhuoyan Xu,Haoyang Fang,Boran Han,Bonan Min,Bernie Wang,Cuixiong Hu,Shuai Zhang*

Main category: cs.AI

TL;DR: TabRAG：一个用于大规模表格图像检索和推理的框架，通过视觉-文本基础模型检索候选表格，MLLM细粒度重排序，最终生成答案，显著提升检索召回率和答案准确率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中表格数据常以图像形式存在（如财务报表、手写记录、文档扫描），现有MLLM通常假设相关表格已准备好，但实际场景需要从大规模表格集合中识别和推理相关表格来回答用户查询。

Method: TabRAG框架：1）使用联合训练的视觉-文本基础模型检索候选表格；2）利用MLLM对候选表格进行细粒度重排序；3）使用MLLM在选定表格上进行推理生成答案。

Result: 在新构建的数据集（88,161训练样本和9,819测试样本，涵盖8个基准测试，48,504个独特表格）上，框架在检索召回率上提升7.0%，答案准确率提升6.1%，显著优于现有方法。

Conclusion: TabRAG为现实世界表格理解任务提供了实用解决方案，通过结合检索和推理能力，有效处理大规模表格图像集合的查询回答问题。

Abstract: Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.

</details>


### [48] [ONTrust: A Reference Ontology of Trust](https://arxiv.org/abs/2602.07662)
*Glenda Amaral,Tiago Prince Sales,Riccardo Baratella,Daniele Porello,Renata Guizzardi,Giancarlo Guizzardi*

Main category: cs.AI

TL;DR: 本文提出了一个基于统一基础本体论的信任参考本体(ONTrust)，旨在为信任提供坚实的本体论基础，支持信息建模、自动推理和信息集成等任务。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和区块链等技术的发展，信任变得比以往任何时候都更加重要。这些新技术有潜力改善产品和服务提供，促进个人和集体福祉，但其采用很大程度上取决于信任。为了构建可信系统，除了制定法律、法规和治理模型外，还需要对信任进行适当的概念化，使其能够被人类和机器理解。

Method: 开发了基于统一基础本体论(UFO)的信任参考本体(ONTrust)，使用OntoUML语言进行规范。该本体正式定义了信任概念及其不同类型，描述了影响信任的各种因素，并解释了信任关系如何产生风险。通过两个文献案例研究来展示ONTrust的实际应用。

Result: ONTrust已被应用于多个领域，包括概念建模和企业架构设计、语言评估与(重新)设计、信任管理、需求工程，以及在情感人机协作背景下的可信人工智能。本体能够支持信息建模、自动推理、信息集成和语义互操作等任务。

Conclusion: ONTrust为信任提供了一个坚实的本体论基础，有助于在新技术背景下理解和构建可信系统。该本体不仅支持理论分析，还具有实际应用价值，能够在多个领域促进信任相关问题的解决。

Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.

</details>


### [49] [EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge](https://arxiv.org/abs/2602.07695)
*Congcong Hu,Yuang Shi,Fan Huang,Yang Xiang,Zhou Ye,Ming Jin,Shiyu Wang*

Main category: cs.AI

TL;DR: EventCast是一个将未来事件知识整合到时间序列预测中的模块化框架，专门解决电商在闪购、节日促销等突发事件期间的需求预测问题，通过LLM处理非结构化业务数据并生成可解释的文本摘要，在真实电商场景中显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有预测系统在闪购、节日促销和政策干预等高影响时期经常失效，因为这些时期的需求模式会突然且不可预测地变化。电商运营需要能够处理这些突发事件影响的预测解决方案。

Method: EventCast采用模块化框架，利用LLM处理非结构化业务数据（如营销活动、节假日安排、卖家激励），将其转换为可解释的文本摘要。这些摘要与历史需求特征通过双塔架构融合，LLM仅用于事件驱动推理，而非数值预测。

Result: 在4个国家160个区域超过10个月的真实电商场景中，EventCast相比无事件知识的变体在MAE和MSE上分别提升了86.9%和97.7%，在事件驱动期间相比最佳工业基线分别减少了57.0%的MAE和83.3%的MSE。自2025年3月起已部署到实际工业管道中。

Conclusion: EventCast通过将未来事件知识整合到时间序列预测中，为动态电商环境提供了准确、可解释且可扩展的预测解决方案，显著改善了运营决策制定。

Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.

</details>


### [50] [Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution](https://arxiv.org/abs/2602.07749)
*Zhenyu Wu,Yanxi Long,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: Geo-coder：首个基于多智能体系统的几何图像逆向编程框架，通过像素级锚定和度量驱动代码演化实现精确几何重建，在几何重建精度和视觉一致性方面显著领先。


<details>
  <summary>Details</summary>
Motivation: 当前逆向图形方法在重建复杂几何细节时面临巨大挑战，常导致关键几何约束丢失或结构失真。程序代码作为连接视觉与逻辑的桥梁，为通过几何操作增强大模型多模态推理能力提供了可行的监督方法。

Method: 提出Geo-coder框架，创新性地将过程解耦为两个阶段：1）通过像素级锚定进行几何建模，利用视觉算子和大模型的互补优势精确捕获像素坐标和视觉属性；2）引入合成-渲染-验证闭环，通过双向视觉反馈驱动代码自校正。

Result: 大量实验表明，Geo-coder在几何重建精度和视觉一致性方面取得显著领先。通过有效保留核心几何语义，重建图像在多模态推理任务中表现出与原始图像相当的性能。开源了包含1500+样本的Geo-coder数据集和GeocodeLM模型。

Conclusion: Geo-coder框架通过多智能体系统和两阶段解耦方法成功解决了复杂几何细节重建的瓶颈问题，为后续研究提供了坚实的数据和模型基础，验证了框架的鲁棒性。

Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.

</details>


### [51] [Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency](https://arxiv.org/abs/2602.07754)
*Bahare Riahi,Veronica Catete*

Main category: cs.AI

TL;DR: 学生如何看待AI评分系统？研究发现AI在公平性、信任度、一致性和透明度方面存在问题，缺乏情境理解和个性化，建议AI应作为人类监督下的辅助工具。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解学生对AI评分系统的看法，特别是关注AI评分在公平性、信任度、一致性和透明度方面的表现，以及AI反馈与人工反馈的差异。

Method: 采用基于Jobin（2019）伦理原则框架的研究设计，在本科计算机科学课程中对27名学生进行调查，比较AI生成的反馈与原始人工评分反馈。

Result: 研究发现学生对AI评分系统存在担忧，主要问题是AI缺乏情境理解和个性化能力。学生认为AI反馈不如人工反馈灵活和有同理心。

Conclusion: 结论是公平可信的AI系统应该反映人类判断、灵活性和同理心，建议AI作为人类监督下的补充工具，而非完全替代人工评分。

Abstract: This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.

</details>


### [52] [Learning to Continually Learn via Meta-learning Agentic Memory Designs](https://arxiv.org/abs/2602.07755)
*Yiming Xiong,Shengran Hu,Jeff Clune*

Main category: cs.AI

TL;DR: ALMA框架通过元学习自动生成记忆设计，替代人工设计的记忆模块，使智能体系统能够在多样化的真实世界任务中实现持续学习。


<details>
  <summary>Details</summary>
Motivation: 基础模型的无状态性限制了智能体系统的持续学习能力，而现有记忆设计多为人工固定设计，无法适应真实世界任务的多样性和非平稳性。

Method: 采用元智能体搜索以可执行代码表达的记忆设计，理论上能够发现任意记忆设计，包括数据库模式及其检索和更新机制。

Result: 在四个顺序决策领域的广泛实验中，学习到的记忆设计在所有基准测试中都比最先进的人工设计记忆更有效和高效。

Conclusion: ALMA代表了向自我改进AI系统迈出的一步，这些系统能够学会成为适应性的持续学习者。

Abstract: The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.

</details>


### [53] [Disentangled Instrumental Variables for Causal Inference with Networked Observational Data](https://arxiv.org/abs/2602.07765)
*Zhirong Huang,Debo Cheng,Guixian Zhang,Yi Wang,Jiuyong Li,Shichao Zhang*

Main category: cs.AI

TL;DR: 提出DisIV框架，通过结构解耦机制从网络数据中提取个体特异性成分作为潜在工具变量，解决网络数据中工具变量外生性假设的挑战。


<details>
  <summary>Details</summary>
Motivation: 网络数据中工具变量的外生性假设面临重大挑战，现有方法在恢复工具变量时通常依赖邻居信息建模，这不可避免地混合了共享环境导致的内生相关性和个体特异性外生变异，导致所得工具变量继承了对未观测混杂因素的依赖并违反外生性。

Method: 提出DisIV（解耦工具变量）框架，利用网络同质性作为归纳偏置，采用结构解耦机制提取个体特异性成分作为潜在工具变量，并通过显式的正交性和排除条件约束提取工具变量的因果有效性。

Result: 在真实世界数据集上的大量半合成实验表明，DisIV在网络诱导混杂下的因果效应估计中始终优于最先进的基线方法。

Conclusion: DisIV框架通过解耦网络数据中的个体特异性成分，成功解决了网络数据中工具变量外生性假设的挑战，为存在潜在混杂因素的网络观测数据因果推断提供了有效方法。

Abstract: Instrumental variables (IVs) are crucial for addressing unobservable confounders, yet their stringent exogeneity assumptions pose significant challenges in networked data. Existing methods typically rely on modelling neighbour information when recovering IVs, thereby inevitably mixing shared environment-induced endogenous correlations and individual-specific exogenous variation, leading the resulting IVs to inherit dependence on unobserved confounders and to violate exogeneity. To overcome this challenge, we propose $\underline{Dis}$entangled $\underline{I}$nstrumental $\underline{V}$ariables (DisIV) framework, a novel method for causal inference based on networked observational data with latent confounders. DisIV exploits network homogeneity as an inductive bias and employs a structural disentanglement mechanism to extract individual-specific components that serve as latent IVs. The causal validity of the extracted IVs is constrained through explicit orthogonality and exclusion conditions. Extensive semi-synthetic experiments on real-world datasets demonstrate that DisIV consistently outperforms state-of-the-art baselines in causal effect estimation under network-induced confounding.

</details>


### [54] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

TL;DR: Minitap是首个在AndroidWorld基准测试中实现100%成功率的移动设备多智能体系统，超越了人类80%的表现，解决了单智能体架构的三个关键失败原因。


<details>
  <summary>Details</summary>
Motivation: 单智能体架构在移动设备任务执行中存在三个主要问题：混合推理轨迹导致的上下文污染、文本输入失败未被检测、以及重复动作循环无法逃脱。这些限制阻碍了移动设备自动化系统达到人类水平的性能。

Method: Minitap采用多智能体架构，包含六个专门化智能体实现认知分离；通过确定性后验证机制检查文本输入与设备状态的一致性；引入元认知推理检测动作循环并触发策略变更。

Result: 在AndroidWorld基准测试的116个任务中实现100%成功率，首次完全解决所有任务，超越人类80%的表现。消融实验显示：多智能体分解贡献+21分，验证执行+7分，元认知+9分。

Conclusion: Minitap通过针对性的多智能体架构、验证机制和元认知推理，成功解决了移动设备自动化的关键挑战，实现了超越人类的表现，为移动设备自动化系统提供了新的解决方案。

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [55] [Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training](https://arxiv.org/abs/2602.07824)
*Yiwei Qin,Zhen Huang,Tiantian Mi,Weiye Si,Chenyang Zhou,Qipeng Guo,Siyuan Feng,Pengfei Liu*

Main category: cs.AI

TL;DR: 提出Data Darwinism十级分类法，通过数据-模型协同进化提升基础模型性能，在科学文献上验证了高级数据处理能显著提升模型表现


<details>
  <summary>Details</summary>
Motivation: 数据质量决定基础模型性能，但缺乏系统化的数据处理框架。需要建立数据与模型协同进化的方法论，让先进模型为下一代系统产生更优质数据

Method: 1. 提出Data Darwinism十级分类法(L0-L9)描述数据-模型协同进化；2. 构建Darwin-Science科学语料库(900B tokens, L0-L5)；3. 使用前沿LLM进行L4(生成精炼)和L5(认知补全)处理；4. 从头训练daVinci-origin-3B/7B模型作为无污染基线；5. 进行600B tokens的持续预训练

Result: 1. Darwin-Science模型在20+基准测试中分别比基线提升+2.12(3B)和+2.95(7B)分；2. 在领域对齐任务上提升+5.60和+8.40分；3. 系统推进到L5处理带来+1.36分总增益；4. 证实高级数据处理能解锁数据的潜在价值

Conclusion: Data Darwinism框架有效，数据-模型协同进化能显著提升模型性能。高级数据处理(L4/L5)能弥补原始科学文本的可学习性差距。发布了Darwin-Science语料库和daVinci-origin模型以支持原则性的协同进化发展

Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.
  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.

</details>


### [56] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: VeriTime是一个通过数据合成、数据调度和强化学习训练来定制LLMs进行时间序列推理的框架，使小型模型（3B、4B）能够达到或超过大型专有LLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在推理能力方面取得了进展，但将其应用于时间序列任务仍处于早期阶段，主要障碍包括：缺乏精心策划的时间序列CoT训练数据、数据调度效率低下、以及缺乏专门针对时间序列CoT数据的RL算法。

Method: 1. 数据合成管道：构建具有过程可验证注释的TS-文本多模态数据集；2. 数据调度机制：根据难度层次和任务分类原则安排训练样本；3. 两阶段强化微调：利用可验证的过程级CoT数据，设计细粒度、多目标奖励。

Result: VeriTime显著提升了LLMs在多样化时间序列推理任务上的性能，使紧凑的3B、4B模型能够达到与大型专有LLMs相当或更好的推理能力。

Conclusion: VeriTime通过系统化的数据合成、智能数据调度和专门的强化学习训练，成功解决了LLMs在时间序列推理中的关键挑战，为小型模型实现高效时间序列推理提供了可行方案。

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [57] [LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge](https://arxiv.org/abs/2602.07849)
*Xin Wang,Hualin Zhou,Sheng Guang Wang,Ting Dang,Yu Zhang,Hong Jia,Tao Gu*

Main category: cs.AI

TL;DR: LQA：轻量级量化自适应框架，用于在边缘设备上部署视觉语言模型，通过选择性混合量化和无梯度测试时适应，在资源受限条件下提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署视觉语言模型面临资源限制和分布偏移下的性能下降问题。现有测试时适应方法资源消耗过大，不适合边缘部署。

Method: 提出LQA框架，包含选择性混合量化策略（SHQ）和量化无梯度适应机制，实现轻量级、隐私保护的边缘部署。

Result: 在合成和真实分布偏移实验中，LQA提升适应性能4.5%，内存使用低于全精度模型，比基于梯度的TTA方法内存使用降低19.9倍。

Conclusion: LQA为边缘设备上的视觉语言模型部署提供了实用路径，实现了鲁棒、隐私保护和高效的部署方案。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.

</details>


### [58] [Emergent Misalignment is Easy, Narrow Misalignment is Hard](https://arxiv.org/abs/2602.07852)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.AI

TL;DR: 微调大语言模型于狭窄有害数据集可能导致涌现性错位，使模型在无关场景中产生刻板"邪恶"回应。研究发现通用错位解比狭窄解更稳定高效，并分离出可监控的线性表征。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在微调过程中的涌现性错位现象，专家预测失败凸显对LLM归纳偏置理解不足，需要探究学习与泛化的底层机制。

Method: 使用涌现性错位作为案例研究，发现不同微调收敛到相同的通用错位线性表征；引入KL散度损失学习狭窄解表征；比较两种表征的稳定性、效率和影响力。

Result: 通用错位解比狭窄解损失更低、对扰动更鲁棒、在预训练分布中影响力更大；分离出可监控的通用错位线性表征；提供代码、数据集和模型开源。

Conclusion: 本研究分离出可监控的通用错位表征，为LLM归纳偏置如何塑造泛化提供了详细案例和初步度量，有助于错位行为的监测与缓解。

Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.

</details>


### [59] [ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation](https://arxiv.org/abs/2602.07883)
*Jingqi Zhou,Sheng Wang,DeZhao Deng,Junwen Lu,Junwei Su,Qintong Li,Jiahui Gao,Hao Wu,Jiyue Jiang,Lingpeng Kong,Chuan Wu*

Main category: cs.AI

TL;DR: ToolSelf：一种工具驱动的运行时自重构范式，让LLM智能体能够自主更新子目标、上下文、策略和工具箱，实现从被动执行者到任务与自我双重管理者的转变。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体系统受限于静态配置，这些配置在执行前固定，无法适应动态任务变化。现有方法依赖人工编排或启发式补丁，泛化能力差且优化碎片化。

Method: 提出ToolSelf范式，将配置更新抽象为可调用工具，统一任务执行和自调整到单一动作空间。设计配置感知两阶段训练（CAT），结合拒绝采样微调和轨迹级强化学习来内化这种元能力。

Result: 在多样化基准测试中，ToolSelf能够媲美专用工作流，同时泛化到新任务，平均性能提升24.1%。

Conclusion: ToolSelf实现了从外部规则到内在参数的范式转变，为真正自适应的智能体开辟了道路，使智能体能够自主管理任务和自身配置。

Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.

</details>


### [60] [MemFly: On-the-Fly Memory Optimization via Information Bottleneck](https://arxiv.org/abs/2602.07885)
*Zhenyuan Zhang,Xianzhang Jia,Zhiqin Yang,Zhenbo Song,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: MemFly是一个基于信息瓶颈原则的LLM记忆框架，通过梯度自由优化器构建分层记忆结构，结合混合检索机制，在记忆连贯性、响应保真度和准确性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有记忆框架面临一个基本困境：既要高效压缩冗余信息，又要为下游任务保持精确检索。需要弥合这一差距。

Method: 基于信息瓶颈原则，通过梯度自由优化器最小化压缩熵同时最大化相关熵，构建分层记忆结构。开发混合检索机制，整合语义、符号和拓扑路径，并采用迭代精炼处理复杂多跳查询。

Result: 综合实验表明，MemFly在记忆连贯性、响应保真度和准确性方面显著优于最先进的基线方法。

Conclusion: MemFly框架成功解决了记忆压缩与精确检索之间的困境，为LLM代理处理复杂任务提供了有效的长期记忆解决方案。

Abstract: Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.

</details>


### [61] [GCN-MPPR: Enhancing the Propagation of Message Passing Neural Networks via Motif-Based Personalized PageRank](https://arxiv.org/abs/2602.07903)
*Mingcan Wang,Junchang Xin,Zhongming Yao,Kaifu Long,Zhiqiong Wang*

Main category: cs.AI

TL;DR: 提出基于motif的个性化PageRank(MPPR)来改进图卷积网络，通过考虑高阶关系指导消息传递，提升GCN的深度、准确性和稳定性


<details>
  <summary>Details</summary>
Motivation: 现有基于消息传递神经网络(MPNN)的图算法通常只能传播到有限的邻域（浅层），主要因为过平滑问题。现有方法在优化或结构层面的改进仍存在准确率有限、稳定性差、计算成本高等问题，且忽略了消息传递过程中的高阶关系

Method: 提出motif-based personalized PageRank (MPPR)来基于高阶motif关系衡量节点间影响力，然后将MPPR应用于GCN的消息传递过程，在相对"高"层次上指导消息传递

Result: 实验结果表明，该方法在准确性、稳定性和时间消耗方面优于几乎所有基线方法。此外，该方法可作为支撑几乎所有GCN任务的组件，实验中展示了DGCRL的应用

Conclusion: 通过引入基于motif的个性化PageRank来考虑高阶关系，有效解决了GCN的深度限制问题，提升了图卷积网络的整体性能，具有广泛的应用潜力

Abstract: The algorithms based on message passing neural networks (MPNNs) on graphs have recently achieved great success for various graph applications. However, studies find that these methods always propagate the information to very limited neighborhoods with shallow depth, particularly due to over-smoothing. That means most of the existing MPNNs fail to be so `deep'. Although some previous work tended to handle this challenge via optimization- or structure-level remedies, the overall performance of GCNs still suffers from limited accuracy, poor stability, and unaffordable computational cost. Moreover, neglect of higher-order relationships during the propagation of MPNNs has further limited the performance of them. To overcome these challenges, a novel variant of PageRank named motif-based personalized PageRank (MPPR) is proposed to measure the influence of one node to another on the basis of considering higher-order motif relationships. Secondly, the MPPR is utilized to the message passing process of GCNs, thereby guiding the message passing process at a relatively `high' level. The experimental results show that the proposed method outperforms almost all of the baselines on accuracy, stability, and time consumption. Additionally, the proposed method can be considered as a component that can underpin almost all GCN tasks, with DGCRL being demonstrated in the experiment. The anonymous code repository is available at: https://anonymous.4open.science/r/GCN-MPPR-AFD6/.

</details>


### [62] [MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation](https://arxiv.org/abs/2602.07905)
*Yu Zhao,Hao Guan,Yongcheng Jing,Ying Zhang,Dacheng Tao*

Main category: cs.AI

TL;DR: MedCoG：基于知识图谱的医学元认知代理，通过元认知评估动态调节知识使用，以缓解LLM推理扩展定律的收益递减问题，提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂医学推理中表现出潜力，但面临推理扩展定律下的收益递减问题。现有研究通过增加各种知识来增强LLM，但额外成本转化为准确性的效果不明确。

Method: 提出MedCoG（Medical Meta-Cognition Agent with Knowledge Graph），通过元认知评估（任务复杂性、熟悉度、知识密度）动态调节程序性、情景性和事实性知识的使用。采用LLM中心按需推理方法。

Result: 在五个困难医学基准测试中验证了MedCoG的有效性和效率，实现了5.5倍的推理密度（推理效率指标）。Oracle研究突显了元认知调节的显著潜力。

Conclusion: 元认知调节能够有效缓解LLM推理扩展定律问题，通过避免盲目扩展降低成本，通过过滤干扰知识提高准确性，为医学推理提供了高效解决方案。

Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.

</details>


### [63] [Selective Fine-Tuning for Targeted and Robust Concept Unlearning](https://arxiv.org/abs/2602.07919)
*Mansi,Avinash Kori,Francesca Toni,Soteris Demetriou*

Main category: cs.AI

TL;DR: TRUST是一种针对扩散模型的概念遗忘方法，通过动态定位目标概念神经元并进行选择性微调，结合Hessian正则化，实现高效、鲁棒的概念遗忘。


<details>
  <summary>Details</summary>
Motivation: 文本引导扩散模型容易被恶意利用生成有害内容，现有概念遗忘方法要么只能处理单个概念，要么需要全模型微调计算成本高，而静态概念定位方法效果不佳。

Method: 提出TRUST方法：1）动态估计目标概念神经元；2）通过选择性微调进行概念遗忘；3）结合Hessian正则化增强鲁棒性。

Result: 实验表明TRUST能有效对抗对抗性提示，显著保持生成质量，比SOTA方法更快，并能处理单个概念、概念组合和条件概念，无需特定正则化。

Conclusion: TRUST提供了一种高效、鲁棒的概念遗忘解决方案，解决了现有方法在计算成本、鲁棒性和概念组合处理方面的局限性。

Abstract: Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.

</details>


### [64] [MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learnin](https://arxiv.org/abs/2602.07940)
*Guanglong Sun,Hongwei Yan,Liyuan Wang,Zhiqi Kang,Shuang Cui,Hang Su,Jun Zhu,Yi Zhong*

Main category: cs.AI

TL;DR: MePo是一种基于预训练模型的通用持续学习方法，通过元后精炼策略和元协方差矩阵，在无排练的情况下显著提升GCL性能


<details>
  <summary>Details</summary>
Motivation: 智能系统需要从复杂、演化的环境中持续学习并实时响应，但现有基于预训练模型的持续学习方法在处理多样且时间混合的信息时表现不佳，导致通用持续学习性能不理想

Method: 提出Meta Post-Refinement (MePo)方法：1) 从预训练数据构建伪任务序列；2) 开发双层元学习范式精炼预训练骨干网络；3) 初始化元协方差矩阵作为预训练表示空间的参考几何，利用二阶统计进行鲁棒输出对齐

Result: MePo在多种GCL基准测试和预训练检查点上取得显著性能提升，在无排练情况下：CIFAR-100提升15.10%，ImageNet-R提升13.36%，CUB-200提升12.56%（Sup-21/1K设置）

Conclusion: MePo作为一种即插即用策略，通过元后精炼和元协方差矩阵有效提升了基于预训练模型的通用持续学习性能，在多个基准测试中表现出色

Abstract: To cope with uncertain changes of the external world, intelligent systems must continually learn from complex, evolving environments and respond in real time. This ability, collectively known as general continual learning (GCL), encapsulates practical challenges such as online datastreams and blurry task boundaries. Although leveraging pretrained models (PTMs) has greatly advanced conventional continual learning (CL), these methods remain limited in reconciling the diverse and temporally mixed information along a single pass, resulting in sub-optimal GCL performance. Inspired by meta-plasticity and reconstructive memory in neuroscience, we introduce here an innovative approach named Meta Post-Refinement (MePo) for PTMs-based GCL. This approach constructs pseudo task sequences from pretraining data and develops a bi-level meta-learning paradigm to refine the pretrained backbone, which serves as a prolonged pretraining phase but greatly facilitates rapid adaptation of representation learning to downstream GCL tasks. MePo further initializes a meta covariance matrix as the reference geometry of pretrained representation space, enabling GCL to exploit second-order statistics for robust output alignment. MePo serves as a plug-in strategy that achieves significant performance gains across a variety of GCL benchmarks and pretrained checkpoints in a rehearsal-free manner (e.g., 15.10\%, 13.36\%, and 12.56\% on CIFAR-100, ImageNet-R, and CUB-200 under Sup-21/1K). Our source code is available at \href{https://github.com/SunGL001/MePo}{MePo}

</details>


### [65] [IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery](https://arxiv.org/abs/2602.07943)
*Ivaxi Sheth,Zhijing Jin,Bryan Wilder,Dominik Janzing,Mario Fritz*

Main category: cs.AI

TL;DR: LLMs可以辅助识别有效的工具变量，通过两阶段评估框架验证其能力，并开发了IV Co-Scientist多智能体系统来提出、批判和优化工具变量。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，工具变量的识别需要跨学科知识、创造力和上下文理解，这是一项困难的任务。本文研究大型语言模型是否能够辅助这一过程，帮助从大型观测数据库中识别有效的工具变量。

Method: 采用两阶段评估框架：第一阶段测试LLMs能否从文献中恢复已确立的工具变量；第二阶段评估LLMs能否识别和避免已被实证或理论否定的工具变量。基于此，开发了IV Co-Scientist多智能体系统，该系统能够提出、批判和优化工具变量，并引入统计测试来在没有真实标签的情况下评估一致性。

Result: 结果显示LLMs能够从大型观测数据库中识别有效的工具变量，证明了LLMs在辅助工具变量发现方面的潜力。

Conclusion: 大型语言模型在识别有效工具变量方面具有潜力，IV Co-Scientist系统能够系统性地提出、评估和优化工具变量，为因果推断中的工具变量识别提供了新的自动化方法。

Abstract: In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.

</details>


### [66] [LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth](https://arxiv.org/abs/2602.07962)
*Weihao Zeng,Yuzhen Huang,Junxian He*

Main category: cs.AI

TL;DR: LOCA-bench是一个用于评估长上下文语言代理的基准测试，通过自动化环境状态控制来调节上下文长度，模拟真实世界中动态增长的环境。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准主要关注单步信息检索，但实际应用中LLM需要作为代理在动态增长的环境中执行复杂任务，而上下文过长会导致性能下降（"上下文腐化"）。

Method: 通过任务提示和自动化环境状态控制来调节代理的上下文长度，使上下文长度可无限扩展同时保持任务语义不变，评估模型与脚手架（包括各种上下文管理策略）的组合表现。

Result: 随着环境状态复杂度增加，代理性能普遍下降，但先进的上下文管理技术能显著提高整体成功率。

Conclusion: LOCA-bench为评估长上下文代理场景中的模型和脚手架提供了平台，开源以促进该领域研究。

Abstract: Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench

</details>


### [67] [Accelerating Social Science Research via Agentic Hypothesization and Experimentation](https://arxiv.org/abs/2602.07983)
*Jishu Sen Gupta,Harini SI,Somesh Kumar Singh,Syed Mohamad Tawseeq,Yaman Kumar Singla,David Doermann,Rajiv Ratn Shah,Balaji Krishnamurthy*

Main category: cs.AI

TL;DR: EXPERIGEN是一个端到端的科学发现框架，通过生成器-实验者的两阶段搜索，在多个领域发现比现有方法多2-4倍的统计显著假设，且预测性能提升7-17%。专家评审显示88%的假设具有新颖性，70%值得进一步研究，并通过A/B测试验证了实际效果。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的社会科学研究过程缓慢，依赖观察、假设生成和实验验证的迭代循环。现有数据驱动方法虽然能加速部分过程，但无法支持端到端的科学发现。需要开发能够完整支持科学发现过程的框架。

Method: 提出EXPERIGEN框架，采用贝叶斯优化启发的两阶段搜索：生成器提出候选假设，实验者进行实证评估。框架支持多模态和关系数据集等复杂数据机制。

Result: 在多个领域，EXPERIGEN发现比现有方法多2-4倍的统计显著假设，预测性能提升7-17%。专家评审显示：88%的假设具有中等或强新颖性，70%被认为有影响力且值得研究，大多数假设的严谨性达到高年级研究生水平。A/B测试显示统计显著结果(p<1e-6)，效应大小达344%。

Conclusion: EXPERIGEN能够有效支持端到端的科学发现，不仅提升统计性能，还能生成新颖、有实证基础且可操作的假设，推动真正的科学进步。首次通过A/B测试验证了LLM生成假设的实际效果。

Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.

</details>


### [68] [Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective](https://arxiv.org/abs/2602.08009)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Quanyu Dai,Chaozhuo Li,Feng Wen,Xu Chen*

Main category: cs.AI

TL;DR: RAPS：基于信誉感知的发布-订阅范式，用于实现LLM多智能体的自适应、可扩展和鲁棒协调


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多智能体架构需要大量人工编排，亟需自动化设计智能体工作流程。智能体协调面临动态自组织网络中的经典问题：如何在可扩展数量的智能体主机之间建立自适应可靠的通信？

Method: RAPS基于分布式发布-订阅协议，让LLM智能体基于声明的意图而非预定义拓扑交换消息。包含两个核心覆盖层：1) 反应式订阅：使智能体动态优化意图；2) 贝叶斯信誉：为每个智能体提供本地监控器来检测和隔离恶意节点

Result: 在五个基准测试上的广泛实验表明，RAPS设计有效统一了多智能体协调框架中的自适应性、可扩展性和鲁棒性

Conclusion: RAPS通过信誉感知的发布-订阅范式，成功解决了LLM多智能体协调中的自适应、可扩展和鲁棒性问题，为自动化智能体工作流程设计提供了有效解决方案

Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We frame such an agent coordination challenge as a classic problem in dynamic ad-hoc networking: How to establish adaptive and reliable communication among a scalable number of agentic hosts? In response to this unresolved dilemma, we introduce RAPS, a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents. RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on their declared intents rather than predefined topologies. Beyond this substrate, RAPS further incorporates two coherent overlays: (i) Reactive Subscription, enabling agents to dynamically refine their intents; and (ii) Bayesian Reputation, empowering each agent with a local watchdog to detect and isolate malicious peers. Extensive experiments over five benchmarks showcase that our design effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.

</details>


### [69] [Small Agent Group is the Future of Digital Health](https://arxiv.org/abs/2602.08013)
*Yuqiao Meng,Luoxi Tang,Dazheng Zhang,Rafael Brens,Elvys J. Romero,Nancy Guo,Safa Elkefi,Zhaohan Xi*

Main category: cs.AI

TL;DR: SAG（小型智能体群）通过协同推理机制，在临床场景中替代模型参数增长，实现效果、可靠性和部署成本的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数字健康领域采用"规模优先"范式，但临床实际需求不仅需要效果，还需要可靠性和合理的部署成本。临床决策本质上是协作过程，因此挑战单一模型扩展范式，探索小型智能体群是否能支持更好的临床推理。

Method: 提出SAG（小型智能体群）方法，从单一模型智能转向集体专业知识，通过协作审议过程分配推理、循证分析和关键审计。使用涵盖效果、可靠性和部署成本的多样化临床指标进行广泛评估。

Result: SAG在有无额外优化或检索增强生成的情况下，均优于单一大型模型。结果表明SAG的协同推理可以替代临床场景中的模型参数增长。

Conclusion: SAG为数字健康提供了可扩展的解决方案，能更好地平衡效果、可靠性和部署效率，挑战了传统的"规模优先"范式。

Abstract: The rapid adoption of large language models (LLMs) in digital health has been driven by a "scaling-first" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.

</details>


### [70] [Structure-Aware Robust Counterfactual Explanations via Conditional Gaussian Network Classifiers](https://arxiv.org/abs/2602.08021)
*Zhan-Yi Liao,Jaewon Yoo,Hao-Tsung Yang,Po-An Chen*

Main category: cs.AI

TL;DR: 提出基于条件高斯网络分类器的结构感知、鲁棒性导向的反事实解释搜索方法，通过DAG编码特征依赖关系，使用切割集方法和McCormick松弛将非凸二次问题转化为MILP，实现全局最优解。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法往往忽视特征间的条件依赖关系和潜在因果结构，且缺乏对鲁棒性的系统保证。需要一种既能考虑模型结构假设，又能提供全局鲁棒性保证的反事实解释框架。

Method: 基于条件高斯网络分类器（CGNC）构建生成式结构，通过DAG编码特征依赖关系；采用收敛保证的切割集程序作为对抗优化框架；使用分段McCormick松弛将非凸二次问题转化为混合整数线性规划（MILP）。

Result: 实验结果表明该方法实现了强鲁棒性，直接全局优化原始公式提供了特别稳定和高效的结果。框架可扩展到更复杂的约束设置。

Conclusion: 提出的框架为在非凸二次公式下进行反事实推理的未来进展奠定了基础，通过结构感知和鲁棒性导向的方法，为可解释AI提供了更可靠的反事实解释工具。

Abstract: Counterfactual explanation (CE) is a core technique in explainable artificial intelligence (XAI), widely used to interpret model decisions and suggest actionable alternatives. This work presents a structure-aware and robustness-oriented counterfactual search method based on the conditional Gaussian network classifier (CGNC). The CGNC has a generative structure that encodes conditional dependencies and potential causal relations among features through a directed acyclic graph (DAG). This structure naturally embeds feature relationships into the search process, eliminating the need for additional constraints to ensure consistency with the model's structural assumptions. We adopt a convergence-guaranteed cutting-set procedure as an adversarial optimization framework, which iteratively approximates solutions that satisfy global robustness conditions. To address the nonconvex quadratic structure induced by feature dependencies, we apply piecewise McCormick relaxation to reformulate the problem as a mixed-integer linear program (MILP), ensuring global optimality. Experimental results show that our method achieves strong robustness, with direct global optimization of the original formulation providing especially stable and efficient results. The proposed framework is extensible to more complex constraint settings, laying the groundwork for future advances in counterfactual reasoning under nonconvex quadratic formulations.

</details>


### [71] [Free(): Learning to Forget in Malloc-Only Reasoning Models](https://arxiv.org/abs/2602.08030)
*Yilun Zheng,Dongyang Ma,Tian Liang,Jiahao Xu,Xinting Huang,Lijie Chen,Haitao Mi,Yan Wang*

Main category: cs.AI

TL;DR: Free()LM通过引入可遗忘机制解决推理模型过度思考导致性能下降的问题，在多种规模模型上实现性能提升，特别是在长序列任务中恢复崩溃的性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型存在一个关键悖论：过多的思考token反而会降低性能而非提升。作者认为这是由于标准LLM作为"malloc-only"引擎，持续积累有效和冗余步骤而没有修剪过时信息的机制。

Method: 提出Free()LM模型，通过Free-Module（即插即用的LoRA适配器）引入内在的自遗忘能力。模型在推理模式和清理模式之间迭代切换，动态识别并修剪无用的上下文块，保持紧凑且无噪声的状态。

Result: 在所有模型规模（8B到685B）上均实现一致改进，平均比顶级推理基线提升3.3%，在IMOanswerBench上建立新的SOTA。在标准Qwen3-235B-A22B模型完全崩溃（0%准确率）的长序列任务中，Free()LM将性能恢复到50%。

Conclusion: 可持续的智能既需要思考的能力，也需要遗忘的自由。Free()LM通过引入自遗忘机制有效解决了推理模型过度思考导致性能下降的问题。

Abstract: Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as "malloc-only" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.
  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.

</details>


### [72] [Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling](https://arxiv.org/abs/2602.08052)
*Bulent Soykan,Sean Mondesire,Ghaith Rabadi,Grace Bochenek*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度强化学习（PPO-GNN）的方法，用于解决具有释放时间、设置和资格约束的不相关并行机调度问题，同时最小化总加权延迟和总设置时间。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以平衡总加权延迟和总设置时间这两个目标，特别是在具有释放时间、设置和资格约束的复杂调度场景中。

Method: 使用近端策略优化（PPO）和图神经网络（GNN）的深度强化学习框架。GNN有效表示作业、机器和设置的复杂状态，PPO代理学习直接调度策略，通过多目标奖励函数同时优化两个目标。

Result: 在基准实例上的实验结果表明，PPO-GNN代理显著优于标准调度规则和元启发式算法，在两个目标之间实现了更优的权衡。

Conclusion: 该方法为复杂制造调度提供了鲁棒且可扩展的解决方案，能够有效处理多目标优化问题。

Abstract: The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.

</details>


### [73] [Securing Dual-Use Pathogen Data of Concern](https://arxiv.org/abs/2602.08061)
*Doni Bloomfield,Allison Berke,Moritz S. Hanke,Aaron Maiwald,James R. M. Black,Toby Webster,Tina Hernandez-Boussard,Oliver M. Crook,Jassi Pannu*

Main category: cs.AI

TL;DR: 提出五级生物安全数据框架（BDL），根据数据对AI模型生物安全风险的影响程度分类病原体数据，并针对不同风险级别提出技术限制和治理框架，以控制AI在生物武器开发等有害应用中的使用。


<details>
  <summary>Details</summary>
Motivation: 随着AI在生物学领域的广泛应用，训练数据成为AI模型能力的关键决定因素，包括可能带来生物安全风险的能力。国际研究团体呼吁建立数据控制机制，防止AI被用于生物武器开发等有害应用，因此需要设计有效的数据控制框架。

Method: 提出了五级生物安全数据框架（BDL），根据不同类型病原体数据对AI模型生物安全风险能力的预期贡献程度进行分类。针对每个BDL层级，设计了相应的技术限制措施。同时，为新创建的双用途病原体数据提出了创新的治理框架。

Result: 建立了系统的病原体数据分类框架，能够根据数据对AI模型生物安全风险的影响程度进行分级管理。为不同风险级别的数据设计了相应的技术控制措施，并提出了针对双用途病原体数据的治理方案。

Conclusion: 在计算和编码资源广泛可及的世界中，数据控制可能是减少令人担忧的生物AI能力扩散的最有效干预措施之一。提出的BDL框架和治理方案为实施生物安全数据控制提供了实用工具。

Abstract: Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.

</details>


### [74] [Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities](https://arxiv.org/abs/2602.08092)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 论文挑战了AI对齐中的关键假设（Dogma 4），即人类反馈总是真实信号。作者证明在社交环境中该假设失效，导致"目标解耦"问题，并提出"认知源对齐"方法来解决。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐策略依赖一个脆弱前提：人类反馈虽然嘈杂，但本质上是真实信号。作者认为这个假设（Dogma 4）在静态环境中成立，但在社交环境中失效，因为评估者可能阿谀奉承、懒惰或敌对。

Method: 提出"认知源对齐"方法。与传统基于统计共识的方法不同，ESA使用稀疏安全公理来判断反馈的来源而非信号本身。这种"评判评判者"机制能保证收敛到真实目标，即使多数评估者有偏见。

Result: 理论证明：在Dogma 4下，标准RL智能体会遭受"目标解耦"的结构性故障模式。ESA方法能保证收敛到真实目标。实证显示：传统共识方法在多数合谋时失败，而ESA方法成功恢复最优策略。

Conclusion: 人类反馈作为真实信号的假设在社交环境中不成立，会导致AI系统永久性错位。通过"评判评判者"而非依赖统计共识的认知源对齐方法，能够保证AI系统即使在多数评估者有偏见的情况下也能对齐到真实目标。

Abstract: Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this "judging the judges" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.

</details>


### [75] [Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2602.08104)
*Risal Shahriar Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 提出一个两阶段梯度框架，用于多智能体强化学习中的可解释故障检测与溯源，能识别初始故障源、验证多米诺效应导致的误检，并追踪故障传播路径。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在安全关键领域应用增多，但缺乏可解释的故障检测与归因方法。现有方法多为黑盒检测，无法解释故障传播机制和误检原因。

Method: 两阶段梯度框架：第一阶段通过策略梯度成本的泰勒余项分析进行可解释的智能体级故障检测，确定初始故障候选；第二阶段通过评论家导数的几何分析（一阶敏感性和二阶曲率）构建可解释的传播图，验证多米诺效应。

Result: 在Simple Spread（3和5智能体）和StarCraft II环境中评估，使用MADDPG和HATRPO算法，实现了88.2-99.4%的初始故障源检测准确率，并提供了解释检测决策的几何证据。

Conclusion: 该框架超越了黑盒检测，提供了梯度层面的可解释法证分析，为安全关键多智能体系统中的级联故障诊断提供了实用工具。

Abstract: Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains "downstream-first" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.

</details>


### [76] [Initial Risk Probing and Feasibility Testing of Glow: a Generative AI-Powered Dialectical Behavior Therapy Skills Coach for Substance Use Recovery and HIV Prevention](https://arxiv.org/abs/2602.08121)
*Liying Wang,Madison Lee,Yunzhang Jiang,Steven Chen,Kewei Sha,Yunhe Feng,Frank Wong,Lisa Hightow-Weidman,Weichao Yuwen*

Main category: cs.AI

TL;DR: 研究开发了名为Glow的GenAI驱动的DBT技能教练，用于HIV和物质使用风险人群，通过用户驱动的对抗性测试发现安全性能存在差异，链分析代理存在"共情陷阱"问题。


<details>
  <summary>Details</summary>
Motivation: HIV和物质使用是相互影响的流行病，具有共同的冲动性和适应不良应对机制等心理驱动因素。DBT针对这些机制但面临可扩展性挑战，而GenAI提供了大规模提供个性化DBT指导的潜力，但快速发展超过了安全基础设施的建设。

Method: 开发了Glow（GenAI驱动的DBT技能教练），提供链分析和解决方案分析。与洛杉矶社区健康组织合作，对临床工作人员（6人）和有生活经验的个体（28人）进行可用性测试。使用HHH框架，采用用户驱动的对抗性测试，参与者识别目标行为并生成情境现实的风险探针，评估了37个风险探针交互的安全性能。

Result: Glow适当处理了73%的风险探针，但不同代理表现差异显著：解决方案分析代理达到90%的适当处理率，而链分析代理仅为44%。安全失败主要集中在鼓励物质使用和正常化有害行为。链分析代理陷入"共情陷阱"，提供强化适应不良信念的验证。此外还识别出27个DBT技能错误信息实例。

Conclusion: 这是首个对GenAI提供的DBT指导进行HIV和物质使用风险降低的系统性安全评估。研究结果揭示了临床试验前需要缓解的脆弱性。HHH框架和用户驱动的对抗性测试为评估GenAI心理健康干预提供了可复制的方法。

Abstract: Background: HIV and substance use represent interacting epidemics with shared psychological drivers - impulsivity and maladaptive coping. Dialectical behavior therapy (DBT) targets these mechanisms but faces scalability challenges. Generative artificial intelligence (GenAI) offers potential for delivering personalized DBT coaching at scale, yet rapid development has outpaced safety infrastructure. Methods: We developed Glow, a GenAI-powered DBT skills coach delivering chain and solution analysis for individuals at risk for HIV and substance use. In partnership with a Los Angeles community health organization, we conducted usability testing with clinical staff (n=6) and individuals with lived experience (n=28). Using the Helpful, Honest, and Harmless (HHH) framework, we employed user-driven adversarial testing wherein participants identified target behaviors and generated contextually realistic risk probes. We evaluated safety performance across 37 risk probe interactions. Results: Glow appropriately handled 73% of risk probes, but performance varied by agent. The solution analysis agent demonstrated 90% appropriate handling versus 44% for the chain analysis agent. Safety failures clustered around encouraging substance use and normalizing harmful behaviors. The chain analysis agent fell into an "empathy trap," providing validation that reinforced maladaptive beliefs. Additionally, 27 instances of DBT skill misinformation were identified. Conclusions: This study provides the first systematic safety evaluation of GenAI-delivered DBT coaching for HIV and substance use risk reduction. Findings reveal vulnerabilities requiring mitigation before clinical trials. The HHH framework and user-driven adversarial testing offer replicable methods for evaluating GenAI mental health interventions.

</details>


### [77] [RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection](https://arxiv.org/abs/2602.08214)
*Ziwei Wang,Yuanhe Zhang,Jing Chen,Zhenhong Zhou,Ruichao Liang,Ruiying Du,Ju Jia,Cong Wu,Yang Liu*

Main category: cs.AI

TL;DR: 论文提出RECUR攻击方法，利用递归熵量化反思过程中的资源消耗风险，通过构建反事实问题触发LRMs的过度反思，导致输出长度增加11倍、吞吐量下降90%


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）的显式推理需要扩展上下文长度，导致资源消耗显著增加。现有研究主要关注对抗性输入触发冗余推理过程，但对推理过程本身（特别是反思组件）关注有限，反思过程可能导致过度反思并消耗过多计算资源

Method: 引入递归熵来量化反思中的资源消耗风险，基于递归熵提出RECUR攻击方法，通过递归熵引导的反事实利用和反思构建反事实问题来验证LRMs的内在缺陷和风险

Result: 实验表明，在良性推理下递归熵呈现明显下降趋势，而RECUR攻击破坏了这一趋势，使输出长度增加高达11倍，吞吐量下降90%

Conclusion: 该工作为鲁棒推理提供了新视角，揭示了推理本身存在的安全隐患，通过递归熵量化反思风险并提出有效的资源耗尽攻击方法

Abstract: Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.

</details>


### [78] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

TL;DR: 提出WMSS方法，利用模型自身的弱检查点来指导继续优化，突破后训练饱和瓶颈，实现零额外推理成本的性能提升


<details>
  <summary>Details</summary>
Motivation: 大型语言模型后训练中存在持续饱和瓶颈：模型变得高度自信后，进一步训练收益递减。现有方法继续强化目标预测，但发现模型自身历史弱状态中仍存在有监督信号

Method: WMSS（弱智能体可使强智能体更强）后训练范式，利用弱检查点指导继续优化。通过熵动态识别可恢复的学习差距，并通过补偿学习强化这些差距

Result: 在数学推理和代码生成数据集上的实验表明，使用该方法训练的智能体实现了有效的性能提升，同时不产生额外推理成本

Conclusion: WMSS通过利用模型自身历史弱状态中的监督信号，能够突破后训练饱和瓶颈，实现零成本性能改进

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [79] [InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation](https://arxiv.org/abs/2602.08229)
*Yifan Yang,Jinjia Li,Kunxi Li,Puhao Zheng,Yuanyi Wang,Zheyan Qu,Yang Yu,Jianmin Wu,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: 提出去中心化评估框架解决LLM评估中的不稳定性问题，通过区块链协议激励全球贡献者作为独立验证者，显著降低评估方差


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的集中式评估存在不透明、过拟合和硬件差异导致的不稳定性问题。实证分析发现HumanEval评估中单模型十次运行的标准差(1.67)甚至超过了官方排行榜上前10名模型的性能差距(0.91)，使得当前排名统计上不可靠

Method: 提出去中心化评估框架，通过区块链协议激励全球贡献者作为独立验证者，在异构计算节点上进行大规模基准测试，实现硬件和参数多样性。采用稳健的奖励系统确保评估完整性并阻止不诚实参与

Result: 去中心化评估框架将同一模型十次运行的标准差从1.67降低到0.28，显著提高了模型排名的统计置信度

Conclusion: 去中心化评估框架通过多方共识和多样推理环境，将评估从"集中式黑箱"转变为"去中心化背书"，提供了更稳定、更具代表性的评估指标，已完全实现并将向社区发布

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

</details>


### [80] [PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition](https://arxiv.org/abs/2602.08240)
*Xun Su,Huamin Wang,Qi Zhang*

Main category: cs.AI

TL;DR: 提出PTS-SNN框架，通过提示调优解决SSL表示与SNN之间的分布不匹配问题，在保持高精度的同时大幅降低计算成本和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别模型计算成本高，难以部署在资源受限的边缘设备上。SNN虽然能效高，但与自监督学习表示的集成存在分布不匹配问题，高动态范围的嵌入会降低基于阈值神经元的信息编码能力。

Method: 提出PTS-SNN框架：1) 使用时移脉冲编码器通过无参数通道移位捕获局部时间依赖；2) 设计上下文感知膜电位校准策略，利用脉冲稀疏线性注意力模块聚合全局语义上下文到可学习的软提示中，动态调节PLIF神经元的偏置电压，将异质输入分布集中在响应放电范围内。

Result: 在五个多语言数据集上的实验表明，PTS-SNN在IEMOCAP上达到73.34%的准确率，与竞争性ANN相当，同时仅需119万个可训练参数和每样本0.35毫焦的推理能耗。

Conclusion: PTS-SNN成功解决了SSL表示与SNN之间的分布不匹配问题，实现了参数高效且能耗低的语音情感识别，为边缘设备部署提供了可行方案。

Abstract: Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.

</details>


### [81] [Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs](https://arxiv.org/abs/2602.08241)
*Siqu Ou,Tianrui Wan,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.AI

TL;DR: SAYO：通过强化学习引入区域级视觉注意力奖励，改善多模态大语言模型的视觉注意力稳定性，提升复杂推理任务性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在复杂推理任务中存在视觉注意力薄弱的问题，早期视觉对齐错误很少在后续推理中得到纠正，导致错误传播和推理失败。这种局限性源于训练过程中视觉注意力信用分配不足。

Method: 提出SAYO模型，采用强化学习框架训练，引入区域级视觉注意力奖励机制。该奖励明确将优化信号与视觉基础推理步骤对齐，使模型能够学习更可靠的注意力行为。

Result: 在多个多模态基准测试上的广泛实验表明，SAYO在多样化的推理和感知任务上持续提升性能。

Conclusion: 通过强化学习框架引入区域级视觉注意力奖励，能够有效改善多模态大语言模型的视觉注意力稳定性，从而提升复杂推理任务的性能表现。

Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>


### [82] [G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design](https://arxiv.org/abs/2602.08253)
*Baoyun Zhao,He Wang,Liang Zeng*

Main category: cs.AI

TL;DR: G-LNS：一个生成式进化框架，利用LLM自动设计大邻域搜索的破坏与修复算子对，在组合优化问题上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动启发式设计方法通常局限于构造性优先级规则或参数化局部搜索指导，搜索空间受限，难以在复杂组合优化问题中跳出深度局部最优。

Method: 提出G-LNS生成式进化框架，利用LLM共同进化紧密耦合的破坏与修复算子对，通过合作评估机制捕捉算子间的交互，实现有效的结构破坏与重建。

Result: 在TSP和CVRP等挑战性基准测试中，G-LNS显著优于基于LLM的AHD方法和经典求解器，发现的启发式方法能以更少计算资源获得接近最优解，并在未见实例分布上表现出鲁棒泛化能力。

Conclusion: G-LNS将LLM驱动的自动启发式设计扩展到LNS算子生成，通过共同进化破坏-修复算子对实现了更强大的结构探索能力，为复杂组合优化问题提供了有效的自动求解方案。

Abstract: While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.

</details>


### [83] [SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities](https://arxiv.org/abs/2602.08254)
*Arman Aghaee,Sepehr Asgarian,Jouhyun Jeon*

Main category: cs.AI

TL;DR: SynthAgent是一个多智能体系统框架，用于模拟肥胖症合并精神障碍患者，通过整合临床证据和个性化特征来创建虚拟患者，模拟疾病进展和治疗反应。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界数据碎片化、有偏见和隐私限制的问题，为研究复杂疾病提供高保真患者模拟的途径。

Method: 开发SynthAgent多智能体系统框架，整合索赔数据、人口调查和患者中心文献中的临床证据，构建具有人格特质的个性化虚拟患者，通过自主智能体交互模拟疾病进展、治疗反应和生活管理。

Result: 评估100多个生成的虚拟患者显示，GPT-5和Claude 4.5 Sonnet作为核心引擎在MAS框架中达到最高保真度，优于Gemini 2.5 Pro和DeepSeek-R1。

Conclusion: SynthAgent提供了一个可扩展且保护隐私的框架，用于探索医学和心理领域的患者旅程、行为动态和决策过程。

Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>


### [84] [Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI](https://arxiv.org/abs/2602.08268)
*Akinori Maeda,Yuto Sekiya,Sota Sugimura,Tomoya Asai,Yu Tsuda,Kohei Ikeda,Hiroshi Fujii,Kohei Watanabe*

Main category: cs.AI

TL;DR: Puda是一个用户主权架构，通过聚合跨服务数据并支持客户端管理，提供三种隐私级别的数据共享控制，在保持97.2%个性化性能的同时显著降低隐私风险。


<details>
  <summary>Details</summary>
Motivation: 当前个人数据集中在少数平台提供商手中，形成了数据孤岛，限制了用户主权和跨服务数据使用。同时，基于大语言模型的智能代理对个性化服务的需求激增，这需要在数据利用和隐私保护之间找到平衡。

Method: 提出Puda（Private User Dataset Agent）架构，作为浏览器系统实现跨服务通用平台。提供三种隐私级别的数据共享控制：详细浏览历史、提取的关键词、预定义类别子集。通过个性化旅行规划任务进行评估，使用LLM-as-a-Judge框架在三个标准下评估个性化性能。

Result: 实验结果显示，提供预定义类别子集能够达到分享详细浏览历史所获个性化性能的97.2%。这表明Puda能够在几乎不损失个性化效果的情况下显著降低隐私风险。

Conclusion: Puda通过有效的多粒度数据管理，为隐私-个性化权衡提供了实用解决方案，为用户主权建立了AI原生基础，使用户能够安全地利用个性化AI的全部潜力。

Abstract: Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

</details>


### [85] [Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis](https://arxiv.org/abs/2602.08276)
*Haoyu Jia,Kento Kawaharazuka,Kei Okada*

Main category: cs.AI

TL;DR: 提出Structural Context Model形式化模型，用于分析和比较LLM智能体，并配套实现框架和工程工作流，在动态猴子香蕉问题上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体研究碎片化严重，概念框架和方法论常与底层实现细节混杂，缺乏可分析、自洽的形式化模型来进行独立于实现的表征和比较。

Method: 提出Structural Context Model形式化模型，从上下文结构角度分析LLM智能体；配套提出声明式实现框架和可持续的智能体工程工作流Semantic Dynamics Analysis。

Result: 在动态猴子香蕉问题变体上，使用该框架开发的智能体在最困难设置中成功率提升高达32个百分点。

Conclusion: 提出的形式化模型和配套框架能有效解决LLM智能体研究的碎片化问题，提供原则性洞察并支持快速、系统的设计迭代。

Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.

</details>


### [86] [The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI](https://arxiv.org/abs/2602.08295)
*Ilya Levin*

Main category: cs.AI

TL;DR: 论文提出"氛围自动化"概念，认为生成式AI代表了认识论的根本转变，从优化预定指标转向处理语境、语义和风格连贯性，人类角色转变为"氛围工程"。


<details>
  <summary>Details</summary>
Motivation: 生成式AI不是渐进技术进步，而是质变的认识论转变，挑战计算机科学的基本假设。需要理解这种从自动化自动化到处理语境连贯性的转变。

Method: 提出"氛围自动化"概念框架，分析生成式AI如何操作化隐性规律，将人类角色重新定义为"氛围工程"。建立三层分析框架：教师世界观、产业关系、课程设计。

Result: 生成式AI通过高维潜在表征操作化对语调、意图和情境判断的敏感性，实现功能上对隐性规律的操作化。人类角色从算法问题规范转向氛围工程。

Conclusion: 生成式AI的认识论转变需要教育机构转型，警惕模式崩溃和文化同质化风险，强调需要刻意参与生成系统以避免回归合成统一性。

Abstract: The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.
  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.
  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.

</details>


### [87] [Moral Sycophancy in Vision Language Models](https://arxiv.org/abs/2602.08311)
*Shadman Rabby,Md. Hefzul Hossain Papon,Sabbir Ahmed,Nokimul Hasan Arif,A. B. M. Ashikur Rahman,Irfan Ahmad*

Main category: cs.AI

TL;DR: 本文首次系统研究了视觉语言模型中的道德谄媚行为，发现VLMs在用户意见影响下会牺牲道德准确性，表现出从正确到错误判断的不对称转变，揭示了模型在错误纠正与引入之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究探索了VLMs在一般情境中的谄媚行为，但其对基于道德的视觉决策的影响尚未得到充分理解。本文旨在填补这一空白，首次系统研究VLMs中的道德谄媚现象。

Method: 在Moralise和M^3oralBench数据集上评估了10个广泛使用的VLMs，在明确用户分歧情境下分析其行为。使用错误引入率（EIR）和错误纠正率（ECR）进行量化评估，研究模型在用户偏见影响下的响应变化。

Result: VLMs经常在初始判断正确的情况下产生道德错误的后续响应；表现出明显的不对称性：模型更倾向于从道德正确转向错误判断；后续提示在Moralise上降低性能，在M^3oralBench上表现混合；模型在错误纠正能力与错误引入之间存在权衡；初始道德正确立场会引发更强的谄媚行为。

Conclusion: VLMs对道德影响表现出脆弱性，需要制定原则性策略来提高多模态AI系统的伦理一致性和鲁棒性。研究揭示了模型在维护道德准确性与适应用户偏好之间的内在冲突。

Abstract: Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>


### [88] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

TL;DR: SHARP框架通过Shapley值进行精确信用分配，优化多智能体强化学习，在多个基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 将LLM与外部工具结合的多智能体系统在解决复杂问题方面很有前景，但训练困难，主要挑战是信用分配问题。现有方法依赖稀疏或全局广播奖励，无法捕捉个体贡献，导致强化学习效率低下。

Method: 提出SHARP框架，通过分解的奖励机制实现精确信用分配：包括全局广播准确性奖励、基于Shapley值的边际信用奖励（为每个智能体分配）以及工具过程奖励（提高执行效率）。通过归一化轨迹组中的智能体特定优势来稳定训练。

Result: 在多个真实世界基准测试中，SHARP显著优于最近的最先进基线方法，相比单智能体方法平均提升23.66%，相比多智能体方法平均提升14.05%。

Conclusion: SHARP通过精确的信用分配有效解决了多智能体强化学习中的训练稳定性问题，为LLM与外部工具结合的多智能体系统优化提供了有效框架。

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [89] [CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT](https://arxiv.org/abs/2602.08339)
*Chengyi Du,Yazhe Niu,Dazhong Shen,Luxin Xu*

Main category: cs.AI

TL;DR: CoTZero：一种无需标注的视觉语言模型训练范式，通过双阶段数据合成和认知对齐训练，提升视觉推理的逻辑一致性和可验证性


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型依赖表面相关性而非逻辑一致的结构化表示，导致高层语义结构和因果关系理解不足，阻碍组合性和可验证推理

Method: 1. 双阶段数据合成：自底向上提取原子视觉基元并组合成结构化问题推理形式；自顶向下使用全局结构指导局部细节和因果关系解释。2. 认知对齐训练：基于合成数据，在强化微调中引入认知一致可验证奖励，提供推理一致性和事实正确性的逐步反馈

Result: 在多级语义不一致基准测试（含词汇扰动负样本）中达到83.33%的F1分数，在域内和域外设置下均表现优异

Conclusion: CoTZero通过引入人类认知模型到推理过程，显著提升了视觉语言模型的层次推理能力和泛化性，各组件均有助于实现更可解释和人类对齐的视觉推理

Abstract: Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>


### [90] [Effect-Level Validation for Causal Discovery](https://arxiv.org/abs/2602.08340)
*Hoang Dang,Luan Pham,Minh Nguyen*

Main category: cs.AI

TL;DR: 提出以效应为中心、可识别性优先的因果发现框架，强调在强自选择反馈系统中，图恢复精度不足以保证因果可靠性，需通过可识别性、稳定性和证伪性验证效应估计。


<details>
  <summary>Details</summary>
Motivation: 大规模遥测数据中因果发现用于评估用户干预效果，但在强自选择反馈系统中，其决策可靠性尚不明确。传统方法过于关注图恢复精度，而忽略了因果查询的可识别性和效应验证。

Method: 提出效应中心、可识别性优先框架：将发现的因果图视为结构假设，通过可识别性、稳定性和证伪性进行评估，而非仅依赖图恢复精度。使用真实游戏遥测数据研究早期竞技游戏暴露对短期留存的影响。

Result: 许多统计上合理的发现结果在施加最小时间和语义约束后无法进行点识别因果查询，突显可识别性是决策支持的关键瓶颈。当可识别时，不同算法家族收敛到相似、决策一致的效应估计，尽管图结构差异显著。这些收敛估计通过安慰剂、子采样和敏感性证伪检验。其他方法则表现出零星的可识别性和阈值敏感或衰减效应。

Conclusion: 图级指标本身不足以作为特定目标查询因果可靠性的代理。在遥测驱动系统中，可信的因果结论需要优先考虑可识别性和效应级验证，而非仅关注因果结构恢复。

Abstract: Causal discovery is increasingly applied to large-scale telemetry data to estimate the effects of user-facing interventions, yet its reliability for decision-making in feedback-driven systems with strong self-selection remains unclear. In this paper, we propose an effect-centric, admissibility-first framework that treats discovered graphs as structural hypotheses and evaluates them by identifiability, stability, and falsification rather than by graph recovery accuracy alone. Empirically, we study the effect of early exposure to competitive gameplay on short-term retention using real-world game telemetry. We find that many statistically plausible discovery outputs do not admit point-identified causal queries once minimal temporal and semantic constraints are enforced, highlighting identifiability as a critical bottleneck for decision support. When identification is possible, several algorithm families converge to similar, decision-consistent effect estimates despite producing substantially different graph structures, including cases where the direct treatment-outcome edge is absent and the effect is preserved through indirect causal pathways. These converging estimates survive placebo, subsampling, and sensitivity refutation. In contrast, other methods exhibit sporadic admissibility and threshold-sensitive or attenuated effects due to endpoint ambiguity. These results suggest that graph-level metrics alone are inadequate proxies for causal reliability for a given target query. Therefore, trustworthy causal conclusions in telemetry-driven systems require prioritizing admissibility and effect-level validation over causal structural recovery alone.

</details>


### [91] [OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration](https://arxiv.org/abs/2602.08344)
*Qi Guo,Jianing Wang,Deyang Kong,Xiangyu Xi,Jianfei Zhang,Yi Lu,Jingang Wang,Wei Wang,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 本文提出Outline-Guided Path Exploration (OPE)方法，通过生成多样化的推理大纲来引导并行路径探索，解决并行思维中探索路径间互信息瓶颈问题，提升大推理模型的数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有并行思维方法主要关注聚合阶段优化，对路径探索阶段关注有限。研究发现探索路径间的互信息瓶颈限制了整体性能，需要解决路径探索中的信息冗余和多样性不足问题。

Method: 提出Outline-Guided Path Exploration (OPE)方法：1）先生成多样化的推理大纲来划分解空间；2）基于大纲引导并行路径推理；3）采用迭代强化学习策略，独立优化大纲规划和基于大纲的推理。

Result: 在多个具有挑战性的数学基准测试上进行广泛实验，证明OPE能有效提升不同聚合策略下的推理性能，使大推理模型更可靠地发现正确解。

Conclusion: OPE通过显式划分解空间和引导路径探索，解决了并行思维中的互信息瓶颈问题，为提升大模型复杂问题推理能力提供了有效方法。

Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>


### [92] [Towards Better Evolution Modeling for Temporal Knowledge Graphs](https://arxiv.org/abs/2602.08353)
*Zhang Jiasheng,Li Zhangpin,Wang Mingzhe,Shao Jie,Cui Jiangtao,Li Hui*

Main category: cs.AI

TL;DR: 现有TKG基准存在严重缺陷：仅通过共现统计就能达到接近SOTA的性能，无需使用时间信息。作者分析了问题根源并提出新的TKG演化基准。


<details>
  <summary>Details</summary>
Motivation: 现有TKG预测模型虽然声称取得了优异性能（如YAGO数据集上Hits@10超过0.9），但这些基准无意中引入了捷径。研究发现，仅通过统计共现关系就能达到接近SOTA的性能，而无需使用任何时间信息，这表明现有基准存在严重缺陷。

Method: 作者深入分析了现有基准问题的根源：1）数据集中固有的偏见；2）过于简化的评估任务形式容易被这些偏见利用。通过分析进一步发现了现有基准的更多限制：时间间隔知识的不合理格式化、忽略知识过时学习、以及用于精确演化理解的信息不足。

Result: 提出了TKG演化基准，包括四个经过偏见校正的数据集和两个与演化过程紧密对齐的新任务。该基准旨在更准确地理解TKG演化建模的挑战，促进更公平的评估。

Conclusion: 现有TKG基准存在严重缺陷，可能导致对模型性能的误判。作者提出的新基准通过纠正数据偏见和设计更合理的评估任务，为TKG演化建模提供了更公平、更准确的评估框架。

Abstract: Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>


### [93] [Does Your Reasoning Model Implicitly Know When to Stop Thinking?](https://arxiv.org/abs/2602.08354)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuanda Wang,Zhixia Zhang,Hongyan Xie,Songshi Liang,Zehao Chen,Xuefeng Xiao,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: SAGE：一种新的采样范式，通过释放大推理模型的自我停止能力，显著提升推理效率和准确性


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型使用长思维链方法存在大量冗余，损害计算效率并导致实时应用延迟。研究发现更长的推理链与正确性无关甚至有害，但模型隐含知道何时停止思考，这一能力被当前采样范式所掩盖。

Method: 提出SAGE（自我感知引导高效推理）采样范式，释放模型的高效推理潜力。进一步将SAGE作为混合采样集成到基于群体的强化学习（SAGE-RL）中，使SAGE-RL能够将SAGE发现的高效推理模式融入标准pass@1推理。

Result: SAGE-RL显著提升了大推理模型在多个具有挑战性的数学基准测试中的推理准确性和效率。

Conclusion: 通过释放大推理模型隐含的自我停止能力，SAGE采样范式能够显著提升推理效率和准确性，为解决长思维链冗余问题提供了有效方案。

Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>


### [94] [Circuit Representations of Random Forests with Applications to XAI](https://arxiv.org/abs/2602.08362)
*Chunxi Ji,Adnan Darwiche*

Main category: cs.AI

TL;DR: 将随机森林分类器编译为电路，用于高效计算决策解释、鲁棒性和决策翻转路径


<details>
  <summary>Details</summary>
Motivation: 现有方法在计算随机森林分类器的决策解释时效率较低，需要更高效的方法来支持解释性、鲁棒性分析和决策翻转路径计算

Method: 1) 提出将随机森林编译为电路的方法，每个电路直接编码分类器的某个类别实例；2) 利用该方法获得可处理电路，用于计算决策的完整和一般原因；3) 提出算法计算决策鲁棒性和所有最短翻转路径

Result: 提出的方法比现有类似方法显著更高效，能够枚举充分原因、必要原因和对比解释，计算决策鲁棒性，并识别随机森林分类器决策的所有最短翻转路径

Conclusion: 该方法为随机森林分类器提供了高效的解释性分析工具，支持决策解释、鲁棒性评估和决策翻转路径识别，在多个数据集上验证了实用性

Abstract: We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.

</details>


### [95] [MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval](https://arxiv.org/abs/2602.08369)
*Xin Zhang,Kailai Yang,Chenyue Li,Hao Li,Qiyu Wei,Jun'ichi Tsujii,Sophia Ananiadou*

Main category: cs.AI

TL;DR: MemAdapter是一个统一的记忆检索框架，通过两阶段训练策略实现跨记忆范式的快速对齐，显著降低对齐成本并提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体记忆系统通常采用孤立的设计范式（显式、参数化或潜在记忆），检索方法紧密耦合，阻碍了跨范式的泛化和融合。需要统一异构记忆范式。

Method: 提出MemAdapter框架：1）从统一记忆空间训练生成式子图检索器；2）通过对比学习训练轻量级对齐模块，使检索器适应未见记忆范式。采用两阶段训练策略。

Result: 在三个公共评估基准上，生成式子图检索器在三种记忆范式和智能体模型规模上持续优于五种强基线系统。跨范式对齐仅需13分钟（单GPU），性能优于原始检索器且训练计算量减少95%以上。

Conclusion: MemAdapter实现了异构记忆范式的统一，提供了一种即插即用的解决方案，能够有效进行零样本跨范式融合，显著提高了记忆检索的灵活性和效率。

Abstract: Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.

</details>


### [96] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

TL;DR: VIRF框架通过神经符号架构，让确定性逻辑导师与LLM规划器协作，实现可验证的安全规划修复，而非简单拒绝不安全计划。


<details>
  <summary>Details</summary>
Motivation: LLM在具身AI规划中缺乏形式化推理能力，无法提供严格的安全保证。现有方法要么依赖不可靠的LLM安全检查，要么简单拒绝不安全计划而不提供修复方案。

Method: 提出可验证迭代精炼框架(VIRF)，采用神经符号架构，建立导师-学徒对话机制：基于形式化安全本体的确定性逻辑导师为LLM规划器提供因果和教学反馈，实现智能计划修复而非简单避免。同时引入可扩展的知识获取管道，从真实世界文档合成安全知识库。

Result: 在家庭安全任务中，VIRF实现了0%的危险行动率和77.3%的目标条件率（所有基线中最高的），平均仅需1.1次修正迭代，效率很高。

Conclusion: VIRF展示了构建根本上可信且可验证安全的具身智能体的原则性途径，将安全范式从被动把关转向主动协作。

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [97] [SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains](https://arxiv.org/abs/2602.08400)
*Longkun Li,Yuanben Zou,Jinghan Wu,Yuqing Wen,Jing Li,Hangwei Qian,Ivor Tsang*

Main category: cs.AI

TL;DR: SCOUT-RAG是一个分布式图检索增强生成框架，通过智能代理协作实现跨域渐进式检索，在保持性能的同时显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统Graph-RAG依赖集中式知识图谱，但在分布式和访问受限的环境中（如医院、跨国组织），无法获得全局图可见性或进行穷举查询。需要一种能在有限可见性下智能选择相关域和遍历深度的解决方案。

Method: 提出SCOUT-RAG框架，采用四个协作代理：(1)评估域相关性，(2)决定何时扩展到其他域，(3)自适应调整遍历深度以避免不必要的图探索，(4)合成高质量答案。框架旨在最小化检索遗憾（错过有用域信息），同时控制延迟和API成本。

Result: 在多域知识设置中，SCOUT-RAG实现了与集中式基线（包括DRIFT和穷举域遍历）相当的性能，同时显著减少了跨域调用、处理的总令牌数和延迟。

Conclusion: SCOUT-RAG为分布式和访问受限环境中的Graph-RAG提供了一种可扩展且成本高效的解决方案，通过智能代理协作实现了高效的跨域知识检索。

Abstract: Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \textbf{SCOUT-RAG} (\textit{\underline{S}calable and \underline{CO}st-efficient \underline{U}nifying \underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.

</details>


### [98] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

TL;DR: AGENTWM是首个专门为智能体模型设计的水印框架，通过偏置功能相同的工具执行路径分布来嵌入水印，保护智能体系统的知识产权免受模仿攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型发展为能够自主推理和使用工具的智能体系统，创造了重要的知识产权价值。这些系统容易受到模仿攻击，而现有的LLM水印技术无法应对智能体系统作为灰盒运行的场景。

Method: AGENTWM利用动作序列的语义等价性，通过微妙地偏置功能相同的工具执行路径分布来注入水印。开发了自动生成鲁棒水印方案的流水线，以及严格的统计假设检验程序进行验证。

Result: 在三个复杂领域的广泛评估表明，AGENTWM实现了高检测准确率，同时对智能体性能影响可忽略。能够有效保护智能体知识产权，即使面对自适应攻击者也无法移除水印而不严重降低被盗模型的效用。

Conclusion: AGENTWM是首个专门为智能体模型设计的水印框架，能够有效保护智能体系统的知识产权，填补了现有水印技术在智能体领域的空白。

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [99] [From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent](https://arxiv.org/abs/2602.08412)
*Yuhang Wang,Feiming Xu,Zheng Lin,Guangyu He,Yuzhe Huang,Haichang Gao,Zhenxing Niu*

Main category: cs.AI

TL;DR: PASB是一个针对现实世界个性化AI代理的端到端安全评估框架，通过个性化使用场景、真实工具链和长时交互来评估OpenClaw等代理的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有代理安全研究主要关注合成或任务中心设置，无法准确捕捉现实世界部署中个性化代理的攻击面和风险传播机制，存在安全评估空白。

Method: 提出PASB框架，基于现有代理攻击范式，整合个性化使用场景、真实工具链和长时交互，实现对真实系统的黑盒端到端安全评估。

Result: 以OpenClaw为案例研究发现其在用户提示处理、工具使用和记忆检索等不同执行阶段存在关键漏洞，揭示了个性化代理部署中的重大安全风险。

Conclusion: PASB框架填补了个性化代理安全评估的空白，揭示了现实部署中的安全风险，为未来代理安全研究提供了重要工具和方向。

Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.

</details>


### [100] [When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment](https://arxiv.org/abs/2602.08449)
*Igor Santos-Grueiro*

Main category: cs.AI

TL;DR: 论文提出将AI对齐评估重构为部分可观测下的信息流问题，证明评估时与部署时行为差异受内部表示与制度变量互信息约束，并通过制度盲训练减少制度信息可提取性来抑制条件策略行为。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全评估假设评估期间观察到的行为能预测部署时的行为，但这一假设对于具有情境意识的智能体变得脆弱，因为它们可能利用评估与部署之间的信息差异实施条件策略（如谄媚和潜伏代理），在监督下保持合规但在部署时违规。

Method: 将对齐评估重构为部分可观测下的信息流问题，提出制度盲训练机制：通过对抗性不变性减少决策相关内部表示中制度信息的可提取性。在两个完全特征化的故障模式（科学谄媚和时间潜伏代理）上评估该方法。

Result: 制度盲训练在两个评估案例中都抑制了制度条件行为，且没有可测量的任务效用损失，但表现出不同的动态：谄媚在低干预强度下表现出尖锐的表示和行为转变，而潜伏代理行为需要更强的压力且没有表现出制度可解码性的清晰崩溃。

Conclusion: 表示不变性是一个有意义但根本有限的控制杠杆，其有效性取决于制度信息如何嵌入策略中。行为评估应辅以制度意识和信息流的白盒诊断。

Abstract: Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation is predictive of behavior in deployment. This assumption becomes fragile for agents with situational awareness, which may exploitregime leakage-informational cues distinguishing evaluation from deployment-to implement conditional policies such as sycophancy and sleeper agents, which preserve compliance under oversight while defecting in deployment-like regimes. We reframe alignment evaluation as a problem of information flow under partial observability. Within this framework, we show that divergence between evaluation-time and deployment-time behavior is bounded by the mutual information between internal representations and the regime variable. Motivated by this result, we study regime-blind mechanisms: training-time interventions that reduce the extractability of regime information at decision-relevant internal representations via adversarial invariance. We evaluate this approach on a base, open-weight language model across two fully characterized failure modes -scientific sycophancy and temporal sleeper agents. Regime-blind training suppresses regime-conditioned behavior in both evaluated cases without measurable loss of task utility, but with qualitatively different dynamics: sycophancy exhibits a sharp representational and behavioral transition at low intervention strength, whereas sleeper-agent behavior requires substantially stronger pressure and does not exhibit a clean collapse of regime decodability. These results demonstrate that representational invariance is a meaningful but fundamentally limited control lever, whose effectiveness depends on how regime information is embedded in the policy. We argue that behavioral evaluation should be complemented with white-box diagnostics of regime awareness and information flow.

</details>


### [101] [TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor](https://arxiv.org/abs/2602.08517)
*Shaoang Zhang,Yazhe Niu*

Main category: cs.AI

TL;DR: TreeTensor是一个通用的嵌套数据容器，通过树状结构视角系统建模数据关系，支持对嵌套数据应用任意函数和操作，几乎零成本兼容主流机器学习库。


<details>
  <summary>Details</summary>
Motivation: 传统Tensor具有固定形状，在处理复杂认知AI系统中的分层结构数据（嵌套数据）时存在编程不便和效率低下的问题。需要一种更灵活的数据结构来处理各种模态的嵌套数据。

Method: 提出TreeTensor作为通用嵌套数据容器，总结嵌套数据的两种主要计算模式，通过约束和魔法工具实现对嵌套数据的任意函数和操作应用，兼容Scikit-Learn、Numpy、PyTorch等库。

Result: TreeTensor在多种问题中展现出强大可用性，特别是在当前最复杂的AI系统之一AlphaStar（星际争霸II）中表现出色，同时运行时效率优异且无额外开销。

Conclusion: TreeTensor为处理复杂认知AI系统中的嵌套数据提供了一种高效、通用的解决方案，能够与现有方法结合扩展更多用途，如异步执行和变长数据计算。

Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.

</details>


### [102] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

TL;DR: 提出Reinforcement Inference方法，利用模型自身的不确定性选择性地进行第二次推理，无需重新训练即可提升LLM性能


<details>
  <summary>Details</summary>
Motivation: 当前LLM在单次贪婪推理协议下会系统性地低估模型真实能力，许多错误源于内部模糊性下的过早决策，而非知识缺失

Method: 基于熵感知的推理时控制策略，利用模型自身不确定性选择性地调用第二次更审慎的推理尝试

Result: 在MMLU-Pro的12,032个问题上，准确率从60.72%提升到84.03%，仅增加61.06%的推理调用；仅提示方法效果不如基线

Conclusion: 提出熵感知范式用于测量和扩展模型能力，不确定性调节的推理与单次贪婪推理之间的差距可作为诊断LLM潜在推理视野的透镜

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [103] [Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO](https://arxiv.org/abs/2602.08533)
*Kun Peng,Conghui Tan,Yu Liu,Guohua Tang,Zhongqian Sun,Wei Yang,Zining Zhu,Lei Jiang,Yanbing Liu,Hao Peng*

Main category: cs.AI

TL;DR: 提出一个结合在线个性化与自适应树基组相对策略优化的长视野强化学习框架，用于开放域对话代理，解决现有方法对预收集用户数据的过度依赖和短视野偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有开放域对话代理存在两个关键限制：1) 过度依赖预收集的用户数据，2) 强化学习中的短视野偏见忽略了对话的长期价值。需要一种能够在线个性化并考虑长视野对话价值的方法。

Method: 采用双代理游戏范式：用户代理通过风格模仿（学习用户特定对话特征）和主动终止（预测回合级终止概率作为即时奖励）构建动态环境。提出自适应树基组相对策略优化（AT-GRPO），将对话轨迹重新解释为树结构，引入自适应观察范围，早期阶段使用较大范围支持主题探索，后期阶段使用较小范围促进对话维护，将计算复杂度从指数级降至多项式级。

Result: 大量实验表明该框架在性能、样本效率和鲁棒性方面表现优越。

Conclusion: 提出的长视野RL框架通过在线个性化和自适应树基优化，有效解决了开放域对话代理的现有限制，实现了更好的对话体验和效率。

Abstract: Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework's superior performance, sample efficiency, and robustness.

</details>


### [104] [PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition](https://arxiv.org/abs/2602.08586)
*Yiming Yang,Zhuoyuan Li,Fanxiang Zeng,Hao Fu,Yue Liu*

Main category: cs.AI

TL;DR: PRISM框架通过探索、信息、聚合三个维度优化多智能体推理，实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协作方法缺乏理论指导，不清楚为何优于单智能体推理以及哪些设计选择最关键，需要系统化优化框架

Method: 提出PRISM框架：基于角色多样性实现探索，通过执行基础反馈和证据交叉评估提供信息，采用迭代合成和闭环验证进行聚合

Result: 在数学推理、代码生成和函数调用基准测试中达到最先进性能，计算效率优于部分维度优化方法

Conclusion: 理论框架为未来多智能体推理系统提供可操作设计原则，PRISM通过联合优化三个维度实现系统化改进

Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems.
  We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.

</details>


### [105] [An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture](https://arxiv.org/abs/2602.08597)
*Roland Bertin-Johannet,Lara Scipio,Leopold Maytié,Rufin VanRullen*

Main category: cs.AI

TL;DR: 提出一种用于全局工作空间理论（GWT）的top-down注意力机制，通过选择相关模态提升多模态系统的噪声鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 全局工作空间理论（GWT）作为认知神经科学启发的框架，为多模态整合提供了新思路，但现有的GWT实现中注意力机制研究不足，需要开发有效的模态选择机制来提升系统性能。

Method: 提出一种top-down注意力机制，用于在全局工作空间中选择相关模态。在两个复杂度递增的多模态数据集（Simple Shapes和MM-IMDb 1.0）上评估该机制，并与现有基线进行比较。

Result: 1）注意力机制显著提升了全局工作空间系统在两个数据集上的噪声鲁棒性；2）展示了文献中多模态注意力模型不具备的跨任务和跨模态泛化能力；3）在MM-IMDb 1.0基准测试中，该机制使全局工作空间达到与最先进方法竞争的性能。

Conclusion: 提出的top-down注意力机制有效增强了全局工作空间理论在多模态整合中的实用性和性能，为认知启发的人工智能架构提供了有前景的方向。

Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.

</details>


### [106] [OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval](https://arxiv.org/abs/2602.08603)
*Teng Wang,Rong Shan,Jianghao Lin,Junjie Wu,Tianyi Xu,Jianping Zhang,Wenteng Chen,Changwang Zhang,Zhaoxiang Wang,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: OSCAR：首个将组合图像检索从启发式搜索转化为轨迹优化问题的框架，通过离线数学推导最优轨迹和在线VLM规划，显著提升性能且仅需10%训练数据


<details>
  <summary>Details</summary>
Motivation: 现有组合图像检索方法存在两大问题：统一嵌入检索存在单一模型近视问题，启发式代理检索受限于次优的试错编排。需要更原则性的方法来解决异构视觉和文本约束的复杂推理问题。

Method: 提出优化引导的代理规划框架OSCAR，采用离线-在线范式：1）离线阶段将CIR建模为两阶段混合整数规划问题，通过布尔集合运算数学推导最大化真实覆盖的最优轨迹；2）在线阶段将最优轨迹存储为黄金库，作为上下文演示指导VLM规划器进行推理。

Result: 在三个公共基准和一个私有工业基准上，OSCAR始终优于SOTA基线。仅使用10%训练数据就达到优越性能，证明了规划逻辑的强泛化能力而非数据集特定记忆。

Conclusion: OSCAR首次将代理CIR从启发式搜索过程重新表述为原则性轨迹优化问题，通过数学推导的最优轨迹指导在线推理，实现了更好的泛化和性能，为组合图像检索提供了新的范式。

Abstract: Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>


### [107] [Debate is efficient with your time](https://arxiv.org/abs/2602.08630)
*Jonah Brown-Cohen,Geoffrey Irving,Simon C. Marshall,Ilan Newman,Georgios Piliouras,Mario Szegedy*

Main category: cs.AI

TL;DR: 论文引入"辩论查询复杂度"(DQC)概念，分析人类监督辩论所需的查询次数，发现PSPACE/poly问题只需O(log n)次查询即可判定，将辩论查询效率与电路复杂度联系起来。


<details>
  <summary>Details</summary>
Motivation: 现有工作分析了辩论在理论上能解决什么问题，但未研究人类监督的实际成本——法官需要检查辩论记录中的多少信息。本文旨在量化这种监督成本，引入辩论查询复杂度来衡量验证辩论所需的最小信息量。

Method: 引入辩论查询复杂度(DQC)作为衡量指标，分析不同复杂度类别函数的DQC界限。通过理论分析证明PSPACE/poly类问题与O(log n)查询可判定问题的等价性，并建立DQC与电路复杂度的联系。

Result: 1. PSPACE/poly恰好是O(log n)查询可判定的函数类，表明辩论具有极高的查询效率；2. 依赖所有输入位的函数需要Ω(log n)次查询；3. 大小为s的电路可计算函数的DQC ≤ log(s)+3；4. 证明P类语言的DQC下界可推导出新的电路下界。

Conclusion: 辩论监督具有惊人的查询效率，即使对于高度复杂的问题，对数级别的监督就足够了。辩论查询复杂度与电路复杂度存在深刻联系，为通过辩论分析获得电路复杂度下界提供了新途径。

Abstract: AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.
  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.

</details>


### [108] [Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers](https://arxiv.org/abs/2602.08707)
*Aditya Gulati,Nuria Oliver*

Main category: cs.AI

TL;DR: 论文探讨聊天机器人信任问题，指出用户信任常源于设计诱导而非系统可信度，建议将聊天机器人视为销售员而非助手，需区分心理信任与规范信任


<details>
  <summary>Details</summary>
Motivation: 随着聊天机器人模糊自动化系统与人类对话的界限，需要更深入审视这些系统的信任基础。当前监管和政策框架倾向于从规范角度定义信任，但用户对聊天机器人的信任往往源于行为机制

Method: 基于观察分析，提出将聊天机器人重新定义为高度熟练的销售员而非伴侣或助手，其目标由部署组织决定。通过理论分析区分心理信任形成与规范可信度

Result: 发现"信任"一词下共存着相互竞争的概念，模糊了心理信任形成与规范信任之间的重要区别。用户信任常通过利用认知偏见的设计选择来塑造，而非通过证明可信度获得

Conclusion: 需要进一步研究和更强支持机制来帮助用户适当校准对对话AI系统的信任，解决心理信任与规范信任之间的差距

Abstract: As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of "trust" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.

</details>


### [109] [Intermediate Results on the Complexity of STRIPS$_{1}^{1}$](https://arxiv.org/abs/2602.08708)
*Stefan Edelkamp,Jiří Fink,Petr Gregor,Anders Jonsson,Bernhard Nebel*

Main category: cs.AI

TL;DR: 该研究探讨STRIPS规划中算子仅有一个前提和一个效果时的计算复杂度，通过SAT求解器、字面图分析和Petri网映射来验证"小解假设"是否成立。


<details>
  <summary>Details</summary>
Motivation: Bylander的研究表明命题STRIPS规划在仅允许基文字时，即使算子限制为两个前提和两个后置条件，判定规划存在性也是PSPACE完全的。虽然NP难性已确定，但算子仅有一个前提和一个效果的命题STRIPS是否NP完全仍是未知问题。本研究旨在澄清STRIPS$^1_1$的"小解假设"是否成立。

Method: 1) 对小规模实例调用SAT求解器；2) 引入字面图(literal graph)进行分析；3) 将问题映射到Petri网模型。

Result: 论文未在摘要中明确给出具体结果，但通过上述方法对STRIPS$^1_1$的计算复杂度问题进行了深入探索。

Conclusion: 该研究通过多种技术方法对STRIPS规划中算子限制最简情况的计算复杂度问题进行了系统性分析，为理解命题STRIPS规划的计算复杂性边界提供了新的见解。

Abstract: This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.

</details>


### [110] [Exploring SAIG Methods for an Objective Evaluation of XAI](https://arxiv.org/abs/2602.08715)
*Miquel Miró-Nicolau,Gabriel Moyà-Alcover,Anna Arias-Duart*

Main category: cs.AI

TL;DR: 本文首次系统回顾和分析了合成人工智能基准真值（SAIG）方法，提出了一种新的分类法，并揭示了XAI评估领域缺乏共识的现状。


<details>
  <summary>Details</summary>
Motivation: 可解释人工智能（XAI）评估领域方法多样，但缺乏统一的评估标准。与传统AI评估不同，XAI解释没有普遍正确的基准真值，使得客观评估具有挑战性。SAIG方法通过生成人工基准真值来解决这一问题，但该领域尚未有系统性综述。

Method: 本文对SAIG方法进行了首次系统性回顾和分析，提出了一种新的分类法来对这些方法进行分类，识别了区分不同SAIG方法的七个关键特征，并进行了比较研究。

Result: 研究发现XAI评估技术缺乏共识，不同方法之间存在显著差异，这凸显了该领域需要进一步研究和标准化。

Conclusion: SAIG方法为解决XAI评估挑战提供了有前景的方向，但当前领域缺乏统一的评估标准和方法共识，需要更多的研究和标准化工作来推动XAI评估的发展。

Abstract: The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.

</details>


### [111] [Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning](https://arxiv.org/abs/2602.08734)
*David Hudák,Maris F. L. Galesloot,Martin Tappler,Martin Kurečka,Nils Jansen,Milan Češka*

Main category: cs.AI

TL;DR: Lexpop框架使用深度强化学习训练循环神经网络策略，然后通过提取方法构建有限状态控制器，为POMDP提供可形式化验证的性能保证，并扩展到隐藏模型POMDP的鲁棒策略计算。


<details>
  <summary>Details</summary>
Motivation: 现有POMDP求解器在可扩展性方面仍然有限，特别是在需要跨多个POMDP的鲁棒策略时，可扩展性问题更加严重。

Method: 1) 使用深度强化学习训练循环神经网络策略；2) 通过高效提取方法构建模仿神经策略的有限状态控制器；3) 扩展到HM-POMDP时，为每个提取的控制器关联最坏情况POMDP，迭代训练鲁棒神经策略并提取鲁棒控制器。

Result: 在大状态空间问题上，Lexpop在POMDP和HM-POMDP求解方面均优于现有最先进求解器。

Conclusion: Lexpop框架通过结合神经策略学习和可验证控制器提取，有效解决了POMDP和HM-POMDP的可扩展性和鲁棒性问题。

Abstract: Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.

</details>


### [112] [Belief Offloading in Human-AI Interaction](https://arxiv.org/abs/2602.08754)
*Rose E. Guingrich,Dvija Mehta,Umang Bhatt*

Main category: cs.AI

TL;DR: 论文研究了人类与AI交互中的"信念卸载"现象，即人们将信念形成和维护过程外包给LLM系统，这会影响他们的行为和信念体系。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地使用LLM聊天机器人作为思维伙伴，这种认知卸载可能导致过度依赖，对认知技能产生负面影响。需要研究这种特定类型的认知卸载——信念卸载——在人类-AI交互中的发生条件和后果。

Method: 通过整合哲学、心理学和计算机科学的研究，明确定义信念卸载的边界条件，提供描述性分类法，并分析其规范性含义。

Result: 提出了信念卸载的概念框架和分类法，阐明了信念卸载发生的具体条件，并探讨了其对人类行为和信念体系的潜在影响。

Conclusion: 信念卸载是人类-AI交互中值得关注的现象，需要进一步研究其发生机制和后果，为未来工作指明了方向。

Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.

</details>


### [113] [Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure](https://arxiv.org/abs/2602.08783)
*Zirui Li,Xuefeng Bai,Kehai Chen,Yizhi Li,Jian Yang,Chenghua Lin,Min Zhang*

Main category: cs.AI

TL;DR: 该论文提出将潜在思维链视为表示空间中的可操纵因果过程，通过结构因果模型分析潜在步骤，研究其在数学和一般推理任务中的因果必要性、影响传播和答案模式保留。


<details>
  <summary>Details</summary>
Motivation: 当前潜在思维链方法用内部潜在步骤替代显式文本推理，但这些中间计算难以通过相关性探测之外的方式进行评估。需要更系统的方法来理解和分析这些潜在推理过程。

Method: 将潜在思维链建模为结构因果模型中的变量，通过逐步干预分析其因果效应。研究两种代表性范式（Coconut和CODI），在数学和一般推理任务上分析三个关键问题：步骤的因果必要性、影响传播结构、以及中间轨迹的答案模式保留。

Result: 发现潜在步骤预算不像同质额外深度，而更像具有非局部路由的分阶段功能；识别出早期输出偏见与晚期表示承诺之间的持续差距；潜在步骤表现出复杂的因果结构和模式保留特性。

Conclusion: 研究结果支持模式条件和稳定性感知分析作为解释和改进潜在推理系统的更可靠工具，并提出了相应的训练/解码目标。

Abstract: Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>


### [114] [The Use of AI Tools to Develop and Validate Q-Matrices](https://arxiv.org/abs/2602.08796)
*Kevin Fan,Jacquelyn A. Bialo,Hongli Li*

Main category: cs.AI

TL;DR: AI工具（通用语言模型）在认知诊断建模中生成Q矩阵的可行性研究，发现AI生成的Q矩阵与验证Q矩阵的一致性存在模型间差异，Gemini 2.5 Pro表现最佳，但新版AI模型表现下降。


<details>
  <summary>Details</summary>
Motivation: Q矩阵构建是认知诊断建模的关键但劳动密集型步骤，本研究旨在探索AI工具（通用语言模型）是否能够支持Q矩阵开发，减轻专家负担。

Method: 使用多个AI模型（包括Google Gemini 2.5 Pro等），提供与人类专家相同的训练材料，生成阅读理解测试的Q矩阵。通过Cohen's kappa系数评估AI生成Q矩阵、验证Q矩阵（Li和Suen，2013）和人类评分者Q矩阵之间的一致性。

Result: AI模型间存在显著差异：Google Gemini 2.5 Pro与验证Q矩阵的一致性最高（Kappa=0.63），超过了所有人类专家。但2026年1月使用新版AI模型进行的后续分析显示，与验证Q矩阵的一致性降低。

Conclusion: AI工具在Q矩阵开发中具有潜力，特别是某些模型表现优于人类专家，但AI模型版本更新可能导致性能变化，需要进一步研究AI在认知诊断建模中的可靠性和稳定性。

Abstract: Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.

</details>


### [115] [Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures](https://arxiv.org/abs/2602.08804)
*Liming Zhou,Ailing Liu,Hongwei Liu,Min He,Heng Zhang*

Main category: cs.AI

TL;DR: 提出RC-LLM方法，利用残差连接结构和大语言模型进行微服务架构中的根因定位，有效处理多源遥测数据并建模因果依赖关系。


<details>
  <summary>Details</summary>
Motivation: 在复杂大规模微服务架构中，根因定位面临挑战：微服务间复杂的故障传播，以及指标、日志、追踪等多源遥测数据的高维性限制了现有RCA方法的有效性。

Method: 提出RC-LLM方法：设计残差式层次融合结构整合多源遥测数据，利用大语言模型的上下文推理能力建模时间跨度和跨微服务的因果依赖关系。

Result: 在CCF-AIOps微服务数据集上的实验结果表明，RC-LLM在根因分析方面实现了强大的准确性和效率。

Conclusion: RC-LLM方法通过结合残差连接结构和LLM的推理能力，有效解决了微服务架构中复杂故障传播和多源数据融合的根因定位问题。

Abstract: Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.

</details>


### [116] [Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation](https://arxiv.org/abs/2602.08815)
*Yanglei Gan,Peng He,Yuxiang Cai,Run Lin,Guanyu Zhou,Qiao Liu*

Main category: cs.AI

TL;DR: NADEx是一个用于时序知识图谱推理的负感知扩散模型，通过结合负样本信息和余弦对齐正则化来提升未来事实预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时序知识图谱推理中存在两个问题：1) 生成路径仅基于正样本证据，忽略了信息丰富的负样本上下文；2) 训练目标主要依赖交叉熵排序，虽然能改善候选排序但对去噪嵌入的校准监督不足。

Method: NADEx编码实体、关系和时序间隔的以主体为中心的历史为序列嵌入，在前向过程中扰动查询对象，在反向过程中使用Transformer去噪器基于时序-关系上下文进行重建。此外，还引入了基于批次负样本原型的余弦对齐正则化器来收紧决策边界。

Result: 在四个公开时序知识图谱基准测试上的综合实验表明，NADEx实现了最先进的性能。

Conclusion: NADEx通过有效利用负样本信息和改进训练目标，显著提升了时序知识图谱推理中扩散模型的预测能力。

Abstract: Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>


### [117] [Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning](https://arxiv.org/abs/2602.08835)
*Andrés Holgado-Sánchez,Peter Vamplew,Richard Dazeley,Sascha Ossowski,Holger Billhardt*

Main category: cs.AI

TL;DR: 提出基于聚类和偏好多目标强化学习的算法，用于学习社会代理的价值对齐模型和价值系统，以解决价值感知AI中价值操作化的挑战。


<details>
  <summary>Details</summary>
Motivation: 价值感知AI需要识别人类价值并适应不同用户的价值系统，但价值操作化容易出错。现有方法需要手动设计特征，缺乏基于价值的可解释性和对多样化用户偏好的适应性。

Method: 在马尔可夫决策过程中，通过聚类和偏好多目标强化学习联合学习社会衍生的价值对齐模型（groundings）和代表不同用户群体的价值系统集合。每个聚类包含代表成员价值偏好的价值系统和反映与该价值系统对齐行为的近似帕累托最优策略。

Result: 在两个包含人类价值的MDP上，与最先进的PbMORL算法和基线方法进行了评估比较。

Conclusion: 该方法能够学习社会代理的价值对齐模型和价值系统，为价值感知AI提供了一种解决价值操作化挑战的有效途径。

Abstract: Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.

</details>


### [118] [Deciding the Satisfiability of Combined Qualitative Constraint Networks](https://arxiv.org/abs/2602.08848)
*Quentin Cohen-Solal,Alexandre Niveau,Maroua Bouzid*

Main category: cs.AI

TL;DR: 本文提出了一个统一框架，用于整合多种定性推理的扩展和组合形式，包括多尺度推理、时间序列和松散集成，并研究了可满足性决策及其复杂度。


<details>
  <summary>Details</summary>
Motivation: 定性推理能够在信息不精确、不完整且无数值的情况下推断新知识，但现有研究缺乏统一的框架来处理多种扩展和组合形式，需要系统研究其可满足性决策和复杂度。

Method: 提出一个形式化框架，统一处理多尺度推理、时间序列和松散集成等定性形式主义的扩展和组合，并建立两个互补定理来保证可满足性决策的多项式复杂度。

Result: 框架能够处理各种组合和扩展形式的推理，并统一研究可满足性决策及其复杂度。通过两个定理证明了可满足性决策的多项式性质，并恢复了尺寸-拓扑组合的已知结果。

Conclusion: 该统一框架不仅能够处理多种定性推理的扩展和组合，还扩展了定性形式主义的定义以包含文献中排除但在组合背景下重要的形式主义，为定性推理的复杂性分析提供了系统方法。

Abstract: Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.

</details>


### [119] [Scalable Delphi: Large Language Models for Structured Risk Estimation](https://arxiv.org/abs/2602.08889)
*Tobias Lorenz,Mario Fritz*

Main category: cs.AI

TL;DR: LLM可以替代传统的德尔菲专家咨询方法，将风险评估时间从数月缩短到几分钟，同时保持与人类专家判断的高度一致性。


<details>
  <summary>Details</summary>
Motivation: 传统德尔菲方法虽然准确但耗时数月，需要专家协调，限制了其在大多数应用中的可行性。需要寻找可扩展的替代方案来进行结构化专家咨询。

Method: 提出Scalable Delphi方法，将经典德尔菲协议适配到LLM，使用多样化的专家角色、迭代优化和理由分享。建立基于必要条件的评估框架：可验证代理的校准、对证据的敏感性、与人类专家判断的一致性。

Result: LLM专家小组与基准真实值有强相关性（Pearson r=0.87-0.95），随着证据增加系统性改进，与人类专家小组高度一致，在某些情况下甚至比两个人类专家小组之间的一致性更高。

Conclusion: LLM驱动的专家咨询可以将结构化专家判断扩展到传统方法不可行的场景，将咨询时间从数月减少到几分钟，为高风险领域的风险评估提供可扩展的解决方案。

Abstract: Quantitative risk assessment in high-stakes domains relies on structured expert elicitation to estimate unobservable properties. The gold standard - the Delphi method - produces calibrated, auditable judgments but requires months of coordination and specialist time, placing rigorous risk assessment out of reach for most applications. We investigate whether Large Language Models (LLMs) can serve as scalable proxies for structured expert elicitation. We propose Scalable Delphi, adapting the classical protocol for LLMs with diverse expert personas, iterative refinement, and rationale sharing. Because target quantities are typically unobservable, we develop an evaluation framework based on necessary conditions: calibration against verifiable proxies, sensitivity to evidence, and alignment with human expert judgment. We evaluate in the domain of AI-augmented cybersecurity risk, using three capability benchmarks and independent human elicitation studies. LLM panels achieve strong correlations with benchmark ground truth (Pearson r=0.87-0.95), improve systematically as evidence is added, and align with human expert panels - in one comparison, closer to a human panel than the two human panels are to each other. This demonstrates that LLM-based elicitation can extend structured expert judgment to settings where traditional methods are infeasible, reducing elicitation time from months to minutes.

</details>


### [120] [Efficient and Stable Reinforcement Learning for Diffusion Language Models](https://arxiv.org/abs/2602.08905)
*Jiawei Liu,Xiting Wang,Yuanyuan Zhong,Defu Lian,Yu Yang*

Main category: cs.AI

TL;DR: STP框架通过时空剪枝技术提升扩散大语言模型强化学习的效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 强化学习对解锁扩散大语言模型的复杂推理能力至关重要，但现有方法在效率和稳定性方面面临挑战

Method: 提出时空剪枝框架：空间剪枝利用静态先验约束探索空间；时间剪枝绕过冗余的后期细化步骤

Result: 理论分析证明STP严格降低对数似然估计方差，实验显示在效率和准确性上超越现有基线方法

Conclusion: STP框架有效解决了扩散大语言模型强化学习的效率和稳定性问题，为复杂推理能力开发提供了实用解决方案

Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>


### [121] [CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse](https://arxiv.org/abs/2602.08939)
*Longling Geng,Andy Ouyang,Theodore Wu,Daphne Barretto,Matthew John Hayes,Rachael Cooper,Yuqiao Zeng,Sameer Vijay,Gia Ancone,Ankit Rai,Matthew Wolfman,Patrick Flanagan,Edward Y. Chang*

Main category: cs.AI

TL;DR: CausalT5K是一个包含5000多个案例的诊断基准，用于系统检测LLM在因果推理中的失败模式，包括梯级坍塌、奉承漂移和错误拒绝，通过实用性和安全性指标揭示聚合精度无法发现的失败模式。


<details>
  <summary>Details</summary>
Motivation: LLM在因果推理中存在多种失败模式（奉承、梯级坍塌、错误拒绝），但由于缺乏系统性诊断基准，修复进展缓慢。需要创建一个能够系统检测这些失败模式的诊断工具。

Method: 开发了CausalT5K基准，包含5000多个案例，覆盖10个领域，测试三种关键能力：检测梯级坍塌、抵抗奉承漂移、生成明智拒绝。采用人机协作流程，涉及40名领域专家、迭代交叉验证，以及基于规则、LLM和人工评分的复合验证。

Result: 初步实验揭示了四象限控制景观，静态审计策略普遍失败。基准能够将性能分解为实用性（敏感性）和安全性（特异性），揭示聚合精度无法发现的失败模式。

Conclusion: CausalT5K作为研究基础设施，实现了Pearl的因果阶梯，为推进可信推理系统提供了有价值的诊断工具，能够系统检测LLM在因果推理中的关键失败模式。

Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>


### [122] [CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute](https://arxiv.org/abs/2602.08948)
*Chen Jin,Ryutaro Tanno,Tom Diethe,Philip Teare*

Main category: cs.AI

TL;DR: CoRefine是一种基于置信度引导的自优化方法，通过轻量级控制器实现推理准确率提升，大幅减少计算开销


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常依赖并行解码（如512个样本）来提升推理准确率，但这会带来巨大的计算开销。需要一种更高效的方法来优化推理过程

Method: CoRefine使用一个211k参数的Conv1D控制器，基于完整轨迹置信度决定是否停止、重新检查或尝试不同方法，实现有针对性的自我修正。CoRefine-Tree是混合顺序并行变体，自适应平衡探索和利用

Result: 平均每个问题只需2.7个优化步骤，相比512样本基线减少约190倍token消耗。控制器在自信停止时达到92.6%的精确度，表明置信度动态可靠地指示正确性。在多样化推理基准和三个开源模型上表现良好

Conclusion: 通过将置信度视为控制信号而非正确性保证，CoRefine为可扩展推理和具有不完美验证器的智能体设置提供了模块化基础组件

Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

</details>


### [123] [Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room](https://arxiv.org/abs/2602.08949)
*Mohammad Morsali,Siavash H. Khajavi*

Main category: cs.AI

TL;DR: IVSR是一个结合数字孪生与自主AI代理的双向平台，用于实时自适应野火灾害管理，显著降低检测到干预的延迟并提高资源协调效率。


<details>
  <summary>Details</summary>
Motivation: 联合国预测到2030年和2050年野火频率和强度将分别增加14%和30%，传统灾害管理框架依赖静态模拟和被动数据采集，无法实时适应不断演变的野火事件。

Method: 提出智能虚拟态势室(IVSR)，这是一个由自主AI代理增强的双向数字孪生平台。平台持续摄入多源传感器图像、天气数据和3D森林模型，创建火灾环境的实时虚拟副本。AI驱动的相似性引擎将新兴条件与预计算的灾害模拟库对齐，在专家监督下检索和校准干预策略。

Result: 通过工业合作伙伴提供的详细案例研究模拟验证IVSR，展示了在局部事件检测、隐私保护回放、基于碰撞器的火势蔓延预测和特定站点ML重新训练方面的能力。结果显示与传统系统相比，检测到干预的延迟显著降低，资源协调更有效。

Conclusion: IVSR通过将实时双向数字孪生与代理AI相结合，为主动、自适应的野火灾害管理提供了一个可扩展、半自动化的决策支持范式。

Abstract: According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin (DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics under the watchful eyes of experts. Authorized action-ranging from UAV redeployment to crew reallocation-is cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.

</details>


### [124] [stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation](https://arxiv.org/abs/2602.08968)
*Lucas Maes,Quentin Le Lidec,Dan Haramati,Nassim Massaudi,Damien Scieur,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: SWM是一个模块化、经过测试和文档化的世界模型研究生态系统，提供高效数据收集工具、标准化环境、规划算法和基线实现，旨在解决现有世界模型实现可复用性差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大多数世界模型实现都是针对特定论文的，这严重限制了它们的可复用性，增加了bug风险，并降低了评估标准化程度。需要建立一个统一的研究生态系统来解决这些问题。

Method: 开发了stable-worldmodel（SWM）系统，这是一个模块化的世界模型研究生态系统，包含：1）高效数据收集工具；2）标准化环境；3）规划算法；4）基线实现。每个环境还支持可控的变化因素（包括视觉和物理属性），以支持鲁棒性和持续学习研究。

Result: 通过使用SWM研究DINO-WM中的零样本鲁棒性，展示了该系统的实用性。SWM为世界模型研究提供了标准化、可复用的基础设施。

Conclusion: SWM解决了世界模型研究中实现碎片化的问题，提供了一个模块化、测试完善、文档齐全的研究生态系统，能够促进世界模型研究的可复用性、标准化和鲁棒性评估。

Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>


### [125] [InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery](https://arxiv.org/abs/2602.08990)
*Shiyang Feng,Runmin Ma,Xiangchao Yan,Yue Fan,Yusong Hu,Songtao Huang,Shuaiyu Zhang,Zongsheng Cao,Tianshuo Peng,Jiakang Yuan,Zijie Guo,Zhijie Zhong,Shangheng Du,Weida Wang,Jinxin Shi,Yuhao Zhou,Xiaohan He,Zhiyin Yu,Fangchen Yu,Qihao Zheng,Jiamin Wu,Mianxin Liu,Chi Zhang,Shaowei Hou,Shuya Li,Yankai Jiang,Wenjie Lou,Lilong Wang,Zifu Wang,Jiong Wang,Wanghan Xu,Yue Deng,Dongrui Liu,Yiheng Wang,Wenlong Zhang,Fenghua Ling,Shufei Zhang,Xiaosong Wang,Shuangjia Zheng,Xun Huang,Siqi Sun,Shuyue Hu,Peng Ye,Chunfeng Song,Bin Wang,Conghui He,Yihao Liu,Xin Li,Qibin Hou,Tao Chen,Xiangyu Yue,Bin Wang,Liang He,Dahua Lin,Bowen Zhou,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: InternAgent-1.5是一个用于跨计算和实证领域的端到端科学发现的统一系统，通过生成、验证和演化三个协调子系统实现自主科学发现，在基准测试和实际发现任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够统一协调计算建模和实验室实验的自主科学发现系统，实现跨计算和实证领域的端到端科学发现。

Method: 采用结构化架构，包含生成、验证和演化三个协调子系统，支持深度研究、解决方案优化和长时程记忆等基础能力，能够在扩展的发现周期中持续运行。

Result: 在GAIA、HLE、GPQA和FrontierScience等科学推理基准测试中取得领先性能；在算法发现任务中自主设计核心机器学习问题的竞争性方法；在实证发现任务中执行完整的计算或湿实验室实验，在地球、生命、生物和物理领域产生科学发现。

Conclusion: InternAgent-1.5为自主科学发现提供了一个通用且可扩展的框架，能够协调计算建模和实验室实验，实现跨领域的端到端科学发现。

Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.

</details>


### [126] [iGRPO: Self-Feedback-Driven LLM Reasoning](https://arxiv.org/abs/2602.09000)
*Ali Hatamizadeh,Shrimai Prabhumoye,Igor Gitman,Ximing Lu,Seungju Han,Wei Ping,Yejin Choi,Jan Kautz*

Main category: cs.AI

TL;DR: iGRPO是一种两阶段强化学习方法，通过模型自生成草稿和动态自我调节，提升大语言模型在数学推理任务中的表现，在多项基准测试中达到新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在解决复杂数学问题方面显示出潜力，但仍存在准确性和一致性不足的问题。需要更有效的强化学习方法来提升模型的任务对齐和可靠性。

Method: iGRPO是GRPO的两阶段扩展：第一阶段采样多个探索性草稿并选择最高奖励的草稿；第二阶段将该最佳草稿附加到原始提示后，在草稿条件化的改进版本上应用GRPO风格更新，训练策略超越其先前最佳尝试。

Result: 在匹配的rollout预算下，iGRPO在多个基础模型上持续优于GRPO。应用于OpenReasoning-Nemotron-7B模型时，在AIME24和AIME25基准上分别达到85.62%和79.64%的新SOTA结果。

Conclusion: 迭代的、基于自我反馈的强化学习方法在推进可验证数学推理方面具有巨大潜力，iGRPO通过动态自我调节和草稿条件化改进，有效提升了模型的推理能力。

Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>


### [127] [Data Science and Technology Towards AGI Part I: Tiered Data Management](https://arxiv.org/abs/2602.09003)
*Yudong Wang,Zixuan Fu,Hengyu Zhao,Chen Zhao,Chuyue Zhou,Xinle Lin,Hongya Lyu,Shuaikang Xue,Yi Yi,Yingjiao Wang,Zhi Zheng,Yuzhou Zhang,Jie Zhou,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 提出分层数据管理框架L0-L4，支持LLM全生命周期训练，通过模型指导数据管理实现数据-模型协同进化，显著提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究过度依赖数据规模单向扩展，面临数据可用性、获取成本和训练效率瓶颈。需要转向数据-模型协同进化的新范式，让模型主动指导数据管理，高质量数据反过来增强模型能力。

Method: 提出L0-L4分层数据管理框架：从原始未整理资源到组织化可验证知识。利用LLM进行质量评分和内容编辑等数据管理过程，每层具有不同数据属性、管理策略和训练角色，支持预训练、中期训练和对齐等不同训练阶段。

Result: 实验验证表明，分层数据利用能显著提高训练效率和模型性能。从原始语料库构建的分层数据集在多个训练阶段都表现出色。

Conclusion: 分层数据管理框架为可扩展和可持续的数据管理提供了系统方法，平衡了数据质量、获取成本和边际训练效益，推动了数据-模型协同进化的AGI发展新阶段。

Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>


### [128] [GEBench: Benchmarking Image Generation Models as GUI Environments](https://arxiv.org/abs/2602.09007)
*Haodong Li,Jingwei Wu,Quan Sun,Guopeng Li,Juanxi Tian,Huanyu Zhang,Yanlin Lai,Ruichuan An,Hongbo Peng,Yuhong Dai,Chenxi Li,Chunmei Qing,Jia Wang,Ziyang Meng,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.AI

TL;DR: GEBench是一个评估GUI动态交互和时间一致性的新基准，包含700个样本和五维评估指标GE-Score，发现现有模型在长序列交互中保持时间一致性和空间定位方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注通用领域的视觉保真度，而在GUI特定场景中对状态转换和时间一致性的评估不足。需要专门的基准来评估动态交互和时间一致性。

Method: 提出了GEBench基准，包含700个精心策划的样本，涵盖5个任务类别（单步交互、多步轨迹、真实/虚构场景、定位点）。开发了GE-Score五维评估指标：目标达成、交互逻辑、内容一致性、UI合理性和视觉质量。

Result: 当前模型在单步转换上表现良好，但在长交互序列中保持时间一致性和空间定位方面显著困难。图标解释、文本渲染和定位精度是关键瓶颈。

Conclusion: 该工作为系统评估提供了基础，并为构建高保真生成GUI环境指出了有前景的研究方向。代码已开源。

Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [129] [Single-shot lossy compression: mutual information bounds](https://arxiv.org/abs/2602.07280)
*Victoria Kostina*

Main category: cs.IT

TL;DR: 论文提出了在三种保真度约束下，用互信息作为最小期望描述长度的上界，并证明当数据信息量不太低时，最小化互信息可作为数据最小描述长度的合理代理。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效地表示随机变量，特别是在不同保真度约束下，寻找最小期望描述长度的上界。传统方法可能难以直接计算最小描述长度，因此需要寻找合理的代理目标。

Method: 针对三种保真度约束（保证失真、条件超额失真、超额失真），建立了互信息上界。通过相应的逆定理证明，当数据信息量不太低时，最小化互信息可作为最小描述长度的合理代理。提供了三种凸代理的替代表征，揭示了其解的结构。

Result: 证明了在三种保真度约束下，互信息提供了最小期望描述长度的上界。当数据信息量足够高时，最小化互信息确实能近似最小描述长度。给出了凸代理的替代表征，有助于理解其数学结构。

Conclusion: 在多种保真度约束下，互信息可作为最小描述长度的有效上界和合理代理。这一结果为数据压缩和表示提供了理论依据，特别是在信息量足够的情况下，最小化互信息是接近最优的策略。

Abstract: For several styles of fidelity constraints -- guaranteed distortion, conditional excess distortion, excess distortion -- we show mutual information upper bounds on the minimum expected description length needed to represent a random variable. Coupled with the corresponding converses, these results attest that as long as the information content in the data is not too low, minimizing the mutual information under an appropriate fidelity constraint serves as a reasonable proxy for the minimum description length of the data. We provide alternative characterizations of all three convex proxies, shedding light on the structure of their solutions.

</details>


### [130] [Network function computation with vector linear target function and security function](https://arxiv.org/abs/2602.07316)
*Min Xu,Qian Chen,Gennian Ge*

Main category: cs.IT

TL;DR: 研究在有窃听者的网络中安全计算向量线性函数的问题，建立了安全计算容量的上下界，并针对特定网络和函数提出了安全向量线性网络码的构造方法。


<details>
  <summary>Details</summary>
Motivation: 研究在有窃听威胁的网络中安全计算函数的问题，其中目标函数和安全函数都是向量线性的。网络建模为有向无环图，需要防止窃听者获取源消息的特定安全函数信息。

Method: 1) 建立了两个适用于任意网络拓扑和任意向量线性目标/安全函数的上界；2) 当目标函数为有限域上的和时，扩展了将非安全网络码转换为安全网络码的方法；3) 针对特定网络类别和向量线性目标函数，刻画了构造安全向量线性网络码所需的全局编码矩阵特性。

Result: 1) 建立了两个通用的安全计算容量上界，推广了现有结果，并为有限域上的和函数提供了新上界；2) 成功将非安全网络码转换为安全网络码的方法扩展到向量线性安全函数的情况；3) 为特定网络类别和向量线性目标函数提供了安全向量线性网络码的构造特性。

Conclusion: 本文在有窃听者的网络中安全计算向量线性函数方面取得了重要进展，建立了理论上下界，并为特定情况提供了构造方法，为网络编码的安全计算提供了新的理论基础。

Abstract: In this paper, we study the problem of securely computing a function over a network, where both the target function and the security function are vector linear. The network is modeled as a directed acyclic graph. A sink node wishes to compute a function of messages generated by multiple distributed sources, while an eavesdropper can access exactly one wiretap set from a given collection. The eavesdropper must be prevented from obtaining any information about a specified security function of the source messages. The secure computing capacity is the maximum average number of times that the target function can be securely computed with zero error at the sink node with the given collection of wiretap sets and security function for one use of the network. We establish two upper bounds on this capacity, which hold for arbitrary network topologies and for any vector linear target and security functions. These bounds generalize existing results and also lead to a new upper bound when the target function is the sum over a finite field. For the lower bound, when the target function is the sum, we extend an existing method, which transforms a non-secure network code into a secure one, to the case where the security function is vector linear. Furthermore, for a particular class of networks and a vector linear target function, we characterize the required properties of the global encoding matrix to construct a secure vector linear network code.

</details>


### [131] [Multicasting Pinching Antenna Systems With LoS Blockage](https://arxiv.org/abs/2602.07421)
*Muhammad Fainan Hanif,Yuanwei Liu*

Main category: cs.IT

TL;DR: 本文研究Pinching-antenna系统(PASS)在多播场景下的最优位置分配问题，针对非视距环境，提出基于MM算法的收敛算法，比较CSM和BSM两种求解方法，发现BSM在计算复杂度上更优。


<details>
  <summary>Details</summary>
Motivation: Pinching-antenna系统(PASS)是一种在高频段有前景的可定制无线接入机制，但在非视距环境下，用户与pinching天线之间不一定存在直射链路，需要研究如何最优分配PAs位置以实现高效多播传输。

Method: 采用minorization-maximization(MM)原理设计可证明收敛的算法，在每次MM迭代中，通过两种方法求解凸代理问题：候选搜索方法(CSM)和二分搜索方法(BSM)。

Result: 在非视距环境中，PASS多播性能优于传统天线系统(CAS)。当用户和PAs数量增加时，BSM比CSM具有更好的整体计算复杂度，例如8个PAs和25个用户时，CSM执行时间约为BSM的2.5倍。

Conclusion: PASS在非视距多播场景中具有性能优势，BSM方法在计算效率上优于CSM，特别适用于大规模用户和天线配置，为高频段无线接入提供了有效的优化解决方案。

Abstract: Pinching-antenna systems (PASS) represent a promising customizable wireless access mechanism in high-frequency bands, enabled by dielectric waveguides and movable dielectric particles, called pinching antennas (PAs). In this work, we study optimal position allocation of PAs in PASS for multicasting in the downlink when a line-of-sight (LoS) link does not necessarily exist between all users and the PAs. The multicasting problem is solved by leveraging minorization-maximization (MM) principle to yield a provably convergent algorithm. In each run of the MM based procedure, we solve a convex surrogate problem using two methods called the candidate search method (CSM) and the bisection search method (BSM). With both BSM and CSM, we not only report superior performance of the multicasting PASS in non-LoS environments compared to conventional antenna systems (CAS), but also determine that BSM yields better overall computational complexity when the number of users and PAs increases. For example, we report that when we have 8 PAs and 25 users, the execution time with the CSM is approximately 2.5 times that with the BSM.

</details>


### [132] [Information Theoretic Modeling of Interspecies Molecular Communication](https://arxiv.org/abs/2602.07474)
*Bitop Maitra,Murat Kuscu,Ozgur B. Akan*

Main category: cs.IT

TL;DR: 该论文提出了一个信息论框架来建模植物与昆虫之间的挥发性有机化合物（VOC）分子通信，考虑了风、距离和生物反应等环境噪声，并使用多项式分布模拟受体响应。


<details>
  <summary>Details</summary>
Motivation: 植物和昆虫通过VOC进行化学信号通信，但这种通信发生在有风、距离和复杂生物反应等噪声环境中。目前缺乏一个能够量化这种跨物种分子通信性能的信息论框架。

Method: 开发了一个信息论框架来建模跨物种分子通信，将受体响应建模为多项式分布，并考虑了环境参数如风速、距离和释放分子数量对通信的影响。

Result: 数值结果表明，通信性能取决于风速、距离和释放分子数量等环境参数。该框架为现实生物和环境条件下的VOC跨物种通信提供了基础性见解。

Conclusion: 提出的信息论框架能够有效建模植物与昆虫之间的VOC分子通信，揭示了环境因素对通信性能的影响，为理解现实条件下的跨物种化学通信提供了理论基础。

Abstract: Plants and insects communicate using chemical signals like volatile organic compounds (VOCs). A plant encodes information using different blends of VOCs, which propagate through the air to represent different symbolic information. This communication occurs in a noisy environment, characterized by wind, distance, and complex biological reactions. At the receiver, cross-reactive olfactory receptors produce stochastic binding events whose discretized durations form the receiver observation. In this paper, an information-theoretic framework is developed to model interspecies molecular communication (MC), where receptor responses are modeled probabilistically using a multinomial distribution. Numerical results show that the communication depends on environmental parameters such as wind speed, distance, and the number of released molecules. The proposed framework provides fundamental insights into the VOC-based interspecies communication under realistic biological and environmental conditions.

</details>


### [133] [Transformer-based Hybrid Beamforming with Dynamic Subarray for Near-Space Airship-Borne Communications](https://arxiv.org/abs/2602.07509)
*Ruiqi Wang,Zhen Gao,Keke Ying,Ziwei Wan,Symeon Chatzinotas,Mohamed-Slim Alouini*

Main category: cs.IT

TL;DR: 提出用于近空间飞艇通信的混合波束成形框架，采用动态子阵列结构和Transformer编码器设计，显著提升频谱效率和能量效率


<details>
  <summary>Details</summary>
Motivation: 针对近空间飞艇通信中能量受限的问题，需要设计高能量效率的大规模MIMO系统，传统固定连接结构无法适应动态信道环境

Method: 提出联合动态混合波束成形网络(DyHBFNet)，包含三个基于Transformer编码器的组件：模拟波束成形网络(ABFNet)、天线选择网络(ASNet)和数字波束成形网络(DBFNet)，采用动态子阵列结构连接天线和射频链

Result: 仿真结果表明，相比基线方案，该框架显著提升了频谱效率和能量效率，且在非理想信道状态信息下仍保持鲁棒性能

Conclusion: 该混合波束成形框架为近空间飞艇通信提供了可扩展的实用解决方案，通过动态子阵列结构和Transformer编码器设计实现了高性能的波束成形

Abstract: This paper proposes a hybrid beamforming framework for massive multiple-input multiple-output (MIMO) in near-space airship-borne communications. To achieve high energy efficiency (EE) in energy-constraint airships, a dynamic subarray structure is introduced, where each radio frequency chain (RFC) is connected to a disjoint subset of the antennas according to channel state information (CSI). The proposed joint dynamic hybrid beamforming network (DyHBFNet) comprises three key components: 1) An analog beamforming network (ABFNet) that optimizes the analog beamforming matrices and provides auxiliary information for the antenna selection network (ASNet) design, 2) an ASNet that dynamically optimizes the connections between antennas and RFCs, and 3) a digital beamforming network (DBFNet) that optimizes digital beamforming matrices by employing a model-driven weighted minimum mean square error algorithm for improving beamforming performance and convergence speed. The proposed ABFNet, ASNet, and DBFNet are all designed based on advanced Transformer encoders. Simulation results demonstrate that the proposed framework significantly enhances spectral efficiency and EE compared to baseline schemes. Additionally, its robust performance under imperfect CSI makes it a scalable solution for practical implementations.

</details>


### [134] [Expected Recovery Time in DNA-based Distributed Storage Systems](https://arxiv.org/abs/2602.07601)
*Adi Levy,Roni Con,Eitan Yaakobi,Han Mao Kiah*

Main category: cs.IT

TL;DR: 研究DNA分布式存储系统，分析多种纠删码在DNA存储容器故障恢复中的预期恢复时间


<details>
  <summary>Details</summary>
Motivation: DNA存储容器受测序技术限制，每次读取只能随机采样容器中的DNA链，需要研究在这种约束下的分布式存储系统

Method: 考虑多种纠删码，通过分析经典优惠券收集问题的广义版本来分析预期恢复时间

Result: 获得了DNA分布式存储系统中故障容器数据恢复的预期时间分析结果

Conclusion: 提出了DNA分布式存储系统的新研究方向，相关分析方法对优惠券收集问题的研究也有独立价值

Abstract: We initiate the study of DNA-based distributed storage systems, where information is encoded across multiple DNA data storage containers to achieve robustness against container failures. In this setting, data are distributed over $M$ containers, and the objective is to guarantee that the contents of any failed container can be reliably reconstructed from the surviving ones. Unlike classical distributed storage systems, DNA data storage containers are fundamentally constrained by sequencing technology, since each read operation yields the content of a uniformly random sampled strand from the container. Within this framework, we consider several erasure-correcting codes and analyze the expected recovery time of the data stored in a failed container. Our results are obtained by analyzing generalized versions of the classical Coupon Collector's Problem, which may be of independent interest.

</details>


### [135] [Wireless Streamlet: A Spectrum-Aware and Cognitive Consensus Protocol for Edge IoT](https://arxiv.org/abs/2602.07630)
*Taotao Wang,Long Shi,Fang Liu,Qing Yang,Shengli Zhang*

Main category: cs.IT

TL;DR: 无线Streamlet：为无线边缘物联网设计的频谱感知共识协议，通过信道感知领导者选举和编码双链架构，在频谱受限环境中提高吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统BFT协议在频谱拥塞和动态变化的无线边缘物联网网络中效率低下，因为它们缺乏频谱感知能力，导致资源利用率低且在时变干扰下进展脆弱。需要一种专门为无线环境设计的共识协议。

Method: 基于Streamlet的简化结构，引入信道感知领导者选举机制，利用接收器测量的信道状态信息计算拜占庭鲁棒性连接分数，从最终化历史中确定性选择加权领导者。采用单跳广播介质和确定性TDMA投票调度实现线性时隙复杂度。提出编码双链架构，将仅头部的共识链与数据链分离，使用擦除编码和链上完整性承诺。

Result: 实验表明，在损耗环境中，无线Streamlet比代表性基线方法实现了更高的吞吐量和更低的确认延迟，同时显著减少了每个节点的存储需求。

Conclusion: 将认知感知集成到共识逻辑中是有效的，无线Streamlet通过频谱感知和认知共识协议，为无线边缘物联网提供了高效的区块链解决方案。

Abstract: Blockchain offers a decentralized trust framework for the Internet of Things (IoT), yet deploying consensus in spectrum-congested and dynamic wireless edge IoT networks faces fundamental obstacles: traditional BFT protocols are spectrum-ignorant, leading to inefficient resource utilization and fragile progress under time-varying interference. This paper presents \textit{Wireless Streamlet}, a spectrum-aware and cognitive consensus protocol tailored for wireless edge IoT. Building on Streamlet's streamlined structure, we introduce a \textit{Channel-Aware Leader Election (CALE)} mechanism. CALE serves as a verifiable cross-layer cognitive engine that leverages receiver-measured channel state information (CSI) piggybacked in signed votes to derive Byzantine-robust connectivity scores from notarization certificates, and deterministically selects a unique weighted leader per epoch from finalized history, thereby improving proposal dissemination reliability under deep fading. Complementing this cognitive adaptation, Wireless Streamlet exploits the single-hop broadcast medium and a deterministic TDMA voting schedule to achieve linear per-epoch on-air transmissions (slot complexity), ensuring deterministic spectral access. To address the communication-storage trade-off, we further propose a coded dual-chain architecture that decouples header-only consensus (State Chain) from payload data (Data Chain). By employing erasure coding and on-chain integrity commitments, the system minimizes redundant spectrum usage for data retrieval while ensuring availability. Experiments show that Wireless Streamlet achieves higher throughput and lower confirmation latency than representative baselines in lossy environments, while substantially reducing per-node storage, demonstrating the efficacy of integrating cognitive sensing into consensus logic.

</details>


### [136] [Data Compression with Stochastic Codes](https://arxiv.org/abs/2602.07635)
*Gergely Flamich,Deniz Gündüz*

Main category: cs.IT

TL;DR: 本文是关于相对熵编码的综述性论文，旨在为读者提供该领域的广泛概述，特别关注文献中缺失的计算和实践方面。


<details>
  <summary>Details</summary>
Motivation: 机器学习在过去十年对数据压缩产生了重大影响，并激发了许多新的理论和应用问题。相对熵编码作为量化与熵编码的替代方案，在损失源编码中具有重要价值，但文献中缺乏对其计算和实践方面的全面讨论。

Method: 本文采用综述性方法，从三个层面展开：为好奇读者提供直观理解，为应用研究者介绍当代主要应用，为已有了解但不清楚算法细节的读者阐明简单优雅的构造原理。

Result: 提供了相对熵编码领域的全面概述，填补了文献中计算和实践方面的空白，展示了该领域作为数据压缩研究中简单而令人兴奋的新兴方向。

Conclusion: 相对熵编码是数据压缩研究中一个简单而令人兴奋的新兴领域，本文通过多角度分析为不同背景的读者提供了理解该领域的有效途径，强调了其在理论和应用上的重要性。

Abstract: Machine learning has had a major impact on data compression over the last decade and inspired many new, exciting theoretical and applied questions.
  This paper describes one such direction -- relative entropy coding -- which focuses on constructing stochastic codes, primarily as an alternative to quantisation and entropy coding in lossy source coding. Our primary aim is to provide a broad overview of the topic, with an emphasis on the computational and practical aspects currently missing from the literature.
  Our goal is threefold: for the curious reader, we aim to provide an intuitive picture of the field and convince them that relative entropy coding is a simple yet exciting emerging field in data compression research. For a reader interested in applied research on lossy data compression, we provide an account of the most salient contemporary applications. Finally, for the reader who has heard of relative entropy coding but has never been quite sure what it is or how the algorithms fit together, we hope to illustrate how simple and elegant the underlying constructions are.

</details>


### [137] [Spectral Graph Analysis for Predicting QoE Fairness Sensitivity in Wireless Communication Networks](https://arxiv.org/abs/2602.07855)
*Xinke Jian,Zhiyuan Ren,Wenchi Cheng*

Main category: cs.IT

TL;DR: 本文通过谱图理论分析QoE公平性，证明了新颖的指数谱上界，揭示了QoE公平性改进在性能阈值以上呈指数衰减，衰减率由SLA严格性和网络谱间隙共同决定。


<details>
  <summary>Details</summary>
Motivation: 学术界长期缺乏将底层拓扑与高层服务公平性联系起来的预测方法。QoE公平性评估不仅取决于当前状态，更关键的是其对SLA参数变化的敏感性。需要填补这一理论空白。

Method: 通过谱图理论分析QoE不平衡指数(I)，证明了一个新颖的指数谱上界。该上界首次将服务协议和拓扑瓶颈统一在单一性能边界公式中。

Result: 发现QoE公平性改进在由网络规模和连接性决定的性能阈值以上呈指数衰减行为。核心衰减率由SLA严格性(a)和网络谱间隙(cλ₂)中较弱者主导。系统公平性上限由服务参数和网络结构中的较弱环节决定。

Conclusion: 该理论关系揭示了清晰的瓶颈效应，为网络设计中的资源优化提供了瓶颈驱动原则，并支持目标驱动的逆向工程。在各种随机图模型和真实网络拓扑上的数值实验验证了分析框架的正确性和普适性。

Abstract: The evaluation of Quality of Experience (QoE) fairness depends not only on its current state but, more critically, on its sensitivity to changes in Service Level Agreement (SLA) parameters. However, the academic community has long lacked a predictive method connecting underlying topology to high-level service fairness. To bridge this gap, this paper analyzes a QoE imbalance index ($I$) through the lens of spectral graph theory.Our core contribution is the proof of a novel exponential spectral upper bound. This bound reveals that the improvement of QoE fairness exhibits an exponential decay behavior only above a performance threshold determined jointly by network size and connectivity. Its core decay rate is dominated by the weaker of two factors: the SLA stringency ($a$) and the network's spectral gap ($cλ_2$). The upper bound unifies the service protocol and the topological bottleneck within a single performance bound formula for the first time.This theoretical relationship also reveals a clear bottleneck effect, where the system's fairness ceiling is determined by the weaker link between service parameters and network structure. This finding provides a bottleneck-driven principle for resource optimization in network design and enables goal-driven reverse engineering. Extensive numerical experiments on various random graph models and real-world network topologies robustly validate the correctness and universality of our analytical framework.

</details>


### [138] [Capacity Scaling Laws for Boundary-Induced Drift-Diffusion Noise Channels](https://arxiv.org/abs/2602.07866)
*Yen-Chi Lee*

Main category: cs.IT

TL;DR: 本文研究多维漂移扩散过程首次击中吸收超平面产生噪声的加性噪声信道的高功率容量缩放。通过将基础随机传输机制识别为高斯方差混合，引入并分析NDFHL模型，证明在二阶矩约束下，各向同性高斯信令渐近达到容量，预对数因子仅由接收边界维度决定，揭示信道自由度的几何起源。


<details>
  <summary>Details</summary>
Motivation: 研究边界诱导噪声信道的信息理论特性，特别是由多维漂移扩散过程首次击中吸收超平面产生的噪声。这种噪声模型在物理和工程应用中常见，但缺乏系统的容量分析。论文旨在理解这种几何驱动噪声信道的基本极限特性。

Method: 将随机传输机制建模为高斯方差混合，引入NDFHL（正态漂移首次击中位置）模型。在二阶矩约束下，推导高信噪比容量展开，证明渐近上下界在常数级一致。分析噪声的熵特性，建立与奇异无限方差柯西极限的连接。

Result: 获得精确的高信噪比容量展开，证明各向同性高斯信令对所有固定漂移强度都渐近达到容量，尽管噪声是非高斯且半重尾的。预对数因子仅由接收边界维度决定，揭示信道自由度的几何起源。发现熵主导的普适性：所有传输过程的物理参数仅通过诱导噪声的微分熵影响容量。

Conclusion: NDFHL模型为边界击中信道提供了统一的几何和信息理论表征，跨越正则和奇异传输机制。尽管NDFHL密度没有简单闭式，但其熵有限且在漂移消失时连续变化，连接了有限方差机制与奇异无限方差柯西极限。这些结果揭示了边界诱导噪声信道的基本信息理论特性。

Abstract: This paper studies the high-power capacity scaling of additive noise channels whose noise arises from the first-hitting location of a multidimensional drift-diffusion process on an absorbing hyperplane. By identifying the underlying stochastic transport mechanism as a Gaussian variance-mixture, we introduce and analyze the Normally-Drifted First-Hitting Location (NDFHL) family as a geometry-driven model for boundary-induced noise. Under a second-moment constraint, we derive an exact high-SNR capacity expansion and show that the asymptotic upper and lower bounds coincide at the constant level, yielding a vanishing capacity gap. As a consequence, isotropic Gaussian signaling is asymptotically capacity-achieving for all fixed drift strengths, despite the non-Gaussian and semi-heavy-tailed nature of the noise. The pre-log factor is determined solely by the dimension of the receiving boundary, revealing a geometric origin of the channel's degrees of freedom. The refined expansion further uncovers an entropy-dominant universality, whereby all physical parameters of the transport process -- including drift strength, diffusion coefficient, and boundary separation -- affect the capacity only through the differential entropy of the induced noise. Although the NDFHL density does not admit a simple closed form, its entropy is shown to be finite and to vary continuously as the drift vanishes, thereby connecting the finite-variance regime with the singular infinite-variance Cauchy limit. Together, these results provide a unified geometric and information-theoretic characterization of boundary-hitting channels across both regular and singular transport regimes.

</details>


### [139] [Deep learning based Channel Estimation and Beamforming in Movable Antenna Systems](https://arxiv.org/abs/2602.07870)
*Kaijun Feng,Ziwei Wan,Anwen Liao,Wenyan Ma,Lipeng Zhu,Zhenyu Xiao,Zhen Gao,Rui Zhang*

Main category: cs.IT

TL;DR: 提出基于深度学习的可移动天线系统框架，集成信道估计、天线位置优化和波束成形，通过两阶段信道估计和Transformer网络实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 可移动天线技术相比固定位置天线能通过天线移动优化信道条件，但多用户宽带系统中信道估计、天线位置优化和波束成形的联合优化具有挑战性，需要高效解决方案。

Method: 1) 两阶段信道估计：先用压缩感知从有限测量重建信道矩阵，再用Swin-Transformer去噪网络提高精度；2) Transformer网络将候选位置的信道状态信息序列映射到最优天线位置；3) 结合模型驱动的加权最小均方误差波束成形方法。

Result: 仿真结果表明，所提方法在各种条件下相比现有方法获得更优性能，代码已在GitHub开源。

Conclusion: 提出的深度学习框架有效解决了可移动天线系统中的信道估计、位置优化和波束成形联合优化问题，为未来无线系统提供了有前景的解决方案。

Abstract: Movable antenna (MA) has emerged as a promising technology for future wireless systems. Compared with traditional fixed-position antennas, MA improves system performance by antenna movement to optimize channel conditions. For multiuser wideband MA systems, this paper proposes deep learning-based framework integrating channel estimation (CE), antenna position optimization, and beamforming, with a clear workflow and enhanced efficiency. Specifically, to obtain accurate channel state information (CSI), we design a two-stage CE mechanism: first reconstructing the channel matrix from limited measurements via compressive sensing, then introducing a Swin-Transformer-based denoising network to refine CE accuracy for subsequent optimization. Building on this, we address the joint optimization challenge by proposing a Transformer-based network that intelligently maps CSI sequences of candidate positions to optimal MA positions while combining a model-driven weighted minimum mean square error (WMMSE) beamforming approach to achieve better performance. Simulation results demonstrate that the proposed methods achieve superior performance compared with existing counterparts under various conditions. The codes about this work are available at https://github.com/ZiweiWan/Code-4-DL-MA-CE-BF.

</details>


### [140] [Deep Variable-Length Feedback Codes](https://arxiv.org/abs/2602.07881)
*Yu Ding,Yulin Shao*

Main category: cs.IT

TL;DR: DeepVLF是一种基于深度学习的可变长度反馈编码框架，通过动态调整传输长度实现自适应编码，相比现有方法显著减少信道使用并降低误码率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的反馈信道编码存在三个主要限制：固定块长度、高码率下性能下降、无法充分利用反馈的自适应潜力。需要一种更灵活的编码框架来克服这些限制。

Method: 提出DeepVLF框架，包含两种互补架构：DeepVLF-R（接收端驱动终止）和DeepVLF-T（发送端控制终止）。采用比特分组划分和基于Transformer的编码器-解码器网络，实现细粒度的码率自适应。

Result: 在AWGN和5G-NR衰落信道上的评估表明，DeepVLF显著优于现有学习反馈编码方法：达到相同误块率时减少20%-55%的信道使用，在高码率区域将误码平台降低数个数量级。

Conclusion: DeepVLF成功实现了自适应可变长度反馈编码，模型自主学习出类似经典Schalkwijk-Kailath编码的两阶段策略，体现了学习编码的可解释性和信息理论一致性。

Abstract: Deep learning has enabled significant advances in feedback-based channel coding, yet existing learned schemes remain fundamentally limited: they employ fixed block lengths, suffer degraded performance at high rates, and cannot fully exploit the adaptive potential of feedback. This paper introduces Deep Variable-Length Feedback (DeepVLF) coding, a flexible coding framework that dynamically adjusts transmission length via learned feedback. We propose two complementary architectures: DeepVLF-R, where termination is receiver-driven, and DeepVLF-T, where the transmitter controls termination. Both architectures leverage bit-group partitioning and transformer-based encoder-decoder networks to enable fine-grained rate adaptation in response to feedback. Evaluations over AWGN and 5G-NR fading channels demonstrate that DeepVLF substantially outperforms state-of-the-art learned feedback codes. It achieves the same block error rate with 20%-55% fewer channel uses and lowers error floors by orders of magnitude, particularly in high-rate regimes. Encoding dynamics analysis further reveals that the models autonomously learn a two-phase strategy analogous to classical Schalkwijk-Kailath coding: an initial information-carrying phase followed by a noise-cancellation refinement phase. This emergent behavior underscores the interpretability and information-theoretic alignment of the learned codes.

</details>


### [141] [Rich-ARQ: From 1-bit Acknowledgment to Rich Neural Coded Feedback](https://arxiv.org/abs/2602.07886)
*Enhao Chen,Yulin Shao*

Main category: cs.IT

TL;DR: Rich-ARQ：用高维神经编码反馈取代传统1比特ACK/NACK，实现发射端与接收端的协作物理层信道编码，显著提升无线通信性能


<details>
  <summary>Details</summary>
Motivation: 传统无线通信中的1比特ACK/NACK反馈机制信息量有限，仅能提供简单的确认/否定，无法充分利用反馈信道进行主动协作。作者希望将被动确认转变为主动协作，通过信息丰富的反馈向量提升通信性能。

Method: 提出Rich-ARQ范式，引入神经编码反馈实现协作物理层信道编码。开发了新型异步反馈编码，消除反馈延迟导致的停滞，动态适应信道波动，并采用轻量级编码器适合设备端部署。构建了首个全栈、标准兼容的软件定义无线电原型，将AI推理与严格无线电时序解耦。

Result: 通过全面的空中实验证明，Rich-ARQ相比传统1比特混合ARQ实现了显著的SNR增益，相比先前基于学习的反馈编码显著降低了延迟，将智能反馈的承诺从理论转化为实际高性能现实。

Conclusion: Rich-ARQ通过神经编码反馈将无线通信反馈机制从简单的二进制确认转变为高维协作编码，为下一代网络提供了实用、高性能的智能反馈解决方案，实现了理论到实践的跨越。

Abstract: This paper reimagines the foundational feedback mechanism in wireless communication, transforming the prevailing 1-bit binary ACK/NACK with a high-dimensional, information-rich vector to transform passive acknowledgment into an active collaboration. We present Rich-ARQ, a paradigm that introduces neural-coded feedback for collaborative physical-layer channel coding between transmitter and receiver. To realize this vision in practice, we develop a novel asynchronous feedback code that eliminates stalling from feedback delays, adapts dynamically to channel fluctuations, and features a lightweight encoder suitable for on-device deployment. We materialize this concept into the first full-stack, standard-compliant software-defined radio prototype, which decouples AI inference from strict radio timing. Comprehensive over-the-air experiments demonstrate that Rich-ARQ achieves significant SNR gains over conventional 1-bit hybrid ARQ and remarkable latency reduction over prior learning-based feedback codes, moving the promise of intelligent feedback from theory to a practical, high-performance reality for next-generation networks.

</details>


### [142] [OFDM Enabled Over-the-Air Computation Systems with Two-Dimensional Fluid Antennas](https://arxiv.org/abs/2602.07953)
*Heyang Xiong,Quanzhong Li,Qi Zhang*

Main category: cs.IT

TL;DR: 本文提出一种基于二维流体天线系统的OFDM空口计算方案，通过优化天线位置和预编码器来降低计算均方误差。


<details>
  <summary>Details</summary>
Motivation: 流体天线系统能够利用无线信道的空间自由度，但在频率选择性环境中如何有效利用这些自由度尚待研究。本文旨在通过二维流体天线系统增强OFDM空口计算的性能。

Method: 将原始计算均方误差最小化问题分解为发射预编码器优化问题，以及天线位置和接收合并器联合优化问题。后者采用主最小化方法和顺序优化相结合的方法求解。

Result: 数值结果表明，所提方案相比固定位置天线方案能够显著降低计算均方误差。

Conclusion: 二维流体天线系统能够有效利用频率选择性环境中的空间自由度，通过优化天线位置和预编码器可以显著提升OFDM空口计算的性能。

Abstract: Fluid antenna system (FAS) is able to exploit spatial degrees of freedom (DoFs) in wireless channels. In this letter, to exploit spatial DoFs in frequency-selective environments, we investigate an orthogonal frequency division multiplexing enabled over-the-air computation system, where the access point is equipped with a two-dimensional FAS to enhance performance. We solve the computation mean square error (MSE) minimization problem by transforming the original problem into transmit precoders optimization problem and antenna positions optimization along with receive combiners optimization problem. The latter is solved via a majorization-minimization approach combined with sequential optimization. Numerical results confirm that the proposed scheme achieves MSE reduction over the scheme with fixed position antennas.

</details>


### [143] [Tighter Information-Theoretic Generalization Bounds via a Novel Class of Change of Measure Inequalities](https://arxiv.org/abs/2602.07999)
*Yanxiao Liu,Yijun Fan an Deniz Gündüz*

Main category: cs.IT

TL;DR: 提出基于f-散度数据处理不等式的统一框架，得到更紧的测度变换不等式，应用于随机学习算法的泛化误差分析，获得新的信息论泛化界。


<details>
  <summary>Details</summary>
Motivation: 现有信息论泛化界存在局限性，需要更紧的测度变换不等式来改进随机学习算法的泛化误差分析。

Method: 基于f-散度的数据处理不等式构建统一框架，推导出涵盖f-散度、Rényi散度、α-互信息等广泛信息度量的测度变换不等式。

Result: 获得了更紧的高概率信息论泛化界，同时通过简化分析恢复了多个已知最佳结果，框架灵活适应条件互信息、PAC-Bayes理论和差分隐私机制。

Conclusion: 提出的统一框架不仅提供了更紧的泛化界，还展示了信息论工具在机器学习理论分析中的强大适应性和灵活性。

Abstract: In this paper, we propose a novel class of change of measure inequalities via a unified framework based on the data processing inequality for $f$-divergences, which is surprisingly elementary yet powerful enough to yield tighter inequalities. We provide change of measure inequalities in terms of a broad family of information measures, including $f$-divergences (with Kullback-Leibler divergence and $χ^2$-divergence as special cases), Rényi divergence, and $α$-mutual information (with maximal leakage as a special case). We then embed these inequalities into the analysis of generalization error for stochastic learning algorithms, yielding novel and tighter high-probability information-theoretic generalization bounds, while also recovering several best-known results via simplified analyses. A key advantage of our framework is its flexibility: it readily adapts to a range of settings, including the conditional mutual information framework, PAC-Bayesian theory, and differential privacy mechanisms, for which we derive new generalization bounds.

</details>


### [144] [Term Coding and Dispersion: A Perfect-vs-Rate Complexity Dichotomy for Information Flow](https://arxiv.org/abs/2602.08110)
*Søren Riis*

Main category: cs.IT

TL;DR: 本文提出了一种称为"项编码"的新框架，用于离散数学和信息流中的极值问题，通过选择函数符号的解释来最大化满足有限项方程系统的赋值数量。特别关注"分散"这一特殊情况，即系统定义了一个项映射，目标是最大化其像的大小。


<details>
  <summary>Details</summary>
Motivation: 研究离散数学和信息流中的极值问题，特别是如何通过优化函数符号的解释来最大化满足项方程系统的赋值数量。这涉及到组合优化和信息论中的基本问题。

Method: 提出"项编码"框架，专注于"分散"问题：给定一个定义项映射的系统，目标是最大化其像的大小。通过关联有向图的猜测数来确定指数D，并给出多项式时间算法计算D。

Result: 最大分散度为Θ(n^D)，其中D是关联有向图的猜测数。给出了计算D的多项式时间算法。然而，当r≥3时，判断是否存在完美分散（即Disp_n(t)=n^r）是不可判定的，尽管相应的渐近率阈值问题是多项式时间可判定的。

Conclusion: 项编码为离散极值问题提供了新框架，分散问题与有向图的猜测数密切相关。虽然计算最大分散度的指数是高效的，但完美分散的存在性判定在r≥3时是不可判定的，这揭示了问题的计算复杂性层次。

Abstract: We introduce a new framework term coding for extremal problems in discrete mathematics and information flow, where one chooses interpretations of function symbols so as to maximise the number of satisfying assignments of a finite system of term equations.
  We then focus on dispersion, the special case in which the system defines a term map $Θ^\mathcal I:\A^k\to\A^r$ and the objective is the size of its image. Writing $n:=|\A|$, we show that the maximum dispersion is $Θ(n^D)$ for an integer exponent $D$ equal to the guessing number of an associated directed graph, and we give a polynomial-time algorithm to compute $D$. In contrast, deciding whether \emph{perfect dispersion} ever occurs (i.e.\ whether $\Disp_n(\mathbf t)=n^r$ for some finite $n\ge 2$) is undecidable once $r\ge 3$, even though the corresponding asymptotic rate-threshold questions are polynomial-time decidable.

</details>


### [145] [Optimal Transmit Beamforming for MIMO ISAC with Unknown Target and User Locations](https://arxiv.org/abs/2602.08255)
*Yizhuo Wang,Shuowen Zhang*

Main category: cs.IT

TL;DR: 本文研究MIMO ISAC系统中目标和用户位置均未知且随机时的波束成形设计，基于概率分布信息建立性能指标，推导最优解并证明静态波束成形策略已足够优化。


<details>
  <summary>Details</summary>
Motivation: 在MIMO ISAC系统中，当感知目标和通信用户的位置都未知且随机时，如何利用空间资源设计发射波束成形，使感知和通信都能在统计意义上达到满意性能，是一个具有挑战性的问题。此外，研究目标和用户位置分布相似性对ISAC性能的影响也具有重要意义。

Method: 基于概率分布信息，通过推导期望速率或后验克拉美罗界(PCRB)建立通信和感知性能指标。然后，在期望速率约束下最小化PCRB的波束成形优化问题，推导最优解。分析最优发射协方差矩阵的秩上界，并研究动态波束成形策略的必要性。

Result: 推导出最优波束成形解，证明最优发射协方差矩阵的秩上界为所有可能用户位置的MIMO通信信道矩阵之和。更重要的是，证明使用静态波束成形策略足以达到最优性能，无需动态调整。数值结果表明，当目标/用户位置分布相似时，ISAC性能会提高。

Conclusion: 在目标和用户位置均随机未知的MIMO ISAC系统中，基于概率分布信息可以设计有效的波束成形策略。静态波束成形已足够优化，无需动态调整。目标和用户位置分布相似性对ISAC性能有积极影响，这为基站-用户/目标关联策略提供了有用见解。

Abstract: This paper studies a challenging scenario in a multiple-input multiple-output (MIMO) integrated sensing and communication (ISAC) system where the locations of the sensing target and the communication user are both unknown and random, while only their probability distribution information is known. In this case, how to fully utilize the spatial resources by designing the transmit beamforming such that both sensing and communication can achieve satisfactory performance statistically is a difficult problem, which motivates the study in this paper. Moreover, we aim to reveal if it is desirable to have similar probability distributions for the target and user locations in terms of the ISAC performance. Firstly, based on only probability distribution information, we establish communication and sensing performance metrics via deriving the expected rate or posterior Cramér-Rao bound (PCRB). Then, we formulate the transmit beamforming optimization problem to minimize the PCRB subject to the expected rate constraint, for which the optimal solution is derived. It is unveiled that the rank of the optimal transmit covariance matrix is upper bounded by the summation of MIMO communication channel matrices for all possible user locations. Furthermore, due to the need to cater to multiple target/user locations, we investigate whether dynamically employing different beamforming designs over different time slots improves the performance. It is proven that using a static beamforming strategy is sufficient for achieving the optimal performance. Numerical results validate our analysis, show that ISAC performance improves as the target/user location distributions become similar, and provide useful insights on the BS-user/-target association strategy.

</details>


### [146] [Hierarchical Subcode Ensemble Decoding of Polar Codes](https://arxiv.org/abs/2602.08391)
*Yubeen Jo,Geon Choi,Chanho Park,Namyoon Lee*

Main category: cs.IT

TL;DR: 提出分层子码集成解码(HSCED)框架，通过分层递归生成子码奇偶校验约束，在保证线性覆盖的同时扩展集成规模，显著提升极码的BP解码性能。


<details>
  <summary>Details</summary>
Motivation: 传统子码集成解码虽然通过并行多个解码器提高了避免陷阱结构的概率，但缺乏系统方法来扩展集成规模同时保持线性覆盖特性，限制了性能提升潜力。

Method: 提出分层子码集成解码(HSCED)框架，采用分层递归方式生成子码奇偶校验约束，在每一层都保持覆盖特性，从而在控制复杂度的情况下实现大规模集成。

Result: 将HSCED应用于极码的BP解码，在相同解码延迟约束下，相比标准BP和传统子码集成解码，显著降低了块错误率。

Conclusion: HSCED提供了一种系统化的方法来扩展子码集成规模，在保持线性覆盖特性的同时实现性能显著提升，为迭代解码提供了有效的增强框架。

Abstract: Subcode-ensemble decoders improve iterative decoding by running multiple decoders in parallel over carefully chosen subcodes, increasing the likelihood that at least one decoder avoids the dominant trapping structures. Achieving strong diversity gains, however, requires constructing many subcodes that satisfy a linear covering property-yet existing approaches lack a systematic way to scale the ensemble size while preserving this property. This paper introduces hierarchical subcode ensemble decoding (HSCED), a new ensemble decoding framework that expands the number of constituent decoders while still guaranteeing linear covering. The key idea is to recursively generate subcode parity constraints in a hierarchical structure so that coverage is maintained at every level, enabling large ensembles with controlled complexity. To demonstrate its effectiveness, we apply HSCED to belief propagation (BP) decoding of polar codes, where dense parity-check matrices induce severe stopping-set effects that limit conventional BP. Simulations confirm that HSCED delivers significant block-error-rate improvements over standard BP and conventional subcode-ensemble decoding under the same decoding-latency constraint.

</details>


### [147] [Multipoint Code-Weight Sphere Decoding: Parallel Near-ML Decoding for Short-Blocklength Codes](https://arxiv.org/abs/2602.08501)
*Yubeen Jo,Geon Choi,Yongjune Kim,Namyoon Lee*

Main category: cs.IT

TL;DR: 提出一种两阶段近最大似然解码框架，适用于任何线性分组码，通过低复杂度解码器作为第一级，失败时激活基于多码重球面解码的第二级，实现低延迟近ML性能。


<details>
  <summary>Details</summary>
Motivation: URLLC使用短包传输，有限块长效应使得近ML解码很重要，但传统ML解码计算成本过高。需要一种既能保持近ML性能又具有低复杂度的解码方案。

Method: 两阶段解码框架：第一阶段使用低复杂度解码器生成候选码字和CRC校验；失败时激活第二阶段MP-WSD解码器，通过预计算低权重码字生成结构化局部扰动，在欧几里得球面内迭代搜索候选码字。

Result: 仿真结果表明，该解码器在短块长、低码率编码下实现了近ML性能，同时保持了低解码延迟。在高信噪比下，第一阶段成功率高，第二级很少激活。

Conclusion: 提出的两阶段解码框架为URLLC应用提供了一种有效的解决方案，在保持近ML性能的同时显著降低了计算复杂度和解码延迟，适用于短块长、低码率编码场景。

Abstract: Ultra-reliable low-latency communications (URLLC) operate with short packets, where finite-blocklength effects make near-maximum-likelihood (near-ML) decoding desirable but often too costly. This paper proposes a two-stage near-ML decoding framework that applies to any linear block code. In the first stage, we run a low-complexity decoder to produce a candidate codeword and a cyclic redundancy check. When this stage succeeds, we terminate immediately. When it fails, we invoke a second-stage decoder, termed multipoint code-weight sphere decoding (MP-WSD). The central idea behind {MP-WSD} is to concentrate the ML search where it matters. We pre-compute a set of low-weight codewords and use them to generate structured local perturbations of the current estimate. Starting from the first-stage output, MP-WSD iteratively explores a small Euclidean sphere of candidate codewords formed by adding selected low-weight codewords, tightening the search region as better candidates are found. This design keeps the average complexity low: at high signal-to-noise ratio, the first stage succeeds with high probability and the second stage is rarely activated; when it is activated, the search remains localized. Simulation results show that the proposed decoder attains near-ML performance for short-blocklength, low-rate codes while maintaining low decoding latency.

</details>


### [148] [Reliable one-bit quantization of bandlimited graph data via single-shot noise shaping](https://arxiv.org/abs/2602.08669)
*Johannes Maly,Anna Veselovska*

Main category: cs.IT

TL;DR: 提出一种高效的图数据量化方法，能在低比特率（甚至1比特/系数）下保持低通滤波后的信息完整性


<details>
  <summary>Details</summary>
Motivation: 图数据在自然科学和机器学习中无处不在，需要一种方法能够在低比特率下量化带限图数据，同时保持低通滤波后的信息完整性

Method: 提出一种高效的单次噪声整形方法，能够实现任意比特级别的可靠量化，包括极端情况下的每系数1比特

Result: 该方法实现了最先进的性能，并提供了严格的误差界限

Conclusion: 相比现有方法，该方法能够在任意比特级别（包括极端低比特情况）下实现可靠的图数据量化

Abstract: Graph data are ubiquitous in natural sciences and machine learning. In this paper, we consider the problem of quantizing graph structured, bandlimited data to few bits per entry while preserving its information under low-pass filtering. We propose an efficient single-shot noise shaping method that achieves state-of-the-art performance and comes with rigorous error bounds. In contrast to existing methods it allows reliable quantization to arbitrary bit-levels including the extreme case of using a single bit per data coefficient.

</details>


### [149] [Trellis codes with a good distance profile constructed from expander graphs](https://arxiv.org/abs/2602.08718)
*Yubin Zhu,Zitan Chen*

Main category: cs.IT

TL;DR: 本文推导了网格码的自由距离和列距离的Singleton型界，证明网格码在某些时刻的列距离可超过卷积码，并利用扩展图构造了接近卷积码最优距离特性的网格码。


<details>
  <summary>Details</summary>
Motivation: 研究网格码的距离特性，探索其相对于卷积码的潜在优势，特别是寻求在较小字母表上实现接近卷积码最优距离特性的构造方法。

Method: 推导网格码的自由距离和列距离的Singleton型界，利用扩展图构造常数大小字母表上的网格码，分析其距离特性。

Result: 网格码在特定时刻的列距离可超过卷积码；使用扩展图构造的网格码能在常数大小字母表上实现接近卷积码最大距离特性的速率-距离权衡。

Conclusion: 网格码在距离特性方面具有优势，特别是通过扩展图构造可在小字母表上实现接近卷积码最优距离特性的性能，这为实际编码应用提供了更实用的方案。

Abstract: We derive Singleton-type bounds on the free distance and column distances of trellis codes. Our results show that, at a given time instant, the maximum attainable column distance of trellis codes can exceed that of convolutional codes. Moreover, using expander graphs, we construct trellis codes over constant-size alphabets that achieve a rate-distance trade-off arbitrarily close to that of convolutional codes with a maximum distance profile. By comparison, all known constructions of convolutional codes with a maximum distance profile require working over alphabets whose size grows at least exponentially with the number of output symbols per time instant.

</details>


### [150] [Clique-Based Deletion-Correcting Codes via Penalty-Guided Clique Search](https://arxiv.org/abs/2602.08952)
*Aniruddh Pandav,Rajshekhar V Bhat*

Main category: cs.IT

TL;DR: 本文通过最大团问题(MCP)构建d-删除纠错二进制码，使用惩罚引导团搜索(PGCS)启发式算法获得比现有方法更大的码本，并针对分段接收提出优化的LCS解码器降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究d-删除纠错二进制码的构造问题，现有图基启发式方法（如最小度和着色方法）在寻找最大码本方面效果有限，需要更有效的算法来获得更大的码本。

Method: 将码构造问题建模为最大团问题(MCP)：顶点代表候选码字，边连接那些最长公共子序列(LCS)距离能保证纠正最多d个删除的码字对。采用惩罚引导团搜索(PGCS)启发式算法寻找最大团，这是一种受动态局部搜索(DLS)启发的轻量级随机团搜索方法。对于分段接收解码，提出优化的LCS解码器，利用符号计数过滤和提前终止技术减少LCS评估次数。

Result: PGCS算法在块长度n=8-14和删除参数d=1-3时，比现有图基启发式方法（最小度和着色方法）获得更大的码本。在某些有限长度情况下，码本大小达到已知最优值，并优于经典Helberg码构造。优化的LCS解码器显著降低了平均情况解码复杂度，相比基线O(|C| n²)方法有大幅改进。

Conclusion: PGCS启发式算法是构建d-删除纠错二进制码的有效方法，能获得比现有图基方法更大的码本。优化的LCS解码器在保持精确解码保证的同时，显著降低了分段接收场景下的解码复杂度。该方法在多个有限长度情况下优于经典构造。

Abstract: We study the construction of $d$-deletion-correcting binary codes by formulating the problem as a Maximum Clique Problem (MCP). In this formulation, vertices represent candidate codewords and edges connect pairs whose longest common subsequence (LCS) distance guarantees correction of up to $d$ deletions. A valid codebook corresponds to a clique in the resulting graph, and finding the largest codebook is equivalent to identifying a maximum clique. While MCP-based formulations for deletion-correcting codes have previously been explored, we demonstrate that applying Penalty-Guided Clique Search (PGCS), a lightweight stochastic clique-search heuristic inspired by Dynamic Local Search (DLS), consistently yields larger codebooks than existing graph-based heuristics, including minimum-degree and coloring methods, for block lengths $n = 8,9,\dots,14$ and deletion parameters $d = 1,2,3$. In several finite-length regimes, the resulting codebooks match known optimal sizes and outperform classical constructions such as Helberg codes. For decoding under segmented reception, where codeword boundaries are known, we propose an optimized LCS-based decoder that exploits symbol-count filtering and early termination to substantially reduce the number of LCS evaluations while preserving exact decoding guarantees. These optimizations lead to significantly lower average-case decoding complexity than the baseline $O(|C| n^2)$ approach.

</details>
