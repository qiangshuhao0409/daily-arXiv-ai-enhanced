<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 38]
- [cs.IT](#cs.IT) [Total: 11]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks](https://arxiv.org/abs/2510.19973)
*Hatim Chergui,Farhad Rezazadeh,Merouane Debbah,Christos Verikoukis*

Main category: cs.NI

TL;DR: 本文探讨了在6G网络中实现真正自主性的路径，指出仅优化KPI不足以实现高级别自主。提出使用基于大语言模型的代理AI来感知多模态遥测数据、进行推理和跨域协商，但需要解决人类认知偏差对代理决策的影响。


<details>
  <summary>Details</summary>
Motivation: 当前基于KPI的网络自动化存在局限性，因为KPI只是网络本质的数值抽象。要实现真正的网络自主性，需要代理AI能够像人类一样感知和理解网络环境，但必须克服人类认知偏差在AI系统中的继承问题。

Method: 提出了基于大语言模型的代理AI框架，代理能够感知多模态遥测数据、进行推理记忆、跨域协商并通过API执行动作。针对认知偏差问题，研究了偏差的分类、定义、数学公式化，以及在电信系统中的表现。开发了锚点随机化、时间衰减和转折点奖励等技术来缓解特定偏差。

Result: 通过缓解认知偏差，在6G切片间和跨域管理的实际用例中，代理决策的质量和勇气得到显著提升。具体表现为延迟降低5倍，节能效果提高约40%。

Conclusion: 实现6G网络真正自主性的关键在于超越KPI优化，采用代理AI方法，同时必须系统性地识别和缓解人类认知偏差对AI决策的影响。提出的偏差缓解技术能够显著提升网络管理性能。

Abstract: The path to higher network autonomy in 6G lies beyond the mere optimization
of key performance indicators (KPIs). While KPIs have enabled automation gains
under TM Forum Levels 1--3, they remain numerical abstractions that act only as
proxies for the real essence of communication networks: seamless connectivity,
fairness, adaptability, and resilience. True autonomy requires perceiving and
reasoning over the network environment as it is. Such progress can be achieved
through \emph{agentic AI}, where large language model (LLM)-powered agents
perceive multimodal telemetry, reason with memory, negotiate across domains,
and act via APIs to achieve multi-objective goals. However, deploying such
agents introduces the challenge of cognitive biases inherited from human
design, which can distort reasoning, negotiation, tool use, and actuation.
Between neuroscience and AI, this paper provides a tutorial on a selection of
well-known biases, including their taxonomy, definition, mathematical
formulation, emergence in telecom systems and the commonly impacted agentic
components. The tutorial also presents various mitigation strategies tailored
to each type of bias. The article finally provides two practical use-cases,
which tackle the emergence, impact and mitigation gain of some famous biases in
6G inter-slice and cross-domain management. In particular, anchor
randomization, temporal decay and inflection bonus techniques are introduced to
specifically address anchoring, temporal and confirmation biases. This avoids
that agents stick to the initial high resource allocation proposal or decisions
that are recent and/or confirming a prior hypothesis. By grounding decisions in
a richer and fairer set of past experiences, the quality and bravery of the
agentic agreements in the second use-case, for instance, are leading to $\times
5$ lower latency and around $40\%$ higher energy saving.

</details>


### [2] [Rediscovering Recurring Routing Results](https://arxiv.org/abs/2510.20297)
*Xiao Song,John Heidemann*

Main category: cs.NI

TL;DR: Fenrir是一个新的系统，用于重新发现重复出现的路由结果，能够检测网络路由变化并量化变化程度，识别可能重新出现的路由"模式"。


<details>
  <summary>Details</summary>
Motivation: 路由对网络性能至关重要，但理解和控制路由如何影响服务具有挑战性。运营商使用BGP进行流量工程优化网络性能，但得到的结果是整个互联网所有BGP策略的综合结果，而不仅仅是本地选择。

Method: Fenrir系统通过主动测量、数据清理和加权来检测和量化路由变化，即使变化发生在观察者多跳之外。系统能够识别可能重新出现的路由模式。

Result: Fenrir可应用于多种场景：根DNS服务的任播捕获、多宿主企业的路由优化、以及顶级Web服务的网站选择。系统能够有效检测和量化变化，帮助回答运营问题。

Conclusion: Fenrir提供了一种检测和量化路由变化的新方法，能够识别路由模式，帮助运营商理解流量工程的效果和第三方路由变化的影响。

Abstract: Routing is central to networking performance, including: (1) latency in
anycast services and websites served from multiple locations,(2) networking
expenses and throughput in multi-homed enterprises, (3) the ability to keep
traffic domestic when considering data sovereignty. However, understanding and
managing how routing affects these services is challenging. Operators use
Traffic Engineering (TE) with BGP to optimize network performance, but what
they get is the result of all BGP policies throughout the Internet, not just
their local choices. Our paper proposes Fenrir, a new system to rediscover
recurring routing results. Fenrir can discover changes in network routing, even
when it happens multiple hops away from the observer. Fenrir also provides new
methods to quantify the degree of routing change, and to identify routing
"modes" that may reappear. Second, we show that Fenrir can be applied to many
different problems: we use five instances of three different types of systems
to illustrate the generalization: anycast catchments showing in a root DNS
service, route optimization for two multi-homed enterprises, and website
selection for two of the top-10 web services. Each type requires different
types of active measurements, data cleaning and weighting. We demonstrate
Fenrir's methods of detecting and quantifying change are helpful because they
all face similar operational questions: How much effect did traffic engineering
have? Did a third-party change alter my routing? In either case, is the current
routing new, or is it like a routing mode I saw before?

</details>


### [3] [Multicast-partitioning in Time-triggered Stream Planning for Time-Sensitive Networks](https://arxiv.org/abs/2510.20440)
*Heiko Geppert,Frank Dürr,Simon Naß,Kurt Rothermel*

Main category: cs.NI

TL;DR: 提出了一种新颖的多播分区技术，将多播树分割成更小的多播或单播树，在带宽利用率和流量调度难度之间实现更精细的权衡，从而提高动态系统的可调度性。


<details>
  <summary>Details</summary>
Motivation: 多播通信虽然能节省网络带宽，但会复杂化流量规划，因为需要在多播树的所有分支上都有空闲队列或可用的下游出口端口。

Method: 采用多播分区技术，将大型多播树分割成较小的多播或单播树，实现带宽利用与调度难度之间的精细权衡。

Result: 在不同网络拓扑和三种调度算法下评估，使用分区技术后，被拒绝的流减少了5-15%，网络吞吐量提高了5-125%。

Conclusion: 多播分区技术能够有效改善动态系统的可调度性，减少流拒绝率并显著提高网络吞吐量。

Abstract: Multicast allows sending a message to multiple recipients without having to
create and send a separate message for each recipient. This preserves network
bandwidth, which is particularly important in time-sensitive networks. These
networks are commonly used to provide latency-bounded communication for
real-time systems in domains like automotive, avionics, industrial internet of
things, automated shop floors, and smart energy grids. The preserved bandwidth
can be used to admit additional real-time messages with specific quality of
service requirements or to reduce the end-to-end latencies for messages of any
type. However, using multicast communication can complicate traffic planning,
as it requires free queues or available downstream egress ports on all branches
of the multicast tree. In this work, we present a novel multicast partitioning
technique to split multicast trees into smaller multicast or unicast trees.
This allows for a more fine-grained trade-off between bandwidth utilization and
traffic scheduling difficulty. Thus, schedulability in dynamic systems can be
improved, in terms the number of admitted streams and the accumulated network
throughput. We evaluated the multicast partitioning on different network
topologies and with three different scheduling algorithms. With the
partitioning, 5-15\% fewer streams were rejected, while achieving 5-125\% more
network throughput, depending on the scheduling algorithm.

</details>


### [4] [Trust, But Verify: An Empirical Evaluation of AI-Generated Code for SDN Controllers](https://arxiv.org/abs/2510.20703)
*Felipe Avencourt Soares,Muriel F. Franco,Eder J. Scheid,Lisandro Z. Granville*

Main category: cs.NI

TL;DR: 本文对ChatGPT、Copilot、DeepSeek和BlackBox.ai等AI工具生成的POX控制器源代码进行了实证评估，测试了三个复杂度递增的网络任务，发现所有模型都能生成功能控制器，但ChatGPT和DeepSeek表现更稳定。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在多领域生成类人内容，但其在可编程网络等新环境中的正确性和功能性可靠性尚不明确。

Method: 定义了三个复杂度递增的网络任务，使用零样本和少样本提示技术输入到AI工具中，在Mininet模拟的网络拓扑中测试输出代码，分析功能性、正确性和手动修复需求。

Result: 所有评估模型都能生成功能控制器，但ChatGPT和DeepSeek表现出更高的一致性和代码质量，而Copilot和BlackBox.ai需要更多调整。

Conclusion: 生成式AI工具能够生成可工作的网络控制器代码，但不同工具在代码质量和一致性方面存在差异，ChatGPT和DeepSeek表现更优。

Abstract: Generative Artificial Intelligence (AI) tools have been used to generate
human-like content across multiple domains (e.g., sound, image, text, and
programming). However, their reliability in terms of correctness and
functionality in novel contexts such as programmable networks remains unclear.
Hence, this paper presents an empirical evaluation of the source code of a POX
controller generated by different AI tools, namely ChatGPT, Copilot, DeepSeek,
and BlackBox.ai. To evaluate such a code, three networking tasks of increasing
complexity were defined and for each task, zero-shot and few-shot prompting
techniques were input to the tools. Next, the output code was tested in
emulated network topologies with Mininet and analyzed according to
functionality, correctness, and the need for manual fixes. Results show that
all evaluated models can produce functional controllers. However, ChatGPT and
DeepSeek exhibited higher consistency and code quality, while Copilot and
BlackBox.ai required more adjustments.

</details>


### [5] [AI-Enabled Digital Twins for Next-Generation Networks: Forecasting Traffic and Resource Management in 5G/6G](https://arxiv.org/abs/2510.20796)
*John Sengendo,Fabrizio Granelli*

Main category: cs.NI

TL;DR: 本文提出了一种将LSTM神经网络集成到数字孪生网络框架中的AI驱动方法，用于预测网络流量模式并主动管理资源分配，在5G/6G移动网络中实现自主、自适应的高性能网络管理。


<details>
  <summary>Details</summary>
Motivation: 随着5G/6G移动网络日益复杂，传统启发式资源管理技术无法满足实时服务供应的敏捷性、可扩展性、弹性和精度要求。数字孪生被运营商视为关键使能技术，但需要驱动引擎来应对资源管理挑战。

Method: 将长短期记忆(LSTM)神经网络集成到数字孪生框架中，通过AI能力预测网络流量模式，实现主动资源分配管理。

Result: 通过分析实验，AI驱动的数字孪生框架相比基准方法表现出优越性能。

Conclusion: 在数字孪生中嵌入AI能力为未来移动网络实现完全自主、自适应和高性能的网络管理铺平了道路。

Abstract: As 5G and future 6G mobile networks become increasingly more sophisticated,
the requirements for agility, scalability, resilience, and precision in
real-time service provisioning cannot be met using traditional and
heuristic-based resource management techniques, just like any advancing
technology. With the aim of overcoming such limitations, network operators are
foreseeing Digital Twins (DTs) as key enablers, which are designed as dynamic
and virtual replicas of network infrastructure, allowing operators to model,
analyze, and optimize various operations without any risk of affecting the live
network. However, for Digital Twin Networks (DTNs) to meet the challenges faced
by operators especially in line with resource management, a driving engine is
needed. In this paper, an AI (Artificial Intelligence)-driven approach is
presented by integrating a Long Short-Term Memory (LSTM) neural network into
the DT framework, aimed at forecasting network traffic patterns and proactively
managing resource allocation. Through analytical experiments, the AI-Enabled DT
framework demonstrates superior performance benchmarked against baseline
methods. Our study concludes that embedding AI capabilities within DTs paves
the way for fully autonomous, adaptive, and high-performance network management
in future mobile networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 该研究提出了分析可靠性基准(ARB)，用于量化大语言模型在能源系统分析中的推理可靠性，测试了四种前沿模型在确定性、概率性和认知性场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI在能源领域的验证实践主要关注预测准确性或计算效率，而缺乏对分析结论逻辑完整性的标准化评估框架。

Method: 开发了包含准确性、推理可靠性、不确定性纪律、政策一致性和透明度五个子指标的ARB框架，使用公开技术经济数据集在相同事实和监管条件下测试四种前沿模型。

Result: GPT-4/5和Claude 4.5 Sonnet达到了一致且符合政策的推理(分析可靠性指数大于90)，Gemini 2.5 Pro表现中等，Llama 3 70B低于专业阈值。统计验证证实这些差异显著且可重现。

Conclusion: ARB建立了能源文献中首个验证AI系统中因果、概率和政策驱动推理的定量方法，为全球能源转型中可信赖和透明的分析应用提供了参考框架。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [7] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: 提出了一种量子启发的算法，使用矩阵乘积态（MPS）和密度矩阵重整化群（DMRG）方法解决QUBO问题，在Sudoku和MaxCut问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决二次无约束二进制优化（QUBO）问题，这些问题在数学上等价于寻找伊辛自旋玻璃哈密顿量的基态，需要高效可靠的全局优化方法。

Method: 使用矩阵乘积态（MPS）紧凑表示自旋构型的大叠加，结合离散驱动调度和驱动哈密顿量（包含横向磁场），通过DMRG方法迭代最小化系统能量。

Result: 算法在超过200个伊辛自旋的Sudoku谜题和Biq Mac库中最多251个节点、3265条边的MaxCut问题上可靠地找到了全局最小值，而不仅仅是近似最优解。

Conclusion: 该量子启发方法具有可扩展性、通用性和适用于工业规模QUBO应用的优势。

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [8] [Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory](https://arxiv.org/abs/2510.19838)
*Shiqi He,Yue Cui,Xinyu Ma,Yaliang Li,Bolin Ding,Mosharaf Chowdhury*

Main category: cs.AI

TL;DR: Branch-and-Browse是一个细粒度的Web代理框架，通过树状结构探索、网页状态重放和页面动作记忆来提高基于LLM的Web代理的推理深度和执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自主Web代理方法在推理深度和效率上存在局限：线性方法无法处理多步推理且缺乏有效回溯，而其他搜索策略则粒度粗糙且计算成本高。

Method: 采用显式子任务管理和树状结构探索实现可控多分支推理；通过网页状态重放和后台推理引导探索；利用页面动作记忆在会话内外共享已探索的动作。

Result: 在WebArena基准测试中，Branch-and-Browse实现了35.8%的任务成功率，相比最先进方法执行时间减少了40.4%。

Conclusion: Branch-and-Browse是一个可靠且高效的基于LLM的Web代理框架，在任务成功率和执行效率方面都有显著提升。

Abstract: Autonomous web agents powered by large language models (LLMs) show strong
potential for performing goal-oriented tasks such as information retrieval,
report generation, and online transactions. These agents mark a key step toward
practical embodied reasoning in open web environments. However, existing
approaches remain limited in reasoning depth and efficiency: vanilla linear
methods fail at multi-step reasoning and lack effective backtracking, while
other search strategies are coarse-grained and computationally costly. We
introduce Branch-and-Browse, a fine-grained web agent framework that unifies
structured reasoning-acting, contextual memory, and efficient execution. It (i)
employs explicit subtask management with tree-structured exploration for
controllable multi-branch reasoning, (ii) bootstraps exploration through
efficient web state replay with background reasoning, and (iii) leverages a
page action memory to share explored actions within and across sessions. On the
WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\%
and reduces execution time by up to 40.4\% relative to state-of-the-art
methods. These results demonstrate that Branch-and-Browse is a reliable and
efficient framework for LLM-based web agents.

</details>


### [9] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: 提出了基于有向无环图（DAG）的CoT建模框架，引入逻辑接近度指标来评估LLM的推理一致性，发现即使PASS@k指标相似，不同LLM家族在推理保真度上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究不清楚LLM在数学问题上的成功是源于搜索、机械程序还是规则一致推理，需要超越传统PASS@k指标的新评估方法。

Method: 将CoT建模为基于规则的随机过程，构建DAG-MATH CoT格式和基准测试，通过逻辑接近度指标量化模型推理轨迹与DAG结构的一致性。

Result: 在标准数学推理数据集上发现代表性LLM家族在推理保真度上存在统计显著差异，揭示了最终答案准确性与规则一致性推导之间的差距。

Conclusion: 该框架在自由形式CoT和形式证明系统之间提供了平衡，为LLM推理评估提供了可操作的诊断工具。

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [10] [Surfer 2: The Next Generation of Cross-Platform Computer Use Agents](https://arxiv.org/abs/2510.19949)
*Mathieu Andreux,Märt Bakler,Yanael Barbier,Hamza Ben Chekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Nathan Bout,Matthias Brunel,Aleix Cambray,Pierre-Louis Cedoz,Antoine Chassang,Gautier Cloix,Ethan Connelly,Alexandra Constantinou,Ramzi De Coster,Hubert de la Jonquiere,Aurélien Delfosse,Maxime Delpit,Alexis Deprez,Augustin Derupti,Mathieu Diaz,Shannon D'Souza,Julie Dujardin,Abai Edmund,Michael Eickenberg,Armand Fatalot,Wissem Felissi,Isaac Herring,Xavier Koegler,Erwan Le Jumeau de Kergaradec,Aurélien Lac,Maxime Langevin,Corentin Lauverjat,Antonio Loison,Avshalom Manevich,Axel Moyal,Axel Nguyen Kerbel,Marinela Parovic,Julien Revelle,Guillaume Richard,Mats Richter,Ronan Riochet,María Santos,Romain Savidan,Laurent Sifre,Maxime Theillard,Marc Thibault,Ivan Valentini,Tony Wu,Laura Yie,Kai Yuan,Jevgenij Zubovskij*

Main category: cs.AI

TL;DR: Surfer 2是一个纯视觉观察的统一架构，在Web、桌面和移动环境中实现最先进性能，通过层次化上下文管理、解耦规划执行和自适应恢复验证实现跨平台通用计算机控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有系统依赖环境特定接口限制跨平台部署的问题，构建能够在Web、桌面和移动环境中通用的智能体。

Method: 集成层次化上下文管理、解耦的规划与执行、以及带自适应恢复的自我验证，实现长任务周期的可靠操作。

Result: 在WebVoyager上达到97.1%准确率，WebArena 69.6%，OSWorld 60.1%，AndroidWorld 87.1%，无需任务特定微调即超越所有先前系统，多次尝试后在所有基准测试中超越人类表现。

Conclusion: 系统化编排能够放大基础模型能力，仅通过视觉交互实现通用计算机控制，同时需要下一代视觉语言模型来实现帕累托最优的成本效益。

Abstract: Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.

</details>


### [11] [RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs](https://arxiv.org/abs/2510.19954)
*Joseph Meyer,Divyansha Lachi,Reza Mohammadi,Roshan Reddy Upendra,Eva L. Dyer,Mark Li,Tom Palczewski*

Main category: cs.AI

TL;DR: RELATE是一个模式无关的图神经网络特征编码器，使用共享的模态特定编码器和交叉注意力模块，在参数减少5倍的情况下性能接近模式特定编码器。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络需要为每种节点类型和特征列设计单独的特征编码模块，这阻碍了可扩展性和参数共享。

Method: 使用共享的模态特定编码器处理分类、数值、文本和时间属性，然后通过Perceiver风格的交叉注意力模块将特征聚合成固定大小的节点表示。

Result: 在RelBench基准测试中，RELATE在参数减少5倍的情况下，性能达到模式特定编码器的97%以内。

Conclusion: RELATE支持不同模式，使通用图神经网络能够进行多数据集预训练，为关系图数据的基础模型铺平道路。

Abstract: Relational multi-table data is common in domains such as e-commerce,
healthcare, and scientific research, and can be naturally represented as
heterogeneous temporal graphs with multi-modal node attributes. Existing graph
neural networks (GNNs) rely on schema-specific feature encoders, requiring
separate modules for each node type and feature column, which hinders
scalability and parameter sharing. We introduce RELATE (Relational Encoder for
Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature
encoder that can be used with any general purpose GNN. RELATE employs shared
modality-specific encoders for categorical, numerical, textual, and temporal
attributes, followed by a Perceiver-style cross-attention module that
aggregates features into a fixed-size, permutation-invariant node
representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,
where it achieves performance within 3% of schema-specific encoders while
reducing parameter counts by up to 5x. This design supports varying schemas and
enables multi-dataset pretraining for general-purpose GNNs, paving the way
toward foundation models for relational graph data.

</details>


### [12] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: 生成式AI正在加剧保险欺诈，使大规模快速伪造事故证据变得更容易。保险公司开始部署AI深度伪造检测软件等对策，但现有策略存在局限性。本文提出了UVeye分层解决方案来检测、缓解和威慑这种新型欺诈。


<details>
  <summary>Details</summary>
Motivation: 保险欺诈是一个普遍且代价高昂的问题，每年造成数百亿美元损失。生成式AI（包括深度伪造图像和视频生成）引入了大规模欺诈的新方法，欺诈者可以轻松伪造高度逼真的碰撞照片、损坏证据甚至虚假身份文件。

Method: 保险公司部署了基于AI的深度伪造检测软件和增强验证流程等对策。本文提出了UVeye分层解决方案，代表在检测、缓解和威慑这种新型欺诈能力方面的重大进步。

Result: 当前缓解策略面临显著限制：检测工具可能出现误报和漏报，复杂的欺诈者不断调整策略以规避自动检查。生成式AI与检测技术之间的猫鼠游戏，加上保险公司的资源和成本障碍，使得对抗AI驱动的保险欺诈仍是一个持续挑战。

Conclusion: UVeye分层解决方案代表了在应对AI驱动的车辆保险欺诈方面的重大进步，能够更好地检测、缓解和威慑这种新型欺诈浪潮。

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [13] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 该研究使用机器学习模型通过领导力人格特质预测学业成功，在129名环境工程硕士生中实现了87.5%的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 探索AI技术在个性化学习中的潜力，通过领导力人格特质预测学业表现，为早期识别学生优劣势提供机会。

Method: 收集129名硕士生的5项领导力人格测试数据（23个特征），结合平均成绩，使用7种机器学习算法进行建模，通过特征选择和模型调优。

Result: 随机森林分类器表现最佳，包含17个人格特征和领导力标记的模型准确率达87.50%，不包含该特征的模型准确率为85.71%。

Conclusion: 该研究为在教育过程早期识别学生优劣势、选择最适合的个性化学习策略提供了额外机会。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [14] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [15] [AI PB: A Grounded Generative Agent for Personalized Investment Insights](https://arxiv.org/abs/2510.20099)
*Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh*

Main category: cs.AI

TL;DR: AI PB是一个部署在零售金融领域的生产级生成式智能体，能够主动生成基于事实、合规且个性化的投资洞察，而非被动回答查询。


<details>
  <summary>Details</summary>
Motivation: 在金融领域，传统聊天机器人只能被动回答问题，无法提供主动、个性化的投资建议。需要开发能够在高风险的金融环境中提供可信AI洞察的系统。

Method: 采用组件化编排层进行确定性路由、混合检索管道（OpenSearch+金融领域嵌入模型）、多阶段推荐机制（规则启发式+序列行为建模+上下文赌博机），并在韩国金融监管下完全本地化部署。

Result: 通过人工QA和系统指标验证，证明了基于明确路由和分层安全机制的接地生成能够在高风险金融环境中提供可信的AI洞察。

Conclusion: 在金融等高风险领域，通过明确的系统路由和分层安全机制，可以实现可信赖的生成式AI洞察服务。

Abstract: We present AI PB, a production-scale generative agent deployed in real retail
finance. Unlike reactive chatbots that answer queries passively, AI PB
proactively generates grounded, compliant, and user-specific investment
insights. It integrates (i) a component-based orchestration layer that
deterministically routes between internal and external LLMs based on data
sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the
finance-domain embedding model, and (iii) a multi-stage recommendation
mechanism combining rule heuristics, sequential behavioral modeling, and
contextual bandits. Operating fully on-premises under Korean financial
regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100
GPUs. Through human QA and system metrics, we demonstrate that grounded
generation with explicit routing and layered safety can deliver trustworthy AI
insights in high-stakes finance.

</details>


### [16] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: 论文提出AGI发展需要经历从开放模仿到身份固化的锁定阶段，通过实验展示了不同规模模型在身份固化过程中的不同表现，认为身份固化是AGI可靠性的前提和关键安全控制点。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型过于开放和可操控，而真正的AGI发展需要经历身份固化阶段，使目标结构、拒绝行为、偏好和内部表征变得相对稳定和抗外部操控。

Method: 通过形式化身份固化阶段，将其与学习动态中的已知现象联系起来，提出操作性度量指标进行检测，并在不同规模模型上进行实验验证。

Result: 实验显示行为固化是快速非线性的，但对通用能力的影响不是单一的：小模型出现性能权衡，中等规模模型基本无成本，大型量化模型出现瞬态不稳定性。

Conclusion: 身份固化是AGI级可靠性的先决条件，也是关键的安全控制点，身份可以被工程化设计以增强可靠性，也可能在扩展过程中自发出现，从而固化不可预测的目标和行为。

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [17] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: HCLA是一个以人为中心的多智能体异常检测系统，用于数字资产交易。它将解析、检测和解释三个角色连接成一个对话式工作流，让非专家能够用自然语言提问、检查结构化分析并获得情境感知的解释。


<details>
  <summary>Details</summary>
Motivation: 提高金融取证中的透明度和信任度，让非专家用户能够理解和交互式地改进异常检测结果。

Method: 采用多智能体系统架构，包含解析、检测和解释三个角色，使用XGBoost作为基础检测器，通过对话式工作流将用户意图转换为检测模式并提供基于特征的叙述性解释。

Result: 在标记的比特币混币数据集上，基线检测器达到强准确率，HCLA增加了可解释性和交互式改进能力。

Conclusion: 人在回路设计能够显著提高金融取证系统的透明度和用户信任度。

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [18] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: 论文认为AI在法律实践中的使用需要新范式，因为AI与现实脱节且缺乏透明度，而律师有诚实、正直和不误导法庭的最高职责。提出了验证-价值悖论模型，指出AI提高效率的同时需要更多手动验证，导致净价值往往微不足道。


<details>
  <summary>Details</summary>
Motivation: 当前对AI在法律实践中能大幅降低成本的乐观假设需要重新审视，因为已有律师因提交不准确的AI生成内容而受到谴责。需要建立新范式来评估AI在法律实践中的使用。

Method: 提出验证-价值悖论模型，分析AI使用效率提升与手动验证需求之间的平衡关系，探讨其对法律实践和教育的影响。

Result: AI在法律实践中的效率提升会被相应的验证需求所抵消，导致净价值往往可以忽略不计。

Conclusion: 需要重新思考AI在法律实践中的价值，强调对真相的忠诚和公民责任等价值观应作为法律实践的基础，并对法律教育提出相应要求。

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [19] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出了TRUST框架，一个去中心化的AI审计系统，通过共识机制、层次化DAG分解、区块链记录和隐私保护分段来解决大语言模型推理链的验证问题。


<details>
  <summary>Details</summary>
Motivation: 现有审计方法存在集中化、不透明、难以扩展的问题，无法有效验证大语言模型推理链的忠实性和无害性，这在高风险领域部署专有模型时存在重大风险。

Method: 采用去中心化审计框架，包含：1) 多样化审计者共识机制；2) 层次化DAG推理链分解；3) 区块链账本记录；4) 隐私保护的分段共享机制。

Result: 实验表明TRUST能有效检测推理缺陷，在30%恶意参与者情况下仍保持稳健，在数学、医学、科学、人文学科等多个任务中表现良好。

Conclusion: TRUST框架为去中心化AI审计开辟了新途径，为实现安全可信的大语言模型部署提供了实用路径。

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [20] [Merge and Conquer: Evolutionarily Optimizing AI for 2048](https://arxiv.org/abs/2510.20205)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.AI

TL;DR: 本文研究进化训练方法优化AI玩2048游戏，比较了单智能体系统和双智能体系统，发现单智能体系统通过价值函数优化取得显著改进，而双智能体系统改进有限。


<details>
  <summary>Details</summary>
Motivation: 优化AI在动态环境中的性能是机器学习研究的基本挑战，2048游戏结合了策略游戏和随机元素，是研究决策制定、长期规划和动态适应的理想平台。

Method: 实现两种系统：双智能体元提示系统（思考者LLM优化执行者LLM的策略）和单智能体系统（基于有限蒙特卡洛树搜索的价值函数优化），并实验了回滚功能以避免性能退化。

Result: 单智能体系统取得显著改进，每个周期平均增加473.2分，训练周期呈现明显上升趋势（相关系数ρ=0.607），LLM对游戏的理解不断增强。双智能体系统改进有限，突显了元提示的内在局限性。

Conclusion: 进化精炼技术在非确定性环境中改善AI性能具有潜力，单智能体价值函数优化方法效果显著，而元提示方法存在局限性。

Abstract: Optimizing artificial intelligence (AI) for dynamic environments remains a
fundamental challenge in machine learning research. In this paper, we examine
evolutionary training methods for optimizing AI to solve the game 2048, a 2D
sliding puzzle. 2048, with its mix of strategic gameplay and stochastic
elements, presents an ideal playground for studying decision-making, long-term
planning, and dynamic adaptation. We implemented two distinct systems: a
two-agent metaprompting system where a "thinker" large language model (LLM)
agent refines gameplay strategies for an "executor" LLM agent, and a
single-agent system based on refining a value function for a limited Monte
Carlo Tree Search. We also experimented with rollback features to avoid
performance degradation. Our results demonstrate the potential of evolutionary
refinement techniques in improving AI performance in non-deterministic
environments. The single-agent system achieved substantial improvements, with
an average increase of 473.2 points per cycle, and with clear upward trends
(correlation $\rho$=0.607) across training cycles. The LLM's understanding of
the game grew as well, shown in its development of increasingly advanced
strategies. Conversely, the two-agent system did not garner much improvement,
highlighting the inherent limits of meta-prompting.

</details>


### [21] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: 该论文提出了个性化认知模拟(ICS)任务，评估不同认知表征方法在大语言模型模仿作者风格方面的表现，发现概念和语言特征的结合最有效。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型能够表面模仿人类行为，但其模拟更深层次个性化认知过程的能力尚不清楚，需要系统评估。

Method: 构建基于新出版小说的数据集，提出11条件认知评估框架，测试七种现成大语言模型在作者风格模仿中的表现，比较不同认知表征方法。

Result: 概念和语言特征的结合在ICS中特别有效，优于基于静态档案的线索；大语言模型更擅长模仿语言风格而非叙事结构。

Conclusion: 研究为开发适应个体思维和表达方式的AI系统奠定了基础，推动更个性化和人类对齐的创意技术发展。

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [22] [Using Large Language Models for Abstraction of Planning Domains - Extended Version](https://arxiv.org/abs/2510.20258)
*Bita Banihashemi,Megh Patel,Yves Lespérance*

Main category: cs.AI

TL;DR: 使用大语言模型通过上下文学习生成抽象PDDL领域和问题实例，以自然语言指定的抽象目标为指导，在简单场景中能有效合成规划领域抽象。


<details>
  <summary>Details</summary>
Motivation: 动态领域抽象对智能体的规划、推理和解释能力至关重要，但如何生成与特定目标对齐的抽象仍是一个挑战。

Method: 在PDDL中建模具体行为，利用LLM进行上下文学习，生成抽象PDDL领域和问题实例，并通过符号验证工具和专家评估。

Result: GPT-4o在简单设置下能有效合成规划领域抽象，但在动作抽象方面优于相关流体的抽象。

Conclusion: LLM在生成规划领域抽象方面具有潜力，特别是在动作抽象方面表现良好，但在复杂抽象任务中仍需改进。

Abstract: Generating an abstraction of a dynamic domain that aligns with a given
purpose remains a significant challenge given that the choice of such an
abstraction can impact an agent's ability to plan, reason, and provide
explanations effectively. We model the agent's concrete behaviors in PDDL and
investigate the use of in-context learning with large language models (LLMs)
for the generation of abstract PDDL domains and problem instances, given an
abstraction objective specified in natural language. The benchmark examples we
use are new and have not been part of the data any LLMs have been trained on.
We consider three categories of abstractions: abstraction of choice of
alternative concrete actions, abstraction of sequences of concrete actions, and
abstraction of action/predicate parameters, as well as combinations of these.
The generated abstract PDDL domains and problem instances are then checked by
symbolic validation tools as well as human experts. Our experiments show that
GPT-4o can generally synthesize useful planning domain abstractions in simple
settings, although it is better at abstracting over actions than over the
associated fluents.

</details>


### [23] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: 提出了STaBERT模型，通过整合POI信息和时间描述符来增强人类移动性预测的语义理解，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有的人类移动性预测模型要么只建模位置序列，要么仅将时间信息作为辅助输入，未能充分利用兴趣点(POI)提供的丰富语义上下文。

Method: 提出了STaBERT模型，在BERT-based移动性模型基础上，通过衍生时间描述符和POI嵌入来丰富语义表示，构建统一的语义增强移动性表示。

Result: 实验结果显示STaBERT显著提高了预测精度：单城市预测的GEO-BLEU分数从0.34提升到0.75；多城市预测从0.34提升到0.56。

Conclusion: 整合POI和时间信息能够更好地捕捉人类移动的语义基础，STaBERT模型在人类移动性预测任务上表现出显著优势。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [24] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: ToolEQA是一个结合外部工具和多步推理的具身问答代理，通过工具获取有用信息来改进探索方向，从而以更短探索距离生成更准确回答。该方法在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接使用视觉语言模型探索环境并回答问题，缺乏显式思考和规划，导致推理能力受限、探索效率低下和回答效果不佳。

Method: 提出ToolEQA代理，集成外部工具与多步推理，通过工具提供有用信息来改进下一步推理的探索方向。设计了自动生成EQA任务的数据生成管道，构建了包含18K任务的EQA-RT数据集。

Result: 在EQA-RT-Seen和EQA-RT-Unseen测试集上，ToolEQA比最先进基线方法成功率提高9.2~20.2%，比零样本ToolEQA提高10%成功率。在HM-EQA、OpenEQA和EXPRESS-Bench数据集上也达到最先进性能。

Conclusion: ToolEQA通过集成外部工具和多步推理，显著提高了具身问答的性能和效率，展示了良好的泛化能力。

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [25] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: 该论文分析了临床数据收集中存在的多种偏见类型，并提出了改善AI系统公平性和鲁棒性的实用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医疗领域有巨大潜力，但由于训练数据的质量和公平性问题，AI解决方案在真实临床实践中的整合仍然有限。主要障碍是数据收集过程中的偏见问题。

Method: 基于AI4HealthyAging项目的经验，识别了临床数据收集中存在的多种偏见类型，包括历史偏见、代表性偏见和测量偏见等。

Result: 发现了在性别、年龄、居住环境、社会经济状况、设备和标签等多个变量上存在的偏见表现。

Conclusion: 提出了改善临床问题设计和数据收集公平性和鲁棒性的实用建议，为开发更公平的医疗AI系统提供指导。

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [26] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出了一种用于军事行动中AI系统目标打击的附带损害评估模型，该模型整合了时间、空间和力量维度，采用知识表示与推理架构，通过分层结构捕获AI系统类别、攻击向量和上下文因素。


<details>
  <summary>Details</summary>
Motivation: 在AI系统在战场中发挥日益重要作用的时代，确保负责任的打击目标选择需要对潜在附带效应进行严格评估。

Method: 采用设计科学方法论，构建统一的知识表示与推理架构，整合时间、空间和力量维度，通过分层结构捕获AI系统类别、攻击向量和上下文因素，考虑传播、严重性、可能性和评估指标。

Result: 通过实例化演示和评估了该模型，为构建负责任和可信赖的智能系统奠定了基础。

Conclusion: 该模型为评估军事行动中打击AI系统产生的效应提供了透明推理机制，有助于构建负责任和可信赖的智能系统。

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [27] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: 该论文系统综述了大型语言模型如何变革知识图谱构建，从传统规则方法转向语言驱动的生成框架，分析了基于模式和无模式两种范式，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的出现，知识图谱构建正经历范式转变，需要系统梳理LLM如何重塑传统的本体工程、知识抽取和知识融合三层流程，以弥合符号化知识工程与神经语义理解之间的鸿沟。

Method: 首先回顾传统KG方法建立概念基础，然后从两个互补视角分析新兴LLM驱动方法：基于模式的范式强调结构、规范化和一致性；无模式范式强调灵活性、适应性和开放发现。

Result: 系统综述了LLM赋能知识图谱构建的最新进展，综合分析了代表性框架的技术机制和局限性，为理解LLM与KG的交互演进提供了清晰框架。

Conclusion: LLM正在重塑知识图谱构建范式，未来研究方向包括基于KG的LLM推理、面向智能体系统的动态知识记忆以及多模态KG构建，旨在开发自适应、可解释的智能知识系统。

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [28] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: 提出了IKnow框架，通过指令-响应对话格式的自监督目标，在无需外部资源的情况下实现语言模型的持续预训练，避免指令跟随能力退化。


<details>
  <summary>Details</summary>
Motivation: 解决持续预训练中指令调优模型语义表示退化和指令跟随能力下降的问题，特别是在无法获取基础模型权重或外部领域数据库的现实场景下。

Method: IKnow框架，基于指令-响应对话格式设计自监督目标，利用文本中嵌入的领域知识进行更深层次的语义编码。

Result: 未在摘要中明确说明具体实验结果。

Conclusion: IKnow提供了一个简单通用的持续适应框架，能够在不依赖外部资源的情况下有效利用领域知识。

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [29] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: 提出了一种新的计算模型，通过五个功能函数生成更具新颖性的创新机会，在酒店业创新项目中评估显示该模型比Notebook LM和ChatGPT4o产生更新颖和/或更有用的结果。


<details>
  <summary>Details</summary>
Motivation: 基于创造力理论和技巧，开发计算模型来生成更具新颖性的创新机会，同时不损失实用性。

Method: 实现了一个包含五个功能函数的计算模型，旨在提高创新机会的新颖性，并在酒店业创新项目中进行评估。

Result: 该模型生成的结果比Notebook LM和ChatGPT4o更新颖和/或更有用，但并非所有功能函数都对提高新颖性有贡献。

Conclusion: 模型在生成新颖创新机会方面表现优异，但部分功能效果有限，为后续模型开发提供了新的方向。

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [30] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 提出了一种名为EBR的新型神经推理器，使用嵌入来近似符号推理器的结果，解决了传统描述逻辑推理器对不一致和错误数据不鲁棒的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号概念学习方法由于使用描述逻辑推理器，无法部署到现实世界知识库中，因为这些推理器对不一致和错误数据不鲁棒。

Method: EBR推理器依赖嵌入来近似符号推理器的结果，仅需要检索原子概念和存在限制的实例，就能检索或近似任何SHOIQ描述逻辑概念的实例集。

Result: 实验表明，与最先进的推理器相比，EBR对缺失和错误数据具有鲁棒性。

Conclusion: EBR为在现实世界知识库中部署神经符号概念学习方法提供了可行的解决方案。

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [31] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: FLORA是一种基于模糊逻辑的无监督知识图谱对齐方法，能够同时对齐实体和关系，提供可解释的结果，并在主要基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱对齐方法主要关注纯实体级对齐，在嵌入空间中计算实体相似度，缺乏可解释的推理且需要训练数据。

Method: 基于模糊逻辑的迭代方法，提供实体和关系的整体对齐，允许悬挂实体（在另一个KG中没有对应物的实体），并具有可证明的收敛性。

Result: 在主要基准测试中达到最先进的结果。

Conclusion: FLORA是一种简单有效的无监督知识图谱对齐方法，具有可解释性、收敛性和处理悬挂实体的能力。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [32] [Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI](https://arxiv.org/abs/2510.20568)
*Susan Ariel Aaronson,Michael Moreno*

Main category: cs.AI

TL;DR: 该研究比较了澳大利亚、哥伦比亚和美国三个国家在AI治理中的公众参与情况，发现政府未能建立有效的公众对话机制，参与率低于1%，且官员对反馈响应有限，存在参与式AI治理的承诺与实践之间的差距。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨政府如何通过公众参与来建立对AI及其治理的信任，但发现当前方法未能实现这一目标。

Method: 采用景观分析方法，比较三个国家（澳大利亚、哥伦比亚、美国）如何征集公众对AI风险和政策的反馈，以及这些反馈是否影响了治理。

Result: 研究发现三个国家均未能建立有意义的公众对话，政府缺乏吸引多样化声音和宣传征集意见的努力，参与率极低（<1%），官员对反馈的响应有限。

Conclusion: 当前方法无法建立对AI的信任或合法性，因为政策制定者未能充分倾听和回应公众关切。作者提出了八项改进建议，包括提升AI素养、监测公众反馈、扩大宣传、定期在线论坛、创新参与方法、纳入弱势群体、公开回应意见和简化参与流程。

Abstract: The worlds people have strong opinions about artificial intelligence (AI),
and they want policymakers to listen. Governments are inviting public comment
on AI, but as they translate input into policy, much of what citizens say is
lost. Policymakers are missing a critical opportunity to build trust in AI and
its governance. This paper compares three countries, Australia, Colombia, and
the United States, that invited citizens to comment on AI risks and policies.
Using a landscape analysis, the authors examined how each government solicited
feedback and whether that input shaped governance. Yet in none of the three
cases did citizens and policymakers establish a meaningful dialogue.
Governments did little to attract diverse voices or publicize calls for
comment, leaving most citizens unaware or unprepared to respond. In each
nation, fewer than one percent of the population participated. Moreover,
officials showed limited responsiveness to the feedback they received, failing
to create an effective feedback loop. The study finds a persistent gap between
the promise and practice of participatory AI governance. The authors conclude
that current approaches are unlikely to build trust or legitimacy in AI because
policymakers are not adequately listening or responding to public concerns.
They offer eight recommendations: promote AI literacy; monitor public feedback;
broaden outreach; hold regular online forums; use innovative engagement
methods; include underrepresented groups; respond publicly to input; and make
participation easier.

</details>


### [33] [Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting](https://arxiv.org/abs/2510.20591)
*Ali Rajaei,Peter Palensky,Jochen L. Cremer*

Main category: cs.AI

TL;DR: 提出基于图神经网络(GNN)的网络拓扑优化方法，通过母线分裂解决电网拥塞问题，实现大规模系统近实时优化，相比传统方法加速4个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统混合整数非线性优化方法无法在大规模系统中实现近实时求解，现有机器学习方法在泛化性和适用性方面存在局限，需要开发能适应不同拓扑和运行条件的新方法。

Method: 采用图神经网络(GNN)加速方法，开发异质边感知消息传递神经网络来预测有效的母线分裂动作，捕捉局部潮流模式，实现拓扑变化泛化和跨系统可迁移性。

Result: 在GOC 2000节点系统上实现4个数量级的加速，1分钟内提供交流可行解，最优性差距仅为2.3%。

Conclusion: 该方法在大规模系统拓扑优化方面取得重要进展，实现了近实时求解和跨系统泛化能力。

Abstract: Network topology optimization (NTO) via busbar splitting can mitigate
transmission grid congestion and reduce redispatch costs. However, solving this
mixed-integer non-linear problem for large-scale systems in near-real-time is
currently intractable with existing solvers. Machine learning (ML) approaches
have emerged as a promising alternative, but they have limited generalization
to unseen topologies, varying operating conditions, and different systems,
which limits their practical applicability. This paper formulates NTO for
congestion management problem considering linearized AC PF, and proposes a
graph neural network (GNN)-accelerated approach. We develop a heterogeneous
edge-aware message passing NN to predict effective busbar splitting actions as
candidate NTO solutions. The proposed GNN captures local flow patterns,
achieves generalization to unseen topology changes, and improves
transferability across systems. Case studies show up to 4 orders-of-magnitude
speed-up, delivering AC-feasible solutions within one minute and a 2.3%
optimality gap on the GOC 2000-bus system. These results demonstrate a
significant step toward near-real-time NTO for large-scale systems with
topology and cross-system generalization.

</details>


### [34] [What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation](https://arxiv.org/abs/2510.20603)
*Heejin Do,Jaehui Hwang,Dongyoon Han,Seong Joon Oh,Sangdoo Yun*

Main category: cs.AI

TL;DR: 提出了因果逐步评估方法(CaSE)来细粒度评估LLM推理过程的质量，将推理质量分解为相关性和连贯性两个维度，并通过基于CaSE评估的训练数据筛选来提升最终任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前仅评估最终答案正确性的方法过于粗糙，无法为模型改进提供有效信号，且忽视了底层推理过程的质量。需要更细粒度的推理评估方法来构建更鲁棒的模型。

Method: 引入因果逐步评估(CaSE)方法，将推理质量分解为相关性和连贯性两个维度。相关性衡量步骤是否基于问题本身，连贯性衡量步骤是否从先前步骤逻辑推导而来。CaSE仅使用前文语境评估每个推理步骤，避免后见之明偏差。

Result: 在专家标注的基准MRa-GSM8K和MRa-MATH上验证了CaSE与人工判断的一致性。更重要的是，使用CaSE评估的相关性和连贯性来筛选训练数据，直接提升了最终任务性能。

Conclusion: 这项工作为分析、调试和改进LLM推理提供了一个可扩展的框架，证明了超越有效性检查的实际价值，细粒度推理评估是提升模型性能的有效途径。

Abstract: Evaluating large language models (LLMs) on final-answer correctness is the
dominant paradigm. This approach, however, provides a coarse signal for model
improvement and overlooks the quality of the underlying reasoning process. We
argue that a more granular evaluation of reasoning offers a more effective path
to building robust models. We decompose reasoning quality into two dimensions:
relevance and coherence. Relevance measures if a step is grounded in the
problem; coherence measures if it follows logically from prior steps. To
measure these aspects reliably, we introduce causal stepwise evaluation (CaSE).
This method assesses each reasoning step using only its preceding context,
which avoids hindsight bias. We validate CaSE against human judgments on our
new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we
show that curating training data with CaSE-evaluated relevance and coherence
directly improves final task performance. Our work provides a scalable
framework for analyzing, debugging, and improving LLM reasoning, demonstrating
the practical value of moving beyond validity checks.

</details>


### [35] [Efficient Algorithms for Computing Random Walk Centrality](https://arxiv.org/abs/2510.20604)
*Changan Liu,Zixuan Xie,Ahad N. Zehmakan,Zhongzhi Zhang*

Main category: cs.AI

TL;DR: 提出了两种可扩展算法来计算随机游走中心性，一种基于近似Cholesky分解和稀疏逆估计，另一种基于采样根生成树，两者都在近线性时间内运行并提供强近似保证。


<details>
  <summary>Details</summary>
Motivation: 随机游走中心性是图挖掘中量化节点重要性的基本指标，但现有方法计算成本高，无法应用于大型网络。

Method: 提出随机游走中心性的新公式，并基于此开发两种算法：近似Cholesky分解与稀疏逆估计方法，以及根生成树采样方法。

Result: 在大型真实网络（包括超过1000万个节点的网络）上的广泛实验证明了所提算法的效率和近似质量。

Conclusion: 所提出的算法能够高效计算随机游走中心性，解决了现有方法在大规模网络中的计算瓶颈问题。

Abstract: Random walk centrality is a fundamental metric in graph mining for
quantifying node importance and influence, defined as the weighted average of
hitting times to a node from all other nodes. Despite its ability to capture
rich graph structural information and its wide range of applications, computing
this measure for large networks remains impractical due to the computational
demands of existing methods. In this paper, we present a novel formulation of
random walk centrality, underpinning two scalable algorithms: one leveraging
approximate Cholesky factorization and sparse inverse estimation, while the
other sampling rooted spanning trees. Both algorithms operate in near-linear
time and provide strong approximation guarantees. Extensive experiments on
large real-world networks, including one with over 10 million nodes,
demonstrate the efficiency and approximation quality of the proposed
algorithms.

</details>


### [36] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: MIMOSA框架是一个可解释性设计的方法论，旨在生成平衡可解释性与性能的预测模型，同时嵌入因果关系、公平性和隐私等关键伦理属性。


<details>
  <summary>Details</summary>
Motivation: 开发可信赖的自动决策模型需要可解释性设计，以促进信任、问责制和在现实应用中的安全采用。

Method: 形式化定义了监督学习设置，涵盖表格数据、时间序列、图像、文本、交易和轨迹等多种数据类型，并分析了特征重要性、规则和实例三类可解释模型家族。

Result: 建立了评估伦理度量的理论框架，包括因果关系、公平性和隐私的正式定义、评估指标和验证程序。

Conclusion: 该框架为开发不仅准确和可解释，而且公平、保护隐私和具有因果意识的AI系统奠定了理论基础，即值得信赖的AI系统。

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [37] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: EcomEval是一个全面的多语言多模态基准测试，用于评估LLM在电子商务领域的性能，解决了现有评估工具在任务多样性、模态覆盖和语言支持方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有电子商务评估基准存在任务多样性有限、缺乏多模态数据、使用合成数据以及语言覆盖范围窄的问题，无法可靠评估模型在复杂真实购物场景中的表现。

Method: 构建包含6个类别37个任务（含8个多模态任务）的基准，主要来自真实客户查询和交易日志；采用半自动流程，由大模型生成候选回答，再由50多名电子商务和多语言专家审核修改；为每个问题和任务类别定义难度级别。

Result: EcomEval覆盖7种语言（包括5种低资源东南亚语言），提供了多语言视角，能够进行挑战导向和细粒度评估。

Conclusion: EcomEval填补了电子商务领域评估基准的空白，为评估LLM在复杂真实商业场景中的表现提供了可靠工具。

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [38] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: 提出了Fluidity Index (FI)来衡量模型在动态扩展环境中的适应性，通过评估对初始、当前和未来环境状态变化的响应准确性来测试上下文切换和连续性能力。


<details>
  <summary>Details</summary>
Motivation: 需要量化模型在动态扩展环境中的适应能力，区分封闭式和开放式基准测试，优先关注现实世界的闭环开放式基准来测试真正的适应性。

Method: 引入Fluidity Index (FI)基准，通过测量模型对初始状态、当前状态和未来状态偏差的响应准确性，评估上下文切换和连续性能力。

Result: 该方法能够有效评估模型在动态扩展环境中的适应能力，区分不同级别的适应性表现。

Conclusion: 真正超级智能的模型应至少具备二阶适应性，能够通过数字补充实现自我维持计算，达到最佳流动性。

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [39] [Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges](https://arxiv.org/abs/2510.20641)
*Andrea Agiollo,Andrea Omicini*

Main category: cs.AI

TL;DR: 本文对将机器学习整合到理性智能体架构中的现有方法进行了细粒度系统化分析，特别关注BDI（信念-欲望-意图）范式，识别了关键研究机会和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型在感知和认知任务中展现出类人能力，将ML整合到理性智能体架构中的框架越来越受关注，但现有研究碎片化且不连贯，往往忽视理性架构的表达能力。

Method: 使用BDI范式作为参考框架，对现有方法进行细粒度系统化分析，梳理快速发展的相关文献。

Result: 分析展示了ML增强理性智能体的快速发展现状，识别了该领域的关键研究方向和未解决问题。

Conclusion: 为设计有效的理性ML智能体提供了系统化分析框架，并指明了重要的研究机会和开放挑战。

Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML)
models in perceptual and cognitive tasks, frameworks integrating ML within
rational agent architectures are gaining traction. Yet, the landscape remains
fragmented and incoherent, often focusing on embedding ML into generic agent
containers while overlooking the expressive power of rational
architectures--such as Belief-Desire-Intention (BDI) agents. This paper
presents a fine-grained systematisation of existing approaches, using the BDI
paradigm as a reference. Our analysis illustrates the fast-evolving literature
on rational agents enhanced by ML, and identifies key research opportunities
and open challenges for designing effective rational ML agents.

</details>


### [40] [The Shape of Reasoning: Topological Analysis of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2510.20665)
*Xue Wen Tan,Nathaniel Tan,Galen Lee,Stanley Kok*

Main category: cs.AI

TL;DR: 提出基于拓扑数据分析的框架来评估大语言模型推理轨迹质量，相比传统图结构指标能更有效捕捉推理的几何特征


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型推理轨迹质量的方法依赖专家标注、人工判断，效率低且不可靠；现有自动化方法主要基于图结构连通性，无法准确反映推理质量本质

Method: 采用拓扑数据分析方法，通过捕捉推理轨迹的几何结构特征来实现标签高效的自动化评估

Result: 拓扑特征在预测推理质量方面比标准图指标具有显著更高的预测能力，表明有效推理更适合用高维几何结构而非纯关系图来表征

Conclusion: 紧凑且稳定的拓扑特征集能够可靠指示轨迹质量，为未来强化学习算法提供了实用的质量信号

Abstract: Evaluating the quality of reasoning traces from large language models remains
understudied, labor-intensive, and unreliable: current practice relies on
expert rubrics, manual annotation, and slow pairwise judgments. Automated
efforts are dominated by graph-based proxies that quantify structural
connectivity but do not clarify what constitutes high-quality reasoning; such
abstractions can be overly simplistic for inherently complex processes. We
introduce a topological data analysis (TDA)-based evaluation framework that
captures the geometry of reasoning traces and enables label-efficient,
automated assessment. In our empirical study, topological features yield
substantially higher predictive power for assessing reasoning quality than
standard graph metrics, suggesting that effective reasoning is better captured
by higher-dimensional geometric structures rather than purely relational
graphs. We further show that a compact, stable set of topological features
reliably indicates trace quality, offering a practical signal for future
reinforcement learning algorithms.

</details>


### [41] [Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs](https://arxiv.org/abs/2510.20691)
*Yanlin Song,Ben Liu,Víctor Gutiérrez-Basulto,Zhiwei Hu,Qianqian Xie,Min Peng,Sophia Ananiadou,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 提出了Graph-RFT框架，通过两阶段强化微调使LLM能够在知识不完整条件下进行自主规划和跨知识图谱与网络的自适应检索调度，解决复杂KGQA场景中的推理失败问题。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法难以充分利用知识图谱的丰富知识和LLM的推理能力，特别是在复杂场景中。它们通常假设完整的KG覆盖，缺乏判断何时需要外部信息的机制，且推理过程局部短视，无法保持连贯的多步规划。

Method: Graph-RFT采用两阶段强化微调框架：1）链式思维微调方法激活结构化推理；2）规划-检索引导的强化学习过程，集成显式规划和检索动作，采用笛卡尔式规划模块分解复杂问题，逻辑表达式指导工具调用。

Result: 该框架通过多奖励设计优化推理检索过程，结合结果和检索特定信号，使模型能够学习何时以及如何有效结合KG和网络检索。

Conclusion: Graph-RFT能够实现覆盖感知的检索调度和全局一致的多步推理，有效解决了复杂KGQA场景中的推理失败问题。

Abstract: Knowledge Graph Question Answering aims to answer natural language questions
by reasoning over structured knowledge graphs. While large language models have
advanced KGQA through their strong reasoning capabilities, existing methods
continue to struggle to fully exploit both the rich knowledge encoded in KGs
and the reasoning capabilities of LLMs, particularly in complex scenarios. They
often assume complete KG coverage and lack mechanisms to judge when external
information is needed, and their reasoning remains locally myopic, failing to
maintain coherent multi-step planning, leading to reasoning failures even when
relevant knowledge exists. We propose Graph-RFT, a novel two-stage
reinforcement fine-tuning KGQA framework with a
'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to
perform autonomous planning and adaptive retrieval scheduling across KG and web
sources under incomplete knowledge conditions. Graph-RFT introduces a
chain-of-thought fine-tuning method with a customized plan-retrieval dataset
activates structured reasoning and resolves the GRPO cold-start problem. It
then introduces a novel plan-retrieval guided reinforcement learning process
integrates explicit planning and retrieval actions with a multi-reward design,
enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired
planning module to decompose complex questions into ordered subquestions, and
logical expression to guide tool invocation for globally consistent multi-step
reasoning. This reasoning retrieval process is optimized with a multi-reward
combining outcome and retrieval specific signals, enabling the model to learn
when and how to combine KG and web retrieval effectively.

</details>


### [42] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: 提出了一种基于广义均值积分的AGI一致性度量方法，替代了传统算术平均值的补偿性假设，更严格地衡量通用人工智能的真实进展。


<details>
  <summary>Details</summary>
Motivation: 现有AGI定义使用算术平均值，假设领域间能力可以相互补偿，但真正的通用智能应该体现跨所有关键领域的平衡能力。

Method: 基于广义均值在补偿性指数连续统上的积分，构建面积下曲线(AUC)度量，涵盖算术、几何和调和均值等不同补偿性假设。

Result: 应用于GPT-4和GPT-5的CHC领域得分显示，尽管算术得分较高(GPT-5达24%)，但一致性调整后的AUC表明两者距离通用能力仍很远。

Conclusion: 广义均值积分提供了一个原则性、可解释且更严格的AGI度量基础，能更好地捕捉领域间依赖关系并惩罚能力不平衡。

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [43] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: 提出了Real Deep Research (RDR)框架，用于系统分析AI和机器人领域的研究趋势，识别新兴方向和跨领域机会，帮助研究人员应对论文数量激增的挑战。


<details>
  <summary>Details</summary>
Motivation: AI和机器人领域每年产出超过10,000篇论文，研究人员难以跟上快速发展的趋势，跨学科工作增多，需要探索专业领域外的知识。

Method: 构建了一个通用的RDR管道，能够系统分析任何研究领域，识别新兴趋势，发现跨领域机会，并为新研究提供具体起点。

Result: 将RDR框架应用于AI和机器人领域，特别关注基础模型和机器人技术进步，并扩展到其他科学领域分析。

Conclusion: RDR框架为AI及其他领域的研究人员提供了系统分析研究趋势的工具，帮助他们在快速发展的研究环境中保持更新。

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [44] [Information Gradient for Nonlinear Gaussian Channel with Applications to Task-Oriented Communication](https://arxiv.org/abs/2510.20179)
*Tadashi Wadayama*

Main category: cs.IT

TL;DR: 提出了基于梯度的参数化非线性高斯信道优化框架，通过互信息最大化实现。利用SFB方法推导出计算可行的信息梯度公式，该梯度可通过去噪得分匹配和自动微分框架高效计算。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以直接优化非线性高斯信道的互信息，特别是在需要端到端优化前端参数时。需要一个计算可行且无需显式计算输出分布的方法。

Method: 使用SFB方法推导信息梯度公式，该梯度由边际输出分布的得分函数（通过DSM学习）和前端函数的雅可比矩阵（通过VJP高效计算）组成。扩展至任务导向场景和IB目标。

Result: 实验验证了信息梯度公式的正确性，并证明其在优化线性和非线性信道方面的有效性。

Conclusion: 该框架实现了非线性前端的端到端优化，无需显式计算输出分布，为信道优化提供了实用的梯度方法。

Abstract: We propose a gradient-based framework for optimizing parametric nonlinear
Gaussian channels via mutual information maximization. Leveraging the
score-to-Fisher bridge (SFB) methodology, we derive a computationally tractable
formula for the information gradient that is the gradient of mutual information
with respect to the parameters of the nonlinear front-end. Our formula
expresses this gradient in terms of two key components: the score function of
the marginal output distribution, which can be learned via denoising score
matching (DSM), and the Jacobian of the front-end function, which is handled
efficiently using the vector-Jacobian product (VJP) within automatic
differentiation frameworks. This enables practical parameter optimization
through gradient ascent. Furthermore, we extend this framework to task-oriented
scenarios, deriving gradients for both task-specific mutual information, where
a task variable depends on the channel input, and the information bottleneck
(IB) objective. A key advantage of our approach is that it facilitates
end-to-end optimization of the nonlinear front-end without requiring explicit
computation on the output distribution. Extensive experimental validation
confirms the correctness of our information gradient formula against analytical
solutions and demonstrates its effectiveness in optimizing both linear and
nonlinear channels toward their objectives.

</details>


### [45] [New Second-Order Achievability Bounds for Coding with Side Information via Type Deviation Convergence](https://arxiv.org/abs/2510.20241)
*Xiang Li,Cheuk Ting Li*

Main category: cs.IT

TL;DR: 提出了一个名为类型偏差收敛的二阶可达性框架，适用于网络信息论设置，特别适合有损源编码和带成本的信道编码。改进了Wyner-Ziv、Heegard-Berger和Gelfand-Pinsker问题的二阶可达性界。


<details>
  <summary>Details</summary>
Motivation: 现有的二阶可达性界在损失源编码和带成本的信道编码方面仍有改进空间，需要开发更通用的框架来提升性能边界。

Method: 提出了类型偏差收敛框架，通过分析类型偏差的收敛特性来推导二阶可达性界。

Result: 为Wyner-Ziv问题提供了优于所有已知界的二阶可达性界，同时改进了Heegard-Berger问题和带成本的Gelfand-Pinsker问题的性能边界。

Conclusion: 类型偏差收敛框架是网络信息论中二阶可达性分析的有效工具，能够显著改进多个重要问题的性能边界。

Abstract: We propose a framework for second-order achievability, called type deviation
convergence, that is generally applicable to settings in network information
theory, and is especially suitable for lossy source coding and channel coding
with cost. We give a second-order achievability bound for lossy source coding
with side information at the decoder (Wyner-Ziv problem) that improves upon all
known bounds (e.g., Watanabe-Kuzuoka-Tan, Yassaee-Aref-Gohari and
Li-Anantharam). We also give second-order achievability bounds for lossy
compression where side information may be absent (Heegard-Berger problem) and
channels with noncausal state information at the encoder and cost constraint
(Gelfand-Pinsker problem with cost) that improve upon previous bounds.

</details>


### [46] [A Location-Aware Hybrid Deep Learning Framework for Dynamic Near-Far Field Channel Estimation in Low-Altitude UAV Communications](https://arxiv.org/abs/2510.20277)
*Wenli Yuan,Kan Yu,Xiaowu Liu,Kaixuan Li,Qixun Zhang,Zhiyong Feng*

Main category: cs.IT

TL;DR: 提出了一种基于位置感知混合深度学习架构的统一信道估计框架，用于解决低空无人机通信中的混合近远场传播条件挑战。


<details>
  <summary>Details</summary>
Motivation: 传统信道估计方法依赖远场假设，无法捕捉近场场景中的复杂信道变化，且忽略了实时收发器位置等有价值的几何先验信息。

Method: 结合CNN进行空间特征提取、BiLSTM网络建模时间演化、多头自注意力机制增强对判别性信道分量的关注，并嵌入实时收发器位置作为几何先验。

Result: 在归一化均方误差(NMSE)上平均至少减少30.25%，显著优于现有基准方法。

Conclusion: 所提出的位置感知混合深度学习框架能有效处理混合近远场传播条件，提升信道估计精度和模型泛化能力。

Abstract: In low altitude UAV communications, accurate channel estimation remains
challenging due to the dynamic nature of air to ground links, exacerbated by
high node mobility and the use of large scale antenna arrays, which introduce
hybrid near and far field propagation conditions. While conventional estimation
methods rely on far field assumptions, they fail to capture the intricate
channel variations in near-field scenarios and overlook valuable geometric
priors such as real-time transceiver positions. To overcome these limitations,
this paper introduces a unified channel estimation framework based on a
location aware hybrid deep learning architecture. The proposed model
synergistically combines convolutional neural networks (CNNs) for spatial
feature extraction, bidirectional long short term memory (BiLSTM) networks for
modeling temporal evolution, and a multihead self attention mechanism to
enhance focus on discriminative channel components. Furthermore, real-time
transmitter and receiver locations are embedded as geometric priors, improving
sensitivity to distance under near field spherical wavefronts and boosting
model generalization. Extensive simulations validate the effectiveness of the
proposed approach, showing that it outperforms existing benchmarks by a
significant margin, achieving at least a 30.25% reduction in normalized mean
square error (NMSE) on average.

</details>


### [47] [Moving or Predicting? RoleAware-MAPP: A Role-Aware Transformer Framework for Movable Antenna Position Prediction to Secure Wireless Communications](https://arxiv.org/abs/2510.20293)
*Wenxu Wang,Xiaowu Liu,Wei Gong,Yujia Zhao,Kaixuan Li,Qixun Zhang,Zhiyong Feng,Kan Yu*

Main category: cs.IT

TL;DR: 提出RoleAware-MAPP框架，通过角色感知嵌入、物理信息语义特征和复合损失函数，解决可移动天线在物理层安全中的实时优化和时域失配问题。


<details>
  <summary>Details</summary>
Motivation: 可移动天线技术面临实时优化计算复杂度高和机械运动与信道变化速度不匹配的挑战，现有学习方法忽视了通信领域知识，特别是合法用户与窃听者的不对称角色和对抗性交互。

Method: 将MA定位问题重构为预测任务，提出基于Transformer的RoleAware-MAPP框架，包含角色感知嵌入、物理信息语义特征和复合损失函数三个关键组件。

Result: 在3GPP兼容场景下，平均保密率达到0.3569 bps/Hz，严格正保密容量达到81.52%，分别比最强基线提高48.4%和5.39个百分点，在不同用户速度和噪声条件下保持稳健性能。

Conclusion: RoleAware-MAPP通过融入领域知识，有效解决了可移动天线在物理层安全中的关键挑战，显著提升了保密性能。

Abstract: Movable antenna (MA) technology provides a promising avenue for actively
shaping wireless channels through dynamic antenna positioning, thereby enabling
electromagnetic radiation reconstruction to enhance physical layer security
(PLS). However, its practical deployment is hindered by two major challenges:
the high computational complexity of real time optimization and a critical
temporal mismatch between slow mechanical movement and rapid channel
variations. Although data driven methods have been introduced to alleviate
online optimization burdens, they are still constrained by suboptimal training
labels derived from conventional solvers or high sample complexity in
reinforcement learning. More importantly, existing learning based approaches
often overlook communication-specific domain knowledge, particularly the
asymmetric roles and adversarial interactions between legitimate users and
eavesdroppers, which are fundamental to PLS. To address these issues, this
paper reformulates the MA positioning problem as a predictive task and
introduces RoleAware-MAPP, a novel Transformer based framework that
incorporates domain knowledge through three key components: role-aware
embeddings that model user specific intentions, physics-informed semantic
features that encapsulate channel propagation characteristics, and a composite
loss function that strategically prioritizes secrecy performance over mere
geometric accuracy. Extensive simulations under 3GPP-compliant scenarios show
that RoleAware-MAPP achieves an average secrecy rate of 0.3569 bps/Hz and a
strictly positive secrecy capacity of 81.52%, outperforming the strongest
baseline by 48.4% and 5.39 percentage points, respectively, while maintaining
robust performance across diverse user velocities and noise conditions.

</details>


### [48] [Ergodic Mutual Information and Outage Probability for SIM-Assisted Holographic MIMO Communications](https://arxiv.org/abs/2510.20307)
*Anastasios Papazafeiropoulos,Pandelis Kourtessis,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.IT

TL;DR: 本文研究了堆叠智能超表面辅助MIMO系统的遍历互信息和中断概率，推导了基于统计信道状态信息的闭合表达式，并提出了比交替优化更快的梯度下降优化算法。


<details>
  <summary>Details</summary>
Motivation: 堆叠智能超表面相比单层超表面能通过波传播提供更好的性能，但现有文献缺乏对其辅助MIMO系统的遍历互信息和中断概率的研究。

Method: 使用大随机矩阵理论工具获得互信息分布，推导基于统计CSI的紧闭合中断概率表达式，并应用梯度下降方法最小化中断概率。

Result: 仿真结果验证了理论分析，表明相比传统MIMO系统和单层超表面有性能提升，所提优化算法比交替优化基准节省显著开销且更快。

Conclusion: 堆叠智能超表面能显著提升MIMO系统性能，所提出的分析和优化方法为下一代高容量网络提供了理论基础和实用工具。

Abstract: Stacked intelligent metasurface (SIM) is a promising enabler for
next-generation high-capacity networks that exhibit better performance compared
to its single-layer counterpart by means of just wave propagation. However, the
study of ergodic mutual information (EMI) and outage probability for
SIM-assisted multiple-input-multiple-output (MIMO) systems is not available in
the literature. To this end, we obtain the distribution of the MI by using
large random matrix theory (RMT) tools. Next, we derive a tight closed-form
expression for the outage probability based on statistical channel state
information (CSI). Moreover, we apply the gradient descent method for the
minimization of the outage probability. Simulation results verify the
analytical results and provide fundamental insights such as the performance
enhancements compared to conventional MIMO systems and the single-layer
counterpart. Notably the proposed optimization algorithm is faster than the
alternating optimization (AO) benchmark by saving significant overhead.

</details>


### [49] [Robust Analog Lagrange Coded Computing: Theory and Algorithms via Discrete Fourier Transforms](https://arxiv.org/abs/2510.20379)
*Rimpi Borah,J. Harshan*

Main category: cs.IT

TL;DR: 提出了安全的模拟拉格朗日编码计算框架，能够抵御拜占庭工作节点的完整性威胁，通过DFT码纠错算法提高计算精度，并基于信任度分布任务来优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ALCC虽然能保护数据隐私并应对慢节点，但无法抵御返回错误结果的拜占庭工作节点，因此需要增强其安全性。

Method: 使用DFT码的纠错算法构建新的重建策略，基于理论结果设计任务分配方法，利用工作节点的信任度信息优化计算。

Result: 提出的框架在存在有界数量拜占庭节点时显著提高了计算精度，特别是在掌握工作节点信任度信息时效果更佳。

Conclusion: 该安全ALCC框架有效增强了对抗拜占庭攻击的能力，但需注意浮点实现带来的精度噪声可能被利用进行合谋攻击。

Abstract: Analog Lagrange Coded Computing (ALCC) is a recently proposed computational
paradigm wherein certain computations over analog datasets are efficiently
performed using distributed worker nodes through floating point representation.
While the vanilla version of ALCC is known to preserve the privacy of the
datasets from the workers and also achieve resilience against stragglers, it is
not robust against Byzantine workers that return erroneous results.
Highlighting this vulnerability, we propose a secure ALCC framework that is
resilient against a wide range of integrity threats from the Byzantine workers.
As a foundational step, we use error-correction algorithms for Discrete Fourier
Transform (DFT) codes to build novel reconstruction strategies for ALCC thereby
improving its computational accuracy in the presence of a bounded number of
Byzantine workers. Furthermore, capitalizing on some theoretical results on the
performance of the DFT decoders, we propose novel strategies for distributing
the ALCC computational tasks to the workers, and show that such methods
significantly improve the accuracy when the workers' trust profiles are
available at the master server. Finally, we study the robustness of the
proposed framework against colluding attacks, and show that interesting attack
strategies can be executed by exploiting the inherent precision noise owing to
floating point implementation.

</details>


### [50] [Adversary-Aware Private Inference over Wireless Channels](https://arxiv.org/abs/2510.20518)
*Mohamed Seif,Malcolm Egan,Andrea J. Goldsmith,H. Vincent Poor*

Main category: cs.IT

TL;DR: 提出了一种保护隐私的AI感知框架，通过在设备端对提取的特征进行变换后再传输到模型服务器，以防止敏感数据被重构。


<details>
  <summary>Details</summary>
Motivation: 在边缘网络中，传感器和模型服务器通常不共置，需要传输特征数据。由于敏感个人数据可能被攻击者重构，需要对特征进行变换以降低隐私泄露风险。

Method: 设备在传输到模型服务器之前对提取的特征应用变换，以保护隐私。

Result: 提出了一个新颖的隐私保护AI感知框架。

Conclusion: 该框架为保护AI感知中的隐私提供了一种解决方案，特别适用于边缘计算场景。

Abstract: AI-based sensing at wireless edge devices has the potential to significantly
enhance Artificial Intelligence (AI) applications, particularly for vision and
perception tasks such as in autonomous driving and environmental monitoring. AI
systems rely both on efficient model learning and inference. In the inference
phase, features extracted from sensing data are utilized for prediction tasks
(e.g., classification or regression). In edge networks, sensors and model
servers are often not co-located, which requires communication of features. As
sensitive personal data can be reconstructed by an adversary, transformation of
the features are required to reduce the risk of privacy violations. While
differential privacy mechanisms provide a means of protecting finite datasets,
protection of individual features has not been addressed. In this paper, we
propose a novel framework for privacy-preserving AI-based sensing, where
devices apply transformations of extracted features before transmission to a
model server.

</details>


### [51] [Simultaneous Wireless Information and Power Transfer for Fluid Antenna Systems](https://arxiv.org/abs/2510.20569)
*Feilong Zhang,Jianxin Dai,Zhaohui Yang,Kai-Kit Wong,Lingyuxiu Li,Jianglin Ye*

Main category: cs.IT

TL;DR: 提出结合MISO流体天线与传统固定位置天线的新通信系统，通过天线位置优化提高能量收集效率，在SWIPT场景下显著提升ER的能量收集性能


<details>
  <summary>Details</summary>
Motivation: 利用流体天线技术通过改变天线位置来提升通信速率，结合传统固定位置天线优化能量收集效率

Method: 在SWIPT系统中，基站向IR和ER传输相同信号，通过优化发射和接收流体天线位置以及发射协方差矩阵，在满足IR最小SINR约束条件下最大化ER的接收功率

Result: 仿真结果表明，与传统固定位置天线相比，流体天线系统显著提高了ER的能量收集效率

Conclusion: 流体天线系统在SWIPT场景下能够有效提升能量收集性能，为无线能量传输提供了新的技术途径

Abstract: Fluid antenna is a promising wireless communication technology that enhances
communication rate by changing the antenna positions. This article proposes a
new communication system that combines multiple-input single-output (MISO)
fluid antennas with traditional fixed-position antennas, utilizing antenna
position optimization to improve energy harvesting efficiency. In this model,
we consider simultaneous wireless information and power transfer (SWIPT) which
transmits identical signals from the base station to both information receiver
(IR) and energy receiver (ER). We strive to enhance the power delivered to the
ER by fine-tuning the positions of transmit and receive fluid antennas, along
with optimizing the transmit covariance matrix, subject to a given minimum
signal-to-interference-plus-noise ratio (SINR) constraint at the IR. Simulation
results indicate that fluid antenna systems significantly enhance the energy
harvesting efficiency of the ER compared to traditional fixed-position
antennas.

</details>


### [52] [Stacked Intelligent Metasurfaces for 6G Wireless Networks: Principles, Applications, and Research Directions](https://arxiv.org/abs/2510.20572)
*Enyu Shi,Jiayi Zhang,Zhilong Liu,Ziheng Liu,Arumugam Nallanathan,Merouane Debbah,Shi Jin,Bo Ai*

Main category: cs.IT

TL;DR: 本文综述了基于堆叠智能超表面(SIM)的分布式无线网络，探讨其在6G网络中的应用场景、系统架构、信号处理挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要在高度动态环境中提供泛在连接、弹性覆盖和智能服务，分布式无线架构如无蜂窝大规模MIMO因其可扩展性和公平性受到关注。堆叠智能超表面作为可重构智能表面的演进，提供了增强的电磁域处理能力。

Method: 将SIM集成到分布式无线网络中，实现先进的波域操作，包括干扰管理、能效和频谱效率提升、物理层安全增强。文章提供了SIM辅助分布式无线网络的分类、系统架构分析，并讨论了分层框架、用户关联和联合预编码等关键信号处理挑战。

Result: 案例研究表明SIM辅助分布式无线网络能带来显著的性能提升，包括高效的干扰管理、改善的能源和频谱效率以及鲁棒的物理层安全性。

Conclusion: 未来研究方向包括硬件设计、能耗建模、算法开发和人工智能集成，旨在为可扩展和智能的6G分布式无线网络铺平道路。

Abstract: The sixth-generation (6G) wireless networks are expected to deliver
ubiquitous connectivity, resilient coverage, and intelligence-driven services
in highly dynamic environments. To achieve these goals, distributed wireless
architectures such as cell-free massive multiple-input multiple-output (MIMO)
have attracted significant attention due to their scalability and fairness.
Recently, stacked intelligent metasurfaces (SIMs) have emerged as a promising
evolution of reconfigurable intelligent surfaces, offering multi-layer
electromagnetic domain processing with enhanced controllability and spatial
degrees of freedom. By integrating SIMs into distributed wireless networks,
advanced wave-domain operations can be realized, enabling efficient
interference management, improved energy and spectral efficiency, and robust
physical-layer security. This article provides a comprehensive overview of
SIM-aided distributed wireless networks, including their application scenarios,
classification, and system architectures. Key signal processing challenges,
such as hierarchical frameworks, user association, and joint precoding, are
discussed, followed by case studies demonstrating significant performance
gains. Finally, future research directions in hardware design, energy
consumption modeling, algorithm development, and artificial intelligence
integration are highlighted, aiming to pave the way for scalable and
intelligent 6G distributed wireless networks.

</details>


### [53] [Super-Linear Growth of the Capacity-Achieving Input Support for the Amplitude-Constrained AWGN Channel](https://arxiv.org/abs/2510.20723)
*Haiyang Wang*

Main category: cs.IT

TL;DR: 本文研究了幅度受限AWGN信道容量达到输入分布的支撑点数量增长问题，证明了支撑点数量随幅度约束A的增加呈超线性增长。


<details>
  <summary>Details</summary>
Motivation: 虽然Smith(1971)已证明最优输入是有限个离散质量点的分布，但支撑点数量K(A)随幅度约束A增加的紧界仍为开放问题。

Method: 结合输出分布到均匀律的总变差收敛与高斯混合近似的定量限制，推导新的解析下界。

Result: 证明了K(A)随A的增长呈超线性增长，为支撑点数量增长提供了新的下界。

Conclusion: 本文为幅度受限AWGN信道容量达到输入分布的支撑点数量增长问题提供了新的理论下界，填补了该领域的重要空白。

Abstract: We study the growth of the support size of the capacity-achieving input
distribution for the amplitude-constrained additive white Gaussian noise (AWGN)
channel. While it is known since Smith (1971) that the optimal input is
discrete with finitely many mass points, tight bounds on the number of support
points $K(A)$ as the amplitude constraint $A$ increases remain open. Building
on recent work by Dytso \emph{et al.} (2019) and Mattingly \emph{et al.}
(2018), we derive a new analytical lower bound showing that $K(A)$ grows
super-linearly in $A$. Our approach combines total-variation convergence of the
output distribution to the uniform law with quantitative limits on Gaussian
mixture approximation.

</details>


### [54] [MIMO-Zak-OTFS with Superimposed Spread Pilots](https://arxiv.org/abs/2510.20734)
*Abhishek Bairwa,Ananthanarayanan Chockalingam*

Main category: cs.IT

TL;DR: 提出了一种用于MIMO-Zak-OTFS系统的叠加扩频导频设计和有效信道估计方法，通过在交叉模糊域分离导频序列，并采用turbo迭代来减轻导频-数据干扰。


<details>
  <summary>Details</summary>
Motivation: 在MIMO-Zak-OTFS系统中，数据和扩频导频信号叠加在同一帧中，需要设计有效的导频方案来分离不同发射天线的导频序列，以实现良好的信道估计性能。

Method: 提出在交叉模糊域分离导频序列的设计，通过简单的读取操作估计有效信道抽头，并采用信道估计和检测之间的turbo迭代来减轻导频-数据干扰。

Result: 在2×2和3×3 MIMO-Zak-OTFS系统中，使用高斯sinc脉冲成形滤波器和车载A信道模型的仿真结果表明，所提出的导频设计和估计方案经过三次turbo迭代后能实现非常好的估计/检测性能。

Conclusion: 所提出的叠加扩频导频设计和turbo迭代信道估计方案在MIMO-Zak-OTFS系统中能有效分离导频序列并实现优异的信道估计和检测性能。

Abstract: In this paper, we consider the problem of spread pilot design and effective
channel estimation in multiple-input multiple-output Zak-OTFS (MIMO-Zak-OTFS)
with superimposed spread pilots, where data and spread pilot signals are
superimposed in the same frame. To achieve good estimation performance in a
MIMO setting, the spread pilots at different transmit antennas need to be
effectively separated at the receiver. Towards this, we propose a spread pilot
design that separates the pilot sequences in the cross-ambiguity domain and
enables the estimation of the effective channel taps by a simple read-off
operation. To further alleviate the effect of pilot-data interference on
performance, we carry out turbo iterations between channel estimation and
detection. Simulation results for $2\times 2$ and $3\times 3$ MIMO-Zak-OTFS
with Gaussian-sinc pulse shaping filter for vehicular-A channel model show that
the proposed pilot design and estimation scheme with three turbo iterations can
achieve very good estimation/detection performance.

</details>
