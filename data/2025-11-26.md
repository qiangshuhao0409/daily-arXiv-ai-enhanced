<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.IT](#cs.IT) [Total: 14]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Layered Protocol Architecture for the Internet of Agents](https://arxiv.org/abs/2511.19699)
*Charles Fleming,Vijoy Pandey,Ramana Kompella,Luca Muscariello*

Main category: cs.NI

TL;DR: 提出在现有网络架构栈上增加两个新层(L8和L9)，以支持智能体间的语义化协作通信，解决LLM在上下文窗口和计算能力方面的限制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在上下文窗口限制，无法无限扩展内存和计算能力，需要智能体协作来解决复杂问题。现有网络架构是为主机间数据传输设计，缺乏对智能体语义协作的支持。

Method: 在OSI和TCP/IP栈基础上新增两个层：L8智能体通信层（标准化消息结构、言语行为、交互模式）和L9智能体语义协商层（建立共享上下文，定义概念、任务和参数）。

Result: 构建了支持智能体发现、协商和锁定共享上下文的通信框架，为分布式智能体协作提供基础。

Conclusion: L8和L9层为可扩展的分布式智能体协作提供了必要基础，将推动下一代多智能体系统的发展。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance improvements and the ability to learn domain-specific languages (DSLs), including APIs and tool interfaces. This capability has enabled the creation of AI agents that can perform preliminary computations and act through tool calling, now being standardized via protocols like MCP. However, LLMs face fundamental limitations: their context windows cannot grow indefinitely, constraining their memory and computational capacity. Agent collaboration emerges as essential for solving increasingly complex problems, mirroring how computational systems rely on different types of memory to scale. The "Internet of Agents" (IoA) represents the communication stack that enables agents to scale by distributing computation across collaborating entities.
  Current network architectural stacks (OSI and TCP/IP) were designed for data delivery between hosts and processes, not for agent collaboration with semantic understanding. To address this gap, we propose two new layers: an \textbf{Agent Communication Layer (L8)} and an \textbf{Agent Semantic Negotiation Layer (L9)}. L8 formalizes the \textit{structure} of communication, standardizing message envelopes, speech-act performatives (e.g., REQUEST, INFORM), and interaction patterns (e.g., request-reply, publish-subscribe), building on protocols like MCP. L9, which does not exist today, formalizes the \textit{meaning} of communication, enabling agents to discover, negotiate, and lock a "Shared Context" -- a formal schema defining the concepts, tasks, and parameters relevant to their interaction. Together, these layers provide the foundation for scalable, distributed agent collaboration, enabling the next generation of multi-agentic systems.

</details>


### [2] [Field Test of 5G New Radio (NR) UL-MIMO and UL-256QAM for HD Live-Streaming](https://arxiv.org/abs/2511.19868)
*Kasidis Arunruangsirilert*

Main category: cs.NI

TL;DR: 本文研究了5G网络中UL-MIMO和UL-256QAM对高清直播流媒体性能的实际影响，通过修改商用设备固件进行对比分析。


<details>
  <summary>Details</summary>
Motivation: 随着用户生成内容特别是高清直播视频的指数级增长，对移动网络上行能力提出了更高要求。虽然5G NR标准引入了UL-MIMO和UL-256QAM等上行增强功能，但这些功能对实时应用的实际影响尚未得到充分理解。

Method: 通过修改商用用户设备的调制解调器固件，选择性启用和禁用UL-MIMO和UL-256QAM功能，在相同设备上进行对比分析。使用RTMP协议在商用5G网络上评估高清直播性能，并分析5G射频参数来量化频谱效率影响。

Result: 评估了丢帧率和连接稳定性等关键指标，并基于CSI框架分析了CSI-RSRP、CSI-RSRQ和CSI-SINR等参数来量化频谱效率影响。

Conclusion: 该研究为理解5G上行增强功能在实际直播应用中的性能表现提供了实证分析，填补了理论数据速率与实际应用性能之间的认知空白。

Abstract: The exponential growth of User-Generated Content (UGC), especially High-Definition (HD) live video streaming, places a significant demand on the uplink capabilities of mobile networks. To address this, the 5G New Radio (NR) standard introduced key uplink enhancements, including Uplink Multi-Input Multi-Output (UL-MIMO) and Uplink 256QAM, to improve throughput and spectral efficiency. However, while the benefits of these features for raw data rates are well-documented, their practical impact on real-time applications like live-streaming is not yet well understood. This paper investigates the performance of UL-MIMO and UL-256QAM for HD live-streaming over a commercial 5G network using the Real-Time Messaging Protocol (RTMP). To ensure a fair assessment, we conduct a comparative analysis by modifying the modem firmware of commercial User Equipment (UE), allowing these features to be selectively enabled and disabled on the same device. Performance is evaluated based on key metrics, including dropped video frames and connection stability. Furthermore, this study analyzes 5G Radio Frequency (RF) parameters to quantify the spectral efficiency impact, specifically examining metrics derived from the Channel State Information (CSI) framework, including Reference Signal Received Power (CSI-RSRP), Reference Signal Received Quality (CSI-RSRQ), and Signal-to-Interference-plus-Noise Ratio (CSI-SINR).

</details>


### [3] [Performance Evaluation of Uplink 256QAM on Commercial 5G New Radio (NR) Networks](https://arxiv.org/abs/2511.19870)
*Kasidis Arunruangsirilert,Pasapong Wongprasert,Jiro Katto*

Main category: cs.NI

TL;DR: 评估5G SA网络中上行256QAM的实际性能表现，发现在被动天线网络中利用率低于20%，但在Massive MIMO部署中可达50%以上，平均提升8.22%吞吐量并降低7.97ms延迟。


<details>
  <summary>Details</summary>
Motivation: 尽管3GPP Release 14已引入上行256QAM，但由于RAN和UE厂商支持不足以及许可费用问题，运营商部署犹豫。需要评估其在商用5G网络中的实际价值。

Method: 通过修改调制解调器固件，在日本和泰国的商用5G SA网络中对比开启和关闭上行256QAM的性能，测试不同频段、移动特性和部署方案。

Result: 被动天线网络中256QAM利用率低于20%，平均吞吐量提升8.22%；Massive MIMO部署中利用率超过50%；在链路满载时可降低TCP延迟平均7.97ms。

Conclusion: 上行256QAM在Massive MIMO部署中表现更好，虽然吞吐量增益有限，但能显著降低延迟，对网络性能有积极影响。

Abstract: While Uplink 256QAM (UL-256QAM) has been introduced since 2016 as a part of 3GPP Release 14, the adoption was quite poor as many Radio Access Network (RAN) and User Equipment (UE) vendors didn't support this feature. With the introduction of 5G, the support of UL-256QAM has been greatly improved due to a big re-haul of RAN by Mobile Network Operators (MNOs). However, many RAN manufacturers charge MNOs for licenses to enable UL-256QAM per cell basis. This led to some MNOs hesitating to enable the feature on some of their gNodeB or cells to save cost.
  Since it's known that 256QAM modulation requires a very good channel condition to operate, but UE has a very limited transmission power budget. In this paper, 256QAM utilization, throughput and latency impact from enabling UL-256QAM will be evaluated on commercial 5G Standalone (SA) networks in two countries: Japan and Thailand on various frequency bands, mobility characteristics, and deployment schemes. By modifying the modem firmware, UL-256QAM can be turned off and compared to the conventional UL-64QAM. The results show that UL-256QAM utilization was less than 20% when deployed on a passive antenna network resulting in an average of 8.22% improvement in throughput. However, with Massive MIMO deployment, more than 50% utilization was possible on commercial networks. Furthermore, despite a small uplink throughput gain, enabling UL-256QAM can lower the latency when the link is fully loaded with an average improvement of 7.97 ms in TCP latency observed across various test cases with two TCP congestion control algorithms.

</details>


### [4] [Evaluations of High Power User Equipment (HPUE) in Urban Environment](https://arxiv.org/abs/2511.19871)
*Kasidis Arunruangsirilert,Pasapong Wongprasert,Jiro Katto*

Main category: cs.NI

TL;DR: 本文评估了5G TDD网络中高功率用户设备(HPUE)在城区商用网络中的性能表现，包括上行吞吐量、调制效率、重传率和功耗等指标，并在SA和NSA模式下进行对比。


<details>
  <summary>Details</summary>
Motivation: 5G TDD网络虽然下行吞吐量高，但由于高频段路径损耗大，上行性能受到影响，特别是在小区边缘区域。随着元宇宙、物联网等应用对上行性能要求的提高，3GPP引入HPUE来改善上行吞吐量和用户体验。

Method: 通过在商用5G网络上进行测试，评估HPUE在SA和NSA模式下的性能，包括上行吞吐量、调制效率、重传率和功耗等指标。通过修改调制解调器固件，比较不同功率等级和天线配置的性能。

Result: HPUE能够显著提升上行吞吐量、调制效率和可靠性，特别是在小区边缘区域，同时降低了重传率。不同功率等级和天线配置对性能有显著影响。

Conclusion: HPUE是提升5G TDD网络上行性能的有效技术，能够满足新兴应用对高上行吞吐量的需求，特别是在非理想网络条件下和小区边缘区域。

Abstract: While Time Division Duplexing (TDD) 5G New Radio (NR) networks offers higher downlink throughput due to the utilization of the middle frequency band, the uplink performance is negatively impacted due to higher path loss associated with higher frequencies, which degrade the users QoE in less optimal conditions. With the growing demand for high performance uplink throughput from novel applications such as Metaverse, Internet of Things (IoTs) and Smart City, 3GPP introduced High Power User Equipment (HPUE) on 5G TDD bands, allowing UEs to utilize more than 23 dBm of power for transmission to improve throughput, QoE, and reliability, especially at the cell edges. In this paper, the performance of HPUE is evaluated in the urban area on a commercial 5G network in terms of Uplink Throughput, Modulation Efficiency, Re-transmission Rate (ReTx Rate), and Power Consumption in both Standalone (SA) and Non-Standalone (NSA) modes. Through modem firmware modification, the performance is also compared across different power classes and antenna configurations.

</details>


### [5] [POMDP-Based Routing for DTNs with Partial Knowledge and Dependent Failures](https://arxiv.org/abs/2511.20241)
*Gregory F. Stock,Alexander Haberl,Juan A. Fraire,Holger Hermanns*

Main category: cs.NI

TL;DR: 该论文研究了在延迟容忍网络(DTN)中使用部分可观测马尔可夫决策过程(POMDP)进行路由决策，提出了依赖节点故障(DNF)模型来捕捉相关节点故障，并通过实验证明POMDP方法在不确定条件下能提高投递率和延迟性能。


<details>
  <summary>Details</summary>
Motivation: 传统MDP方法假设完全状态可观测性，但在分区DTN中，每个节点只能获得部分网络状态信息，这种假设不成立。因此需要研究POMDP在DTN路由不确定性下的作用。

Method: 引入依赖节点故障(DNF)模型，通过连续时间马尔可夫链(CTMC)建模可修复节点状态来捕捉相关节点故障。使用JuliaPOMDP实现模型，并通过DtnSim与DTN仿真集成。

Result: 评估表明，基于POMDP的路由在不确定条件下能提高投递率和延迟性能，同时保持可扩展性。

Conclusion: POMDP为未来DTN部署中的决策制定提供了有原则的基础，具有重要潜力。

Abstract: Routing in Delay-Tolerant Networks (DTNs) is inherently challenging due to sparse connectivity, long delays, and frequent disruptions. While Markov Decision Processes (MDPs) have been used to model uncertainty, they assume full state observability - an assumption that breaks down in partitioned DTNs, where each node operates with inherently partial knowledge of the network state. In this work, we investigate the role of Partially Observable Markov Decision Processes (POMDPs) for DTN routing under uncertainty. We introduce and evaluate a novel model: Dependent Node Failures (DNF), which captures correlated node failures via repairable node states modeled as Continuous-Time Markov Chains (CTMCs). We implement the model using JuliaPOMDP and integrate it with DTN simulations via DtnSim. Our evaluation demonstrates that POMDP-based routing yields improved delivery ratios and delay performance under uncertain conditions while maintaining scalability. These results highlight the potential of POMDPs as a principled foundation for decision-making in future DTN deployments.

</details>


### [6] [RIS-Assisted Downlink Pinching-Antenna Systems: GNN-Enabled Optimization Approaches](https://arxiv.org/abs/2511.20305)
*Changpeng He,Yang Lu,Yanqing Xu,Chong-Yung Chi,Bo Ai,Arumugam Nallanathan*

Main category: cs.NI

TL;DR: 本文研究RIS辅助的多波导夹持天线系统用于多用户下行信息传输，提出了一种新颖的三阶段图神经网络来联合优化天线位置、RIS相位和波束成形向量。


<details>
  <summary>Details</summary>
Motivation: 研究新兴的PASS系统与RIS集成对无线通信的未知影响，探索如何通过智能优化提升系统性能。

Method: 采用图结构拓扑表示RIS辅助的PASS系统，提出三阶段图神经网络：第一阶段学习基于用户位置的PA位置，第二阶段根据复合信道条件学习RIS相位偏移，第三阶段确定波束成形向量。通过无监督训练和三种实现策略与凸优化集成。

Result: 数值结果验证了所提GNN的有效性，展示了其良好的泛化能力、性能可靠性和实时适用性，并分析了关键参数对RIS辅助PASS系统的影响。

Conclusion: 提出的三阶段GNN方法能够有效优化RIS辅助PASS系统，在推理时间和解的最优性之间提供权衡，为智能无线通信系统设计提供了新思路。

Abstract: This paper investigates a reconfigurable intelligent surface (RIS)-assisted multi-waveguide pinching-antenna (PA) system (PASS) for multi-user downlink information transmission, motivated by the unknown impact of the integration of emerging PASS and RIS on wireless communications. First, we formulate sum rate (SR) and energy efficiency (EE) maximization problems in a unified framework, subject to constraints on the movable region of PAs, total power budget, and tunable phase of RIS elements. Then, by leveraging a graph-structured topology of the RIS-assisted PASS, a novel three-stage graph neural network (GNN) is proposed, which learns PA positions based on user locations, and RIS phase shifts according to composite channel conditions at the first two stages, respectively, and finally determines beamforming vectors. Specifically, the proposed GNN is achieved through unsupervised training, together with three implementation strategies for its integration with convex optimization, thus offering trade-offs between inference time and solution optimality. Extensive numerical results are provided to validate the effectiveness of the proposed GNN, and to support its unique attributes of viable generalization capability, good performance reliability, and real-time applicability. Moreover, the impact of key parameters on RIS-assisted PASS is illustrated and analyzed.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder](https://arxiv.org/abs/2511.19577)
*Abhay Goyal,Navin Kumar,Kimberly DiMeola,Rafael Trujillo,Soorya Ram Shimgekar,Christian Poellabauer,Pi Zonooz,Ermonda Gjoni-Markaj,Declan Barry,Lynn Madden*

Main category: cs.AI

TL;DR: 本研究探讨使用可穿戴设备和AI方法预测慢性疼痛和鸦片使用障碍患者的疼痛峰值，发现机器学习模型表现良好但大语言模型效果有限。


<details>
  <summary>Details</summary>
Motivation: 慢性疼痛和鸦片使用障碍是相互关联的常见疾病，目前缺乏基于证据的综合治疗方法。可穿戴设备有潜力监测复杂患者信息，但大语言模型在此领域的应用尚未探索。

Method: 使用可穿戴设备收集数据，采用多种AI方法（包括机器学习和LLMs）分析疼痛峰值及其临床相关性。

Result: 机器学习模型在预测疼痛峰值方面达到较高准确率（>0.7），但LLMs在提供疼痛峰值洞察方面表现有限。

Conclusion: 可穿戴设备实时监测结合先进AI模型可促进疼痛峰值的早期检测，支持个性化干预。鉴于LLMs整体表现有限，需要开发能提供可操作洞察的LLMs。

Abstract: Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.

</details>


### [8] [Fara-7B: An Efficient Agentic Model for Computer Use](https://arxiv.org/abs/2511.19663)
*Ahmed Awadallah,Yash Lara,Raghav Magazine,Hussein Mozannar,Akshay Nambi,Yash Pandya,Aravind Rajeswaran,Corby Rosset,Alexey Taymanov,Vibhav Vineet,Spencer Whitehead,Andrew Zhao*

Main category: cs.AI

TL;DR: FaraGen是一个用于多步骤网页任务的合成数据生成系统，能够生成多样化的任务和解决方案，并以低成本产生验证轨迹。基于这些数据训练的Fara-7B模型在多个基准测试中表现出色，甚至能与更大的前沿模型竞争。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理（CUAs）的发展受到缺乏大规模高质量人类与计算机交互数据集的限制，而LLMs的成功依赖于丰富的文本数据，因此需要为CUA轨迹创建类似的数据集。

Method: 开发FaraGen系统：从常用网站生成多样化任务，产生多个解决方案尝试，并使用多个验证器筛选成功轨迹。利用这些数据训练Fara-7B模型，该模型仅通过截图感知计算机，通过预测坐标执行动作，且足够小可以在设备上运行。

Result: FaraGen以约1美元的成本产生验证轨迹，具有高吞吐量、产量和多样性。Fara-7B在WebVoyager、Online-Mind2Web和WebTailBench等基准测试中优于同类规模的CUA模型，并与更大的前沿模型竞争。

Conclusion: 可扩展的数据生成系统在推进小型高效代理模型方面具有关键优势。Fara-7B作为开源模型发布，同时发布了WebTailBench基准测试。

Abstract: Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.

</details>


### [9] [HeaRT: A Hierarchical Circuit Reasoning Tree-Based Agentic Framework for AMS Design Optimization](https://arxiv.org/abs/2511.19669)
*Souradip Poddar,Chia-Tung Ho,Ziming Wei,Weidong Cao,Haoxing Ren,David Z. Pan*

Main category: cs.AI

TL;DR: HeaRT是一个基础推理引擎，用于自动化设计循环，在电路设计优化中实现>97%的推理准确率和>98%的Pass@1性能，同时比SOTA基线节省50%以上的计算资源。


<details>
  <summary>Details</summary>
Motivation: 传统AI驱动的AMS设计自动化算法受限于对高质量数据集的依赖、跨架构迁移性差以及缺乏自适应机制，需要更智能、自适应、类人风格的设计优化方法。

Method: 提出HeaRT基础推理引擎，作为自动化循环的核心组件，实现智能自适应设计优化。

Result: 在40电路基准测试中，HeaRT推理准确率>97%，Pass@1性能>98%，即使电路复杂度增加也能保持性能，计算资源消耗仅为SOTA基线的<0.5倍。在尺寸和拓扑设计适应任务中，收敛速度提高3倍以上。

Conclusion: HeaRT是实现智能自适应电路设计优化的有效方法，能够显著提高设计效率并保持原有设计意图。

Abstract: Conventional AI-driven AMS design automation algorithms remain constrained by their reliance on high-quality datasets to capture underlying circuit behavior, coupled with poor transferability across architectures, and a lack of adaptive mechanisms. This work proposes HeaRT, a foundational reasoning engine for automation loops and a first step toward intelligent, adaptive, human-style design optimization. HeaRT consistently demonstrates reasoning accuracy >97% and Pass@1 performance >98% across our 40-circuit benchmark repository, even as circuit complexity increases, while operating at <0.5x real-time token budget of SOTA baselines. Our experiments show that HeaRT yields >3x faster convergence in both sizing and topology design adaptation tasks across diverse optimization approaches, while preserving prior design intent.

</details>


### [10] [FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking](https://arxiv.org/abs/2511.19671)
*Rishab Sharma,Iman Saberi,Elham Alipour,Jie JW Wu,Fatemeh Fard*

Main category: cs.AI

TL;DR: 提出了FISCAL框架用于生成金融事实核查的合成数据，并训练了轻量级验证器MiniCheck-FISCAL，在多个金融数据集上表现出色，接近大模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前金融应用中大型语言模型存在事实可靠性差和计算效率低的问题，需要既能保证准确性又轻量化的解决方案。

Method: 使用FISCAL框架生成合成金融数据FISCAL-data，并基于此训练轻量级验证器MiniCheck-FISCAL进行金融数值声明验证。

Result: MiniCheck-FISCAL超越基线模型和GPT-3.5 Turbo，接近Mixtral-8x22B等大模型（20倍大小）的准确性，在外部数据集上媲美GPT-4o和Claude-3.5。

Conclusion: 领域特定的合成数据结合高效微调，可使紧凑模型在金融AI应用中实现最先进的准确性、鲁棒性和可扩展性。

Abstract: Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).

</details>


### [11] [Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions](https://arxiv.org/abs/2511.19749)
*Farzan Karimi-Malekabadi,Pooya Razavi,Sonya Powers*

Main category: cs.AI

TL;DR: 研究表明大型语言模型（特别是GPT-4o-mini）能够以83-94%的准确率识别教育评估项目与内容标准的对齐状态，显著减少人工审核负担，建议采用LLM筛选与人工审核相结合的混合流程。


<details>
  <summary>Details</summary>
Motivation: 传统的人工对齐审核准确但耗时费力，特别是在大规模项目库中。研究旨在探索LLM能否在不牺牲准确性的前提下加速这一过程。

Method: 使用12,000多个K-5年级项目-技能对，测试了三种LLM（GPT-3.5 Turbo、GPT-4o-mini和GPT-4o）在三个任务上的表现：识别不对齐项目、从完整标准集中选择正确技能、在分类前缩小候选列表。

Result: GPT-4o-mini在识别对齐状态方面达到83-94%的准确率；数学表现强劲但阅读较低；预过滤候选技能使正确技能出现在前五建议中的概率超过95%。

Conclusion: LLM特别是结合候选过滤策略，能显著减少项目审核的人工负担同时保持对齐准确性，建议开发LLM筛选与人工审核的混合流程作为可扩展解决方案。

Abstract: As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.

</details>


### [12] [Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs](https://arxiv.org/abs/2511.19773)
*Meng Lu,Ran Xu,Yi Fang,Wenxuan Zhang,Yue Yu,Gaurav Srivastava,Yuchen Zhuang,Mohamed Elhoseiny,Charles Fleming,Carl Yang,Zhengzhong Tu,Yang Xie,Guanghua Xiao,Hanrui Wang,Di Jin,Wenqi Shi,Xuan Wang*

Main category: cs.AI

TL;DR: VISTA-Gym是一个可扩展的训练环境，旨在提升视觉语言模型的工具集成视觉推理能力，通过统一多模态推理任务、标准化视觉工具接口和执行交互循环来实现视觉代理强化学习。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在图像理解方面表现出色，但在多步骤视觉交互推理方面能力有限，特别是在工具选择、调用和协调方面存在困难。

Method: 开发VISTA-Gym环境，统一7个任务13个数据集的多模态推理任务，提供标准化视觉工具接口、可执行交互循环和可验证反馈信号。通过多轮轨迹采样和端到端强化学习训练VISTA-R1模型。

Result: 在11个公共推理密集型VQA基准测试中，VISTA-R1-8B模型比同规模的最先进基线模型性能提升9.51%-18.72%。

Conclusion: VISTA-Gym是一个有效的训练平台，能够解锁视觉语言模型的工具集成推理能力。

Abstract: While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.

</details>


### [13] [NOEM$^{3}$A: A Neuro-Symbolic Ontology-Enhanced Method for Multi-Intent Understanding in Mobile Agents](https://arxiv.org/abs/2511.19780)
*Ioannis Tzachristas,Aifen Sui*

Main category: cs.AI

TL;DR: 提出了一种神经符号框架，通过将结构化意图本体与紧凑语言模型集成，实现移动AI代理的多意图理解。该方法利用检索增强提示、logit偏置和可选分类头，将符号意图结构注入输入和输出表示中。


<details>
  <summary>Details</summary>
Motivation: 解决移动AI代理中多意图理解的问题，通过在紧凑语言模型中注入符号意图结构，实现在有限计算资源下的准确意图理解。

Method: 集成结构化意图本体与紧凑语言模型，使用检索增强提示、logit偏置和可选分类头来注入符号意图结构。

Result: 在MultiWOZ 2.3的模糊/复杂对话子集上，3B参数的Llama模型通过本体增强接近GPT-4的准确率（85% vs 90%），同时显著降低能耗和内存占用。

Conclusion: 符号对齐是实现在设备上准确高效自然语言理解的有效策略，本体增强模型能产生更接地气、更清晰的多意图解释。

Abstract: We introduce a neuro-symbolic framework for multi-intent understanding in mobile AI agents by integrating a structured intent ontology with compact language models. Our method leverages retrieval-augmented prompting, logit biasing and optional classification heads to inject symbolic intent structure into both input and output representations. We formalize a new evaluation metric-Semantic Intent Similarity (SIS)-based on hierarchical ontology depth, capturing semantic proximity even when predicted intents differ lexically. Experiments on a subset of ambiguous/demanding dialogues of MultiWOZ 2.3 (with oracle labels from GPT-o3) demonstrate that a 3B Llama model with ontology augmentation approaches GPT-4 accuracy (85% vs 90%) at a tiny fraction of the energy and memory footprint. Qualitative comparisons show that ontology-augmented models produce more grounded, disambiguated multi-intent interpretations. Our results validate symbolic alignment as an effective strategy for enabling accurate and efficient on-device NLU.

</details>


### [14] [KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)](https://arxiv.org/abs/2511.19798)
*Weizhi Liu,Xi Chen,Zekun Jiang,Liang Zhao,Kunyuan Jiang,Ruisi Tang,Li Wang,Mingke You,Hanyu Zhou,Hongyu Chen,Qiankun Xiong,Yong Nie,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: KOM是一个用于膝骨关节炎管理的多智能体系统，能够自动化评估、风险预测和治疗处方，在临床工作流程中显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 膝骨关节炎影响全球6亿多人，个性化多学科干预虽然有效但资源需求大，难以在资源有限环境中实施。

Method: 开发KOM多智能体系统，自动化KOA评估、风险预测和治疗处方，支持基于患者个体特征生成定制管理计划。

Result: 在基准实验中，KOM在影像分析和处方生成方面优于通用大语言模型；随机三臂模拟研究显示，KOM与临床医生合作使诊断和规划时间减少38.5%，治疗质量提高。

Conclusion: KOM有助于实现KOA管理的自动化，其模块化架构为开发其他慢性病的AI辅助管理系统提供了宝贵见解。

Abstract: Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.

</details>


### [15] [A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization](https://arxiv.org/abs/2511.19829)
*Ke Chen,Yifeng Wang,Hassan Almosapeeh,Haohan Wang*

Main category: cs.AI

TL;DR: 提出了一种基于评估指导的提示优化方法，通过建立系统化的提示评估框架和执行无关的评估器来指导查询相关的提示重写，实现稳定、可解释的跨模型改进。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法主要优化静态模板，在复杂动态场景中效果有限；查询相关方法依赖不稳定的文本反馈或黑盒奖励模型，提供弱且不可解释的优化信号；提示质量本身缺乏统一系统定义。

Method: 首先建立面向性能的系统化提示评估框架，开发并微调执行无关的评估器直接预测多维质量分数，然后让评估器指导度量感知的优化器以可解释、查询相关的方式诊断失败模式并重写提示。

Result: 评估器在预测提示性能方面达到最强准确度，评估指导的优化在8个数据集和3个骨干模型上持续超越静态模板和查询相关基线方法。

Conclusion: 提出了统一、基于度量的提示质量视角，证明了评估指导的优化管道能够在多样化任务中提供稳定、可解释且模型无关的改进。

Abstract: Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.

</details>


### [16] [Reinforcement Learning with $ω$-Regular Objectives and Constraints](https://arxiv.org/abs/2511.19849)
*Dominik Wagner,Leon Witzman,Luke Ong*

Main category: cs.AI

TL;DR: 提出了一种结合ω-正则目标与显式约束的强化学习方法，通过线性规划算法在满足约束条件下最大化目标达成概率


<details>
  <summary>Details</summary>
Motivation: 传统强化学习的标量奖励表达能力有限，容易导致奖励破解，且单一性能指标掩盖了安全与性能之间的权衡

Method: 基于线性规划的模型强化学习算法，将ω-正则目标与约束分开处理，建立到约束极限平均问题的转换

Result: 算法在极限情况下能产生在指定阈值内满足ω-正则约束的同时最大化ω-正则目标达成概率的策略

Conclusion: 该方法能同时解决强化学习中的表达能力和安全-性能权衡问题，具有最优性保持保证

Abstract: Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.
  We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.

</details>


### [17] [MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support](https://arxiv.org/abs/2511.19864)
*Valerie Lockhart,Dan McCreary,Troy A. Peterson*

Main category: cs.AI

TL;DR: MicroSims是一个用于快速生成轻量级交互式教育模拟的AI框架，具有标准化设计模式、iframe架构和可修改代码，解决了传统模拟开发成本高、技术复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 传统教育模拟开发需要大量资源和技术专长，阻碍了广泛应用。MicroSims旨在通过AI辅助生成、通用嵌入和无编程定制，降低教育模拟的创建门槛。

Method: 采用标准化设计模式支持AI生成，iframe架构实现通用嵌入和安全沙箱，透明可修改代码支持定制和教学透明度，包含设计原则、技术架构、元数据标准和开发流程。

Result: 基于物理教育研究和STEM元分析，交互模拟可将概念理解提升30-40%。MicroSims在保持这些益处的同时解决了成本、技术复杂性和平台依赖等障碍。

Conclusion: MicroSims框架对教育公平有重要意义，能够支持全球教育工作者按需创建定制化、与课程对齐的模拟，并为基于AI的自适应学习系统奠定基础。

Abstract: Educational simulations have long been recognized as powerful tools for enhancing learning outcomes, yet their creation has traditionally required substantial resources and technical expertise. This paper introduces MicroSims a novel framework for creating lightweight, interactive educational simulations that can be rapidly generated using artificial intelligence, universally embedded across digital learning platforms, and easily customized without programming knowledge. MicroSims occupy a unique position at the intersection of three key innovations: (1) standardized design patterns that enable AI-assisted generation, (2) iframe-based architecture that provides universal embedding and sandboxed security, and (3) transparent, modifiable code that supports customization and pedagogical transparency. We present a comprehensive framework encompassing design principles, technical architecture, metadata standards, and development workflows. Drawing on empirical research from physics education studies and meta-analyses across STEM disciplines, we demonstrate that interactive simulations can improve conceptual understanding by up to 30-40\% compared to traditional instruction. MicroSims extend these benefits while addressing persistent barriers of cost, technical complexity, and platform dependence. This work has significant implications for educational equity, and low-cost intelligent interactive textbooks that enabling educators worldwide to create customized, curriculum-aligned simulations on demand. We discuss implementation considerations, present evidence of effectiveness, and outline future directions for AI-powered adaptive learning systems built on the MicroSim foundation.

</details>


### [18] [Agentic AI-Empowered Conversational Embodied Intelligence Networks in 6G](https://arxiv.org/abs/2511.19865)
*Mingkai Chen,Zijie Feng,Lei Wang,Yaser Khamayseh*

Main category: cs.AI

TL;DR: 提出了协作对话具身智能网络（CC-EIN），通过多模态特征融合、自适应语义通信、任务协调和可解释性模块，解决6G时代多具身智能设备在复杂任务执行中的协作挑战。


<details>
  <summary>Details</summary>
Motivation: 在6G时代，多具身智能设备（MEIDs）的语义协作对复杂任务执行至关重要。现有系统在多模态信息融合、自适应通信和决策可解释性方面面临挑战。

Method: CC-EIN包含四个核心模块：PerceptiNet进行图像和雷达数据的跨模态融合生成统一语义表示；自适应语义通信策略根据任务紧急性和信道质量动态调整编码方案和传输功率；语义驱动协作机制支持任务分解和异构设备间的无冲突协调；InDec模块通过Grad-CAM可视化增强决策透明度。

Result: 在地震后救援场景的仿真中，CC-EIN实现了95.4%的任务完成率和95%的传输效率，同时保持了强大的语义一致性和能源效率。

Conclusion: CC-EIN有效解决了多具身智能设备协作中的关键挑战，为6G时代的智能协作系统提供了可行的解决方案。

Abstract: In the 6G era, semantic collaboration among multiple embodied intelligent devices (MEIDs) becomes crucial for complex task execution. However, existing systems face challenges in multimodal information fusion, adaptive communication, and decision interpretability. To address these limitations, we propose a collaborative Conversational Embodied Intelligence Network (CC-EIN) integrating multimodal feature fusion, adaptive semantic communication, task coordination, and interpretability. PerceptiNet performs cross-modal fusion of image and radar data to generate unified semantic representations. An adaptive semantic communication strategy dynamically adjusts coding schemes and transmission power according to task urgency and channel quality. A semantic-driven collaboration mechanism further supports task decomposition and conflict-free coordination among heterogeneous devices. Finally, the InDec module enhances decision transparency through Grad-CAM visualization. Simulation results in post-earthquake rescue scenarios demonstrate that CC-EIN achieves 95.4% task completion rate and 95% transmission efficiency while maintaining strong semantic consistency and energy efficiency.

</details>


### [19] [Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy](https://arxiv.org/abs/2511.19872)
*Daniel I Jackson,Emma L Jensen,Syed-Amad Hussain,Emre Sezgin*

Main category: cs.AI

TL;DR: LLMs在自我效能感评估中表现出稳定的回答模式，但自我评估与实际能力不匹配，高自信模型可能表现差，低自信模型反而准确。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型的自我评估能力，因为自我评估是可靠智能的关键方面，而现有评估主要关注任务准确性。

Method: 使用10项一般自我效能感量表(GSES)在四种条件下测试十个LLM：无任务、计算推理、社会推理和摘要任务，并分析回答的稳定性和准确性。

Result: 模型在不同条件下表现出显著不同的自我效能水平，总体得分低于人类标准。所有模型在计算和社会问题上都达到完美准确率，但摘要表现差异很大。自我评估不能可靠反映实际能力。

Conclusion: 心理测量提示提供了对LLM沟通行为的结构化洞察，但不能提供校准的性能估计，自我评估与实际能力存在脱节。

Abstract: Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.

</details>


### [20] [RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2511.19895)
*Yuanyuan Lin,Xiangyu Ouyang,Teng Zhang,Kaixin Sui*

Main category: cs.AI

TL;DR: RPM-MCTS是一种基于蒙特卡洛树搜索的代码生成方法，通过知识检索作为过程奖励模型来评估中间算法步骤，无需复杂训练，同时利用沙箱执行反馈定位和修正错误步骤，在减少15%token消耗的同时提升代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于树搜索的代码生成方法难以有效评估中间算法步骤，无法及时定位和修正错误步骤，导致生成错误代码且计算成本增加。

Method: 提出RPM-MCTS方法：1) 使用知识检索作为过程奖励模型评估中间步骤；2) 在扩展阶段采用相似性过滤去除冗余节点；3) 利用沙箱执行反馈定位错误步骤并进行针对性修正。

Result: 在四个公开代码生成基准测试中，RPM-MCTS优于当前最先进方法，同时实现约15%的token消耗减少。使用RPM-MCTS构建的数据对基础模型进行全微调可显著提升其代码能力。

Conclusion: RPM-MCTS通过知识检索和沙箱反馈机制有效解决了代码生成中中间步骤评估和错误修正的问题，在提升性能的同时降低了计算成本。

Abstract: Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.

</details>


### [21] [Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity](https://arxiv.org/abs/2511.19925)
*Qiyao Wei,Edward Morrell,Lea Goetz,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 本文提出了一种利用知识图谱生成基准数据集的新方法，用于评估LLM输出的语义相似性方法，解决了现有基准依赖人工标注、成本高、领域适用性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM文本输出的语义相似性方法存在缺陷，可能更关注句法而非语义内容，且现有基准依赖主观人工判断、生成成本高、领域适用性有限、等价定义不明确。

Method: 利用知识图谱生成语义相似或不相似的自然语言陈述对，其中不相似对分为四种子类型。在四个不同领域生成基准数据集，并比较传统NLP评分和LLM作为评判者的语义相似性方法。

Result: 语义变化的子类型和基准领域都会影响语义相似性方法的性能，没有一种方法始终表现最优。LLM作为评判者在检测文本语义内容方面存在重要影响。

Conclusion: 该方法为评估LLM输出的语义相似性提供了有效的基准生成方案，揭示了不同语义相似性方法的性能差异和局限性，特别是LLM作为评判者的应用效果。

Abstract: Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.

</details>


### [22] [A System-Level Taxonomy of Failure Modes in Large Language Model Applications](https://arxiv.org/abs/2511.19933)
*Vaishali Vinay*

Main category: cs.AI

TL;DR: 本文提出了大语言模型在真实应用中的15种隐藏故障模式分类法，分析了现有评估方法的不足，并提供了构建可靠LLM系统的设计原则。


<details>
  <summary>Details</summary>
Motivation: 大语言模型被快速集成到决策支持工具和自动化工作流中，但其在生产环境中的行为仍未被充分理解，且故障模式与传统机器学习模型有根本差异。

Method: 提出系统级的故障模式分类法，分析评估与监控实践的差距，考察部署挑战，并制定设计原则。

Result: 识别了15种隐藏故障模式，包括多步推理漂移、潜在不一致性、上下文边界退化等，揭示了现有基准测试的局限性。

Conclusion: 通过将LLM可靠性框架化为系统工程问题而非纯模型中心问题，为未来评估方法、AI系统鲁棒性和可靠LLM部署研究提供了分析基础。

Abstract: Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.

</details>


### [23] [M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2511.19969)
*Weizi Shao,Taolin Zhang,Zijie Zhou,Chen Chen,Chengyu Wang,Xiaofeng He*

Main category: cs.AI

TL;DR: 提出了M^3Prune框架，通过修剪多模态多代理层次通信图中的冗余边，在保持任务性能的同时显著降低token开销。


<details>
  <summary>Details</summary>
Motivation: 现有的多代理系统虽然性能优异，但存在显著的token开销和计算成本问题，限制了大规模部署。

Method: 采用分层通信图修剪方法：首先进行模态内图稀疏化识别关键边，然后构建动态通信拓扑进行模态间图稀疏化，最后逐步修剪冗余边。

Result: 在通用和领域特定的mRAG基准测试中，该方法持续优于单代理和鲁棒多代理系统，同时显著减少token消耗。

Conclusion: M^3Prune框架有效解决了多代理系统的效率问题，实现了任务性能与token开销之间的最佳平衡。

Abstract: Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.

</details>


### [24] [Reducing Latency of LLM Search Agent via Speculation-based Algorithm-System Co-Design](https://arxiv.org/abs/2511.20048)
*Zixiao Huang,Wen Zeng,Tianyu Fu,Tengxuan Liu,Yizhou Sun,Ke Hong,Xinhao Yang,Chengchun Liu,Yan Li,Quanlu Zhang,Guohao Dai,Zhenhua Zhu,Yu Wang*

Main category: cs.AI

TL;DR: SPAgent通过推测执行机制减少LLM搜索代理的延迟，采用算法-系统协同设计，在保持准确性的同时实现1.65倍加速


<details>
  <summary>Details</summary>
Motivation: 现有LLM搜索代理存在严重延迟问题，因为每个步骤都需要串行推理和工具执行。传统推测执行范式虽然能打破串行执行，但收益有限

Method: 提出SPAgent框架：算法层面引入两阶段自适应推测机制，在安全时选择性省略验证；系统层面使用两级调度器根据引擎负载调节推测请求

Result: 在广泛实验设置中，SPAgent实现了最高1.65倍的端到端加速，同时保持相同甚至更高的准确性

Conclusion: SPAgent能够显著降低多步骤搜索代理的延迟，使其具备实际部署的可行性

Abstract: LLM-based search agents achieve strong performance but suffer from severe latency, as each step requires serialized LLM reasoning followed by action of tool execution. We revisit this bottleneck through the lens of speculation. While traditional predict-verify speculation paradigm can break serial execution, its benefit remains limited, as it retains the full original workload and adds extra inference overhead. We observe that early agent steps often involve simple evidence-gathering, where correct actions can often be predicted without full reasoning. Building on these observations, we present SPAgent, an algorithm-system co-design framework that expands the role of speculation in search agents to reduce latency. Algorithmically, SPAgent introduces a two-phase adaptive speculation mechanism that selectively omits verification when safe. System-wise, a two-level scheduler regulates speculative requests based on engine load to ensure speculation remains beneficial. We implement SPAgent in real-world systems. Across extensive experimental settings, SPAgent achieves up to $1.65\times$ end-to-end speedup while maintaining same or even achieving higher accuracy, enabling practical deployment of multi-step search agents.

</details>


### [25] ["Are We Done Yet?": A Vision-Based Judge for Autonomous Task Completion of Computer Use Agents](https://arxiv.org/abs/2511.20067)
*Marta Sumyk,Oleksandr Kosovan*

Main category: cs.AI

TL;DR: 提出了一个基于视觉语言模型的自主评估框架，通过屏幕截图和任务描述直接评估任务完成情况，显著提高了计算机使用代理的任务成功率和自我纠正能力。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理在自主操作数字界面时，往往难以可靠地判断给定任务是否已完成，需要有效的评估和反馈机制来提高其可靠性。

Method: 使用视觉语言模型从屏幕截图和任务描述中直接评估任务完成情况，构建了包含42个macOS应用程序和1,260个人工标注任务的数据集。

Result: 框架在任务成功检测中达到73%的准确率，当应用评估器反馈时，整体任务成功率平均相对提高了27%。

Conclusion: 基于视觉的评估可以作为有效的反馈机制，显著提高自主计算机使用代理的可靠性和自我纠正能力。

Abstract: Computer Use Agents (CUAs) are designed to autonomously operate digital interfaces, yet they often fail to reliably determine whether a given task has been completed. We present an autonomous evaluation and feedback framework that uses vision-language models to assess task completion directly from screenshots and task descriptions. Our dataset covers 42 built-in macOS applications and 1,260 human-labeled tasks across a wide range of scenarios. Our framework achieves up to 73 percent accuracy in task success detection and yields an average relative improvement of 27 percent in overall task success when evaluator feedback is applied. These results show that vision-based evaluation can serve as an effective feedback mechanism that improves the reliability and self-correction of autonomous computer-use agents.

</details>


### [26] [VICoT-Agent: A Vision-Interleaved Chain-of-Thought Framework for Interpretable Multimodal Reasoning and Scalable Remote Sensing Analysis](https://arxiv.org/abs/2511.20085)
*Chujie Wang,Zhiyuan Luo,Ruiqi Liu,Can Ran,Shenghua Fan,Xi Chen,Chu He*

Main category: cs.AI

TL;DR: 提出了VICoT多模态代理框架，通过动态整合视觉工具到思维链中实现显式多轮推理，在遥感图像分析任务中显著优于现有SOTA框架。


<details>
  <summary>Details</summary>
Motivation: 遥感图像分析任务正从传统目标识别向复杂智能推理演进，对模型推理能力和工具调用灵活性提出更高要求。

Method: 采用基于堆栈的推理结构和模块化MCP兼容工具套件，实现多轮交错视觉语言推理；提出推理堆栈蒸馏方法将复杂代理行为迁移到轻量模型。

Result: 在多个遥感基准测试中，VICoT在推理透明度、执行效率和生成质量方面显著优于现有SOTA框架。

Conclusion: VICoT框架通过显式多轮推理和工具动态整合，有效提升了遥感图像分析的推理能力和灵活性，同时通过蒸馏方法实现了轻量化部署。

Abstract: The current remote sensing image analysis task is increasingly evolving from traditional object recognition to complex intelligence reasoning, which places higher requirements on the model's reasoning ability and the flexibility of tool invocation. To this end, we propose a new multimodal agent framework, Vision-Interleaved Chain-of-Thought Framework (VICoT), which implements explicit multi-round reasoning by dynamically incorporating visual tools into the chain of thought. Through a stack-based reasoning structure and a modular MCP-compatible tool suite, VICoT enables LLMs to efficiently perform multi-round, interleaved vision-language reasoning tasks with strong generalization and flexibility.We also propose the Reasoning Stack distillation method to migrate complex Agent behaviors to small, lightweight models, which ensures the reasoning capability while significantly reducing complexity. Experiments on multiple remote sensing benchmarks demonstrate that VICoT significantly outperforms existing SOTA frameworks in reasoning transparency, execution efficiency, and generation quality.

</details>


### [27] [From data to concepts via wiring diagrams](https://arxiv.org/abs/2511.20138)
*Jason Lo,Mohammadnima Jafari*

Main category: cs.AI

TL;DR: 本文提出了准骨架布线图的概念，证明了其与Hasse图的对应关系，并设计了从序列数据中提取布线图的算法，成功应用于自主代理在电脑游戏中的行为分析。


<details>
  <summary>Details</summary>
Motivation: 布线图是表示抽象概念（如时序过程）的标记有向图，需要开发能够从序列数据中自动提取布线图的算法，以分析复杂系统的行为模式。

Method: 引入准骨架布线图概念，证明其与Hasse图的对应关系，设计基于此的布线图提取算法，并与DBSCAN和凝聚层次聚类等标准聚类技术进行比较。

Result: 算法成功识别了自主代理在电脑游戏中的获胜策略，在数据扰动情况下仍表现良好，优于传统聚类方法。

Conclusion: 该研究将范畴论、图论、聚类、强化学习和数据工程技术相结合，为从序列数据中提取结构化表示提供了有效方法。

Abstract: A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.

</details>


### [28] [Towards Benign Memory Forgetting for Selective Multimodal Large Language Model Unlearning](https://arxiv.org/abs/2511.20196)
*Zhen Zeng,Leijiang Gu,Zhangling Duan,Feng Li,Zenglin Shi,Cees G. M. Snoek,Meng Wang*

Main category: cs.AI

TL;DR: 提出了SMFA方法，通过记忆遗忘适配器和保留锚点引导的掩码机制，实现多模态大语言模型对隐私敏感信息的精确可控遗忘，同时保持模型的通用图像理解能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型可能会无意中记忆隐私敏感信息，现有遗忘方法在移除这些知识时往往会损害模型的通用图像理解性能。

Method: SMFA首先微调模型将敏感响应替换为拒绝回答，生成记忆遗忘适配器，然后应用保留锚点引导的掩码机制来防止对无关知识和理解能力的干扰。

Result: 实验表明，与现有方法不同，SMFA能够实现精确可控的遗忘，同时保持模型的基础图像理解能力。

Conclusion: SMFA方法能够有效解决多模态大语言模型隐私敏感信息遗忘问题，在移除敏感知识的同时保持模型的通用视觉理解能力。

Abstract: Multimodal Large Language Models (MLLMs) achieve remarkable capabilities but can inadvertently memorize privacy-sensitive information. Although existing unlearning methods can remove such knowledge, they fail to achieve benign forgetting because they often degrade the model's general image understanding performance. To address this, we propose the Sculpted Memory Forgetting Adapter (SMFA), which confines forgetting to targeted memory regions while preserving overall capabilities. SMFA first fine-tunes the model to replace sensitive responses with refusals, yielding a memory forgetting adapter, and then applies a retaining anchor-guided masking mechanism to prevent interference with unrelated knowledge and understanding ability. To systematically evaluate selective MLLM unlearning, we introduce S-MLLMUn Bench, the first benchmark designed to jointly assess the removal of sensitive knowledge and retention of general visual understanding. Extensive experiments show that, unlike prior methods, SMFA achieves precise and controllable unlearning while maintaining the model's foundational image understanding.

</details>


### [29] [Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025](https://arxiv.org/abs/2511.20200)
*Yitian Huang,Yuxuan Lei,Jianxun Lian,Hao Liao*

Main category: cs.AI

TL;DR: 团队MSRA_SC在CPDC 2025挑战赛中提出的统一解决方案，通过上下文工程和GRPO训练在两个赛道取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决工具调用稳定性、执行可靠性和角色扮演指导的问题，同时避免小样本过拟合，提升任务导向对话性能。

Method: 1. 上下文工程：动态工具剪枝和角色裁剪进行输入压缩，结合参数归一化和函数合并等后处理技术；2. GPU赛道采用GRPO训练，用强化学习替代监督微调。

Result: 最终评估中：Task 2 API第1名，Task 1 API第2名，Task 3 API和GPU赛道均第3名。

Conclusion: 提出的简单而有效的框架在两个赛道都表现出色，证明了方法的有效性。

Abstract: This report presents the solution and results of our team MSRA\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution

</details>


### [30] [CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents](https://arxiv.org/abs/2511.20216)
*Haebin Seong,Sungmin Kim,Minchan Kim,Yongjun Cho,Myunchul Joe,Suhwan Choi,Jaeyoon Jung,Jiyong Youn,Yoonshik Kim,Samwoo Seong,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: CostNav是首个微导航经济测试平台，通过成本收益分析评估自主送货机器人的商业可行性，揭示了导航研究指标与商业部署之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有导航基准只关注任务成功率，忽略了商业部署所需的经济可行性。为了填补导航研究与实际商业应用之间的鸿沟，需要评估完整的经济生命周期。

Method: CostNav建模完整的经济生命周期，包括硬件、训练、能源、维护成本和配送收入，使用行业参数，并将缩小规模的模拟投影到实际配送场景。

Result: 基准测试显示43.0%的服务水平协议合规率，但商业上不可行：每次运行亏损30.009美元，无盈亏平衡点，99.7%的运行成本来自碰撞导致的维护费用。

Conclusion: CostNav填补了导航研究与商业部署之间的差距，为评估基于规则的导航、模仿学习和成本感知强化学习提供了基础，支持跨导航范式的经济权衡决策。

Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.

</details>


### [31] [Actionable and diverse counterfactual explanations incorporating domain knowledge and causal constraints](https://arxiv.org/abs/2511.20236)
*Szymon Bobek,Łukasz Bałec,Grzegorz J. Nalepa*

Main category: cs.AI

TL;DR: 提出DANCE方法，通过整合特征依赖关系和因果约束来生成多样化、可操作且符合知识约束的反事实解释，确保反事实的合理性和现实可行性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法往往忽略现实数据集中的复杂依赖关系，导致产生不现实或不实用的修改。受网络安全在电子邮件营销领域应用的启发，需要确保反事实的合理性和现实可行性。

Method: 从数据中学习线性和非线性约束，或整合专家提供的依赖图，确保反事实的合理性和可操作性。通过保持特征关系的一致性，生成符合现实约束的解释。同时平衡合理性、多样性和稀疏性。

Result: 在140个公共数据集上的广泛评估表明，该方法能够生成有意义、领域相关的反事实，在广泛使用的指标上优于其他现有方法。

Conclusion: DANCE方法有效解决了现有算法在特征依赖关系处理方面的关键局限性，能够生成既合理又实用的反事实解释。

Abstract: Counterfactual explanations enhance the actionable interpretability of machine learning models by identifying the minimal changes required to achieve a desired outcome of the model. However, existing methods often ignore the complex dependencies in real-world datasets, leading to unrealistic or impractical modifications. Motivated by cybersecurity applications in the email marketing domain, we propose a method for generating Diverse, Actionable, and kNowledge-Constrained Explanations (DANCE), which incorporates feature dependencies and causal constraints to ensure plausibility and real-world feasibility of counterfactuals. Our method learns linear and nonlinear constraints from data or integrates expert-provided dependency graphs, ensuring counterfactuals are plausible and actionable. By maintaining consistency with feature relationships, the method produces explanations that align with real-world constraints. Additionally, it balances plausibility, diversity, and sparsity, effectively addressing key limitations in existing algorithms. The work is developed based on a real-life case study with Freshmail, the largest email marketing company in Poland and supported by a joint R&D project Sendguard. Furthermore, we provide an extensive evaluation using 140 public datasets, which highlights its ability to generate meaningful, domain-relevant counterfactuals that outperform other existing approaches based on widely used metrics. The source code for reproduction of the results can be found in a GitHub repository we provide.

</details>


### [32] [SMoG: Schema Matching on Graph](https://arxiv.org/abs/2511.20285)
*Mingyu Jeon,Jaeyoung Suh,Suwan Cho*

Main category: cs.AI

TL;DR: SMoG是一个用于医疗领域模式匹配的新框架，通过迭代执行简单的1跳SPARQL查询，结合知识图谱增强LLM，解决了现有方法的多跳查询复杂性和存储密集问题。


<details>
  <summary>Details</summary>
Motivation: 医疗领域中不同的电子健康记录系统需要与标准模型（如OMOP CDM）对齐，但现有LLM方法存在幻觉问题和缺乏最新领域知识，而基于知识图谱的方法又存在查询复杂和存储密集的问题。

Method: 提出SMoG框架，采用迭代执行简单1跳SPARQL查询的策略，直接从SPARQL端点查询，无需向量检索，提高了可解释性和可靠性。

Result: 在真实医疗数据集上的实验表明，SMoG达到了与最先进基线方法相当的性能，验证了其在知识图谱增强模式匹配中的有效性和效率。

Conclusion: SMoG通过简单高效的1跳SPARQL查询策略，成功解决了知识图谱增强模式匹配中的复杂性和存储问题，为医疗数据集成提供了可靠解决方案。

Abstract: Schema matching is a critical task in data integration, particularly in the medical domain where disparate Electronic Health Record (EHR) systems must be aligned to standard models like OMOP CDM. While Large Language Models (LLMs) have shown promise in schema matching, they suffer from hallucination and lack of up-to-date domain knowledge. Knowledge Graphs (KGs) offer a solution by providing structured, verifiable knowledge. However, existing KG-augmented LLM approaches often rely on inefficient complex multi-hop queries or storage-intensive vector-based retrieval methods. This paper introduces SMoG (Schema Matching on Graph), a novel framework that leverages iterative execution of simple 1-hop SPARQL queries, inspired by successful strategies in Knowledge Graph Question Answering (KGQA). SMoG enhances explainability and reliability by generating human-verifiable query paths while significantly reducing storage requirements by directly querying SPARQL endpoints. Experimental results on real-world medical datasets demonstrate that SMoG achieves performance comparable to state-of-the-art baselines, validating its effectiveness and efficiency in KG-augmented schema matching.

</details>


### [33] [Improving Language Agents through BREW](https://arxiv.org/abs/2511.20297)
*Shashank Kirtania,Param Biyani,Priyanshu Gupta,Yasharth Bajpai,Roshni Iyer,Sumit Gulwani,Gustavo Soares*

Main category: cs.AI

TL;DR: BREW框架通过构建和精化经验学习知识库来优化LLM智能体，在保持计算效率的同时显著提升任务精度并减少API调用。


<details>
  <summary>Details</summary>
Motivation: 当前基于PPO和GRPO的智能体训练方法计算开销大，且生成的策略难以解释、适应或增量改进，需要更实用的优化方案。

Method: 引入BREW框架，通过知识库构建和精化来优化智能体，采用任务分级器和行为准则学习洞察，利用状态空间搜索确保鲁棒性，并有效分区智能体记忆以提高检索效率。

Result: 在OSWorld、τ²Bench和SpreadsheetBench等基准测试中，BREW实现了10-20%的任务精度提升，10-15%的API/工具调用减少，执行时间更快，同时保持与基础模型相当的计算效率。

Conclusion: 将知识库确立为模块化、可控的智能体优化基底，提供透明、可解释和可扩展的行为塑造机制，与将记忆视为静态上下文的先前工作形成对比。

Abstract: Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\%$ improvement in task precision, $10-15\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.

</details>


### [34] [Data Augmentation Techniques to Reverse-Engineer Neural Network Weights from Input-Output Queries](https://arxiv.org/abs/2511.20312)
*Alexander Beiser,Flavio Martinelli,Wulfram Gerstner,Johanni Brea*

Main category: cs.AI

TL;DR: 提出新的数据增强技术来改进教师网络参数的反向工程，能够恢复参数数量是训练数据100倍的网络


<details>
  <summary>Details</summary>
Motivation: 现有方法在教师网络参数数量超过训练数据时失效，因为学生会过度拟合查询数据而不是对齐教师参数

Method: 设计专门针对网络隐藏层表示空间采样的新数据增强技术，而非使用标准的旋转、翻转等增强方法

Result: 扩展了可恢复网络大小的最先进范围，能够恢复参数数量是训练数据100倍的网络

Conclusion: 专门针对网络隐藏层表示空间设计的数据增强技术能显著改进教师网络参数的反向工程效果

Abstract: Network weights can be reverse-engineered given enough informative samples of a network's input-output function. In a teacher-student setup, this translates into collecting a dataset of the teacher mapping -- querying the teacher -- and fitting a student to imitate such mapping. A sensible choice of queries is the dataset the teacher is trained on. But current methods fail when the teacher parameters are more numerous than the training data, because the student overfits to the queries instead of aligning its parameters to the teacher. In this work, we explore augmentation techniques to best sample the input-output mapping of a teacher network, with the goal of eliciting a rich set of representations from the teacher hidden layers. We discover that standard augmentations such as rotation, flipping, and adding noise, bring little to no improvement to the identification problem. We design new data augmentation techniques tailored to better sample the representational space of the network's hidden layers. With our augmentations we extend the state-of-the-art range of recoverable network sizes. To test their scalability, we show that we can recover networks of up to 100 times more parameters than training data-points.

</details>


### [35] [Active Inference in Discrete State Spaces from First Principles](https://arxiv.org/abs/2511.20321)
*Patrick Kenny*

Main category: cs.AI

TL;DR: 本文澄清了主动推理与自由能原理的关系，提出在离散状态空间中实现主动推理的优化问题可表述为约束散度最小化问题，无需依赖期望自由能概念。


<details>
  <summary>Details</summary>
Motivation: 澄清主动推理概念，将其与自由能原理分离，展示主动推理可通过标准平均场方法实现，而不必诉诸期望自由能。

Method: 将主动推理在离散状态空间中的优化问题重新表述为约束散度最小化问题，使用标准平均场方法求解。感知建模时使用变分自由能，行动建模时使用带有熵正则化的不同泛函。

Result: 证明了主动推理可通过约束散度最小化实现，感知建模与变分自由能一致，行动建模与期望自由能泛函相差一个熵正则化项。

Conclusion: 主动推理可以与自由能原理分离，通过约束散度最小化框架实现，为理解主动推理提供了更清晰的理论基础。

Abstract: We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.

</details>


### [36] [NNGPT: Rethinking AutoML with Large Language Models](https://arxiv.org/abs/2511.20333)
*Roman Kochnev,Waleed Khalid,Tolgay Atinc Uzun,Xi Zhang,Yashkumar Sanjaybhai Dhameliya,Furui Qin,Chandini Vysyaraju,Raghuvir Duvvuri,Avi Goyal,Dmitry Ignatov,Radu Timofte*

Main category: cs.AI

TL;DR: NNGPT是一个开源框架，将大语言模型转化为自改进的AutoML引擎，用于神经网络开发。它通过生成新模型扩展神经网络数据集，实现基于生成-评估-自改进闭环系统的持续微调。


<details>
  <summary>Details</summary>
Motivation: 构建自改进AI系统是AI领域的根本挑战。传统框架无法实现持续的自改进能力，需要开发能够自主生成、评估和学习神经网络模型的系统。

Method: 集成五个协同的LLM管道：零样本架构合成、超参数优化、代码感知精度预测、检索增强的PyTorch块合成、强化学习。基于LEMUR数据集，通过单一提示生成并验证网络架构、预处理代码和超参数，端到端执行并学习结果。

Result: NN-RAG在1,289个目标上达到73%可执行性；3-shot提示提升常见数据集精度；哈希去重节省数百次运行；单次预测匹配基于搜索的AutoML；HPO在LEMUR上RMSE 0.60优于Optuna；代码感知预测器RMSE 0.14，Pearson r=0.78；已生成超过5K验证模型。

Conclusion: NNGPT被证明是一个自主的AutoML引擎，能够有效实现神经网络的自动化开发和自改进。代码、提示和检查点将公开发布以促进可重复性和社区使用。

Abstract: Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.

</details>


### [37] [VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning](https://arxiv.org/abs/2511.20422)
*Bo Pang,Chenxi Xu,Jierui Ren,Guoping Wang,Sheng Li*

Main category: cs.AI

TL;DR: VibraVerse是一个大规模几何-声学对齐数据集，通过明确的因果链（3D几何→物理属性→模态参数→声学信号）连接物理世界的感知模型，并提出了CLASP对比学习框架来实现跨模态的物理一致性对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习框架缺乏物理一致性，忽视了物体几何、材料、振动模式和产生声音之间的内在因果关系。需要建立基于物理定律而非统计相关性的感知模型。

Method: 构建VibraVerse数据集，包含3D模型的物理属性（密度、杨氏模量、泊松比）和体积几何，计算模态特征频率和特征向量进行冲击声合成；提出CLASP对比学习框架进行跨模态对齐。

Result: 在几何到声音预测、声音引导形状重建和跨模态表示学习等基准任务上，基于VibraVerse训练的模型表现出更高的准确性、可解释性和跨模态泛化能力。

Conclusion: VibraVerse为物理一致性和因果可解释的多模态学习建立了基准，为声音引导的具身感知和物理世界深度理解提供了基础。

Abstract: Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.

</details>


### [38] [DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs](https://arxiv.org/abs/2511.20468)
*Yuanhao Li,Mingshan Liu,Hongbo Wang,Yiding Zhang,Yifei Ma,Wei Tan*

Main category: cs.AI

TL;DR: DRAFT-RL是一个新颖的多智能体强化学习框架，通过集成链式草稿推理，让每个智能体生成多个推理草稿，通过同行评估和奖励模型选择最优路径，提升LLM智能体的推理能力和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于多智能体反思的LLM框架通常依赖单次响应，缺乏推理探索的结构多样性，限制了智能体的稳健性和可解释性。

Method: 提出DRAFT-RL框架，将链式草稿推理集成到多智能体RL训练中：每个智能体为每个查询生成多个草稿，通过同行智能体和学习的奖励模型评估，选择最有希望的轨迹，并通过actor-critic学习优化未来推理策略。

Result: 在代码合成、符号数学和知识密集型问答等复杂推理任务上，DRAFT-RL在准确性和收敛速度方面显著优于现有的反思和基于RL的智能体。

Conclusion: DRAFT-RL通过显式的多路径探索、同行引导的反思和奖励对齐的选择，实现了更稳健和可解释的LLM智能体行为。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed

</details>


### [39] [Universe of Thoughts: Enabling Creative Reasoning with Large Language Models](https://arxiv.org/abs/2511.20471)
*Yuto Suzuki,Farnoush Banaei-Kashani*

Main category: cs.AI

TL;DR: 本文提出了一个基于大语言模型的创造性推理框架，包含组合、探索和转化三种推理范式，并开发了UoT方法来实现这些创造性过程，在需要创新问题解决的任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型主要关注常规问题解决，缺乏创造性推理能力。在药物发现、商业策略等解决方案空间广阔且常规方案次优的领域，发现创新解决方案的创造性推理至关重要。

Method: 提出了基于认知科学原理的计算框架，包含组合、探索和转化三种创造性推理范式，并开发了Universe of Thoughts (UoT)方法来实现这些创造性过程。

Result: 与最先进的推理技术和代表性商业模型相比，UoT在创造性推理方面表现出优越性能，特别是在需要创新问题解决的任务中。

Conclusion: UoT框架为LLMs提供了系统探索思维宇宙以生成创造性解决方案的能力，填补了现有推理模型在创造性推理方面的空白。

Abstract: Reasoning based on Large Language Models (LLMs) has garnered increasing attention due to outstanding performance of these models in mathematical and complex logical tasks. Beginning with the Chain-of-Thought (CoT) prompting technique, numerous reasoning methods have emerged that decompose problems into smaller, sequential steps (or thoughts). However, existing reasoning models focus on conventional problem-solving and do not necessarily generate creative solutions by ``creative reasoning''. In domains where the solution space is expansive and conventional solutions are suboptimal, such as drug discovery or business strategization, creative reasoning to discover innovative solutions is crucial. To address this gap, first we introduce a computational framework for creative reasoning inspired by established cognitive science principles. With this framework, we propose three core creative reasoning paradigms, namely, \textit{combinational}, \textit{exploratory}, and \textit{transformative} reasoning, where each offers specific directions for systematic exploration of the universe of thoughts to generate creative solutions. Next, to materialize this framework using LLMs, we introduce the \textit{Universe of Thoughts} (or \textit{UoT}, for short), a novel set of methods to implement the aforementioned three creative processes. Finally, we introduce three novel tasks that necessitate creative problem-solving, along with an evaluation benchmark to assess creativity from three orthogonal perspectives: feasibility as constraint, and utility and novelty as metrics. With a comparative analysis against the state-of-the-art (SOTA) reasoning techniques as well as representative commercial models with reasoning capability, we show that UoT demonstrates superior performance in creative reasoning.

</details>


### [40] [Quantifying the Privacy Implications of High-Fidelity Synthetic Network Traffic](https://arxiv.org/abs/2511.20497)
*Van Tran,Shinan Liu,Tian Li,Nick Feamster*

Main category: cs.AI

TL;DR: 本文提出了针对合成网络流量的隐私度量方法，评估了不同生成模型的隐私风险，发现MIA攻击成功率可达88%，网络标识符恢复率可达100%，并识别了影响攻击成功率的关键因素。


<details>
  <summary>Details</summary>
Motivation: 解决合成网络流量数据的稀缺性和隐私担忧，但现有生成模型缺乏对隐私泄露程度的系统评估方法。

Method: 引入了一套全面的隐私度量方法，结合标准方法（如成员推理攻击MIA和数据提取攻击）与网络特定标识符和属性，系统评估不同代表性生成模型的脆弱性。

Result: 结果显示不同模型和数据集间隐私风险存在显著差异：MIA成功率0%-88%，网络标识符恢复率可达100%，训练数据多样性和生成模型对训练数据的拟合程度是影响攻击结果的关键因素。

Conclusion: 研究结果为设计和部署最小化隐私泄露的生成模型提供了可操作的指导，为更安全的合成网络流量生成奠定了基础。

Abstract: To address the scarcity and privacy concerns of network traffic data, various generative models have been developed to produce synthetic traffic. However, synthetic traffic is not inherently privacy-preserving, and the extent to which it leaks sensitive information, and how to measure such leakage, remain largely unexplored. This challenge is further compounded by the diversity of model architectures, which shape how traffic is represented and synthesized. We introduce a comprehensive set of privacy metrics for synthetic network traffic, combining standard approaches like membership inference attacks (MIA) and data extraction attacks with network-specific identifiers and attributes. Using these metrics, we systematically evaluate the vulnerability of different representative generative models and examine the factors that influence attack success. Our results reveal substantial variability in privacy risks across models and datasets. MIA success ranges from 0% to 88%, and up to 100% of network identifiers can be recovered from generated traffic, highlighting serious privacy vulnerabilities. We further identify key factors that significantly affect attack outcomes, including training data diversity and how well the generative model fits the training data. These findings provide actionable guidance for designing and deploying generative models that minimize privacy leakage, establishing a foundation for safer synthetic network traffic generation.

</details>


### [41] [FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization](https://arxiv.org/abs/2511.20510)
*Yuto Suzuki,Paul Awolade,Daniel V. LaBarbera,Farnoush Banaei-Kashani*

Main category: cs.AI

TL;DR: FRAGMENTA是一个用于药物先导化合物优化的端到端框架，包含基于Q学习的生成模型和通过对话反馈学习的智能体系统，在癌症药物发现实验中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 分子生成在药物发现中很重要，但类别特异性数据集通常只有不到100个训练样本，现有片段化方法多样性不足且错过关键片段，模型调优需要化学家和AI工程师之间缓慢的间接协作。

Method: 1) 将片段化重构为"词汇选择"问题，使用动态Q学习联合优化片段化和生成；2) 智能体AI系统通过领域专家的对话反馈来精炼目标，逐步学习领域知识以实现自动化调优。

Result: 在真实世界癌症药物发现实验中，FRAGMENTA的人类-智能体配置识别的高分分子数量是基线的近两倍，完全自主的智能体-智能体系统优于传统的人类-人类调优。

Conclusion: FRAGMENTA证明了智能体调优在捕捉专家意图方面的有效性，能够从循环中移除AI工程师并最终实现自动化调优。

Abstract: Molecule generation using generative AI is vital for drug discovery, yet class-specific datasets often contain fewer than 100 training examples. While fragment-based models handle limited data better than atom-based approaches, existing heuristic fragmentation limits diversity and misses key fragments. Additionally, model tuning typically requires slow, indirect collaboration between medicinal chemists and AI engineers. We introduce FRAGMENTA, an end-to-end framework for drug lead optimization comprising: 1) a novel generative model that reframes fragmentation as a "vocabulary selection" problem, using dynamic Q-learning to jointly optimize fragmentation and generation; and 2) an agentic AI system that refines objectives via conversational feedback from domain experts. This system removes the AI engineer from the loop and progressively learns domain knowledge to eventually automate tuning. In real-world cancer drug discovery experiments, FRAGMENTA's Human-Agent configuration identified nearly twice as many high-scoring molecules as baselines. Furthermore, the fully autonomous Agent-Agent system outperformed traditional Human-Human tuning, demonstrating the efficacy of agentic tuning in capturing expert intent.

</details>


### [42] [Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam](https://arxiv.org/abs/2511.20526)
*Xinran Wang,Boran Zhu,Shujuan Zhou,Ziwen Long,Dehua Zhou,Shu Zhang*

Main category: cs.AI

TL;DR: 本研究比较了ChatGPT-4o和DeepSeek-R1在中国药师执业资格考试中的表现，发现DeepSeek-R1在准确率上显著优于ChatGPT-4o（90.0% vs 76.1%）。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在数字健康教育和评估中的应用日益增多，需要评估其在专业认证任务中的能力。中国药师执业资格考试作为标准化基准，适合评估模型在临床和理论能力方面的表现。

Method: 收集2017-2021年药师执业资格考试的2,306道纯文本选择题，排除包含表格或图像的问题。将中文原题输入两个模型，评估准确率，使用卡方检验和Fisher精确检验进行统计分析。

Result: DeepSeek-R1总体准确率显著高于ChatGPT-4o（90.0% vs 76.1%，p < 0.001）。在基础和临床综合模块中，DeepSeek-R1表现一致更优。虽然逐年比较也显示DeepSeek-R1优势，但具体年份单元差异未达统计显著性。

Conclusion: DeepSeek-R1在药师执业资格考试中表现出色，表明领域特定模型在此类任务中具有潜力，但仍需在法律和伦理敏感场景中保持人工监督。

Abstract: Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain underexplored.In China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.

</details>


### [43] [Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models](https://arxiv.org/abs/2511.20531)
*Shamima Hossain*

Main category: cs.AI

TL;DR: 提出了一个基于知识图谱的视觉语言模型推理框架，通过多步验证提高事实准确性


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)虽然功能强大，但经常产生事实不准确的输出，缺乏稳健的推理能力。在多模态环境中整合外部知识的研究仍然不足

Method: 使用结构化知识图谱进行多跳验证，包括视觉实体识别、知识图谱遍历和基于事实的标题优化。评估了层次化、三元组和项目符号三种知识表示方法

Result: 在混合数据集上的初步实验显示，该方法将事实准确性提高了约31%，揭示了推理模式和失败案例的关键见解

Conclusion: 这项工作展示了整合外部知识来推进VLM推理的潜力，为更可靠和知识丰富的多模态系统铺平了道路

Abstract: Visual Language Models (VLMs) are powerful generative tools but often produce factually inaccurate outputs due to a lack of robust reasoning capabilities. While extensive research has been conducted on integrating external knowledge for reasoning in large language models (LLMs), such efforts remain underexplored in VLMs, where the challenge is compounded by the need to bridge multiple modalities seamlessly. This work introduces a framework for knowledge-guided reasoning in VLMs, leveraging structured knowledge graphs for multi-hop verification using image-captioning task to illustrate our framework. Our approach enables systematic reasoning across multiple steps, including visual entity recognition, knowledge graph traversal, and fact-based caption refinement. We evaluate the framework using hierarchical, triple-based and bullet-point based knowledge representations, analyzing their effectiveness in factual accuracy and logical inference. Empirical results show that our approach improves factual accuracy by approximately 31% on preliminary experiments on a curated dataset of mixtures from Google Landmarks v2, Conceptual captions and Coco captions revealing key insights into reasoning patterns and failure modes. This work demonstrates the potential of integrating external knowledge for advancing reasoning in VLMs, paving the way for more reliable and knowledgable multimodal systems.

</details>


### [44] [PaTAS: A Parallel System for Trust Propagation in Neural Networks Using Subjective Logic](https://arxiv.org/abs/2511.20586)
*Koffi Ismael Ouattara,Ioannis Krontiris,Theo Dimitrakos,Dennis Eisermann,Frank Kargl*

Main category: cs.AI

TL;DR: 提出PaTAS框架，使用主观逻辑在神经网络中建模和传播信任度，通过信任节点和信任函数并行评估模型可靠性，能够区分良性输入和对抗性输入。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标如准确率和精度无法捕捉模型预测的不确定性或可靠性，特别是在对抗性或退化条件下。需要可量化的信任评估系统来确保AI系统在安全关键应用中的可信度。

Method: 使用主观逻辑，通过信任节点和信任函数并行传播输入、参数和激活信任；定义参数信任更新机制在训练中优化参数可靠性；采用推理路径信任评估方法计算实例特定信任。

Result: 在真实世界和对抗性数据集上的实验表明，PaTAS产生可解释、对称且收敛的信任估计，有效区分良性输入和对抗性输入，识别模型置信度与实际可靠性不一致的情况。

Conclusion: PaTAS为在神经网络架构中实现透明和可量化的信任推理提供了原则性基础，能够在整个AI生命周期中评估模型可靠性。

Abstract: Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics such as accuracy and precision fail to capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the \emph{Parallel Trust Assessment System (PaTAS)}, a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through \emph{Trust Nodes} and \emph{Trust Functions} that propagate input, parameter, and activation trust across the network. The framework defines a \emph{Parameter Trust Update} mechanism to refine parameter reliability during training and an \emph{Inference-Path Trust Assessment (IPTA)} method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a principled foundation for evaluating model reliability across the AI lifecycle.

</details>


### [45] [Building a Foundation Model for Trajectory from Scratch](https://arxiv.org/abs/2511.20610)
*Gaspard Merten,Mahmoud Sakr,Gilles Dejaegere*

Main category: cs.AI

TL;DR: 本教程展示了从GPT-2开始构建轨迹基础模型的最小实现步骤和代码，比较了TrajFM和TrajGPT等代表性模型，并介绍了TimesFM的补丁方法，旨在支持SIGSPATIAL社区构建和评估移动性基础模型。


<details>
  <summary>Details</summary>
Motivation: 基础模型在人工智能中具有变革性，但为移动轨迹从头开始构建基础模型的方法尚不明确或缺乏文档记录，需要填补这一空白。

Method: 通过逐步代码驱动过程演示如何将GPT-2适配时空数据，回顾比较代表性轨迹基础模型，并引入相关领域的补充技术。

Result: 提供了构建轨迹基础模型的实现指南和代码示例，比较了不同模型的架构创新和差异。

Conclusion: 创建这些教育材料对于支持SIGSPATIAL社区构建和评估移动性基础模型、提高移动AI研究清晰度和同行评审效果是及时且必要的。

Abstract: Foundation models are transformative in artificial intelligence, but building them from scratch, especially for mobility trajectories, is not yet clear or documented. This tutorial bridges this gap by demonstrating the steps and code of a minimal implementation of a trajectory-focused foundation model starting from GPT-2. Through a concise, step-by-step, code-driven process, we demonstrate adapting GPT-2 for spatiotemporal data. We then review and compare representative trajectory foundation models, such as TrajFM and TrajGPT, highlighting their architectural innovations and differences. Additionally, we introduce complementary techniques from related domains, like TimesFM's patching approach. Targeted at researchers and practitioners, this tutorial aims to explain the concepts and terminology of foundation models, at the implementation level. We find it timely and indispensable to create this educational material in order to support the SIGSPATIAL community in building and evaluating mobility foundation models, enhancing both research clarity and peer-review effectiveness in mobility AI.

</details>


### [46] [Copyright Detection in Large Language Models: An Ethical Approach to Generative AI Development](https://arxiv.org/abs/2511.20623)
*David Szczecina,Senan Gaffori,Edmond Li*

Main category: cs.AI

TL;DR: 开发了一个开源版权检测平台，帮助内容创作者验证其作品是否被用于LLM训练数据，相比现有方法减少了10-30%的计算开销。


<details>
  <summary>Details</summary>
Motivation: LLM广泛使用引发版权内容未经授权使用的担忧，现有检测框架计算密集且对独立创作者不友好，需要可扩展、透明且用户友好的解决方案。

Method: 通过优化API调用提高效率，改进相似性检测，优化数据集验证，并提供直观用户界面和可扩展后端。

Result: 实现了计算开销减少10-30%，提高了检测准确性和用户体验。

Conclusion: 该框架有助于提高AI开发的透明度，促进负责任AI发展和版权执法的进一步研究。

Abstract: The widespread use of Large Language Models (LLMs) raises critical concerns regarding the unauthorized inclusion of copyrighted content in training data. Existing detection frameworks, such as DE-COP, are computationally intensive, and largely inaccessible to independent creators. As legal scrutiny increases, there is a pressing need for a scalable, transparent, and user-friendly solution. This paper introduce an open-source copyright detection platform that enables content creators to verify whether their work was used in LLM training datasets. Our approach enhances existing methodologies by facilitating ease of use, improving similarity detection, optimizing dataset validation, and reducing computational overhead by 10-30% with efficient API calls. With an intuitive user interface and scalable backend, this framework contributes to increasing transparency in AI development and ethical compliance, facilitating the foundation for further research in responsible AI development and copyright enforcement.

</details>


### [47] [Fighting AI with AI: Leveraging Foundation Models for Assuring AI-Enabled Safety-Critical Systems](https://arxiv.org/abs/2511.20627)
*Anastasia Mavridou,Divya Gopinath,Corina S. Păsăreanu*

Main category: cs.AI

TL;DR: 提出REACT和SemaLens两个AI驱动组件，解决安全关键系统中AI组件集成的验证挑战，通过LLM和VLM桥接自然语言需求与形式化规范之间的语义鸿沟。


<details>
  <summary>Details</summary>
Motivation: AI组件（特别是DNN）在安全关键系统中的集成面临根本性验证挑战，AI系统的不透明性以及高层需求与低层网络表示之间的语义鸿沟阻碍了传统验证方法。

Method: REACT使用大语言模型连接非正式自然语言需求与形式化规范，实现早期验证；SemaLens利用视觉语言模型基于人类可理解概念对DNN感知系统进行推理、测试和监控。

Result: 构建了从非正式需求到验证实现的全流程管道，解决了AI系统验证中的语义鸿沟和可扩展性问题。

Conclusion: 利用AI自身能力解决AI系统验证挑战是可行的，提出的方法为安全关键系统中AI组件的可信集成提供了有效解决方案。

Abstract: The integration of AI components, particularly Deep Neural Networks (DNNs), into safety-critical systems such as aerospace and autonomous vehicles presents fundamental challenges for assurance. The opacity of AI systems, combined with the semantic gap between high-level requirements and low-level network representations, creates barriers to traditional verification approaches. These AI-specific challenges are amplified by longstanding issues in Requirements Engineering, including ambiguity in natural language specifications and scalability bottlenecks in formalization. We propose an approach that leverages AI itself to address these challenges through two complementary components. REACT (Requirements Engineering with AI for Consistency and Testing) employs Large Language Models (LLMs) to bridge the gap between informal natural language requirements and formal specifications, enabling early verification and validation. SemaLens (Semantic Analysis of Visual Perception using large Multi-modal models) utilizes Vision Language Models (VLMs) to reason about, test, and monitor DNN-based perception systems using human-understandable concepts. Together, these components provide a comprehensive pipeline from informal requirements to validated implementations.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [48] [The Quality of Information: A Weighted Entropy Approach to Near-Optimal Mastermind](https://arxiv.org/abs/2511.19446)
*Serkan Gür*

Main category: cs.IT

TL;DR: 本文提出了一种基于信息论的Mastermind游戏求解策略，使用加权熵启发式和遗传算法优化，实现了接近理论最优的性能。


<details>
  <summary>Details</summary>
Motivation: 传统Mastermind游戏求解方法性能有限，需要开发更高效的信息论策略来接近理论最优解。

Method: 采用基于Belis-Guias框架的加权熵启发式，通过遗传算法优化上下文相关的效用权重，并引入阶段加权启发式。

Result: 固定权重方法平均猜测次数4.3565，最大5次；阶段加权方法平均4.3488次，最大6次，接近理论最优4.3403。

Conclusion: 该方法在保持计算效率的同时显著提升性能，通过原则性的信息估值实现了接近理论极限的Mastermind游戏求解。

Abstract: This paper presents a novel class of information-theoretic strategies for solving the game of Mastermind, achieving state-of-the-art performance among known heuristic methods. The core contribution is the application of a weighted entropy heuristic, based on the Belis-Guias, u framework, which assigns context-dependent utility values to each of the possible feedback types. A genetic algorithm optimization approach discovers interpretable weight patterns that reflect strategic game dynamics. First, I demonstrate that a single, fixed vector of optimized weights achieves a remarkable 4.3565 average guesses with a maximum of 5. Building upon this, I introduce a stage-weighted heuristic with distinct utility vectors for each turn, achieving 4.3488 average guesses with a maximum of 6, approaching the theoretical optimum of 4.3403 by less than 0.2%. The method retains the computational efficiency of classical one-step-ahead heuristics while significantly improving performance through principled information valuation. A complete implementation and all optimized parameters are provided for full reproducibility.

</details>


### [49] [The Semiotic Channel Principle: Measuring the Capacity for Meaning in LLM Communication](https://arxiv.org/abs/2511.19550)
*Davide Picca*

Main category: cs.IT

TL;DR: 本文提出了一个分析大语言模型的符号学框架，将其概念化为随机符号引擎，其输出需要人类主动的、不对称的解释。框架形式化了表达丰富性（符号广度）与解释稳定性（可解读性）之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM分析往往关注模型内部机制，但缺乏对模型输出文本符号特征的系统分析框架。需要从可观察的文本产物角度来理解LLM的符号表达能力。

Method: 使用信息论工具量化符号广度（源熵）和可解读性（消息与人类解释之间的互信息）。引入生成复杂度参数λ来建模两者间的权衡关系，定义符号通道和容量约束。

Result: 开发了一个可操作的框架，能够实证测量符号广度和可解读性，并在模型分析、提示优化、风险分析和自适应系统等四个关键应用中验证了其有效性。

Conclusion: 这种基于容量的符号学方法为理解、评估和设计LLM介导的通信提供了一个严谨且可操作的工具包，将分析重点从模型内部转向可观察的文本产物。

Abstract: This paper proposes a novel semiotic framework for analyzing Large Language Models (LLMs), conceptualizing them as stochastic semiotic engines whose outputs demand active, asymmetric human interpretation. We formalize the trade-off between expressive richness (semiotic breadth) and interpretive stability (decipherability) using information-theoretic tools. Breadth is quantified as source entropy, and decipherability as the mutual information between messages and human interpretations. We introduce a generative complexity parameter (lambda) that governs this trade-off, as both breadth and decipherability are functions of lambda. The core trade-off is modeled as an emergent property of their distinct responses to $λ$. We define a semiotic channel, parameterized by audience and context, and posit a capacity constraint on meaning transmission, operationally defined as the maximum decipherability by optimizing lambda. This reframing shifts analysis from opaque model internals to observable textual artifacts, enabling empirical measurement of breadth and decipherability. We demonstrate the framework's utility across four key applications: (i) model profiling; (ii) optimizing prompt/context design; (iii) risk analysis based on ambiguity; and (iv) adaptive semiotic systems. We conclude that this capacity-based semiotic approach offers a rigorous, actionable toolkit for understanding, evaluating, and designing LLM-mediated communication.

</details>


### [50] [Joint Satellite Power Consumption and Handover Optimization for LEO Constellations](https://arxiv.org/abs/2511.19745)
*Yassine Afif,Mohammed Almekhlafi,Antoine Lesage-Landry,Gunes Karabulut Kurt*

Main category: cs.IT

TL;DR: 该论文提出了一种LEO卫星通信系统中考虑切换频率的功率分配优化方法，通过联合优化传输功率、用户-卫星关联和切换惩罚，在保持高吞吐量的同时控制切换频率。


<details>
  <summary>Details</summary>
Motivation: LEO卫星星座系统中，由于卫星动态拓扑导致的频繁切换会带来额外的信令开销和功耗，随着星座规模增大，这成为显著负担。

Method: 将功率分配问题建模为混合整数凹线性规划问题，考虑功率和关联约束，使用现成求解器求解，并与用户关联最近可见卫星的基线方法进行对比。

Result: 蒙特卡洛仿真表明，所提方法能有效控制切换频率同时保持高用户吞吐量，用户速率提升约40%且切换频率不会不成比例增加。

Conclusion: 提出的切换感知优化策略在显著提升用户速率的同时有效控制切换频率，证明了该方法的有效性。

Abstract: In satellite constellation-based communication systems, continuous user coverage requires frequent handoffs due to the dynamic topology induced by the Low Earth Orbit (LEO) satellites. Each handoff between a satellite and ground users introduces additional signaling and power consumption, which can become a significant burden as the size of the constellation continues to increase. This work focuses on the optimization of the total transmission rate in a LEO-to-user system, by jointly considering the total transmitted power, user-satellite associations, and power consumption, the latter being handled through a penalty on handoff events. We consider a system where LEO satellites serve users located in remote areas with no terrestrial connectivity, and formulate the power allocation problem as a mixed-integer concave linear program (MICP) subject to power and association constraints. Our approach can be solved with off-the-shelf solvers and is benchmarked against a naive baseline where users associate to their closest visible satellite. Extensive Monte Carlo simulations demonstrate the effectiveness of the proposed method in controlling the handoff frequency while maintaining high user throughput. These performance gains highlight the effectiveness of our handover-aware optimization strategy, which ensures that user rates improve significantly, by about 40%, without incurring a disproportionate rise in the handoff frequency.

</details>


### [51] [One-Shot Coding and Applications](https://arxiv.org/abs/2511.19556)
*Yanxiao Liu*

Main category: cs.IT

TL;DR: 本文扩展了泊松函数表示法在一次性信息理论中的应用，将其应用于更复杂的场景，其中原始版本无法直接应用。


<details>
  <summary>Details</summary>
Motivation: 研究一次性信息理论中的可实现性部分，旨在推导出一次性可实现结果，这些结果在应用于无记忆源和信道或具有遍历行为的记忆系统时，能够蕴含现有的（一阶和二阶）渐近结果。

Method: 扩展泊松函数表示法的适用性，开发进一步扩展以处理原始版本无法直接应用的更复杂场景。

Result: 成功将泊松函数表示法扩展到各种更复杂的场景，提供了统一的一次性编码方案。

Conclusion: 泊松函数表示法在一次性信息理论中具有广泛的应用潜力，通过进一步扩展可以处理更复杂的编码问题。

Abstract: One-shot information theory addresses scenarios in source coding and channel coding where the signal blocklength is assumed to be 1. In this case, each source and channel can be used only once, and the sources and channels are arbitrary and not required to be memoryless or ergodic. We study the achievability part of one-shot information theory, i.e., we consider explicit coding schemes in the oneshot scenario. The objective is to derive one-shot achievability results that can imply existing (first-order and second-order) asymptotic results when applied to memoryless sources and channels, or applied to systems with memory that behave ergodically.
  Poisson functional representation was first proposed as a one-shot channel simulation technique by Li and El Gamal [118] for proving a strong functional representation lemma. It was later extended to the Poisson matching lemma by Li and Anantharam [117], which provided a unified one-shot coding scheme for a broad class of information-theoretic problems. The main contribution of this thesis is to extend the applicability of Poisson functional representation to various more complicated scenarios, where the original version cannot be applied directly and further extensions must be developed.

</details>


### [52] [A Hybrid Dominant-Interferer Approximation for SINR Coverage in Poisson Cellular Networks](https://arxiv.org/abs/2511.19568)
*Sunder Ram Krishnan,Junaid Farooq,Kumar Vijay Mishra,Xingchen Liu,S. Unnikrishna Pillai,Theodore S. Rappaport*

Main category: cs.IT

TL;DR: 提出了一种混合近似框架，结合蒙特卡洛采样和拉普拉斯泛函表示，用于无线网络中的干扰建模，解决了传统随机几何和概率模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统随机几何方法涉及复杂嵌套积分和特殊函数，概率干扰模型仅在受限参数下可处理，需要一种更通用、数值稳定的干扰建模方法。

Method: 结合蒙特卡洛采样主导干扰源和拉普拉斯泛函表示残余远场干扰，构建主导+尾部的模块化结构。

Result: 该方法提供了数值稳定、路径损耗无关的估计器，适用于噪声受限和干扰受限场景，并推导了理论误差界。

Conclusion: 该混合框架有效解决了无线网络干扰建模的挑战，验证了其相对于传统方法的优势。

Abstract: Accurate radio propagation and interference modeling is essential for the design and analysis of modern cellular networks. Stochastic geometry offers a rigorous framework by treating base station locations as a Poisson point process and enabling coverage characterization through spatial averaging, but its expressions often involve nested integrals and special functions that limit general applicability. Probabilistic interference models seek closed-form characterizations through moment-based approximations, yet these expressions remain tractable only for restricted parameter choices and become unwieldy when interference moments lack closed-form representations. This work introduces a hybrid approximation framework that addresses these challenges by combining Monte Carlo sampling of a small set of dominant interferers with a Laplace functional representation of the residual far-field interference. The resulting dominant-plus-tail structure provides a modular, numerically stable, and path-loss-agnostic estimator suitable for both noise-limited and interference-limited regimes. We further derive theoretical error bounds that decrease with the number of dominant interferers and validate the approach against established stochastic geometry and probabilistic modeling benchmarks.

</details>


### [53] [Computer-aided Characterization of Fundamental Limits of Coded Caching with Linear Coding](https://arxiv.org/abs/2511.19639)
*Niccolò Brembilla,Yinbin Ma,Pietro Belotti,Federico Malucelli,Daniela Tuninetti*

Main category: cs.IT

TL;DR: 本文提出了一个高效的计算机辅助框架，用于在编码缓存系统中表征线性编码约束下的基本极限。该框架考虑非香农型不等式，利用对称结构和问题特定约束来降低线性规划的复杂度，得到了比先前分析方法更紧的反向界。


<details>
  <summary>Details</summary>
Motivation: 受Tian以及Cao和Xu先前工作的启发，本文旨在在编码缓存系统中表征线性编码约束下的基本极限，特别是考虑非香农型不等式以得到更紧的界限。

Method: 提出了一个计算机辅助框架，考虑对可表示多拟阵有效的非香农型不等式，并利用编码缓存的对称结构和问题特定约束来降低线性规划的复杂度。

Result: 推导的反向界比先前已知的分析方法更紧，并证明了在线性编码放置和传输约束下某些可实现内存-负载权衡点的最优性。

Conclusion: 结果表明，结合最小公共信息构造的小型结构化需求子集可能足以表征线性编码下的最优权衡。

Abstract: Inspired by prior work by Tian and by Cao and Xu, this paper presents an efficient computer-aided framework to characterize the fundamental limits of coded caching systems under the constraint of linear coding. The proposed framework considers non-Shannon-type inequalities which are valid for representable polymatroids (and hence for linear codes), and leverages symmetric structure and problem-specific constraints of coded caching to reduce the complexity of the linear program. The derived converse bounds are tighter compared to previous known analytic methods, and prove the optimality of some achievable memory-load tradeoff points under the constraint of linear coding placement and delivery. These results seem to indicate that small, structured demand subsets combined with minimal common information constructions may be sufficient to characterize optimal tradeoffs under linear coding.

</details>


### [54] [Two-Step Decoding of Binary $2\times2$ Sum-Rank-Metric Codes](https://arxiv.org/abs/2511.19812)
*Hao Wu,Bocong Chen,Guanghui Zhang,Hongwei Liu*

Main category: cs.IT

TL;DR: 本文解决了Chen-Cheng-Qi提出的开放问题，证明二进制和秩度量码的解码可以完全简化为其组成汉明码的解码，无需额外的d1≥2/3dsr条件限制。


<details>
  <summary>Details</summary>
Motivation: Chen-Cheng-Qi在2025年提出了一个开放问题：能否将2×2矩阵块的二进制和秩度量码的解码完全简化为组成汉明码C1和C2的解码，而不需要他们快速解码器中的额外条件d1≥2/3dsr？

Method: 提出了一个简单的两步解码过程：首先对C2进行唯一解码，然后对C1应用单次错误/擦除解码。

Result: 证明限制性假设d1≥2/3dsr在理论上是不必要的，解码器可以达到⌊(dsr-1)/2⌋的唯一解码能力，总体复杂度为T2+T1。

Conclusion: 这种简化在black-box模型中是渐进最优的，任何和秩解码器都必须固有地解码组成汉明码。对于F4上的BCH或Goppa实例化，解码器运行时间为O(ℓ²)。

Abstract: We resolve an open problem posed by Chen--Cheng--Qi (IEEE Trans.\ Inf.\ Theory, 2025): can decoding of binary sum-rank-metric codes $\SR(C_1,C_2)$ with $2\times2$ matrix blocks be reduced entirely to decoding the constituent Hamming-metric codes $C_1$ and $C_2$ without the additional requirement $d_1\ge\tfrac{2}{3}d_{\mathrm{sr}}$ that underlies their fast decoder? We answer this in the affirmative by exhibiting a simple two-step procedure: first uniquely decode $C_2$, then apply a single error/erasure decoding of $C_1$.This shows that the restrictive hypothesis $d_1\ge\tfrac{2}{3}d_{\mathrm{sr}}$ is theoretically unnecessary.The resulting decoder achieves unique decoding up to $\lfloor (d_{\mathrm{sr}}-1)/2\rfloor$ with overall cost $T_2+T_1$, where $T_2$ and $T_1$ are the complexities of the Hamming decoders for $C_2$ and $C_1$, respectively. We further show that this reduction is asymptotically optimal in a black-box model, as any sum-rank decoder must inherently decode the constituent Hamming codes.For BCH or Goppa instantiations over $\F_4$, the decoder runs in $O(\ell^2)$ time.

</details>


### [55] [Towards Edge General Intelligence: Knowledge Distillation for Mobile Agentic AI](https://arxiv.org/abs/2511.19947)
*Yuxuan Wu,Linghan Ma,Ruichen Zhang,Yinqiu Liu,Dusit Niyato,Shunpu Tang,Zehui Xiong,Zhu Han,Zhaohui Yang,Kaibin Huang,Zhaoyang Zhang,Kai-Kit Wong*

Main category: cs.IT

TL;DR: 本调查探讨了知识蒸馏在边缘通用智能中的应用，重点研究了针对无线通信和移动网络的KD技术，以及适合边缘部署的新型架构，旨在为移动代理AI提供高效的KD驱动框架。


<details>
  <summary>Details</summary>
Motivation: 移动边缘设备部署智能代理AI面临计算、能源和存储资源有限的挑战，需要寻找高效解决方案来支持动态、资源受限环境中的自主智能操作。

Method: 调查了知识蒸馏技术在边缘通用智能中的集成，包括信道感知自蒸馏、跨模型CSI反馈蒸馏、鲁棒调制/分类蒸馏等专门针对无线通信的KD技术，以及Mamba、RWKV和跨架构蒸馏等新型架构。

Result: KD技术能够实现边缘设备上的高效、通信感知和可扩展智能，支持视觉、语音和多模态任务的EGI应用。

Conclusion: 知识蒸馏是边缘通用智能的关键推动者，但仍面临挑战，需要进一步研究KD在EGI中的发展方向。

Abstract: Edge General Intelligence (EGI) represents a paradigm shift in mobile edge computing, where intelligent agents operate autonomously in dynamic, resource-constrained environments. However, the deployment of advanced agentic AI models on mobile and edge devices faces significant challenges due to limited computation, energy, and storage resources. To address these constraints, this survey investigates the integration of Knowledge Distillation (KD) into EGI, positioning KD as a key enabler for efficient, communication-aware, and scalable intelligence at the wireless edge. In particular, we emphasize KD techniques specifically designed for wireless communication and mobile networking, such as channel-aware self-distillation, cross-model Channel State Information (CSI) feedback distillation, and robust modulation/classification distillation. Furthermore, we review novel architectures natively suited for KD and edge deployment, such as Mamba, RWKV (Receptance, Weight, Key, Value) and Cross-Architecture distillation, which enhance generalization capabilities. Subsequently, we examine diverse applications in which KD-driven architectures enable EGI across vision, speech, and multimodal tasks. Finally, we highlight the key challenges and future directions for KD in EGI. This survey aims to provide a comprehensive reference for researchers exploring KD-driven frameworks for mobile agentic AI in the era of EGI.

</details>


### [56] [Explainable Deep Learning for Secrecy Energy-Efficiency Maximization in Ambient Backscatter Multi-User NOMA Systems](https://arxiv.org/abs/2511.20108)
*Miled Alam,Abdul Karim Gizzini,Laurent Clavier*

Main category: cs.IT

TL;DR: 研究多用户下行NOMA系统中多个环境反向散射通信辅助下的保密能效，提出优化方法和深度学习预测器，显著提升能效并降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究环境反向散射通信辅助的NOMA系统在被动窃听者存在下的保密能效，解决多设备场景下的优化难题，为下一代物联网应用提供能效和安全保障。

Method: 针对两个反向散射设备推导闭式解，对多设备场景提出网格搜索和粒子群优化方法，并设计前馈神经网络预测器来近似最优解。

Result: AmBC显著提升保密能效，在高噪声环境下相比传统NOMA提升达615%；FNN模型达到95%以上准确率且降低复杂度；SHAP分析显示主导复合信道特征最具影响力。

Conclusion: 证明了可解释人工智能在构建可信赖的能效安全AmBC-NOMA系统方面的潜力，为下一代物联网应用提供了有效解决方案。

Abstract: In this paper, we investigate the secrecy energy-efficiency (SEE) of a multi-user downlink non-orthogonal multiple access (NOMA) system assisted by multiple ambient backscatter communications (AmBC) in the presence of a passive eavesdropper. We analyze both the trade-off and the ratio between the achievable secrecy sum-rate and total power consumption. In the special case of two backscatter devices (BDs), we derive closed-form solutions for the optimal reflection coefficients and power allocation by exploiting the structure of the SEE objective and the Pareto boundary of the feasible set. When more than two BDs are present, the problem becomes analytically intractable. To address this, we propose two efficient optimization techniques: (i) an exhaustive grid-based benchmark method, and (ii) a scalable particle swarm optimization algorithm. Furthermore, we design a deep learning-based predictor using a feedforward neural network (FNN), which closely approximates the optimal solutions. Numerical results show that the inclusion of AmBC significantly improves SEE, with gains up to 615% compared to conventional NOMA in high-noise regimes. Additionally, the FNN model achieves more than 95% accuracy compared to the optimal baseline, while reducing complexity. Finally, we employ SHAP (SHapley Additive exPlanations) to interpret the learned model, revealing that the most influential features correspond to the dominant composite channel components, in accordance with the theoretical system model. This demonstrates the potential of explainable artificial intelligence to build trust in energy-efficient and secure AmBC-NOMA systems for next-generation internet of things applications.

</details>


### [57] [On hierarchical secure aggregation against relay and user collusion](https://arxiv.org/abs/2511.20117)
*Min Xu,Xuejiao Han,Kai Wan,Gennian Ge*

Main category: cs.IT

TL;DR: 本文研究了同构网络中的分层安全聚合（HSA），针对中继和用户共谋攻击，建立了通信负载的基本界限，并设计了通信最优方案。


<details>
  <summary>Details</summary>
Motivation: 安全聚合是联邦学习中保护隐私的基础技术，但现有研究主要关注单层结构。本文针对分层网络中的中继和用户共谋问题，旨在设计既能完成聚合任务又能保护非共谋用户数据隐私的传输方案。

Method: 采用两阶段通信框架：用户向中继发送掩码数据，中继处理并转发编译消息给服务器进行精确求和恢复。利用网络函数计算的方法构建通信最优方案，并在循环网络中实现所需密钥大小的下界。

Result: 建立了用户-中继链路和中继-服务器链路的通信负载基本界限，推导了共谋弹性的可达阈值。当共谋中继和用户数量低于关键阈值时，构建了通信最优方案。在循环网络中证明了所需密钥大小下界的可达性。

Conclusion: 通过建立HSA与网络函数计算之间的联系，本文推进了安全聚合中通信效率和信息论安全的理论界限，为分层网络中的隐私保护提供了理论基础和实用方案。

Abstract: Secure aggregation (SA) is fundamental to privacy preservation in federated learning (FL), enabling model aggregation while preventing disclosure of individual user updates. This paper addresses hierarchical secure aggregation (HSA) against relay and user collusion in homogeneous networks, where each user connects to $n$ relays and each relay serves $m$ users. In the two-phase communication framework, users transmit masked data to relays, which then process and forward compiled messages to the server for exact sum recovery. The primary objective is to devise a transmission scheme such that the server can finish the aggregation task, while any group of $T_h$ colluding relays and $T_u$ colluding users cannot reveal any information about the data owned by the non-colluding users. In this study, we establish fundamental limits on the communication load, defined as the ratio of transmitted information size to original data size, for each user-relay link and each relay-server link. Achievable thresholds for collusion resilience are also derived. When the number of colluding relays and users falls below certain critical thresholds, we construct communication-optimal schemes using methods from network function computation. A limitation of these schemes is their reliance on large random keys. To address this, we derive a lower bound on the required key size and prove its achievability in cyclic networks, where users are connected to relays in a cyclic wrap-around manner. By establishing a connection between HSA and network function computation, this work advances the theoretical limits of communication efficiency and information-theoretic security in secure aggregation.

</details>


### [58] [General Multi-User Distributed Computing](https://arxiv.org/abs/2511.20127)
*Ali Khalesi*

Main category: cs.IT

TL;DR: 提出了一个统一的学习和信息理论框架GMUDC，用于多用户分布式计算和推理，通过联合优化计算、通信和精度来满足异构目标函数需求。


<details>
  <summary>Details</summary>
Motivation: 为分布式和联邦学习系统建立可扩展且资源最优的信息能量基础，特别适用于航空、卫星和边缘智能网络等能源和数据效率至关重要的场景。

Method: 引入双重分析：淬火设计考虑固定的子函数分配和网络拓扑，退火设计捕获从给定集合中均匀随机抽取分配和链接时的平均性能。

Result: 揭示了在计算和通信预算下，计算负载、通信负载和重构失真之间权衡的基本限制，建立了连接泛化能力与网络拓扑和资源分配的光谱覆盖对偶性。

Conclusion: 该框架为可扩展和资源最优的分布式系统提供了理论基础，能够实现高效且拓扑感知的分布式设计。

Abstract: This work develops a unified {learning- and information-theoretic} framework for distributed computation and inference across multiple users and servers. The proposed \emph{General Multi-User Distributed Computing (GMUDC)} model characterizes how computation, communication, and accuracy can be jointly optimized when users demand heterogeneous target functions that are arbitrary transformations of shared real-valued subfunctions. Without any separability assumption, and requiring only that each target function lies in a reproducing-kernel Hilbert space associated with a shift-invariant kernel, the framework remains valid for arbitrary connectivity and task-assignment topologies. A dual analysis is introduced: the \emph{quenched design} considers fixed assignments of subfunctions and network topology, while the \emph{annealed design} captures the averaged performance when assignments and links are drawn uniformly at random from a given ensemble. These formulations reveal the fundamental limits governing the trade-offs among computing load, communication load, and reconstruction distortion under computational and communication budgets~$Γ$ and~$Δ$. The analysis establishes a spectral-coverage duality linking generalization capability with network topology and resource allocation, leading to provably efficient and topology-aware distributed designs. The resulting principles provide an \emph{information-energy foundation} for scalable and resource-optimal distributed and federated learning systems, with direct applications to aeronautical, satellite, and edge-intelligent networks where energy and data efficiency are critical.

</details>


### [59] [CSI Prediction Frameworks for Enhanced 5G Link Adaptation: Performance-Complexity Trade-offs](https://arxiv.org/abs/2511.20160)
*Francisco Díaz-Ruiz,Francisco J. Martín-Vega,Jose A. Cortés,Gerardo Gómez,Mari Carmen Aguayo*

Main category: cs.IT

TL;DR: 本文提出并评估了两种适用于TDD和FDD系统的CSI预测框架，在有效SINR域工作以降低复杂度。比较了经典维纳滤波器和基于GRU、LSTM、DNN的深度学习框架，结果显示维纳滤波器在计算复杂度更低的情况下性能接近GRU，但GRU在不同信道场景下具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 准确的CSI对于高效链路自适应至关重要，但信道老化、用户移动性和反馈延迟等挑战显著影响自适应调制编码的性能。

Method: 提出两种在有效SINR域工作的CSI预测框架，比较维纳滤波器与基于GRU、LSTM、DNN的深度学习方法的性能。

Result: 仿真结果表明，在可获得信道二阶统计信息的情况下，维纳滤波器在MSE和吞吐量方面性能接近GRU，且计算复杂度更低。但GRU模型在不同信道场景下表现出更好的泛化能力。

Conclusion: 基于学习的解决方案更适合TDD系统（基站处理计算），而经典方法的低复杂度使其成为FDD设置（功率受限的用户设备进行预测）的优选方案。

Abstract: Accurate and timely channel state information (CSI) is fundamental for efficient link adaptation. However, challenges such as channel aging, user mobility, and feedback delays significantly impact the performance of adaptive modulation and coding (AMC). This paper proposes and evaluates two CSI prediction frameworks applicable to both time division duplexing (TDD) and frequency division duplexing (FDD) systems. The proposed methods operate in the effective signal to interference plus noise ratio (SINR) domain to reduce complexity while preserving predictive accuracy. A comparative analysis is conducted between a classical Wiener filter and state-of-the-art deep learning frameworks based on gated recurrent units (GRUs), long short-term memory (LSTM) networks, and a delayed deep neural network (DNN). The evaluation considers the accuracy of the prediction in terms of mean squared error (MSE), the performance of the system, and the complexity of the implementation regarding floating point operations (FLOPs). Furthermore, we investigate the generalizability of both approaches under various propagation conditions. The simulation results show that the Wiener filter performs close to GRU in terms of MSE and throughput with lower computational complexity, provided that the second-order statistics of the channel are available. However, the GRU model exhibits enhanced generalization across different channel scenarios. These findings suggest that while learningbased solutions are well-suited for TDD systems where the base station (BS) handles the computation, the lower complexity of classical methods makes them a preferable choice for FDD setups, where prediction occurs at the power-constrained user equipment (UE).

</details>


### [60] [Unified Block Signal Processing Framework for LPWANs: Sequence Index Modulation Spreading](https://arxiv.org/abs/2511.20364)
*Wenkun Wen,Tierui Min,Long Yuan,Minghua Xia*

Main category: cs.IT

TL;DR: 提出了一种用于LPWAN的通用块信号传输统一框架，解决传统逐符号方法的局限性，支持准正交码字和异步多用户分离。


<details>
  <summary>Details</summary>
Motivation: 低功耗广域网需要高接收灵敏度和高效的物理层信号处理，传统逐符号方法存在局限性，需要统一的块传输框架。

Method: 框架包含三个关键组件：信号块向量、块内结构生成器和信号基础矩阵，利用循环移位扩频序列形成准正交码字。

Result: 实现了可靠的异步多用户分离，建立了块同步概念基础，提供了基于块相关匹配的统一解调结构。

Conclusion: 该工作推进了下一代LPWAN的可扩展和高效物理层设计，支持灵活系统实现。

Abstract: Low-power wide-area networks (LPWANs) demand high receiver sensitivity and efficient physical-layer signal processing. This paper introduces a unified framework for generalized block signal transmission in LPWANs, addressing the limitations of conventional symbol-by-symbol approaches. The framework comprises three key components: the signal block vector, the intra-block structure generator, and the signal basis matrix, and leverages quasi-orthogonal codewords formed through cyclically shifted spreading sequences. The resulting quasi-orthogonality enables reliable multi-user separation, particularly under asynchronous access. The framework establishes a conceptual foundation for block synchronization and provides a unified demodulation structure based on block correlation matching. It further supports flexible and systematic implementation, as demonstrated through applications to frequency-shift keying and chirp spread spectrum. This work advances scalable and efficient physical-layer design for next-generation LPWANs.

</details>


### [61] [Dimension-counting bounds for equi-isoclinic subspaces](https://arxiv.org/abs/2511.20642)
*Joseph W. Iverson,Kaysie Rose O*

Main category: cs.IT

TL;DR: 本文在最优子空间填充和等斜子空间理论中做出了四个贡献：新的块相干性下界、偶数维等斜子空间精确计数、等斜子空间数量新上界，以及特定维度下上界可达性的证明。


<details>
  <summary>Details</summary>
Motivation: 研究最优子空间填充和等斜子空间的理论问题，旨在改进相关参数的下界和上界，并精确计算特定情况下的等斜子空间数量。

Method: 主要采用维度计数方法，通过数学分析和几何论证来推导各种界限和计数结果。

Result: 获得了块相干性的新下界、偶数维等斜子空间的精确计数、等斜子空间数量的新上界，并证明了在特定维度下该上界可达。

Conclusion: 维度计数方法在最优子空间填充和等斜子空间理论中具有重要作用，能够有效推导各种界限和精确计数结果。

Abstract: We make four contributions to the theory of optimal subspace packings and equi-isoclinic subspaces: (1) a new lower bound for block coherence, (2) an exact count of equi-isoclinic subspaces of even dimension $r$ in $\mathbb{R}^{2r+1}$ with parameter $α\neq \tfrac{1}{2}$, (3) a new upper bound for the number of $r$-dimensional equi-isoclinic subspaces in $\mathbb{R}^d$ or $\mathbb{C}^d$, and (4) a proof that when $d=2r$, a further refinement of this bound is attained for every $r$ in the complex case and every $r=2^k$ in the real case. For each of these contributions, the proof ultimately relies on a dimension count.

</details>
