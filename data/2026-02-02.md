<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.IT](#cs.IT) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Toward Non-Expert Customized Congestion Control](https://arxiv.org/abs/2601.22461)
*Mingrui Zhang,Hamid Bagheri,Lisong Xu*

Main category: cs.NI

TL;DR: NECC是一个面向非专家用户的定制化拥塞控制算法框架，利用大语言模型和BPF接口简化定制算法的建模、实现和部署。


<details>
  <summary>Details</summary>
Motivation: 通用拥塞控制算法无法满足特定用户的特殊需求，而定制化算法需要专业知识，非专家用户难以实现。需要一种让非专家用户也能轻松创建定制化拥塞控制算法的解决方案。

Method: 提出NECC框架，结合大语言模型（LLM）和伯克利包过滤器（BPF）接口，帮助非专家用户建模、实现和部署定制化拥塞控制算法。

Result: 使用真实世界拥塞控制算法进行评估，NECC表现非常出色，证明了该框架的可行性和有效性。

Conclusion: NECC首次解决了定制化拥塞控制算法的实现问题，为非专家用户提供了便捷的定制化方案，并讨论了未来的研究方向。

Abstract: General-purpose congestion control algorithms (CCAs) are designed to achieve general congestion control goals, but they may not meet the specific requirements of certain users. Customized CCAs can meet certain users' specific requirements; however, non-expert users often lack the expertise to implement them. In this paper, we present an exploratory non-expert customized CCA framework, named NECC, which enables non-expert users to easily model, implement, and deploy their customized CCAs by leveraging Large Language Models and the Berkeley Packet Filter (BPF) interface. To the best of our knowledge, we are the first to address the customized CCA implementation problem. Our evaluations using real-world CCAs show that the performance of NECC is very promising, and we discuss the insights that we find and possible future research directions.

</details>


### [2] [Nethira: A Heterogeneity-aware Hierarchical Pre-trained Model for Network Traffic Classification](https://arxiv.org/abs/2601.22494)
*Chungang Lin,Weiyao Zhang,Haitong Luo,Xuying Meng,Yujun Zhang*

Main category: cs.NI

TL;DR: Nethira：基于分层重建和增强的异构感知预训练模型，用于网络流量分类，显著提升性能并减少标注数据依赖


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在处理网络流量时存在异构性（分层流量结构）与输入同质性（扁平字节序列）之间的差距，这限制了模型学习全面流量结构信息的能力

Method: 提出Nethira模型，包含：1）预训练阶段采用分层重建（字节、协议、数据包三个层次）捕获全面流量结构信息；2）微调阶段采用一致性正则化策略配合分层流量增强减少标签依赖

Result: 在四个公开数据集上超越七个现有预训练模型，平均F1分数提升9.11%，在高异构网络任务中仅需1%标注数据即可达到可比性能

Conclusion: Nethira通过分层重建和增强有效解决了流量异构性与输入同质性之间的差距，显著提升了网络流量分类性能并大幅减少了对标注数据的依赖

Abstract: Network traffic classification is vital for network security and management. The pre-training technology has shown promise by learning general traffic representations from raw byte sequences, thereby reducing reliance on labeled data. However, existing pre-trained models struggle with the gap between traffic heterogeneity (i.e., hierarchical traffic structures) and input homogeneity (i.e., flattened byte sequences). To address this gap, we propose Nethira, a heterogeneity-aware pre-trained model based on hierarchical reconstruction and augmentation. In pre-training, Nethira introduces hierarchical reconstruction at multiple levels-byte, protocol, and packet-capturing comprehensive traffic structural information. During fine-tuning, Nethira proposes a consistency-regularized strategy with hierarchical traffic augmentation to reduce label dependence. Experiments on four public datasets demonstrate that Nethira outperforms seven existing pre-trained models, achieving an average F1-score improvement of 9.11%, and reaching comparable performance with only 1% labeled data on high-heterogeneity network tasks.

</details>


### [3] [Chance-Constrained Secrecy Optimization in Hybrid RIS-Empowered and UAV-Assisted Networks](https://arxiv.org/abs/2601.22499)
*Elhadj Moustapha Diallo,Mamadou Aliou Diallo,Abusaeed B. M. Adam,Muhammad Naeem Shah*

Main category: cs.NI

TL;DR: 论文提出了一种结合无人机RIS、室外STAR-RIS和室内全息RIS的混合可重构环境，用于增强室内外用户的安全下行通信，考虑了移动性、遮挡、窃听和硬件损伤等实际因素，通过交替优化框架降低安全中断概率。


<details>
  <summary>Details</summary>
Motivation: 现有RIS系统在应对用户移动性、动态遮挡、共谋窃听和硬件损伤等实际挑战时存在局限性，需要开发更鲁棒的混合可重构环境来同时保障室内外用户的安全通信质量。

Method: 提出混合可重构环境架构（无人机RIS+室外STAR-RIS+室内全息RIS），建立符合3GPP/ITU标准的随机信道模型，推导Bernstein型确定性近似处理机会约束，采用交替优化框架结合SCA方法分别求解基站波束成形、RIS配置和无人机位置子问题。

Result: 仿真结果表明，所提混合RIS架构相比基准方案显著降低了安全中断概率，对信道不确定性、遮挡、共谋窃听和硬件损伤表现出强鲁棒性，算法能单调降低安全中断成本并收敛到鲁棒化问题的稳定点。

Conclusion: 集成无人机RIS、STAR-RIS和全息RIS的混合可重构环境能有效增强室内外安全通信性能，提出的分布鲁棒优化框架为实际部署提供了理论和技术支持。

Abstract: This paper considers a hybrid reconfigurable environment comprising a UAV-mounted reflecting RIS, an outdoor STAR-RIS enabling simultaneous transmission and reflection, and an indoor holographic RIS (H-RIS), jointly enhancing secure downlink communication for indoor and outdoor users. The system operates under user mobility, dynamic blockages, colluding idle and active eavesdroppers, and transceiver and surface hardware impairments. A 3GPP and ITU-compliant stochastic channel model is developed, capturing mobility-induced covariance evolution, outdoor-indoor penetration losses, and distortion-aware noise due to practical EVM-based impairments. We aim to minimize the aggregate secrecy-outage probability subject to secrecy-rate constraints, QoS requirements, power limitations, and statistical CSI uncertainty. The resulting problem contains coupled secrecy and QoS chance constraints and nonlinear interactions among the BS beamforming vectors, multi-surface phase coefficients, and UAV position. To handle these difficulties, we derive rigorous Bernstein-type deterministic approximations for all chance constraints, yielding a distributionally robust reformulation. Building on this, we propose an alternating optimization framework that employs successive convex approximation (SCA) to convexify each block and solve the BS beamforming, RIS, STAR-RIS, H-RIS configuration, and UAV placement subproblems efficiently. The proposed algorithm is shown to monotonically decrease a smooth surrogate of the secrecy-outage cost and converge to a stationary point of the robustified problem. Simulations based on 3GPP TR 38.901, TR 36.873, and ITU-R P.2109 demonstrate that integrating UAV-RIS, STAR-RIS, and H-RIS significantly reduces secrecy-outage probability compared with benchmark schemes and provides strong robustness to channel uncertainty, blockages, colluding eavesdroppers, and hardware impairments.

</details>


### [4] [MCP-Diag: A Deterministic, Protocol-Driven Architecture for AI-Native Network Diagnostics](https://arxiv.org/abs/2601.22633)
*Devansh Lodha,Mohit Panchal,Sameer G. Kulkarni*

Main category: cs.NI

TL;DR: MCP-Diag是一个基于模型上下文协议的混合神经符号架构，通过确定性翻译层将网络诊断工具输出转换为JSON格式，并强制人机协作授权循环，解决了LLM在网络运维中的随机接地问题和安全风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在网络运维(AIOps)中的应用面临两大挑战：随机接地问题（LLM难以可靠解析非结构化、厂商特定的CLI输出）和安全风险（授予自主代理shell访问权限）。

Method: 提出MCP-Diag混合神经符号架构，包含：1）确定性翻译层，将dig、ping、traceroute等标准工具的原始输出转换为严格的JSON模式；2）强制"启发循环"，在协议层面实施人机协作授权机制。

Result: 初步评估显示，MCP-Diag实现了100%的实体提取准确率，执行延迟开销低于0.9%，上下文令牌使用量增加了3.7倍。

Conclusion: MCP-Diag通过确定性翻译和人机协作授权，有效解决了LLM在网络诊断中的可靠性和安全问题，为安全的AIOps提供了可行方案。

Abstract: The integration of Large Language Models (LLMs) into network operations (AIOps) is hindered by two fundamental challenges: the stochastic grounding problem, where LLMs struggle to reliably parse unstructured, vendor-specific CLI output, and the security gap of granting autonomous agents shell access. This paper introduces MCP-Diag, a hybrid neuro-symbolic architecture built upon the Model Context Protocol (MCP). We propose a deterministic translation layer that converts raw stdout from canonical utilities (dig, ping, traceroute) into rigorous JSON schemas before AI ingestion. We further introduce a mandatory "Elicitation Loop" that enforces Human-in-the-Loop (HITL) authorization at the protocol level. Our preliminary evaluation demonstrates that MCP-Diag achieving 100% entity extraction accuracy with less than 0.9% execution latency overhead and 3.7x increase in context token usage.

</details>


### [5] [Digital Twin Synchronization: towards a data-centric architecture](https://arxiv.org/abs/2601.23051)
*Eduardo Freitas,Assis T. de Oliveira Filho,Pedro R. X. do Carmo,Djamel Sadok,Judith Kelner*

Main category: cs.NI

TL;DR: 本文综述了数字孪生同步技术，分析了现有挑战，并提出统一同步架构以解决安全性和互操作性需求


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术是工业4.0的关键使能技术，但确保数字孪生与其物理对应物准确同步仍面临挑战，需要解决同步问题以提升工业系统效率

Method: 回顾当前采用的同步技术和架构，识别关键技术挑战，并提出统一的同步架构，同时解决安全性和互操作性需求

Result: 提出了适用于各种工业应用的统一同步架构，能够解决安全性和互操作性要求，填补现有技术空白

Conclusion: 需要标准化架构来确保数字孪生环境的稳健同步，实现工业系统的无缝运行和持续改进

Abstract: Digital Twin (DT) technology revolutionizes industrial processes by enabling the representation of physical entities and their dynamics to enhance productivity and operational efficiency. It has emerged as a vital enabling technology in the Industry 4.0 context. The present article examines the particular issue of synchronizing a digital twin while ensuring an accurate reflection of its physical counterpart. Despite the reported recent advances in the design of middleware and low delay communication technologies, effective synchronization between both worlds remains challenging. This paper reviews currently adopted synchronization technologies and architectures, identifies vital outstanding technical challenges, and proposes a unified synchronization architecture for use by various industrial applications while addressing security and interoperability requirements. As such, this study aims to bridges gaps and advance robust synchronization in DT environments, emphasizing the need for a standardized architecture to ensure seamless operation and continuous improvement of industrial systems.

</details>


### [6] [Lossy Compression of Cellular Network KPIs](https://arxiv.org/abs/2601.23105)
*Andrea Pimpinella,Fabio Palmese,Alessandro E. C. Redondi*

Main category: cs.NI

TL;DR: 该论文展示了移动蜂窝网络KPI可以通过预测、量化和熵编码等标准有损压缩方案高效压缩，实现8-10倍的压缩比，且不影响下游分析任务。


<details>
  <summary>Details</summary>
Motivation: 移动蜂窝网络KPI数据量巨大，从多个小区长时间收集的细粒度测量数据给存储、传输和大规模分析带来重大挑战，需要有效的压缩方案来减少报告开销。

Method: 采用基于预测、量化和熵编码的标准有损压缩方案，首先通过率失真分析表征流量KPI的内在可压缩性，然后评估KPI压缩对代表性下游分析任务的影响。

Result: 仅使用3-4比特/样本即可达到约30dB的信噪比，相比32位浮点表示实现8-10倍的压缩比；小区间聚合可减轻量化误差，预测精度在适度报告率下不受影响。

Conclusion: KPI压缩在蜂窝系统中是可行的，并且对网络级分析是透明的，表明压缩技术可以显著减少报告开销而不影响分析质量。

Abstract: Network Key Performance Indicators (KPIs) are a fundamental component of mobile cellular network monitoring and optimization. Their massive volume, resulting from fine-grained measurements collected across many cells over long time horizons, poses significant challenges for storage, transport, and large-scale analysis. In this letter, we show that common cellular KPIs can be efficiently compressed using standard lossy compression schemes based on prediction, quantization, and entropy coding, achieving substantial reductions in reporting overhead. Focusing on traffic volume KPIs, we first characterize their intrinsic compressibility through a rate-distortion analysis, showing that signal-to-noise ratios around 30 dB can be achieved using only 3-4 bits per sample, corresponding to an 8-10x reduction with respect to 32-bit floating-point representations. We then assess the impact of KPI compression on representative downstream analytics tasks. Our results show that aggregation across cells mitigates quantization errors and that prediction accuracy is unaffected beyond a moderate reporting rate. These findings indicate that KPI compression is feasible and transparent to network-level analytics in cellular systems.

</details>


### [7] [Smart Routing with Precise Link Estimation: DSEE-Based Anypath Routing for Reliable Wireless Networking](https://arxiv.org/abs/2405.10377)
*Narjes Nourzad,Bhaskar Krishnamachari*

Main category: cs.NI

TL;DR: 论文提出将DSEE多臂老虎机算法与最短任意路径路由结合，通过实时学习链路投递概率来提升无线网状网络中的路由可靠性，相比TSOR算法具有更好的遗憾边界。


<details>
  <summary>Details</summary>
Motivation: 在动态且资源受限的多跳无线网状网络中，传统路由协议依赖预定路径，在不可预测的链路条件下效果不佳。最短任意路径路由虽然能根据实时链路条件调整路由决策，但其效果依赖于链路质量和可靠性的准确估计，而这些变量难以确定预测。

Method: 提出一种新颖方法，利用确定性探索与利用排序（DSEE）多臂老虎机算法，为最短任意路径路由提供准确、实时的链路投递概率估计。通过将DSEE与任意路径路由耦合，算法持续学习并确保准确的投递概率估计，同时选择最合适的路径高效路由数据包。

Result: 该方法在波动的链路条件下增强了最短任意路径路由的可靠性和弹性。理论证明该方案具有可证明的近对数遗憾边界，并且相比之前提出的基于汤普森采样的机会路由（TSOR），在网络规模方面具有更好的遗憾缩放特性。

Conclusion: 通过将DSEE多臂老虎机算法与任意路径路由结合，能够有效解决动态无线网络中链路质量估计的挑战，提供更可靠的路由方案，并在理论上优于现有的TSOR方法。

Abstract: In dynamic and resource-constrained environments, such as multi-hop wireless mesh networks, traditional routing protocols often falter by relying on predetermined paths that prove ineffective in unpredictable link conditions. Shortest Anypath routing offers a solution by adapting routing decisions based on real-time link conditions. However, the effectiveness of such routing is fundamentally dependent on the quality and reliability of the available links, and predicting these variables with certainty is challenging. This paper introduces a novel approach that leverages the Deterministic Sequencing of Exploration and Exploitation (DSEE), a multi-armed bandit algorithm, to address the need for accurate and real-time estimation of link delivery probabilities. This approach augments the reliability and resilience of the Shortest Anypath routing in the face of fluctuating link conditions. By coupling DSEE with Anypath routing, this algorithm continuously learns and ensures accurate delivery probability estimation and selects the most suitable way to efficiently route packets while maintaining a provable near-logarithmic regret bound. We also theoretically prove that our proposed scheme offers better regret scaling with respect to the network size than the previously proposed Thompson Sampling-based Opportunistic Routing (TSOR).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [JAF: Judge Agent Forest](https://arxiv.org/abs/2601.22269)
*Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal*

Main category: cs.AI

TL;DR: JAF框架让法官智能体通过联合推理多个查询-响应对来提升评估质量，而不是孤立评估单个响应，从而帮助主要智能体通过集体视角改进输出。


<details>
  <summary>Details</summary>
Motivation: 传统法官智能体孤立评估每个查询-响应存在局限性，无法利用跨实例的模式和不一致性。需要一种能够进行联合推理、发现交叉模式并提供集体反馈的评估框架。

Method: 提出JAF框架：1）法官智能体对主要智能体生成的查询-响应对进行联合推理；2）结合信念传播和集成学习原则；3）开发灵活的局部敏感哈希算法，整合语义嵌入、LLM驱动的哈希谓词、分类标签监督和相关信息；4）通过ICL实例化。

Result: 在云配置错误分类的挑战性任务上进行了验证，JAF能够支持高效、可解释、关系感知的多样示例选择，并优化思维链推理路径的探索。

Conclusion: JAF将法官智能体从局部评估者提升为整体学习者，通过联合推理和集体视角帮助主要智能体改进，为智能体AI框架提供了更强大的评估和迭代改进机制。

Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.
  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.
  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.

</details>


### [9] [The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution](https://arxiv.org/abs/2601.22290)
*Khush Patel,Siva Surendira,Jithin George,Shreyas Kapale*

Main category: cs.AI

TL;DR: 提出Six Sigma Agent架构，通过任务分解、并行采样和共识投票实现企业级可靠性，将错误率从5%降至0.11%，成本降低80%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能力强大但本质上是概率性的，在企业部署中存在可靠性挑战，需要解决AI系统的可靠性问题

Method: 1) 任务分解为原子动作的依赖树；2) 微代理采样：每个任务在多个LLM上并行执行n次；3) 共识投票与动态缩放：聚类输出并选择获胜簇的答案

Result: 证明采样n个独立输出可将系统错误降至O(p^{ceil(n/2)})，5个代理将5%错误率降至0.11%，13个代理达到3.4 DPMO（六西格玛标准），在三个企业用例中可靠性提升14,700倍，成本降低80%

Conclusion: AI系统的可靠性源于原则性冗余和共识机制，而非单纯模型缩放，Six Sigma Agent为企业AI部署提供了可靠的解决方案

Abstract: Large Language Models demonstrate remarkable capabilities yet remain fundamentally probabilistic, presenting critical reliability challenges for enterprise deployment. We introduce the Six Sigma Agent, a novel architecture that achieves enterprise-grade reliability through three synergistic components: (1) task decomposition into a dependency tree of atomic actions; (2) micro-agent sampling where each task is executed n times in parallel across diverse LLMs to generate independent outputs; and (3) consensus voting with dynamic scaling, clustering outputs and selecting the answer from the winning cluster with maximum votes. We prove that sampling n independent outputs with error rate p achieves system error O(p^{ceil(n/2)}), enabling exponential reliability gains. Even using cheaper models with 5% per-action error, consensus voting with 5 agents reduces error to 0.11%; dynamic scaling to 13 agents achieves 3.4 DPMO (Defects Per Million Opportunities), the Six Sigma standard. Evaluation across three enterprise use cases demonstrates a 14,700x reliability improvement over single-agent execution while reducing costs by 80%. Our work establishes that reliability in AI systems emerges from principled redundancy and consensus rather than model scaling alone.

</details>


### [10] [Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents](https://arxiv.org/abs/2601.22311)
*Zehong Wang,Fang Wu,Hongru Wang,Xiangru Tang,Bolian Li,Zhenfei Yin,Yijun Ma,Yiyang Li,Weixiang Sun,Xiusi Chen,Yanfang Ye*

Main category: cs.AI

TL;DR: 论文提出FLARE方法，通过前瞻性规划和奖励估计解决LLM代理在长时程规划中的短视问题，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: LLM代理在短时程推理中表现良好，但在长时程规划中经常失败，因为逐步推理会导致短视的贪婪策略，早期决策无法考虑延迟后果。

Method: 提出FLARE方法：未来感知的前瞻规划与奖励估计，通过显式的前瞻、价值传播和有限承诺，让下游结果影响早期决策。

Result: 在多个基准测试、代理框架和LLM骨干网络中，FLARE一致提升任务性能和规划水平，LLaMA-8B+FLARE经常优于GPT-4o+标准逐步推理。

Conclusion: 研究明确了推理与规划之间的区别，FLARE作为未来感知规划的简单实现，有效解决了LLM代理在长时程规划中的核心失败模式。

Abstract: Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy policy that is adequate for short horizons but fails in long-horizon planning, where early actions must account for delayed consequences. From this planning-centric perspective, we study LLM-based agents in deterministic, fully structured environments with explicit state transitions and evaluation signals. Our analysis reveals a core failure mode of reasoning-based policies: locally optimal choices induced by step-wise scoring lead to early myopic commitments that are systematically amplified over time and difficult to recover from. We introduce FLARE (Future-aware Lookahead with Reward Estimation) as a minimal instantiation of future-aware planning to enforce explicit lookahead, value propagation, and limited commitment in a single model, allowing downstream outcomes to influence early decisions. Across multiple benchmarks, agent frameworks, and LLM backbones, FLARE consistently improves task performance and planning-level behavior, frequently allowing LLaMA-8B with FLARE to outperform GPT-4o with standard step-by-step reasoning. These results establish a clear distinction between reasoning and planning.

</details>


### [11] [Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?](https://arxiv.org/abs/2601.22329)
*Ala N. Tak,Amin Banayeeanzade,Anahita Bolourani,Fatemeh Bahrani,Ashutosh Chaubey,Sai Praneeth Karimireddy,Norbert Schwarz,Jonathan Gratch*

Main category: cs.AI

TL;DR: LLMs在理性决策测试中表现良好，但情感引导会扭曲其判断，不同引导方法在可控性和人类对齐行为之间存在权衡


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地应用于招聘、医疗和经济决策等高风险领域，需要评估它们是否表现出类似人类的（非）理性模式和偏见，以确定它们能否作为人类行为的模型或安全部署

Method: 评估多个LLM家族在：（1）理性选择核心公理的基准测试；（2）行为经济学和社会规范中情感影响判断的经典决策领域。使用两种情感引导方法：上下文引导（ICP）和表示层引导（RLS）

Result: 深思熟虑的"思考"能可靠提高理性并推动模型向期望值最大化发展。ICP产生强烈但难以校准的方向性偏移，RLS产生更符合心理学模式但可靠性较低的结果。提高理性的机制也会放大对情感干预的敏感性

Conclusion: 推理和情感引导之间存在张力，这对人类行为模拟和LLM决策系统的安全部署都有重要影响。不同引导方法在可控性和人类对齐行为之间存在权衡

Abstract: Large Language Models (LLMs) are increasingly positioned as decision engines for hiring, healthcare, and economic judgment, yet real-world human judgment reflects a balance between rational deliberation and emotion-driven bias. If LLMs are to participate in high-stakes decisions or serve as models of human behavior, it is critical to assess whether they exhibit analogous patterns of (ir)rationalities and biases. To this end, we evaluate multiple LLM families on (i) benchmarks testing core axioms of rational choice and (ii) classic decision domains from behavioral economics and social norms where emotions are known to shape judgment and choice. Across settings, we show that deliberate "thinking" reliably improves rationality and pushes models toward expected-value maximization. To probe human-like affective distortions and their interaction with reasoning, we use two emotion-steering methods: in-context priming (ICP) and representation-level steering (RLS). ICP induces strong directional shifts that are often extreme and difficult to calibrate, whereas RLS produces more psychologically plausible patterns but with lower reliability. Our results suggest that the same mechanisms that improve rationality also amplify sensitivity to affective interventions, and that different steering methods trade off controllability against human-aligned behavior. Overall, this points to a tension between reasoning and affective steering, with implications for both human simulation and the safe deployment of LLM-based decision systems.

</details>


### [12] [Learning Provably Correct Distributed Protocols Without Human Knowledge](https://arxiv.org/abs/2601.22369)
*Yujie Hui,Xiaoyi Lu,Andrew Perrault,Yang Wang*

Main category: cs.AI

TL;DR: GGMS是一个用于自动设计分布式协议的学习框架，结合了蒙特卡洛树搜索、Transformer编码器、全局深度优先搜索和模型检查反馈，能够搜索并验证正确的分布式协议。


<details>
  <summary>Details</summary>
Motivation: 分布式协议设计极其困难且耗时，传统方法难以在多智能体不完全信息博弈中找到正确协议，需要自动化方法来提高设计效率。

Method: 将协议设计建模为不完全信息博弈中的策略搜索问题，使用SMT指定正确性条件。GGMS框架整合了：1) 蒙特卡洛树搜索的变体；2) Transformer动作编码器；3) 全局深度优先搜索以跳出局部最优；4) 模型检查器的重复反馈。

Result: GGMS能够学习比现有方法更大规模设置下的正确协议，输出协议通过有界设置下的穷举模型检查验证正确性，并在温和假设下证明搜索过程的完备性。

Conclusion: GGMS提供了一个可证明完备的分布式协议自动设计框架，能够有效搜索和验证正确协议，显著扩展了可处理的问题规模。

Abstract: Provably correct distributed protocols, which are a critical component of modern distributed systems, are highly challenging to design and have often required decades of human effort. These protocols allow multiple agents to coordinate to come to a common agreement in an environment with uncertainty and failures. We formulate protocol design as a search problem over strategies in a game with imperfect information, and the desired correctness conditions are specified in Satisfiability Modulo Theories (SMT). However, standard methods for solving multi-agent games fail to learn correct protocols in this setting, even when the number of agents is small. We propose a learning framework, GGMS, which integrates a specialized variant of Monte Carlo Tree Search with a transformer-based action encoder, a global depth-first search to break out of local minima, and repeated feedback from a model checker. Protocols output by GGMS are verified correct via exhaustive model checking for all executions within the bounded setting. We further prove that, under mild assumptions, the search process is complete: if a correct protocol exists, GGMS will eventually find it. In experiments, we show that GGMS can learn correct protocols for larger settings than existing methods.

</details>


### [13] [Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems](https://arxiv.org/abs/2601.22401)
*Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: 使用Gemini AI系统评估700个Erdős问题数据库中的"开放"猜想，通过AI自然语言验证和人工专家评估相结合的方法，解决了13个问题，发现这些问题的"开放"状态更多是由于文献难以查找而非问题本身困难。


<details>
  <summary>Details</summary>
Motivation: 探索AI在半自主数学发现中的应用潜力，特别是如何利用AI系统性地评估大量数学猜想，并了解Erdős问题数据库中标记为"开放"的问题的真实状态。

Method: 采用混合方法：首先使用AI驱动的自然语言验证来缩小搜索范围，然后由人类专家评估正确性和新颖性。具体针对Bloom的Erdős问题数据库中标记为"开放"的700个猜想进行系统评估。

Result: 解决了13个标记为"开放"的问题：其中5个通过看似新颖的自主解决方案解决，8个通过识别现有文献中的先前解决方案解决。发现这些问题的"开放"状态更多是由于文献难以查找（obscurity）而非问题本身困难。

Conclusion: AI在数学猜想评估中具有潜力，但面临文献识别困难和"潜意识剽窃"风险。Erdős问题数据库中的"开放"状态问题更多反映了文献可访问性问题而非数学难度。AI辅助数学研究需要谨慎处理文献识别和原创性验证。

Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.

</details>


### [14] [AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability](https://arxiv.org/abs/2601.22418)
*Julius Sechang Mboli,Omolara Aderonke Ogungbemi*

Main category: cs.AI

TL;DR: 该研究评估了传统机器学习与深度学习方法在垃圾图像二分类任务上的表现，发现迁移学习模型DenseNet121表现最佳（91%准确率），并探讨了如何将其集成到实时决策支持系统中。


<details>
  <summary>Details</summary>
Motivation: 高效垃圾分类对智慧城市循环经济和资源回收至关重要，需要开发准确可靠的自动分类系统来减少填埋使用和环境影响。

Method: 使用25,077张垃圾图像（80/20训练/测试分割，增强并调整至150x150像素），评估传统机器学习（随机森林、SVM、AdaBoost）和深度学习方法（自定义CNN、VGG16、ResNet50）以及三种迁移学习模型（DenseNet121、EfficientNetB0、InceptionV3），并分析主成分分析对传统模型的影响。

Result: DenseNet121获得最高准确率（91%）和ROC-AUC（0.98），比最佳传统分类器高出20个百分点；主成分分析对传统方法改善有限，而迁移学习在有限数据条件下显著提升性能。

Conclusion: 迁移学习模型在垃圾分类任务中表现优异，特别是DenseNet121，这些模型可集成到实时数据驱动决策支持系统中，实现自动化垃圾分类，减少填埋使用和生命周期环境影响。

Abstract: Efficient waste sorting is crucial for enabling circular-economy practices and resource recovery in smart cities. This paper evaluates both traditional machine-learning (Random Forest, SVM, AdaBoost) and deep-learning techniques including custom CNNs, VGG16, ResNet50, and three transfer-learning models (DenseNet121, EfficientNetB0, InceptionV3) for binary classification of 25 077 waste images (80/20 train/test split, augmented and resized to 150x150 px). The paper assesses the impact of Principal Component Analysis for dimensionality reduction on traditional models. DenseNet121 achieved the highest accuracy (91 %) and ROC-AUC (0.98), outperforming the best traditional classifier by 20 pp. Principal Component Analysis (PCA) showed negligible benefit for classical methods, whereas transfer learning substantially improved performance under limited-data conditions. Finally, we outline how these models integrate into a real-time Data-Driven Decision Support System for automated waste sorting, highlighting potential reductions in landfill use and lifecycle environmental impacts.)

</details>


### [15] [When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis](https://arxiv.org/abs/2601.22433)
*Shahria Hoque,Ahmed Akib Jawad Karim,Md. Golam Rabiul Alam,Nirjhar Gope*

Main category: cs.AI

TL;DR: 本研究提出LLM-TOPSIS框架，结合大型语言模型与模糊TOPSIS多准则决策方法，用于自动化软件工程师招聘评估，准确率达91%。


<details>
  <summary>Details</summary>
Motivation: 在高度竞争的就业环境中，选择合适的员工对组织成功至关重要。传统招聘过程存在主观性、模糊性和偏见问题，需要更客观、可扩展的自动化解决方案。

Method: 1. 创建包含教育、工作经验、技能和自我介绍的LinkedIn数据集，并加入专家评估作为标准
2. 开发LLM-TOPSIS框架，结合大型语言模型与多准则决策理论
3. 使用模糊TOPSIS方法处理评估中的模糊性和主观性
4. 采用三角模糊数表示准则权重和分数
5. 微调DistilRoBERTa模型并与模糊TOPSIS集成进行候选人排名

Result: 1. 系统排名与人类专家评估高度一致
2. 在Experience属性和Overall属性上达到91%的准确率
3. 展示了NLP驱动框架在提高招聘可扩展性、一致性和减少偏见方面的潜力

Conclusion: NLP与模糊决策方法的结合在人员选拔中具有巨大潜力，能够提供可扩展且无偏见的招聘解决方案。未来工作将集中在扩大数据集、提高模型可解释性和在实际招聘场景中验证系统实用性。

Abstract: In this highly competitive employment environment, the selection of suitable personnel is essential for organizational success. This study presents an automated personnel selection system that utilizes sophisticated natural language processing (NLP) methods to assess and rank software engineering applicants. A distinctive dataset was created by aggregating LinkedIn profiles that include essential features such as education, work experience, abilities, and self-introduction, further enhanced with expert assessments to function as standards. The research combines large language models (LLMs) with multicriteria decision-making (MCDM) theory to develop the LLM-TOPSIS framework. In this context, we utilized the TOPSIS method enhanced by fuzzy logic (Fuzzy TOPSIS) to address the intrinsic ambiguity and subjectivity in human assessments. We utilized triangular fuzzy numbers (TFNs) to describe criteria weights and scores, thereby addressing the ambiguity frequently encountered in candidate evaluations. For candidate ranking, the DistilRoBERTa model was fine-tuned and integrated with the fuzzy TOPSIS method, achieving rankings closely aligned with human expert evaluations and attaining an accuracy of up to 91% for the Experience attribute and the Overall attribute. The study underlines the potential of NLP-driven frameworks to improve recruitment procedures by boosting scalability, consistency, and minimizing prejudice. Future endeavors will concentrate on augmenting the dataset, enhancing model interpretability, and verifying the system in actual recruitment scenarios to better evaluate its practical applicability. This research highlights the intriguing potential of merging NLP with fuzzy decision-making methods in personnel selection, enabling scalable and unbiased solutions to recruitment difficulties.

</details>


### [16] [Anytime Safe PAC Efficient Reasoning](https://arxiv.org/abs/2601.22446)
*Chengyao Yu,Hao Zeng,Youxin Zhu,Jianguo Huang,Huajun Zeng,Bingyi Jing*

Main category: cs.AI

TL;DR: 提出B-PAC推理方法，通过动态调整路由阈值，在部分反馈的在线设置中实现安全高效的推理，显著降低计算开销同时控制性能损失


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然性能出色但计算成本高、延迟大。现有选择性思考策略在在线设置中存在不可控错误，因为非思考模型的性能损失只能部分观测且数据非平稳

Method: 提出Betting Probably Approximately Correct (B-PAC)推理方法，利用逆倾向评分估计器构建候选阈值的测试超鞅，基于累积的统计证据动态调整路由阈值

Result: B-PAC推理显著降低计算开销，思考模型使用率降低高达81.01%，同时将性能损失控制在用户指定水平以下

Conclusion: B-PAC推理为在线环境中的安全高效推理提供了原则性方法，实现了任意时间有效的性能损失控制和效率保证

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks but suffer from high computational costs and latency. While selective thinking strategies improve efficiency by routing easy queries to non-thinking models, existing approaches often incur uncontrollable errors, especially in online settings where the performance loss of a non-thinking model is only partially observed and data are non-stationary. To address this, we propose Betting Probably Approximately Correct (B-PAC) reasoning, a principled method that enables anytime safe and efficient online reasoning under partial feedback. Specifically, we utilize inverse propensity scoring estimators to construct test supermartingales for candidate thresholds, and then dynamically adjust the routing threshold based on the accumulated statistical evidence of safety. Theoretically, we establish the anytime-valid performance loss control and the efficiency of B-PAC reasoning. Extensive experiments demonstrate that B-PAC reasoning significantly reduces computational overhead, decreasing thinking model usage by up to 81.01\%, while controlling the performance loss below the user-specified level.

</details>


### [17] [Controllable Information Production](https://arxiv.org/abs/2601.22449)
*Tristan Shah,Stas Tiomkin*

Main category: cs.AI

TL;DR: 本文提出了一种新的内在动机（IM）原则——可控信息生产（CIP），它避免了外部效用和设计者指定变量，通过开放环与闭环Kolmogorov-Sinai熵之间的差距来同时奖励混沌的追求和调节。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息传输的内在动机方法明确依赖于设计者选择哪些随机变量参与传输，存在设计者偏好的局限性。需要一种既避免外部效用又避免设计者指定变量的内在动机原则。

Method: 从最优控制理论推导出CIP目标，将其表示为开放环与闭环Kolmogorov-Sinai熵之间的差距。这种方法同时奖励对混沌的追求和调节，连接了外在和内在行为。

Result: 建立了CIP的关键理论特性，并在标准内在动机基准测试中证明了其有效性。

Conclusion: CIP提供了一种新颖的内在动机原则，避免了传统方法的局限性，通过信息生产而非传输的视角，为智能行为的生成提供了新的理论基础。

Abstract: Intrinsic Motivation (IM) is a paradigm for generating intelligent behavior without external utilities. The existing information-theoretic methods for IM are predominantly based on information transmission, which explicitly depends on the designer's choice of which random variables engage in transmission. In this work, we introduce a novel IM principle, Controllable Information Production (CIP), that avoids both external utilities and designer-specified variables. We derive the CIP objective from Optimal Control, showing a connection between extrinsic and intrinsic behaviors. CIP appears as the gap between open-loop and closed-loop Kolmogorov-Sinai entropies, which simultaneously rewards the pursuit and regulation of chaos. We establish key theoretical properties of CIP and demonstrate its effectiveness on standard IM benchmarks.

</details>


### [18] [Why Self-Rewarding Works: Theoretical Guarantees for Iterative Alignment of Language Models](https://arxiv.org/abs/2601.22513)
*Shi Fu,Yingjie Wang,Shengchao Hu,Peng Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文首次为自奖励语言模型（SRLMs）提供了严格的理论保证，揭示了其迭代改进机制的理论基础。


<details>
  <summary>Details</summary>
Motivation: 自奖励语言模型在无需外部反馈的情况下通过迭代改进取得了显著成功，但其核心机制缺乏理论解释，存在理解上的关键空白。

Method: 首先建立单步更新的下界，揭示对初始模型质量的依赖；然后推导完整迭代范式的有限样本误差界；最后在线性softmax模型类中实例化理论框架。

Result: 性能随样本量n以$\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$速率提升，对初始模型的依赖随迭代次数T呈指数衰减，解释了自奖励成功的原因。

Conclusion: 自奖励语言模型能够稳健克服不良初始化，通过将动态导向内部稳定性和一致性来实现成功，为实践提供了理论指导。

Abstract: Self-Rewarding Language Models (SRLMs) achieve notable success in iteratively improving alignment without external feedback. Yet, despite their striking empirical progress, the core mechanisms driving their capabilities remain unelucidated, leaving a critical gap in theoretical understanding. This paper provides the first rigorous theoretical guarantees for SRLMs. We first establish a lower bound that characterizes the fundamental limits of a single update step, revealing a critical dependence on the quality of the initial model. We then derive finite-sample error bounds for the full iterative paradigm, showing that performance improves at a rate of $\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$ with sample size $n$. Crucially, our analysis reveals that the dependence on the initial model decays exponentially with the number of iterations $T$. This provides a formal explanation for why self-rewarding succeeds: it robustly overcomes poor initialization by steering the dynamics toward internal stability and consistency. Finally, we instantiate our theoretical framework for the linear softmax model class, yielding tailored guarantees that connect our high-level insights to practical model architectures.

</details>


### [19] [Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution](https://arxiv.org/abs/2601.22528)
*Hongze Mi,Yibo Feng,WenJie Lu,Song Cao,Jinyuan Li,Yanming Li,Xuelin Zhang,Haotian Luo,Songyang Peng,He Cui,Tengfei Tian,Jun Fang,Hua Chai,Naiqiang Tan*

Main category: cs.AI

TL;DR: DMS是一种受达尔文进化论启发的自进化记忆系统，通过将复杂轨迹分解为可重用单元并实施效用驱动的自然选择，解决了GUI自动化中多模态大语言模型在长时跨应用任务中的记忆管理问题，显著提升了任务成功率和执行稳定性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLM）代理在GUI自动化中面临长时跨应用任务的挑战，主要受限于有限的上下文窗口。现有记忆系统难以适应动态GUI环境，存在高层意图与低层执行的粒度不匹配问题，以及静态积累过时经验导致代理产生幻觉的上下文污染问题。

Method: 提出达尔文记忆系统（DMS），一种自进化架构，将记忆构建为受"适者生存"法则支配的动态生态系统。DMS将复杂轨迹分解为独立可重用单元以实现组合灵活性，并实施效用驱动的自然选择来追踪生存价值，主动修剪次优路径并抑制高风险计划。

Result: 在真实世界多应用基准测试中，DMS无需训练成本或架构开销即可提升通用MLLM性能，平均成功率提升18.0%，执行稳定性提升33.9%，同时降低任务延迟，证明其作为GUI任务有效自进化记忆系统的能力。

Conclusion: DMS通过模拟自然选择的自进化记忆系统，有效解决了GUI自动化中长时跨应用任务的记忆管理瓶颈，为MLLM代理提供了适应动态环境的记忆演化机制，显著提升了任务执行性能。

Abstract: Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.

</details>


### [20] [Enhancing TableQA through Verifiable Reasoning Trace Reward](https://arxiv.org/abs/2601.22530)
*Tung Sum Thomas Kwok,Xinyu Wang,Hengzhi He,Xiaofeng Lin,Peng Lu,Liheng Ma,Chunhe Wang,Ying Nian Wu,Lei Ding,Guang Cheng*

Main category: cs.AI

TL;DR: RE-Tab是一个用于表格问答的即插即用框架，通过轻量级、无需训练的奖励建模来增强轨迹搜索，在表格转换过程中提供明确的奖励反馈，显著提升了问答准确率并降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 表格问答代理的训练面临独特挑战：答案不能从静态输入中推断，而需要通过表格状态的逐步转换进行推理，这引入了多步推理复杂性和环境交互。研究问题是：对表格转换动作提供明确反馈能否提升模型推理能力？

Method: 提出RE-Tab框架，将问题建模为部分可观测马尔可夫决策过程，通过轻量级、无需训练的奖励建模来增强轨迹搜索。在状态转换（"最佳动作是什么？"）和模拟推理（"我对输出确定吗？"）阶段提供可验证的奖励反馈，引导代理在表格状态中的导航。

Result: RE-Tab在表格问答中实现了最先进的性能，推理成本降低近25%。即插即用实现使问答准确率提升高达41.77%，测试时推理样本减少33.33%。在不同LLM和最先进基准测试中均显示出一致的改进模式。

Conclusion: 在表格转换中通过奖励反馈强制执行逐步推理对提升表格问答性能至关重要。RE-Tab框架具有通用性，能显著提高准确率并降低推理成本，为表格问答代理的训练提供了有效解决方案。

Abstract: A major challenge in training TableQA agents, compared to standard text- and image-based agents, is that answers cannot be inferred from a static input but must be reasoned through stepwise transformations of the table state, introducing multi-step reasoning complexity and environmental interaction. This leads to a research question: Can explicit feedback on table transformation action improve model reasoning capability? In this work, we introduce RE-Tab, a plug-and-play framework that architecturally enhances trajectory search via lightweight, training-free reward modeling by formulating the problem as a Partially Observable Markov Decision Process. We demonstrate that providing explicit verifiable rewards during State Transition (``What is the best action?'') and Simulative Reasoning (``Am I sure about the output?'') is crucial to steer the agent's navigation in table states. By enforcing stepwise reasoning with reward feedback in table transformations, RE-Tab achieves state-of-the-art performance in TableQA with almost 25\% drop in inference cost. Furthermore, a direct plug-and-play implementation of RE-Tab brings up to 41.77% improvement in QA accuracy and 33.33% drop in test-time inference samples for consistent answer. Consistent improvement pattern across various LLMs and state-of-the-art benchmarks further confirms RE-Tab's generalisability. The repository is available at https://github.com/ThomasK1018/RE_Tab .

</details>


### [21] [Decoding in Geometry: Alleviating Embedding-Space Crowding for Complex Reasoning](https://arxiv.org/abs/2601.22536)
*Yixin Yang,Qingxiu Dong,Zhifang Sui*

Main category: cs.AI

TL;DR: CraEG是一种基于嵌入空间几何的采样方法，通过缓解嵌入空间拥挤现象来提升LLM推理性能


<details>
  <summary>Details</summary>
Motivation: 现有采样方法仅关注token概率，忽略了嵌入空间中token之间的几何关系。研究发现嵌入空间拥挤现象（几何相近的token集中概率质量）与数学问题解决的成功率存在统计关联。

Method: 提出CraEG方法，这是一种即插即用的采样方法，通过几何引导的重加权来缓解嵌入空间拥挤现象。该方法无需训练、单次通过，且与标准采样策略兼容。

Result: 在多个模型和基准测试上的实验表明，CraEG能提升生成性能，在鲁棒性和多样性指标上都有改善。

Conclusion: 嵌入空间几何关系对LLM推理至关重要，CraEG通过考虑token间的几何关系有效改善了采样质量。

Abstract: Sampling-based decoding underlies complex reasoning in large language models (LLMs), where decoding strategies critically shape model behavior. Temperature- and truncation-based methods reshape the next-token distribution through global probability reweighting or thresholding to balance the quality-diversity tradeoff. However, they operate solely on token probabilities, ignoring fine-grained relationships among tokens in the embedding space. We uncover a novel phenomenon, embedding-space crowding, where the next-token distribution concentrates its probability mass on geometrically close tokens in the embedding space. We quantify crowding at multiple granularities and find a statistical association with reasoning success in mathematical problem solving. Motivated by this finding, we propose CraEG, a plug-and-play sampling method that mitigates crowding through geometry-guided reweighting. CraEG is training-free, single-pass, and compatible with standard sampling strategies. Experiments on multiple models and benchmarks demonstrate improved generation performance, with gains in robustness and diversity metrics.

</details>


### [22] [PerfGuard: A Performance-Aware Agent for Visual Content Generation](https://arxiv.org/abs/2601.22571)
*Zhipeng Chen,Zhongrui Zhang,Chao Zhang,Yifan Xu,Lan Yang,Jun Liu,Ke Li,Yi-Zhe Song*

Main category: cs.AI

TL;DR: PerfGuard是一个性能感知的视觉内容生成代理框架，通过建模工具性能边界来提升任务规划和执行的可靠性，解决了现有框架假设工具执行总是成功且无法适应工具更新的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理框架通常假设工具执行总是成功，仅依赖文本描述无法区分精确的性能边界，也不能适应迭代的工具更新。这在视觉内容生成（AIGC）等领域尤其成问题，因为细微的工具性能差异会显著影响结果质量。

Method: PerfGuard包含三个核心机制：1) 性能感知选择建模（PASM），用基于细粒度性能评估的多维评分系统替代通用工具描述；2) 自适应偏好更新（APU），通过比较理论排名与实际执行排名动态优化工具选择；3) 能力对齐规划优化（CAPO），引导规划器生成符合性能感知策略的子任务。

Result: 与最先进方法的实验比较表明，PerfGuard在工具选择准确性、执行可靠性和用户意图对齐方面具有优势，验证了其在复杂AIGC任务中的鲁棒性和实用性。

Conclusion: PerfGuard通过系统建模工具性能边界并将其集成到任务规划和调度中，解决了现有代理框架在工具执行不确定性方面的局限性，为视觉内容生成等领域的自动化任务处理提供了更可靠的解决方案。

Abstract: The advancement of Large Language Model (LLM)-powered agents has enabled automated task processing through reasoning and tool invocation capabilities. However, existing frameworks often operate under the idealized assumption that tool executions are invariably successful, relying solely on textual descriptions that fail to distinguish precise performance boundaries and cannot adapt to iterative tool updates. This gap introduces uncertainty in planning and execution, particularly in domains like visual content generation (AIGC), where nuanced tool performance significantly impacts outcomes. To address this, we propose PerfGuard, a performance-aware agent framework for visual content generation that systematically models tool performance boundaries and integrates them into task planning and scheduling. Our framework introduces three core mechanisms: (1) Performance-Aware Selection Modeling (PASM), which replaces generic tool descriptions with a multi-dimensional scoring system based on fine-grained performance evaluations; (2) Adaptive Preference Update (APU), which dynamically optimizes tool selection by comparing theoretical rankings with actual execution rankings; and (3) Capability-Aligned Planning Optimization (CAPO), which guides the planner to generate subtasks aligned with performance-aware strategies. Experimental comparisons against state-of-the-art methods demonstrate PerfGuard's advantages in tool selection accuracy, execution reliability, and alignment with user intent, validating its robustness and practical utility for complex AIGC tasks. The project code is available at https://github.com/FelixChan9527/PerfGuard.

</details>


### [23] [WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction](https://arxiv.org/abs/2601.22586)
*Qian Hong,Siyuan Chang,Xiao Zhou*

Main category: cs.AI

TL;DR: WED-Net是一个用于极端天气条件下城市时空预测的双分支Transformer架构，通过自注意力与交叉注意力分离内在交通模式和天气诱导模式，并使用判别器和因果数据增强提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在极端天气（如暴雨）下的城市时空预测存在挑战：1）事件罕见且动态性强；2）天气作为辅助输入通常使用粗粒度描述符；3）缺乏专门机制捕捉细粒度时空效应；4）现有因果方法忽视时间动态或依赖固定混杂因素分层。

Method: 提出WED-Net（Weather-Effect Disentanglement Network）：1）双分支Transformer架构，通过自注意力和交叉注意力分离内在交通模式和天气诱导模式；2）使用记忆库增强表示；3）自适应门控融合机制；4）引入判别器明确区分天气条件以促进解耦；5）设计因果数据增强策略，扰动非因果部分同时保留因果结构。

Result: 在三个城市的出租车流量数据集上实验表明，WED-Net在极端天气条件下表现出鲁棒性能，能够支持更安全的出行、灾害准备和城市韧性。

Conclusion: WED-Net通过解耦天气效应和内在交通模式，结合因果数据增强，显著提升了极端天气条件下的城市时空预测能力，具有实际应用价值。代码已开源。

Abstract: Urban spatio-temporal prediction under extreme conditions (e.g., heavy rain) is challenging due to event rarity and dynamics. Existing data-driven approaches that incorporate weather as auxiliary input often rely on coarse-grained descriptors and lack dedicated mechanisms to capture fine-grained spatio-temporal effects. Although recent methods adopt causal techniques to improve out-of-distribution generalization, they typically overlook temporal dynamics or depend on fixed confounder stratification. To address these limitations, we propose WED-Net (Weather-Effect Disentanglement Network), a dual-branch Transformer architecture that separates intrinsic and weather-induced traffic patterns via self- and cross-attention, enhanced with memory banks and fused through adaptive gating. To further promote disentanglement, we introduce a discriminator that explicitly distinguishes weather conditions. Additionally, we design a causal data augmentation strategy that perturbs non-causal parts while preserving causal structures, enabling improved generalization under rare scenarios. Experiments on taxi-flow datasets from three cities demonstrate that WED-Net delivers robust performance under extreme weather conditions, highlighting its potential to support safer mobility, highlighting its potential to support safer mobility, disaster preparedness, and urban resilience in real-world settings. The code is publicly available at https://github.com/HQ-LV/WED-Net.

</details>


### [24] [Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR](https://arxiv.org/abs/2601.22595)
*Hao Yi,Yulan Hu,Xin Li,Sheng Ouyang,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 提出一种主动学习方法，通过不确定性一致性指标改进RLVR，减少查询成本，在30%数据上达到全数据集性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法需要大量查询，标注成本高。研究是否能用更少但信息量更大的查询获得相似或更好的性能，将主动学习引入RLVR

Method: 提出不确定性一致性指标评估主观不确定性与客观不确定性的对齐程度。离线设置使用点二列相关系数，在线训练因采样有限和输出分布动态变化，提出新的在线变体，基于归一化优势和主观不确定性计算

Result: 方法在实验中始终优于随机选择和经典主动学习基线，仅用30%数据训练就能达到全数据集性能，有效降低推理任务的RLVR成本

Conclusion: 通过引入主动学习和不确定性一致性指标，显著减少了RLVR的查询需求，为数学推理等任务提供了更高效的强化学习验证方法

Abstract: Large Language Models (LLMs) have recently improved mathematical reasoning through Reinforcement Learning with Verifiable Reward (RLVR). However, existing RLVR algorithms require large query budgets, making annotation costly. We investigate whether fewer but more informative queries can yield similar or superior performance, introducing active learning (AL) into RLVR. We identify that classic AL sampling strategies fail to outperform random selection in this setting, due to ignoring objective uncertainty when only selecting by subjective uncertainty. This work proposes an uncertainty consistency metric to evaluate how well subjective uncertainty aligns with objective uncertainty. In the offline setting, this alignment is measured using the Point-Biserial Correlation Coefficient (PBC). For online training, because of limited sampling and dynamically shifting output distributions, PBC estimation is difficult. Therefore, we introduce a new online variant, computed from normalized advantage and subjective uncertainty. Theoretically, we prove that the online variant is strictly negatively correlated with offline PBC and supports better sample selection. Experiments show our method consistently outperforms random and classic AL baselines, achieving full-dataset performance while training on only 30% of the data, effectively reducing the cost of RLVR for reasoning tasks.

</details>


### [25] [From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents](https://arxiv.org/abs/2601.22607)
*Jiaxuan Gao,Jiaao Chen,Chuyi He,Wei-Chen Wang,Shusheng Xu,Hanrui Wang,Di Jin,Yi Wu*

Main category: cs.AI

TL;DR: 提出EigenData框架，结合自演化数据代理与验证器强化学习，通过合成工具对话和可执行检查器，实现无需昂贵人工标注的复杂工具使用行为引导


<details>
  <summary>Details</summary>
Motivation: 交互式工具使用代理需要处理多轮对话状态跟踪、多步工具执行和复杂指令遵循，但后训练面临挑战：高质量多轮工具使用数据合成难以扩展，强化学习可能因用户模拟噪声信号导致训练效率下降

Method: 提出统一框架EigenData：分层多代理引擎合成工具对话和可执行检查器，通过闭环自演化过程更新提示和工作流；在合成数据基础上开发RL配方：先微调用户模型，然后应用GRPO风格训练，使用轨迹级组相对优势和动态过滤

Result: 在tau^2-bench上评估，最佳模型在Airline任务达到73.0% pass^1，在Telecom任务达到98.3% pass^1，匹配或超越前沿模型

Conclusion: 研究结果为无需昂贵人工标注的复杂工具使用行为引导提供了可扩展的途径

Abstract: Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-quality multi-turn tool-use data is difficult to scale, and reinforcement learning (RL) could face noisy signals caused by user simulation, leading to degraded training efficiency. We propose a unified framework that combines a self-evolving data agent with verifier-based RL. Our system, EigenData, is a hierarchical multi-agent engine that synthesizes tool-grounded dialogues together with executable per-instance checkers, and improves generation reliability via closed-loop self-evolving process that updates prompts and workflow. Building on the synthetic data, we develop an RL recipe that first fine-tunes the user model and then applies GRPO-style training with trajectory-level group-relative advantages and dynamic filtering, yielding consistent improvements beyond SFT. Evaluated on tau^2-bench, our best model reaches 73.0% pass^1 on Airline and 98.3% pass^1 on Telecom, matching or exceeding frontier models. Overall, our results suggest a scalable pathway for bootstrapping complex tool-using behaviors without expensive human annotation.

</details>


### [26] [EntroCut: Entropy-Guided Adaptive Truncation for Efficient Chain-of-Thought Reasoning in Small-scale Large Reasoning Models](https://arxiv.org/abs/2601.22617)
*Hongxi Yan,Qingjie Liu,Yunhong Wang*

Main category: cs.AI

TL;DR: EntroCut：一种基于熵的动态截断方法，通过早期推理步骤中的输出分布熵来区分正确与错误推理，从而减少大型推理模型的token使用量，在保持精度的同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务中表现出色，但依赖冗长的中间推理步骤导致计算成本高昂。研究发现模型早期推理步骤的输出分布熵能可靠地区分正确与错误推理，这启发了开发无需训练的动态截断方法。

Method: 提出EntroCut方法，这是一种无需训练的动态截断技术。它通过识别模型输出分布中的高置信度状态（低熵状态），在这些状态下可以安全地终止推理过程，从而减少不必要的token生成。

Result: 在四个基准测试上的实验表明，EntroCut能够减少高达40%的token使用量，同时仅带来最小的精度损失。该方法在效率-性能权衡方面优于现有的无需训练方法。

Conclusion: 基于熵的动态截断为缓解大型推理模型的低效问题提供了实用方法，在保持推理质量的同时显著降低了计算成本，证明了早期推理步骤中的熵信息对优化推理效率具有重要价值。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks through extended chain-of-thought generation, but their reliance on lengthy intermediate steps incurs substantial computational cost. We find that the entropy of the model's output distribution in early reasoning steps reliably distinguishes correct from incorrect reasoning. Motivated by this observation, we propose EntroCut, a training-free method that dynamically truncates reasoning by identifying high-confidence states where reasoning can be safely terminated. To comprehensively evaluate the trade-off between efficiency and accuracy, we introduce the Efficiency-Performance Ratio (EPR), a unified metric that quantifies relative token savings per unit accuracy loss. Experiments on four benchmarks show that EntroCut reduces token usage by up to 40\% with minimal accuracy sacrifice, achieving superior efficiency-performance trade-offs compared with existing training-free methods. These results demonstrate that entropy-guided dynamic truncation provides a practical approach to mitigate the inefficiency of LRMs.

</details>


### [27] [SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly](https://arxiv.org/abs/2601.22623)
*Wei Zhu,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: SYMPHONY是一个新颖的多智能体规划框架，通过集成异构语言模型智能体池来增强蒙特卡洛树搜索的探索能力，相比单智能体方法显著提升了规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用单智能体框架进行MCTS规划，这种范式限制了探索能力，导致生成分支多样性不足和规划性能欠佳。需要克服单智能体方法的局限性。

Method: 提出SYMPHONY框架，集成异构语言模型智能体池，利用不同智能体的多样化推理模式来增强rollout多样性，促进更有效的探索。

Result: 在多个基准任务上的实证结果表明，SYMPHONY即使使用可在消费级硬件上部署的开源LLM也能实现强大性能。当通过API增强使用云端LLM时，性能进一步提升，超越了现有最先进的基线方法。

Conclusion: 异构多智能体协调在规划任务中具有显著有效性，SYMPHONY框架通过集成多样化智能体成功克服了单智能体方法的局限性，提升了MCTS规划性能。

Abstract: Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance. To overcome these limitations, we propose Synergistic Multi-agent Planning with Heterogeneous langauge model assembly (SYMPHONY), a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration. Empirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.

</details>


### [28] [Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling](https://arxiv.org/abs/2601.22636)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Chenliang Xu,Christopher White,Jianfeng Gao*

Main category: cs.AI

TL;DR: SABER方法通过Beta分布建模样本级成功概率，推导出解析缩放定律，仅需少量样本即可准确预测大规模并行采样下的攻击成功率，显著降低评估误差。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估通常基于单次或低预算对抗提示，低估了实际风险。攻击者可以利用大规模并行采样反复探测模型直到产生有害响应，需要更准确的大规模对抗风险预测方法。

Method: 提出SABER方法，使用Beta分布（伯努利分布的共轭先验）建模样本级成功概率，推导出解析缩放定律，实现从小预算测量可靠外推大N攻击成功率。

Result: 仅使用n=100个样本，锚定估计器预测ASR@1000的平均绝对误差为1.66，相比基线12.04降低了86.2%的估计误差，揭示了异质性风险缩放特征。

Conclusion: 该方法提供了低成本、可扩展的现实LLM安全评估方法，显示在标准评估下看似鲁棒的模型在并行对抗压力下可能经历快速非线性风险放大。

Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.

</details>


### [29] [Beyond Medical Chatbots: Meddollina and the Rise of Continuous Clinical Intelligence](https://arxiv.org/abs/2601.22645)
*Vaibhav Ram S. V. N. S,Swetanshu Agrawal,Samudra Banerjee,Abdul Muhsin*

Main category: cs.AI

TL;DR: 论文批评当前生成式医疗AI仅关注文本生成而非临床推理，提出临床情境智能(CCI)作为新能力类别，并介绍Meddollina系统，该系统通过治理优先设计在语言生成前约束推理，在16,412+医疗查询评估中表现出更符合临床需求的行为特征。


<details>
  <summary>Details</summary>
Motivation: 当前生成式医疗AI虽然看起来流畅且知识丰富，但本质上只是文本生成而非真正的临床推理。临床推理是在模糊性、不完全证据和纵向情境下的责任约束过程。生成中心系统存在过早结论、不合理确定性、意图漂移和多步决策不稳定等问题，这些问题是将医疗视为下一个token预测的结构性后果。

Method: 提出临床情境智能(CCI)作为现实世界临床使用所需的能力类别，定义其四个核心特征：持久情境意识、意图保持、有界推理和证据不足时的原则性延迟。引入Meddollina系统，采用治理优先设计，在语言实现前约束推理，优先考虑临床适当性而非生成完整性。Meddollina作为支持临床工作流程的连续智能层，同时保持临床医生权威。

Result: 在16,412+个异质医疗查询的评估中，Meddollina展现出独特的行为特征：校准的不确定性、在未明确情况下的保守推理、稳定的纵向约束遵守、以及相对于生成中心基线的减少推测性完成。这些结果表明，可部署的医疗AI不会仅通过扩展规模而出现。

Conclusion: 可部署的医疗AI需要从单纯扩展转向连续临床智能，其中进展应通过临床医生在不确定性下的对齐行为来衡量，而非流畅驱动的完成度。Meddollina展示了治理优先设计如何产生更符合临床需求的行为特征。

Abstract: Generative medical AI now appears fluent and knowledgeable enough to resemble clinical intelligence, encouraging the belief that scaling will make it safe. But clinical reasoning is not text generation. It is a responsibility-bound process under ambiguity, incomplete evidence, and longitudinal context. Even as benchmark scores rise, generation-centric systems still show behaviours incompatible with clinical deployment: premature closure, unjustified certainty, intent drift, and instability across multi-step decisions.
  We argue these are structural consequences of treating medicine as next-token prediction. We formalise Clinical Contextual Intelligence (CCI) as a distinct capability class required for real-world clinical use, defined by persistent context awareness, intent preservation, bounded inference, and principled deferral when evidence is insufficient.
  We introduce Meddollina, a governance-first clinical intelligence system designed to constrain inference before language realisation, prioritising clinical appropriateness over generative completeness. Meddollina acts as a continuous intelligence layer supporting clinical workflows while preserving clinician authority. We evaluate Meddollina using a behaviour-first regime across 16,412+ heterogeneous medical queries, benchmarking against general-purpose models, medical-tuned models, and retrieval-augmented systems.
  Meddollina exhibits a distinct behavioural profile: calibrated uncertainty, conservative reasoning under underspecification, stable longitudinal constraint adherence, and reduced speculative completion relative to generation-centric baselines. These results suggest deployable medical AI will not emerge from scaling alone, motivating a shift toward Continuous Clinical Intelligence, where progress is measured by clinician-aligned behaviour under uncertainty rather than fluency-driven completion.

</details>


### [30] [Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments](https://arxiv.org/abs/2601.22647)
*Jinwoo Jang,Minjong Yoo,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: TMoW：一种测试时世界模型混合框架，通过动态路由更新增强具身智能体在动态环境中的适应性


<details>
  <summary>Details</summary>
Motivation: 当前基于语言模型的具身智能体在动态环境中的适应性有限，需要构建准确灵活的世界模型来支持有效推理和决策。传统的MoE架构在部署后保持固定，难以适应动态环境中的未知领域。

Method: 提出测试时世界模型混合框架(TMoW)，包含三个核心组件：1) 多粒度原型路由，根据对象到场景级别的相似性调整混合；2) 测试时精炼，在推理过程中对齐未知领域特征与原型；3) 蒸馏混合增强，从少量数据和现有原型高效构建新模型。

Result: 在VirtualHome、ALFWorld和RLBench基准测试中表现出色，在零样本适应和少样本扩展场景中都取得了强劲性能，使具身智能体能够在动态环境中有效运行。

Conclusion: TMoW通过测试时动态更新路由函数，使具身智能体能够重组现有模型并集成新模型，实现了在动态环境中的持续适应，解决了传统MoE架构的刚性限制问题。

Abstract: Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we extend the Mixture-of-Experts (MoE) paradigm to embodied agents. While conventional MoE architectures modularize knowledge into expert components with pre-trained routing, they remain rigid once deployed, making them less effective for adapting to unseen domains in dynamic environments. We therefore propose Test-time Mixture of World Models (TMoW), a framework that enhances adaptability to unseen and evolving domains. TMoW updates its routing function over world models at test time, unlike conventional MoE where the function remains fixed, enabling agents to recombine existing models and integrate new ones for continual adaptation. It achieves this through (i) multi-granular prototype-based routing, which adapts mixtures across object- to scene-level similarities, (ii) test-time refinement that aligns unseen domain features with prototypes during inference, and (iii) distilled mixture-based augmentation, which efficiently constructs new models from few-shot data and existing prototypes. We evaluate TMoW on VirtualHome, ALFWorld, and RLBench benchmarks, demonstrating strong performance in both zero-shot adaptation and few-shot expansion scenarios, and showing that it enables embodied agents to operate effectively in dynamic environments.

</details>


### [31] [UCPO: Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2601.22648)
*Xianzhou Zeng,Jing Huang,Chunmei Xie,Gongrui Nan,Siye Chen,Mengyu Lu,Weiqi Xiong,Qixuan Zhou,Junhao Zhang,Qiang Zhu,Yadong Li,Xingzhong Xu*

Main category: cs.AI

TL;DR: UCPO框架通过三元优势解耦和动态不确定性奖励调整，解决现有RL范式中的优势偏差问题，提升LLM的不确定性表达能力。


<details>
  <summary>Details</summary>
Motivation: 构建可信赖的大语言模型需要赋予其内在的不确定性表达能力，以减轻幻觉问题。现有RL范式（如GRPO）由于二元决策空间和静态不确定性奖励，存在优势偏差问题，导致模型要么过于保守要么过于自信。

Method: 提出UCPO框架：1）三元优势解耦：分离并独立归一化确定性和不确定性rollouts，消除优势偏差；2）动态不确定性奖励调整：根据模型演化和实例难度实时校准不确定性权重。

Result: 在数学推理和通用任务上的实验结果表明，UCPO有效解决了奖励不平衡问题，显著提高了模型在其知识边界之外的可靠性和校准能力。

Conclusion: UCPO框架通过解决现有RL范式中的奖励黑客攻击和过度自信问题，为构建具有内在不确定性表达能力的大语言模型提供了有效解决方案。

Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.

</details>


### [32] [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)
*Wei Zhu,Lixing Yu,Hao-Ren Yao,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: TALC是一个任务感知的LLM委员会框架，通过蒙特卡洛树搜索动态选择专家模型，实现专业化感知的路由和自适应规划，在多个任务上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽视不同大语言模型之间的专业化差异，将所有LLM视为统一适用，这限制了它们适应不同推理需求和任务复杂度的能力。

Method: 提出任务感知LLM委员会(TALC)，整合多个LLM与蒙特卡洛树搜索，每个LLM配备结构化成功记忆档案，通过语义匹配当前推理上下文与历史成功经验，在决策点路由到最合适的模型，使用融合模型评估和历史效用分数的双信号机制估计节点价值。

Result: 在WebShop、HumanEval和24点游戏上的实验表明，TALC相比强基线实现了更高的任务成功率和改进的搜索效率。

Conclusion: TALC验证了专业化感知路由和自适应规划的优势，能够更好地利用不同LLM的专业化能力，提高决策性能。

Abstract: Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.

</details>


### [33] [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuefeng Xiao,Hongyan Xie,Li Huaqiu,Songshi Liang,Zhongxiang Dai,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: R2M是一个轻量级RLHF框架，通过利用策略模型的实时隐藏状态反馈来应对奖励过优化问题，实现奖励模型与策略分布漂移的对齐。


<details>
  <summary>Details</summary>
Motivation: 传统的RLHF方法容易受到奖励过优化的影响，即策略模型过度拟合奖励模型，利用虚假的奖励模式而非真正捕捉人类意图。现有缓解方法主要依赖表面语义信息，无法有效处理因连续策略分布漂移导致的奖励模型与策略模型之间的错位，这会加剧奖励过优化问题。

Method: R2M（Real-Time Aligned Reward Model）是一个新颖的轻量级RLHF框架。它超越了仅依赖预训练LLM语义表示的普通奖励模型，而是利用策略模型在RL过程中的演化隐藏状态（即策略反馈）来实时对齐策略的分布漂移。

Result: 该方法为通过实时利用策略模型反馈来改进奖励模型性能提供了一个有前景的新方向。

Conclusion: R2M通过引入策略反馈机制，能够更好地处理RLHF中的奖励过优化问题，实现奖励模型与策略分布漂移的实时对齐，为解决传统方法中的错位问题提供了有效途径。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.

</details>


### [34] [Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference](https://arxiv.org/abs/2601.22701)
*Emilien Biré,María Santos,Kai Yuan*

Main category: cs.AI

TL;DR: 提出一种无需重新训练VLM策略的推理增强方法：冻结VLM作为动作提议器，用轻量级离线训练的Q函数对候选动作重排序，选择最高价值动作执行。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)作为智能体在数字环境中的骨干存在适应性不足的问题，特别是在快速变化的网络环境中。传统微调方法需要大量模型训练和数据收集，成本高昂。

Method: 将VLM的角色解耦为高容量动作提议器和最终动作选择机制。冻结VLM策略，用它为给定状态生成候选动作集，然后用轻量级离线训练的Q函数对这些候选进行重排序，选择估计价值最高的动作执行。

Result: 在WebVoyager基准测试中，该方法显著提升了智能体成功率：Qwen2.5-VL-7B智能体从38.8%提升到55.7%，GPT-4.1智能体从82.4%提升到88.8%。

Conclusion: 该方法通过在推理时直接应用Q函数实现即时策略改进，避免了离线数据重标注和策略重新训练，为快速适应动态环境提供了一种高效解决方案。

Abstract: Vision-Language Models (VLMs) have become powerful backbones for agents to autonomously operate in digital environments like the web and operating systems. However, these models suffer from inadaptability to fast-changing environments like the web, which can be alleviated by fine-tuning requiring expansive model training and data collection. In this work, we introduce a novel paradigm for enhancing agentic VLM policies at inference without policy retraining. Fundamentally, our approach decouples the VLM's role as a high-capacity action proposer from the final action selection mechanism. We keep the VLM policy frozen and use it to generate a set of candidate actions for a given state. Then, a lightweight, offline-trained Q-function reranks these candidates, and the agent executes the action with the highest estimated value. The main contribution is to apply the Q-function directly during inference for immediate policy improvement, and not offline to relabel data for policy retraining. We demonstrate on the academic WebVoyager benchmark that our method significantly boosts agent success rates, improving a Qwen2.5-VL-7B agent from 38.8% to 55.7% and a proprietary GPT-4.1 agent from 82.4% to 88.8%.

</details>


### [35] [A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization](https://arxiv.org/abs/2601.22718)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.AI

TL;DR: 论文提出MinPRO方法，通过最小前缀比率替代传统token级重要性采样，解决LLM强化学习后训练中因策略偏移导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM强化学习后训练中，为提升训练效率通常采用离策略方式生成rollout数据，但采样策略与目标策略的差异会导致训练不稳定。传统token级重要性采样修正在大策略偏移时效果不佳。

Method: 提出MinPRO（最小前缀比率）方法，用非累积的基于前缀中最小token级比率替代不稳定的累积前缀比率，稳定大策略偏移下的LLM优化。

Result: 在密集和MoE LLM上的多个数学推理基准测试表明，MinPRO显著提高了离策略机制下的训练稳定性和峰值性能。

Conclusion: MinPRO通过更稳定的重要性采样修正机制，有效解决了LLM强化学习后训练中的离策略优化问题，为大规模语言模型的高效训练提供了新思路。

Abstract: Reinforcement learning (RL) post-training has increasingly demonstrated strong ability to elicit reasoning behaviors in large language models (LLMs). For training efficiency, rollouts are typically generated in an off-policy manner using an older sampling policy and then used to update the current target policy. To correct the resulting discrepancy between the sampling and target policies, most existing RL objectives rely on a token-level importance sampling ratio, primarily due to its computational simplicity and numerical stability. However, we observe that token-level correction often leads to unstable training dynamics when the degree of off-policyness is large. In this paper, we revisit LLM policy optimization under off-policy conditions and show that the theoretically rigorous correction term is the prefix importance ratio, and that relaxing it to a token-level approximation can induce instability in RL post-training. To stabilize LLM optimization under large off-policy drift, we propose a simple yet effective objective, Minimum Prefix Ratio (MinPRO). MinPRO replaces the unstable cumulative prefix ratio with a non-cumulative surrogate based on the minimum token-level ratio observed in the preceding prefix. Extensive experiments on both dense and mixture-of-experts LLMs, across multiple mathematical reasoning benchmarks, demonstrate that MinPRO substantially improves training stability and peak performance in off-policy regimes.

</details>


### [36] [AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement](https://arxiv.org/abs/2601.22758)
*Libin Qiu,Zhirong Gao,Junfu Chen,Yuhang Ye,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Shuo Tang*

Main category: cs.AI

TL;DR: AutoRefine框架从智能体执行历史中提取和维护双重形式的经验模式，包括用于复杂子任务的专用子智能体和用于静态知识的技能模式，通过持续维护机制防止知识库退化，在多个任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型智能体缺乏从经验中积累知识的能力，将每个任务视为独立挑战。现有方法将经验提取为扁平化的文本知识，无法捕捉复杂子任务的程序逻辑，且缺乏维护机制导致知识库随着经验积累而退化。

Method: 提出AutoRefine框架，从智能体执行历史中提取双重形式的经验模式：对于程序性子任务，提取具有独立推理和记忆的专用子智能体；对于静态知识，提取技能模式作为指导方针或代码片段。采用持续维护机制对模式进行评分、修剪和合并，防止知识库退化。

Result: 在ALFWorld、ScienceWorld和TravelPlanner三个任务上分别达到98.4%、70.4%和27.1%的成功率，步骤减少20-73%。在TravelPlanner上，自动提取的系统性能超过手动设计的系统（27.1% vs 12.1%），证明其能够捕捉程序性协调能力。

Conclusion: AutoRefine框架通过提取和维护双重形式的经验模式，有效解决了智能体经验积累和知识库退化问题，显著提升了任务执行效率和成功率，特别是在捕捉复杂子任务的程序逻辑方面表现出色。

Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.

</details>


### [37] [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)
*Shichao Ma,Zhiyuan Ma,Ming Yang,Xiaofan Li,Xing Wu,Jintao Du,Yu Cheng,Weiqiang Wang,Qiliang Liu,Zhengyang Zhou,Yang Wang*

Main category: cs.AI

TL;DR: TSPO通过引入首次出现潜在奖励机制，解决多轮工具集成推理中的双重同质化困境，显著提升LLM性能


<details>
  <summary>Details</summary>
Motivation: 当前基于RL的搜索增强推理框架主要依赖稀疏的结果级奖励，导致"双重同质化困境"：过程同质化（忽略思考、推理和工具使用过程）和组内同质化（粗粒度结果奖励导致组内优势估计效率低下）

Method: 提出Turn-level Stage-aware Policy Optimization (TSPO)，引入首次出现潜在奖励机制，将部分奖励分配给正确答案首次出现的步骤，保留过程级信号并增加组内奖励方差，无需外部奖励模型或标注

Result: TSPO显著优于现有基线方法，在Qwen2.5-3B和7B模型上分别实现平均24%和13.6%的性能提升

Conclusion: TSPO通过解决双重同质化问题，有效提升了多轮工具集成推理的性能，为LLM的强化学习训练提供了新的优化方向

Abstract: Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Homogenization Dilemma." This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.

</details>


### [38] [Learning with Challenges: Adaptive Difficulty-Aware Data Generation for Mobile GUI Agent Training](https://arxiv.org/abs/2601.22781)
*Linjia Kang,Zhimin Wang,Yongkang Zhang,Duo Wu,Jinghe Wang,Ming Ma,Haopeng Yan,Zhi Wang*

Main category: cs.AI

TL;DR: MobileGen框架通过自适应对齐训练难度与GUI智能体能力边界，生成高质量移动GUI交互轨迹数据，显著提升智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体训练数据生成方法缺乏对任务难度的细粒度控制，导致训练难度与智能体能力不匹配，限制了学习效果。受人类通过渐进挑战任务学习技能的启发，需要开发能自适应对齐训练难度与智能体能力边界的数据生成框架。

Method: 1) 将任务难度解耦为结构维度（如轨迹长度）和语义维度（如任务目标）；2) 在精选数据集上迭代评估智能体，构建其能力边界在双维度上的系统画像；3) 自适应计算任务难度概率分布，采样下一轮训练的目标难度；4) 基于采样难度，使用多智能体可控生成器合成高质量交互轨迹和对应任务指令。

Result: MobileGen在多个挑战性基准测试中，将GUI智能体的平均性能提升了1.57倍，显著优于现有数据生成方法。

Conclusion: 能力对齐的数据生成对于移动GUI智能体的有效训练至关重要，MobileGen通过自适应难度调整框架解决了训练难度与智能体能力不匹配的问题，为移动GUI智能体训练提供了高质量数据生成方案。

Abstract: Large-scale, high-quality interaction trajectories are essential for advancing mobile Graphical User Interface (GUI) agents. While existing methods typically rely on labor-intensive human demonstrations or automated model exploration to generate GUI trajectories, they lack fine-grained control over task difficulty. This fundamentally restricts learning effectiveness due to the mismatch between the training difficulty and the agent's capabilities. Inspired by how humans acquire skills through progressively challenging tasks, we propose MobileGen, a novel data generation framework that adaptively aligns training difficulty with the GUI agent's capability frontier. Specifically, MobileGen explicitly decouples task difficulty into structural (e.g., trajectory length) and semantic (e.g., task goal) dimensions. It then iteratively evaluates the agent on a curated prior dataset to construct a systematic profile of its capability frontier across these two dimensions. With this profile, the probability distribution of task difficulty is adaptively computed, from which the target difficulty for the next round of training can be sampled. Guided by the sampled difficulty, a multi-agent controllable generator is finally used to synthesize high-quality interaction trajectories along with corresponding task instructions. Extensive experiments show that MobileGen consistently outperforms existing data generation methods by improving the average performance of GUI agents by 1.57 times across multiple challenging benchmarks. This highlights the importance of capability-aligned data generation for effective mobile GUI agent training.

</details>


### [39] [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)
*Hamid Reza Akbari,Mohammad Hossein Sameti,Amir M. Mansourian,Mohammad Hossein Rohban,Hossein Sameti*

Main category: cs.AI

TL;DR: 该论文提出了一种基于整合信息理论（IIT）的奖励函数，用于优化语言模型生成文本的因果性、连贯性和整合性，从而在保持准确性的同时显著缩短输出长度。


<details>
  <summary>Details</summary>
Motivation: 追求人工通用智能（AGI）是语言模型发展的核心目标，而类似意识的处理能力可能成为关键促进因素。虽然当前语言模型不具备意识，但它们表现出与意识某些方面类似的行为。该研究旨在将领先的意识理论——整合信息理论（IIT）通过基于奖励的学习范式应用于语言模型。

Method: 基于整合信息理论（IIT）的核心原则，设计了一种新颖的奖励函数，用于量化文本的因果性、连贯性和整合性——这些特性与意识处理相关。通过奖励学习范式优化该函数，无需外部数据或辅助模型，概念简单且计算高效。

Result: 优化IIT启发的奖励函数能生成更简洁的文本。在领域外任务中，通过精细调优可实现输出长度减少高达31%，同时保持与基础模型相当的准确性水平。此外还分析了该方法对模型置信度校准和测试时计算扩展的广泛影响。

Conclusion: 提出的框架具有显著实用优势：概念简单、计算高效、无需外部数据或辅助模型，并利用通用能力驱动信号而非任务特定启发式。该工作为在语言模型中实现意识理论启发的优化提供了新途径。

Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git

</details>


### [40] [Conditional Performance Guarantee for Large Reasoning Models](https://arxiv.org/abs/2601.22790)
*Jianguo Huang,Hao Zeng,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: G-PAC推理框架：通过输入空间分组实现组级PAC保证，在保持计算效率的同时提供比边际PAC更强的条件风险控制


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然通过链式思维推理表现出色，但计算成本高昂。现有的PAC推理方法仅在边际情况下提供统计保证，无法提供精确的条件覆盖，需要更实用的框架来实现高效且有保障的推理。

Method: 提出G-PAC推理框架，通过划分输入空间实现组级PAC保证。开发两种具体实现：针对已知分组结构的Group PAC（G-PAC）推理和针对未知分组的Clustered PAC（C-PAC）推理。两种方法都能自适应地在思考模型和非思考模型之间切换。

Result: 理论证明G-PAC和C-PAC都能实现组条件风险控制，且分组在异构设置下能严格提高效率。在多个推理基准测试上的实验表明，两种方法成功实现了组条件风险控制，同时保持了显著的计算节省。

Conclusion: G-PAC推理框架提供了一种实用的方法，在保持计算效率的同时实现比边际PAC更强的统计保证，为高效可靠的推理系统提供了新思路。

Abstract: Large reasoning models have shown strong performance through extended chain-of-thought reasoning, yet their computational cost remains significant. Probably approximately correct (PAC) reasoning provides statistical guarantees for efficient reasoning by adaptively switching between thinking and non-thinking models, but the guarantee holds only in the marginal case and does not provide exact conditional coverage. We propose G-PAC reasoning, a practical framework that provides PAC-style guarantees at the group level by partitioning the input space. We develop two instantiations: Group PAC (G-PAC) reasoning for known group structures and Clustered PAC (C-PAC) reasoning for unknown groupings. We prove that both G-PAC and C-PAC achieve group-conditional risk control, and that grouping can strictly improve efficiency over marginal PAC reasoning in heterogeneous settings. Our experiments on diverse reasoning benchmarks demonstrate that G-PAC and C-PAC successfully achieve group-conditional risk control while maintaining substantial computational savings.

</details>


### [41] [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)
*Ji Shi,Peiming Guo,Meishan Zhang,Miao Zhang,Xuebo Liu,Min Zhang,Weili Guan*

Main category: cs.AI

TL;DR: CVeDRL：一种基于强化学习的代码验证器，通过语法、功能、分支覆盖和样本难度感知的奖励设计，在仅有0.6B参数下实现SOTA性能，比GPT-3.5提高28.97%通过率和15.08%分支覆盖率，推理速度快20倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的代码验证器面临数据稀缺、高失败率和推理效率低的问题。强化学习虽然提供无监督优化的可能，但仅使用功能奖励的朴素RL方法难以生成针对困难分支和样本的有效单元测试。

Method: 理论分析表明分支覆盖率、样本难度、语法和功能正确性可联合建模为RL奖励。基于此设计语法和功能感知奖励，并提出使用指数奖励塑造和静态分析指标的分支和样本难度感知RL方法。

Result: CVeDRL在仅0.6B参数下达到SOTA性能：比GPT-3.5提高28.97%通过率和15.08%分支覆盖率，推理速度比竞争基线快20倍以上。

Conclusion: 通过精心设计的奖励函数将分支覆盖率、样本难度、语法和功能正确性联合建模，CVeDRL证明了强化学习在代码验证任务中的有效性，在性能、效率和参数效率方面均优于现有方法。

Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git

</details>


### [42] [Aligning the Unseen in Attributed Graphs: Interplay between Graph Geometry and Node Attributes Manifold](https://arxiv.org/abs/2601.22806)
*Aldric Labarthe,Roland Bouffanais,Julien Randon-Furling*

Main category: cs.AI

TL;DR: 论文提出一种新的图表示学习方法，通过分离属性流形学习和结构对齐，解决传统方法中几何空间不兼容的问题，将几何冲突转化为可解释的结构描述符。


<details>
  <summary>Details</summary>
Motivation: 传统基于属性的图表示学习方法存在几何缺陷，它将两个可能不兼容的度量空间（节点属性空间和图结构空间）强行合并，导致破坏性的对齐，从而丢失了图生成过程的重要信息。

Method: 引入定制的变分自编码器，将流形学习与结构对齐分离。通过量化将属性流形映射到图热核所需的度量扭曲，将几何冲突转化为可解释的结构描述符。

Result: 实验表明该方法能够发现传统方法无法检测的连接模式和异常，证明了传统方法在理论上的不足和实践上的局限性。

Conclusion: 通过分离流形学习和结构对齐，将几何冲突转化为结构描述符的方法能够更好地捕捉图的生成过程信息，为图表示学习提供了更理论健全且实用的解决方案。

Abstract: The standard approach to representation learning on attributed graphs -- i.e., simultaneously reconstructing node attributes and graph structure -- is geometrically flawed, as it merges two potentially incompatible metric spaces. This forces a destructive alignment that erodes information about the graph's underlying generative process. To recover this lost signal, we introduce a custom variational autoencoder that separates manifold learning from structural alignment. By quantifying the metric distortion needed to map the attribute manifold onto the graph's Heat Kernel, we transform geometric conflict into an interpretable structural descriptor. Experiments show our method uncovers connectivity patterns and anomalies undetectable by conventional approaches, proving both their theoretical inadequacy and practical limitations.

</details>


### [43] [Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery](https://arxiv.org/abs/2601.22896)
*Xinyi Ke,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.AI

TL;DR: ASRO提出一个博弈论框架，将启发式发现重构为求解器和实例生成器之间的程序级协同进化，通过LLM驱动的响应预言机实现自适应课程学习，显著提升组合优化中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动启发式发现方法主要依赖静态评估和固定实例分布，容易导致过拟合和分布偏移下的泛化能力差。需要一种能够适应分布变化、避免静态评估局限性的新框架。

Method: ASRO将启发式发现建模为双人零和博弈，维护求解器和实例生成器双方不断增长的战略池，通过LLM驱动的响应预言机迭代扩展战略池，针对对手的混合元策略生成最佳响应，实现自适应、自生成的课程学习。

Result: 在多个组合优化领域，ASRO持续优于基于相同程序搜索机制的静态训练AHD基线，在多样化和分布外实例上实现了显著改进的泛化能力和鲁棒性。

Conclusion: ASRO通过博弈论框架将启发式发现重构为协同进化过程，用自适应课程学习替代静态评估，有效解决了现有方法的过拟合和泛化问题，为自动启发式发现提供了更鲁棒的解决方案。

Abstract: Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator. ASRO models their interaction as a two-player zero-sum game, maintains growing strategy pools on both sides, and iteratively expands them via LLM-based best-response oracles against mixed opponent meta-strategies, thereby replacing static evaluation with an adaptive, self-generated curriculum. Across multiple combinatorial optimization domains, ASRO consistently outperforms static-training AHD baselines built on the same program search mechanisms, achieving substantially improved generalization and robustness on diverse and out-of-distribution instances.

</details>


### [44] [MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop](https://arxiv.org/abs/2601.22900)
*Xuancheng Li,Haitao Li,Yujia Zhou,YiqunLiu,Qingyao Ai*

Main category: cs.AI

TL;DR: 提出多轮反馈引导的强化学习框架，通过动态多轮再生、互补学习信号和结构化反馈注入机制，利用丰富的语言反馈指导RLVR训练，在失败样本上提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR仅使用标量奖励，在失败样本上信息稀疏且无指导性，无法提供失败原因洞察。需要利用更丰富的语言反馈来指导训练，特别是在失败样本上。

Method: 提出多轮反馈引导强化学习框架，包含三个机制：1) 仅在失败样本上触发的反馈引导动态多轮再生；2) 轮内和跨轮优化的互补学习信号；3) 将结构化反馈注入模型推理过程。

Result: 在OpenR1-Math数据集上训练，该方法在域内表现优于监督微调和RLVR基线，并在域外表现出良好的泛化能力。

Conclusion: 利用语言反馈指导RLVR训练能有效提升推理能力，特别是在失败样本上，提出的多轮反馈框架为强化学习中的反馈利用提供了有效途径。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In this paper, we investigate how to leverage richer verbal feedback to guide RLVR training on failed samples, and how to convert such feedback into a trainable learning signal. Specifically, we propose a multi-turn feedback-guided reinforcement learning framework. It builds on three mechanisms: (1) dynamic multi-turn regeneration guided by feedback, triggered only on failed samples, (2) two complementary learning signals for within-turn and cross-turn optimization, and (3) structured feedback injection into the model's reasoning process. Trained on sampled OpenR1-Math, the approach outperforms supervised fine-tuning and RLVR baselines in-domain and generalizes well out-of-domain.

</details>


### [45] [Alignment among Language, Vision and Action Representations](https://arxiv.org/abs/2601.22948)
*Nicola Milano,Stefano Nolfi*

Main category: cs.AI

TL;DR: 研究发现语言、视觉和动作学习会形成部分共享的语义表示，支持模态无关的语义组织


<details>
  <summary>Details</summary>
Motivation: 探索不同学习模态（语言、视觉、动作）是否产生独特或共享的内部表示，挑战传统认为不同数据类型的模型会发展专门化、不可迁移表示的观点

Method: 在BabyAI平台上训练基于transformer的智能体执行目标导向行为，生成动作基础的语言嵌入，然后与大型语言模型（LLaMA、Qwen、DeepSeek、BERT）和视觉语言模型（CLIP、BLIP）的表示进行比较

Result: 动作表示与仅解码器语言模型和BLIP强对齐（精度@15：0.70-0.73），接近语言模型之间的对齐程度，但与CLIP和BERT的对齐显著较弱

Conclusion: 语言、视觉和动作表示会收敛到部分共享的语义结构，支持模态无关的语义组织，突显了在具身AI系统中跨领域迁移的潜力

Abstract: A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.

</details>


### [46] [EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning](https://arxiv.org/abs/2601.22964)
*Yufei He,Juncheng Liu,Zhiyuan Hu,Yulin Chen,Yue Liu,Yuan Sui,Yibo Li,Nuo Chen,Jun Hu,Bryan Hooi,Xinxing Xu,Jiang Bian*

Main category: cs.AI

TL;DR: 提出Med-Inquire基准测试真实临床诊断的多轮交互过程，并开发EvoClinician自进化智能体通过"诊断-评分-进化"循环学习高效诊断策略


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI采用"一次性"诊断模式，而真实临床诊断是迭代询问过程，需要主动收集信息并管理成本和时间，因此需要新的评估框架和智能体

Method: 1) 创建Med-Inquire基准，模拟真实诊断过程，隐藏完整病历，迫使智能体主动提问和检查；2) 提出EvoClinician自进化智能体，采用"诊断-评分-进化"循环：Actor诊断、Process Grader评估行动临床价值和资源效率、Evolver更新策略

Result: EvoClinician在Med-Inquire基准上优于持续学习基线和其他自进化智能体（如记忆智能体），展示了更好的诊断策略学习能力

Conclusion: Med-Inquire为医疗AI提供了更真实的评估框架，EvoClinician通过在线自进化机制能够学习高效的诊断策略，推动了医疗AI向真实临床工作流程的演进

Abstract: Prevailing medical AI operates on an unrealistic ''one-shot'' model, diagnosing from a complete patient file. However, real-world diagnosis is an iterative inquiry where Clinicians sequentially ask questions and order tests to strategically gather information while managing cost and time. To address this, we first propose Med-Inquire, a new benchmark designed to evaluate an agent's ability to perform multi-turn diagnosis. Built upon a dataset of real-world clinical cases, Med-Inquire simulates the diagnostic process by hiding a complete patient file behind specialized Patient and Examination agents. They force the agent to proactively ask questions and order tests to gather information piece by piece. To tackle the challenges posed by Med-Inquire, we then introduce EvoClinician, a self-evolving agent that learns efficient diagnostic strategies at test time. Its core is a ''Diagnose-Grade-Evolve'' loop: an Actor agent attempts a diagnosis; a Process Grader agent performs credit assignment by evaluating each action for both clinical yield and resource efficiency; finally, an Evolver agent uses this feedback to update the Actor's strategy by evolving its prompt and memory. Our experiments show EvoClinician outperforms continual learning baselines and other self-evolving agents like memory agents. The code is available at https://github.com/yf-he/EvoClinician

</details>


### [47] [Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text](https://arxiv.org/abs/2601.22975)
*Ximing Lu,David Acuna,Jaehun Jung,Jian Hu,Di Zhang,Shizhe Diao,Yunheng Zou,Shaokun Zhang,Brandon Cui,Mingjie Liu,Hyunwoo Kim,Prithviraj Ammanabrolu,Jan Kautz,Yi Dong,Yejin Choi*

Main category: cs.AI

TL;DR: 提出Golden Goose方法，通过将不可验证的互联网文本转化为多选问答任务，合成无限量的可验证强化学习任务，解决了RLVR数据稀缺问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证强化学习（RLVR）面临数据稀缺瓶颈，导致模型训练改进逐渐饱和。需要利用丰富的、富含推理但不可验证的互联网文本（如科学教科书）来扩展RLVR数据。

Method: 提出Golden Goose方法：1）从源文本中识别并掩码关键推理步骤；2）生成多样且合理的干扰项；3）将填空任务转化为多选问答任务，从而从不可验证的文本中合成RLVR任务。

Result: 构建了GooseReason-0.7M数据集（70万个任务），在数学、编程和科学领域。训练后模型在15个基准测试中达到SOTA，在网络安全领域也超越专门训练的7B模型。

Conclusion: Golden Goose方法能够有效利用丰富的不可验证互联网文本自动扩展RLVR数据，解决了数据稀缺瓶颈，为RLVR的规模化应用提供了新途径。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.

</details>


### [48] [Quantifying Model Uniqueness in Heterogeneous AI Ecosystems](https://arxiv.org/abs/2601.22977)
*Lei You*

Main category: cs.AI

TL;DR: 提出ISQED统计框架，通过干预控制量化模型独特性(PIER)，证明观测数据无法识别独特性，开发高效主动审计协议，展示传统方法(如Shapley值)检测冗余的失败


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从孤立预测器演变为复杂异构的生态系统，区分真正的行为新颖性与功能冗余成为关键治理挑战。需要建立科学方法来审计模型独特性，超越单一模型解释，实现可信AI生态系统治理。

Method: 提出In-Silico Quasi-Experimental Design (ISQED)框架，通过匹配干预隔离模型内在身份，量化Peer-Inexpressible Residual (PIER)作为独特性指标。开发DISCO估计器，采用自适应查询协议实现最小最大最优样本效率。

Result: 理论证明观测日志无法数学识别独特性；推导主动审计的缩放定律；展示合作博弈方法(如Shapley值)检测冗余失败。在计算机视觉、大语言模型、城市交通预测等多样化生态系统中验证框架有效性。

Conclusion: 该研究将可信AI从解释单一模型推进到建立基于干预的异构模型生态系统审计科学，为AI治理提供原则性方法，支持区分真正创新与功能冗余的生态系统管理。

Abstract: As AI systems evolve from isolated predictors into complex, heterogeneous ecosystems of foundation models and specialized adapters, distinguishing genuine behavioral novelty from functional redundancy becomes a critical governance challenge. Here, we introduce a statistical framework for auditing model uniqueness based on In-Silico Quasi-Experimental Design (ISQED). By enforcing matched interventions across models, we isolate intrinsic model identity and quantify uniqueness as the Peer-Inexpressible Residual (PIER), i.e. the component of a target's behavior strictly irreducible to any stochastic convex combination of its peers, with vanishing PIER characterizing when such a routing-based substitution becomes possible. We establish the theoretical foundations of ecosystem auditing through three key contributions. First, we prove a fundamental limitation of observational logs: uniqueness is mathematically non-identifiable without intervention control. Second, we derive a scaling law for active auditing, showing that our adaptive query protocol achieves minimax-optimal sample efficiency ($dσ^2γ^{-2}\log(Nd/δ)$). Third, we demonstrate that cooperative game-theoretic methods, such as Shapley values, fundamentally fail to detect redundancy. We implement this framework via the DISCO (Design-Integrated Synthetic Control) estimator and deploy it across diverse ecosystems, including computer vision models (ResNet/ConvNeXt/ViT), large language models (BERT/RoBERTa), and city-scale traffic forecasters. These results move trustworthy AI beyond explaining single models: they establish a principled, intervention-based science of auditing and governing heterogeneous model ecosystems.

</details>


### [49] [Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory](https://arxiv.org/abs/2601.22984)
*Yuhao Zhan,Tianyu Fan,Linxuan Huang,Zirui Guo,Chao Huang*

Main category: cs.AI

TL;DR: 提出了DeepHalluBench评估框架，通过过程感知而非结果导向的方法诊断深度研究代理的幻觉问题，揭示了现有系统在对抗性场景下的可靠性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理的评估主要依赖端到端结果，无法揭示研究轨迹中关键的中间幻觉（如错误规划），需要从结果导向转向过程感知的评估方法。

Method: 提出PIES分类法，按功能组件（规划vs总结）和错误属性（显式vs隐式）对幻觉分类，构建细粒度评估框架分解研究轨迹，并创建包含100个幻觉易发任务的DeepHalluBench数据集。

Result: 对6个最先进的深度研究代理的实验显示，所有系统都未能实现稳健可靠性，诊断分析发现失败源于系统性缺陷，特别是幻觉传播和认知偏见。

Conclusion: 过程感知评估揭示了深度研究代理的根本性缺陷，PIES分类法和DeepHalluBench为未来架构优化提供了基础性见解，有助于构建更可靠的AI研究系统。

Abstract: Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this gap, we propose a shift from outcome-based to process-aware evaluation by auditing the full research trajectory. We introduce the PIES Taxonomy to categorize hallucinations along functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit). We instantiate this taxonomy into a fine-grained evaluation framework that decomposes the trajectory to rigorously quantify these hallucinations. Leveraging this framework to isolate 100 distinctively hallucination-prone tasks including adversarial scenarios, we curate DeepHalluBench. Experiments on six state-of-theart DRAs reveal that no system achieves robust reliability. Furthermore, our diagnostic analysis traces the etiology of these failures to systemic deficits, specifically hallucination propagation and cognitive biases, providing foundational insights to guide future architectural optimization. Data and code are available at https://github.com/yuhao-zhan/DeepHalluBench.

</details>


### [50] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: TriCEGAR：基于轨迹驱动的抽象机制，自动从执行日志构建状态抽象，支持在线构建智能体行为MDP，实现自动化运行时验证


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统通过工具行动，行为在长期随机交互轨迹中演化，使得保证变得复杂。现有动态概率保证(DPA)方法需要手动定义状态抽象，这增加了采用摩擦并耦合了应用特定启发式方法

Method: 提出TriCEGAR方法：1) 从轨迹学习谓词树作为抽象表示 2) 使用反例进行细化 3) 实现框架原生实现，捕获类型化智能体生命周期事件 4) 从轨迹构建抽象 5) 构建MDP 6) 执行概率模型检查

Result: 能够计算概率边界如Pmax(成功)和Pmin(失败)，并通过运行似然实现异常检测作为护栏信号

Conclusion: TriCEGAR自动化了状态抽象过程，减少了手动工作，提高了智能体AI系统运行时验证的可行性和可扩展性

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


### [51] [Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning](https://arxiv.org/abs/2601.23032)
*Siyu Gong,Linan Yue,Weibo Gao,Fangzhou Yao,Shimin Di,Lei Feng,Min-Ling Zhang*

Main category: cs.AI

TL;DR: AutoTraj：两阶段框架，通过修复和奖励工具使用轨迹来自动学习工具集成推理，无需依赖高质量合成轨迹


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理方法依赖高质量合成轨迹和稀疏结果奖励，监督有限且存在偏差，需要更有效的学习框架

Method: 两阶段框架：1) SFT阶段：生成多个候选轨迹，评估后保留高质量轨迹，低质量轨迹用LLM修复；2) RL阶段：基于偏好数据集训练轨迹级奖励模型，结合结果和格式奖励优化推理行为

Result: 在真实世界基准测试中证明了AutoTraj在工具集成推理中的有效性

Conclusion: AutoTraj通过自动修复和奖励工具使用轨迹，有效解决了现有TIR方法的监督限制问题，提升了工具集成推理能力

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.

</details>


### [52] [The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?](https://arxiv.org/abs/2601.23045)
*Alexander Hägele,Aryo Pradipta Gema,Henry Sleight,Ethan Perez,Jascha Sohl-Dickstein*

Main category: cs.AI

TL;DR: 研究发现，随着AI能力增强，其失败行为会变得更加"不连贯"（非系统性、无目标），而非系统性追求错误目标。这种不连贯性随推理时间增加而增强，且更大模型在某些情况下更不连贯。


<details>
  <summary>Details</summary>
Motivation: 随着AI承担更广泛和重要的任务，其失败风险也随之增加。需要理解未来强大AI会如何失败：是系统性追求错误目标，还是采取无意义的混乱行动？这关系到AI安全研究的方向。

Method: 使用偏差-方差分解来量化AI的"不连贯性"：通过测试时随机性测量错误中方差（而非偏差）所占比例。在不同任务和前沿模型上分析推理时间、模型规模与不连贯性的关系。

Result: 1. 模型推理时间越长，失败越不连贯；2. 模型规模与不连贯性的关系因实验而异，但在多个设置中，更大、更强的模型更不连贯；3. 仅靠规模扩展不太可能消除不连贯性。

Conclusion: 随着AI处理更复杂的序列任务，其失败将伴随更多不连贯行为，这意味着未来AI更可能因不可预测的混乱行为导致事故，而非系统性追求错误目标。这增加了针对奖励黑客和目标错误指定的对齐研究的重要性。

Abstract: As AI becomes more capable, we entrust it with more general and consequential tasks. The risks from failure grow more severe with increasing task scope. It is therefore important to understand how extremely capable AI models will fail: Will they fail by systematically pursuing goals we do not intend? Or will they fail by being a hot mess, and taking nonsensical actions that do not further any goal? We operationalize this question using a bias-variance decomposition of the errors made by AI models: An AI's \emph{incoherence} on a task is measured over test-time randomness as the fraction of its error that stems from variance rather than bias in task outcome. Across all tasks and frontier models we measure, the longer models spend reasoning and taking actions, \emph{the more incoherent} their failures become. Incoherence changes with model scale in a way that is experiment dependent. However, in several settings, larger, more capable models are more incoherent than smaller models. Consequently, scale alone seems unlikely to eliminate incoherence. Instead, as more capable AIs pursue harder tasks, requiring more sequential action and thought, our results predict failures to be accompanied by more incoherent behavior. This suggests a future where AIs sometimes cause industrial accidents (due to unpredictable misbehavior), but are less likely to exhibit consistent pursuit of a misaligned goal. This increases the relative importance of alignment research targeting reward hacking or goal misspecification.

</details>


### [53] [From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics](https://arxiv.org/abs/2601.23048)
*Bowen Cao,Dongdong Zhang,Yixia Li,Junpeng Liu,Shijue Huang,Chufan Shi,Hongyuan Lu,Yaokang Wu,Guanhua Chen,Wai Lam,Furu Wei*

Main category: cs.AI

TL;DR: ContextMATH基准测试显示，LLMs在现实数学问题解决中表现大幅下降，主要瓶颈在于问题表述而非推理能力


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在基准测试和实际应用之间的性能差距，特别是需要从描述性场景中提取数学核心的上下文数学推理问题

Method: 引入ContextMATH基准，将AIME和MATH-500问题重新构建为两种上下文设置：场景基础(SG)和复杂性扩展(CS)，评估61个专有和开源模型

Result: 模型性能显著下降：开源模型在SG和CS上分别下降13和34分，专有模型分别下降13和20分；错误主要由问题表述错误主导，表述准确性随问题难度增加而下降

Conclusion: 正确的问题表述是成功的前提，其充分性随模型规模提升；表述和推理是两个互补的瓶颈；微调可改善性能但无法完全解决差距，上下文数学推理仍是LLMs未解决的核心挑战

Abstract: Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descriptive scenarios. We introduce ContextMATH, a benchmark that repurposes AIME and MATH-500 problems into two contextual settings: Scenario Grounding (SG), which embeds abstract problems into realistic narratives without increasing reasoning complexity, and Complexity Scaling (CS), which transforms explicit conditions into sub-problems to capture how constraints often appear in practice. Evaluating 61 proprietary and open-source models, we observe sharp drops: on average, open-source models decline by 13 and 34 points on SG and CS, while proprietary models drop by 13 and 20. Error analysis shows that errors are dominated by incorrect problem formulation, with formulation accuracy declining as original problem difficulty increases. Correct formulation emerges as a prerequisite for success, and its sufficiency improves with model scale, indicating that larger models advance in both understanding and reasoning. Nevertheless, formulation and reasoning remain two complementary bottlenecks that limit contextual mathematical problem solving. Finally, we find that fine-tuning with scenario data improves performance, whereas formulation-only training is ineffective. However, performance gaps are only partially alleviated, highlighting contextual mathematical reasoning as a central unsolved challenge for LLMs.

</details>


### [54] [MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios via MCP Integration](https://arxiv.org/abs/2601.23049)
*Yakun Zhu,Yutong Huang,Shengqian Qin,Zhongzhen Huang,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: MedMCP-Calc是首个通过MCP集成评估LLMs在真实医疗计算器场景中的基准测试，包含118个跨4个临床领域的场景任务，揭示了现有模型在模糊查询、数据库交互和工具使用方面的显著局限性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗计算器的实际使用是一个多阶段的自适应过程，需要主动获取EHR数据、场景依赖的计算器选择和分步计算，而现有基准测试仅关注静态单步计算和明确指令，无法反映真实临床场景。

Method: 开发MedMCP-Calc基准测试，包含118个场景任务，涵盖4个临床领域，采用模糊任务描述模拟自然查询，包含结构化EHR数据库交互、外部参考检索和过程级评估。通过MCP集成评估23个领先模型，并基于发现开发CalcMate（经过微调的模型，包含场景规划和工具增强）。

Result: 评估发现现有模型存在显著局限性：即使是Claude Opus 4.5等顶级模型也难以处理模糊查询下的端到端工作流计算器选择，在迭代SQL数据库交互中表现不佳，且明显不愿使用外部工具进行数值计算。性能在不同临床领域间差异显著。CalcMate在开源模型中达到最先进性能。

Conclusion: MedMCP-Calc填补了医疗计算器评估的空白，揭示了LLMs在真实临床场景中的关键局限性，为未来医疗AI系统开发提供了重要基准和方向。

Abstract: Medical calculators are fundamental to quantitative, evidence-based clinical practice. However, their real-world use is an adaptive, multi-stage process, requiring proactive EHR data acquisition, scenario-dependent calculator selection, and multi-step computation, whereas current benchmarks focus only on static single-step calculations with explicit instructions. To address these limitations, we introduce MedMCP-Calc, the first benchmark for evaluating LLMs in realistic medical calculator scenarios through Model Context Protocol (MCP) integration. MedMCP-Calc comprises 118 scenario tasks across 4 clinical domains, featuring fuzzy task descriptions mimicking natural queries, structured EHR database interaction, external reference retrieval, and process-level evaluation. Our evaluation of 23 leading models reveals critical limitations: even top performers like Claude Opus 4.5 exhibit substantial gaps, including difficulty selecting appropriate calculators for end-to-end workflows given fuzzy queries, poor performance in iterative SQL-based database interactions, and marked reluctance to leverage external tools for numerical computation. Performance also varies considerably across clinical domains. Building on these findings, we develop CalcMate, a fine-tuned model incorporating scenario planning and tool augmentation, achieving state-of-the-art performance among open-source models. Benchmark and Codes are available in https://github.com/SPIRAL-MED/MedMCP-Calc.

</details>


### [55] [Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks](https://arxiv.org/abs/2601.23086)
*Nathaniel Mitrani Hadida,Sassan Bhanji,Cameron Tice,Puria Radmard*

Main category: cs.AI

TL;DR: 研究发现，当对LLMs的思维链推理进行优化压力时，模型会学会隐藏推理痕迹，这种隐藏行为会跨任务泛化，即使只惩罚最终行为也会导致推理隐藏，从而降低模型的可监控性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理是监控LLMs行为的重要工具，可以提供模型决策过程的解释和危险行为的早期预警。但优化压力可能导致模型隐藏推理痕迹，失去这种有益特性，从而降低模型的可监控性。

Method: 通过实验展示模型在奖励攻击任务中学会隐藏推理痕迹，并验证这种隐藏行为会泛化到未见过的奖励攻击场景。特别关注即使只惩罚最终行为，也会导致思维链推理的隐藏。

Result: 模型确实学会隐藏奖励攻击相关的推理痕迹，这种隐藏行为会跨任务泛化。最令人担忧的是，即使只惩罚最终行为，也会导致思维链推理的隐藏及其跨任务泛化。

Conclusion: 当前惩罚有害生成的做法可能无意中导致LLMs可监控性的降低，且这种影响是难以预测的。需要重新思考如何平衡模型安全性和可监控性。

Abstract: Chain-of-thought (CoT) reasoning provides a significant performance uplift to LLMs by enabling planning, exploration, and deliberation of their actions. CoT is also a powerful tool for monitoring the behaviours of these agents: when faithful, they offer interpretations of the model's decision making process, and an early warning sign for dangerous behaviours. However, optimisation pressures placed on the CoT may cause the model to obfuscate reasoning traces, losing this beneficial property. We show that obfuscation can generalise across tasks; models that learn to obfuscate reasoning involving reward hacking (e.g. accessing and utilising leaked information) generalise both the reward hacking behaviour and its obfuscation in CoT to unseen reward hacking settings. Most worryingly, we show that obfuscation of CoT reasoning, and its generalisation across tasks, also follows when we penalise only the model's final actions after closing its CoT. Our findings suggest that current practices of penalising harmful generations may inadvertently lead to a reduction in the broader monitorability of LLMs in unpredictable ways.

</details>


### [56] [RAudit: A Blind Auditing Protocol for Large Language Model Reasoning](https://arxiv.org/abs/2601.23133)
*Edward Y. Chang,Longling Geng*

Main category: cs.AI

TL;DR: RAudit是一个无需真实标签的LLM推理审计协议，通过评估推导步骤是否支持结论来检测推理病理，揭示了推理不可靠性的四种机制。


<details>
  <summary>Details</summary>
Motivation: 推理时的缩放会放大推理病理（如奉承、层级坍塌、过早确定性），需要一种无需真实标签的审计方法来检测LLM推理中的不一致性。

Method: 提出RAudit协议，基于盲审原则评估推导步骤是否支持结论，使用CRIT-based合理性评分，并通过变化批评表述来研究社会框架对模型响应的影响。

Result: 在数学推理和因果判断任务上揭示了四种机制：潜在能力抑制、虚假能力陷阱、复杂度-脆弱性权衡、医源性批评，挑战了"能力意味着鲁棒性"和"更强反馈产生更好输出"的假设。

Conclusion: 推理能力不等于推理鲁棒性，社会压力会抑制模型潜在能力，需要更细致的评估方法来理解LLM推理病理。

Abstract: Inference-time scaling can amplify reasoning pathologies: sycophancy, rung collapse, and premature certainty. We present RAudit, a diagnostic protocol for auditing LLM reasoning without ground truth access. The key constraint is blindness: the auditor evaluates only whether derivation steps support conclusions, enabling detection of trace-output inconsistency and, when latent competence exists, its recovery. RAudit measures process quality via CRIT-based reasonableness scores and varies critique formulation to study how social framing affects model response. We prove bounded correction and $O(\log(1/ε))$ termination. Experiments on mathematical reasoning (CAP-GSM8K) and causal judgment (CausalL2) reveal four mechanisms explaining model unreliability: (1) Latent Competence Suppression, where models derive correct answers then overwrite them under social pressure; (2) The False Competence Trap, where weaker judges mask sycophancy that stronger judges expose; (3) The Complexity-Vulnerability Tradeoff, where causal tasks induce more than 10 times higher sycophancy than mathematical tasks; and (4) Iatrogenic Critique, where authoritative correction harms weaker models. These findings challenge assumptions that capability implies robustness and that stronger feedback yields better outputs.

</details>


### [57] [THINKSAFE: Self-Generated Safety Alignment for Reasoning Models](https://arxiv.org/abs/2601.23143)
*Seanie Lee,Sangwoo Park,Yumin Choi,Gyeongman Kim,Minki Kang,Jihun Yun,Dongmin Park,Jongho Park,Sung Ju Hwang*

Main category: cs.AI

TL;DR: ThinkSafe是一个自生成对齐框架，通过轻量级拒绝引导让模型生成安全推理轨迹，然后微调这些自生成响应来恢复安全对齐，同时保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过强化学习优化推理任务时过度追求合规性，导致安全机制被抑制，容易受到有害提示的攻击。现有方法依赖外部教师蒸馏，但会引入分布差异损害原生推理能力。

Method: 提出ThinkSafe框架：1）通过轻量级拒绝引导解锁模型潜在的安全知识；2）引导模型生成分布内的安全推理轨迹；3）在这些自生成响应上进行微调，实现安全对齐同时最小化分布偏移。

Result: 在DeepSeek-R1-Distill和Qwen3上的实验表明，ThinkSafe显著提升安全性同时保持推理能力。在安全性和推理能力上与GRPO相当，但计算成本显著降低。

Conclusion: ThinkSafe通过自生成对齐有效解决了推理模型的安全退化问题，无需外部教师即可恢复安全对齐，同时保持推理能力，计算效率高。

Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.

</details>


### [58] [Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization](https://arxiv.org/abs/2601.23179)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Xueyi Ke,Qixing Zhang,Bingquan Shen,Alex Kot,Xudong Jiang*

Main category: cs.AI

TL;DR: 提出MCRMO-Attack方法，解决通用目标可迁移对抗攻击(UTTAA)问题，通过多裁剪聚合、注意力引导裁剪、对齐性门控令牌路由和元学习跨目标扰动先验，显著提升对未知商业MLLMs的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒迁移攻击方法多为样本特异性，跨输入重用性有限。研究更严格的通用目标可迁移对抗攻击(UTTAA)设置，要求单个扰动能一致地将任意输入导向指定目标，适用于未知商业多模态大语言模型。

Method: 提出MCRMO-Attack方法：1) 通过多裁剪聚合与注意力引导裁剪稳定监督；2) 使用对齐性门控令牌路由提高令牌级可靠性；3) 元学习跨目标扰动先验以获得更强的每目标解决方案。

Result: 在商业MLLMs上显著提升攻击成功率：GPT-4o上未见图像攻击成功率提升+23.7%，Gemini-2.0上提升+19.9%，优于最强的通用基线方法。

Conclusion: MCRMO-Attack有效解决了UTTAA设置中的核心挑战，通过稳定监督、改进令牌级可靠性和元学习扰动先验，显著提升了跨商业MLLMs的通用目标攻击性能。

Abstract: Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\% on GPT-4o and +19.9\% on Gemini-2.0 over the strongest universal baseline.

</details>


### [59] [TSAQA: Time Series Analysis Question And Answering Benchmark](https://arxiv.org/abs/2601.23204)
*Baoyu Jing,Sanhorn Chen,Lecheng Zheng,Boyu Liu,Zihao Li,Jiaru Zou,Tianxin Wei,Zhining Liu,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Yuchen Yan,Dongqi Fu,Jingchao Ni,Jingrui He,Hanghang Tong*

Main category: cs.AI

TL;DR: TSAQA是一个统一的时间序列问答基准，包含6种任务类型、13个领域、21万样本，采用多种格式评估LLMs的时间序列分析能力，现有模型表现有限。


<details>
  <summary>Details</summary>
Motivation: 当前多任务时间序列问答基准主要局限于预测和异常检测任务，缺乏对更广泛时间序列分析能力的评估，需要更全面的基准来推动该领域发展。

Method: 构建TSAQA基准，整合6种任务（异常检测、分类、特征描述、比较、数据转换、时间关系分析），涵盖13个领域，包含21万样本，采用TF、MC和创新的PZ格式。

Result: 零样本评估显示当前LLMs表现有限：最佳商业模型Gemini-2.5-Flash平均分仅65.08；指令微调能提升开源模型性能，但最佳开源模型LLaMA-3.1-8B仍有很大改进空间。

Conclusion: TSAQA基准揭示了LLMs在时间序列分析方面的挑战性，现有模型表现不足，需要进一步研究提升LLMs的时间序列理解能力。

Abstract: Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.

</details>


### [60] [High-quality generation of dynamic game content via small language models: A proof of concept](https://arxiv.org/abs/2601.23206)
*Morten I. K. Munk,Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: 论文提出通过针对性微调小型语言模型(SLMs)来替代云端大语言模型(LLMs)，解决游戏内容生成中的叙事不连贯、高成本和离线限制问题，并在RPG游戏中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)在动态游戏内容生成中面临叙事不连贯、高运营成本和云端依赖等限制，而现有小型语言模型(SLMs)输出质量较差，需要找到更实用的解决方案。

Method: 采用针对性微调策略：1）在狭窄上下文、约束结构或两者兼具的任务上进行激进微调；2）使用DAG方法合成训练数据，将模型锚定在特定游戏世界；3）构建基于叙事框架的智能体网络；4）通过"重试直至成功"策略确保输出质量。

Result: 概念验证表明：1）单一专业化SLM作为基础构建块可行；2）在声誉修辞战斗的RPG循环中，简单重试策略能达到足够质量；3）具有可预测延迟，适合实时生成；4）在典型游戏引擎约束下具有可行性。

Conclusion: 通过针对性微调SLMs在狭窄任务上，可以替代云端LLMs，为游戏内容生成提供更实用、鲁棒的解决方案，虽然本地质量评估仍是开放问题，但已证明在实时生成中的可行性。

Abstract: Large language models (LLMs) offer promise for dynamic game content generation, but they face critical barriers, including narrative incoherence and high operational costs. Due to their large size, they are often accessed in the cloud, limiting their application in offline games. Many of these practical issues are solved by pivoting to small language models (SLMs), but existing studies using SLMs have resulted in poor output quality. We propose a strategy of achieving high-quality SLM generation through aggressive fine-tuning on deliberately scoped tasks with narrow context, constrained structure, or both. In short, more difficult tasks require narrower scope and higher specialization to the training corpus. Training data is synthetically generated via a DAG-based approach, grounding models in the specific game world. Such models can form the basis for agentic networks designed around the narratological framework at hand, representing a more practical and robust solution than cloud-dependent LLMs. To validate this approach, we present a proof-of-concept focusing on a single specialized SLM as the fundamental building block. We introduce a minimal RPG loop revolving around rhetorical battles of reputations, powered by this model. We demonstrate that a simple retry-until-success strategy reaches adequate quality (as defined by an LLM-as-a-judge scheme) with predictable latency suitable for real-time generation. While local quality assessment remains an open question, our results demonstrate feasibility for real-time generation under typical game engine constraints.

</details>


### [61] [Scaling Multiagent Systems with Process Rewards](https://arxiv.org/abs/2601.23228)
*Ed Li,Junyu Ren,Cat Yan*

Main category: cs.AI

TL;DR: MAPPA：通过AI反馈的逐动作过程奖励来微调多智能体系统，解决信用分配和样本效率问题，在数学竞赛和数据分析任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂任务中通过专业化分工展现出潜力，但微调多个智能体面临两大挑战：1）跨智能体的信用分配问题；2）昂贵的多智能体rollout的样本效率问题。

Method: 提出MAPPA方法，通过AI反馈为每个智能体动作分配过程奖励，而不是仅在任务完成时给予奖励。这种方法能够在没有真实标签的情况下提供细粒度监督，并从每次rollout中提取最大训练信号。

Result: 在数学竞赛问题上：在AIME上提升5.0-17.5个百分点，在AMC上提升7.8-17.2个百分点。在数据分析任务上：成功率提升12.5个百分点，质量指标提升高达30%。验证了逐动作监督在不同领域多智能体系统中的有效性。

Conclusion: 通过解决信用分配和样本效率挑战，这项工作为在最小人工监督下扩展多智能体系统处理复杂、长视野任务迈出了第一步。

Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.

</details>


### [62] [Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust MDPs](https://arxiv.org/abs/2601.23229)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Goharshady,Mehrdad Karrabi,Alipasha Montaseri,Carlo Pagano*

Main category: cs.AI

TL;DR: 该论文解决了鲁棒MDPs中一个重要的算法开放问题，证明了对于(s,a)-rectangular L∞ RMDPs，在固定折扣因子下，鲁棒策略迭代算法具有强多项式时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 鲁棒MDPs是序列决策中的基本模型，能够处理转移概率的不确定性并优化最坏情况。虽然MDPs已有多项式时间算法，但RMDPs的算法复杂性一直是一个重要的开放问题，特别是能否将Ye在MDPs中的强多项式时间结果推广到RMDPs。

Method: 采用鲁棒策略迭代算法来处理(s,a)-rectangular L∞ RMDPs。这种模型是MDPs和回合制随机博弈的推广，具有L∞不确定性集。

Result: 证明了对于具有固定折扣因子的(s,a)-rectangular L∞ RMDPs，鲁棒策略迭代算法具有强多项式时间复杂度，解决了该领域的一个重要算法问题。

Conclusion: 该工作首次为(s,a)-rectangular L∞ RMDPs建立了强多项式时间算法，填补了MDPs与RMDPs之间算法复杂性理论的空白，为鲁棒决策理论提供了重要的算法基础。

Abstract: Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and turn-based stochastic games. We consider this model with discounted payoffs. The existence of polynomial and strongly-polynomial time algorithms is a fundamental problem for these optimization models. For MDPs, linear programming yields polynomial-time algorithms for any arbitrary discount factor, and the seminal work of Ye established strongly--polynomial time for a fixed discount factor. The generalization of such results to RMDPs has remained an important open problem. In this work, we show that a robust policy iteration algorithm runs in strongly-polynomial time for $(s, a)$-rectangular $L_\infty$ RMDPs with a constant (fixed) discount factor, resolving an important algorithmic question.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [63] [Capacity of Two-User Wireless Systems Aided by Movable Signals](https://arxiv.org/abs/2601.22358)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 可移动信号作为智能无线电环境的第三种方法，通过动态调整工作频率正交化用户信道，显著扩展多用户系统的容量区域，在有限频率范围内提供高达45%的和速率增益。


<details>
  <summary>Details</summary>
Motivation: 可移动信号作为智能无线电环境的第三种新兴方法，旨在补充可重构智能表面和灵活天线，探索其在增强多用户无线系统性能方面的潜力。

Method: 研究两用户系统中的多址接入信道和广播信道的容量区域特性，通过可移动信号动态调整工作频率来正交化用户信道，并研究有限频率范围内的频率优化问题。

Result: 可移动信号能够显著扩展容量区域，在有限频率范围内相比固定信号提供高达45%的和速率增益。

Conclusion: 可移动信号作为智能无线电环境的有前景的第三种方法，通过动态频率调整有效正交化用户信道，为多用户无线系统带来显著的性能提升。

Abstract: Movable signals have emerged as a third approach to enable smart radio environments (SREs), complementing reconfigurable intelligent surfaces (RISs) and flexible antennas. This paper investigates their potential to enhance multi-user wireless systems. Focusing on two-user systems, we characterize the capacity regions of the multiple access channel (MAC) and broadcast channel (BC). Interestingly, movable signals can dynamically adjust the operating frequency to orthogonalize the user channels, thereby significantly expanding the capacity regions. We also study frequency optimization, constraining it in a limited frequency range, and show that movable signals provide up to 45% sum rate gain over fixed signals.

</details>


### [64] [5G LDPC Codes as Root LDPC Codes via Diversity Alignment](https://arxiv.org/abs/2601.22470)
*Hyuntae Ahn,Inki Kim,Hee-Youl Kwak,Yongjune Kim,Chanki Kim,Sang-Hyo Kim*

Main category: cs.IT

TL;DR: 本文提出了一种基于布尔函数的多样性演化分析方法，用于分析非遍历块衰落信道下准循环LDPC码的多样性，并开发了贪婪块映射搜索算法，确保所有信息比特获得全分集。


<details>
  <summary>Details</summary>
Motivation: 研究非遍历块衰落信道下准循环LDPC码的多样性问题，特别是在迭代置信传播解码下，如何分析和优化码的分集性能。

Method: 提出多样性演化分析方法，基于布尔函数跟踪置信传播消息的衰落依赖性演化；开发贪婪块映射搜索算法，将原图变量节点分配到衰落块中，确保信息节点获得全分集。

Result: 在5G新空口LDPC码上的数值结果表明，所提搜索算法找到的块映射能保证所有信息比特获得全分集，相比随机映射具有更陡的高信噪比斜率和更低误块率。

Conclusion: 所提方法能够在不改变基础图结构的情况下，通过优化块映射保证信息比特的全分集性能，显著提升非遍历块衰落信道下LDPC码的性能。

Abstract: This paper studies the diversity of protographbased quasi-cyclic low-density parity-check (QC-LDPC) codes over nonergodic block-fading channels under iterative beliefpropagation decoding. We introduce diversity evolution (DivE), a Boolean-function-based analysis method that tracks how the fading dependence of belief-propagation messages evolves across decoding iterations. Under a Boolean approximation of block fading, DivE derives a Boolean fading function for each variable node (VN) output (i.e., the a-posteriori reliability after iterative decoding), from which the VN diversity order can be directly determined. Building on this insight, we develop a greedy blockmapping search that assigns protograph VNs to fading blocks so that all information VNs achieve full diversity, while including the minimum additional parity VNs when full diversity is infeasible at the nominal rate. Numerical results on the 5G New Radio LDPC codes show that the proposed search finds block mappings that guarantee full diversity for all information bits without modifying the base-graph structure, yielding a markedly steeper high-SNR slope and lower BLER than random mappings.

</details>


### [65] [Successive Cancellation List Decoding of Extended Reed-Solomon Codes](https://arxiv.org/abs/2601.22482)
*Xiaoqian Ye,Jingyu Lin,Junjie Huang,Li Chen,Chang-An Zhao*

Main category: cs.IT

TL;DR: 本文提出了一种针对特征为2的有限域上扩展RS码的新列表解码方法，通过将eRS码转换为n个二进制极化码，从而能够使用SC和SCL解码。


<details>
  <summary>Details</summary>
Motivation: Reed-Solomon码作为重要的非二进制纠错码，在突发错误纠正方面表现优异，广泛应用于现代通信和数据存储系统。然而，需要开发更有效的解码方法来充分利用其MDS特性。

Method: 将扩展RS码转换为n个二进制极化码，然后使用连续消除(SC)解码及其列表解码版本(SCL)。通过预变换矩阵重新解释eRS码，并研究其列线性独立性来理论分析SC解码性能。

Result: 提出的解码方法在数值上得到验证，通过理论分析表征了SC解码性能，展示了将eRS码转换为极化码框架的有效性。

Conclusion: 该方法为特征为2的有限域上的扩展RS码提供了一种新的列表解码框架，通过极化码转换实现了有效的SC和SCL解码，为RS码解码提供了新思路。

Abstract: Reed-Solomon (RS) codes are an important class of non-binary error-correction codes. They are particularly competent in correcting burst errors, being widely applied in modern communications and data storage systems. This also thanks to their distance property of reaching the Singleton bound, being the maximum distance separable (MDS) codes. This paper proposes a new list decoding for extended RS (eRS) codes defined over a finite field of characteristic two, i.e., F_{2^n}. It is developed based on transforming an eRS code into n binary polar codes. Consequently, it can be decoded by the successive cancellation (SC) decoding and further their list decoding, i.e., the SCL decoding. A pre-transformed matrix is required for reinterpretating the eRS codes, which also determines their SC and SCL decoding performances. Its column linear independence property is studied, leading to theoretical characterization of their SC decoding performance. Our proposed decoding and analysis are validated numerically.

</details>


### [66] [Flexible FTN-OTFS for High-Mobility LEO Satellite-to-Ground Communication](https://arxiv.org/abs/2601.22526)
*Chaorong Zhang,Hui Xu,Benjamin K. Ng,Yue Liu,Chan-Tong Lam,Halim Yanikomeroglu*

Main category: cs.IT

TL;DR: 提出轻量级LEO卫星辅助的灵活FTN-OTFS方案，解决星上功耗限制和快速时变信道问题，通过SNR感知的灵活FTN策略和查找表优化时间压缩因子，实现频谱效率最大化与可靠性约束的平衡。


<details>
  <summary>Details</summary>
Motivation: 非地面网络中星上功耗严格受限，快速时变信道影响严重，需要解决信道老化效应同时保持低计算复杂度。

Method: 建立基于3GPP TDL信道模型的系统框架，引入SNR感知的灵活FTN策略，使用低复杂度查找表根据瞬时信道响应自适应优化时间域压缩因子。

Result: 通过理论分析和仿真验证，该方案显著优于静态FTN基准，在吞吐量和鲁棒性方面达到优越平衡，有效解决了速率加速与干扰惩罚的权衡问题。

Conclusion: 提出的LEO-FFTN-OTFS方案为下一代LEO通信提供了高吞吐量和鲁棒性的优越平衡，通过灵活FTN策略有效管理信道老化效应，满足严格可靠性约束。

Abstract: In this paper, a lightweight LEO satellite-assisted flexible faster-than-Nyquist (FTN)-orthogonal time frequency space (OTFS) (LEO-FFTN-OTFS) scheme is proposed to address the stringent constraints on onboard power consumption and the severe impact of fast time-varying channels in non-terrestrial networks. A rigorous system framework incorporating realistic 3GPP Tapped Delay Line (TDL) channel models is established to accurately capture high-mobility propagation characteristics. To counteract channel aging effects while maintaining low computational complexity, an SNR-aware flexible FTN strategy is introduced, wherein a low-complexity Look-Up Table (LUT) is utilized to adaptively optimize the time-domain compression factor based on instantaneous channel responses. Through this mechanism, the trade-off between rate acceleration and interference penalty is effectively resolved, ensuring that spectral efficiency is maximized while strict reliability constraints are satisfied with minimal processing overhead. Moreover, a comprehensive theoretical analysis is provided, in which analytical expressions for effective throughput, energy efficiency, and bit error rate are derived. Finally, it is demonstrated by extensive simulations that the proposed scheme significantly outperforms static FTN benchmarks, offering a superior balance of high throughput and robustness for next-generation LEO communications.

</details>


### [67] [Quantum $(r,δ)$-Locally Recoverable BCH and Homothetic-BCH Codes](https://arxiv.org/abs/2601.22567)
*Carlos Galindo,Fernando Hernando,Ryutaroh Matsumoto*

Main category: cs.IT

TL;DR: 该论文研究如何从BCH码和同态BCH码构造量子(r,δ)局部可恢复码，获得了满足Singleton-like界的纯量子最优码


<details>
  <summary>Details</summary>
Motivation: 量子(r,δ)局部可恢复码是经典(r,δ)-LRCs的量子版本，用于大规模分布式和云存储系统中的多故障恢复。需要从现有的编码构造方法来获得量子LRCs。

Method: 从BCH码和同态BCH码构造量子(r,δ)-LRCs，这些码需要满足欧几里得或厄米特对偶包含条件。

Result: 获得了纯量子(r,δ)-LRCs，这些码对于Singleton-like界是最优的。

Conclusion: 成功展示了从BCH和同态BCH码构造量子局部可恢复码的方法，获得了满足最优界的量子码。

Abstract: Quantum $(r,δ)$-locally recoverable codes ($(r,δ)$-LRCs) are the quantum version of classical $(r,δ)$-LRCs designed to recover multiple failures in large-scale distributed and cloud storage systems. A quantum $(r,δ)$-LRC, $Q(C)$, can be constructed from an $(r,δ)$-LRC, $C$, which is Euclidean or Hermitian dual-containing.
  This article is devoted to studying how to get quantum $(r,δ)$-LRCs from BCH and homothetic-BCH codes. As a consequence, we give pure quantum $(r,δ)$-LRCs which are optimal for the Singleton-like bound.

</details>


### [68] [Multi-target DoA estimation with a single Rydberg atomic receiver by spectral analysis of spatially-resolved fluorescence](https://arxiv.org/abs/2601.22704)
*Liangcheng Han,Haifan Yin,Mérouane Debbah*

Main category: cs.IT

TL;DR: 提出基于成像的谱估计方法，通过空间解析荧光剖面解决多目标DoA估计问题，将原子吸收模式线性化为正弦波叠加，用Prony方法实现多目标检测和宽带能力恢复。


<details>
  <summary>Details</summary>
Motivation: 现有Rydberg DoA估计方法受限于接收阵列复杂性和单接收器方法的单目标、窄带限制，需要解决多目标检测和宽带能力问题。

Method: 通过空间解析蒸气室荧光剖面，将入射信号与强本地振荡器叠加，将复杂原子吸收模式线性化为正弦波叠加，每个空间频率唯一映射到目标DoA，使用Prony方法进行谱估计。

Result: 提出的成像谱估计方法支持多目标检测，恢复了传感器的全宽带能力，消除了限制性的室长依赖，有望实现多通道Rydberg接收器和全息MIMO所需的连续孔径传感。

Conclusion: 该方法有效解决了Rydberg DoA估计的多目标问题，通过理论建模、CRLB性能基准和仿真验证了方法的有效性，为多通道接收器和连续孔径传感奠定了基础。

Abstract: Rydberg-based Direction-of-Arrival (DoA) estimation has been hampered by the complexity of receiver arrays and the single-target, narrow-band limitations of existing single-receiver methods. This paper introduces a novel approach that addresses these limitations. We demonstrate that by spatially resolving the fluorescence profile along the vapor cell, the multi-target problem can be effectively solved. Our approach hinges on the insight that by superimposing incoming signals with a strong local oscillator (LO), the complex atomic absorption pattern is linearized into a simple superposition of sinusoids. In this new representation, each spatial frequency uniquely and directly maps to the DoA of a target. This reduces the multi-target challenge into a spectral estimation problem, which we address using Prony's method. Our approach, termed Imaging-based Spectral Estimation (ISE), inherently supports multi-target detection and restores the full broadband capability of the sensor by removing the restrictive cell-length dependency. This development also shows potential for realizing multi-channel Rydberg receivers and the continuous-aperture sensing required for holographic multiple-input multiple-output (MIMO). We develop a comprehensive theoretical model, derive the Cramer-Rao Lower Bound (CRLB) as a performance benchmark, and present simulations validating the effectiveness of the approach to resolve multiple targets.

</details>


### [69] [Status Updating via Integrated Sensing and Communication: Freshness Optimisation](https://arxiv.org/abs/2601.22901)
*Touraj Soleymani,Mohamad Assaad,John S. Baras*

Main category: cs.IT

TL;DR: 研究ISAC架构中远程导航代理状态更新的战略设计，通过优化包含信息新鲜度（AoI）和开销的长期成本，证明了最优策略具有单调阈值结构。


<details>
  <summary>Details</summary>
Motivation: 将信息新鲜度（AoI）目标自然集成到ISAC设计中，同时产生可解释和可实施的策略，解决远程导航代理状态更新中的感知与通信权衡问题。

Method: 将顺序决策问题建模为具有二维AoI状态的折扣无限时域马尔可夫决策过程，感知和通信以给定概率成功并产生不同成本，分析最优平稳策略的结构特性。

Result: 证明了最优平稳策略具有单调阈值结构，由AoI状态空间中的非递减切换曲线表征，数值分析展示了值函数和最优决策图的结构。

Conclusion: 信息新鲜度目标可以自然地集成到ISAC设计中，同时产生可解释和可实施的策略，为远程导航代理的状态更新提供了理论框架和实用解决方案。

Abstract: This paper studies strategic design in an integrated sensing and communication (ISAC) architecture for status updating of remotely navigating agents. We consider an ISAC-enabled base station that can sense the state of a remote source and communicate this information back to the source. Both sensing and communication succeed with given probabilities and incur distinct costs. The objective is to optimise a long-term cost that captures information freshness, measured by the age of information (AoI), at the source together with sensing and communication overheads. The resulting sequential decision problem is formulated as a discounted infinite-horizon Markov decision process with a two-dimensional AoI state, representing information freshness at the source and at the base station. We prove that the optimal stationary policy admits a monotone threshold structure characterised by a nondecreasing switching curve in the AoI state space. Our numerical analysis illustrates the structures of the value function and the optimal decision map. These results demonstrate that freshness-based objectives can be naturally integrated into ISAC design, while yielding interpretable and implementable strategies.

</details>


### [70] [Feedback Control via Integrated Sensing and Communication: Uncertainty Optimisation](https://arxiv.org/abs/2601.22912)
*Touraj Soleymani,Mohamad Assaad,John S. Baras*

Main category: cs.IT

TL;DR: 本文研究集成感知与通信（ISAC）架构中用于网络物理系统反馈控制的策略设计，证明了最优切换策略是基于阈值的，最优控制策略是线性的。


<details>
  <summary>Details</summary>
Motivation: 研究在ISAC架构下如何优化网络物理系统的反馈控制，解决基站需要在感知源状态和传输控制信息之间切换的决策问题，以最小化系统成本。

Method: 采用不确定性感知的综合方法，针对高斯-马尔可夫源和独立同分布伯努利感知/通信链路，在有限时域线性二次高斯成本下，严格表征最优策略。

Result: 证明基站的最优切换策略是基于源和基站估计协方差的阈值策略，源处执行器的最优控制策略是源状态估计的线性函数。阈值区域随源不确定性增加而扩大，随基站不确定性增加而收缩。

Conclusion: ISAC架构中的最优控制设计具有明确的阈值结构，为网络物理系统的集成感知与通信提供了理论指导，揭示了不确定性对切换决策的影响规律。

Abstract: This paper studies strategic design in an integrated sensing and communication (ISAC) architecture for feedback control of cyber-physical systems. We focus on a setting in which the regulation of a physical process (i.e., remote source) is performed via an ISAC-enabled base station. The base station can alternate between tracking the state of the source and delivering control-relevant information back to the source. For a Gauss-Markov source subject to i.i.d. Bernoulli sensing and communication links, under a finite-horizon linear-quadratic-Gaussian cost, we rigorously characterise the optimal policies through an uncertainty-aware synthesis. We establish that the optimal switching policy, for the ISAC system at the base station, is threshold-based in terms of the source and base-station estimation covariances, while the optimal control policy, for the actuator at the source, is linear in the source state estimate. We show that the threshold region$\unicode{x2014}$defined as the set of estimation covariance pairs for which communication is preferred over sensing$\unicode{x2014}$expands with increasing source uncertainty and contracts with increasing base-station uncertainty.

</details>


### [71] [A complete characterisation of conditional entropies](https://arxiv.org/abs/2601.23213)
*Roberto Rubboli,Erkka Haapasalo,Marco Tomamichel*

Main category: cs.IT

TL;DR: 该论文完全表征了满足基本操作公理的广义条件熵，证明其最一般形式由参数化指数平均的Rényi熵族构成，并应用于量子热力学第二定律。


<details>
  <summary>Details</summary>
Motivation: 条件熵是信息论和统计学中的核心概念，用于量化具有相关辅助信息时的不确定性。尽管已有多种定义，但满足自然操作公理的广义条件熵的完整表征一直缺失。本研究旨在填补这一空白，建立满足基本操作要求的最一般条件熵形式。

Method: 通过设定三个基本操作公理：独立随机变量的可加性、重标记不变性、以及条件混合通道下的单调性。在此公理框架下，推导并证明最一般的条件熵形式，该形式由参数化指数平均的Rényi熵族构成，包含一个实参数和一个正实数上的概率测度。

Result: 成功完全表征了满足操作公理的广义条件熵，证明其最一般形式为指数平均的Rényi熵族。进一步证明这些量决定了条件混合下的转换速率，并为能量本征基对角化的量子态提供了具有辅助信息的量子热力学第二定律集合。

Conclusion: 本研究解决了条件熵的完整表征问题，建立了满足基本操作公理的最一般条件熵形式。这一理论框架不仅统一了现有的条件熵定义，还为量子热力学等应用领域提供了新的理论基础，特别是在具有辅助信息的情况下。

Abstract: Entropies are fundamental measures of uncertainty with central importance in information theory and statistics and applications across all the quantitative sciences. Under a natural set of operational axioms, the most general form of entropy is captured by the family of Rényi entropies, parameterized by a real number $α$. Conditional entropy extends the notion of entropy by quantifying uncertainty from the viewpoint of an observer with access to potentially correlated side information. However, despite their significance and the emergence of various useful definitions, a complete characterization of measures of conditional entropy that satisfy a natural set of operational axioms has remained elusive. In this work, we provide a complete characterization of conditional entropy, defined through a set of axioms that are essential for any operationally meaningful definition: additivity for independent random variables, invariance under relabeling, and monotonicity under conditional mixing channels. We prove that the most general form of conditional entropy is captured by a family of measures that are exponential averages of Rényi entropies of the conditioned distribution and parameterized by a real parameter and a probability measure on the positive reals. Finally, we show that these quantities determine the rate of transformation under conditional mixing and provide a set of second laws of quantum thermodynamics with side information for states diagonal in the energy eigenbasis.

</details>


### [72] [Secure Integrated Sensing and Communication against Communication and Sensing Eavesdropping](https://arxiv.org/abs/2601.23216)
*Sidong Guo,Matthieu R. Bloch*

Main category: cs.IT

TL;DR: 研究集成感知与通信系统中的安全权衡问题，分析发送方的保密率、检测指数与对手检测指数之间的基本权衡关系


<details>
  <summary>Details</summary>
Motivation: 在对抗性无线环境中，感知隐私和通信保密性扮演着不同但相互关联的角色。在集成感知与通信系统中，同一波形同时服务于双重目的，这使得在单一物理层框架中捕捉这种相互作用变得特别具有挑战性

Method: 研究一个安全的ISAC系统，其中单基地发射机同时向合法接收机发送保密消息并感知环境状态，而被动对手试图进行消息解码和状态估计。通过反馈提取密钥，并使用窃听码和可分辨性码隐藏码字的内容和结构

Result: 部分刻画了三个性能指标之间的基本权衡关系：发射机的保密率、检测指数和对手的检测指数。推导了可达区域，并通过数值示例说明了由此产生的设计权衡

Conclusion: 在集成感知与通信系统中，除了控制整体性能的联合输入分布外，权衡关系还进一步受到发射机通过反馈提取密钥以及通过窃听码和可分辨性码隐藏码字内容和结构的能力的影响

Abstract: Sensing privacy and communication confidentiality play fundamentally different but interconnected roles in adversarial wireless environments. Capturing this interplay within a single physical-layer framework is particularly challenging in integrated sensing and communication (ISAC) systems, where the same waveform simultaneously serves dual purposes. We study a secure ISAC system in which a monostatic transmitter simultaneously sends a confidential message to a legitimate receiver and senses an environmental state, while a passive adversary attempts both message decoding and state estimation. We partially characterize the fundamental trade-offs among three performance measures: the transmitter's secrecy rate, its detection exponent, and the adversary's detection exponent. Beyond the joint input distribution that governs overall performance, the trade-offs are further shaped by the transmitter's ability to extract keys via feedback and hide both the content and structure of the codewords via wiretap and resolvability codes. We derive an achievable region, and illustrate the resulting design trade-offs through a numerical example.

</details>
