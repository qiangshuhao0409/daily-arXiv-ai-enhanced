<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.IT](#cs.IT) [Total: 7]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Towards a Non-Binary View of IPv6 Adoption](https://arxiv.org/abs/2507.11678)
*Sulyab Thottungal Valapu,John Heidemann*

Main category: cs.NI

TL;DR: 本文研究了IPv6部署的现状，从客户端、服务器和云提供商的角度分析了IPv6的使用情况，发现其部署仍存在显著差异和不足。


<details>
  <summary>Details</summary>
Motivation: 随着IPv6部署的增加，需要更细致地评估其使用情况，而非简单的二元状态。

Method: 从客户端、服务器和云提供商三个角度分析IPv6流量、服务支持情况和云平台部署率。

Result: 客户端IPv6流量波动大，仅12.5%的顶级网站完全支持IPv6，云平台IPv6部署率因易用性差异显著。

Conclusion: IPv6部署虽在增长，但许多服务仍滞后，云提供商可通过优化易用性提升部署率。

Abstract: Twelve years have passed since World IPv6 Launch Day, but what is the current
state of IPv6 deployment? Prior work has examined IPv6 status as a binary: can
you use IPv6, or not? As deployment increases we must consider a more nuanced,
non-binary perspective on IPv6: how much and often can a user or a service use
IPv6? We consider this question as a client, server, and cloud provider.
Considering the client's perspective, we observe user traffic. We see that the
fraction of IPv6 traffic a user sends varies greatly, both across users and
day-by-day, with a standard deviation of over 15%. We show this variation
occurs for two main reasons. First, IPv6 traffic is primarily human-generated,
thus showing diurnal patterns. Second, some services are IPv6-forward and
others IPv6-laggards, so as users do different things their fraction of IPv6
varies. We look at server-side IPv6 adoption in two ways. First, we expand
analysis of web services to examine how many are only partially IPv6 enabled
due to their reliance on IPv4-only resources. Our findings reveal that only
12.5% of top 100k websites qualify as fully IPv6-ready. Finally, we examine
cloud support for IPv6. Although all clouds and CDNs support IPv6, we find that
tenant deployment rates vary significantly across providers. We find that ease
of enabling IPv6 in the cloud is correlated with tenant IPv6 adoption rates,
and recommend best practices for cloud providers to improve IPv6 adoption. Our
results suggest IPv6 deployment is growing, but many services lag, presenting a
potential for improvement.

</details>


### [2] [On QoE-Aware Traffic Management for Real-time, Interactive Video with Time-variant Spatial Complexity](https://arxiv.org/abs/2507.11798)
*Szilveszter Nádas,Lars Ernström,David Lindero,Jonathan Lynam*

Main category: cs.NI

TL;DR: 研究了视频内容的空间复杂度与QoE的关系，提出基于效用的动态资源分配方法，优于静态分配和均等QoE分配。


<details>
  <summary>Details</summary>
Motivation: 探索视频内容的空间复杂度对QoE的影响，并优化资源分配以提升性能。

Method: 分析不同内容类型的空间复杂度，引入效用概念管理资源分配偏好，比较动态与静态分配方法。

Result: 动态QoE感知资源分配显著优于静态分配，效用方法提升平均QoE并控制最差情况。

Conclusion: 效用驱动的动态资源分配是优化视频QoE的有效方法。

Abstract: We analyzed spatial complexity, defined as the relationship between the
required bitrate and a corresponding picture Quality of Experience (QoE)
metric, for realistic, long, real-time, interactive video clips. Apart from
variation across different content types, e.g., game genres, we discovered
time-variability within a clip from second to second, and explored the
ramifications for traffic management. We introduced utility as an elegant way
to manage resource sharing preferences. Our analysis of resource sharing
methods shows that frequent QoE-aware reallocation has significant performance
advantages compared to static rate allocation, even in case the latter is based
on rich information about long-term average spatial complexity. We have also
shown that utility-based resource allocation has clear advantages over methods
targeting equal QoE allocation, it increases the average QoE, while it still
controls the worst case QoE.

</details>


### [3] [Native-AI Empowered Scalable Architectures and Solutions for Future Non-Terrestrial Networks: An Overview](https://arxiv.org/abs/2507.11935)
*Jikang Deng,Fizza Hassan,Hui Zhou,Saad Al-Ahmadi,Mohamed-Slim Alouini,Daniel B. Da Costa*

Main category: cs.NI

TL;DR: 本文探讨了将开放无线接入网（ORAN）与非地面网络（NTN）结合的框架，以解决NTN在开发和运维（DevOps）中的挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 6G网络中，NTN和ORAN的结合面临智能化和可扩展性挑战，需要一种全面的解决方案。

Method: 提出基于ORAN的NTN框架，包括灵活的前传分割、增强的RAN智能控制器、可扩展部署架构和多域服务管理。

Result: 框架展示了ORAN如何提升NTN的智能化和可扩展性，并提出了未来研究方向。

Conclusion: ORAN与NTN的结合为6G网络提供了高效、可靠的解决方案，未来需进一步探索与其他技术的结合和应用场景。

Abstract: As the path toward 6G networks is being charted, the emerging applications
have motivated evolutions of network architectures to realize the efficient,
reliable, and flexible wireless networks. Among the potential architectures,
the non-terrestrial network (NTN) and open radio access network (ORAN) have
received increasing interest from both academia and industry. Although the
deployment of NTNs ensures coverage, enhances spectral efficiency, and improves
the resilience of wireless networks. The high altitude and mobility of NTN
present new challenges in the development and operations (DevOps) lifecycle,
hindering intelligent and scalable network management due to the lack of native
artificial intelligence (AI) capability. With the advantages of ORAN in
disaggregation, openness, virtualization, and intelligence, several works
propose integrating ORAN principles into the NTN, focusing mainly on ORAN
deployment options based on transparent and regenerative systems. However, a
holistic view of how to effectively combine ORAN and NTN throughout the DevOps
lifecycle is still missing, especially regarding how intelligent ORAN addresses
the scalability challenges in NTN. Motivated by this, in this paper, we first
provide the background knowledge about ORAN and NTN, outline the
state-of-the-art research on ORAN for NTNs, and present the DevOps challenges
that motivate the adoption of ORAN solutions. We then propose the ORAN-based
NTN framework, discussing its features and architectures in detail. These
include the discussion about flexible fronthaul split, RAN intelligent
controllers (RICs) enhancement for distributed learning, scalable deployment
architecture, and multi-domain service management. Finally, the future research
directions, including combinations of the ORAN-based NTN framework and other
enabling technologies and schemes, as well as the candidate use cases, are
highlighted.

</details>


### [4] [FastReChain: Highly Responsive and Low-Overhead Centralized Route Scheduling in Clos Datacenter Networks](https://arxiv.org/abs/2507.12265)
*Zihan Zhu,Dongchao Wu,Zhanbang Zhang,Jian Yang*

Main category: cs.NI

TL;DR: 提出了一种集中式调度算法，支持动态调度，适用于光交换机数据中心网络，实现理论最大吞吐量，并减少重排次数。


<details>
  <summary>Details</summary>
Motivation: 解决数据中心网络中光交换机因无缓冲和长切换时间导致的动态调度问题。

Method: 采用替换链概念和位集优化，设计集中式调度算法。

Result: 算法在双向Clos网络中实现最大吞吐量，重排次数接近最小，运行时间显著优于其他算法。

Conclusion: 该算法灵活高效，适用于实际环境，支持动态调度和频繁调整。

Abstract: Ever since Clos topologies were used in datacenter networks (DCNs), a
practical centralized scheduling algorithm that supports dynamic scheduling has
been absent. The introduction of optical switches in DCNs as a future-proof
solution exacerbates this problem due to several properties of optical
switches, such as the fact that they are generally bufferless and therefore
rely on centralized scheduling, and that they have long switching times and
therefore require the number of rearrangements to be minimized.
  In this paper, we propose a centralized scheduling algorithm that achieves
theoretical maximum throughput even in one-rate bidirectional Clos networks,
while producing schemes with near-minimal numbers of rearrangements. It is the
only algorithm that directly supports bidirectional Clos networks and has a
time efficiency high enough to support dynamic scheduling to date. For static
minimal rewiring, its running time ranges from a fraction to a few hundredths
of other algorithms, and the number of rearrangements has also been steadily
improved, allowing for more frequent adjustments and less impact on ongoing
communications. In addition, the algorithm is very flexible and can support
various functional requirements in real-world environments. We achieve this
result through the replacement chain concept and bitset optimization.

</details>


### [5] [LLM-Based Config Synthesis requires Disambiguation](https://arxiv.org/abs/2507.12443)
*Rajdeep Mondal,Nikolaj Bjorner,Todd Millstein,Alan Tang,George Varghese*

Main category: cs.NI

TL;DR: 论文探讨了LLM在程序合成中的意图模糊问题，提出原型系统Clarify，通过消歧模块解决用户意图不明确的问题。


<details>
  <summary>Details</summary>
Motivation: 用户意图模糊是LLM程序合成中的主要问题，尤其在网络配置（如路由映射和ACL）中，优先级难以推断。

Method: 提出Clarify系统，结合LLM与消歧模块（Disambiguator），逐步合成并验证路由策略。

Result: 在大型云环境中，复杂ACL存在大量重叠，验证了模糊问题的现实性；Clarify在小规模合成任务中表现良好。

Conclusion: Clarify通过消歧模块有效解决意图模糊问题，适用于LLM合成更新意图但集成模糊的场景。

Abstract: Beyond hallucinations, another problem in program synthesis using LLMs is
ambiguity in user intent. We illustrate the ambiguity problem in a networking
context for LLM-based incremental configuration synthesis of route-maps and
ACLs. These structures frequently overlap in header space, making the relative
priority of actions impossible for the LLM to infer without user interaction.
Measurements in a large cloud identify complex ACLs with 100's of overlaps,
showing ambiguity is a real problem. We propose a prototype system, Clarify,
which uses an LLM augmented with a new module called a Disambiguator that helps
elicit user intent. On a small synthetic workload, Clarify incrementally
synthesizes routing policies after disambiguation and then verifies them. Our
treatment of ambiguities is useful more generally when the intent of updates
can be correctly synthesized by LLMs, but their integration is ambiguous and
can lead to different global behaviors.

</details>


### [6] [CRAFT: Latency and Cost-Aware Genetic-Based Framework for Node Placement in Edge-Fog Environments](https://arxiv.org/abs/2507.12445)
*Soheil Mahdizadeh,Amir Mahdi Rasouli,Mohammad Pourashory,Sadra Galavani,Mohsen Ansari*

Main category: cs.NI

TL;DR: 本文提出了一种基于遗传算法的边缘-雾节点部署策略，旨在降低延迟和成本，实验结果显示延迟和成本分别降低了2.77%和31.15%。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）中降低延迟是关键问题，云计算无法满足实时需求，边缘和雾计算通过将计算节点靠近终端用户提供了更低延迟和更强处理能力。

Method: 采用遗传算法优化边缘和雾节点的部署策略，目标是实现延迟和成本的最小化。

Result: 仿真结果显示，所提框架实现了最高2.77%的延迟降低和31.15%的成本降低。

Conclusion: 基于遗传算法的节点部署策略能有效优化边缘-雾计算框架，显著降低延迟和成本。

Abstract: Reducing latency in the Internet of Things (IoT) is a critical concern. While
cloud computing facilitates communication, it falls short of meeting real-time
requirements reliably. Edge and fog computing have emerged as viable solutions
by positioning computing nodes closer to end users, offering lower latency and
increased processing power. An edge-fog framework comprises various components,
including edge and fog nodes, whose strategic placement is crucial as it
directly impacts latency and system cost. This paper presents an effective and
tunable node placement strategy based on a genetic algorithm to address the
optimization problem of deploying edge and fog nodes. The main objective is to
minimize latency and cost through optimal node placement. Simulation results
demonstrate that the proposed framework achieves up to 2.77% latency and 31.15%
cost reduction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [A Study on the Application of Artificial Intelligence in Ecological Design](https://arxiv.org/abs/2507.11595)
*Hengyue Zhao*

Main category: cs.AI

TL;DR: 探讨AI能否推动人类与自然的关系从支配转向相互依存，并通过案例研究展示AI在生态设计中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究人类与自然关系能否通过AI实现真正的相互依存，以及AI如何改变生态设计的理论与实践。

Method: 通过案例研究分析AI在数据、图像识别和生态恢复中的应用，并结合原型设计（AI辅助水体修复）提出设计路径。

Result: AI能够结合科学洞察、艺术实践和环境保护，为可持续技术生态系统提供研究方向。

Conclusion: AI在生态设计中具有潜力，可为未来可持续技术生态系统研究提供指导。

Abstract: This paper asks whether our relationship with nature can move from human
dominance to genuine interdependence, and whether artificial intelligence (AI)
can mediate that shift. We examine a new ecological-design paradigm in which AI
interacts with non-human life forms. Through case studies we show how artists
and designers apply AI for data analysis, image recognition, and ecological
restoration, producing results that differ from conventional media. We argue
that AI not only expands creative methods but also reframes the theory and
practice of ecological design. Building on the author's prototype for
AI-assisted water remediation, the study proposes design pathways that couple
reinforcement learning with plant-based phytoremediation. The findings
highlight AI's potential to link scientific insight, artistic practice, and
environmental stewardship, offering a roadmap for future research on
sustainable, technology-enabled ecosystems.

</details>


### [8] [General Modular Harness for LLM Agents in Multi-Turn Gaming Environments](https://arxiv.org/abs/2507.11633)
*Yuxuan Zhang,Haoyang Yu,Lanxiang Hu,Haojian Jin,Hao Zhang*

Main category: cs.AI

TL;DR: 提出了一种模块化设计的LLM代理框架，包含感知、记忆和推理组件，适用于多轮游戏环境，无需领域特定工程。


<details>
  <summary>Details</summary>
Motivation: 通过游戏环境测试模块化设计对代理性能的影响，推动通用代理的发展。

Method: 使用经典和现代游戏套件作为测试平台，分析各模块在动态交互环境中的作用。

Result: 实验表明该框架显著提升性能，不同模块在不同场景中贡献各异（如记忆在长时谜题中占主导）。

Conclusion: 模块化设计有效提升通用代理能力，游戏环境的多样性验证了其潜力。

Abstract: We introduce a modular harness design for LLM agents that composes of
perception, memory, and reasoning components, enabling a single LLM or VLM
backbone to tackle a wide spectrum of multi turn gaming environments without
domain-specific engineering. Using classic and modern game suites as
low-barrier, high-diversity testbeds, our framework provides a unified workflow
for analyzing how each module affects performance across dynamic interactive
settings. Extensive experiments demonstrate that the harness lifts gameplay
performance consistently over un-harnessed baselines and reveals distinct
contribution patterns, for example, memory dominates in long-horizon puzzles
while perception is critical in vision noisy arcades. These findings highlight
the effectiveness of our modular harness design in advancing general-purpose
agent, given the familiarity and ubiquity of games in everyday human
experience.

</details>


### [9] [Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification](https://arxiv.org/abs/2507.11662)
*Moises Andrade,Joonhyuk Cha,Brandon Ho,Vriksha Srihari,Karmesh Yadav,Zsolt Kira*

Main category: cs.AI

TL;DR: MLLMs作为验证器在复杂任务中存在一致性偏差问题，提出SGV方法显著提升验证性能。


<details>
  <summary>Details</summary>
Motivation: 在缺乏明确成功标准的领域（如计算机使用），如何将人类直觉转化为可扩展的规则是一个挑战。MLLMs因其世界知识和对齐人类偏好的能力成为潜在解决方案。

Method: 提出Self-Grounded Verification (SGV)，通过无条件与条件生成结合，利用MLLMs自身的采样机制优化验证过程。

Result: SGV使MLLMs验证器的准确率提升20%，并在多个任务中实现实时监督，性能超越之前最佳方法48%。

Conclusion: SGV有效解决了MLLMs作为验证器时的一致性偏差问题，显著提升了复杂任务中的验证性能。

Abstract: Verifiers -- functions assigning rewards to agent behavior -- have been key
for AI progress in domains like math and board games. However, extending these
gains to domains without clear-cut success criteria (e.g.,computer use) remains
a challenge: while humans can recognize suitable outcomes, translating this
intuition into scalable rules is non-trivial. Multimodal Large Language
Models(MLLMs) emerge as a promising solution, given their world knowledge,
human-preference alignment, and reasoning skills. We evaluate MLLMs as
verifiers of agent trajectories across web navigation, computer use, and
robotic manipulation, and identify a critical limitation: agreement bias, a
strong tendency for MLLMs to favor information in their context window, often
generating chains of thought to rationalize flawed behavior. This bias is
pervasive across models, resilient to test-time scaling, and can impact several
methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs
despite MLLMs showing strong, human-aligned priors on desired behavior. To
address this, we propose Self-Grounded Verification (SGV), a lightweight method
that enables more effective use of MLLMs' knowledge and reasoning by harnessing
their own sampling mechanisms via unconditional and conditional generation. SGV
operates in two steps: first, the MLLM is elicited to retrieve broad priors
about task completion, independent of the data under evaluation. Then,
conditioned on self-generated priors, it reasons over and evaluates a candidate
trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in
accuracy and failure detection rates, and can perform real-time supervision of
heterogeneous agents, boosting task completion of a GUI specialist in OSWorld,
a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting
a new state of the art on the benchmark, surpassing the previous best by 48%.

</details>


### [10] [ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making](https://arxiv.org/abs/2507.11733)
*Srikanth Vemula*

Main category: cs.AI

TL;DR: ClarifAI是一种结合案例推理（CBR）和本体驱动方法的新方法，旨在提升AI的透明度和可解释性，适用于高风险的决策场景。


<details>
  <summary>Details</summary>
Motivation: 解决AI在复杂决策场景中的透明度和可解释性问题，满足不同利益相关者的需求。

Method: 结合案例推理（CBR）和本体驱动方法，设计理论框架和架构蓝图。

Result: ClarifAI能够显著提升AI系统的可解释性，适用于高风险环境。

Conclusion: ClarifAI为AI在关键决策过程中的应用铺平了道路，具有广泛的应用潜力。

Abstract: This Study introduces Clarity and Reasoning Interface for Artificial
Intelligence(ClarifAI), a novel approach designed to augment the transparency
and interpretability of artificial intelligence (AI) in the realm of improved
decision making. Leveraging the Case-Based Reasoning (CBR) methodology and
integrating an ontology-driven approach, ClarifAI aims to meet the intricate
explanatory demands of various stakeholders involved in AI-powered
applications. The paper elaborates on ClarifAI's theoretical foundations,
combining CBR and ontologies to furnish exhaustive explanation mechanisms. It
further elaborates on the design principles and architectural blueprint,
highlighting ClarifAI's potential to enhance AI interpretability across
different sectors and its applicability in high-stake environments. This
research delineates the significant role of ClariAI in advancing the
interpretability of AI systems, paving the way for its deployment in critical
decision-making processes.

</details>


### [11] [Auto-Formulating Dynamic Programming Problems with Large Language Models](https://arxiv.org/abs/2507.11737)
*Chenyu Zhou,Jingyuan Yang,Linwei Xin,Yitian Chen,Ziyan He,Dongdong Ge*

Main category: cs.AI

TL;DR: 论文提出了DP-Bench基准和DPLM模型，用于自动化动态规划问题的建模，通过DualReflect数据生成方法解决了训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 动态规划问题建模通常需要专业知识，而现有LLM方法难以直接应用。论文旨在通过自动化方法解决这一问题。

Method: 提出DPLM模型和DualReflect数据生成方法，结合前向和后向生成以扩展训练数据。

Result: DPLM在性能上媲美现有LLM，并在复杂问题上表现更优。

Conclusion: DualReflect方法展示了前向和后向生成的互补性，为低数据环境下的动态规划建模提供了有效解决方案。

Abstract: Dynamic programming (DP) is a fundamental method in operations research, but
formulating DP models has traditionally required expert knowledge of both the
problem context and DP techniques. Large Language Models (LLMs) offer the
potential to automate this process. However, DP problems pose unique challenges
due to their inherently stochastic transitions and the limited availability of
training data. These factors make it difficult to directly apply existing
LLM-based models or frameworks developed for other optimization problems, such
as linear or integer programming. We introduce DP-Bench, the first benchmark
covering a wide range of textbook-level DP problems to enable systematic
evaluation. We present Dynamic Programming Language Model (DPLM), a
7B-parameter specialized model that achieves performance comparable to
state-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on
hard problems. Central to DPLM's effectiveness is DualReflect, our novel
synthetic data generation pipeline, designed to scale up training data from a
limited set of initial examples. DualReflect combines forward generation for
diversity and backward generation for reliability. Our results reveal a key
insight: backward generation is favored in low-data regimes for its strong
correctness guarantees, while forward generation, though lacking such
guarantees, becomes increasingly valuable at scale for introducing diverse
formulations. This trade-off highlights the complementary strengths of both
approaches and the importance of combining them.

</details>


### [12] [Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity](https://arxiv.org/abs/2507.11787)
*Chandrashekar Muniyappa,Eunjin Kim*

Main category: cs.AI

TL;DR: 综述探讨了群体智能在基于语义相似性的文档搜索中的应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 群体智能因其高效性在解决计算机优化问题中广泛应用，本文旨在总结其在语义相似性文档搜索中的最新进展。

Method: 通过文献综述方法，分析群体智能算法在语义相似性文档搜索中的应用。

Result: 总结了群体智能在此领域的最新发展，并提出了未来研究的潜在方向。

Conclusion: 群体智能在语义相似性文档搜索中具有潜力，未来研究应进一步探索其优化和应用。

Abstract: Swarm Intelligence (SI) is gaining a lot of popularity in artificial
intelligence, where the natural behavior of animals and insects is observed and
translated into computer algorithms called swarm computing to solve real-world
problems. Due to their effectiveness, they are applied in solving various
computer optimization problems. This survey will review all the latest
developments in Searching for documents based on semantic similarity using
Swarm Intelligence algorithms and recommend future research directions.

</details>


### [13] [A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS](https://arxiv.org/abs/2507.11916)
*Ehsan Futuhi,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: 论文提出了一种基于GPU的深度优先搜索（DFS）批处理方法，扩展了经典搜索算法（如IDA*和BTS），并在3x3魔方和4x4滑块拼图上验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: GPU技术的快速发展为经典搜索算法提供了并行处理的新机会，但目前很少有算法在搜索过程中充分利用GPU。

Method: 提出了一种成本限制的深度优先搜索（CB-DFS）方法，结合CPU和GPU的并行能力，扩展了IDA*和BTS算法。

Result: 在3x3魔方和4x4滑块拼图上的实验表明，GPU操作可以高效批处理，且分析了超参数、启发式神经网络大小和硬件资源对性能的影响。

Conclusion: 该方法成功将GPU批处理应用于DFS，保持了最优性保证，为搜索算法的并行化提供了新思路。

Abstract: The rapid advancement of GPU technology has unlocked powerful parallel
processing capabilities, creating new opportunities to enhance classic search
algorithms. A recent successful application of GPUs is in compressing large
pattern database (PDB) heuristics using neural networks while preserving
heuristic admissibility. However, very few algorithms have been designed to
exploit GPUs during search. Several variants of A* exist that batch GPU
computations. In this paper we introduce a method for batching GPU computations
in depth first search. In particular, we describe a new cost-bounded
depth-first search (CB-DFS) method that leverages the combined parallelism of
modern CPUs and GPUs. This is used to create algorithms like \emph{Batch IDA*},
an extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an
extensions of Budgeted Tree Search. Our approach builds on the general approach
used by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality
guarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding
tile puzzle (STP), showing that GPU operations can be efficiently batched in
DFS. Additionally, we conduct extensive experiments to analyze the effects of
hyperparameters, neural network heuristic size, and hardware resources on
performance.

</details>


### [14] [Aime: Towards Fully-Autonomous Multi-Agent Framework](https://arxiv.org/abs/2507.11988)
*Yexuan Shi,Mingyu Wang,Yunxiang Cao,Hongjie Lai,Junjian Lan,Xin Han,Yu Wang,Jie Geng,Zhenan Li,Zihao Xia,Xiang Chen,Chen Li,Jian Xu,Wenbo Duan,Yuanshuo Zhu*

Main category: cs.AI

TL;DR: Aime是一个新型多智能体框架，通过动态反应式规划和执行解决传统plan-and-execute框架的局限性，提升适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体系统（MAS）在动态环境中因静态规划和执行、固定能力及低效通信而表现不佳。

Method: Aime引入动态规划器、动态执行器工厂和集中式进度管理模块，实现实时策略调整、按需创建智能体及全局状态感知。

Result: 在多个基准测试中，Aime表现优于现有最先进的专用智能体，适应性和任务成功率显著提升。

Conclusion: Aime为多智能体协作提供了更灵活、高效的基础框架。

Abstract: Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are
emerging as a powerful paradigm for solving complex, multifaceted problems.
However, the potential of these systems is often constrained by the prevalent
plan-and-execute framework, which suffers from critical limitations: rigid plan
execution, static agent capabilities, and inefficient communication. These
weaknesses hinder their adaptability and robustness in dynamic environments.
This paper introduces Aime, a novel multi-agent framework designed to overcome
these challenges through dynamic, reactive planning and execution. Aime
replaces the conventional static workflow with a fluid and adaptive
architecture. Its core innovations include: (1) a Dynamic Planner that
continuously refines the overall strategy based on real-time execution
feedback; (2) an Actor Factory that implements Dynamic Actor instantiation,
assembling specialized agents on-demand with tailored tools and knowledge; and
(3) a centralized Progress Management Module that serves as a single source of
truth for coherent, system-wide state awareness. We empirically evaluated Aime
on a diverse suite of benchmarks spanning general reasoning (GAIA), software
engineering (SWE-bench Verified), and live web navigation (WebVoyager). The
results demonstrate that Aime consistently outperforms even highly specialized
state-of-the-art agents in their respective domains. Its superior adaptability
and task success rate establish Aime as a more resilient and effective
foundation for multi-agent collaboration.

</details>


### [15] [Understanding visual attention beehind bee-inspired UAV navigation](https://arxiv.org/abs/2507.11992)
*Pranav Rajbhandari,Abhi Veda,Matthew Garratt,Mandayam Srinivasan,Sridhar Ravi*

Main category: cs.AI

TL;DR: 论文研究了基于光流的强化学习无人机导航策略，发现训练后的智能体主要关注光流的不连续区域和大光流区域，行为类似昆虫飞行。


<details>
  <summary>Details</summary>
Motivation: 生物系统（如蜜蜂）利用有限感官和计算能力实现飞行和避障，启发无人机导航研究。

Method: 使用强化学习训练智能体仅通过光流输入在障碍隧道中导航，并分析其注意力模式。

Result: 智能体主要关注光流不连续和大光流区域，行为类似昆虫飞行，策略适用于物理无人机控制。

Conclusion: 该策略可能为开发简单无人机控制法则提供有效方法。

Abstract: Bio-inspired design is often used in autonomous UAV navigation due to the
capacity of biological systems for flight and obstacle avoidance despite
limited sensory and computational capabilities. In particular, honeybees mainly
use the sensory input of optic flow, the apparent motion of objects in their
visual field, to navigate cluttered environments. In our work, we train a
Reinforcement Learning agent to navigate a tunnel with obstacles using only
optic flow as sensory input. We inspect the attention patterns of trained
agents to determine the regions of optic flow on which they primarily base
their motor decisions. We find that agents trained in this way pay most
attention to regions of discontinuity in optic flow, as well as regions with
large optic flow magnitude. The trained agents appear to navigate a cluttered
tunnel by avoiding the obstacles that produce large optic flow, while
maintaining a centered position in their environment, which resembles the
behavior seen in flying insects. This pattern persists across independently
trained agents, which suggests that this could be a good strategy for
developing a simple explicit control law for physical UAVs.

</details>


### [16] [Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs](https://arxiv.org/abs/2507.12110)
*Ye Han,Lijun Zhang,Dejian Meng,Zhuang Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种拓扑增强的多智能体强化学习方法（TPE-MARL），用于优化混合交通中联网自动驾驶车辆（CAVs）的协作决策，通过压缩高维状态信息和减少搜索空间，显著提升了探索与利用的平衡。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）中探索与利用的权衡问题因联合状态-动作空间的指数增长而加剧，特别是在混合交通场景中。

Method: 构建动态交通流的游戏拓扑张量压缩高维状态信息，并以QMIX为基础，结合访问计数和智能体互信息，建立拓扑增强的MARL框架。

Result: 在不同交通密度和CAV渗透率下的仿真表明，TPE-MARL在交通效率、安全性、决策平滑性和任务完成度上表现优异，且决策合理性接近或超过人类驾驶员。

Conclusion: TPE-MARL有效解决了MARL中的探索与利用问题，适用于混合和全自动驾驶场景。

Abstract: The exploration-exploitation trade-off constitutes one of the fundamental
challenges in reinforcement learning (RL), which is exacerbated in multi-agent
reinforcement learning (MARL) due to the exponential growth of joint
state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)
method for optimizing cooperative decision-making of connected and autonomous
vehicles (CAVs) in mixed traffic. This work presents two primary contributions:
First, we construct a game topology tensor for dynamic traffic flow,
effectively compressing high-dimensional traffic state information and decrease
the search space for MARL algorithms. Second, building upon the designed game
topology tensor and using QMIX as the backbone RL algorithm, we establish a
topology-enhanced MARL framework incorporating visit counts and agent mutual
information. Extensive simulations across varying traffic densities and CAV
penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations
encompassing training dynamics, exploration patterns, macroscopic traffic
performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL
successfully balances exploration and exploitation. Consequently, it exhibits
superior performance in terms of traffic efficiency, safety, decision
smoothness, and task completion. Furthermore, the algorithm demonstrates
decision-making rationality comparable to or exceeding that of human drivers in
both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is
available at
\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.

</details>


### [17] [Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation](https://arxiv.org/abs/2507.12186)
*Edward Kim,Hanna Kurniawati*

Main category: cs.AI

TL;DR: 提出了一种新的在线近似POMDP求解器，通过深度采样未来历史并逐步更新策略，性能损失受采样误差平均而非最大值限制，实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在线规划中采样稀疏性问题，提升动态环境下POMDP求解的性能。

Method: 提出Partially Observable Reference Policy Programming，深度采样未来历史并逐步更新策略。

Result: 理论证明性能损失受采样误差平均限制，实验在大规模动态问题中表现优于现有方法。

Conclusion: 该算法在理论和实践中均表现出色，适用于动态复杂环境。

Abstract: This paper proposes Partially Observable Reference Policy Programming, a
novel anytime online approximate POMDP solver which samples meaningful future
histories very deeply while simultaneously forcing a gradual policy update. We
provide theoretical guarantees for the algorithm's underlying scheme which say
that the performance loss is bounded by the average of the sampling
approximation errors rather than the usual maximum, a crucial requirement given
the sampling sparsity of online planning. Empirical evaluations on two
large-scale problems with dynamically evolving environments -- including a
helicopter emergency scenario in the Corsica region requiring approximately 150
planning steps -- corroborate the theoretical results and indicate that our
solver considerably outperforms current online benchmarks.

</details>


### [18] [BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2507.12207)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: BuildEvo利用大型语言模型自动设计高效且可解释的建筑能耗预测启发式方法，结合物理原理，提升泛化能力和透明度。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法精度不足，而高级模型缺乏透明性且忽视物理原理，难以泛化。

Method: 通过进化过程，引导大型语言模型结合建筑特性和操作数据的物理洞察，自动构建和优化启发式方法。

Result: 在基准测试中达到先进性能，泛化能力更强且预测逻辑透明。

Conclusion: BuildEvo推动了自动化设计基于物理的鲁棒启发式方法，为复杂能源系统提供可信模型。

Abstract: Accurate building energy forecasting is essential, yet traditional heuristics
often lack precision, while advanced models can be opaque and struggle with
generalization by neglecting physical principles. This paper introduces
BuildEvo, a novel framework that uses Large Language Models (LLMs) to
automatically design effective and interpretable energy prediction heuristics.
Within an evolutionary process, BuildEvo guides LLMs to construct and enhance
heuristics by systematically incorporating physical insights from building
characteristics and operational data (e.g., from the Building Data Genome
Project 2). Evaluations show BuildEvo achieves state-of-the-art performance on
benchmarks, offering improved generalization and transparent prediction logic.
This work advances the automated design of robust, physically grounded
heuristics, promoting trustworthy models for complex energy systems.

</details>


### [19] [Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning](https://arxiv.org/abs/2507.12215)
*Yuhao Chen,Shuochen Liu,Yuanjie Lyu,Chao Zhang,Jiayao Shi,Tong Xu*

Main category: cs.AI

TL;DR: 论文提出了一种针对中国象棋（Xiangqi）的训练框架Xiangqi-R1，通过多阶段训练提升LLMs在空间战略推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在复杂棋盘游戏中的空间战略推理能力，填补现有研究的不足。

Method: 采用多阶段训练：1）微调合法移动预测；2）结合战略注释；3）使用GRPO强化学习。

Result: Xiangqi-R1在移动合法性和分析准确性上分别提升18%和22%。

Conclusion: 该框架为复杂空间领域的通用战略智能提供了可行路径。

Abstract: Game playing has long served as a fundamental benchmark for evaluating
Artificial General Intelligence (AGI). While Large Language Models (LLMs) have
demonstrated impressive capabilities in general reasoning, their effectiveness
in spatial strategic reasoning, which is critical for complex and fully
observable board games, remains insufficiently explored. In this work, we adopt
Chinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate
rules and spatial complexity. To advance LLMs' strategic competence in such
environments, we propose a training framework tailored to Xiangqi, built upon a
large-scale dataset of five million board-move pairs enhanced with expert
annotations and engine evaluations. Building on this foundation, we introduce
Xiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning
for legal move prediction to capture basic spatial rules, (2) incorporating
strategic annotations to improve decision-making, and (3) applying
reinforcement learning via Group Relative Policy Optimization (GRPO) with
multi-dimensional reward signals to enhance reasoning stability. Our
Experimental results indicate that, despite their size and power,
general-purpose LLMs struggle to achieve satisfactory performance in these
tasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an
18% rise in move legality and a 22% boost in analysis accuracy. Our results
point to a promising path for creating general strategic intelligence in
spatially complex areas.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [20] [Sub-Connected Hybrid Beamfocusing Design for RSMA-Enabled Near-Field Communications with Imperfect CSI and SIC](https://arxiv.org/abs/2507.11854)
*Jiasi Zhou,Ruirui Chen,Yanjing Sun,Chintha Tellambura*

Main category: cs.IT

TL;DR: 研究探讨了近场通信中波束聚焦是否能完全消除干扰，结合RSMA和HAD架构，提出了一种优化算法。


<details>
  <summary>Details</summary>
Motivation: 探讨波束聚焦在完美和不完美CSI下是否能完全消除干扰，验证RSMA在干扰管理中的优势。

Method: 采用子连接HAD架构和RSMA，提出惩罚性BCD算法和低复杂度算法优化波束聚焦和速率分配。

Result: 波束聚焦无法完全消除干扰；RSMA在干扰管理中优于SDMA；HAD架构性能接近最优。

Conclusion: 波束聚焦不足，RSMA结合HAD架构是有效的干扰管理方案。

Abstract: Near-field spherical waves inherently encode both direction and distance
information, enabling spotlight-like beam focusing for targeted interference
mitigation. However, whether such beam focusing can fully eliminate
interference under perfect and imperfect channel state information (CSI),
rendering advanced interference management schemes unnecessary, remains an open
question. To address this, we investigate rate-splitting multiple access
(RSMA)-enabled near-field communications (NFC) under imperfect SCI. Our
transmit scheme employs a sub-connected hybrid analog-digital (HAD)
architecture to reduce hardware overhead while incorporating imperfect
successive interference cancellation (SIC) for practical implementation. A
minimum rate maximization problem is formulated by jointly optimizing the
analog beamfocuser, the digital beamfocuser, and the common rate allocation. To
solve the non-convex problem, we develop a penalty-based block coordinate
descent (BCD) algorithm, deriving closed-form expressions for the optimal
analog and digital beamfocusers solutions. Furthermore, to reduce computational
complexity, we propose a low-complexity algorithm, where analog and digital
beamfocusers are designed in two separate stages. Simulation results underscore
that: 1) beamfocusing alone is insufficient to fully suppress interference even
under perfect CSI; 2) RSMA exhibits superior interference management over SDMA
under imperfect CSI and SIC conditions; 3) sub-connected HAD architecture
delivers near-optimal digital beamfocusing performance with fewer radio
frequency chains.

</details>


### [21] [The Role of Rank in Mismatched Low-Rank Symmetric Matrix Estimation](https://arxiv.org/abs/2507.12019)
*Panpan Niu,Yuhao Liu,Teng Fu,Jie Fan,Chaowen Deng,Zhongyi Huang*

Main category: cs.IT

TL;DR: 论文研究了贝叶斯统计学家在恢复受高斯噪声干扰的低秩信号矩阵时的性能，分析了信号秩、功率和信噪比不匹配对估计误差的影响。


<details>
  <summary>Details</summary>
Motivation: 低秩信号恢复在机器学习、信号处理和统计中有广泛应用，但信号参数不匹配对估计性能的影响尚未充分研究。

Method: 利用高斯正交系综（GOE）谱和k维球面积分的渐近行为，推导了贝叶斯估计器的渐近均方误差（MSE）表达式。

Result: 分析了信号秩不匹配对渐近MSE的影响，并考虑了球面和高斯信号的情况。

Conclusion: 研究为低秩信号恢复中的参数不匹配问题提供了理论支持，对实际应用具有指导意义。

Abstract: We investigate the performance of a Bayesian statistician tasked with
recovering a rank-\(k\) signal matrix \(\bS \bS^{\top} \in \mathbb{R}^{n \times
n}\), corrupted by element-wise additive Gaussian noise. This problem lies at
the core of numerous applications in machine learning, signal processing, and
statistics. We derive an analytic expression for the asymptotic mean-square
error (MSE) of the Bayesian estimator under mismatches in the assumed signal
rank, signal power, and signal-to-noise ratio (SNR), considering both sphere
and Gaussian signals. Additionally, we conduct a rigorous analysis of how rank
mismatch influences the asymptotic MSE. Our primary technical tools include the
spectrum of Gaussian orthogonal ensembles (GOE) with low-rank perturbations and
asymptotic behavior of \(k\)-dimensional spherical integrals.

</details>


### [22] [On the error correction of iterative bounded distance decoding of generalized LDPC codes](https://arxiv.org/abs/2507.12073)
*David Burshtein*

Main category: cs.IT

TL;DR: 论文研究了正则广义LDPC（GLDPC）码的解码性能，放宽了左度较小的码的纠错条件，并扩展了随机错误的分析。


<details>
  <summary>Details</summary>
Motivation: 探讨GLDPC码在比特翻转有界距离解码算法下的纠错能力，特别是针对左度较小的码和随机错误情况。

Method: 通过分析Tanner图的必要条件，证明失败事件的概率随码长增加而趋近于零，并扩展到随机错误的纠错能力。

Result: 证明了左度较小的码可以放宽纠错条件，并给出了随机错误纠错能力的下界。

Conclusion: 研究结果适用于非二进制GLDPC码，并通过数值示例验证了理论分析。

Abstract: Consider an ensemble of regular generalized LDPC (GLDPC) codes and assume
that the same component code is associated with each parity check node. To
decode a GLDPC code from the ensemble, we use the bit flipping bounded distance
decoding algorithm, which is an extension of the bit flipping algorithm for
LDPC codes. Previous work has shown conditions, under which, for a typical code
in the ensemble with blocklength sufficiently large, a positive constant
fraction of worst case errors can be corrected. In this work we first show that
these requirements can be relaxed for ensembles with small left degrees. While
previous work on GLDPC codes has considered expander graph arguments, our
analysis formulates a necessary condition that the Tanner graph needs to
satisfy for a failure event and then shows that the probability of this event
vanishes for a sufficiently large blocklength. We then extend the analysis to
random error correction and derive a lower bound on the fraction of random
errors that can be corrected asymptotically. We discuss the extension of our
results to non-binary GLDPC codes and present numerical examples.

</details>


### [23] [Lowering Error Floors for Hard Decision Decoding of OFEC Code](https://arxiv.org/abs/2507.12155)
*Jasper Lagendijk,Yunus Can Gültekin,Alexios Balatsoukas-Stimming,Gabriele Liga,Alex Alvarado*

Main category: cs.IT

TL;DR: 提出了一种新的停滞模式去除算法，显著降低了OFEC码硬判决解码的错误平台。


<details>
  <summary>Details</summary>
Motivation: 停滞模式会导致OFEC码硬判决解码的错误平台，现有算法存在改进空间。

Method: 提出了一种新颖的停滞模式去除算法。

Result: 该算法将现有算法的错误平台降低了一个数量级。

Conclusion: 新算法在降低错误平台方面表现优异，具有实际应用潜力。

Abstract: Stall patterns are known to cause an error floor in hard decision decoding of
the OFEC code. We propose a novel stall pattern removal algorithm that lowers
the error floor of state-of-the-art algorithms by an order of magnitude

</details>


### [24] [Characterization and constructions of binary self-orthogonal singly-even linear codes](https://arxiv.org/abs/2507.12240)
*Kangquan Li,Hao Chen,Wengang Jin,Longjiang Qu*

Main category: cs.IT

TL;DR: 本文研究了二进制自正交（SO）线性码中的单偶（singly-even）情况，提供了完整的特征描述和生成方法，并构造了多类新的SO单偶线性码。


<details>
  <summary>Details</summary>
Motivation: 二进制自正交码在量子信息理论和格设计中有重要应用，但单偶SO码的研究较少，本文旨在填补这一空白。

Method: 通过已知SO码生成新码，提供布尔函数的构造条件，并确定码的权重分布。

Result: 构造了三类无限SO单偶线性码，并验证了其极小性和违反Aschikhmin-Barg条件的特性。

Conclusion: 本文的方法可进一步构造更多具有特定性质的二进制SO线性码。

Abstract: Recent research has focused extensively on constructing binary
self-orthogonal (SO) linear codes due to their applications in quantum
information theory, lattice design, and related areas. Despite significant
activity, the fundamental characterization remains unchanged: binary SO codes
are necessarily even (all codeword weights even), while doubly-even codes
(weights divisible by $4$) are automatically SO.
  This paper advances the theory by addressing the understudied case of
singly-even (even but not doubly-even) SO codes. We first provide a complete
characterization of binary SO linear codes, and a necessary and sufficient
condition for binary SO singly-even linear codes is given. Moreover, we give a
general approach to generating many binary SO linear codes from two known SO
linear codes, yielding three infinite classes of binary SO singly-even linear
codes with few weights. Note that these new codes are also minimal and violate
the Aschikhmin-Barg condition. Their weight distributions are determined.
Furthermore, we give a necessary and sufficient condition for a Boolean
function $f$ such that the linear code proposed from $f$ via a well-known
generic construction is SO singly-even, and a general approach to constructing
Boolean functions satisfying this condition is provided, yielding several
infinite classes of binary SO singly-even minimal linear codes with few
weights. Finally, we would like to emphasize that using the methods in this
paper, we can construct more binary linear codes that are SO, singly-even,
minimal, violating the AB condition, and with few weights at the same time.

</details>


### [25] [Neural Polar Decoders for Deletion Channels](https://arxiv.org/abs/2507.12329)
*Ziv Aharoni,Henry D. Pfister*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a neural polar decoder (NPD) for deletion channels with
a constant deletion rate. Existing polar decoders for deletion channels exhibit
high computational complexity of $O(N^4)$, where $N$ is the block length. This
limits the application of polar codes for deletion channels to
short-to-moderate block lengths. In this work, we demonstrate that employing
NPDs for deletion channels can reduce the computational complexity. First, we
extend the architecture of the NPD to support deletion channels. Specifically,
the NPD architecture consists of four neural networks (NNs), each replicating
fundamental successive cancellation (SC) decoder operations. To support
deletion channels, we change the architecture of only one. The computational
complexity of the NPD is $O(AN\log N)$, where the parameter $A$ represents a
computational budget determined by the user and is independent of the channel.
We evaluate the new extended NPD for deletion channels with deletion rates
$\delta\in\{0.01, 0.1\}$ and we verify the NPD with the ground truth given by
the trellis decoder by Tal et al. We further show that due to the reduced
complexity of the NPD, we are able to incorporate list decoding and further
improve performance. We believe that the extended NPD presented here could have
applications in future technologies like DNA storage.

</details>


### [26] [Efficient Remote Monitoring through Noisy Random Access with Retransmissions](https://arxiv.org/abs/2507.12368)
*Sergey Foss,Dmitriy Kim,Andrey Turlikov*

Main category: cs.IT

TL;DR: 论文研究了在噪声干扰下，设备通过随机多址方案向基站传输罕见事件信息的系统性能，分析了重传对性能的影响，并提出了两种效率标准及最优重传次数。


<details>
  <summary>Details</summary>
Motivation: 研究噪声环境下多址信道中消息丢失的问题，即使没有传输冲突，噪声也可能导致消息丢失，因此需要优化重传策略以提高系统性能。

Method: 引入独立两状态马尔可夫链模型描述事件发生，分析重传对系统性能的影响，提出两种效率标准并确定最优重传次数。

Result: 确定了两种效率标准下的最优重传次数，为系统参数提供了优化依据。

Conclusion: 通过优化重传次数，可以有效提高罕见事件监控系统的性能，特别是在噪声环境下。

Abstract: We consider a rare event monitoring system consisting of a set of devices and
a base station, where devices transmit information about rare events to the
base station using a random multiple access scheme. We introduce a model in
which the presence of noise in the multiple access channel can cause message
loss even in the absence of transmission collisions. The occurrence of events
is modeled by a family of independent two-state Markov chains (with states 0
and 1). We analyze how repeated transmissions affect system performance. Two
efficiency criteria are proposed and studied: the maximum probability that a
message about an event from a fixed device is successfully delivered to the
base station and the maximum frequency at which the base station successfully
receives updates about the entire system. For each criterion, we determine the
optimal number of retransmissions as a function of the system parameters.

</details>
