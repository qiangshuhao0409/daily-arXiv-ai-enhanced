<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 17]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)](https://arxiv.org/abs/2509.05447)
*Zhongyuan Zhao,Gunjan Verma,Ananthram Swami,Santiago Segarra*

Main category: cs.NI

TL;DR: 提出基于图神经网络的分布式链路稀疏化方案，通过调整竞争阈值减少延迟容忍业务的调度开销，同时保持网络容量。


<details>
  <summary>Details</summary>
Motivation: 密集无线网络中分布式链路调度算法产生大量信令开销，导致拥塞、能耗和无线电足迹扩大等问题。

Method: 使用图神经网络模块，基于流量统计和网络拓扑调整各链路的竞争阈值，使不太可能成功的链路退出调度竞争。采用离线约束无监督学习算法平衡最小化调度开销和确保总效用达标两个目标。

Result: 在最多500个链路的模拟无线多跳网络中，该技术有效缓解了网络拥塞并减少了四个不同分布式链路调度协议的无线电足迹。

Conclusion: 基于GNN的链路稀疏化方案能够显著降低调度开销，同时保持网络性能，适用于密集无线网络环境。

Abstract: In wireless networks characterized by dense connectivity, the significant
signaling overhead generated by distributed link scheduling algorithms can
exacerbate issues like congestion, energy consumption, and radio footprint
expansion. To mitigate these challenges, we propose a distributed link
sparsification scheme employing graph neural networks (GNNs) to reduce
scheduling overhead for delay-tolerant traffic while maintaining network
capacity. A GNN module is trained to adjust contention thresholds for
individual links based on traffic statistics and network topology, enabling
links to withdraw from scheduling contention when they are unlikely to succeed.
Our approach is facilitated by a novel offline constrained {unsupervised}
learning algorithm capable of balancing two competing objectives: minimizing
scheduling overhead while ensuring that total utility meets the required level.
In simulated wireless multi-hop networks with up to 500 links, our link
sparsification technique effectively alleviates network congestion and reduces
radio footprints across four distinct distributed link scheduling protocols.

</details>


### [2] [Joint Routing, Resource Allocation, and Energy Optimization for Integrated Access and Backhaul with Open RAN](https://arxiv.org/abs/2509.05467)
*Reshma Prasad,Maxime Elkael,Gabriele Gemmi,Osama M. Bushnaq,Debashisha Mishra,Prasanna Raut,Jennifer Simonjan,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: 本文通过两种优化目标（能源最小化和吞吐量最大化）来解决IAB网络中的路由和资源分配问题，并基于O-RAN架构集成实用解决方案，在实际数据上验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络的发展，移动网络运营商需要满足多样化需求同时管理日益增长的能源消耗。IAB网络虽然能够支持密集细胞部署，但多跳无线回传需要合适的路由和资源分配决策，同时细胞密集化使能源优化变得至关重要。

Method: 研究开发了一个新题的容量模型，将功率水平与可实现的数据速率相关联。提出了两种实用的大规模方法来解决优化问题，并利用O-RAN架构引入的闭环控制框架来集成解决方案。

Result: 在基于网络运营商在意大利米兰收集的两个月流量数据构建的多样化场景中评估，结果显示所提方法能够有效减少激活节点数量以节省能源，并在使用FR3频段或上中频段的情况下，在一天中的高峰时段实现每个用户设备约100 Mbps的最低数据速率。

Conclusion: 结果验证了我们的框架在下一代IAB网络部署和优化中的实际应用性，为6G网络的能源效率和性能优化提供了有效的解决方案。

Abstract: As networks evolve towards 6G, Mobile Network Operators (MNOs) must
accommodate diverse requirements and at the same time manage rising energy
consumption. Integrated Access and Backhaul (IAB) networks facilitate dense
cellular deployments with reduced infrastructure complexity. However, the
multi-hop wireless backhauling in IAB networks necessitates proper routing and
resource allocation decisions to meet the performance requirements. At the same
time, cell densification makes energy optimization crucial. This paper
addresses the joint optimization of routing and resource allocation in IAB
networks through two distinct objectives: energy minimization and throughput
maximization. We develop a novel capacity model that links power levels to
achievable data rates. We propose two practical large-scale approaches to solve
the optimization problems and leverage the closed-loop control framework
introduced by the Open Radio Access Network (O-RAN) architecture to integrate
the solutions. The approaches are evaluated on diverse scenarios built upon
open data of two months of traffic collected by network operators in the city
of Milan, Italy. Results show that the proposed approaches effectively reduces
number of activated nodes to save energy and achieves approximately 100 Mbps of
minimum data rate per User Equipment (UE) during peak hours of the day using
spectrum within the Frequency Range (FR) 3, or upper midband. The results
validate the practical applicability of our framework for next-generation IAB
network deployment and optimization.

</details>


### [3] [Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]](https://arxiv.org/abs/2509.05759)
*Jinkun Geng,Shuai Mu,Anirudh Sivaraman,Balaji Prabhakar*

Main category: cs.NI

TL;DR: Tiga是一个新的地理复制可扩展事务数据库设计，能够在1个广域往返时间(WRTT)内提交事务，通过整合并发控制和共识机制，使用同步时钟主动为事务分配时间戳来实现高效序列化。


<details>
  <summary>Details</summary>
Motivation: 现有的地理复制事务数据库系统在延迟和吞吐量方面存在性能瓶颈，需要设计一个能够在广域网络环境下实现低延迟高吞吐量的事务处理系统。

Method: Tiga整合并发控制和共识机制，使用同步时钟主动为事务分配未来时间戳，在大多数情况下实现1 WRTT提交，在异常情况下回退到1.5-2 WRTT的慢路径。

Result: Tiga在性能评估中表现优异，相比现有解决方案，吞吐量提高1.3-7.2倍，延迟降低1.4-4.6倍。

Conclusion: Tiga通过创新的主动时间戳排序机制，成功实现了在广域网络环境下高效的事务处理，为地理复制数据库系统提供了新的设计思路。

Abstract: This paper presents Tiga, a new design for geo-replicated and scalable
transactional databases such as Google Spanner. Tiga aims to commit
transactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of
scenarios, while maintaining high throughput with minimal computational
overhead. Tiga consolidates concurrency control and consensus, completing both
strictly serializable execution and consistent replication in a single round.
It uses synchronized clocks to proactively order transactions by assigning each
a future timestamp at submission. In most cases, transactions arrive at servers
before their future timestamps and are serialized according to the designated
timestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed
and proactive ordering fails, in which case Tiga falls back to a slow path,
committing in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can
commit more transactions at 1-WRTT latency, and incurs much less throughput
overhead. Evaluation results show that Tiga outperforms all baselines,
achieving 1.3--7.2$\times$ higher throughput and 1.4--4.6$\times$ lower
latency. Tiga is open-sourced at
https://github.com/New-Consensus-Concurrency-Control/Tiga.

</details>


### [4] [On-Dyn-CDA: A Real-Time Cost-Driven Task Offloading Algorithm for Vehicular Networks with Reduced Latency and Task Loss](https://arxiv.org/abs/2509.05889)
*Mahsa Paknejad,Parisa Fard Moshiri,Murat Simsek,Burak Kantarci,Hussein T. Mouftah*

Main category: cs.NI

TL;DR: 提出了一种在线动态成本驱动算法(On-Dyn-CDA)，用于车辆网络中的实时任务处理，显著降低了执行时间和任务丢失率。


<details>
  <summary>Details</summary>
Motivation: 车辆网络中实时任务处理面临低延迟和最小化任务丢弃率的挑战，需要高效的任务执行算法来最大化完成任务数量并减少整体延迟。

Method: 研究了静态和动态版本的优化算法，静态版本假设所有任务可用，动态版本处理实时到达的任务。区分在线和离线情况，在线版本将执行时间纳入卸载决策过程。提出了On-Dyn-CDA算法。

Result: On-Dyn-CDA在最复杂场景下仅需0.05秒完成执行，相比动态PSO的1330.05秒。任务丢失率降低3.42%，平均延迟减少29.22%。无需数据集或训练阶段，计算复杂度低。

Conclusion: On-Dyn-CDA算法在动态环境中具有高效性和可扩展性，显著优于传统PSO方法，适用于车辆网络的实时任务处理需求。

Abstract: Real-time task processing is a critical challenge in vehicular networks,
where achieving low latency and minimizing dropped task ratio depend on
efficient task execution. Our primary objective is to maximize the number of
completed tasks while minimizing overall latency, with a particular focus on
reducing number of dropped tasks. To this end, we investigate both static and
dynamic versions of an optimization algorithm. The static version assumes full
task availability, while the dynamic version manages tasks as they arrive. We
also distinguish between online and offline cases: the online version
incorporates execution time into the offloading decision process, whereas the
offline version excludes it, serving as a theoretical benchmark for optimal
performance. We evaluate our proposed Online Dynamic Cost-Driven Algorithm
(On-Dyn-CDA) against these baselines. Notably, the static Particle Swarm
Optimization (PSO) baseline assumes all tasks are transferred to the RSU and
processed by the MEC, and its offline version disregards execution time, making
it infeasible for real-time applications despite its optimal performance in
theory. Our novel On-Dyn-CDA completes execution in just 0.05 seconds under the
most complex scenario, compared to 1330.05 seconds required by Dynamic PSO. It
also outperforms Dynamic PSO by 3.42% in task loss and achieves a 29.22%
reduction in average latency in complex scenarios. Furthermore, it requires
neither a dataset nor a training phase, and its low computational complexity
ensures efficiency and scalability in dynamic environments.

</details>


### [5] [ALPHA: LLM-Enabled Active Learning for Human-Free Network Anomaly Detection](https://arxiv.org/abs/2509.05936)
*Xuanhao Luo,Shivesh Madan Nath Jha,Akruti Sinha,Zhizhen Li,Yuchen Liu*

Main category: cs.NI

TL;DR: ALPHA是首个无需人工干预的主动学习日志分析流水线，通过语义嵌入、聚类采样和LLM辅助标注实现自动化异常检测，在减少人工标注的同时达到与全监督方法相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统日志分析方法依赖专家知识或全监督学习，需要大量标注数据和人工努力，难以实现自动化异常检测。

Method: 集成语义嵌入、基于聚类的代表性采样、LLM辅助少样本标注，并提出两步少样本优化策略自适应选择提示词，通过标签传播实现大规模训练。

Result: 在真实日志数据集上的实验表明，ALPHA在减少人工参与的同时达到了与全监督方法相当的检测准确率，并支持LLM驱动的可解释根因分析。

Conclusion: ALPHA提供了一个可扩展且成本效益高的解决方案，实现了真正自动化的基于日志的异常检测。

Abstract: Network log data analysis plays a critical role in detecting security threats
and operational anomalies. Traditional log analysis methods for anomaly
detection and root cause analysis rely heavily on expert knowledge or fully
supervised learning models, both of which require extensive labeled data and
significant human effort. To address these challenges, we propose ALPHA, the
first Active Learning Pipeline for Human-free log Analysis. ALPHA integrates
semantic embedding, clustering-based representative sampling, and large
language model (LLM)-assisted few-shot annotation to automate the anomaly
detection process. The LLM annotated labels are propagated across clusters,
enabling large-scale training of an anomaly detector with minimal supervision.
To enhance the annotation accuracy, we propose a two-step few-shot refinement
strategy that adaptively selects informative prompts based on the LLM's
observed error patterns. Extensive experiments on real-world log datasets
demonstrate that ALPHA achieves detection accuracy comparable to fully
supervised methods while mitigating human efforts in the loop. ALPHA also
supports interpretable analysis through LLM-driven root cause explanations in
the post-detection stage. These capabilities make ALPHA a scalable and
cost-efficient solution for truly automated log-based anomaly detection.

</details>


### [6] [An Axiomatic Analysis of Path Selection Strategies for Multipath Transport in Path-Aware Networks](https://arxiv.org/abs/2509.05938)
*Alissa Baumeister,Sina Keshvadi*

Main category: cs.NI

TL;DR: 本文通过公理化分析量化了路径感知网络中个体性能优化与网络稳定性之间的权衡，发现混合策略（如Epsilon-Greedy）在高竞争场景下既能实现最高效率，又能缓解贪婪方法的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 路径感知网络架构（如SCION）和多路径传输协议（如MPTCP、MPQUIC）的结合虽然能带来性能提升，但也产生了个体性能优化与整体网络稳定性之间的显著权衡，需要量化分析这种权衡关系。

Method: 采用公理化分析方法，在模拟的路径感知环境中评估了从贪婪（Min-RTT）、协作（Round-Robin）到混合方法（Epsilon-Greedy）的一系列算法，针对效率、丢包避免、稳定性和公平性等公理进行评估。

Result: 纯贪婪策略在低竞争时高效，但随着竞争代理数量增加，由于群体效应导致严重网络不稳定，丢包率增加超过18,000%；协作策略确保公平稳定但无法充分利用高容量路径；混合策略（如Epsilon-Greedy）在高竞争场景中实现了最高效率同时缓解了不稳定性。

Conclusion: 可调节的混合算法对于设计下一代网络的鲁棒高性能路径选择机制至关重要，能够有效解决个体优化与网络稳定性的冲突。

Abstract: Path-aware networking architectures like SCION provide end-hosts with
explicit control over inter-domain routing, while multipath transport protocols
like MPTCP and MPQUIC enable the concurrent use of multiple paths. This
combination promises significant gains in performance and policy enforcement,
but it also creates a stark trade-off between individual performance
optimization and overall network stability. This paper quantifies this
trade-off through a rigorous axiomatic analysis. We evaluate a spectrum of
algorithms, from greedy (Min-RTT) and cooperative (Round-Robin) to hybrid
approaches (Epsilon-Greedy), against axioms of Efficiency, Loss Avoidance,
Stability, and Fairness in a simulated path-aware environment.
  Our simulations reveal that purely greedy strategies, while efficient under
low contention, induce catastrophic packet loss, increasing by over >18,000% as
the number of competing agents grow, due to herd effects that cause severe
network instability. Conversely, cooperative strategies ensure fairness and
stability but at the cost of underutilizing high-capacity paths. Crucially, we
demonstrate that hybrid strategies resolve this conflict. The Epsilon-Greedy
algorithm, for instance, achieves the highest efficiency of all tested
strategies in high-contention scenarios while mitigating the instability
inherent to the greedy approach. Our axiomatic analysis suggests that tunable,
hybrid algorithms are essential for designing robust and high-performance path
selection mechanisms for next-generation networks.

</details>


### [7] [Large Language Models for Next-Generation Wireless Network Management: A Survey and Tutorial](https://arxiv.org/abs/2509.05946)
*Bisheng Wei,Ruihong Jiang,Ruichen Zhang,Yinqiu Liu,Dusit Niyato,Yaohua Sun,Yang Lu,Yonghui Li,Shiwen Mao,Chau Yuen,Marco Di Renzo,Mugen Peng*

Main category: cs.NI

TL;DR: 这篇论文系统性评估了大语言模型在6G网络优化中的应用，提出了基于自然语言驱动的优化框架，解决传统方法在实时性、扩展性和动态处理方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 6G网络优化问题的复杂性和规模迅速增长，传统优化方法无法满足实时适应性、扩展性和动态处理用户意图的需求，需要找到更有效的解决方案。

Method: 采用大语言模型实现自然语言驱动的问题形式化、上下文感知推理和适应性解决方案精炼，包括自然语言建模、求解器协作和解决方案验证等关键技术。

Result: 通过代表性案例研究展示了LLMs在优化问题形式化、低空经济网络和意图网络等实际场景中的转型潜力，为下一代无线网络提供了更健壮、可扩展和可信赖的优化解决方案。

Conclusion: 论文为6G网络优化问题提供了一个系统性的LLM优化框架评估，识别了当前的研究挑战和未来方向，有助于推动更健壮、可扩展和可信赖的LLM优化解决方案的发展。

Abstract: The rapid advancement toward sixth-generation (6G) wireless networks has
significantly intensified the complexity and scale of optimization problems,
including resource allocation and trajectory design, often formulated as
combinatorial problems in large discrete decision spaces. However, traditional
optimization methods, such as heuristics and deep reinforcement learning (DRL),
struggle to meet the demanding requirements of real-time adaptability,
scalability, and dynamic handling of user intents in increasingly heterogeneous
and resource-constrained network environments. Large language models (LLMs)
present a transformative paradigm by enabling natural language-driven problem
formulation, context-aware reasoning, and adaptive solution refinement through
advanced semantic understanding and structured reasoning capabilities. This
paper provides a systematic and comprehensive survey of LLM-enabled
optimization frameworks tailored for wireless networks. We first introduce
foundational design concepts and distinguish LLM-enabled methods from
conventional optimization paradigms. Subsequently, we critically analyze key
enabling methodologies, including natural language modeling, solver
collaboration, and solution verification processes. Moreover, we explore
representative case studies to demonstrate LLMs' transformative potential in
practical scenarios such as optimization formulation, low-altitude economy
networking, and intent networking. Finally, we discuss current research
challenges, examine prominent open-source frameworks and datasets, and identify
promising future directions to facilitate robust, scalable, and trustworthy
LLM-enabled optimization solutions for next-generation wireless networks.

</details>


### [8] [Optimized Split Computing Framework for Edge and Core Devices](https://arxiv.org/abs/2509.06049)
*Andrea Tassi,Oluwatayo Yetunde Kolawole,Joan Pujol Roig,Daniel Warren*

Main category: cs.NI

TL;DR: 这篇论文提出了一种用于分开计算应用的优化框架，通过将前向神经网络分割在用户设备、边缘节点和核心节点上执行，以减少用户设备的计算资源占用并控制推理时间。


<details>
  <summary>Details</summary>
Motivation: 电动机网络需要支持具有严格要求的服务，确保高质量用户体验。电动机网络预计将支持严格要求的服务，以确保高质量的用户体验。由于用户设备资源有限，直接在用户设备上运行前向神经网络模型是一个本质上具有挑战性的问题。

Method: 提出一种优化框架，将前向神经网络模型分割成多个部分，分别在用户设备、边缘节点和核心节点上执行。还提供了解决优化问题的高效吧济策略。

Result: 证明该框架在异构环境中具有稳健性，消除了重新训练的需求，并将用户设备的内存占用减少33.6%，CPU占用减少60%。

Conclusion: 该方法通过分开计算有效减少了用户设备的计算负担，同时保持了推理性能，为边缘计算领域提供了一种可行的解决方案。

Abstract: With mobile networks expected to support services with stringent requirements
that ensure high-quality user experience, the ability to apply Feed-Forward
Neural Network (FFNN) models to User Equipment (UE) use cases has become
critical. Given that UEs have limited resources, running FFNNs directly on UEs
is an intrinsically challenging problem. This letter proposes an optimization
framework for split computing applications where an FFNN model is partitioned
into multiple sections, and executed by UEs, edge- and core-located nodes to
reduce the required UE computational footprint while containing the inference
time. An efficient heuristic strategy for solving the optimization problem is
also provided. The proposed framework is shown to be robust in heterogeneous
settings, eliminating the need for retraining and reducing the UE's memory
(CPU) footprint by over 33.6% (60%).

</details>


### [9] [Understanding BBRv3 Performance in AQM-Enabled WiFi Networks](https://arxiv.org/abs/2509.06245)
*Shyam Kumar Shrestha,Jonathan Kua,Shiva Raj Pokhrel*

Main category: cs.NI

TL;DR: 通过模块化实验测试床和可视化工具比较BBRv3和CUBIC在Wi-Fi网络中的性能，发现BBRv3在AQM环境下显著提升了公平性和收敛性


<details>
  <summary>Details</summary>
Motivation: 评估TCP拥塞控制算法在无线网络中的性能，尤其是在不同主动队列管理(AQM)方案下的表现

Method: 使用模块化实验测试床和轻量级可视化工具，在MikroTik路由器的Wi-Fi链路上比较BBRv3和CUBIC算法，测试不同AQM方案(PFIFO、FQ-CoDel、CAKE)

Result: BBRv3在AQM环境下显著提高了公平性和收敛性，尤其是与FQ-CoDel结合时效果最佳

Conclusion: 该可视化工具和模块化测试床为在实际家庭无线网络中评估下一代TCP变体提供了实用基础

Abstract: We present a modular experimental testbed and lightweight visualization tool
for evaluating TCP congestion control performance in wireless networks. We
compare Google's latest Bottleneck Bandwidth and Round-trip time version 3
(BBRv3) algorithm with loss-based CUBIC under varying Active Queue Management
(AQM) schemes, namely PFIFO, FQ-CoDel, and CAKE, on a Wi-Fi link using a
commercial MikroTik router. Our real-time dashboard visualizes metrics such as
throughput, latency, and fairness across competing flows. Results show that
BBRv3 significantly improves fairness and convergence under AQM, especially
with FQ-CoDel. Our visualization tool and modular testbed provide a practical
foundation for evaluating next-generation TCP variants in real-world
AQM-enabled home wireless networks.

</details>


### [10] [Network-Aware Control of AGVs in an Industrial Scenario: A Simulation Study Based on ROS 2 and Gazebo](https://arxiv.org/abs/2509.06451)
*Filippo Bragato,Tullia Fontana,Marco Giordani,Malte Schellmann,Josef Eichinger,Michele Zorzi*

Main category: cs.NI

TL;DR: 基于Gazebo和ROS 2的综合通信与控制模拟框架，用于分析网络性能对AGV车轨迹控制精度的影响


<details>
  <summary>Details</summary>
Motivation: 网络化控制系统中，通信与控制密切相关，网络延迟和错误可导致AGV车轨迹偏移，需要研究两者的关联性

Method: 开发了一个基于Gazebo模拟器和ROS 2的可视化模拟框架，明确统计通信指标（延迟、包丢失）和控制指标（均方误差MSE）

Result: 获得了网络性能（特别是包接收比PRR）与控制精度之间的相关性结果

Conclusion: 证明了通信与控制在NCS中的密切关联，提供的模拟框架有助于分析网络问题对AGV车轨迹控制的影响

Abstract: Networked Control System (NCS) is a paradigm where sensors, controllers, and
actuators communicate over a shared network. One promising application of NCS
is the control of Automated Guided Vehicles (AGVs) in the industrial
environment, for example to transport goods efficiently and to autonomously
follow predefined paths or routes. In this context, communication and control
are tightly correlated, a paradigm referred to as Joint Communication and
Control (JCC), since network issues such as delays or errors can lead to
significant deviations of the AGVs from the planned trajectory. In this paper,
we present a simulation framework based on Gazebo and Robot Operating System 2
(ROS 2) to simulate and visualize, respectively, the complex interaction
between the control of AGVs and the underlying communication network. This
framework explicitly incorporates communication metrics, such as delay and
packet loss, and control metrics, especially the Mean Squared Error (MSE)
between the optimal/desired and actual path of the AGV in response to driving
commands. Our results shed light into the correlation between the network
performance, particularly Packet Reception Ratio (PRR), and accuracy of
control.

</details>


### [11] [Empirical Evaluation of a 5G Transparent Clock for Time Synchronization in a TSN-5G Network](https://arxiv.org/abs/2509.06454)
*Julia Caleya-Sanchez,Pablo Muñoz,Jorge Sánchez-Garrido,Emilio Florentín,Felix Delgado-Ferro,Pablo Rodriguez-Martin,Pablo Ameigeiras*

Main category: cs.NI

TL;DR: 这篇论文通过实验评估了在TSN-5G网络中实现透明时钟(TC)的性能，达到了500纳秒的同步精度，满足工业应用<1微秒的要求。


<details>
  <summary>Details</summary>
Motivation: 工业IoT和产业4.0/5.0应用需要高精度时间同步，但TSN-5G网络中的振荡和非对称延迟带来挑战。虽然3GPP定义了透明时钟模式，但缺少实践验证。

Method: 在商用TSN交换机上实现单时钟5G端到端TC，计算5G中的居留时间并在从节点恢复时钟域，通过修改PTP消息传输速率进行同步评估。

Result: 实验结果显示峰到峰同步精度为500纳秒，远超工业要求(<1微秒)，对特定PTP消息传输速率只有最小的同步偏差。

Conclusion: 透明时钟模式在TSN-5G网络中可以实现高精度时间同步，为工业IoT提供了可靠的时钟同步方案。

Abstract: Time synchronization is essential for industrial IoT and Industry 4.0/5.0
applications, but achieving high synchronization accuracy in Time-Sensitive
Networking (TSN)-5G networks is challenging due to jitter and asymmetric
delays. 3GPP TS 23.501 defines three 5G synchronization modes: time-aware
system, boundary clock (BC), and transparent clock (TC), where TC offers a
promising solution. However, to the best of our knowledge, there is no
empirical evaluation of TC in a TSN-5G network. This paper empirically
evaluates an 5G end-to-end TC in a TSN-5G network, implemented on commercial
TSN switches with a single clock. For TC development, we compute the residence
time in 5G and recover the clock domain at the slave node. We deploy a TSN-5G
testbed with commercial equipment for synchronization evaluation by modifying
the Precision Timing Protocol (PTP) message transmission rates. Experimental
results show a peak-to-peak synchronization of 500 ns, meeting the industrial
requirement of < 1 us, with minimal synchronization offsets for specific PTP
message transmission rates.

</details>


### [12] [Five Blind Men and the Internet: Towards an Understanding of Internet Traffic](https://arxiv.org/abs/2509.06515)
*Ege Cem Kirci,Ayush Mishra,Laurent Vanbever*

Main category: cs.NI

TL;DR: 通过全球472个互联网交换中心的公开流量数据，这项研究揭示了全球互联网流量在2023-2024年增长49.2%，发现了区域性日常模式和稳定的利用率，为长期监测提供了可靠基础。


<details>
  <summary>Details</summary>
Motivation: 互联网作为世界最大的网络缺乏透明、细粒度的流量观测视角，阻碍了网络社区对其动态的理解。

Method: 利用公开可用的互联网交换中心流量统计数据，对全球472个IXP进行了两年的综合研究，覆盖全球IXP端口容量的87%。

Result: 发现全球流量增长49.2%（年化24.5%），尿洞时间流量达300Tbps，揭示了区域性日常模式和事件驱动异常，证明IXP流量作为整体互联网增长的可靠代理指标。

Conclusion: 该研究为长期互联网流量监测提供了可验证的基础，揭示了网络设计与功能的相互作用，为研究人员和运营商提供了探索互联网演化生态系统的可访问框架。

Abstract: The Internet, the world's largest and most pervasive network, lacks a
transparent, granular view of its traffic patterns, volumes, and growth trends,
hindering the networking community's understanding of its dynamics. This paper
leverages publicly available Internet Exchange Point traffic statistics to
address this gap, presenting a comprehensive two-year study (2023-2024) from
472 IXPs worldwide, capturing approximately 300 Tbps of peak daily aggregate
traffic by late 2024. Our analysis reveals a 49.2% global traffic increase
(24.5% annualized), uncovers regionally distinct diurnal patterns and
event-driven anomalies, and demonstrates stable utilization rates, reflecting
predictable infrastructure scaling. By analyzing biases and confirming high
self-similarity, we establish IXP traffic as a robust proxy for overall
Internet growth and usage behavior. With transparent, replicable data--covering
87% of the worldwide IXP port capacity--and plans to release our dataset, this
study offers a verifiable foundation for long-term Internet traffic monitoring.
In particular, our findings shed light on the interplay between network design
and function, providing an accessible framework for researchers and operators
to explore the Internet's evolving ecosystem.

</details>


### [13] [Ghost Points Matter: Far-Range Vehicle Detection with a Single mmWave Radar in Tunnel](https://arxiv.org/abs/2509.06639)
*Chenming He,Rui Xia,Chengzhen Meng,Xiaoran Fan,Dequan Wang,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.NI

TL;DR: mmTunnel是一个毫米波雷达系统，通过多路径射线追踪算法校正隧道中的幽灵点，实现远距离车辆检测，在真实隧道环境中达到93.7%的平均F1分数。


<details>
  <summary>Details</summary>
Motivation: 隧道车辆检测对交通监控和事故响应至关重要，但现有研究不足。主要挑战是处理多路径反射产生的幽灵点，这些点会导致严重的定位误差和误报。

Method: 提出多路径射线追踪算法，利用地面平面约束和信号路径损耗识别最可能的反射路径；引入曲线到平面分割方法简化隧道表面建模，降低计算延迟实现实时处理。

Result: 在两个测试隧道中，系统平均F1分数达到93.7%，实时处理；在遮挡场景下F1分数仍高于91%；在真实交通条件下达到91.5%的F1分数。

Conclusion: mmTunnel系统通过校正而非简单移除幽灵点，有效解决了隧道中多路径反射问题，实现了高精度、实时的车辆检测，在复杂隧道环境中表现出色。

Abstract: Vehicle detection in tunnels is crucial for traffic monitoring and accident
response, yet remains underexplored. In this paper, we develop mmTunnel, a
millimeter-wave radar system that achieves far-range vehicle detection in
tunnels. The main challenge here is coping with ghost points caused by
multi-path reflections, which lead to severe localization errors and false
alarms. Instead of merely removing ghost points, we propose correcting them to
true vehicle positions by recovering their signal reflection paths, thus
reserving more data points and improving detection performance, even in
occlusion scenarios. However, recovering complex 3D reflection paths from
limited 2D radar points is highly challenging. To address this problem, we
develop a multi-path ray tracing algorithm that leverages the ground plane
constraint and identifies the most probable reflection path based on signal
path loss and spatial distance. We also introduce a curve-to-plane segmentation
method to simplify tunnel surface modeling such that we can significantly
reduce the computational delay and achieve real-time processing.
  We have evaluated mmTunnel with comprehensive experiments. In two test
tunnels, we conducted controlled experiments in various scenarios with cars and
trucks. Our system achieves an average F1 score of 93.7% for vehicle detection
while maintaining real-time processing. Even in the challenging occlusion
scenarios, the F1 score remains above 91%. Moreover, we collected extensive
data from a public tunnel with heavy traffic at times and show our method could
achieve an F1 score of 91.5% in real-world traffic conditions.

</details>


### [14] [Sovereign AI for 6G: Towards the Future of AI-Native Networks](https://arxiv.org/abs/2509.06700)
*Swarna Bindu Chetty,David Grace,Simon Saunders,Paul Harris,Eirini Eleni Tsiropoulou,Tony Quek,Hamed Ahmadi*

Main category: cs.NI

TL;DR: 本文提出"主权AI"作为6G网络的战略要务，通过架构、运营和治理框架实现国家或运营商对AI开发、部署和生命周期的控制，特别是在O-RAN架构中部署主权AI应用以确保安全合规。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI、LLMs和LTM技术的发展，电信行业从5G云中心向AI原生6G架构转型，虽然带来了实时自动化等新能力，但也引入了数据主权、安全性和监管合规等关键风险，特别是当AI模型在外部训练、部署或治理时。

Method: 提出主权AI概念，设计架构、运营和治理框架，重点研究如何在O-RAN架构中部署基于主权AI的xApps和rApps到近实时和非实时RICs中，实现策略对齐控制、安全模型更新和可信基础设施上的联邦学习。

Result: 分析了全球战略、技术使能因素以及在安全、人才和模型治理方面的挑战，确认主权AI不仅是监管需求，更是安全、弹性和道德对齐的6G网络的基础支柱。

Conclusion: 主权AI是6G网络不可或缺的战略要素，能够确保国家安全、监管合规和道德对齐，为未来移动网络提供安全可靠的基础架构。

Abstract: The advent of Generative Artificial Intelligence (GenAI), Large Language
Models (LLMs), and Large Telecom Models (LTM) significantly reshapes mobile
networks, especially as the telecom industry transitions from 5G's
cloud-centric to AI-native 6G architectures. This transition unlocks
unprecedented capabilities in real-time automation, semantic networking, and
autonomous service orchestration. However, it introduces critical risks related
to data sovereignty, security, explainability, and regulatory compliance
especially when AI models are trained, deployed, or governed externally. This
paper introduces the concept of `Sovereign AI' as a strategic imperative for
6G, proposing architectural, operational, and governance frameworks that enable
national or operator-level control over AI development, deployment, and
life-cycle management. Focusing on O-RAN architecture, we explore how sovereign
AI-based xApps and rApps can be deployed Near-RT and Non-RT RICs to ensure
policy-aligned control, secure model updates, and federated learning across
trusted infrastructure. We analyse global strategies, technical enablers, and
challenges across safety, talent, and model governance. Our findings underscore
that Sovereign AI is not just a regulatory necessity but a foundational pillar
for secure, resilient, and ethically-aligned 6G networks.

</details>


### [15] [VariSAC: V2X Assured Connectivity in RIS-Aided ISAC via GNN-Augmented Reinforcement Learning](https://arxiv.org/abs/2509.06763)
*Huijun Tang,Wang Zeng,Ming Du,Pinlong Zhao,Pengfei Jiao,Huaming Wu,Hongjian Sun*

Main category: cs.NI

TL;DR: VariSAC是一个基于图神经网络和深度强化学习的框架，用于在RIS辅助的ISAC车联网系统中实现连续可靠连接，通过统一度量CCR来优化信道分配、功率控制和RIS配置。


<details>
  <summary>Details</summary>
Motivation: 解决车联网中V2I和V2V连接需求共存、网络拓扑高度动态异构带来的统一可靠性建模和资源优化挑战。

Method: 提出CCR统一度量指标，使用带残差适配器的GNN编码系统状态，结合SAC强化学习算法联合优化信道分配、功率控制和RIS配置。

Result: 在真实城市数据集上的实验表明，VariSAC在连续V2I ISAC连接性和V2V传输可靠性方面均优于现有基线方法。

Conclusion: 该框架能够在高度动态的车载环境中实现持久连接，为RIS辅助的ISAC车联网系统提供了有效的可靠性保障解决方案。

Abstract: The integration of Reconfigurable Intelligent Surfaces (RIS) and Integrated
Sensing and Communication (ISAC) in vehicular networks enables dynamic spatial
resource management and real-time adaptation to environmental changes. However,
the coexistence of distinct vehicle-to-infrastructure (V2I) and
vehicle-to-vehicle (V2V) connectivity requirements, together with highly
dynamic and heterogeneous network topologies, presents significant challenges
for unified reliability modeling and resource optimization. To address these
issues, we propose VariSAC, a graph neural network (GNN)-augmented deep
reinforcement learning framework for assured, time-continuous connectivity in
RIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically,
we introduce the Continuous Connectivity Ratio (CCR), a unified metric that
characterizes the sustained temporal reliability of V2I connections and the
probabilistic delivery guarantees of V2V links, thus unifying their continuous
reliability semantics. Next, we employ a GNN with residual adapters to encode
complex, high-dimensional system states, capturing spatial dependencies among
vehicles, base stations (BS), and RIS nodes. These representations are then
processed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channel
allocation, power control, and RIS configurations to maximize CCR-driven
long-term rewards. Extensive experiments on real-world urban datasets
demonstrate that VariSAC consistently outperforms existing baselines in terms
of continuous V2I ISAC connectivity and V2V delivery reliability, enabling
persistent connectivity in highly dynamic vehicular environments.

</details>


### [16] [Resilience of Mega-Satellite Constellations: How Node Failures Impact Inter-Satellite Networking Over Time?](https://arxiv.org/abs/2509.06766)
*Binquan Guo,Zehui Xiong,Zhou Zhang,Baosheng Li,Dusit Niyato,Chau Yuen,Zhu Han*

Main category: cs.NI

TL;DR: 该研究分析了巨型卫星星座中节点故障对星际网络的影响，提出了服务感知的时间中介中心性度量方法，发现星链星座对节点故障具有内在韧性，动态拓扑和重路由机制对网络恢复至关重要。


<details>
  <summary>Details</summary>
Motivation: 巨型卫星星座利用星际链路提供全球低延迟通信服务，但恶劣的太空环境使卫星容易发生故障，导致节点移除并破坏星际网络连接。了解卫星节点故障对端到端服务的影响至关重要。

Method: 将巨型卫星星座表示为离散时间图，提出服务感知的时间中介中心性度量来量化节点重要性，开发分析框架识别关键节点并评估节点故障影响，在星链星座设置上进行模拟。

Result: 模拟显示卫星网络对节点故障具有内在韧性，动态拓扑能部分恢复连接并减轻长期影响。重路由机制的整合对于释放完全韧性潜力、确保星际网络快速恢复至关重要。

Conclusion: 卫星星座的动态拓扑特性使其对节点故障具有天然韧性，但需要结合重路由机制才能充分发挥这种韧性，确保星际网络的快速恢复和持续服务。

Abstract: Mega-satellite constellations have the potential to leverage inter-satellite
links to deliver low-latency end-to-end communication services globally,
thereby extending connectivity to underserved regions. However, harsh space
environments make satellites vulnerable to failures, leading to node removals
that disrupt inter-satellite networking. With the high risk of satellite node
failures, understanding their impact on end-to-end services is essential. This
study investigates the importance of individual nodes on inter-satellite
networking and the resilience of mega satellite constellations against node
failures. We represent the mega-satellite constellation as discrete temporal
graphs and model node failure events accordingly. To quantify node importance
for targeted services over time, we propose a service-aware temporal
betweenness metric. Leveraging this metric, we develop an analytical framework
to identify critical nodes and assess the impact of node failures. The
framework takes node failure events as input and efficiently evaluates their
impacts across current and subsequent time windows. Simulations on the Starlink
constellation setting reveal that satellite networks inherently exhibit
resilience to node failures, as their dynamic topology partially restore
connectivity and mitigate the long-term impact. Furthermore, we find that the
integration of rerouting mechanisms is crucial for unleashing the full
resilience potential to ensure rapid recovery of inter-satellite networking.

</details>


### [17] [BatStation: Toward In-Situ Radar Sensing on 5G Base Stations with Zero-Shot Template Generation](https://arxiv.org/abs/2509.06898)
*Zhihui Gao,Zhecun Liu,Tingjun Chen*

Main category: cs.NI

TL;DR: BatStation是一个轻量级雷达感知框架，集成到5G基站中，利用上行资源网格提取雷达信号，实现雷达检测、分类和定位，满足5G网络实时性要求。


<details>
  <summary>Details</summary>
Motivation: 解决现有雷达信号与商用5G信号共存问题，利用密集部署的5G基站进行雷达感知，实现高效自适应的频谱共享。

Method: 通过三个关键组件：雷达信号分离消除5G传输干扰、资源网格重塑对齐时频分辨率、基于合成数据训练的零样本模板相关实现检测分类定位。

Result: 在真实5G流量测试中，检测概率达97.02%(PUCCH)和79.23%(PUSCH)，分类准确率最高97%，定位误差频率2.68-6.20MHz、时间24.6-32.4微秒，运行时延仅0.11/0.94ms。

Conclusion: BatStation成功实现了轻量级、实时的雷达感知解决方案，为5G基站雷达感知提供了可行方案，满足实际部署需求。

Abstract: The coexistence between incumbent radar signals and commercial 5G signals
necessitates a versatile and ubiquitous radar sensing for efficient and
adaptive spectrum sharing. In this context, leveraging the densely deployed 5G
base stations (BS) for radar sensing is particularly promising, offering both
wide coverage and immediate feedback to 5G scheduling. However, the targeting
radar signals are superimposed with concurrent 5G uplink transmissions received
by the BS, and practical deployment also demands a lightweight, portable radar
sensing model. This paper presents BatStation, a lightweight, in-situ radar
sensing framework seamlessly integrated into 5G BSs. BatStation leverages
uplink resource grids to extract radar signals through three key components:
(i) radar signal separation to cancel concurrent 5G transmissions and reveal
the radar signals, (ii) resource grid reshaping to align time-frequency
resolution with radar pulse characteristics, and (iii) zero-shot template
correlation based on a portable model trained purely on synthetic data that
supports detection, classification, and localization of radar pulses without
fine-tuning using experimental data. We implement BatStation on a
software-defined radio (SDR) testbed and evaluate its performance with real 5G
traffic in the CBRS band. Results show robust performance across diverse radar
types, achieving detection probabilities of 97.02% (PUCCH) and 79.23% (PUSCH),
classification accuracy up to 97.00%, and median localization errors of
2.68-6.20 MHz (frequency) and 24.6-32.4 microseconds (time). Notably,
BatStation achieves this performance with a runtime latency of only 0.11/0.94
ms on GPU/CPU, meeting the real-time requirement of 5G networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts](https://arxiv.org/abs/2509.05323)
*Adam Cole,Mick Grierson*

Main category: cs.AI

TL;DR: 通过提取和可视化视频渣果变换器的交叉注意力地图，探索注意力机制在文本到视频生成中的时空间行为，并将其作为艺术创作的原材料。


<details>
  <summary>Details</summary>
Motivation: 受早期视频艺术家操控模拟视频信号的启发，探索AI模型内部工作机制的可解释性，并将其重新定义为创意媒介。

Method: 基于开源Wan模型建立工具，提取和可视化交叉注意力地图，通过探索性探针和艺术案例研究进行分析。

Result: 开发了能够揭示视频生成模型时空间注意力行为的可解释性工具，证明了注意力地图既可作为分析工具也可作为艺术创作原材料的潜力。

Conclusion: 本研究为艺术可解释AI（XAIxArts）领域做出贡献，鼓励艺术家将AI内部工作机制重新认定为创意媒介，促进技术与艺术的融合。

Abstract: This paper presents an artistic and technical investigation into the
attention mechanisms of video diffusion transformers. Inspired by early video
artists who manipulated analog video signals to create new visual aesthetics,
this study proposes a method for extracting and visualizing cross-attention
maps in generative video models. Built on the open-source Wan model, our tool
provides an interpretable window into the temporal and spatial behavior of
attention in text-to-video generation. Through exploratory probes and an
artistic case study, we examine the potential of attention maps as both
analytical tools and raw artistic material. This work contributes to the
growing field of Explainable AI for the Arts (XAIxArts), inviting artists to
reclaim the inner workings of AI as a creative medium.

</details>


### [19] [Perception Graph for Cognitive Attack Reasoning in Augmented Reality](https://arxiv.org/abs/2509.05324)
*Rongqian Chen,Shu Hong,Rifatul Islam,Mahdi Imani,G. Gary Tan,Tian Lan*

Main category: cs.AI

TL;DR: 提出Perception Graph模型来检测和分析AR系统中的认知攻击，通过量化感知扭曲程度来保护用户决策能力


<details>
  <summary>Details</summary>
Motivation: AR系统在战术环境中部署增加，但其依赖无缝人机交互的特性使其容易受到认知攻击，这些攻击会操纵用户感知并严重损害用户决策能力

Method: 引入Perception Graph模型，首先模拟人类从MR环境中解释关键信息的过程，然后用语义有意义的结构表示结果，计算反映感知扭曲程度的量化分数

Result: 模型能够提供稳健且可测量的方法来检测和分析认知攻击的影响

Conclusion: Perception Graph为AR系统提供了一种有效检测和防御认知攻击的新方法，通过量化感知扭曲来保护用户决策完整性

Abstract: Augmented reality (AR) systems are increasingly deployed in tactical
environments, but their reliance on seamless human-computer interaction makes
them vulnerable to cognitive attacks that manipulate a user's perception and
severely compromise user decision-making. To address this challenge, we
introduce the Perception Graph, a novel model designed to reason about human
perception within these systems. Our model operates by first mimicking the
human process of interpreting key information from an MR environment and then
representing the outcomes using a semantically meaningful structure. We
demonstrate how the model can compute a quantitative score that reflects the
level of perception distortion, providing a robust and measurable method for
detecting and analyzing the effects of such cognitive attacks.

</details>


### [20] [SynDelay: A Synthetic Dataset for Delivery Delay Prediction](https://arxiv.org/abs/2509.05325)
*Liming Xu,Yunbo Long,Alexandra Brintrup*

Main category: cs.AI

TL;DR: 这篇论文提出了SynDelay数据集，一个用于配送延误预测的合成数据集，以解决供应链管理中高质量公开数据缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 人工智能在供应链管理中的预测任务（如配送延误预测）受到高质量公开数据集稀缺的限制，现有数据集存在专利性、规模小或维护不一致等问题，影响了可复现性和基准测试。

Method: 使用基于真实数据训练的先进生成模型创建SynDelay合成数据集，保持实际配送模式的同时确保隐私性。

Result: 虽然数据集不能完全避免噪声或不一致性，但提供了具有挑战性和实用性的测试平台，并提供了基准结果和评估指标。

Conclusion: SynDelay通过Supply Chain Data Hub公开可用，为供应链AI预测模型提供了重要的数据支持，并鼓励社区贡献数据集、模型和评估方法来推进该领域的研究。

Abstract: Artificial intelligence (AI) is transforming supply chain management, yet
progress in predictive tasks -- such as delivery delay prediction -- remains
constrained by the scarcity of high-quality, openly available datasets.
Existing datasets are often proprietary, small, or inconsistently maintained,
hindering reproducibility and benchmarking. We present SynDelay, a synthetic
dataset designed for delivery delay prediction. Generated using an advanced
generative model trained on real-world data, SynDelay preserves realistic
delivery patterns while ensuring privacy. Although not entirely free of noise
or inconsistencies, it provides a challenging and practical testbed for
advancing predictive modelling. To support adoption, we provide baseline
results and evaluation metrics as initial benchmarks, serving as reference
points rather than state-of-the-art claims. SynDelay is publicly available
through the Supply Chain Data Hub, an open initiative promoting dataset sharing
and benchmarking in supply chain AI. We encourage the community to contribute
datasets, models, and evaluation practices to advance research in this area.
All code is openly accessible at https://supplychaindatahub.org.

</details>


### [21] [MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset](https://arxiv.org/abs/2509.05330)
*Seyed Muhammad Hossein Mousavi,Atiye Ilanloo*

Main category: cs.AI

TL;DR: MVRS数据集是一个多模态情感识别数据集，包含13名参与者的同步眼动追踪、身体运动、肌电和皮电信号数据，通过VR情感刺激收集，验证了多模态融合在情感识别中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前情感识别领域缺乏包含身体运动和生理信号的多模态数据集，限制了该领域的发展。

Method: 使用VR情感刺激（放松、恐惧、压力、悲伤、喜悦），同步采集眼动追踪、身体运动、肌电和皮电信号，采用早期和晚期融合技术提取特征并进行分类评估。

Result: 数据集质量良好，情感类别可分性得到验证，为多模态情感计算提供了有价值的数据资源。

Conclusion: MVRS数据集填补了多模态情感识别数据集的空白，证明了多模态融合在情感识别中的潜力，对医疗、教育和汽车系统等领域有重要应用价值。

Abstract: Automatic emotion recognition has become increasingly important with the rise
of AI, especially in fields like healthcare, education, and automotive systems.
However, there is a lack of multimodal datasets, particularly involving body
motion and physiological signals, which limits progress in the field. To
address this, the MVRS dataset is introduced, featuring synchronized recordings
from 13 participants aged 12 to 60 exposed to VR based emotional stimuli
(relaxation, fear, stress, sadness, joy). Data were collected using eye
tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR
signals (Arduino UNO), all timestamp aligned. Participants followed a unified
protocol with consent and questionnaires. Features from each modality were
extracted, fused using early and late fusion techniques, and evaluated with
classifiers to confirm the datasets quality and emotion separability, making
MVRS a valuable contribution to multimodal affective computing.

</details>


### [22] [Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning](https://arxiv.org/abs/2509.05346)
*Bo Yuan,Jiazi Hu*

Main category: cs.AI

TL;DR: 本研究对GPT-4o、DeepSeek-V3和GLM-4.5三种先进大语言模型在个性化学习辅导任务中的表现进行了实证比较，发现GPT-4o在提供信息丰富、结构良好的反馈方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型越来越多地被设想为个性化学习的智能助手，但在真实学习场景中的系统化对比评估仍然有限，需要实证研究来验证其实际效果。

Method: 使用包含学生答案和正确性标签的数据集，要求每个LLM分析测验、推断学生掌握情况并生成针对性指导。采用Gemini作为虚拟评委进行多维度两两比较，并使用Bradley-Terry模型分析结果。

Result: GPT-4o总体上更受青睐，产生的反馈信息更丰富、结构更好；DeepSeek-V3和GLM-4.5展现出间歇性优势但一致性较低。

Conclusion: 研究证明了将LLMs部署为高级教学助手进行个性化支持的可行性，并为未来LLM驱动的个性化学习实证研究提供了方法学指导。

Abstract: While Large Language Models (LLMs) are increasingly envisioned as intelligent
assistants for personalized learning, systematic head-to-head evaluations
within authentic learning scenarios remain limited. This study conducts an
empirical comparison of three state-of-the-art LLMs on a tutoring task that
simulates a realistic learning setting. Using a dataset comprising a student's
answers to ten questions of mixed formats with correctness labels, each LLM is
required to (i) analyze the quiz to identify underlying knowledge components,
(ii) infer the student's mastery profile, and (iii) generate targeted guidance
for improvement. To mitigate subjectivity and evaluator bias, we employ Gemini
as a virtual judge to perform pairwise comparisons along various dimensions:
accuracy, clarity, actionability, and appropriateness. Results analyzed via the
Bradley-Terry model indicate that GPT-4o is generally preferred, producing
feedback that is more informative and better structured than its counterparts,
while DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower
consistency. These findings highlight the feasibility of deploying LLMs as
advanced teaching assistants for individualized support and provide
methodological guidance for future empirical research on LLM-driven
personalized learning.

</details>


### [23] [SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis](https://arxiv.org/abs/2509.05363)
*Lijie Ding,Changwoo Do*

Main category: cs.AI

TL;DR: SasAgent是一个基于大语言模型的多代理AI系统，用于自动化小角散射数据分析，通过SasView工具和文本交互实现高效科学工作流


<details>
  <summary>Details</summary>
Motivation: 为了解决小角散射数据分析的复杂性，提高自动化水平，利用大语言模型来简化科学工作流程

Method: 采用多代理架构，包括协调代理和三个专业代理（SLD计算、合成数据生成、实验数据拟合），使用LLM友好的工具集和Gradio用户界面

Result: 系统能够解释复杂提示、精确计算散射长度密度、生成准确散射数据，并以高精度拟合实验数据集

Conclusion: 这项工作展示了LLM驱动的AI系统在简化科学工作流程和增强SAS研究自动化方面的巨大潜力

Abstract: We introduce SasAgent, a multi-agent AI system powered by large language
models (LLMs) that automates small-angle scattering (SAS) data analysis by
leveraging tools from the SasView software and enables user interaction via
text input. SasAgent features a coordinator agent that interprets user prompts
and delegates tasks to three specialized agents for scattering length density
(SLD) calculation, synthetic data generation, and experimental data fitting.
These agents utilize LLM-friendly tools to execute tasks efficiently. These
tools, including the model data tool, Retrieval-Augmented Generation (RAG)
documentation tool, bump fitting tool, and SLD calculator tool, are derived
from the SasView Python library. A user-friendly Gradio-based interface
enhances user accessibility. Through diverse examples, we demonstrate
SasAgent's ability to interpret complex prompts, calculate SLDs, generate
accurate scattering data, and fit experimental datasets with high precision.
This work showcases the potential of LLM-driven AI systems to streamline
scientific workflows and enhance automation in SAS research.

</details>


### [24] [Characterizing Fitness Landscape Structures in Prompt Engineering](https://arxiv.org/abs/2509.05375)
*Arend Hintze*

Main category: cs.AI

TL;DR: 本文通过自相关分析揭示了提示工程优化景观的结构特征，发现系统化提示生成产生平滑衰减的自相关，而多样化生成则呈现非单调模式，表明存在崎岖、分层结构的景观。


<details>
  <summary>Details</summary>
Motivation: 虽然提示工程已成为优化大语言模型性能的关键技术，但其底层优化景观仍未被充分理解。当前方法将提示优化视为黑盒问题，应用复杂搜索算法但未描述其导航的景观拓扑结构。

Method: 使用语义嵌入空间中的自相关分析来系统分析提示工程中的适应度景观结构。通过在错误检测任务上进行实验，采用两种不同的提示生成策略：系统枚举（1,024个提示）和新颖性驱动的多样化（1,000个提示）。

Result: 发现系统化提示生成产生平滑衰减的自相关，而多样化生成呈现非单调模式，在中间语义距离处出现峰值相关性，表明存在崎岖、分层结构的景观。跨10个错误检测类别的任务特定分析显示不同错误类型具有不同程度的崎岖性。

Conclusion: 研究结果为理解提示工程景观中优化的复杂性提供了实证基础，揭示了不同提示生成策略产生的根本不同的景观拓扑结构。

Abstract: While prompt engineering has emerged as a crucial technique for optimizing
large language model performance, the underlying optimization landscape remains
poorly understood. Current approaches treat prompt optimization as a black-box
problem, applying sophisticated search algorithms without characterizing the
landscape topology they navigate. We present a systematic analysis of fitness
landscape structures in prompt engineering using autocorrelation analysis
across semantic embedding spaces. Through experiments on error detection tasks
with two distinct prompt generation strategies -- systematic enumeration (1,024
prompts) and novelty-driven diversification (1,000 prompts) -- we reveal
fundamentally different landscape topologies. Systematic prompt generation
yields smoothly decaying autocorrelation, while diversified generation exhibits
non-monotonic patterns with peak correlation at intermediate semantic
distances, indicating rugged, hierarchically structured landscapes.
Task-specific analysis across 10 error detection categories reveals varying
degrees of ruggedness across different error types. Our findings provide an
empirical foundation for understanding the complexity of optimization in prompt
engineering landscapes.

</details>


### [25] [Code Like Humans: A Multi-Agent Solution for Medical Coding](https://arxiv.org/abs/2509.05378)
*Andreas Motzfeldt,Joakim Edin,Casper L. Christensen,Christian Hardmeier,Lars Maaløe,Anna Rogers*

Main category: cs.AI

TL;DR: 提出了Code Like Humans框架，这是首个支持完整ICD-10编码系统（7万+标签）的医疗编码代理框架，在罕见诊断代码上达到最佳性能


<details>
  <summary>Details</summary>
Motivation: 医疗编码需要将非结构化临床记录映射到诊断和程序的字母数字代码，现有方法无法支持完整的ICD-10编码系统

Method: 基于大语言模型的代理框架，实现了人类专家的官方编码指南

Result: 在罕见诊断代码上达到最佳性能，但微调判别分类器在高频代码上仍保持优势

Conclusion: 贡献了系统性能分析并识别了系统性的'盲点'（被系统低估的代码），为未来工作指明方向

Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric
codes for diagnoses and procedures. We introduce Code Like Humans: a new
agentic framework for medical coding with large language models. It implements
official coding guidelines for human experts, and it is the first solution that
can support the full ICD-10 coding system (+70K labels). It achieves the best
performance to date on rare diagnosis codes (fine-tuned discriminative
classifiers retain an advantage for high-frequency codes, to which they are
limited). Towards future work, we also contribute an analysis of system
performance and identify its `blind spots' (codes that are systematically
undercoded).

</details>


### [26] [Murphys Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381)
*Madhava Gaikwad*

Main category: cs.AI

TL;DR: 本文提出了Alignment Gap概念，统一解释RLHF等对齐方法中的常见失败模式，通过KL-tilting形式化说明优化压力如何放大代理奖励与人类意图的偏差，并提出了Alignment Trilemma和MAPS框架来指导未来设计。


<details>
  <summary>Details</summary>
Motivation: 现有的人类偏好对齐方法（如RLHF、DPO等）虽然有效，但存在奖励黑客、奉承行为、标注者漂移和错误泛化等重复出现的失败模式，需要统一的视角来理解这些结构性限制。

Method: 使用KL-tilting形式化方法分析优化压力对代理奖励与真实人类意图偏差的放大效应，组织失败模式为Murphy定律目录，提出Alignment Trilemma理论框架，并通过小规模实证研究进行验证。

Result: 建立了Alignment Gap概念框架，系统化了对齐失败模式，提出了优化强度、价值捕获和泛化能力之间的三难权衡，并开发了MAPS（错误设定、标注、压力、漂移）实用设计框架。

Conclusion: 本文不是提出不可能定理，而是提供了一个重构对齐辩论的视角，围绕结构性限制和权衡，为未来对齐方法设计提供更清晰的指导。

Abstract: Large language models are increasingly aligned to human preferences through
reinforcement learning from human feedback (RLHF) and related methods such as
Direct Preference Optimization (DPO), Constitutional AI, and RLAIF. While
effective, these methods exhibit recurring failure patterns i.e., reward
hacking, sycophancy, annotator drift, and misgeneralization. We introduce the
concept of the Alignment Gap, a unifying lens for understanding recurring
failures in feedback-based alignment. Using a KL-tilting formalism, we
illustrate why optimization pressure tends to amplify divergence between proxy
rewards and true human intent. We organize these failures into a catalogue of
Murphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to
frame trade-offs among optimization strength, value capture, and
generalization. Small-scale empirical studies serve as illustrative support.
Finally, we propose the MAPS framework (Misspecification, Annotation, Pressure,
Shift) as practical design levers. Our contribution is not a definitive
impossibility theorem but a perspective that reframes alignment debates around
structural limits and trade-offs, offering clearer guidance for future design.

</details>


### [27] [From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation](https://arxiv.org/abs/2509.05469)
*Chenguang Wang,Xiang Yan,Yilong Dai,Ziyi Wang,Susu Xu*

Main category: cs.AI

TL;DR: 一个多代理系统，直接在真实街道图像上编辑和重新设计自行车设施，通过车道定位、提示优化、设计生成和自动评估等步骤生成现实且上下文适宜的设计方案。


<details>
  <summary>Details</summary>
Motivation: 传统街道设计渲染方案苦于集体冲议和协同决策，而现有AI生成方法需要大量领域特定数据且难以精确控制复杂街道场景中的空间变化。

Method: 开发了一个多代理系统，集成了车道定位、提示优化、设计生成和自动评估等模块，能够在真实街道图像上直接编辑和重新设计自行车设施。

Result: 在多样化城市场景中进行实验，系统能够适应不同的道路几何形状和环境条件，一质地产生视觉一致且符合指令要求的结果。

Conclusion: 这项工作为应用多代理流水线到交通基础设施规划和设施设计领域奠定了基础。

Abstract: Realistic visual renderings of street-design scenarios are essential for
public engagement in active transportation planning. Traditional approaches are
labor-intensive, hindering collective deliberation and collaborative
decision-making. While AI-assisted generative design shows transformative
potential by enabling rapid creation of design scenarios, existing generative
approaches typically require large amounts of domain-specific training data and
struggle to enable precise spatial variations of design/configuration in
complex street-view scenes. We introduce a multi-agent system that edits and
redesigns bicycle facilities directly on real-world street-view imagery. The
framework integrates lane localization, prompt optimization, design generation,
and automated evaluation to synthesize realistic, contextually appropriate
designs. Experiments across diverse urban scenarios demonstrate that the system
can adapt to varying road geometries and environmental conditions, consistently
yielding visually coherent and instruction-compliant results. This work
establishes a foundation for applying multi-agent pipelines to transportation
infrastructure planning and facility design.

</details>


### [28] [TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation](https://arxiv.org/abs/2509.05550)
*Zixi Li*

Main category: cs.AI

TL;DR: TreeGPT是一种新颖的神经架构，结合了transformer注意力机制和全局父子聚合，用于处理抽象语法树，在神经程序合成任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖序列处理或图神经网络，无法有效捕捉AST的层次结构信息，需要一种能够同时处理局部依赖和全局树结构的混合架构。

Method: 采用混合设计：使用self-attention捕捉局部依赖，TreeFFN通过迭代消息传递建模层次树结构，核心创新是全局父子聚合机制，包含可选的增强功能如门控聚合、残差连接和双向传播。

Result: 在ARC Prize 2025数据集上达到96%准确率，显著优于transformer基线(1.3%)、Grok-4(15.9%)和SOAR(52%)等模型，仅使用150万参数。消融研究表明边缘投影是最关键组件。

Conclusion: TreeGPT通过结合注意力机制和树结构聚合，在程序合成任务中实现了卓越性能，证明了混合架构在处理层次树结构数据方面的有效性。

Abstract: We introduce TreeGPT, a novel neural architecture that combines
transformer-based attention mechanisms with global parent-child aggregation for
processing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.
Unlike traditional approaches that rely solely on sequential processing or
graph neural networks, TreeGPT employs a hybrid design that leverages both
self-attention for capturing local dependencies and a specialized Tree
Feed-Forward Network (TreeFFN) for modeling hierarchical tree structures
through iterative message passing.
  The core innovation lies in our Global Parent-Child Aggregation mechanism,
formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \in
E_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents the
hidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges
involving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This
formulation enables each node to progressively aggregate information from the
entire tree structure through $T$ iterations.
  Our architecture integrates optional enhancements including gated aggregation
with learnable edge weights, residual connections for gradient stability, and
bidirectional propagation for capturing both bottom-up and top-down
dependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging
visual reasoning benchmark requiring abstract pattern recognition and rule
inference. Experimental results demonstrate that TreeGPT achieves 96\%
accuracy, significantly outperforming transformer baselines (1.3\%),
large-scale models like Grok-4 (15.9\%), and specialized program synthesis
methods like SOAR (52\%) while using only 1.5M parameters. Our comprehensive
ablation study reveals that edge projection is the most critical component,
with the combination of edge projection and gating achieving optimal
performance.

</details>


### [29] [OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision](https://arxiv.org/abs/2509.05578)
*Ruixun Liu,Lingyu Kong,Derun Li,Hang Zhao*

Main category: cs.AI

TL;DR: OccVLA是一个将3D占用表示集成到多模态推理过程中的新框架，通过将密集3D占用作为预测输出和监督信号，从2D视觉输入直接学习细粒度空间结构，在自动驾驶任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言推理方面表现强劲，但缺乏稳健的3D空间理解能力，这对自动驾驶至关重要。主要挑战包括构建有效3D表示的困难以及大规模3D视觉语言预训练的缺失导致的细粒度空间细节损失。

Method: 提出OccVLA框架，将3D占用表示作为预测输出和监督信号，使模型能够直接从2D视觉输入学习细粒度空间结构。占用预测被视为隐式推理过程，在推理时可跳过而不影响性能，不增加额外计算开销。

Result: 在nuScenes基准测试中实现了轨迹规划的最先进结果，在3D视觉问答任务上表现出优越性能。

Conclusion: OccVLA为自动驾驶提供了一个可扩展、可解释且完全基于视觉的解决方案，成功解决了3D空间理解的挑战。

Abstract: Multimodal large language models (MLLMs) have shown strong vision-language
reasoning abilities but still lack robust 3D spatial understanding, which is
critical for autonomous driving. This limitation stems from two key challenges:
(1) the difficulty of constructing accessible yet effective 3D representations
without expensive manual annotations, and (2) the loss of fine-grained spatial
details in VLMs due to the absence of large-scale 3D vision-language
pretraining. To address these challenges, we propose OccVLA, a novel framework
that integrates 3D occupancy representations into a unified multimodal
reasoning process. Unlike prior approaches that rely on explicit 3D inputs,
OccVLA treats dense 3D occupancy as both a predictive output and a supervisory
signal, enabling the model to learn fine-grained spatial structures directly
from 2D visual inputs. The occupancy predictions are regarded as implicit
reasoning processes and can be skipped during inference without performance
degradation, thereby adding no extra computational overhead. OccVLA achieves
state-of-the-art results on the nuScenes benchmark for trajectory planning and
demonstrates superior performance on 3D visual question-answering tasks,
offering a scalable, interpretable, and fully vision-based solution for
autonomous driving.

</details>


### [30] [MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions](https://arxiv.org/abs/2509.05685)
*Jian Yang,Jiahui Wu,Li Fang,Hongchao Fan,Bianying Zhang,Huijie Zhao,Guangyi Yang,Rui Xin,Xiong You*

Main category: cs.AI

TL;DR: MSRFormer是一个多尺度道路网络表示学习框架，通过空间流卷积和图变换器处理道路网络的异质性和层次性，在真实数据集上优于基线方法16%


<details>
  <summary>Details</summary>
Motivation: 城市道路网络的异质性和层次性给准确表示学习带来挑战，传统图神经网络因同质性假设和单一结构尺度限制而效果不佳

Method: 使用空间流卷积从轨迹数据提取小尺度特征，识别尺度依赖的空间交互区域，通过图变换器捕捉多尺度复杂空间依赖，采用残差连接和对比学习生成最终表示

Result: 在两个真实数据集的道路网络分析任务中，MSRFormer优于基线方法，在交通相关任务和复杂道路网络结构中表现更佳，相比最强基线提升达16%

Conclusion: 该研究为开发任务无关的道路网络表示模型提供了实用框架，揭示了尺度效应与空间交互流异质性之间的关联模式

Abstract: Transforming road network data into vector representations using deep
learning has proven effective for road network analysis. However, urban road
networks' heterogeneous and hierarchical nature poses challenges for accurate
representation learning. Graph neural networks, which aggregate features from
neighboring nodes, often struggle due to their homogeneity assumption and focus
on a single structural scale. To address these issues, this paper presents
MSRFormer, a novel road network representation learning framework that
integrates multi-scale spatial interactions by addressing their flow
heterogeneity and long-distance dependencies. It uses spatial flow convolution
to extract small-scale features from large trajectory datasets, and identifies
scale-dependent spatial interaction regions to capture the spatial structure of
road networks and flow heterogeneity. By employing a graph transformer,
MSRFormer effectively captures complex spatial dependencies across multiple
scales. The spatial interaction features are fused using residual connections,
which are fed to a contrastive learning algorithm to derive the final road
network representation. Validation on two real-world datasets demonstrates that
MSRFormer outperforms baseline methods in two road network analysis tasks. The
performance gains of MSRFormer suggest the traffic-related task benefits more
from incorporating trajectory data, also resulting in greater improvements in
complex road network structures with up to 16% improvements compared to the
most competitive baseline method. This research provides a practical framework
for developing task-agnostic road network representation models and highlights
distinct association patterns of the interplay between scale effects and flow
heterogeneity of spatial interactions.

</details>


### [31] [Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs](https://arxiv.org/abs/2509.05714)
*Zhaoyu Fan,Kaihang Pan,Mingze Zhou,Bosheng Qin,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Fei Wu,Yueting Zhuang*

Main category: cs.AI

TL;DR: CogEdit是一个评估多模态大语言模型元认知知识编辑能力的新基准，包含三个层次：反事实驱动编辑、边界约束编辑和噪声鲁棒编辑。提出的MIND框架通过元知识记忆、博弈论交互和标签精炼显著提升了编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑基准主要关注认知层面的修改，缺乏对更深层次元认知过程的关注，需要开发能够评估模型自我意识和反思能力的基准。

Method: 提出CogEdit基准评估三个元认知层次，并开发MIND框架：构建元知识记忆实现自我意识，使用博弈论交互监控知识激活，通过标签精炼实现噪声鲁棒更新。

Result: 大量实验表明MIND框架显著优于现有认知编辑方法，在传统和元认知知识编辑基准上都取得了强劲性能。

Conclusion: CogEdit基准和MIND框架有效填补了元认知知识编辑的空白，为多模态大语言模型提供了更全面的知识编辑评估和改进方法。

Abstract: Knowledge editing enables multimodal large language models (MLLMs) to
efficiently update outdated or incorrect information. However, existing
benchmarks primarily emphasize cognitive-level modifications while lacking a
focus on deeper meta-cognitive processes. To bridge this gap, we introduce
CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge
editing abilities across three levels: (1) Counterfactual-Driven Editing,
assessing self-awareness of knowledge correctness changes; (2) Boundary
Constraint Editing, ensuring appropriate generalization without unintended
interference; and (3) Noise-Robust Editing, promoting reflective evaluation of
uncertain information. To advance meta-cognitive editing, we propose MIND
(Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that
constructs a meta-knowledge memory for self-awareness, employs game-theoretic
interactions to monitor knowledge activation, and incorporates label refinement
for noise-robust updates. Extensive experiments show that MIND significantly
outperforms existing cognitive editing approaches, achieving strong performance
on both traditional and meta-cognitive knowledge editing benchmarks.

</details>


### [32] [Hyperbolic Large Language Models](https://arxiv.org/abs/2509.05757)
*Sarang Patil,Zeyong Zhang,Yiran Huang,Tengfei Ma,Mengjia Xu*

Main category: cs.AI

TL;DR: 该论文综述了双曲几何在大语言模型中的应用，通过四种主要技术分类探讨如何利用双曲空间增强语义表示学习和多尺度推理能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据具有高度非欧几里得的层次结构特征，而传统LLMs在处理这类结构时存在局限，双曲几何因其对树状层次结构的有效建模能力而成为有前景的解决方案。

Method: 提出四类主要技术：通过指数/对数映射的双曲LLMs、双曲微调模型、完全双曲LLMs、以及双曲状态空间模型，并建立了分类体系。

Result: 系统梳理了双曲几何在LLMs中的最新进展，提供了技术分类框架和应用前景分析。

Conclusion: 双曲几何为LLMs处理层次结构数据提供了有前景的方向，未来需要在理论分析和实际应用方面进一步探索。

Abstract: Large language models (LLMs) have achieved remarkable success and
demonstrated superior performance across various tasks, including natural
language processing (NLP), weather forecasting, biological protein folding,
text generation, and solving mathematical problems. However, many real-world
data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein
networks, transportation networks, financial networks, brain networks, and
linguistic structures or syntactic trees in natural languages. Effectively
learning intrinsic semantic entailment and hierarchical relationships from
these raw, unstructured input data using LLMs remains an underexplored area.
Due to its effectiveness in modeling tree-like hierarchical structures,
hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity
as an expressive latent representation space for complex data modeling across
domains such as graphs, images, languages, and multi-modal data. Here, we
provide a comprehensive and contextual exposition of recent advancements in
LLMs that leverage hyperbolic geometry as a representation space to enhance
semantic representation learning and multi-scale reasoning. Specifically, the
paper presents a taxonomy of the principal techniques of Hyperbolic LLMs
(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log
maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)
hyperbolic state-space models. We also explore crucial potential applications
and outline future research directions. A repository of key papers, models,
datasets, and code implementations is available at
https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.

</details>


### [33] [DRF: LLM-AGENT Dynamic Reputation Filtering Framework](https://arxiv.org/abs/2509.05764)
*Yuwei Lou,Hao Hu,Shaocong Ma,Zongfei Zhang,Liang Wang,Jidong Ge,Xianping Tao*

Main category: cs.AI

TL;DR: DRF是一个动态信誉过滤框架，通过构建交互评分网络量化智能体性能，设计信誉评分机制评估智能体诚实度和能力，使用UCB策略提升智能体选择效率，显著提高多智能体系统的任务完成质量和协作效率。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI发展，基于大语言模型的多智能体系统面临智能体性能难以量化和缺乏信誉评估机制的问题，需要解决这些挑战来提升系统效能。

Method: 提出DRF框架：1）构建交互评分网络量化智能体性能；2）设计信誉评分机制评估智能体诚实度和能力；3）集成基于上置信界(UCB)的策略优化智能体选择效率。

Result: 实验表明DRF在逻辑推理和代码生成任务中显著提高了任务完成质量和协作效率，为多智能体系统处理大规模任务提供了新方法。

Conclusion: DRF框架有效解决了多智能体系统中的性能量化和信誉评估问题，通过动态信誉过滤机制提升了系统整体性能，为大规模任务处理提供了可行方案。

Abstract: With the evolution of generative AI, multi - agent systems leveraging large -
language models(LLMs) have emerged as a powerful tool for complex tasks.
However, these systems face challenges in quantifying agent performance and
lack mechanisms to assess agent credibility. To address these issues, we
introduce DRF, a dynamic reputation filtering framework. DRF constructs an
interactive rating network to quantify agent performance, designs a reputation
scoring mechanism to measure agent honesty and capability, and integrates an
Upper Confidence Bound - based strategy to enhance agent selection efficiency.
Experiments show that DRF significantly improves task completion quality and
collaboration efficiency in logical reasoning and code - generation tasks,
offering a new approach for multi - agent systems to handle large - scale
tasks.

</details>


### [34] [Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation](https://arxiv.org/abs/2509.05772)
*Nasser Alkhulaifi,Ismail Gokay Dogan,Timothy R. Cargan,Alexander L. Bowler,Direnc Pekaslan,Nicholas J. Watson,Isaac Triguero*

Main category: cs.AI

TL;DR: 这篇论文提出了一种结合自动特征工程（AFE）和决策聚焦学习（DFL）的框架，用于解决实际电力系统中的电池能量存储系统（BESS）优化问题，在小数据集情况下提高了运营成本效果。


<details>
  <summary>Details</summary>
Motivation: 传统的预测-优化（PTO）方法将预测和优化分离处理，导致预测错误传播到决策中，而新兴的DFL方法虽然集成了预测和优化，但在实际应用中面临数据稀缺和变异性挑战。

Method: 提出AFE-DFL框架，利用自动特征工程提取更丰富的表征，在小数据集上实现电价和需求预测，同时优化BESS运营以最小化成本。

Result: 在英国实际房屋数据集上验证，DFL法比PTO法平均节省更多运营成本，而添加AFE后让DFL性能提升22.9-56.5%。

Conclusion: 结果证明DFL在实际应用中的可行性，领域特定的AFE能够增强DFL效果，减少对领域专业知识的依赖，并为面临类似挑战的能源管理系统带来经济效益。

Abstract: Decision-making under uncertainty in energy management is complicated by
unknown parameters hindering optimal strategies, particularly in Battery Energy
Storage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat
forecasting and optimisation as separate processes, allowing prediction errors
to cascade into suboptimal decisions as models minimise forecasting errors
rather than optimising downstream tasks. The emerging Decision-Focused Learning
(DFL) methods overcome this limitation by integrating prediction and
optimisation; however, they are relatively new and have been tested primarily
on synthetic datasets or small-scale problems, with limited evidence of their
practical viability. Real-world BESS applications present additional
challenges, including greater variability and data scarcity due to collection
constraints and operational limitations. Because of these challenges, this work
leverages Automated Feature Engineering (AFE) to extract richer representations
and improve the nascent approach of DFL. We propose an AFE-DFL framework
suitable for small datasets that forecasts electricity prices and demand while
optimising BESS operations to minimise costs. We validate its effectiveness on
a novel real-world UK property dataset. The evaluation compares DFL methods
against PTO, with and without AFE. The results show that, on average, DFL
yields lower operating costs than PTO and adding AFE further improves the
performance of DFL methods by 22.9-56.5% compared to the same models without
AFE. These findings provide empirical evidence for DFL's practical viability in
real-world settings, indicating that domain-specific AFE enhances DFL and
reduces reliance on domain expertise for BESS optimisation, yielding economic
benefits with broader implications for energy management systems facing similar
challenges.

</details>


### [35] [Chatbot To Help Patients Understand Their Health](https://arxiv.org/abs/2509.05818)
*Won Seok Jang,Hieu Tran,Manav Mistry,SaiKiran Gandluri,Yifan Zhang,Sharmin Sultana,Sunjae Kown,Yuan Zhang,Zonghai Yao,Hong Yu*

Main category: cs.AI

TL;DR: NoteAid-Chatbot是一个基于多智能体LLM和强化学习的对话AI，通过'学习即对话'框架促进患者理解医疗信息，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 患者需要具备相关知识才能积极参与自身护理，但医疗信息理解存在障碍，需要开发能够有效教育患者的对话系统。

Method: 基于轻量级LLaMA 3.2 3B模型，采用两阶段训练：先在合成医疗对话数据上进行监督微调，然后通过强化学习（PPO）在模拟出院场景中进行奖励优化。

Result: NoteAid-Chatbot展现出关键的新兴行为（清晰性、相关性、结构化对话），在图灵测试中超越非专家人类表现，证明轻量模型也能处理多轮交互和复杂教育策略。

Conclusion: 该框架展示了低成本PPO强化学习在现实开放域对话中的可行性，扩展了RL对齐方法的适用性，具有广泛的领域应用前景。

Abstract: Patients must possess the knowledge necessary to actively participate in
their care. We present NoteAid-Chatbot, a conversational AI that promotes
patient understanding via a novel 'learning as conversation' framework, built
on a multi-agent large language model (LLM) and reinforcement learning (RL)
setup without human-labeled data. NoteAid-Chatbot was built on a lightweight
LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on
conversational data synthetically generated using medical conversation
strategies, followed by RL with rewards derived from patient understanding
assessments in simulated hospital discharge scenarios. Our evaluation, which
includes comprehensive human-aligned assessments and case studies, demonstrates
that NoteAid-Chatbot exhibits key emergent behaviors critical for patient
education, such as clarity, relevance, and structured dialogue, even though it
received no explicit supervision for these attributes. Our results show that
even simple Proximal Policy Optimization (PPO)-based reward modeling can
successfully train lightweight, domain-specific chatbots to handle multi-turn
interactions, incorporate diverse educational strategies, and meet nuanced
communication objectives. Our Turing test demonstrates that NoteAid-Chatbot
surpasses non-expert human. Although our current focus is on healthcare, the
framework we present illustrates the feasibility and promise of applying
low-cost, PPO-based RL to realistic, open-ended conversational domains,
broadening the applicability of RL-based alignment methods.

</details>


### [36] [MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration](https://arxiv.org/abs/2509.05933)
*Md Hasebul Hasan,Mahir Labib Dihan,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: MapAgent是一个分层多智能体框架，专门用于地理空间推理任务，通过解耦规划与执行来提高工具选择准确性和API协调能力


<details>
  <summary>Details</summary>
Motivation: 现有AI代理框架主要针对数学、编程等领域，在地理空间任务中表现不足，需要空间推理、多跳规划和实时地图交互能力

Method: 采用分层多智能体架构：高层规划器分解查询为子目标，专用地图工具代理并行协调相关API获取地理空间数据，简单模块无需额外代理开销

Result: 在四个地理空间基准测试(MapEval-Textual、MapEval-API、MapEval-Visual、MapQA)上显著优于现有工具增强和代理基线方法

Conclusion: MapAgent通过分层设计和专用工具集有效解决了地理空间推理的挑战，降低了认知负荷，提高了工具选择的准确性

Abstract: Agentic AI has significantly extended the capabilities of large language
models (LLMs) by enabling complex reasoning and tool use. However, most
existing frameworks are tailored to domains such as mathematics, coding, or web
automation, and fall short on geospatial tasks that require spatial reasoning,
multi-hop planning, and real-time map interaction. To address these challenges,
we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with
customized toolsets and agentic scaffolds for map-integrated geospatial
reasoning. Unlike existing flat agent-based approaches that treat tools
uniformly-often overwhelming the LLM when handling similar but subtly different
geospatial APIs-MapAgent decouples planning from execution. A high-level
planner decomposes complex queries into subgoals, which are routed to
specialized modules. For tool-heavy modules-such as map-based services-we then
design a dedicated map-tool agent that efficiently orchestrates related APIs
adaptively in parallel to effectively fetch geospatial data relevant for the
query, while simpler modules (e.g., solution generation or answer extraction)
operate without additional agent overhead. This hierarchical design reduces
cognitive load, improves tool selection accuracy, and enables precise
coordination across similar APIs. We evaluate MapAgent on four diverse
geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and
MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented
and agentic baselines. We open-source our framwork at
https://github.com/Hasebul/MapAgent.

</details>


### [37] [Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL](https://arxiv.org/abs/2509.06024)
*Haoyang He,Zihua Rong,Kun Ji,Chenyang Li,Qing Huang,Chong Xia,Lan Yang,Honggang Zhang*

Main category: cs.AI

TL;DR: 提出了DRER框架，通过推理质量奖励和动态长度优势来改进强化学习中的奖励信号，提升大语言模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 现有的基于规则的奖励函数只评估答案格式和正确性，无法判断推理链是否真正改善答案，且任务特定训练对逻辑深度的控制有限

Method: DRER框架包含：(i)推理质量奖励 - 对提升正确答案概率的推理链给予细粒度奖励；(ii)动态长度优势 - 对偏离验证阈值的响应长度进行优势衰减

Result: 7B模型在400训练步后达到GPT-o3-mini水平，CoT增强答案的平均置信度提升30%，在多个逻辑推理数据集和AIME24数学基准上表现出泛化能力

Conclusion: DRER有效塑造了CoT行为，为增强大语言模型的形式推理能力提供了实用路径

Abstract: Reinforcement learning (RL) has recently become the dominant paradigm for
strengthening the reasoning abilities of large language models (LLMs). Yet the
rule-based reward functions commonly used on mathematical or programming
benchmarks assess only answer format and correctness, providing no signal as to
whether the induced Chain-of-Thought (CoT) actually improves the answer.
Furthermore, such task-specific training offers limited control over logical
depth and therefore may fail to reveal a model's genuine reasoning capacity. We
propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward
framework that reshapes both reward and advantage signals. (i) A Reasoning
Quality Reward assigns fine-grained credit to those reasoning chains that
demonstrably raise the likelihood of the correct answer, directly incentivising
the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage
decays the advantage of responses whose length deviates from a
validation-derived threshold, stabilising training. To facilitate rigorous
assessment, we also release Logictree, a dynamically constructed deductive
reasoning dataset that functions both as RL training data and as a
comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B
model attains GPT-o3-mini level performance on Logictree with 400 trianing
steps, while the average confidence of CoT-augmented answers rises by 30%. The
model further exhibits generalisation across diverse logical-reasoning
datasets, and the mathematical benchmark AIME24. These results illuminate how
RL shapes CoT behaviour and chart a practical path toward enhancing
formal-reasoning skills in large language models. All code and data are
available in repository https://github.com/Henryhe09/DRER.

</details>


### [38] [Reverse-Engineered Reasoning for Open-Ended Generation](https://arxiv.org/abs/2509.06160)
*Haozhe Wang,Haoran Que,Qixin Xu,Minghao Liu,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Wei Ye,Tong Yang,Wenhao Huang,Ge Zhang,Fangzhen Lin*

Main category: cs.AI

TL;DR: REER是一种新的深度推理范式，通过从已知良好解决方案反向工程推导出推理过程，解决了开放创造性生成中的推理挑战


<details>
  <summary>Details</summary>
Motivation: 传统强化学习和指令蒸馏方法在开放创造性生成中存在局限性：RL缺乏清晰奖励信号，蒸馏方法成本高且受限于教师模型能力

Method: REER（反向工程推理）范式，从已知良好解决方案反向计算发现潜在的逐步深度推理过程，采用可扩展的无梯度方法

Result: 构建了DeepWriting-20K大规模数据集（20,000条深度推理轨迹），DeepWriter-8B模型超越开源基线，性能与GPT-4o和Claude 3.5等专有模型相当甚至更优

Conclusion: REER为开放创造性任务中的深度推理提供了新的有效范式，通过反向工程方法解决了传统方法的局限性，在性能上取得了显著突破

Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in
verifiable domains like mathematics, its application to open-ended, creative
generation remains a critical challenge. The two dominant methods for
instilling reasoning -- reinforcement learning (RL) and instruction
distillation -- falter in this area; RL struggles with the absence of clear
reward signals and high-quality reward models, while distillation is
prohibitively expensive and capped by the teacher model's capabilities. To
overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a
new paradigm that fundamentally shifts the approach. Instead of building a
reasoning process ``forwards'' through trial-and-error or imitation, REER works
``backwards'' from known-good solutions to computationally discover the latent,
step-by-step deep reasoning process that could have produced them. Using this
scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a
large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.
Our model, DeepWriter-8B, trained on this data, not only surpasses strong
open-source baselines but also achieves performance competitive with, and at
times superior to, leading proprietary models like GPT-4o and Claude 3.5.

</details>


### [39] [From Long to Short: LLMs Excel at Trimming Own Reasoning Chains](https://arxiv.org/abs/2509.06174)
*Wei Han,Geng Zhan,Sicheng Yu,Chenyu Wang,Bryan Hooi*

Main category: cs.AI

TL;DR: EDIT是一种测试时缩放方法，通过约束引导生成和联合跟踪长度与答案分布，帮助大型推理模型找到最短的正确推理路径，平衡简洁性和正确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)在处理简单问题时容易过度思考，产生冗长复杂的推理轨迹，影响可解释性。研究发现LRMs难以平衡正确性和简洁性等多个生成目标。

Method: 提出EDIT方法，采用约束引导生成，在测试时联合跟踪长度和答案分布，选择在简洁性和正确性之间达到最优平衡的响应。

Result: 在各种模型和数据集上的广泛实验表明，EDIT显著提高了推理效率，产生紧凑而信息丰富的输出，改善了可读性和用户体验。

Conclusion: EDIT有效解决了LRMs的过度思考问题，通过动态推理修剪实现了推理效率的显著提升，为大型推理模型的实用化提供了重要解决方案。

Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward
over conventional instruction-following LLMs. By applying test-time scaling to
generate extended reasoning paths, they establish many SOTAs across a wide
range of complex reasoning tasks. However, recent studies show that LRMs are
prone to suffer from overthinking -- the tendency to overcomplicate simple
problems, leading to excessive strategy switching and long, convoluted
reasoning traces that hinder their interpretability. To mitigate this issue, we
conduct a systematic investigation into the reasoning efficiency of a broad set
of LRMs and uncover a common dilemma: the difficulty in balancing multiple
generation objectives such as correctness and brevity. Based on this discovery,
we propose a test-time scaling method, EDIT (Efficient Dynamic Inference
Trimming), which efficiently guides LRMs to identify the shortest correct
reasoning paths at test time. EDIT employs constraint-guided generation while
jointly tracking length and answer distributions under varying constraints,
allowing it to select responses that strike an optimal balance between
conciseness and correctness. Extensive experiments across diverse models and
datasets show that EDIT substantially enhance the reasoning efficiency,
producing compact yet informative outputs that improve readability and user
experience.

</details>


### [40] [PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments](https://arxiv.org/abs/2509.06235)
*Olivier Schipper,Yudi Zhang,Yali Du,Mykola Pechenizkiy,Meng Fang*

Main category: cs.AI

TL;DR: 基于LLM的多游戏室内竞争环境评测框架PillagerBench和战术系统TactiCrafter，在Minecraft中实现了团队对抗场景的高效评测和适应性学习


<details>
  <summary>Details</summary>
Motivation: 当前LLM基于的代理在竞争性多游戏室环境中的表现尚未充分探索，需要一个公平可复现的评测框架来推动该领域的研究

Method: 开发PillagerBench框架提供扩展API、多轮测试和规则基础对手；设计TactiCrafter系统通过人类可读战术促进团队合作，学习因果依赖关系并适应对手策略

Result: TactiCrafter在性能上超过基线方法，通过自我对抗展现出适应性学习能力，并在多个游戏循环中完成战略迭代演化

Conclusion: 研究为竞争性多游戏室环境的多代理系统提供了有效评测工具和适应性解决方案，并通过开源PillagerBench推动该领域的进一步研究

Abstract: LLM-based agents have shown promise in various cooperative and strategic
reasoning tasks, but their effectiveness in competitive multi-agent
environments remains underexplored. To address this gap, we introduce
PillagerBench, a novel framework for evaluating multi-agent systems in
real-time competitive team-vs-team scenarios in Minecraft. It provides an
extensible API, multi-round testing, and rule-based built-in opponents for
fair, reproducible comparisons. We also propose TactiCrafter, an LLM-based
multi-agent system that facilitates teamwork through human-readable tactics,
learns causal dependencies, and adapts to opponent strategies. Our evaluation
demonstrates that TactiCrafter outperforms baseline approaches and showcases
adaptive learning through self-play. Additionally, we analyze its learning
process and strategic evolution over multiple game episodes. To encourage
further research, we have open-sourced PillagerBench, fostering advancements in
multi-agent AI for competitive environments.

</details>


### [41] [Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning](https://arxiv.org/abs/2509.06239)
*Manvi Jha,Jiaxin Wan,Deming Chen*

Main category: cs.AI

TL;DR: Proof2Silicon是一个端到端硬件合成框架，通过PREFACE的强化学习提示优化生成可验证的Dafny代码，然后自动转换为C代码并最终合成RTL硬件设计，实现了从自然语言规范到硅实现的自动化流程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但生成的代码经常无法通过形式化验证，这在硬件和安全关键领域是基本要求。需要一种方法能够生成形式化验证正确的代码。

Method: 1) 使用PREFACE的验证器驱动RL代理迭代优化提示生成，确保Dafny代码正确性；2) 将验证后的Dafny程序自动转换为可合成的高级C代码；3) 使用Vivado HLS生成RTL实现

Result: 在100个任务的基准测试中，PREFACE的RL引导提示优化将Dafny验证成功率提高了21%，Proof2Silicon实现了高达72%的端到端硬件合成成功率

Conclusion: 该研究展示了一个稳健、可扩展且自动化的流程，用于LLM驱动的形式化验证硬件合成，连接了自然语言规范和硅实现

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
automated code generation but frequently produce code that fails formal
verification, an essential requirement for hardware and safety-critical
domains. To overcome this fundamental limitation, we previously proposed
PREFACE, a model-agnostic framework based on reinforcement learning (RL) that
iteratively repairs the prompts provided to frozen LLMs, systematically
steering them toward generating formally verifiable Dafny code without costly
fine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis
framework that embeds the previously proposed PREFACE flow to enable the
generation of correctness-by-construction hardware directly from natural
language specifications. Proof2Silicon operates by: (1) leveraging PREFACE's
verifier-driven RL agent to optimize prompt generation iteratively, ensuring
Dafny code correctness; (2) automatically translating verified Dafny programs
into synthesizable high-level C using Dafny's Python backend and PyLog; and (3)
employing Vivado HLS to produce RTL implementations. Evaluated rigorously on a
challenging 100-task benchmark, PREFACE's RL-guided prompt optimization
consistently improved Dafny verification success rates across diverse LLMs by
up to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis
success rate of up to 72%, generating RTL designs through Vivado HLS synthesis
flows. These results demonstrate a robust, scalable, and automated pipeline for
LLM-driven, formally verified hardware synthesis, bridging natural-language
specification and silicon realization.

</details>


### [42] [REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents](https://arxiv.org/abs/2509.06269)
*Vishal Raman,Vijai Aravindh R,Abhijith Ragav*

Main category: cs.AI

TL;DR: REMI是一个基于因果模式记忆架构的多模态生活方式助手，通过整合个人因果知识图谱、因果推理引擎和基于模式的规划模块，提供可解释的个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化AI助手难以整合复杂的个人数据和因果知识，导致建议过于通用且缺乏解释性。

Method: 使用个人因果图谱记录用户生活事件和习惯，通过目标导向的因果遍历、外部知识增强和假设推理，结合可适应计划模式生成定制化行动方案，由大语言模型协调各组件。

Result: 基于CSM的智能体相比基准LLM智能体能够提供更符合上下文、更贴近用户的推荐建议。

Conclusion: 这项工作展示了在个性化智能体中实现记忆增强和因果推理的新方法，推动了透明可信赖的AI生活方式助手的发展。

Abstract: Personalized AI assistants often struggle to incorporate complex personal
data and causal knowledge, leading to generic advice that lacks explanatory
power. We propose REMI, a Causal Schema Memory architecture for a multimodal
lifestyle agent that integrates a personal causal knowledge graph, a causal
reasoning engine, and a schema based planning module. The idea is to deliver
explainable, personalized recommendations in domains like fashion, personal
wellness, and lifestyle planning. Our architecture uses a personal causal graph
of the user's life events and habits, performs goal directed causal traversals
enriched with external knowledge and hypothetical reasoning, and retrieves
adaptable plan schemas to generate tailored action plans. A Large Language
Model orchestrates these components, producing answers with transparent causal
explanations. We outline the CSM system design and introduce new evaluation
metrics for personalization and explainability, including Personalization
Salience Score and Causal Reasoning Accuracy, to rigorously assess its
performance. Results indicate that CSM based agents can provide more context
aware, user aligned recommendations compared to baseline LLM agents. This work
demonstrates a novel approach to memory augmented, causal reasoning in
personalized agents, advancing the development of transparent and trustworthy
AI lifestyle assistants.

</details>


### [43] [TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning](https://arxiv.org/abs/2509.06278)
*Chuang Jiang,Mingyue Cheng,Xiaoyu Tao,Qingyang Mao,Jie Ouyang,Qi Liu*

Main category: cs.AI

TL;DR: TableMind是一个基于大语言模型的表格推理代理，通过自主多轮工具调用、安全沙箱代码执行以及规划自反思能力，显著提升了表格数据的数值计算精度和推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有纯文本方法在复杂数值计算和细粒度操作上表现不佳，而工具集成推理方法又缺乏真正的自主适应性，需要更灵活、准确的表格推理解决方案。

Method: 采用两阶段微调范式：先在高质量推理轨迹上进行监督微调建立有效工具使用模式，然后通过强化微调优化多目标策略，特别提出了Rank-Aware Policy Optimization (RAPO)方法。

Result: 在多个主流基准测试中，TableMind相比竞争基线取得了优越性能，在推理准确性和计算精度方面都有显著提升。

Conclusion: TableMind通过自主工具调用、安全代码执行和高级规划能力，为表格推理任务提供了一个高效、准确的解决方案，在数值计算密集型领域具有重要应用价值。

Abstract: Table reasoning is crucial for leveraging structured data in domains such as
finance, healthcare, and scientific research. While large language models
(LLMs) show promise in multi-step reasoning, purely text-based methods often
struggle with the complex numerical computations and fine-grained operations
inherently required in this task. Tool-integrated reasoning improves
computational accuracy via explicit code execution, yet existing systems
frequently rely on rigid patterns, supervised imitation, and lack true
autonomous adaptability. In this paper, we present TableMind, an LLM-driven
table reasoning agent that (i) autonomously performs multi-turn tool
invocation, (ii) writes and executes data-analyzing code in a secure sandbox
environment for data analysis and precise numerical reasoning, and (iii)
exhibits high-level capabilities such as planning and self-reflection to adapt
strategies. To realize these capabilities, we adopt a two-stage fine-tuning
paradigm built on top of a powerful pre-trained language model: supervised
fine-tuning on high-quality reasoning trajectories to establish effective tool
usage patterns, followed by reinforcement fine-tuning to optimize
multi-objective strategies. In particular, we propose Rank-Aware Policy
Optimization (RAPO), which increases the update weight of high-quality
trajectories when their output probabilities are lower than those of
low-quality ones, thereby guiding the model more consistently toward better and
more accurate answers. Extensive experiments on several mainstream benchmarks
demonstrate that TableMind achieves superior performance compared to
competitive baselines, yielding substantial gains in both reasoning accuracy
and computational precision.

</details>


### [44] [SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents](https://arxiv.org/abs/2509.06283)
*Xuan-Phi Nguyen,Shrey Pandit,Revanth Gangi Reddy,Austin Xu,Silvio Savarese,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 通过控制性强化学习训练理性优化模型，发展自主单代理深度研究能力，在Humanity's Last Exam测试中达到28.7%性能


<details>
  <summary>Details</summary>
Motivation: 为大语言模型开发复杂的理性推理和工具使用能力，特别是在深度研究应用中实现自主动态决策，而非依赖多代理系统的静态工作流

Method: 采用控制性强化学习方法，使用全合成数据训练理性优化模型，集成最小化网络爬虫和Python工具

Result: 最佳模型SFR-DR-20B在Humanity's Last Exam测试中达到28.7%的性能水平

Conclusion: 通过控制性RL训练理性优化模型可有效提升自主单代理的深度研究能力，为代理智能研究提供了新的方向

Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning
and tool-use capabilities has become a key focus in agentic AI research,
especially with recent advances in reasoning-oriented (``thinking'') models.
Such capabilities are key to unlocking a number of important applications. One
such application is Deep Research (DR), which requires extensive search and
reasoning over many sources. Our work in this paper focuses on the development
of native Autonomous Single-Agent models for DR featuring minimal web crawling
and Python tool integration. Unlike multi-agent systems, where agents take up
pre-defined roles and are told what to do at each step in a static workflow, an
autonomous single-agent determines its next action dynamically based on
context, without manual directive. While prior work has proposed training
recipes for base or instruction-tuned LLMs, we focus on continual reinforcement
learning (RL) of reasoning-optimized models to further enhance agentic skills
while preserving reasoning ability. Towards this end, we propose a simple RL
recipe with entirely synthetic data, which we apply to various open-source
LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam
benchmark. In addition, we conduct key analysis experiments to provide more
insights into our methodologies.

</details>


### [45] [From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs](https://arxiv.org/abs/2509.06284)
*Jiaxiang Chen,Zhuo Wang,Mingxi Zou,Zhucong Li,Zhijian Zhou,Song Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: 提出了一个从隐式探索转向结构化推理的框架，通过指导原则和精炼机制来提升大语言模型的推理稳定性、错误纠正能力和学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的推理方法依赖隐式探索，导致推理路径不稳定、缺乏错误纠正机制，且无法有效从过往经验中学习。

Method: 从成功轨迹中提取结构化推理模式，从失败中获取反思信号；在推理过程中逐步遵循指导原则，并在每一步后应用精炼来纠正错误和稳定推理过程。

Result: 在BBH和其他四个基准测试（GSM8K、MATH-500、MBPP、HumanEval）上一致优于强基线，在多样推理任务中表现优异。

Conclusion: 结构化推理结合逐步执行和精炼提高了稳定性和泛化能力，指导原则跨域迁移良好，支持跨模型协作，在效果和可扩展性上匹配或超越监督微调。

Abstract: Large language models (LLMs) have advanced general-purpose reasoning, showing
strong performance across diverse tasks. However, existing methods often rely
on implicit exploration, where the model follows stochastic and unguided
reasoning paths-like walking without a map. This leads to unstable reasoning
paths, lack of error correction, and limited learning from past experience. To
address these issues, we propose a framework that shifts from implicit
exploration to structured reasoning through guideline and refinement. First, we
extract structured reasoning patterns from successful trajectories and
reflective signals from failures. During inference, the model follows these
guidelines step-by-step, with refinement applied after each step to correct
errors and stabilize the reasoning process. Experiments on BBH and four
additional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method
consistently outperforms strong baselines across diverse reasoning tasks.
Structured reasoning with stepwise execution and refinement improves stability
and generalization, while guidelines transfer well across domains and flexibly
support cross-model collaboration, matching or surpassing supervised
fine-tuning in effectiveness and scalability.

</details>


### [46] [Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models](https://arxiv.org/abs/2509.06307)
*Lei Shu,Dong Zhao*

Main category: cs.AI

TL;DR: 大语言模型在住宅能源绿化改造决策中表现有潜力，在技术目标下效果更好，但需要提高准确性、一致性和上下文处理能力


<details>
  <summary>Details</summary>
Motivation: 传统能源改造决策方法存在普遍性和可解释性不足的问题，智能连接社区和生成式AI的发展为解决这些挑战提供了新机会

Method: 评估七种大语言模型(ChatGPT、DeepSeek、Gemini、Grok、Llama、Claude)在两种目标下的表现：最大化CO2减排(技术)和最小化回报期(社会技术)，使用49个美国州400房屋的数据集，从准确性、一致性、敏感性和推理能力四个维度进行分析

Result: LLM在许多情况下能生成有效建议，在不细调的情况下顶部1匹配率达54.5%，前5匹配率92.8%。技术目标下表现更好，社会技术决策受到经济特性和地方上下文限制。模型间一致性低，高性能模型偏离其他模型。LLM对位置和建筑形状敏感，对技术和住户行为敏感性较低

Conclusion: 大语言模型在能源改造决策中展现出了作为助手工具的潜力，但要实现可靠应用，还需要在准确性、一致性和上下文处理方面进一步改进

Abstract: Conventional approaches to building energy retrofit decision making suffer
from limited generalizability and low interpretability, hindering adoption in
diverse residential contexts. With the growth of Smart and Connected
Communities, generative AI, especially large language models (LLMs), may help
by processing contextual information and producing practitioner readable
recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok,
Llama, and Claude) on residential retrofit decisions under two objectives:
maximizing CO2 reduction (technical) and minimizing payback period
(sociotechnical). Performance is assessed on four dimensions: accuracy,
consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49
US states. LLMs generate effective recommendations in many cases, reaching up
to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning.
Performance is stronger for the technical objective, while sociotechnical
decisions are limited by economic trade offs and local context. Agreement
across models is low, and higher performing models tend to diverge from others.
LLMs are sensitive to location and building geometry but less sensitive to
technology and occupant behavior. Most models show step by step, engineering
style reasoning, but it is often simplified and lacks deeper contextual
awareness. Overall, LLMs are promising assistants for energy retrofit decision
making, but improvements in accuracy, consistency, and context handling are
needed for reliable practice.

</details>


### [47] [Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation](https://arxiv.org/abs/2509.06337)
*Jianpeng Zhao,Chenyu Yuan,Weiming Luo,Haoling Xie,Guangwei Zhang,Steven Jige Quan,Zixuan Yuan,Pengyang Wang,Denghui Zhang*

Main category: cs.AI

TL;DR: 本文探索使用大语言模型模拟虚拟调查受访者的新范式，提出了PAS和FAS两种模拟设置，并构建了LLM-S^3基准套件来评估LLM在生成准确人口统计响应方面的能力。


<details>
  <summary>Details</summary>
Motivation: 传统问卷调查方法成本高、耗时长且规模有限，需要寻找更高效、可扩展的替代方案来支持社会科学研究和政策制定。

Method: 提出了Partial Attribute Simulation (PAS)和Full Attribute Simulation (FAS)两种模拟设置，使用GPT-3.5/4 Turbo、LLaMA 3.0/3.1-8B等主流LLM，在11个真实世界公共数据集上进行评估。

Result: 研究揭示了预测性能的一致趋势，识别了失败模式，并证明了上下文和提示设计对模拟保真度的影响。

Conclusion: 这项工作为LLM驱动的调查模拟奠定了严格基础，为社会学研究和政策评估提供了可扩展且经济高效的工具。

Abstract: Questionnaire-based surveys are foundational to social science research and
public policymaking, yet traditional survey methods remain costly,
time-consuming, and often limited in scale. This paper explores a new paradigm:
simulating virtual survey respondents using Large Language Models (LLMs). We
introduce two novel simulation settings, namely Partial Attribute Simulation
(PAS) and Full Attribute Simulation (FAS), to systematically evaluate the
ability of LLMs to generate accurate and demographically coherent responses. In
PAS, the model predicts missing attributes based on partial respondent
profiles, whereas FAS involves generating complete synthetic datasets under
both zero-context and context-enhanced conditions. We curate a comprehensive
benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey
Simulation), that spans 11 real-world public datasets across four sociological
domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA
3.0/3.1-8B) reveals consistent trends in prediction performance, highlights
failure modes, and demonstrates how context and prompt design impact simulation
fidelity. This work establishes a rigorous foundation for LLM-driven survey
simulations, offering scalable and cost-effective tools for sociological
research and policy evaluation. Our code and dataset are available at:
https://github.com/dart-lab-research/LLM-S-Cube-Benchmark

</details>


### [48] [Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent](https://arxiv.org/abs/2509.06341)
*Issue Yishu Wang,Kakam Chong,Xiaofeng Wang,Xu Yan,DeXin Kong,Chen Ju,Ming Chen,Shuai Xiao,Shuguang Han,jufeng chen*

Main category: cs.AI

TL;DR: 提出了一个基于心理理论的多轮电子商务议价评估框架，用于测试卖家代理在长期谈判中提取和跟踪买家意图的能力，并构建了大规模基准数据集。


<details>
  <summary>Details</summary>
Motivation: 在线二手市场中，多轮议价是买卖双方互动的关键环节。LLM作为卖家代理需要准确理解和跟踪买家在长期谈判中的累积意图，这对议价效果至关重要。

Method: 1) 构建大规模电子商务议价基准数据集（622个类别，9,892个产品，3,014个任务）；2) 开发基于心理理论的轮级评估框架，包含标注的买家意图；3) 建立从海量对话数据中提取可靠意图的自动化流程。

Result: 提出了一个超越仅结果指标的多轮议价评估框架，能够系统性地测试卖家代理的意图理解和跟踪能力。

Conclusion: 该研究为电子商务议价对话中的卖家代理评估提供了全面的框架和方法，特别强调了意图跟踪在多轮谈判中的重要性。

Abstract: In online second-hand marketplaces, multi-turn bargaining is a crucial part
of seller-buyer interactions. Large Language Models (LLMs) can act as seller
agents, negotiating with buyers on behalf of sellers under given business
constraints. A critical ability for such agents is to track and accurately
interpret cumulative buyer intents across long negotiations, which directly
impacts bargaining effectiveness. We introduce a multi-turn evaluation
framework for measuring the bargaining ability of seller agents in e-commerce
dialogues. The framework tests whether an agent can extract and track buyer
intents. Our contributions are: (1) a large-scale e-commerce bargaining
benchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a
turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated
buyer intents, moving beyond outcome-only metrics; and (3) an automated
pipeline that extracts reliable intent from massive dialogue data.

</details>


### [49] [A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research](https://arxiv.org/abs/2509.06355)
*Yunzhe Wang,Volkan Ustun,Chris McGroarty*

Main category: cs.AI

TL;DR: DECOY是一个新颖的多智能体模拟器，将3D地形中的战略长期规划抽象为高级离散化模拟，同时保持低级环境保真度。使用CS:GO作为测试平台，仅通过移动决策来准确模拟游戏玩法。


<details>
  <summary>Details</summary>
Motivation: 现代复杂多智能体交互模拟环境需要在高度保真细节和计算效率之间取得平衡。需要开发既能保持环境真实性又能高效运行的模拟器来推进战略多智能体规划研究。

Method: 采用路径点系统简化和离散化连续状态和动作，配合基于真实CS:GO比赛数据训练的神经预测和生成模型来重建事件结果。仅使用移动决策作为战术定位，不显式建模低级机制如瞄准和射击。

Result: 广泛评估表明，从人类数据生成的DECOY回放与原始游戏中观察到的回放高度匹配。模拟环境能够准确重现游戏玩法。

Conclusion: DECOY提供了一个有价值的工具，可用于推进战略多智能体规划和行为生成的研究，在保持计算效率的同时实现了高保真度的模拟。

Abstract: Modern simulation environments for complex multi-agent interactions must
balance high-fidelity detail with computational efficiency. We present DECOY, a
novel multi-agent simulator that abstracts strategic, long-horizon planning in
3D terrains into high-level discretized simulation while preserving low-level
environmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a
testbed, our framework accurately simulates gameplay using only movement
decisions as tactical positioning -- without explicitly modeling low-level
mechanics such as aiming and shooting. Central to our approach is a waypoint
system that simplifies and discretizes continuous states and actions, paired
with neural predictive and generative models trained on real CS:GO tournament
data to reconstruct event outcomes. Extensive evaluations show that replays
generated from human data in DECOY closely match those observed in the original
game. Our publicly available simulation environment provides a valuable tool
for advancing research in strategic multi-agent planning and behavior
generation.

</details>


### [50] [Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning](https://arxiv.org/abs/2509.06409)
*Yihong Luo,Wenwu He,Zhuo-Xu Cui,Dong Liang*

Main category: cs.AI

TL;DR: DiagCoT是一个多阶段框架，通过监督微调通用视觉语言模型，仅使用自由文本报告模拟放射科医生的逐步诊断推理，在疾病分类、病理定位和报告生成方面显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 开发可解释且具备诊断能力的AI系统，通过利用非结构化临床叙述作为监督信号，模拟放射科医生的逐步推理过程。

Method: 结合对比图像-报告调优进行领域对齐，使用思维链监督捕获推理逻辑，并通过临床奖励信号进行强化调优以提高事实准确性和流畅性。

Result: 在MIMIC-CXR基准测试中，零样本疾病分类AUC从0.52提升至0.76，病理定位mIoU从0.08提升至0.31，报告生成BLEU从0.11提升至0.33，在长尾疾病和外部数据集上优于最先进模型。

Conclusion: DiagCoT通过将非结构化临床叙述转化为结构化监督，为开发可解释且具备诊断能力的放射学AI系统提供了可扩展的方法。

Abstract: This study presents DiagCoT, a multi-stage framework that applies supervised
fine-tuning to general-purpose vision-language models (VLMs) to emulate
radiologists' stepwise diagnostic reasoning using only free-text reports.
DiagCoT combines contrastive image-report tuning for domain alignment,
chain-of-thought supervision to capture inferential logic, and reinforcement
tuning with clinical reward signals to enhance factual accuracy and fluency. On
the MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC
from 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08
to 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33
(absolute gain of 0.22). It outperformed state-of-the-art models including
LLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By
converting unstructured clinical narratives into structured supervision,
DiagCoT offers a scalable approach for developing interpretable and
diagnostically competent AI systems for radiology.

</details>


### [51] [Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning](https://arxiv.org/abs/2509.06436)
*Song Yu,Xiaofei Xu,Ke Deng,Li Li,Lin Tian*

Main category: cs.AI

TL;DR: 基于多气组树状结构的协作推理框架TOA，通过分块处理长上下文输入，动态交换信息，有效解决LLM在长上下文任务中的"中间信息漏失"问题


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长上下文任务中存在的"中间信息漏失"问题，避免现有方法中的关键信息丢弃或注意力分散问题

Method: 提出树状气组(TOA)框架，将输入分成块由独立气组处理，每个气组生成局部认知，然后沿树状结构动态交换信息进行协作推理，使用前缀哈希缓存和适应性剪枝策略提高效率

Result: 基于LLaMA3.1-8B的TOA在各种长上下文任务上显著超过多个基线方法，与Gemini1.5-pro等更大的商业模型得到相当的性能，同时保持相当的API开销

Conclusion: TOA框架通过多气组协作推理有效减轻位置偏差和幻觉问题，在保持高效的同时显著提升了长上下文处理能力

Abstract: Large language models (LLMs) face persistent challenges when handling
long-context tasks, most notably the lost in the middle issue, where
information located in the middle of a long input tends to be underutilized.
Some existing methods that reduce input have the risk of discarding key
information, while others that extend context windows often lead to attention
dispersion. To address these limitations, we propose Tree of Agents (TOA), a
multi-agent reasoning framework that segments the input into chunks processed
by independent agents. Each agent generates its local cognition, then agents
dynamically exchange information for collaborative reasoning along
tree-structured paths. TOA enables agents to probe different reasoning orders
for multi-perspective understanding, effectively mitigating position bias and
reducing hallucinations. To improve processing efficiency, we incorporate
prefix-hash caching and adaptive pruning strategies, achieving significant
performance improvements with comparable API overhead. Experiments show that
TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple
baselines and demonstrates comparable performance to the latest and much larger
commercial models, such as Gemini1.5-pro, on various long-context tasks. Code
is available at https://github.com/Aireduce952/Tree-of-Agents.

</details>


### [52] [HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data](https://arxiv.org/abs/2509.06444)
*Cheng Qian,Hainan Zhang,Yongxin Tong,Hong-Wei Zheng,Zhiming Zheng*

Main category: cs.AI

TL;DR: HyFedRAG是一个联邦RAG框架，用于处理医疗领域异构隐私数据，通过边缘-云协作机制支持SQL、知识图谱和文档等多种数据格式，在保护隐私的同时提升检索质量和系统效率。


<details>
  <summary>Details</summary>
Motivation: 集中式RAG系统在处理异构和隐私敏感的医疗数据时面临困难，特别是在分布式医疗环境中，患者数据分散在SQL、知识图谱和临床笔记等多种格式中，且存在隐私约束和边缘设备处理能力限制的问题。

Method: 设计基于Flower的边缘-云协作RAG框架，使用边缘端LLM将异构数据转换为标准化隐私保护表示，服务器端LLM进行全局推理；集成轻量级本地检索器和隐私感知LLM，提供三种匿名化工具；设计三层缓存策略优化延迟和计算效率。

Result: 在PMC-Patients数据集上的实验结果表明，HyFedRAG在检索质量、生成一致性和系统效率方面优于现有基线方法。

Conclusion: 该框架为结构化异构数据上的RAG提供了可扩展且符合隐私要求的解决方案，在敏感和多样化数据环境中释放了LLMs的潜力。

Abstract: Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive
data, especially in distributed healthcare settings where patient data spans
SQL, knowledge graphs, and clinical notes. Clinicians face difficulties
retrieving rare disease cases due to privacy constraints and the limitations of
traditional cloud-based RAG systems in handling diverse formats and edge
devices. To address this, we introduce HyFedRAG, a unified and efficient
Federated RAG framework tailored for Hybrid data modalities. By leveraging an
edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across
diverse data sources while preserving data privacy. Our key contributions are:
(1) We design an edge-cloud collaborative RAG framework built on Flower, which
supports querying structured SQL data, semi-structured knowledge graphs, and
unstructured documents. The edge-side LLMs convert diverse data into
standardized privacy-preserving representations, and the server-side LLMs
integrates them for global reasoning and generation. (2) We integrate
lightweight local retrievers with privacy-aware LLMs and provide three
anonymization tools that enable each client to produce semantically rich,
de-identified summaries for global inference across devices. (3) To optimize
response latency and reduce redundant computation, we design a three-tier
caching strategy consisting of local cache, intermediate representation cache,
and cloud inference cache. Experimental results on PMC-Patients demonstrate
that HyFedRAG outperforms existing baselines in terms of retrieval quality,
generation consistency, and system efficiency. Our framework offers a scalable
and privacy-compliant solution for RAG over structural-heterogeneous data,
unlocking the potential of LLMs in sensitive and diverse data environments.

</details>


### [53] [Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set](https://arxiv.org/abs/2509.06463)
*Chengwei Wu,Li Du,Hanyu Zhao,Yiming Ju,Jiapu Wang,Tengfei Pan*

Main category: cs.AI

TL;DR: 本文提出了一种新的指令数据选择方法，通过最大化指令深度和语义覆盖来持续提升大语言模型在下游任务中的对齐性能，实现"加速扩展"。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在下游任务中的应用需求增长，提高模型对齐性能和效率变得至关重要。但由于指令集分布的复杂性，当前方法在指令池不断扩大时无法持续提升性能。

Method: 首先研究指令数据集分布与对齐模型性能关系的关键因素，发现指令深度和语义空间覆盖是决定性因素。然后设计指令选择算法，同时最大化所选指令的深度和语义覆盖。

Result: 实验结果表明，相比最先进的基线方法，该方法能够以更快的速度持续提升模型性能，可解释开发集上70%以上的模型损失。

Conclusion: 指令深度和语义覆盖是影响对齐模型性能的关键因素，提出的指令选择方法能够实现可持续的性能提升和加速扩展。

Abstract: With the growing demand for applying large language models to downstream
tasks, improving model alignment performance and efficiency has become crucial.
Such a process involves selecting informative instructions from a candidate
pool. However, due to the complexity of instruction set distributions, the key
factors driving the performance of aligned models remain unclear. As a result,
current instruction set refinement methods fail to improve performance as the
instruction pool expands continuously. To address this issue, we first
investigate the key factors that influence the relationship between instruction
dataset distribution and aligned model performance. Based on these insights, we
propose a novel instruction data selection method. We identify that the depth
of instructions and the coverage of the semantic space are the crucial factors
determining downstream performance, which could explain over 70\% of the model
loss on the development set. We then design an instruction selection algorithm
to simultaneously maximize the depth and semantic coverage of the selected
instructions. Experimental results demonstrate that, compared to
state-of-the-art baseline methods, it can sustainably improve model performance
at a faster pace and thus achieve \emph{``Accelerated Scaling''}.

</details>


### [54] [MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents](https://arxiv.org/abs/2509.06477)
*Pengxiang Zhao,Guangyi Liu,Yaozhen Liang,Weiqing He,Zhengxi Lu,Yuehao Huang,Yaxuan Guo,Kexin Zhang,Hao Wang,Liang Liu,Yong Liu*

Main category: cs.AI

TL;DR: MAS-Bench是一个用于评估GUI-快捷方式混合智能代理的基准测试，专注于移动领域，包含139个复杂任务、88个预定义快捷方式和7个评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性评估GUI操作与快捷方式（如API、深度链接）混合代理的框架，需要填补这一空白以提升GUI代理在各种平台上的效率。

Method: 构建包含139个复杂任务、11个真实应用、88个预定义快捷方式知识库的基准测试，评估代理自主生成快捷方式和创建可重用低成本工作流的能力。

Result: 实验表明混合代理比纯GUI代理获得显著更高的成功率和效率，证明了评估代理快捷方式生成能力的有效性。

Conclusion: MAS-Bench填补了关键评估空白，为创建更高效、更鲁棒的智能代理提供了基础平台。

Abstract: To enhance the efficiency of GUI agents on various platforms like smartphones
and computers, a hybrid paradigm that combines flexible GUI operations with
efficient shortcuts (e.g., API, deep links) is emerging as a promising
direction. However, a framework for systematically benchmarking these hybrid
agents is still underexplored. To take the first step in bridging this gap, we
introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut
hybrid agents with a specific focus on the mobile domain. Beyond merely using
predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously
generate shortcuts by discovering and creating reusable, low-cost workflows. It
features 139 complex tasks across 11 real-world applications, a knowledge base
of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation
metrics. The tasks are designed to be solvable via GUI-only operations, but can
be significantly accelerated by intelligently embedding shortcuts. Experiments
show that hybrid agents achieve significantly higher success rates and
efficiency than their GUI-only counterparts. This result also demonstrates the
effectiveness of our method for evaluating an agent's shortcut generation
capabilities. MAS-Bench fills a critical evaluation gap, providing a
foundational platform for future advancements in creating more efficient and
robust intelligent agents.

</details>


### [55] [MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization](https://arxiv.org/abs/2509.06490)
*Niki Kotecha,Ehecatl Antonio del Rio Chanona*

Main category: cs.AI

TL;DR: 提出结合强化学习和多目标进化算法的方法，用于供应链动态多目标优化，通过CVaR引入风险敏感决策，在库存管理案例中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统多目标优化方法难以适应供应链的动态特性，需要实时平衡成本降低、服务水平提升和环境可持续性等冲突目标

Method: 结合强化学习(RL)和多目标进化算法(MOEAs)，利用MOEAs搜索策略神经网络参数空间生成Pareto前沿策略集，引入条件风险价值(CVaR)进行风险敏感决策

Result: 在案例研究中展示了应对供应链动态变化的能力，在库存管理案例中优于最先进方法

Conclusion: 该方法不仅提高了决策效率，还为管理不确定性和优化供应链性能提供了更稳健的框架

Abstract: In supply chain management, decision-making often involves balancing multiple
conflicting objectives, such as cost reduction, service level improvement, and
environmental sustainability. Traditional multi-objective optimization methods,
such as linear programming and evolutionary algorithms, struggle to adapt in
real-time to the dynamic nature of supply chains. In this paper, we propose an
approach that combines Reinforcement Learning (RL) and Multi-Objective
Evolutionary Algorithms (MOEAs) to address these challenges for dynamic
multi-objective optimization under uncertainty. Our method leverages MOEAs to
search the parameter space of policy neural networks, generating a Pareto front
of policies. This provides decision-makers with a diverse population of
policies that can be dynamically switched based on the current system
objectives, ensuring flexibility and adaptability in real-time decision-making.
We also introduce Conditional Value-at-Risk (CVaR) to incorporate
risk-sensitive decision-making, enhancing resilience in uncertain environments.
We demonstrate the effectiveness of our approach through case studies,
showcasing its ability to respond to supply chain dynamics and outperforming
state-of-the-art methods in an inventory management case study. The proposed
strategy not only improves decision-making efficiency but also offers a more
robust framework for managing uncertainty and optimizing performance in supply
chains.

</details>


### [56] [Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers](https://arxiv.org/abs/2509.06493)
*Ran Xin,Zeyu Zheng,Yanchen Nie,Kun Yuan,Xia Xiao*

Main category: cs.AI

TL;DR: BFS-Prover-V2是一个结合多轮离线强化学习和规划增强多智能体搜索的系统，在自动定理证明领域实现了最先进的性能


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在自动定理证明中训练时强化学习和推理时计算扩展的双重挑战

Method: 1) 多轮离线强化学习框架，采用多阶段专家迭代流程；2) 规划增强的多智能体搜索架构，使用通用推理模型作为高层规划器分解复杂定理

Result: 在MiniF2F和ProofNet测试集上分别达到95.08%和41.4%的准确率

Conclusion: 该系统在形式数学领域取得突破性成果，其强化学习和推理技术可推广到其他需要长程多轮推理和复杂搜索的领域

Abstract: The integration of Large Language Models (LLMs) into automated theorem
proving has shown immense promise, yet is fundamentally constrained by
challenges in scaling up both training-time reinforcement learning (RL) and
inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system
designed to address this dual scaling problem. We present two primary
innovations. The first is a novel multi-turn off-policy RL framework for
continually improving the performance of LLM step-prover at training time. This
framework, inspired by the principles of AlphaZero, utilizes a multi-stage
expert iteration pipeline featuring adaptive tactic-level data filtering and
periodic retraining to surmount the performance plateaus that typically curtail
long-term RL in LLM-based agents. The second innovation is a planner-enhanced
multi-agent search architecture that scales reasoning capabilities at inference
time. This architecture employs a general reasoning model as a high-level
planner to iteratively decompose complex theorems into a sequence of simpler
subgoals. This hierarchical approach substantially reduces the search space,
enabling a team of parallel prover agents to collaborate efficiently by
leveraging a shared proof cache. We demonstrate that this dual approach to
scaling yields state-of-the-art results on established formal mathematics
benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F
and ProofNet test sets respectively. While demonstrated in the domain of formal
mathematics, the RL and inference techniques presented in this work are of
broader interest and may be applied to other domains requiring long-horizon
multi-turn reasoning and complex search.

</details>


### [57] [An AI system to help scientists write expert-level empirical software](https://arxiv.org/abs/2509.06503)
*Eser Aygün,Anastasiya Belyaeva,Gheorghe Comanici,Marc Coram,Hao Cui,Jake Garrison,Renee Johnston Anton Kast,Cory Y. McLean,Peter Norgaard,Zahra Shamsi,David Smalling,James Thompson,Subhashini Venugopalan,Brian P. Williams,Chujun He,Sarah Martinson,Martyna Plomecka,Lai Wei,Yuchen Zhou,Qian-Ze Zhu,Matthew Abraham,Erica Brand,Anna Bulanova,Jeffrey A. Cardille,Chris Co,Scott Ellsworth,Grace Joseph,Malcolm Kane,Ryan Krueger,Johan Kartiwa,Dan Liebling,Jan-Matthis Lueckmann,Paul Raccuglia,Xuefei,Wang,Katherine Chou,James Manyika,Yossi Matias,John C. Platt,Lizzie Dorfman,Shibl Mourad,Michael P. Brenner*

Main category: cs.AI

TL;DR: AI系统使用大语言模型和树搜索自动生成专家级科学软件，在多个领域超越人类开发的方法，显著加速科学发现进程


<details>
  <summary>Details</summary>
Motivation: 科学发现过程经常受限于手动创建计算实验软件的缓慢速度，需要自动化工具来加速科学进步

Method: 结合大型语言模型(LLM)和树搜索(TS)技术，系统性地改进质量指标并智能导航解决方案空间

Result: 在生物信息学中发现40种优于人类方法的新单细胞数据分析方法；在流行病学中生成14个超越CDC集成模型的COVID-19住院预测模型；在多个领域产生最先进软件

Conclusion: 该系统通过为多样化任务设计和实施新颖解决方案，代表了加速科学进步的重要一步

Abstract: The cycle of scientific discovery is frequently bottlenecked by the slow,
manual creation of software to support computational experiments. To address
this, we present an AI system that creates expert-level scientific software
whose goal is to maximize a quality metric. The system uses a Large Language
Model (LLM) and Tree Search (TS) to systematically improve the quality metric
and intelligently navigate the large space of possible solutions. The system
achieves expert-level results when it explores and integrates complex research
ideas from external sources. The effectiveness of tree search is demonstrated
across a wide range of benchmarks. In bioinformatics, it discovered 40 novel
methods for single-cell data analysis that outperformed the top human-developed
methods on a public leaderboard. In epidemiology, it generated 14 models that
outperformed the CDC ensemble and all other individual models for forecasting
COVID-19 hospitalizations. Our method also produced state-of-the-art software
for geospatial analysis, neural activity prediction in zebrafish, time series
forecasting and numerical solution of integrals. By devising and implementing
novel solutions to diverse tasks, the system represents a significant step
towards accelerating scientific progress.

</details>


### [58] [CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning](https://arxiv.org/abs/2509.06641)
*Zhou-Peng Shou,Zhi-Qiang You,Fang Wang,Hai-Bo Liu*

Main category: cs.AI

TL;DR: 本文提出一种基于"意图草稿"的零检出多模态推理组件，通过三模块流水线实现人类认知过程，解决多模态大模型复杂推理中的"短接"和上下文理解不充分问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型在复杂跨模态推理中存在的"短接"问题和上下文理解不充分的问题，提高模型的深度理解能力。

Method: 设计了一个插拔即用的三模块流水线：意图感知器、策略生成器和策略选择器，通过生成和筛选"意图草稿"策略来指导最终推理，无需参数微调。

Result: 在IntentBench、WorldSense和Daily-Omni数据集上验证了方法的普适性和稳健收益，相比各自基线完整的"三模块"方案在不同推理引擎和流水线组合中均有一致收益，最高提升约9.51个百分点。

Conclusion: 信息论分析显示该过程能够降低条件熵和提高信息利用效率，压制意外的短接推理，证明了"意图草稿"推理组件在零检出场景中的实际价值和可移植性。

Abstract: Targeting the issues of "shortcuts" and insufficient contextual understanding
in complex cross-modal reasoning of multimodal large models, this paper
proposes a zero-shot multimodal reasoning component guided by human-like
cognitive strategies centered on an "intent sketch". The component comprises a
plug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and
Strategy Selector-that explicitly constructs a "understand-plan-select"
cognitive process. By generating and filtering "intent sketch" strategies to
guide the final reasoning, it requires no parameter fine-tuning and achieves
cross-model transfer solely through in-context engineering.
Information-theoretic analysis shows that this process can reduce conditional
entropy and improve information utilization efficiency, thereby suppressing
unintended shortcut reasoning. Experiments on IntentBench, WorldSense, and
Daily-Omni validate the method's generality and robust gains; compared with
their respective baselines, the complete "three-module" scheme yields
consistent improvements across different reasoning engines and pipeline
combinations, with gains up to approximately 9.51 percentage points,
demonstrating the practical value and portability of the "intent sketch"
reasoning component in zero-shot scenarios.

</details>


### [59] [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)
*Wenjun Li,Zhi Chen,Jingru Lin,Hannan Cao,Wei Han,Sheng Liang,Zhi Zhang,Kuicai Dong,Dexun Li,Chen Zhang,Yong Liu*

Main category: cs.AI

TL;DR: 这篇论文是第一篇专注于深度研究系统强化学习基础的综述，系统化地分析了DeepSeek-R1之后的工作，涵盖数据合成、RL方法、训练系统、架构协调和评估基准等方面。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究系统主要使用SFT和DPO方法，但这些方法存在模仿偏差、暴露偏差、模式依赖等问题，且依赖人工定义的决策点和子技能。强化学习能够通过优化轨迹级策略，实现探索、恢复行为和原则性信用分配，减少对人类先验和评分者偏见的依赖。

Method: 论文从三个维度系统化分析工作：(i)数据合成与整理；(ii)面向智能体研究的RL方法，包括稳定性、样本效率、长上下文处理、奖励与信用设计、多目标优化和多模态集成；(iii)智能体RL训练系统和框架。同时涵盖智能体架构协调以及评估基准。

Result: 论文提炼了重复出现的模式，揭示了基础设施瓶颈，并为使用RL训练鲁棒、透明的深度研究智能体提供了实用指导。

Conclusion: 强化学习为深度研究系统提供了更优的解决方案，能够更好地处理工具交互研究中的闭环优化问题，减少对人类先验知识的依赖，是未来发展的重点方向。

Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by
coordinating reasoning, search across the open web and user files, and tool
use, are moving toward hierarchical deployments with a Planner, Coordinator,
and Executors. In practice, training entire stacks end-to-end remains
impractical, so most work trains a single planner connected to core tools such
as search, browsing, and code. While SFT imparts protocol fidelity, it suffers
from imitation and exposure biases and underuses environment feedback.
Preference alignment methods such as DPO are schema and proxy-dependent,
off-policy, and weak for long-horizon credit assignment and multi-objective
trade-offs. A further limitation of SFT and DPO is their reliance on human
defined decision points and subskills through schema design and labeled
comparisons. Reinforcement learning aligns with closed-loop, tool-interaction
research by optimizing trajectory-level policies, enabling exploration,
recovery behaviors, and principled credit assignment, and it reduces dependence
on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations
of deep research systems. It systematizes work after DeepSeek-R1 along three
axes: (i) data synthesis and curation; (ii) RL methods for agentic research
covering stability, sample efficiency, long context handling, reward and credit
design, multi-objective optimization, and multimodal integration; and (iii)
agentic RL training systems and frameworks. We also cover agent architecture
and coordination, as well as evaluation and benchmarks, including recent QA,
VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We
distill recurring patterns, surface infrastructure bottlenecks, and offer
practical guidance for training robust, transparent deep research agents with
RL.

</details>


### [60] [VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction](https://arxiv.org/abs/2509.06736)
*Jie Yang,Jiajun Chen,Zhangyue Yin,Shuo Chen,Yuxin Wang,Yiran Guo,Yuan Li,Yining Zheng,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的State-based Function Call (SFC)方法，通过维护显式系统状态意识和直接状态迁移，在车辆智能座舱环境中显著提升了执行准确性和减少延迟。


<details>
  <summary>Details</summary>
Motivation: 传统函数调用(FC)方法在车辆智能座舱这种复杂环境中存在效率低下、错误恢复能力尾等问题，需要多次探索性调用来构建环境意识，影响了性能。

Method: 研究者首先构建了VehicleWorld环境，包含30个模块、250个API和680个属性。通过系统分析发现直接状态预测在环境控制中更优称，从而提出SFC方法，维护显式系统状态意识并实现直接状态迁移。

Result: 实验结果显示，SFC方法显著超过传统FC方法，实现了更高的执行准确性和更低的延迟。

Conclusion: 这项研究为车辆智能座舱的API代理提供了更高效的方案，SFC方法通过状态基础的函数调用显著提升了系统性能，为复杂环境中的代理执行开启了新方向。

Abstract: Intelligent vehicle cockpits present unique challenges for API Agents,
requiring coordination across tightly-coupled subsystems that exceed typical
task environments' complexity. Traditional Function Calling (FC) approaches
operate statelessly, requiring multiple exploratory calls to build
environmental awareness before execution, leading to inefficiency and limited
error recovery. We introduce VehicleWorld, the first comprehensive environment
for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties
with fully executable implementations that provide real-time state information
during agent execution. This environment enables precise evaluation of vehicle
agent behaviors across diverse, challenging scenarios. Through systematic
analysis, we discovered that direct state prediction outperforms function
calling for environmental control. Building on this insight, we propose
State-based Function Call (SFC), a novel approach that maintains explicit
system state awareness and implements direct state transitions to achieve
target conditions. Experimental results demonstrate that SFC significantly
outperforms traditional FC approaches, achieving superior execution accuracy
and reduced latency. We have made all implementation code publicly available on
Github https://github.com/OpenMOSS/VehicleWorld.

</details>


### [61] [Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting](https://arxiv.org/abs/2509.06770)
*Shashidhar Reddy Javaji,Bhavul Gauri,Zining Zhu*

Main category: cs.AI

TL;DR: 这篇论文提出了一个评估迭代精炼的框架，用于测量大语言模型在多轮工作流中的表现，包括思想创造、代码和数学领域，通过控制对话和多种提示策略来分析迭代的效果。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型已经被广泛使用于多轮工作流，但我们仍缺乏明确的方法来衡量迭代在什么情况下有帮助以及在什么情况下有害，需要一个统一的评估框架来进行系统性分析。

Method: 研究设计了一个协议，在每个任务中运行12轮的控制对话，使用从模糊"改进它"反馈到有目标导航的各种提示策略，并记录每轮输出。通过领域适宜的检查来评分结果（代码的单元测试；数学的答案等效性加推理合理性；思想创造的原创性和可行性），并使用三类指标跟踪轮次行为：语义移动、轮间变化和输出大小增长。

Result: 在不同模型和任务中，收益具有领域依赖性：在思想和代码中收益来得早，而在数学中后期轮次在详细指导下重要。在前几轮之后，模糊的反馈常常停滞或逆转正确性，而有目标的提示可靠地推动领域特定的质量轴（思想创造中的新颖性vs可行性；代码中的速度vs可读性；数学中详细说明超过探索并驱动后期收益）。还观察到一致的领域模式：思想创造在轮次间语义移动更多，代码往往输出大小增长但语义变化少，数学初始固定但可通过后期详细迭代突破这一路径。

Conclusion: 该框架和指标使得迭代可以在不同模型之间进行测量和比较，并为何时导航、停止或切换策略提供信号，为评估和优化大语言模型的迭代精炼能力提供了重要工具。

Abstract: Large language models (LLMs) are now used in multi-turn workflows, but we
still lack a clear way to measure when iteration helps and when it hurts. We
present an evaluation framework for iterative refinement that spans ideation,
code, and math. Our protocol runs controlled 12-turn conversations per task,
utilizing a variety of prompts ranging from vague ``improve it'' feedback to
targeted steering, and logs per-turn outputs. We score outcomes with
domain-appropriate checks (unit tests for code; answer-equivalence plus
reasoning-soundness for math; originality and feasibility for ideation) and
track turn-level behavior with three families of metrics: semantic movement
across turns, turn-to-turn change, and output size growth. Across models and
tasks, gains are domain-dependent: they arrive early in ideas and code, but in
math late turns matter when guided by elaboration. After the first few turns,
vague feedback often plateaus or reverses correctness, while targeted prompts
reliably shift the intended quality axis (novelty vs. feasibility in ideation;
speed vs. readability in code; in math, elaboration outperforms exploration and
drives late-turn gains). We also observe consistent domain patterns: ideation
moves more in meaning across turns, code tends to grow in size with little
semantic change, and math starts fixed but can break that path with late,
elaborative iteration.Together, the framework and metrics make iteration
measurable and comparable across models, and signal when to steer, stop, or
switch strategies.

</details>


### [62] [RAFFLES: Reasoning-based Attribution of Faults for LLM Systems](https://arxiv.org/abs/2509.06822)
*Chenyang Zhu,Spencer Hong,Jingyu Wu,Kushal Chawla,Charlotte Tang,Youbing Yin,Nathan Wolfe,Erin Babinsky,Daben Liu*

Main category: cs.AI

TL;DR: RAFFLES是一个用于评估长时域多组件LLM代理系统的迭代式评估架构，通过推理和迭代精化来识别系统故障点和原因，在Who&When基准测试中显著超越了现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前的长时域多组件LLM代理系统评估方法存在局限，主要关注单一指标或端到端结果，难以识别系统故障的具体位置和原因。需要能够推理、探测、迭代并理解系统复杂逻辑的评估框架。

Method: RAFFLES采用迭代式多组件流水线架构，包含一个中央Judge系统性地调查故障，以及一组专门的Evaluators评估系统组件和Judge自身的推理质量，构建假设历史。

Result: 在Who&When数据集上，RAFFLES在算法生成数据集上达到43%以上的代理-步骤故障对准确率（相比之前最佳16.6%有显著提升），在手工制作数据集上达到20%以上（超越之前最佳8.8%）。

Conclusion: RAFFLES展示了向自主系统引入自动化故障检测的重要进展，相比劳动密集型人工审查具有明显优势，为长时域多组件LLM代理系统的评估提供了有效解决方案。

Abstract: We have reached a critical roadblock in the development and enhancement of
long-horizon, multi-component LLM agentic systems: it is incredibly tricky to
identify where these systems break down and why. Evaluation capabilities that
currently exist today (e.g., single pass LLM-as-a-judge) are limited in that
they often focus on individual metrics or capabilities, end-to-end outcomes,
and are narrowly grounded on the preferences of humans. We argue that to match
the agentic capabilities, evaluation frameworks must also be able to reason,
probe, iterate, and understand the complex logic passing through these systems
over long horizons. In this paper, we present RAFFLES - an evaluation
architecture that incorporates reasoning and iterative refinement.
Specifically, RAFFLES operates as an iterative, multi-component pipeline, using
a central Judge to systematically investigate faults and a set of specialized
Evaluators to assess not only the system's components but also the quality of
the reasoning by the Judge itself, thereby building a history of hypotheses. We
tested RAFFLES against several baselines on the Who&When dataset, a benchmark
designed to diagnose the "who" (agent) and "when" (step) of a system's failure.
RAFFLES outperforms these baselines, achieving an agent-step fault pair
accuracy of over 43% on the Algorithmically-Generated dataset (a substantial
increase from the previously published best of 16.6%) and over 20% on the
Hand-Crafted dataset (surpassing the previously published best of 8.8%). These
results demonstrate a key step towards introducing automated fault detection
for autonomous systems over labor-intensive manual human review.

</details>


### [63] [Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet](https://arxiv.org/abs/2509.06861)
*James Xu Zhao,Bryan Hooi,See-Kiong Ng*

Main category: cs.AI

TL;DR: 该研究评估了测试时扩展（test-time scaling）在知识密集任务中的效果，发现增加推理链长度并不能持续提高准确性，反而可能导更多幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展技术在多个领域表现优异，但在知识密集任务中的效果仍不明确，需要评估其对事实准确性和幻觉率的影响。

Method: 使用12个推理模型在2个知识密集标准测试集上进行综合评估，分析测试时计算量增加对准确性和幻觉行为的影响。

Result: 增加测试时计算并不能持续提高准确性，反而在许多情况下会导致更多幻觉。模型通过更长思考后选择放弃回答，而非改善事实记忆。某些模型会尝试回答之前未回答的问题，但很多导致幻觉。

Conclusion: 尽管存在限制，但与不进行思考相比，启用测试时扩展仍然有益。扩展推理可能导致确认偏见，产生过信心幻觉。

Abstract: Test-time scaling increases inference-time computation by allowing models to
generate long reasoning chains, and has shown strong performance across many
domains. However, in this work, we show that this approach is not yet effective
for knowledge-intensive tasks, where high factual accuracy and low
hallucination rates are essential. We conduct a comprehensive evaluation of
test-time scaling using 12 reasoning models on two knowledge-intensive
benchmarks. Our results reveal that increasing test-time computation does not
consistently improve accuracy and, in many cases, it even leads to more
hallucinations. We then analyze how extended reasoning affects hallucination
behavior. We find that reduced hallucinations often result from the model
choosing to abstain after thinking more, rather than from improved factual
recall. Conversely, for some models, longer reasoning encourages attempts on
previously unanswered questions, many of which result in hallucinations. Case
studies show that extended reasoning can induce confirmation bias, leading to
overconfident hallucinations. Despite these limitations, we observe that
compared to non-thinking, enabling thinking remains beneficial. Code and data
are available at https://github.com/XuZhao0/tts-knowledge

</details>


### [64] [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents](https://arxiv.org/abs/2509.06917)
*Jiacheng Miao,Joe R. Davis,Jonathan K. Pritchard,James Zou*

Main category: cs.AI

TL;DR: Paper2Agent是一个自动化框架，可将研究论文转换为AI智能体，使静态论文变成动态的交互式研究助手，通过自然语言处理复杂科学查询。


<details>
  <summary>Details</summary>
Motivation: 传统研究论文需要读者投入大量精力来理解和适应代码、数据和方法，这造成了传播和重用的障碍。Paper2Agent旨在将研究输出从被动产物转变为主动系统，加速下游使用、采用和发现。

Method: 使用多个智能体系统分析论文和相关代码库，构建模型上下文协议(MCP)服务器，然后迭代生成和运行测试来优化MCP。生成的论文MCP可以灵活连接到聊天智能体(如Claude Code)，通过自然语言执行复杂科学查询并调用原始论文的工具和工作流。

Result: 通过深入案例研究验证了有效性：创建了利用AlphaGenome解释基因组变异的智能体，以及基于ScanPy和TISSUE的单细胞和空间转录组学分析智能体。这些智能体能够复现原始论文结果并正确执行新的用户查询。

Conclusion: Paper2Agent通过将静态论文转化为动态交互式AI智能体，为知识传播引入了新范式，并为AI协作科学家生态系统奠定了基础。

Abstract: We introduce Paper2Agent, an automated framework that converts research
papers into AI agents. Paper2Agent transforms research output from passive
artifacts into active systems that can accelerate downstream use, adoption, and
discovery. Conventional research papers require readers to invest substantial
effort to understand and adapt a paper's code, data, and methods to their own
work, creating barriers to dissemination and reuse. Paper2Agent addresses this
challenge by automatically converting a paper into an AI agent that acts as a
knowledgeable research assistant. It systematically analyzes the paper and the
associated codebase using multiple agents to construct a Model Context Protocol
(MCP) server, then iteratively generates and runs tests to refine and robustify
the resulting MCP. These paper MCPs can then be flexibly connected to a chat
agent (e.g. Claude Code) to carry out complex scientific queries through
natural language while invoking tools and workflows from the original paper. We
demonstrate Paper2Agent's effectiveness in creating reliable and capable paper
agents through in-depth case studies. Paper2Agent created an agent that
leverages AlphaGenome to interpret genomic variants and agents based on ScanPy
and TISSUE to carry out single-cell and spatial transcriptomics analyses. We
validate that these paper agents can reproduce the original paper's results and
can correctly carry out novel user queries. By turning static papers into
dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for
knowledge dissemination and a foundation for the collaborative ecosystem of AI
co-scientists.

</details>


### [65] [Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference](https://arxiv.org/abs/2509.06942)
*Xiangwei Shen,Zhimin Li,Zhantao Yang,Shiyi Zhang,Yingfang Zhang,Donghao Li,Chunyu Wang,Qinglin Lu,Yansong Tang*

Main category: cs.AI

TL;DR: 直接对齐模型Direct-Align通过预定义噪声前置避免多步去噪计算，结合语义相对偏好优化SRPO实现在线奖励调整，提升了FLUX.1.dev模型的真实感和美学质量3倍以上


<details>
  <summary>Details</summary>
Motivation: 解决现有模型依赖多步去噪计算奖励的计算成本高问题，以及需要连续离线调整奖励模型才能达到期望的美学质量

Method: 提出Direct-Align方法预定义噪声前置通过插值恢复原图像，避免后期时间步的过度优化；以及SRPO技术将奖励作为文本条件信号，支持在线根据正负提示扩充进行奖励调整

Result: 对FLUX.1.dev模型进行精细调整后，人类评估的真实感和美学质量提升了3倍以上

Conclusion: 该研究通过效率更高的去噪方法和灵活的在线奖励调整机制，成功解决了流行人工智能对齐方法的计算效率和迭代灵活性问题

Abstract: Recent studies have demonstrated the effectiveness of directly aligning
diffusion models with human preferences using differentiable reward. However,
they exhibit two primary challenges: (1) they rely on multistep denoising with
gradient computation for reward scoring, which is computationally expensive,
thus restricting optimization to only a few diffusion steps; (2) they often
need continuous offline adaptation of reward models in order to achieve desired
aesthetic quality, such as photorealism or precise lighting effects. To address
the limitation of multistep denoising, we propose Direct-Align, a method that
predefines a noise prior to effectively recover original images from any time
steps via interpolation, leveraging the equation that diffusion states are
interpolations between noise and target images, which effectively avoids
over-optimization in late timesteps. Furthermore, we introduce Semantic
Relative Preference Optimization (SRPO), in which rewards are formulated as
text-conditioned signals. This approach enables online adjustment of rewards in
response to positive and negative prompt augmentation, thereby reducing the
reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model
with optimized denoising and online reward adjustment, we improve its
human-evaluated realism and aesthetic quality by over 3x.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [66] [Multiport Network Modeling and Optimization for Reconfigurable Pinching-Antenna Systems](https://arxiv.org/abs/2509.05612)
*Zhaolin Wang,Jiaqi Xu,Chongjun Ouyang,Xidong Mu,Yuanwei Liu*

Main category: cs.IT

TL;DR: 重配缩放天线系统(PASS)通过多端口网络理论建立模型，实现放射振幅和相位的全控制，并研究了理想和实际方向性耦合器基PAs的形成优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统缩放天线(PA)在放射振幅和相位控制方面有限，需要一种重配系统来扩展其功能，实现更灵活的放射特性控制。

Method: 通过多端口网络理论建立一个通用且物理一致的PASS模型，识别理想重配性的基本约束，提出实际的方向性耦合器基PA模型，并研究理想和实际情况下的材料形成优化问题。

Result: 数值结果显示：在单用户场景中，(i)优化PA位置时，性能提升主要来自振幅重配能力，DC基PAs接近理想性能；(ii)固定PA位置时，振幅和相位重配能力都很关键，DC基PAs会导致不可忽视的性能损失。

Conclusion: PASS系统通过多端网络理论建模型实现了对缩放天线放射特性的全面控制，方向性耦合器基实现在优化位置时接近理想性能，但在固定位置时需要考虑振幅和相位的共同控制。

Abstract: A reconfigurable pinching-antenna system (PASS) is presented, endowing
pinching antennas (PAs) with both amplitude- and phase-controllable radiation
beyond conventional implementations. To characterize this feature, a general
and physically consistent model is established for PASS via multiport network
theory. Within this model, the fundamental constraint of ideal
reconfigurability of PAs is identified, allowing the full control of signal
amplitudes and phases. A practical directional-coupler (DC)-based PA model is
then proposed, enabling both amplitude-only control and amplitude-constrained
phase control. Beamforming optimization is investigated for both ideal and
practical cases: an optimal solution is obtained for ideal PAs, whereas a
high-quality iterative algorithm is developed for DC-based PAs. Numerical
results suggest that in single-user scenarios: (i) with optimized PA positions,
performance gains arise primarily from amplitude reconfigurability and DC-based
PAs approach ideal performance, and (ii) with fixed PA positions, both
amplitude and phase reconfigurability are critical and DC-based PAs incur
non-negligible loss.

</details>


### [67] [Study of Iterative Detection, Decoding and Channel Estimation for RIS-Aided MIMO Networks](https://arxiv.org/abs/2509.05875)
*Roberto C. G. Porto,Rodrigo C. de Lamare*

Main category: cs.IT

TL;DR: 通过迭代检测、解码和频道估计方案，利用LDPC码和编码导频提高多RIS助多天线系统的频道估计准确性，减少导频符号需求


<details>
  <summary>Details</summary>
Motivation: 解决多反射智能表面(RIS)助力的多天线系统中，频道估计的挑战，特别是在非稀疏传播环境下减少导频符号需求的同时提高估计准确性

Method: 提出一种新题的频道估计技术，利用低密度奇偶校验(LDPC)码和迭代处理。关键思想是利用编码导频来改善迭代过程，不仅使用导频比特还利用编码包中的奇偶比特来精炼频道估计

Result: 在sub-6 GHz场景下进行了模拟，考虑了非稀疏传播、多RIS部署以及LOS和NLOS条件。数值结果显示，该方案和估计器在性能上远超竞争方案

Conclusion: 提出的迭代检测、解码和频道估计方案能够在多RIS助力的多天线系统中实现显著的性能收益，通过利用LDPC码和编码导频有效提高了频道估计的准确性并减少了导频符号的需求量

Abstract: This work proposes an iterative detection, decoding and channel estimation
scheme for multiple-antenna systems assisted by multiple reflective intelligent
surfaces (RIS). A novel channel estimation technique that exploits low-density
parity-check (LDPC) codes and iterative processing is developed to enhance
estimation accuracy while reducing the number of required pilot symbols. The
key idea is to exploit encoded pilots to improve the iterative process,
enabling the use of not only pilot bits but also parity bits from the coded
packet to refine channel estimation. Simulations analyze a sub-6 GHz scenario
where the channel propagation is not sparse and multiple RIS are deployed,
considering both LOS and NLOS conditions. Numerical results show significant
performance gains for the proposed scheme and estimator over competing
approaches.

</details>


### [68] [Beyond Diagonal IRS Aided OFDM: Rate Maximization under Frequency-Dependent Reflection](https://arxiv.org/abs/2509.06378)
*Ye Yuan,Shuowen Zhang*

Main category: cs.IT

TL;DR: 本文研究基于超对角智能反射面(BD-IRS)的宽带OFDM系统，通过联合优化可调电容矩阵和功率分配来最大化系统可达速率，提出了一种有效的交替优化和逐次凸逼近算法。


<details>
  <summary>Details</summary>
Motivation: 在实际电路结构下，BD-IRS的反射矩阵依赖于电路参数和工作频率，导致不同子载波间存在耦合，给BD-IRS设计带来新挑战。

Method: 首先建模BD-IRS反射矩阵与可调电容矩阵的关系，然后提出交替优化和逐次凸逼近算法来联合优化可调电容矩阵和OFDM子载波功率分配。

Result: 数值结果表明所提出的设计方案优于基准设计。

Conclusion: 该研究为宽带OFDM系统中BD-IRS的设计提供了有效的解决方案，能够显著提升系统性能。

Abstract: This paper studies a broadband orthogonal frequency division multiplexing
(OFDM) system aided by a beyond diagonal intelligent reflecting surface
(BD-IRS), where inter-connections exist among different elements such that the
reflection matrix can exhibit a beyond diagonal structure. Under practical
circuit structures, the reflection matrix of the BD-IRS is generally dependent
on the circuit parameters (e.g., capacitance matrix for all tunable capacitors)
as well as the operating frequency, which leads to couplings among the BD-IRS
reflection matrices over different sub-carriers and consequently new challenges
in the BD-IRS design. Motivated by this, we first model the relationship
between the BD-IRS reflection matrices over different sub-carriers and the
tunable capacitance matrix, and then formulate the joint optimization problem
of the tunable capacitance matrix and power allocation over OFDM sub-carriers
to maximize the achievable rate of the OFDM system. Despite the non-convexity
of the problem, we propose an effective algorithm for finding a high-quality
feasible solution via leveraging alternating optimization and successive convex
approximation. Numerical results show the superiority of our proposed design
over benchmark designs.

</details>


### [69] [Trace Repair Never Loses to Classical Repair: Exact and Explicit Helper Nodes Selection](https://arxiv.org/abs/2509.06492)
*Wilton Kim,Stanislav Kruglik,Han Mao Kiah*

Main category: cs.IT

TL;DR: 本文研究了Reed-Solomon码在扩展域上的追踪修复问题，精确确定了修复子空间的维度，并提供了达到最优修复带宽的显式辅助节点选择方案。


<details>
  <summary>Details</summary>
Motivation: 基于Guruswami-Wootters的追踪框架和Liu-Wan-Xing的最新工作，旨在进一步降低Reed-Solomon码修复的带宽消耗。

Method: 使用分圆陪集理论精确确定修复子空间W_k的维度，并构造显式的辅助节点集合来实现最优带宽修复。

Result: 证明了修复带宽为(n-d-1)log|B|比特，其中d=dim(W_k)，且(n-d-1)≤kt，表明追踪修复方法不劣于经典修复方案。

Conclusion: 该工作为Reed-Solomon码提供了理论上的最优修复带宽界限和具体的实现方案，在分布式存储系统的纠删码修复中具有重要应用价值。

Abstract: We study the repair of Reed--Solomon codes over $\mathbb{F}=\mathbb{B}^t$
using traces over $\mathbb{B}$. Building on the trace framework of
Guruswami--Wootters (2017), recent work of Liu--Wan--Xing (2024) reduced repair
bandwidth by studying a related subspace $\mathcal{W}_k$. In this work, we
determine the dimension of $\mathcal{W}_k$ exactly using cyclotomic cosets and
provide an explicit set of helper nodes that attains bandwidth $(n-d-1)\log
|\mathbb{B}|$ bits with $d=\text{dim}(\mathcal{W}_k)$. Moreover, we show that
$(n-d-1)\le kt$, and so, trace repair never loses to the classical repair.

</details>


### [70] [On catastrophicity of convolutional codes and their encoders over $\Z_{p^r}$](https://arxiv.org/abs/2509.06670)
*Mohammed El Oued*

Main category: cs.IT

TL;DR: 本文证明了卷积码在Z_{p^r}上存在最小p-编码器，证实了相关猜想，并提出了新的多项式不变量来刻画自由码的非灾难性条件。


<details>
  <summary>Details</summary>
Motivation: 解决关于Z_{p^r}上卷积码存在最小p-编码器的猜想，该猜想暗示当输入序列系数限制在{0,...,p-1}时，所有此类卷积码都是非灾难性的。

Method: 引入新的多项式不变量来刻画自由码，建立自由码在Z_{p^r}上非灾难性的充要条件，并提供构造性方法获得最小p-编码器。

Result: 证实了猜想，为任意Z_{p^r}上的卷积码提供了获得最小p-编码器的构造方法。

Conclusion: 所有Z_{p^r}上的卷积码都存在最小p-编码器，且在限制输入系数条件下都是非灾难性的，这通过新的多项式不变量和构造性方法得到证明。

Abstract: This paper investigates the existence of minimal $p$-encoders for
convolutional codes over $\mathbb{Z}_{p^r}$, where $p$ is a prime. This
addresses a conjecture from \cite{k}, which posits that every such code admits
a minimal $p$-encoder, implying that all convolutional codes over
$\mathbb{Z}_{p^r}$ are noncatastrophic when input sequences are restricted to
coefficients in $\{0, \dots, p-1\}$. Our contributions include the introduction
of a new polynomial invariant that characterizes free codes, which enables us
to establish a necessary and sufficient condition for a free code over
$\mathbb{Z}_{p^r}$ to be noncatastrophic in the usual sense (where input
coefficients are from $\mathbb{Z}_{p^r}$). Based on these findings, we affirm
the conjecture by providing a constructive method for obtaining a minimal
$p$-encoder for any convolutional code over $\mathbb{Z}_{p^r}$.

</details>


### [71] [Codes Correcting Transpositions of Consecutive Symbols](https://arxiv.org/abs/2509.06692)
*Mladen Kovačević,Keshav Goyal,Han Mao Kiah*

Main category: cs.IT

TL;DR: 研究q元字符串中连续符号转置错误的纠错问题，提出了多种纠错码构造方法并分析了其性能界限


<details>
  <summary>Details</summary>
Motivation: 解决q元字符串中连续符号转置（交换）错误的纠错问题，这类错误在实际通信和数据存储中经常发生

Method: 提出了一系列纠错码构造方法：1）纠正任意位置单个转置错误的码族；2）二进制字母表上的改进构造；3）纠正t个常数量转置错误的码；4）纠正τn比例转置错误的码；5）纠正所有可能转置模式的码构造

Result: 证明了所提码族具有渐近最优冗余度，获得了纠正t个转置错误的码基数界限，推导了最优码的渐近率下界，给出了q元转置信道的零错误容量下界

Conclusion: 该研究为转置错误纠错提供了系统的理论框架和实用的编码构造，建立了相关性能界限，对实际通信系统的错误控制具有重要意义

Abstract: The problem of correcting transpositions (or swaps) of consecutive symbols in
$ q $-ary strings is studied. A family of codes correcting a transposition at
an arbitrary location is described and proved to have asymptotically optimal
redundancy. Additionally, an improved construction is given over a binary
alphabet. Bounds on the cardinality of codes correcting $ t = \textrm{const} $
transpositions are obtained. A lower bound on the achievable asymptotic rate of
optimal codes correcting $ t = \tau n $ transpositions is derived. Finally, a
construction of codes correcting all possible patterns of transpositions is
presented, and the corresponding lower bound on the zero-error capacity of the
$ q $-ary transposition channel is stated.

</details>


### [72] [Rate-Optimal Streaming Codes over Three-Node Relay Networks with Burst Erasures](https://arxiv.org/abs/2509.06912)
*Zhipeng Li,Wenjie Ma,Zhifang Zhang*

Main category: cs.IT

TL;DR: 本文研究三节点中继网络中的流式编码，针对突发包擦除和延迟约束T，提出了在T≥b₁+b₂+b₁b₂/|b₁-b₂|条件下实现最优码率的流式码构造方法


<details>
  <summary>Details</summary>
Motivation: 解决三节点中继网络中突发包擦除问题，在延迟约束下实现最优码率的流式编码，扩展现有最优码率构造方法的适用范围

Method: 构造流式编码方案，针对源-中继和中继-目标信道分别存在最大长度b₁和b₂的突发擦除，在滑动窗口T+1个连续包内满足延迟约束

Result: 提出了在T≥b₁+b₂+b₁b₂/|b₁-b₂|条件下能够实现最优码率的流式码构造，丰富了最优码率流式码的家族

Conclusion: 该构造方法扩展了Singhvi等人先前工作的条件，为三节点中继网络提供了更广泛适用的最优码率流式编码解决方案

Abstract: This paper investigates streaming codes over three-node relay networks under
burst packet erasures with a delay constraint $T$. In any sliding window of
$T+1$ consecutive packets, the source-to-relay and relay-to-destination
channels may introduce burst erasures of lengths at most $b_1$ and $b_2$,
respectively. Singhvi et al. proposed a construction achieving the optimal code
rate when $\max\{b_1,b_2\}\mid (T-b_1-b_2)$. We construct streaming codes with
the optimal rate under the condition
  $T\geq b_1+b_2+\frac{b_1b_2}{|b_1-b_2|}$, thereby enriching the family of
rate-optimal streaming codes for three-node relay networks.

</details>


### [73] [Row-Column Twisted Reed-Solomon codes](https://arxiv.org/abs/2509.06919)
*Anuj Kumar Bhagat,Harshdeep Singh,Ritumoni Sarma*

Main category: cs.IT

TL;DR: 提出了一种新的行列扭曲Reed-Solomon码(RCTRS)，证明其MDS性质和与其他码的不等价性


<details>
  <summary>Details</summary>
Motivation: 受到扭曲Reed-Solomon码和列扭曲Reed-Solomon码的启发，尝试构造新的非RS MDS码家族

Method: 通过确定其Schur平方的维数，证明这些MDS码与Reed-Solomon码以及列扭曲Reed-Solomon码的不等价性

Result: 构造了一系列新的行列扭曲Reed-Solomon码，证明其具有MDS性质且与现有码类不等价

Conclusion: RCTRS码构成了一个新的非RS MDS码家族，为编码理论提供了新的结构

Abstract: In this article, we present a new class of codes known as row-column twisted
Reed-Solomon codes (abbreviated as RCTRS), motivated by the works of
\cite{beelen2017twisted} and \cite{liu2025column}. We explicitly provide
conditions for such codes to be MDS and also ensure their existence. By
determining the dimensions of their Schur squares, we prove that these MDS
codes are not equivalent to Reed-Solomon codes, thus presenting a new family of
non-RS MDS codes. Additionally, we prove that these MDS codes are also not
equivalent to column twisted Reed-Solomon codes described in
\cite{liu2025column}, showing the novelty of our construction.

</details>
