<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 10]
- [cs.AI](#cs.AI) [Total: 29]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Trace-driven Path Emulation of Satellite Networks using Hypatia](https://arxiv.org/abs/2510.27027)
*Martin Ottens,Kai-Steffen Hielscher,Reinhard German*

Main category: cs.NI

TL;DR: 提出了一种基于Hypatia框架的追踪驱动仿真方法，用于评估LEO卫星巨型星座网络中的互联网协议和应用行为，通过记录仿真路径特征并在实时环境中重放，实现了高精度的网络行为复现。


<details>
  <summary>Details</summary>
Motivation: 传统离散事件仿真器无法评估真实应用，需要一种能够桥接仿真和实时测试床的方法来评估LEO卫星星座网络中的互联网协议和应用行为。

Method: 扩展Hypatia框架，在非实时仿真中记录网络路径特征（延迟、带宽等），生成追踪文件，在实时仿真环境中重放这些特征，支持真实软件和人工交互评估。

Result: 在多种卫星星座场景下，仿真和仿真运行之间观察到高达0.96的高相关性指标，验证了该方法的有效性。

Conclusion: 该方法能够准确复现卫星网络行为，且易于适应现有仿真模型，但存在仿真到仿真反馈缺失和同步问题等挑战。

Abstract: The increasing prevalence LEO satellite mega-constellations for global
Internet coverage requires new approaches to evaluate the behavior of existing
Internet protocols and applications. Traditional discrete event simulators like
Hypatia allow for modeling these environments but fall short in evaluating real
applications. This paper builds upon our previous work, in which we proposed a
system design for trace-driven emulation of such satellite networks, bridging
the gab between simulations and real-time testbeds. By extending the Hypatia
framework, we record network path characteristics, e.g., delay and bandwidth,
between two endpoints in the network during non-real-time simulations. Path
characteristics are exported to Trace Files, which are replayed in real-time
emulation environments on real systems, enabling evaluations with real software
and human interaction. An advantage of our approach is its easy adaptability to
existing simulation models. Our extensive evaluation involves multiple
scenarios with different satellite constellations, illustrating the approach's
accuracy in reproducing the behavior of satellite networks. Between full
simulation, which serves as a baseline for our evaluation, and emulation runs,
we observe high correlation metrics of up to 0.96, validating the approach's
effectiveness. Challenges such as the lack of emulation-to-simulation feedback
and synchronization issues are discussed.

</details>


### [2] [TheaterQ: A Qdisc for Dynamic Network Emulation](https://arxiv.org/abs/2510.27057)
*Martin Ottens,Kai-Steffen Hielscher,Reinhard German*

Main category: cs.NI

TL;DR: TheaterQ是一个Linux qdisc，用于动态网络仿真，解决了NetEm等传统工具静态参数的限制。它使用包含网络特征时间线的跟踪文件，实现高精度动态网络仿真，无需用户空间参与，特征更新分辨率可达1微秒。


<details>
  <summary>Details</summary>
Motivation: 传统网络仿真工具如NetEm使用静态参数，无法准确模拟动态变化的网络环境，限制了现代通信协议开发的需求。

Method: 通过跟踪文件（Trace Files）包含网络特征的时间线，在Linux内核层面实现动态网络仿真，支持多个qdisc实例同步，处理延迟、带宽、丢包、重复和重排序等网络特征。

Result: 评估显示TheaterQ具有高精度，性能与现有工具相当，为现代通信协议开发提供了灵活的解决方案。

Conclusion: TheaterQ是一个开源的动态网络仿真工具，在GPLv2许可下可用，能够有效支持动态网络环境的仿真需求。

Abstract: TheaterQ is a Linux qdisc designed for dynamic network emulation, addressing
the limitations of static parameters in traditional tools like NetEm. By
utilizing Trace Files containing timelines with network characteristics,
TheaterQ achieves high-accuracy emulation of dynamic networks without involving
the userspace and allows for resolutions of characteristic updates of up to 1
microsecond. Features include synchronization across mutliple qdisc instances
and handling of delays, bandwidth, packet loss, duplication, and reordering.
Evaluations show TheaterQ's accuracy and its comparable performance to existing
tools, offering a flexible solution for modern communication protocol
development. TheaterQ is available as open-source software under the GPLv2
license.

</details>


### [3] [Analytical Model of NR-V2X Mode 2 with Re-Evaluation Mechanism](https://arxiv.org/abs/2510.27108)
*Shuo Zhu,Siyu Lin*

Main category: cs.NI

TL;DR: 本文分析了NR-V2X中的重评估机制，通过建立分析模型和DTMC消息生成器，发现该机制提高了传输可靠性但仍需降低延迟。


<details>
  <summary>Details</summary>
Motivation: V2X通信环境复杂，存在资源碰撞问题。3GPP在NR-V2X中引入重评估机制，但现有研究较少且未考虑可变流量，难以进行分析比较。

Method: 建立NR-V2X Mode 2的分析模型，使用离散时间马尔可夫链构建消息生成器，模拟3GPP高级V2X服务的流量模式。

Result: 重评估机制提高了NR-V2X传输的可靠性，但在降低延迟方面仍需局部改进。

Conclusion: 重评估机制对NR-V2X性能有积极影响，但需要进一步优化以减少延迟问题。

Abstract: Massive message transmissions, unpredictable aperiodic messages, and
high-speed moving vehicles contribute to the complex wireless environment,
resulting in inefficient resource collisions in Vehicle to Everything (V2X). In
order to achieve better medium access control (MAC) layer performance, 3GPP
introduced several new features in NR-V2X. One of the most important is the
re-evaluation mechanism. It allows the vehicle to continuously sense resources
before message transmission to avoid resource collisions. So far, only a few
articles have studied the re-evaluation mechanism of NR-V2X, and they mainly
focus on network simulator that do not consider variable traffic, which makes
analysis and comparison difficult. In this paper, an analytical model of NR-V2X
Mode 2 is established, and a message generator is constructed by using discrete
time Markov chain (DTMC) to simulate the traffic pattern recommended by 3GPP
advanced V2X services. Our study shows that the re-evaluation mechanism
improves the reliability of NR-V2X transmission, but there are still local
improvements needed to reduce latency.

</details>


### [4] [Stochastic Geometry of Cylinders: Characterizing Inter-Nodal Distances for 3D UAV Networks](https://arxiv.org/abs/2510.27111)
*Yunfeng Jiang,Zhiming Huang,Jianping Pan*

Main category: cs.NI

TL;DR: 提出了首个精确分析有限三维无线网络覆盖概率的框架，解决了长期以来因空间独立性丧失和链路距离与干扰耦合而无法分析的问题。


<details>
  <summary>Details</summary>
Motivation: 有限三维无线网络中覆盖概率的分析表征长期以来一直是一个开放性问题，主要障碍包括有限节点设置中空间独立性的丧失，以及有界几何中链路距离与干扰的耦合。

Method: 利用拉普拉斯变换的独立性结构、卷积几何和导数特性，在圆柱区域内基于二项点过程建模的有限3D网络中，开发了精确的分析框架。

Result: 广泛的蒙特卡洛模拟验证了分析结果，并显示相比传统基于泊松的模型有显著的精度提升。

Conclusion: 该结果可推广到任何受限的三维无线系统，包括空中、水下和机器人网络。

Abstract: The analytical characterization of coverage probability in finite
three-dimensional wireless networks has long remained an open problem, hindered
by the loss of spatial independence in finite-node settings and the coupling
between link distances and interference in bounded geometries. This paper
closes this gap by presenting the first exact analytical framework for coverage
probability in finite 3D networks modeled by a binomial point process within a
cylindrical region. To bypass the intractability that has long hindered such
analyses, we leverage the independence structure, convolution geometry, and
derivative properties of Laplace transforms, yielding a formulation that is
both mathematically exact and computationally efficient. Extensive Monte Carlo
simulations verify the analysis and demonstrate significant accuracy gains over
conventional Poisson-based models. The results generalize to any confined 3D
wireless system, including aerial, underwater, and robotic networks.

</details>


### [5] [Study of Cluster-Based Routing Based on Machine Learning for UAV Networks in 6G](https://arxiv.org/abs/2510.27121)
*Luis Antonio L. F. da Costa,Rodrigo C. de Lamare,Rafael Kunst,Edison Pignaton de Freitas*

Main category: cs.NI

TL;DR: 提出了一种基于机器学习的动态集群形成和集群头选择框架，用于6G无人机网络，通过XGBoost移动性预测和复合优化策略提升网络性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持无人机群和空中边缘计算等先进应用，但在飞行自组织网络中实现这一愿景需要智能自适应的集群机制来确保高效路由和资源利用。

Method: 使用XGBoost进行移动性预测，基于信号强度和空间邻近度的复合优化策略来选择最优集群头，在集中式(5G)和分布式(6G)拓扑中进行仿真评估。

Result: 在分布式场景中，提出的模型在延迟、抖动和吞吐量方面取得了显著改进。

Conclusion: 将机器学习与集群技术相结合，能够增强下一代空中网络的可扩展性、稳定性和性能。

Abstract: The sixth generation (6G) wireless networks are envisioned to deliver
ultra-low latency, massive connectivity, and high data rates, enabling advanced
applications such as autonomous {unmaned aerial vehicles (UAV)} swarms and
aerial edge computing. However, realizing this vision in Flying Ad Hoc Networks
(FANETs) requires intelligent and adaptive clustering mechanisms to ensure
efficient routing and resource utilization. This paper proposes a novel machine
learning-driven framework for dynamic cluster formation and cluster head
selection in 6G-enabled FANETs. The system leverages mobility prediction using
{Extreme Gradient Boosting (XGBoost)} and a composite optimization strategy
based on signal strength and spatial proximity to identify optimal cluster
heads. To evaluate the proposed method, comprehensive simulations were
conducted in both centralized (5G) and decentralized (6G) topologies using
realistic video traffic patterns. Results show that the proposed model achieves
significant improvements in delay, jitter, and throughput in decentralized
scenarios. These findings demonstrate the potential of combining machine
learning with clustering techniques to enhance scalability, stability, and
performance in next-generation aerial networks.

</details>


### [6] [Effective Delayed Patching for Transient Malware Control on Networks](https://arxiv.org/abs/2510.27137)
*Minh Phu Vuong,Chul-Ho Lee,Do Young Eun*

Main category: cs.NI

TL;DR: 提出了一种考虑补丁延迟的新型网络防御策略，通过识别关键边来划分感染区域和健康区域，在有限资源下有效控制恶意软件传播


<details>
  <summary>Details</summary>
Motivation: 现有补丁策略大多忽略补丁延迟的影响，而实际中补丁延迟普遍存在且对防御效果有重要影响

Method: 基于易感-感染流行病网络模型，识别关键边形成边界，将最可能感染节点与健康节点分离，通过约束图划分问题确定需要补丁的节点

Result: 数值验证表明该补丁策略在有限资源和存在补丁延迟的情况下，显著优于其他基线策略，能更好地保护健康节点

Conclusion: 考虑补丁延迟的补丁策略能更有效地控制恶意软件早期传播，在资源受限的实际场景中具有重要应用价值

Abstract: Patching nodes is an effective network defense strategy for malware control
at early stages, and its performance is primarily dependent on how accurately
the infection propagation is characterized. In this paper, we aim to design a
novel patching policy based on the susceptible-infected epidemic network model
by incorporating the influence of patching delay--the type of delay that has
been largely overlooked in designing patching policies in the literature, while
being prevalent in practice. We first identify 'critical edges' that form a
boundary to separate the most likely infected nodes from the nodes which would
still remain healthy after the patching delay. We next leverage the critical
edges to determine which nodes to be patched in light of limited patching
resources at early stages. To this end, we formulate a constrained graph
partitioning problem and use its solution to identify a set of nodes to patch
or vaccinate under the limited resources, to effectively prevent malware
propagation from getting through the healthy region. We numerically validate
that our patching policy significantly outperforms other baseline policies in
protecting the healthy nodes under limited patching resources and in the
presence of patching delay.

</details>


### [7] [Selected Results from the REDMARS2 Project: Recursive Delay-Tolerant Networking using Bundle-in-Bundle Encapsulation](https://arxiv.org/abs/2510.27325)
*Marius Feldmann,Tobias Nöthlich,Felix Walter,Maximilian Nitsch,Juan A. Fraire,Georg A. Murzik,Fiona Fuchs*

Main category: cs.NI

TL;DR: 将RINA概念集成到DTN协议中，使用BIBE实现基于范围的分隔机制，构建可扩展的DTN架构，并在太阳系互联网场景中验证可行性


<details>
  <summary>Details</summary>
Motivation: 探索RINA架构概念与DTN协议的集成，解决延迟和中断容忍网络的可扩展性问题

Method: 使用Bundle-in-Bundle Encapsulation (BIBE)实现基于范围的分隔机制，构建可扩展的DTN架构，并在包含无人机和卫星中继的太阳系互联网场景中进行实地测试

Result: 与欧洲航天局合作的实地测试证实了BIBE作为可扩展、递归和互操作DTN架构基础的可行性

Conclusion: BIBE为构建可扩展、递归和互操作的DTN架构提供了实用基础，在太阳系互联网等复杂通信场景中具有应用价值

Abstract: This whitepaper presents parts of the results of the REDMARS2 project
conducted in 2021-2022, exploring the integration of Recursive Internetwork
Architecture (RINA) concepts into Delay- and Disruption-Tolerant Networking
(DTN) protocols. Using Bundle-in-Bundle Encapsulation (BIBE), we implemented
scope-based separation mechanisms resulting in scalable DTNs. A key
contribution of this work is the demonstration of practical BIBE-based use
cases, including a realistic Solar System Internet communication scenario
involving unmanned aerial vehicles (UAVs) and satellite relays. The evaluation,
supported by field tests in collaboration with the European Space Agency (ESA),
confirmed the viability of BIBE as a foundation for scalable, recursive, and
interoperable DTN architectures.

</details>


### [8] [Challenging Tribal Knowledge -- Large Scale Measurement Campaign on Decentralized NAT Traversal](https://arxiv.org/abs/2510.27500)
*Dennis Trautwein,Cornelius Ihle,Moritz Schubotz,Bela Gipp*

Main category: cs.NI

TL;DR: 本文对完全去中心化的NAT穿越协议DCUtR进行了首次大规模纵向测量研究，基于IPFS网络的440万次穿越尝试，建立了70%±7.1%的成功率基准，并证明TCP和QUIC在NAT穿越中表现相当。


<details>
  <summary>Details</summary>
Motivation: 现有P2P系统的去中心化承诺受到NAT穿越挑战的限制，现有解决方案往往重新引入中心化。需要研究完全去中心化的NAT穿越协议的实际表现。

Method: 在基于libp2p的IPFS生产网络中，对DCUtR协议进行大规模纵向测量研究，分析了来自167个国家、85,000多个不同网络的440多万次穿越尝试。

Result: 建立了70%±7.1%的NAT打洞成功率基准；实证反驳了UDP在NAT穿越中的优越性，证明TCP和QUIC成功率相当（约70%）；97.6%的成功连接在第一次尝试中建立。

Conclusion: DCUtR协议在无许可环境中表现良好，成功与中继特性无关且机制高效。提出了协议增强路线图以实现普遍连接，并贡献完整数据集促进进一步研究。

Abstract: The promise of decentralized peer-to-peer (P2P) systems is fundamentally
gated by the challenge of Network Address Translation (NAT) traversal, with
existing solutions often reintroducing the very centralization they seek to
avoid. This paper presents the first large-scale, longitudinal measurement
study of a fully decentralized NAT traversal protocol, Direct Connection
Upgrade through Relay (DCUtR), within the production libp2p-based IPFS network.
Drawing on over 4.4 million traversal attempts from 85,000+ distinct networks
across 167 countries, we provide a definitive empirical analysis of modern P2P
connectivity. We establish a contemporary baseline success rate of $70\% \pm
7.1\%$ for the hole-punching stage, providing a crucial new benchmark for the
field. Critically, we empirically refute the long-held 'tribal knowledge' of
UDP's superiority for NAT traversal, demonstrating that DCUtR's high-precision,
RTT-based synchronization yields statistically indistinguishable success rates
for both TCP and QUIC ($\sim70\%$). Our analysis further validates the
protocol's design for permissionless environments by showing that success is
independent of relay characteristics and that the mechanism is highly
efficient, with $97.6\%$ of successful connections established on the first
attempt. Building on this analysis, we propose a concrete roadmap of protocol
enhancements aimed at achieving universal connectivity and contribute our
complete dataset to foster further research in this domain.

</details>


### [9] [Asynchronous Risk-Aware Multi-Agent Packet Routing for Ultra-Dense LEO Satellite Networks](https://arxiv.org/abs/2510.27506)
*Ke He,Thang X. Vu,Le He,Lisheng Fan,Symeon Chatzinotas,Bjorn Ottersten*

Main category: cs.NI

TL;DR: PRIMAL是一个事件驱动的多智能体路由框架，专为超密集LEO星座设计，通过风险感知的异步决策优化延迟和负载均衡。


<details>
  <summary>Details</summary>
Motivation: 现有路由方法无法应对超密集LEO星座的大规模、动态拓扑和显著延迟带来的复杂性，它们依赖不切实际的同步决策且缺乏风险意识。

Method: 采用事件驱动的多智能体框架，每个卫星在自身事件驱动时间线上独立行动，通过原对偶方法管理最坏性能退化风险，学习目标QoS的完整成本分布并约束尾部风险。

Result: 在1584颗卫星的LEO星座模拟中，相比风险无视基线，排队延迟减少超过70%，负载场景下端到端延迟降低近12毫秒。

Conclusion: PRIMAL解决了朴素最短路径查找与拥塞避免之间的核心冲突，证明了自主风险感知是实现鲁棒路由的关键。

Abstract: The rise of ultra-dense LEO constellations creates a complex and asynchronous
network environment, driven by their massive scale, dynamic topologies, and
significant delays. This unique complexity demands an adaptive packet routing
algorithm that is asynchronous, risk-aware, and capable of balancing diverse
and often conflicting QoS objectives in a decentralized manner. However,
existing methods fail to address this need, as they typically rely on
impractical synchronous decision-making and/or risk-oblivious approaches. To
tackle this gap, we introduce PRIMAL, an event-driven multi-agent routing
framework designed specifically to allow each satellite to act independently on
its own event-driven timeline, while managing the risk of worst-case
performance degradation via a principled primal-dual approach. This is achieved
by enabling agents to learn the full cost distribution of the targeted QoS
objectives and constrain tail-end risks. Extensive simulations on a LEO
constellation with 1584 satellites validate its superiority in effectively
optimizing latency and balancing load. Compared to a recent risk-oblivious
baseline, it reduces queuing delay by over 70%, and achieves a nearly 12 ms
end-to-end delay reduction in loaded scenarios. This is accomplished by
resolving the core conflict between naive shortest-path finding and congestion
avoidance, highlighting such autonomous risk-awareness as a key to robust
routing.

</details>


### [10] [Rethinking Telemetry Design for Fine-Grained Anomaly Detection in 5G User Planes](https://arxiv.org/abs/2510.27664)
*Niloy Saha,Noura Limam,Yang Xiao,Raouf Boutaba*

Main category: cs.NI

TL;DR: Kestrel是一种基于草图的遥测系统，用于5G用户平面，以低成本提供细粒度的QoS异常检测，相比现有方法在检测精度和带宽效率方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有5G用户平面QoS异常检测方法面临基本权衡：粗粒度的每类计数器轻量但会掩盖瞬时和每流异常，而每包遥测明信片成本过高且随线路速率线性增长。选择性明信片方案会遗漏低于阈值或短暂发生的异常。

Method: Kestrel扩展Count-Min Sketch，添加直方图增强的桶和每队列分区，将每包测量压缩为紧凑摘要同时保留异常相关信号。开发了考虑草图碰撞的正式可检测性保证，产生最大化异常可分离性的尺寸规则和分箱策略。

Result: 在配备Intel Tofino交换机的5G测试平台上评估显示，Kestrel比现有选择性明信片方案检测精度提高10%，同时减少导出带宽10倍。

Conclusion: Kestrel通过草图基础的方法在5G用户平面中实现了细粒度QoS异常检测，在检测精度和带宽效率之间取得了良好平衡，解决了现有方法面临的权衡问题。

Abstract: Detecting QoS anomalies in 5G user planes requires fine-grained per-flow
visibility, but existing telemetry approaches face a fundamental trade-off.
Coarse per-class counters are lightweight but mask transient and per-flow
anomalies, while per-packet telemetry postcards provide full visibility at
prohibitive cost that grows linearly with line rate. Selective postcard schemes
reduce overhead but miss anomalies that fall below configured thresholds or
occur during brief intervals. We present Kestrel, a sketch-based telemetry
system for 5G user planes that provides fine-grained visibility into key metric
distributions such as latency tails and inter-arrival times at a fraction of
the cost of per-packet postcards. Kestrel extends Count-Min Sketch with
histogram-augmented buckets and per-queue partitioning, which compress
per-packet measurements into compact summaries while preserving
anomaly-relevant signals. We develop formal detectability guarantees that
account for sketch collisions, yielding principled sizing rules and binning
strategies that maximize anomaly separability. Our evaluations on a 5G testbed
with Intel Tofino switches show that Kestrel achieves 10% better detection
accuracy than existing selective postcard schemes while reducing export
bandwidth by 10x.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [SUSTAINABLE Platform: Seamless Smart Farming Integration Towards Agronomy Automation](https://arxiv.org/abs/2510.26989)
*Agorakis Bompotas,Konstantinos Koutras,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Dimitra Gariza,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.AI

TL;DR: SUSTAINABLE是一个智能农业平台，整合物联网、AI、卫星成像和基于角色的任务编排，旨在实现高效、可追溯和可持续的农业，以葡萄栽培为试点用例。


<details>
  <summary>Details</summary>
Motivation: 全球农业面临粮食需求增长、气候变化和可持续实践需求的挑战，需要智能化解决方案。

Method: 集成物联网、人工智能、卫星成像技术，采用基于角色的任务编排方法，特别针对地中海葡萄园定制。

Result: 开发了具备卫星指数集成、实时环境数据和角色感知任务管理功能的智能农业平台。

Conclusion: SUSTAINABLE平台通过技术创新为农业可持续发展提供了可行的解决方案，特别是在葡萄栽培领域具有应用潜力。

Abstract: The global agricultural sector is undergoing a transformative shift, driven
by increasing food demands, climate variability and the need for sustainable
practices. SUSTAINABLE is a smart farming platform designed to integrate IoT,
AI, satellite imaging, and role-based task orchestration to enable efficient,
traceable, and sustainable agriculture with a pilot usecase in viticulture.
This paper explores current smart agriculture solutions, presents a comparative
evaluation, and introduces SUSTAINABLE's key features, including satellite
index integration, real-time environmental data, and role-aware task management
tailored to Mediterranean vineyards.

</details>


### [12] [CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions](https://arxiv.org/abs/2510.26852)
*Lingyue Fu,Xin Ding,Yaoming Zhu,Shao Zhang,Lin Qiu,Weiwen Liu,Weinan Zhang,Xuezhi Cao,Xunliang Cai,Jiaxin Ding,Yong Yu*

Main category: cs.AI

TL;DR: 提出了CATArena评估平台，通过开放式评分和竞技式学习框架来评估LLM智能体的学习能力，解决了现有基准测试中分数饱和和依赖专家标注的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要评估固定场景下的端到端性能，存在分数饱和、依赖专家标注等问题，无法有效评估智能体的学习能力这一核心进化驱动力。

Method: 提出迭代式竞争性同伴学习框架，让智能体通过重复交互和反馈优化策略；引入CATArena评估平台，包含四种棋盘和卡牌游戏，采用开放式评分系统。

Result: 实验结果表明CATArena能够为智能体核心能力（特别是学习能力和策略编码）提供可靠、稳定和可扩展的基准测试。

Conclusion: CATArena通过开放式评分和竞技学习框架，有效解决了现有基准测试的局限性，为评估智能体学习能力提供了新的解决方案。

Abstract: Large Language Model (LLM) agents have evolved from basic text generation to
autonomously completing complex tasks through interaction with external tools.
However, current benchmarks mainly assess end-to-end performance in fixed
scenarios, restricting evaluation to specific skills and suffering from score
saturation and growing dependence on expert annotation as agent capabilities
improve. In this work, we emphasize the importance of learning ability,
including both self-improvement and peer-learning, as a core driver for agent
evolution toward human-level intelligence. We propose an iterative, competitive
peer-learning framework, which allows agents to refine and optimize their
strategies through repeated interactions and feedback, thereby systematically
evaluating their learning capabilities. To address the score saturation issue
in current benchmarks, we introduce CATArena, a tournament-style evaluation
platform featuring four diverse board and card games with open-ended scoring.
By providing tasks without explicit upper score limits, CATArena enables
continuous and dynamic evaluation of rapidly advancing agent capabilities.
Experimental results and analyses involving both minimal and commercial code
agents demonstrate that CATArena provides reliable, stable, and scalable
benchmarking for core agent abilities, particularly learning ability and
strategy coding.

</details>


### [13] [Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base](https://arxiv.org/abs/2510.26854)
*Yu Li,Yuan Huang,Tao Wang,Caiyu Fan,Xiansheng Cai,Sihan Hu,Xinzijian Liu,Cheng Shi,Mingjun Xu,Zhen Wang,Yan Wang,Xiangqi Jin,Tianhan Zhang,Linfeng Zhang,Lei Wang,Youjin Deng,Pan Zhang,Weijie Sun,Xingyu Li,Weinan E,Linfeng Zhang,Zhiyuan Yao,Kun Chen*

Main category: cs.AI

TL;DR: 提出了一个可扩展的框架来解压缩科学推理，构建可验证的长思维链知识库，并投影为SciencePedia百科全书。通过Socratic代理生成300万个第一原理问题，多模型生成思维链，经过严格过滤后构建知识库，支持逆向知识搜索和文章合成。


<details>
  <summary>Details</summary>
Motivation: 科学材料通常压缩推理过程，只呈现结论而省略推导链条，这阻碍了验证过程，也抑制了跨领域链接，因为压缩了建立概念间逻辑和因果联系的路径。

Method: 采用端点驱动的还原策略：Socratic代理根据约200门课程生成300万个第一原理问题；多个独立求解器模型生成长思维链；通过提示净化和跨模型答案共识进行严格过滤；构建Brainstorm搜索引擎进行逆向知识搜索；Plato合成器将验证链叙述为连贯文章。

Result: 初始SciencePedia包含约20万个细粒度条目，涵盖数学、物理、化学、生物、工程和计算。在六个学科评估中，基于检索思维链的Plato合成文章比无检索基线具有显著更高的知识点密度和更低的实际错误率。

Conclusion: 基于可验证长思维链知识库的推理中心方法能够实现可信赖的跨领域科学综合，并为不断扩展的百科全书奠定基础。

Abstract: Most scientific materials compress reasoning, presenting conclusions while
omitting the derivational chains that justify them. This compression hinders
verification by lacking explicit, step-wise justifications and inhibits
cross-domain links by collapsing the very pathways that establish the logical
and causal connections between concepts. We introduce a scalable framework that
decompresses scientific reasoning, constructing a verifiable Long
Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent
encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven,
reductionist strategy: a Socratic agent, guided by a curriculum of around 200
courses, generates approximately 3 million first-principles questions. To
ensure high fidelity, multiple independent solver models generate LCoTs, which
are then rigorously filtered by prompt sanitization and cross-model answer
consensus, retaining only those with verifiable endpoints. This verified corpus
powers the Brainstorm Search Engine, which performs inverse knowledge search --
retrieving diverse, first-principles derivations that culminate in a target
concept. This engine, in turn, feeds the Plato synthesizer, which narrates
these verified chains into coherent articles. The initial SciencePedia
comprises approximately 200,000 fine-grained entries spanning mathematics,
physics, chemistry, biology, engineering, and computation. In evaluations
across six disciplines, Plato-synthesized articles (conditioned on retrieved
LCoTs) exhibit substantially higher knowledge-point density and significantly
lower factual error rates than an equally-prompted baseline without retrieval
(as judged by an external LLM). Built on this verifiable LCoT knowledge base,
this reasoning-centric approach enables trustworthy, cross-domain scientific
synthesis at scale and establishes the foundation for an ever-expanding
encyclopedia.

</details>


### [14] [The Denario project: Deep knowledge AI agents for scientific discovery](https://arxiv.org/abs/2510.26887)
*Francisco Villaescusa-Navarro,Boris Bolliet,Pablo Villanueva-Domingo,Adrian E. Bayer,Aidan Acquah,Chetana Amancharla,Almog Barzilay-Siegal,Pablo Bermejo,Camille Bilodeau,Pablo Cárdenas Ramírez,Miles Cranmer,Urbano L. França,ChangHoon Hahn,Yan-Fei Jiang,Raul Jimenez,Jun-Young Lee,Antonio Lerario,Osman Mamun,Thomas Meier,Anupam A. Ojha,Pavlos Protopapas,Shimanto Roy,David N. Spergel,Pedro Tarancón-Álvarez,Ujjwal Tiwari,Matteo Viel,Digvijay Wadekar,Chi Wang,Bonny Y. Wang,Licong Xu,Yossi Yovel,Shuwen Yue,Wen-Han Zhou,Qiyao Zhu,Jiajun Zou,Íñigo Zubeldia*

Main category: cs.AI

TL;DR: Denario是一个AI多智能体科学研究助手系统，能够执行生成想法、文献检索、制定研究计划、编写执行代码、制作图表以及起草和评审科学论文等多种任务。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够协助科学家进行跨学科研究的AI系统，提高科研效率并探索AI在科学研究中的应用潜力。

Method: 采用模块化架构，结合Cmbagent作为深度研究后端，支持从特定任务到端到端科学分析的不同工作流程。

Result: 系统在多个科学领域（天体物理学、生物学、生物物理学等）生成了多篇AI论文，并通过领域专家评估验证了其能力。

Conclusion: Denario展示了AI在科学研究中的强大潜力，但也存在局限性，需要关注AI驱动研究的伦理影响及其与科学哲学的关系。

Abstract: We present Denario, an AI multi-agent system designed to serve as a
scientific research assistant. Denario can perform many different tasks, such
as generating ideas, checking the literature, developing research plans,
writing and executing code, making plots, and drafting and reviewing a
scientific paper. The system has a modular architecture, allowing it to handle
specific tasks, such as generating an idea, or carrying out end-to-end
scientific analysis using Cmbagent as a deep-research backend. In this work, we
describe in detail Denario and its modules, and illustrate its capabilities by
presenting multiple AI-generated papers generated by it in many different
scientific disciplines such as astrophysics, biology, biophysics, biomedical
informatics, chemistry, material science, mathematical physics, medicine,
neuroscience and planetary science. Denario also excels at combining ideas from
different disciplines, and we illustrate this by showing a paper that applies
methods from quantum physics and machine learning to astrophysical data. We
report the evaluations performed on these papers by domain experts, who
provided both numerical scores and review-like feedback. We then highlight the
strengths, weaknesses, and limitations of the current system. Finally, we
discuss the ethical implications of AI-driven research and reflect on how such
technology relates to the philosophy of science. We publicly release the code
at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run
directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and
the full app will be deployed on the cloud.

</details>


### [15] [Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations](https://arxiv.org/abs/2510.26905)
*Pedro Antonio Alarcón Granadeno,Arturo Miguel Bernal Russell,Sofia Nelson,Demetrius Hernandez,Maureen Petterson,Michael Murphy,Walter J. Scheirer,Jane Cleland-Huang*

Main category: cs.AI

TL;DR: 提出认知包络概念，为AI决策建立推理边界，解决基础模型在物理信息系统中产生的幻觉、过度泛化和上下文错位等问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型在物理信息系统中增强了感知、推理和规划能力，但也带来了新的错误类型，导致错误决策。

Method: 引入认知包络概念，建立推理边界来约束AI生成的决策，同时补充元认知和传统安全包络的使用。

Result: 提出了认知包络的概念框架，需要制定实用指南和系统化流程来定义、验证和保证认知包络。

Conclusion: 认知包络是解决基础模型在物理信息系统中决策错误的重要方法，需要建立系统化的实施流程。

Abstract: Cyber-physical systems increasingly rely on Foundational Models such as Large
Language Models (LLMs) and Vision-Language Models (VLMs) to increase autonomy
through enhanced perception, inference, and planning. However, these models
also introduce new types of errors, such as hallucinations,
overgeneralizations, and context misalignments, resulting in incorrect and
flawed decisions. To address this, we introduce the concept of Cognition
Envelopes, designed to establish reasoning boundaries that constrain
AI-generated decisions while complementing the use of meta-cognition and
traditional safety envelopes. As with safety envelopes, Cognition Envelopes
require practical guidelines and systematic processes for their definition,
validation, and assurance.

</details>


### [16] [Causal Masking on Spatial Data: An Information-Theoretic Case for Learning Spatial Datasets with Unimodal Language Models](https://arxiv.org/abs/2510.27009)
*Jared Junkin,Samuel Nathanson*

Main category: cs.AI

TL;DR: 在非序列数据上应用因果掩码是可行的训练方法，在棋类等空间结构数据中甚至优于序列化方法


<details>
  <summary>Details</summary>
Motivation: 研究在具有空间结构的数据上应用因果掩码的可行性，传统观点认为因果掩码不适合非序列数据，但缺乏直接研究

Method: 在棋类数据上训练双向和因果自注意力模型，比较空间（棋盘状态）和序列（走棋顺序）两种表示方法

Result: 在空间棋盘状态上训练的模型（即使使用因果掩码）始终比在序列数据上训练的模型具有更强的游戏能力

Conclusion: 在空间数据上应用因果掩码是训练单模态LLM的可行方法，在某些领域甚至优于序列化方法

Abstract: Language models are traditionally designed around causal masking. In domains
with spatial or relational structure, causal masking is often viewed as
inappropriate, and sequential linearizations are instead used. Yet the question
of whether it is viable to accept the information loss introduced by causal
masking on nonsequential data has received little direct study, in part because
few domains offer both spatial and sequential representations of the same
dataset. In this work, we investigate this issue in the domain of chess, which
naturally supports both representations. We train language models with
bidirectional and causal self-attention mechanisms on both spatial
(board-based) and sequential (move-based) data. Our results show that models
trained on spatial board states - \textit{even with causal masking} -
consistently achieve stronger playing strength than models trained on
sequential data. While our experiments are conducted on chess, our results are
methodological and may have broader implications: applying causal masking to
spatial data is a viable procedure for training unimodal LLMs on spatial data,
and in some domains is even preferable to sequentialization.

</details>


### [17] [e1: Learning Adaptive Control of Reasoning Effort](https://arxiv.org/abs/2510.27042)
*Michael Kleinman,Matthew Trager,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 提出自适应努力控制方法，让AI模型根据用户指定的努力水平自动调整推理长度，在保持性能的同时显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 用户需要在输出质量与延迟/成本之间进行权衡，但现有方法要求预先知道问题难度来设置token预算，这在实际中难以实现。

Method: 基于强化学习的自适应方法，训练模型使用相对于当前平均思维链长度的用户指定比例token数，无需数据集和阶段特定调优。

Result: 在1.5B到32B参数规模的模型中，该方法能将思维链长度减少约3倍，同时保持或改进相对于RL训练基础模型的性能。

Conclusion: 自适应努力控制方法提供了动态调整成本-精度权衡的能力，模型能自动按任务难度比例分配资源，产生更好的成本-精度权衡曲线。

Abstract: Increasing the thinking budget of AI models can significantly improve
accuracy, but not all questions warrant the same amount of reasoning. Users may
prefer to allocate different amounts of reasoning effort depending on how they
value output quality versus latency and cost. To leverage this tradeoff
effectively, users need fine-grained control over the amount of thinking used
for a particular query, but few approaches enable such control. Existing
methods require users to specify the absolute number of desired tokens, but
this requires knowing the difficulty of the problem beforehand to appropriately
set the token budget for a query. To address these issues, we propose Adaptive
Effort Control, a self-adaptive reinforcement learning method that trains
models to use a user-specified fraction of tokens relative to the current
average chain-of-thought length for each query. This approach eliminates
dataset- and phase-specific tuning while producing better cost-accuracy
tradeoff curves compared to standard methods. Users can dynamically adjust the
cost-accuracy trade-off through a continuous effort parameter specified at
inference time. We observe that the model automatically learns to allocate
resources proportionally to the task difficulty and, across model scales
ranging from 1.5B to 32B parameters, our approach enables approximately 3x
reduction in chain-of-thought length while maintaining or improving performance
relative to the base model used for RL training.

</details>


### [18] [Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement](https://arxiv.org/abs/2510.27051)
*Aaditya Shukla,Sidney Knowles,Meenakshi Madugula,Dave Farris,Ryan Angilly,Santiago Pombo,Anbang Xu,Lu An,Abhinav Balasubramanian,Tan Yu,Jiaxiang Ren,Rama Akkiraju*

Main category: cs.AI

TL;DR: 在NVInfo AI中实现了一个基于MAPE的数据飞轮系统，通过收集用户反馈和针对性微调，显著提升了企业AI代理的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 企业AI代理需要持续适应以保持准确性、降低延迟并符合用户需求，传统方法难以应对实际部署中的故障模式。

Method: 采用MAPE驱动的数据飞轮构建闭环系统，收集495个负样本分析故障模式，使用NVIDIA NeMo微服务进行针对性微调。

Result: 路由错误从5.25%降至4%，模型大小减少10倍，延迟改善70%；查询重述准确率提升3.7%，延迟降低40%。

Conclusion: 人机回环反馈结合数据飞轮可将企业AI代理转变为自改进系统，为构建稳健、自适应的企业AI代理提供了可复制的蓝图。

Abstract: Enterprise AI agents must continuously adapt to maintain accuracy, reduce
latency, and remain aligned with user needs. We present a practical
implementation of a data flywheel in NVInfo AI, NVIDIA's Mixture-of-Experts
(MoE) Knowledge Assistant serving over 30,000 employees. By operationalizing a
MAPE-driven data flywheel, we built a closed-loop system that systematically
addresses failures in retrieval-augmented generation (RAG) pipelines and
enables continuous learning. Over a 3-month post-deployment period, we
monitored feedback and collected 495 negative samples. Analysis revealed two
major failure modes: routing errors (5.25\%) and query rephrasal errors
(3.2\%). Using NVIDIA NeMo microservices, we implemented targeted improvements
through fine-tuning. For routing, we replaced a Llama 3.1 70B model with a
fine-tuned 8B variant, achieving 96\% accuracy, a 10x reduction in model size,
and 70\% latency improvement. For query rephrasal, fine-tuning yielded a 3.7\%
gain in accuracy and a 40\% latency reduction. Our approach demonstrates how
human-in-the-loop (HITL) feedback, when structured within a data flywheel,
transforms enterprise AI agents into self-improving systems. Key learnings
include approaches to ensure agent robustness despite limited user feedback,
navigating privacy constraints, and executing staged rollouts in production.
This work offers a repeatable blueprint for building robust, adaptive
enterprise AI agents capable of learning from real-world usage at scale.

</details>


### [19] [CombiGraph-Vis: A Curated Multimodal Olympiad Benchmark for Discrete Mathematical Reasoning](https://arxiv.org/abs/2510.27094)
*Hamed Mahdavi,Pouria Mahdavinia,Alireza Farhadi,Pegah Mohammadipour,Samira Malek,Majid Daliri,Pedram Mohammadipour,Alireza Hashemi,Amir Khasahmadi,Vasant Honavar*

Main category: cs.AI

TL;DR: 该论文评估了LLMs在证明评分方面的能力，包括错误检测和部分学分分配，并提出了基于代理的工作流程来改进评分一致性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在解决奥林匹克数学问题方面取得显著进展，需要评估它们在证明评分方面的能力，特别是检测错误、判断严重程度以及分配公平分数。

Method: 使用90个Gemini 2.5 Pro生成的解决方案和MathArena解决方案集，引入代理工作流程提取参考解决方案并自动推导问题特定的评分标准，进行多步骤评分过程。

Result: 模型能够可靠地标记错误解决方案，但在部分学分分配方面存在校准差距。提出的工作流程在人类评分一致性和部分学分处理方面表现更好。

Conclusion: 代理工作流程能够提高证明评分的准确性和一致性，为未来研究提供了代码、数据和提示/日志。

Abstract: State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based
Olympiad problems to solving most of the IMO 2025 problems, with leading
systems reportedly handling 5 of 6 problems. Given this progress, we assess how
well these models can grade proofs: detecting errors, judging their severity,
and assigning fair scores beyond binary correctness. We study proof-analysis
capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we
grade on a 1-4 scale with detailed error annotations, and on MathArena solution
sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models
can reliably flag incorrect (including subtly incorrect) solutions but exhibit
calibration gaps in how partial credit is assigned. To address this, we
introduce agentic workflows that extract and analyze reference solutions and
automatically derive problem-specific rubrics for a multi-step grading process.
We instantiate and compare different design choices for the grading workflows,
and evaluate their trade-offs. Across our annotated corpus and MathArena, our
proposed workflows achieve higher agreement with human grades and more
consistent handling of partial credit across metrics. We release all code,
data, and prompts/logs to facilitate future research.

</details>


### [20] [Glia: A Human-Inspired AI for Automated Systems Design and Optimization](https://arxiv.org/abs/2510.27176)
*Pouya Hamadanian,Pantea Karimi,Arash Nasr-Esfahany,Kimia Noorbakhsh,Joseph Chandler,Ali ParandehGheibi,Mohammad Alizadeh,Hari Balakrishnan*

Main category: cs.AI

TL;DR: Glia是一个用于网络系统设计的AI架构，使用LLM在多智能体工作流中自主设计计算机系统机制，性能达到人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 探索AI是否能像人类专家一样自主设计计算机系统机制，展现创造性和推理能力。

Method: 采用基于LLM的多智能体架构，每个智能体专门负责推理、实验和分析，通过评估框架将抽象推理与实证反馈相结合。

Result: 应用于分布式GPU集群的LLM推理场景时，Glia设计了新的请求路由、调度和自动扩展算法，在更短时间内达到人类专家水平，并获得了对工作负载行为的新见解。

Conclusion: 通过将推理LLM与结构化实验相结合，AI能够为复杂系统问题产生创造性且可理解的设计方案。

Abstract: Can an AI autonomously design mechanisms for computer systems on par with the
creativity and reasoning of human experts? We present Glia, an AI architecture
for networked systems design that uses large language models (LLMs) in a
human-inspired, multi-agent workflow. Each agent specializes in reasoning,
experimentation, and analysis, collaborating through an evaluation framework
that grounds abstract reasoning in empirical feedback. Unlike prior
ML-for-systems methods that optimize black-box policies, Glia generates
interpretable designs and exposes its reasoning process. When applied to a
distributed GPU cluster for LLM inference, it produces new algorithms for
request routing, scheduling, and auto-scaling that perform at human-expert
levels in significantly less time, while yielding novel insights into workload
behavior. Our results suggest that by combining reasoning LLMs with structured
experimentation, an AI can produce creative and understandable designs for
complex systems problems.

</details>


### [21] [From product to system network challenges in system of systems lifecycle management](https://arxiv.org/abs/2510.27194)
*Vahid Salehi,Josef Vilsmeier,Shirui Wang*

Main category: cs.AI

TL;DR: 提出了一个面向系统之系统（SoS）生命周期管理的实用参考框架，将MBSE作为语义骨干、PLM作为治理和配置层、CAD-CAE作为模型衍生领域，数字线程和数字孪生作为持续反馈，并基于四个原则构建从产品中心到网络中心开发的三步路线图。


<details>
  <summary>Details</summary>
Motivation: 传统线性生命周期模型在处理现代网络化系统中的互操作性、变体管理、可追溯性和跨组织治理方面已显不足，需要新的方法来管理复杂性并实现可扩展的SoS价值流。

Method: 基于当前文献和行业经验，提出了包含参考架构和数据模型、端到端配置主权、有明确审查节点的策划模型以及可衡量价值贡献的四个原则，并通过三步路线图实现从产品中心到网络中心开发的过渡。

Result: 该方法提高了变更稳健性、缩短了处理时间、改善了重用性，并为可持续性决策提供了信息支持。

Conclusion: 该框架旨在帮助决策者和实践者管理复杂性，设计可扩展的SoS价值流，实现从孤立产品到网络化系统的转型。

Abstract: Today, products are no longer isolated artifacts, but nodes in networked
systems. This means that traditional, linearly conceived life cycle models are
reaching their limits: Interoperability across disciplines, variant and
configuration management, traceability, and governance across organizational
boundaries are becoming key factors. This collective contribution classifies
the state of the art and proposes a practical frame of reference for SoS
lifecycle management, model-based systems engineering (MBSE) as the semantic
backbone, product lifecycle management (PLM) as the governance and
configuration level, CAD-CAE as model-derived domains, and digital thread and
digital twin as continuous feedback. Based on current literature and industry
experience, mobility, healthcare, and the public sector, we identify four
principles: (1) referenced architecture and data models, (2) end-to-end
configuration sovereignty instead of tool silos, (3) curated models with clear
review gates, and (4) measurable value contributions along time, quality, cost,
and sustainability. A three-step roadmap shows the transition from product- to
network- centric development: piloting with reference architecture, scaling
across variant and supply chain spaces, organizational anchoring (roles,
training, compliance). The results are increased change robustness, shorter
throughput times, improved reuse, and informed sustainability decisions. This
article is aimed at decision-makers and practitioners who want to make
complexity manageable and design SoS value streams to be scalable.

</details>


### [22] [Fints: Efficient Inference-Time Personalization for LLMs with Fine-Grained Instance-Tailored Steering](https://arxiv.org/abs/2510.27206)
*Kounianhua Du,Jianxing Liu,Kangning Zhang,Wenxiang Jiao,Yuan Lu,Jiarui Jin,Weiwen Liu,Yong Yu,Weinan Zhang*

Main category: cs.AI

TL;DR: 提出了一个细粒度的实例定制化引导框架，通过动态生成样本级干扰向量来增强LLM个性化适应能力，解决了现有方法在处理动态用户模式和数据稀疏场景时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有参数化适应方法在处理动态用户模式和高数据稀疏场景时存在适应性差和数据效率低的问题，需要更灵活高效的个性化技术。

Method: 开发了细粒度引导组件从注意力层和MLP层捕获细微信号，结合输入感知聚合模块将这些信号合成为上下文相关的增强向量，在模型前向传播中注入个性化适应。

Result: 在短文本到长文本生成、网页函数调用等多种场景的实验中，该方法在快速变化环境中显著提升个性化性能，同时在不同交互模式和上下文长度下保持鲁棒性。

Conclusion: 该方法具有高度灵活性和数据效率，与现有方法正交且可作为插件组件兼容不同个性化技术，在快速变化分布和高数据稀疏场景中表现优异。

Abstract: The rapid evolution of large language models (LLMs) has intensified the
demand for effective personalization techniques that can adapt model behavior
to individual user preferences. Despite the non-parametric methods utilizing
the in-context learning ability of LLMs, recent parametric adaptation methods,
including personalized parameter-efficient fine-tuning and reward modeling
emerge. However, these methods face limitations in handling dynamic user
patterns and high data sparsity scenarios, due to low adaptability and data
efficiency. To address these challenges, we propose a fine-grained and
instance-tailored steering framework that dynamically generates sample-level
interference vectors from user data and injects them into the model's forward
pass for personalized adaptation. Our approach introduces two key technical
innovations: a fine-grained steering component that captures nuanced signals by
hooking activations from attention and MLP layers, and an input-aware
aggregation module that synthesizes these signals into contextually relevant
enhancements. The method demonstrates high flexibility and data efficiency,
excelling in fast-changing distribution and high data sparsity scenarios. In
addition, the proposed method is orthogonal to existing methods and operates as
a plug-in component compatible with different personalization techniques.
Extensive experiments across diverse scenarios--including short-to-long text
generation, and web function calling--validate the effectiveness and
compatibility of our approach. Results show that our method significantly
enhances personalization performance in fast-shifting environments while
maintaining robustness across varying interaction modes and context lengths.
Implementation is available at https://github.com/KounianhuaDu/Fints.

</details>


### [23] [GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation](https://arxiv.org/abs/2510.27210)
*Tao Liu,Chongyu Wang,Rongjie Li,Yingchen Yu,Xuming He,Bai Song*

Main category: cs.AI

TL;DR: 提出了GUI-Rise框架，通过结构化推理、动作预测和历史总结增强多模态大语言模型在GUI导航中的表现，在跨域场景下取得最先进结果


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在GUI导航代理中存在跨域泛化能力不足和历史信息利用效率低的问题

Method: 开发了结合结构化推理、动作预测和历史总结的框架，通过监督微调伪标记轨迹和GRPO强化学习训练GUI-Rise代理

Result: 在标准基准测试中，在相同训练数据条件下取得最先进结果，特别是在跨域场景下表现优异

Conclusion: 该框架能够保持跨不同GUI导航任务的稳健推理和泛化能力

Abstract: While Multimodal Large Language Models (MLLMs) have advanced GUI navigation
agents, current approaches face limitations in cross-domain generalization and
effective history utilization. We present a reasoning-enhanced framework that
systematically integrates structured reasoning, action prediction, and history
summarization. The structured reasoning component generates coherent
Chain-of-Thought analyses combining progress estimation and decision reasoning,
which inform both immediate action predictions and compact history summaries
for future steps. Based on this framework, we train a GUI agent,
\textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled
trajectories and reinforcement learning with Group Relative Policy Optimization
(GRPO). This framework employs specialized rewards, including a history-aware
objective, directly linking summary quality to subsequent action performance.
Comprehensive evaluations on standard benchmarks demonstrate state-of-the-art
results under identical training data conditions, with particularly strong
performance in out-of-domain scenarios. These findings validate our framework's
ability to maintain robust reasoning and generalization across diverse GUI
navigation tasks. Code is available at https://leon022.github.io/GUI-Rise.

</details>


### [24] [Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines](https://arxiv.org/abs/2510.27329)
*Kristina Levina,Nikolaos Pappas,Athanasios Karapantelakis,Aneta Vulgarakis Feljan,Jendrik Seipp*

Main category: cs.AI

TL;DR: 本文提出了三种奖励机（RM）的泛化形式来解决长时程无序子任务问题，并引入了基于耦合RM的组合学习算法CoRM，在无序子任务的长时程问题上比现有RM算法具有更好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统的奖励机在处理长时程问题中无序子任务时存在局限性，当无序子任务数量增加时，需要学习的信息量呈指数级增长，这限制了奖励机在复杂任务中的应用。

Method: 提出了三种RM泛化形式：数值RM、议程RM和耦合RM，并开发了基于耦合RM的组合学习算法CoRM。耦合RM将状态与议程中的每个子任务关联，实现更高效的学习。

Result: 实验表明，CoRM算法在包含无序子任务的长时程问题上，比现有最先进的RM算法具有更好的扩展性能。

Conclusion: 通过引入数值RM、议程RM和耦合RM这三种泛化形式，以及CoRM组合学习算法，有效解决了传统奖励机在处理无序子任务长时程问题时的扩展性限制。

Abstract: Reward machines (RMs) inform reinforcement learning agents about the reward
structure of the environment. This is particularly advantageous for complex
non-Markovian tasks because agents with access to RMs can learn more
efficiently from fewer samples. However, learning with RMs is ill-suited for
long-horizon problems in which a set of subtasks can be executed in any order.
In such cases, the amount of information to learn increases exponentially with
the number of unordered subtasks. In this work, we address this limitation by
introducing three generalisations of RMs: (1) Numeric RMs allow users to
express complex tasks in a compact form. (2) In Agenda RMs, states are
associated with an agenda that tracks the remaining subtasks to complete. (3)
Coupled RMs have coupled states associated with each subtask in the agenda.
Furthermore, we introduce a new compositional learning algorithm that leverages
coupled RMs: Q-learning with coupled RMs (CoRM). Our experiments show that CoRM
scales better than state-of-the-art RM algorithms for long-horizon problems
with unordered subtasks.

</details>


### [25] [Discriminative Rule Learning for Outcome-Guided Process Model Discovery](https://arxiv.org/abs/2510.27343)
*Ali Norouzifar,Wil van der Aalst*

Main category: cs.AI

TL;DR: 提出了一种基于结果感知的过程发现方法，通过区分理想和不理想的过程执行轨迹，分别学习过程模型，从而揭示关键行为差异。


<details>
  <summary>Details</summary>
Motivation: 传统过程发现方法不考虑执行结果差异，导致模型不适合一致性检查和性能分析，无法捕捉理想与不理想执行之间的关键行为区别。

Method: 通过学习控制流特征上的可解释判别规则，将具有相似理想性特征的轨迹分组，然后在每个组内分别应用过程发现技术。

Result: 该方法在多个真实事件日志上得到验证，能够有效隔离和可视化关键过程模式，生成聚焦且可解释的模型。

Conclusion: 结果感知的过程发现方法能够揭示理想和不理想过程执行的驱动因素，为过程改进提供更深入的洞察。

Abstract: Event logs extracted from information systems offer a rich foundation for
understanding and improving business processes. In many real-world
applications, it is possible to distinguish between desirable and undesirable
process executions, where desirable traces reflect efficient or compliant
behavior, and undesirable ones may involve inefficiencies, rule violations,
delays, or resource waste. This distinction presents an opportunity to guide
process discovery in a more outcome-aware manner. Discovering a single process
model without considering outcomes can yield representations poorly suited for
conformance checking and performance analysis, as they fail to capture critical
behavioral differences. Moreover, prioritizing one behavior over the other may
obscure structural distinctions vital for understanding process outcomes. By
learning interpretable discriminative rules over control-flow features, we
group traces with similar desirability profiles and apply process discovery
separately within each group. This results in focused and interpretable models
that reveal the drivers of both desirable and undesirable executions. The
approach is implemented as a publicly available tool and it is evaluated on
multiple real-life event logs, demonstrating its effectiveness in isolating and
visualizing critical process patterns.

</details>


### [26] [An In-depth Study of LLM Contributions to the Bin Packing Problem](https://arxiv.org/abs/2510.27353)
*Julien Herrmann,Guillaume Pallez*

Main category: cs.AI

TL;DR: 重新评估LLM在数学发现中的贡献，发现LLM生成的装箱问题启发式算法虽然可读但难以理解，并提出更简单高效的算法，强调需要严格验证LLM输出的科学价值。


<details>
  <summary>Details</summary>
Motivation: 重新评估LLM对数学发现的贡献，质疑之前关于LLM能为装箱问题提供新见解的说法，强调需要严格验证LLM生成内容的科学价值。

Method: 详细分析LLM生成的启发式算法行为与可解释性，并为特定装箱问题实例设计新的算法类别。

Result: 发现LLM生成的启发式算法对领域专家也难以理解，提出的新算法更简单、高效、可解释且泛化能力更强，表明原问题实例本身相对简单。

Conclusion: LLM对装箱问题的贡献被高估，基于错误假设，强调评估LLM生成输出科学价值时需要严格验证和情境化。

Abstract: Recent studies have suggested that Large Language Models (LLMs) could provide
interesting ideas contributing to mathematical discovery. This claim was
motivated by reports that LLM-based genetic algorithms produced heuristics
offering new insights into the online bin packing problem under uniform and
Weibull distributions. In this work, we reassess this claim through a detailed
analysis of the heuristics produced by LLMs, examining both their behavior and
interpretability. Despite being human-readable, these heuristics remain largely
opaque even to domain experts. Building on this analysis, we propose a new
class of algorithms tailored to these specific bin packing instances. The
derived algorithms are significantly simpler, more efficient, more
interpretable, and more generalizable, suggesting that the considered instances
are themselves relatively simple. We then discuss the limitations of the claim
regarding LLMs' contribution to this problem, which appears to rest on the
mistaken assumption that the instances had previously been studied. Our
findings instead emphasize the need for rigorous validation and
contextualization when assessing the scientific value of LLM-generated outputs.

</details>


### [27] [ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use](https://arxiv.org/abs/2510.27363)
*Mengjie Deng,Guanting Dong,Zhicheng Dou*

Main category: cs.AI

TL;DR: ToolScope是一个代理框架，通过统一全局规划和局部多模态感知，提升多模态大语言模型在视觉问答任务中的工具使用能力，在多个基准测试中平均性能提升达6.69%。


<details>
  <summary>Details</summary>
Motivation: 多模态信息具有复杂性和多样性，当前多模态大语言模型在推理过程中灵活高效地使用外部工具仍是一个未被充分探索的挑战。

Method: ToolScope包含三个主要组件：全局导航器（提供高层策略指导）、代理执行器（通过集成搜索、代码和感知工具迭代增强局部感知）、响应合成器（整合推理过程生成用户友好的输出）。

Result: 在四个VQA基准测试（VQA 2.0、ScienceQA、MAT-Search和MathVista）上评估，展示出强大的泛化能力，平均性能提升达+6.69%。

Conclusion: ToolScope框架通过统一的全局规划和局部感知，有效解决了多模态大语言模型在长视野VQA任务中的视觉上下文退化问题，显著提升了模型性能。

Abstract: Recently, large language models (LLMs) have demonstrated remarkable
problem-solving capabilities by autonomously integrating with external tools
for collaborative reasoning. However, due to the inherently complex and diverse
nature of multimodal information, enabling multimodal large language models
(MLLMs) to flexibly and efficiently utilize external tools during reasoning
remains an underexplored challenge. In this work, we introduce ToolScope, an
agentic framework designed to unify global planning with local multimodal
perception, adopting a specialized Perceive tool to mitigates visual context
degradation in long-horizon VQA task. ToolScope comprises three primary
components: the Global Navigator, the Agentic Executor, and the Response
Synthesizer. The Global Navigator functions as a "telescope", offering
high-level strategic guidance. The Agentic Executor operates iteratively to
augment MLLM with local perception through the integration of external
tools-Search, Code, and Perceive. Finally, the Response Synthesizer
consolidates and organizes the reasoning process into a coherent, user-friendly
output. We evaluate ToolScope on four VQA benchmarks across diverse domains,
including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong
generalization capabilities, achieving an average performance improvement of up
to +6.69% across all datasets.

</details>


### [28] [Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints](https://arxiv.org/abs/2510.27383)
*Yueyang Wang,Mehmet Dogar,Gustav Markkula*

Main category: cs.AI

TL;DR: 提出一个结合视觉和运动约束的多智能体强化学习框架，用于模拟行人-驾驶员交互，在真实无信号人行横道数据集上验证了约束模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏灵活性或忽略感知和运动约束等底层机制，需要更真实地模拟行人-驾驶员交互行为。

Method: 使用多智能体强化学习框架，集成行人和驾驶员的视觉和运动约束，评估四种模型变体（无约束、仅运动约束、仅视觉约束、两者皆有）。

Result: 结合视觉和运动约束的模型表现最佳，运动约束产生更平滑的运动，视觉约束引入感知不确定性导致更谨慎行为，在数据有限情况下优于监督学习模型。

Conclusion: 带有人类约束的多智能体强化学习是模拟真实道路使用者交互的有前景方法，能够考虑个体差异并有效处理小数据集。

Abstract: Modelling pedestrian-driver interactions is critical for understanding human
road user behaviour and developing safe autonomous vehicle systems. Existing
approaches often rely on rule-based logic, game-theoretic models, or
'black-box' machine learning methods. However, these models typically lack
flexibility or overlook the underlying mechanisms, such as sensory and motor
constraints, which shape how pedestrians and drivers perceive and act in
interactive scenarios. In this study, we propose a multi-agent reinforcement
learning (RL) framework that integrates both visual and motor constraints of
pedestrian and driver agents. Using a real-world dataset from an unsignalised
pedestrian crossing, we evaluate four model variants, one without constraints,
two with either motor or visual constraints, and one with both, across
behavioural metrics of interaction realism. Results show that the combined
model with both visual and motor constraints performs best. Motor constraints
lead to smoother movements that resemble human speed adjustments during
crossing interactions. The addition of visual constraints introduces perceptual
uncertainty and field-of-view limitations, leading the agents to exhibit more
cautious and variable behaviour, such as less abrupt deceleration. In this
data-limited setting, our model outperforms a supervised behavioural cloning
model, demonstrating that our approach can be effective without large training
datasets. Finally, our framework accounts for individual differences by
modelling parameters controlling the human constraints as population-level
distributions, a perspective that has not been explored in previous work on
pedestrian-vehicle interaction modelling. Overall, our work demonstrates that
multi-agent RL with human constraints is a promising modelling approach for
simulating realistic road user interactions.

</details>


### [29] [Dialogue as Discovery: Navigating Human Intent Through Principled Inquiry](https://arxiv.org/abs/2510.27410)
*Jianwen Sun,Yukang Feng,Yifan Chang,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yu Dai,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 提出名为Nous的AI代理，通过主动询问解决用户意图不确定性，基于信息理论框架训练，在科学图表生成任务中展现高效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作中的"意图表达鸿沟"问题，即用户难以向AI有效传达复杂高维想法，导致低效试错循环，特别是面对不同专业水平的用户时。

Method: 从被动指令遵循转向苏格拉底式协作范式，提出基于信息理论的训练框架，将对话信息增益定义为内在奖励信号，无需依赖人工偏好标注或外部奖励模型。

Result: 在科学图表生成任务中，Nous实现了领先的效率和输出质量，对不同专业水平的用户保持鲁棒性，且设计具有领域无关性，展现出超越图表生成的泛化能力。

Conclusion: 该工作为解决复杂人机协作中用户意图不确定性提供了一个原则性、可扩展且自适应的范式。

Abstract: A fundamental bottleneck in human-AI collaboration is the "intention
expression gap," the difficulty for humans to effectively convey complex,
high-dimensional thoughts to AI. This challenge often traps users in
inefficient trial-and-error loops and is exacerbated by the diverse expertise
levels of users. We reframe this problem from passive instruction following to
a Socratic collaboration paradigm, proposing an agent that actively probes for
information to resolve its uncertainty about user intent. we name the proposed
agent Nous, trained to acquire proficiency in this inquiry policy. The core
mechanism of Nous is a training framework grounded in the first principles of
information theory. Within this framework, we define the information gain from
dialogue as an intrinsic reward signal, which is fundamentally equivalent to
the reduction of Shannon entropy over a structured task space. This reward
design enables us to avoid reliance on costly human preference annotations or
external reward models. To validate our framework, we develop an automated
simulation pipeline to generate a large-scale, preference-based dataset for the
challenging task of scientific diagram generation. Comprehensive experiments,
including ablations, subjective and objective evaluations, and tests across
user expertise levels, demonstrate the effectiveness of our proposed framework.
Nous achieves leading efficiency and output quality, while remaining robust to
varying user expertise. Moreover, its design is domain-agnostic, and we show
evidence of generalization beyond diagram generation. Experimental results
prove that our work offers a principled, scalable, and adaptive paradigm for
resolving uncertainty about user intent in complex human-AI collaboration.

</details>


### [30] [DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains](https://arxiv.org/abs/2510.27419)
*Tian Liang,Wenxiang Jiao,Zhiwei He,Jiahao Xu,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: DeepCompress框架通过自适应长度奖励机制，动态分类问题为简单或困难，鼓励简单问题用短推理链，困难问题用长推理链，同时提升LRM的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在认知效率问题：简单问题"过度思考"，复杂问题"思考不足"。现有方法通过SFT或带长度奖励的RL提高效率，但往往以牺牲准确性为代价。

Method: 提出DeepCompress框架，采用自适应长度奖励机制，实时根据模型能力动态分类问题为简单或困难，对简单问题鼓励短推理链，对困难问题鼓励长推理链。

Result: 在数学基准测试中，DeepCompress始终优于基线方法，在显著提高token效率的同时实现了更优的准确性。

Conclusion: 挑战了一贯偏好短推理路径的做法，证明长响应可能包含更广泛的正确解决方案。DeepCompress使模型能自主调整CoT长度，压缩已掌握问题的推理，扩展困难问题的推理。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive capabilities but
suffer from cognitive inefficiencies like ``overthinking'' simple problems and
``underthinking'' complex ones. While existing methods that use supervised
fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can
improve efficiency, they often do so at the cost of accuracy. This paper
introduces \textbf{DeepCompress}, a novel framework that simultaneously
enhances both the accuracy and efficiency of LRMs. We challenge the prevailing
approach of consistently favoring shorter reasoning paths, showing that longer
responses can contain a broader range of correct solutions for difficult
problems. DeepCompress employs an adaptive length reward mechanism that
dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on
the model's evolving capability. It encourages shorter, more efficient
reasoning for ``Simple'' problems while promoting longer, more exploratory
thought chains for ``Hard'' problems. This dual-reward strategy enables the
model to autonomously adjust its Chain-of-Thought (CoT) length, compressing
reasoning for well-mastered problems and extending it for those it finds
challenging. Experimental results on challenging mathematical benchmarks show
that DeepCompress consistently outperforms baseline methods, achieving superior
accuracy while significantly improving token efficiency.

</details>


### [31] [GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data Generation through Formal Language](https://arxiv.org/abs/2510.27448)
*Yuhao Zhang,Dingxin Hu,Tinghao Yu,Hao Liu,Yiting Liu*

Main category: cs.AI

TL;DR: GeoFM是一种新的几何数据合成方法，使用形式语言在度量空间中探索条件组合，通过符号引擎确保正确性，生成的合成数据在几何推理任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在数学几何推理方面面临高质量几何数据稀缺的挑战，现有合成几何数据方法存在多样性不足、噪声多、图像变化有限且与真实几何图偏离等问题。

Method: 使用形式语言在度量空间中探索条件组合，通过符号引擎确保生成几何问题的正确性，生成与原始问题不同但正确的高保真几何问题。

Result: 使用GeoFM合成数据训练的模型在MathVista几何问题解决任务上比GPT-4o高出18.7%，在GeoQA上高出16.5%；在领先开源模型上分别高出5.7%和2.7%。

Conclusion: GeoFM方法能有效生成高质量、多样化的几何数据，显著提升多模态大语言模型在几何推理任务上的性能。

Abstract: Multi-modal Large Language Models (MLLMs) have gained significant attention
in both academia and industry for their capabilities in handling multi-modal
tasks. However, these models face challenges in mathematical geometric
reasoning due to the scarcity of high-quality geometric data. To address this
issue, synthetic geometric data has become an essential strategy. Current
methods for generating synthetic geometric data involve rephrasing or expanding
existing problems and utilizing predefined rules and templates to create
geometric images and problems. However, these approaches often produce data
that lacks diversity or is prone to noise. Additionally, the geometric images
synthesized by existing methods tend to exhibit limited variation and deviate
significantly from authentic geometric diagrams. To overcome these limitations,
we propose GeoFM, a novel method for synthesizing geometric data. GeoFM uses
formal languages to explore combinations of conditions within metric space,
generating high-fidelity geometric problems that differ from the originals
while ensuring correctness through a symbolic engine. Experimental results show
that our synthetic data significantly outperforms existing methods. The model
trained with our data surpass the proprietary GPT-4o model by 18.7\% on
geometry problem-solving tasks in MathVista and by 16.5\% on GeoQA.
Additionally, it exceeds the performance of a leading open-source model by
5.7\% on MathVista and by 2.7\% on GeoQA.

</details>


### [32] [Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for Interpretable Deconstruction of Reasoning System Performance](https://arxiv.org/abs/2510.27544)
*Nikolaus Holzer,William Fishell,Baishakhi Ray,Mark Santolucito*

Main category: cs.AI

TL;DR: TempoBench是第一个基于形式化基础的可验证诊断基准，通过参数化难度来系统分析LLM的推理能力，包含时间轨迹评估(TTE)和时间因果评估(TCE)两个基准。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖临时生成的数据集（可能编码偏见且无法验证），要么依赖Lean等数学证明系统（不适合捕捉基于决策链的任务），导致LLM推理基准在推理结构和现实世界对齐方面存在不足。

Method: 使用两个评估基准：时间轨迹评估(TTE)测试LLM理解和模拟多步推理系统执行的能力；时间因果评估(TCE)测试LLM进行多步因果推理和从复杂系统中提取因果关系的能力。

Result: 模型在TCE-normal上得分65.6%，在TCE-hard上仅得7.5%，表明最先进的LLMs能理解TCE任务但随着系统复杂性增加表现显著下降。

Conclusion: TempoBench填补了现有LLM推理基准的空白，提供了形式化可验证的评估框架，揭示了LLMs在复杂因果推理方面的局限性。

Abstract: Large Language Models (LLMs) are increasingly excelling and outpacing human
performance on many tasks. However, to improve LLM reasoning, researchers
either rely on ad-hoc generated datasets or formal mathematical proof systems
such as the Lean proof assistant. Whilst ad-hoc generated methods can capture
the decision chains of real-world reasoning processes, they may encode some
inadvertent bias in the space of reasoning they cover; they also cannot be
formally verified. On the other hand, systems like Lean can guarantee
verifiability, but are not well-suited to capture the nature of agentic
decision chain-based tasks. This creates a gap both in performance for
functions such as business agents or code assistants, and in the usefulness of
LLM reasoning benchmarks, whereby these fall short in reasoning structure or
real-world alignment. We introduce TempoBench, the first formally grounded and
verifiable diagnostic benchmark that parametrizes difficulty to systematically
analyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks
to break down reasoning ability. First, temporal trace evaluation (TTE) tests
the ability of an LLM to understand and simulate the execution of a given
multi-step reasoning system. Subsequently, temporal causal evaluation (TCE)
tests an LLM's ability to perform multi-step causal reasoning and to distill
cause-and-effect relations from complex systems. We find that models score
65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art
LLMs clearly understand the TCE task but perform poorly as system complexity
increases. Our code is available at our
\href{https://github.com/nik-hz/tempobench}{GitHub repository}.

</details>


### [33] [SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning](https://arxiv.org/abs/2510.27568)
*Ali Asgarov,Umid Suleymanov,Aadyant Khatri*

Main category: cs.AI

TL;DR: SIGMA是一个多智能体检索增强框架，通过专门化智能体独立推理、定向搜索和协调机制，在数学推理任务上显著优于现有系统，性能提升7.4%。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强模型存在单一视角、搜索策略僵化、多源信息整合困难等问题，无法有效支持复杂的数学推理任务。

Method: 引入SIGMA框架，协调专门化智能体进行独立推理、生成假设性段落进行定向检索，通过协调机制整合多源知识。

Result: 在MATH500、AIME和GPQA等挑战性基准测试中，SIGMA一致优于开源和闭源系统，绝对性能提升7.4%。

Conclusion: 多智能体按需知识集成显著提高了推理准确性和效率，为复杂知识密集型问题解决提供了可扩展方法。

Abstract: Solving mathematical reasoning problems requires not only accurate access to
relevant knowledge but also careful, multi-step thinking. However, current
retrieval-augmented models often rely on a single perspective, follow
inflexible search strategies, and struggle to effectively combine information
from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge
Integration for AGentic Mathematical reAsoning), a unified framework that
orchestrates specialized agents to independently reason, perform targeted
searches, and synthesize findings through a moderator mechanism. Each agent
generates hypothetical passages to optimize retrieval for its analytic
perspective, ensuring knowledge integration is both context-sensitive and
computation-efficient. When evaluated on challenging benchmarks such as
MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms
both open- and closed-source systems, achieving an absolute performance
improvement of 7.4%. Our results demonstrate that multi-agent, on-demand
knowledge integration significantly enhances both reasoning accuracy and
efficiency, offering a scalable approach for complex, knowledge-intensive
problem-solving. We will release the code upon publication.

</details>


### [34] [InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM Research](https://arxiv.org/abs/2510.27598)
*Yunze Wu,Dayuan Fu,Weiye Si,Zhen Huang,Mohan Jiang,Keyu Li,Shijie Xia,Jie Sun,Tianze Xu,Xiangkun Hu,Pengrui Lu,Xiaojie Cai,Lyumanshan Ye,Wenhong Zhu,Yang Xiao,Pengfei Liu*

Main category: cs.AI

TL;DR: 提出了InnovatorBench基准测试平台，用于评估AI代理在LLM研究中的端到端能力，包含20个任务和配套的ResearchGym研究环境。实验显示前沿模型在代码驱动任务中表现良好，但在算法相关任务和长期决策中存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在简化环境中仅评估狭窄技能，无法真实反映AI代理在科学研究中的端到端能力，需要更现实的评估平台。

Method: 开发InnovatorBench基准测试平台，包含20个任务覆盖数据构建、过滤、增强、损失设计、奖励设计和支架构建；创建ResearchGym研究环境提供丰富动作空间、分布式执行和异步监控；实现轻量级ReAct代理结合推理与可执行规划。

Result: 前沿模型在代码驱动研究任务中表现有希望，但在脆弱算法相关任务和长期决策制定方面存在困难，如缺乏耐心、资源管理差和过度依赖模板推理；代理需要超过11小时才能达到最佳性能。

Conclusion: InnovatorBench基准测试具有挑战性，展示了其作为下一代基于代码的研究基准的潜力，揭示了当前AI代理在科学研究自动化中的局限性。

Abstract: AI agents could accelerate scientific discovery by automating hypothesis
formation, experiment design, coding, execution, and analysis, yet existing
benchmarks probe narrow skills in simplified settings. To address this gap, we
introduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end
assessment of agents performing Large Language Model (LLM) research. It
comprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss
Design, Reward Design, and Scaffold Construction, which require runnable
artifacts and assessment of correctness, performance, output quality, and
uncertainty. To support agent operation, we develop ResearchGym, a research
environment offering rich action spaces, distributed and long-horizon
execution, asynchronous monitoring, and snapshot saving. We also implement a
lightweight ReAct agent that couples explicit reasoning with executable
planning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.
Our experiments demonstrate that while frontier models show promise in
code-driven research tasks, they struggle with fragile algorithm-related tasks
and long-horizon decision making, such as impatience, poor resource management,
and overreliance on template-based reasoning. Furthermore, agents require over
11 hours to achieve their best performance on InnovatorBench, underscoring the
benchmark's difficulty and showing the potential of InnovatorBench to be the
next generation of code-based research benchmark.

</details>


### [35] [VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation](https://arxiv.org/abs/2510.27617)
*Heng Ping,Arijit Bhattacharjee,Peiyu Zhang,Shixuan Li,Wei Yang,Anzhe Cheng,Xiaole Zhang,Jesse Thomason,Ali Jannesari,Nesreen Ahmed,Paul Bogdan*

Main category: cs.AI

TL;DR: VeriMoA是一个无需训练的多智能体框架，通过质量引导缓存和多路径生成策略，显著提升硬件描述语言(HDL)生成的性能，在多个基准测试中Pass@1指标提升15-30%。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在硬件描述语言生成方面面临参数知识有限和领域特定约束的挑战，而多智能体方法存在噪声传播和推理空间探索受限的问题。

Method: 提出VeriMoA框架：1）质量引导缓存机制，维护所有中间HDL输出并进行质量排序；2）多路径生成策略，利用C++和Python作为中间表示，将规范到HDL的翻译分解为两阶段过程。

Result: 在VerilogEval 2.0和RTLLM 2.0基准测试中，VeriMoA实现了Pass@1指标15-30%的提升，特别是使小模型能够匹配大模型和微调替代方案。

Conclusion: VeriMoA提供了一个无需训练的有效解决方案，通过协同创新显著提升了HDL生成的性能，使小模型能够达到大模型的性能水平。

Abstract: Automation of Register Transfer Level (RTL) design can help developers meet
increasing computational demands. Large Language Models (LLMs) show promise for
Hardware Description Language (HDL) generation, but face challenges due to
limited parametric knowledge and domain-specific constraints. While prompt
engineering and fine-tuning have limitations in knowledge coverage and training
costs, multi-agent architectures offer a training-free paradigm to enhance
reasoning through collaborative generation. However, current multi-agent
approaches suffer from two critical deficiencies: susceptibility to noise
propagation and constrained reasoning space exploration. We propose VeriMoA, a
training-free mixture-of-agents (MoA) framework with two synergistic
innovations. First, a quality-guided caching mechanism to maintain all
intermediate HDL outputs and enables quality-based ranking and selection across
the entire generation process, encouraging knowledge accumulation over layers
of reasoning. Second, a multi-path generation strategy that leverages C++ and
Python as intermediate representations, decomposing specification-to-HDL
translation into two-stage processes that exploit LLM fluency in high-resource
languages while promoting solution diversity. Comprehensive experiments on
VerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves
15--30% improvements in Pass@1 across diverse LLM backbones, especially
enabling smaller models to match larger models and fine-tuned alternatives
without requiring costly training.

</details>


### [36] [Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning](https://arxiv.org/abs/2510.27623)
*Qiusi Zhan,Hyeonjeong Ha,Rui Yang,Sirui Xu,Hanyang Chen,Liang-Yan Gui,Yu-Xiong Wang,Huan Zhang,Heng Ji,Daniel Kang*

Main category: cs.AI

TL;DR: BEAT框架首次在多模态大语言模型（MLLM）驱动的具身智能体中植入视觉后门攻击，使用环境中的物体作为触发条件，攻击成功率高达80%，同时保持正常的任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在具身智能体中的广泛应用，基于视觉输入的感知、推理和规划能力带来了新的安全风险——视觉后门攻击，攻击者可以通过特定视觉触发条件操控智能体执行恶意多步策略。

Method: BEAT采用两阶段训练方案：首先进行监督微调（SFT），然后引入对比触发学习（CTL）。CTL将触发识别建模为偏好学习，通过对比有触发和无触发输入来锐化决策边界，确保精确的后门激活。

Result: 在各种具身智能体基准测试和MLLM模型中，BEAT实现了高达80%的攻击成功率，同时保持强大的良性任务性能，并能可靠地泛化到分布外的触发位置。与朴素SFT相比，CTL在有限后门数据下将后门激活准确率提升了39%。

Conclusion: 这些发现揭示了MLLM基具身智能体中一个关键且未被探索的安全风险，强调了在现实世界部署前需要开发鲁棒防御机制的重要性。

Abstract: Multimodal large language models (MLLMs) have advanced embodied agents by
enabling direct perception, reasoning, and planning task-oriented actions from
visual inputs. However, such vision driven embodied agents open a new attack
surface: visual backdoor attacks, where the agent behaves normally until a
visual trigger appears in the scene, then persistently executes an
attacker-specified multi-step policy. We introduce BEAT, the first framework to
inject such visual backdoors into MLLM-based embodied agents using objects in
the environments as triggers. Unlike textual triggers, object triggers exhibit
wide variation across viewpoints and lighting, making them difficult to implant
reliably. BEAT addresses this challenge by (1) constructing a training set that
spans diverse scenes, tasks, and trigger placements to expose agents to trigger
variability, and (2) introducing a two-stage training scheme that first applies
supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning
(CTL). CTL formulates trigger discrimination as preference learning between
trigger-present and trigger-free inputs, explicitly sharpening the decision
boundaries to ensure precise backdoor activation. Across various embodied agent
benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while
maintaining strong benign task performance, and generalizes reliably to
out-of-distribution trigger placements. Notably, compared to naive SFT, CTL
boosts backdoor activation accuracy up to 39% under limited backdoor data.
These findings expose a critical yet unexplored security risk in MLLM-based
embodied agents, underscoring the need for robust defenses before real-world
deployment.

</details>


### [37] [Validity Is What You Need](https://arxiv.org/abs/2510.27628)
*Sebastian Benthall,Andrew Clark*

Main category: cs.AI

TL;DR: 论文提出了Agentic AI的现实主义定义，认为它是在复杂企业环境中自主工作的软件交付机制，强调验证比基础模型更重要，LLMs只是实现有效性的可选方案之一。


<details>
  <summary>Details</summary>
Motivation: 重新定义Agentic AI，澄清其作为应用而非基础模型的本质，强调用户验证的重要性，指出当前对LLMs的过度关注可能忽视了更简单有效的解决方案。

Method: 通过比较Agentic AI与SaaS的相似性，分析其在企业环境中的实际应用需求，提出基于用户验证的评估框架。

Result: 发现Agentic AI的成功主要依赖于最终用户和主要利益相关者的验证，而非基础模型的复杂性；在良好验证机制下，简单的模型可能比LLMs更有效。

Conclusion: Agentic AI的核心是有效性验证，LLMs只是实现这一目标的可能工具之一，在某些情况下更简单、快速和可解释的模型可能表现更好。

Abstract: While AI agents have long been discussed and studied in computer science,
today's Agentic AI systems are something new. We consider other definitions of
Agentic AI and propose a new realist definition. Agentic AI is a software
delivery mechanism, comparable to software as a service (SaaS), which puts an
application to work autonomously in a complex enterprise setting. Recent
advances in large language models (LLMs) as foundation models have driven
excitement in Agentic AI. We note, however, that Agentic AI systems are
primarily applications, not foundations, and so their success depends on
validation by end users and principal stakeholders. The tools and techniques
needed by the principal users to validate their applications are quite
different from the tools and techniques used to evaluate foundation models.
Ironically, with good validation measures in place, in many cases the
foundation models can be replaced with much simpler, faster, and more
interpretable models that handle core logic. When it comes to Agentic AI,
validity is what you need. LLMs are one option that might achieve it.

</details>


### [38] [Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training](https://arxiv.org/abs/2510.27630)
*Dayuan Fu,Yunze Wu,Xiaojie Cai,Lyumanshan Ye,Shijie Xia,Zhen Huang,Weiye Si,Tianze Xu,Jie Sun,Keyu Li,Mohan Jiang,Junfei Wang,Qishuo Hua,Pengrui Lu,Yang Xiao,Pengfei Liu*

Main category: cs.AI

TL;DR: Apollo是一个集成异步人类指导与动作级数据过滤的采样框架，用于训练LLM智能体处理长时域、领域专业化任务，相比传统方法显著提升了训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前训练LLM智能体的方法存在局限性：行为克隆需要密集人工标注成本过高，结果驱动采样在领域专业化任务中容易失败。需要一种更高效的方法来处理长时域任务。

Method: Apollo框架允许人类在智能体偏离正确轨迹时进行干预（提供先验知识、策略建议等），而不是全程跟踪。同时应用监督控制来过滤次优动作，防止错误传播。

Result: 在InnovatorBench上的实验显示，使用Apollo训练的GLM-4.5模型相比未训练基线提升超过50%，相比无人交互训练变体提升28%。

Conclusion: Apollo证明了人类在环采样在长时域、领域专业化任务中的关键作用，其轻量级设计能够持续交互30+小时，以较低成本产生有价值的轨迹数据。

Abstract: Large Language Model (LLM) agents have recently shown strong potential in
domains such as automated coding, deep research, and graphical user interface
manipulation. However, training them to succeed on long-horizon,
domain-specialized tasks remains challenging. Current methods primarily fall
into two categories. The first relies on dense human annotations through
behavior cloning, which is prohibitively expensive for long-horizon tasks that
can take days or months. The second depends on outcome-driven sampling, which
often collapses due to the rarity of valid positive trajectories on
domain-specialized tasks. We introduce Apollo, a sampling framework that
integrates asynchronous human guidance with action-level data filtering.
Instead of requiring annotators to shadow every step, Apollo allows them to
intervene only when the agent drifts from a promising trajectory, by providing
prior knowledge, strategic advice, etc. This lightweight design makes it
possible to sustain interactions for over 30 hours and produces valuable
trajectories at a lower cost. Apollo then applies supervision control to filter
out sub-optimal actions and prevent error propagation. Together, these
components enable reliable and effective data collection in long-horizon
environments. To demonstrate the effectiveness of Apollo, we evaluate it using
InnovatorBench. Our experiments show that when applied to train the GLM-4.5
model on InnovatorBench, Apollo achieves more than a 50% improvement over the
untrained baseline and a 28% improvement over a variant trained without human
interaction. These results highlight the critical role of human-in-the-loop
sampling and the robustness of Apollo's design in handling long-horizon,
domain-specialized tasks.

</details>


### [39] [MolChord: Structure-Sequence Alignment for Protein-Guided Drug Design](https://arxiv.org/abs/2510.27671)
*Wei Zhang,Zekun Guo,Yingce Xia,Peiran Jin,Shufang Xie,Tao Qin,Xiang-Yang Li*

Main category: cs.AI

TL;DR: MolChord是一个用于基于结构的药物设计的新方法，通过整合文本描述和序列表示来对齐蛋白质和分子结构，并使用偏好优化来引导分子生成具有所需药理特性。


<details>
  <summary>Details</summary>
Motivation: 解决基于结构的药物设计中蛋白质结构表示与分子表示的有效对齐问题，以及确保生成的药物与其药理特性之间的对齐。

Method: 使用NatureLM作为分子生成器，结合基于扩散的结构编码器；通过整合偏好数据构建属性感知数据集，并使用直接偏好优化(DPO)来优化对齐过程。

Result: 在CrossDocked2020数据集上的实验结果表明，该方法在关键评估指标上达到了最先进的性能。

Conclusion: MolChord展示了作为基于结构药物设计实用工具的潜力，能够有效生成具有所需药理特性的候选分子。

Abstract: Structure-based drug design (SBDD), which maps target proteins to candidate
molecular ligands, is a fundamental task in drug discovery. Effectively
aligning protein structural representations with molecular representations, and
ensuring alignment between generated drugs and their pharmacological
properties, remains a critical challenge. To address these challenges, we
propose MolChord, which integrates two key techniques: (1) to align protein and
molecule structures with their textual descriptions and sequential
representations (e.g., FASTA for proteins and SMILES for molecules), we
leverage NatureLM, an autoregressive model unifying text, small molecules, and
proteins, as the molecule generator, alongside a diffusion-based structure
encoder; and (2) to guide molecules toward desired properties, we curate a
property-aware dataset by integrating preference data and refine the alignment
process using Direct Preference Optimization (DPO). Experimental results on
CrossDocked2020 demonstrate that our approach achieves state-of-the-art
performance on key evaluation metrics, highlighting its potential as a
practical tool for SBDD.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [40] [Multi-hop Parallel Image Semantic Communication for Distortion Accumulation Mitigation](https://arxiv.org/abs/2510.26844)
*Bingyan Xie,Jihong Park,Yongpeng Wu,Wenjun Zhang,Tony Quek*

Main category: cs.IT

TL;DR: 提出多跳并行图像语义通信框架，通过并行残差补偿链路和粗到精残差压缩方案解决多跳无线图像传输中的失真累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信方案主要关注单跳场景，忽略了多跳无线图像传输的挑战。由于语义通信本质上有损，失真会在多跳中累积，导致性能显著下降。

Method: 提出MHPSC框架，在每跳引入并行残差补偿链路对抗失真累积。设计粗到精残差压缩方案：深度学习残差压缩器压缩残差，自适应算术编码进一步压缩，残差分布估计模块预测先验分布以实现精细压缩。

Result: 实验结果表明MHPSC在仅轻微增加传输带宽的情况下，优于现有语义通信和传统分离编码方案。

Conclusion: MHPSC框架能够确保鲁棒的多跳图像传输，有效解决语义通信在多跳场景中的失真累积问题。

Abstract: Existing semantic communication schemes primarily focus on single-hop
scenarios, overlooking the challenges of multi-hop wireless image transmission.
As semantic communication is inherently lossy, distortion accumulates over
multiple hops, leading to significant performance degradation. To address this,
we propose the multi-hop parallel image semantic communication (MHPSC)
framework, which introduces a parallel residual compensation link at each hop
against distortion accumulation. To minimize the associated transmission
bandwidth overhead, a coarse-to-fine residual compression scheme is designed. A
deep learning-based residual compressor first condenses the residuals, followed
by the adaptive arithmetic coding (AAC) for further compression. A residual
distribution estimation module predicts the prior distribution for the AAC to
achieve fine compression performances. This approach ensures robust multi-hop
image transmission with only a minor increase in transmission bandwidth.
Experimental results confirm that MHPSC outperforms both existing semantic
communication and traditional separated coding schemes.

</details>


### [41] [Inferring the Chemotaxis Distortion Function from Cellular Decision Strategies](https://arxiv.org/abs/2510.26988)
*Fardad Vakilipoor,Johannes Konrad,Maximilian Schäfer*

Main category: cs.IT

TL;DR: 该论文提出了逆向Blahut-Arimoto算法(IBAA)来量化系统决策标准，通过信息论框架分析细胞在不确定性下的智能决策过程，并在细胞凋亡和趋化性场景中验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究细胞如何在噪声信号通路中处理环境信号并做出上下文依赖的决策，特别是理解细胞如何应对不确定性进行智能决策。

Method: 应用基于率失真理论(RDT)的信息论框架，提出逆向Blahut-Arimoto算法(IBAA)来计算失真函数，量化系统的决策标准，并使用局部兴奋全局抑制(LEGI)模型模拟趋化性响应。

Result: 在细胞凋亡场景中准确估计了理论失真函数，在趋化性响应中计算了从细胞视角的失真函数，揭示了细胞的状态依赖决策标准。

Conclusion: 该通用框架可扩展到生物和工程系统中需要高效信息处理的场景，为理解细胞智能决策提供了新的理论工具。

Abstract: Cellular intelligence enables cells to process environmental signals and make
context-dependent decisions, as exemplified by chemotaxis, where cells navigate
chemical gradients despite noisy signaling pathways. To investigate how cells
deal with uncertainty, we apply an information-theoretic framework based on
rate distortion theory (RDT). The Blahut-Arimoto algorithm (BAA) computes
optimal decision strategies that minimize mutual information while satisfying
distortion constraints, balancing sensing accuracy with distortion constraint
equivalent to resource cost. We propose the inverse Blahut-Arimoto algorithm
(IBAA) to compute the distortion function, which quantifies the system's
decision-making criteria for realizing a decision strategy to map input signals
to outputs. This general framework extends beyond chemotaxis to biological and
engineered systems requiring efficient information processing under
uncertainty. We validate the proposed IBAA by accurately estimating theoretical
distortion functions in a cellular apoptosis scenario. Additionally, using the
local excitation global inhibition (LEGI) model to simulate chemotactic
responses, we compute the distortion functions from the cell's perspective. Our
finding reveals a state-dependent decision criteria by the cell.

</details>


### [42] [Multilevel constructions of constant dimension codes based on one-factorization of complete graphs](https://arxiv.org/abs/2510.27071)
*Dengming Xu,Mengmeng LI*

Main category: cs.IT

TL;DR: 该论文提出了一种基于多级构造的恒定维码构建方法，通过选择合适的骨架码并使用准挂起块，改进了特定参数下恒定维码的下界。


<details>
  <summary>Details</summary>
Motivation: 恒定维码在随机网络编码中具有重要应用，多级构造是构建恒定维码的有效方法之一。

Method: 首先基于与完全图一因子化相关的二元向量变换选择适当的骨架码，然后使用准挂起块构造恒定维码，最后利用最优Ferrers图秩度量码的已知构造计算维度。

Result: 改进了参数为$\overline{A}_q(n,8,6)$（其中$16\leq n\leq 19$）的恒定维码的下界。

Conclusion: 所提出的多级构造方法能有效改进恒定维码的下界，特别是在特定参数范围内。

Abstract: Constant dimension codes (CDCs) have become an important object in coding
theory due to their application in random network coding. The multilevel
construction is one of the most effective ways to construct constant dimension
codes. The paper is devoted to constructing CDCs by the multilevel
construction. Precisely, we first choose an appropriate skeleton code based on
the transformations of binary vectors related to the one-factorization of
complete graphs; then we construct CDCs by using the chosen skeleton code,
where quasi-pending blocks are used; finally, we calculate the dimensions by
use of known constructions of optimal Ferrers diagram rank metric codes. As
applications, we improve the lower bounds of $\overline{A}_q(n,8,6)$ for
$16\leq n\leq 19.$

</details>


### [43] [Secure Communication in the Presence of an RIS-Enhanced Eavesdropper in MIMO Networks](https://arxiv.org/abs/2510.27147)
*Gaoyuan Zhang,Ruisong Si,Boyuan Li,Zijian Li,Baofeng Ji,Chenqi Zhu,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: 提出了一种针对RIS增强移动窃听攻击的MIMO无线网络安全通信方案，通过随机比特翻转和SVD预编码来最小化窃听者的互信息，无需窃听者完整信道状态信息。


<details>
  <summary>Details</summary>
Motivation: 解决RIS增强移动窃听攻击对MIMO无线网络安全的威胁，传统方法需要窃听者完整信道状态信息，这在现实中难以获取。

Method: 使用随机比特翻转方案最小化秘密消息与窃听者接收数据间的互信息，结合SVD预编码优化功率分配，确保合法接收者不受干扰。

Result: 方案在实际中可行，无需窃听者完整瞬时信道状态信息，能有效对抗各种攻击场景，通过仿真验证了理论分析和方案的有效性。

Conclusion: 提出的轻量级安全通信方案能有效对抗RIS增强的移动窃听攻击，具有实用性和鲁棒性，适用于多种攻击场景。

Abstract: We pay our attention towards secure and robust communication in the presence
of a Reconfigurable Intelligent Surface (RIS)-enhanced mobile eavesdropping
attacker in Multiple-Input Multiple-Output (MIMO)wireless
networks.Specifically,we first provide a unifying framework that generalizes
specific intelligent wiretap model wherein the passive eavesdropper configured
with any number of antennas is potentially mobile and can actively optimize its
received signal strength with the help of RIS by intelligently manipulating
wiretap channel characteristics.To effectively mitigate this intractable
threat,we then propose a novel and lightweight secure communication scheme from
the perspective of information theory.The main idea is that the data processing
can in some cases be observed as communication channel,and a random
bit-flipping scheme is then carefully involved for the legitimate transmitter
to minimize the mutual information between the secret message and the passive
eavesdropper's received data.The Singular Value Decomposition (SVD)-based
precoding strategy is also implemented to optimize power allocation,and thus
ensure that the legitimate receiver is not subject to interference from this
random bit-flipping.The corresponding results depict that our secure
communication scheme is practically desired, which does not require any a prior
knowledge of the eavesdropper's full instantaneous Channel State Information
(ICSI). Furthermore,we consider the RIS optimization problem from the
eavesdropper's perspective,and provide RIS phase shift design solutions under
different attacking scenarios.Finally,the optimal detection schemes
respectively for the legitimate user and the eavesdropper are provided,and
comprehensive simulations are presented to verify our theoretical analysis and
show the effectiveness and robustness of our secure communication scheme across
a wide range of attacking scenarios.

</details>


### [44] [Byzantine Attacks in RIS-Enhanced Cooperative Spectrum Sensing: A Decision Fusion Perspective](https://arxiv.org/abs/2510.27175)
*Gaoyuan Zhang,Gaolei Song,Boyuan Li,Zijian Li,Baofeng Ji,Ruijuan Zheng,Guoqiang Zheng,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: 研究了RIS增强和中继辅助协作频谱感知中的拜占庭攻击，提出了不依赖全局信道状态信息和决策融合规则的统一攻击框架。


<details>
  <summary>Details</summary>
Motivation: 在移动认知无线电网络中，RIS和中继辅助的协作频谱感知面临拜占庭攻击威胁，传统方法对全局瞬时信道状态信息和决策融合规则依赖过强，不够实用。

Method: 构建RIS增强和中继辅助的CSS配置，开发信道和攻击感知的硬决策融合规则，提出小规模和大规模攻击场景下的最优拜占庭攻击策略。

Result: 最优攻击策略不需要全局瞬时信道状态信息，攻击有效性主要取决于拜占庭节点比例而非信道动态性，攻击策略可能不唯一。

Conclusion: 成功减轻了对全局信道状态信息和决策融合规则的依赖，使信道感知方法更加实用，并通过仿真验证了理论分析。

Abstract: From the perspective of hard decision fusion, we investigate Byzantine
attacks in Reconfigurable Intelligent Surface (RIS)-enhanced and
decode-and-forward relay-assisted Cooperative Spectrum Sensing (CSS) for mobile
Cognitive Radio Networks (CRNs) in this paper. Specially, a RIS-enhanced and
decode-and-forward relay-assisted CSS configuration is first constructed under
dynamic channel scenarios due to user mobility. Subsequently, the channel- and
attack-aware hard decision fusion rules are developed, and the optimal
channel-aware Byzantine attack strategies are then developed under both
small-scale and large-scale attacking scenarios. The corresponding results
depict that the optimal attack strategy does not require any a prior knowledge
of the global instantaneous Channel State Information (ICSI) (e.g. false alarm
probability and detection probability of all the secondary users), although
perfect acquisition of ICSI is clearly always not affordable from the attacker
perspective, which is further exacerbated by the RIS and decode-and-forward
relays involved in CSS and the potential high mobility of secondary users that
leads to fast fading channels. Furthermore, our counterintuitive results also
indicate that, regardless of the attacker's awareness of the decision fusion
rule, the optimal Byzantine attack can be achieved through a unifying
framework, the explicit attack strategy may be not unique, and the attacking
effectiveness is primarily determined by the fraction of the Byzantine nodes
rather than the channel dynamics. That is, to make the channel-aware approach
more practical, the challenge that the heavy reliance on the global ICSI and
decision fusion rule in obtaining the Byzantine attacks is successfully
relaxed. Finally, we empirically validate our theoretical analysis through
extensive simulations across a wide range of attacking scenarios.

</details>


### [45] [Dual-Scale Antenna Deployment for Pinching Antenna Systems](https://arxiv.org/abs/2510.27185)
*Xu Gan,Zhaolin Wang,Yuanwei Liu*

Main category: cs.IT

TL;DR: 提出了一种用于夹持天线系统的双尺度部署框架，包含粗调阶段和精调阶段，通过联合优化传输预编码、天线辐射功率和天线部署来最大化能效。


<details>
  <summary>Details</summary>
Motivation: 传统蜂窝系统和MIMO系统的能效有限，需要开发新的天线部署策略来提高无线通信系统的能源效率。

Method: 采用双尺度部署框架，包含粗调阶段的大范围天线转移和精调阶段的高精度重定位，提出基于惩罚的交替优化算法解决非凸耦合问题。

Result: 仿真验证了理论结果的准确性，PASS系统比传统无蜂窝架构能效提高约70%，比MIMO系统提高近两倍。

Conclusion: DSD分辨率和部署协议对实现PASS最大能效至关重要，所提框架能显著提升无线通信系统的能源效率。

Abstract: A dual-scale deployment (DSD) framework for pinching antenna systems (PASS)
is proposed. 1) In the first coarse stage, the pinching antenna (PA) is
transferred over a large-scale range at the waveguide level. 2) The refinement
stage performs small-scale relocation of the PA with high precision. Four PA
deployment protocols are provided in the proposed DSD framework. Then, a
practical power consumption model is proposed, based on which the theoretical
energy efficiency formulas for PASS are derived. The transmit precoding, PA
radiation power, and PA deployment are jointly optimized to maximize the energy
efficiency under the provided PA deployment protocols. To solve this
non-convex, highly coupled problem, a low-complexity penalty-based alternating
optimization algorithm is proposed. Simulation results validate the accuracy of
theoretical results and the convergence of the proposed algorithm. It is
demonstrated that: 1) PASS delivers about 70% higher energy efficiency than the
conventional cell-free architecture and nearly twofold improvement relative to
MIMO systems; 2) it is essential to specify the DSD resolution and deployment
protocol to achieve the maximum energy efficiency for PASS.

</details>


### [46] [Cross-Band Channel Impulse Response Prediction: Leveraging 3.5 GHz Channels for Upper Mid-Band](https://arxiv.org/abs/2510.27349)
*Fan-Hao Lin,Chi-Jui Sung,Chu-Hsiang Huang,Hui Chen,Chao-Kai Wen,Henk Wymeersch*

Main category: cs.IT

TL;DR: CIR-UNext是一个深度学习框架，利用3.5GHz信道冲激响应预测7GHz信道，通过注意力U-Net变体实现增益和相位预测，在复杂环境中表现出色，并可扩展为信道到通信映射的基础模型。


<details>
  <summary>Details</summary>
Motivation: 6G网络中准确跨频段信道预测至关重要，特别是在上中频段(FR3, 7-24GHz)，穿透损耗和阻塞问题严重。射线追踪计算密集，高频数据采集成本高，需要更高效的解决方案。

Method: 提出CIR-UNext框架，集成基于射线追踪的数据集流水线和注意力U-Net变体(AU-Net)，用于增益和相位预测，并扩展为Channel2ComMap基础模型用于MIMO-OFDM系统吞吐量预测。

Result: AU-Net-Aux模型在未见复杂环境中实现中值增益误差0.58dB和相位预测误差0.27弧度，在MIMO-OFDM系统吞吐量预测方面优于现有方法。

Conclusion: CIR-UNext为跨频段预测提供了高效可扩展的解决方案，支持6G网络中的定位、波束管理、数字孪生和智能资源分配等应用。

Abstract: Accurate cross-band channel prediction is essential for 6G networks,
particularly in the upper mid-band (FR3, 7--24 GHz), where penetration loss and
blockage are severe. Although ray tracing (RT) provides high-fidelity modeling,
it remains computationally intensive, and high-frequency data acquisition is
costly. To address these challenges, we propose CIR-UNext, a deep learning
framework designed to predict 7 GHz channel impulse responses (CIRs) by
leveraging abundant 3.5 GHz CIRs. The framework integrates an RT-based dataset
pipeline with attention U-Net (AU-Net) variants for gain and phase prediction.
The proposed AU-Net-Aux model achieves a median gain error of 0.58 dB and a
phase prediction error of 0.27 rad on unseen complex environments. Furthermore,
we extend CIR-UNext into a foundation model, Channel2ComMap, for throughput
prediction in MIMO-OFDM systems, demonstrating superior performance compared
with existing approaches. Overall, CIR-UNext provides an efficient and scalable
solution for cross-band prediction, enabling applications such as localization,
beam management, digital twins, and intelligent resource allocation in 6G
networks.

</details>


### [47] [Weight Enumerators From Equivalence Relations and MacWilliams Identities](https://arxiv.org/abs/2510.27358)
*S. T. Dougherty,C. Fernández-Córdoba*

Main category: cs.IT

TL;DR: 本文研究了有限域、有限阿贝尔群和有限Frobenius环上的码，定义了基于等价关系的重量枚举器，并确定了MacWilliams关系成立的条件。


<details>
  <summary>Details</summary>
Motivation: 研究码的重量枚举器及其在等价关系下的推广，探索MacWilliams关系在不同等价关系下的适用性。

Method: 定义了基于等价关系的重量枚举器，分析了MacWilliams关系成立的条件，并研究了特定等价关系下的重量枚举器。

Result: 确定了在哪些情况下基于等价关系的重量枚举器满足MacWilliams关系，并对特定等价关系进行了深入研究。

Conclusion: 为码的重量枚举器理论提供了更一般的框架，扩展了MacWilliams关系的适用范围。

Abstract: In this paper, we consider codes over finite fields, finite abelian groups,
and finite Frobenius rings. For such codes, the complete weight enumerator and
the Hamming weight enumerator serve as powerful tools. These two types of
weight enumerators satisfy the MacWilliams relations. We define the weight
enumerator of a code with respect to an equivalence relation and determine in
which cases the MacWilliams relations hold for this weight enumerator. We also
study some weight enumerators for specific equivalence relations.

</details>
