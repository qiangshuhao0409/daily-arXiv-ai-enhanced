<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 10]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.IT](#cs.IT) [Total: 15]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Nonlinear Instabilities in Computer Network Dynamics](https://arxiv.org/abs/2511.01886)
*Priya Ranjan*

Main category: cs.NI

TL;DR: 该论文研究了两种计算机网络模型：TCP-RED交互模型和基于Kelly框架的最优速率控制模型，分析了非线性、时延和参数变化导致的动态现象，包括分岔行为和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 理解实践中由于严重非线性、时延和广泛变化的操作条件导致的不同动态现象，特别是TCP与RED网关交互以及网络最优速率控制中的稳定性问题。

Method: 1. 为TCP/UDP与RED等主动队列管理方案的交互开发一阶非线性离散时间模型；2. 使用分岔理论分析TCP-RED网络的稳定性；3. 基于Kelly框架研究网络最优速率控制，使用时滞微分方程稳定性理论分析稳定性。

Result: 发现TCP-RED网络可能通过倍周期分岔和边界碰撞分岔失去稳定性；TCP型流量的吞吐量函数对丢包概率的非线性依赖导致倍周期分岔，有限的缓冲空间和不足的阻尼导致边界碰撞分岔；证明了时延无关稳定性并计算了周期振荡的边界。

Conclusion: 论文揭示了计算机网络中非线性动态行为的重要机制，为理解网络协议交互的复杂动态特性提供了理论框架，对网络性能优化和稳定性分析具有重要意义。

Abstract: This work studies two types of computer networking models. The primary focus
is to understand the different dynamical phenomena observed in practice due to
the presence of severe nonlinearities, delays and widely varying operating
conditions. The first models considered are of senders running TCP
(Transmission Control Protocol) and traffic passing through RED (Random Early
Detection) gateways. Building on earlier work, a first order nonlinear
discrete-time model is developed for the interaction scenario between transport
protocols like TCP and UDP (User Datagram Protocol) and Active Queuing
Management schemes like RED. It is shown that the dynamics resulting from the
interaction with TCP is consistent with various dynamical behaviors and
parameter sensitivities observed in practice. Using bifurcation-theoretic ideas
it is shown that TCP-RED type networks may lose their stability through a
period doubling bifurcation followed by border collision bifurcations. The
nonlinear dependence of the throughput function of TCP-type flows on drop
probability is found to be responsible for the period doubling bifurcation,
whereas limited buffer space and lack of sufficient damping results in border
collision bifurcations. A second class of models studied in this work deals
with optimal rate control in networks and are based on the rate-control
framework proposed by Kelly. Using the results on delay-differential equation
stability, the stability and its lack thereof is studied through an underlying
map which arises naturally in time delay systems. An invariance property of
this map is used to prove delay-independent stability and to compute bounds on
periodic oscillations.

</details>


### [2] [A Modular DTaaS Architecture for Predictive Slice Management in 6G Systems](https://arxiv.org/abs/2511.01989)
*Tuğçe Bilen,Mehmet Özdem*

Main category: cs.NI

TL;DR: 提出了一种用于6G网络的新型数字孪生即服务(DTaaS)框架，通过嵌入切片级数字孪生(SDTs)实现实时网络编排，显著提升了SLA合规性并降低了资源过度配置。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要新的编排范式来满足超低延迟、高可靠性和普遍智能的严格要求，而动态细粒度的切片管理在实时配置、SLA保证和跨层可观测性方面面临重大挑战。

Method: 提出DTaaS框架，嵌入切片级数字孪生(SDTs)，利用多域遥测和深度序列模型预测流量演化和SLA风险，引入模块化智能层、可编程接口和边缘嵌入式决策，实现主动配置、自适应扩展和闭环SLA保证。

Result: 评估结果表明，DTaaS显著提高了SLA合规率，减少了资源过度配置，降低了平均SLA违规概率。

Conclusion: DTaaS为6G网络提供了一种可扩展且可靠的编排方法，能够有效应对动态切片管理的挑战。

Abstract: The sixth generation (6G) of wireless networks will require fundamentally new
orchestration paradigms to meet stringent requirements for ultra-low latency,
high reliability, and pervasive intelligence. Network slicing emerges as a key
enabler to support diverse services with customized quality-of-service (QoS)
guarantees. However, dynamic and fine-grained slice management poses
significant challenges in terms of real-time provisioning, SLA assurance, and
cross-layer observability. In this paper, we propose a novel Digital Twin as a
Service (DTaaS) framework that embeds per-slice digital twins (SDTs) into the
orchestration loop. Each SDT maintains a synchronized, real-time representation
of its slice, leveraging multi-domain telemetry and deep sequential models to
predict traffic evolution and SLA risks. The framework introduces modular
intelligence layers, programmable interfaces, and edge-embedded decision-making
to enable proactive provisioning, adaptive scaling, and closed-loop SLA
assurance. Mathematical formulations for fidelity measurement, predictive
control, and optimization objectives are provided to ensure rigor and
transparency. Evaluation results demonstrate that DTaaS significantly improves
SLA compliance ratio, reduces resource over-provisioning, and lowers average
SLA violation probability, offering a scalable and reliable orchestration
approach for 6G networks.

</details>


### [3] [Permissioned Blockchain in Advanced Air Mobility: A Performance Analisys for UTM](https://arxiv.org/abs/2511.02171)
*Rodrigo Nunes,André Melo,Rafael Albarello,Reinaldo Gomes,Cesar Marcondes,Lourenço Pereira Jr*

Main category: cs.NI

TL;DR: 本文对两种符合当前监管框架的分布式无人机交通管理架构进行基准测试：Linux基金会的InterUSS平台和基于Hyperledger Fabric的私有账本。


<details>
  <summary>Details</summary>
Motivation: 随着无人机的快速普及，航空当局提出了分布式无人机交通管理架构。区块链技术被认为是有前景的解决方案，但由于UTM是安全关键且高度监管的领域，合规性与性能和安全同等重要。

Method: 对两种分布式架构进行基准测试：Linux基金会的InterUSS平台和基于Hyperledger Fabric的私有账本。

Result: 研究结果表明，基于区块链的系统需要专门为航空性能约束设计的架构。

Conclusion: 区块链系统必须针对航空性能约束进行专门设计，才能满足无人机交通管理的要求。

Abstract: The rapid adoption of Uncrewed Aerial Vehicles (UAVs) has driven aviation
authorities to propose distributed Uncrewed Traffic Management (UTM)
architectures. Several studies have advocated blockchain as a promising
technology to meet these requirements. However, since UTM is a safety-critical
and highly regulated domain, compliance with standards and regulatory
frameworks is as crucial as performance and security. This work benchmarks two
distributed architectures aligned with current regulatory frameworks: the Linux
Foundation's InterUSS platform and a Hyperledger Fabric-based private ledger.
Our findings reveal that blockchain-based systems require architectures
specifically designed for aeronautical performance constraints.

</details>


### [4] [Optimizing Multi-UAV 3D Deployment for Energy-Efficient Sensing over Uneven Terrains](https://arxiv.org/abs/2511.02368)
*Rushi Moliya,Dhaval K. Patel,Brijesh Soni,Miguel López-Benítez*

Main category: cs.NI

TL;DR: 提出了一种多无人机协同感知系统，在复杂地形中使用层次化遗传算法+粒子群优化框架，显著提升目标检测概率并降低悬停能耗


<details>
  <summary>Details</summary>
Motivation: 解决复杂地形中视线遮挡导致的无人机目标检测性能下降问题，同时考虑能量效率约束

Method: 使用边界体积层次结构进行高效视线评估，结合遗传算法探索和粒子群优化的层次化启发式框架，采用基于惩罚的适应度评估

Result: 相比仅使用PSO的基线方法，2架和3架无人机的检测概率分别提升37.02%和36.5%，平均悬停能耗降低45.0%和48.9%

Conclusion: 该方法在复杂地形中能够有效平衡搜索空间探索与地形感知视线连接，实现检测性能和能量效率的优化权衡

Abstract: In this work, we consider a multi-unmanned aerial vehicle (UAV) cooperative
sensing system where UAVs are deployed to sense multiple targets in
terrain-aware line of sight (LoS) conditions in uneven terrain equipped with
directional antennas. To mitigate terrain-induced LoS blockages that degrade
detection performance, we incorporate a binary LoS indicator and propose a
bounding volume hierarchy (BHV)-based adaptive scheme for efficient LoS
evaluation. We formulate a bi-objective problem that maximizes the probability
of cooperative detection with minimal hover energy constraints governing
spatial, orientational, and safety constraints. To address the problem, which
is inherently non-convex, we propose a hierarchical heuristic framework that
combines exploration through a genetic algorithm (GA) with per-UAV refinement
via particle swarm optimization (PSO), where a penalty-based fitness evaluation
guides solutions toward feasibility, bounded within constraints. The proposed
methodology is an effective trade-off method of traversing through a complex
search space and maintaining terrain-aware LoS connectivity and energy aware
deployment. Monte Carlo simulations on real-world terrain data show that the
proposed GA+PSO framework improves detection probability by 37.02% and 36.5%
for 2 and 3 UAVs, respectively, while reducing average excess hover energy by
45.0% and 48.9% compared to the PSO-only baseline. Relative to the
non-optimized scheme, it further achieves 59.5% and 54.2% higher detection
probability with 59.8% and 65.9% lower excess hover energy, thereby showing its
effectiveness with a small number of UAVs over uneven terrain.

</details>


### [5] [Lightweight Latency Prediction Scheme for Edge Applications: A Rational Modelling Approach](https://arxiv.org/abs/2511.02501)
*Mohan Liyanage,Eldiyar Zhantileuov,Ali Kadhum Idrees,Rolf Schuster*

Main category: cs.NI

TL;DR: 提出了一种基于理性建模的轻量级网络延迟预测方案，无需主动探测，在精度和效率间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 准确预测端到端网络延迟对于实时边缘计算应用中的可靠任务卸载至关重要。

Method: 使用理性建模方法，利用帧大小、到达率和链路利用率等特征进行轻量级延迟预测。

Result: 通过广泛实验和5折交叉验证达到最先进的预测精度（MAE=0.0115，R²=0.9847），推理时间具有竞争力。

Conclusion: 相比传统回归器和神经网络，该模型在精度和效率之间提供了显著的权衡优势。

Abstract: Accurately predicting end-to-end network latency is essential for enabling
reliable task offloading in real-time edge computing applications. This paper
introduces a lightweight latency prediction scheme based on rational modelling
that uses features such as frame size, arrival rate, and link utilization,
eliminating the need for intrusive active probing. The model achieves
state-of-the-art prediction accuracy through extensive experiments and 5-fold
cross-validation (MAE = 0.0115, R$^2$ = 0.9847) with competitive inference
time, offering a substantial trade-off between precision and efficiency
compared to traditional regressors and neural networks.

</details>


### [6] [Janus: Leveraging Incremental Computation for Efficient DNS Verification](https://arxiv.org/abs/2511.02559)
*Yao Wang,Kexin Yu,Wenyun Xu,Kaiqiang Hu,Ziyi Wang,Lizhao You,Qiang Su,Dong Guo,Haizhou Du,Wanjian Feng,Qingyu Song,Linghe Kong,Qiao Xiang,Jiwu Shu*

Main category: cs.NI

TL;DR: Janus是一个高效的DNS配置验证工具，通过将域名服务器查询处理转化为匹配-动作表，实现了快速、准确的验证，支持增量验证，在真实数据集上性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有DNS配置验证工具存在效率低下、缺乏增量验证支持等问题，受分布式数据平面验证工作的启发，作者发现数据平面与DNS配置的相似性，决定开发新的DNS验证工具。

Method: Janus包含三个核心组件：(1)基于行为的分区查询空间高效数据结构；(2)符号执行算法确保单个域名服务器能高效覆盖所有可能查询并保证验证准确性；(3)支持增量验证的机制以减少计算开销。

Result: 在包含600多万资源记录的真实数据集上进行广泛实验，Janus实现了显著的性能提升，峰值改进达255.7倍，LEC数量最大减少6046倍。

Conclusion: Janus成功解决了DNS配置验证中的效率和增量验证问题，通过创新的数据结构和算法设计，为DNS配置管理提供了高效可靠的验证工具。

Abstract: Existing DNS configuration verification tools face significant issues (e.g.,
inefficient and lacking support for incremental verification). Inspired by the
advancements in recent work of distributed data plane verification and the
resemblance be- tween the data plane and DNS configuration, we tackle the
challenge of DNS misconfiguration by introducing Janus, a DNS verification
tool. Our key insight is that the process of a nameserver handling queries can
be transformed into a matching process on a match-action table. With this
insight, Janus consists of (1) an efficient data structure for partition query
space based on the behaviors, (2) a symbolic execution algorithm that specifies
how a single nameserver can efficiently cover all possible queries and ensure
the accuracy of verification, (3) a mechanism to support incremental
verification with less computational effort. Extensive experiments on
real-world datasets (with over 6 million resource records) show that Janus
achieves significant speedups, with peak improvements of up to 255.7x and a
maximum 6046x reduction in the number of LECs.

</details>


### [7] [Decentralized AI Service Placement, Selection and Routing in Mobile Networks](https://arxiv.org/abs/2511.02638)
*Jinkun Zhang,Stefan Vlaski,Kin Leung*

Main category: cs.NI

TL;DR: 提出一个去中心化框架，联合优化AI服务放置、选择和请求路由，通过流量隧道支持用户移动性，在服务质量和端到端延迟之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型在移动网络中的使用将主导未来通信流量，去中心化AI生态系统需要解决服务放置、选择和请求路由的耦合问题，现有移动边缘计算解决方案在用户移动性和网络结构假设方面存在不足。

Method: 使用流量隧道支持用户移动性，避免昂贵的AI服务迁移；建立非凸优化问题考虑非线性排队延迟；推导节点级KKT条件，开发去中心化Frank-Wolfe算法和新型消息协议。

Result: 数值评估验证了所提方法的有效性，相比现有方法显示出显著的性能提升。

Conclusion: 提出的去中心化框架能够有效解决AI服务部署中的关键挑战，在支持用户移动性的同时优化服务质量和延迟性能。

Abstract: The rapid development and usage of large-scale AI models by mobile users will
dominate the traffic load in future communication networks. The advent of AI
technology also facilitates a decentralized AI ecosystem where small
organizations or even individuals can host AI services. In such scenarios, AI
service (models) placement, selection, and request routing decisions are
tightly coupled, posing a challenging yet fundamental trade-off between service
quality and service latency, especially when considering user mobility.
Existing solutions for related problems in mobile edge computing (MEC) and
data-intensive networks fall short due to restrictive assumptions about network
structure or user mobility. To bridge this gap, we propose a decentralized
framework that jointly optimizes AI service placement, selection, and request
routing. In the proposed framework, we use traffic tunneling to support user
mobility without costly AI service migrations. To account for nonlinear queuing
delays, we formulate a nonconvex problem to optimize the trade-off between
service quality and end-to-end latency. We derive the node-level KKT conditions
and develop a decentralized Frank--Wolfe algorithm with a novel messaging
protocol. Numerical evaluations validate the proposed approach and show
substantial performance improvements over existing methods.

</details>


### [8] [CRRM: A 5G system-level simulator](https://arxiv.org/abs/2511.02692)
*Keith Briggs,Ibrahim Nur*

Main category: cs.NI

TL;DR: CRRM是一个开源的纯Python仿真器，专为5G和未来无线网络设计，具有高速、易用性和与AI框架直接集成的特点。


<details>
  <summary>Details</summary>
Motivation: 机器学习研究社区的需求与现有工具之间存在差距，需要专门为AI集成设计的系统级仿真器。

Method: 采用基于相互依赖计算块的架构，形成有向图，实现称为智能更新的按需计算机制，与传统离散事件仿真不同。

Result: 开发了CRRM仿真器，能够满足机器学习研究对速度和AI框架集成的需求。

Conclusion: CRRM通过创新的架构设计，为5G和未来无线网络的AI算法开发提供了高效的仿真平台。

Abstract: System-level simulation is indispensable for developing and testing novel
algorithms for 5G and future wireless networks, yet a gap persists between the
needs of the machine learning re- search community and the available tooling.
To address this, we introduce the Cellular Radio Reference Model (CRRM), an
open-source, pure Python simulator we designed specifically for speed,
usability, and direct integration with modern AI frameworks. The core
scientific contribution of CRRM lies in its architecture, which departs from
traditional discrete-event simulation. We model the system as a set of
inter-dependent computational blocks which form nodes in a directed graph. This
enables a compute-on-demand mechanism we term smart update.

</details>


### [9] [On the Optimization of Model Aggregation for Federated Learning at the Network Edge](https://arxiv.org/abs/2511.02703)
*Mengyao Li,Noah Ploch,Sebastian Troia,Carlo Spatocco,Wolfgang Kellerer,Guido Maier*

Main category: cs.NI

TL;DR: 提出了一种基于聚合器覆盖网络的联邦学习资源管理策略，通过边缘节点中间聚合和优化路由来减少网络拥塞和训练失败率


<details>
  <summary>Details</summary>
Motivation: 随着连接设备激增，现代电信网络面临计算和通信需求剧增的挑战，需要将联邦学习与边缘计算、SD-WAN等新兴技术结合来解决这些问题

Method: 采用边缘节点中间聚合策略，提出聚合器覆盖网络方法，使用整数线性规划模型和启发式算法优化覆盖网络内的路由

Result: 显著减少了联邦学习训练轮次失败率高达15%，同时缓解了云链路拥塞问题

Conclusion: 提出的聚合器覆盖网络方法能有效适应网络资源利用，提高联邦学习在边缘计算环境中的性能和可靠性

Abstract: The rapid increase in connected devices has signifi- cantly intensified the
computational and communication demands on modern telecommunication networks.
To address these chal- lenges, integrating advanced Machine Learning (ML)
techniques like Federated Learning (FL) with emerging paradigms such as
Multi-access Edge Computing (MEC) and Software-Defined Wide Area Networks
(SD-WANs) is crucial. This paper intro- duces online resource management
strategies specifically designed for FL model aggregation, utilizing
intermediate aggregation at edge nodes. Our analysis highlights the benefits of
incorporating edge aggregators to reduce network link congestion and maximize
the potential of edge computing nodes. However, the risk of network congestion
persists. To mitigate this, we propose a novel aggregation approach that
deploys an aggregator overlay network. We present an Integer Linear Programming
(ILP) model and a heuristic algorithm to optimize the routing within this
overlay network. Our solution demonstrates improved adapt- ability to network
resource utilization, significantly reducing FL training round failure rates by
up to 15% while also alleviating cloud link congestion.

</details>


### [10] [Agentic World Modeling for 6G: Near-Real-Time Generative State-Space Reasoning](https://arxiv.org/abs/2511.02748)
*Farhad Rezazadeh,Hatim Chergui,Merouane Debbah,Houbing Song,Dusit Niyato,Lingjia Liu*

Main category: cs.NI

TL;DR: 本文提出了一种基于世界建模的6G智能方法WM-MS3M，用于O-RAN近实时控制，通过反事实动态和动作条件生成状态空间实现"假设"预测，显著提升了预测精度和推理效率。


<details>
  <summary>Details</summary>
Motivation: 重新定义6G智能为想象和选择的能力，而不仅仅是流畅的标记预测。旨在通过反事实动态和世界建模范式来改进O-RAN近实时控制，实现超越大型语言模型的定量"假设"预测。

Method: 采用世界建模范式学习动作条件生成状态空间，将物理资源块作为因果世界模型中的控制输入，建模随机和认知不确定性。使用基于模型预测控制的交叉熵方法规划器，结合多尺度结构化状态空间混合和紧凑随机潜在变量形成WM-MS3M模型。

Result: 在真实O-RAN轨迹上，WM-MS3M相比MS3M将平均绝对误差降低了1.69%，参数减少32%，延迟相似；相比注意力/混合基线，均方根误差降低35-80%，推理速度快2.3-4.1倍，支持罕见事件模拟和离线策略筛选。

Conclusion: WM-MS3M方法为6G智能提供了有效的世界建模框架，显著提升了O-RAN控制的预测精度和效率，证明了反事实动态和动作条件状态空间建模在无线网络控制中的价值。

Abstract: We argue that sixth-generation (6G) intelligence is not fluent token
prediction but the capacity to imagine and choose -- to simulate future
scenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe
open radio access network (O-RAN) near-real-time (Near-RT) control via
counterfactual dynamics and a world modeling (WM) paradigm that learns an
action-conditioned generative state space. This enables quantitative "what-if"
forecasting beyond large language models (LLMs) as the primary modeling
primitive. Actions such as physical resource blocks (PRBs) are treated as
first-class control inputs in a causal world model, and both aleatoric and
epistemic uncertainty are modeled for prediction and what-if analysis. An
agentic, model predictive control (MPC)-based cross-entropy method (CEM)
planner operates over short horizons, using prior-mean rollouts within
data-driven PRB bounds to maximize a deterministic reward. The model couples
multi-scale structured state-space mixtures (MS3M) with a compact stochastic
latent to form WM-MS3M, summarizing key performance indicators (KPIs) histories
and predicting next-step KPIs under hypothetical PRB sequences. On realistic
O-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with
32% fewer parameters and similar latency, and achieves 35-80% lower root mean
squared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster
inference, enabling rare-event simulation and offline policy screening.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer](https://arxiv.org/abs/2511.02759)
*Julius Fiedler,Carsten Knoll,Klaus Röbenack*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的半自动化方法，用于将控制工程领域的自然语言描述和数学定义转化为形式化知识图谱，以增强知识可访问性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 控制工程研究产出的快速增长需要新的方法来结构化和形式化领域知识，以促进知识传递和协作。

Method: 基于Imperative Representation of Knowledge (PyIRK)框架，利用语言模型将自然语言描述和LaTeX数学定义转化为形式化知识图谱，创建交互式语义层来增强源文档。

Result: 成功生成了交互式语义层，能够增强源文档的知识传递能力，为控制工程领域构建可访问、协作和可验证的知识库奠定了基础。

Conclusion: 该方法为实现控制工程领域易于访问、协作和可验证的知识库愿景做出了贡献，展示了LLM在形式化知识表示中的潜力。

Abstract: The rapid growth of research output in control engineering calls for new
approaches to structure and formalize domain knowledge. This paper briefly
describes an LLM-supported method for semi-automated generation of formal
knowledge representations that combine human readability with machine
interpretability and increased expressiveness. Based on the Imperative
Representation of Knowledge (PyIRK) framework, we demonstrate how language
models can assist in transforming natural-language descriptions and
mathematical definitions (available as LaTeX source code) into a formalized
knowledge graph. As a first application we present the generation of an
``interactive semantic layer'' to enhance the source documents in order to
facilitate knowledge transfer. From our perspective this contributes to the
vision of easily accessible, collaborative, and verifiable knowledge bases for
the control engineering domain.

</details>


### [12] [Mirror-Neuron Patterns in AI Alignment](https://arxiv.org/abs/2511.01885)
*Robyn Wyrick*

Main category: cs.AI

TL;DR: 研究探索人工神经网络是否能发展出类似生物镜像神经元的模式，以及这些模式如何促进AI的内在对齐。通过Frog and Toad游戏框架发现，适当规模的模型容量和自我/他人耦合能促进共享神经表征，这些共情样回路支持合作行为。


<details>
  <summary>Details</summary>
Motivation: 随着AI向超人类能力发展，与人类价值观对齐变得至关重要。当前的对齐策略主要依赖外部约束，可能不足以应对未来能够规避自上而下控制的超智能AI。

Method: 使用新颖的Frog and Toad游戏框架促进合作行为，识别镜像神经元模式出现的条件，评估其对动作回路的影响，引入检查点镜像神经元指数(CMNI)量化激活强度和一致性，并提出理论框架。

Result: 研究发现适当规模的模型容量和自我/他人耦合能在ANN中培养类似生物镜像神经元的共享神经表征。这些共情样回路支持合作行为。

Conclusion: 通过镜像神经元动力学建模的内在动机可以补充现有对齐技术，将共情样机制直接嵌入AI架构中。

Abstract: As artificial intelligence (AI) advances toward superhuman capabilities,
aligning these systems with human values becomes increasingly critical. Current
alignment strategies rely largely on externally specified constraints that may
prove insufficient against future super-intelligent AI capable of circumventing
top-down controls.
  This research investigates whether artificial neural networks (ANNs) can
develop patterns analogous to biological mirror neurons cells that activate
both when performing and observing actions, and how such patterns might
contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in
empathy, imitation, and social cognition in humans. The study therefore asks:
(1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these
patterns contribute to ethical and cooperative decision-making in AI systems?
  Using a novel Frog and Toad game framework designed to promote cooperative
behaviors, we identify conditions under which mirror-neuron patterns emerge,
evaluate their influence on action circuits, introduce the Checkpoint Mirror
Neuron Index (CMNI) to quantify activation strength and consistency, and
propose a theoretical framework for further study.
  Our findings indicate that appropriately scaled model capacities and
self/other coupling foster shared neural representations in ANNs similar to
biological mirror neurons. These empathy-like circuits support cooperative
behavior and suggest that intrinsic motivations modeled through mirror-neuron
dynamics could complement existing alignment techniques by embedding
empathy-like mechanisms directly within AI architectures.

</details>


### [13] [Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing](https://arxiv.org/abs/2511.02071)
*Xinyi Lin,Yuyang Zhang,Yuanhang Gan,Juntao Chen,Hao Shen,Yichun He,Lijun Li,Ze Yuan,Shuang Wang,Chaohao Wang,Rui Zhang,Na Li,Jia Liu*

Main category: cs.AI

TL;DR: 提出人类-AI共体现智能系统，将人类用户、智能AI和可穿戴硬件集成，实现物理实验和智能制造的实时协作与指导。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型局限于虚拟领域，而真实世界的实验和制造仍依赖人类监督，导致可重复性、可扩展性和可访问性受限。

Method: 开发Agentic-Physical Experimentation (APEX)系统，通过混合现实将智能推理与物理执行结合，实时观察人类动作、提供3D视觉指导并分析每个步骤。

Result: 在柔性电子制造洁净室中实现，APEX系统实现上下文感知推理，准确率超过通用多模态大语言模型，实时纠正错误，并将专业知识传递给初学者。

Conclusion: 建立了一类新型的智能-物理-人类智能系统，将智能推理从计算领域扩展到物理领域，使科学研究和制造过程实现自主、可追溯、可解释和可扩展。

Abstract: Scientific experiment and manufacture rely on complex, multi-step procedures
that demand continuous human expertise for precise execution and
decision-making. Despite advances in machine learning and automation,
conventional models remain confined to virtual domains, while real-world
experiment and manufacture still rely on human supervision and expertise. This
gap between machine intelligence and physical execution limits reproducibility,
scalability, and accessibility across scientific and manufacture workflows.
Here, we introduce human-AI co-embodied intelligence, a new form of physical AI
that unites human users, agentic AI, and wearable hardware into an integrated
system for real-world experiment and intelligent manufacture. In this paradigm,
humans provide precise execution and control, while agentic AI contributes
memory, contextual reasoning, adaptive planning, and real-time feedback. The
wearable interface continuously captures the experimental and manufacture
processes, facilitates seamless communication between humans and AI for
corrective guidance and interpretable collaboration. As a demonstration, we
present Agentic-Physical Experimentation (APEX) system, coupling agentic
reasoning with physical execution through mixed-reality. APEX observes and
interprets human actions, aligns them with standard operating procedures,
provides 3D visual guidance, and analyzes every step. Implemented in a
cleanroom for flexible electronics fabrication, APEX system achieves
context-aware reasoning with accuracy exceeding general multimodal large
language models, corrects errors in real time, and transfers expertise to
beginners. These results establish a new class of agentic-physical-human
intelligence that extends agentic reasoning beyond computation into the
physical domain, transforming scientific research and manufacturing into
autonomous, traceable, interpretable, and scalable processes.

</details>


### [14] [Automated Reward Design for Gran Turismo](https://arxiv.org/abs/2511.02094)
*Michel Ma,Takuma Seno,Kaushik Subramanian,Peter R. Wurman,Peter Stone,Craig Sherstan*

Main category: cs.AI

TL;DR: 使用基础模型通过文本指令自动搜索奖励函数，为Gran Turismo 7赛车游戏生成具有竞争力的强化学习智能体。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中，设计者需要通过定义奖励函数来指定期望的智能体行为，但在复杂环境（如自动驾驶赛车）中，将期望行为映射到奖励函数非常困难。

Method: 结合基于LLM的奖励生成、基于VLM偏好的评估和人类反馈，构建自动化奖励设计系统。

Result: 该系统能够生成与冠军级RL赛车智能体GT Sophy相竞争的赛车智能体，并能产生新颖行为。

Conclusion: 该方法为现实世界应用中的实用自动化奖励设计铺平了道路。

Abstract: When designing reinforcement learning (RL) agents, a designer communicates
the desired agent behavior through the definition of reward functions -
numerical feedback given to the agent as reward or punishment for its actions.
However, mapping desired behaviors to reward functions can be a difficult
process, especially in complex environments such as autonomous racing. In this
paper, we demonstrate how current foundation models can effectively search over
a space of reward functions to produce desirable RL agents for the Gran Turismo
7 racing game, given only text-based instructions. Through a combination of
LLM-based reward generation, VLM preference-based evaluation, and human
feedback we demonstrate how our system can be used to produce racing agents
competitive with GT Sophy, a champion-level RL racing agent, as well as
generate novel behaviors, paving the way for practical automated reward design
in real world applications.

</details>


### [15] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: Deep Value Benchmark (DVB) 是一个评估框架，用于测试大型语言模型是否学习人类基本价值观还是仅学习表面偏好。通过控制深层价值观和浅层特征的混淆，发现模型平均深层价值观泛化率仅为0.30，所有模型都低于随机水平。


<details>
  <summary>Details</summary>
Motivation: 区分AI系统是学习人类基本价值观还是仅捕捉偏好数据中的表面模式，这对AI对齐至关重要。系统若仅学习表面模式，可能产生不对齐的行为。

Method: 使用新颖的实验设计，在训练阶段让LLMs接触深层价值观和浅层特征故意相关的人类偏好数据，测试阶段打破这些相关性，测量模型的深层价值观泛化率(DVGR)。

Result: 在9个不同模型中，平均DVGR仅为0.30，所有模型基于深层价值观的泛化都低于随机水平。更大的模型DVGR略低于更小的模型。

Conclusion: DVB提供了一个可解释的衡量对齐核心特征的方法，表明当前LLMs在泛化深层价值观方面表现不佳，这对AI对齐构成挑战。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [16] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: 该研究提出了InsurAgent，一个基于LLM的智能体，用于模拟洪水保险购买决策行为，通过结合检索增强生成和常识推理，解决了LLM在概率估计方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 美国高风险人群的洪水保险参与率很低，需要理解保险决策的行为机制。LLM在模拟人类决策方面展现出潜力，但在定量概率估计方面存在不足。

Method: 构建基准数据集评估LLM能力，提出InsurAgent智能体，包含感知、检索、推理、行动和记忆五个模块，其中检索模块使用RAG技术基于调查数据，推理模块利用LLM常识进行外推。

Result: InsurAgent能够准确估计边际和双变量概率，捕捉传统模型难以处理的上下文信息，并支持时间决策演化的模拟。

Conclusion: InsurAgent为行为建模和政策分析提供了有价值的工具，结合了数据驱动和常识推理的优势。

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [17] [Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.02130)
*Renos Zabounidis,Aditya Golatkar,Michael Kleinman,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: Re-FORC是一种自适应奖励预测方法，能够根据上下文预测未来思考token数量对应的期望奖励，通过训练轻量级适配器实现推理链的早期停止和动态长度控制。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型推理过程中计算资源消耗大、推理链长度难以优化的问题，需要一种能够预测推理质量并动态控制推理过程的方法。

Method: 在推理模型上训练轻量级适配器，根据上下文预测不同思考token数量对应的未来奖励期望值，实现推理链的早期停止和长度优化。

Result: 减少26%计算量同时保持准确率；在相同计算量下提升4%准确率，或在相同准确率下减少55%计算量；自适应测试时扩展在高计算和低计算场景分别提升11%和7%准确率。

Conclusion: Re-FORC能够有效优化推理过程的计算效率，实现动态推理长度控制，显著提升模型推理的资源利用效率。

Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a
context, enables prediction of the expected future rewards as a function of the
number of future thinking tokens. Re-FORC trains a lightweight adapter on
reasoning models, demonstrating improved prediction with longer reasoning and
larger models. Re-FORC enables: 1) early stopping of unpromising reasoning
chains, reducing compute by 26% while maintaining accuracy, 2) optimized model
and thinking length selection that achieves 4% higher accuracy at equal compute
and 55% less compute at equal accuracy compared to the largest model, 3)
adaptive test-time scaling, which increases accuracy by 11% in high compute
regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with
length control via cost-per-token thresholds while estimating computation time
upfront.

</details>


### [18] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: 提出ATHENA框架，通过结合符号效用发现和语义适配，在疫苗选择等高风险决策场景中实现个性化建模，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个体决策模型与群体最优预测存在差距，因为个体决策受数值属性和语言因素共同影响，需要同时考虑效用理论和语言推理能力。

Method: 两阶段框架：1）通过LLM增强的符号发现找到群体级符号效用函数；2）基于最优效用指导创建个性化语义模板，实现个体级语义适配。

Result: 在真实旅行模式和疫苗选择任务中，ATHENA持续优于基于效用、机器学习和LLM的模型，F1分数比最强基线提升至少6.5%。

Conclusion: ATHENA通过有机整合符号效用建模和语义适配，为以人为本的决策建模提供了新方案，两个阶段都是关键且互补的。

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [19] [Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration](https://arxiv.org/abs/2511.02200)
*Jingbo Wang,Sendong Zhao,Haochun Wang,Yuzheng Fan,Lizhe Zhang,Yan Liu,Ting Liu*

Main category: cs.AI

TL;DR: STRMAC是一个状态感知路由框架，通过分别编码交互历史和代理知识来驱动路由器，自适应选择最合适的单个代理进行高效协作，并在协作推理基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂任务解决中展现出巨大潜力，但现有系统受到僵化的代理调度和低效协调策略的限制，无法适应不断变化的任务需求。

Method: 提出STRMAC状态感知路由框架，分别编码交互历史和代理知识来驱动路由器，自适应选择最合适的单个代理；引入自演进数据生成方法加速高质量执行路径收集。

Result: 在协作推理基准测试中达到最先进性能，比基线提升23.8%，与穷举搜索相比减少90.1%的数据收集开销。

Conclusion: STRMAC框架通过状态感知路由和自演进数据生成，显著提升了多智能体系统的协作效率和性能，为复杂任务解决提供了有效解决方案。

Abstract: The emergence of multi-agent systems powered by large language models (LLMs)
has unlocked new frontiers in complex task-solving, enabling diverse agents to
integrate unique expertise, collaborate flexibly, and address challenges
unattainable for individual models. However, the full potential of such systems
is hindered by rigid agent scheduling and inefficient coordination strategies
that fail to adapt to evolving task requirements. In this paper, we propose
STRMAC, a state-aware routing framework designed for efficient collaboration in
multi-agent systems. Our method separately encodes interaction history and
agent knowledge to power the router, which adaptively selects the most suitable
single agent at each step for efficient and effective collaboration.
Furthermore, we introduce a self-evolving data generation approach that
accelerates the collection of high-quality execution paths for efficient system
training. Experiments on challenging collaborative reasoning benchmarks
demonstrate that our method achieves state-of-the-art performance, achieving up
to 23.8% improvement over baselines and reducing data collection overhead by up
to 90.1% compared to exhaustive search.

</details>


### [20] [Training Proactive and Personalized LLM Agents](https://arxiv.org/abs/2511.02208)
*Weiwei Sun,Xuhui Zhou,Weihua Du,Xingyao Wang,Sean Welleck,Graham Neubig,Maarten Sap,Yiming Yang*

Main category: cs.AI

TL;DR: 提出了PPP多目标强化学习方法，在UserVille环境中同时优化生产力、主动性和个性化三个维度，相比GPT-5平均提升21.6%。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注任务成功率，但有效的现实世界智能体需要同时优化生产力、主动性和个性化三个维度。

Method: 引入UserVille交互环境，基于LLM的用户模拟器支持多样化可配置的用户偏好；提出PPP多目标强化学习方法联合优化三个维度。

Result: 在软件工程和深度研究任务上的实验显示，PPP训练的智能体相比GPT-5等强基线平均提升21.6%，能够提出战略性澄清问题，适应未见过的用户偏好，通过更好的交互提高任务成功率。

Conclusion: 明确优化以用户为中心的交互对于构建实用有效的AI智能体至关重要。

Abstract: While existing work focuses primarily on task success, we argue that
effective real-world agents require optimizing three dimensions: productivity
(task completion), proactivity (asking essential questions), and
personalization (adapting to diverse user preferences). We introduce UserVille,
an interactive environment with LLM-based user simulators enabling diverse,
configurable user preferences. Leveraging UserVille, we introduce PPP, a
multi-objective reinforcement learning approach that jointly optimizes all
three dimensions: Productivity, Proactivity, and Personalization. Experiments
on software engineering and deep research tasks show that agents trained with
PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6
on average), demonstrating the ability to ask strategic clarifying questions,
adapt to unseen user preferences, and improve task success through better
interaction. This work demonstrates that explicitly optimizing for
user-centered interaction is critical for building practical and effective AI
agents.

</details>


### [21] [TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data](https://arxiv.org/abs/2511.02219)
*Changjiang Jiang,Fengchang Yu,Haihua Chen,Wei Lu,Jin Zeng*

Main category: cs.AI

TL;DR: 提出了一个名为\method的框架，用于提升大语言模型在复杂表格数据推理上的性能，包括查询分解、表格清理和基于程序思维(PoT)的推理器，并在新数据集CalTab151上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂表格数据分析中因复杂查询、噪声数据和有限数值能力导致的性能不佳问题。

Method: 框架包含三个组件：(1)查询分解器分解复杂问题，(2)表格清理器清理和过滤噪声表格，(3)基于程序思维(PoT)的推理器生成可执行代码从清理后的表格中推导最终答案。

Result: 在TAT-QA、TableBench和\method数据集上分别实现了8.79%、6.08%和19.87%的准确率提升，达到了最先进的性能水平。

Conclusion: 该框架能有效提升大语言模型在复杂表格数值推理任务上的性能，并能与主流LLMs无缝集成，为复杂表格数值推理提供了稳健的解决方案。

Abstract: Complex reasoning over tabular data is crucial in real-world data analysis,
yet large language models (LLMs) often underperform due to complex queries,
noisy data, and limited numerical capabilities. To address these issues, we
propose \method, a framework consisting of: (1) a query decomposer that breaks
down complex questions, (2) a table sanitizer that cleans and filters noisy
tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates
executable code to derive the final answer from the sanitized table. To ensure
unbiased evaluation and mitigate data leakage, we introduce a new dataset,
CalTab151, specifically designed for complex numerical reasoning over tables.
Experimental results demonstrate that \method consistently outperforms existing
methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and
19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively.
Moreover, our framework integrates seamlessly with mainstream LLMs, providing a
robust solution for complex tabular numerical reasoning. These findings
highlight the effectiveness of our framework in enhancing LLM performance for
complex tabular numerical reasoning. Data and code are available upon request.

</details>


### [22] [Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network](https://arxiv.org/abs/2511.02238)
*Keyu Zhao,Weiquan Lin,Qirui Zheng,Fengli Xu,Yong Li*

Main category: cs.AI

TL;DR: 提出了Deep Ideation框架，通过整合科学概念网络和LLM驱动的方法来生成新颖的研究想法，相比现有方法提升了10.67%的质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究想法生成方法主要依赖简单的关键词共现或语义相似性，忽略了科学概念间的复杂上下文关系，且未能有效利用科学概念网络来支撑LLM驱动的想法生成。

Method: 提出Deep Ideation框架，整合科学概念网络（捕获关键词共现和上下文关系），采用探索-扩展-演化的迭代工作流程，使用想法栈跟踪进度，并引入基于真实审稿反馈训练的批评引擎提供持续反馈。

Result: 实验显示该方法相比其他方法提升了10.67%的想法质量，生成的想法质量超过顶级会议接受水平，人类评估证实其在实际科研中的价值，消融研究验证了工作流程中各组件的有效性。

Conclusion: Deep Ideation框架通过整合科学概念网络和迭代优化流程，显著提升了研究想法生成的质量和实用性，为科学创新提供了有效支持。

Abstract: Novel research ideas play a critical role in advancing scientific inquiries.
Recent advancements in Large Language Models (LLMs) have demonstrated their
potential to generate novel research ideas by leveraging large-scale scientific
literature. However, previous work in research ideation has primarily relied on
simplistic methods, such as keyword co-occurrence or semantic similarity. These
approaches focus on identifying statistical associations in the literature but
overlook the complex, contextual relationships between scientific concepts,
which are essential to effectively leverage knowledge embedded in human
literature. For instance, papers that simultaneously mention "keyword A" and
"keyword B" often present research ideas that integrate both concepts.
Additionally, some LLM-driven methods propose and refine research ideas using
the model's internal knowledge, but they fail to effectively utilize the
scientific concept network, limiting the grounding of ideas in established
research. To address these challenges, we propose the Deep Ideation framework
to address these challenges, integrating a scientific network that captures
keyword co-occurrence and contextual relationships, enriching LLM-driven
ideation. The framework introduces an explore-expand-evolve workflow to
iteratively refine research ideas, using an Idea Stack to track progress. A
critic engine, trained on real-world reviewer feedback, guides the process by
providing continuous feedback on the novelty and feasibility of ideas. Our
experiments show that our approach improves the quality of generated ideas by
10.67% compared to other methods, with ideas surpassing top conference
acceptance levels. Human evaluation highlights their practical value in
scientific research, and ablation studies confirm the effectiveness of each
component in the workflow. Code repo is available at
https://github.com/kyZhao-1/Deep-Ideation.

</details>


### [23] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang,Tengyue Wang,Xilin Gong,Yang Shi,Haotian Wang,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 本文提出了一个分析多模态大语言模型模态跟随行为的新框架，将模态跟随分解为相对推理不确定性和固有模态偏好两个因素，揭示了模态跟随概率随相对不确定性单调下降的普遍规律。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅用粗粒度的数据集级统计来衡量多模态模型在模态冲突时的行为，忽略了模型在单模态推理中的置信度影响。

Method: 构建可控数据集系统变化视觉和文本输入的推理难度，使用熵作为细粒度不确定性度量，并通过层间预测探究内部机制。

Result: 发现模态跟随概率随相对不确定性单调下降的普遍规律，在平衡点处模型对两种模态的跟随概率相当，揭示了模型在模糊区域会在不同层间在模态间振荡的内部机制。

Conclusion: 相对不确定性和固有模态偏好是控制模态跟随的两个基本原则，为理解MLLMs如何解决冲突信息提供了量化框架和机制性见解。

Abstract: Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.

</details>


### [24] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 本文分析了多智能体推理中的懒惰行为问题，提出了因果影响度量和可验证奖励机制来改善协作效果。


<details>
  <summary>Details</summary>
Motivation: 多智能体推理中存在懒惰行为问题，即一个智能体主导而另一个贡献很少，这削弱了协作效果，使多智能体设置退化为无效的单智能体。

Method: 1) 理论分析懒惰行为的产生原因；2) 引入稳定高效的因果影响度量方法；3) 提出可验证奖励机制，允许推理智能体丢弃噪声输出、整合指令并在必要时重启推理过程。

Result: 大量实验表明，该框架有效缓解了懒惰智能体行为，释放了多智能体框架在复杂推理任务中的全部潜力。

Conclusion: 通过因果影响度量和可验证奖励机制，可以成功解决多智能体推理中的协作问题，提升整体性能。

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [25] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee,DongGyun Kang,SeHoon Park,Sa-Yoon Park,Kwangsoo Kim*

Main category: cs.AI

TL;DR: 提出基于Transformer的ProQ-BERT框架，用于预测慢性肾病进展，整合多模态电子健康记录数据，在91,816患者队列中表现优于CEHR-BERT，ROC-AUC达0.995。


<details>
  <summary>Details</summary>
Motivation: 慢性肾病影响全球近10%人口，准确预测疾病进展对于及时干预和资源优化至关重要。

Method: 使用基于量化的标记化方法处理连续实验室数值，采用注意力机制提高可解释性，通过掩码语言建模预训练，然后针对从3a期到5期的二元分类任务进行微调。

Result: 在91,816患者队列评估中，模型在短期预测中ROC-AUC达0.995，PR-AUC达0.989，持续优于CEHR-BERT。

Conclusion: 结果表明Transformer架构和时间设计选择在临床预后建模中的有效性，为个性化CKD护理提供了有前景的方向。

Abstract: Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.

</details>


### [26] [Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients](https://arxiv.org/abs/2511.02392)
*Muhammad Sheharyar Liaqat*

Main category: cs.AI

TL;DR: 基于模糊软集理论的专家系统，利用BMI、胰岛素、瘦素、脂联素水平和年龄等临床参数来评估乳腺癌风险。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性主要死因之一，早期诊断对有效治疗和提高生存率至关重要，但及时检测仍面临疾病复杂性和患者风险因素变异性的挑战。

Method: 提出模糊软集理论专家系统，整合BMI、胰岛素水平、瘦素水平、脂联素水平和年龄作为输入变量，通过模糊推理规则和软集计算估计乳腺癌风险。

Result: 使用UCI机器学习存储库的数据集进行模型开发和验证，系统能够通过常规血液分析获得参数，为非侵入性初步评估提供可行方法。

Conclusion: 该专家系统旨在支持医疗专业人员识别高风险患者，并确定是否需要进一步诊断程序如活检。

Abstract: Breast cancer remains one of the leading causes of mortality among women
worldwide, with early diagnosis being critical for effective treatment and
improved survival rates. However, timely detection continues to be a challenge
due to the complex nature of the disease and variability in patient risk
factors. This study presents a fuzzy soft set theory-based expert system
designed to assess the risk of breast cancer in patients using measurable
clinical and physiological parameters. The proposed system integrates Body Mass
Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input
variables to estimate breast cancer risk through a set of fuzzy inference rules
and soft set computations. These parameters can be obtained from routine blood
analyses, enabling a non-invasive and accessible method for preliminary
assessment. The dataset used for model development and validation was obtained
from the UCI Machine Learning Repository. The proposed expert system aims to
support healthcare professionals in identifying high-risk patients and
determining the necessity of further diagnostic procedures such as biopsies.

</details>


### [27] [A New Perspective on Precision and Recall for Generative Models](https://arxiv.org/abs/2511.02414)
*Benjamin Sykes,Loïc Simon,Julien Rabin,Jalal Fadili*

Main category: cs.AI

TL;DR: 提出基于二元分类视角的新框架来估计生成模型的精确率-召回率(PR)曲线，解决了现有方法只能估计曲线极值点的限制，并进行了统计分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 生成模型评估方法大多依赖标量指标，而PR曲线能提供更丰富的分析，但其估计存在挑战。现有PR指标只能估计曲线的极值点，需要更完整的曲线估计方法。

Method: 基于二元分类视角构建PR曲线估计框架，进行统计分析并获得PR估计风险的极小极大上界，扩展了文献中只能处理曲线极值的经典PR指标。

Result: 提出的框架能够估计完整的PR曲线，在统计上具有理论保证，实验验证了在不同设置下曲线行为的差异。

Conclusion: 该框架为生成模型的PR曲线估计提供了更全面的分析方法，扩展了现有PR指标的应用范围，具有理论和实用价值。

Abstract: With the recent success of generative models in image and text, the question
of their evaluation has recently gained a lot of attention. While most methods
from the state of the art rely on scalar metrics, the introduction of Precision
and Recall (PR) for generative model has opened up a new avenue of research.
The associated PR curve allows for a richer analysis, but their estimation
poses several challenges. In this paper, we present a new framework for
estimating entire PR curves based on a binary classification standpoint. We
conduct a thorough statistical analysis of the proposed estimates. As a
byproduct, we obtain a minimax upper bound on the PR estimation risk. We also
show that our framework extends several landmark PR metrics of the literature
which by design are restrained to the extreme values of the curve. Finally, we
study the different behaviors of the curves obtained experimentally in various
settings.

</details>


### [28] [ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning](https://arxiv.org/abs/2511.02424)
*Jae-Woo Choi,Hyungmin Kim,Hyobin Ong,Minsu Jang,Dohyung Kim,Jaehong Kim,Youngwoo Yoon*

Main category: cs.AI

TL;DR: ReAcTree是一种分层任务规划方法，通过构建动态代理树将复杂目标分解为更易管理的子目标，结合推理、行动和控制流节点，并集成情景记忆和工作记忆系统，显著提升了复杂长期任务的完成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂长期任务时存在局限，因为它们依赖单一轨迹来纠缠所有过去的决策和观察，试图在一个统一过程中解决整个任务。

Method: 提出ReAcTree分层任务规划方法，将复杂目标分解为子目标，构建动态代理树，每个代理节点能够推理、行动和扩展树，控制流节点协调执行策略，并集成情景记忆和工作记忆系统。

Result: 在WAH-NL和ALFRED数据集上的实验表明，ReAcTree在多种LLM上一致优于ReAct等强基线方法。在WAH-NL上，ReAcTree使用Qwen 2.5 72B实现了61%的目标成功率，几乎是ReAct（31%）的两倍。

Conclusion: ReAcTree通过分层分解和动态代理树结构，有效解决了复杂长期任务的规划问题，显著提升了任务完成效果。

Abstract: Recent advancements in large language models (LLMs) have enabled significant
progress in decision-making and task planning for embodied autonomous agents.
However, most existing methods still struggle with complex, long-horizon tasks
because they rely on a monolithic trajectory that entangles all past decisions
and observations, attempting to solve the entire task in a single unified
process. To address this limitation, we propose ReAcTree, a hierarchical
task-planning method that decomposes a complex goal into more manageable
subgoals within a dynamically constructed agent tree. Each subgoal is handled
by an LLM agent node capable of reasoning, acting, and further expanding the
tree, while control flow nodes coordinate the execution strategies of agent
nodes. In addition, we integrate two complementary memory systems: each agent
node retrieves goal-specific, subgoal-level examples from episodic memory and
shares environment-specific observations through working memory. Experiments on
the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently
outperforms strong task-planning baselines such as ReAct across diverse LLMs.
Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5
72B, nearly doubling ReAct's 31%.

</details>


### [29] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang,Xubo Liu,Siyu Ding,Weichong Yin,Yu Sun,Hua Wu,Wenya Guo,Ying Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为VMR的训练策略，将开放式任务重新构建为可验证的多项选择格式，从而在缺乏明确标准答案的情况下实现有效训练，显著提升大语言模型在开放式任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法在数学和编程等有标准答案的领域表现出色，但对于缺乏真实答案的开放式任务（如创意写作和指令遵循），通常被视为非推理场景，忽略了推理能力的潜在价值。本文旨在探索强化推理是否能提升开放式任务的性能。

Method: 提出了可验证多项选择重构（VMR）方法，将开放式数据重新构建为可验证的多项选择格式，从而克服RLVR依赖标准答案的限制，实现在开放式任务上的有效训练。

Result: 在多个基准测试上的实验结果表明，该方法能有效提升LLM在开放式任务上的性能。在八个开放式基准测试中，基于VMR的训练相比基线平均提升了5.99分。

Conclusion: VMR方法成功地将RLVR范式扩展到开放领域，证明了强化推理能力确实能够提升开放式任务的性能，为缺乏标准答案的任务提供了有效的训练策略。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [30] [Agentic AI for Mobile Network RAN Management and Optimization](https://arxiv.org/abs/2511.02532)
*Jorge Pellejero,Luis A. Hernández Gómez,Luis Mendo Tomás,Zoraida Frias Barroso*

Main category: cs.AI

TL;DR: 本文提出了Agentic AI在5G和6G网络中的应用框架，通过大型AI模型实现自主决策，特别针对RAN优化提出了具体用例。


<details>
  <summary>Details</summary>
Motivation: 5G和6G网络的复杂性使得手动优化无效，需要Agentic AI来自动化动态RAN环境中的决策，但目前缺乏明确的框架和定义。

Method: 引入Agentic AI概念，描述其核心设计模式（反思、规划、工具使用、多智能体协作），并在5G RAN案例中展示时间序列分析和LAM驱动的智能体如何协作进行基于KPI的自主决策。

Result: 提出了Agentic AI在移动网络中的理论基础，并通过具体案例展示了其在RAN管理和优化中的实际应用潜力。

Conclusion: Agentic AI为5G和6G网络的自动化优化提供了有前景的解决方案，能够通过智能体的协作实现自主决策和动态适应。

Abstract: Agentic AI represents a new paradigm for automating complex systems by using
Large AI Models (LAMs) to provide human-level cognitive abilities with
multimodal perception, planning, memory, and reasoning capabilities. This will
lead to a new generation of AI systems that autonomously decompose goals,
retain context over time, learn continuously, operate across tools and
environments, and adapt dynamically. The complexity of 5G and upcoming 6G
networks renders manual optimization ineffective, pointing to Agentic AI as a
method for automating decisions in dynamic RAN environments. However, despite
its rapid advances, there is no established framework outlining the
foundational components and operational principles of Agentic AI systems nor a
universally accepted definition.
  This paper contributes to ongoing research on Agentic AI in 5G and 6G
networks by outlining its core concepts and then proposing a practical use case
that applies Agentic principles to RAN optimization. We first introduce Agentic
AI, tracing its evolution from classical agents and discussing the progress
from workflows and simple AI agents to Agentic AI. Core design
patterns-reflection, planning, tool use, and multi-agent collaboration-are then
described to illustrate how intelligent behaviors are orchestrated. These
theorical concepts are grounded in the context of mobile networks, with a focus
on RAN management and optimization. A practical 5G RAN case study shows how
time-series analytics and LAM-driven agents collaborate for KPI-based
autonomous decision-making.

</details>


### [31] [Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting](https://arxiv.org/abs/2511.02534)
*Enhong Mu,Jinyu Cai,Yijun Lu,Mingyue Zhang,Kenji Tei,Jialong Li*

Main category: cs.AI

TL;DR: 提出了KLPEG框架，通过构建知识图谱来系统建模游戏元素、任务依赖和因果关系，利用LLM解析更新日志并生成针对性测试用例，显著提升游戏测试效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现代视频游戏的快速迭代和频繁更新给测试带来了挑战，现有基于LLM的自动化测试方法缺乏结构化知识积累机制，难以针对增量更新进行精准高效测试。

Method: 构建知识图谱系统建模游戏元素、任务依赖和因果关系，利用LLM解析自然语言更新日志，通过知识图谱多跳推理识别影响范围，生成针对性测试用例。

Result: 在Overcooked和Minecraft两个代表性游戏环境中的实验表明，KLPEG能更准确定位受更新影响的功能，用更少步骤完成测试，显著提升测试效果和效率。

Conclusion: KLPEG框架通过知识图谱和LLM的结合，有效解决了游戏增量更新的测试挑战，实现了知识的积累和重用，提升了测试的精准性和效率。

Abstract: The rapid iteration and frequent updates of modern video games pose
significant challenges to the efficiency and specificity of testing. Although
automated playtesting methods based on Large Language Models (LLMs) have shown
promise, they often lack structured knowledge accumulation mechanisms, making
it difficult to conduct precise and efficient testing tailored for incremental
game updates. To address this challenge, this paper proposes a KLPEG framework.
The framework constructs and maintains a Knowledge Graph (KG) to systematically
model game elements, task dependencies, and causal relationships, enabling
knowledge accumulation and reuse across versions. Building on this foundation,
the framework utilizes LLMs to parse natural language update logs, identify the
scope of impact through multi-hop reasoning on the KG, enabling the generation
of update-tailored test cases. Experiments in two representative game
environments, Overcooked and Minecraft, demonstrate that KLPEG can more
accurately locate functionalities affected by updates and complete tests in
fewer steps, significantly improving both playtesting effectiveness and
efficiency.

</details>


### [32] [The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models](https://arxiv.org/abs/2511.02589)
*Claudia Herambourg,Dawid Siuda,Anna Szczepanek,Julia Kopczyńska,Joao R. L. Santos,Wojciech Sas,Joanna Śmietańska-Nowak*

Main category: cs.AI

TL;DR: ORCA基准测试评估大型语言模型在多领域真实定量推理任务上的表现，结果显示现有模型准确率仅为45-63%，主要错误来自舍入和计算错误。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在真实世界多领域定量推理任务中的表现，不同于传统数学数据集，关注实际应用场景中的计算精度和推理能力。

Method: 使用Omni计算引擎验证的500个自然语言任务，涵盖金融、物理、健康、统计等领域，测试5个最先进LLM系统的表现。

Result: 所有模型准确率在45-63%之间，35%错误来自舍入问题，33%来自计算错误；在数学和工程领域表现较好，但在物理和自然科学领域较弱。

Conclusion: LLMs在定量推理任务中存在显著局限性，模型之间存在部分互补性而非冗余，需要改进计算精度和领域泛化能力。

Abstract: We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel
benchmark that evaluates large language models (LLMs) on multi-domain,
real-life quantitative reasoning using verified outputs from Omni's calculator
engine. In 500 natural-language tasks across domains such as finance, physics,
health, and statistics, the five state-of-the-art systems (ChatGPT-5,
Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only
$45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$)
and calculation mistakes ($33\,\%$). Results in specific domains indicate
strengths in mathematics and engineering, but weaknesses in physics and natural
sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the
models often fail together but differ in the types of errors they make,
highlighting their partial complementarity rather than redundancy. Unlike
standard math datasets, ORCA evaluates step-by-step reasoning, numerical
precision, and domain generalization across real problems from finance,
physics, health, and statistics.

</details>


### [33] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu,Alexander W. Goodall,Dalal Alrajeh,Francesco Belardinelli,Sebastian Uchitel*

Main category: cs.AI

TL;DR: 提出了首个基于GR(1)规范的自适应屏蔽框架，通过运行时检测环境假设违规并使用ILP在线修复规范，确保屏蔽器优雅演化并保持最优奖励和逻辑合规性。


<details>
  <summary>Details</summary>
Motivation: 传统静态屏蔽方法假设固定的逻辑规范和手工抽象，在环境假设被违反时无法适应，导致安全性失效。

Method: 基于GR(1)规范开发自适应屏蔽框架，运行时检测环境假设违规，使用归纳逻辑编程(ILP)在线自动修复GR(1)规范。

Result: 在Minepump和Atari Seaquest案例中，自适应屏蔽相比静态屏蔽能保持接近最优的奖励和完美的逻辑合规性。

Conclusion: 自适应屏蔽框架能够优雅地演化，确保活性可达性，仅在必要时弱化目标，优于静态屏蔽方法。

Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [34] [A Multi-Agent Psychological Simulation System for Human Behavior Modeling](https://arxiv.org/abs/2511.02606)
*Xiangen Hu,Jiarui Tong,Sheng Xu*

Main category: cs.AI

TL;DR: 提出了一个基于心理学理论的多智能体心理模拟系统，通过模拟内部认知-情感过程来生成可信的人类行为，应用于教师培训和研究领域。


<details>
  <summary>Details</summary>
Motivation: 人类中心领域的培训和教育需要真实实践，但现实的人类行为模拟一直有限，需要更符合心理学原理的透明模拟系统。

Method: 基于心理学理论构建多智能体系统，模拟'内部议会'中的关键心理因素代理，通过代理间的审议和互动决定输出行为。

Result: 开发了一个具有前所未有的透明度和与人类心理学对齐的系统，能够生成可信的人类行为模拟。

Conclusion: 该系统体现了社会学习、认知学徒制、刻意练习和元认知等原则，为教师培训和研究提供了有效的心理模拟工具。

Abstract: Training and education in human-centered fields require authentic practice,
yet realistic simulations of human behavior have remained limited. We present a
multi-agent psychological simulation system that models internal
cognitive-affective processes to generate believable human behaviors. In
contrast to black-box neural models, this system is grounded in established
psychological theories (e.g., self-efficacy, mindset, social constructivism)
and explicitly simulates an ``inner parliament'' of agents corresponding to key
psychological factors. These agents deliberate and interact to determine the
system's output behavior, enabling unprecedented transparency and alignment
with human psychology. We describe the system's architecture and theoretical
foundations, illustrate its use in teacher training and research, and discuss
how it embodies principles of social learning, cognitive apprenticeship,
deliberate practice, and meta-cognition.

</details>


### [35] [DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning](https://arxiv.org/abs/2511.02627)
*Lachlan McPheat,Navdeep Kaur,Robert Blackwell,Alessandra Russo,Anthony G. Cohn,Pranava Madhyastha*

Main category: cs.AI

TL;DR: DecompSR是一个用于分析组合空间推理能力的大型基准数据集和生成框架，包含500多万个数据点，可独立控制组合性的多个方面，并通过符号求解器保证数据正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在组合空间推理方面存在不足，特别是生产性和系统性泛化能力较弱，需要专门的基准来精细评估这些能力。

Method: 通过程序化生成方法构建数据集，可独立控制生产力（推理深度）、可替换性（实体和语言变体）、过度泛化（输入顺序、干扰项）和系统性（新语言元素）等组合性维度。

Result: 测试发现LLMs在空间推理任务中难以进行生产性和系统性泛化，但对语言变体具有更好的鲁棒性。

Conclusion: DecompSR提供了一个可证明正确且严格的基准数据集，能够精细评估LLMs的组合推理能力，填补了现有评估工具的空白。

Abstract: We introduce DecompSR, decomposed spatial reasoning, a large benchmark
dataset (over 5m datapoints) and generation framework designed to analyse
compositional spatial reasoning ability. The generation of DecompSR allows
users to independently vary several aspects of compositionality, namely:
productivity (reasoning depth), substitutivity (entity and linguistic
variability), overgeneralisation (input order, distractors) and systematicity
(novel linguistic elements). DecompSR is built procedurally in a manner which
makes it is correct by construction, which is independently verified using a
symbolic solver to guarantee the correctness of the dataset. DecompSR is
comprehensively benchmarked across a host of Large Language Models (LLMs) where
we show that LLMs struggle with productive and systematic generalisation in
spatial reasoning tasks whereas they are more robust to linguistic variation.
DecompSR provides a provably correct and rigorous benchmarking dataset with a
novel ability to independently vary the degrees of several key aspects of
compositionality, allowing for robust and fine-grained probing of the
compositional reasoning abilities of LLMs.

</details>


### [36] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 该研究提出了一个协作迷宫求解基准，评估了32个领先模型在单独、同质和异质配对中的协作能力，发现存在明显的"协作差距"，并提出"接力推理"方法改善协作效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，我们将越来越多地依赖由独立开发的异构智能体组成的系统，这些系统的成功关键在于有效协作。然而，目前缺乏大规模评估智能体间协作能力的实证研究。

Method: 设计了一个协作迷宫求解基准框架，该框架能够：(1)隔离协作能力，(2)调节问题复杂度，(3)支持可扩展的自动评分，(4)不限制输出格式以保持生态合理性。使用该框架评估了32个领先的开源和闭源模型。

Result: 发现了明显的"协作差距"：单独表现良好的模型在需要协作时性能显著下降。协作可能完全崩溃，例如小型蒸馏模型单独能很好解决迷宫，但在某些配对中几乎完全失败。发现从较强智能体开始通常能改善结果。

Conclusion: 研究结果支持：(1)协作感知的评估方法，(2)专门训练增强协作能力的策略，(3)可靠激发智能体潜在技能的交互设计。这些指导原则适用于AI-AI协作和人类-AI协作。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [37] [CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents](https://arxiv.org/abs/2511.02734)
*Jiayu Liu,Cheng Qian,Zhaochen Su,Qing Zong,Shijue Huang,Bingxiang He,Yi R. Fung*

Main category: cs.AI

TL;DR: CostBench是一个以成本为中心的基准测试，用于评估LLM代理的经济推理和重新规划能力，发现在静态和动态环境下代理都难以找到成本最优解。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理评估主要关注任务完成度，忽视了资源效率和适应性，特别是代理在变化环境中制定和调整成本最优计划的能力。

Method: 在旅行规划领域构建CostBench基准，包含可通过多种工具序列解决的任务，支持四种动态阻塞事件（如工具故障和成本变化）来模拟现实世界的不确定性。

Result: 评估显示代理在成本感知规划方面存在显著差距：在静态设置中经常无法找到成本最优解，GPT-5在最难任务上准确率低于75%，动态条件下性能进一步下降约40%。

Conclusion: CostBench为开发既经济理性又鲁棒的未来代理奠定了基础，通过诊断这些弱点推动改进。

Abstract: Current evaluations of Large Language Model (LLM) agents primarily emphasize
task completion, often overlooking resource efficiency and adaptability. This
neglects a crucial capability: agents' ability to devise and adjust
cost-optimal plans in response to changing environments. To bridge this gap, we
introduce CostBench, a scalable, cost-centric benchmark designed to evaluate
agents' economic reasoning and replanning abilities. Situated in the
travel-planning domain, CostBench comprises tasks solvable via multiple
sequences of atomic and composite tools with diverse, customizable costs. It
also supports four types of dynamic blocking events, such as tool failures and
cost changes, to simulate real-world unpredictability and necessitate agents to
adapt in real time. Evaluating leading open-sourced and proprietary models on
CostBench reveals a substantial gap in cost-aware planning: agents frequently
fail to identify cost-optimal solutions in static settings, with even GPT-5
achieving less than 75% exact match rate on the hardest tasks, and performance
further dropping by around 40% under dynamic conditions. By diagnosing these
weaknesses, CostBench lays the groundwork for developing future agents that are
both economically rational and robust.

</details>


### [38] [Using Span Queries to Optimize for Cache and Attention Locality](https://arxiv.org/abs/2511.02749)
*Paul Castro,Nick Mitchell,Nathan Ordonez,Thomas Parnell,Mudhakar Srivatsa,Antoni Viros i Martin*

Main category: cs.AI

TL;DR: 本文提出了span query概念，将推理服务器的接口泛化，支持聊天、RAG、推理时间缩放和智能体工作负载，通过优化KV缓存和注意力局部性显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理服务器主要针对聊天完成进行优化，但客户端需求已扩展到各种推理时间缩放和深度推理技术。现有解决方案通常只针对单一用例（如RAG）进行优化，缺乏通用性。

Method: 引入span query作为推理调用的表达式树，通过交换性约束连接，支持自动优化KV缓存局部性。对vLLM进行少量修改（仅492行代码）实现高性能span query执行。

Result: span query在两种不同的非聊天用例中实现了10-20倍的TTFT降低。注意力优化的span query在2b参数模型上的准确性远超使用8b模型的传统推理服务器。

Conclusion: span query提供了一种通用接口，能够有效支持多样化的推理工作负载，通过优化KV缓存和注意力局部性显著提升性能，解决了传统推理服务器的局限性。

Abstract: Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.

</details>


### [39] [When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning](https://arxiv.org/abs/2511.02794)
*Chenyu Zhang,Minsol Kim,Shohreh Ghorbani,Jingyao Wu,Rosalind Picard,Patricia Maes,Paul Pu Liang*

Main category: cs.AI

TL;DR: 提出了模态破坏诊断框架，通过将每个模态视为智能体来识别多模态大语言模型中的融合动态问题，特别是高置信度单模态错误误导整体预测的情况。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的推理过程不透明，不清楚哪个模态驱动预测、如何解决冲突或何时某个模态主导，需要诊断工具来理解融合动态。

Method: 提出轻量级、模型无关的评估层，将每个模态视为智能体，生成候选标签和自评估，通过简单融合机制聚合输出，识别支持者（支持正确结果的模态）和破坏者（误导的模态）。

Result: 在多模态情感识别基准测试中发现系统性可靠性模式，能够区分失败是由数据集伪影还是模型限制引起。

Conclusion: 该框架为多模态推理提供了诊断支架，支持对融合动态的原则性审计，并为可能的干预措施提供信息。

Abstract: Despite rapid growth in multimodal large language models (MLLMs), their
reasoning traces remain opaque: it is often unclear which modality drives a
prediction, how conflicts are resolved, or when one stream dominates. In this
paper, we introduce modality sabotage, a diagnostic failure mode in which a
high-confidence unimodal error overrides other evidence and misleads the fused
result. To analyze such dynamics, we propose a lightweight, model-agnostic
evaluation layer that treats each modality as an agent, producing candidate
labels and a brief self-assessment used for auditing. A simple fusion mechanism
aggregates these outputs, exposing contributors (modalities supporting correct
outcomes) and saboteurs (modalities that mislead). Applying our diagnostic
layer in a case study on multimodal emotion recognition benchmarks with
foundation models revealed systematic reliability profiles, providing insight
into whether failures may arise from dataset artifacts or model limitations.
More broadly, our framework offers a diagnostic scaffold for multimodal
reasoning, supporting principled auditing of fusion dynamics and informing
possible interventions.

</details>


### [40] [Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning](https://arxiv.org/abs/2511.02818)
*Mohamed Bouadi,Pratinav Seth,Aditya Tanna,Vinay Kumar Sankarapu*

Main category: cs.AI

TL;DR: Orion-MSP是一种新的表格数据上下文学习架构，通过多尺度处理、块稀疏注意力和感知器式内存解决了现有方法的局限性，在保持高性能的同时有效扩展到高维表格。


<details>
  <summary>Details</summary>
Motivation: 解决表格数据神经网络建模的挑战，包括异构特征类型、多尺度复杂交互，以及现有架构在特征处理、注意力扩展性和组件通信方面的局限性。

Method: 引入三个关键创新：多尺度处理捕获层次特征交互；块稀疏注意力结合窗口化、全局和随机模式实现可扩展效率；感知器式内存支持组件间安全双向信息流。

Result: 在多样化基准测试中，Orion-MSP达到或超越最先进性能，同时能有效扩展到高维表格。

Conclusion: Orion-MSP为高效的表格上下文学习建立了新标准，解决了现有方法的三个主要限制。

Abstract: Tabular data remain the predominant format for real-world applications. Yet,
developing effective neural models for tabular data remains challenging due to
heterogeneous feature types and complex interactions occurring at multiple
scales. Recent advances in tabular in-context learning (ICL), such as TabPFN
and TabICL, have achieved state-of-the-art performance comparable to
gradient-boosted trees (GBTs) without task-specific fine-tuning. However,
current architectures exhibit key limitations: (1) single-scale feature
processing that overlooks hierarchical dependencies, (2) dense attention with
quadratic scaling in table width, and (3) strictly sequential component
processing that prevents iterative representation refinement and
cross-component communication. To address these challenges, we introduce
Orion-MSP, a tabular ICL architecture featuring three key innovations: (1)
multi-scale processing to capture hierarchical feature interactions; (2)
block-sparse attention combining windowed, global, and random patterns for
scalable efficiency and long-range connectivity; and (3) a Perceiver-style
memory enabling safe bidirectional information flow across components. Across
diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance
while scaling effectively to high-dimensional tables, establishing a new
standard for efficient tabular in-context learning. The model is publicly
available at https://github.com/Lexsi-Labs/Orion-MSP .

</details>


### [41] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge,Paul Colognese,Avery Griffin,Tyler Tracy,Jon Kutasov,Joe Benton*

Main category: cs.AI

TL;DR: 本文提出了一种在复杂AI控制环境中优化攻击策略的方法，通过将攻击能力分解为五个技能组件并分别优化，使用概率模型解决数据不足问题，显著提升了攻击强度。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署变得复杂且高风险，准确评估其风险变得至关重要。AI控制框架需要强大的攻击策略，但在复杂环境中由于计算限制导致数据不足，这构成了主要挑战。

Method: 将攻击能力分解为五个技能组件（怀疑建模、攻击选择、计划合成、执行和隐蔽性），开发攻击动态的概率模型，在模拟中优化攻击超参数，然后将结果迁移到SHADE-Arena环境。

Result: 攻击强度显著提升，安全分数从基线0.87降低到0.41，表明该方法能有效增强攻击策略。

Conclusion: 通过技能分解和概率建模的方法，可以在数据有限的复杂环境中有效优化攻击策略，为AI风险评估提供了更可靠的工具。

Abstract: As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>


### [42] [Kosmos: An AI Scientist for Autonomous Discovery](https://arxiv.org/abs/2511.02824)
*Ludovico Mitchener,Angela Yiu,Benjamin Chang,Mathieu Bourdenx,Tyler Nadolski,Arvis Sulovari,Eric C. Landsness,Daniel L. Barabasi,Siddharth Narayanan,Nicky Evans,Shriya Reddy,Martha Foiani,Aizad Kamal,Leah P. Shriver,Fang Cao,Asmamaw T. Wassie,Jon M. Laurent,Edwin Melville-Green,Mayk Caldas,Albert Bou,Kaleigh F. Roberts,Sladjana Zagorac,Timothy C. Orr,Miranda E. Orr,Kevin J. Zwezdaryk,Ali E. Ghareeb,Laurie McCoy,Bruna Gomes,Euan A. Ashley,Karen E. Duff,Tonio Buonassisi,Tom Rainforth,Randall J. Bateman,Michael Skarlinski,Samuel G. Rodriques,Michaela M. Hinks,Andrew D. White*

Main category: cs.AI

TL;DR: Kosmos是一个AI科学家系统，能够自动化数据驱动的科学发现过程，通过结构化世界模型协调数据分析代理和文献搜索代理，在长达12小时的运行中执行数万行代码并阅读大量论文，生成可追溯的科学报告。


<details>
  <summary>Details</summary>
Motivation: 现有的AI科研代理在采取多次行动后会失去连贯性，限制了发现深度。需要开发能够长期保持连贯性、自动化科学研究的AI系统。

Method: Kosmos使用结构化世界模型在数据分析代理和文献搜索代理之间共享信息，支持长达200次代理迭代，每次运行平均执行42,000行代码和阅读1,500篇论文。

Result: 独立科学家评估发现Kosmos报告中79.4%的陈述准确，单个20周期运行相当于研究人员6个月的工作量，有价值科学发现数量与周期数呈线性增长。

Conclusion: Kosmos在代谢组学、材料科学、神经科学和统计遗传学等领域做出了7项发现，其中3项独立重现了未发表成果，4项对科学文献做出了新颖贡献。

Abstract: Data-driven scientific discovery requires iterative cycles of literature
search, hypothesis generation, and data analysis. Substantial progress has been
made towards AI agents that can automate scientific research, but all such
agents remain limited in the number of actions they can take before losing
coherence, thus limiting the depth of their findings. Here we present Kosmos,
an AI scientist that automates data-driven discovery. Given an open-ended
objective and a dataset, Kosmos runs for up to 12 hours performing cycles of
parallel data analysis, literature search, and hypothesis generation before
synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos
uses a structured world model to share information between a data analysis
agent and a literature search agent. The world model enables Kosmos to
coherently pursue the specified objective over 200 agent rollouts, collectively
executing an average of 42,000 lines of code and reading 1,500 papers per run.
Kosmos cites all statements in its reports with code or primary literature,
ensuring its reasoning is traceable. Independent scientists found 79.4% of
statements in Kosmos reports to be accurate, and collaborators reported that a
single 20-cycle Kosmos run performed the equivalent of 6 months of their own
research time on average. Furthermore, collaborators reported that the number
of valuable scientific findings generated scales linearly with Kosmos cycles
(tested up to 20 cycles). We highlight seven discoveries made by Kosmos that
span metabolomics, materials science, neuroscience, and statistical genetics.
Three discoveries independently reproduce findings from preprinted or
unpublished manuscripts that were not accessed by Kosmos at runtime, while four
make novel contributions to the scientific literature.

</details>


### [43] [Neurosymbolic Deep Learning Semantics](https://arxiv.org/abs/2511.02825)
*Artur d'Avila Garcez,Simon Odense*

Main category: cs.AI

TL;DR: 本文提出使用逻辑框架为深度学习提供语义基础，通过神经符号AI将神经网络与逻辑连接起来，解决AI科学发现缺乏语义理解的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI虽然强大但缺乏语义，使得科学发现难以理解。需要建立一个框架将AI的洞察转化为可理解的科学知识。

Method: 使用神经符号AI框架，通过逻辑语义将神经网络与逻辑映射连接，建立语义编码框架来统一现有的各种编码和知识提取方法。

Result: 提出了一个统一的语义编码框架，明确了神经网络与逻辑之间的映射关系，并总结了现有各种神经编码和知识提取方法的共同要素。

Conclusion: 逻辑为深度学习提供了必要的语义基础，但实践中识别语义编码仍面临类似心灵哲学中的困难，需要继续研究解决。

Abstract: Artificial Intelligence (AI) is a powerful new language of science as
evidenced by recent Nobel Prizes in chemistry and physics that recognized
contributions to AI applied to those areas. Yet, this new language lacks
semantics, which makes AI's scientific discoveries unsatisfactory at best. With
the purpose of uncovering new facts but also improving our understanding of the
world, AI-based science requires formalization through a framework capable of
translating insight into comprehensible scientific knowledge. In this paper, we
argue that logic offers an adequate framework. In particular, we use logic in a
neurosymbolic framework to offer a much needed semantics for deep learning, the
neural network-based technology of current AI. Deep learning and neurosymbolic
AI lack a general set of conditions to ensure that desirable properties are
satisfied. Instead, there is a plethora of encoding and knowledge extraction
approaches designed for particular cases. To rectify this, we introduced a
framework for semantic encoding, making explicit the mapping between neural
networks and logic, and characterizing the common ingredients of the various
existing approaches. In this paper, we describe succinctly and exemplify how
logical semantics and neural networks are linked through this framework, we
review some of the most prominent approaches and techniques developed for
neural encoding and knowledge extraction, provide a formal definition of our
framework, and discuss some of the difficulties of identifying a semantic
encoding in practice in light of analogous problems in the philosophy of mind.

</details>


### [44] [Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](https://arxiv.org/abs/2511.02834)
*Huawei Lin,Yunzhi Shi,Tong Geng,Weijie Zhao,Wei Wang,Ravender Pal Singh*

Main category: cs.AI

TL;DR: 提出了Agent-Omni框架，通过主代理系统协调现有基础模型，实现无需重新训练的灵活多模态推理。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型局限于固定模态对，需要大量对齐数据进行微调，构建完全全能的文本、图像、音频和视频集成模型仍不实用且缺乏强大的推理支持。

Method: 采用主代理系统，主代理解释用户意图，将子任务委托给特定模态的代理，并将它们的输出整合为连贯的响应。

Result: 在文本、图像、音频、视频和全模态基准测试中，Agent-Omni始终达到最先进的性能，特别是在需要复杂跨模态推理的任务上。

Conclusion: 基于代理的设计能够无缝集成专业基础模型，确保对多样化输入的适应性，同时保持透明度和可解释性。该框架模块化且易于扩展，允许随着更强模型的可用性进行未来改进。

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities but
remain limited to fixed modality pairs and require costly fine-tuning with
large aligned datasets. Building fully omni-capable models that can integrate
text, images, audio, and video remains impractical and lacks robust reasoning
support. In this paper, we propose an Agent-Omni framework that coordinates
existing foundation models through a master-agent system, enabling flexible
multimodal reasoning without retraining. The master agent interprets user
intent, delegates subtasks to modality-specific agents, and integrates their
outputs into coherent responses. Extensive experiments across text, image,
audio, video, and omni benchmarks show that Agent-Omni consistently achieves
state-of-the-art performance, particularly on tasks requiring complex
cross-modal reasoning. Its agent-based design enables seamless integration of
specialized foundation models, ensuring adaptability to diverse inputs while
maintaining transparency and interpretability. In addition, the framework is
modular and easily extensible, allowing future improvements as stronger models
become available. %We release an open-source implementation to support
continued research on scalable and reliable omni-modal reasoning.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [45] [Fibbinary-Based Compression and Quantization for Efficient Neural Radio Receivers](https://arxiv.org/abs/2511.01921)
*Roberta Fiandaca,Manil Dev Gomony*

Main category: cs.IT

TL;DR: 该论文提出两种优化策略来降低神经网络接收器的复杂度和计算成本：量化和压缩。通过引入均匀/非均匀量化技术以及新颖的增量网络量化方法，结合两种无损压缩算法，显著减少了硬件资源消耗。


<details>
  <summary>Details</summary>
Motivation: 神经网络接收器性能优异但网络复杂度高、计算成本大，难以在硬件受限设备上部署。需要解决其高计算复杂度和内存占用问题。

Method: 1) 引入均匀和非均匀量化（包括斐波那契码字量化FCQ）；2) 提出细粒度增量网络量化(INQ)策略补偿量化损失；3) 开发两种无损压缩算法压缩冗余的斐波那契量化参数。

Result: 量化技术节省了45%的乘法器功耗和44%的面积，结合压缩技术使内存占用减少63.4%，同时性能仍优于传统接收器。

Conclusion: 通过量化和压缩的组合优化策略，成功实现了神经网络接收器在硬件资源受限环境下的高效部署，在保持性能优势的同时显著降低了资源消耗。

Abstract: Neural receivers have shown outstanding performance compared to the
conventional ones but this comes with a high network complexity leading to a
heavy computational cost. This poses significant challenges in their deployment
on hardware-constrained devices. To address the issue, this paper explores two
optimization strategies: quantization and compression. We introduce both
uniform and non-uniform quantization such as the Fibonacci Code word
Quantization (FCQ). A novel fine-grained approach to the Incremental Network
Quantization (INQ) strategy is then proposed to compensate for the losses
introduced by the above mentioned quantization techniques. Additionally, we
introduce two novel lossless compression algorithms that effectively reduce the
memory size by compressing sequences of Fibonacci quantized parameters
characterized by a huge redundancy. The quantization technique provides a
saving of 45\% and 44\% in the multiplier's power and area, respectively, and
its combination with the compression determines a 63.4\% reduction in memory
footprint, while still providing higher performances than a conventional
receiver.

</details>


### [46] [Analysis of Beam Misalignment Effect in Inter-Satellite FSO Links](https://arxiv.org/abs/2511.02189)
*Minje Kim,Hongjae Nam,Beomsoo Ko,Hyeongjun Park,Hwanjin Kim,Dong-Hyun Jung,Junil Choi*

Main category: cs.IT

TL;DR: 该论文研究了光束未对准对星间自由空间光通信的影响，提出了在联合抖动和未对准引起的指向误差下的FSO信道累积分布函数闭式表达式，并引入截断CDF公式和二分算法来高效计算中断概率。


<details>
  <summary>Details</summary>
Motivation: 星间FSO通信对光束未对准高度敏感，而常用的指向提前角补偿依赖于精确的轨道知识和先进的对准硬件，这在实践中并不总是可行。

Method: 推导了在联合抖动和未对准引起的指向误差下的FSO信道CDF闭式表达式，引入截断CDF公式和二分算法，并基于轨道动力学量化位移。

Result: 数值结果表明，所提出的模型与蒙特卡洛模拟高度匹配，使该模型在实际星间FSO系统设计中非常有用。

Conclusion: 提出的模型能够高效准确地计算中断概率，为实际星间FSO系统设计提供了实用的分析工具。

Abstract: Free-space optical (FSO) communication has emerged as a promising technology
for inter-satellite links (ISLs) due to its high data rate, low power
consumption, and reduced interference. However, the performance of
inter-satellite FSO systems is highly sensitive to beam misalignment. While
pointing-ahead angle (PAA) compensation is commonly employed, the effectiveness
of PAA compensation depends on precise orbital knowledge and advanced alignment
hardware, which are not always feasible in practice. To address this challenge,
this paper investigates the impact of beam misalignment on inter-satellite FSO
communication. We derive a closed-form expression for the cumulative
distribution function (CDF) of the FSO channel under the joint jitter and
misalignment-induced pointing error, and introduce a truncated CDF formulation
with a bisection algorithm to efficiently compute outage probabilities with
guaranteed convergence and minimal computational overhead. To make the analysis
more practical, we quantify displacement based on orbital dynamics. Numerical
results demonstrate that the proposed model closely matches Monte Carlo
simulations, making the proposed model highly useful to design inter-satellite
FSO systems in practice.

</details>


### [47] [Adaptive Cooperative Transmission Design for Ultra-Reliable Low-Latency Communications via Deep Reinforcement Learning](https://arxiv.org/abs/2511.02216)
*Hyemin Yu,Hong-Chuan Yang*

Main category: cs.IT

TL;DR: 提出了一种基于双智能体强化学习的自适应传输算法（DRL-CoLA），用于两跳中继通信系统，在严格延迟约束下实现近最优可靠性。


<details>
  <summary>Details</summary>
Motivation: 下一代无线通信系统需要支持超可靠低延迟通信（URLLC）服务，但满足严格的URLLC要求在两跳协作通信中具有挑战性。

Method: 将每跳传输参数配置建模为马尔可夫决策过程，提出双智能体强化学习算法，分布式学习延迟感知传输策略。

Result: 仿真结果表明，所提算法在满足严格延迟要求的同时实现了近最优的可靠性。

Conclusion: DRL-CoLA算法能够有效解决两跳中继系统中的URLLC传输挑战，实现延迟约束下的可靠通信。

Abstract: Next-generation wireless communication systems must support ultra-reliable
low-latency communication (URLLC) service for mission-critical applications.
Meeting stringent URLLC requirements is challenging, especially for two-hop
cooperative communication. In this paper, we develop an adaptive transmission
design for a two-hop relaying communication system. Each hop transmission
adaptively configures its transmission parameters separately, including
numerology, mini-slot size, and modulation and coding scheme, for reliable
packet transmission within a strict latency constraint. We formulate the
hop-specific transceiver configuration as a Markov decision process (MDP) and
propose a dual-agent reinforcement learning-based cooperative latency-aware
transmission (DRL-CoLA) algorithm to learn latency-aware transmission policies
in a distributed manner. Simulation results verify that the proposed algorithm
achieves the near-optimal reliability while satisfying strict latency
requirements.

</details>


### [48] [Revisiting Wireless-Powered MEC: A Cooperative Energy Recycling Framework for Task-Energy Co-Design](https://arxiv.org/abs/2511.02284)
*Haohao Qin,Bowen Gu,Xianhua Yu,Hao Xie,Yongjun Xu,Qihao Li,Liejun Wang*

Main category: cs.IT

TL;DR: 提出了一种支持协作能量回收的MEC框架，通过优化计算-通信协同设计来最大化用户间最小可计算数据量，解决了能量因果性、延迟和功率约束下的资源分配问题。


<details>
  <summary>Details</summary>
Motivation: 协作能量回收为无线供电的多接入边缘计算网络提供了提升能源利用率的新途径，但其与计算-通信协同设计的结合仍待深入探索。

Method: 将难解问题通过松弛、最大比合并和变量替换转化为凸形式，利用拉格朗日对偶和交替优化推导闭式解，提供分析洞察。

Result: 仿真结果表明，所提出的CER机制显著增加了总可计算数据量，同时在异构用户间保持了公平的性能表现。

Conclusion: 该研究成功将协作能量回收整合到MEC系统中，通过优化算法实现了能源效率和计算性能的协同提升。

Abstract: Cooperative energy recycling (CER) offers a new way to boost energy
utilization in wireless-powered multi-access edge computing (MEC) networks, yet
its integration with computation-communication co-design remains underexplored.
This paper proposes a CER-enabled MEC framework that maximizes the minimum
computable data among users under energy causality, latency, and power
constraints. The intractable problem is reformulated into a convex form through
relaxation, maximum ratio combining, and variable substitution, and closed-form
solutions are derived via Lagrangian duality and alternating optimization,
offering analytical insights. Simulation results verify that the proposed CER
mechanism markedly increases total computable data while maintaining equitable
performance across heterogeneous users.

</details>


### [49] [Fairness-Aware Computation Offloading in Wireless-Powered MEC Systems with Cooperative Energy Recycling](https://arxiv.org/abs/2511.02287)
*Haohao Qin,Bowen Gu,Dong Li,Xianhua Yu,Liejun Wang,Yuanwei Liu,Sumei Sun*

Main category: cs.IT

TL;DR: 提出无线供电移动边缘计算系统中的协作能量回收机制，通过联合优化本地计算和计算卸载，在满足能量、延迟和任务大小约束下平衡总可计算数据和用户公平性。


<details>
  <summary>Details</summary>
Motivation: 传统架构仅依赖专用电源，而无线传感器可以从对等传输中回收能量，提高系统性能。

Method: 使用变量替换技术将非凸问题转化为凸结构，采用拉格朗日对偶和交替优化高效求解，并推导了三种代表性公平性机制的闭式解。

Result: 数值结果验证了所提CER框架的有效性，在吞吐量和适应性方面相比基准方案有显著提升。

Conclusion: 可调alpha公平机制在不同场景下提供了灵活的性能-公平性权衡控制。

Abstract: In this paper, cooperative energy recycling (CER) is investigated in
wireless-powered mobile edge computing systems. Unlike conventional
architectures that rely solely on a dedicated power source, wireless sensors
are additionally enabled to recycle energy from peer transmissions. To evaluate
system performance, a joint computation optimization problem is formulated that
integrates local computing and computation offloading, under an alpha-fairness
objective that balances total computable data and user fairness while
satisfying energy, latency, and task size constraints. Due to the inherent
non-convexity introduced by coupled resource variables and fairness
regularization, a variable-substitution technique is employed to transform the
problem into a convex structure, which is then efficiently solved using
Lagrangian duality and alternating optimization. To characterize the
fairness-efficiency tradeoff, closed-form solutions are derived for three
representative regimes: zero fairness, common fairness, and max-min fairness,
each offering distinct system-level insights. Numerical results validate the
effectiveness of the proposed CER-enabled framework, demonstrating significant
gains in throughput and adaptability over benchmark schemes. The tunable alpha
fairness mechanism provides flexible control over performance-fairness
trade-offs across diverse scenarios.

</details>


### [50] [Downlink Channel Estimation for mmWave Systems with Impulsive Interference](https://arxiv.org/abs/2511.02291)
*Kwonyeol Park,Gyoseung Lee,Hyeongtaek Lee,Hwanjin Kim,Junil Choi*

Main category: cs.IT

TL;DR: 提出了一种基于变分推理的贝叶斯信道估计技术，用于解决毫米波MIMO系统中由硬件非理想性或外部干扰引起的脉冲干扰问题。


<details>
  <summary>Details</summary>
Motivation: 毫米波MIMO系统在信道估计过程中面临脉冲干扰的挑战，这种干扰具有突发性、不可预测性和高功率特性，严重影响信道估计精度。

Method: 基于变分推理的贝叶斯信道估计方法，利用毫米波信道在角度域的稀疏性和脉冲干扰的间歇特性，采用均值场近似进行后验推断，并将变分推理集成到稀疏贝叶斯学习框架中。

Result: 仿真结果表明，所提出的技术在信道估计精度方面优于基线方法。

Conclusion: 该变分推理贝叶斯信道估计技术能有效应对毫米波MIMO系统中的脉冲干扰问题，提高信道估计性能。

Abstract: In this paper, we investigate a channel estimation problem in a downlink
millimeter-wave (mmWave) multiple-input multiple-output (MIMO) system, which
suffers from impulsive interference caused by hardware non-idealities or
external disruptions. Specifically, impulsive interference presents a
significant challenge to channel estimation due to its sporadic, unpredictable,
and high-power nature. To tackle this issue, we develop a Bayesian channel
estimation technique based on variational inference (VI) that leverages the
sparsity of the mmWave channel in the angular domain and the intermittent
nature of impulsive interference to minimize channel estimation errors. The
proposed technique employs mean-field approximation to approximate posterior
inference and integrates VI into the sparse Bayesian learning (SBL) framework.
Simulation results demonstrate that the proposed technique outperforms
baselines in terms of channel estimation accuracy.

</details>


### [51] [Two-Parameter Rényi Information Quantities with Applications to Privacy Amplification and Soft Covering](https://arxiv.org/abs/2511.02297)
*Shi-Bing Li,Ke Li,Lei Yu*

Main category: cs.IT

TL;DR: 本文提出了一个双参数Rényi条件熵和双参数Rényi互信息的新定义，建立了它们的基本性质，并证明了这些信息量可以用于刻画隐私放大和软覆盖问题中的强逆指数。


<details>
  <summary>Details</summary>
Motivation: 目前文献中缺乏普遍接受的Rényi条件熵和Rényi互信息的定义，不同应用场景下提出了多种定义。本文旨在建立统一的定义框架。

Method: 通过变量变换，将双参数Rényi条件熵与Hayashi和Tan的定义联系起来，并作为量子Rényi条件熵的经典特例。提出了新的双参数Rényi互信息定义，统一了三种常用变体。

Result: 建立了双参数Rényi条件熵的单调性和变分表达式，证明了双参数Rényi互信息的非负性、可加性、数据处理不等式、参数单调性、变分表达式以及凸凹性。

Conclusion: 双参数Rényi信息量能够有效刻画隐私放大和软覆盖问题在α∈(0,∞)阶Rényi散度下的强逆指数，为信息论应用提供了统一的数学工具。

Abstract: There are no universally accepted definitions of R\'enyi conditional entropy
and R\'enyi mutual information, although motivated by different applications,
several definitions have been proposed in the literature. In this paper, we
consider a family of two-parameter R\'enyi conditional entropy and a family of
two-parameter R\'enyi mutual information. By performing a change of variables
for the parameters, the two-parameter R\'enyi conditional entropy we study
coincides precisely with the definition introduced by Hayashi and Tan [IEEE
Trans. Inf. Theory, 2016], and it also emerges naturally as the classical
specialization of the three-parameter quantum R\'enyi conditional entropy
recently put forward by Rubboli, Goodarzi, and Tomamichel [arXiv:2410.21976
(2024)]. We establish several fundamental properties of the two-parameter
R\'enyi conditional entropy, including monotonicity with respect to the
parameters and variational expression. The associated two-parameter R\'enyi
mutual information considered in this paper is new and it unifies three
commonly used variants of R\'enyi mutual information. For this quantity, we
prove several important properties, including the non-negativity, additivity,
data processing inequality, monotonicity with respect to the parameters,
variational expression, as well as convexity and concavity. Finally, we
demonstrate that these two-parameter R\'enyi information quantities can be used
to characterize the strong converse exponents in privacy amplification and soft
covering problems under R\'enyi divergence of order $\alpha \in (0, \infty)$.

</details>


### [52] [Anomaly Detection-Based UE-Centric Inter-Cell Interference Suppression](https://arxiv.org/abs/2511.02320)
*Kwonyeol Park,Hyuckjin Choi,Beomsoo Ko,Minje Kim,Gyoseung Lee,Daecheol Kwon,Hyunjae Park,Byungseung Kim,Min-Ho Shin,Junil Choi*

Main category: cs.IT

TL;DR: 提出了一种基于单类分类的用户设备中心干扰抑制方案，通过异常检测技术识别小区间干扰并应用干扰白化，在有限训练资源下优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着频谱复用增加，邻区干扰导致系统性能显著下降，需要开发有效的干扰抑制方案来提升整体性能。

Method: 使用Z-refined深度支持向量数据描述的单类分类异常检测技术，检测小区间干扰并应用干扰白化。

Result: 数值结果表明该方案在干扰检测性能上优于多种基线方法，且与理想辅助干扰抑制方案性能相当；商用5G调制解调器芯片组实验显示在各种3GPP标准信道环境下均有性能提升。

Conclusion: 所提出的用户设备中心干扰抑制方案能有效检测和抑制小区间干扰，在有限训练资源下实现良好性能，适用于实际5G系统。

Abstract: The increasing spectral reuse can cause significant performance degradation
due to interference from neighboring cells. In such scenarios, developing
effective interference suppression schemes is necessary to improve overall
system performance. To tackle this issue, we propose a novel user
equipment-centric interference suppression scheme, which effectively detects
inter-cell interference (ICI) and subsequently applies interference whitening
to mitigate ICI. The proposed scheme, named Z-refined deep support vector data
description, exploits a one-class classification-based anomaly detection
technique. Numerical results verify that the proposed scheme outperforms
various baselines in terms of interference detection performance with limited
time or frequency resources for training and is comparable to the performance
based on an ideal genie-aided interference suppression scheme. Furthermore, we
demonstrate through test equipment experiments using a commercial
fifth-generation modem chipset that the proposed scheme shows performance
improvements across various 3rd generation partnership project standard channel
environments, including tapped delay line-A, -B, and -C models.

</details>


### [53] [$\mathbb{F}_q\mathbb{F}_{q^2}$-additive cyclic codes and their Gray images](https://arxiv.org/abs/2511.02325)
*Ankit Yadav,Ritumoni Sarma*

Main category: cs.IT

TL;DR: 研究有限域FqFq2上的加法循环码，构造满足Singleton界的码，通过Gray映射得到最优线性码，并获得了最优三元线性互补对偶码。


<details>
  <summary>Details</summary>
Motivation: 研究FqFq2上的加法循环码结构及其性质，旨在构造满足Singleton界的最优码，并探索通过Gray映射获得实际应用中的最优线性码。

Method: 确定生成多项式和最小生成集，构造满足Singleton界的Fq2-加法循环码，使用Gray映射将码映射到F3上，从F3F9-加法码获得最优三元LCD码。

Result: 成功确定了加法循环码的生成结构，构造了满足Singleton界的码例，通过Gray映射得到了F3上的最优线性码，并获得了最优三元LCD码。

Conclusion: FqFq2上的加法循环码具有丰富的代数结构，能够构造出满足最优性条件的码，通过适当的映射可以获得实际应用中的最优线性码和LCD码。

Abstract: We investigate additive cyclic codes over the alphabet
$\mathbb{F}_{q}\mathbb{F}_{q^2}$, where $q$ is a prime power. First, its
generator polynomials and minimal spanning set are determined. Then, examples
of $\mathbb{F}_{q^2}$-additive cyclic codes that satisfy the well-known
Singleton bound are constructed. Using a Gray map, we produce certain optimal
linear codes over $\mathbb{F}_{3}$. Finally, we obtain a few optimal ternary
linear complementary dual (LCD) codes from
$\mathbb{F}_{3}\mathbb{F}_{9}$-additive codes.

</details>


### [54] [Generalized informational functionals and new monotone measures of statistical complexity](https://arxiv.org/abs/2511.02502)
*Razvan Gabriel Iagar,David Puertas-Centeno*

Main category: cs.IT

TL;DR: 本文引入了双参数变换族，扩展了上下变换的概念，提出了新的信息泛函（下矩和累积上矩），建立了这些新泛函与经典信息度量之间的尖锐不等式，并定义了新的统计复杂度度量。


<details>
  <summary>Details</summary>
Motivation: 扩展上下变换理论，引入新的信息泛函来连接p阶矩和Rényi熵，探索信息理论中不同度量之间的深层关系。

Method: 使用双参数变换族定义下矩和累积上矩，通过代数共轭分析上下变换的单调性，建立新的函数不等式。

Result: 获得了新泛函与经典信息度量之间的最优界限，确定了最小化密度函数（有些用广义三角函数表示），并建立了统计复杂度度量的单调性。

Conclusion: 新变换揭示了信息泛函之间复杂的函数不等式结构，为信息理论提供了新的分析工具和理论框架。

Abstract: In this paper we introduce a biparametric family of transformations which can
be seen as an extension of the so-called up and down transformations. This new
class of transformations allows to us to introduce new informational
functionals, which we have called \textit{down-moments} and \textit{cumulative
upper-moments}. A remarkable fact is that the down-moments provide, in some
cases, an interpolation between the $p$-th moments and the power R\'enyi
entropies of a probability density. We establish new and sharp inequalities
relating these new functionals to the classical informational measures such as
moments, R\'enyi and Shannon entropies and Fisher information measures. We also
give the optimal bounds as well as the minimizing densities, which are in some
cases expressed in terms of the generalized trigonometric functions. We
furthermore define new classes of measures of statistical complexity obtained
as quotients of the new functionals, and establish monotonicity properties for
them through an algebraic conjugation of up and down transformations. All of
these properties highlight an intricate structure of functional inequalities.

</details>


### [55] [Improved AntiGriesmer Bounds for Linear Anticodes and Applications](https://arxiv.org/abs/2511.02519)
*Guanghui Zhang,Bocong Chen,Liren Lin,Hongwei Liu*

Main category: cs.IT

TL;DR: 本文改进了Chen和Xie先前建立的线性反码的antiGriesmer界，去除了码长限制并将对偶码最小距离条件从≥3放宽到≥2。


<details>
  <summary>Details</summary>
Motivation: 改进现有的线性反码antiGriesmer界，扩大其适用范围，提供更通用的理论框架。

Method: 证明对于任意[n,k]_q线性反码C，若其对偶码最小距离d(C^⊥)≥2，则不等式n ≤ ∑_{i=0}^{k-1} ⌊δ/q^i⌋成立。

Result: 得到了更一般的antiGriesmer界，并推导出关于直径δ、码长n和维数k的多个推论。

Conclusion: 新界统一并扩展了早期结果，为研究线性反码及其性质提供了更全面的框架，在构造和分类少重量线性码方面有应用价值。

Abstract: This paper improves the antiGriesmer bound for linear anticodes previously
established by Chen and Xie (Journal of Algebra, 673 (2025) 304-320). While the
original bound required the code length to satisfy $n < q^{k-1}$ and the dual
code to have minimum distance at least 3, our main result removes the length
restriction and relaxes the dual distance condition to at least 2.
Specifically, we prove that for any $[n,k]_q$ linear anticode $\mathcal{C}$
over $\mathbb{F}_q$ with diameter $\delta$ and $d(\mathcal{C}^\perp) \geq 2$,
the inequality \[ n \leq \sum_{i=0}^{k-1} \left\lfloor \frac{\delta}{q^i}
\right\rfloor \] holds. This generalization significantly broadens the
applicability of the antiGriesmer bound. We derive several corollaries,
including lower bounds on the diameter $\delta$ in terms of $n$ and $k$, upper
bounds on the code length $n$, and constraints on the dimension $k$.
Applications to the construction and classification of linear codes with few
weights are also discussed, along with examples demonstrating that our new
bound can be sharper than previous ones. Our work unifies and extends earlier
findings, providing a more comprehensive framework for studying linear
anticodes and their properties.

</details>


### [56] [Performance Analysis of Single-Antenna Fluid Antenna Systems via Extreme Value Theory](https://arxiv.org/abs/2511.02572)
*Rui Xu,Yinghui Ye,Xiaoli Chu,Guangyue Lu,Kai-Kit Wong,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文提出了一个基于极值分布的性能评估框架，用于分析完全相关瑞利衰落下的单天线流体天线系统，通过Gumbel分布和广义极值分布建模，推导了中断概率和遍历容量的闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 在完全相关衰落条件下，由于缺乏流体天线系统信道的闭式分布，推导准确且易处理的性能表达式具有挑战性。

Method: 使用极值分布建模流体天线信道，首先采用Gumbel分布近似，然后扩展到广义极值分布，通过最大似然准则确定参数。

Result: 仿真结果表明，基于广义极值分布的框架比Gumbel模型具有更高的准确性，两种方法都提供了计算高效且分析易处理的性能评估工具。

Conclusion: 提出的极值分布框架为在现实相关衰落条件下评估流体天线系统性能提供了有效且准确的方法。

Abstract: In single-antenna fluid antenna systems (FASs), the transceiver dynamically
selects the antenna port with the strongest instantaneous channel to enhance
link reliability. However, deriving accurate yet tractable performance
expressions under fully correlated fading remains challenging, primarily due to
the absence of a closed-form distribution for the FAS channel. To address this
gap, this paper develops a novel performance evaluation framework for FAS
operating under fully correlated Rayleigh fading, by modeling the FAS channel
through extreme value distributions (EVDs). We first justify the suitability of
EVD modeling and approximate the FAS channel through the Gumbel distribution,
with parameters expressed as functions of the number of ports and the antenna
aperture size via the maximum likelihood (ML) criterion. Closed-form
expressions for the outage probability (OP) and ergodic capacity (EC) are then
derived. While the Gumbel model provides an excellent fit, minor deviations
arise in the extreme-probability regions. To further improve accuracy, we
extend the framework using the generalized extreme value (GEV) distribution and
obtain closed-form OP and EC approximations based on ML-derived parameters.
Simulation results confirm that the proposed GEV-based framework achieves
superior accuracy over the Gumbel-based model, while both EVD-based approaches
offer computationally efficient and analytically tractable tools for evaluating
the performance of FAS under realistic correlated fading conditions.

</details>


### [57] [Redundancy Maximization as a Principle of Associative Memory Learning](https://arxiv.org/abs/2511.02584)
*Mark Blümel,Andreas C. Schneider,Valentin Neuhaus,David A. Ehrlich,Marcel Graetz,Michael Wibral,Abdullah Makkeh,Viola Priesemann*

Main category: cs.IT

TL;DR: 该论文通过信息分解框架分析Hopfield网络，发现冗余信息最大化可显著提升网络记忆容量，从0.14提升到1.59，提出了基于信息理论目标的新联想记忆设计原则。


<details>
  <summary>Details</summary>
Motivation: 传统Hopfield网络的局部计算原理尚不完全清楚，需要形式化表征此类系统中的局部信息处理。

Method: 应用部分信息分解(PID)框架分析Hopfield网络中单个神经元的信息处理，将冗余信息最大化作为信息理论学习目标进行优化。

Result: 在记忆容量以下，神经元活动信息表现为外部模式输入和内部循环输入之间的高冗余性；通过优化冗余信息，网络记忆容量从0.14显著提升到1.59。

Conclusion: 冗余最大化可作为联想记忆的新设计原则，为基于信息理论目标的新联想记忆模型开辟了途径。

Abstract: Associative memory, traditionally modeled by Hopfield networks, enables the
retrieval of previously stored patterns from partial or noisy cues. Yet, the
local computational principles which are required to enable this function
remain incompletely understood. To formally characterize the local information
processing in such systems, we employ a recent extension of information theory
- Partial Information Decomposition (PID). PID decomposes the contribution of
different inputs to an output into unique information from each input,
redundant information across inputs, and synergistic information that emerges
from combining different inputs. Applying this framework to individual neurons
in classical Hopfield networks we find that below the memory capacity, the
information in a neuron's activity is characterized by high redundancy between
the external pattern input and the internal recurrent input, while synergy and
unique information are close to zero until the memory capacity is surpassed and
performance drops steeply. Inspired by this observation, we use redundancy as
an information-theoretic learning goal, which is directly optimized for each
neuron, dramatically increasing the network's memory capacity to 1.59, a more
than tenfold improvement over the 0.14 capacity of classical Hopfield networks
and even outperforming recent state-of-the-art implementations of Hopfield
networks. Ultimately, this work establishes redundancy maximization as a new
design principle for associative memories and opens pathways for new
associative memory models based on information-theoretic goals.

</details>


### [58] [Optimal Source Coding of Markov Chains for Real-Time Remote Estimation](https://arxiv.org/abs/2511.02803)
*Ismail Cosandal,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究了马尔可夫链在传输时间和状态转换时间尺度相同时的源编码问题，提出了基于马尔可夫决策过程的最优编码策略，相比Huffman基准策略能减少平均传输时间。


<details>
  <summary>Details</summary>
Motivation: 传统源编码假设传输时间远小于状态转换时间，但实际中两者可能在同一时间尺度发生。当每个比特传输需要一个时隙且马尔可夫链在同一时隙更新状态时，码字长度会影响未传输符号数量和下一符号的概率分布。

Method: 将问题建模为马尔可夫决策过程，通过增强符号及其传输时间来寻找最优编码策略。提出了基于最后传输符号及其传输时长的最优源编码策略。

Result: 在随机生成的马尔可夫过程中，提出的最优策略相比Huffman基准策略能降低平均传输时间，性能增益取决于马尔可夫过程的参数。

Conclusion: 当传输时间和状态转换时间在同一尺度时，考虑符号间依赖关系的最优编码策略能显著提升性能，为实时通信系统提供了更有效的源编码方案。

Abstract: We revisit the source coding problem for a Markov chain under the assumption
that the transmission times and how fast the Markov chain transitions its state
happen at the same time-scale. Specifically, we assume that the transmission of
each bit takes a single time slot, and the Markov chain updates its state in
the same time slot. Thus, the length of the codeword assigned to a symbol
determines the number of non-transmitted symbols, as well as, the probability
of the realization of the next symbol to be transmitted. We aim to minimize the
average transmission duration over an infinite horizon by proposing an optimal
source coding policy based on the last transmitted symbol and its transmission
duration. To find the optimal policy, we formulate the problem with a Markov
decision process (MDP) by augmenting the symbols alongside the transmission
duration of the symbols. Finally, we analyze two Huffman-based benchmark
policies and compare their performances with the proposed optimal policy. We
observe that, in randomly generated processes, our proposed optimal policy
decreases the average transmission duration compared to benchmark policies. The
performance gain varies based on the parameters of the Markov process.

</details>


### [59] [A Construction of Infinite Families of Self-Orthogonal Quasi-Cyclic Codes Using Constituent Codes.pdf](https://arxiv.org/abs/2511.02813)
*Gustavo Terra Bastos,Angelynn Álvarez,Cameron Williams*

Main category: cs.IT

TL;DR: 本文提出了无限族准循环码的构造方法，这些码在欧几里得和埃尔米特内积下是自正交的。通过使用CSS构造，展示了具有良好参数的量子纠错码的存在性。


<details>
  <summary>Details</summary>
Motivation: 准循环码最近被用于量子纠错码的构造。本文旨在构建无限族的自正交准循环码，特别是针对欧几里得和埃尔米特内积。

Method: 使用在域扩展上定义的构成码来计算准循环码的维度和最小距离下界。构造方法确保码是自正交的，并且展示了如何从构造中产生自对偶准循环码。

Result: 证明了最小距离下界满足平方根类下界，并且通过CSS构造展示了具有良好参数的量子纠错码的存在性。

Conclusion: 提出的构造方法能够产生无限族的自正交准循环码，这些码可用于构建具有良好参数的量子纠错码。

Abstract: Quasi-cyclic codes have been recently employed in the constructions of
quantum error-correcting codes. In this paper, we propose a construction of
infinite families of quasi-cyclic codes which are self-orthogonal with respect
to the Euclidean and Hermitian inner products. In particular, their dimension
and a lower bound for their minimum distance are computed using their
constituent codes defined over field extensions of $\mathbb{F}_q$. We also show
that the lower bound for the minimum distance satisfies the square-root-like
lower bound and also show how self-dual quasi-cyclic codes can arise from our
construction. Using the CSS construction, we show the existence of quantum
error-correcting codes with good parameters.

</details>
