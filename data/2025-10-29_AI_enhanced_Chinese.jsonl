{"id": "2510.24359", "categories": ["cs.AI", "cs.SY", "eess.SY", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24359", "abs": "https://arxiv.org/abs/2510.24359", "authors": ["Pedram Fard", "Alaleh Azhir", "Neguine Rezaii", "Jiazi Tian", "Hossein Estiri"], "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine", "comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535", "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u7528\u4e8eN-of-1\u51b3\u7b56\u652f\u6301\uff0c\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u534f\u8c03\u667a\u80fd\uff0c\u4f7f\u533b\u7597AI\u66f4\u52a0\u900f\u660e\u3001\u516c\u5e73\u4e14\u4ee5\u4e2a\u4f53\u4e3a\u4e2d\u5fc3\u3002", "motivation": "\u5f53\u524d\u533b\u7597AI\u7cfb\u7edf\u670d\u52a1\u4e8e\u5e73\u5747\u60a3\u8005\uff0c\u4f46\u5728\u7f55\u89c1\u53d8\u5f02\u3001\u591a\u75c5\u5171\u5b58\u548c\u4ee3\u8868\u6027\u4e0d\u8db3\u4eba\u7fa4\u7b49\u8fb9\u7f18\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u635f\u5bb3\u4e86\u516c\u5e73\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u6309\u5668\u5b98\u7cfb\u7edf\u3001\u60a3\u8005\u7fa4\u4f53\u548c\u5206\u6790\u6a21\u5f0f\u5206\u7ec4\u7684\u667a\u80fd\u4f53\u5171\u4eab\u6a21\u578b\u5e93\u548c\u8bc1\u636e\u5408\u6210\u5de5\u5177\uff0c\u901a\u8fc7\u534f\u8c03\u5c42\u6574\u5408\u7ed3\u679c\uff0c\u63d0\u4f9b\u5e26\u7f6e\u4fe1\u533a\u95f4\u7684\u98ce\u9669\u8bc4\u4f30\u3001\u5f02\u5e38\u503c\u6807\u8bb0\u548c\u76f8\u5173\u8bc1\u636e\u3002", "result": "\u9a8c\u8bc1\u91cd\u70b9\u4ece\u7fa4\u4f53\u5e73\u5747\u8f6c\u5411\u4e2a\u4f53\u53ef\u9760\u6027\uff0c\u6d4b\u91cf\u4f4e\u5bc6\u5ea6\u533a\u57df\u7684\u8bef\u5dee\u3001\u5c0f\u6837\u672c\u6821\u51c6\u548c\u98ce\u9669-\u8986\u76d6\u6743\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u534f\u8c03\u667a\u80fd\uff0c\u4f7f\u533b\u7597AI\u66f4\u7b26\u5408\u533b\u5b66\u9996\u8981\u539f\u5219\uff1a\u63d0\u4f9b\u900f\u660e\u3001\u516c\u5e73\u4e14\u4ee5\u4e2a\u4f53\u4e3a\u4e2d\u5fc3\u7684\u62a4\u7406\u3002"}}
{"id": "2510.24242", "categories": ["cs.NI", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24242", "abs": "https://arxiv.org/abs/2510.24242", "authors": ["Zihan Li", "Jiahao Yang", "Yuxin Zhang", "Zhe Chen", "Yue Gao"], "title": "Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models", "comment": "15 pages, 11 figures", "summary": "Large vision-language models (LVLMs) have recently demonstrated great\npotential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by\nlow Earth orbit (LEO) satellites. However, their deployment in real-world LEO\nsatellite systems remains largely unexplored, hindered by limited onboard\ncomputing resources and brief satellite-ground contacts. We propose Grace, a\nsatellite-ground collaborative system designed for near-realtime LVLM inference\nin RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime\ninference, but larger ones on ground stations (GSs) to guarantee end-to-end\nperformance. Grace is comprised of two main phases that are asynchronous\nsatellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch\nalgorithm. Firstly, we still the knowledge archive of GS RAG to satellite\narchive with tailored adaptive update algorithm during limited satellite-ground\ndata exchange period. Secondly, propose a confidence-based test algorithm that\neither processes the task onboard the satellite or offloads it to the GS.\nExtensive experiments based on real-world satellite orbital data show that\nGrace reduces the average latency by 76-95% compared to state-of-the-art\nmethods, without compromising inference accuracy.", "AI": {"tldr": "Grace\u662f\u4e00\u4e2a\u536b\u661f-\u5730\u9762\u534f\u540c\u7cfb\u7edf\uff0c\u7528\u4e8e\u9065\u611f\u4efb\u52a1\u4e2d\u7684\u8fd1\u5b9e\u65f6\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u901a\u8fc7\u5f02\u6b65\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5728\u6709\u9650\u536b\u661f-\u5730\u9762\u63a5\u89e6\u65f6\u95f4\u5185\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9065\u611f\u4efb\u52a1\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u53d7\u9650\u4e8e\u661f\u4e0a\u8ba1\u7b97\u8d44\u6e90\u548c\u77ed\u6682\u536b\u661f-\u5730\u9762\u63a5\u89e6\u65f6\u95f4\uff0c\u5728\u5b9e\u9645LEO\u536b\u661f\u7cfb\u7edf\u4e2d\u90e8\u7f72\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u536b\u661f-\u5730\u9762\u534f\u540c\u67b6\u6784\uff1a\u5728\u536b\u661f\u4e0a\u90e8\u7f72\u7d27\u51d1\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u63a8\u7406\uff0c\u5728\u5730\u9762\u7ad9\u90e8\u7f72\u66f4\u5927\u6a21\u578b\u4fdd\u8bc1\u6027\u80fd\uff1b\u5305\u542b\u5f02\u6b65RAG\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u536b\u661f\u8f68\u9053\u6570\u636e\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGrace\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u5ef6\u8fdf\u964d\u4f4e76-95%\uff0c\u4e14\u4e0d\u635f\u5bb3\u63a8\u7406\u7cbe\u5ea6\u3002", "conclusion": "Grace\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u661f\u4e0a\u8d44\u6e90\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8fd1\u5b9e\u65f6\u9065\u611f\u4efb\u52a1\u5904\u7406\uff0c\u4e3aLVLM\u5728\u536b\u661f\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.24190", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24190", "abs": "https://arxiv.org/abs/2510.24190", "authors": ["Hong Niu", "Jiancheng An", "Chau Yuen"], "title": "Flexible Intelligent Layered Metasurfaces for Downlink Multi-user MISO Communications", "comment": "13 pages", "summary": "Stacked intelligent metasurfaces (SIMs) have recently gained attention as a\nparadigm for wave-domain signal processing with reduced reliance on costly\nradio-frequency (RF) chains. However, conventional SIMs rely on uniform\ninter-layer spacing and require deep stacking to ensure processing capability,\nresulting in severe power attenuation in practice. To address this issue, we\npropose a flexible intelligent layered metasurface (FILM) architecture\nconsisting of two shape-controllable flexible metasurface layers. By replacing\nrigid metasurfaces with flexible ones in both layers, the transmission\ncoefficient matrix can be dynamically adjusted, significantly decreasing the\nnumber of required layers while maintaining signal processing performance.\nFirstly, we develop a two-layer FILM-assisted multi-user multiple-input\nsingle-output (MU-MISO) system, wherein we formulate a channel fitting problem\naimed at reducing the difference between the FILM-induced and target channels.\nThen, we solve this non-convex problem by employing an alternating optimization\n(AO) method, featuring closed-form phase shift updates and a gradient\ndescent-based shape optimization. Furthermore, we analyze the upper bound on\nsum-rate and the complexity of computation to provide insights into design\ntrade-offs. Finally, simulation results demonstrated that the proposed\ntransmissive FILM architecture achieves over 200\\% improvement in sum-rate and\nmore than 7 dB bit-error rate (BER) gain compared to the conventional\nseven-layer SIMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u67d4\u6027\u667a\u80fd\u5206\u5c42\u8d85\u8868\u9762\uff08FILM\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u4f7f\u7528\u5f62\u72b6\u53ef\u63a7\u7684\u67d4\u6027\u8d85\u8868\u9762\u5c42\u66ff\u4ee3\u4f20\u7edf\u521a\u6027\u8d85\u8868\u9762\uff0c\u5728\u4fdd\u6301\u4fe1\u53f7\u5904\u7406\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u6240\u9700\u5c42\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5806\u53e0\u667a\u80fd\u8d85\u8868\u9762\u529f\u7387\u8870\u51cf\u4e25\u91cd\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5806\u53e0\u667a\u80fd\u8d85\u8868\u9762\uff08SIMs\uff09\u4f9d\u8d56\u5747\u5300\u5c42\u95f4\u8ddd\u4e14\u9700\u8981\u6df1\u5ea6\u5806\u53e0\u6765\u786e\u4fdd\u5904\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u529f\u7387\u8870\u51cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u51cf\u5c11\u5c42\u6570\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u7684\u65b0\u578b\u67b6\u6784\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u5c42FILM\u8f85\u52a9\u7684\u591a\u7528\u6237\u591a\u8f93\u5165\u5355\u8f93\u51fa\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3\u4fe1\u9053\u62df\u5408\u95ee\u9898\uff0c\u5305\u62ec\u95ed\u5f0f\u76f8\u79fb\u66f4\u65b0\u548c\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u5f62\u72b6\u4f18\u5316\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u900f\u5c04\u5f0fFILM\u67b6\u6784\u76f8\u6bd4\u4f20\u7edf\u7684\u4e03\u5c42SIMs\uff0c\u5728\u603b\u901f\u7387\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc7200%\u7684\u63d0\u5347\uff0c\u5728\u8bef\u7801\u7387\u4e0a\u83b7\u5f97\u4e86\u8d85\u8fc77 dB\u7684\u589e\u76ca\u3002", "conclusion": "FILM\u67b6\u6784\u901a\u8fc7\u4f7f\u7528\u67d4\u6027\u8d85\u8868\u9762\u5c42\u52a8\u6001\u8c03\u6574\u4f20\u8f93\u7cfb\u6570\u77e9\u9635\uff0c\u5728\u663e\u8457\u51cf\u5c11\u6240\u9700\u5c42\u6570\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u4fe1\u53f7\u5904\u7406\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u4f20\u7edfSIMs\u7684\u529f\u7387\u8870\u51cf\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2510.23691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23691", "abs": "https://arxiv.org/abs/2510.23691", "authors": ["Zihao Wang", "Xujing Li", "Yining Ye", "Junjie Fang", "Haoming Wang", "Longxiang Liu", "Shihao Liang", "Junting Lu", "Zhiyong Wu", "Jiazhan Feng", "Wanjun Zhong", "Zili Li", "Yu Wang", "Yu Miao", "Bo Zhou", "Yuanfan Li", "Hao Wang", "Zhongkai Zhao", "Faming Wu", "Zhengxuan Jiang", "Weihao Tan", "Heyuan Yao", "Shi Yan", "Xiangyang Li", "Yitao Liang", "Yujia Qin", "Guang Shi"], "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents", "comment": null, "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.", "AI": {"tldr": "Game-TARS\u662f\u4e00\u4e2a\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u952e\u76d8\u9f20\u6807\u52a8\u4f5c\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u5728\u64cd\u4f5c\u7cfb\u7edf\u3001\u7f51\u9875\u548c\u6a21\u62df\u6e38\u620f\u7b49\u5f02\u6784\u9886\u57df\u8fdb\u884c\u5927\u89c4\u6a21\u6301\u7eed\u9884\u8bad\u7ec3\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u8de8\u5f02\u6784\u9886\u57df\uff08\u64cd\u4f5c\u7cfb\u7edf\u3001\u7f51\u9875\u3001\u6a21\u62df\u6e38\u620f\uff09\u5de5\u4f5c\u7684\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u907f\u514dAPI\u6216GUI\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u6301\u7eed\u9884\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u952e\u76d8\u9f20\u6807\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4f7f\u7528\u8d85\u8fc7500B tokens\u7684\u591a\u6837\u5316\u8f68\u8ff9\u548c\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5173\u952e\u6280\u672f\u5305\u62ec\u8870\u51cf\u6301\u7eed\u635f\u5931\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\u548c\u9ad8\u6548\u7684\u7a00\u758f\u601d\u7ef4\u7b56\u7565\u5e73\u8861\u63a8\u7406\u6df1\u5ea6\u4e0e\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754cMinecraft\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u662f\u4e4b\u524d\u6700\u4f18\u6a21\u578b\u76842\u500d\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u7f51\u98753D\u6e38\u620f\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u65b0\u624b\u6c34\u5e73\uff0c\u5728FPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aGPT-5\u3001Gemini-2.5-Pro\u548cClaude-4-Sonnet\u3002", "conclusion": "\u7b80\u5355\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u8868\u793a\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e3a\u5b9e\u73b0\u5177\u6709\u5e7f\u6cdb\u8ba1\u7b97\u673a\u4f7f\u7528\u80fd\u529b\u7684\u901a\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2510.24595", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.24595", "abs": "https://arxiv.org/abs/2510.24595", "authors": ["Azadeh Pourkabirian", "Kai Li", "Photios A. Stavrou", "Wei Ni"], "title": "A New Hybrid Precoding Approach for Multi-user Massive MIMO over Fading Channels", "comment": null, "summary": "Hybrid precoding is an indispensable technique to harness the full potential\nof a multi-user massive multiple-input, multiple-output (MU-MMIMO) system. In\nthis paper, we propose a new hybrid precoding approach that combines digital\nand analog precoding to optimize data transmission over multiple antennas. This\napproach steers signals in specific directions, leading to maximizing sum-rate\nand suppressing side-lobe interference. When dealing with complex signals,\nchanges in phase are naturally associated with changes in angle, and these\nvariations are inherently correlated. The correlation between the angle and\nphase is essential for accurately determining the channel characteristics. An\nimportant aspect of this approach is that we model the angle and phase as\ncorrelated variables following a bivariate Gaussian distribution, and for the\nfirst time, we define a joint angle and phase entropy to measure the\nuncertainty of angle and phase variations in wireless channels. This entropy is\ncrucial to adapt the proposed precoding method with variations. Simulation\nresult validate the accuracy of our analytical findings, demonstrating 18.31%\nincrease in sum-rate and an 11.47% improvement in robustness compared to other\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u5b57\u548c\u6a21\u62df\u9884\u7f16\u7801\u7684\u6df7\u5408\u9884\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u89d2\u5ea6\u548c\u76f8\u4f4d\u71b5\u6765\u4f18\u5316\u591a\u7528\u6237\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u6570\u636e\u4f20\u8f93\uff0c\u663e\u8457\u63d0\u5347\u4e86\u548c\u901f\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6df7\u5408\u9884\u7f16\u7801\u5bf9\u4e8e\u53d1\u6325\u591a\u7528\u6237\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u6f5c\u529b\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u89e3\u51b3\u590d\u6742\u4fe1\u53f7\u4e2d\u89d2\u5ea6\u548c\u76f8\u4f4d\u53d8\u5316\u7684\u76f8\u5173\u6027\u95ee\u9898\uff0c\u4ee5\u51c6\u786e\u786e\u5b9a\u4fe1\u9053\u7279\u6027\u3002", "method": "\u5c06\u89d2\u5ea6\u548c\u76f8\u4f4d\u5efa\u6a21\u4e3a\u670d\u4ece\u53cc\u53d8\u91cf\u9ad8\u65af\u5206\u5e03\u7684\u76f8\u5173\u53d8\u91cf\uff0c\u9996\u6b21\u5b9a\u4e49\u4e86\u8054\u5408\u89d2\u5ea6\u548c\u76f8\u4f4d\u71b5\u6765\u8861\u91cf\u65e0\u7ebf\u4fe1\u9053\u4e2d\u89d2\u5ea6\u548c\u76f8\u4f4d\u53d8\u5316\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u8c03\u6574\u9884\u7f16\u7801\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u5206\u6790\u7ed3\u679c\u7684\u51c6\u786e\u6027\uff0c\u76f8\u6bd4\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u548c\u901f\u7387\u63d0\u9ad8\u4e8618.31%\uff0c\u9c81\u68d2\u6027\u63d0\u5347\u4e8611.47%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u9884\u7f16\u7801\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u89d2\u5ea6\u548c\u76f8\u4f4d\u71b5\u6709\u6548\u4f18\u5316\u4e86\u6570\u636e\u4f20\u8f93\u6027\u80fd\uff0c\u5728\u63d0\u5347\u548c\u901f\u7387\u548c\u6291\u5236\u65c1\u74e3\u5e72\u6270\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.24215", "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24215", "abs": "https://arxiv.org/abs/2510.24215", "authors": ["Vishal Halder", "Alexandre Reiffers-Masson", "Abdeldjalil A\u00efssa-El-Bey", "Gugan Thoppe"], "title": "What Can Be Recovered Under Sparse Adversarial Corruption? Assumption-Free Theory for Linear Measurements", "comment": null, "summary": "Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ be an arbitrary, known matrix\nand $\\mathbf{e}$ a $q$-sparse adversarial vector. Given $\\mathbf{y} =\n\\mathbf{A} x^* + \\mathbf{e}$ and $q$, we seek the smallest set containing\n$x^*$-hence the one conveying maximal information about $x^*$-that is uniformly\nrecoverable from $\\mathbf{y}$ without knowing $\\mathbf{e}$. While exact\nrecovery of $x^*$ via strong (and often impractical) structural assumptions on\n$\\mathbf{A}$ or $x^*$ (for example, restricted isometry, sparsity) is well\nstudied, recoverability for arbitrary $\\mathbf{A}$ and $x^*$ remains open. Our\nmain result shows that the best that one can hope to recover is $x^* +\n\\ker(\\mathbf{U})$, where $\\mathbf{U}$ is the unique projection matrix onto the\nintersection of rowspaces of all possible submatrices of $\\mathbf{A}$ obtained\nby deleting $2q$ rows. Moreover, we prove that every $x$ that minimizes the\n$\\ell_0$-norm of $\\mathbf{y} - \\mathbf{A} x$ lies in $x^* + \\ker(\\mathbf{U})$,\nwhich then gives a constructive approach to recover this set.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5b58\u5728q-\u7a00\u758f\u5bf9\u6297\u6027\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u89c2\u6d4b\u6570\u636ey=Ax*+e\u4e2d\u6062\u590dx*\u7684\u6700\u5c0f\u53ef\u6062\u590d\u96c6\u5408\u3002\u4e3b\u8981\u7ed3\u679c\u8868\u660e\uff0c\u6700\u4f73\u53ef\u6062\u590d\u96c6\u5408\u662fx*+ker(U)\uff0c\u5176\u4e2dU\u662f\u6295\u5f71\u77e9\u9635\uff0c\u4e14\u8be5\u96c6\u5408\u53ef\u4ee5\u901a\u8fc7\u6700\u5c0f\u5316y-Ax\u7684\u21130\u8303\u6570\u6765\u6784\u9020\u6027\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u57fa\u4e8e\u5bf9\u77e9\u9635A\u6216x*\u7684\u5f3a\u7ed3\u6784\u5047\u8bbe\uff08\u5982\u53d7\u9650\u7b49\u8ddd\u6027\u3001\u7a00\u758f\u6027\uff09\u6765\u5b9e\u73b0\u7cbe\u786e\u6062\u590d\uff0c\u4f46\u5bf9\u4e8e\u4efb\u610fA\u548cx*\u7684\u53ef\u6062\u590d\u6027\u95ee\u9898\u4ecd\u7136\u5f00\u653e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728\u4efb\u610f\u77e9\u9635\u548c\u672a\u77e5\u5411\u91cf\u60c5\u51b5\u4e0b\uff0c\u9762\u5bf9\u7a00\u758f\u5bf9\u6297\u6027\u566a\u58f0\u65f6\u7684\u53ef\u6062\u590d\u6027\u754c\u9650\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6240\u6709\u53ef\u80fd\u5220\u96642q\u884c\u540e\u5f97\u5230\u7684A\u5b50\u77e9\u9635\u7684\u884c\u7a7a\u95f4\u4ea4\u96c6\uff0c\u5b9a\u4e49\u6295\u5f71\u77e9\u9635U\uff0c\u5e76\u8bc1\u660e\u6700\u5c0f\u53ef\u6062\u590d\u96c6\u5408\u4e3ax*+ker(U)\u3002\u540c\u65f6\u8bc1\u660e\u4e86\u6240\u6709\u6700\u5c0f\u5316y-Ax\u7684\u21130\u8303\u6570\u7684x\u90fd\u4f4d\u4e8e\u8be5\u96c6\u5408\u4e2d\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u6784\u9020\u6027\u6062\u590d\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u5728q-\u7a00\u758f\u5bf9\u6297\u6027\u566a\u58f0\u4e0b\uff0c\u4ecey\u4e2d\u53ef\u4e00\u81f4\u6062\u590d\u7684\u6700\u5c0f\u96c6\u5408\u662fx*+ker(U)\uff0c\u5176\u4e2dU\u662f\u6295\u5f71\u5230\u6240\u6709\u5220\u96642q\u884c\u540e\u7684A\u5b50\u77e9\u9635\u884c\u7a7a\u95f4\u4ea4\u96c6\u7684\u552f\u4e00\u6295\u5f71\u77e9\u9635\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4efb\u610f\u77e9\u9635\u548c\u672a\u77e5\u5411\u91cf\u5728\u7a00\u758f\u5bf9\u6297\u6027\u566a\u58f0\u4e0b\u7684\u53ef\u6062\u590d\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u754c\u9650\uff0c\u5e76\u7ed9\u51fa\u4e86\u6784\u9020\u6027\u6062\u590d\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u5728\u65e0\u5f3a\u7ed3\u6784\u5047\u8bbe\u60c5\u51b5\u4e0b\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.23734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23734", "abs": "https://arxiv.org/abs/2510.23734", "authors": ["Eamon Duede"], "title": "AI and the Decentering of Disciplinary Creativity", "comment": null, "summary": "This paper examines the role of artificial intelligence in scientific\nproblem-solving, with a focus on its implications for disciplinary creativity.\nDrawing on recent work in the philosophy of creativity, I distinguish between\ncreative approaches and creative products, and introduce the concept of\ndisciplinary creativity -the creative application of discipline-specific\nexpertise to a valued problem within that field. Through two cases in\nmathematics, I show that while computation can extend disciplinary creativity,\ncertain approaches involving AI can serve to displace it. This displacement has\nthe potential to alter (and, perhaps, diminish) the value of scientific\npursuit.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u4f5c\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u6570\u5b66\u6848\u4f8b\u8868\u660e\uff0c\u867d\u7136\u8ba1\u7b97\u80fd\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9bAI\u65b9\u6cd5\u53ef\u80fd\u53d6\u4ee3\u5b83\uff0c\u4ece\u800c\u6539\u53d8\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002", "motivation": "\u7814\u7a76AI\u5982\u4f55\u5f71\u54cd\u79d1\u5b66\u9886\u57df\u7684\u521b\u9020\u529b\uff0c\u7279\u522b\u662f\u5b66\u79d1\u521b\u9020\u529b\uff08\u5728\u7279\u5b9a\u9886\u57df\u5185\u5e94\u7528\u4e13\u4e1a\u77e5\u8bc6\u89e3\u51b3\u6709\u4ef7\u503c\u95ee\u9898\u7684\u521b\u9020\u6027\u65b9\u5f0f\uff09\uff0c\u63a2\u8ba8AI\u53ef\u80fd\u5bf9\u79d1\u5b66\u8ffd\u6c42\u4ef7\u503c\u4ea7\u751f\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u521b\u9020\u529b\u54f2\u5b66\u7406\u8bba\uff0c\u533a\u5206\u521b\u9020\u6027\u65b9\u6cd5\u548c\u521b\u9020\u6027\u4ea7\u54c1\uff0c\u5f15\u5165\u5b66\u79d1\u521b\u9020\u529b\u6982\u5ff5\u3002\u901a\u8fc7\u4e24\u4e2a\u6570\u5b66\u6848\u4f8b\u8fdb\u884c\u5206\u6790\uff0c\u6bd4\u8f83\u8ba1\u7b97\u548cAI\u65b9\u6cd5\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u4e0d\u540c\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8ba1\u7b97\u53ef\u4ee5\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9bAI\u65b9\u6cd5\u4f1a\u53d6\u4ee3\u5b66\u79d1\u521b\u9020\u529b\uff0c\u8fd9\u79cd\u53d6\u4ee3\u53ef\u80fd\u6539\u53d8\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\uff0c\u751a\u81f3\u53ef\u80fd\u4f7f\u5176\u8d2c\u503c\u3002", "conclusion": "AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u5177\u6709\u53cc\u91cd\u4f5c\u7528\uff1a\u4e00\u65b9\u9762\u8ba1\u7b97\u80fd\u589e\u5f3a\u5b66\u79d1\u521b\u9020\u529b\uff0c\u53e6\u4e00\u65b9\u9762\u67d0\u4e9bAI\u65b9\u6cd5\u53ef\u80fd\u53d6\u4ee3\u5b66\u79d1\u521b\u9020\u529b\uff0c\u8fd9\u9700\u8981\u8c28\u614e\u8003\u8651AI\u5bf9\u79d1\u5b66\u4ef7\u503c\u4f53\u7cfb\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.24611", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.24611", "abs": "https://arxiv.org/abs/2510.24611", "authors": ["Azadeh Pourkabirian", "Amir Masoud Rahmani", "Kai Li", "Wei Ni"], "title": "Strategic Task Offloading for Delay-Sensitive IoT Applications: A Game-Theory-Based Demand-Supply Mechanism with Participation Incentives", "comment": null, "summary": "Delay-sensitive Internet of Things (IoT) applications have drawn significant\nattention. Running many of these applications on IoT devices is challenging due\nto the limited processing resources of these devices and the need for real-time\nresponses. Task offloading can minimize latency by transferring computationally\nintensive tasks from IoT devices to resource-rich edge servers, ensuring delay\nand performance guarantees. In this paper, we develop a task-offloading\napproach for delay-sensitive IoT applications in edge computing environments.\nUnlike existing schemes, we model the task offloading problem as an economic\ndemand and supply model to achieve market balance. The proposed model avoids\nunder- and over-supply, ensuring the computational resources at edge servers\n(supply) are allocated in a manner that best meets the processing and\ncomputational needs of user devices (demand). Given the multi-agent nature of\ntask offloading involving users and service providers with different\npreferences and objectives, we design a game-theoretic framework using a\nVickrey-Clarke-Groves (VCG) auction. This framework analyzes agent interactions\nand decision-making processes. Additionally, we develop an incentive mechanism\nto encourage both parties to participate in the auction. The mechanism\nmaximizes user task offloading to edge servers and motivates edge servers to\nshare their computational resources, achieving profitability for both IoT users\nand edge servers. Simulations demonstrate our method maximizes social welfare,\nensures truthfulness, maintains market balance, and provides latency guarantees\nfor delay-sensitive IoT applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ecf\u6d4e\u4f9b\u9700\u6a21\u578b\u548cVCG\u62cd\u5356\u7684\u7269\u8054\u7f51\u4efb\u52a1\u5378\u8f7d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e02\u573a\u5e73\u8861\u673a\u5236\u4f18\u5316\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u4e3a\u5ef6\u8fdf\u654f\u611f\u578bIoT\u5e94\u7528\u63d0\u4f9b\u5ef6\u8fdf\u4fdd\u8bc1\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u5904\u7406\u8d44\u6e90\u6709\u9650\uff0c\u800c\u5ef6\u8fdf\u654f\u611f\u578b\u5e94\u7528\u9700\u8981\u5b9e\u65f6\u54cd\u5e94\u3002\u4efb\u52a1\u5378\u8f7d\u53ef\u4ee5\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u8f6c\u79fb\u5230\u8d44\u6e90\u4e30\u5bcc\u7684\u8fb9\u7f18\u670d\u52a1\u5668\uff0c\u4f46\u73b0\u6709\u65b9\u6848\u7f3a\u4e4f\u6709\u6548\u7684\u8d44\u6e90\u5206\u914d\u673a\u5236\u3002", "method": "\u5c06\u4efb\u52a1\u5378\u8f7d\u5efa\u6a21\u4e3a\u7ecf\u6d4e\u4f9b\u9700\u6a21\u578b\uff0c\u4f7f\u7528VCG\u62cd\u5356\u8bbe\u8ba1\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u5f00\u53d1\u6fc0\u52b1\u673a\u5236\u4fc3\u8fdb\u7528\u6237\u548c\u670d\u52a1\u63d0\u4f9b\u5546\u53c2\u4e0e\u62cd\u5356\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u3001\u786e\u4fdd\u771f\u5b9e\u6027\u3001\u7ef4\u6301\u5e02\u573a\u5e73\u8861\uff0c\u5e76\u4e3a\u5ef6\u8fdf\u654f\u611f\u578bIoT\u5e94\u7528\u63d0\u4f9b\u5ef6\u8fdf\u4fdd\u8bc1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u7ecf\u6d4e\u6a21\u578b\u548c\u62cd\u5356\u673a\u5236\u7684\u4efb\u52a1\u5378\u8f7d\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u7528\u6237\u548c\u670d\u52a1\u63d0\u4f9b\u5546\u7684\u53cc\u8d62\u3002"}}
{"id": "2510.24246", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24246", "abs": "https://arxiv.org/abs/2510.24246", "authors": ["Hiroaki Hashida", "Boya Di"], "title": "Precoding-free Hierarchical Rate-Splitting Multiple Access via Stacked Intelligent Metasurface", "comment": "Submitted to IEEE Internet of Things Journal", "summary": "Interference management is a central bottleneck in dense multi-antenna\nwireless networks. Therefore, in this study, we present a digital\nprecoding-free hierarchical rate-splitting multiple access (HRSMA) architecture\nassisted by a stacked intelligent metasurface (SIM) to achieve high spectral\nefficiency and user fairness with reduced hardware complexity. In the proposed\nsystem, the base station performs only scalar power allocation, while a\nmulti-layer SIM acts as a wave-domain processor that spatially separates users\nand mitigates interference via nonlinear wavefront reconfiguration. This design\neliminates the need for digital or hybrid precoding, drastically reducing the\nbaseband computations. A joint optimization problem is formulated to maximize\nthe minimum user rate by jointly optimizing SIM phase shifts, power allocation,\nand user grouping. To efficiently solve the resulting non-convex problem, an\nalternating optimization algorithm is developed, combining simultaneous\nperturbation stochastic approximation (SPSA) for SIM configuration and power\ncontrol with clustering-based grouping refinement. Simulation results\ndemonstrate that the proposed SIM-aided HRSMA achieves substantial gains in\nboth spectral efficiency and fairness compared to hybrid beamforming and\nnon-precoding baselines. Specifically, SIM-aided HRSMA attains comparable or\nsuperior minimum rates with significantly fewer active antennas by exploiting\nthe additional wave-domain degrees of freedom provided by multi-layer SIMs.\nThese findings highlight the potential of SIM-aided HRSMA as a low-cost,\nenergy-efficient, and scalable solution for beyond-6G networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5806\u53e0\u667a\u80fd\u8d85\u8868\u9762(SIM)\u7684\u65e0\u6570\u5b57\u9884\u7f16\u7801\u5206\u5c42\u901f\u7387\u5206\u5272\u591a\u5740\u63a5\u5165(HRSMA)\u67b6\u6784\uff0c\u901a\u8fc7\u6ce2\u57df\u5904\u7406\u5b9e\u73b0\u5e72\u6270\u7ba1\u7406\u548c\u7528\u6237\u5206\u79bb\uff0c\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u590d\u6742\u5ea6\u548c\u57fa\u5e26\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u5bc6\u96c6\u591a\u5929\u7ebf\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5e72\u6270\u7ba1\u7406\u662f\u6838\u5fc3\u74f6\u9888\uff0c\u9700\u8981\u5728\u9ad8\u9891\u8c31\u6548\u7387\u548c\u7528\u6237\u516c\u5e73\u6027\u7684\u540c\u65f6\u964d\u4f4e\u786c\u4ef6\u590d\u6742\u5ea6\u3002", "method": "\u57fa\u7ad9\u4ec5\u6267\u884c\u6807\u91cf\u529f\u7387\u5206\u914d\uff0c\u591a\u5c42SIM\u4f5c\u4e3a\u6ce2\u57df\u5904\u7406\u5668\u901a\u8fc7\u975e\u7ebf\u6027\u6ce2\u524d\u91cd\u6784\u5b9e\u73b0\u7528\u6237\u7a7a\u95f4\u5206\u79bb\u548c\u5e72\u6270\u6291\u5236\u3002\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u8054\u5408\u4f18\u5316SIM\u76f8\u79fb\u3001\u529f\u7387\u5206\u914d\u548c\u7528\u6237\u5206\u7ec4\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cSIM\u8f85\u52a9\u7684HRSMA\u5728\u9891\u8c31\u6548\u7387\u548c\u516c\u5e73\u6027\u65b9\u9762\u76f8\u6bd4\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u548c\u65e0\u9884\u7f16\u7801\u57fa\u51c6\u65b9\u6848\u6709\u663e\u8457\u63d0\u5347\uff0c\u80fd\u4ee5\u66f4\u5c11\u7684\u6709\u6e90\u5929\u7ebf\u5b9e\u73b0\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6700\u5c0f\u901f\u7387\u3002", "conclusion": "SIM\u8f85\u52a9\u7684HRSMA\u6709\u671b\u6210\u4e3a\u8d856G\u7f51\u7edc\u7684\u4f4e\u6210\u672c\u3001\u9ad8\u80fd\u6548\u548c\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23744", "abs": "https://arxiv.org/abs/2510.23744", "authors": ["Eline M. Bovy", "Caleb Probine", "Marnix Suilen", "Ufuk Topcu", "Nils Jansen"], "title": "Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability", "comment": "Accepted at NeurIPS 2025", "summary": "Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete\nmodel uncertainty. ME-POMDPs represent a finite set of POMDPs that share the\nsame state, action, and observation spaces, but may arbitrarily vary in their\ntransition, observation, and reward models. Such models arise, for instance,\nwhen multiple domain experts disagree on how to model a problem. The goal is to\nfind a single policy that is robust against any choice of POMDP within the set,\ni.e., a policy that maximizes the worst-case reward across all POMDPs. We\ngeneralize and expand on existing work in the following way. First, we show\nthat ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which\nwe call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any\narbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its\ntransition and reward functions or only in its observation and reward\nfunctions, while preserving (optimal) policies. We then devise exact and\napproximate (point-based) algorithms to compute robust policies for AB-POMDPs,\nand thus ME-POMDPs. We demonstrate that we can compute policies for standard\nPOMDP benchmarks extended to the multi-environment setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u73af\u5883POMDPs\uff08ME-POMDPs\uff09\u53ca\u5176\u6269\u5c55\u5f62\u5f0fAB-POMDPs\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u79bb\u6563\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684POMDP\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u7cbe\u786e\u548c\u8fd1\u4f3c\u7684\u7b97\u6cd5\u6765\u8ba1\u7b97\u9c81\u68d2\u7b56\u7565\u3002", "motivation": "\u5f53\u591a\u4e2a\u9886\u57df\u4e13\u5bb6\u5bf9\u95ee\u9898\u5efa\u6a21\u5b58\u5728\u5206\u6b67\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u6846\u67b6\u3002ME-POMDPs\u6269\u5c55\u4e86\u6807\u51c6POMDPs\uff0c\u80fd\u591f\u8868\u793a\u4e00\u7ec4\u5171\u4eab\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u89c2\u6d4b\u7a7a\u95f4\u4f46\u53ef\u80fd\u4efb\u610f\u53d8\u5316\u5176\u8f6c\u79fb\u3001\u89c2\u6d4b\u548c\u5956\u52b1\u6a21\u578b\u7684POMDPs\u96c6\u5408\u3002", "method": "\u9996\u5148\u5c06ME-POMDPs\u63a8\u5e7f\u5230\u5177\u6709\u521d\u59cb\u4fe1\u5ff5\u96c6\u5408\u7684AB-POMDPs\uff1b\u7136\u540e\u8bc1\u660e\u4efb\u610fME-POMDP\u53ef\u4ee5\u7b80\u5316\u4e3a\u4ec5\u5728\u8f6c\u79fb\u548c\u5956\u52b1\u51fd\u6570\u6216\u4ec5\u5728\u89c2\u6d4b\u548c\u5956\u52b1\u51fd\u6570\u4e0a\u53d8\u5316\u7684ME-POMDP\uff1b\u6700\u540e\u8bbe\u8ba1\u4e86\u7cbe\u786e\u548c\u8fd1\u4f3c\uff08\u57fa\u4e8e\u70b9\u7684\uff09\u7b97\u6cd5\u6765\u8ba1\u7b97\u9c81\u68d2\u7b56\u7565\u3002", "result": "\u6210\u529f\u5c06\u6807\u51c6POMDP\u57fa\u51c6\u6269\u5c55\u5230\u591a\u73af\u5883\u8bbe\u7f6e\uff0c\u5e76\u80fd\u591f\u8ba1\u7b97\u76f8\u5e94\u7684\u7b56\u7565\u3002", "conclusion": "ME-POMDPs\u548cAB-POMDPs\u4e3a\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u5728\u591a\u73af\u5883\u8bbe\u7f6e\u4e0b\u8ba1\u7b97\u9c81\u68d2\u7b56\u7565\u3002"}}
{"id": "2510.24480", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24480", "abs": "https://arxiv.org/abs/2510.24480", "authors": ["Qing Xue", "Yun Lan", "Jiajia Guo", "Qianbin Chen", "Shaodan Ma"], "title": "Joint Active and Passive Beamforming with Sensing-Assisted Discrete Phase Shifts for Dual-RIS ISAC Systems", "comment": null, "summary": "Targeting the requirements of 6G, this paper investigates a semi-passive\ndual-reconfigurable intelligent surface (RIS)-assisted integrated sensing and\ncommunication (ISAC) system, tackling the max-min user\nsignal-to-interference-plus-noise ratio (SINR) problem via joint active and\npassive beamforming to enhance system performance and ensure user fairness.\nAddressing this challenge, we first utilize dual RISs for user angle estimation\nto simplify the solution process of the formulated problem, an efficient\nalternating optimization algorithm is then developed. Specifically,\nsemi-definite relaxation and the bisection method are employed to solve the\ntransmit beamforming optimization subproblem. For the RIS discrete phase\nshifts, a sensing-assisted approach is adopted to constrain the optimization\nsearch space, with two distinct low-complexity search strategies introduced for\ndifferent RIS sizes. Numerical simulation results demonstrate that the proposed\nalgorithm achieves performance close to the ideal continuous phase shift\nbenchmark, outperforms conventional discrete phase shift optimization\nalgorithms, and exhibits a significant improvement over single-RIS systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u534a\u88ab\u52a8\u53ccRIS\u8f85\u52a9\u7684ISAC\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4e3b\u52a8\u548c\u88ab\u52a8\u6ce2\u675f\u6210\u5f62\u89e3\u51b3\u6700\u5927-\u6700\u5c0f\u7528\u6237SINR\u95ee\u9898\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u5e76\u786e\u4fdd\u7528\u6237\u516c\u5e73\u6027\u3002", "motivation": "\u9488\u5bf96G\u9700\u6c42\uff0c\u7814\u7a76\u53ccRIS\u8f85\u52a9\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u6765\u589e\u5f3a\u7cfb\u7edf\u6027\u80fd\u548c\u7528\u6237\u516c\u5e73\u6027\u3002", "method": "\u9996\u5148\u5229\u7528\u53ccRIS\u8fdb\u884c\u7528\u6237\u89d2\u5ea6\u4f30\u8ba1\u7b80\u5316\u95ee\u9898\u6c42\u89e3\uff0c\u7136\u540e\u5f00\u53d1\u9ad8\u6548\u7684\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u5305\u62ec\u534a\u5b9a\u677e\u5f1b\u548c\u4e8c\u5206\u6cd5\u89e3\u51b3\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u5b50\u95ee\u9898\uff0c\u5e76\u5bf9RIS\u79bb\u6563\u76f8\u79fb\u91c7\u7528\u611f\u77e5\u8f85\u52a9\u65b9\u6cd5\u7ea6\u675f\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u6570\u503c\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u6027\u80fd\u63a5\u8fd1\u7406\u60f3\u8fde\u7eed\u76f8\u79fb\u57fa\u51c6\uff0c\u4f18\u4e8e\u4f20\u7edf\u79bb\u6563\u76f8\u79fb\u4f18\u5316\u7b97\u6cd5\uff0c\u76f8\u6bd4\u5355RIS\u7cfb\u7edf\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u53ccRIS\u8f85\u52a9\u7684ISAC\u7cfb\u7edf\u901a\u8fc7\u63d0\u51fa\u7684\u4f18\u5316\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u5728\u79bb\u6563\u76f8\u79fb\u7ea6\u675f\u4e0b\u4ecd\u80fd\u83b7\u5f97\u63a5\u8fd1\u8fde\u7eed\u76f8\u79fb\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.23746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23746", "abs": "https://arxiv.org/abs/2510.23746", "authors": ["Laura Mismetti", "Marvin Alberts", "Andreas Krause", "Mara Graziani"], "title": "Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra", "comment": null, "summary": "Tandem Mass Spectrometry enables the identification of unknown compounds in\ncrucial fields such as metabolomics, natural product discovery and\nenvironmental analysis. However, current methods rely on database matching from\npreviously observed molecules, or on multi-step pipelines that require\nintermediate fragment or fingerprint prediction. This makes finding the correct\nmolecule highly challenging, particularly for compounds absent from reference\ndatabases. We introduce a framework that, by leveraging test-time tuning,\nenhances the learning of a pre-trained transformer model to address this gap,\nenabling end-to-end de novo molecular structure generation directly from the\ntandem mass spectra and molecular formulae, bypassing manual annotations and\nintermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on\ntwo popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.\nTest-time tuning on experimental spectra allows the model to dynamically adapt\nto novel spectra, and the relative performance gain over conventional\nfine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground\ntruth, the generated molecular candidates remain structurally accurate,\nproviding valuable guidance for human interpretation and more reliable\nidentification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d4b\u8bd5\u65f6\u8c03\u4f18\u7684\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3transformer\u6a21\u578b\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u8fdb\u884c\u7aef\u5230\u7aef\u7684\u4ece\u5934\u5206\u5b50\u7ed3\u6784\u751f\u6210\uff0c\u65e0\u9700\u6570\u636e\u5e93\u5339\u914d\u6216\u4e2d\u95f4\u6b65\u9aa4\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u5e93\u5339\u914d\u6216\u9700\u8981\u4e2d\u95f4\u7247\u6bb5\u9884\u6d4b\u7684\u591a\u6b65\u9aa4\u6d41\u7a0b\uff0c\u96be\u4ee5\u8bc6\u522b\u53c2\u8003\u6570\u636e\u5e93\u4e2d\u4e0d\u5b58\u5728\u7684\u5316\u5408\u7269\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8c03\u4f18\u589e\u5f3a\u9884\u8bad\u7ec3transformer\u6a21\u578b\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u751f\u6210\u5206\u5b50\u7ed3\u6784\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u548c\u4e2d\u95f4\u6b65\u9aa4\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5NPLIB1\u548cMassSpecGym\u4e0a\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5DiffMS\u63d0\u9ad8\u4e86100%\u548c20%\uff0c\u6d4b\u8bd5\u65f6\u8c03\u4f18\u5728MassSpecGym\u4e0a\u6bd4\u4f20\u7edf\u5fae\u8c03\u6027\u80fd\u63d0\u534762%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u52a8\u6001\u9002\u5e94\u65b0\u8d28\u8c31\uff0c\u5373\u4f7f\u9884\u6d4b\u4e0e\u771f\u5b9e\u7ed3\u6784\u6709\u504f\u5dee\uff0c\u751f\u6210\u7684\u5206\u5b50\u5019\u9009\u4ecd\u4fdd\u6301\u7ed3\u6784\u51c6\u786e\u6027\uff0c\u4e3a\u4eba\u5de5\u89e3\u91ca\u63d0\u4f9b\u6709\u4ef7\u503c\u6307\u5bfc\u3002"}}
{"id": "2510.24546", "categories": ["cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24546", "abs": "https://arxiv.org/abs/2510.24546", "authors": ["Lingyi Wang", "Rashed Shelim", "Walid Saad", "Naren Ramakrishnan"], "title": "Dual-Mind World Models: A General Framework for Learning in Dynamic Wireless Networks", "comment": null, "summary": "Despite the popularity of reinforcement learning (RL) in wireless networks,\nexisting approaches that rely on model-free RL (MFRL) and model-based RL (MBRL)\nare data inefficient and short-sighted. Such RL-based solutions cannot\ngeneralize to novel network states since they capture only statistical patterns\nrather than the underlying physics and logic from wireless data. These\nlimitations become particularly challenging in complex wireless networks with\nhigh dynamics and long-term planning requirements. To address these\nlimitations, in this paper, a novel dual-mind world model-based learning\nframework is proposed with the goal of optimizing completeness-weighted age of\ninformation (CAoI) in a challenging mmWave V2X scenario. Inspired by cognitive\npsychology, the proposed dual-mind world model encompasses a pattern-driven\nSystem 1 component and a logic-driven System 2 component to learn dynamics and\nlogic of the wireless network, and to provide long-term link scheduling over\nreliable imagined trajectories. Link scheduling is learned through end-to-end\ndifferentiable imagined trajectories with logical consistency over an extended\nhorizon rather than relying on wireless data obtained from environment\ninteractions. Moreover, through imagination rollouts, the proposed world model\ncan jointly reason network states and plan link scheduling. During intervals\nwithout observations, the proposed method remains capable of making efficient\ndecisions. Extensive experiments are conducted on a realistic simulator based\non Sionna with real-world physical channel, ray-tracing, and scene objects with\nmaterial properties. Simulation results show that the proposed world model\nachieves a significant improvement in data efficiency and achieves strong\ngeneralization and adaptation to unseen environments, compared to the\nstate-of-the-art RL baselines, and the world model approach with only System 1.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5fc3\u667a\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u5f0f\u9a71\u52a8\u7684\u7cfb\u7edf1\u548c\u903b\u8f91\u9a71\u52a8\u7684\u7cfb\u7edf2\u7ec4\u4ef6\uff0c\u5728\u6beb\u7c73\u6ce2V2X\u573a\u666f\u4e2d\u4f18\u5316\u5b8c\u6574\u6027\u52a0\u6743\u4fe1\u606f\u5e74\u9f84\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u65e0\u5173RL\u548c\u6a21\u578b\u57fa\u7840RL\u7684\u65b9\u6cd5\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u6570\u636e\u6548\u7387\u4f4e\u3001\u76ee\u5149\u77ed\u6d45\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u7684\u7f51\u7edc\u72b6\u6001\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ea\u6355\u6349\u7edf\u8ba1\u6a21\u5f0f\u800c\u975e\u5e95\u5c42\u7269\u7406\u548c\u903b\u8f91\u3002\u5728\u590d\u6742\u65e0\u7ebf\u7f51\u7edc\u7684\u9ad8\u52a8\u6001\u6027\u548c\u957f\u671f\u89c4\u5212\u9700\u6c42\u4e0b\uff0c\u8fd9\u4e9b\u9650\u5236\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u63d0\u51fa\u53cc\u5fc3\u667a\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u5305\u542b\u6a21\u5f0f\u9a71\u52a8\u7684\u7cfb\u7edf1\u7ec4\u4ef6\u548c\u903b\u8f91\u9a71\u52a8\u7684\u7cfb\u7edf2\u7ec4\u4ef6\uff0c\u5b66\u4e60\u65e0\u7ebf\u7f51\u7edc\u7684\u52a8\u6001\u548c\u903b\u8f91\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u60f3\u8c61\u8f68\u8ff9\u8fdb\u884c\u94fe\u8def\u8c03\u5ea6\uff0c\u800c\u975e\u4f9d\u8d56\u73af\u5883\u4ea4\u4e92\u83b7\u53d6\u7684\u65e0\u7ebf\u6570\u636e\u3002", "result": "\u5728\u57fa\u4e8eSionna\u7684\u771f\u5b9e\u6a21\u62df\u5668\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u548c\u9002\u5e94\u80fd\u529b\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684RL\u57fa\u7ebf\u548c\u4ec5\u4f7f\u7528\u7cfb\u7edf1\u7684\u4e16\u754c\u6a21\u578b\u65b9\u6cd5\u3002", "conclusion": "\u53cc\u5fc3\u667a\u4e16\u754c\u6a21\u578b\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6a21\u5f0f\u8bc6\u522b\u548c\u903b\u8f91\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u7ebf\u7f51\u7edc\u4e2dRL\u65b9\u6cd5\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u957f\u671f\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23772", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23772", "abs": "https://arxiv.org/abs/2510.23772", "authors": ["Vivek Veeriah", "Federico Barbero", "Marcus Chiam", "Xidong Feng", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Johan Obando-Ceron", "Jiaxin Shi", "Shaobo Hou", "Satinder Singh", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions", "comment": "Accepted at the Creative AI Track, NeurIPS 2025", "summary": "The rapid advancement of Generative AI has raised significant questions\nregarding its ability to produce creative and novel outputs. Our recent work\ninvestigates this question within the domain of chess puzzles and presents an\nAI system designed to generate puzzles characterized by aesthetic appeal,\nnovelty, counter-intuitive and unique solutions. We briefly discuss our method\nbelow and refer the reader to the technical paper for more details. To assess\nour system's creativity, we presented a curated booklet of AI-generated puzzles\nto three world-renowned experts: International Master for chess compositions\nAmatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All\nthree are noted authors on chess aesthetics and the evolving role of computers\nin the game. They were asked to select their favorites and explain what made\nthem appealing, considering qualities such as their creativity, level of\nchallenge, or aesthetic design.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0fAI\u5728\u8c61\u68cb\u8c1c\u9898\u9886\u57df\u7684\u521b\u9020\u529b\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u751f\u6210\u5177\u6709\u7f8e\u5b66\u5438\u5f15\u529b\u3001\u65b0\u9896\u6027\u548c\u53cd\u76f4\u89c9\u72ec\u7279\u89e3\u6cd5\u7684AI\u7cfb\u7edf\uff0c\u5e76\u7531\u56fd\u9645\u8c61\u68cb\u4e13\u5bb6\u8bc4\u4f30\u5176\u521b\u9020\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4eba\u4eec\u5bf9\u5176\u4ea7\u751f\u521b\u9020\u6027\u65b0\u9896\u8f93\u51fa\u7684\u80fd\u529b\u5b58\u5728\u7591\u95ee\uff0c\u7279\u522b\u662f\u5728\u8c61\u68cb\u8c1c\u9898\u8fd9\u79cd\u9700\u8981\u9ad8\u5ea6\u521b\u9020\u6027\u7684\u9886\u57df\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u95e8\u751f\u6210\u8c61\u68cb\u8c1c\u9898\u7684AI\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u4ea7\u751f\u5177\u6709\u7f8e\u5b66\u5438\u5f15\u529b\u3001\u65b0\u9896\u6027\u3001\u53cd\u76f4\u89c9\u548c\u72ec\u7279\u89e3\u6cd5\u7684\u8c1c\u9898\uff0c\u5e76\u9080\u8bf7\u4e09\u4f4d\u4e16\u754c\u77e5\u540d\u8c61\u68cb\u4e13\u5bb6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e09\u4f4d\u56fd\u9645\u8c61\u68cb\u5927\u5e08\uff08Amatzia Avni\u3001Jonathan Levitt\u548cMatthew Sadler\uff09\u5bf9AI\u751f\u6210\u7684\u8c1c\u9898\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u9009\u62e9\u4e86\u4ed6\u4eec\u6700\u559c\u6b22\u7684\u8c1c\u9898\u5e76\u89e3\u91ca\u4e86\u5176\u5438\u5f15\u529b\u6240\u5728\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u751f\u6210\u5f0fAI\u5728\u8c61\u68cb\u8c1c\u9898\u9886\u57df\u80fd\u591f\u4ea7\u751f\u5177\u6709\u521b\u9020\u6027\u548c\u7f8e\u5b66\u4ef7\u503c\u7684\u8f93\u51fa\uff0c\u4e13\u5bb6\u8bc4\u4f30\u8bc1\u5b9e\u4e86AI\u7cfb\u7edf\u5728\u521b\u9020\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.23807", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23807", "abs": "https://arxiv.org/abs/2510.23807", "authors": ["Hamid R. Tizhoosh"], "title": "Why Foundation Models in Pathology Are Failing", "comment": null, "summary": "In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.", "AI": {"tldr": "\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u5305\u62ec\u8bca\u65ad\u51c6\u786e\u7387\u4f4e\u3001\u9c81\u68d2\u6027\u5dee\u3001\u51e0\u4f55\u4e0d\u7a33\u5b9a\u3001\u8ba1\u7b97\u9700\u6c42\u5927\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u6e90\u4e8e\u901a\u7528\u57fa\u7840\u6a21\u578b\u5047\u8bbe\u4e0e\u4eba\u4f53\u7ec4\u7ec7\u590d\u6742\u6027\u7684\u6982\u5ff5\u4e0d\u5339\u914d\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u975e\u533b\u5b66\u9886\u57df\u53d6\u5f97\u4e86\u9769\u547d\u6027\u7a81\u7834\uff0c\u4f46\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5feb\u901f\u5e94\u7528\u5e76\u672a\u5e26\u6765\u9884\u671f\u7684\u764c\u75c7\u8bca\u65ad\u3001\u9884\u540e\u548c\u591a\u6a21\u6001\u68c0\u7d22\u7a81\u7834\uff0c\u53cd\u800c\u66b4\u9732\u51fa\u4e25\u91cd\u7f3a\u9677\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u5206\u6790\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u4e03\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u6839\u672c\u539f\u56e0\uff1a\u751f\u7269\u590d\u6742\u6027\u3001\u65e0\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u8fc7\u5ea6\u6cdb\u5316\u3001\u8fc7\u5ea6\u67b6\u6784\u590d\u6742\u6027\u3001\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u521b\u65b0\u3001\u6570\u636e\u4e0d\u8db3\u4ee5\u53ca\u4e0e\u7ec4\u7ec7\u5207\u7247\u5c3a\u5bf8\u76f8\u5173\u7684\u6839\u672c\u8bbe\u8ba1\u7f3a\u9677\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u6982\u5ff5\u4e0a\u4e0e\u7ec4\u7ec7\u5f62\u6001\u5b66\u672c\u8d28\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u548c\u5b89\u5168\u9690\u60a3\u3002", "conclusion": "\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u9700\u8981\u4ece\u6839\u672c\u4e0a\u91cd\u65b0\u601d\u8003\u8bbe\u8ba1\u8303\u5f0f\uff0c\u800c\u975e\u7b80\u5355\u5957\u7528\u901a\u7528AI\u6a21\u578b\u3002"}}
{"id": "2510.23822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23822", "abs": "https://arxiv.org/abs/2510.23822", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "comment": null, "summary": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning\nremain challenging for large language models (LLMs). Sequential prompting\nmethods are prone to context drift, loss of goal information, and recurrent\nfailure cycles, while hierarchical prompting methods often weaken cross-level\ncontinuity or incur substantial runtime overhead. We introduce ReCAP (Recursive\nContext-Aware Reasoning and Planning), a hierarchical framework with shared\ncontext for reasoning and planning in LLMs. ReCAP combines three key\nmechanisms: (i) plan-ahead decomposition, in which the model generates a full\nsubtask list, executes the first item, and refines the remainder; (ii)\nstructured re-injection of parent plans, maintaining consistent multi-level\ncontext during recursive return; and (iii) memory-efficient execution, bounding\nthe active prompt so costs scale linearly with task depth. Together these\nmechanisms align high-level goals with low-level actions, reduce redundant\nprompting, and preserve coherent context updates across recursion. Experiments\ndemonstrate that ReCAP substantially improves subgoal alignment and success\nrates on various long-horizon reasoning benchmarks, achieving a 32% gain on\nsynchronous Robotouille and a 29% improvement on asynchronous Robotouille under\nthe strict pass@1 protocol.", "AI": {"tldr": "ReCAP\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9012\u5f52\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u548c\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u5212\u5206\u89e3\u3001\u7ed3\u6784\u5316\u91cd\u6ce8\u5165\u7236\u8ba1\u5212\u548c\u5185\u5b58\u9ad8\u6548\u6267\u884c\u4e09\u4e2a\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u987a\u5e8f\u63d0\u793a\u65b9\u6cd5\u5bb9\u6613\u4ea7\u751f\u4e0a\u4e0b\u6587\u6f02\u79fb\u3001\u76ee\u6807\u4fe1\u606f\u4e22\u5931\u548c\u5faa\u73af\u5931\u8d25\u95ee\u9898\uff0c\u800c\u5206\u5c42\u63d0\u793a\u65b9\u6cd5\u5219\u524a\u5f31\u8de8\u5c42\u8fde\u7eed\u6027\u6216\u5e26\u6765\u5927\u91cf\u8fd0\u884c\u65f6\u5f00\u9500\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u6301\u591a\u7ea7\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u7684\u9ad8\u6548\u89c4\u5212\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u4e09\u4e2a\u5173\u952e\u673a\u5236\uff1a(1) \u63d0\u524d\u8ba1\u5212\u5206\u89e3 - \u751f\u6210\u5b8c\u6574\u5b50\u4efb\u52a1\u5217\u8868\uff0c\u6267\u884c\u7b2c\u4e00\u9879\u5e76\u4f18\u5316\u5269\u4f59\u4efb\u52a1\uff1b(2) \u7ed3\u6784\u5316\u91cd\u6ce8\u5165\u7236\u8ba1\u5212 - \u5728\u9012\u5f52\u8fd4\u56de\u65f6\u4fdd\u6301\u591a\u7ea7\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff1b(3) \u5185\u5b58\u9ad8\u6548\u6267\u884c - \u9650\u5236\u6d3b\u52a8\u63d0\u793a\uff0c\u4f7f\u6210\u672c\u968f\u4efb\u52a1\u6df1\u5ea6\u7ebf\u6027\u6269\u5c55\u3002", "result": "\u5728\u5404\u79cd\u957f\u65f6\u7a0b\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u5b50\u76ee\u6807\u5bf9\u9f50\u548c\u6210\u529f\u7387\uff0c\u5728\u540c\u6b65Robotouille\u4e0a\u83b7\u5f9732%\u7684\u63d0\u5347\uff0c\u5728\u5f02\u6b65Robotouille\u4e0a\u83b7\u5f9729%\u7684\u6539\u8fdb\uff08\u4e25\u683cpass@1\u534f\u8bae\uff09\u3002", "conclusion": "ReCAP\u6846\u67b6\u80fd\u591f\u6709\u6548\u5bf9\u9f50\u9ad8\u5c42\u76ee\u6807\u4e0e\u4f4e\u5c42\u52a8\u4f5c\uff0c\u51cf\u5c11\u5197\u4f59\u63d0\u793a\uff0c\u5e76\u5728\u9012\u5f52\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u8fde\u8d2f\u7684\u4e0a\u4e0b\u6587\u66f4\u65b0\uff0c\u4e3a\u957f\u65f6\u7a0b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u548c\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23824", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23824", "abs": "https://arxiv.org/abs/2510.23824", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "comment": "Accepted at MIT URTC 2025", "summary": "Coordinating multiple autonomous agents in shared environments under\ndecentralized conditions is a long-standing challenge in robotics and\nartificial intelligence. This work addresses the problem of decentralized goal\nassignment for multi-agent path planning, where agents independently generate\nranked preferences over goals based on structured representations of the\nenvironment, including grid visualizations and scenario data. After this\nreasoning phase, agents exchange their goal rankings, and assignments are\ndetermined by a fixed, deterministic conflict-resolution rule (e.g., agent\nindex ordering), without negotiation or iterative coordination. We\nsystematically compare greedy heuristics, optimal assignment, and large\nlanguage model (LLM)-based agents in fully observable grid-world settings. Our\nresults show that LLM-based agents, when provided with well-designed prompts\nand relevant quantitative information, can achieve near-optimal makespans and\nconsistently outperform traditional heuristics. These findings underscore the\npotential of language models for decentralized goal assignment in multi-agent\npath planning and highlight the importance of information structure in such\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u6bd4\u8f83\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0LLM\u667a\u80fd\u4f53\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u4e0b\u80fd\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u6761\u4ef6\u4e0b\u591a\u667a\u80fd\u4f53\u5728\u5171\u4eab\u73af\u5883\u4e2d\u7684\u534f\u8c03\u6311\u6218\uff0c\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u5206\u914d\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u667a\u80fd\u4f53\u57fa\u4e8e\u73af\u5883\u7ed3\u6784\u5316\u8868\u793a\u72ec\u7acb\u751f\u6210\u76ee\u6807\u504f\u597d\u6392\u540d\uff0c\u901a\u8fc7\u56fa\u5b9a\u51b2\u7a81\u89e3\u51b3\u89c4\u5219\u8fdb\u884c\u76ee\u6807\u5206\u914d\uff0c\u65e0\u9700\u534f\u5546\u6216\u8fed\u4ee3\u534f\u8c03\u3002\u7cfb\u7edf\u6bd4\u8f83\u4e86\u8d2a\u5fc3\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548cLLM\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "result": "LLM\u667a\u80fd\u4f53\u5728\u63d0\u4f9b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u76f8\u5173\u5b9a\u91cf\u4fe1\u606f\u65f6\uff0c\u80fd\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u5b8c\u5de5\u65f6\u95f4\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4fe1\u606f\u7ed3\u6784\u5728\u6b64\u7c7b\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.23856", "categories": ["cs.AI", "68Txx"], "pdf": "https://arxiv.org/pdf/2510.23856", "abs": "https://arxiv.org/abs/2510.23856", "authors": ["Segev Shlomov", "Alon Oved", "Sami Marreed", "Ido Levy", "Offer Akrabi", "Avi Yaeli", "\u0141ukasz Str\u0105k", "Elizabeth Koumpan", "Yinon Goldshtein", "Eilam Shapira", "Nir Mashkif", "Asaf Adi"], "title": "From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production", "comment": "AAAI Conference on Artificial Intelligence", "summary": "Agents are rapidly advancing in automating digital work, but enterprises face\na harder challenge: moving beyond prototypes to deployed systems that deliver\nmeasurable business value. This path is complicated by fragmented frameworks,\nslow development, and the absence of standardized evaluation practices.\nGeneralist agents have emerged as a promising direction, excelling on academic\nbenchmarks and offering flexibility across task types, applications, and\nmodalities. Yet, evidence of their use in production enterprise settings\nremains limited. This paper reports IBM's experience developing and piloting\nthe Computer Using Generalist Agent (CUGA), which has been open-sourced for the\ncommunity (https://github.com/cuga-project/cuga-agent). CUGA adopts a\nhierarchical planner--executor architecture with strong analytical foundations,\nachieving state-of-the-art performance on AppWorld and WebArena. Beyond\nbenchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing\ntalent acquisition domain, addressing enterprise requirements for scalability,\nauditability, safety, and governance. To support assessment, we introduce\nBPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary\nevaluations, CUGA approached the accuracy of specialized agents while\nindicating potential for reducing development time and cost. Our contribution\nis twofold: presenting early evidence of generalist agents operating at\nenterprise scale, and distilling technical and organizational lessons from this\ninitial pilot. We outline requirements and next steps for advancing\nresearch-grade architectures like CUGA into robust, enterprise-ready systems.", "AI": {"tldr": "IBM\u5f00\u53d1\u4e86\u901a\u7528\u4ee3\u7406CUGA\uff0c\u91c7\u7528\u5206\u5c42\u89c4\u5212-\u6267\u884c\u67b6\u6784\uff0c\u5728\u4f01\u4e1a\u4e1a\u52a1\u6d41\u7a0b\u5916\u5305\u4eba\u624d\u62db\u8058\u9886\u57df\u8fdb\u884c\u8bd5\u70b9\uff0c\u5c55\u793a\u4e86\u901a\u7528\u4ee3\u7406\u5728\u4f01\u4e1a\u89c4\u6a21\u5e94\u7528\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u90e8\u7f72AI\u4ee3\u7406\u7cfb\u7edf\u9762\u4e34\u7684\u6311\u6218\uff1a\u6846\u67b6\u788e\u7247\u5316\u3001\u5f00\u53d1\u7f13\u6162\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u5b9e\u8df5\uff0c\u63a8\u52a8\u901a\u7528\u4ee3\u7406\u4ece\u539f\u578b\u8d70\u5411\u5b9e\u9645\u4f01\u4e1a\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5206\u5c42\u89c4\u5212-\u6267\u884c\u67b6\u6784\uff0c\u57fa\u4e8e\u5f3a\u5206\u6790\u57fa\u7840\uff0c\u5728AppWorld\u548cWebArena\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u4f01\u4e1a\u4e1a\u52a1\u6d41\u7a0b\u5916\u5305\u4eba\u624d\u62db\u8058\u9886\u57df\u8fdb\u884c\u8bd5\u70b9\u8bc4\u4f30\u3002", "result": "CUGA\u5728BPO-TA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63a5\u8fd1\u4e13\u7528\u4ee3\u7406\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u793a\u51fa\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u548c\u6210\u672c\u7684\u6f5c\u529b\uff0c\u4e3a\u4f01\u4e1a\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u65e9\u671f\u8bc1\u636e\u3002", "conclusion": "\u901a\u7528\u4ee3\u7406\u5728\u4f01\u4e1a\u89c4\u6a21\u5e94\u7528\u5177\u6709\u53ef\u884c\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5c06\u7814\u7a76\u7ea7\u67b6\u6784\u8f6c\u5316\u4e3a\u4f01\u4e1a\u5c31\u7eea\u7cfb\u7edf\uff0c\u5e76\u603b\u7ed3\u4e86\u6280\u672f\u548c\u7ec4\u7ec7\u5c42\u9762\u7684\u7ecf\u9a8c\u6559\u8bad\u3002"}}
{"id": "2510.23881", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23881", "abs": "https://arxiv.org/abs/2510.23881", "authors": ["Xidong Feng", "Vivek Veeriah", "Marcus Chiam", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Federico Barbero", "Johan Obando-Ceron", "Jiaxin Shi", "Satinder Singh", "Shaobo Hou", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Generating Creative Chess Puzzles", "comment": null, "summary": "While Generative AI rapidly advances in various domains, generating truly\ncreative, aesthetic, and counter-intuitive outputs remains a challenge. This\npaper presents an approach to tackle these difficulties in the domain of chess\npuzzles. We start by benchmarking Generative AI architectures, and then\nintroduce an RL framework with novel rewards based on chess engine search\nstatistics to overcome some of those shortcomings. The rewards are designed to\nenhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.\nOur RL approach dramatically increases counter-intuitive puzzle generation by\n10x, from 0.22\\% (supervised) to 2.5\\%, surpassing existing dataset rates\n(2.1\\%) and the best Lichess-trained model (0.4\\%). Our puzzles meet novelty\nand diversity benchmarks, retain aesthetic themes, and are rated by human\nexperts as more creative, enjoyable, and counter-intuitive than composed book\npuzzles, even approaching classic compositions. Our final outcome is a curated\nbooklet of these AI-generated puzzles, which is acknowledged for creativity by\nthree world-renowned experts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8e\u8c61\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u65b0\u578b\u5956\u52b1\u51fd\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8c1c\u9898\u7684\u521b\u9020\u6027\u3001\u53cd\u76f4\u89c9\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u521b\u9020\u771f\u6b63\u5177\u6709\u521b\u9020\u6027\u3001\u7f8e\u5b66\u4ef7\u503c\u548c\u53cd\u76f4\u89c9\u6027\u7684\u8f93\u51fa\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u751f\u6210\u9886\u57df\u3002", "method": "\u9996\u5148\u5bf9\u751f\u6210\u5f0fAI\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7136\u540e\u5f15\u5165\u57fa\u4e8e\u8c61\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u6765\u589e\u5f3a\u8c1c\u9898\u7684\u72ec\u7279\u6027\u3001\u53cd\u76f4\u89c9\u6027\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u53cd\u76f4\u89c9\u8c1c\u9898\u751f\u6210\u7387\u4ece0.22%\u5927\u5e45\u63d0\u5347\u81f32.5%\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6570\u636e\u96c6(2.1%)\u548c\u6700\u4f73Lichess\u8bad\u7ec3\u6a21\u578b(0.4%)\u3002\u751f\u6210\u7684\u8c1c\u9898\u5728\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u4e2d\u88ab\u8ba4\u4e3a\u6bd4\u4f20\u7edf\u4e66\u7c4d\u8c1c\u9898\u66f4\u5177\u521b\u9020\u6027\u3001\u8da3\u5473\u6027\u548c\u53cd\u76f4\u89c9\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u5177\u6709\u9ad8\u5ea6\u521b\u9020\u6027\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\uff0c\u4e09\u4f4d\u4e16\u754c\u77e5\u540d\u4e13\u5bb6\u8ba4\u53ef\u4e86\u8fd9\u4e9bAI\u751f\u6210\u8c1c\u9898\u7684\u521b\u9020\u529b\uff0c\u6700\u7ec8\u5f62\u6210\u4e86\u7cbe\u9009\u7684\u8c1c\u9898\u624b\u518c\u3002"}}
{"id": "2510.23882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23882", "abs": "https://arxiv.org/abs/2510.23882", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins", "comment": null, "summary": "This work investigates the use of digital twins for dynamical system modeling\nand control, integrating physics-based, data-driven, and hybrid approaches with\nboth traditional and AI-driven controllers. Using a miniature greenhouse as a\ntest platform, four predictive models Linear, Physics-Based Modeling (PBM),\nLong Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are\ndeveloped and compared under interpolation and extrapolation scenarios. Three\ncontrol strategies Model Predictive Control (MPC), Reinforcement Learning (RL),\nand Large Language Model (LLM) based control are also implemented to assess\ntrade-offs in precision, adaptability, and implementation effort. Results show\nthat in modeling HAM provides the most balanced performance across accuracy,\ngeneralization, and computational efficiency, while LSTM achieves high\nprecision at greater resource cost. Among controllers, MPC delivers robust and\npredictable performance, RL demonstrates strong adaptability, and LLM-based\ncontrollers offer flexible human-AI interaction when coupled with predictive\ntools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08\u7ebf\u6027\u3001\u7269\u7406\u5efa\u6a21\u3001LSTM\u3001\u6df7\u5408\u5efa\u6a21\uff09\u548c\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff08MPC\u3001RL\u3001LLM\uff09\u5728\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0HAM\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u5747\u8861\uff0cMPC\u63a7\u5236\u6700\u7a33\u5065\uff0cRL\u9002\u5e94\u6027\u6700\u5f3a\uff0cLLM\u63a7\u5236\u5668\u5728\u4eba\u673a\u4ea4\u4e92\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u6570\u5b57\u5b6a\u751f\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u548c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u6574\u5408\u7269\u7406\u57fa\u7840\u3001\u6570\u636e\u9a71\u52a8\u548c\u6df7\u5408\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4f20\u7edf\u4e0eAI\u9a71\u52a8\u63a7\u5236\u5668\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u5fae\u578b\u6e29\u5ba4\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5f00\u53d1\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08\u7ebf\u6027\u3001PBM\u3001LSTM\u3001HAM\uff09\u548c\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff08MPC\u3001RL\u3001LLM\uff09\uff0c\u5728\u63d2\u503c\u548c\u5916\u63a8\u573a\u666f\u4e0b\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "HAM\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u5747\u8861\uff1bLSTM\u7cbe\u5ea6\u9ad8\u4f46\u8d44\u6e90\u6d88\u8017\u5927\uff1bMPC\u63a7\u5236\u7a33\u5065\u53ef\u9884\u6d4b\uff1bRL\u9002\u5e94\u6027\u5f3a\uff1bLLM\u63a7\u5236\u5668\u7ed3\u5408\u9884\u6d4b\u5de5\u5177\u53ef\u5b9e\u73b0\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "conclusion": "HAM\u6a21\u578b\u5728\u5efa\u6a21\u4e2d\u63d0\u4f9b\u6700\u4f73\u5e73\u8861\u6027\u80fd\uff0cMPC\u63a7\u5236\u6700\u53ef\u9760\uff0cRL\u5728\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0cLLM\u63a7\u5236\u5668\u5728\u4eba\u673a\u534f\u4f5c\u65b9\u9762\u5177\u6709\u72ec\u7279\u4ef7\u503c\uff0c\u4e3a\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u7684\u6a21\u578b\u548c\u63a7\u5236\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2510.23883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23883", "abs": "https://arxiv.org/abs/2510.23883", "authors": ["Shrestha Datta", "Shahriar Kabir Nahin", "Anshuman Chhabra", "Prasant Mohapatra"], "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges", "comment": null, "summary": "Agentic AI systems powered by large language models (LLMs) and endowed with\nplanning, tool use, memory, and autonomy, are emerging as powerful, flexible\nplatforms for automation. Their ability to autonomously execute tasks across\nweb, software, and physical environments creates new and amplified security\nrisks, distinct from both traditional AI safety and conventional software\nsecurity. This survey outlines a taxonomy of threats specific to agentic AI,\nreviews recent benchmarks and evaluation methodologies, and discusses defense\nstrategies from both technical and governance perspectives. We synthesize\ncurrent research and highlight open challenges, aiming to support the\ndevelopment of secure-by-design agent systems.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u9762\u4e34\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u63d0\u51fa\u4e86\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u76f8\u5e94\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5177\u5907\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u3001\u8bb0\u5fc6\u548c\u81ea\u4e3b\u80fd\u529b\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5728\u81ea\u52a8\u5316\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u5b83\u4eec\u5e26\u6765\u4e86\u4f20\u7edfAI\u5b89\u5168\u548c\u8f6f\u4ef6\u5b89\u5168\u4e4b\u5916\u7684\u65b0\u578b\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u4e13\u95e8\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u667a\u80fd\u4f53AI\u7279\u6709\u7684\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u56de\u987e\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4ece\u6280\u672f\u548c\u6cbb\u7406\u4e24\u4e2a\u89d2\u5ea6\u5206\u6790\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u667a\u80fd\u4f53AI\u5b89\u5168\u9886\u57df\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u8bc6\u522b\u51fa\u5173\u952e\u5a01\u80c1\u7c7b\u578b\u548c\u76f8\u5e94\u7684\u5e94\u5bf9\u63aa\u65bd\u3002", "conclusion": "\u667a\u80fd\u4f53AI\u7cfb\u7edf\u9700\u8981\u5b89\u5168\u4f18\u5148\u7684\u8bbe\u8ba1\u7406\u5ff5\uff0c\u5f53\u524d\u7814\u7a76\u4ecd\u9762\u4e34\u8bf8\u591a\u5f00\u653e\u6311\u6218\uff0c\u9700\u8981\u7ee7\u7eed\u63a2\u7d22\u4ee5\u786e\u4fdd\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2510.23925", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23925", "abs": "https://arxiv.org/abs/2510.23925", "authors": ["Guohao Sun", "Hang Hua", "Jian Wang", "Jiebo Luo", "Sohail Dianat", "Majid Rabbani", "Raghuveer Rao", "Zhiqiang Tao"], "title": "Latent Chain-of-Thought for Visual Reasoning", "comment": "NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u644a\u9500\u53d8\u5206\u63a8\u7406\u7684\u53ef\u6269\u5c55\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u6837\u6027\u5f3a\u5316\u5b66\u4e60\u6539\u8fdbLVLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u6548\u679c\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u7b97\u6cd5\uff08SFT\u3001PPO\u3001GRPO\uff09\u5728\u672a\u89c1\u63a8\u7406\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u6709\u504f\u5956\u52b1\u6a21\u578b\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u63a8\u7406\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5c06LVLMs\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u540e\u9a8c\u63a8\u65ad\uff0c\u4f7f\u7528\u644a\u9500\u53d8\u5206\u63a8\u7406\u548c\u591a\u6837\u6027\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5f15\u5165\u7a00\u758f\u5956\u52b1\u51fd\u6570\u9f13\u52b1\u591a\u6837\u5316\u7684\u6f5c\u5728CoT\uff0c\u5e76\u91c7\u7528\u8d1d\u53f6\u65af\u63a8\u65ad\u7f29\u653e\u7b56\u7565\u3002", "result": "\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u679c\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdbLVLMs\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u53d8\u5206\u63a8\u7406\u7684\u8bad\u7ec3\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347LVLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u514b\u670d\u786e\u5b9a\u6027\u91c7\u6837\u9650\u5236\u548c\u5956\u52b1\u6b3a\u9a97\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2510.23942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23942", "abs": "https://arxiv.org/abs/2510.23942", "authors": ["Sridhar Mahadevan"], "title": "Decentralized Causal Discovery using Judo Calculus", "comment": "54 pages", "summary": "We describe a theory and implementation of an intuitionistic decentralized\nframework for causal discovery using judo calculus, which is formally defined\nas j-stable causal inference using j-do-calculus in a topos of sheaves. In\nreal-world applications -- from biology to medicine and social science --\ncausal effects depend on regime (age, country, dose, genotype, or lab\nprotocol). Our proposed judo calculus formalizes this context dependence\nformally as local truth: a causal claim is proven true on a cover of regimes,\nnot everywhere at once. The Lawvere-Tierney modal operator j chooses which\nregimes are relevant; j-stability means the claim holds constructively and\nconsistently across that family. We describe an algorithmic and implementation\nframework for judo calculus, combining it with standard score-based,\nconstraint-based, and gradient-based causal discovery methods. We describe\nexperimental results on a range of domains, from synthetic to real-world\ndatasets from biology and economics. Our experimental results show the\ncomputational efficiency gained by the decentralized nature of sheaf-theoretic\ncausal discovery, as well as improved performance over classical causal\ndiscovery methods.", "AI": {"tldr": "\u63d0\u51fa\u76f4\u89c9\u4e3b\u4e49\u5206\u6563\u5f0f\u56e0\u679c\u53d1\u73b0\u6846\u67b6judo calculus\uff0c\u4f7f\u7528j-\u7a33\u5b9a\u56e0\u679c\u63a8\u65ad\u548cj-do-calculus\u5728\u5c42\u62d3\u6251\u4e2d\u5f62\u5f0f\u5316\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u56e0\u679c\u6548\u5e94", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u56e0\u679c\u6548\u5e94\u4f9d\u8d56\u4e8e\u73af\u5883\uff08\u5e74\u9f84\u3001\u56fd\u5bb6\u3001\u5242\u91cf\u3001\u57fa\u56e0\u578b\u7b49\uff09\uff0c\u9700\u8981\u5f62\u5f0f\u5316\u8fd9\u79cd\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027", "method": "\u7ed3\u5408\u5c42\u7406\u8bba\u548cLawvere-Tierney\u6a21\u6001\u7b97\u5b50j\uff0c\u5f00\u53d1judo calculus\u7b97\u6cd5\u6846\u67b6\uff0c\u4e0e\u57fa\u4e8e\u5206\u6570\u3001\u7ea6\u675f\u548c\u68af\u5ea6\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u7ed3\u5408", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5c42\u7406\u8bba\u56e0\u679c\u53d1\u73b0\u7684\u5206\u6563\u6027\u8d28\u5e26\u6765\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff0c\u6027\u80fd\u4f18\u4e8e\u7ecf\u5178\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5", "conclusion": "judo calculus\u4e3a\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u7684\u6539\u8fdb"}}
{"id": "2510.23965", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23965", "abs": "https://arxiv.org/abs/2510.23965", "authors": ["Aymane El Gadarri", "Ali Aouad", "Vivek F. Farias"], "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity", "comment": null, "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3asign estimator\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u66ff\u6362\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u5728LLM\u5bf9\u9f50\u4e2d\u63d0\u4f9b\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u4eba\u53e3\u5e73\u5747\u6548\u7528\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u4eba\u7c7b\u504f\u597d\u5f02\u8d28\u6027\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u5bf9\u4eba\u7c7b\u504f\u597d\u7684\u5f02\u8d28\u6027\u5f88\u8106\u5f31\uff0c\u57fa\u4e8e\u6210\u5bf9\u6bd4\u8f83\u6570\u636e\u7684\u6734\u7d20\u6982\u7387\u6a21\u578b\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u4eba\u53e3\u5e73\u5747\u6548\u7528\u4f30\u8ba1\uff0c\u8fd9\u662f\u793e\u4f1a\u798f\u5229\u7684\u89c4\u8303\u8861\u91cf\u6807\u51c6\u3002", "method": "\u63d0\u51fasign estimator\u65b9\u6cd5\uff0c\u5728\u805a\u5408\u6b65\u9aa4\u4e2d\u7528\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u66ff\u6362\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u5b9e\u73b0\u4e00\u81f4\u7684\u5e8f\u6570\u5bf9\u9f50\uff0c\u5e76\u83b7\u5f97\u8be5\u8bbe\u7f6e\u4e2d\u9996\u4e2a\u591a\u9879\u5f0f\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u3002", "result": "\u5728\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684LLM\u5bf9\u9f50\u73b0\u5b9e\u6a21\u62df\u4e2d\uff0csign estimator\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u62df\u4eba\u7269\u9762\u677f\u4e0a\u7684\u504f\u597d\u626d\u66f2\uff0c\u5c06\uff08\u89d2\u5ea6\uff09\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u4e86\u8fd135%\uff0c\u4e0e\u771f\u5b9e\u4eba\u53e3\u504f\u597d\u7684\u4e0d\u4e00\u81f4\u6027\u4ece12%\u964d\u81f38%\u3002", "conclusion": "sign estimator\u65b9\u6cd5\u5728\u4fdd\u6301\u73b0\u6709LLM\u5bf9\u9f50\u7ba1\u9053\u5b9e\u73b0\u7b80\u5355\u6027\u7684\u540c\u65f6\uff0c\u4f18\u4e8e\u660e\u786e\u5efa\u6a21\u7528\u6237\u5f02\u8d28\u6027\u5e76\u9700\u8981\u8ddf\u8e2a\u4e2a\u4f53\u7ea7\u504f\u597d\u6570\u636e\u7684\u9762\u677f\u6570\u636e\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2510.23989", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u4e2a\u4f53\u7684\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027(SIR)\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u6765\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\u3002", "motivation": "\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u524d\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u8861\u91cf\u4e2a\u4f53\u5f02\u8d28\u6027\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\u7684\u65b9\u6cd5\uff0c\u590d\u6742\u4ea4\u4e92\u5173\u7cfb\u672a\u88ab\u5145\u5206\u6355\u6349\uff0c\u4e14\u4e2a\u4f53\u7ea7\u79fb\u52a8\u6570\u636e\u7a00\u758f\u4e0d\u9002\u5408\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5c06\u4e2a\u4f53\u7684\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027(SIR)\u4e0e\u5c40\u90e8\u7a7a\u95f4\u4e0a\u4e0b\u6587\u76f8\u7ed3\u5408\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u7a00\u758f\u4e2a\u4f53\u7ea7\u6570\u636e\u6765\u6355\u6349\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u4e0e\u7a7a\u95f4\u73af\u5883\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u4e2a\u4f53\u7684SIR\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u80fd\u589e\u5f3a\u6a21\u578b\u9884\u6d4b\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u6761\u4ef6\u6a21\u578b\u80fd\u6355\u6349\u5230\u5177\u6709\u76f8\u4f3c\u4e8b\u524d\u79fb\u52a8\u6a21\u5f0f\u4f46SIR\u4e0d\u540c\u7684\u4e2a\u4f53\u5728\u79fb\u52a8\u6a21\u5f0f\u4e0a\u7684\u5dee\u5f02\u6027\u53d8\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u4e2a\u4f53\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\u7eb3\u5165\u9884\u6d4b\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u76f8\u4f3c\u4e8b\u524d\u884c\u4e3a\u4f46\u97e7\u6027\u4e0d\u540c\u7684\u4e2a\u4f53\u3002"}}
{"id": "2510.24013", "categories": ["cs.AI", "cs.LG", "cs.NE", "math.CO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.24013", "abs": "https://arxiv.org/abs/2510.24013", "authors": ["\u0130brahim O\u011fuz \u00c7etinkaya", "\u0130. Esra B\u00fcy\u00fcktahtak\u0131n", "Parshin Shojaee", "Chandan K. Reddy"], "title": "Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling", "comment": null, "summary": "Our study contributes to the scheduling and combinatorial optimization\nliterature with new heuristics discovered by leveraging the power of Large\nLanguage Models (LLMs). We focus on the single-machine total tardiness (SMTT)\nproblem, which aims to minimize total tardiness by sequencing n jobs on a\nsingle processor without preemption, given processing times and due dates. We\ndevelop and benchmark two novel LLM-discovered heuristics, the EDD Challenger\n(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date\n(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that\nemployed simpler rule-based heuristics, we evaluate our LLM-discovered\nalgorithms using rigorous criteria, including optimality gaps and solution time\nderived from a mixed-integer programming (MIP) formulation of SMTT. We compare\ntheir performance against state-of-the-art heuristics and exact methods across\nvarious job sizes (20, 100, 200, and 500 jobs). For instances with more than\n100 jobs, exact methods such as MIP and dynamic programming become\ncomputationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD\nrule and another widely used algorithm in the literature. MDDC consistently\noutperforms traditional heuristics and remains competitive with exact\napproaches, particularly on larger and more complex instances. This study shows\nthat human-LLM collaboration can produce scalable, high-performing heuristics\nfor NP-hard constrained combinatorial optimization, even under limited\nresources when effectively configured.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d1\u73b0\u65b0\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u6765\u89e3\u51b3\u5355\u673a\u603b\u5ef6\u8bef\u95ee\u9898\uff0c\u63d0\u51fa\u4e86EDDC\u548cMDDC\u4e24\u79cd\u7b97\u6cd5\uff0c\u5728100-500\u4e2a\u4f5c\u4e1a\u7684\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u89e3\u51b3\u5355\u673a\u603b\u5ef6\u8bef\u95ee\u9898\u65f6\u6027\u80fd\u6709\u9650\uff0c\u800c\u7cbe\u786e\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u8ba1\u7b97\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u9ad8\u6548\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u4e0eLLM\u534f\u4f5c\u53d1\u73b0\u65b0\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5EDDC\u548cMDDC\uff0c\u57fa\u4e8e\u7ecf\u5178\u7684EDD\u548cMDD\u89c4\u5219\uff0c\u4f7f\u7528\u6df7\u5408\u6574\u6570\u89c4\u5212\u4f5c\u4e3a\u57fa\u51c6\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u5bf9\u4e8e\u8d85\u8fc7100\u4e2a\u4f5c\u4e1a\u7684\u5b9e\u4f8b\uff0cEDDC\u6539\u8fdb\u4e86\u7ecf\u5178EDD\u89c4\u5219\uff0cMDDC\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u66f4\u5927\u66f4\u590d\u6742\u7684\u5b9e\u4f8b\u4e0a\u4e0e\u7cbe\u786e\u65b9\u6cd5\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u4eba\u7c7b\u4e0eLLM\u534f\u4f5c\u53ef\u4ee5\u6709\u6548\u4ea7\u751f\u53ef\u6269\u5c55\u7684\u9ad8\u6027\u80fd\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8eNP\u96be\u7ea6\u675f\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u914d\u7f6e\u3002"}}
{"id": "2510.24028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24028", "abs": "https://arxiv.org/abs/2510.24028", "authors": ["Tingyue Pan", "Mingyue Cheng", "Shilong Zhang", "Zhiding Liu", "Xiaoyu Tao", "Yucong Luo", "Jintao Zhang", "Qi Liu"], "title": "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting", "comment": null, "summary": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines.", "AI": {"tldr": "OneCast\u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u3001\u6a21\u5757\u5316\u7684\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u7ec4\u4ef6\uff0c\u5206\u522b\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6295\u5f71\u6a21\u5757\u548c\u57fa\u4e8e\u79bb\u6563\u6269\u6563\u7684\u673a\u5236\u8fdb\u884c\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u9762\u4e34\u7684\u9886\u57df\u7279\u5b9a\u8d8b\u52bf\u53d8\u5316\u548c\u4e0d\u4e00\u81f4\u5468\u671f\u6027\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5c06\u65f6\u95f4\u5e8f\u5217\u89c6\u4e3a\u672a\u5206\u5316\u7684\u5e8f\u5217\uff0c\u6ca1\u6709\u663e\u5f0f\u89e3\u8026\u5176\u5185\u5728\u7ed3\u6784\u7ec4\u4ef6\u3002", "method": "\u63d0\u51faOneCast\u6846\u67b6\uff1a1\uff09\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u7ec4\u4ef6\uff1b2\uff09\u5b63\u8282\u6027\u7ec4\u4ef6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6295\u5f71\u6a21\u5757\u4f7f\u7528\u53ef\u89e3\u91ca\u57fa\u51fd\u6570\u91cd\u5efa\u5468\u671f\u6a21\u5f0f\uff1b3\uff09\u8d8b\u52bf\u7ec4\u4ef6\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u5206\u8bcd\u5668\u7f16\u7801\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u4f7f\u7528\u63a9\u7801\u79bb\u6563\u6269\u6563\u673a\u5236\u8fdb\u884c\u63a8\u65ad\uff1b4\uff09\u4e24\u4e2a\u5206\u652f\u8f93\u51fa\u7ed3\u5408\u751f\u6210\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728\u516b\u4e2a\u9886\u57df\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cOneCast\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u89e3\u8026\u65f6\u95f4\u5e8f\u5217\u7684\u7ed3\u6784\u7ec4\u4ef6\u5e76\u5206\u522b\u5efa\u6a21\uff0cOneCast\u80fd\u591f\u6709\u6548\u6355\u6349\u5b63\u8282\u6027\u6a21\u5f0f\u540c\u65f6\u8ddf\u8e2a\u9886\u57df\u7279\u5b9a\u8d8b\u52bf\uff0c\u5728\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.24031", "categories": ["cs.AI", "cs.CR", "H.3.3, I.2.7, I.5.3, I.2.5,"], "pdf": "https://arxiv.org/pdf/2510.24031", "abs": "https://arxiv.org/abs/2510.24031", "authors": ["Peng Cai", "Reza Ryan", "Nickson M. Karie"], "title": "LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models", "comment": "33 pages, 10 figures", "summary": "System logs are a cornerstone of cybersecurity, supporting proactive breach\nprevention and post-incident investigations. However, analyzing vast amounts of\ndiverse log data remains significantly challenging, as high costs, lack of\nin-house expertise, and time constraints make even basic analysis difficult for\nmany organizations. This study introduces LLMLogAnalyzer, a clustering-based\nlog analysis chatbot that leverages Large Language Models (LLMs) and Machine\nLearning (ML) algorithms to simplify and streamline log analysis processes.\nThis innovative approach addresses key LLM limitations, including context\nwindow constraints and poor structured text handling capabilities, enabling\nmore effective summarization, pattern extraction, and anomaly detection tasks.\nLLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.\nResults demonstrate significant performance improvements over state-of-the-art\nLLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent\ngains ranging from 39% to 68% across different tasks. The system also exhibits\nstrong robustness, achieving a 93% reduction in interquartile range (IQR) when\nusing ROUGE-1 scores, indicating significantly lower result variability. The\nframework's effectiveness stems from its modular architecture comprising a\nrouter, log recognizer, log parser, and search tools. This design enhances LLM\ncapabilities for structured text analysis while improving accuracy and\nrobustness, making it a valuable resource for both cybersecurity experts and\nnon-technical users.", "AI": {"tldr": "LLMLogAnalyzer\u662f\u4e00\u4e2a\u57fa\u4e8e\u805a\u7c7b\u7684\u65e5\u5fd7\u5206\u6790\u804a\u5929\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7b80\u5316\u65e5\u5fd7\u5206\u6790\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u6bd4\u73b0\u6709LLM\u804a\u5929\u673a\u5668\u4eba\u6027\u80fd\u63d0\u534739%-68%\u3002", "motivation": "\u7cfb\u7edf\u65e5\u5fd7\u662f\u7f51\u7edc\u5b89\u5168\u7684\u6838\u5fc3\uff0c\u4f46\u5206\u6790\u5927\u91cf\u591a\u6837\u5316\u65e5\u5fd7\u6570\u636e\u9762\u4e34\u9ad8\u6210\u672c\u3001\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u548c\u65f6\u95f4\u9650\u5236\u7b49\u6311\u6218\uff0c\u8bb8\u591a\u7ec4\u7ec7\u96be\u4ee5\u8fdb\u884c\u57fa\u672c\u5206\u6790\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5305\u62ec\u8def\u7531\u5668\u3001\u65e5\u5fd7\u8bc6\u522b\u5668\u3001\u65e5\u5fd7\u89e3\u6790\u5668\u548c\u641c\u7d22\u5de5\u5177\uff0c\u7ed3\u5408\u805a\u7c7b\u65b9\u6cd5\u548cLLM\u6280\u672f\uff0c\u89e3\u51b3LLM\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u7ed3\u6784\u5316\u6587\u672c\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\u65e5\u5fd7\u548c\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4ChatGPT\u3001ChatPDF\u548cNotebookLM\u7b49\u6700\u5148\u8fdb\u7684LLM\u804a\u5929\u673a\u5668\u4eba\uff0c\u6027\u80fd\u663e\u8457\u63d0\u534739%-68%\uff0c\u9c81\u68d2\u6027\u589e\u5f3a\uff0c\u4f7f\u7528ROUGE-1\u5206\u6570\u7684\u56db\u5206\u4f4d\u8ddd\u51cf\u5c1193%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u589e\u5f3aLLM\u5728\u7ed3\u6784\u5316\u6587\u672c\u5206\u6790\u4e2d\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u4e13\u5bb6\u548c\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "\u6bd4\u8f83\u7ecf\u5178\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\u8ddf\u8f66\u884c\u4e3a\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u4f18\u4e8e\u7269\u7406\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u666e\u53ca\uff0c\u9700\u8981\u7406\u89e3\u5176\u9a7e\u9a76\u884c\u4e3a\u4ee5\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u548c\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u7ecf\u5178\u6a21\u578b\uff08IDM\u3001OVM\u3001OVRV\u3001CACC\uff09\u548c\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754cEV\u8ddf\u968fICE\u8f66\u8f86\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5efa\u6a21\u548c\u53c2\u6570\u6821\u51c6\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cRMSE\u5206\u522b\u4e3a0.0046\uff08\u4e2d\u7b49\u95f4\u8ddd\uff09\u30010.0016\uff08\u957f\u95f4\u8ddd\uff09\u548c0.0025\uff08\u8d85\u957f\u95f4\u8ddd\uff09\uff1b\u7ecf\u5178\u6a21\u578b\u4e2dCACC\u8868\u73b0\u6700\u597d\uff0c\u957f\u95f4\u8dddRMSE\u4e3a2.67\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6a21\u62dfEV\u884c\u4e3a\u548c\u5206\u6790\u6df7\u5408\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u52a8\u6001\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "HistoLens\u662f\u4e00\u4e2a\u900f\u660e\u7684AI\u75c5\u7406\u5b66\u52a9\u624b\uff0c\u8ba9\u533b\u751f\u80fd\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u95ee\u5e76\u83b7\u5f97\u5e26\u6709\u89c6\u89c9\u8bc1\u660e\u7684\u5206\u6790\u62a5\u544a\uff0c\u4fdd\u6301\u533b\u751f\u4e3b\u5bfc\u5730\u4f4d\u7684\u540c\u65f6\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u548c\u4fe1\u5fc3\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u533b\u751f\u771f\u6b63\u4fe1\u4efbAI\uff0c\u9700\u8981\u89e3\u51b3AI\u9ed1\u76d2\u95ee\u9898\uff0c\u8ba9\u533b\u751f\u80fd\u7406\u89e3AI\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c31\u50cf\u54a8\u8be2\u540c\u4e8b\u4e00\u6837\u3002", "method": "\u5f00\u53d1\u4e86HistoLens\u7cfb\u7edf\uff0c\u5141\u8bb8\u75c5\u7406\u5b66\u5bb6\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u95ee\uff0c\u7cfb\u7edf\u5c06\u95ee\u9898\u8f6c\u6362\u4e3a\u7cbe\u786e\u67e5\u8be2\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u62a5\u544a\u548c\u70ed\u56fe\u89c6\u89c9\u8bc1\u660e\uff0c\u5e76\u8bad\u7ec3AI\u4e13\u6ce8\u4e8e\u60a3\u8005\u7ec4\u7ec7\u800c\u5ffd\u7565\u80cc\u666f\u566a\u58f0\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u75c5\u7406\u5b66\u5bb6\u4fdd\u6301\u4e13\u5bb6\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f7f\u7528\u53ef\u4fe1\u8d56\u7684AI\u52a9\u624b\u9a8c\u8bc1\u89c1\u89e3\uff0c\u5b9e\u73b0\u66f4\u5feb\u3001\u66f4\u81ea\u4fe1\u7684\u8bca\u65ad\u3002", "conclusion": "\u900f\u660e\u7684AI\u7cfb\u7edf\u5982HistoLens\u80fd\u591f\u5efa\u7acb\u533b\u751f\u5bf9AI\u7684\u4fe1\u4efb\uff0c\u901a\u8fc7\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7fAI\u6210\u4e3a\u771f\u6b63\u7684\u534f\u4f5c\u4f19\u4f34\u800c\u975e\u9ed1\u76d2\u5de5\u5177\u3002"}}
{"id": "2510.24145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24145", "abs": "https://arxiv.org/abs/2510.24145", "authors": ["Yu Luo", "Jiamin Jiang", "Jingfei Feng", "Lei Tao", "Qingliang Zhang", "Xidao Wen", "Yongqian Sun", "Shenglin Zhang", "Jielong Huang", "Nan Qi", "Dan Pei"], "title": "From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems", "comment": null, "summary": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems.", "AI": {"tldr": "OpsAgent\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u81ea\u6f14\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e91\u7cfb\u7edf\u4e8b\u4ef6\u7ba1\u7406\uff0c\u901a\u8fc7\u514d\u8bad\u7ec3\u6570\u636e\u5904\u7406\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u900f\u660e\u8bca\u65ad\uff0c\u5e76\u5728OPENRCA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u4e8b\u4ef6\u7ba1\u7406\u52b3\u52a8\u5bc6\u96c6\u4e14\u6613\u9519\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\u3001\u53ef\u89e3\u91ca\u6027\u6709\u9650\u4e14\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u963b\u788d\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u514d\u8bad\u7ec3\u6570\u636e\u5904\u7406\u5c06\u5f02\u6784\u53ef\u89c2\u6d4b\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u5b9e\u73b0\u900f\u660e\u8bca\u65ad\uff0c\u5e76\u5f15\u5165\u53cc\u81ea\u6f14\u5316\u673a\u5236\u652f\u6301\u6301\u7eed\u80fd\u529b\u589e\u957f\u3002", "result": "\u5728OPENRCA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u7cfb\u7edf\u5177\u6709\u6cdb\u5316\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u81ea\u6f14\u5316\u80fd\u529b\u3002", "conclusion": "OpsAgent\u662f\u4e00\u4e2a\u5b9e\u9645\u53ef\u90e8\u7f72\u4e14\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e91\u7cfb\u7edf\u7684\u957f\u671f\u8fd0\u7ef4\u3002"}}
{"id": "2510.24151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24151", "abs": "https://arxiv.org/abs/2510.24151", "authors": ["Bingsen Qiu", "Zijian Liu", "Xiao Liu", "Haoshen Yang", "Zeren Gao", "Bingjie Wang", "Feier Zhang", "Yixuan Qin", "Chunyan Li"], "title": "BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data", "comment": null, "summary": "Building training-ready multi-hop question answering (QA) datasets that truly\nstress a model's retrieval and reasoning abilities remains highly challenging\nrecently. While there have been a few recent evaluation datasets that capture\nthe characteristics of hard-to-search but easy-to-verify problems -- requiring\nthe integration of ambiguous, indirect, and cross-domain cues -- these data\nresources remain scarce and are mostly designed for evaluation, making them\nunsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).\nMeanwhile, manually curating non-trivially retrievable questions -- where\nanswers cannot be found through a single direct query but instead require\nmulti-hop reasoning over oblique and loosely connected evidence -- incurs\nprohibitive human costs and fails to scale, creating a critical data bottleneck\nfor training high-capability retrieval-and-reasoning agents.\n  To address this, we present an automated framework for generating\nhigh-difficulty, training-ready multi-hop questions from semi-structured\nknowledge sources. The system (i) grows diverse, logically labeled evidence\nclusters through Natural Language Inference (NLI)-based relation typing and\ndiversity-aware expansion; (ii) applies reverse question construction to\ncompose oblique cues so that isolated signals are underinformative but their\ncombination uniquely identifies the target entity; and (iii) enforces quality\nwith a two-step evaluation pipeline that combines multi-model consensus\nfiltering with structured constraint decomposition and evidence-based matching.\nThe result is a scalable process that yields complex, retrieval-resistant yet\nverifiable questions suitable for SFT/RL training as well as challenging\nevaluation, substantially reducing human curation effort while preserving the\ndifficulty profile of strong evaluation benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u6e90\u751f\u6210\u9ad8\u96be\u5ea6\u3001\u53ef\u7528\u4e8e\u8bad\u7ec3\u7684\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u4e0d\u9002\u5408\u76d1\u7763\u5fae\u8c03\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u4e3b\u8981\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30\uff0c\u4e0d\u9002\u5408\u76d1\u7763\u5fae\u8c03\u6216\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002\u624b\u52a8\u6784\u5efa\u975e\u5e73\u51e1\u53ef\u68c0\u7d22\u95ee\u9898\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u8fd9\u6210\u4e3a\u8bad\u7ec3\u9ad8\u80fd\u529b\u68c0\u7d22\u63a8\u7406\u4ee3\u7406\u7684\u5173\u952e\u6570\u636e\u74f6\u9888\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a(i)\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u5173\u7cfb\u5206\u7c7b\u548c\u591a\u6837\u6027\u611f\u77e5\u6269\u5c55\u751f\u6210\u591a\u6837\u5316\u903b\u8f91\u6807\u8bb0\u7684\u8bc1\u636e\u7c07\uff1b(ii)\u5e94\u7528\u9006\u5411\u95ee\u9898\u6784\u5efa\u6765\u7ec4\u5408\u95f4\u63a5\u7ebf\u7d22\uff1b(iii)\u901a\u8fc7\u591a\u6a21\u578b\u5171\u8bc6\u8fc7\u6ee4\u548c\u7ed3\u6784\u5316\u7ea6\u675f\u5206\u89e3\u7684\u4e24\u6b65\u8bc4\u4f30\u6d41\u7a0b\u786e\u4fdd\u8d28\u91cf\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u89c4\u6a21\u5316\u751f\u6210\u590d\u6742\u3001\u68c0\u7d22\u62b5\u6297\u4f46\u53ef\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u4fdd\u6301\u5f3a\u8bc4\u4f30\u57fa\u51c6\u7684\u96be\u5ea6\u7279\u5f81\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4e3a\u8bad\u7ec3\u9ad8\u80fd\u529b\u68c0\u7d22\u63a8\u7406\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24161", "categories": ["cs.AI", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "AI": {"tldr": "BLM\u2081\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u8de8\u7a7a\u95f4\u4f20\u8f93\u3001\u8de8\u4efb\u52a1\u5b66\u4e60\u548c\u8de8\u5177\u8eab\u6cdb\u5316\uff0c\u5728\u6570\u5b57\u548c\u7269\u7406\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u5bb6\u65cf\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u548c\u4e0d\u540c\u5177\u8eab\u4e4b\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8de8\u7a7a\u95f4\u3001\u8de8\u5177\u8eab\u6a21\u578b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u7cbe\u9009\u6570\u5b57\u8bed\u6599\u6ce8\u5165\u5177\u8eab\u77e5\u8bc6\uff0c\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u610f\u56fe\u6865\u63a5\u63a5\u53e3\u8bad\u7ec3\u7b56\u7565\u6a21\u5757\uff0c\u63d0\u53d6MLLM\u7684\u9ad8\u7ea7\u8bed\u4e49\u6765\u6307\u5bfc\u63a7\u5236\u3002", "result": "\u5355\u4e2aBLM\u2081\u5b9e\u4f8b\u5728\u6570\u5b57\u4efb\u52a1\u4e2d\u63d0\u5347\u7ea66%\uff0c\u5728\u7269\u7406\u4efb\u52a1\u4e2d\u63d0\u5347\u7ea63%\uff0c\u4f18\u4e8eMLLMs\u3001ELLMs\u3001VLAs\u548cGMLMs\u56db\u4e2a\u6a21\u578b\u5bb6\u65cf\u3002", "conclusion": "BLM\u2081\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u7684\u65e0\u7f1d\u64cd\u4f5c\uff0c\u5e76\u5728\u4e0d\u540c\u5177\u8eab\u548c\u4efb\u52a1\u95f4\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24166", "abs": "https://arxiv.org/abs/2510.24166", "authors": ["Xin Yang", "Yuhang Zhang", "Wei Li", "Xin Lin", "Wenbin Zou", "Chen Xu"], "title": "UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration", "comment": null, "summary": "Motion planning is a critical component of autonomous vehicle decision-making\nsystems, directly determining trajectory safety and driving efficiency. While\ndeep learning approaches have advanced planning capabilities, existing methods\nremain confined to single-dataset training, limiting their robustness in\nplanning.\n  Through systematic analysis, we discover that vehicular trajectory\ndistributions and history-future correlations demonstrate remarkable\nconsistency across different datasets. Based on these findings, we propose\nUniPlanner, the first planning framework designed for multi-dataset integration\nin autonomous vehicle decision-making. UniPlanner achieves unified\ncross-dataset learning through three synergistic innovations.\n  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates\nhistory-future trajectory pairs from multiple datasets, using historical\ntrajectory similarity to retrieve relevant futures and generate cross-dataset\nplanning guidance.\n  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust\nhistory-future correlations from multiple datasets, transforming historical\ntrajectories into universal planning priors. Its gradient-free design ensures\nthe introduction of valuable priors while preventing shortcut learning, making\nthe planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)\nparadigm implements adaptive dropout to selectively suppress planning priors\nduring training for robust learning, while enabling full prior utilization\nduring inference to maximize planning performance.", "AI": {"tldr": "UniPlanner\u662f\u9996\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u591a\u6570\u636e\u96c6\u96c6\u6210\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u521b\u65b0\u7ec4\u4ef6\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u7edf\u4e00\u5b66\u4e60\uff0c\u63d0\u5347\u89c4\u5212\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u89c4\u5212\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6570\u636e\u96c6\u4e2d\u7684\u8f66\u8f86\u8f68\u8ff9\u5206\u5e03\u548c\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\u5177\u6709\u663e\u8457\u4e00\u81f4\u6027\uff0c\u8fd9\u4e3a\u591a\u6570\u636e\u96c6\u96c6\u6210\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "method": "1. HFTDN\uff1a\u901a\u8fc7\u5386\u53f2\u8f68\u8ff9\u76f8\u4f3c\u6027\u68c0\u7d22\u76f8\u5173\u672a\u6765\u8f68\u8ff9\uff0c\u751f\u6210\u8de8\u6570\u636e\u96c6\u89c4\u5212\u6307\u5bfc\n2. GFTM\uff1a\u65e0\u68af\u5ea6\u8f68\u8ff9\u6620\u5c04\u5668\uff0c\u5b66\u4e60\u9c81\u68d2\u7684\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\uff0c\u8f6c\u5316\u4e3a\u901a\u7528\u89c4\u5212\u5148\u9a8c\n3. S2D\u8303\u5f0f\uff1a\u8bad\u7ec3\u65f6\u9009\u62e9\u6027\u6291\u5236\u89c4\u5212\u5148\u9a8c\u5b9e\u73b0\u9c81\u68d2\u5b66\u4e60\uff0c\u63a8\u7406\u65f6\u5145\u5206\u5229\u7528\u5148\u9a8c\u6700\u5927\u5316\u6027\u80fd", "result": "UniPlanner\u5b9e\u73b0\u4e86\u8de8\u6570\u636e\u96c6\u7edf\u4e00\u5b66\u4e60\uff0c\u80fd\u591f\u751f\u6210\u66f4\u9c81\u68d2\u7684\u89c4\u5212\u8f68\u8ff9\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u89c4\u5212\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u6570\u636e\u96c6\u96c6\u6210\u5728\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u53ef\u884c\u6027\uff0cUniPlanner\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u7ec4\u4ef6\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c4\u5212\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24168", "abs": "https://arxiv.org/abs/2510.24168", "authors": ["Weihua Cheng", "Ersheng Ni", "Wenlong Wang", "Yifei Sun", "Junming Liu", "Wangyu Shen", "Yirong Chen", "Botian Shi", "Ding Wang"], "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction", "comment": "Submitted to WWW2025", "summary": "The rapid progress of Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) has enabled agentic systems capable of perceiving and acting\nacross diverse environments. A challenging yet impactful frontier is the\ndevelopment of GUI agents, which must navigate complex desktop and web\ninterfaces while maintaining robustness and generalization. Existing paradigms\ntypically model tasks as long-chain executions, concatenating historical\ntrajectories into the context. While approaches such as Mirage and GTA1 refine\nplanning or introduce multi-branch action selection, they remain constrained by\ntwo persistent issues: Dependence on historical trajectories, which amplifies\nerror propagation. And Local exploration bias, where \"decision-first,\nobservation-later\" mechanisms overlook critical interface cues. We introduce\nthe Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the\nprinciple of observe first, then decide. MGA models each step as an\nindependent, context-rich environment state represented by a triad: current\nscreenshot, task-agnostic spatial information, and a dynamically updated\nstructured memory. Experiments on OSworld benchmarks, real desktop applications\n(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves\nsubstantial gains in robustness, generalization, and efficiency compared to\nstate-of-the-art baselines. The code is publicly available at:\n{https://anonymous.4open.science/r/MGA-3571}.", "AI": {"tldr": "\u63d0\u51fa\u4e86Memory-Driven GUI Agent (MGA)\uff0c\u901a\u8fc7\"\u5148\u89c2\u5bdf\u540e\u51b3\u7b56\"\u539f\u5219\u89e3\u51b3GUI\u4ee3\u7406\u4e2d\u7684\u5386\u53f2\u8f68\u8ff9\u4f9d\u8d56\u548c\u5c40\u90e8\u63a2\u7d22\u504f\u5dee\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5bf9\u5386\u53f2\u8f68\u8ff9\u7684\u4f9d\u8d56\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u653e\u5927\uff0c\u4ee5\u53ca\"\u51b3\u7b56\u4f18\u5148\u3001\u89c2\u5bdf\u6ede\u540e\"\u673a\u5236\u5ffd\u7565\u4e86\u5173\u952e\u754c\u9762\u7ebf\u7d22\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u6cdb\u5316\u7684GUI\u4ea4\u4e92\u65b9\u6cd5\u3002", "method": "MGA\u5c06GUI\u4ea4\u4e92\u91cd\u6784\u4e3a\"\u5148\u89c2\u5bdf\u540e\u51b3\u7b56\"\u539f\u5219\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u5efa\u6a21\u4e3a\u72ec\u7acb\u7684\u73af\u5883\u72b6\u6001\u4e09\u5143\u7ec4\uff1a\u5f53\u524d\u622a\u56fe\u3001\u4efb\u52a1\u65e0\u5173\u7684\u7a7a\u95f4\u4fe1\u606f\u3001\u52a8\u6001\u66f4\u65b0\u7684\u7ed3\u6784\u5316\u8bb0\u5fc6\u3002", "result": "\u5728OSworld\u57fa\u51c6\u6d4b\u8bd5\u3001\u771f\u5b9e\u684c\u9762\u5e94\u7528\uff08Chrome\u3001VSCode\u3001VLC\uff09\u548c\u8de8\u4efb\u52a1\u8fc1\u79fb\u5b9e\u9a8c\u4e2d\uff0cMGA\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MGA\u901a\u8fc7\u89c2\u5bdf\u4f18\u5148\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86GUI\u4ee3\u7406\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u684c\u9762\u548c\u7f51\u9875\u754c\u9762\u81ea\u52a8\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.24284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24284", "abs": "https://arxiv.org/abs/2510.24284", "authors": ["Wenhao Wang", "Peizhi Niu", "Zhao Xu", "Zhaoyu Chen", "Jian Du", "Yaxin Du", "Xianghe Pang", "Keduan Huang", "Yanfeng Wang", "Qiang Yan", "Siheng Chen"], "title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on external tools to perform\ncomplex, realistic tasks, yet their ability to utilize the rapidly expanding\nModel Contextual Protocol (MCP) ecosystem remains limited. Existing MCP\nresearch covers few servers, depends on costly manual curation, and lacks\ntraining support, hindering progress toward real-world deployment. To overcome\nthese limitations, we introduce MCP-Flow, an automated web-agent-driven\npipeline for large-scale server discovery, data synthesis, and model training.\nMCP-Flow collects and filters data from 1166 servers and 11536 tools, producing\n68733 high-quality instruction-function call pairs and 6439 trajectories, far\nexceeding prior work in scale and diversity. Extensive experiments demonstrate\nMCP-Flow's effectiveness in driving superior MCP tool selection, function-call\ngeneration, and enhanced agentic task performance. MCP-Flow thus provides a\nscalable foundation for advancing LLM agents' proficiency in real-world MCP\nenvironments. MCP-Flow is publicly available at\n\\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "AI": {"tldr": "MCP-Flow\u662f\u4e00\u4e2a\u81ea\u52a8\u5316web-agent\u9a71\u52a8\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u670d\u52a1\u5668\u53d1\u73b0\u3001\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728MCP\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MCP\u7814\u7a76\u8986\u76d6\u670d\u52a1\u5668\u5c11\uff0c\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6574\u7406\uff0c\u7f3a\u4e4f\u8bad\u7ec3\u652f\u6301\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u6765\u63a8\u8fdbLLM\u5728\u771f\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u5316web-agent\u9a71\u52a8\u7ba1\u9053\uff0c\u4ece1166\u4e2a\u670d\u52a1\u5668\u548c11536\u4e2a\u5de5\u5177\u4e2d\u6536\u96c6\u548c\u8fc7\u6ee4\u6570\u636e\uff0c\u751f\u621068733\u4e2a\u9ad8\u8d28\u91cf\u6307\u4ee4-\u51fd\u6570\u8c03\u7528\u5bf9\u548c6439\u4e2a\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMCP-Flow\u5728MCP\u5de5\u5177\u9009\u62e9\u3001\u51fd\u6570\u8c03\u7528\u751f\u6210\u548c\u4ee3\u7406\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8fdc\u8d85\u5148\u524d\u5de5\u4f5c\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002", "conclusion": "MCP-Flow\u4e3a\u63a8\u8fdbLLM\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u719f\u7ec3\u5ea6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2510.24297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24297", "abs": "https://arxiv.org/abs/2510.24297", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms", "comment": null, "summary": "One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which\ncan be addressed by building and using state and/or action abstractions in\nparallel to the tree search such that information can be shared among nodes of\nthe same layer. The primary usage of abstractions for MCTS is to enhance the\nUpper Confidence Bound (UCB) value during the tree policy by aggregating visits\nand returns of an abstract node. However, this direct usage of abstractions\ndoes not take the case into account where multiple actions with the same parent\nmight be in the same abstract node, as these would then all have the same UCB\nvalue, thus requiring a tiebreak rule. In state-of-the-art abstraction\nalgorithms such as pruned On the Go Abstractions (pruned OGA), this case has\nnot been noticed, and a random tiebreak rule was implicitly chosen. In this\npaper, we propose and empirically evaluate several alternative\nintra-abstraction policies, several of which outperform the random policy\nacross a majority of environments and parameter settings.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9MCTS\u4e2d\u62bd\u8c61\u7b97\u6cd5\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u5185\u90e8\u62bd\u8c61\u7b56\u7565\u6765\u66ff\u4ee3\u968f\u673a\u5e73\u5c40\u51b3\u80dc\u89c4\u5219\uff0c\u5e76\u5728\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "motivation": "MCTS\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u72b6\u6001/\u52a8\u4f5c\u62bd\u8c61\u6765\u89e3\u51b3\uff0c\u4f46\u73b0\u6709\u62bd\u8c61\u7b97\u6cd5\uff08\u5982pruned OGA\uff09\u5728\u5904\u7406\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u5185\u591a\u4e2a\u52a8\u4f5c\u65f6\u4f7f\u7528\u968f\u673a\u5e73\u5c40\u51b3\u80dc\u89c4\u5219\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u591a\u79cd\u66ff\u4ee3\u7684\u5185\u90e8\u62bd\u8c61\u7b56\u7565\uff0c\u7528\u4e8e\u5904\u7406\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u5185\u591a\u4e2a\u52a8\u4f5c\u7684UCB\u503c\u76f8\u540c\u60c5\u51b5\u3002", "result": "\u591a\u4e2a\u63d0\u51fa\u7684\u5185\u90e8\u62bd\u8c61\u7b56\u7565\u5728\u5927\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "conclusion": "\u5185\u90e8\u62bd\u8c61\u7b56\u7565\u7684\u9009\u62e9\u5bf9MCTS\u62bd\u8c61\u7b97\u6cd5\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u9488\u5bf9\u5177\u4f53\u73af\u5883\u9009\u62e9\u5408\u9002\u7684\u7b56\u7565\u3002"}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5185\u90e8\u884c\u4e3a\u7684\u81ea\u6211\u6307\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u8f93\u5165\u95ee\u9898\u4e0e\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u5173\u77e9\u9635\u79e9\u6765\u5224\u65ad\u63a8\u7406\u6b63\u786e\u6027\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u5373\u53ef\u6709\u6548\u9a8c\u8bc1LLM\u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\uff08\u5982\u8bad\u7ec3\u9a8c\u8bc1\u5668\u6216\u590d\u6742\u63d0\u793a\uff09\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u901a\u7528\u7684LLM\u8f93\u51fa\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u5229\u7528LLM\u5185\u90e8\u884c\u4e3a\uff0c\u8ba1\u7b97\u8f93\u5165\u95ee\u9898\u4e0e\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u5173\u77e9\u9635\u79e9\u4f5c\u4e3a\u63a8\u7406\u6b63\u786e\u6027\u6307\u6807\uff0c\u8bbe\u8ba1\u7b80\u5355\u7684\u5373\u63d2\u5373\u7528Self-Indicator\u65b9\u6cd5\u5bf9\u5019\u9009\u63a8\u7406\u8def\u5f84\u8fdb\u884c\u91cd\u52a0\u6743\u3002", "result": "\u5728\u591a\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u5bb6\u65cf\u7684LLM\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u8d85\u8fc775%\u7684\u51c6\u786e\u7387\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u63a8\u7406\u8def\u5f84\uff0c\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc78%\u3002", "conclusion": "LLM\u7684\u5185\u90e8\u884c\u4e3a\u5df2\u9690\u542b\u5176\u63a8\u7406\u8def\u5f84\u7684\u53ef\u4fe1\u5ea6\u4fe1\u606f\uff0cSelf-Indicator\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u7684LLM\u8f93\u51fa\u9a8c\u8bc1\u65b9\u6848\u3002"}}
{"id": "2510.24303", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24303", "abs": "https://arxiv.org/abs/2510.24303", "authors": ["Deniz Gorur", "Antoni Rago", "Francesca Toni"], "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting", "comment": null, "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5224\u65ad\u9884\u6d4b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u540c\u667a\u80fd\u4f53\u5bf9\u4e3b\u5f20\u771f\u5b9e\u6027\u4ea7\u751f\u5206\u6b67\u5e76\u6536\u96c6\u6b63\u53cd\u8bc1\u636e\uff0c\u4f7f\u7528\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\u8868\u793a\uff0c\u7ed3\u5408\u591a\u79cd\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u6765\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5224\u65ad\u9884\u6d4b\u4f5c\u4e3a\u57fa\u4e8e\u4eba\u7c7b\u5224\u65ad\u7684\u672a\u6765\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\uff0c\u53ef\u89c6\u4e3a\u4e3b\u5f20\u9a8c\u8bc1\u7684\u4e00\u79cd\u5f62\u5f0f\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bc1\u636e\u6536\u96c6\u548c\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u89c6\u89d2\u8bc1\u636e\u6574\u5408\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u4e3b\u5f20\u9a8c\u8bc1\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u79cd\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\uff1aArgLLM\uff08\u751f\u6210\u548c\u8bc4\u4f30QBAF\uff09\u3001RbAM\uff08\u57fa\u4e8e\u5173\u7cfb\u8bba\u8bc1\u6316\u6398\u751f\u6210QBAF\uff09\u3001RAG-ArgLLM\uff08\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684ArgLLM\u6269\u5c55\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6\u5224\u65ad\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u591a\u4e2a\u667a\u80fd\u4f53\u7684\u8bc1\u636e\u53ef\u4ee5\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u4e3a\u4e3b\u5f20\u9a8c\u8bc1\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u5224\u65ad\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u901a\u8fc7\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u8bc1\u636e\u7ec4\u5408\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.24337", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24337", "abs": "https://arxiv.org/abs/2510.24337", "authors": ["Daria Kravets-Meinke", "Hannah Schmid-Petri", "Sonja Niemann", "Ute Schmid"], "title": "Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research", "comment": null, "summary": "Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly\nbeing used in communication research for content analysis. Studies show that\ngLLMs can outperform both crowd workers and trained coders, such as research\nassistants, on various coding tasks relevant to communication science, often at\na fraction of the time and cost. Additionally, gLLMs can decode implicit\nmeanings and contextual information, be instructed using natural language,\ndeployed with only basic programming skills, and require little to no annotated\ndata beyond a validation dataset - constituting a paradigm shift in automated\ncontent analysis. Despite their potential, the integration of gLLMs into the\nmethodological toolkit of communication research remains underdeveloped. In\ngLLM-assisted quantitative content analysis, researchers must address at least\nseven critical challenges that impact result quality: (1) codebook development,\n(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)\niterative refinement, (6) validation of the model's reliability, and\noptionally, (7) performance enhancement. This paper synthesizes emerging\nresearch on gLLM-assisted quantitative content analysis and proposes a\ncomprehensive best-practice guide to navigate these challenges. Our goal is to\nmake gLLM-based content analysis more accessible to a broader range of\ncommunication researchers and ensure adherence to established disciplinary\nquality standards of validity, reliability, reproducibility, and research\nethics.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f20\u64ad\u5b66\u7814\u7a76\u5185\u5bb9\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u6307\u51fa\u5176\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u9762\u4e34\u4e03\u5927\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f20\u64ad\u5b66\u5185\u5bb9\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u8d85\u8d8a\u4f17\u5305\u5de5\u4f5c\u8005\u548c\u8bad\u7ec3\u6709\u7d20\u7684\u7f16\u7801\u5458\uff0c\u4f46\u5176\u5728\u7814\u7a76\u65b9\u6cd5\u8bba\u4e2d\u7684\u6574\u5408\u4ecd\u4e0d\u5145\u5206\uff0c\u9700\u8981\u7cfb\u7edf\u6307\u5bfc\u6765\u786e\u4fdd\u7814\u7a76\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u65b0\u5174\u7814\u7a76\uff0c\u63d0\u51fa\u5e94\u5bf9\u4e03\u5927\u5173\u952e\u6311\u6218\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff1a\u4ee3\u7801\u672c\u5f00\u53d1\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u3001\u53c2\u6570\u8c03\u4f18\u3001\u8fed\u4ee3\u4f18\u5316\u3001\u53ef\u9760\u6027\u9a8c\u8bc1\u548c\u6027\u80fd\u63d0\u5347\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ee5\u66f4\u4f4e\u6210\u672c\u548c\u65f6\u95f4\u5b8c\u6210\u5185\u5bb9\u5206\u6790\u4efb\u52a1\uff0c\u5e76\u80fd\u89e3\u7801\u9690\u542b\u610f\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4f46\u9700\u8981\u7cfb\u7edf\u7684\u65b9\u6cd5\u8bba\u6307\u5bfc\u6765\u786e\u4fdd\u7814\u7a76\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u4e3a\u4f20\u64ad\u5b66\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4f7f\u7528\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5185\u5bb9\u5206\u6790\u7684\u5168\u9762\u6307\u5357\uff0c\u65e8\u5728\u4f7f\u8be5\u65b9\u6cd5\u66f4\u6613\u7528\uff0c\u5e76\u786e\u4fdd\u7b26\u5408\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u7814\u7a76\u4f26\u7406\u7b49\u5b66\u79d1\u8d28\u91cf\u6807\u51c6\u3002"}}
{"id": "2510.24339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24339", "abs": "https://arxiv.org/abs/2510.24339", "authors": ["Yunxuan Jiang", "Silan Hu", "Xiaoning Wang", "Yuanyuan Zhang", "Xiangyu Chang"], "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation", "comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents", "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.", "AI": {"tldr": "VDSAgents\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u9884\u6d4b\u6027-\u53ef\u8ba1\u7b97\u6027-\u7a33\u5b9a\u6027(PCS)\u539f\u5219\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7aef\u5230\u7aef\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u4ec5\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u63a8\u7406\uff0c\u7f3a\u4e4f\u79d1\u5b66\u548c\u7406\u8bba\u539f\u5219\u6307\u5bfc\uff0c\u5728\u5904\u7406\u566a\u58f0\u548c\u590d\u6742\u771f\u5b9e\u6570\u636e\u96c6\u65f6\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8ePCS\u539f\u5219\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u91c7\u7528\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u7a0b\u5904\u7406\u6570\u636e\u6e05\u6d17\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u9636\u6bb5\u7531\u4e13\u95e8\u667a\u80fd\u4f53\u8d1f\u8d23\uff0c\u7ed3\u5408\u6270\u52a8\u5206\u6790\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "\u57289\u4e2a\u4e0d\u540c\u7279\u5f81\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528DeepSeek-V3\u548cGPT-4o\u4f5c\u4e3a\u540e\u7aef\uff0cVDSAgents\u6301\u7eed\u4f18\u4e8eAutoKaggle\u548cDataInterpreter\u7b49\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u5c06PCS\u539f\u5219\u5d4c\u5165LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u4fe1\u548c\u9c81\u68d2\u7684\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.24342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24342", "abs": "https://arxiv.org/abs/2510.24342", "authors": ["Silin Chen", "Yuzhong Chen", "Zifan Wang", "Junhao Wang", "Zifeng Jia", "Keith M Kendrick", "Tuo Zhang", "Lin Zhao", "Dezhong Yao", "Tianming Liu", "Xi Jiang"], "title": "A Unified Geometric Space Bridging AI Models and the Human Brain", "comment": null, "summary": "For decades, neuroscientists and computer scientists have pursued a shared\nambition: to understand intelligence and build it. Modern artificial neural\nnetworks now rival humans in language, perception, and reasoning, yet it is\nstill largely unknown whether these artificial systems organize information as\nthe brain does. Existing brain-AI alignment studies have shown the striking\ncorrespondence between the two systems, but such comparisons remain bound to\nspecific inputs and tasks, offering no common ground for comparing how AI\nmodels with different kinds of modalities-vision, language, or multimodal-are\nintrinsically organized. Here we introduce a groundbreaking concept of\nBrain-like Space: a unified geometric space in which every AI model can be\nprecisely situated and compared by mapping its intrinsic spatial attention\ntopological organization onto canonical human functional brain networks,\nregardless of input modality, task, or sensory domain. Our extensive analysis\nof 151 Transformer-based models spanning state-of-the-art large vision models,\nlarge language models, and large multimodal models uncovers a continuous\narc-shaped geometry within this space, reflecting a gradual increase of\nbrain-likeness; different models exhibit distinct distribution patterns within\nthis geometry associated with different degrees of brain-likeness, shaped not\nmerely by their modality but by whether the pretraining paradigm emphasizes\nglobal semantic abstraction and whether the positional encoding scheme\nfacilitates deep fusion across different modalities. Moreover, the degree of\nbrain-likeness for a model and its downstream task performance are not\n\"identical twins\". The Brain-like Space provides the first unified framework\nfor situating, quantifying, and comparing intelligence across domains,\nrevealing the deep organizational principles that bridge machines and the\nbrain.", "AI": {"tldr": "\u63d0\u51fa\u4e86Brain-like Space\u6982\u5ff5\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u51e0\u4f55\u7a7a\u95f4\uff0c\u53ef\u5c06\u4e0d\u540c\u6a21\u6001\u7684AI\u6a21\u578b\u6620\u5c04\u5230\u4eba\u7c7b\u529f\u80fd\u8111\u7f51\u7edc\u8fdb\u884c\u6bd4\u8f83\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u8111\u76f8\u4f3c\u5ea6\u7684\u8fde\u7eed\u51e0\u4f55\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u8111-AI\u5bf9\u9f50\u7814\u7a76\u5c40\u9650\u4e8e\u7279\u5b9a\u8f93\u5165\u548c\u4efb\u52a1\uff0c\u7f3a\u4e4f\u6bd4\u8f83\u4e0d\u540c\u6a21\u6001AI\u6a21\u578b\u5185\u5728\u7ec4\u7ec7\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5c06AI\u6a21\u578b\u7684\u5185\u5728\u7a7a\u95f4\u6ce8\u610f\u529b\u62d3\u6251\u7ec4\u7ec7\u6620\u5c04\u5230\u6807\u51c6\u4eba\u7c7b\u529f\u80fd\u8111\u7f51\u7edc\uff0c\u6784\u5efaBrain-like Space\uff0c\u5206\u6790151\u4e2aTransformer\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728Brain-like Space\u4e2d\u5448\u73b0\u8fde\u7eed\u7684\u5f27\u72b6\u51e0\u4f55\u5206\u5e03\uff0c\u8111\u76f8\u4f3c\u5ea6\u4e0e\u9884\u8bad\u7ec3\u8303\u5f0f\u548c\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u76f8\u5173\uff0c\u4e14\u8111\u76f8\u4f3c\u5ea6\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0d\u5b8c\u5168\u4e00\u81f4\u3002", "conclusion": "Brain-like Space\u4e3a\u8de8\u9886\u57df\u667a\u80fd\u63d0\u4f9b\u4e86\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u8fde\u63a5\u673a\u5668\u4e0e\u5927\u8111\u7684\u6df1\u5c42\u7ec4\u7ec7\u539f\u5219\u3002"}}
{"id": "2510.24383", "categories": ["cs.AI", "cs.CY", "cs.MA", "I.2.11; I.2.1; I.2.4; K.4.1; K.4.3"], "pdf": "https://arxiv.org/pdf/2510.24383", "abs": "https://arxiv.org/abs/2510.24383", "authors": ["Juraj Mavra\u010di\u0107"], "title": "Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents", "comment": "First published on 19/10/2025. Canonical archived record and DOI:\n  10.5281/zenodo.17391796", "summary": "Policy Cards are introduced as a machine-readable, deployment-layer standard\nfor expressing operational, regulatory, and ethical constraints for AI agents.\nThe Policy Card sits with the agent and enables it to follow required\nconstraints at runtime. It tells the agent what it must and must not do. As\nsuch, it becomes an integral part of the deployed agent. Policy Cards extend\nexisting transparency artifacts such as Model, Data, and System Cards by\ndefining a normative layer that encodes allow/deny rules, obligations,\nevidentiary requirements, and crosswalk mappings to assurance frameworks\nincluding NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can\nbe validated automatically, version-controlled, and linked to runtime\nenforcement or continuous-audit pipelines. The framework enables verifiable\ncompliance for autonomous agents, forming a foundation for distributed\nassurance in multi-agent ecosystems. Policy Cards provide a practical mechanism\nfor integrating high-level governance with hands-on engineering practice and\nenabling accountable autonomy at scale.", "AI": {"tldr": "Policy Cards\u662f\u4e00\u79cd\u673a\u5668\u53ef\u8bfb\u7684\u90e8\u7f72\u5c42\u6807\u51c6\uff0c\u7528\u4e8e\u8868\u8fbeAI\u4ee3\u7406\u7684\u64cd\u4f5c\u3001\u76d1\u7ba1\u548c\u4f26\u7406\u7ea6\u675f\uff0c\u4f5c\u4e3a\u4ee3\u7406\u7684\u7ec4\u6210\u90e8\u5206\u5728\u8fd0\u884c\u65f6\u6267\u884c\u7ea6\u675f\u8981\u6c42\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3AI\u4ee3\u7406\u5728\u90e8\u7f72\u548c\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u9700\u8981\u9075\u5faa\u7684\u64cd\u4f5c\u3001\u76d1\u7ba1\u548c\u4f26\u7406\u7ea6\u675f\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u5c06\u9ad8\u5c42\u6cbb\u7406\u4e0e\u5de5\u7a0b\u5b9e\u8df5\u7ed3\u5408\u7684\u5b9e\u9645\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u5305\u542b\u5141\u8bb8/\u62d2\u7edd\u89c4\u5219\u3001\u4e49\u52a1\u3001\u8bc1\u636e\u8981\u6c42\u548c\u4e0eNIST AI RMF\u3001ISO/IEC 42001\u3001\u6b27\u76dfAI\u6cd5\u6848\u7b49\u4fdd\u8bc1\u6846\u67b6\u6620\u5c04\u7684\u89c4\u8303\u5c42\uff0c\u6269\u5c55\u73b0\u6709\u7684\u900f\u660e\u5ea6\u5de5\u4ef6\u3002", "result": "\u6bcf\u4e2aPolicy Card\u53ef\u4ee5\u81ea\u52a8\u9a8c\u8bc1\u3001\u7248\u672c\u63a7\u5236\uff0c\u5e76\u94fe\u63a5\u5230\u8fd0\u884c\u65f6\u6267\u884c\u6216\u6301\u7eed\u5ba1\u8ba1\u7ba1\u9053\uff0c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u5408\u89c4\u6027\u3002", "conclusion": "Policy Cards\u4e3a\u591a\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5206\u5e03\u5f0f\u4fdd\u8bc1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u53ef\u95ee\u8d23\u7684\u81ea\u4e3b\u6027\u3002"}}
{"id": "2510.24390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24390", "abs": "https://arxiv.org/abs/2510.24390", "authors": ["Xianjun Gao", "Jianchun Liu", "Hongli Xu", "Liusheng Huang"], "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion", "comment": null, "summary": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.", "AI": {"tldr": "Orion\u662f\u4e00\u4e2a\u9ad8\u6548\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4f9d\u8d56\u611f\u77e5\u7684\u67e5\u8be2\u5206\u89e3\u548c\u903b\u8f91\u5e76\u884c\u5185\u5bb9\u6269\u5c55\uff0c\u89e3\u51b3\u4e86LLM\u5728\u5b9e\u65f6Web\u5e94\u7528\u4e2d\u63a8\u7406\u6548\u7387\u4e0e\u8d28\u91cf\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u5b9e\u65f6Web\u5e94\u7528\u4e2d\u9762\u4e34\u5173\u952e\u6311\u6218\uff1a\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u9ad8\u8d28\u91cf\u590d\u6742\u63a8\u7406\u548c\u4f4e\u5ef6\u8fdf\u9ad8\u541e\u5410\u91cf\u7684\u4ea4\u4e92\u670d\u52a1\u8981\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "method": "Orion\u5c06\u67e5\u8be2\u63a8\u7406\u5206\u89e3\u4e3a\u4e24\u4e2a\u534f\u540c\u9636\u6bb5\uff1a1) \u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u5c11\u6837\u672c\u63d0\u793a\u751f\u6210\u903b\u8f91\u7ed3\u6784\u5316\u7684\u5173\u952e\u70b9\uff1b2) \u57fa\u4e8e\u4f9d\u8d56\u56fe\u5e76\u884c\u6269\u5c55\u5185\u5bb9\u4ee5\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u3002\u8fd8\u5f15\u5165\u4e86\u7ba1\u9053\u8c03\u5ea6\u673a\u5236\u5b9e\u73b0\u8de8\u67e5\u8be2\u5e76\u884c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cOrion\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e864.33\u500d\u7684token\u751f\u6210\u901f\u5ea6\u63d0\u5347\u548c3.42\u500d\u7684\u7b54\u6848\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u70b9\u95f4\u4f9d\u8d56\u5173\u7cfb\u5c06\u63a8\u7406\u8d28\u91cf\u63d0\u5347\u4e8618.75%\u3002", "conclusion": "Orion\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86LLM\u63a8\u7406\u5728Web\u670d\u52a1\u4e2d\u7684\u6548\u7387\u4e0e\u8d28\u91cf\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5206\u89e3\u548c\u5e76\u884c\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.24397", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24397", "abs": "https://arxiv.org/abs/2510.24397", "authors": ["Jiarui Qin", "Yunjia Xi", "Junjie Huang", "Renting Rui", "Di Yin", "Weiwen Liu", "Yong Yu", "Weinan Zhang", "Xing Sun"], "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training", "comment": "46 pages", "summary": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training.", "AI": {"tldr": "\u63d0\u51fa\u4e86APTBench\u6846\u67b6\uff0c\u5c06\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u4efb\u52a1\u8f6c\u6362\u4e3a\u9002\u5408\u57fa\u7840\u6a21\u578b\u7684\u591a\u9009\u6216\u6587\u672c\u8865\u5168\u95ee\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u667a\u80fd\u4f53\u80fd\u529b", "motivation": "\u5f53\u524d\u9884\u8bad\u7ec3\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u9759\u6001\u6280\u80fd\uff0c\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff1b\u800c\u667a\u80fd\u4f53\u57fa\u51c6\u901a\u5e38\u9488\u5bf9\u540e\u8bad\u7ec3\u6a21\u578b\uff0c\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u652f\u6301\u591a\u8f6e\u4efb\u52a1\u6267\u884c\uff0c\u9700\u8981\u80fd\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u8bc4\u4f30\u667a\u80fd\u4f53\u6f5c\u529b\u7684\u57fa\u51c6", "method": "\u5c06\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u4efb\u52a1\u548c\u6210\u529f\u8f68\u8ff9\u8f6c\u6362\u4e3a\u591a\u9009\u6216\u6587\u672c\u8865\u5168\u95ee\u9898\uff0c\u805a\u7126\u89c4\u5212\u548c\u884c\u52a8\u7b49\u6838\u5fc3\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u8986\u76d6\u8f6f\u4ef6\u5de5\u7a0b\u548c\u6df1\u5ea6\u7814\u7a76\u7b49\u5173\u952e\u573a\u666f", "result": "\u76f8\u6bd4\u73b0\u6709\u901a\u7528\u57fa\u51c6\uff0cAPTBench\u80fd\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\u7684\u4e0b\u6e38\u6027\u80fd\uff0c\u540c\u65f6\u6bd4\u540e\u8bad\u7ec3\u7684\u5168\u89c4\u6a21\u7aef\u5230\u7aef\u8bc4\u4f30\u66f4\u8f7b\u91cf\u4e14\u6210\u672c\u6548\u76ca\u66f4\u9ad8", "conclusion": "APTBench\u586b\u8865\u4e86\u9884\u8bad\u7ec3\u9636\u6bb5\u667a\u80fd\u4f53\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u66f4\u6709\u6548\u5730\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5de5\u5177"}}
{"id": "2510.24411", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24411", "abs": "https://arxiv.org/abs/2510.24411", "authors": ["Qiushi Sun", "Mukai Li", "Zhoumianze Liu", "Zhihui Xie", "Fangzhi Xu", "Zhangyue Yin", "Kanzhi Cheng", "Zehao Li", "Zichen Ding", "Qi Liu", "Zhiyong Wu", "Zhuosheng Zhang", "Ben Kao", "Lingpeng Kong"], "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows", "comment": "work in progress", "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86MobileRisk-Live\u52a8\u6001\u6c99\u76d2\u73af\u5883\u548cOS-Sentinel\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u79fb\u52a8AI\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534710%-30%\u3002", "motivation": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u673a\u4ee3\u7406\u5728\u79fb\u52a8\u5e73\u53f0\u7b49\u6570\u5b57\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u7c7b\u4eba\u80fd\u529b\uff0c\u4f46\u5176\u6f5c\u5728\u7684\u5b89\u5168\u64cd\u4f5c\u98ce\u9669\uff08\u5982\u7cfb\u7edf\u7834\u574f\u548c\u9690\u79c1\u6cc4\u9732\uff09\u5f15\u53d1\u4e86\u91cd\u5927\u62c5\u5fe7\uff0c\u800c\u68c0\u6d4b\u8fd9\u4e9b\u5b89\u5168\u98ce\u9669\u5728\u590d\u6742\u79fb\u52a8\u73af\u5883\u4e2d\u7684\u6311\u6218\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86OS-Sentinel\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5668\u68c0\u6d4b\u663e\u5f0f\u7cfb\u7edf\u7ea7\u8fdd\u89c4\u548c\u57fa\u4e8eVLM\u7684\u4e0a\u4e0b\u6587\u5224\u65ad\u5668\u8bc4\u4f30\u4e0a\u4e0b\u6587\u98ce\u9669\u548c\u4ee3\u7406\u884c\u4e3a\uff0c\u5e76\u6784\u5efa\u4e86MobileRisk-Live\u52a8\u6001\u6c99\u76d2\u73af\u5883\u548c\u5b89\u5168\u68c0\u6d4b\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOS-Sentinel\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534710%-30%\uff0c\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684\u81ea\u4e3b\u79fb\u52a8\u4ee3\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u79fb\u52a8\u4ee3\u7406\u5b89\u5168\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u51fa\u7684\u6df7\u5408\u68c0\u6d4b\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u79fb\u52a8\u73af\u5883\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4fc3\u8fdb\u66f4\u5b89\u5168\u53ef\u9760\u7684\u81ea\u4e3b\u79fb\u52a8\u4ee3\u7406\u53d1\u5c55\u3002"}}
{"id": "2510.24435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24435", "abs": "https://arxiv.org/abs/2510.24435", "authors": ["Benjamin Grando Moreira"], "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning", "comment": "12 pages", "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT\u3001Claude\u3001DeepSeek\u7b49\uff09\u7684\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u75288\u4e2a\u5b9a\u5236\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86LLMs\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u7684\u663e\u8457\u5dee\u5f02\u548c\u56f0\u96be\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u63a8\u8fdb\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u8fd9\u8d85\u8d8a\u4e86\u5355\u7eaf\u7684\u8bed\u8a00\u4efb\u52a1\u8868\u73b0\uff0c\u6d89\u53ca\u7406\u89e3\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4fe1\u606f\u3001\u8fdb\u884c\u63a8\u7406\u4ee5\u53ca\u4ee5\u903b\u8f91\u6709\u6548\u7684\u65b9\u5f0f\u5f97\u51fa\u7ed3\u8bba\u3002", "method": "\u4f7f\u75288\u4e2a\u5b9a\u5236\u8bbe\u8ba1\u7684\u63a8\u7406\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u591a\u4e2aLLM\uff08\u5305\u62ecGPT\u3001Claude\u3001DeepSeek\u3001Gemini\u7b49\uff09\u7684\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u6280\u80fd\uff0c\u5e76\u5c06LLM\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u6307\u51fa\u4e86LLMs\u5728\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u7684\u9886\u57df\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u65b9\u9762\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u660e\u663e\u7684\u56f0\u96be\uff0c\u8fd9\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2510.24442", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24442", "abs": "https://arxiv.org/abs/2510.24442", "authors": ["Yiding Wang", "Yuxuan Chen", "Fanxu Meng", "Xifan Chen", "Xiaolei Yang", "Muhan Zhang"], "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents", "comment": null, "summary": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.", "AI": {"tldr": "Law in Silico\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6cd5\u5f8b\u793e\u4f1a\u6a21\u62df\u6846\u67b6\uff0c\u80fd\u591f\u6a21\u62df\u4e2a\u4f53\u51b3\u7b56\u548c\u7acb\u6cd5\u3001\u88c1\u51b3\u3001\u6267\u6cd5\u7b49\u5236\u5ea6\u673a\u5236\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u6709\u6548\u590d\u73b0\u5b8f\u89c2\u72af\u7f6a\u8d8b\u52bf\u5e76\u4e3a\u6cd5\u5f8b\u7406\u8bba\u9a8c\u8bc1\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6cd5\u5f8b\u5b9e\u9a8c\u6210\u672c\u9ad8\u6602\u6216\u96be\u4ee5\u5b9e\u65bd\uff0c\u5229\u7528AI\u7cfb\u7edf\u6a21\u62df\u6cd5\u5f8b\u793e\u4f1a\u6210\u4e3a\u9a8c\u8bc1\u548c\u53d1\u5c55\u6cd5\u5f8b\u7406\u8bba\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002LLM\u51ed\u501f\u5176\u4e16\u754c\u77e5\u8bc6\u548c\u89d2\u8272\u626e\u6f14\u80fd\u529b\uff0c\u662f\u6784\u5efa\u6cd5\u5f8b\u793e\u4f1a\u6a21\u62df\u7684\u7406\u60f3\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86Law in Silico\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u6a21\u62df\u5305\u542b\u4e2a\u4f53\u51b3\u7b56\u4ee5\u53ca\u7acb\u6cd5\u3001\u88c1\u51b3\u3001\u6267\u6cd5\u7b49\u5236\u5ea6\u673a\u5236\u7684\u6cd5\u5f8b\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u6bd4\u8f83\u6a21\u62df\u72af\u7f6a\u7387\u4e0e\u73b0\u5b9e\u6570\u636e\u53d1\u73b0\uff0cLLM\u667a\u80fd\u4f53\u80fd\u591f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u590d\u73b0\u5b8f\u89c2\u72af\u7f6a\u8d8b\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e0e\u73b0\u5b9e\u89c2\u5bdf\u4e00\u81f4\u7684\u89c1\u89e3\u3002\u5fae\u89c2\u5c42\u9762\u6a21\u62df\u663e\u793a\uff0c\u8fd0\u4f5c\u826f\u597d\u3001\u900f\u660e\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6cd5\u5f8b\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u4fdd\u62a4\u5f31\u52bf\u4e2a\u4f53\u7684\u6743\u5229\u3002", "conclusion": "LLM\u80fd\u591f\u6709\u6548\u6a21\u62df\u6cd5\u5f8b\u7cfb\u7edf\uff0c\u4e3a\u6cd5\u5f8b\u7406\u8bba\u9a8c\u8bc1\u548c\u884c\u653f\u652f\u6301\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u826f\u597d\u6cd5\u5f8b\u7cfb\u7edf\u5bf9\u5f31\u52bf\u7fa4\u4f53\u4fdd\u62a4\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.24459", "categories": ["cs.AI", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24459", "abs": "https://arxiv.org/abs/2510.24459", "authors": ["Habtom Kahsay Gidey", "Niklas Huber", "Alexander Lenz", "Alois Knoll"], "title": "Affordance Representation and Recognition for Autonomous Agents", "comment": null, "summary": "The autonomy of software agents is fundamentally dependent on their ability\nto construct an actionable internal world model from the structured data that\ndefines their digital environment, such as the Document Object Model (DOM) of\nweb pages and the semantic descriptions of web services. However, constructing\nthis world model from raw structured data presents two critical challenges: the\nverbosity of raw HTML makes it computationally intractable for direct use by\nfoundation models, while the static nature of hardcoded API integrations\nprevents agents from adapting to evolving services.\n  This paper introduces a pattern language for world modeling from structured\ndata, presenting two complementary architectural patterns. The DOM Transduction\nPattern addresses the challenge of web page complexity by distilling} a\nverbose, raw DOM into a compact, task-relevant representation or world model\noptimized for an agent's reasoning core. Concurrently, the Hypermedia\nAffordances Recognition Pattern enables the agent to dynamically enrich its\nworld model by parsing standardized semantic descriptions to discover and\nintegrate the capabilities of unknown web services at runtime. Together, these\npatterns provide a robust framework for engineering agents that can efficiently\nconstruct and maintain an accurate world model, enabling scalable, adaptive,\nand interoperable automation across the web and its extended resources.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u67b6\u6784\u6a21\u5f0f\uff1aDOM\u8f6c\u6362\u6a21\u5f0f\u548c\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\uff0c\u7528\u4e8e\u4ece\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u8f6f\u4ef6\u4ee3\u7406\u7684\u4e16\u754c\u6a21\u578b\uff0c\u89e3\u51b3HTML\u5197\u4f59\u548cAPI\u9759\u6001\u96c6\u6210\u95ee\u9898\u3002", "motivation": "\u8f6f\u4ef6\u4ee3\u7406\u9700\u8981\u4ece\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u53ef\u64cd\u4f5c\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u539f\u59cbHTML\u8fc7\u4e8e\u5197\u957f\u96be\u4ee5\u76f4\u63a5\u5904\u7406\uff0c\u786c\u7f16\u7801API\u96c6\u6210\u65e0\u6cd5\u9002\u5e94\u670d\u52a1\u6f14\u5316\u3002", "method": "DOM\u8f6c\u6362\u6a21\u5f0f\u5c06\u5197\u957f\u7684\u539f\u59cbDOM\u63d0\u70bc\u4e3a\u7d27\u51d1\u7684\u4efb\u52a1\u76f8\u5173\u8868\u793a\uff1b\u8d85\u5a92\u4f53\u529f\u80fd\u8bc6\u522b\u6a21\u5f0f\u901a\u8fc7\u89e3\u6790\u6807\u51c6\u5316\u8bed\u4e49\u63cf\u8ff0\u6765\u52a8\u6001\u53d1\u73b0\u548c\u96c6\u6210\u672a\u77e5Web\u670d\u52a1\u3002", "result": "\u8fd9\u4e24\u79cd\u6a21\u5f0f\u5171\u540c\u4e3a\u5de5\u7a0b\u5316\u4ee3\u7406\u63d0\u4f9b\u4e86\u7a33\u5065\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u6784\u5efa\u548c\u7ef4\u62a4\u51c6\u786e\u7684\u4e16\u754c\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u5f0f\u8bed\u8a00\u4f7f\u4ee3\u7406\u80fd\u591f\u5728Web\u53ca\u5176\u6269\u5c55\u8d44\u6e90\u4e0a\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u4e92\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u3002"}}
{"id": "2510.24461", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24461", "abs": "https://arxiv.org/abs/2510.24461", "authors": ["Korneel Van den Berghe", "Stein Stroobants", "Vijay Janapa Reddi", "G. C. H. E. de Croon"], "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks", "comment": null, "summary": "Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\u548c\u5f15\u5165\u7279\u6743\u5f15\u5bfc\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86SNN\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3SNN\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u975e\u53ef\u5fae\u8109\u51b2\u795e\u7ecf\u5143\u9700\u8981\u66ff\u4ee3\u68af\u5ea6\u4f46\u4f18\u5316\u7279\u6027\u4e0d\u660e\u786e\uff0c\u4ee5\u53caSNN\u7684\u72b6\u6001\u52a8\u6001\u9700\u8981\u5728\u5e8f\u5217\u4e0a\u8bad\u7ec3\u4f46\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u65e9\u671f\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u6709\u9650\uff0c\u963b\u788d\u7f51\u7edc\u5ea6\u8fc7\u9884\u70ed\u671f\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\uff0c\u53d1\u73b0\u8f83\u6d45\u659c\u7387\u80fd\u589e\u52a0\u6df1\u5c42\u68af\u5ea6\u5e45\u5ea6\u4f46\u51cf\u5c11\u4e0e\u771f\u5b9e\u68af\u5ea6\u5bf9\u9f50\uff1b\u63d0\u51fa\u5229\u7528\u7279\u6743\u5f15\u5bfc\u7b56\u7565\u6765\u5f15\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73af\u5883\u7684\u5728\u7ebf\u4ea4\u4e92\uff1b\u7ed3\u5408\u81ea\u9002\u5e94\u659c\u7387\u8c03\u5ea6\u65b9\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u56de\u62a5\u8fbe\u5230400\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u6280\u672f\uff08\u5982\u884c\u4e3a\u514b\u9686\u548cTD3BC\uff0c\u6700\u591a\u53ea\u80fd\u8fbe\u5230-200\u5206\uff09\uff0c\u8bad\u7ec3\u548c\u6700\u7ec8\u90e8\u7f72\u6027\u80fd\u63d0\u53472.1\u500d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5bf9SNN\u4e2d\u66ff\u4ee3\u68af\u5ea6\u5b66\u4e60\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u4e3a\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u5668\u5728\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u8fdb\u5c55\u3002"}}
{"id": "2510.24528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24528", "abs": "https://arxiv.org/abs/2510.24528", "authors": ["Zihan Chen", "Song Wang", "Xingbo Fu", "Chengshuai Shi", "Zhenyu Lei", "Cong Shen", "Jundong Li"], "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning", "comment": null, "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u793a\u4f8b\u548c\u57fa\u4e8e\u56fe\u7684\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\u51cf\u5c11\u5bf9LLM\u6570\u636e\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u964d\u4f4eICL\u7684\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u4e3a\u65b0\u9896\u6216\u56f0\u96be\u4efb\u52a1\u6536\u96c6\u9ad8\u8d28\u91cf\u793a\u4f8b\u6210\u672c\u9ad8\u6602\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u9700\u8981\u51cf\u5c11\u5bf9LLM\u6570\u636e\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "method": "\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u5229\u7528\u8de8\u4efb\u52a1\u793a\u4f8b\u63d0\u793aLLM\u5bf9\u5c11\u91cf\u76ee\u6807\u4efb\u52a1\u5b9e\u4f8b\u8fdb\u884c\u4f2a\u6807\u6ce8\uff0c\u7136\u540e\u5f15\u5165\u57fa\u4e8e\u56fe\u7684\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\u5c06\u6807\u7b7e\u4fe1\u606f\u4f20\u64ad\u5230\u5269\u4f59\u76ee\u6807\u793a\u4f8b\u4e2d\uff0c\u65e0\u9700\u989d\u5916LLM\u67e5\u8be2\u3002", "result": "\u5728\u4e94\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u52b2\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6d41\u7a0b\u7ed3\u5408\u4e86\u8de8\u4efb\u52a1\u76d1\u7763\u7684\u7075\u6d3b\u6027\u548c\u65e0LLM\u4f20\u64ad\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3aICL\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24551", "abs": "https://arxiv.org/abs/2510.24551", "authors": ["Gang Chen", "Changshuo Liu", "Gene Anne Ooi", "Marcus Tan", "Zhongle Xie", "Jianwei Yin", "James Wei Luen Yip", "Wenqiao Zhang", "Jiaqi Zhu", "Beng Chin Ooi"], "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5c06\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u6709\u6548\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\u652f\u6301\u533b\u7597AI\u5e94\u7528\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u533b\u7597\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u6df1\u5165\u7406\u89e3\u533b\u7597\u4efb\u52a1\u548c\u53ef\u5b9e\u73b0\u7684\u76ee\u6807\u3002\u5f53\u524d\u90e8\u7f72\u9700\u8981\u66f4\u597d\u7684\u6570\u636e\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u91cd\u65b0\u5b9a\u4f4d\u6570\u636e\u751f\u547d\u5468\u671f\uff0c\u6784\u5efa\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u57fa\u7840\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u7684\u96c6\u6210\u3001\u8868\u793a\u548c\u68c0\u7d22\uff0c\u5305\u62ec\u8bed\u4e49\u5411\u91cf\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u67e5\u8be2\u3002", "result": "\u8be5\u751f\u6001\u7cfb\u7edf\u80fd\u591f\u4e3a\u4e0a\u6e38\u6a21\u578b\u7ec4\u4ef6\u63d0\u4f9b\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u540c\u65f6\u4f5c\u4e3a\u77e5\u8bc6\u68c0\u7d22\u540e\u7aef\u652f\u6301\u4efb\u52a1\u7279\u5b9a\u63a8\u7406\u3002", "conclusion": "\u8fd9\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u6709\u6548\u7684\u533b\u7597AI\u7cfb\u7edf\u90e8\u7f72\uff0c\u6539\u5584\u533b\u7597\u670d\u52a1\u4ea4\u4ed8\u3002"}}
{"id": "2510.24645", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24645", "abs": "https://arxiv.org/abs/2510.24645", "authors": ["Zengzhuang Xu", "Bingguang Hao", "Zechuan Wang", "Yuntao Wen", "Maolin Wang", "Yang Liu", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Chenyi Zhuang", "Jinjie Gu", "Leilei Gan", "Xiangyu Zhao", "Shi Gu"], "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling", "comment": null, "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.", "AI": {"tldr": "FunReason-MT\u662f\u4e00\u4e2a\u7528\u4e8e\u5408\u6210\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u8bad\u7ec3\u6570\u636e\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u73af\u5883-API\u56fe\u4ea4\u4e92\u3001\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u548c\u5f15\u5bfc\u8fed\u4ee3\u94fe\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5728BFCL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff08\u5982\u968f\u673a\u73af\u5883\u91c7\u6837\u6216\u591a\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\uff09\u4e0d\u8db3\u4ee5\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u5b58\u5728\u76ee\u6807\u6a21\u578b\u8bad\u7ec3\u3001\u5de5\u5177\u67b6\u6784\u9694\u79bb\u548c\u591a\u8f6e\u903b\u8f91\u4f9d\u8d56\u7b49\u5b9e\u9645\u6311\u6218\u3002", "method": "FunReason-MT\u6846\u67b6\u91c7\u7528\uff1a1\uff09\u73af\u5883-API\u56fe\u4ea4\u4e92\u6536\u96c6\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff1b2\uff09\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u7b80\u5316\u56f0\u96be\u67e5\u8be2\u6784\u5efa\uff1b3\uff09\u5f15\u5bfc\u8fed\u4ee3\u94fe\u751f\u6210\u590d\u6742\u601d\u7ef4\u94fe\u3002", "result": "\u5728Berkeley Function-Calling Leaderboard (BFCLv3)\u4e0a\uff0c\u57fa\u4e8eFunReason-MT\u751f\u6210\u6570\u636e\u6784\u5efa\u76844B\u6a21\u578b\u5728\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5927\u591a\u6570\u95ed\u6e90\u6a21\u578b\u3002\u5728BFCLv4\u4e0a\u7684\u8fdb\u4e00\u6b65\u6027\u80fd\u6539\u8fdb\u8bc1\u5b9e\u4e86\u5176\u53ef\u9760\u6027\u3002", "conclusion": "FunReason-MT\u4e3a\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5f3a\u5927\u7684\u6570\u636e\u6e90\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u6570\u636e\u5408\u6210\u7684\u590d\u6742\u6027\u969c\u788d\u3002"}}
{"id": "2510.24650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24650", "abs": "https://arxiv.org/abs/2510.24650", "authors": ["Nitin Rai", "Daeun", "Choi", "Nathan S. Boyd", "Arnold W. Schumann"], "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning", "comment": "26 pages, 8 figures, and 2 tables", "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5206\u6790\u4e86\u7ea640\u7bc7\u5173\u4e8e\u57fa\u7840\u6a21\u578b\u5728\u4f5c\u7269\u5b9a\u70b9\u75c5\u5bb3\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u6587\u732e\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u4e86\u8be5\u9886\u57df\u7684\u6700\u65b0\u53d1\u5c55\u8d8b\u52bf\u548c\u6311\u6218\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u5b9e\u65f6\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f5c\u7269\u5b9a\u70b9\u75c5\u5bb3\u7ba1\u7406\u4ece\u624b\u5de5\u7279\u5f81\u63d0\u53d6\u53d1\u5c55\u5230\u5927\u89c4\u6a21\u81ea\u52a8\u7279\u5f81\u5b66\u4e60\u3002\u57fa\u7840\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u5904\u7406\u4f5c\u7269\u75c5\u5bb3\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u65b9\u5f0f\uff0c\u80fd\u591f\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff0c\u89e3\u91ca\u75c7\u72b6\u6587\u672c\uff0c\u63a8\u7406\u75c7\u72b6\u4e0e\u7ba1\u7406\u7684\u5173\u7cfb\uff0c\u5e76\u4e3a\u79cd\u690d\u8005\u548c\u6559\u80b2\u8005\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u95ee\u7b54\u652f\u6301\u3002", "method": "\u901a\u8fc7\u7b5b\u9009\u7ea640\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u91cd\u70b9\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u5173\u6ce8\u5176\u5728\u667a\u80fd\u55b7\u6d12\u7cfb\u7edf\u4e2d\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\uff1a(a) \u57fa\u7840\u6a21\u578b\u57282023-24\u5e74\u83b7\u5f97\u5e7f\u6cdb\u5173\u6ce8\uff1b(b) \u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u5feb\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u8868\u91cf\u589e\u957f5-10\u500d\uff1b(c) \u5f3a\u5316\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u5728\u667a\u80fd\u55b7\u6d12\u4e2d\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff1b(d) \u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u5b57\u5b6a\u751f\u53ef\u865a\u62df\u6a21\u62df\u5b9a\u70b9\u55b7\u6d12\uff1b(e) \u89e3\u51b3\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u5bf9\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff1b(f) \u4eba\u673a\u534f\u4f5c\u4ecd\u7136\u6709\u9650\uff1b(g) \u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u5b9e\u65f6\u53cd\u9988\u5c06\u63a8\u52a8\u4e0b\u4e00\u4ee3\u5b9a\u70b9\u75c5\u5bb3\u7ba1\u7406\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u6b63\u5728\u53d8\u9769\u4f5c\u7269\u5b9a\u70b9\u75c5\u5bb3\u7ba1\u7406\u9886\u57df\uff0c\u7279\u522b\u662f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u672a\u6765\u9700\u8981\u89e3\u51b3\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u52a0\u5f3a\u4eba\u673a\u534f\u4f5c\uff0c\u5e76\u53d1\u5c55\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u5b9e\u65f6\u53cd\u9988\u7cfb\u7edf\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u7530\u95f4\u75c5\u5bb3\u7ba1\u7406\u3002"}}
{"id": "2510.24663", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24663", "abs": "https://arxiv.org/abs/2510.24663", "authors": ["Yifu Lu", "Shengjie Liu", "Li Dong"], "title": "OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs", "comment": "9 pages, 4 figures", "summary": "Agentic tool use has gained traction with the rise of agentic tool calling,\nyet most existing work overlooks the complexity of multi-turn tool\ninteractions. We introduce OrchDAG, a synthetic data generation pipeline that\nmodels tool execution as directed acyclic graphs (DAGs) with controllable\ncomplexity. Using this dataset, we benchmark model performance and propose a\ngraph-based reward to enhance RLVR training. Experiments show that the dataset\npresents a challenging but solvable benchmark, and the proposed reward is\neffective when combined with GRPO-style algorithms, highlighting the importance\nof leveraging topological structure and data complexity in multi-turn tool use.", "AI": {"tldr": "OrchDAG\u662f\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u5177\u6709\u53ef\u63a7\u590d\u6742\u5ea6\u7684\u6709\u5411\u65e0\u73af\u56fe\uff0c\u7528\u4e8e\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u5ffd\u7565\u4e86\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bad\u7ec3\u65b9\u6cd5\u6765\u5904\u7406\u590d\u6742\u7684\u5de5\u5177\u4f7f\u7528\u573a\u666f\u3002", "method": "\u5f15\u5165OrchDAG\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u6765\u589e\u5f3aRLVR\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u4f46\u53ef\u89e3\u51b3\u7684\u57fa\u51c6\uff0c\u6240\u63d0\u51fa\u7684\u5956\u52b1\u4e0eGRPO\u98ce\u683c\u7b97\u6cd5\u7ed3\u5408\u65f6\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\uff0c\u5229\u7528\u62d3\u6251\u7ed3\u6784\u548c\u6570\u636e\u590d\u6742\u6027\u81f3\u5173\u91cd\u8981\uff0c\u56fe\u7ed3\u6784\u5956\u52b1\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.24690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24690", "abs": "https://arxiv.org/abs/2510.24690", "authors": ["Shengjie Liu", "Li Dong", "Zhenyu Zhang"], "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning", "comment": "4 pages, 2 figures, short paper, NeurIPS 2025 workshop on Bridging\n  Language, Agent, and World Models for Reasoning and Planning", "summary": "We present a framework for uncovering and exploiting dependencies among tools\nand documents to enhance exemplar artifact generation. Our method begins by\nconstructing a tool knowledge graph from tool schemas,including descriptions,\narguments, and output payloads, using a DeepResearch-inspired analysis. In\nparallel, we derive a complementary knowledge graph from internal documents and\nSOPs, which is then fused with the tool graph. To generate exemplar plans, we\nadopt a deep-sparse integration strategy that aligns structural tool\ndependencies with procedural knowledge. Experiments demonstrate that this\nunified framework effectively models tool interactions and improves plan\ngeneration, underscoring the benefits of linking tool graphs with domain\nknowledge graphs for tool-augmented reasoning and planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u8fc7\u6784\u5efa\u5de5\u5177\u548c\u6587\u6863\u77e5\u8bc6\u56fe\u8c31\u6765\u589e\u5f3a\u8303\u4f8b\u5de5\u4ef6\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u5de5\u5177\u56fe\u8c31\u548c\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u6765\u6539\u8fdb\u5de5\u5177\u589e\u5f3a\u7684\u63a8\u7406\u548c\u89c4\u5212\u3002", "motivation": "\u4e3a\u4e86\u63ed\u793a\u548c\u5229\u7528\u5de5\u5177\u4e0e\u6587\u6863\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u5347\u8303\u4f8b\u5de5\u4ef6\u7684\u751f\u6210\u8d28\u91cf\uff0c\u9700\u8981\u5c06\u5de5\u5177\u7684\u7ed3\u6784\u5316\u4f9d\u8d56\u4e0e\u7a0b\u5e8f\u6027\u77e5\u8bc6\u76f8\u7ed3\u5408\u3002", "method": "\u4ece\u5de5\u5177\u6a21\u5f0f\u6784\u5efa\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\uff0c\u540c\u65f6\u4ece\u5185\u90e8\u6587\u6863\u548cSOP\u6784\u5efa\u8865\u5145\u77e5\u8bc6\u56fe\u8c31\uff0c\u7136\u540e\u5c06\u4e24\u8005\u878d\u5408\uff0c\u91c7\u7528\u6df1\u5ea6-\u7a00\u758f\u96c6\u6210\u7b56\u7565\u5bf9\u9f50\u7ed3\u6784\u5de5\u5177\u4f9d\u8d56\u4e0e\u7a0b\u5e8f\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u7edf\u4e00\u6846\u67b6\u80fd\u6709\u6548\u5efa\u6a21\u5de5\u5177\u4ea4\u4e92\u5e76\u6539\u8fdb\u89c4\u5212\u751f\u6210\u3002", "conclusion": "\u5c06\u5de5\u5177\u56fe\u8c31\u4e0e\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u5bf9\u4e8e\u5de5\u5177\u589e\u5f3a\u7684\u63a8\u7406\u548c\u89c4\u5212\u5177\u6709\u663e\u8457\u76ca\u5904\u3002"}}
{"id": "2510.16620", "categories": ["cs.IT", "cs.AI", "cs.CR", "cs.LG", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16620", "abs": "https://arxiv.org/abs/2510.16620", "authors": ["Yingyao Zhou", "Natasha Devroye", "Onur G\u00fcnl\u00fc"], "title": "Feedback Lunch: Deep Feedback Codes for Wiretap Channels", "comment": null, "summary": "We consider reversely-degraded wiretap channels, for which the secrecy\ncapacity is zero if there is no channel feedback. This work focuses on a seeded\nmodular code design for the Gaussian wiretap channel with channel output\nfeedback, combining universal hash functions for security and learned\nfeedback-based codes for reliability to achieve positive secrecy rates. We\nstudy the trade-off between communication reliability and information leakage,\nillustrating that feedback enables agreeing on a secret key shared between\nlegitimate parties, overcoming the security advantage of the wiretapper. Our\nfindings also motivate code designs for sensing-assisted secure communication,\nto be used in next-generation integrated sensing and communication methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u65af\u7a83\u542c\u4fe1\u9053\u7684\u79cd\u5b50\u6a21\u5757\u5316\u4ee3\u7801\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u54c8\u5e0c\u51fd\u6570\u548c\u57fa\u4e8e\u53cd\u9988\u7684\u5b66\u4e60\u4ee3\u7801\uff0c\u5728\u4fe1\u9053\u8f93\u51fa\u53cd\u9988\u4e0b\u5b9e\u73b0\u4e86\u6b63\u4fdd\u5bc6\u7387\u3002", "motivation": "\u7814\u7a76\u53cd\u5411\u9000\u5316\u7a83\u542c\u4fe1\u9053\uff0c\u5728\u6ca1\u6709\u4fe1\u9053\u53cd\u9988\u65f6\u4fdd\u5bc6\u5bb9\u91cf\u4e3a\u96f6\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u5229\u7528\u53cd\u9988\u6765\u514b\u670d\u7a83\u542c\u8005\u7684\u5b89\u5168\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u79cd\u5b50\u6a21\u5757\u5316\u4ee3\u7801\u8bbe\u8ba1\uff0c\u7ed3\u5408\u901a\u7528\u54c8\u5e0c\u51fd\u6570\u63d0\u4f9b\u5b89\u5168\u6027\uff0c\u5b66\u4e60\u57fa\u4e8e\u53cd\u9988\u7684\u4ee3\u7801\u63d0\u4f9b\u53ef\u9760\u6027\uff0c\u7814\u7a76\u901a\u4fe1\u53ef\u9760\u6027\u4e0e\u4fe1\u606f\u6cc4\u9732\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u53cd\u9988\u4f7f\u5408\u6cd5\u65b9\u80fd\u591f\u534f\u5546\u5171\u4eab\u5bc6\u94a5\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u6b63\u4fdd\u5bc6\u7387\uff0c\u514b\u670d\u4e86\u7a83\u542c\u8005\u7684\u5b89\u5168\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u542f\u53d1\u4e86\u611f\u77e5\u8f85\u52a9\u5b89\u5168\u901a\u4fe1\u7684\u4ee3\u7801\u8bbe\u8ba1\uff0c\u53ef\u7528\u4e8e\u4e0b\u4e00\u4ee3\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u65b9\u6cd5\u3002"}}
