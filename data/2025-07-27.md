<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Frame-Based Zero-Shot Semantic Channel Equalization for AI-Native Communications](https://arxiv.org/abs/2507.17835)
*Simone Fiorellino,Claudio Battiloro,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.NI

TL;DR: 提出了一种零样本的Parseval Frame Equalizer（PFE），用于对齐异构编码器的潜在空间，解决语义信道噪声问题，并通过动态优化策略平衡资源消耗与性能。


<details>
  <summary>Details</summary>
Motivation: 未来AI原生无线网络中，异构编码器的潜在空间不匹配会导致语义信道噪声，降低系统性能。

Method: 提出PFE方法，无需重新训练即可对齐潜在空间，并引入动态优化策略协调通信、计算和学习资源。

Result: 仿真验证了PFE在保持语义一致性和满足延迟、精度约束方面的有效性。

Conclusion: PFE能有效解决语义噪声问题，并在动态网络条件下优化系统性能。

Abstract: In future AI-native wireless networks, the presence of mismatches between the
latent spaces of independently designed and trained deep neural network (DNN)
encoders may impede mutual understanding due to the emergence of semantic
channel noise. This undermines the receiver's ability to interpret transmitted
representations, thereby reducing overall system performance. To address this
issue, we propose the Parseval Frame Equalizer (PFE), a zero-shot, frame-based
semantic channel equalizer that aligns latent spaces of heterogeneous encoders
without requiring system retraining. PFE enables dynamic signal compression and
expansion, mitigating semantic noise while preserving performance on downstream
tasks. Building on this capability, we introduce a dynamic optimization
strategy that coordinates communication, computation, and learning resources to
balance energy consumption, end-to-end (E2E) latency, and task performance in
multi-agent semantic communication scenarios. Extensive simulations confirm the
effectiveness of our approach in maintaining semantic consistency and meeting
long-term constraints on latency and accuracy under diverse and time-varying
network conditions.

</details>


### [2] [ARCADE: A RAN Diagnosis Methodology in a Hybrid AI Environment for 6G Networks](https://arxiv.org/abs/2507.17861)
*Daniel Ricardo Cunha Oliveira,Rodrigo Moreira,Flávio de Oliveira Silva*

Main category: cs.NI

TL;DR: 论文提出了一种名为ARCADE的方法，用于检测和评估蜂窝接入网络中的异常，并探讨了在6G网络演进中如何通过混合架构增强AI应用。


<details>
  <summary>Details</summary>
Motivation: 当前5G网络中的NWDAF功能有限，需要更全面的方法以实现网络自动化，尤其是在尚未充分探索的网络部分。

Method: 提出了ARCADE方法，结合混合网络分析架构，用于检测和诊断蜂窝接入网络中的异常。

Result: 展示了ARCADE在6G网络中的实际应用，证明了混合架构如何扩展AI在网络中的功能。

Conclusion: ARCADE和混合架构为6G网络中的AI应用提供了更全面的解决方案，推动了网络自动化的进一步发展。

Abstract: Artificial Intelligence (AI) plays a key role in developing 6G networks.
While current specifications already include Network Data Analytics Function
(NWDAF) as a network element responsible for providing information about the
core, a more comprehensive approach will be needed to enable automation of
network segments that are not yet fully explored in the context of 5G. In this
paper, we present Automated Radio Coverage Anomalies Detection and Evaluation
(ARCADE), a methodology for identifying and diagnosing anomalies in the
cellular access network. Furthermore, we demonstrate how a hybrid architecture
of network analytics functions in the evolution toward 6G can enhance the
application of AI in a broader network context, using ARCADE as a practical
example of this approach.

</details>


### [3] [Talk with the Things: Integrating LLMs into IoT Networks](https://arxiv.org/abs/2507.17865)
*Alakesh Kalita*

Main category: cs.NI

TL;DR: 本文提出了一种基于边缘计算的框架，将大型语言模型（LLM）与物联网（IoT）网络结合，实现自然语言控制、上下文感知决策和增强自动化。通过轻量级RAG模型在边缘设备上的部署，降低了延迟并提升了隐私保护。实验验证了模型大小与推理时间及准确性的权衡。


<details>
  <summary>Details</summary>
Motivation: 结合LLM和IoT网络，构建智能、响应迅速且用户友好的系统，解决传统IoT系统中延迟高、隐私保护不足的问题。

Method: 提出模块化、轻量级的RAG模型，部署在边缘计算设备上，通过LLaMA 3和Gemma 2B模型实现本地化处理。

Result: 实验结果表明，模型大小与推理时间及准确性之间存在权衡关系，框架在智能家居原型中验证了其有效性。

Conclusion: LLM与IoT的结合具有广阔的应用前景，但仍面临模型大小与性能平衡等挑战。

Abstract: The convergence of Large Language Models (LLMs) and Internet of Things (IoT)
networks open new opportunities for building intelligent, responsive, and
user-friendly systems. This work presents an edge-centric framework that
integrates LLMs into IoT architectures to enable natural language-based
control, context-aware decision-making, and enhanced automation. The proposed
modular and lightweight Retrieval Augmented Generation (RAG)-based LLMs are
deployed on edge computing devices connected to IoT gateways, enabling local
processing of user commands and sensor data for reduced latency, improved
privacy, and enhanced inference quality. We validate the framework through a
smart home prototype using LLaMA 3 and Gemma 2B models for controlling smart
devices. Experimental results highlight the trade-offs between model accuracy
and inference time with respect to models size. At last, we also discuss the
potential applications that can use LLM-based IoT systems, and a few key
challenges associated with such systems.

</details>


### [4] [Enabling Scalability in Asynchronous and Bidirectional Communication in LPWAN](https://arxiv.org/abs/2507.17905)
*Mahbubur Rahman*

Main category: cs.NI

TL;DR: 论文提出了一种基于SNOW技术的LPWAN改进方法，通过D-OFDM和Gold码实现大规模异步传感器的高效并行数据传输。


<details>
  <summary>Details</summary>
Motivation: 解决LPWAN在大规模传感器网络中数据传输效率和低延迟的挑战，以满足新兴IoT和CPS应用的需求。

Method: 利用D-OFDM子载波和Gold码的伪随机噪声序列，实现基站对同一子载波上多个异步传感器数据的并行解码。

Result: 实验结果显示，改进后的SNOW技术可提升约9倍的扩展性，同时保持数据收集的时效性和传感器的能效。

Conclusion: 该方法为需要大规模传感器网络的新兴IoT和CPS应用提供了可行的解决方案。

Abstract: LPWANs have become ubiquitous due to their ability to connect sensors over
large geographic areas in a single hop. It is, however, very challenging to
achieve massive scalability in LPWANs, where numerous sensors can transmit data
efficiently and with low latency, which emerging IoT and CPS applications may
require. In this paper, we address the above challenges by significantly
advancing an LPWAN technology called SNOW. SNOW exploits distributed orthogonal
frequency division multiplexing, D-OFDM, subcarriers to enable parallel
reception of data to a BS from multiple asynchronous sensors, each using a
different subcarrier. In this paper, we achieve massive scalability in SNOW by
enabling the BS to decode concurrent data from numerous asynchronous sensors on
the same subcarrier while parallelly decoding from other subcarriers as well.
Additionally, we enable numerous asynchronous sensors to receive distinct data
from the BS on the same subcarrier while other sensors also receive data
parallelly on other subcarriers. To do this, we develop a set of Gold
code-based pseudorandom noise or PN sequences that are mutually non-interfering
within and across the subcarriers. Each sensor uses its PN sequence from the
set for encoding or decoding data on its subcarriers, enabling massive
concurrency. Our evaluation results demonstrate that we can achieve
approximately 9x more scalability in SNOW while being timely in data collection
at the BS and energy efficient at the sensors. This may enable emerging IoT and
CPS applications requiring tens of thousands of sensors with longer battery
life and making data-driven, time-sensitive decisions.

</details>


### [5] [Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of Information Optimization in Vehicular Networks](https://arxiv.org/abs/2507.18328)
*Xiao Xu,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen,Khaled B. Letaief*

Main category: cs.NI

TL;DR: 论文研究了5G NR V2I Mode 2下车联网中的公平访问和信息时效性（AoI）问题，提出了一种联合优化框架，通过调整SPS选择窗口和MOEA/D算法，平衡公平性和AoI。


<details>
  <summary>Details</summary>
Motivation: 车辆在相邻车道行驶时速度不同，导致RSU驻留时间和通信时长差异，引发网络资源访问不公平，可能影响驾驶安全。同时，Mode 2的抢占机制需要同时优化公平访问和AoI。

Method: 定义了公平性指数，采用SHS建模AoI，通过自适应调整SPS选择窗口，并应用基于LLM的MOEA/D算法进行多目标优化。

Result: 仿真结果表明，所提方案能有效平衡公平访问和最小化AoI。

Conclusion: 提出的联合优化框架成功解决了Mode 2下的公平性和AoI问题，为车联网提供了及时且相关的数据传输保障。

Abstract: In this paper, we consider the fair access problem and the Age of Information
(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in
vehicular networks. Specifically, vehicles follow Mode 2 to communicate with
Roadside Units (RSUs) to obtain accurate data for driving
assistance.Nevertheless, vehicles often have different velocity when they are
moving in adjacent lanes, leading to difference in RSU dwelltime and
communication duration. This results in unfair access to network resources,
potentially influencing driving safety. To ensure the freshness of received
data, the AoI should be analyzed. Mode 2 introduces a novel preemption
mechanism, necessitating simultaneous optimization of fair access and AoI to
guarantee timely and relevant data delivery. We propose a joint optimization
framework for vehicular network, defining a fairness index and employing
Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By
adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)
in Mode 2, we address the optimization of fairness and AoI. We apply a large
language model (LLM)-Based Multi-objective Evolutionary Algorithm Based on
Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate
the effectiveness of our scheme in balancing fair access and minimizing AoI.

</details>


### [6] [Improving Wi-Fi 8 Latency with Coordinated Spatial Reuse](https://arxiv.org/abs/2507.18480)
*David Nunez,Francesc Wilhelmi,Lorenzo Galati-Giordano,Giovanni Geraci,Boris Bellalta*

Main category: cs.NI

TL;DR: 论文研究了Wi-Fi 8网络中协调空间复用（Co-SR）的性能，通过模拟实验验证了其在密集环境中提升频谱效率和降低延迟的效果。


<details>
  <summary>Details</summary>
Motivation: 新兴应用（如云游戏、XR和视频流服务）对高吞吐量、低延迟和高可靠性提出了严格要求，需要优化频谱资源利用。

Method: 提出了一种与Wi-Fi 8标准化工作一致的Co-SR实现，并通过Wi-Fi模拟器在四AP的WLAN中评估其性能。

Result: 实验结果显示，与分布式协调功能（DCF）相比，Co-SR的延迟降低了31%至95%。

Conclusion: Co-SR在Wi-Fi 8网络中能显著提升性能，适用于密集环境。

Abstract: IEEE 802.11 networks continuously adapt to meet the stringent requirements of
emerging applications like cloud gaming, eXtended Reality (XR), and video
streaming services, which require high throughput, low latency, and high
reliability. To address these challenges, Coordinated Spatial Reuse (Co-SR) can
potentially contribute to optimizing spectrum resource utilization. This
mechanism is expected to enable simultaneous transmissions, thereby boosting
spectral efficiency in dense environments and increasing the overall network
performance. In this paper, we shed light on the performance of Co-SR for Wi-Fi
8 networks. For that, we propose an implementation of Co-SR aligned with
ongoing Wi-Fi 8 standardization efforts. The evaluation is done on a Wi-Fi
simulator, which allows us to study the performance of the proposed Co-SR
mechanisms in relevant scenarios. The results obtained in a Wireless Local Area
Network (WLAN) consisting of four APs show delay reduction with Co-SR ranging
from 31% to 95% when compared to Distributed Coordination Function (DCF).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics](https://arxiv.org/abs/2507.17777)
*Theofanis Aravanis,Grigorios Chrimatopoulos,Mohammad Ferdows,Michalis Xenos,Efstratios Em Tzirtzilakis*

Main category: cs.AI

TL;DR: 该研究结合符号回归（SR）和答案集编程（ASP），从数值模拟数据中推导出可解释的数学方程，用于描述三维不可压缩流体的速度和压力场，同时确保物理合理性。


<details>
  <summary>Details</summary>
Motivation: 在流体力学中，理解流动物理与准确预测同样重要，而传统机器学习方法缺乏可解释性。符号回归（SR）能够揭示复杂系统中的数学关系，无需先验模型假设。

Method: 使用PySR库从数值模拟数据中推导符号方程，并结合ASP框架确保方程的物理合理性。

Result: 推导的方程不仅近似模拟了抛物线速度分布和压力降，还与文献中的解析解完全一致。

Conclusion: 研究展示了SR简化复杂流动行为为可解释方程的能力，以及知识表示方法提升数据驱动模型可靠性的潜力。

Abstract: Unlike conventional Machine-Learning (ML) approaches, often criticized as
"black boxes", Symbolic Regression (SR) stands out as a powerful tool for
revealing interpretable mathematical relationships in complex physical systems,
requiring no a priori assumptions about models' structures. Motivated by the
recognition that, in fluid mechanics, an understanding of the underlying flow
physics is as crucial as accurate prediction, this study applies SR to model a
fundamental three-dimensional (3D) incompressible flow in a rectangular
channel, focusing on the (axial) velocity and pressure fields under laminar
conditions. By employing the PySR library, compact symbolic equations were
derived directly from numerical simulation data, revealing key characteristics
of the flow dynamics. These equations not only approximate the parabolic
velocity profile and pressure drop observed in the studied fluid flow, but also
perfectly coincide with analytical solutions from the literature. Furthermore,
we propose an innovative approach that integrates SR with the
knowledge-representation framework of Answer Set Programming (ASP), combining
the generative power of SR with the declarative reasoning strengths of ASP. The
proposed hybrid SR/ASP framework ensures that the SR-generated symbolic
expressions are not only statistically accurate, but also physically plausible,
adhering to domain-specific principles. Overall, the study highlights two key
contributions: SR's ability to simplify complex flow behaviours into concise,
interpretable equations, and the potential of knowledge-representation
approaches to improve the reliability and alignment of data-driven SR models
with domain principles. Insights from the examined 3D channel flow pave the way
for integrating such hybrid approaches into efficient frameworks, [...] where
explainable predictions and real-time data analysis are crucial.

</details>


### [8] [I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis](https://arxiv.org/abs/2507.17874)
*SaiBarath Sundar,Pranav Satheesan,Udayaadithya Avadhanam*

Main category: cs.AI

TL;DR: I2I-STRADA是一种用于数据分析的智能代理架构，通过结构化推理过程提升分析任务的连贯性和洞察力。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统在数据分析中忽视了结构化推理过程，导致任务执行缺乏一致性。

Method: 提出I2I-STRADA架构，通过模块化子任务模拟分析推理的认知步骤。

Result: 在DABstep和DABench基准测试中表现优于现有系统，尤其在规划连贯性和洞察对齐方面。

Conclusion: 结构化认知工作流对数据分析代理设计至关重要。

Abstract: Recent advances in agentic systems for data analysis have emphasized
automation of insight generation through multi-agent frameworks, and
orchestration layers. While these systems effectively manage tasks like query
translation, data transformation, and visualization, they often overlook the
structured reasoning process underlying analytical thinking. Reasoning large
language models (LLMs) used for multi-step problem solving are trained as
general-purpose problem solvers. As a result, their reasoning or thinking steps
do not adhere to fixed processes for specific tasks. Real-world data analysis
requires a consistent cognitive workflow: interpreting vague goals, grounding
them in contextual knowledge, constructing abstract plans, and adapting
execution based on intermediate outcomes. We introduce I2I-STRADA
(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an
agentic architecture designed to formalize this reasoning process. I2I-STRADA
focuses on modeling how analysis unfolds via modular sub-tasks that reflect the
cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench
benchmarks show that I2I-STRADA outperforms prior systems in planning coherence
and insight alignment, highlighting the importance of structured cognitive
workflows in agent design for data analysis.

</details>


### [9] [SMARTAPS: Tool-augmented LLMs for Operations Management](https://arxiv.org/abs/2507.17927)
*Timothy Tin Long Yu,Mahdi Mostajabdaveh,Jabo Serge Byusa,Rindra Ramamonjison,Giuseppe Carenini,Kun Mao,Zirui Zhou,Yong Zhang*

Main category: cs.AI

TL;DR: 论文介绍了SmartAPS，一个基于增强工具的大型语言模型（LLM）的对话系统，旨在为运营规划师提供更易访问的高级规划系统（APS）。


<details>
  <summary>Details</summary>
Motivation: 传统APS因咨询师的高昂定制和维护费用而难以普及，SmartAPS旨在通过自然语言交互降低使用门槛。

Method: SmartAPS结合LLM和工具增强技术，提供自然语言聊天界面，支持查询、反事实推理、推荐和场景分析。

Result: 系统通过直观的交互方式帮助运营规划师更好地管理操作。

Conclusion: SmartAPS展示了LLM在提升传统工具可访问性和用户体验方面的潜力。

Abstract: Large language models (LLMs) present intriguing opportunities to enhance user
interaction with traditional algorithms and tools in real-world applications.
An advanced planning system (APS) is a sophisticated software that leverages
optimization to help operations planners create, interpret, and modify an
operational plan. While highly beneficial, many customers are priced out of
using an APS due to the ongoing costs of consultants responsible for
customization and maintenance. To address the need for a more accessible APS
expressed by supply chain planners, we present SmartAPS, a conversational
system built on a tool-augmented LLM. Our system provides operations planners
with an intuitive natural language chat interface, allowing them to query
information, perform counterfactual reasoning, receive recommendations, and
execute scenario analysis to better manage their operation. A short video
demonstrating the system has been released: https://youtu.be/KtIrJjlDbyw

</details>


### [10] [Synthesis of timeline-based planning strategies avoiding determinization](https://arxiv.org/abs/2507.17988)
*Dario Della Monica,Angelo Montanari,Pietro Sala*

Main category: cs.AI

TL;DR: 本文识别了一种定性时间轴规划的子集，其计划存在性问题可直接映射到确定性有限自动机的非空问题，从而合成策略。


<details>
  <summary>Details</summary>
Motivation: 定性时间轴规划的计划存在性问题是PSPACE完全的，但非确定性自动机无法直接用于合成策略，需进行昂贵的确定性转换。本文旨在找到可直接映射到确定性自动机的子集。

Method: 识别一种定性时间轴规划的子集，其计划存在性问题可直接映射到确定性有限自动机的非空问题。同时识别了Allen关系的一个最大子集，适用于该确定性片段。

Result: 成功找到可直接映射到确定性自动机的规划子集，并确定了适用的Allen关系子集。

Conclusion: 本文提供了一种更高效的策略合成方法，避免了非确定性自动机的确定性转换步骤。

Abstract: Qualitative timeline-based planning models domains as sets of independent,
but
  interacting, components whose behaviors over time, the timelines, are
governed
  by sets of qualitative temporal constraints (ordering relations), called
  synchronization rules.
  Its plan-existence problem has been shown to be PSPACE-complete; in
  particular, PSPACE-membership has been proved via reduction to the
  nonemptiness problem for nondeterministic finite automata.
  However, nondeterministic automata cannot be directly used to synthesize
  planning strategies as a costly determinization step is needed.
  In this paper, we identify a fragment of qualitative timeline-based planning
  whose plan-existence problem can be directly mapped into the nonemptiness
  problem of deterministic finite automata, which can then
  synthesize strategies.
  In addition, we identify a maximal subset of Allen's relations that fits into
  such a deterministic fragment.

</details>


### [11] [E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI](https://arxiv.org/abs/2507.18004)
*Yusen Peng,Shuhua Mao*

Main category: cs.AI

TL;DR: 论文提出E.A.R.T.H.框架，通过错误生成、放大、精炼选择、转换和反馈利用，将AI生成的错误转化为创意资产，显著提升创造力。


<details>
  <summary>Details</summary>
Motivation: 探索AI如何超越模仿实现真正的创造力，认为“创意潜力隐藏在失败中”。

Method: 采用五阶段生成管道（E.A.R.T.H.），结合认知科学和生成模型，使用LLaMA-2-7B-Chat等工具，通过复合奖励函数评估新颖性、惊喜和相关性。

Result: 创造力评分提升70.4%，口号更短、更新颖，跨模态测试显示强对齐，人类评价中60%输出得分≥4.0。

Conclusion: 错误驱动和反馈驱动的生成方法能有效增强AI创造力，为自进化、与人类对齐的创意AI提供可行路径。

Abstract: How can AI move beyond imitation toward genuine creativity? This paper
proposes the E.A.R.T.H. framework, a five-stage generative pipeline that
transforms model-generated errors into creative assets through Error
generation, Amplification, Refine selection, Transform, and Harness feedback.
Drawing on cognitive science and generative modeling, we posit that "creative
potential hides in failure" and operationalize this via structured prompts,
semantic scoring, and human-in-the-loop evaluation. Implemented using
LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the
pipeline employs a composite reward function based on novelty, surprise, and
relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to
1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%
improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a
4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment
(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs
scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones
(3.99). Feedback highlights stylistic precision and emotional resonance. These
results demonstrate that error-centered, feedback-driven generation enhances
creativity, offering a scalable path toward self-evolving, human-aligned
creative AI.

</details>


### [12] [Does visualization help AI understand data?](https://arxiv.org/abs/2507.18022)
*Victoria R. Li,Johnathan Sun,Martin Wattenberg*

Main category: cs.AI

TL;DR: 研究发现，AI系统（如GPT 4.1和Claude 3.5）在处理数据时，若数据附带散点图，其描述数据的精确度和准确性会提高，尤其是在复杂数据集上。


<details>
  <summary>Details</summary>
Motivation: 探讨图表是否对AI系统分析数据有帮助。

Method: 使用两种商业视觉语言模型（GPT 4.1和Claude 3.5），在三种代表性分析任务中测试数据附带散点图的效果，并与空白图表和数据不匹配的图表进行对比。

Result: AI系统在数据附带散点图时表现更好，尤其是在复杂数据集上。

Conclusion: 初步证据表明，AI系统与人类类似，可以从可视化中受益。

Abstract: Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.

</details>


### [13] [Multi-Agent Guided Policy Optimization](https://arxiv.org/abs/2507.18059)
*Yueheng Li,Guangming Xie,Zongqing Lu*

Main category: cs.AI

TL;DR: MAGPO是一种新的多智能体强化学习框架，通过集中式训练与分散式执行的结合，解决了现有CTDE方法在集中式训练利用不足和缺乏理论保证的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CTDE方法在集中式训练利用不足且缺乏理论保证，MAGPO旨在通过集中式指导和分散式执行的结合来解决这些问题。

Method: MAGPO采用自回归联合策略进行可扩展的协调探索，并明确将其与分散策略对齐，以确保在部分可观测性下的可部署性。

Result: 在6个不同环境的43个任务中，MAGPO表现优于现有CTDE基线，并匹配或超越完全集中式方法。

Conclusion: MAGPO为分散式多智能体学习提供了一个有理论保证且实用的解决方案。

Abstract: Due to practical constraints such as partial observability and limited
communication, Centralized Training with Decentralized Execution (CTDE) has
become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning
(MARL). However, existing CTDE methods often underutilize centralized training
or lack theoretical guarantees. We propose Multi-Agent Guided Policy
Optimization (MAGPO), a novel framework that better leverages centralized
training by integrating centralized guidance with decentralized execution.
MAGPO uses an auto-regressive joint policy for scalable, coordinated
exploration and explicitly aligns it with decentralized policies to ensure
deployability under partial observability. We provide theoretical guarantees of
monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across
6 diverse environments. Results show that MAGPO consistently outperforms strong
CTDE baselines and matches or surpasses fully centralized approaches, offering
a principled and practical solution for decentralized multi-agent learning. Our
code and experimental data can be found in https://github.com/liyheng/MAGPO.

</details>


### [14] [AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/abs/2507.18074)
*Yixiu Liu,Yang Nan,Weixian Xu,Xiangkun Hu,Lyumanshan Ye,Zhen Qin,Pengfei Liu*

Main category: cs.AI

TL;DR: ASI-Arch是一种自主超级智能系统，用于AI研究中的神经架构发现，突破了人类认知限制，实现了从自动化优化到自动化创新的范式转变。


<details>
  <summary>Details</summary>
Motivation: AI研究受限于人类认知能力，发展速度受限，需要突破这一瓶颈。

Method: ASI-Arch通过自主假设、实现、训练和验证新型架构概念，进行端到端科学研究。

Result: 系统完成了1,773次实验，发现了106种创新的线性注意力架构，性能超越人类设计。

Conclusion: ASI-Arch为AI系统的自我加速研究提供了蓝图，并首次证明了科学发现的规模化计算定律。

Abstract: While AI systems demonstrate exponentially improving capabilities, the pace
of AI research itself remains linearly bounded by human cognitive capacity,
creating an increasingly severe development bottleneck. We present ASI-Arch,
the first demonstration of Artificial Superintelligence for AI research
(ASI4AI) in the critical domain of neural architecture discovery--a fully
autonomous system that shatters this fundamental constraint by enabling AI to
conduct its own architectural innovation. Moving beyond traditional Neural
Architecture Search (NAS), which is fundamentally limited to exploring
human-defined spaces, we introduce a paradigm shift from automated optimization
to automated innovation. ASI-Arch can conduct end-to-end scientific research in
the domain of architecture discovery, autonomously hypothesizing novel
architectural concepts, implementing them as executable code, training and
empirically validating their performance through rigorous experimentation and
past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000
GPU hours, culminating in the discovery of 106 innovative, state-of-the-art
(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed
unexpected strategic insights invisible to human players, our AI-discovered
architectures demonstrate emergent design principles that systematically
surpass human-designed baselines and illuminate previously unknown pathways for
architectural innovation. Crucially, we establish the first empirical scaling
law for scientific discovery itself--demonstrating that architectural
breakthroughs can be scaled computationally, transforming research progress
from a human-limited to a computation-scalable process. We provide
comprehensive analysis of the emergent design patterns and autonomous research
capabilities that enabled these breakthroughs, establishing a blueprint for
self-accelerating AI systems.

</details>


### [15] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: 提出了一种自动化临床数据管道的Agentic AI框架，通过模块化代理处理从数据摄入到推理的全流程，减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 医疗领域机器学习解决方案构建和部署成本高、流程复杂，需解决数据预处理碎片化、模型兼容性和隐私问题。

Method: 采用模块化代理系统，包括数据摄入、匿名化、特征提取、模型匹配、预处理推荐与实施、模型推理等步骤。

Result: 在老年医学、姑息治疗和结肠镜影像数据集上验证，自动化流程显著减少专家干预需求。

Conclusion: 该框架为临床环境中的AI应用提供了可扩展、经济高效的解决方案。

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [16] [Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes](https://arxiv.org/abs/2507.18123)
*Sedigh Khademi,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila,Jim Black*

Main category: cs.AI

TL;DR: 该研究利用自然语言处理（NLP）和主动学习技术，开发了一种分类器，用于从急诊科分诊笔记中快速检测潜在的疫苗安全问题，以弥补临床试验中安全数据收集时间不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于临床试验中安全数据收集时间有限，且疫苗广泛使用后需要快速监测安全问题，因此需要开发一种高效的方法来识别潜在的疫苗安全问题。

Method: 研究结合了自然语言处理（NLP）和主动学习技术，通过数据增强和主动学习优化标注过程，开发了一个分类器。

Result: 该分类器能够更准确、高效地从急诊科分诊笔记中检测疫苗安全问题，减少了假阳性并提高了模型性能。

Conclusion: 通过结合NLP和主动学习，该研究为疫苗安全监测提供了一种快速、高效的方法，弥补了传统方法的不足。

Abstract: The rapid development of COVID-19 vaccines has showcased the global
communitys ability to combat infectious diseases. However, the need for
post-licensure surveillance systems has grown due to the limited window for
safety data collection in clinical trials and early widespread implementation.
This study aims to employ Natural Language Processing techniques and Active
Learning to rapidly develop a classifier that detects potential vaccine safety
issues from emergency department notes. ED triage notes, containing expert,
succinct vital patient information at the point of entry to health systems, can
significantly contribute to timely vaccine safety signal surveillance. While
keyword-based classification can be effective, it may yield false positives and
demand extensive keyword modifications. This is exacerbated by the infrequency
of vaccination-related ED presentations and their similarity to other reasons
for ED visits. NLP offers a more accurate and efficient alternative, albeit
requiring annotated data, which is often scarce in the medical field. Active
learning optimizes the annotation process and the quality of annotated data,
which can result in faster model implementation and improved model performance.
This work combines active learning, data augmentation, and active learning and
evaluation techniques to create a classifier that is used to enhance vaccine
safety surveillance from ED triage notes.

</details>


### [17] [Logical Characterizations of GNNs with Mean Aggregation](https://arxiv.org/abs/2507.18145)
*Moritz Schönherr,Carsten Lutz*

Main category: cs.AI

TL;DR: 研究了使用均值聚合函数的图神经网络（GNNs）的表达能力，发现其在非均匀设置下与比率模态逻辑等价，表达力介于最大聚合和求和聚合之间；在均匀设置下，其表达力低于求和和最大聚合。


<details>
  <summary>Details</summary>
Motivation: 探索均值聚合GNNs的表达能力，比较其与其他聚合方式（如最大和求和）的差异。

Method: 在非均匀和均匀设置下分析均值聚合GNNs的表达能力，并与模态逻辑和MSO（Monadic Second-Order Logic）进行对比。

Result: 非均匀设置下，均值GNNs表达力与比率模态逻辑等价；均匀设置下，其表达力低于求和和最大聚合。

Conclusion: 均值聚合GNNs的表达力在不同设置下表现不同，且受假设条件影响。

Abstract: We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.

</details>


### [18] [Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory](https://arxiv.org/abs/2507.18178)
*Mutian Yang,Jiandong Gao,Ji Wu*

Main category: cs.AI

TL;DR: 该论文提出了一种认知归因框架，将大语言模型（LLMs）的知识和推理能力解耦，通过快速和慢速思维模式分析其贡献。


<details>
  <summary>Details</summary>
Motivation: 受双系统认知理论启发，旨在区分LLMs中知识和推理的作用，以提升模型分析、可解释性和开发能力。

Method: 通过快速思维（知识检索）和慢速思维（推理调整）两种认知模式，量化知识和推理的贡献，并在15个LLMs和3个数据集上验证。

Result: 发现推理调整具有领域特异性，参数扩展对知识和推理均有提升，且知识和推理分别集中在网络的不同层次。

Conclusion: 该框架为理解LLMs提供了新视角，并对扩展定律、知识编辑和小模型推理限制等研究提供了新见解。

Abstract: While large language models (LLMs) leverage both knowledge and reasoning
during inference, the capacity to distinguish between them plays a pivotal role
in model analysis, interpretability, and development. Inspired by dual-system
cognitive theory, we propose a cognition attribution framework to decouple the
contribution of knowledge and reasoning. In particular, the cognition of LLMs
is decomposed into two distinct yet complementary phases: knowledge retrieval
(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs
are prompted to generate answers under two different cognitive modes, fast
thinking and slow thinking, respectively. The performance under different
cognitive modes is analyzed to quantify the contribution of knowledge and
reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results
reveal: (1) reasoning adjustment is domain-specific, benefiting
reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and
potentially imparing knowledge-intensive domains. (2) Parameter scaling
improves both knowledge and reasoning, with knowledge improvements being more
pronounced. Additionally, parameter scaling make LLMs reasoning significantly
more prudent, while moderately more intelligent. (3) Knowledge primarily
resides in lower network layers, while reasoning operates in higher layers. Our
framework not only helps understand LLMs from a "decoupling" perspective, but
also provides new insights into existing research, including scaling laws,
hierarchical knowledge editing, and limitations of small-model reasoning.

</details>


### [19] [Comparing Non-minimal Semantics for Disjunction in Answer Set Programming](https://arxiv.org/abs/2507.18198)
*Felicidad Aguado,Pedro Cabalar,Brais Muñiz,Gilberto Pérez,Concepción Vidal*

Main category: cs.AI

TL;DR: 本文比较了四种不遵循模型最小化原则的析取语义，证明其中三种方法（Forks、Justified Models和DI语义的合理松弛）实际上是一致的，且比第四种方法（Strongly Supported Models）更强。


<details>
  <summary>Details</summary>
Motivation: 探讨非最小化析取语义的替代方案，比较不同方法的异同。

Method: 比较四种析取语义，包括Justified Models、Strongly Supported Models、Forks和DI语义，分析其定义和关系。

Result: 证明三种方法（Forks、Justified Models和DI语义的合理松弛）一致，且比Strongly Supported Models更强。

Conclusion: 这些方法为析取语义提供了新的视角，且某些方法在语义上更强大。

Abstract: In this paper, we compare four different semantics for disjunction in Answer
Set Programming that, unlike stable models, do not adhere to the principle of
model minimality. Two of these approaches, Cabalar and Mu\~niz' \emph{Justified
Models} and Doherty and Szalas' \emph{Strongly Supported Models}, directly
provide an alternative non-minimal semantics for disjunction. The other two,
Aguado et al's \emph{Forks} and Shen and Eiter's \emph{Determining Inference}
(DI) semantics, actually introduce a new disjunction connective, but are
compared here as if they constituted new semantics for the standard disjunction
operator. We are able to prove that three of these approaches (Forks, Justified
Models and a reasonable relaxation of the DI semantics) actually coincide,
constituting a common single approach under different definitions. Moreover,
this common semantics always provides a superset of the stable models of a
program (in fact, modulo any context) and is strictly stronger than the fourth
approach (Strongly Supported Models), that actually treats disjunctions as in
classical logic.

</details>


### [20] [Foundations for Risk Assessment of AI in Protecting Fundamental Rights](https://arxiv.org/abs/2507.18290)
*Antonino Rotolo,Beatrice Ferrigno,Jose Miguel Angel Garcia Godinez,Claudio Novelli,Giovanni Sartor*

Main category: cs.AI

TL;DR: 本文提出了一个定性风险评估框架，结合定义平衡和可废止推理，以解决欧盟AI法案中的法律合规和基本权利保护问题。


<details>
  <summary>Details</summary>
Motivation: 应对AI部署中的法律复杂性和基本权利冲突，提供一种动态的、可操作的风险评估方法。

Method: 采用定义平衡（比例分析）和可废止推理，分析AI部署场景及其对基本权利的多层次影响。

Result: 提出了一个分层的风险评估框架，适用于高风险AI系统和通用AI系统，并计划开发形式化模型和算法。

Conclusion: 该框架为AI风险评估提供了理论基础，未来将结合实践应用，支持负责任的AI治理。

Abstract: This chapter introduces a conceptual framework for qualitative risk
assessment of AI, particularly in the context of the EU AI Act. The framework
addresses the complexities of legal compliance and fundamental rights
protection by itegrating definitional balancing and defeasible reasoning.
Definitional balancing employs proportionality analysis to resolve conflicts
between competing rights, while defeasible reasoning accommodates the dynamic
nature of legal decision-making. Our approach stresses the need for an analysis
of AI deployment scenarios and for identifying potential legal violations and
multi-layered impacts on fundamental rights. On the basis of this analysis, we
provide philosophical foundations for a logical account of AI risk analysis. In
particular, we consider the basic building blocks for conceptually grasping the
interaction between AI deployment scenarios and fundamental rights,
incorporating in defeasible reasoning definitional balancing and arguments
about the contextual promotion or demotion of rights. This layered approach
allows for more operative models of assessment of both high-risk AI systems and
General Purpose AI (GPAI) systems, emphasizing the broader applicability of the
latter. Future work aims to develop a formal model and effective algorithms to
enhance AI risk assessment, bridging theoretical insights with practical
applications to support responsible AI governance.

</details>


### [21] [The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams](https://arxiv.org/abs/2507.18337)
*Peter Baumgartner,Lachlan McGinness*

Main category: cs.AI

TL;DR: 提出了一种自动批改物理考试的方法，结合计算机代数系统、SMT求解器和项重写系统，利用大语言模型处理学生答案，并通过自动推理技术评估正确性。


<details>
  <summary>Details</summary>
Motivation: 解决批改学生物理考试答案的挑战性问题，提高效率和准确性。

Method: 结合计算机代数系统、SMT求解器和项重写系统，利用大语言模型预处理学生答案，再通过自动推理技术评估正确性。

Result: 在2023年澳大利亚物理奥林匹克竞赛的1500多份真实学生答案上进行了评估。

Conclusion: 该方法有效解决了物理考试自动批改的难题，展示了实际应用的潜力。

Abstract: We present our method for automatically marking Physics exams. The marking
problem consists in assessing typed student answers for correctness with
respect to a ground truth solution. This is a challenging problem that we seek
to tackle using a combination of a computer algebra system, an SMT solver and a
term rewriting system. A Large Language Model is used to interpret and remove
errors from student responses and rewrite these in a machine readable format.
Once formalized and language-aligned, the next step then consists in applying
automated reasoning techniques for assessing student solution correctness. We
consider two methods of automated theorem proving: off-the-shelf SMT solving
and term rewriting systems tailored for physics problems involving
trigonometric expressions. The development of the term rewrite system and
establishing termination and confluence properties was not trivial, and we
describe it in some detail in the paper. We evaluate our system on a rich pool
of over 1500 real-world student exam responses from the 2023 Australian Physics
Olympiad.

</details>


### [22] [Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios](https://arxiv.org/abs/2507.18368)
*Zhuang Qiang Bok,Watson Wei Khong Chua*

Main category: cs.AI

TL;DR: ConDiFi是一个评估LLMs在金融任务中发散和收敛思维的基准，发现GPT-4o在创新性和可操作性上表现不佳，而DeepSeek-R1和Cohere Command R+表现优异。


<details>
  <summary>Details</summary>
Motivation: 金融领域需要LLMs不仅能做出最优决策，还需在不确定性下生成创造性未来情景，现有基准未能全面评估这些能力。

Method: ConDiFi包含607个发散思维提示和990个多步对抗性选择题，用于评估14个领先模型。

Result: GPT-4o在创新性和可操作性上表现不佳，而DeepSeek-R1和Cohere Command R+在生成可投资见解方面表现突出。

Conclusion: ConDiFi为评估LLMs在金融领域的安全和战略部署提供了新视角。

Abstract: Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step
logic. In finance, however, professionals must not only converge on optimal
decisions but also generate creative, plausible futures under uncertainty. We
introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent
thinking in LLMs for financial tasks.
  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990
multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we
evaluated 14 leading models and uncovered striking differences. Despite high
fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models
like DeepSeek-R1 and Cohere Command R+ rank among the top for generating
actionable, insights suitable for investment decisions. ConDiFi provides a new
perspective to assess reasoning capabilities essential to safe and strategic
deployment of LLMs in finance.

</details>


### [23] [Revisiting LLM Reasoning via Information Bottleneck](https://arxiv.org/abs/2507.18391)
*Shiye Lei,Zhihao Cheng,Kai Jia,Dacheng Tao*

Main category: cs.AI

TL;DR: 论文提出了一种基于信息瓶颈（IB）原则的理论框架IBRO，用于优化大语言模型（LLM）的推理能力，并通过轻量级的IB正则化方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLM推理方法缺乏理论支持，限制了方法的发展。

Method: 提出IB-aware reasoning optimization（IBRO）框架，推导出token级替代目标，并提出轻量级IB正则化方法。

Result: 在多个数学推理基准和RL算法中验证了IB正则化的有效性，性能一致提升。

Conclusion: IBRO框架为LLM推理提供了理论支持，轻量级IB正则化方法易于实现且高效。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in reasoning capabilities through reinforcement learning with verifiable
rewards (RLVR). By leveraging simple rule-based rewards, RL effectively
incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning
trajectories, progressively guiding them toward correct answers. However,
existing approaches remain largely heuristic and intuition-driven, limiting the
development of principled methodologies. In this paper, we present a
theoretical characterization of LLM reasoning grounded in information
bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),
a framework that encourages reasoning trajectories to be both informative about
the final correct answer and generalizable across diverse prompts. We derive a
practical token-level surrogate objective and propose an efficient
approximation, resulting in the lightweight IB regularization method. This
technique integrates seamlessly into existing RL-based post-training frameworks
without additional computational overhead, requiring only a one-line code
modification. Empirically, we validate IB regularization across multiple
mathematical reasoning benchmarks and RL algorithms, demonstrating consistent
improvements in LLM reasoning performance.

</details>


### [24] [Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation](https://arxiv.org/abs/2507.18398)
*Kwong Ho Li,Wathsala Karunarathne*

Main category: cs.AI

TL;DR: 论文研究了强化学习在呼叫中心路由优化中的应用，比较了基于模型的方法（VI）和无模型方法（PPO），PPO在测试中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 优化呼叫中心的路由策略，以减少客户等待时间和员工空闲时间。

Method: 比较了基于模型的值迭代（VI）和无模型的近端策略优化（PPO），使用MDP框架和SBR模型。

Result: PPO在1000次测试中表现最优，客户等待时间和员工空闲时间最低，但训练时间较长。

Conclusion: PPO在呼叫中心路由优化中效果显著，尽管训练成本较高。

Abstract: This paper investigates the application of Reinforcement Learning (RL) to
optimise call routing in call centres to minimise client waiting time and staff
idle time. Two methods are compared: a model-based approach using Value
Iteration (VI) under known system dynamics, and a model-free approach using
Proximal Policy Optimisation (PPO) that learns from experience. For the
model-based approach, a theoretical model is used, while a simulation model
combining Discrete Event Simulation (DES) with the OpenAI Gym environment is
developed for model-free learning. Both models frame the problem as a Markov
Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with
Poisson client arrivals and exponentially distributed service and abandonment
times. For policy evaluation, random, VI, and PPO policies are evaluated using
the simulation model. After 1,000 test episodes, PPO consistently achives the
highest rewards, along with the lowest client waiting time and staff idle time,
despite requiring longer training time.

</details>


### [25] [GPU Accelerated Compact-Table Propagation](https://arxiv.org/abs/2507.18413)
*Enrico Santi,Fabio Tardivo,Agostino Dovier,Andrea Formisano*

Main category: cs.AI

TL;DR: 本文探讨了如何利用现代GPU的计算能力增强Compact-Table（CT）算法，以处理大规模表约束问题。


<details>
  <summary>Details</summary>
Motivation: 传统的CPU方法难以有效处理具有数百或数千个有效案例的复杂表约束问题。

Method: 设计并实现了GPU加速的CT算法，并将其集成到现有约束求解器中。

Result: 通过实验验证了GPU加速CT在大量实例中的有效性。

Conclusion: GPU加速的CT算法为处理大规模表约束问题提供了高效解决方案。

Abstract: Constraint Programming developed within Logic Programming in the Eighties;
nowadays all Prolog systems encompass modules capable of handling constraint
programming on finite domains demanding their solution to a constraint solver.
This work focuses on a specific form of constraint, the so-called table
constraint, used to specify conditions on the values of variables as an
enumeration of alternative options. Since every condition on a set of finite
domain variables can be ultimately expressed as a finite set of cases, Table
can, in principle, simulate any other constraint. These characteristics make
Table one of the most studied constraints ever, leading to a series of
increasingly efficient propagation algorithms. Despite this, it is not uncommon
to encounter real-world problems with hundreds or thousands of valid cases that
are simply too many to be handled effectively with standard CPU-based
approaches. In this paper, we deal with the Compact-Table (CT) algorithm, the
state-of-the-art propagation algorithms for Table. We describe how CT can be
enhanced by exploiting the massive computational power offered by modern GPUs
to handle large Table constraints. In particular, we report on the design and
implementation of GPU-accelerated CT, on its integration into an existing
constraint solver, and on an experimental validation performed on a significant
set of instances.

</details>


### [26] [On the Performance of Concept Probing: The Influence of the Data (Extended Version)](https://arxiv.org/abs/2507.18550)
*Manuel de Sousa Ribeiro,Afonso Leote,João Leite*

Main category: cs.AI

TL;DR: 研究探讨了概念探测中训练数据对性能的影响，并提供了两个常用数据集的标签。


<details>
  <summary>Details</summary>
Motivation: 概念探测帮助解释神经网络，但现有研究忽视了训练数据对探测模型的影响。

Method: 在图像分类任务中，分析训练数据对概念探测模型性能的作用。

Result: 发现训练数据对探测模型性能有显著影响，并公开了两个数据集的标签。

Conclusion: 强调了训练数据在概念探测中的重要性，为未来研究提供了资源。

Abstract: Concept probing has recently garnered increasing interest as a way to help
interpret artificial neural networks, dealing both with their typically large
size and their subsymbolic nature, which ultimately renders them unfeasible for
direct human interpretation. Concept probing works by training additional
classifiers to map the internal representations of a model into human-defined
concepts of interest, thus allowing humans to peek inside artificial neural
networks. Research on concept probing has mainly focused on the model being
probed or the probing model itself, paying limited attention to the data
required to train such probing models. In this paper, we address this gap.
Focusing on concept probing in the context of image classification tasks, we
investigate the effect of the data used to train probing models on their
performance. We also make available concept labels for two widely used
datasets.

</details>


### [27] [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
*Shanghai AI Lab,:,Yicheng Bao,Guanxu Chen,Mingkang Chen,Yunhao Chen,Chiyu Chen,Lingjie Chen,Sirui Chen,Xinquan Chen,Jie Cheng,Yu Cheng,Dengke Deng,Yizhuo Ding,Dan Ding,Xiaoshan Ding,Yi Ding,Zhichen Dong,Lingxiao Du,Yuyu Fan,Xinshun Feng,Yanwei Fu,Yuxuan Gao,Ruijun Ge,Tianle Gu,Lujun Gui,Jiaxuan Guo,Qianxi He,Yuenan Hou,Xuhao Hu,Hong Huang,Kaichen Huang,Shiyang Huang,Yuxian Jiang,Shanzhe Lei,Jie Li,Lijun Li,Hao Li,Juncheng Li,Xiangtian Li,Yafu Li,Lingyu Li,Xueyan Li,Haotian Liang,Dongrui Liu,Qihua Liu,Zhixuan Liu,Bangwei Liu,Huacan Liu,Yuexiao Liu,Zongkai Liu,Chaochao Lu,Yudong Lu,Xiaoya Lu,Zhenghao Lu,Qitan Lv,Caoyuan Ma,Jiachen Ma,Xiaoya Ma,Zhongtian Ma,Lingyu Meng,Ziqi Miao,Yazhe Niu,Yuezhang Peng,Yuan Pu,Han Qi,Chen Qian,Xingge Qiao,Jingjing Qu,Jiashu Qu,Wanying Qu,Wenwen Qu,Xiaoye Qu,Qihan Ren,Qingnan Ren,Qingyu Ren,Jing Shao,Wenqi Shao,Shuai Shao,Dongxing Shi,Xin Song,Xinhao Song,Yan Teng,Xuan Tong,Yingchun Wang,Xuhong Wang,Shujie Wang,Xin Wang,Yige Wang,Yixu Wang,Yuanfu Wang,Futing Wang,Ruofan Wang,Wenjie Wang,Yajie Wang,Muhao Wei,Xiaoyu Wen,Fenghua Weng,Yuqi Wu,Yingtong Xiong,Xingcheng Xu,Chao Yang,Yue Yang,Yang Yao,Yulei Ye,Zhenyun Yin,Yi Yu,Bo Zhang,Qiaosheng Zhang,Jinxuan Zhang,Yexin Zhang,Yinqiang Zheng,Hefeng Zhou,Zhanhui Zhou,Pengyu Zhu,Qingzi Zhu,Yubo Zhu,Bowen Zhou*

Main category: cs.AI

TL;DR: SafeWork-R1是一种多模态推理模型，通过SafeLadder框架实现能力与安全的协同进化，显著提升安全性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有对齐方法（如RLHF）仅学习人类偏好而缺乏内在安全推理能力的问题。

Method: 采用SafeLadder框架，结合大规模渐进式安全导向的强化学习后训练和多原则验证器。

Result: 在安全相关基准上平均提升46.54%，优于GPT-4.1和Claude Opus 4，且不损害通用能力。

Conclusion: SafeLadder框架证明安全与能力可协同进化，为构建可靠、可信的通用AI提供通用方案。

Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [28] [Action-List Reinforcement Learning Syndrome Decoding for Binary Linear Block Codes](https://arxiv.org/abs/2507.17893)
*Milad Taghipour,Bane Vasic*

Main category: cs.IT

TL;DR: 该论文探讨了如何利用强化学习技术提升线性分组码的解码性能，通过翻转比特和优化决策。提出了将迭代解码过程映射为马尔可夫决策过程（MDP）的方法，并设计了多种减少状态数的策略。实验结果表明，所提方法在低密度奇偶校验码（LDPC）上表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升线性分组码的解码性能，尤其是通过强化学习优化解码过程，以应对复杂性和性能的挑战。

Method: 1. 将解码过程映射为MDP；2. 提出截断MDP以减少状态数；3. 设计基于Deep-Q网络的动作列表解码器；4. 利用代码的自同构群；5. 提出反馈方法增强现有解码器。

Result: 实验证明，所提方法在LDPC码上显著提升了性能，同时降低了复杂度。

Conclusion: 强化学习方法可有效提升解码性能，且通过状态减少和反馈机制进一步优化了复杂度。

Abstract: This paper explores the application of reinforcement learning techniques to
enhance the performance of decoding of linear block codes based on flipping
bits and finding optimal decisions. We describe the methodology for mapping the
iterative decoding process into Markov Decision Processes (MDPs) and propose
different methods to reduce the number of states in the MDP. A truncated MDP is
proposed to reduce the number of states in the MDP by learning a Hamming ball
with a specified radius around codewords. We then propose a general scheme for
reinforcement learning based decoders applicable to any class of codes to
improve the performance of decoders. We call this scheme an action-list
decoding. We design an action-list decoder based on the Deep-Q network values
that substantially enhance performance. We also get benefit of automorphism
group of code to further improve the code performance. Additionally, we propose
a feedback-based method to exploit and enhance the performance of existing
high-performing decoders by applying reinforcement learning algorithms after
the existing decoders. These approaches effectively reduces the complexity of
the reinforcement learning block. Finally, we present experimental results for
the Low-Density Parity Check (LDPC) codes over the Binary Symmetric Channel
(BSC) to demonstrate the efficiency of the proposed methods.

</details>


### [29] [Minimax Data Sanitization with Distortion Constraint and Adversarial Inference](https://arxiv.org/abs/2507.17942)
*Amirarsalan Moatazedian,Yauhen Yakimenka,Rémi A. Chou,Jörg Kliewer*

Main category: cs.IT

TL;DR: 研究隐私保护数据共享场景，通过优化重构器和对手的损失函数，实现数据共享与隐私保护的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决在数据共享中如何保护隐私的问题，同时确保授权重构器能准确重构数据，而未经授权的对手无法单独准确估计数据。

Method: 提出一种基于约束的极小极大优化问题，采用交替更新策略训练私有化器、重构器和对手。

Result: 在特定高斯和二元场景下获得理论最优解，并验证了所提方法的有效性。

Conclusion: 该方法在隐私保护和数据共享之间实现了有效平衡，为类似问题提供了理论框架和实用工具。

Abstract: We study a privacy-preserving data-sharing setting where a privatizer
transforms private data into a sanitized version observed by an authorized
reconstructor and two unauthorized adversaries, each with access to side
information correlated with the private data.
  The reconstructor is evaluated under a distortion function, while each
adversary is evaluated using a separate loss function. The privatizer ensures
the reconstructor distortion remains below a fixed threshold while maximizing
the minimum loss across the two adversaries. This two-adversary setting models
cases where individual users cannot reconstruct the data accurately, but their
combined side information enables estimation within the distortion threshold.
The privatizer maximizes individual loss while permitting accurate
reconstruction only through collaboration. This echoes secret-sharing
principles, but with lossy rather than perfect recovery. We frame this as a
constrained data-driven minimax optimization problem and propose a data-driven
training procedure that alternately updates the privatizer, reconstructor, and
adversaries. We also analyze the Gaussian and binary cases as special scenarios
where optimal solutions can be obtained. These theoretical optimal results are
benchmarks for evaluating the proposed minimax training approach.

</details>


### [30] [Deep Learning-based Position-domain Channel Extrapolation for Cell-Free Massive MIMO](https://arxiv.org/abs/2507.17950)
*Jiajia Guo,Chao-Kai Wen,Xiao Li,Shi Jin*

Main category: cs.IT

TL;DR: 提出了一种基于深度学习的PCEnet框架，利用用户位置信息优化无蜂窝大规模MIMO系统中的信道获取性能。


<details>
  <summary>Details</summary>
Motivation: 减少信道获取开销，利用用户位置信息提升信道获取效率。

Method: 通过神经网络从获取的信道推断用户位置，利用位置信息设计导频符号并重构其他信道。

Result: 仿真结果显示，PCEnet框架可将导频和反馈开销减少高达50%。

Conclusion: PCEnet框架通过位置信息显著提升了信道获取性能，同时简化了流程并降低了延迟。

Abstract: To reduce channel acquisition overhead, spatial, time, and frequency-domain
channel extrapolation techniques have been widely studied. In this paper, we
propose a novel deep learning-based Position-domain Channel Extrapolation
framework (named PCEnet) for cell-free massive multiple-input multiple-output
(MIMO) systems. The user's position, which contains significant channel
characteristic information, can greatly enhance the efficiency of channel
acquisition. In cell-free massive MIMO, while the propagation environments
between different base stations and a specific user vary and their respective
channels are uncorrelated, the user's position remains constant and unique
across all channels. Building on this, the proposed PCEnet framework leverages
the position as a bridge between channels to establish a mapping between the
characteristics of different channels, thereby using one acquired channel to
assist in the estimation and feedback of others. Specifically, this approach
first utilizes neural networks (NNs) to infer the user's position from the
obtained channel. {The estimated position, shared among BSs through a central
processing unit (CPU)}, is then fed into an NN to design pilot symbols and
concatenated with the feedback information to the channel reconstruction NN to
reconstruct other channels, thereby significantly enhancing channel acquisition
performance. Additionally, we propose a simplified strategy where only the
estimated position is used in the reconstruction process without modifying the
pilot design, thereby reducing latency. Furthermore, we introduce a position
label-free approach that infers the relative user position instead of the
absolute position, eliminating the need for ground truth position labels during
the localization NN training. Simulation results demonstrate that the proposed
PCEnet framework reduces pilot and feedback overheads by up to 50%.

</details>


### [31] [A Novel Coded Computing Approach for Distributed Multi-Task Learning](https://arxiv.org/abs/2507.18025)
*Minquan Cheng,Yongkang Wang,Lingyu Zhang,Youlong Wu*

Main category: cs.IT

TL;DR: 提出了一种新的编码分布式多任务学习（DMTL）方案，显著降低通信成本，并在理论和实践中均达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 在大规模分布式多任务学习中，通信瓶颈严重限制系统性能，尤其是在异构数据放置场景下。

Method: 将通信过程建模为矩阵分解问题，并提出编码DMTL方案，优化上下行链路的编码矩阵设计。

Result: 理论分析表明，该方案在温和条件下达到通信开销的理论下限，适用于同构和异构计算环境。

Conclusion: 该方案不仅解决了DMTL中的通信瓶颈问题，还可扩展到其他分布式线性可分离计算问题。

Abstract: Distributed multi-task learning (DMTL) effectively improves model
generalization performance through the collaborative training of multiple
related models. However, in large-scale learning scenarios, communication
bottlenecks severely limit practical system performance. In this paper, we
investigate the communication bottleneck within a typical DMTL system that
employs non-linear global updates. This system involves distributed workers,
assisted by a central server, who collaboratively learn distinct models derived
from a non-linear aggregation of their local model parameters. We first
characterize the communication process as a matrix decomposition problem. It
transforms workers' data storage constraints into structural characteristics of
the uplink encoding matrix, and worker data retrieval demands into Maximum
Distance Separable (MDS) properties of the downlink encoding matrix. Building
on this, we propose a novel coded DTML scheme that can greatly reduce the
communication cost of the DTML with heterogeneous data placement. Theoretical
analysis demonstrates that the proposed scheme achieves the theoretical lower
bound for communication overhead under mild conditions. Remarkably, this
optimality holds for both traditional homogeneous computing environments and
various heterogeneous scenarios. Furthermore, our scheme is extensible to a
distributed linearly separable computation problem where the target function
involves multiple linear combinations of local update values. This indicates
that our scheme offers a new way of tackling heterogeneous data placement
challenges in various distributed applications.

</details>


### [32] [On the Role of Age and Semantics of Information in Remote Estimation of Markov Sources](https://arxiv.org/abs/2507.18514)
*Jiping Luo,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 本文研究了有限状态马尔可夫链的语义感知远程估计，提出了一种结合AoCE和AoI的传输策略优化方法，并开发了高效算法Insec-SPI。


<details>
  <summary>Details</summary>
Motivation: 优化有限状态马尔可夫链的远程估计性能，同时满足传输频率约束。

Method: 采用最大后验概率（MAP）估计器，结合AoCE和AoI指标，将问题建模为约束马尔可夫决策过程（CMDP），并提出混合策略。

Result: 证明了最优简单混合策略的存在性，并开发了高效算法Insec-SPI，显著提升了估计质量。

Conclusion: 结合AoCE和AoI的传输策略优于单独使用任一指标，验证了方法的有效性。

Abstract: This paper investigates the semantics-aware remote estimation of a
finite-state Markov chain. We employ the maximum a posteriori (MAP) estimator
and aim to devise a transmission policy to optimize estimation performance
subject to a transmission frequency constraint. We leverage two metrics, namely
the Age of Consecutive Error (AoCE) and the Age of Information (AoI), to
quantify, respectively, the significance of estimation error at the transmitter
and the predictability of outdated information at the receiver. The optimal
transmission problem is formulated as a constrained Markov decision process
(CMDP) with unbounded costs. We show the existence of an optimal simple mixture
policy, which randomly selects between two deterministic switching policies
with a fixed probability. Notably, each switching policy triggers a
transmission only when the AoCE exceeds a threshold value that depends on both
the AoI and the instantaneous estimation error. We further derive sufficient
conditions under which the switching policy reduces to a simple threshold
policy; that is, it admits identical thresholds for all estimation errors.
Leveraging these results, we develop an efficient structure-aware algorithm,
Insec-SPI, that computes the optimal policy with reduced computation overhead.
Our results demonstrate that incorporating both AoI and AoCE yields
significantly improved estimation quality compared to using either metric
alone.

</details>


### [33] [Covert Communications in MEC-Based Networked ISAC Systems Towards Low-Altitude Economy](https://arxiv.org/abs/2507.18194)
*Weihao Mao,Yang Lu,Bo Ai,Tony Q. S. Quek*

Main category: cs.IT

TL;DR: 该论文研究了基于移动边缘计算（MEC）的网络化ISAC系统中的隐蔽传输设计，旨在优化无人机（UAV）的通信、感知和计算资源以及轨迹，以最小化总能耗。


<details>
  <summary>Details</summary>
Motivation: 低空经济（LAE）依赖ISAC、MEC和隐蔽通信，但现有系统在协调通信、感知和计算资源方面存在挑战，需要优化设计以提高效率和隐蔽性。

Method: 通过交替优化算法分解问题，分别优化通信、感知、计算资源和无人机轨迹，采用逐次凸近似和信任域算法求解。

Result: 仿真验证了算法的有效性，揭示了通信、感知和计算之间的权衡关系。

Conclusion: 提出的算法在优化LAE系统性能方面具有显著优势，为未来研究提供了实用框架。

Abstract: Low-altitude economy (LAE) is an emerging business model, which heavily
relies on integrated sensing and communications (ISAC), mobile edge computing
(MEC), and covert communications. This paper investigates the convert
transmission design in MEC-based networked ISAC systems towards LAE, where an
MEC server coordinates multiple access points to simultaneously receive
computation tasks from multiple unmanned aerial vehicles (UAVs), locate a
target in a sensing area, and maintain UAVs' covert transmission against
multiple wardens. We first derive closed-form expressions for the detection
error probability (DEP) at wardens. Then, we formulate a total energy
consumption minimization problem by optimizing communication, sensing, and
computation resources as well as UAV trajectories, subject to the requirements
on quality of MEC services, DEP, and radar signal-to-interference-and-noise
ratio, and the causality of UAV trajectories. An alternating optimization based
algorithm is proposed to handle the considered problem, which decomposes it
into two subproblems: joint optimization of communication, sensing, and
computation resources, and UAV trajectory optimization. The former is addressed
by a successive convex approximation based algorithm, while the latter is
solved via a trust-region based algorithm. Simulations validate the
effectiveness of the proposed algorithm compared with various benchmarks, and
reveal the trade-offs among communication, sensing, and computation in LAE
systems.

</details>


### [34] [Hermitian hull of some GRS codes and new EAQMDS codes](https://arxiv.org/abs/2507.18361)
*Oisin Campion,Rodrigo San-José*

Main category: cs.IT

TL;DR: 研究了广义Reed-Solomon码的Hermitian壳的维度问题，通过将其转化为格点计数问题，给出了壳维度的显式公式，并确定了纠缠辅助量子纠错码所需的最大纠缠对数。


<details>
  <summary>Details</summary>
Motivation: 探索广义Reed-Solomon码的Hermitian壳维度，以优化纠缠辅助量子纠错码的设计。

Method: 将壳维度问题转化为格点计数问题，并求解该问题。

Result: 提供了壳维度的显式公式，确定了纠缠辅助量子纠错码所需的最大纠缠对数。

Conclusion: 该方法灵活，可用于构建多种纠缠辅助量子MDS码，并获得新的参数。

Abstract: We study the Hermitian hull of a particular family of generalized
Reed-Solomon codes. The problem of computing the dimension of the hull is
translated to a counting problem in a lattice. By solving this problem, we
provide explicit formulas for the dimension of the hull, which determines the
minimum number required of maximally entangled pairs for the associated
entanglement-assisted quantum error-correcting codes. This flexible
construction allows to obtain a wide range of entanglement-assisted quantum MDS
codes, as well as new parameters.

</details>


### [35] [AI/ML Life Cycle Management for Interoperable AI Native RAN](https://arxiv.org/abs/2507.18538)
*Chu-Hsiang Huang,Chao-Kai Wen,Geoffrey Ye Li*

Main category: cs.IT

TL;DR: 论文探讨了AI/ML在5G RAN中的应用及标准化生命周期管理框架的进展，提出了LCM架构和挑战。


<details>
  <summary>Details</summary>
Motivation: AI/ML在5G RAN中的应用面临模型漂移、供应商锁定和透明度不足等问题，需标准化LCM框架以推动大规模采用。

Method: 3GPP从Rel-16到Rel-20逐步引入AI/ML标准化功能，包括模型传输、执行、性能监控和闭环控制。

Result: 提出了五块LCM架构、KPI驱动监控机制和跨供应商协作方案，但仍存在资源高效监控等挑战。

Conclusion: 这些发展为6G的AI原生收发器奠定了基础。

Abstract: Artificial intelligence (AI) and machine learning (ML) models are rapidly
permeating the 5G Radio Access Network (RAN), powering beam management, channel
state information (CSI) feedback, positioning, and mobility prediction.
However, without a standardized life-cycle management (LCM) framework,
challenges, such as model drift, vendor lock-in, and limited transparency,
hinder large-scale adoption. 3GPP Releases 16-20 progressively evolve AI/ML
from experimental features to managed, interoperable network functions.
Beginning with the Network Data Analytics Function (NWDAF) in Rel-16,
subsequent releases introduced standardized interfaces for model transfer,
execution, performance monitoring, and closed-loop control, culminating in
Rel-20's two-sided CSI-compression Work Item and vendor-agnostic LCM profile.
This article reviews the resulting five-block LCM architecture, KPI-driven
monitoring mechanisms, and inter-vendor collaboration schemes, while
identifying open challenges in resource-efficient monitoring, environment drift
detection, intelligent decision-making, and flexible model training. These
developments lay the foundation for AI-native transceivers as a key enabler for
6G.

</details>
