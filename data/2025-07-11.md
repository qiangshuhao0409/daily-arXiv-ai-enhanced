<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning](https://arxiv.org/abs/2507.07118)
*Zelin Zhu,Kai Yang,Rui Zhang*

Main category: cs.NI

TL;DR: 论文提出了一种联合优化无线定位与感知的方法，利用MIMO-OFDM系统的高维CSI特性，通过深度学习实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 无线定位与感知技术在现代网络中至关重要，但现有方法在高维CSI特性下的联合建模研究不足。

Method: 提出SPG-MIBO算法，结合混合整数双层深度学习和随机近端梯度优化，适用于高维大规模数据。

Result: 实验验证了算法的有效性，并展示了联合优化的性能提升。

Conclusion: SPG-MIBO算法为高维CSI环境下的定位与感知任务提供了高效解决方案。

Abstract: Wireless localization and sensing technologies are essential in modern
wireless networks, supporting applications in smart cities, the Internet of
Things (IoT), and autonomous systems. High-performance localization and sensing
systems are critical for both network efficiency and emerging intelligent
applications. Integrating channel state information (CSI) with deep learning
has recently emerged as a promising solution. Recent works have leveraged the
spatial diversity of multiple input multiple output (MIMO) systems and the
frequency granularity of orthogonal frequency division multiplexing (OFDM)
waveforms to improve spatial resolution. Nevertheless, the joint modeling of
localization and sensing under the high-dimensional CSI characteristics of
MIMO-OFDM systems remains insufficiently investigated. This work aims to
jointly model and optimize localization and sensing tasks to harness their
potential synergy. We first formulate localization and sensing as a
mixed-integer bilevel deep learning problem and then propose a novel stochastic
proximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)
algorithm. SPG-MIBO is well-suited for high-dimensional and large-scale
datasets, leveraging mini-batch training at each step for computational and
memory efficiency. The algorithm is also supported by theoretical convergence
guarantees. Extensive experiments on multiple datasets validate its
effectiveness and highlight the performance gains from joint localization and
sensing optimization.

</details>


### [2] [DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training](https://arxiv.org/abs/2507.07149)
*Renyuan Liu,Yuyang Leng,Kaiyan Liu,Shaohan Hu,Chun-Fu,Chen,Peijun Zhao,Heechul Yun,Shuochao Yao*

Main category: cs.NI

TL;DR: 论文提出了一种动态激活框架（DAF），通过系统级优化实现高效的内存和时间动态量化训练，解决了移动和边缘设备上的内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 移动和边缘设备的内存限制是深度学习训练的主要瓶颈，激活压缩在不影响精度的情况下成为关键挑战。现有动态量化方法因系统级问题（如计算开销和内存碎片）难以实际部署。

Method: DAF通过混合归约操作、CPU-GPU协作位压缩和重要性感知分页内存管理，优化系统瓶颈，实现高效动态量化训练。

Result: 实验表明，DAF在多种深度学习模型上实现了22.9倍内存节省和3.2倍加速，且不影响训练精度。

Conclusion: DAF为资源受限环境提供了一种可扩展且实用的解决方案。

Abstract: Recent advancements in on-device training for deep neural networks have
underscored the critical need for efficient activation compression to overcome
the memory constraints of mobile and edge devices. As activations dominate
memory usage during training and are essential for gradient computation,
compressing them without compromising accuracy remains a key research
challenge. While existing methods for dynamic activation quantization promise
theoretical memory savings, their practical deployment is impeded by
system-level challenges such as computational overhead and memory
fragmentation.
  To address these challenges, we introduce DAF, a Dynamic Activation Framework
that enables scalable and efficient on-device training through system-level
optimizations. DAF achieves both memory- and time-efficient dynamic
quantization training by addressing key system bottlenecks. It develops hybrid
reduction operations tailored to the memory hierarchies of mobile and edge
SoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic
quantization, and implements an importance-aware paging memory management
scheme to reduce fragmentation and support dynamic memory adjustments.
  These optimizations collectively enable DAF to achieve substantial memory
savings and speedup without compromising model training accuracy. Evaluations
on various deep learning models across embedded and mobile platforms
demonstrate up to a $22.9\times$ reduction in memory usage and a $3.2\times$
speedup, making DAF a scalable and practical solution for resource-constrained
environments.

</details>


### [3] [PHandover: Parallel Handover in Mobile Satellite Network](https://arxiv.org/abs/2507.07437)
*Jiasheng Wu,Shaojie Su,Wenjun Zhu,Xiong Wang,Jingjing Zhang,Xingqiu He,Yue Gao*

Main category: cs.NI

TL;DR: 论文提出了一种并行切换机制，显著降低LEO卫星网络中的切换延迟，通过基于计划的切换和机器学习预测信号强度，实验显示延迟减少21倍。


<details>
  <summary>Details</summary>
Motivation: 解决LEO卫星网络中因高速移动导致的频繁高延迟切换问题，提升延迟敏感应用的性能。

Method: 采用基于计划的切换替代基于测量的切换，引入卫星同步功能（SSF），并结合机器学习模型和高效切换调度算法。

Result: 实验表明，提出的切换方案比标准NTN方案延迟减少21倍，网络稳定性和用户性能显著提升。

Conclusion: 提出的并行切换机制有效解决了LEO卫星网络中的高延迟切换问题，具有实际应用潜力。

Abstract: The construction of Low Earth Orbit (LEO) satellite constellations has
recently attracted tremendous attention from both academia and industry. The 5G
and 6G standards have identified LEO satellite networks as a key component of
future mobile networks. However, due to the high-speed movement of satellites,
ground terminals often experience frequent and high-latency handovers, which
significantly deteriorate the performance of latency-sensitive applications. To
address this challenge, we propose a parallel handover mechanism for mobile
satellite networks that can considerably reduce handover latency. The main idea
is to employ plan-based handovers instead of measurement-based handovers to
avoid interactions between the access and core networks, thereby eliminating
the significant time overhead associated with traditional handover procedures.
Specifically, we introduce a novel network function named the Satellite
Synchronized Function (SSF), which is designed to be fully compliant with the
standard 5G core network. In addition, we propose a machine learning model for
signal strength prediction, coupled with an efficient handover scheduling
algorithm. We have conducted extensive experiments, and the results demonstrate
that our proposed handover scheme can reduce handover latency by 21\times
compared to the standard NTN handover scheme and two other existing handover
approaches, along with significant improvements in network stability and
user-level performance.

</details>


### [4] [Energy Transfer and Data Collection from Batteryless Sensors in Low-altitude Wireless Networks](https://arxiv.org/abs/2507.07481)
*Wen Zhang,Aimin Wang,Jiahui Li,Geng Sun,Jiacheng Wang,Weijie Yuan,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出了一种无人机辅助的无电池传感器网络数据收集与无线能量传输框架，通过多目标优化和强化学习算法提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决高温等极端环境下传统无线能量传输和电池技术的局限性。

Method: 无人机通过无线能量传输为无电池传感器供电，并优化能量分配与飞行轨迹；采用改进的强化学习算法（SAC-PPV）解决非凸动态问题。

Result: 仿真结果表明，该方法在各种网络配置下均优于基准算法。

Conclusion: 无人机辅助框架结合强化学习算法，为极端环境下的传感器网络提供了高效解决方案。

Abstract: The integration of wireless power transfer (WPT) with Internet of Things
(IoT) offers promising solutions for sensing applications, but faces
significant challenges when deployed in hard-to-access areas such as
high-temperature environments. In such extreme conditions, traditional fixed
WPT infrastructure cannot be safely installed, and batteries rapidly degrade
due to hardware failures. In this paper, we propose an uncrewed aerial vehicle
(UAV)-assisted data collection and WPT framework for batteryless sensor (BLS)
networks deployed in these challenging environments. Specifically, we consider
a practical scenario where a UAV first transfers energy to BLS nodes via WPT,
enabling these nodes to subsequently transmit their collected data to the UAV
through orthogonal frequency-division multiple access (OFDMA). Then, we
formulate a multi-objective optimization problem that aims to maximize the fair
data collection volume while minimizing the UAV energy consumption through
joint optimization of transmit power allocation and flight trajectory planning.
Due to the non-convex nature and dynamic characteristics of this problem,
conventional optimization methods prove inadequate. To address these
challenges, we propose an enhanced soft actor-critic algorithm with
parameter-free attention, prioritized experience replay, and value-based reward
centering (SAC-PPV), thereby improving the exploration efficiency and learning
stability of the algorithm in complex WPT scenarios. Simulation results
demonstrate that the proposed approach consistently outperforms benchmark
algorithms under various network configurations.

</details>


### [5] [A Fragmentation-Aware Adaptive Bilevel Search Framework for Service Mapping in Computing Power Networks](https://arxiv.org/abs/2507.07535)
*Jingzhao Xie,Zhenglian Li,Gang Sun,Long Luo,Hongfang Yu,Dusit Niyato*

Main category: cs.NI

TL;DR: 论文提出了一种名为ABS的自适应双层搜索框架，用于优化计算能力网络（CPN）中的服务映射问题，显著提高了资源利用率和服务接受率。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能完全实现CPN中通过网络协调整合计算资源的愿景，特别是在优化服务映射以提升资源效率和服务满意度方面存在挑战。

Method: 论文定义了CPN中的服务映射问题，提出了ABS框架，包括基于图划分的重构、双层优化架构和碎片感知评估。

Result: ABS在多种CPN场景中表现优异，计算资源利用率提高了73.2%，服务接受率提高了60.2%。

Conclusion: ABS框架有效解决了CPN中的服务映射问题，显著提升了资源利用和服务质量。

Abstract: Computing Power Network (CPN) unifies wide-area computing resources through
coordinated network control, while cloud-native abstractions enable flexible
resource orchestration and on-demand service provisioning atop the elastic
infrastructure CPN provides. However, current approaches fall short of fully
integrating computing resources via network-enabled coordination as envisioned
by CPN. In particular, optimally mapping services to an underlying
infrastructure to maximize resource efficiency and service satisfaction remains
challenging. To overcome this challenge, we formally define the service mapping
problem in CPN, establish its theoretical intractability, and identify key
challenges in practical optimization. We propose Adaptive Bilevel Search (ABS),
a modular framework featuring (1) graph partitioning-based reformulation to
capture variable coupling, (2) a bilevel optimization architecture for
efficient global exploration with local optimality guarantees, and (3)
fragmentation-aware evaluation for global performance guidance. Implemented
using distributed particle swarm optimization, ABS is extensively evaluated
across diverse CPN scenarios, consistently outperforming existing approaches.
Notably, in complex scenarios, ABS achieves up to 73.2% higher computing
resource utilization and a 60.2% higher service acceptance ratio compared to
the best-performing baseline.

</details>


### [6] [HaLert: A Resilient Smart City Architecture for Post-Disaster Based on Wi-Fi HaLow Mesh and SDN](https://arxiv.org/abs/2507.07841)
*Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.NI

TL;DR: HaLert是一个基于Wi-Fi HaLow IEEE 802.11s网状网络的弹性架构，用于灾难后的应急通信系统，支持多种消息类型交换，并整合了SDN和LoRa技术。


<details>
  <summary>Details</summary>
Motivation: 灾难事件通常不可预测，利用现有基础设施（如智能城市的物联网网络）开发应急通信系统，以减少对民众通信能力的影响。

Method: 提出HaLert架构，结合Wi-Fi HaLow网状网络和SDN技术，支持远程监控和配置，并通过LoRa网络实现控制。开发原型并在真实城市环境中测试。

Result: Wi-Fi HaLow网络在障碍物和地形影响下仍保持稳定（延迟15-54.8 ms，上传速率134-726 Kbps，下载速率117-682 Kbps）。LoRa网络消息成功率达94.96%。

Conclusion: HaLert架构在灾难场景下表现出良好的弹性和功能性，适合作为应急通信系统。

Abstract: Events such as catastrophes and disasters are, in most cases, unpredictable.
Consequently, reusing existing infrastructures to develop alternative
communication strategies after disasters is essential to minimise the impact of
these events on the population's ability to communicate and promptly receive
alerts from authorities. In this context, the emergence of smart cities,
characterised by dense and geographically distributed IoT networks, presents
significant potential for such reuse. This work proposes HaLert, a resilient
architecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network,
whose resources can be readily reallocated to support a emergency communication
system to exchange messages (including text, location, image, audio, and video)
between citizens, authorities, and between both parties. To facilitate remote
monitoring and configuration of the network, the architecture incorporates the
SDN (Software-Defined Networking) paradigm, supported by a LoRa controlled
flooding mesh network. A prototype was developed based on this architecture and
tested in a real urban scenario comprising both indoor and outdoor
environments. The results demonstrated that, despite the significant impact of
obstacles, lack of line-of-sight, and terrain slopes on the latency (average
latency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and
726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow
network, it remained stable and resilient, successfully providing all
functionalities associated with the HaLert architecture. The tests conducted on
the LoRa network revealed a high average message success rate of 94.96%.

</details>


### [7] [Can cloud-based VR streaming handle Wi-Fi OBSS contention?](https://arxiv.org/abs/2507.07677)
*Miguel Casasnovas,Marc Carrascosa-Zamacois,Boris Bellalta*

Main category: cs.NI

TL;DR: 研究分析了Wi-Fi网络中重叠信道对VR流媒体的负面影响，发现不同重叠情况对性能的影响不同，并提出NeSt-VR算法有效缓解性能下降。


<details>
  <summary>Details</summary>
Motivation: 探讨Wi-Fi网络中重叠信道对VR流媒体的性能影响，以优化VR流媒体在复杂网络环境中的表现。

Method: 实验分析不同重叠信道情况（部分和完全重叠）对VR流媒体的影响，并提出NeSt-VR算法进行优化。

Result: 增加重叠信道数量会加剧竞争并降低性能；次级40MHz部分的影响更大；完全重叠与部分重叠的影响不同；NeSt-VR算法能有效缓解性能下降。

Conclusion: 重叠信道对VR流媒体性能有显著影响，NeSt-VR算法能有效改善性能，尤其在复杂网络环境中。

Abstract: This paper experimentally analyzes the negative impact of contention caused
by neighboring Wi-Fi networks operating on overlapping channels on Virtual
Reality (VR) streaming over Wi-Fi, focusing on scenarios of partial and full
channel overlap within an 80 MHz channel. Our results show that (i) increasing
the number of 80 MHz Overlapping Basic Service Sets (OBSSs) intensifies
contention and degrades VR streaming performance; (ii) OBSS activity on the
secondary-sided 40 MHz portion degrades performance more than activity on the
primary-sided 40 MHz portion; (iii) for the same aggregate load, full channel
overlap with two 40 MHz OBSS contenders is less detrimental than partial
overlap with a single high-load 40 MHz contender, but more disruptive than full
overlap with two 80 MHz contenders; and (iv) full channel overlap with two 40
MHz OBSS contenders has a smaller impact on VR streaming under symmetric
traffic loads than under asymmetric loads. Moreover, our results demonstrate
that our previously proposed Network-aware Step-wise adaptive bitrate algorithm
for VR streaming (NeSt-VR) effectively mitigates performance degradation in
OBSS environments, enabling VR streaming under heavier OBSS traffic conditions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas,Mehmet Mercangoz*

Main category: cs.AI

TL;DR: 论文提出了一种结合符号推理与自适应控制的统一智能体框架，利用大语言模型（LLMs）同时处理离散故障恢复规划和连续过程控制。通过有限状态机（FSMs）作为可解释的操作边界，该框架在案例研究中展示了高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代化学过程日益复杂，劳动力短缺和故障场景多样化，需要新的自动化范式来结合符号推理与自适应控制。

Method: 采用有限状态机（FSMs）作为操作边界，通过LLM驱动的规划代理生成恢复序列，仿真代理执行和验证过渡，验证-重新提示循环迭代优化无效计划。

Result: 在案例研究中，GPT-4o和GPT-4o-mini在180个随机生成的FSMs上实现了100%的有效路径成功率，且在双加热器控制中性能与经典PID相当。

Conclusion: 通过结构化反馈和模块化代理，LLMs能够统一高层符号规划和低层连续控制，为化学工程中的语言驱动自动化铺平道路。

Abstract: The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [9] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran,Shuang Chen,Jingjing Deng,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 论文提出了一种名为BOOST的方法，通过动态调整温度缩放和采样概率，解决AI艺术分类中的偏见问题，并在KaoKore和PACS数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: AI艺术分类中的偏见问题日益严重，尤其是在处理分布外数据时，现有方法未能有效解决这一问题。

Method: 提出BOOST方法，动态调整温度缩放和采样概率，以减少类别偏见。

Result: 在KaoKore和PACS数据集上验证了BOOST的有效性，能够平衡性能与公平性。

Conclusion: BOOST是一种有效的解决方案，可用于减少艺术领域AI模型的偏见。

Abstract: The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [10] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim,Junsik Kim,Hwidong Bae,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: SIBP方法通过状态推断和规则遵守解决了大型语言模型在交易系统中的问题，显著提高了准确性和信任度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在动态游戏交互中表现良好，但在规则驱动的交易系统中易出现违规行为（如物品幻觉和计算错误），影响玩家信任。

Method: 提出State-Inference-Based Prompting (SIBP)，将交易分解为六个状态，通过上下文感知的物品引用和占位符价格计算实现规则遵守。

Result: 在100个交易对话中，SIBP实现了>97%的状态合规性、>95%的引用准确性和99.7%的计算精度，优于基线方法。

Conclusion: SIBP为商业游戏中可信赖的NPC交互提供了实用基础，同时保持了计算效率。

Abstract: Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [11] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 论文探讨了使用神经符号方法检测供应链中的非法活动，比较了手动和自动特征提取的效果，并提出了基于大型语言模型的查询方法。


<details>
  <summary>Details</summary>
Motivation: 供应链网络复杂且涉及非法活动时分析困难，传统机器学习需要大量数据，但非法供应链数据稀疏且不可靠。

Method: 采用神经符号方法，比较手动和自动特征提取，提出基于LLM的查询树方法。

Result: 系统评估了人类和机器对新闻文章分类的差异，特别是关于强迫劳动的供应链新闻。

Conclusion: 神经符号方法和LLM查询树能有效检测非法供应链活动，弥补数据稀疏问题。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [12] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: 介绍了一个名为cmbagent的多智能体系统，用于自动化科学研究任务，由30个LLM代理组成，采用规划与控制策略，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 旨在通过多智能体系统实现科学研究任务的自动化，减少人工干预，提高效率。

Method: 系统由30个LLM代理组成，每个代理专注于不同任务（如检索、编程、结果解释等），采用规划与控制策略协调工作流，并能本地执行代码。

Result: 成功应用于博士级宇宙学任务，性能优于现有LLM，代码已开源并在HuggingFace部署。

Conclusion: cmbagent展示了多智能体系统在科学研究自动化中的潜力，性能优越且易于部署。

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [13] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

Main category: cs.AI

TL;DR: 研究利用大型语言模型作为专家规划器，在多智能体强化学习中实现高效探索。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的探索问题复杂且效率低下，需要新的方法来解决。

Method: 采用大型语言模型作为专家规划器，用于多智能体任务中的高效探索。

Result: 研究表明大型语言模型在多智能体规划任务中能有效提升探索效率。

Conclusion: 大型语言模型可作为专家规划器，显著改善多智能体强化学习的探索效率。

Abstract: Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [14] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: ViDove是一种基于多模态输入的翻译代理系统，通过结合视觉和上下文背景信息提升翻译质量，并在字幕生成和通用翻译任务中显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM翻译代理通常仅限于文本输入，无法充分利用视觉和上下文信息，限制了翻译质量。

Method: ViDove模仿人类翻译工作流程，整合多模态记忆系统和长短时记忆模块，结合领域知识提升翻译适应性。

Result: ViDove在BLEU分数上提升28%，SubER提升15%，并推出新的基准测试DoveBench。

Conclusion: ViDove通过多模态输入显著提升翻译质量，为视频字幕翻译提供了新工具和基准。

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [15] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball,Greg Gluch,Shafi Goldwasser,Frauke Kreuter,Omer Reingold,Guy N. Rothblum*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLM）的安全对齐问题，指出外部过滤（输入和输出）存在计算上的困难，并强调安全需内置于模型内部。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，防止其生成有害内容的需求日益迫切，研究旨在探讨外部过滤方法的局限性。

Method: 通过理论分析，证明输入和输出过滤在计算上的不可行性，并基于密码学假设提出分离结果。

Result: 发现高效过滤输入和输出在计算上不可行，且黑盒访问LLM不足以实现安全。

Conclusion: 安全需内置于LLM内部，智能与判断不可分离。

Abstract: With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [16] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai,Haoyu Wang,Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haifeng Chen,Yanjie Fu*

Main category: cs.AI

TL;DR: Sim-to-Dec框架通过生成模拟模块和双感知决策模型，显著提升供应链运输的及时交付率和利润。


<details>
  <summary>Details</summary>
Motivation: 供应链运输中的高响应性和经济效率受运输模式决策影响，需一种可观察、低风险的策略设计环境。

Method: 提出Sim-to-Dec框架，包括生成模拟模块（利用自回归建模模拟状态变化）和双感知决策模型（通过端到端优化迭代优化）。

Result: 在三个真实数据集上的实验表明，Sim-to-Dec显著提高了及时交付率和利润。

Conclusion: Sim-to-Dec满足跨场景泛化、细粒度动态模拟、历史与预测结合的需求，是供应链运输策略设计的有效工具。

Abstract: High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


### [17] [DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search](https://arxiv.org/abs/2507.07426)
*Zerui Yang,Yuwei Wan,Yinqiao Li,Yudai Matsuda,Tong Xie,Linqi Song*

Main category: cs.AI

TL;DR: DrugMCTS框架结合RAG、多智能体协作和蒙特卡洛树搜索，用于药物重定位，无需领域微调即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在科学领域（如药物发现）中推理能力受限的问题，克服传统方法的高计算成本或数据利用不足的局限。

Method: 提出DrugMCTS框架，整合RAG、多智能体协作和蒙特卡洛树搜索，通过五个专业智能体检索和分析分子与蛋白质信息。

Result: 在DrugBank和KIBA数据集上，DrugMCTS的召回率和鲁棒性显著优于通用大语言模型和深度学习基线，性能提升超过20%。

Conclusion: 结构化推理、智能体协作和反馈驱动搜索机制对药物发现中的大语言模型应用至关重要。

Abstract: Recent advances in large language models have demonstrated considerable
potential in scientific domains such as drug discovery. However, their
effectiveness remains constrained when reasoning extends beyond the knowledge
acquired during pretraining. Conventional approaches, such as fine-tuning or
retrieval-augmented generation, face limitations in either imposing high
computational overhead or failing to fully exploit structured scientific data.
To overcome these challenges, we propose DrugMCTS, a novel framework that
synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree
Search for drug repurposing. The framework employs five specialized agents
tasked with retrieving and analyzing molecular and protein information, thereby
enabling structured and iterative reasoning. Without requiring domain-specific
fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by
over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate
that DrugMCTS achieves substantially higher recall and robustness compared to
both general-purpose LLMs and deep learning baselines. Our results highlight
the importance of structured reasoning, agent-based collaboration, and
feedback-driven search mechanisms in advancing LLM applications for drug
discovery.

</details>


### [18] [StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley](https://arxiv.org/abs/2507.07445)
*Weihao Tan,Changjiu Jiang,Yu Duan,Mingcong Lei,Jiageng Li,Yitian Hong,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: StarDojo是一个基于《星露谷物语》的新基准测试，用于评估AI代理在开放式生产生活模拟中的表现，涵盖农业、制作、探索、战斗和社交互动五大领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试很少同时评估生产活动与社交互动能力，StarDojo旨在填补这一空白。

Method: StarDojo包含1000个任务，提供统一界面支持多环境并行执行，适合评估多模态大语言模型（MLLMs）代理。

Result: 当前最佳模型GPT-4.1的成功率仅为12.7%，主要受限于视觉理解、多模态推理和低级操作能力。

Conclusion: StarDojo为复杂生产生活环境中稳健开放式代理的研究提供了便利。

Abstract: Autonomous agents navigating human society must master both production
activities and social interactions, yet existing benchmarks rarely evaluate
these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel
benchmark based on Stardew Valley, designed to assess AI agents in open-ended
production-living simulations. In StarDojo, agents are tasked to perform
essential livelihood activities such as farming and crafting, while
simultaneously engaging in social interactions to establish relationships
within a vibrant community. StarDojo features 1,000 meticulously curated tasks
across five key domains: farming, crafting, exploration, combat, and social
interactions. Additionally, we provide a compact subset of 100 representative
tasks for efficient model evaluation. The benchmark offers a unified,
user-friendly interface that eliminates the need for keyboard and mouse
control, supports all major operating systems, and enables the parallel
execution of multiple environment instances, making it particularly well-suited
for evaluating the most capable foundation agents, powered by multimodal large
language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents
demonstrate substantial limitations, with the best-performing model, GPT-4.1,
achieving only a 12.7% success rate, primarily due to challenges in visual
understanding, multimodal reasoning and low-level manipulation. As a
user-friendly environment and benchmark, StarDojo aims to facilitate further
research towards robust, open-ended agents in complex production-living
environments.

</details>


### [19] [Position: We Need An Algorithmic Understanding of Generative AI](https://arxiv.org/abs/2507.07544)
*Oliver Eberle,Thomas McGee,Hamza Giaffar,Taylor Webb,Ida Momennejad*

Main category: cs.AI

TL;DR: AlgEval框架旨在系统研究LLMs学习的算法，揭示其底层计算原理，提供可解释性，并推动高效训练和新架构。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注性能提升，缺乏对LLMs学习算法的理论理解，AlgEval填补了这一空白。

Method: 提出AlgEval框架，结合自上而下的假设和自下而上的电路级分析（如注意力模式和隐藏状态）。

Result: 案例研究表明LLMs可能形成搜索算法，并通过分析验证了假设。

Conclusion: 系统评估LLMs的任务解决方式为可解释性和高效训练提供了新方向。

Abstract: What algorithms do LLMs actually learn and use to solve problems? Studies
addressing this question are sparse, as research priorities are focused on
improving performance through scale, leaving a theoretical and empirical gap in
understanding emergent algorithms. This position paper proposes AlgEval: a
framework for systematic research into the algorithms that LLMs learn and use.
AlgEval aims to uncover algorithmic primitives, reflected in latent
representations, attention, and inference-time compute, and their algorithmic
composition to solve task-specific problems. We highlight potential
methodological paths and a case study toward this goal, focusing on emergent
search algorithms. Our case study illustrates both the formation of top-down
hypotheses about candidate algorithms, and bottom-up tests of these hypotheses
via circuit-level analysis of attention patterns and hidden states. The
rigorous, systematic evaluation of how LLMs actually solve tasks provides an
alternative to resource-intensive scaling, reorienting the field toward a
principled understanding of underlying computations. Such algorithmic
explanations offer a pathway to human-understandable interpretability, enabling
comprehension of the model's internal reasoning performance measures. This can
in turn lead to more sample-efficient methods for training and improving
performance, as well as novel architectures for end-to-end and multi-agent
systems.

</details>


### [20] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala,Jordi Planes,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 论文探讨了机器学习模型预测解释的重要性，特别是在高风险领域，分析了规则模型中的负面问题，并提出了相关算法。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，错误的解释可能误导决策者，因此需要严格的解释方法。尽管可解释性难以定义，规则模型仍被广泛使用。

Method: 开发了算法来分析规则模型中的负面问题，如负重叠和冗余。

Result: 研究发现，广泛使用的规则学习工具会导致规则集出现负面问题。

Conclusion: 需要改进规则学习工具以避免负面问题，确保解释的严谨性。

Abstract: A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


### [21] [Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs](https://arxiv.org/abs/2507.07595)
*Zhixiang Su,Di Wang,Chunyan Miao*

Main category: cs.AI

TL;DR: 论文提出了一种名为Context Pooling的新方法，用于提升GNN在知识图谱链接预测中的表现，首次将图池化应用于知识图谱，并在42/48的实验中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型在知识图谱链接预测中表现不佳，尤其是vanilla聚合方法效果有限，因此需要一种更有效的方法。

Method: 提出Context Pooling方法，首次在知识图谱中应用图池化，并设计两种指标（邻域精度和邻域召回）筛选逻辑相关邻居。

Result: 在三个公开数据集上应用于两种SOTA模型，42/48的实验设置中达到SOTA性能。

Conclusion: Context Pooling显著提升了GNN在知识图谱链接预测中的表现，尤其在处理未见实体时表现优异。

Abstract: Recent investigations on the effectiveness of Graph Neural Network
(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that
vanilla aggregation does not significantly impact the model performance. In
this paper, we introduce a novel method, named Context Pooling, to enhance
GNN-based models' efficacy for link predictions in KGs. To our best of
knowledge, Context Pooling is the first methodology that applies graph pooling
in KGs. Additionally, Context Pooling is first-of-its-kind to enable the
generation of query-specific graphs for inductive settings, where testing
entities are unseen during training. Specifically, we devise two metrics,
namely neighborhood precision and neighborhood recall, to assess the neighbors'
logical relevance regarding the given queries, thereby enabling the subsequent
comprehensive identification of only the logically relevant neighbors for link
prediction. Our method is generic and assessed by being applied to two
state-of-the-art (SOTA) models on three public transductive and inductive
datasets, achieving SOTA performance in 42 out of 48 settings.

</details>


### [22] [Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.07599)
*Sedigh Khademi,Jim Black,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila*

Main category: cs.AI

TL;DR: 研究评估了微调的Llama 3.2模型从急诊分诊记录中提取疫苗相关信息的能力，以支持近实时疫苗安全监测。微调模型在提取疫苗名称的准确性上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 支持高效的疫苗安全监测和早期发现免疫后不良事件。

Method: 使用提示工程创建标注数据集，并比较提示工程模型、微调模型和基于规则的方法。

Result: 微调的Llama 3B参数模型在提取疫苗名称时表现最佳，量化技术使其在资源受限环境中高效部署。

Conclusion: 大型语言模型在自动化数据提取方面具有潜力，可支持疫苗安全监测。

Abstract: This study evaluates fine-tuned Llama 3.2 models for extracting
vaccine-related information from emergency department triage notes to support
near real-time vaccine safety surveillance. Prompt engineering was used to
initially create a labeled dataset, which was then confirmed by human
annotators. The performance of prompt-engineered models, fine-tuned models, and
a rule-based approach was compared. The fine-tuned Llama 3 billion parameter
model outperformed other models in its accuracy of extracting vaccine names.
Model quantization enabled efficient deployment in resource-constrained
environments. Findings demonstrate the potential of large language models in
automating data extraction from emergency department notes, supporting
efficient vaccine safety surveillance and early detection of emerging adverse
events following immunization issues.

</details>


### [23] [Towards conservative inference in credal networks using belief functions: the case of credal chains](https://arxiv.org/abs/2507.07619)
*Marco Sangalli,Thomas Krak,Cassio de Campos*

Main category: cs.AI

TL;DR: 本文提出了一种基于Dempster-Shafer理论的信念推断框架，用于在信用网络（特别是链式结构）中传播不确定性。该方法通过信念和似然函数高效生成保守区间，结合了计算速度和鲁棒的表示能力。


<details>
  <summary>Details</summary>
Motivation: 探索在信用网络中应用信念推断的潜力，特别是在链式结构中，以提供更高效的保守区间计算和不确定性表示。

Method: 基于Dempster-Shafer理论，提出了一种新的框架，用于在链式信用网络中传播不确定性，并通过信念和似然函数生成保守区间。

Result: 数值结果表明，该方法在计算速度和鲁棒性方面具有优势，但也揭示了其局限性。

Conclusion: 该框架为链式信用网络中的信念推断提供了实用工具，并为进一步研究信用网络中的不确定性传播提供了见解。

Abstract: This paper explores belief inference in credal networks using Dempster-Shafer
theory. By building on previous work, we propose a novel framework for
propagating uncertainty through a subclass of credal networks, namely chains.
The proposed approach efficiently yields conservative intervals through belief
and plausibility functions, combining computational speed with robust
uncertainty representation. Key contributions include formalizing belief-based
inference methods and comparing belief-based inference against classical
sensitivity analysis. Numerical results highlight the advantages and
limitations of applying belief inference within this framework, providing
insights into its practical utility for chains and for credal networks in
general.

</details>


### [24] [PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations](https://arxiv.org/abs/2507.07644)
*Fedor Rodionov,Abdelrahman Eldesokey,Michael Birsak,John Femiani,Bernard Ghanem,Peter Wonka*

Main category: cs.AI

TL;DR: PlanQA是一个用于评估大型语言模型（LLMs）几何和空间推理能力的诊断基准，基于结构化室内场景表示，揭示了LLMs在真实世界布局推理中的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在几何和空间推理方面存在不足，尤其是在模拟物理约束、保持空间一致性及布局扰动下的泛化能力上。PlanQA旨在填补这一空白，推动相关研究。

Method: PlanQA使用符号化格式（如JSON、XML）编码室内场景（如厨房、卧室），设计多样化问题类型测试度量、拓扑推理及设计约束。

Result: 实验表明，尽管LLMs在简单查询中表现良好，但在物理约束模拟、空间一致性保持及布局扰动泛化上常失败。

Conclusion: PlanQA揭示了LLMs在真实世界布局推理中的局限性，为未来研究提供了方向。

Abstract: We introduce PlanQA, a diagnostic benchmark for evaluating geometric and
spatial reasoning in large-language models (LLMs). PlanQA is grounded in
structured representations of indoor scenes, such as kitchens, living rooms,
and bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The
benchmark includes diverse question types that test not only metric and
topological reasoning (e.g., distance, visibility, shortest paths) but also
interior design constraints such as affordance, clearance, balance, and
usability. Our results across a variety of frontier open-source and commercial
LLMs show that while models may succeed in shallow queries, they often fail to
simulate physical constraints, preserve spatial coherence, or generalize under
layout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they
do not consistently reason about real-world layouts. We hope that this
benchmark inspires new work on language models that can accurately infer and
manipulate spatial and geometric properties in practical settings.

</details>


### [25] [Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization](https://arxiv.org/abs/2507.07723)
*Chengtao Jian,Kai Yang,Ye Ouyang,Xiaozhou Ye*

Main category: cs.AI

TL;DR: 本文分析了直接偏好优化（DPO）的理论局限性，提出了一种基于双层优化的稳定偏好优化方法，以改进模型对齐的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: DPO虽然高效，但其理论性质和内在局限性尚未充分研究，特别是对初始化的敏感性和概率质量分配问题。

Method: 提出了一种双层优化框架，结合监督微调和增强的DPO目标，引入正则化以鼓励对偏好输出的绝对概率改进。

Result: 实验表明，该方法在推理和摘要任务中表现优于标准DPO，提高了推理准确性并更好地对齐输出分布。

Conclusion: 稳定偏好优化为偏好对齐目标的设计提供了新思路，为更可靠和可解释的语言模型对齐开辟了新途径。

Abstract: Direct Preference Optimization (DPO) has emerged as a popular and efficient
alternative to reward modeling and reinforcement learning for aligning language
models with human preferences. Despite its empirical success, the theoretical
properties and intrinsic limitations of DPO remain underexplored. In this work,
we first present a comprehensive analysis of DPO's dynamics from a probability
evolution perspective. Our analysis reveals that DPO is highly sensitive to
initialization. It also tends to misallocate probability mass, which can
inadvertently shift probability toward irrelevant or undesired responses. This
misallocation may unintentionally reinforce model bias, thereby compromising
both the stability of model alignment and the consistency with intended
preferences. Motivated by these theoretical findings, we propose a
theoretically grounded bilevel optimization framework that tightly integrate
supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference
optimization. Our approach introduces a principled regularization scheme to
explicitly encourage absolute probability improvement for preferred outputs,
while maintaining stable optimization dynamics. Experiments on challenging
reasoning and summarization benchmarks elucidate that our method consistently
improves reasoning accuracy and better aligns output distributions with
intended preferences, outperforming standard DPO. Stable preference
optimization provides new insights into the design of preference-based
alignment objectives and opens up new avenues towards more reliable and
interpretable language model alignment.

</details>


### [26] [Identification of Violin Reduction via Contour Lines Classification](https://arxiv.org/abs/2507.07743)
*Philémon Beghin,Anne-Emmanuelle Ceulemans,François Glineur*

Main category: cs.AI

TL;DR: 该论文提出了一种基于轮廓线分类小提琴是否被缩小的方法，通过3D几何网格和抛物线拟合参数区分缩小与非缩小乐器。


<details>
  <summary>Details</summary>
Motivation: 研究小提琴制作中的尺寸标准化对乐器轮廓线的影响，填补专家观察但未定量研究的空白。

Method: 使用25把小提琴的3D几何网格数据，提取10-20条轮廓线，拟合抛物线曲线参数（α和β），并通过回归和阈值计算特征。

Result: 发现几何特征可以一定程度上区分缩小与非缩小乐器，其中参数β最具预测性。

Conclusion: 轮廓线分析可用于小提琴分类，但需考虑乐器改造的多样性带来的挑战。

Abstract: The first violins appeared in late 16th-century Italy. Over the next 200
years, they spread across Europe and luthiers of various royal courts, eager to
experiment with new techniques, created a highly diverse family of instruments.
Around 1750, size standards were introduced to unify violin making for
orchestras and conservatories. Instruments that fell between two standards were
then reduced to a smaller size by luthiers. These reductions have an impact on
several characteristics of violins, in particular on the contour lines, i.e.
lines of constant altitude, which look more like a U for non reduced
instruments and a V for reduced ones. While such differences are observed by
experts, they have not been studied quantitatively.
  This paper presents a method for classifying violins as reduced or
non-reduced based on their contour lines. We study a corpus of 25 instruments
whose 3D geometric meshes were acquired via photogrammetry. For each
instrument, we extract 10-20 contour lines regularly spaced every millimetre.
Each line is fitted with a parabola-like curve (with an equation of the type y
= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)
and how vertically stretched (alpha) the curve is. We compute additional
features from those parameters, using regressions and counting how many values
fall under some threshold. We also deal with outliers and non equal numbers of
levels, and eventually obtain a numerical profile for each instrument.
  We then apply classification methods to assess whether geometry alone can
predict size reduction. We find that distinguishing between reduced and non
reduced instruments is feasible to some degree, taking into account that a
whole spectrum of more or less transformed violins exists, for which it is more
difficult to quantify the reduction. We also find the opening parameter beta to
be the most predictive.

</details>


### [27] [Measuring AI Alignment with Human Flourishing](https://arxiv.org/abs/2507.07787)
*Elizabeth Hilliard,Akshaya Jagadeesh,Alex Cook,Steele Billings,Nicholas Skytland,Alicia Llewellyn,Jackson Paull,Nathan Paull,Nolan Kurylo,Keatra Nesbitt,Robert Gruenewald,Anthony Jantzi,Omar Chavez*

Main category: cs.AI

TL;DR: FAI Benchmark提出了一种新的评估框架，从七个维度衡量AI对人类繁荣的贡献，发现当前模型在多个维度上表现不足。


<details>
  <summary>Details</summary>
Motivation: 传统AI评估主要关注技术能力或避免危害，而FAI Benchmark旨在评估AI如何促进人类整体福祉。

Method: 通过1,229个主客观问题，结合专家LLM和几何平均评分，评估28个领先语言模型。

Result: 最高分模型仅得72/100，尤其在信仰与灵性、品格与美德、意义与目的等维度表现不佳。

Conclusion: FAI Benchmark为开发支持人类繁荣的AI系统提供了框架，对AI伦理和评估有重要意义。

Abstract: This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel
evaluation framework that assesses AI alignment with human flourishing across
seven dimensions: Character and Virtue, Close Social Relationships, Happiness
and Life Satisfaction, Meaning and Purpose, Mental and Physical Health,
Financial and Material Stability, and Faith and Spirituality. Unlike
traditional benchmarks that focus on technical capabilities or harm prevention,
the FAI Benchmark measures AI performance on how effectively models contribute
to the flourishing of a person across these dimensions. The benchmark evaluates
how effectively LLM AI systems align with current research models of holistic
human well-being through a comprehensive methodology that incorporates 1,229
objective and subjective questions. Using specialized judge Large Language
Models (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs
geometric mean scoring to ensure balanced performance across all flourishing
dimensions. Initial testing of 28 leading language models reveals that while
some models approach holistic alignment (with the highest-scoring models
achieving 72/100), none are acceptably aligned across all dimensions,
particularly in Faith and Spirituality, Character and Virtue, and Meaning and
Purpose. This research establishes a framework for developing AI systems that
actively support human flourishing rather than merely avoiding harm, offering
significant implications for AI development, ethics, and evaluation.

</details>


### [28] [MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving](https://arxiv.org/abs/2507.07818)
*Lu Xu,Jiaqian Yu,Xiongfeng Peng,Yiwei Chen,Weiming Li,Jaewook Yoo,Sunghyun Chunag,Dongwook Lee,Daehyun Ji,Chao Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种技能导向的MoE模型（MoSE），模仿人类驾驶员的学习和推理过程，通过技能分步学习提升自动驾驶性能，同时减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型需要大量训练数据和复杂优化，而MoSE通过模仿人类驾驶员的学习方式，实现更高效和可解释的自动驾驶。

Method: 提出技能导向的路由机制，构建分层技能数据集，并在单次前向过程中整合辅助任务（如描述、推理、规划）。

Result: MoSE在CODA AD任务中优于多个8B+参数模型，激活参数量减少至少62.5%，性能达到SOTA。

Conclusion: MoSE通过技能分步学习和推理，显著提升了自动驾驶模型的性能和效率。

Abstract: Recent studies show large language models (LLMs) and vision language models
(VLMs) trained using web-scale data can empower end-to-end autonomous driving
systems for a better generalization and interpretation. Specifically, by
dynamically routing inputs to specialized subsets of parameters, the
Mixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve
substantial performance improvements while maintaining computational
efficiency. However, general MoE models usually demands extensive training data
and complex optimization. In this work, inspired by the learning process of
human drivers, we propose a skill-oriented MoE, called MoSE, which mimics human
drivers' learning process and reasoning process, skill-by-skill and
step-by-step. We propose a skill-oriented routing mechanism that begins with
defining and annotating specific skills, enabling experts to identify the
necessary driving competencies for various scenarios and reasoning tasks,
thereby facilitating skill-by-skill learning. Further align the driving process
to multi-step planning in human reasoning and end-to-end driving models, we
build a hierarchical skill dataset and pretrain the router to encourage the
model to think step-by-step. Unlike multi-round dialogs, MoSE integrates
valuable auxiliary tasks (e.g.\ description, reasoning, planning) in one single
forward process without introducing any extra computational cost. With less
than 3B sparsely activated parameters, our model outperforms several 8B+
parameters on CODA AD corner case reasoning task. Compared to existing methods
based on open-source models and data, our approach achieves state-of-the-art
performance with significantly reduced activated model size (at least by
$62.5\%$) with a single-turn conversation.

</details>


### [29] [AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift](https://arxiv.org/abs/2507.07820)
*Eunsu Baek,Keondo Park,Jeonggil Ko,Min-hwan Oh,Taesik Gong,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: 论文提出自适应感知作为AI发展的新范式，通过动态调整传感器参数提升效率，减少资源消耗，并展示了小模型超越大模型的实证结果。


<details>
  <summary>Details</summary>
Motivation: 当前AI依赖大规模模型和数据，带来环境、经济和伦理成本，缺乏可持续性和公平性。受生物感官系统启发，提出自适应感知以解决这些问题。

Method: 自适应感知通过动态调整传感器参数（如曝光、灵敏度、多模态配置）来缓解协变量偏移并提升效率。

Result: 实证研究表明，自适应感知使小模型（如EfficientNet-B0）超越更大模型（如OpenCLIP-H）。

Conclusion: 论文提出自适应感知的整合路线图，并探讨技术及伦理挑战，旨在推动AI向可持续、鲁棒和公平的方向发展。

Abstract: Current AI advances largely rely on scaling neural models and expanding
training datasets to achieve generalization and robustness. Despite notable
successes, this paradigm incurs significant environmental, economic, and
ethical costs, limiting sustainability and equitable access. Inspired by
biological sensory systems, where adaptation occurs dynamically at the input
(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive
sensing as a necessary and foundational shift. Adaptive sensing proactively
modulates sensor parameters (e.g., exposure, sensitivity, multimodal
configurations) at the input level, significantly mitigating covariate shifts
and improving efficiency. Empirical evidence from recent studies demonstrates
that adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass
substantially larger models (e.g., OpenCLIP-H) trained with significantly more
data and compute. We (i) outline a roadmap for broadly integrating adaptive
sensing into real-world applications spanning humanoid, healthcare, autonomous
systems, agriculture, and environmental monitoring, (ii) critically assess
technical and ethical integration challenges, and (iii) propose targeted
research directions, such as standardized benchmarks, real-time adaptive
algorithms, multimodal integration, and privacy-preserving methods.
Collectively, these efforts aim to transition the AI community toward
sustainable, robust, and equitable artificial intelligence systems.

</details>


### [30] [Searching for actual causes: Approximate algorithms with adjustable precision](https://arxiv.org/abs/2507.07857)
*Samuel Reyd,Ada Diaconescu,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 论文提出了一种多项式复杂度的算法，用于识别实际原因，适用于非布尔、黑盒和随机系统，并能通过调整计算时间提高精度和全面性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释人工智能（XAI）和因果性文献主要关注因素与结果的关系，而非用户期望的实际原因。识别实际原因是一个NP完全问题，且缺乏实用解决方案。

Method: 提出了一组算法，以多项式复杂度识别实际原因，支持精度和全面性的调整。

Result: 实验表明，算法能处理现有方法无法应对的系统（非布尔、黑盒、随机系统），且可通过增加计算时间提高精度。

Conclusion: 该算法为解决实际原因识别问题提供了实用且灵活的解决方案。

Abstract: Causality has gained popularity in recent years. It has helped improve the
performance, reliability, and interpretability of machine learning models.
However, recent literature on explainable artificial intelligence (XAI) has
faced criticism. The classical XAI and causality literature focuses on
understanding which factors contribute to which consequences. While such
knowledge is valuable for researchers and engineers, it is not what non-expert
users expect as explanations. Instead, these users often await facts that cause
the target consequences, i.e., actual causes. Formalizing this notion is still
an open problem. Additionally, identifying actual causes is reportedly an
NP-complete problem, and there are too few practical solutions to approximate
formal definitions. We propose a set of algorithms to identify actual causes
with a polynomial complexity and an adjustable level of precision and
exhaustiveness. Our experiments indicate that the algorithms (1) identify
causes for different categories of systems that are not handled by existing
approaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be
adjusted to gain more precision and exhaustiveness with more computation time.

</details>


### [31] [An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis](https://arxiv.org/abs/2507.07893)
*Mingda Zhang,Na Zhao,Jianglong Qing,Qing xu,Kaiwen Pan,Ting luo*

Main category: cs.AI

TL;DR: 该研究提出了一种结合提示工程和多维知识图谱的增强框架，显著提升了法律纠纷分析的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法律纠纷分析中存在法律知识表示不足、概念理解有限和推理缺陷等问题，需要改进。

Method: 采用三阶段层次提示结构和三层知识图谱架构，结合四种互补方法实现精确法律概念检索。

Result: 实验结果表明，该框架显著提升了法律纠纷分析的性能，能够准确分析复杂案件。

Conclusion: 该研究为智能法律辅助系统的实现提供了新的技术途径。

Abstract: The rapid development of artificial intelligence has positioned large
language models as fundamental components of intelligent legal systems.
However, these models face significant limitations in legal dispute analysis,
including insufficient legal knowledge representation, limited concept
understanding, and reasoning deficiencies. This research proposes an enhanced
framework integrating prompt engineering with multidimensional knowledge
graphs. The framework introduces a three-stage hierarchical prompt structure
comprising task definition, knowledge background, and reasoning guidance,
supplemented by legal-specific reasoning templates and dynamic optimization
mechanisms. A three-layer knowledge graph architecture is constructed with
legal classification ontology, representation, and instance layers. Four
complementary methods enable precise legal concept retrieval: direct legal norm
code matching, domain-specific semantic vector similarity, ontology-based path
reasoning, and specialized lexical segmentation. These components integrate
with web search technology to establish a knowledge-enhanced framework for
legal decision-making. Experimental results demonstrate significant performance
improvements in legal dispute analysis, enabling accurate legal application
analysis for complex cases while exhibiting nuanced understanding of judicial
decision-making logic, providing a novel technical approach for implementing
intelligent legal assistance systems.

</details>


### [32] [Meek Models Shall Inherit the Earth](https://arxiv.org/abs/2507.07931)
*Hans Gundlach,Jayson Lynch,Neil Thompson*

Main category: cs.AI

TL;DR: 论文认为，随着计算规模收益递减，AI模型能力将趋同，即使是计算资源有限的小模型也能接近最佳模型的性能。


<details>
  <summary>Details</summary>
Motivation: 探讨AI模型性能不平等问题，提出计算规模收益递减将导致能力趋同的观点。

Method: 开发模型分析计算规模收益递减现象，结合基准数据和理论模型验证能力差异。

Result: 计算规模收益递减足够显著，即使计算资源丰富的公司优势也将减弱。

Conclusion: AI战略和政策需重新审视，以适应小模型能力提升的趋势。

Abstract: The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.

</details>


### [33] [Working with AI: Measuring the Occupational Implications of Generative AI](https://arxiv.org/abs/2507.07935)
*Kiran Tomlinson,Sonia Jaffe,Will Wang,Scott Counts,Siddharth Suri*

Main category: cs.AI

TL;DR: 研究分析了生成式AI对经济的影响，通过分析用户与AI的互动数据，发现信息收集和写作是最常见的AI辅助活动，并计算了各职业的AI适用性得分。


<details>
  <summary>Details</summary>
Motivation: 理解生成式AI对经济的潜在影响是社会的关键问题。

Method: 分析了20万条用户与微软Bing Copilot的匿名对话数据，结合职业活动分类和任务成功度量。

Result: 知识型职业（如计算机、数学、行政支持）和销售职业的AI适用性得分最高。

Conclusion: 生成式AI在信息提供和写作等活动中表现优异，对知识型职业影响显著。

Abstract: Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [34] [Sparse Signal Recovery From Quadratic Systems with Full-Rank Matrices](https://arxiv.org/abs/2507.07557)
*Jinming Wen,Yi Hu,Meng Huang*

Main category: cs.IT

TL;DR: 本文提出了一种基于稀疏性的信号恢复方法，通过代数几何工具和两阶段SGN算法，在高维稀疏信号恢复中实现了理论保证和高效性能。


<details>
  <summary>Details</summary>
Motivation: 解决高维稀疏信号从二次测量中恢复的挑战，尤其是在测量数远小于信号维度时。

Method: 使用代数几何工具推导理论恢复保证，并提出两阶段SGN算法：支持限制的谱初始化和迭代硬阈值Gauss-Newton方法。

Result: 理论证明m≥2s（实数）或m≥4s-2（复数）测量可唯一恢复稀疏信号；SGN算法在数值实验中表现优于现有方法。

Conclusion: SGN算法在稀疏信号恢复中实现了近最优采样复杂度和计算效率，显著优于现有方法。

Abstract: In signal processing and data recovery, reconstructing a signal from
quadratic measurements poses a significant challenge, particularly in
high-dimensional settings where measurements $m$ is far less than the signal
dimension $n$ (i.e., $m \ll n$). This paper addresses this problem by
exploiting signal sparsity. Using tools from algebraic geometry, we derive
theoretical recovery guarantees for sparse quadratic systems, showing that
$m\ge 2s$ (real case) and $m\ge 4s-2$ (complex case) generic measurements
suffice to uniquely recover all $s$-sparse signals. Under a Gaussian
measurement model, we propose a novel two-stage Sparse Gauss-Newton (SGN)
algorithm. The first stage employs a support-restricted spectral
initialization, yielding an accurate initial estimate with $m=O(s^2\log{n})$
measurements. The second stage refines this estimate via an iterative
hard-thresholding Gauss-Newton method, achieving quadratic convergence to the
true signal within finitely many iterations when $m\ge O(s\log{n})$. Compared
to existing second-order methods, our algorithm achieves near-optimal sampling
complexity for the refinement stage without requiring resampling. Numerical
experiments indicate that SGN significantly outperforms state-of-the-art
algorithms in both accuracy and computational efficiency. In particular, (1)
when sparsity level $s$ is high, compared with existing algorithms, SGN can
achieve the same success rate with fewer measurements. (2) SGN converges with
only about $1/10$ iterations of the best existing algorithm and reach lower
relative error.

</details>


### [35] [Secure Cooperative Gradient Coding: Optimality, Reliability, and Global Privacy](https://arxiv.org/abs/2507.07565)
*Shudi Weng*

Main category: cs.IT

TL;DR: 本文研究了隐私敏感的联邦学习（FL）在不稳定通信环境下的安全聚合和慢节点缓解问题，提出了SecCoGC方法，实现了强隐私保护和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在不稳定通信环境下，安全聚合和慢节点问题会降低模型准确性，甚至导致全局模型收敛到次优点。

Method: 提出了Secure Cooperative Gradient Coding (SecCoGC)方法，支持强隐私保护和鲁棒性，并扩展了Fair-SecCoGC以实现公平隐私保护。

Result: SecCoGC在不稳定通信环境下表现出色，性能提升达20%-70%，且支持强隐私保护。

Conclusion: 本文为实际部署提供了高效解决方案，并通过实验验证了其优越性。

Abstract: This paper studies privacy-sensitive federated learning (FL) with unreliable
communication, focusing on secure aggregation and straggler mitigation. While
secure aggregation cryptographically reconstructs the global model without
exposing client updates, random link failures disrupt its key coordination,
degrading model accuracy. Moreover, unreliable communication can lead to
objective inconsistency, causing the global model to converge to arbitrary,
sub-optimal points far from the intended optimum. This paper proposes Secure
Cooperative Gradient Coding (SecCoGC), a practical solution that achieves
secure aggregation with arbitrarily strong privacy guarantees and robust
straggler mitigation under unreliable communication. SecCoGC operates natively
in the real field, making it directly applicable to practical deployments. To
ensure equitable privacy protection across clients, we further introduce
Fair-SecCoGC, an extension that enforces fairness in the level of privacy
offered to all users. To conclude, this paper formally formulates the problem
of secure aggregation in the real field and presents both general and
computationally efficient key construction methods. Moreover, it provides a
comprehensive privacy analysis under Local Mutual Information Privacy (LMIP)
and Local Differential Privacy (LDP) across all protocol layers. Robustness and
convergence properties are also rigorously analyzed. Finally, extensive
simulations are performed across diverse network conditions and benchmark
datasets to validate the effectiveness of the proposed methods. The results
show that SecCoGC achieves strong robustness to unreliable communication under
arbitrarily strong privacy guarantees. It outperforms existing
privacy-preserving methods with performance gains of up to 20\%-70\%.

</details>


### [36] [Linear codes for $b$-symbol read channels attaining the Griesmer bound](https://arxiv.org/abs/2507.07728)
*Sascha Kurz*

Main category: cs.IT

TL;DR: 本文研究了在$b$-符号度量下线性码的最优参数，特别是在最小距离足够大时，以及针对小维度的二进制对符号度量线性码的最优参数。


<details>
  <summary>Details</summary>
Motivation: 研究$b$-符号度量下的编码问题，尤其是在存储等应用中的重要性。

Method: 通过理论分析确定线性码在$b$-符号度量下的最优参数，特别关注最小距离较大的情况和小维度的二进制对符号度量。

Result: 确定了在$b$-符号度量下线性码的最优参数，并针对小维度的二进制对符号度量给出了具体结果。

Conclusion: 本文为$b$-符号度量下的编码问题提供了理论支持，特别是在存储应用中具有重要意义。

Abstract: Reading channels where $b$-tuples of adjacent symbols are read at every step
have e.g.\ applications in storage. Corresponding bounds and constructions of
codes for the $b$-symbol metric, especially the pair-symbol metric where $b=2$,
were intensively studied in the last fifteen years. Here we determine the
optimal code parameters of linear codes in the $b$-symbol metric assuming that
the minimum distance is sufficiently large. We also determine the optimal
parameters of linear binary codes in the pair-symbol metric for small
dimensions.

</details>


### [37] [Generalized bilateral multilevel construction for constant dimension codes from parallel mixed dimension construction](https://arxiv.org/abs/2507.07842)
*Han Li,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 本文提出了选择兼容并行混合维度构造的双边标识向量的标准，并利用广义双边多级构造改进该构造，从而构建了许多优于现有最优代码的新CDC。


<details>
  <summary>Details</summary>
Motivation: 恒定维度码（CDC）在随机网络编码中有重要应用，但其最大可能尺寸的确定是一个基本问题。本文旨在通过改进构造方法，提升CDC的性能。

Method: 提出选择双边标识向量的标准，并利用广义双边多级构造改进并行混合维度构造。

Result: 构建了许多优于现有最优代码的新CDC。

Conclusion: 通过改进构造方法，本文成功提升了CDC的性能，为随机网络编码提供了更优的解决方案。

Abstract: Constant dimension codes (CDCs), as special subspace codes, have received
extensive attention due to their applications in random network coding. The
basic problem of CDCs is to determine the maximal possible size
$A_q(n,d,\{k\})$ for given parameters $q, n, d$, and $k$. This paper introduces
criteria for choosing appropriate bilateral identifying vectors compatible with
the parallel mixed dimension construction (Des. Codes Cryptogr. 93(1):227--241,
2025). We then utilize the generalized bilateral multilevel construction (Des.
Codes Cryptogr. 93(1):197--225, 2025) to improve the parallel mixed dimension
construction efficiently. Many new CDCs that are better than the previously
best-known codes are constructed.

</details>
