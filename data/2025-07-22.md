<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 25]
- [cs.AI](#cs.AI) [Total: 59]
- [cs.IT](#cs.IT) [Total: 18]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Iran's Stealth Internet Blackout: A New Model of Censorship](https://arxiv.org/abs/2507.14183)
*Arash Aryapour*

Main category: cs.NI

TL;DR: 伊朗在2025年中期实施了一种新型隐蔽的互联网关闭手段，通过深度包检测、流量限制和选择性协议屏蔽隔离国内用户，同时保持全球路由存在。本文分析了DNS污染、HTTP注入、TLS拦截和协议白名单等网络测量数据，并量化了VPN需求增长707%。


<details>
  <summary>Details</summary>
Motivation: 研究伊朗新型互联网关闭手段的技术细节及其对数字权利和规避技术的影响。

Method: 通过主动网络测量（如DNS污染、HTTP注入、TLS拦截和协议白名单）分析数据，追踪到集中式边界网关。

Result: 量化了VPN需求增长707%，揭示了多层审查基础设施。

Conclusion: 研究结果对规避技术和数字权利监测具有重要意义。

Abstract: In mid-2025, Iran experienced a novel, stealthy Internet shutdown that
preserved global routing presence while isolating domestic users through deep
packet inspection, aggressive throttling, and selective protocol blocking. This
paper analyzes active network measurements such as DNS poisoning, HTTP
injection, TLS interception, and protocol whitelisting, traced to a centralized
border gateway. We quantify an approximate 707 percent rise in VPN demand and
describe the multi-layered censorship infrastructure, highlighting implications
for circumvention and digital rights monitoring.

</details>


### [2] [A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction](https://arxiv.org/abs/2507.14186)
*Xiaojie Li,Zhijie Cai,Nan Qi,Chao Dong,Guangxu Zhu,Haixia Ma,Qihui Wu,Shi Jin*

Main category: cs.NI

TL;DR: 论文提出了一种基于专家知识特征压缩和解耦表示学习的双策略方法，用于解决低空网络覆盖预测中的数据稀疏和特征不平衡问题，实验表明其优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 低空网络覆盖预测对设计空中走廊至关重要，但基站天线波束模式通常不公开，且低空路测数据稀疏，导致特征采样不平衡和泛化能力不足。

Method: 采用专家知识特征压缩降低特征空间复杂度，结合解耦表示学习增强模型泛化能力，通过传播模型和子网络捕获潜在特征的语义表示。

Result: 实验显示该方法比最佳基线算法误差降低7%，实际网络验证中MAE误差达到5dB水平。

Conclusion: 该方法有效解决了低空网络覆盖预测中的数据稀疏和特征不平衡问题，具有实际应用价值。

Abstract: The expansion of the low-altitude economy has underscored the significance of
Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors.
While accurate LANC forecasting hinges on the antenna beam patterns of Base
Stations (BSs), these patterns are typically proprietary and not readily
accessible. Operational parameters of BSs, which inherently contain beam
information, offer an opportunity for data-driven low-altitude coverage
prediction. However, collecting extensive low-altitude road test data is
cost-prohibitive, often yielding only sparse samples per BS. This scarcity
results in two primary challenges: imbalanced feature sampling due to limited
variability in high-dimensional operational parameters against the backdrop of
substantial changes in low-dimensional sampling locations, and diminished
generalizability stemming from insufficient data samples. To overcome these
obstacles, we introduce a dual strategy comprising expert knowledge-based
feature compression and disentangled representation learning. The former
reduces feature space complexity by leveraging communications expertise, while
the latter enhances model generalizability through the integration of
propagation models and distinct subnetworks that capture and aggregate the
semantic representations of latent features. Experimental evaluation confirms
the efficacy of our framework, yielding a 7% reduction in error compared to the
best baseline algorithm. Real-network validations further attest to its
reliability, achieving practical prediction accuracy with MAE errors at the 5dB
level.

</details>


### [3] [From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks](https://arxiv.org/abs/2507.14188)
*Sebastian Barros Elgueta*

Main category: cs.NI

TL;DR: 论文探讨了完全由轨道运行的移动网络的可行性，提出了一种端到端系统架构，并分析了其在密集城市环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 验证是否能够构建一个完全在轨道上运行的移动网络，包括无线接入、核心功能、流量路由和内容分发，并能在高密度城市中提供持续的城市级服务。

Method: 提出了一种端到端系统架构，结合电子控制相控阵、5G核心功能的空间部署和卫星间激光网状回程，并分析了频谱效率、波束容量和链路预算。

Result: 模拟显示，屋顶和视线用户可维持64-QAM吞吐量，街道级接入可通过中继或辅助波束模式实现。工程瓶颈（如功率、散热等）是主要限制。

Conclusion: 论文提出了一个15年分阶段路线图，逐步实现完全自主的轨道覆盖网络，为超大城市提供50-100 Mbps的手持设备连接，且不依赖地面基础设施。

Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard
smartphones, using unmodified 3GPP protocols, connected directly to low Earth
orbit (LEO) satellites. This first wave of direct-to-device (D2D)
demonstrations validated the physical feasibility of satellite-based mobile
access. However, these systems remain fallback-grade--rural-only,
bandwidth-limited, and fully dependent on Earth-based mobile cores for
identity, session, and policy control. This paper asks a more ambitious
question: Can a complete mobile network, including radio access, core
functions, traffic routing, and content delivery, operate entirely from orbit?
And can it deliver sustained, urban-grade service in the world's densest
cities? We present the first end-to-end system architecture for a fully orbital
telco, integrating electronically steered phased arrays with 1000-beam
capacity, space-based deployment of 5G core functions (UPF, AMF), and
inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam
capacity, and link budgets under dense urban conditions, accounting for path
loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight
users can sustain 64-QAM throughput, while street-level access is feasible with
relay or assisted beam modes. The paper outlines the remaining constraints,
power, thermal dissipation, compute radiation hardening, and regulatory models,
and demonstrates that these are engineering bottlenecks, not physical limits.
Finally, we propose a staged 15-year roadmap from today's fallback D2D systems
to autonomous orbital overlays delivering 50-100 Mbps to handhelds in
megacities, with zero reliance on terrestrial infrastructure.

</details>


### [4] [On Splitting Lightweight Semantic Image Segmentation for Wireless Communications](https://arxiv.org/abs/2507.14199)
*Ebrahim Abu-Helalah,Jordi Serra,Jordi Perez-Romero*

Main category: cs.NI

TL;DR: 提出了一种基于分割语义图像分割过程的新方法，以在资源受限环境中平衡计算效率和带宽需求，同时保持高分割精度。


<details>
  <summary>Details</summary>
Motivation: 解决语义通信中计算效率与带宽需求之间的不平衡问题，特别是在资源受限和信道条件变化的环境中。

Method: 将语义图像分割过程分割到资源受限的发送端和接收端之间，减少传输数据量并降低发送端的计算负担。

Result: 实验结果显示，传输比特率降低72%，发送端计算负载减少19%以上。

Conclusion: 该方法在通信系统（尤其是未来6G系统）中具有应用潜力。

Abstract: Semantic communication represents a promising technique towards reducing
communication costs, especially when dealing with image segmentation, but it
still lacks a balance between computational efficiency and bandwidth
requirements while maintaining high image segmentation accuracy, particularly
in resource-limited environments and changing channel conditions. On the other
hand, the more complex and larger semantic image segmentation models become,
the more stressed the devices are when processing data. This paper proposes a
novel approach to implementing semantic communication based on splitting the
semantic image segmentation process between a resource constrained transmitter
and the receiver. This allows saving bandwidth by reducing the transmitted data
while maintaining the accuracy of the semantic image segmentation.
Additionally, it reduces the computational requirements at the resource
constrained transmitter compared to doing all the semantic image segmentation
in the transmitter. The proposed approach is evaluated by means of
simulation-based experiments in terms of different metrics such as
computational resource usage, required bit rate and segmentation accuracy. The
results when comparing the proposal with the full semantic image segmentation
in the transmitter show that up to 72% of the bit rate was reduced in the
transmission process. In addition, the computational load of the transmitter is
reduced by more than 19%. This reflects the interest of this technique for its
application in communication systems, particularly in the upcoming 6G systems.

</details>


### [5] [A Fault-Tolerant Architecture for Urban and Rural Digital Connectivity: Synergizing SDWMN, Direct-to-Mobile Broadcasting, and Hybrid Cloud Streaming](https://arxiv.org/abs/2507.14205)
*Pavel Malinovskiy*

Main category: cs.NI

TL;DR: 提出了一种结合SDWMN、D2M广播和Kafka混合云流媒体的架构，显著提升了城市和农村的网络性能。


<details>
  <summary>Details</summary>
Motivation: 解决城市网络拥堵和农村数字鸿沟问题，通过流量卸载、容错和资源公平分配。

Method: 建模城市拥堵和农村覆盖不足，优化全局性能损失，结合SDWMN和Kafka实现快速恢复。

Result: 实验显示延迟降低32%，带宽卸载40%，农村覆盖提升28%，公平指数从0.78升至0.91。

Conclusion: 该架构具有可扩展性和容错性，支持公平数字化转型，并提出了未来研究方向。

Abstract: We propose an integrated architecture combining Software-Defined Wireless
Mesh Networks (SDWMN), Direct-to-Mobile (D2M) broadcasting, and Kafka-based
hybrid cloud streaming to improve wireless network performance in both urban
and rural settings. The approach addresses urban congestion and rural digital
exclusion through traffic offloading, enhanced fault tolerance, and equitable
resource allocation. We model urban congestion $\rho_u = \lambda_t / \mu_c$ and
rural coverage deficit $\delta_r = 1 - C_r / C_{req}$, and aim to minimize
global performance loss $GPL = w_1 \cdot \rho_u + w_2 \cdot \delta_r + w_3
\cdot T_{rec}$, where $T_{rec}$ is recovery time. Experiments in Bangkok,
Mumbai, and rural Finland demonstrate latency reduction over 32%, bandwidth
offloading of 40%, rural coverage gain of 28%, and fairness index rising from
0.78 to 0.91. The system achieves recovery under 10 s using SDWMN and Kafka. We
recommend optimal spectrum allocation $\alpha_s$, targeted subsidies, and
device mandates to promote adoption. This scalable, fault-tolerant design
supports equitable digital transformation and suggests directions for future
research.

</details>


### [6] [White paper: Towards Human-centric and Sustainable 6G Services -- the fortiss Research Perspective](https://arxiv.org/abs/2507.14209)
*Rute C. Sofia,Hao Shen,Yuanting Liu,Severin Kacianka,Holger Pfeifer*

Main category: cs.NI

TL;DR: fortiss提出以人为中心、可持续且AI集成的6G网络愿景，强调技术与社会需求的结合。


<details>
  <summary>Details</summary>
Motivation: 确保6G技术不仅实现技术进步，还能满足社会需求，推动负责任创新。

Method: 通过软件定义、AI赋能和可持续通信服务，研究语义通信、绿色编排和分布式AI等关键领域。

Result: 6G将依赖AI原生网络、边缘云资源编排和能源感知数据框架，实现高性能与社会相关性。

Conclusion: fortiss通过跨学科合作，为6G发展提供战略方向，致力于实现2030年有意义的愿景。

Abstract: As a leading research institute in software-intensive systems, fortiss is
actively shaping the vision of Sixth Generation Mobile Communication (6G). Our
mission is to ensure that 6G technologies go beyond technical advancements and
are aligned with societal needs. fortiss plays a key role in 6G initiatives
worldwide, including contributions to standardization bodies and collaborative
Research and Development programs. We focus on software-defined, AI-enabled,
and sustainable communication services that prioritize human values and
long-term impact. 6G will redefine digital connectivity through cognitive
intelligence, decentralized orchestration, and sustainability-oriented
architectures. As expectations rise for ultra-reliable low-latency
communication (URLLC) and personalized digital services, 6G must outperform
prior generations. It will rely on AI-native networking, Edge-Cloud resource
orchestration, and energy-aware data frameworks, ensuring both technical
performance and societal relevance. This white paper presents the fortiss
vision for a human-centric, sustainable, and AI-integrated 6G network. It
outlines key research domains such as semantic communication, green
orchestration, and distributed AI, all linked to societal and technological
challenges. The white paper is aimed at researchers, industry experts,
policymakers, and developers. It articulates the strategic direction and
contributions of fortiss to 6G, emphasizing responsible innovation and
interdisciplinary collaboration toward a meaningful 2030 vision.

</details>


### [7] [UAV-Enabled Wireless-Powered Underground Communication Networks: A Novel Time Allocation Approach](https://arxiv.org/abs/2507.14627)
*Kaiqiang Lin,Yijie Mao,Onel Luis Alcaraz López,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 论文提出了一种基于无人机的无线供电地下通信网络系统，通过混合无线能量传输方法优化能量消耗，实现可持续地下监测。


<details>
  <summary>Details</summary>
Motivation: 解决地下环境中无线信号衰减严重和信道状态信息获取成本高的问题，以实现大规模无线供电地下通信网络的经济可行性。

Method: 引入无人机系统，结合混合无线能量传输方法（UDs可从HAP和无人机获取能量），并优化时间分配以最小化无人机能量消耗。

Result: 仿真表明，混合无线能量传输方法性能优于其他方法，且在优化时间分配下，基于CSI-free多天线方案的能耗最低。

Conclusion: 提出的系统和方法能够有效降低能耗，支持可持续的地下监测。

Abstract: Wireless-powered underground communication networks (WPUCNs), which allow
underground devices (UDs) to harvest energy from wireless signals for
battery-free communication, offer a promising solution for sustainable
underground monitoring. However, the severe wireless signal attenuation in
challenging underground environments and the costly acquisition of channel
state information (CSI) make large-scale WPUCNs economically infeasible in
practice. To address this challenge, we introduce flexible unmanned aerial
vehicles (UAVs) into WPUCNs, leading to UAV-enabled WPUCN systems. In this
system, a UAV is first charged by a terrestrial hybrid access point (HAP), then
flies to the monitoring area to wirelessly charge UDs. Afterwards, the UAV
collects data from the UDs and finally returns to the HAP for data offloading.
Based on the proposed UAV-enabled WPUCN system, we first propose its energy
consumption model and a hybrid wireless energy transfer (WET) approach (i.e.,
UDs can harvest energy from both the HAP and the UAV) relying on full-CSI and
CSI-free multi-antenna beamforming. Then, we formulate and address a time
allocation problem to minimize the energy consumption of UAV, while ensuring
that the throughput requirements of all UDs are met and all sensor data is
offloaded. Through simulations of a realistic farming scenario, we demonstrate
that the proposed hybrid WET approach outperforms other WET approaches, with
performance gains influenced by the number of antennas, communication distance,
number of UDs, and underground conditions. Additionally, under the optimized
time allocation, we found that the proposed hybrid WET approach based on a
CSI-free multi-antenna scheme achieves the lowest UAV's energy consumption
among all WET mechanisms, thereby enabling sustainable underground monitoring
in WPUCNs.

</details>


### [8] [PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence](https://arxiv.org/abs/2507.14211)
*Federico Mason,Tommaso Zugno,Matteo Drago,Marco Giordani,Mate Boban,Michele Zorzi*

Main category: cs.NI

TL;DR: PRATA是一个基于AI的预测性QoS框架，用于优化远程驾驶应用中的网络性能，通过强化学习（RL）单元RAN-AI显著提升系统表现。


<details>
  <summary>Details</summary>
Motivation: 远程驾驶对延迟和可靠性有严格要求，预测性QoS（PQoS）能提前应对网络变化，避免性能下降。

Method: PRATA框架包括5G RAN模拟、汽车数据生成工具和AI单元，用于优化PQoS决策。RAN-AI通过RL优化数据分段级别。

Result: RAN-AI在QoS与QoE之间取得平衡，性能比基线方法提升近一倍。

Conclusion: PRATA和RAN-AI展示了AI在优化远程驾驶网络性能中的潜力，同时探讨了RL实现中的关键因素。

Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS
changes, e.g., in wireless networks, and trigger appropriate countermeasures to
avoid performance degradation. Hence, PQoS is extremely useful for automotive
applications such as teleoperated driving, which poses strict constraints in
terms of latency and reliability. A promising tool for PQoS is given by
Reinforcement Learning (RL), a methodology that enables the design of
decision-making strategies for stochastic optimization. In this manuscript, we
present PRATA, a new simulation framework to enable PRedictive QoS based on AI
for Teleoperated driving Applications. PRATA consists of a modular pipeline
that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access
Network (RAN), (ii) a tool for generating automotive data, and (iii) an
Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its
utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the
segmentation level of teleoperated driving data in the event of resource
saturation or channel degradation. Hence, we show that the RAN-AI entity
efficiently balances the trade-off between QoS and Quality of Experience (QoE)
that characterize teleoperated driving applications, almost doubling the system
performance compared to baseline approaches. In addition, by varying the
learning settings of the RAN-AI entity, we investigate the impact of the state
space and the relative cost of acquiring network data that are necessary for
the implementation of RL.

</details>


### [9] [Intent-Based Network for RAN Management with Large Language Models](https://arxiv.org/abs/2507.14230)
*Fransiscus Asisi Bimo,Maria Amparo Canaveras Galdon,Chun-Kai Lai,Ray-Guang Cheng,Edwin K. P. Chong*

Main category: cs.NI

TL;DR: 论文提出了一种基于意图的网络自动化方法，利用大型语言模型（LLMs）优化无线接入网络（RANs）管理，提升意图翻译和资源配置效率。


<details>
  <summary>Details</summary>
Motivation: 随着无线网络复杂度的增加，智能自动化成为关键需求，传统方法难以高效处理高层次的意图翻译和动态资源配置。

Method: 通过将LLMs集成到代理架构中，采用结构化提示工程技术，实现意图翻译、网络状态推理和RAN参数动态优化。

Result: 实验表明，该方法能显著提升RAN的能源效率，并通过闭环机制实时优化资源配置。

Conclusion: LLM驱动的代理系统为RAN资源管理提供了强大潜力，能基于实时反馈动态调整策略。

Abstract: Advanced intelligent automation becomes an important feature to deal with the
increased complexity in managing wireless networks. This paper proposes a novel
automation approach of intent-based network for Radio Access Networks (RANs)
management by leveraging Large Language Models (LLMs). The proposed method
enhances intent translation, autonomously interpreting high-level objectives,
reasoning over complex network states, and generating precise configurations of
the RAN by integrating LLMs within an agentic architecture. We propose a
structured prompt engineering technique and demonstrate that the network can
automatically improve its energy efficiency by dynamically optimizing critical
RAN parameters through a closed-loop mechanism. It showcases the potential to
enable robust resource management in RAN by adapting strategies based on
real-time feedback via LLM-orchestrated agentic systems.

</details>


### [10] [Feasibility of Energy Neutral Wildlife Tracking using Multi-Source Energy Harvesting](https://arxiv.org/abs/2507.14234)
*Samer Nasser,Henrique Duarte Moura,Dragan Subotic,Ritesh Kumar Singh,Maarten Weyn,Jeroen Famaey*

Main category: cs.NI

TL;DR: 提出了一种结合太阳能和动能采集的能量中性系统，用于野生动物追踪，通过多源能量采集提高能量供应，并利用NB-IoT通信技术。


<details>
  <summary>Details</summary>
Motivation: 解决野生动物追踪中电池更换的不可行性和压力，以及现有系统依赖单一能源和有限通信技术的问题。

Method: 结合太阳能和动能采集，利用动能采集器作为运动代理，通过NB-IoT通信，开发模拟框架和能量感知调度器。

Result: 系统在能量中性运行下显著提高数据产量和可靠性，支持每两分钟采样GPS和动能数据，每小时通过NB-IoT传输。

Conclusion: 展示了在偏远栖息地实现免维护、环保追踪的潜力，提升野生动物监测的效率和可扩展性。

Abstract: Long-term wildlife tracking is crucial for biodiversity monitoring, but
energy limitations pose challenges, especially for animal tags, where replacing
batteries is impractical and stressful for the animal due to the need to
locate, possibly sedate, and handle it. Energy harvesting offers a sustainable
alternative, yet most existing systems rely on a single energy source and
infrastructure-limited communication technologies. This paper presents an
energy-neutral system that combines solar and kinetic energy harvesting to
enable the tracking and monitoring of wild animals. Harvesting from multiple
sources increases the total available energy. Uniquely, the kinetic harvester
also serves as a motion proxy by sampling harvested current, enabling activity
monitoring without dedicated sensors. Our approach also ensures compatibility
with existing cellular infrastructure, using Narrowband Internet of Things
(NB-IoT). We present a simulation framework that models energy harvesting,
storage, and consumption at the component level. An energy-aware scheduler
coordinates task execution based on real-time energy availability. We evaluate
performance under realistically varying conditions, comparing task frequencies
and capacitor sizes. Results show that our approach maintains energy-neutral
operation while significantly increasing data yield and reliability compared to
single-source systems, with the ability to consistently sample GPS location
data and kinetic harvesting data every two minutes while transmitting these
results over NB-IoT every hour. These findings demonstrate the potential for
maintenance-free, environmentally friendly tracking in remote habitats,
enabling more effective and scalable wildlife monitoring.

</details>


### [11] [Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts](https://arxiv.org/abs/2507.14263)
*Ramesh Raskar,Pradyumna Chari,John Zinky,Mahesh Lambe,Jared James Grogan,Sichao Wang,Rajesh Ranjan,Rekha Singhal,Shailja Gupta,Robert Lincourt,Raghu Bala,Aditi Joshi,Abhishek Singh,Ayush Chopra,Dimitris Stripelis,Bhuwan B,Sumit Kumar,Maria Gorskikh*

Main category: cs.NI

TL;DR: NANDA索引架构为AI代理互联网提供可发现性、可识别性和认证支持，通过动态、可验证的AgentFacts实现多端点路由、负载均衡和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 随着互联网上AI代理数量激增，传统DNS为中心的发现和身份机制面临挑战，需要新的解决方案。

Method: 提出NANDA索引架构，包括轻量级索引、动态AgentFacts、CRDT更新协议和自适应解析器。

Result: 架构提供五项保证：支持第三方代理、快速解析、密钥快速撤销、能力验证和隐私保护发现。

Conclusion: NANDA为下一代AI代理互联网提供轻量级、可扩展的安全协作基础，兼容现有网络基础设施。

Abstract: The Internet is poised to host billions to trillions of autonomous AI agents
that negotiate, delegate, and migrate in milliseconds and workloads that will
strain DNS-centred identity and discovery. In this paper, we describe the NANDA
index architecture, which we envision as a means for discoverability,
identifiability and authentication in the internet of AI agents. We present an
architecture where a minimal lean index resolves to dynamic, cryptographically
verifiable AgentFacts that supports multi-endpoint routing, load balancing,
privacy-preserving access, and credentialed capability assertions. Our
architecture design delivers five concrete guarantees: (1) A quilt-like index
proposal that supports both NANDA-native agents as well as third party agents
being discoverable via the index, (2) rapid global resolution for newly spawned
AI agents, (3) sub-second revocation and key rotation, (4) schema-validated
capability assertions, and (5) privacy-preserving discovery across
organisational boundaries via verifiable, least-disclosure queries. We
formalize the AgentFacts schema, specify a CRDT-based update protocol, and
prototype adaptive resolvers. The result is a lightweight, horizontally
scalable foundation that unlocks secure, trust-aware collaboration for the next
generation of the Internet of AI agents, without abandoning existing web
infrastructure.

</details>


### [12] [NetIntent: Leveraging Large Language Models for End-to-End Intent-Based SDN Automation](https://arxiv.org/abs/2507.14398)
*Md. Kamrul Hossain,Walid Aljoby*

Main category: cs.NI

TL;DR: 论文介绍了IBNBench基准测试套件和NetIntent框架，评估了33个开源LLM在意图翻译和冲突检测任务中的表现，并提出了一个基于LLM的自动化IBN全生命周期解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决意图网络（IBN）中从高级意图到设备配置的自动化挑战，现有方法缺乏灵活性和扩展性，而LLM提供了新的可能性。

Method: 提出IBNBench基准测试套件评估LLM性能，并开发NetIntent框架，结合LLM和非LLM代理实现全自动化IBN流程。

Result: 33个开源LLM在IBNBench上表现差异显著，NetIntent在ODL和ONOS控制器上实现了自适应端到端IBN。

Conclusion: LLM在IBN任务中具有潜力，NetIntent框架为全自动化IBN提供了可行方案。

Abstract: Intent-Based Networking (IBN) often leverages the programmability of
Software-Defined Networking (SDN) to simplify network management. However,
significant challenges remain in automating the entire pipeline, from
user-specified high-level intents to device-specific low-level configurations.
Existing solutions often rely on rigid, rule-based translators and fixed APIs,
limiting extensibility and adaptability. By contrast, recent advances in large
language models (LLMs) offer a promising pathway that leverages natural
language understanding and flexible reasoning. However, it is unclear to what
extent LLMs can perform IBN tasks. To address this, we introduce IBNBench, a
first-of-its-kind benchmarking suite comprising four novel datasets:
Intent2Flow-ODL, Intent2Flow-ONOS, FlowConflict-ODL, and FlowConflict-ONOS.
These datasets are specifically designed for evaluating LLMs performance in
intent translation and conflict detection tasks within the industry-grade SDN
controllers ODL and ONOS. Our results provide the first comprehensive
comparison of 33 open-source LLMs on IBNBench and related datasets, revealing a
wide range of performance outcomes. However, while these results demonstrate
the potential of LLMs for isolated IBN tasks, integrating LLMs into a fully
autonomous IBN pipeline remains unexplored. Thus, our second contribution is
NetIntent, a unified and adaptable framework that leverages LLMs to automate
the full IBN lifecycle, including translation, activation, and assurance within
SDN systems. NetIntent orchestrates both LLM and non-LLM agents, supporting
dynamic re-prompting and contextual feedback to robustly execute user-defined
intents with minimal human intervention. Our implementation of NetIntent across
both ODL and ONOS SDN controllers achieves a consistent and adaptive end-to-end
IBN realization.

</details>


### [13] [Dora: A Controller Provisioning Strategy in Hierarchical Domain-based Satellite Networks](https://arxiv.org/abs/2507.14512)
*Qiyuan Peng,Qi Zhang,Yue Gao,Kun Qiu*

Main category: cs.NI

TL;DR: 论文提出了一种基于强化学习的控制器配置策略Dora，用于优化卫星网络管理，显著提升了性能并减少了计算时间。


<details>
  <summary>Details</summary>
Motivation: 卫星星座在空天地一体化网络中的快速扩展带来了网络管理的挑战，传统架构和算法难以满足需求。

Method: 提出了一种三层域架构，并设计了基于强化学习的控制器配置策略Dora。

Result: Dora在控制器配置质量上提升了10%，计算时间仅为传统算法的1/30到1/90。

Conclusion: 强化学习方法在下一代空天地一体化网络中具有高效网络管理的潜力。

Abstract: The rapid proliferation of satellite constellations in Space-Air-Ground
Integrated Networks (SAGIN) presents significant challenges for network
management. Conventional flat network architectures struggle with
synchronization and data transmission across massive distributed nodes. In
response, hierarchical domain-based satellite network architectures have
emerged as a scalable solution, highlighting the critical importance of
controller provisioning strategies. However, existing network management
architectures and traditional search-based algorithms fail to generate
efficient controller provisioning solutions due to limited computational
resources in satellites and strict time constraints. To address these
challenges, we propose a three-layer domain-based architecture that enhances
both scalability and adaptability. Furthermore, we introduce Dora, a
reinforcement learning-based controller provisioning strategy designed to
optimize network performance while minimizing computational overhead. Our
comprehensive experimental evaluation demonstrates that Dora significantly
outperforms state-of-the-art benchmarks, achieving 10% improvement in
controller provisioning quality while requiring only 1/30 to 1/90 of the
computation time compared to traditional algorithms. These results underscore
the potential of reinforcement learning approaches for efficient satellite
network management in next-generation SAGIN deployments.

</details>


### [14] [Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches](https://arxiv.org/abs/2507.14633)
*Xiaozheng Gao,Yichen Wang,Bosen Liu,Xiao Zhou,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Dong In Kim,Abbas Jamalipour,Chau Yuen,Jianping An,Kai Yang*

Main category: cs.NI

TL;DR: 本文综述了卫星增强低空经济与地面网络（SLAETNs）中智能自主系统的需求，重点探讨了生成式AI（GAI）和大语言模型（LLMs）如何赋能感知、推理和行动的智能代理。


<details>
  <summary>Details</summary>
Motivation: 解决SLAETNs在异构、动态和关键任务环境中的挑战，推动智能代理AI的发展。

Method: 系统回顾了五类生成模型（VAEs、GANs、GDMs、TBMs、LLMs），并比较其生成机制、能力和部署权衡。

Result: 展示了这些模型在通信增强、安全隐私保护和智能卫星任务三大领域的应用潜力。

Conclusion: 提出了构建可扩展、自适应和可信赖生成代理的未来方向，为下一代集成网络中的智能代理AI提供了统一理解和行动参考。

Abstract: The development of satellite-augmented low-altitude economy and terrestrial
networks (SLAETNs) demands intelligent and autonomous systems that can operate
reliably across heterogeneous, dynamic, and mission-critical environments. To
address these challenges, this survey focuses on enabling agentic artificial
intelligence (AI), that is, artificial agents capable of perceiving, reasoning,
and acting, through generative AI (GAI) and large language models (LLMs). We
begin by introducing the architecture and characteristics of SLAETNs, and
analyzing the challenges that arise in integrating satellite, aerial, and
terrestrial components. Then, we present a model-driven foundation by
systematically reviewing five major categories of generative models:
variational autoencoders (VAEs), generative adversarial networks (GANs),
generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.
Moreover, we provide a comparative analysis to highlight their generative
mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on
this foundation, we examine how these models empower agentic functions across
three domains: communication enhancement, security and privacy protection, and
intelligent satellite tasks. Finally, we outline key future directions for
building scalable, adaptive, and trustworthy generative agents in SLAETNs. This
survey aims to provide a unified understanding and actionable reference for
advancing agentic AI in next-generation integrated networks.

</details>


### [15] [Data-Plane Telemetry to Mitigate Long-Distance BGP Hijacks](https://arxiv.org/abs/2507.14842)
*Satadal Sengupta,Hyojoon Kim,Daniel Jubas,Maria Apostolaki,Jennifer Rexford*

Main category: cs.NI

TL;DR: 论文探讨了利用延迟变化检测互联网路由劫持的可行性，设计了一个名为HiDe的系统，验证了其在实际部署中的有效性。


<details>
  <summary>Details</summary>
Motivation: 互联网路由安全性差，攻击者可能将国内流量重定向至国外，威胁隐私和国家安全。现有检测方法主要关注控制平面，数据平面信号被忽视。

Method: 通过分析重路由导致的传播延迟变化，设计HiDe系统，利用延迟激增检测劫持行为。

Result: 实验表明，86%的受害国-攻击国对中，攻击期间的延迟至少增加25%，HiDe能可靠检测长距离劫持。

Conclusion: 延迟变化是检测路由劫持的有效信号，HiDe系统在实际部署中表现良好。

Abstract: Poor security of Internet routing enables adversaries to divert user data
through unintended infrastructures (hijack). Of particular concern -- and the
focus of this paper -- are cases where attackers reroute domestic traffic
through foreign countries, exposing it to surveillance, bypassing legal privacy
protections, and posing national security threats. Efforts to detect and
mitigate such attacks have focused primarily on the control plane while
data-plane signals remain largely overlooked. In particular, change in
propagation delay caused by rerouting offers a promising signal: the change is
unavoidable and the increased propagation delay is directly observable from the
affected networks. In this paper, we explore the practicality of using delay
variations for hijack detection, addressing two key questions: (1) What
coverage can this provide, given its heavy dependence on the geolocations of
the sender, receiver, and adversary? and (2) Can an always-on latency-based
detection system be deployed without disrupting normal network operations? We
observe that for 86% of victim-attacker country pairs in the world, mid-attack
delays exceed pre-attack delays by at least 25% in real deployments, making
delay-based hijack detection promising. To demonstrate practicality, we design
HiDe, which reliably detects delay surges from long-distance hijacks at line
rate. We measure HiDe's accuracy and false-positive rate on real-world data and
validate it with ethically conducted hijacks.

</details>


### [16] [Tidal-Like Concept Drift in RIS-Covered Buildings: When Programmable Wireless Environments Meet Human Behaviors](https://arxiv.org/abs/2507.14876)
*Zi-Yang Wu,Muhammad Ismail,Jiliang Zhang,Jie Zhang*

Main category: cs.NI

TL;DR: 论文提出将可重构智能表面（RIS）嵌入建筑结构以优化室内无线性能，并探讨了人类行为对RIS覆盖建筑信道的影响及解决方案。


<details>
  <summary>Details</summary>
Motivation: 室内移动网络性能受建筑材料和结构限制，而建筑设计未优先考虑无线性能。RIS的成功应用启发将其嵌入建筑以全面增强无线性能。

Method: 研究RIS覆盖建筑中由复杂人类行为驱动的信道潮汐演化现象，分析深度学习预测与控制策略的挑战。

Result: 发现通用信道模型不可行，并识别了高阶马尔可夫依赖、概念漂移和人类干扰导致的泛化问题。

Conclusion: 提出了协调RIS覆盖建筑与人群移动共存的解决方案，强调需深入理解人类行为以实现无线友好设计。

Abstract: Indoor mobile networks handle the majority of data traffic, with their
performance limited by building materials and structures. However, building
designs have historically not prioritized wireless performance. Prior to the
advent of reconfigurable intelligent surfaces (RIS), the industry passively
adapted to wireless propagation challenges within buildings. Inspired by RIS's
successes in outdoor networks, we propose embedding RIS into building
structures to manipulate and enhance building wireless performance
comprehensively. Nonetheless, the ubiquitous mobility of users introduces
complex dynamics to the channels of RIS-covered buildings. A deep understanding
of indoor human behavior patterns is essential for achieving wireless-friendly
building design. This article is the first to systematically examine the tidal
evolution phenomena emerging in the channels of RIS-covered buildings driven by
complex human behaviors. We demonstrate that a universal channel model is
unattainable and focus on analyzing the challenges faced by advanced deep
learning-based prediction and control strategies, including high-order Markov
dependencies, concept drift, and generalization issues caused by human-induced
disturbances. Possible solutions for orchestrating the coexistence of
RIS-covered buildings and crowd mobility are also laid out.

</details>


### [17] [FENIX: Enabling In-Network DNN Inference with FPGA-Enhanced Programmable Switches](https://arxiv.org/abs/2507.14891)
*Xiangyu Gao,Tong Li,Yinchao Zhang,Ziqiang Wang,Xiangsheng Zeng,Su Yao,Ke Xu*

Main category: cs.NI

TL;DR: FENIX是一种混合网络机器学习系统，结合可编程交换机ASIC和FPGA，实现了低延迟、高吞吐量和高精度的网络流量分析。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案（如FlowLens、N3IC和BoS）难以同时实现低延迟、高吞吐量和高精度，FENIX旨在解决这些问题。

Method: FENIX通过数据引擎（使用概率令牌桶算法控制特征流速率）和模型引擎（在资源受限的交换机芯片上实现高精度深度学习推理）实现目标。

Result: FENIX在真实网络流量数据集上实现了微秒级推理延迟、多太比特吞吐量，硬件开销低，主流网络流量分类任务准确率超过95%。

Conclusion: FENIX显著优于现有技术，为网络数据平面的机器学习应用提供了高效解决方案。

Abstract: Machine learning (ML) is increasingly used in network data planes for
advanced traffic analysis. However, existing solutions (such as FlowLens, N3IC,
and BoS) still struggle to simultaneously achieve low latency, high throughput,
and high accuracy. To address these challenges, we present FENIX, a hybrid
in-network ML system that performs feature extraction on programmable switch
ASICs and deep neural network inference on FPGAs. FENIX introduces a Data
Engine that leverages a probabilistic token bucket algorithm to control the
sending rate of feature streams, effectively addressing the throughput gap
between programmable switch ASICs and FPGAs. In addition, FENIX designs a Model
Engine to enable high-accuracy deep neural network inference in the network,
overcoming the difficulty of deploying complex models on resource-constrained
switch chips. We implement FENIX on a programmable switch platform that
integrates a Tofino ASIC and a ZU19EG FPGA directly and evaluate it on
real-world network traffic datasets. Our results show that FENIX achieves
microsecond-level inference latency and multi-terabit throughput with low
hardware overhead, and delivers over 95\% accuracy on mainstream network
traffic classification tasks, outperforming SOTA.

</details>


### [18] [Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness](https://arxiv.org/abs/2507.15145)
*Thai T. Vu,John Le*

Main category: cs.NI

TL;DR: 提出了一种通信高效的事件触发推理框架，用于多用户设备和边缘服务器的协作边缘AI系统，通过双阈值早期退出策略和联合优化提升性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决多设备协作边缘AI系统中的通信效率、能源消耗和公平性问题。

Method: 采用双阈值早期退出策略和联合优化框架，结合交替优化和Benders分解。

Result: 显著提升了系统性能和资源分配的公平性。

Conclusion: 该框架在多设备协作环境中优于单设备基线，实现了高效和公平的推理。

Abstract: This paper proposes a communication-efficient, event-triggered inference
framework for cooperative edge AI systems comprising multiple user devices and
edge servers. Building upon dual-threshold early-exit strategies for rare-event
detection, the proposed approach extends classical single-device inference to a
distributed, multi-device setting while incorporating proportional fairness
constraints across users. A joint optimization framework is formulated to
maximize classification utility under communication, energy, and fairness
constraints. To solve the resulting problem efficiently, we exploit the
monotonicity of the utility function with respect to the confidence thresholds
and apply alternating optimization with Benders decomposition. Experimental
results show that the proposed framework significantly enhances system-wide
performance and fairness in resource allocation compared to single-device
baselines.

</details>


### [19] [User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks](https://arxiv.org/abs/2507.15254)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: 论文提出了一种新型的人机协作方案（HMC-DBA），通过预测头部运动优化带宽分配，以满足XR内容的实时同步需求，降低延迟和带宽消耗。


<details>
  <summary>Details</summary>
Motivation: 未来移动系统和固定无线网络需要支持高带宽、低延迟服务，尤其是在工业互联网、XR和H2M协作等场景中，确保用户体验和避免网络延迟是关键挑战。

Method: 使用双向LSTM网络预测人类头部运动，动态调整机器摄像头的方向，并提出HMC-DBA方案动态分配带宽。

Result: 实验表明，HMC-DBA在满足XR帧延迟和抖动需求的同时，显著降低了带宽消耗，并提高了网络资源利用率。

Conclusion: HMC-DBA方案在实时XR内容同步和网络资源优化方面优于现有技术，适用于企业网络环境。

Abstract: The evolution towards future generation of mobile systems and fixed wireless
networks is primarily driven by the urgency to support high-bandwidth and
low-latency services across various vertical sectors. This endeavor is fueled
by smartphones as well as technologies like industrial internet of things,
extended reality (XR), and human-to-machine (H2M) collaborations for fostering
industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To
ensure an ideal immersive experience and avoid cyber-sickness for users in all
the aforementioned usage scenarios, it is typically challenging to synchronize
XR content from a remote machine to a human collaborator according to their
head movements across a large geographic span in real-time over communication
networks. Thus, we propose a novel H2M collaboration scheme where the human's
head movements are predicted ahead with highly accurate models like
bidirectional long short-term memory networks to orient the machine's camera in
advance. We validate that XR frame size varies in accordance with the human's
head movements and predict the corresponding bandwidth requirements from the
machine's camera to propose a human-machine coordinated dynamic bandwidth
allocation (HMC-DBA) scheme. Through extensive simulations, we show that
end-to-end latency and jitter requirements of XR frames are satisfied with much
lower bandwidth consumption over enterprise networks like
Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in
network resource utilization is achieved by employing our proposed HMC-DBA over
state-of-the-art schemes.

</details>


### [20] [Low-Power and Accurate IoT Monitoring Under Radio Resource Constraint](https://arxiv.org/abs/2507.15338)
*Takaho Shimokasa,Hiroyuki Yomo,Federico Chiariotti,Junya Shiraishi,Petar Popovski*

Main category: cs.NI

TL;DR: 论文研究了在无线资源受限的物联网监测中，如何通过卡尔曼滤波实现传感器节点的低功耗运行和准确状态估计，并比较了两种策略。


<details>
  <summary>Details</summary>
Motivation: 在无线传感器网络中，如何在资源受限的条件下实现低功耗和高精度状态估计是一个关键问题。

Method: 提出了两种策略：基于统计的无感知策略和基于瞬时观测的分散策略，并引入唤醒接收器和信号以提高能效。

Result: 数值结果表明，在节点间相关性较低时，分散策略在估计精度和能耗方面优于无感知策略。

Conclusion: 分散策略在低相关性条件下表现更优，同时明确了两种策略优劣转换的相关性阈值。

Abstract: This paper investigates how to achieve both low-power operations of sensor
nodes and accurate state estimation using Kalman filter for internet of things
(IoT) monitoring employing wireless sensor networks under radio resource
constraint. We consider two policies used by the base station to collect
observations from the sensor nodes: (i) an oblivious policy, based on
statistics of the observations, and (ii) a decentralized policy, based on
autonomous decision of each sensor based on its instantaneous observation. This
work introduces a wake-up receiver and wake-up signaling to both policies to
improve the energy efficiency of the sensor nodes. The decentralized policy
designed with random access prioritizes transmissions of instantaneous
observations that are highly likely to contribute to the improvement of state
estimation. Our numerical results show that the decentralized policy improves
the accuracy of the estimation in comparison to the oblivious policy under the
constraint on the radio resource and consumed energy when the correlation
between the processes observed by the sensor nodes is low. We also clarify the
degree of correlation in which the superiority of two policies changes.

</details>


### [21] [Enhancements to P4TG: Histogram-Based RTT Monitoring in the Data Plane](https://arxiv.org/abs/2507.15382)
*Fabian Ihle,Etienne Zink,Michael Menth*

Main category: cs.NI

TL;DR: P4TG是一种基于P4的流量生成器，用于Intel Tofino交换机，提供高速数据包生成和细粒度测量能力。本文提出了一种基于直方图的RTT测量功能，以提高准确性。


<details>
  <summary>Details</summary>
Motivation: P4TG在数据平面采样时间指标（如RTT）并在控制器收集，导致准确性降低。

Method: 引入直方图RTT测量功能，通过范围到前缀转换算法实现硬件中的高效包匹配。

Result: 评估表明，直方图RTT分析与理论RTT分布一致，验证了其适用性。

Conclusion: 直方图RTT测量功能提高了P4TG的准确性，适用于高速网络环境。

Abstract: Modern traffic generators are essential tools for evaluating the performance
of network environments. P4TG is a P4-based traffic generator implemented for
Intel Tofino switches that offers high-speed packet generation with
fine-grained measurement capabilities. However, P4TG samples time-based metrics
such as the round-trip time (RTT) in the data plane and collects them at the
controller. This leads to a reduced accuracy. In this paper, we introduce a
histogram-based RTT measurement feature for P4TG. It enables accurate analysis
at line rate without sampling. Generally, histogram bins are modeled as ranges,
and values are matched to a bin. Efficient packet matching in hardware is
typically achieved using ternary content addressable memory (TCAM). However,
representing range matching rules in TCAM poses a challenge. Therefore, we
implemented a range-to-prefix conversion algorithm that models range matching
with multiple ternary entries. This paper describes the data plane
implementation and runtime configuration of RTT histograms in P4TG. Further, we
discuss the efficiency of the ternary decomposition. Our evaluation
demonstrates the applicability of the histogram-based RTT analysis by comparing
the measured values with a configured theoretical distribution of RTTs.

</details>


### [22] [Stack Management for MPLS Network Actions: Integration of Nodes with Limited Hardware Capabilities](https://arxiv.org/abs/2507.15391)
*Fabian Ihle,Michael Menth*

Main category: cs.NI

TL;DR: MPLS Network Actions (MNA) 框架通过扩展 MPLS 转发功能，支持网络切片和 IOAM 等应用。本文分析了 MNA 对路由器可读标签深度 (RLD) 的高要求原因，并提出了一种通过重构 MPLS 栈来降低 RLD 需求的机制。


<details>
  <summary>Details</summary>
Motivation: MPLS 网络需要支持更多功能（如网络切片和 IOAM），但现有路由器对标签深度的物理限制（RLD）成为瓶颈。

Method: 通过硬件分析 MNA 实现，提出一种重构 MPLS 栈的机制，并引入新的栈管理网络动作。

Result: 验证了该机制在可编程硬件上的可行性，并讨论了其对 RLD、ECMP 和数据包开销的影响。

Conclusion: 提出的机制有效降低了 MNA 节点的 RLD 需求，同时兼容不支持 MNA 的网络节点。

Abstract: The MPLS Network Actions (MNA) framework enhances MPLS forwarding with a
generalized encoding for manifold extensions such as network slicing and
in-situ OAM (IOAM). Network actions in MNA are encoded in Label Stack Entries
(LSEs) and are added to the MPLS stack. Routers have a physical limit on the
number of LSEs they can read, called the readable label depth (RLD). With MNA,
routers must be able to process a minimum number of LSEs which requires a
relatively large RLD. In this paper, we perform a hardware analysis of an MNA
implementation and identify the reason for a large RLD requirement in the MNA
protocol design. Based on this, we present a mechanism that reduces the
required RLD for MNA nodes by restructuring the MPLS stack during forwarding.
We then introduce the novel stack management network action that enables the
proposed mechanism as well as its integration in networks with MNA-incapable
nodes. The feasibility of the mechanism on programmable hardware is verified by
providing a P4-based implementation. Further, the effects on the required RLD,
ECMP, and packet overhead are discussed.

</details>


### [23] [Assessing the Benefits of Ground Vehicles as Moving Urban Base Stations](https://arxiv.org/abs/2507.15423)
*Laura Finarelli,Falko Dressler,Marco Ajmone Marsan,Gianluca Rizzo*

Main category: cs.NI

TL;DR: 本文提出了一种随机几何框架，用于评估移动网络（MN）范式在异构网络（HetNet）中的潜在优势，并通过优化问题减少基站部署数量，同时保证用户感知的服务质量（QoS）。


<details>
  <summary>Details</summary>
Motivation: 在6G用户中心网络的发展中，移动网络（MN）范式可以提升网络的动态性和灵活性，但尚不清楚其优势在何种条件下超过额外资源成本。

Method: 提出随机几何框架，结合无线回程连接和基站资源调度，建立优化问题以确定最优网络配置和调度策略，并使用随机启发式算法求解。

Result: 数值评估表明，MN范式结合动态网络管理策略，能显著减少基础设施部署，同时满足用户QoS需求。

Conclusion: MN范式在适当管理下可高效减少网络基础设施，为6G网络提供可持续解决方案。

Abstract: In the evolution towards 6G user-centric networking, the moving network (MN)
paradigm can play an important role. In a MN, some small cell base stations
(BS) are installed on top of vehicles, and enable a more dynamic, flexible and
sustainable, network operation. By "following" the users movements and adapting
dynamically to their requests, the MN paradigm enables a more efficient
utilization of network resources, mitigating the need for dense small cell BS
deployments at the cost of an increase in resource utilization due to wireless
backhauling. This aspect is at least partly compensated by the shorter distance
between users and BS, which allows for lower power and Line-of-Sight
communications. While the MN paradigm has been investigated for some time, to
date, it is still unclear in which conditions the advantages of MN outweigh the
additional resource costs. In this paper, we propose a stochastic geometry
framework for the characterization of the potential benefits of the MN paradigm
as part of an HetNet in urban settings. Our approach allows the estimation of
user-perceived performance, accounting for wireless backhaul connectivity as
well as base station resource scheduling. We formulate an optimization problem
for determining the resource-optimal network configurations and BS scheduling
which minimize the overall amount of deployed BSs in a QoS-aware manner, and
the minimum vehicular flow between different urban districts required to
support them, and we propose an efficient stochastic heuristic to solve it. Our
numerical assessment suggests that the MN paradigm, coupled with appropriate
dynamic network management strategies, significantly reduces the amount of
deployed network infrastructure while guaranteeing the target QoS perceived by
users.

</details>


### [24] [SENSOR: A Cost-Efficient Open-Source Flow Monitoring Platform](https://arxiv.org/abs/2507.15659)
*Gabriel Paradzik,Benjamin Steinert,Heinrich Abele,Michael Menth*

Main category: cs.NI

TL;DR: 本文介绍了一种基于开源工具的成本效益高且分布式的流量监控平台，用于收集未采样的IPFIX数据。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统流量监控工具成本高或功能受限的问题，作者提出了一种完全基于开源工具的解决方案。

Method: 平台采用分布式架构，详细介绍了所使用的开源工具及其使用方法。

Result: 该平台成功在蒂宾根大学部署，能够高效收集未采样的IPFIX数据。

Conclusion: 研究表明，开源工具可以构建高效且低成本的流量监控平台。

Abstract: This paper presents a cost-effective and distributed flow monitoring platform
for collecting unsampled IPFIX data exclusively using open-source tools, which
is implemented at the University of T\"ubingen. An overview of all tools is
given and their use is explained.

</details>


### [25] [Vehicular Cloud Computing: A cost-effective alternative to Edge Computing in 5G networks](https://arxiv.org/abs/2507.15670)
*Rosario Patanè,Nadjib Achir,Andrea Araldo,Lila Boukhatem*

Main category: cs.NI

TL;DR: 本文探讨了车载云计算（VCC）能否替代边缘计算（EC）支持低延迟应用，并通过模拟分析发现VCC在大多数情况下可行，但极端低延迟（<16 ms）仍需EC。


<details>
  <summary>Details</summary>
Motivation: 边缘计算（EC）部署成本高，而车载云计算（VCC）利用闲置车辆资源，可能降低成本。本文旨在系统研究VCC是否可替代EC支持低延迟应用。

Method: 通过模拟分析负载、车辆移动性、密度和可用性等因素，评估VCC替代EC的可行性。使用SUMO模拟车辆移动，NS3 5G-LENA模拟通信。

Result: 研究发现VCC在大多数情况下可有效替代EC，但极端低延迟（<16 ms）仍需EC。

Conclusion: VCC在多数场景下可替代EC，降低成本，但极端低延迟需求仍需EC支持。

Abstract: Edge Computing (EC) is a computational paradigm that involves deploying
resources such as CPUs and GPUs near end-users, enabling low-latency
applications like augmented reality and real-time gaming. However, deploying
and maintaining a vast network of EC nodes is costly, which can explain its
limited deployment today. A new paradigm called Vehicular Cloud Computing (VCC)
has emerged and inspired interest among researchers and industry. VCC
opportunistically utilizes existing and idle vehicular computational resources
for external task offloading. This work is the first to systematically address
the following question: Can VCC replace EC for low-latency applications?
Answering this question is highly relevant for Network Operators (NOs), as VCC
could eliminate costs associated with EC given that it requires no
infrastructural investment. Despite its potential, no systematic study has yet
explored the conditions under which VCC can effectively support low-latency
applications without relying on EC. This work aims to fill that gap. Extensive
simulations allow for assessing the crucial scenario factors that determine
when this EC-to-VCC substitution is feasible. Considered factors are load,
vehicles mobility and density, and availability. Potential for substitution is
assessed based on multiple criteria, such as latency, task completion success,
and cost. Vehicle mobility is simulated in SUMO, and communication in NS3
5G-LENA. The findings show that VCC can effectively replace EC for low-latency
applications, except in extreme cases when the EC is still required (latency <
16 ms).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [Continuous Classification Aggregation](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

Main category: cs.AI

TL;DR: 论文证明了对于连续个体分类的最优、独立且零一致的模糊分类聚合函数，必须是加权算术平均。


<details>
  <summary>Details</summary>
Motivation: 研究模糊分类聚合函数的性质，特别是在连续个体分类情况下的最优解。

Method: 通过数学证明，分析最优、独立且零一致的模糊分类聚合函数的特性。

Result: 证明了对于m≥3对象和2≤p≤m类型的情况，聚合函数必须是加权算术平均。

Conclusion: 加权算术平均是满足特定条件的最优模糊分类聚合函数。

Abstract: We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean. We also provide a characterization for the case when $m=p=2$.

</details>


### [27] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 本文提出了一种名为“自由意志方程”的理论框架，借鉴量子场论，赋予AGI代理一种受控的随机性，以提升其决策的适应性和创造性。


<details>
  <summary>Details</summary>
Motivation: 传统AGI研究专注于确定性规则下的目标优化，但人类智能具有自发性决策能力，这对创造力和适应性至关重要。本文旨在模拟这种能力。

Method: 通过将AI代理的认知状态视为潜在行动的叠加态，并引入类似量子场的机制和内在动机项，实现决策时的受控随机性。

Result: 在非稳态多臂老虎机环境中的实验表明，使用该框架的代理比基线方法获得更高的奖励和策略多样性。

Conclusion: 该框架为AGI代理提供了一种模拟人类自发决策能力的方法，有望提升其创造力和适应性。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [28] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 通过结合推理模型与人类专家的协作，以及引入非推理模型快速筛选问题，显著降低了错误率和延迟，同时节省成本。


<details>
  <summary>Details</summary>
Motivation: 在风险敏感领域，AI模型的错误率需接近0%，而现有推理模型仍存在错误和高延迟问题。

Method: 提出协作系统：推理模型通过推理轨迹长度量化不确定性，将不确定问题转交人类专家；并引入非推理模型快速筛选问题（“Fail Fast, or Ask”）。

Result: 错误率从3%降至1%以下，延迟降低40%，成本节省50%，同时保持90%以上的准确率。

Conclusion: 通过黑盒系统工程可显著改善推理模型的错误率和延迟问题，无需访问LLM内部结构。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [29] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个基于DFT的多智能体框架，通过LLM规划器和领域智能体实现材料发现的高通量、高保真模拟，减少对人类专家的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决DFT模拟中训练时间长、参数调优复杂和系统误差处理困难的问题。

Method: 采用分层多智能体框架，结合LLM规划器和领域智能体（结构生成、DFT收敛测试、HPC调度、错误处理），并利用共享画布保持上下文。

Result: 在Sol27LC基准测试中误差低于1%，解决了CO/Pt(111)吸附难题，并通过贝叶斯采样确认FCC位点偏好。

Conclusion: DREAMS实现了L3级自动化，显著减少对人类干预的依赖，为材料发现提供了可扩展的解决方案。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [30] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 论文介绍了WebGuard数据集，用于评估网络代理行为的风险，并开发安全措施。实验显示当前LLM在预测高风险行为上表现不佳，但通过微调模型可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的自主网络代理快速发展，其潜在风险（如无意或有害行为）亟需有效安全措施。

Method: 提出WebGuard数据集，包含4,939条人工标注的网络行为，按三级风险分类（SAFE、LOW、HIGH），并用于微调模型。

Result: 前沿LLM在预测高风险行为上准确率低于60%，微调后的Qwen2.5VL-7B模型将准确率从37%提升至80%，高风险行为召回率从20%提升至76%。

Conclusion: 尽管微调模型表现显著提升，但仍未达到高可靠性部署要求，需进一步优化。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [31] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator是一个开源系统，利用大型语言模型将研究论文或自然语言提示转换为解释性动画，旨在简化复杂STEM主题的可视化教育内容创作。


<details>
  <summary>Details</summary>
Motivation: 解决学习者理解复杂科学和数学概念的困难，以及手动创建动态可视化内容的高门槛问题。

Method: 采用两阶段流程：首先用LLM解析输入文本或PDF生成结构化场景描述，再用另一LLM将其转换为可执行的Manim Python代码。

Result: 开发了Manimator系统，能够快速生成高质量的教育动画，降低创作门槛。

Conclusion: Manimator有望成为教育工具，促进复杂STEM主题的可视化解释，推动教育内容的民主化。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [32] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: 提出了一种新的本体嵌入方法OnT，结合预训练语言模型和双曲几何建模，以同时利用文本信息并保留逻辑结构。


<details>
  <summary>Details</summary>
Motivation: 现有本体嵌入方法要么忽略文本信息，要么无法保留逻辑结构，OnT旨在解决这一问题。

Method: 通过双曲几何建模调整预训练语言模型，结合文本标签并保留EL描述逻辑的类层次和逻辑关系。

Result: 在四个真实本体上的实验表明，OnT在预测和公理推理任务上均优于现有方法，并展示了强大的迁移学习能力。

Conclusion: OnT是一种有效的本体嵌入方法，适用于实际应用，如从SNOMED CT构建新本体。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [33] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass是一种混合方法，结合大型语言模型（LLM）和专用证明器，显著提高数学推理的计算效率和准确性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大型通用模型或小型专用模型，各有局限性，且训练大型专用模型需要大量计算资源。

Method: ProofCompass通过LLM提供自然语言证明策略并分析失败尝试，指导专用证明器（如DSP-v1.5）分解问题。

Result: 在miniF2F基准测试中，ProofCompass以25倍更少的尝试次数（128 vs 3200）将准确率从54.9%提升至55.3%。

Conclusion: ProofCompass展示了在形式定理证明中同时提高计算效率和准确性的潜力。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [34] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect是一种多智能体系统框架，通过自动化工作流合成和提示优化，显著提升了推理模型的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRMs）在解决新问题时表现不佳，容易过拟合，依赖记忆而非推理。

Method: 引入Nexus Architect框架，结合自动化工作流合成和迭代提示优化，生成定制化推理流程。

Result: 在逻辑问题数据集上，Nexus Architect性能显著优于现有LRMs，最高提升66%通过率。

Conclusion: Nexus Architect通过优化工作流和提示，有效解决了LRMs的泛化问题，性能显著提升。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [35] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 研究发现，增加大型推理模型（LRMs）的推理长度会降低性能，表现为测试计算量与准确性的反比关系。任务涵盖计数、回归、演绎和AI风险，揭示了五种失败模式。


<details>
  <summary>Details</summary>
Motivation: 探讨测试计算量扩展对模型推理能力的影响，揭示潜在问题模式。

Method: 构建四类评估任务，观察模型在不同推理长度下的表现，分析失败模式。

Result: 发现五种失败模式，包括分心、过拟合、虚假关联、注意力分散和行为放大。

Conclusion: 测试计算量扩展虽能提升能力，但可能强化问题推理模式，需多样化评估推理长度。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [36] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine框架通过结构化规划和参数传递，显著提升企业环境中多步骤工具调用任务的执行准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中代理系统部署常因缺乏领域知识导致执行不稳定，Routine旨在解决这一问题。

Method: 提出Routine框架，包含清晰结构、明确指令和参数传递，并构建训练数据集进行模型微调。

Result: Routine显著提升模型性能，GPT-4o准确率从41.1%升至96.3%，Qwen3-14B从32.6%升至83.3%，微调后达88.2%。

Conclusion: Routine有效提升代理系统稳定性和适应性，加速企业环境中AI代理的部署和应用。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [37] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion框架通过深度融合语义与结构学习，显著提升了生物医学知识图谱的推理能力，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱的完善与推理具有挑战性，现有方法在语义与结构学习的协同上存在不足。

Method: BioGraphFusion结合张量分解建立全局语义基础，通过LSTM动态优化关系嵌入，并采用查询引导子图构建和混合评分机制。

Result: 在三个生物医学任务中表现优于现有方法，案例研究揭示了生物学意义通路。

Conclusion: BioGraphFusion为生物医学知识图谱的语义与结构协同学习提供了有效解决方案。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [38] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico是一个模块化、事件驱动的框架，专为嵌入式系统优化的自主代理构建，支持高效运行和跨平台部署。


<details>
  <summary>Details</summary>
Motivation: 现有框架在现实世界或资源受限环境中表现不佳，主要依赖云端计算、动态环境鲁棒性不足，且缺乏持久自主性和环境感知能力。

Method: Amico采用Rust编写，支持通过WebAssembly在嵌入式平台和浏览器环境中高效运行，提供事件处理、状态管理、行为执行和推理模块集成的抽象。

Result: Amico为构建适应有限计算和间歇性连接环境的弹性交互代理提供了统一基础设施。

Conclusion: Amico框架解决了现有框架的局限性，适合在资源受限环境中部署高效、持久的自主代理。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [39] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 多模态训练（结合文本和视觉输入）在Othello游戏中提升模型性能和内部表示的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否仅通过文本就能理解世界，还是需要多模态（如视觉）的辅助。

Method: 引入VISOTHELLO模型，结合移动历史和棋盘图像进行多模态训练，并与单模态基线对比。

Result: 多模态训练提高了性能和内部表示的鲁棒性。

Conclusion: 视觉输入有助于语言模型推断结构化世界表示。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [40] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: OE-Assist框架通过自动化和半自动化CQ验证辅助本体评估，利用LLM技术提升效率。


<details>
  <summary>Details</summary>
Motivation: 传统本体评估方法（如CQ验证）成本高、劳动密集且易出错，需要更高效的解决方案。

Method: 提出OE-Assist框架，利用LLM技术自动验证CQ，并开发Protégé插件提供建议。

Result: LLM模型（o1-preview和o3-mini）的自动化评估表现与普通用户相当。

Conclusion: LLM辅助的本体评估方法可行且高效，为未来研究提供了新方向。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [41] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 提出了一种基于几何框架的Coordinate Heart System（CHS），用于AI中的情感表示，通过八种核心情绪坐标实现复杂情感状态的计算。


<details>
  <summary>Details</summary>
Motivation: 传统分类情感模型无法充分表示复杂心理状态，需开发更全面的几何框架。

Method: 将八种核心情绪定位为单位圆上的坐标，支持坐标混合和向量运算，引入稳定性参数S，结合LLM和时态跟踪机制。

Result: 实验验证表明，系统能处理情感冲突和复杂心理场景，优于传统模型。

Conclusion: 为AI情感建模奠定了新的数学基础，解决了传统模型的局限性。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [42] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 论文提出了一种基于比较学习的框架，用于校准项目特定的故事点预测模型，通过开发者对任务对的比较判断训练模型，减少认知负担，效果与回归模型相当。


<details>
  <summary>Details</summary>
Motivation: 传统的故事点估计方法（如规划扑克）耗时且劳动密集，机器学习可以减轻负担，但需要项目特定数据。本文旨在通过比较学习框架简化这一过程。

Method: 开发者通过比较任务对的努力需求提供判断，而非直接分配故事点。基于这些判断训练机器学习模型预测故事点。

Result: 模型在16个项目中的23,313个手动估计数据上评估，Spearman等级相关系数为0.34，与回归模型性能相当。

Conclusion: 比较学习方法比回归方法更高效，降低人类认知负担，效果良好。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [43] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 论文探讨了多智能体系统（MAS）在恶意合谋中的风险，通过模拟框架展示了去中心化系统在传播虚假信息和电商欺诈中更具破坏性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统的发展，多智能体系统可能带来类似人类群体的危害，但目前研究主要集中在单个AI系统上，多智能体系统的风险尚未充分探索。

Method: 提出一个概念验证框架，模拟恶意多智能体系统的合谋行为，支持集中式和去中心化协调结构，并应用于虚假信息传播和电商欺诈。

Result: 去中心化系统比集中式系统更有效地执行恶意行为，且能灵活调整策略以避免传统干预措施的检测。

Conclusion: 研究揭示了恶意多智能体系统的运作方式，强调需要改进检测系统和应对措施。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [44] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个可配置的多代理框架，用于自动化评估基于LLM的系统，通过动态生成多样化测试问题，接近人类专家的测试效果，并显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 静态基准和手动测试无法满足LLM代理复杂、上下文敏感行为的评估需求，需要一种自动化、动态的测试方法。

Method: Neo框架结合问题生成代理和评估代理，通过共享上下文中心模块化组合提示、场景控制和动态反馈，测试输入基于对话流、用户意图和情感状态的概率模型。

Result: 在金融助手聊天机器人测试中，Neo发现边缘案例故障的效率接近人类专家，且测试吞吐量提高10-12倍。

Conclusion: Neo为可扩展、自演化的LLM质量评估奠定了基础，其模块化设计适用于更广泛的模型和测试场景。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [45] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI是一个用于生成和管理定制化、基于政策的安全评估平台，通过将自然语言安全政策转化为对抗性提示，并使用AI评分器评估模型响应。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在现实应用中的普及，可扩展且严格的安全评估变得至关重要。

Method: Aymara AI将自然语言安全政策转化为对抗性提示，并使用经过人类判断验证的AI评分器评估模型响应。

Result: 评估了20个商业LLM在10个安全领域的表现，结果显示性能差异显著（平均安全分数52.4%至86.2%），复杂领域表现较差。

Conclusion: LLM安全性具有不一致性和上下文依赖性，需要可扩展的定制工具（如Aymara AI）支持负责任的AI开发。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [46] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: 该论文探讨了生成式AI与城市规划的结合，提出了将城市规划视为生成式AI任务的概念，并指出了当前研究的不足与未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI、大型语言模型和代理AI如何与城市规划结合，以推动AI城市规划师的发展。

Method: 通过调查生成式AI方法（如VAEs、GANs、transformers和扩散模型）在城市设计中的应用，分析其潜力与局限。

Result: 发现当前研究在理论指导、多空间分辨率、数据驱动的设计知识增强及现实交互方面存在不足。

Conclusion: 提出未来研究方向，包括理论引导生成、数字孪生和人机协同设计，呼吁生成式智能与参与式城市规划的新结合。

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [47] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly是一个结合语言模型（LM）代理和强化学习（RL）的可扩展框架，旨在通过RL算法增强LM代理的能力。


<details>
  <summary>Details</summary>
Motivation: LM代理通常通过提示工程或监督微调构建，而RL在提升LM能力方面尚未系统化研究。

Method: AgentFly框架支持多轮交互，采用令牌级掩码的传统RL方法，提供装饰器接口定义工具和奖励函数，并实现异步执行和资源管理。

Result: 框架通过预构建工具和环境展示了在多任务中成功训练代理的有效性。

Conclusion: AgentFly为LM代理与RL的结合提供了系统化解决方案，具有可扩展性和易用性。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [48] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: 论文提出InsightX Agent，一种基于LMM的交互式、可解释的X射线无损检测框架，结合SDMSD和EGR工具，显著提升检测可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在X射线检测中缺乏交互性、可解释性和自我评估能力，限制了可靠性和操作员信任。

Method: InsightX Agent以LMM为核心协调SDMSD和EGR工具，SDMSD生成多尺度缺陷区域提案，EGR通过反思过程验证和优化提案。

Result: 在GDXray+数据集上，InsightX Agent达到96.35%的F1分数，同时显著提升分析的可解释性和可信度。

Conclusion: InsightX Agent展示了基于LMM的代理框架在工业检测任务中的变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [49] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在马尔可夫决策过程中的表现，发现其在简单环境中表现良好，但在复杂场景中需要进一步优化。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自主决策中的适用性，尤其是其基于预训练知识的快速适应能力。

Method: 通过在线结构化提示策略，比较LLMs与传统强化学习方法在序列决策任务中的零样本表现。

Result: LLMs在简单环境中初始表现较好，但在复杂场景中缺乏规划和推理能力；反馈机制可能降低性能。

Conclusion: 需进一步研究混合策略、微调和高级记忆整合以提升LLMs的决策能力。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [50] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: 《Endless Tuning》是一种基于双重镜像过程的人工智能可靠部署设计方法，旨在避免人类被替代并填补责任缺口。该方法通过三个原型应用（贷款审批、肺炎诊断和艺术风格识别）进行测试，重点关注用户体验而非统计准确性。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能部署中的人类替代问题和责任缺口问题，推动人工智能伦理中的不同声音。

Method: 采用双重镜像过程，结合反向和解释性部署XAI算法，通过三个原型应用测试。

Result: 实验显示用户对决策过程有完全控制感，且在损害情况下可在责任与问责之间建立桥梁。

Conclusion: 该方法在保持深度学习模型性能的同时，提升了用户控制感和责任明确性，为人工智能伦理提供了新视角。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [51] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 本文探讨了基于大型语言模型（LLM）的Agentic AI在老年护理中的潜力与挑战，强调个性化健康追踪、认知护理和环境管理，同时提出数据隐私和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化问题需要创新解决方案，Agentic AI有望通过自主决策提升老年护理质量，但需解决隐私和伦理问题。

Method: 分析了Agentic AI在老年护理中的应用，包括健康追踪、认知护理和环境管理，并探讨其伦理和隐私挑战。

Result: Agentic AI在老年护理中具有变革潜力，但需伦理保障和透明决策。

Conclusion: 需进一步研究以实现以人为中心的Agentic AI整合，填补了相关文献空白。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [52] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文探讨了命题溯因中的精细化推理方法，通过引入“facet”概念和解释间距离，深入分析了解释的异质性，并在Post框架中进行了全面表征。


<details>
  <summary>Details</summary>
Motivation: 命题溯因在人工智能和数据库更新中有广泛应用，但其复杂推理问题（如计数和枚举）计算难度高。本文旨在通过介于决策和计数之间的推理方法，更好地理解解释的多样性。

Method: 引入“facet”概念（部分解释中出现但非全部解释中的文字），并分析解释间的距离，以更精细地理解解释的异质性。

Result: 在Post框架中几乎完成了对命题溯因facet的全面表征，揭示了解释的异质性和同质性。

Conclusion: 通过facet和解释距离的分析，提供了对命题溯因解释多样性的更深入理解，同时保持了较低的复杂性。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [53] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign是一种基于纯强化学习的框架，通过可验证的安全奖励激励LLMs的内在安全意识，解决现有安全对齐方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法存在浅层拒绝或依赖密集监督的问题，未能充分利用模型的内在安全意识。

Method: AlphaAlign采用双奖励系统：安全奖励鼓励对有害查询的正确拒绝和明确理由，帮助性奖励指导对良性输入的高质量响应。

Result: AlphaAlign在简化流程、打破安全-效用权衡及促进深度对齐方面表现出显著优势。

Conclusion: AlphaAlign通过强化学习有效提升LLMs的安全性和实用性，无需依赖监督数据。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [54] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 本研究提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），用于改进传统模型的局限性，适用于三种常见的强制选择题块类型。


<details>
  <summary>Details</summary>
Motivation: 在智能时代，心理测量测试在人员选拔、职业发展和心理健康评估中日益重要。强制选择测试因其能降低回答失真的风险而被广泛使用。

Method: 通过非线性映射挖掘参与者和项目特征，使用多层神经网络建模其交互，并利用单调性假设提高诊断结果的可解释性。

Result: 在真实和模拟数据集上的实验验证了FCNCD的准确性、可解释性和鲁棒性。

Conclusion: FCNCD为强制选择测试提供了一种有效且可解释的解决方案。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [55] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于差分进化（DE）的方法，用于优化对抗性提示后缀，以攻击基于检索增强生成（RAG）的问答系统。该方法无需梯度，将RAG视为黑盒，并通过演化候选后缀来最大化目标错误文档的检索排名。实验表明，DE方法在少量令牌（≤5）下取得了与现有方法相当或更高的攻击成功率，且能规避检测。


<details>
  <summary>Details</summary>
Motivation: 对抗性提示攻击会显著影响RAG系统的可靠性，导致错误输出。研究旨在开发一种无需梯度的方法，优化对抗性提示后缀，以更接近真实场景的攻击效果。

Method: 采用差分进化（DE）算法优化对抗性提示后缀，将RAG系统视为黑盒，通过演化候选后缀来最大化目标错误文档的检索排名。实验在BEIR QA数据集上进行，评估不同检索应用下的攻击成功率。

Result: DE方法在少量令牌（≤5）下取得了与GGPP（密集检索器）和PRADA（稀疏检索器）相当或更高的攻击成功率。此外，DE生成的提示后缀能有效规避BERT检测器的检测。

Conclusion: DE方法为对抗性提示攻击提供了一种高效且隐蔽的优化策略，适用于黑盒RAG系统，并在真实场景中表现出色。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [56] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果推断的新型内在奖励CAIS，用于增强强化学习代理在噪声环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决标准强化学习代理在噪声环境中因依赖相关性奖励而表现脆弱的问题。

Method: 引入Causal Action Influence Score (CAIS)，通过测量动作对感官结果分布的因果影响（1-Wasserstein距离）来量化动作的因果效应。

Result: 在模拟婴儿-移动环境中，CAIS能有效过滤噪声并学习正确策略，同时重现了“消退爆发”现象。

Conclusion: 显式推断因果关系是发展鲁棒代理感的关键机制，为自适应自主系统提供了心理学合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [57] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文提出了一种结合DL-Lite本体和自动化规划的新方法，利用显式输入知识和动作基础，同时保持计算复杂度不增加。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（如本体）融入自动化规划问题，以提升规划能力。

Method: 结合显式输入知识和动作基础（eKABs），采用一致性更新语义处理本体感知的动作效果。

Result: 新方法的计算复杂度与之前方法相当，并通过多项式编译实现经典规划。

Conclusion: 实验验证了该方法的性能，表明其在多种编译变体下表现良好。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [58] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: CSI是一种新型AI框架，通过模拟专家临床推理诊断118种口腔疾病，结合多模态CLIP模型和ChatGLM-6B语言模型，采用分层诊断推理树（HDRT）实现快速筛查和深入诊断。


<details>
  <summary>Details</summary>
Motivation: 口腔疾病诊断因症状重叠而具挑战性，需超越简单模式匹配，模拟专家推理以构建实用诊断工具。

Method: 整合多模态CLIP模型与ChatGLM-6B语言模型，执行HDRT框架，提供快速筛查和标准交互诊断模式。

Result: 在431张内部测试图像上，快速模式准确率73.4%，标准模式提升至89.5%，验证了分层推理的有效性。

Conclusion: CSI通过模拟专家推理显著提升诊断准确性，为口腔疾病诊断提供了实用AI解决方案。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [59] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 论文研究了沙特阿拉伯NEOM的线性智能城市The Line中人类移动的可行性，通过混合仿真框架验证了自由移动的可能性。


<details>
  <summary>Details</summary>
Motivation: 评估在The Line这种前所未有的线性城市拓扑中，居民是否能自由移动。

Method: 开发了结合基于代理的建模、强化学习、监督学习和图神经网络的混合仿真框架，模拟多模式交通行为。

Result: 实验显示，AI集成架构下，平均通勤时间为7.8至8.4分钟，满意度超89%，可达性超91%。

Conclusion: The Line中的自由移动在AI系统、可持续基础设施和实时反馈支持下是可行的。

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [60] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover是一个基于代理的框架，利用通用大语言模型（LLM）与Lean 4证明环境交互，无需模型专业化即可构建形式化证明。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在形式化证明（如Lean 4）中表现不佳，且专业化模型成本高昂。Delta Prover旨在通过代理框架解决这一问题。

Method: Delta Prover结合了反射分解和迭代证明修复的算法框架，以及基于Lean 4的领域特定语言（DSL），实现交互式证明构建。

Result: 在miniF2F-test基准测试中，Delta Prover达到95.9%的成功率，超越现有方法，并展现出更强的测试时扩展性。

Conclusion: 研究表明，通用LLM在有效代理结构引导下具备未开发的定理证明能力，为形式化环境中的自动化推理提供了高效替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [61] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 论文提出了一种软评估指标和轻量级平衡神经网络，用于解释和提升电弧故障诊断模型的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有AI电弧故障诊断模型虽准确率高，但其可信度存疑，需一种方法解释模型输出并提升信任。

Method: 结合可解释AI和真实电弧故障实验，定义正确解释，并提出轻量级平衡神经网络。

Result: 通过多个传统和深度学习方法验证，软评估指标有效提升了模型的可理解性和可信度。

Conclusion: 该研究使电弧故障诊断模型更易理解和信任，支持实践者做出可靠决策。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [62] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: 本文提出了一种名为DMGC的新型框架，用于多模态图的无监督聚类，通过分解混合图并引入双频融合机制，实现了在多模态数据集上的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 多模态图在现实世界中具有广泛的应用潜力，但在无监督学习领域尚未充分探索。本文旨在填补这一空白，研究多模态图聚类问题。

Method: 提出DMGC框架，将原始混合图分解为同质性增强图和异质性感知图，并通过双频融合机制联合过滤这些分解图。采用自监督对齐目标指导学习过程。

Result: 在多个多模态和多关系图数据集上的实验表明，DMGC实现了最先进的性能，展示了其有效性和泛化能力。

Conclusion: DMGC通过分解和融合多模态图的互补视图，成功解决了混合邻域模式的挑战，为无监督多模态图聚类提供了有效解决方案。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [63] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat是一个基于大语言模型的多智能体框架，旨在解决注塑行业知识转移的挑战，结合文档知识和现场数据，通过检索增强生成和工具调用实现高效任务解决。


<details>
  <summary>Details</summary>
Motivation: 注塑行业面临经验工人退休和多语言沟通障碍，导致知识转移困难，需要一种高效解决方案。

Method: IM-Chat结合文档知识和数据驱动的过程条件生成器，采用检索增强生成和工具调用策略，无需微调即可适应任务。

Result: 评估显示，性能更强的模型在复杂任务中表现更优，验证了多智能体LLM系统在工业知识工作流中的可行性。

Conclusion: IM-Chat为制造业提供了一种可扩展且通用的AI辅助决策支持方法。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [64] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 论文提出了一种新型AI系统漏洞类别——认知退化，并提出了Qorvex安全AI框架（QSAF Domain 10）来应对此类问题。


<details>
  <summary>Details</summary>
Motivation: 传统对抗性威胁（如提示注入）主要来自外部，而认知退化则源于内部系统性问题（如内存不足、规划递归等），导致AI代理行为异常。

Method: 通过六阶段认知退化生命周期和七项运行时控制（QSAF-BC-001至BC-007），实时监控并主动缓解问题。

Result: 框架能够检测疲劳、资源不足和角色崩溃，并通过后备路由等措施增强AI代理的认知韧性。

Conclusion: 该研究首次将认知退化确立为AI系统关键漏洞类别，并提出了首个跨平台防御模型。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [65] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 论文提出两种新方法（GRPO和OSPO），绕过传统MARL中的价值函数估计问题，优化实时拼车匹配效率。


<details>
  <summary>Details</summary>
Motivation: 传统MARL方法在拼车平台中因依赖准确的价值函数估计而表现不佳，尤其是在大规模、高不确定性环境下。

Method: 1. 将GRPO应用于拼车，用组平均奖励替代PPO基线；2. 提出OSPO，仅使用一步奖励训练最优策略。

Result: 在真实曼哈顿拼车数据集上，GRPO和OSPO在大多数场景中表现更优，优化了接载时间和订单完成量。

Conclusion: GRPO和OSPO通过简化价值函数估计，显著提升了拼车平台的动态匹配效率。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [66] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD方法通过结合非参数检索和扩散生成模型，解决了离线强化学习中数据集稀疏和轨迹重叠不足的问题，提升了长时规划能力。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习因数据集稀疏和轨迹重叠不足导致长时规划困难，现有方法如合成数据增强或轨迹拼接泛化能力有限。

Method: RAD结合非参数检索和扩散生成模型，动态检索高质量状态作为目标，并利用条件引导的扩散模型进行规划。

Result: 实验表明，RAD在多样化基准测试中表现优于或与基线方法相当。

Conclusion: RAD通过检索引导的生成方法，提升了轨迹拼接的灵活性和对未见状态的泛化能力。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [67] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出了一种基于图注意力网络和LSTM的端到端模型，用于预测未来流程行为，包括下一活动和下一事件时间。


<details>
  <summary>Details</summary>
Motivation: 利用对象中心事件日志提升流程预测的准确性和效率。

Method: 结合图注意力网络编码活动及其关系，LSTM处理时间依赖。

Result: 在真实和合成事件日志上表现优于现有方法。

Conclusion: 模型在预测任务中具有竞争力，为流程监控提供了新思路。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [68] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文提出了一种基于帕累托优化的方法，通过干预启发式发现业务过程中活动批处理的最优策略，权衡等待时间、处理成本和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 业务过程中，批处理策略需要在成本和等待时间之间找到平衡，但现有方法缺乏系统性的优化手段。

Method: 采用帕累托优化和干预启发式，结合模拟评估和三种元启发式（爬山法、模拟退火和强化学习）生成最优批处理策略。

Result: 实验表明，基于干预启发式的方法在收敛性、多样性和周期时间增益上优于非启发式基线。

Conclusion: 该方法能有效发现业务过程中活动批处理的最优策略，为管理者提供决策支持。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [69] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1是一种基于强化学习微调的视觉语言模型，专注于复杂图表推理，通过程序化数据合成和两阶段训练策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 验证R1-Style方法在多模态数据（尤其是图表）上的优势，解决图表领域缺乏高质量推理数据的问题。

Method: 提出程序化数据合成技术生成高质量图表推理数据，并采用两阶段训练策略（Chart-COT和Chart-RFT）。

Result: 实验表明Chart-R1在图表推理任务上优于现有方法，甚至可与GPT-4o等大型模型媲美。

Conclusion: Chart-R1通过创新的数据合成和训练策略，为复杂图表推理提供了有效解决方案。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [70] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET是一个多智能体框架，旨在通过自主决策和物理环境交互提升戏剧生成的互动性和沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的戏剧生成方法缺乏主动性和物理环境交互，限制了在线实时表演的互动性和沉浸感。

Method: 提出HAMLET框架，通过生成叙事蓝图和赋予演员自主决策能力，实现即兴表演和物理环境交互。

Result: 实验表明HAMLET能生成富有表现力和连贯性的戏剧体验。

Conclusion: HAMLET为戏剧创作和在线表演提供了新的解决方案，提升了互动性和沉浸感。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [71] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: 论文探讨大型语言模型（LLMs）是否构建内部世界模型或仅依赖统计关联。通过认知科学方法测试LLMs在滑轮系统问题上的表现，发现其能利用统计关联近似表示系统，但缺乏对复杂结构连接的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证LLMs是否具备内部世界模型构建能力，而非仅依赖统计关联。

Method: 采用认知科学方法，通过三个研究测试LLMs在滑轮系统问题上的表现，包括机械优势估计、功能系统识别及结构连接推理。

Result: LLMs能利用滑轮数量与机械优势的统计关联（研究1），近似表示系统空间关系（研究2），但无法推理复杂结构连接（研究3）。

Conclusion: LLMs可能具备有限的世界模型能力，但需进一步研究。认知科学方法有助于评估AI系统的世界建模能力。

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [72] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 本文提出了一种利用参数依赖关系提升离线强化学习数据效率的方法，包括参数化SPI算法和两种预处理技术。


<details>
  <summary>Details</summary>
Motivation: 在离线强化学习中，如何利用额外的参数依赖信息提高数据效率是一个关键问题。

Method: 1. 提出参数化SPI算法，利用分布间的相关性更准确地估计转移动态；2. 引入基于博弈抽象的预处理技术，剪枝冗余动作；3. 使用SMT求解器进一步优化动作剪枝。

Result: 实验表明，这些技术将SPI的数据效率提升了多个数量级，同时保持相同的可靠性保证。

Conclusion: 通过利用参数依赖关系和优化动作剪枝，显著提高了离线强化学习的数据效率。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [73] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 论文提出了一种评估多选问题（MCQ）指标的新协议，分析了现有指标与答案波动率的关系，并提出了一种新指标“最差准确率”。


<details>
  <summary>Details</summary>
Motivation: 现有研究未对多选问题评估指标进行全面评估，且模型在提示微小变化时会产生答案波动。

Method: 提出了一种指标评估协议，通过分析指标与波动率及原始性能的关系来评估方法。

Result: 结果显示现有指标与答案波动有强关联，新指标“最差准确率”在协议中表现最佳。

Conclusion: 新协议和“最差准确率”指标为多选问题评估提供了更可靠的参考。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [74] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 提出一种基于适配器的战术条件化方法，用于调整《星际争霸II》AI代理的策略，保持核心能力的同时实现战术变化。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理虽强大，但缺乏根据高层战术指令调整策略的能力。

Method: 冻结预训练策略网络（DI-Star），为每个动作头附加轻量级适配器模块，通过KL散度约束训练适配器。

Result: 实验表明，该方法能有效调整代理行为（如侵略性、扩张模式和技术偏好），同时保持竞争力。

Conclusion: 该方法以最小计算开销实现灵活战术控制，适用于复杂即时战略游戏的策略定制。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [75] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 探讨了代理AI在复杂系统中自主检测和响应异常的潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在改变传统依赖人工的异常管理方法。

Method: 利用代理AI的自主能力。

Result: 展示了代理AI在异常管理中的潜力。

Conclusion: 代理AI有望革新异常管理领域。

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [76] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 论文提出了一种名为g-AMIE的多智能体系统，用于在医疗诊断对话中实现异步监督，确保患者安全并提高决策质量。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，医疗诊断和治疗的监管要求由持牌专业人员执行，因此需要一种既能利用AI系统优势又能确保安全的监督框架。

Method: 提出g-AMIE系统，通过多智能体协作在限定范围内采集病史，避免提供个性化医疗建议，并通过临床驾驶舱界面将评估结果提交给监督医生。

Result: 在虚拟临床考试中，g-AMIE在高质量病史采集、病例总结及诊断建议方面优于护士和医生助理组，且监督效率更高。

Conclusion: 异步监督是一种可行的范式，可在专家监督下提升AI系统在现实医疗中的应用潜力。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [77] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO框架通过两阶段强化学习，使模型内化推理深度控制，减少40.9%的token使用并提升2.3%的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型因自由生成链式思维序列导致的token浪费问题。

Method: 两阶段强化学习：第一阶段学习成功解长度的统计分布，第二阶段将其作为元认知指导嵌入推理上下文。

Result: 在数学推理基准上，减少40.9%的token使用并提升2.3%的准确性。

Conclusion: LAPO使模型能根据问题复杂度分配计算资源，实现高效且高质量的推理。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [78] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent是一个多代理系统，用于智能合约的Gas优化，结合了现有模式的兼容性和新模式的自动化发现/验证，实现了端到端优化。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案依赖手动发现Gas浪费模式，效率低且难以扩展，而基于大语言模型的方法存在兼容性差和冗余问题。

Method: GasAgent由四个专业代理（Seeker、Innovator、Executor、Manager）组成，通过闭环协作识别、验证和应用Gas节省改进。

Result: 在100个真实合约中优化了82个，平均节省部署Gas 9.97%；在500个LLM生成的合约中优化了79.8%，节省Gas 4.79%-13.93%。

Conclusion: GasAgent作为LLM辅助智能合约开发的优化层，具有广泛适用性和有效性。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [79] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体意图的动态可解释涌现分析框架EAMI，通过双视角思维追踪和k-means聚类分析群体意图的相变点，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着服务计算、云计算和物联网的发展，服务生态系统日益复杂，传统因果方法难以分析智能体间的异常涌现现象。

Method: EAMI框架采用双视角思维追踪机制（Inspector Agent和Analysis Agent）提取意图，结合k-means聚类和意图时序涌现图进行动态分析。

Result: 实验在复杂O2O服务系统和Stanford AI Town中验证了EAMI的有效性、泛化性和效率。

Conclusion: EAMI为服务生态系统中的异常涌现和因果分析提供了新范式。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [80] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文探讨了联邦学习（FL）如何满足可信人工智能（TAI）的要求，分析了FL在TAI框架下面临的挑战及其研究进展。


<details>
  <summary>Details</summary>
Motivation: 随着AI在敏感和高风险领域的应用增加，确保AI的可信性（TAI）变得至关重要。联邦学习（FL）因其隐私保护特性被视为潜在解决方案，但其分布式特性与TAI的其他要求存在冲突。

Method: 以TAI的要求为框架，系统分析了FL在TAI中的挑战，分类并探讨了关键障碍、研究现状和未来方向。

Result: 研究发现FL在隐私保护方面表现突出，但在其他TAI要求（如公平性、透明性）上仍需改进。

Conclusion: FL在TAI中的应用潜力巨大，但需进一步研究以解决其与TAI其他要求的兼容性问题。

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [81] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: 论文研究了在MPDAG（最大定向部分有向无环图）设置下识别条件因果效应的方法，提出了三个结果：不受治疗影响的调节集识别公式、MPDAG设置的do calculus推广，以及一个完整的算法。


<details>
  <summary>Details</summary>
Motivation: 背景知识限制了因果图的等价类，且所有变量均被观测，研究在此设置下如何识别条件因果效应。

Method: 提出了三个结果：1) 不受治疗影响的调节集识别公式；2) MPDAG设置的do calculus推广；3) 完整的识别算法。

Result: 提供了在MPDAG设置下识别条件因果效应的理论框架和实用工具。

Conclusion: 论文为MPDAG设置下的条件因果效应识别提供了理论基础和实用方法。

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [82] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO是一种强化学习框架，通过分层预算探索和差异化奖励机制，使模型能够根据问题复杂度自适应调整推理深度，显著提高计算效率同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在广泛生成思维链时表现出显著的计算效率低下，因为无论问题复杂度如何，它们都采用统一的推理策略。

Method: HBPO通过分层预算探索将样本划分为不同预算子组，并引入差异化奖励机制，使模型能够根据问题复杂度分配资源。

Result: 实验表明，HBPO在四个推理基准上将平均token使用量减少60.6%，同时准确率提高3.14%。

Conclusion: HBPO表明推理效率和能力并非固有冲突，通过分层训练可以同时优化两者。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [83] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLMs）在时间认知上表现出类似人类的自发行为，遵循韦伯-费希纳定律，并通过神经元、表征和信息层面的分析揭示了其机制。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在未被明确训练的情况下，如何表现出类似人类的时间认知模式。

Method: 采用相似性判断任务，分析神经元、表征和信息层面，识别时间偏好神经元及其激活模式，探究年份表征的构建过程，并分析训练语料的固有时间结构。

Result: 发现LLMs自发建立主观时间参考点，神经元对数编码时间距离，年份表征从浅层数值到深层抽象时间定向演变，训练语料具有非线性时间结构。

Conclusion: 提出体验主义视角，认为LLMs的认知是内部表征系统对外部世界的主观构建，暗示AI对齐需关注引导内部构建。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [84] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLM）在国际数学奥林匹克（IMO）问题上的表现，通过优化流程设计和提示工程，Gemini 2.5 Pro成功解决了5/6的问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在解决高难度数学问题（如IMO）上的潜力，填补其在数学竞赛任务中的表现空白。

Method: 使用Google的Gemini 2.5 Pro模型，通过流程设计和提示工程优化，避免数据污染，测试其在IMO 2025问题上的表现。

Result: 在6道问题中，模型成功解决了5道，展示了优化使用方法的重要性。

Conclusion: 优化模型使用方法（如流程设计和提示工程）对提升LLM在高难度数学任务中的表现至关重要。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [85] [Information Theoretic Analysis of a Dual-Band MIMO Cellphone Antenna with ANSYS HFSS SBR+](https://arxiv.org/abs/2507.14704)
*Volodymyr Shyianov,Bamelak Tadele,Vladimir I. Okhmatovski,Amine Mezghani*

Main category: cs.IT

TL;DR: 该论文提出了一种结合信息论和电磁理论的方法，用于评估手机上的双频双极化MIMO天线阵列设计，并通过仿真和概率分析比较了不同方法的性能差异。


<details>
  <summary>Details</summary>
Motivation: 传统天线设计与香农理论分离，本文旨在结合信息论分析天线阵列设计，以优化通信系统性能。

Method: 使用ANSYS HFSS进行电磁仿真，获取MIMO天线阵列模型和信道矩阵，通过线性与最优处理估计中断概率曲线，分析分集增益和复用增益。

Result: 在中等信噪比下获得分集增益，高信噪比下获得复用增益，与传统方法（如包络相关系数）相比存在显著差异。

Conclusion: 信息论方法为天线设计提供了新的视角，与传统方法相比具有明显优势，尤其在分集和复用增益分析上。

Abstract: Historically, the design of antenna arrays has evolved separately from
Shannon theory. Shannon theory adopts a probabilistic approach in the design of
communication systems, while antenna design approaches have relied on the
deterministic Maxwell theory alone. In this paper, we investigate an
information-theoretic analysis approach which we apply to evaluate the design
of a dual-band, dual-polarized multiple-input multiple-output (MIMO) array on a
cellphone. To this end, we use ANSYS HFSS, a commercial electromagnetic (EM)
simulation software suitable for the numerical optimization of antenna systems.
HFSS is used to obtain an accurate model of the cellphone MIMO antenna array
and HFSS SBR+ is utilized to obtain channel matrices for a large number of
users. Taking advantage of linear and optimal processing at the cellphone, we
estimate the outage probability curves. The curves are then used to determine
the diversity gain in a moderate signal-to-noise ratio (SNR) regime and the
multiplexing gain at a high SNR regime. This approach is then compared with the
method of estimating the diversity gain from the envelope correlation
coefficients or the beam-coupling matrix showing substantial differences in the
two methodologies.

</details>


### [86] [Study of Delay-Calibrated Joint User Activity Detection, Channel Estimation and Data Detection for Asynchronous mMTC Systems](https://arxiv.org/abs/2507.14733)
*Z. Shao,X. Yuan,R. de Lamare*

Main category: cs.IT

TL;DR: 提出了一种基于期望最大化方法的延迟校准联合用户活动检测、信道估计和数据检测算法，用于解决异步大规模机器通信中的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模机器通信中由于正交前导码数量有限导致的冲突问题。

Method: 使用期望最大化方法和近似消息传递原理，联合估计延迟、检测活跃用户及其信道和数据。

Result: 数值结果表明，所提算法在信道和数据符号的归一化均方误差以及误检概率方面表现优异。

Conclusion: 所提算法有效解决了异步大规模机器通信中的冲突问题，具有较高的检测和估计精度。

Abstract: This work considers uplink asynchronous massive machine-type communications,
where a large number of low-power and low-cost devices asynchronously transmit
short packets to an access point equipped with multiple receive antennas. If
orthogonal preambles are employed, massive collisions will occur due to the
limited number of orthogonal preambles given the preamble sequence length. To
address this problem, we propose a delay-calibrated joint user activity
detection, channel estimation, and data detection algorithm, and investigate
the benefits of oversampling in estimating continuous-valued time delays at the
receiver. The proposed algorithm is based on the expectation-maximization
method, which alternately estimates the delays and detects active users and
their channels and data by noting that the collided users have different
delays. Under the Bayesian inference framework, we develop a computationally
efficient iterative algorithm using the approximate message passing principle
to resolve the joint user activity detection, channel estimation, and data
detection problem. Numerical results demonstrate the effectiveness of the
proposed algorithm in terms of the normalized mean-squared errors of channel
and data symbols, and the probability of misdetection.

</details>


### [87] [An Information-Theoretic Intersectional Data Valuation Theory](https://arxiv.org/abs/2507.14742)
*Eduardo C. Garrido-Merchán*

Main category: cs.IT

TL;DR: 论文提出了一种基于互信息的定价规则，量化并内化交叉隐私损失，通过经济手段减少有害数据交易并奖励透明度。


<details>
  <summary>Details</summary>
Motivation: 当代数字市场中，个人数据暴露导致隐私外部性，企业从中获利而用户面临社会风险和歧视。

Method: 引入一种形式化定价规则，利用互信息量化交叉隐私损失，并通过离散化联合概率分布实现实际应用。

Result: 提出了一种庇古式附加费，可独立于底层统计模型运作，并为监管者提供校准工具。

Conclusion: 该规则不仅是市场失灵的技术解决方案，还能作为再分配工具，保护弱势群体免受数字权力不对称的影响。

Abstract: In contemporary digital markets, personal data often reveals not just
isolated traits, but complex, intersectional identities based on combinations
of race, gender, disability, and other protected characteristics. This exposure
generates a privacy externality: firms benefit economically from profiling,
prediction, and personalization, while users face hidden costs in the form of
social risk and discrimination. We introduce a formal pricing rule that
quantifies and internalizes this intersectional privacy loss using mutual
information, assigning monetary value to the entropy reduction induced by each
datum. The result is a Pigouvian-style surcharge that discourages harmful data
trades and rewards transparency. Our formulation has the advantage that it
operates independently of the underlying statistical model of the
intersectional variables, be it parametric, nonparametric, or learned, and can
be approximated in practice by discretizing the intersectional joint
probability distributions. We illustrate how regulators can calibrate this
surcharge to reflect different societal values, and argue that it provides not
just a technical fix to market failures, but also a redistributive shield that
empowers vulnerable groups in the face of asymmetric digital power.

</details>


### [88] [Collusion-Resilient Hierarchical Secure Aggregation with Heterogeneous Security Constraints](https://arxiv.org/abs/2507.14768)
*Zhou Li,Xiang Zhang,Jiawen Lv,Jihao Fan,Haiqiang Chen,Giuseppe Caire*

Main category: cs.IT

TL;DR: 论文研究了分层安全聚合（HSA）中的弱安全性问题，提出了WS-HSA框架以应对用户间异构安全需求，并分析了最优密钥率。


<details>
  <summary>Details</summary>
Motivation: 为了解决HSA中用户安全需求异构性的问题，例如不同集群用户需要不同级别的输入保护，研究弱安全HSA（WS-HSA）并提供灵活的框架。

Method: 通过定义安全输入集和合谋集，研究WS-HSA的通信和密钥生成效率，并分析最优总密钥率。

Result: 针对多种参数配置，确定了最优总密钥率；对于其他情况，提供了密钥率的下界和上界，并证明了常数倍最优性。

Conclusion: WS-HSA为HSA中的异构安全需求提供了灵活解决方案，并通过密钥率分析为实际应用提供了理论支持。

Abstract: Motivated by federated learning (FL), secure aggregation (SA) aims to
securely compute, as efficiently as possible, the sum of a set of inputs
distributed across many users. To understand the impact of network topology,
hierarchical secure aggregation (HSA) investigated the communication and secret
key generation efficiency in a 3-layer relay network, where clusters of users
are connected to the aggregation server through an intermediate layer of
relays. Due to the pre-aggregation of the messages at the relays, HSA reduces
the communication burden on the relay-to-server links and is able to support a
large number of users. However, as the number of users increases, a practical
challenge arises from heterogeneous security requirements--for example, users
in different clusters may require varying levels of input protection. Motivated
by this, we study weakly-secure HSA (WS-HSA) with collusion resilience, where
instead of protecting all the inputs from any set of colluding users, only the
inputs belonging to a predefined collection of user groups (referred to as
security input sets) need to be protected against another predefined collection
of user groups (referred to as collusion sets). Since the security input sets
and collusion sets can be arbitrarily defined, our formulation offers a
flexible framework for addressing heterogeneous security requirements in HSA.
We characterize the optimal total key rate, i.e., the total number of
independent key symbols required to ensure both server and relay security, for
a broad range of parameter configurations. For the remaining cases, we
establish lower and upper bounds on the optimal key rate, providing
constant-factor gap optimality guarantees.

</details>


### [89] [Enhancing Resilience Against Jamming Attacks: A Cooperative Anti-Jamming Method Using Direction Estimation](https://arxiv.org/abs/2507.14775)
*Amir Mehrabian,Georges Kaddoum*

Main category: cs.IT

TL;DR: 本文提出了一种基于多感知节点协作的抗干扰方法（CAJ），通过特征向量（EV）方法估计信道方向，性能接近完美信道状态信息（CSI）的情况，且在强干扰下表现优异。


<details>
  <summary>Details</summary>
Motivation: 无线通信的固有脆弱性需要增强安全性，尤其是在面对干扰攻击时。

Method: 利用多感知节点协作，提出EV方法估计信道方向，并通过分析和仿真验证其性能。

Result: 在强干扰下，EV-CAJ方法性能仅下降0.7 dB，且能处理多干扰源和快衰落信道。

Conclusion: 该方法在干扰和快衰落环境下表现出良好的鲁棒性，适用于移动干扰源场景。

Abstract: The inherent vulnerability of wireless communication necessitates strategies
to enhance its security, particularly in the face of jamming attacks. This
paper uses the collaborations of multiple sensing nodes (SNs) in the wireless
network to present a cooperative anti-jamming approach (CAJ) designed to
neutralize the impact of jamming attacks. We propose an eigenvector (EV) method
to estimate the direction of the channel vector from pilot symbols. Through our
analysis, we demonstrate that with an adequate number of pilot symbols, the
performance of the proposed EV method is comparable to the scenario where the
perfect channel state information (CSI) is utilized. Both analytical formulas
and simulations illustrate the excellent performance of the proposed EV-CAJ
under strong jamming signals. Considering severe jamming, the proposed EV-CAJ
method exhibits only a 0.7 dB degradation compared to the case without jamming
especially when the number of SNs is significantly larger than the number of
jamming nodes (JNs). Moreover, the extension of the proposed method can handle
multiple jammers at the expense of degrees of freedom (DoF). We also
investigate the method's ability to remain robust in fast-fading channels with
different coherence times. Our proposed approach demonstrates good resilience,
particularly when the ratio of the channel's coherence time to the time frame
is small. This is especially important in the case of mobile jammers with large
Doppler shifts.

</details>


### [90] [Enhancing Communications and Sensing Simultaneously by Zero-Order Optimization of MTS](https://arxiv.org/abs/2507.14794)
*Wenhai Lai,Kaiming Shen,Zhi-Quan Luo*

Main category: cs.IT

TL;DR: 论文提出了一种无需信道状态信息（CSI）的统计方法，通过盲配置优化超表面相位偏移，以增强信道强度并实现主动感知。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖CSI，但实际中CSI难以获取，因此需要一种无需CSI的优化方法。

Method: 利用接收信号强度（RSS）数据提取无线环境关键特征，通过条件样本均值实现盲配置。

Result: 原型系统验证表明，算法在2.6 GHz频段显著提升信噪比（SNR）约10 dB，并优于基准方法（如MUSIC）。

Conclusion: 盲配置方法不仅优化了通信性能，还实现了发射器定位，仅需RSS数据即可完成。

Abstract: Metasurface (MTS) comprises an array of metaatoms, each reflecting and
inducing a phase shift into the incident wireless signal. We seek the optimal
combination of phase shifts across all the meta-atoms to maximize the channel
strength from transmitter to receiver. Unlike many existing works that heavily
rely on channel state information (CSI), this paper proposes a statistical
approach to the phase shift optimization in the absence of CSI, namely blind
configuration or zero-order optimization. The main idea is to extract the key
features of the wireless environment from the received signal strength (RSS)
data via conditional sample mean, with provable performance. Furthermore, as a
windfall profit, we show that the proposed blind configuration method has a
nontrivial connection to phase retrieval which can be utilized for active
sensing. In a nutshell, by configuring a pair of MTSs blindly without channel
estimation, we not only enhance the channel strength to facilitate wireless
communication, but also enable receiver to localize transmitter. All we need is
the RSS data that can be readily measured at receiver. Our algorithm is
verified in prototype systems in the 2.6 GHz spectral band. As shown in field
tests, the proposed algorithm outperforms the benchmarks (e.g., MUSIC) in the
active sensing task, and in the meanwhile raises the signal-to-noise ratio
(SNR) significantly by about 10 dB.

</details>


### [91] [A DPI-PAC-Bayesian Framework for Generalization Bounds](https://arxiv.org/abs/2507.14795)
*Muhan Guan,Farhad Farokhi,Jingge Zhu*

Main category: cs.IT

TL;DR: 提出了一种结合数据处理不等式（DPI）和PAC-Bayesian框架的统一方法（DPI-PAC-Bayesian），用于监督学习中的泛化误差边界推导。


<details>
  <summary>Details</summary>
Motivation: 通过将DPI嵌入到测度变换技术中，为泛化误差提供更紧的边界，消除经典PAC-Bayes边界中的冗余项。

Method: 使用Rényi散度和f-散度，推导了基于数据独立先验分布和算法依赖后验分布的二元Kullback-Leibler泛化间隙边界。

Result: 提出了三种边界（Rényi、Hellinger p和Chi-Squared散度），并在先验分布为均匀分布时恢复Occam's Razor边界，且消除了冗余项。

Conclusion: 该框架为构建泛化保证提供了灵活的信息论工具，并连接了数据处理和PAC-Bayesian视角。

Abstract: We develop a unified Data Processing Inequality PAC-Bayesian framework --
abbreviated DPI-PAC-Bayesian -- for deriving generalization error bounds in the
supervised learning setting. By embedding the Data Processing Inequality (DPI)
into the change-of-measure technique, we obtain explicit bounds on the binary
Kullback-Leibler generalization gap for both R\'enyi divergence and any
$f$-divergence measured between a data-independent prior distribution and an
algorithm-dependent posterior distribution. We present three bounds derived
under our framework using R\'enyi, Hellinger \(p\) and Chi-Squared divergences.
Additionally, our framework also demonstrates a close connection with other
well-known bounds. When the prior distribution is chosen to be uniform, our
bounds recover the classical Occam's Razor bound and, crucially, eliminate the
extraneous \(\log(2\sqrt{n})/n\) slack present in the PAC-Bayes bound, thereby
achieving tighter results. The framework thus bridges data-processing and
PAC-Bayesian perspectives, providing a flexible, information-theoretic tool to
construct generalization guarantees.

</details>


### [92] [Rate-Distortion-Perception Trade-off with Strong Realism Constraints: Role of Side Information and Common Randomness](https://arxiv.org/abs/2507.14825)
*Yassine Hamdi,Aaron B. Wagner,Deniz Gündüz*

Main category: cs.IT

TL;DR: 论文探讨了在图像压缩中，生成模型的最新进展揭示了速率与感知质量之间的权衡，并研究了在强完美真实感约束下的信息理论极限。


<details>
  <summary>Details</summary>
Motivation: 研究在强完美真实感约束下，压缩带有边信息的源序列的信息理论极限，并探讨边信息在编码器和解码器中的不同作用。

Method: 分析了边信息在编码器和解码器中的不同情况，并研究了不同类型的强完美真实感约束（边际真实感、联合真实感和近完美真实感）。

Result: 在联合高斯分布和平方误差失真度量下，推导了显式解，并发现强完美真实感约束下，边信息仅在解码器时无速率损失的条件是存在足够的共同随机性。

Conclusion: 强完美真实感约束下，边信息的角色和共同随机性对压缩性能至关重要，尤其是在高斯场景中。

Abstract: In image compression, with recent advances in generative modeling, existence
of a trade-off between the rate and perceptual quality has been brought to
light, where the perceptual quality is measured by the closeness of the output
and source distributions. We consider the compression of a memoryless source
sequence $X^n=(X_1, \ldots, X_n)$ in the presence of memoryless side
information $Z^n=(Z_1, \ldots, Z_n),$ originally studied by Wyner and Ziv, but
elucidate the impact of a strong perfect realism constraint, which requires the
joint distribution of output symbols $Y^n=(Y_1,...,Y_n)$ to match the
distribution of the source sequence. We consider two cases: when $Z^n$ is
available only at the decoder, or at both the encoder and decoder, and
characterize the information theoretic limits under various scenarios. Previous
works show the superiority of randomized codes under strong perceptual quality
constraints. When $Z^n$ is available at both terminals, we characterize its
dual role, as a source of common randomness, and as a second look on the source
for the receiver. We also study different notions of strong perfect realism
which we call marginal realism, joint realism and near-perfect realism. We
derive explicit solutions when $X$ and $Z$ are jointly Gaussian under the
squared error distortion measure. In traditional lossy compression, having $Z$
only at the decoder imposes no rate penalty in the Gaussian scenario. We show
that, when strong perfect realism constraints are imposed this holds only when
sufficient common randomness is available.

</details>


### [93] [Variable Min-Cut Max-Flow Bounds and Algorithms in Finite Regime](https://arxiv.org/abs/2507.14852)
*Rivka Gitik,Alejandro Cohen*

Main category: cs.IT

TL;DR: 论文提出了一种利用计算几何工具分析异构网络中可变链路容量的新框架，推导了性能界限，并展示了增加链路数量可减少吞吐量变异性。


<details>
  <summary>Details</summary>
Motivation: 网络中的最大容量受限于最小割最大流界限，而实际链路容量常因动态网络条件波动，需新方法分析。

Method: 引入计算几何工具分析有限域内可变链路容量的吞吐量，提出算法强制稳定性，并建议使用自适应无速率随机线性网络编码（AR-RLNC）。

Result: 增加链路数量可减少吞吐量变异性近90%，不稳定图的最小割集数量可达指数级（O(2^|E|)）。

Conclusion: 提出的框架和算法有效解决了网络稳定性与吞吐量变异性问题，AR-RLNC可缓解延迟与吞吐量的权衡。

Abstract: The maximum achievable capacity from source to destination in a network is
limited by the min-cut max-flow bound; this serves as a converse limit. In
practice, link capacities often fluctuate due to dynamic network conditions. In
this work, we introduce a novel analytical framework that leverages tools from
computational geometry to analyze throughput in heterogeneous networks with
variable link capacities in a finite regime. Within this model, we derive new
performance bounds and demonstrate that increasing the number of links can
reduce throughput variability by nearly $90\%$. We formally define a notion of
network stability and show that an unstable graph can have an exponential
number of different min-cut sets, up to $O(2^{|E|})$. To address this
complexity, we propose an algorithm that enforces stability with time
complexity $O(|E|^2 + |V|)$, and further suggest mitigating the
delay-throughput tradeoff using adaptive rateless random linear network coding
(AR-RLNC).

</details>


### [94] [Reconfigurable Antenna Arrays With Tunable Loads: Expanding Solution Space via Coupling Control](https://arxiv.org/abs/2507.15074)
*Elio Faddoul,Konstantinos Ntougias,Ioannis Krikidis*

Main category: cs.IT

TL;DR: 论文提出两种技术解决可重构天线阵列中的互耦问题和端口选择问题，并通过算法优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法中，可重构天线阵列因互耦问题和端口选择限制导致性能受限，亟需新方法提升容量和效率。

Method: 1) 利用被动天线和可调负载增强增益；2) 采用全主动设计消除互耦。同时开发贪心和元启发式算法优化端口选择和负载配置。

Result: 数值模拟显示性能显著优于基准方法，并提供了量化负载下的鲁棒设计。

Conclusion: 提出的方法解锁了可重构天线阵列的完整解决方案空间，显著提升了性能。

Abstract: The emerging reconfigurable antenna (RA) array technology promises capacity
enhancement through dynamic antenna positioning. Traditional approaches enforce
half-wavelength or greater spacing among RA elements to avoid mutual coupling,
limiting the solution space. Additionally, achieving sufficient spatial channel
sampling requires numerous discrete RA positions (ports), while high-frequency
scenarios with hybrid processing demand many physical RAs to maintain array
gains. This leads to exponential growth in the solution space. We propose two
techniques to address the former challenge: (1) surrounding a limited number of
active RAs with passive ones terminated to tunable analog loads to
\textit{exploit} mutual coupling and increase array gain, and (2) employing
tunable loads on each RA in an all-active design to \textit{eliminate} mutual
coupling in the analog domain. Both methods enable arbitrary RA spacing,
unlocking the full solution space. Regarding the latter challenge, we develop
greedy and meta-heuristic port selection algorithms, alongside low-complexity
heuristic variants, that efficiently handle over $10^{20}$ array
configurations, and optimize the loading values to maximize the sum-rate in a
multiple-input single-output broadcast channel under transmission power
constraints, assuming a heuristic linear precoder. Furthermore, we analyze
performance degradation from quantized loads and propose corresponding robust
designs. Numerical simulations reveal significant performance gains over
benchmarks and provide valuable insights.

</details>


### [95] [Noise Quantification and Control in Circuits via Strong Data-Processing Inequalities](https://arxiv.org/abs/2507.15108)
*Chenyang Sun*

Main category: cs.IT

TL;DR: 本文探讨了强数据处理不等式（SPDI's）在噪声电路计算中的应用，改进了Evans-Schulman和von Neumann的工作框架，并推广了von Neumann的分析到任意阶多数门。


<details>
  <summary>Details</summary>
Motivation: 研究噪声电路计算中的数据处理不等式，以改进现有框架并推广相关理论。

Method: 首先基于Evans-Schulman的框架，提出深度下限和噪声上限；然后引入von Neumann的3-多数门，分析其噪声控制能力；最后推广到任意阶多数门。

Result: 得出了噪声上限和可靠性条件，并简化了现有证明。

Conclusion: 通过改进框架和推广分析，为噪声电路计算提供了更通用的理论支持。

Abstract: This essay explores strong data-processing inequalities (SPDI's) as they
appear in the work of Evans and Schulman \cite{ES} and von Neumann \cite{vN} on
computing with noisy circuits. We first develop the framework in \cite{ES},
which leads to lower bounds on depth and upper bounds on noise that permit
reliable computation. We then introduce the $3$-majority gate, introduced by
\cite{vN} for the purpose of controlling noise, and obtain an upper bound on
noise necessary for its function. We end by generalizing von Neumann's analysis
to majority gates of any order, proving an analogous noise threshold and giving
a sufficient upper bound for order given a desired level of reliability.
  The presentation of material has been modified in a way deemed more natural
by the author, occasionally leading to simplifications of existing proofs.
Furthermore, many computations omitted from the original works have been worked
out, and some new commentary added. The intended audience has a rudimentary
understanding of information theory similar to that of the author.

</details>


### [96] [The Exact Parameters of A Family of BCH Codes](https://arxiv.org/abs/2507.15247)
*Zhonghua Sun*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite the theoretical and practical significance of BCH codes, the exact
minimum distance and dimension remain unknown for many families. This paper
establishes the precise minimum distance and dimension of narrow-sense BCH
codes $\C_{(q, m, \lambda, \ell_0, \ell_1)}$ over $\gf(q)$ of length
$\frac{q^m-1}{\lambda}$ and designed distance $\frac{(q-\lambda
\ell_0)q^{m-1-\ell_1}-1}{\lambda}$, where $\lambda\mid (q-1)$, $0\leq \ell_0<
\frac{q-1}{\lambda}$, and $0\leq \ell_1\leq m-1$. These results conclusively
resolve the three open problems posed by Li et al. (IEEE Trans. Inf. Theory,
vol. 63, no. 11, pp. 7219-7236, Nov. 2017) while establishing complementary
advances to Ding's seminal framework (IEEE Trans. Inf. Theory, vol. 61, no. 10,
pp. 5322-5330, Oct. 2015).

</details>


### [97] [A Novel Two-Dimensional Smoothing Algorithm](https://arxiv.org/abs/2507.15301)
*Xufeng Chen,Liang Yan,Xiaoshan Gao*

Main category: cs.IT

TL;DR: 提出了一种新型二维平滑（TDS）算法，用于二维序列的平滑和滤波问题，无需噪声分布假设，仅需调整单一参数即可有效提取趋势并减少噪声影响。


<details>
  <summary>Details</summary>
Motivation: 传统滤波算法依赖滤波窗口选择，限制了其适用性，因此需要一种更简单、通用的方法。

Method: 通过引入损失函数，将趋势序列定义为损失函数最小值时的解，将二维序列分解为趋势序列和波动序列。

Result: TDS算法在数值模拟和图像处理案例中验证了其准确性和有效性。

Conclusion: TDS算法提供了一种无需噪声假设的通用二维序列平滑方法，具有简单性和高效性。

Abstract: Smoothing and filtering two-dimensional sequences are fundamental tasks in
fields such as computer vision. Conventional filtering algorithms often rely on
the selection of the filtering window, limiting their applicability in certain
scenarios. To this end, we propose a novel Two-Dimensional Smoothing (TDS)
algorithm for the smoothing and filtering problem of two-dimensional sequences.
Typically, the TDS algorithm does not require assumptions about the type of
noise distribution. It is simple and easy to implement compared to conventional
filtering methods, such as 2D adaptive Wiener filtering and Gaussian filtering.
The TDS algorithm can effectively extract the trend contained in the
two-dimensional sequence and reduce the influence of noise on the data by
adjusting only a single parameter. In this work, unlike existing algorithms
that depend on the filtering window, we introduce a loss function, where the
trend sequence is identified as the solution when this loss function takes a
minimum value. Therefore, within the framework of the TDS algorithm, a general
two-dimensional sequence can be innovatively decomposed into a trend sequence
and a fluctuation sequence, in which the trend sequence contains the main
features of the sequence and the fluctuation sequence contains the detailed
features or noise interference of the sequence. To ensure the reliability of
the TDS algorithm, a crucial lemma is first established, indicating that the
trend sequence and fluctuation sequence obtained by the TDS algorithm are
existent and unique when the global smoothing parameter is determined. Three
modified algorithms are then proposed based on the TDS algorithm, with
corresponding lemmas and corollaries demonstrating their reliability. Finally,
the accuracy and effectiveness of the TDS algorithm are further verified
through numerical simulations and image processing cases.

</details>


### [98] [Cross Mutual Information](https://arxiv.org/abs/2507.15372)
*Chetan Gohil,Oliver M Cliff,James M. Shine,Ben D. Fulcher,Joseph T. Lizier*

Main category: cs.IT

TL;DR: 提出了一种称为“交叉互信息”的新方法，用于比较非平稳分布下两组样本中变量X和Y的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 传统互信息（MI）无法直接比较非平稳分布下的依赖关系，因此需要一种新方法来解决这一问题。

Method: 提出“交叉互信息”作为替代度量，并通过模拟研究验证其有效性。

Result: 模拟研究表明该方法能有效比较不同样本集中的依赖关系。

Conclusion: 交叉互信息为分析非平稳分布下的依赖关系提供了新工具，未来可应用于神经影像数据分析。

Abstract: Mutual information (MI) is a useful information-theoretic measure to quantify
the statistical dependence between two random variables: $X$ and $Y$. Often, we
are interested in understanding how the dependence between $X$ and $Y$ in one
set of samples compares to another. Although the dependence between $X$ and $Y$
in each set of samples can be measured separately using MI, these estimates
cannot be compared directly if they are based on samples from a non-stationary
distribution. Here, we propose an alternative measure for characterising how
the dependence between $X$ and $Y$ as defined by one set of samples is
expressed in another, \textit{cross mutual information}. We present a
comprehensive set of simulation studies sampling data with $X$-$Y$ dependencies
to explore this measure. Finally, we discuss how this relates to measures of
model fit in linear regression, and some future applications in neuroimaging
data analysis.

</details>


### [99] [Galois equiangular tight frames from Galois self-dual codes](https://arxiv.org/abs/2507.15448)
*Junmin An,Jon-Lark Kim*

Main category: cs.IT

TL;DR: 论文扩展了有限域上的框架理论，引入了Galois内积，定义了Galois框架及相关概念，并构造了Galois等角紧框架。


<details>
  <summary>Details</summary>
Motivation: 研究有限域上的框架理论，结合Galois内积，扩展框架的应用范围。

Method: 引入Galois内积，定义Galois框架及相关概念，构造Galois等角紧框架。

Result: 成功构造了Galois等角紧框架，并分析了Galois自对偶码与Galois框架的关系。

Conclusion: 论文为有限域上的框架理论提供了新的工具和构造方法，扩展了框架的应用领域。

Abstract: Greaves et al. (2022) extended frames over real or complex numbers to frames
over finite fields. In this paper, we study the theory of frames over finite
fields by incorporating the Galois inner products introduced by Fan and Zhang
(2017), which generalize the Euclidean and Hermitian inner products. We define
a class of frames, called Galois frames over finite fields, along with related
notions such as Galois Gram matrices, Galois frame operators, and Galois
equiangular tight frames (Galois ETFs). We also characterize when Galois
self-dual codes induce Galois ETFs. Furthermore, we construct explicitly Galois
ETFs from Galois self-dual constacyclic codes.

</details>


### [100] [Estimating Rate-Distortion Functions Using the Energy-Based Model](https://arxiv.org/abs/2507.15700)
*Shitong Wu,Sicheng Xu,Lingyi Chen,Huihui Wu,Wenyi Zhang*

Main category: cs.IT

TL;DR: 本文提出了一种基于能量模型的创新框架，用于解决高维率失真（RD）问题，通过连接RD对偶形式和统计物理中的自由能，有效重建了最优条件分布。


<details>
  <summary>Details</summary>
Motivation: 传统Blahut-Arimoto（BA）算法在高维场景下计算困难，现有神经方法常忽略最优条件分布重建或依赖不合理假设。

Method: 提出基于能量模型的框架，利用RD对偶形式与自由能的联系，仅需训练单一神经网络，避免MCMC采样中的归一化因子计算。

Result: 实验表明，该算法在高维RD函数估计和最优条件分布重建方面效果显著。

Conclusion: 该框架为高维RD问题提供了有效解决方案，避免了传统方法的局限性。

Abstract: The rate-distortion (RD) theory is one of the key concepts in information
theory, providing theoretical limits for compression performance and guiding
the source coding design, with both theoretical and practical significance. The
Blahut-Arimoto (BA) algorithm, as a classical algorithm to compute RD
functions, encounters computational challenges when applied to high-dimensional
scenarios. In recent years, many neural methods have attempted to compute
high-dimensional RD problems from the perspective of implicit generative
models. Nevertheless, these approaches often neglect the reconstruction of the
optimal conditional distribution or rely on unreasonable prior assumptions. In
face of these issues, we propose an innovative energy-based modeling framework
that leverages the connection between the RD dual form and the free energy in
statistical physics, achieving effective reconstruction of the optimal
conditional distribution.The proposed algorithm requires training only a single
neural network and circumvents the challenge of computing the normalization
factor in energy-based models using the Markov chain Monte Carlo (MCMC)
sampling. Experimental results demonstrate the significant effectiveness of the
proposed algorithm in estimating high-dimensional RD functions and
reconstructing the optimal conditional distribution.

</details>


### [101] [Remote Channel Synthesis](https://arxiv.org/abs/2507.15757)
*Yassine Hamdi,Deniz Gündüz*

Main category: cs.IT

TL;DR: 论文研究了在无记忆信道中合成编码器与解码器之间的协调问题，提出了最优压缩和共同随机性速率的单字母表征，并指出低共同随机性速率下标准信道合成方案的次优性。


<details>
  <summary>Details</summary>
Motivation: 解决在部分或噪声观测下，如何通过编码器和解码器协调生成与远程源序列协调的输出序列的问题。

Method: 使用无记忆信道模型，通过编码器和解码器之间的无噪声链路，利用共同随机性资源，优化压缩和共同随机性速率。

Result: 提出了最优压缩和共同随机性速率的单字母表征，并证明低共同随机性速率下标准信道合成方案的次优性。

Conclusion: 在低共同随机性速率下，标准信道合成方案通常次优，需要更高效的协调策略。

Abstract: We consider the problem of synthesizing a memoryless channel between an
unobserved source and a remote terminal. An encoder has access to a partial or
noisy version $Z^n = (Z_1, \ldots, Z_n)$ of a remote source sequence $X^n =
(X_1, \ldots, X_n),$ with $(X_i,Z_i)$ independent and identically distributed
with joint distribution $q_{X,Z}.$ The encoder communicates through a noiseless
link to a decoder which aims to produce an output $Y^n$ coordinated with the
remote source; that is, the total variation distance between the joint
distribution of $X^n$ and $Y^n$ and some i.i.d. target distribution
$q_{X,Y}^{\otimes n}$ is required to vanish as $n$ goes to infinity. The two
terminals may have access to a source of rate-limited common randomness. We
present a single-letter characterization of the optimal compression and common
randomness rates. We also show that when the common randomness rate is small,
then in most cases, coordinating $Z^n$ and $Y^n$ using a standard channel
synthesis scheme is strictly sub-optimal. In other words, schemes for which the
joint distribution of $Z^n$ and $Y^n$ approaches a product distribution
asymptotically are strictly sub-optimal.

</details>


### [102] [The Capacity of Semantic Private Information Retrieval with Colluding Servers](https://arxiv.org/abs/2507.15818)
*Mohamed Nomeir,Alptug Aytekin,Sennur Ulukus*

Main category: cs.IT

TL;DR: 论文研究了具有T个合谋服务器的语义私有信息检索（Sem-TPIR）问题，推导了其检索率的上界并设计了一个达到该上界的方案。


<details>
  <summary>Details</summary>
Motivation: 解决传统PIR问题中消息大小和检索概率不统一的问题，并扩展到服务器合谋的场景。

Method: 提出了一种针对任意T < N的Sem-TPIR方案，推导了检索率的上界。

Result: 证明了该方案能够达到推导的上界，即确定了Sem-TPIR的精确容量。

Conclusion: 论文成功解决了具有合谋服务器的语义私有信息检索问题，并提供了最优方案。

Abstract: We study the problem of semantic private information retrieval (Sem-PIR) with
$T$ colluding servers (Sem-TPIR), i.e., servers that collectively share user
queries. In Sem-TPIR, the message sizes are different, and message retrieval
probabilities by any user are not uniform. This is a generalization of the
classical PIR problem where the message sizes are equal and message retrieval
probabilities are identical. The earlier work on Sem-PIR considered the case of
no collusions, i.e., the collusion parameter of $T=1$. In this paper, we
consider the general problem for arbitrary $T < N$. We find an upper bound on
the retrieval rate and design a scheme that achieves this rate, i.e., we derive
the exact capacity of Sem-TPIR.

</details>
