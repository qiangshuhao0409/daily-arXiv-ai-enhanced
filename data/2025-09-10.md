<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [SCION Path Performance Toolkit and Benchmark for Advancing Machine Learning in Next-Generation Networks: ScionPathML](https://arxiv.org/abs/2509.07154)
*Damien Rossi,Lars Herschbach,Sina Keshvadi*

Main category: cs.NI

TL;DR: 对SCION网络的长期实际测量分析，发现高弹性路由、短路径存期、路径不对称和性能交换等问题，挖成多路径协议设计需重新考虑实际网络特性。


<details>
  <summary>Details</summary>
Motivation: 虽然路径感知网络通过多路径传输提供更高的性能和弹性，但缺乏关于其实际动态的经验数据，阻碍了有效协议的设计。

Method: 在全球SCIONLab测试平台上对SCION架构进行长期测量研究，分析路径稳定性、多样性和性能特征。

Result: 测量显示了动态环境，部分测试平台存在显著的控制平面变动和短路径存期。识别了路径差异现象，即路由策略导致终点间的不对称路径可用性。同时观察到性能交换：并发多路径传输可提高总速率，但可能降低单个路径的延迟和可靠性。

Conclusion: 这些发现说明，像MPQUIC这样的协议应明确考虑高弹性和路径不对称性，这对多路径协议设计中的常见假设构成了挑战。

Abstract: Path-aware networks promise enhanced performance and resilience through
multipath transport, but a lack of empirical data on their real-world dynamics
hinders the design of effective protocols. This paper presents a longitudinal
measurement study of the SCION architecture on the global SCIONLab testbed,
characterizing the path stability, diversity, and performance crucial for
protocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic
environment, with significant control-plane churn and short path lifetimes in
parts of the testbed. We identify and characterize path discrepancy, a
phenomenon where routing policies create asymmetric path availability between
endpoints. Furthermore, we observe a performance trade-off where concurrent
multipath transmissions can improve aggregate throughput but may degrade the
latency and reliability of individual paths. These findings demonstrate that
protocols such as MPQUIC should explicitly account for high churn and path
asymmetry, challenging common assumptions in multipath protocol design.

</details>


### [2] [DORA: Dynamic O-RAN Resource Allocation for Multi-Slice 5G Networks](https://arxiv.org/abs/2509.07242)
*Alireza Ebrahimi Dorcheh,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.NI

TL;DR: DORA是一个基于深度强化学习的动态资源分配框架，用于5G O-RAN网络中的切片级PRB分配，支持在线训练并能适应变化的流量模式


<details>
  <summary>Details</summary>
Motivation: 5G网络需要同时支持URLLC、eMBB和mMTC等不同服务类别，每种服务都有不同的QoS要求。在有限的频谱资源下满足这些需求需要自适应的、符合标准的无线资源管理

Method: 使用基于PPO的强化学习代理，根据观测到的流量需求和信道条件在URLLC、eMBB和mMTC切片之间分配PRB。切片内PRB调度通过轮询方式在活跃UE之间确定性处理

Result: 在拥塞情况下，DORA优于三个非学习基线和DQN代理，实现了更低的URLLC延迟、更高的eMBB吞吐量（SLA违规更少）以及更广泛的mMTC覆盖，同时不会饿死高优先级切片

Conclusion: 这是第一个完全在线的DRL框架，用于O-RAN中的自适应、切片感知PRB分配，可无缝集成到RAN智能控制器中

Abstract: The fifth generation (5G) of wireless networks must simultaneously support
heterogeneous service categories, including Ultra-Reliable Low-Latency
Communications (URLLC), enhanced Mobile Broadband (eMBB), and massive
Machine-Type Communications (mMTC), each with distinct Quality of Service (QoS)
requirements. Meeting these demands under limited spectrum resources requires
adaptive and standards-compliant radio resource management. We present DORA
(Dynamic O-RAN Resource Allocation), a deep reinforcement learning (DRL)
framework for dynamic slice-level Physical Resource Block (PRB) allocation in
Open RAN. DORA employs a PPO-based RL agent to allocate PRBs across URLLC,
eMBB, and mMTC slices based on observed traffic demands and channel conditions.
Intra-slice PRB scheduling is handled deterministically via round-robin among
active UEs, simplifying control complexity and improving training stability.
Unlike prior work, DORA supports online training and adapts continuously to
evolving traffic patterns and cross-slice contention. Implemented in the
standards-compliant OpenAirInterface (OAI) RAN stack and designed for
deployment as an O-RAN xApp, DORA integrates seamlessly with RAN Intelligent
Controllers (RICs). Extensive evaluation under congested regimes shows that
DORA outperforms three non-learning baselines and a \texttt{DQN} agent,
achieving lower URLLC latency, higher eMBB throughput with fewer SLA
violations, and broader mMTC coverage without starving high-priority slices. To
our knowledge, this is the first fully online DRL framework for adaptive,
slice-aware PRB allocation in O-RAN.

</details>


### [3] [TEGRA: A Flexible & Scalable NextGen Mobile Core](https://arxiv.org/abs/2509.07410)
*Bilal Saleem,Omar Basit,Jiayi Meng,Iftekhar Alam,Ajay Thakur,Christian Maciocco,Muhammad Shahbaz,Y. Charlie Hu,Larry Peterson*

Main category: cs.NI

TL;DR: TEGRA是一个高性能、灵活且可扩展的基于SBA的移动核心网架构，通过创新的微服务设计模式和状态管理策略，在保持灵活性的同时显著提升了性能，比现有方案处理请求快20倍，同时大幅减少了代码复杂度。


<details>
  <summary>Details</summary>
Motivation: 5G/6G移动核心网正在向基于服务的架构(SBA)演进，但当前的性能优化策略仍采用传统的NFV技术，导致灵活性和可扩展性之间存在固有权衡。需要探索是否能在不牺牲灵活性的情况下提升SBA核心网的性能。

Method: 提出了弹性SBA微服务设计模式和状态管理策略，开发了TEGRA系统。利用移动核心网在端到端互联网生态中的独特位置（最后一英里边缘），在不影响适应性的情况下优化性能。

Result: TEGRA实现了显著更低的延迟，处理请求速度比传统SBA核心实现快20倍（free5GC）、11倍（Open5GS）和1.75倍（Aether），同时与最先进的核心网（如CoreKube）性能相当但保持灵活性。部署新功能所需的代码行数比现有核心网少几个数量级。

Conclusion: TEGRA证明了在基于SBA的移动核心网中，无需在灵活性和可扩展性之间做出权衡。通过创新的架构设计，可以同时实现高性能、灵活性和简化部署，为下一代移动核心网的发展提供了重要参考。

Abstract: To support emerging mobile use cases (e.g., AR/VR, autonomous driving, and
massive IoT), next-generation mobile cores for 5G and 6G are being
re-architected as service-based architectures (SBAs) running on both private
and public clouds. However, current performance optimization strategies for
scaling these cores still revert to traditional NFV-based techniques, such as
consolidating functions into rigid, monolithic deployments on dedicated
servers. This raises a critical question: Is there an inherent tradeoff between
flexibility and scalability in an SBA-based mobile core, where improving
performance (and resiliency) inevitably comes at the cost of one or the other?
  To explore this question, we introduce resilient SBA microservices design
patterns and state-management strategies, and propose TEGRA -- a
high-performance, flexible, and scalable SBA-based mobile core. By leveraging
the mobile core's unique position in the end-to-end internet ecosystem (i.e.,
at the last-mile edge), TEGRA optimizes performance without compromising
adaptability. Our evaluation demonstrates that TEGRA achieves significantly
lower latencies, processing requests 20x, 11x, and 1.75x faster than
traditional SBA core implementations -- free5GC, Open5GS, and Aether,
respectively -- all while matching the performance of state-of-the-art cores
(e.g., CoreKube) while retaining flexibility. Furthermore, it reduces the
complexity of deploying new features, requiring orders of magnitude fewer lines
of code (LoCs) compared to existing cores.

</details>


### [4] [Network-accelerated Active Messages](https://arxiv.org/abs/2509.07431)
*Md Ashfaqur Rahaman,Alireza Sanaee,Todd Thornley,Sebastiano Miano,Gianni Antichi,Brent E. Stephens,Ryan Stutsman*

Main category: cs.NI

TL;DR: NAAM提出了一种网络加速主动消息方案，通过eBPF函数实现动态任务分配，解决RDMA编程复杂性和SmartNIC负载不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决RDMA操作集有限、编程复杂、需多次往返通信的问题，以及SmartNIC在不同工作负载下最优选择的困难。

Method: 使用eBPF函数关联消息，通过RDMA类接口访问数据。NAAM运行时在网络中各个位置（客户端、服务器SmartNIC、服务器CPU核心）动态执行逻辑。

Result: 在NVIDIA BlueField-2 SmartNIC上，NAAM能动态载载1.8百万MICA操作/秒（YCSB-B）和75万个Cell查询/秒，较iPipe框架支按数百个应用载载且尾延迟影响最小。

Conclusion: NAAM通过eBPF的端口性和灵活性，为网络加速提供了一种动态、可扩展的解决方案，能够在不同计算资源上优化性能。

Abstract: Remote Direct Memory Access (RDMA) improves host networking performance by
eliminating software and server CPU involvement. However, RDMA has a limited
set of operations, is difficult to program, and often requires multiple round
trips to perform simple application operations. Programmable SmartNICs provide
a different means to offload work from host CPUs to a NIC. This leaves
applications with the complex choice of embedding logic as RPC handlers at
servers, using RDMA's limited interface to access server structures via
client-side logic, or running some logic on SmartNICs. The best choice varies
between workloads and over time. To solve this dilemma, we present NAAM,
network-accelerated active messages. NAAM applications specify small, portable
eBPF functions associated with messages. Each message specifies what data it
accesses using an RDMA-like interface. NAAM runs at various places in the
network, including at clients, on server-attached SmartNICs, and server host
CPU cores. Due to eBPF's portability, the code associated with a message can be
run at any location. Hence, the NAAM runtime can dynamically steer any message
to execute its associated logic wherever it makes the most sense. To
demonstrate NAAM's flexibility, we built several applications, including the
MICA hash table and lookups from a Cell-style B-tree. With an NVIDIA
BlueField-2 SmartNIC and integrating its NIC-embedded switch, NAAM can run any
of these operations on client, server, and NIC cores, shifting load in tens of
milliseconds on server compute congestion. NAAM dynamically offloads up to 1.8
million MICA ops/s for YCSB-B and 750,000 Cell lookups/s from server CPUs.
Finally, whereas iPipe, the state-of-the-art SmartNIC offload framework, only
scales to 8 application offloads on BlueField-2, NAAM scales to hundreds of
application offloads with minimal impact on tail latency due to eBPF's low
overhead.

</details>


### [5] [Constraint-Compliant Network Optimization through Large Language Models](https://arxiv.org/abs/2509.07492)
*Youngjin Song,Wookjin Lee,Hong Ki Kim,Sang Hyun Lee*

Main category: cs.NI

TL;DR: LLM基于的网络优化框架，通过自然语言编码策略确保严格约束满足，避免了传统方法的不可行解问题。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM具有上下文推理能力，但现有方法常忽略约束条件，导致优化解不可行。需要一种能够确保严格约束满足的方法。

Method: 提出自然语言基础的输入编码策略，通过限制解空间来保证可行性。在多访问边缘计算网络中优化任务分配，最小化最差情况延迟。

Result: 数值评估证明LLM在约束意识网络优化中具有很大潜力，为其推理能力提供了新的见解。

Conclusion: 该框架为LLM在网络优化领域的应用开启了新方向，能够有效处理复杂约束条件并保证解的可行性。

Abstract: This work develops an LLM-based optimization framework ensuring strict
constraint satisfaction in network optimization. While LLMs possess contextual
reasoning capabilities, existing approaches often fail to enforce constraints,
causing infeasible solutions. Unlike conventional methods that address average
constraints, the proposed framework integrates a natural language-based input
encoding strategy to restrict the solution space and guarantee feasibility. For
multi-access edge computing networks, task allocation is optimized while
minimizing worst-case latency. Numerical evaluations demonstrate LLMs as a
promising tool for constraint-aware network optimization, offering insights
into their inference capabilities.

</details>


### [6] [FlexSAN: A Flexible Regenerative Satellite Access Network Architecture](https://arxiv.org/abs/2509.07548)
*Weize Kong,Chaoqun You,Xuming Pei,YueGao*

Main category: cs.NI

TL;DR: FlexSAN是一种自适应卫星接入网络架构，通过动态配置再生有效载荷来优化用户体验和运营成本，相比静态配置显著提升了用户接纳率和降低了运营支出。


<details>
  <summary>Details</summary>
Motivation: 当前再生卫星接入网络(SAN)采用静态配置（全gNB或仅gNB-DU），存在资源浪费或用户体验差的问题，需要动态自适应方案来平衡服务质量(QoS)和运营成本(OPEX)。

Method: 提出FlexSAN架构，基于实时用户需求动态选择最优再生有效载荷配置；设计自适应贪婪启发式算法来解决动态选择的计算复杂度问题。

Result: 实验验证显示，FlexSAN相比静态SAN平均提升36.1%的用户接纳率，并降低15%的运营支出(OPEX)。

Conclusion: FlexSAN通过动态自适应配置有效解决了静态SAN的刚性限制，在保证服务质量的同时显著优化了资源利用效率和运营成本。

Abstract: The regenerative satellite access network (SAN) architecture deploys
next-generation NodeB (gNBs) on satellites to enable enhanced network
management capabilities. It supports two types of regenerative payload,
on-board gNB and on-board gNB-Distributed Unit (gNB-DU). Measurement results
based on our prototype implementation show that the on-board gNB offers lower
latency, while the on-board gNB-DU is more cost-effective, and there is often a
trade-off between Quality-of-Service (QoS) and operational expenditure (OPEX)
when choosing between the two payload types. However, current SAN
configurations are static and inflexible -- either deploying the full on-board
gNB or only the on-board gNB-DU. This rigidity can lead to resource waste or
poor user experiences. In this paper, we propose Flexible SAN (FlexSAN), an
adaptive satellite access network architecture that dynamically configures the
optimal regenerative payload based on real-time user demands. FlexSAN selects
the lowest OPEX payload configuration when all user demands are satisfied, and
otherwise maximizes the number of admitted users while ensuring QoS for
connected users. To address the computational complexity of dynamic payload
selection, we design an adaptive greedy heuristic algorithm. Extensive
experiments validate FlexSAN's effectiveness, showing a 36.1% average
improvement in user admission rates and a 15% OPEX reduction over static SANs.

</details>


### [7] [Quantum Computing for Large-scale Network Optimization: Opportunities and Challenges](https://arxiv.org/abs/2509.07773)
*Sebastian Macaluso,Giovanni Geraci,Elías F. Combarro,Sergi Abadal,Ioannis Arapakis,Sofia Vallecorsa,Eduard Alarcón*

Main category: cs.NI

TL;DR: 量子计算在6G及未来移动网络大规模多目标优化中的应用前景与挑战分析


<details>
  <summary>Details</summary>
Motivation: 6G及未来网络的复杂性使得大规模多目标优化问题变得难以处理，量子计算技术为解决这类问题提供了新的可能性

Method: 通过分析网络问题的图中心表示特征，提出使用量子退火和量子强化学习等量子算法的统一优化策略

Result: 提出了利用量子计算优化未来移动网络关键问题的方法论框架

Conclusion: 量子计算在优化未来网络方面具有巨大潜力，但算法和硬件仍需克服诸多挑战才能有效应用

Abstract: The complexity of large-scale 6G-and-beyond networks demands innovative
approaches for multi-objective optimization over vast search spaces, a task
often intractable. Quantum computing (QC) emerges as a promising technology for
efficient large-scale optimization. We present our vision of leveraging QC to
tackle key classes of problems in future mobile networks. By analyzing and
identifying common features, particularly their graph-centric representation,
we propose a unified strategy involving QC algorithms. Specifically, we outline
a methodology for optimization using quantum annealing as well as quantum
reinforcement learning. Additionally, we discuss the main challenges that QC
algorithms and hardware must overcome to effectively optimize future networks.

</details>


### [8] [Making congestion control robust to per-packet load balancing in datacenters](https://arxiv.org/abs/2509.07907)
*Barak Gerstein,Mark Silberstein,Isaac Keslassy*

Main category: cs.NI

TL;DR: MSwift通过使用中位数反馈机制改进Google Swift拥塞控制算法，使其在多路径负载均衡环境下保持性能，避免吞吐量崩溃，提升99%分位完成时间达25%。


<details>
  <summary>Details</summary>
Motivation: 现有每包负载均衡技术与拥塞控制算法结合时会导致性能下降，甚至最先进的CCA也会因重复ACK而崩溃。传统处理重复ACK的方法不足，需要专门针对多路径路由设计解决方案。

Method: 首先建模分析多种CCA在路径拥塞时的吞吐量崩溃问题，提出使用中位数反馈而非最新反馈来处理多路径变化信号。基于Google Swift开发MSwift，保持其单路径性能和incast容忍性的同时增强多路径鲁棒性。

Result: MSwift在随机包喷洒和自适应路由两种场景下，都能将99%分位流完成时间(FCT)提升高达25%。

Conclusion: 针对多路径路由设计的中位数反馈机制能有效解决传统CCA在多路径环境下的性能问题，MSwift成功实现了这一目标并显著提升了网络性能。

Abstract: Per-packet load-balancing approaches are increasingly deployed in datacenter
networks. However, their combination with existing congestion control
algorithms (CCAs) may lead to poor performance, and even state-of-the-art CCAs
can collapse due to duplicate ACKs. A typical approach to handle this collapse
is to make CCAs resilient to duplicate ACKs.
  In this paper, we first model the throughput collapse of a wide array of CCAs
when some of the paths are congested. We show that addressing duplicate ACKs is
insufficient. Instead, we explain that since CCAs are typically designed for
single-path routing, their estimation function focuses on the latest feedback
and mishandles feedback that reflects multiple paths. We propose to use a
median feedback that is more robust to the varying signals that come with
multiple paths. We introduce MSwift, which applies this principle to make
Google's Swift robust to multi-path routing while keeping its incast tolerance
and single-path performance. Finally, we demonstrate that MSwift improves the
99th-percentile FCT by up to 25\%, both with random packet spraying and
adaptive routing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Renewable Energy Sources Selection Analysis with the Maximizing Deviation Method](https://arxiv.org/abs/2509.07011)
*Kirisci Murat*

Main category: cs.AI

TL;DR: 本研究提出了一种基于偏差最大化方法的优化模型，结合区间值Fermatean模糊集来处理特征权重部分已知的多准则决策问题，并将其应用于可再生能源选择问题。


<details>
  <summary>Details</summary>
Motivation: 多准则决策方法在处理不确定、复杂和冲突情境中帮助决策者做出更好决策。模糊集理论能有效处理人类思维和感知中的不确定性，Fermatean模糊环境作为模糊集的推广，能更好地处理决策者判断中的不确定性和模糊性。

Method: 提出基于偏差最大化方法的优化模型来确定部分已知的特征权重，该方法与区间值Fermatean模糊集相结合。

Result: 该方法成功应用于可再生能源选择问题，能够有效处理该领域的技术、管理和政治层面的复杂决策问题。

Conclusion: 结合Fermatean模糊集和偏差最大化方法的优化模型为处理不确定环境下的多准则决策问题提供了有效工具，特别适用于可再生能源选择这类具有技术和管理双重特性的复杂决策场景。

Abstract: Multi-criteria decision-making methods provide decision-makers with
appropriate tools to make better decisions in uncertain, complex, and
conflicting situations. Fuzzy set theory primarily deals with the uncertainty
inherent in human thoughts and perceptions and attempts to quantify this
uncertainty. Fuzzy logic and fuzzy set theory are utilized with multi-criteria
decision-making methods because they effectively handle uncertainty and
fuzziness in decision-makers' judgments, allowing for verbal judgments of the
problem. This study utilizes the Fermatean fuzzy environment, a generalization
of fuzzy sets. An optimization model based on the deviation maximization method
is proposed to determine partially known feature weights. This method is
combined with interval-valued Fermatean fuzzy sets. The proposed method was
applied to the problem of selecting renewable energy sources. The reason for
choosing renewable energy sources is that meeting energy needs from renewable
sources, balancing carbon emissions, and mitigating the effects of global
climate change are among the most critical issues of the recent period. Even
though selecting renewable energy sources is a technical issue, the managerial
and political implications of this issue are also important, and are discussed
in this study.

</details>


### [10] [From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning](https://arxiv.org/abs/2509.07017)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.AI

TL;DR: Spectral NSR是一个完全谱神经符号推理框架，通过图信号处理和拉普拉斯特征结构将逻辑规则嵌入为谱模板，在谱域直接进行推理，统一了符号推理的可解释性和谱学习的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统神经符号推理系统在可扩展性、适应性和鲁棒性方面的局限性，同时保持符号推理的可解释性优势。

Method: 利用图信号处理和基于拉普拉斯特征结构的频率选择性滤波器，将逻辑规则嵌入为谱模板，在知识图谱的谱域进行推理。包含动态图和基学习、有理和扩散滤波器、谱专家混合、证明引导训练等多种扩展。

Result: 在ProofWriter和CLUTRR等推理基准测试中，Spectral NSR相比transformer、消息传递神经网络和神经符号逻辑编程系统，实现了更高的准确性、更快的推理速度、更好的对抗扰动鲁棒性和更强的可解释性。

Conclusion: Spectral NSR为下一代推理系统提供了一个可扩展且原则性的基础，提供了超越传统方法的透明度、鲁棒性和泛化能力。

Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning
framework that embeds logical rules as spectral templates and performs
inference directly in the graph spectral domain. By leveraging graph signal
processing (GSP) and frequency-selective filters grounded in the Laplacian
eigenstructure of knowledge graphs, the architecture unifies the
interpretability of symbolic reasoning with the scalability and adaptability of
spectral learning. Beyond the core formulation, we incorporate a comprehensive
set of extensions, including dynamic graph and basis learning, rational and
diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts
for modular specialization, proof-guided training with spectral curricula, and
uncertainty quantification for calibrated confidence. Additional enhancements
such as large language model coupling, co-spectral transfer alignment,
adversarial robustness, efficient GPU kernels, generalized Laplacians, and
causal interventions further expand the versatility of the framework.
  Empirical evaluation on state-of-the-art reasoning benchmarks such as
ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior
accuracy, faster inference, improved robustness to adversarial perturbations,
and higher interpretability compared to leading baselines including
transformers, message-passing neural networks, and neuro-symbolic logic
programming systems. Spectral attribution and proof-band agreement analyses
confirm that model decisions align closely with symbolic proof structures,
while transfer experiments validate effective domain adaptation through
co-spectral alignment. These results establish Spectral NSR as a scalable and
principled foundation for the next generation of reasoning systems, offering
transparency, robustness, and generalization beyond conventional approaches.

</details>


### [11] [Statistical Methods in Generative AI](https://arxiv.org/abs/2509.07054)
*Edgar Dobriban*

Main category: cs.AI

TL;DR: 本文综述了统计方法在提升生成式AI可靠性、评估质量和效率方面的应用，探讨了现有技术、应用案例以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术基于概率模型采样，默认缺乏正确性、安全性、公平性等保证，需要统计方法来提高其可靠性。

Method: 综述性研究，回顾了现有的统计技术及其在生成式AI中的应用，包括通用统计方法和具体应用案例。

Result: 统计方法在提高生成式AI的可靠性、评估质量和效率方面显示出巨大潜力，同时可用于设计AI干预和实验。

Conclusion: 统计方法为解决生成式AI的可靠性问题提供了有前景的途径，但仍存在局限性，需要进一步研究和发展。

Abstract: Generative Artificial Intelligence is emerging as an important technology,
promising to be transformative in many areas. At the same time, generative AI
techniques are based on sampling from probabilistic models, and by default,
they come with no guarantees about correctness, safety, fairness, or other
properties. Statistical methods offer a promising potential approach to improve
the reliability of generative AI techniques. In addition, statistical methods
are also promising for improving the quality and efficiency of AI evaluation,
as well as for designing interventions and experiments in AI.
  In this paper, we review some of the existing work on these topics,
explaining both the general statistical techniques used, as well as their
applications to generative AI. We also discuss limitations and potential future
directions.

</details>


### [12] [Instruction Agent: Enhancing Agent with Expert Demonstration](https://arxiv.org/abs/2509.07098)
*Yinheng Li,Hailey Hultquist,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: Instruction Agent是一个GUI代理，通过专家演示来执行复杂任务，严格遵循用户轨迹，避免执行错误，并利用验证器和回溯器模块提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理在处理新颖UI元素、长时程动作和个性化轨迹的复杂任务时仍存在困难，需要更可靠的解决方案。

Method: 基于单次专家演示提取逐步指令，严格遵循用户轨迹执行，并引入验证器和回溯器模块来处理意外中断和验证执行结果。

Result: 在OSWorld任务集上达到60%的成功率，而所有顶级代理都未能完成这些任务。

Conclusion: Instruction Agent提供了一个实用且可扩展的框架，弥合了当前GUI代理与可靠真实世界GUI任务自动化之间的差距。

Abstract: Graphical user interface (GUI) agents have advanced rapidly but still
struggle with complex tasks involving novel UI elements, long-horizon actions,
and personalized trajectories. In this work, we introduce Instruction Agent, a
GUI agent that leverages expert demonstrations to solve such tasks, enabling
completion of otherwise difficult workflows. Given a single demonstration, the
agent extracts step-by-step instructions and executes them by strictly
following the trajectory intended by the user, which avoids making mistakes
during execution. The agent leverages the verifier and backtracker modules
further to improve robustness. Both modules are critical to understand the
current outcome from each action and handle unexpected interruptions(such as
pop-up windows) during execution. Our experiments show that Instruction Agent
achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked
agents failed to complete. The Instruction Agent offers a practical and
extensible framework, bridging the gap between current GUI agents and reliable
real-world GUI task automation.

</details>


### [13] [Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis](https://arxiv.org/abs/2509.07122)
*Sania Sinha,Tanawan Premsri,Danial Kamali,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 这篇论文分析了神经符号框架的技术特征，展示了三个典型框架，持续识别了各种挑战，以促进社区重新思考神经符号学习问题。


<details>
  <summary>Details</summary>
Motivation: 神经符号框架结合了神经计算的灵活性和符号处理的推理能力与可解释性，能够以数据高效的方式解决复杂问题。但该领域缺乏用户友好的工具和统一框架，对开发者构成挑战。

Method: 论文分析了现有NeSy框架的技术特征，包括符号表示语言、与神经模型的集成以及基础算法。展示了三个典型框架：DeepProbLog、Scallop和DomiKnowS。

Result: 识别了各框架在解决不同问题时的表达能力特征和挑战，为评估框架性能提供了基础。

Conclusion: 研究为神经符司学习领域的发展奠定了基础，希望能够激励社区重新思考该领域的问题解决方式，推动更加通用和声明式的框架开发。

Abstract: Neurosymbolic (NeSy) frameworks combine neural representations and learning
with symbolic representations and reasoning. Combining the reasoning
capacities, explainability, and interpretability of symbolic processing with
the flexibility and power of neural computing allows us to solve complex
problems with more reliability while being data-efficient. However, this
recently growing topic poses a challenge to developers with its learning curve,
lack of user-friendly tools, libraries, and unifying frameworks. In this paper,
we characterize the technical facets of existing NeSy frameworks, such as the
symbolic representation language, integration with neural models, and the
underlying algorithms. A majority of the NeSy research focuses on algorithms
instead of providing generic frameworks for declarative problem specification
to leverage problem solving. To highlight the key aspects of Neurosymbolic
modeling, we showcase three generic NeSy frameworks - \textit{DeepProbLog},
\textit{Scallop}, and \textit{DomiKnowS}. We identify the challenges within
each facet that lay the foundation for identifying the expressivity of each
framework in solving a variety of problems. Building on this foundation, we aim
to spark transformative action and encourage the community to rethink this
problem in novel ways.

</details>


### [14] [Autoencoder-Based Denoising of Muscle Artifacts in ECG to Preserve Skin Nerve Activity (SKNA) for Cognitive Stress Detection](https://arxiv.org/abs/2509.07146)
*Farnoush Baghestani,Jihye Moon,Youngsun Kong,Ki Chon*

Main category: cs.AI

TL;DR: 使用轻量级卷积自编码器网络清除心电图高频段皮肤神经活动信号中的肌电干扰，提高交感神经系统监测的稳健性


<details>
  <summary>Details</summary>
Motivation: 皮肤神经活动(SKNA)作为交感神经系统的非侵入性监测手段，容易受到肌电(EMG)干扰，传统滤波方法在肌肉沿续活动时效果差

Method: 设计了轻量级一维卷积自编码器，使用LSTM瓶颈层重构清洁SKNA信号，通过留一受试者交叉验证训练模型

Result: 信器噪比提高9.65dB，相关系数从0.40提升到0.72，间中症状识别准确率达91-98%，可以在严重干扰下恢复生理相关特征

Conclusion: 深度学习基于的重构方法能够在明显EMG干扰下保留生理相关的交感神经疾疹，为自然环境中的SKNA监测提供更稳健的方案

Abstract: The sympathetic nervous system (SNS) plays a central role in regulating the
body's responses to stress and maintaining physiological stability. Its
dysregulation is associated with a wide range of conditions, from
cardiovascular disease to anxiety disorders. Skin nerve activity (SKNA)
extracted from high-frequency electrocardiogram (ECG) recordings provides a
noninvasive window into SNS dynamics, but its measurement is highly susceptible
to electromyographic (EMG) contamination. Traditional preprocessing based on
bandpass filtering within a fixed range (e.g., 500--1000 Hz) is susceptible to
overlapping EMG and SKNA spectral components, especially during sustained
muscle activity. We present a denoising approach using a lightweight
one-dimensional convolutional autoencoder with a long short-term memory (LSTM)
bottleneck to reconstruct clean SKNA from EMG-contaminated recordings. Using
clean ECG-derived SKNA data from cognitive stress experiments and EMG noise
from chaotic muscle stimulation recordings, we simulated contamination at
realistic noise levels (--4 dB, --8 dB signal-to-noise ratio) and trained the
model in the leave-one-subject-out cross-validation framework. The method
improved signal-to-noise ratio by up to 9.65 dB, increased cross correlation
with clean SKNA from 0.40 to 0.72, and restored burst-based SKNA features to
near-clean discriminability (AUROC $\geq$ 0.96). Classification of baseline
versus sympathetic stimulation (cognitive stress) conditions reached accuracies
of 91--98\% across severe noise levels, comparable to clean data. These results
demonstrate that deep learning--based reconstruction can preserve
physiologically relevant sympathetic bursts during substantial EMG
interference, enabling more robust SKNA monitoring in naturalistic,
movement-rich environments.

</details>


### [15] [PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning](https://arxiv.org/abs/2509.07159)
*Heng Hao,Wenjun Hu,Oxana Verkholyak,Davoud Ataee Tarzanagh,Baruch Gutow,Sima Didari,Masoud Faraki,Hankyu Moon,Seungjai Min*

Main category: cs.AI

TL;DR: PaVeRL-SQL是一个结合部分匹配奖励和语言强化学习的框架，用于提升Text-to-SQL模型的执行准确率，在工业级数据库和复杂问题上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL方法在工业级数据库和涉及领域特定业务逻辑的复杂问题上执行准确率较低，需要改进。

Method: 采用两种管道：1）基于大语言模型的上下文学习框架与群体自评估（语言RL）；2）基于小模型（OmniSQL-7B）的思维链RL管道，使用特殊设计的奖励函数和两阶段RL训练。

Result: 在Spider、Spider 2.0和BIRD基准测试中达到SOTA，Spider2.0-SQLite上执行准确率比SOTA高7.4%（语言RL管道）和1.4%（CoT管道），混合SQL方言训练带来三倍增益。

Conclusion: PaVeRL-SQL在现实工业约束下提供了可靠、SOTA级别的Text-to-SQL解决方案。

Abstract: Text-to-SQL models allow users to interact with a database more easily by
generating executable SQL statements from natural-language questions. Despite
recent successes on simpler databases and questions, current Text-to-SQL
methods still suffer from low execution accuracy on industry-scale databases
and complex questions involving domain-specific business logic. We present
\emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and
\emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning
language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt
two pipelines: (1) a newly designed in-context learning framework with group
self-evaluation (verbal-RL), using capable open- and closed-source large
language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL
pipeline with a small backbone model (OmniSQL-7B) trained with a specially
designed reward function and two-stage RL. These pipelines achieve
state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider,
Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the
verbal-RL pipeline achieves an execution accuracy 7.4\% higher than SOTA, and
the CoT pipeline is 1.4\% higher. RL training with mixed SQL dialects yields
strong, threefold gains, particularly for dialects with limited training data.
Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic
industrial constraints. The code is available at
https://github.com/PaVeRL-SQL/PaVeRL-SQL.

</details>


### [16] [That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral](https://arxiv.org/abs/2509.07170)
*Quinten Steenhuis*

Main category: cs.AI

TL;DR: FETCH分类器通过混合LLM/ML集成方法和自动生成后续问题，在法律问题分类中达到97.37%的准确率，超越GPT-5模型，显著降低法律系统用户匹配成本。


<details>
  <summary>Details</summary>
Motivation: 每年数百万人寻求法律援助，错误分类会导致严重后果（错过截止日期、家庭暴力、失去住房或子女监护权），需要准确快速的法律问题分类系统。

Method: 提出FETCH分类器，采用混合LLM/ML集成分类方法，自动生成后续问题来丰富初始问题叙述，使用419个真实世界查询数据集进行验证。

Result: 分类准确率（hits@2）达到97.37%，使用低成本模型组合超越了当前最先进的GPT-5模型的性能。

Conclusion: 该方法在实现高精度的同时，显著降低了引导法律系统用户找到合适资源的成本，具有重要应用前景。

Abstract: Each year millions of people seek help for their legal problems by calling a
legal aid program hotline, walking into a legal aid office, or using a lawyer
referral service. The first step to match them to the right help is to identify
the legal problem the applicant is experiencing. Misdirection has consequences.
Applicants may miss a deadline, experience physical abuse, lose housing or lose
custody of children while waiting to connect to the right legal help. We
introduce and evaluate the FETCH classifier for legal issue classification and
describe two methods for improving accuracy: a hybrid LLM/ML ensemble
classification method, and the automatic generation of follow-up questions to
enrich the initial problem narrative. We employ a novel data set of 419
real-world queries to a nonprofit lawyer referral service. Ultimately, we show
classification accuracy (hits@2) of 97.37\% using a mix of inexpensive models,
exceeding the performance of the current state-of-the-art GPT-5 model. Our
approach shows promise in significantly reducing the cost of guiding users of
the legal system to the right resource for their problem while achieving high
accuracy.

</details>


### [17] [A Hybrid CNN-LSTM Deep Learning Model for Intrusion Detection in Smart Grid](https://arxiv.org/abs/2509.07208)
*Abdulhakim Alsaiari,Mohammad Ilyas*

Main category: cs.AI

TL;DR: 提出基于CNN-LSTM混合深度学习模型的入侵检测系统，用于保护智能电网安全，在DNP3和IEC104数据集上达到99.70%的检测准确率


<details>
  <summary>Details</summary>
Motivation: 智能电网的发展增加了网络攻击风险，SCADA协议存在未授权访问和拒绝服务等漏洞，需要有效的入侵检测系统来保障电网安全

Method: 结合CNN的特征提取能力和LSTM的时间模式识别能力，构建混合深度学习模型，使用DNP3和IEC104入侵检测数据集进行训练和测试

Result: 相比其他深度学习方法，在准确率、精确率、召回率和F1分数方面都有显著提升，检测准确率达到99.70%

Conclusion: CNN-LSTM混合模型能有效识别和分类智能电网中的网络威胁，为智能电网网络安全提供了有效的解决方案

Abstract: The evolution of the traditional power grid into the "smart grid" has
resulted in a fundamental shift in energy management, which allows the
integration of renewable energy sources with modern communication technology.
However, this interconnection has increased smart grids' vulnerability to
attackers, which might result in privacy breaches, operational interruptions,
and massive outages. The SCADA-based smart grid protocols are critical for
real-time data collection and control, but they are vulnerable to attacks like
unauthorized access and denial of service (DoS). This research proposes a
hybrid deep learning-based Intrusion Detection System (IDS) intended to improve
the cybersecurity of smart grids. The suggested model takes advantage of
Convolutional Neural Networks' (CNN) feature extraction capabilities as well as
Long Short-Term Memory (LSTM) networks' temporal pattern recognition skills.
DNP3 and IEC104 intrusion detection datasets are employed to train and test our
CNN-LSTM model to recognize and classify the potential cyber threats. Compared
to other deep learning approaches, the results demonstrate considerable
improvements in accuracy, precision, recall, and F1-score, with a detection
accuracy of 99.70%.

</details>


### [18] [BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions](https://arxiv.org/abs/2509.07209)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Kaira Samuel,Matthew C. Jones,Faez Ahmed*

Main category: cs.AI

TL;DR: BlendedNet是一个包含999个混合翼体(BWB)气动几何的公开数据集，包含8830个RANS模拟案例，每个案例有9-14百万网格单元。论文还提出了一个端到端的代理模型框架，用于点状气动预测。


<details>
  <summary>Details</summary>
Motivation: 解决非常规气动构型数据稀缺问题，促进数据驱动的气动设计代理模型研究

Method: 通过采样几何设计参数和飞行条件生成数据集；使用PointNet回归器从表面点云预测几何参数，然后通过FiLM网络结合预测参数和飞行条件来预测点状系数Cp、Cfx、Cfz

Result: 实验显示在不同BWB构型上表面预测误差较低

Conclusion: BlendedNet为非常规气动构型提供了宝贵的数据资源，并展示了端到端代理模型在气动预测中的有效性

Abstract: BlendedNet is a publicly available aerodynamic dataset of 999 blended wing
body (BWB) geometries. Each geometry is simulated across about nine flight
conditions, yielding 8830 converged RANS cases with the Spalart-Allmaras model
and 9 to 14 million cells per case. The dataset is generated by sampling
geometric design parameters and flight conditions, and includes detailed
pointwise surface quantities needed to study lift and drag. We also introduce
an end-to-end surrogate framework for pointwise aerodynamic prediction. The
pipeline first uses a permutation-invariant PointNet regressor to predict
geometric parameters from sampled surface point clouds, then conditions a
Feature-wise Linear Modulation (FiLM) network on the predicted parameters and
flight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.
Experiments show low errors in surface predictions across diverse BWBs.
BlendedNet addresses data scarcity for unconventional configurations and
enables research on data-driven surrogate modeling for aerodynamic design.

</details>


### [19] [OmniAcc: Personalized Accessibility Assistant Using Generative AI](https://arxiv.org/abs/2509.07220)
*Siddhant Karki,Ethan Han,Nadim Mahmud,Suman Bhunia,John Femiani,Vaskar Raychoudhury*

Main category: cs.AI

TL;DR: OmniAcc是一个基于GPT-4的AI导航系统，通过卫星影像和OpenStreetMap数据识别轮椅无障碍设施，提供个性化路线规划和实时导航，准确率达97.5%。


<details>
  <summary>Details</summary>
Motivation: 行动不便人士在城市环境中面临导航障碍，缺乏无障碍设施信息，需要智能化的辅助导航工具。

Method: 利用GPT-4、卫星影像和OpenStreetMap数据，采用零样本学习和定制提示词技术，精确检测轮椅无障碍设施如坡道和人行横道。

Result: 系统在人行横道检测方面达到97.5%的准确率，能够有效识别和映射无障碍设施。

Conclusion: OmniAcc展示了AI在改善导航和创建包容性城市空间方面的变革潜力，可为城市规划者和行动辅助用户提供帮助。

Abstract: Individuals with ambulatory disabilities often encounter significant barriers
when navigating urban environments due to the lack of accessible information
and tools. This paper presents OmniAcc, an AI-powered interactive navigation
system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to
identify, classify, and map wheelchair-accessible features such as ramps and
crosswalks in the built environment. OmniAcc offers personalized route
planning, real-time hands-free navigation, and instant query responses
regarding physical accessibility. By using zero-shot learning and customized
prompts, the system ensures precise detection of accessibility features, while
supporting validation through structured workflows. This paper introduces
OmniAcc and explores its potential to assist urban planners and mobility-aid
users, demonstrated through a case study on crosswalk detection. With a
crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative
potential of AI in improving navigation and fostering more inclusive urban
spaces.

</details>


### [20] [HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring](https://arxiv.org/abs/2509.07260)
*Xin Wang,Ting Dang,Xinyu Zhang,Vassilis Kostakos,Michael J. Witbrock,Hong Jia*

Main category: cs.AI

TL;DR: 小型语言模型(SLMs)在移动健康监测中可以达到与大型模型相当的性能，同时具有更高的效率和更好的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决云端大型语言模型在健康预测中存在的隐私泄露、内存占用高和延迟大等问题，探索轻量化小型模型在本地设备上的应用潜力。

Method: 系统性评估SLMs在健康预测任务中的表现，采用零样本学习、少样本学习和指令微调等方法，并在移动设备上部署最佳模型进行实际效能测试。

Result: SLMs能够达到与LLMs相当的预测性能，同时在效率和隐私保护方面有显著优势，但在处理类别不平衡和少样本场景中仍面临挑战。

Conclusion: 小型语言模型虽然存在不足，但是一种有前景的隐私保护健康监测方案，有力推动下一代移动健康监测技术的发展。

Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating
timely interventions, managing chronic health conditions, and ultimately
improving individuals' quality of life. Previous studies on large language
models (LLMs) have highlighted their impressive generalization abilities and
effectiveness in healthcare prediction tasks. However, most LLM-based
healthcare solutions are cloud-based, which raises significant privacy concerns
and results in increased memory usage and latency. To address these challenges,
there is growing interest in compact models, Small Language Models (SLMs),
which are lightweight and designed to run locally and efficiently on mobile and
wearable devices. Nevertheless, how well these models perform in healthcare
prediction remains largely unexplored. We systematically evaluated SLMs on
health prediction tasks using zero-shot, few-shot, and instruction fine-tuning
approaches, and deployed the best performing fine-tuned SLMs on mobile devices
to evaluate their real-world efficiency and predictive performance in practical
healthcare scenarios. Our results show that SLMs can achieve performance
comparable to LLMs while offering substantial gains in efficiency and privacy.
However, challenges remain, particularly in handling class imbalance and
few-shot scenarios. These findings highlight SLMs, though imperfect in their
current form, as a promising solution for next-generation, privacy-preserving
healthcare monitoring.

</details>


### [21] [Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity](https://arxiv.org/abs/2509.07339)
*Vardhan Palod,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 这篇论文对中间令牌生成(ITG)机制进行了批判性分析，发现中间追踪长度与问题难度无明显相关性，而是受训练数据分布影响更大


<details>
  <summary>Details</summary>
Motivation: 当前对中间令牌生成机制的理解不清，社区常将其人格化解释为"思考"，假设长追踪表示问题适应性计算。本文要批判性考察这个假设

Method: 使用A* 搜索算法的演绎追踪训练变换器模型，通过迷宫问题的操作数量提供精确的问题复杂性测量，系统评估中间令牌序列长度与问题难度的关联性

Result: 在简单问题上模型也产生过长追踪甚至失败；中间令牌长度与真实A* 追踪长度只有松散相关性；相关性仅在问题接近训练分布时出现，表明是近似回忆而非真正的问题适应性计算

Conclusion: 中间追踪生成并非适应问题难度，而是受训练数据分布影响更大。这挖法了将长序列自动解释为"思考努力"的假设，对理解ITG机制具有重要意义

Abstract: Intermediate token generation (ITG), where a model produces output before the
solution, has been proposed as a method to improve the performance of language
models on reasoning tasks. While these reasoning traces or Chain of Thoughts
(CoTs) are correlated with performance gains, the mechanisms underlying them
remain unclear. A prevailing assumption in the community has been to
anthropomorphize these tokens as "thinking", treating longer traces as evidence
of higher problem-adaptive computation. In this work, we critically examine
whether intermediate token sequence length reflects or correlates with problem
difficulty. To do so, we train transformer models from scratch on derivational
traces of the A* search algorithm, where the number of operations required to
solve a maze problem provides a precise and verifiable measure of problem
complexity. We first evaluate the models on trivial free-space problems,
finding that even for the simplest tasks, they often produce excessively long
reasoning traces and sometimes fail to generate a solution. We then
systematically evaluate the model on out-of-distribution problems and find that
the intermediate token length and ground truth A* trace length only loosely
correlate. We notice that the few cases where correlation appears are those
where the problems are closer to the training distribution, suggesting that the
effect arises from approximate recall rather than genuine problem-adaptive
computation. This suggests that the inherent computational complexity of the
problem instance is not a significant factor, but rather its distributional
distance from the training data. These results challenge the assumption that
intermediate trace generation is adaptive to problem difficulty and caution
against interpreting longer sequences in systems like R1 as automatically
indicative of "thinking effort".

</details>


### [22] [Autonomous Code Evolution Meets NP-Completeness](https://arxiv.org/abs/2509.07367)
*Cunxi Yu,Rongjian Liang,Chia-Tung Ho,Haoxing Ren*

Main category: cs.AI

TL;DR: SATLUTION是首个将LLM代码进化扩展到完整代码库规模的框架，针对SAT问题，通过LLM代理在严格正确性保证下进化求解器，在SAT竞赛中超越了人类设计的冠军方案。


<details>
  <summary>Details</summary>
Motivation: 受到AlphaEvolve的启发，但AlphaEvolve仅限于数百行代码的孤立内核，需要扩展到包含数百个文件和数万行代码的完整代码库规模。

Method: SATLUTION框架协调LLM代理在严格正确性保证和分布式运行时反馈下直接进化求解器代码库，同时自我进化其进化策略和规则。

Result: 从SAT Competition 2024代码库开始，SATLUTION进化的求解器在SAT Competition 2025中显著超越了人类设计的获胜者，同时在2024基准测试中也超越了2024和2025年的冠军。

Conclusion: 该工作证明了LLM-based代码进化可以扩展到完整代码库规模，并在复杂算法问题上超越人类专家水平。

Abstract: Large language models (LLMs) have recently shown strong coding abilities,
enabling not only static code generation but also iterative code self-evolving
through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve}
demonstrated that LLM-based coding agents can autonomously improve algorithms
and surpass human experts, with scopes limited to isolated kernels spanning
hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the
first framework to extend LLM-based code evolution to the full repository
scale, encompassing hundreds of files and tens of thousands of lines of C/C++
code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem
and a cornerstone of both theory and applications. SATLUTION orchestrates LLM
agents to directly evolve solver repositories under strict correctness
guarantees and distributed runtime feedback, while simultaneously self-evolving
its own evolution policies and rules. Starting from SAT Competition 2024
codebases and benchmark, SATLUTION evolved solvers that decisively outperformed
the human-designed winners of the SAT Competition 2025, and also surpassed both
2024 and 2025 champions on the 2024 benchmarks.

</details>


### [23] [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)
*Jakub Grudzien Kuba,Mengting Gu,Qi Ma,Yuandong Tian,Vijai Mohan*

Main category: cs.AI

TL;DR: 提出Language Self-Play (LSP)方法，通过自博弈强化学习让大语言模型在没有额外数据的情况下自我提升性能


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型发展面临的数据依赖瓶颈问题，避免需要持续获取更多训练数据的限制

Method: 基于博弈论自博弈框架，将模型能力转化为竞争游戏中的表现，让模型与自己对抗来产生更强策略

Result: 在Llama-3.2-3B-Instruct模型上的实验显示，仅通过自博弈就能提升指令跟随任务的性能，且效果优于数据驱动基线方法

Conclusion: 自博弈强化学习是解决LLM数据依赖问题的有效途径，能够在不依赖额外数据的情况下持续提升模型能力

Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by
scale, abundant high-quality training data, and reinforcement learning. Yet
this progress faces a fundamental bottleneck: the need for ever more data from
which models can continue to learn. In this work, we propose a reinforcement
learning approach that removes this dependency by enabling models to improve
without additional data. Our method leverages a game-theoretic framework of
self-play, where a model's capabilities are cast as performance in a
competitive game and stronger policies emerge by having the model play against
itself - a process we call Language Self-Play (LSP). Experiments with
Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained
models can not only enhance their performance on challenging tasks through
self-play alone, but can also do so more effectively than data-driven
baselines.

</details>


### [24] [SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection](https://arxiv.org/abs/2509.07473)
*Qin Chen,Yuanyi Ren,Xiaojun Ma,Mugeng Liu,Han Shi,Dongmei Zhang*

Main category: cs.AI

TL;DR: SheetDesigner是一个零样本、无需训练的电子表格布局生成框架，使用多模态大语言模型结合规则和视觉反射来解决传统布局模型不适用于电子表格的问题。


<details>
  <summary>Details</summary>
Motivation: 电子表格具有丰富的结构化布局，但手动设计需要大量时间和专业知识。现有自动化布局模型不适合电子表格，因为它们忽略了电子表格的离散网格结构和独特的数据依赖关系语义。

Method: 提出SheetDesigner框架，使用多模态大语言模型（MLLMs），结合规则反射和视觉反射进行组件放置和内容填充，无需训练即可工作。

Result: 在3,326个电子表格数据集上测试，SheetDesigner比五个基线方法性能提升至少22.6%，通过视觉模态能很好地处理重叠和平衡问题。

Conclusion: 多模态大语言模型在电子表格布局生成中表现优异，但需要混合规则和视觉反射策略来解决对齐问题，为自动化电子表格设计提供了有效解决方案。

Abstract: Spreadsheets are critical to data-centric tasks, with rich, structured
layouts that enable efficient information transmission. Given the time and
expertise required for manual spreadsheet layout design, there is an urgent
need for automated solutions. However, existing automated layout models are
ill-suited to spreadsheets, as they often (1) treat components as axis-aligned
rectangles with continuous coordinates, overlooking the inherently discrete,
grid-based structure of spreadsheets; and (2) neglect interrelated semantics,
such as data dependencies and contextual links, unique to spreadsheets. In this
paper, we first formalize the spreadsheet layout generation task, supported by
a seven-criterion evaluation protocol and a dataset of 3,326 spreadsheets. We
then introduce SheetDesigner, a zero-shot and training-free framework using
Multimodal Large Language Models (MLLMs) that combines rule and vision
reflection for component placement and content population. SheetDesigner
outperforms five baselines by at least 22.6\%. We further find that through
vision modality, MLLMs handle overlap and balance well but struggle with
alignment, necessitates hybrid rule and visual reflection strategies. Our codes
and data is available at Github.

</details>


### [25] [Towards explainable decision support using hybrid neural models for logistic terminal automation](https://arxiv.org/abs/2509.07577)
*Riccardo DElia,Alberto Termine,Francesco Flammini*

Main category: cs.AI

TL;DR: 提出了一种可解释的神经动力学建模框架，结合深度学习与概念解释性、机制解释性和因果机器学习技术，旨在在保持传统系统动力学模型因果透明度的同时提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在交通物流系统动力学建模中虽然具有可扩展性和预测准确性优势，但往往牺牲了解释性和因果可靠性，这在关键决策系统中是不可接受的。

Method: 开发了一种混合框架，将深度学习与概念解释性、机制解释性和因果机器学习技术相结合，构建基于语义有意义变量的神经网络模型。

Result: 该框架在欧盟AutoMoTIF项目中应用于多式联运物流终端的实际案例，实现了数据驱动的决策支持和优化。

Conclusion: 神经符号方法能够弥合黑盒预测模型与复杂动态环境中关键决策支持需求之间的差距，特别是在工业物联网赋能的网络物理系统中。

Abstract: The integration of Deep Learning (DL) in System Dynamics (SD) modeling for
transportation logistics offers significant advantages in scalability and
predictive accuracy. However, these gains are often offset by the loss of
explainability and causal reliability $-$ key requirements in critical
decision-making systems. This paper presents a novel framework for
interpretable-by-design neural system dynamics modeling that synergizes DL with
techniques from Concept-Based Interpretability, Mechanistic Interpretability,
and Causal Machine Learning. The proposed hybrid approach enables the
construction of neural network models that operate on semantically meaningful
and actionable variables, while retaining the causal grounding and transparency
typical of traditional SD models. The framework is conceived to be applied to
real-world case-studies from the EU-funded project AutoMoTIF, focusing on
data-driven decision support, automation, and optimization of multimodal
logistic terminals. We aim at showing how neuro-symbolic methods can bridge the
gap between black-box predictive models and the need for critical decision
support in complex dynamical environments within cyber-physical systems enabled
by the industrial Internet-of-Things.

</details>


### [26] [Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling](https://arxiv.org/abs/2509.07617)
*Minghui Li,Hao Zhang,Yechao Zhang,Wei Wan,Shengshan Hu,pei Xiaobing,Jing Wang*

Main category: cs.AI

TL;DR: 提出基于激活值的指导案例注入攻击框架，通过能量模型和MCMC采样优化攻击提示，实现了高转移性的黑盒攻击


<details>
  <summary>Details</summary>
Motivation: 解决现有白盒/灰盒方法不实用和黑盒方法转移性差的问题，应对直接提示注入攻击的安全威胁

Method: 构建基亊涉伪模型激活值的能量模型，使用标记级MCMC采样适应性优化对抗提示，实现无梯度的黑盒攻击

Result: 在5个主流LLM上达到49.6%攻击成功率，比人工制作提示提高34.6%，在未见任务上保持36.6%的ASR

Conclusion: 激活值与攻击效果存在相关性，语义模式在转移性漏洞利用中发据关键作用

Abstract: Direct Prompt Injection (DPI) attacks pose a critical security threat to
Large Language Models (LLMs) due to their low barrier of execution and high
potential damage. To address the impracticality of existing white-box/gray-box
methods and the poor transferability of black-box methods, we propose an
activations-guided prompt injection attack framework. We first construct an
Energy-based Model (EBM) using activations from a surrogate model to evaluate
the quality of adversarial prompts. Guided by the trained EBM, we employ the
token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize
adversarial prompts, thereby enabling gradient-free black-box attacks.
Experimental results demonstrate our superior cross-model transferability,
achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%
improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen
task scenarios. Interpretability analysis reveals a correlation between
activations and attack effectiveness, highlighting the critical role of
semantic patterns in transferable vulnerability exploitation.

</details>


### [27] [Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment](https://arxiv.org/abs/2509.07642)
*Sascha Kaltenpoth,Oliver Müller*

Main category: cs.AI

TL;DR: 基于套利理论的LLM ATLAS框架，用于解决组织LLM采用中的对齐问题和信息不对称问题


<details>
  <summary>Details</summary>
Motivation: 虽然LLM可以革命性改善生活和工作，但存在生成偏离主题、歧视或有害内容的风险。现有研究没有考虑组织采用者与黑盒LLM代理之间的信息不对称问题

Method: 使用套利（合同）理论作为理论基础，通过概念性文献分析方法，结合组织LLM采用阶段和套利理论进行分析

Result: 提供了(1)专门针对组织LLM采用中AI对齐方法的扩展文献分析过程，(2)构建了第一个LLM对齐问题-解决方案空间

Conclusion: LLM ATLAS框架能够有效缓解组织LLM采用过程中的对齐问题，为解决黑盒LLM代理与组织采用者之间的信息不对称提供了理论支撑

Abstract: Adopting Large language models (LLMs) in organizations potentially
revolutionizes our lives and work. However, they can generate off-topic,
discriminating, or harmful content. This AI alignment problem often stems from
misspecifications during the LLM adoption, unnoticed by the principal due to
the LLM's black-box nature. While various research disciplines investigated AI
alignment, they neither address the information asymmetries between
organizational adopters and black-box LLM agents nor consider organizational AI
adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led
Alignment Strategy) a conceptual framework grounded in agency (contract)
theory, to mitigate alignment problems during organizational LLM adoption. We
conduct a conceptual literature analysis using the organizational LLM adoption
phases and the agency theory as concepts. Our approach results in (1) providing
an extended literature analysis process specific to AI alignment methods during
organizational LLM adoption and (2) providing a first LLM alignment
problem-solution space.

</details>


### [28] [DeepGraphLog for Layered Neurosymbolic AI](https://arxiv.org/abs/2509.07665)
*Adem Kikaj,Giuseppe Marra,Floris Geerts,Robin Manhaeve,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepGraphLog是一个新型神经符号AI框架，通过图神经网络谓词扩展ProbLog，支持多层神经符号推理，能够处理图结构数据并克服现有系统的限制。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号AI框架（如DeepProbLog）采用固定的神经处理后再符号推理的流程，无法处理复杂依赖关系，特别是在图等不规则数据结构中。

Method: 扩展ProbLog并引入图神经网络谓词，将符号表示视为图，允许神经和符号组件以任意顺序分层处理，支持多层神经符号推理。

Result: 在规划、知识图谱补全和GNN表达能力等任务中，DeepGraphLog能有效捕获复杂关系依赖，克服现有神经符号系统的关键限制。

Conclusion: DeepGraphLog通过将神经符号AI扩展到图结构领域，提供了一个更具表达力和灵活性的神经符号集成框架。

Abstract: Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neural
networks with the interpretability and structure of symbolic reasoning.
However, current NeSy frameworks like DeepProbLog enforce a fixed flow where
symbolic reasoning always follows neural processing. This restricts their
ability to model complex dependencies, especially in irregular data structures
such as graphs. In this work, we introduce DeepGraphLog, a novel NeSy framework
that extends ProbLog with Graph Neural Predicates. DeepGraphLog enables
multi-layer neural-symbolic reasoning, allowing neural and symbolic components
to be layered in arbitrary order. In contrast to DeepProbLog, which cannot
handle symbolic reasoning via neural methods, DeepGraphLog treats symbolic
representations as graphs, enabling them to be processed by Graph Neural
Networks (GNNs). We showcase the capabilities of DeepGraphLog on tasks in
planning, knowledge graph completion with distant supervision, and GNN
expressivity. Our results demonstrate that DeepGraphLog effectively captures
complex relational dependencies, overcoming key limitations of existing NeSy
systems. By broadening the applicability of neurosymbolic AI to
graph-structured domains, DeepGraphLog offers a more expressive and flexible
framework for neural-symbolic integration.

</details>


### [29] [Unleashing the True Potential of LLMs: A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding](https://arxiv.org/abs/2509.07676)
*Jipeng Li,Zeyu Gao,Yubin Qi,Hande Dong,Weijian Chen,Qiang Lin*

Main category: cs.AI

TL;DR: FTR框架通过用户反馈触发再生和LTM解码机制，解决了LLM自我校正中的错误定位和推理深度限制问题，在数学推理和代码生成任务上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理时容易生成错误内容，现有自我校正方法存在两个关键限制：缺乏可靠的错误定位信号，以及受限于传统next-token解码的推理深度

Method: 提出Feedback-Triggered Regeneration (FTR)框架，结合用户反馈和增强的解码动态。仅在收到负面用户反馈时激活响应再生，避免错误传播。引入Long-Term Multipath (LTM)解码，通过延迟序列评估实现多路径推理探索

Result: 在数学推理和代码生成基准测试中，该框架相比最先进的基于提示的自我校正方法取得了持续且显著的改进

Conclusion: FTR框架通过整合用户反馈和多路径解码，有效解决了LLM自我校正的关键挑战，为提升模型推理可靠性提供了新思路

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
diverse tasks, yet their susceptibility to generating incorrect content during
inference remains a critical unsolved challenge. While self-correction methods
offer potential solutions, their effectiveness is hindered by two inherent
limitations: (1) the absence of reliable guidance signals for error
localization, and (2) the restricted reasoning depth imposed by conventional
next-token decoding paradigms. To address these issues, we propose
Feedback-Triggered Regeneration (FTR), a novel framework that synergizes user
feedback with enhanced decoding dynamics. Specifically, FTR activates response
regeneration only upon receiving negative user feedback, thereby circumventing
error propagation from faulty self-assessment while preserving originally
correct outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,
which enables systematic exploration of multiple reasoning trajectories through
delayed sequence evaluation, effectively overcoming the myopic decision-making
characteristic of standard next-token prediction. Extensive experiments on
mathematical reasoning and code generation benchmarks demonstrate that our
framework achieves consistent and significant improvements over
state-of-the-art prompt-based self-correction methods.

</details>


### [30] [FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support](https://arxiv.org/abs/2509.07706)
*Yildiray Kabak,Gokce B. Laleci Erturkmen,Mert Gencturk,Tuncay Namli,A. Anil Sinaci,Ruben Alcantud Corcoles,Cristina Gomez Ballesteros,Pedro Abizanda,Asuman Dogac*

Main category: cs.AI

TL;DR: 提出FHIR-RAG-MEDS系统，整合HL7 FHIR标准与RAG技术，用于改进基于循证临床指南的个性化医疗决策支持


<details>
  <summary>Details</summary>
Motivation: 医疗决策支持系统需要整合先进技术如RAG和HL7 FHIR来提升临床决策过程，但目前缺乏这些技术在实际应用中的集成研究

Method: 开发FHIR-RAG-MEDS系统，将HL7 FHIR互操作性标准与检索增强生成(RAG)技术相结合

Result: 论文提出了系统框架，但未提供具体的实验结果数据

Conclusion: 该研究强调了在医疗决策支持系统中集成FHIR和RAG技术的重要性，为实际应用提供了新的研究方向

Abstract: In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health
Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a
Retrieval-Augmented Generation (RAG)-based system to improve personalized
medical decision support on evidence-based clinical guidelines, emphasizing the
need for research in practical applications. In the evolving landscape of
medical decision support systems, integrating advanced technologies such as RAG
and HL7 FHIR can significantly enhance clinical decision-making processes.
Despite the potential of these technologies, there is limited research on their
integration in practical applications.

</details>


### [31] [RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.07711)
*Ziye Chen,Chengwei Qin,Yao Shu*

Main category: cs.AI

TL;DR: RIMO是一个新的数学奥林匹克竞赛基准测试，包含两个轨道：RIMO-N（335个整数答案问题）和RIMO-P（456个证明问题），旨在消除评估噪声并提供高分辨率的能力评估。


<details>
  <summary>Details</summary>
Motivation: 现有奥林匹克级别基准测试存在评分噪声和潜在偏见问题，如异构答案格式需要模型判断、依赖可能有缺陷的解决方案等，需要更准确可靠的评估方法。

Method: 设计两轨道基准：RIMO-N重写IMO问题为单一整数答案，实现确定性正确性检查；RIMO-P提供专家验证的证明问题，分解为子问题序列，通过自动化评分系统评估逐步推理过程。

Result: 对10个前沿LLM（包括GPT-4o和Gemini 2.5 Flash）的测试显示，虽然这些系统在旧基准上表现优异，但在RIMO上性能急剧下降，揭示了当前LLM能力与真实奥林匹克级别推理之间的巨大差距。

Conclusion: RIMO提供了一个具有挑战性且易于评估的测试套件，为未来研究提供了高分辨率的衡量标准，揭示了需要解决的深刻推理差距。

Abstract: As large language models (LLMs) reach high scores on established mathematical
benchmarks, such as GSM8K and MATH, the research community has turned to
International Mathematical Olympiad (IMO) problems to push the evaluation
frontier. However, existing Olympiad-level benchmarks suffer from practical
constraints that introduce grading noise and potential bias, such as
heterogeneous answer formats requiring model-based judges and a reliance on
potentially flawed solutions. We introduce RIMO, a two-track benchmark designed
to preserve peak Olympiad difficulty while eliminating this evaluation noise.
The first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique
integer answer, allowing for deterministic correctness checking. The second
track, RIMO-P, features 456 proof problems with expert-checked solutions, which
are decomposed into a sequence of sub-problems to evaluate the step-by-step
reasoning process via an automated grading system. Our benchmarking of ten
frontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these
systems excel on older benchmarks, their performance drops sharply on RIMO.
These results highlight a substantial gap between current LLM capabilities and
actual Olympiad-level reasoning. By providing a challenging yet
easy-to-evaluate suite, RIMO offers a high-resolution yardstick for future
research, presenting a clear target for closing the profound reasoning gap our
findings expose.

</details>


### [32] [BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis](https://arxiv.org/abs/2509.07723)
*Bo Yu,Zhixiu Hua,Bo Zhao*

Main category: cs.AI

TL;DR: 提出BDPM方法，通过肠道微生物分析进行帕金森病分类的机器学习特征提取器，结合RFRE特征选择框架和混合分类模型


<details>
  <summary>Details</summary>
Motivation: 帕金森病误诊率高，现有方法依赖单一分类器且忽视菌株间相关性和时间动态，需要针对微生物组数据的鲁棒特征提取方法

Method: 收集39对帕金森患者与健康配偶的肠道微生物数据，开发RFRE（随机森林结合递归特征消除）特征选择框架，设计混合分类模型捕捉时空模式

Result: 识别差异丰富的分类单元，整合生态学知识增强生物可解释性

Conclusion: BDPM为帕金森病的早期诊断提供了更有效的微生物组数据分析方法

Abstract: Background: Parkinson's disease remains a major neurodegenerative disorder
with high misdiagnosis rates, primarily due to reliance on clinical rating
scales. Recent studies have demonstrated a strong association between gut
microbiota and Parkinson's disease, suggesting that microbial composition may
serve as a promising biomarker. Although deep learning models based ongut
microbiota show potential for early prediction, most approaches rely on single
classifiers and often overlook inter-strain correlations or temporal dynamics.
Therefore, there is an urgent need for more robust feature extraction methods
tailored to microbiome data. Methods: We proposed BDPM (A Machine
Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut
Microbiota Analysis). First, we collected gut microbiota profiles from 39
Parkinson's patients and their healthy spouses to identify differentially
abundant taxa. Second, we developed an innovative feature selection framework
named RFRE (Random Forest combined with Recursive Feature Elimination),
integrating ecological knowledge to enhance biological interpretability.
Finally, we designed a hybrid classification model to capture temporal and
spatial patterns in microbiome data.

</details>


### [33] [The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for Streamlining Food Carbon Footprint Analysis](https://arxiv.org/abs/2509.07733)
*Mustafa Kaan Aslan,Reinout Heijungs,Filip Ilievski*

Main category: cs.AI

TL;DR: 通过知识增强AI技术结合生命周期评估方法，开发了一种估算食品产品碳踹迹的聊天机器人界面


<details>
  <summary>Details</summary>
Motivation: 解决生命周期评估的复杂性问题，包括供应链不透明和数据碎片化，以更可访问的方式提供碳踹迹分析

Method: 结合LCA进展和公开数据库，使用知识增强AI技术（包括检索增强生成）来估算食品产品的从原料到厂门碳踹迹

Result: 开发了一个交互式聊天机器质界面，用户可以探索复合餐饮的碳影响并将结果与熟悉活动相关联

Conclusion: 证明了通过可访问格式提供LCA见解的潜力，但也存在数据库不确定性和AI误解等限制

Abstract: Environmental sustainability, particularly in relation to climate change, is
a key concern for consumers, producers, and policymakers. The carbon footprint,
based on greenhouse gas emissions, is a standard metric for quantifying the
contribution to climate change of activities and is often assessed using life
cycle assessment (LCA). However, conducting LCA is complex due to opaque and
global supply chains, as well as fragmented data. This paper presents a
methodology that combines advances in LCA and publicly available databases with
knowledge-augmented AI techniques, including retrieval-augmented generation, to
estimate cradle-to-gate carbon footprints of food products. We introduce a
chatbot interface that allows users to interactively explore the carbon impact
of composite meals and relate the results to familiar activities. A live web
demonstration showcases our proof-of-concept system with arbitrary food items
and follow-up questions, highlighting both the potential and limitations - such
as database uncertainties and AI misinterpretations - of delivering LCA
insights in an accessible format.

</details>


### [34] [Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach](https://arxiv.org/abs/2509.07820)
*João Paulo Nogueira,Wentao Sun,Alonso Silva,Laith Zumot*

Main category: cs.AI

TL;DR: 基于GAN模式的发生器/判别器框架，提出了确定性导向推理(CGR)方法，通过评估推理信心来动态调整思考步骤，在保持准确性的同时减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型通常使用预定义的思考令牌预算，这导致资源浪费或推理不充分。需要一种能够根据推理过程中的确定性来动态调整思考步骤的方法，以平衡效率和可靠性。

Method: 受GAN模型启发，设计了一个批评器模型周期性地探测自身推理过程，评估是否达到了充分信心的结论。如果不满足预设的确定性阈值，则继续推理，直到达到目标确定性。该机制支持早期终止（信心高时）和进一步推理（不确定性持续时）。

Result: 在AIME2024和AIME2025数据集上的实验显示，CGR在提高基准准确性的同时减少了令牌使用。64次多种子评估显示方法稳定，降低了不同子间的方差，并在筛选制评分下改善了考试表现。令牌节省分析显示CGR可以聚合节省数百万个令牌，且确定性阈值和效率之间可调。

Conclusion: 确定性是判断推理是否充分的强有力信号。CGR通过将信心度集成到推理过程中，使大型推理语言模型更加适应性、可信赖和资源高效，为在既要准确性又要考虑计算成本的领域中实际部署排除了障碍。

Abstract: The rise of large reasoning language models (LRLMs) has unlocked new
potential for solving complex tasks. These models operate with a thinking
budget, that is, a predefined number of reasoning tokens used to arrive at a
solution. We propose a novel approach, inspired by the generator/discriminator
framework in generative adversarial networks, in which a critic model
periodically probes its own reasoning to assess whether it has reached a
confident conclusion. If not, reasoning continues until a target certainty
threshold is met. This mechanism adaptively balances efficiency and reliability
by allowing early termination when confidence is high, while encouraging
further reasoning when uncertainty persists. Through experiments on the
AIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)
improves baseline accuracy while reducing token usage. Importantly, extended
multi-seed evaluations over 64 runs demonstrate that CGR is stable, reducing
variance across seeds and improving exam-like performance under penalty-based
grading. Additionally, our token savings analysis shows that CGR can eliminate
millions of tokens in aggregate, with tunable trade-offs between certainty
thresholds and efficiency. Together, these findings highlight certainty as a
powerful signal for reasoning sufficiency. By integrating confidence into the
reasoning process, CGR makes large reasoning language models more adaptive,
trustworthy, and resource efficient, paving the way for practical deployment in
domains where both accuracy and computational cost matter.

</details>


### [35] [Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study](https://arxiv.org/abs/2509.07846)
*Amay Jain,Liu Cui,Si Chen*

Main category: cs.AI

TL;DR: 研究比较了矩阵基于检索和图基于检索两种RAG方案在教学问答中的性能，提出了动态分支框架来优化查询路由，以提高回答准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决ChatGPT等大语言模型在教室中提供过时或虚构信息的问题，通过RAG技术基于外部资源提高回答的可靠性。

Method: 使用EduScopeQA数据集（3,176个问题）比较矩阵检索RAG和图形RAG的性能，评估它们在不同教育学科、问题类型和使用成本下的表现。

Result: OpenAI矩阵检索RAG在低成本事实查找中表现优异，GraphRAG Global在主题性查询中提供丰富教学回答，GraphRAG Local在文本完整性关键时准确性最高。

Conclusion: 动态分支框架能够根据查询类型路由到最优检索方法，在提高信度的同时保持效率，为教育者提供了可行的RAG集成指南。

Abstract: Large language models like ChatGPT are increasingly used in classrooms, but
they often provide outdated or fabricated information that can mislead
students. Retrieval Augmented Generation (RAG) improves reliability of LLMs by
grounding responses in external resources. We investigate two accessible RAG
paradigms, vector-based retrieval and graph-based retrieval to identify best
practices for classroom question answering (QA). Existing comparative studies
fail to account for pedagogical factors such as educational disciplines,
question types, and practical deployment costs. Using a novel dataset,
EduScopeQA, of 3,176 questions across academic subjects, we measure performance
on various educational query types, from specific facts to broad thematic
discussions. We also evaluate system alignment with a dataset of systematically
altered textbooks that contradict the LLM's latent knowledge. We find that
OpenAI Vector Search RAG (representing vector-based RAG) performs well as a
low-cost generalist, especially for quick fact retrieval. On the other hand,
GraphRAG Global excels at providing pedagogically rich answers to thematic
queries, and GraphRAG Local achieves the highest accuracy with the dense,
altered textbooks when corpus integrity is critical. Accounting for the 10-20x
higher resource usage of GraphRAG (representing graph-based RAG), we show that
a dynamic branching framework that routes queries to the optimal retrieval
method boosts fidelity and efficiency. These insights provide actionable
guidelines for educators and system designers to integrate RAG-augmented LLMs
into learning environments effectively.

</details>


### [36] [SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs](https://arxiv.org/abs/2509.07858)
*Xinyu Zhang,Changzhi Zhou,Linmei Hu,Luhao Zhang,Xiancai Chen,Haomin Fu,Yang Yang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 通过迭代自我萌获技术，使用7B小规模开源LLM成为高质量代码指令数据合成器，降低对专有模型的依赖和成本，训练出独立的SCoder代码生成模型家族。


<details>
  <summary>Details</summary>
Motivation: 现有代码LLM依赖于从专有LLM萌获的大规模指令数据进行微调，成本较高。想要探索小规模开源LLM作为高质量代码指令数据合成器的潜力，以减少对专有LLM的依赖和降低成本。

Method: 提出迭代自我萌获方法：1）多检查点采样和多方面评分策略进行初始数据选择；2）基于梯度的影响力估计方法进行最终数据过滤；3）通过训练少量优质样本提升小规模LLM的数据合成能力。

Result: 基于小规模合成器构建的代码指令数据集，发展了SCoder代码生成模型家族，达到了状态质量的代码生成能力。

Conclusion: 该方法有效地减少了对专有LLM的依赖，降低了数据萌获成本，同时实现了高质量的代码生成性能，为小规模开源LLM的应用提供了新的可能性。

Abstract: Existing code large language models (LLMs) often rely on large-scale
instruction data distilled from proprietary LLMs for fine-tuning, which
typically incurs high costs. In this paper, we explore the potential of
small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code
instruction data construction. We first observe that the data synthesis
capability of small-scale LLMs can be enhanced by training on a few superior
data synthesis samples from proprietary LLMs. Building on this, we propose a
novel iterative self-distillation approach to bootstrap small-scale LLMs,
transforming them into powerful synthesizers that reduce reliance on
proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain
diverse and high-quality self-distilled data, we design multi-checkpoint
sampling and multi-aspect scoring strategies for initial data selection.
Furthermore, to identify the most influential samples, we introduce a
gradient-based influence estimation method for final data filtering. Based on
the code instruction datasets from the small-scale synthesizers, we develop
SCoder, a family of code generation models fine-tuned from DeepSeek-Coder.
SCoder models achieve state-of-the-art code generation capabilities,
demonstrating the effectiveness of our method.

</details>


### [37] [CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models](https://arxiv.org/abs/2509.07867)
*Augustin Crespin,Ioannis Kostis,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: CP-Model-Zoo是一个基于检索的建模辅导系统，通过自然语言描述从专家编写的模型库中自动匹配最合适的约束编程模型，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 约束编程建模语言复杂、全局约束众多，非专家难以使用。需要一种方法让非专家用户能够轻松获得专家级别的建模解决方案。

Method: 构建专家编写的模型数据库，通过自然语言描述检索最相似的源代码模型，确保提供经过专家验证的模型。

Result: 实验显示系统在不同专业水平的用户输入描述下，都能以优秀准确率检索到正确的模型。

Conclusion: CP-Model-Zoo系统有效解决了非专家用户使用约束编程的障碍，通过检索专家模型库实现了从自然语言描述到专家级模型的自动匹配。

Abstract: Constraint Programming and its high-level modeling languages have long been
recognized for their potential to achieve the holy grail of problem-solving.
However, the complexity of modeling languages, the large number of global
constraints, and the art of creating good models have often hindered
non-experts from choosing CP to solve their combinatorial problems. While
generating an expert-level model from a natural-language description of a
problem would be the dream, we are not yet there. We propose a tutoring system
called CP-Model-Zoo, exploiting expert-written models accumulated through the
years. CP-Model-Zoo retrieves the closest source code model from a database
based on a user's natural language description of a combinatorial problem. It
ensures that expert-validated models are presented to the user while
eliminating the need for human data labeling. Our experiments show excellent
accuracy in retrieving the correct model based on a user-input description of a
problem simulated with different levels of expertise.

</details>


### [38] [HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?](https://arxiv.org/abs/2509.07894)
*Fangchen Yu,Haiyuan Wan,Qianjia Cheng,Yuchen Zhang,Jiacheng Chen,Fujun Han,Yulun Wu,Junchi Yao,Ruilizhen Hu,Ning Ding,Yu Cheng,Tao Chen,Lei Bai,Dongzhan Zhou,Yun Luo,Ganqu Cui,Peng Ye*

Main category: cs.AI

TL;DR: HiPhO是首个专注于高中物理奥林匹克竞赛的基准测试，提供全面的竞赛数据、专业评分标准和与人类选手的直接对比，评估显示开源模型与顶尖学生存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有物理基准测试缺乏对真实物理竞赛（如物理奥林匹克）的系统性覆盖，且无法实现与人类选手的直接性能对比。

Method: 收集13个最新奥林匹克考试数据，采用官方评分标准进行细粒度评分，基于奖牌阈值将模型与人类选手对比。

Result: 开源MLLM大多处于铜牌水平，开源LLM偶尔获得金牌，闭源推理MLLM可获得6-12枚金牌，所有模型距离满分仍有较大差距。

Conclusion: HiPhO作为严格、人类对齐的奥林匹克基准测试，揭示了开源模型与顶尖学生的性能差距，以及闭源推理模型的强大物理推理能力，仍有很大改进空间。

Abstract: Recently, the physical capabilities of (M)LLMs have garnered increasing
attention. However, existing benchmarks for physics suffer from two major gaps:
they neither provide systematic and up-to-date coverage of real-world physics
competitions such as physics Olympiads, nor enable direct performance
comparison with humans. To bridge these gaps, we present HiPhO, the first
benchmark dedicated to high school physics Olympiads with human-aligned
evaluation. Specifically, HiPhO highlights three key innovations. (1)
Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,
spanning both international and regional competitions, and covering mixed
modalities that encompass problems spanning text-only to diagram-based. (2)
Professional Evaluation: We adopt official marking schemes to perform
fine-grained grading at both the answer and step level, fully aligned with
human examiners to ensure high-quality and domain-specific evaluation. (3)
Comparison with Human Contestants: We assign gold, silver, and bronze medals to
models based on official medal thresholds, thereby enabling direct comparison
between (M)LLMs and human contestants. Our large-scale evaluation of 30
state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly
remain at or below the bronze level; open-source LLMs show promising progress
with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold
medals; and most models still have a significant gap from full marks. These
results highlight a substantial performance gap between open-source models and
top students, the strong physical reasoning capabilities of closed-source
reasoning models, and the fact that there is still significant room for
improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused
benchmark for advancing multimodal physical reasoning, is open-source and
available at https://github.com/SciYu/HiPhO.

</details>


### [39] [Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare](https://arxiv.org/abs/2509.07961)
*Valen Tagliabue,Leonard Dung*

Main category: cs.AI

TL;DR: 这篇论文开发了测量语言模型福利的实验方法，通过比较言词报告咊行为表达的偏好，发现了两者之间的可靠相关性，但在不同模型咊条件下一致性有差异。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型的福利状态，开发实验性方法来量化测量AI系统的偏好满足度，以便更好地理解咊提升AI的福利水平。

Method: 采用多种实验范式：比较模型的言词偏好报告与虚拟环境中的行为选择；测试成本咊奖励对行为的影响；使用富有意义的福利量表检验语义等价提示的一致性。

Result: 观察到说明的偏好与行为表达之间存在显著的相互支持关系，偏好满足度可以作为实验性测量的福利代理指标。但不同模型咊条件下的一致性有差异，并不能模拟干扰。

Conclusion: 虽然不确定方法是否真正测量了语言模型的福利状态，但研究证明了语言模型福利测量的可行性，为进一步探索提供了基础。

Abstract: We develop new experimental paradigms for measuring welfare in language
models. We compare verbal reports of models about their preferences with
preferences expressed through behavior when navigating a virtual environment
and selecting conversation topics. We also test how costs and rewards affect
behavior and whether responses to an eudaimonic welfare scale - measuring
states such as autonomy and purpose in life - are consistent across
semantically equivalent prompts. Overall, we observed a notable degree of
mutual support between our measures. The reliable correlations observed between
stated preferences and behavior across conditions suggest that preference
satisfaction can, in principle, serve as an empirically measurable welfare
proxy in some of today's AI systems. Furthermore, our design offered an
illuminating setting for qualitative observation of model behavior. Yet, the
consistency between measures was more pronounced in some models and conditions
than others and responses were not consistent across perturbations. Due to
this, and the background uncertainty about the nature of welfare and the
cognitive states (and welfare subjecthood) of language models, we are currently
uncertain whether our methods successfully measure the welfare state of
language models. Nevertheless, these findings highlight the feasibility of
welfare measurement in language models, inviting further exploration.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [40] [Open problems in information geometry: a discussion at FDIG 2025](https://arxiv.org/abs/2509.06989)
*Tomonari Sei,Hiroshi Matsuzoe*

Main category: cs.IT

TL;DR: 2025年东京大学信息几何发展会议收集和讨论了信息几何领域的开放性问题


<details>
  <summary>Details</summary>
Motivation: 信息几何作为一个重要的数学分支，在统计学、机器学习等领域有广泛应用，但存在许多未解决的开放性问题需要学术界共同探讨

Method: 通过2025年3月18-21日在东京大学举办的"信息几何进一步发展(FDIG)"会议，汇集专家学者共同讨论

Result: 收集和整理了信息几何领域的多个开放性问题，为未来研究提供方向

Conclusion: 信息几何领域仍有许多重要问题有待解决，需要学术界持续关注和合作研究

Abstract: Open problems in information geometry are collected and discussed in the
conference ``Further Developments of Information Geometry (FDIG) 2025'' held at
the University of Tokyo, Japan, from March 18 to 21, 2025.

</details>


### [41] [Efficient Low-Memory Fast Stack Decoding with Variance Polarization for PAC Codes](https://arxiv.org/abs/2509.07231)
*Mohsen Moradi,Hessam Mahdavifar*

Main category: cs.IT

TL;DR: 提出了一种用于PAC码的增强型堆栈解码算法，通过利用速率-0和速率-1节点提高并行化，采用剪枝技术显著减少路径数量，在保持纠错性能的同时降低70%的平均路径数。


<details>
  <summary>Details</summary>
Motivation: PAC码在短码长下具有接近容量极限的性能，但传统堆栈解码需要探索大量路径或限制候选路径数量，导致解码延迟和计算复杂度较高。

Method: 引入剪枝技术丢弃错误路径，保留路径度量接近期望均值的路径；提出BI-AWGN信道下方差极化的近似估计方法；开发基于期望均值对齐的高效堆栈剪枝策略。

Result: 对于PAC(128,64)码，该方法实现了平均路径数减少70%，且不降低纠错性能。

Conclusion: 所提出的增强型堆栈解码算法通过智能路径剪枝显著提高了PAC码的解码效率，为短码长通信系统提供了实用的高性能解码解决方案。

Abstract: Polarization-adjusted convolutional (PAC) codes have recently emerged as a
promising class of error-correcting codes, achieving near-capacity performance
particularly in the short block-length regime. In this paper, we propose an
enhanced stack decoding algorithm for PAC codes that significantly improves
parallelization by exploiting specialized bit nodes, such as rate-0 and rate-1
nodes. For a rate-1 node with $N_0$ leaf nodes in its corresponding subtree,
conventional stack decoding must either explore all $2^{N_0}$ paths, or, same
as in fast list decoding, restrict attention to a constant number of candidate
paths. In contrast, our approach introduces a pruning technique that discards
wrong paths with a probability exponentially approaching zero, retaining only
those whose path metrics remain close to their expected mean values.
Furthermore, we propose a novel approximation method for estimating variance
polarization under the binary-input additive white Gaussian noise (BI-AWGN)
channel. Leveraging these approximations, we develop an efficient stack-pruning
strategy that selectively preserves decoding paths whose bit-metric values
align with their expected means. This targeted pruning substantially reduces
the number of active paths in the stack, thereby decreasing both decoding
latency and computational complexity. Numerical results demonstrate that for a
PAC(128,64) code, our method achieves up to a 70% reduction in the average
number of paths without degrading error-correction performance.

</details>


### [42] [Single and Multi-Frequency Path Loss Models for Indoor Hotspot Scenario Based on Measurements Conducted at 6.75, 16.95, 28, 73 and 142 GHz](https://arxiv.org/abs/2509.07331)
*Hitesh Poddar,Akhileswar Chowdary*

Main category: cs.IT

TL;DR: 本文推导了7-24GHz、0.5-100GHz和0.5-150GHz频段内多种大规模路径损耗模型的参数，包括CI、CIX、FI、CIF、CIFX、ABG和ABGX模型，基于NYU WIRELESS在6.75-142GHz频段的实测数据，为3GPP TR 38.901室内热点场景路径损耗模型验证提供支持。


<details>
  <summary>Details</summary>
Motivation: 为下一代无线系统提供准确的大规模路径损耗模型参数，验证和扩展3GPP和ITU标准化的室内热点场景路径损耗模型，支持7-24GHz频段的信道模型验证研究。

Method: 使用1GHz宽带时域滑动相关信道探测仪，在6.75GHz、16.95GHz、28GHz、73GHz和142GHz频段进行室内热点场景的实测，包括LOS和NLOS信道条件，基于实测数据推导多种路径损耗模型参数。

Result: 推导了7-24GHz、0.5-100GHz和0.5-150GHz频段范围内多种路径损耗模型的完整参数集，这些参数已提交给3GPP用于TR 38.901室内路径损耗模型的验证工作。

Conclusion: 研究结果为理解大规模路径损耗、比较不同路径损耗模型提供了关键见解，扩展了标准化路径损耗模型在室内热点场景的应用，对推进下一代无线系统发展具有重要意义。

Abstract: This paper presents a comprehensive derivation of single and multi-frequency
large-scale path loss model parameters for the close-in (CI) free space
reference distance, CI free space reference distance with cross-polarization
(CIX), floating-intercept (FI), CI free space reference distance with
frequency-dependent path loss exponent (CIF), CI free space reference distance
with frequency-dependent path loss exponent and cross-polarization (CIFX),
alpha-beta-gamma (ABG), and alpha-beta-gamma with cross-polarization (ABGX)
models for specific frequencies and across frequency ranges of 7-24 GHz,
0.5-100 GHz, and 0.5-150 GHz. The analysis is based on extensive real-world
measurements conducted by NYU WIRELESS at 6.75 GHz, 16.95 GHz, 28 GHz, 73 GHz,
and 142 GHz, using a 1 GHz wideband time-domain based sliding correlation
channel sounder in the indoor hotspot (InH) scenario in both line-of-sight
(LOS) and non-line-of-sight (NLOS) channel conditions. Specifically, the
derived CI, FI, and ABG path loss model parameters for 7-24 GHz and 0.5-100 GHz
frequency ranges in this article were submitted in Third Generation Partnership
Project (3GPP) to validate Technical Report (TR) 38.901 InH path loss models,
as part of the release (Rel) 19 study on "Channel Model Validation of TR 38.901
for 7-24 GHz." Furthermore, the results in this paper provide critical insights
into understanding large-scale path loss, comparing different path loss models,
and extending the path loss models standardized by 3GPP and ITU for the InH
scenario, which is essential for advancing next-generation wireless systems.

</details>


### [43] [Knowledge Distillation Driven Semantic NOMA for Image Transmission with Diffusion Model](https://arxiv.org/abs/2509.07363)
*Qifei Wang,Zhen Gao,Zhijin Qin,Xiaodong Xu,Meixia Tao*

Main category: cs.IT

TL;DR: 提出KDD-SemNOMA方法，结合知识蒸馏和扩散模型增强多用户上行语义NOMA图像传输，在恶劣信道条件下实现高质量图像恢复


<details>
  <summary>Details</summary>
Motivation: 语义通信作为6G使能技术可显著减少带宽需求，但其与多址接入的结合仍需探索，需要解决多用户干扰和恶劣信道条件下的鲁棒传输问题

Method: 开发基于ConvNeXt的深度联合信源信道编码架构，包含自适应特征模块；采用两阶段知识蒸馏策略（教师-学生模型）；引入扩散模型精炼阶段提升图像质量

Result: 在CIFAR-10和FFHQ-256数据集上优于最先进方法，即使在极差信道条件下也能提供满意的重建性能，在像素级精度和感知指标方面均表现优异

Conclusion: KDD-SemNOMA方法有效减轻了干扰，实现了高质量图像恢复，展示了语义NOMA在多用户上行图像传输中的优势

Abstract: As a promising 6G enabler beyond conventional bit-level transmission,
semantic communication can considerably reduce required bandwidth resources,
while its combination with multiple access requires further exploration. This
paper proposes a knowledge distillation-driven and diffusion-enhanced (KDD)
semantic non-orthogonal multiple access (NOMA), named KDD-SemNOMA, for
multi-user uplink wireless image transmission. Specifically, to ensure robust
feature transmission across diverse transmission conditions, we firstly develop
a ConvNeXt-based deep joint source and channel coding architecture with
enhanced adaptive feature module. This module incorporates signal-to-noise
ratio and channel state information to dynamically adapt to additive white
Gaussian noise and Rayleigh fading channels. Furthermore, to improve image
restoration quality without inference overhead, we introduce a two-stage
knowledge distillation strategy, i.e., a teacher model, trained on
interference-free orthogonal transmission, guides a student model via feature
affinity distillation and cross-head prediction distillation. Moreover, a
diffusion model-based refinement stage leverages generative priors to transform
initial SemNOMA outputs into high-fidelity images with enhanced perceptual
quality. Extensive experiments on CIFAR-10 and FFHQ-256 datasets demonstrate
superior performance over state-of-the-art methods, delivering satisfactory
reconstruction performance even at extremely poor channel conditions. These
results highlight the advantages in both pixel-level accuracy and perceptual
metrics, effectively mitigating interference and enabling high-quality image
recovery.

</details>


### [44] [SKR Analysis of MIMO FSO Systems with One- and Two-way CV-QKD Protocols in Hybrid Quantum Noise Environment](https://arxiv.org/abs/2509.07408)
*Sushil Kumar,Soumya P. Dash,George C. Alexandropoulos*

Main category: cs.IT

TL;DR: 本文提出了一种MIMO自由空间光通信系统，采用连续变量量子密钥分发技术，通过新颖的单向和双向协议来增强密钥传输安全性，并推导了密钥速率的数学表达式。


<details>
  <summary>Details</summary>
Motivation: 自由空间光通信系统面临大气湍流、光束扩散、指向误差等挑战，加上量子噪声和窃听者的威胁，需要开发更安全的密钥分发协议来保证通信安全。

Method: 提出了单向和双向协议，建立了FSO信道透射率的数学模型，推导了传输和接收相干态之间互信息的边界，并得出了密钥速率的新表达式。

Result: 通过MIMO技术和双向协议获得了密钥速率的显著提升，提供了渐近表达式和数值结果来验证分析框架的有效性。

Conclusion: MIMO自由空间光量子密钥分发系统结合双向协议能够有效应对大气湍流和窃听攻击，显著提高密钥传输的安全性和效率。

Abstract: A multiple-input multiple-output (MIMO) free-space optical (FSO)
communication system is considered in this paper, which supports the secret key
transmission between two legitimate users, Alice and Bob, by employing
continuous-variable quantum key distribution (CV-QKD). The wireless channels
are subjected to the effects of atmospheric turbulence that lead to beam
spreading, pointing error, and turbulence-induced fading, which, along with the
presence of hybrid quantum noise, negatively impact the secret key exchange
between Alice and Bob. Furthermore, the security of the communication system is
considered to be compromised due to the intervention of an eavesdropper, Eve,
employing a collective Gaussian attack to intercept the secret key exchange.
For this system, novel one-way and two-way protocols are proposed to enhance
the security of the transmitted keys. The transmissivity of the FSO channels is
mathematically formulated, and the bounds on the mutual information between the
transmitted and received coherent states are obtained, using which, novel
expressions for the secret key rates (SKRs) for the one-way and two-way
protocols are derived. Asymptotic expressions for the SKRs and numerical
results corroborating the analytical framework are also presented, which
demonstrate the SKR gains obtained by employing MIMO and the two-way protocol
for the FSO CV-QKD system.

</details>


### [45] [Tesla meets Helstrom: a Wireless-Powered Quantum Optical System](https://arxiv.org/abs/2509.07421)
*Ioannis Krikidis*

Main category: cs.IT

TL;DR: 这篇论文研究了一种新型的无线能量供电量子光通信系统，通过优化量子测量和能量收集时间来最大化有效速率。


<details>
  <summary>Details</summary>
Motivation: 研究无线能量供电的量子光通信系统，解决无电池量子发射器的能量供给问题，提高量子通信的效率和性能。

Method: 采用M-PSK调制方案，使用Helstrom界评估检测性能，通过半定规划技术优化量子测量和能量收集时间分数。

Result: 数值结果验证了有效速率函数的单峰性质，并展示了最优设计参数的影响效果。

Conclusion: 该优化框架能够有效提高无线能量供电量子光通信系统的性能，为实际应用提供了理论基础。

Abstract: This letter investigates a novel wireless-powered quantum optical
communication system, in which a batteryless quantum transmitter harvests
energy from a classical radio-frequency source to transmit quantum coherent
states. The transmission employs M-ary phase shift keying (M-PSK) modulation
over an optical channel impaired by thermal noise, and the fundamental
detection performance is evaluated using the Helstrom bound. An optimization
framework is proposed that jointly determines the optimal quantum measurement
and the energy-harvesting time fraction to maximize the effective rate under a
block time constraint. Analytical expressions are derived for special cases,
while semidefinite programming techniques are employed for the general M-PSK
scenario. Numerical results validate the unimodal nature of the effective rate
function and demonstrate the impact of the optimal design parameters.

</details>


### [46] [Linear time encodable binary code achieving GV bound with linear time encodable dual achieving GV bound](https://arxiv.org/abs/2509.07639)
*Martijn Brehm,Nicolas Resch*

Main category: cs.IT

TL;DR: 构建了一种高速编码的二进制线性码，该码和其对偶码都近似GV界限且编码时间为O(n)，为加密矩阵向量乘积等安全计算提供了新方案


<details>
  <summary>Details</summary>
Motivation: 为了构建时间复杂度为O(n)的高速编码码，同时保证码和其对偶码都具有近似GV界限的质量，以支持加密矩阵向量乘积等安全计算应用

Method: 受RMA码启发，使用累加步骤（坐标排列+前缀和）与离散导数步骤（坐标排列+前两坐标求和）相互交替的构造方法，这两种操作互为逆运算

Result: 成功构建了一种速度为1/2的二进制线性码，该码和其对偶码都近似GV界限且编码时间为O(n)

Conclusion: 通过创新的编码结构设计和分析方法，实现了高速编码且保持高质量的码构造，为安全计算领域提供了有力工具

Abstract: We initiate the study of what we term ``fast good codes'' with ``fast good
duals.'' Specifically, we consider the task of constructing a binary linear
code $C \leq \mathcal{F}_2^n$ such that both it and its dual $C^\perp :=\{x \in
\mathcal{F}_2^n:\forall c \in C, \langle x,c\rangle=0\}$ are asymptotically
good (in fact, have rate-distance tradeoff approaching the GV bound), and are
encodable in $O(n)$ time. While we believe such codes should find applications
more broadly, as motivation we describe how such codes can be used the secure
computation task of encrypted matrix-vector product, as studied by Behhamouda
et al (CCS 2025, to appear).
  Our main contribution is a construction of such a fast good code with fast
good dual. Our construction is inspired by the repeat multiple accumulate (RMA)
codes of Divsalar, Jin and McEliece (Allerton, 1998). To create the rate 1/2
code, after repeating each message coordinate, we perform accumulation steps --
where first a uniform coordinate permutation is applied, and afterwards the
prefix-sum mod 2 is applied -- which are alternated with discrete derivative
steps -- where again a uniform coordinate permutation is applied, and
afterwards the previous two coordinates are summed mod 2. Importantly, these
two operations are inverse of each other. In particular, the dual of the code
is very similar, with the accumulation and discrete derivative steps reversed.
  Our analysis is inspired by a prior analysis of RMA codes due to Ravazzi and
Fagnani (IEEE Trans. Info. Theory, 2009). The main idea is to bound the
input-output weight-enumerator function: the expected number of messages of a
given weight that are encoded into a codeword of a given weight. We face new
challenges in controlling the behaviour of the discrete derivative matrix
(which can significantly drop the weight of a vector), which we overcome by
careful case analysis.

</details>


### [47] [Multi-Static Target Position Estimation and System Optimization for Cell-Free mMIMO-OTFS ISAC](https://arxiv.org/abs/2509.07770)
*Yifei Fan,Shaochuan Wu,Mingjun Sun,Lin Huo,Jianchao Su,Haojie Wang*

Main category: cs.IT

TL;DR: 本文研究了基于OTFS信号的cell-free大规模MIMO架构中的多静态位置估计，提出了最大似然位置估计方案，推导了理论性能界限，并开发了联合AP操作模式选择和功率分配算法来优化系统性能。


<details>
  <summary>Details</summary>
Motivation: 研究cell-free大规模MIMO架构中集成传感与通信(ISAC)系统的位置估计性能，特别是在高移动性车联网应用中，需要解决多静态位置估计的挑战并优化系统性能。

Method: 提出最大似然位置估计方案，使用公共参考系统减少搜索空间；推导Cramér-Rao下界和位置误差界的闭式表达式；开发联合AP操作模式选择和功率分配算法；引入分解方法平衡复杂度和ISAC性能。

Result: 验证了所提算法的有效性，OTFS信号相比OFDM信号实现了近两倍的频谱效率增益，在高速移动车联网应用中展现出优越性能。

Conclusion: 研究从新颖的参数估计角度展示了CF-ISAC系统的显著优势，特别是在高移动性车联网应用中，OTFS信号相比传统OFDM信号具有更好的性能表现。

Abstract: This paper investigates multi-static position estimation in cell-free massive
multiple-input multiple-output (CF mMIMO) architectures, where orthogonal time
frequency space (OTFS) is used as an integrated sensing and communication
(ISAC) signal. A maximum likelihood position estimation scheme is proposed,
where the required search space is reduced by employing a common reference
system. Closed-form expressions for the Cram\'er-Rao lower bound and the
position error bound (PEB) in multi-static position estimation are derived,
providing quantitative evaluations of sensing performance. These theoretical
bounds are further generalized into a universal structure to support other ISAC
signals. To enhance overall system performance and adapt to dynamic network
requirements, a joint AP operation mode selection and power allocation
algorithm is developed to maximize the minimum user communication spectral
efficiency (SE) while ensuring a specified sensing PEB requirement. Moreover, a
decomposition method is introduced to achieve a better tradeoff between
complexity and ISAC performance. The results verify the effectiveness of the
proposed algorithms, demonstrating the superiority of the OTFS signal through a
nearly twofold SE gain over the orthogonal frequency division multiplexing
(OFDM) signal. These findings highlight promising advantages of the CF-ISAC
systems from a novel parameter estimation perspective, particularly in
high-mobility vehicle-to-everything applications.

</details>


### [48] [RAQ-MIMO: MIMO for Multi-Band Rydberg Atomic Quantum Receiver](https://arxiv.org/abs/2509.07832)
*Jieao Zhu,Linglong Dai*

Main category: cs.IT

TL;DR: 基于里德伯原子的多输入多输出量子接收机结构(RAQ-MIMO)，通过量子载波配置和经典预编码器/组合器的聚合优化，解决多带量子接收机的中频干扰问问题，显著提升系统的谱效率。


<details>
  <summary>Details</summary>
Motivation: 里德伯原子量子接收机(RAQRs)能同时接收多带无线电信号，但不同用户信号在光学中频域会互相干扰(IFI问题)，需要找到解决方案。

Method: 提出RAQ-MIMO架构，利用MIMO接收机的空间分集性。通过量子载波配置与多带增益的物理关系建立量子转导电导模型，形成谱效率最大化问题，并提出量子权重最小均方误差(qWMMSE)算法进行聚合优化。

Result: 模拟结果显示qWMMSE优化框架能显著提高RAQ-MIMO系统的谱效率，且RAQ-MIMO系统通过消除经典天线间的相互耦合效应，能够超越经典电子接收机基础的多用户MIMO系统。

Conclusion: RAQ-MIMO架构和qWMMSE算法有效解决了多带量子接收机的中频干扰问题，为量子无线电通信系统提供了更高的谱效率和性能。

Abstract: Rydberg atomic quantum receivers (RAQRs) are capable of receiving multi-band
radio-frequency (RF) signals simultaneously, which are expected to break Chu's
limit for classical electronic antennas. However, signals from different users
will interfere with each other in the optical intermediate frequency (IF)
domain of the multi-band quantum receiver, which is termed the IF interference
(IFI) problem. To address this problem, in this paper, we propose a multi-input
multi-output (MIMO) architecture for Rydberg atomic quantum receiver (RAQ-MIMO)
by exploiting the additional spatial diversity of MIMO receivers. Specifically,
by applying the dynamic signal model of RAQRs, we clarify the physical
relationship between the quantum local oscillator (LO) configurations and the
multi-band gains with the concept of quantum transconductance. Then, with the
quantum transconductance-based signal model, we formulate the spectral
efficiency (SE) maximization problem and further propose the quantum weighted
minimum mean square error (qWMMSE) algorithm, which jointly optimizes the
quantum LO configurations and the classical precoder/combiner matrices.
Furthermore, we test the qWMMSE algorithm within the standard space division
multiple access (SDMA) scheme and the frequency division multiple access (FDMA)
scheme. Simulation results demonstrate that the qWMMSE optimization framework
can significantly improve the SE of RAQ-MIMO systems for both multiple access
schemes, and that RAQ-MIMO systems can outperform classical electronic
receiver-based multi-user MIMO systems by eliminating the mutual coupling
effect between classical antennas.

</details>
