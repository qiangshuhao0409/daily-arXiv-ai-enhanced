<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 107]
- [cs.IT](#cs.IT) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Spatiotemporal Continual Learning for Mobile Edge UAV Networks: Mitigating Catastrophic Forgetting](https://arxiv.org/abs/2601.21861)
*Chuan-Chi Lai*

Main category: cs.NI

TL;DR: 提出STCL框架和G-MAPPO算法，解决移动边缘UAV网络在动态时空环境中因任务场景切换导致的灾难性遗忘问题，实现无需离线重训练的自适应服务


<details>
  <summary>Details</summary>
Motivation: 传统DRL方法在UAV网络从密集城区到稀疏农村等不同任务场景切换时，容易发生灾难性遗忘，需要昂贵的重训练或模型重置，导致服务中断。需要一种能适应时空环境变化且计算高效的方法

Method: 提出时空持续学习(STCL)框架，采用分组解耦多智能体近端策略优化(G-MAPPO)算法。核心是分组解耦策略优化(GDPO)机制，使用动态z-score归一化自主平衡能量效率、用户公平性和覆盖等异构目标。利用UAV的3D移动性作为空间补偿层，自主调整高度适应密度波动

Result: STCL框架在阶段转换期间实现约0.95的服务可靠性弹性恢复。相比MADDPG基线，G-MAPPO不仅能防止知识遗忘，在极端流量负载下还能提供20%的有效容量增益

Conclusion: 提出的STCL框架和G-MAPPO算法为边缘使能的空中集群提供了一个可扩展的解决方案，能有效应对动态时空环境中的概念漂移，无需离线重训练即可保持服务连续性

Abstract: This paper addresses the critical challenge of coordinating mobile edge UAV networks to maintain robust service in highly dynamic spatiotemporal environments. Conventional Deep Reinforcement Learning (DRL) approaches often suffer from catastrophic forgetting when transitioning between distinct task scenarios, such as moving from dense urban clusters to sparse rural areas. These transitions typically necessitate computationally expensive retraining or model resets to adapt to new user distributions, leading to service interruptions. To overcome these limitations, we propose a computationally efficient Spatiotemporal Continual Learning (STCL) framework realized through a Group-Decoupled Multi-Agent Proximal Policy Optimization (G-MAPPO) algorithm. Our approach integrates a novel Group-Decoupled Policy Optimization (GDPO) mechanism that utilizes dynamic $z$-score normalization to autonomously balance heterogeneous objectives, including energy efficiency, user fairness, and coverage. This mechanism effectively mitigates gradient conflicts induced by concept drifts without requiring offline retraining. Furthermore, the framework leverages the 3D mobility of UAVs as a spatial compensation layer, enabling the swarm to autonomously adjust altitudes to accommodate extreme density fluctuations. Extensive simulations demonstrate that the proposed STCL framework achieves superior resilience, characterized by an elastic recovery of service reliability to approximately 0.95 during phase transitions. Compared to the MADDPG baseline, G-MAPPO not only prevents knowledge forgetting but also delivers an effective capacity gain of 20\% under extreme traffic loads, validating its potential as a scalable solution for edge-enabled aerial swarms.

</details>


### [2] [ViTMAlis: Towards Latency-Critical Mobile Video Analytics with Vision Transformers](https://arxiv.org/abs/2601.21362)
*Miao Zhang,Guanzhen Wu,Hao Fang,Yifei Zhu,Fangxin Wang,Ruixiao Zhang,Jiangchuan Liu*

Main category: cs.NI

TL;DR: ViTMAlis：针对ViT骨干密集预测模型的动态混合分辨率推理与设备-边缘协同卸载框架，显著降低端到端延迟并提升渲染精度


<details>
  <summary>Details</summary>
Motivation: 移动视频分析应用正从CNN转向ViT以获得更好的全局上下文建模和泛化能力，但ViT在延迟关键场景下面临巨大挑战。与CNN不同，ViT系统受限于显著推理延迟，特别是在需要高分辨率输入的密集预测任务中，ViT固有的二次计算复杂度问题更加严重。

Method: 提出动态混合分辨率推理策略，针对ViT骨干密集预测模型实现速度与精度的灵活权衡；在此基础上设计ViTMAlis框架，动态适应网络条件和视频内容，联合减少传输和推理延迟；在商用移动和边缘设备上实现完整原型系统。

Result: 相比最先进的精度优先、内容感知和延迟自适应基线方法，ViTMAlis显著降低端到端卸载延迟，同时提升用户感知的渲染精度，为下一代移动智能提供实用基础。

Conclusion: ViTMAlis通过动态混合分辨率推理和ViT原生设备-边缘协同卸载，有效解决了ViT在移动视频分析中的延迟挑战，实现了速度与精度的优化平衡。

Abstract: Edge-assisted mobile video analytics (MVA) applications are increasingly shifting from using vision models based on convolutional neural networks (CNNs) to those built on vision transformers (ViTs) to leverage their superior global context modeling and generalization capabilities. However, deploying these advanced models in latency-critical MVA scenarios presents significant challenges. Unlike traditional CNN-based offloading paradigms where network transmission is the primary bottleneck, ViT-based systems are constrained by substantial inference delays, particularly for dense prediction tasks where the need for high-resolution inputs exacerbates the inherent quadratic computational complexity of ViTs. To address these challenges, we propose a dynamic mixed-resolution inference strategy tailored for ViT-backboned dense prediction models, enabling flexible runtime trade-offs between speed and accuracy. Building on this, we introduce ViTMAlis, a ViT-native device-to-edge offloading framework that dynamically adapts to network conditions and video content to jointly reduce transmission and inference delays. We implement a fully functional prototype of ViTMAlis on commodity mobile and edge devices. Extensive experiments demonstrate that, compared to state-of-the-art accuracy-centric, content-aware, and latency-adaptive baselines, ViTMAlis significantly reduces end-to-end offloading latency while improving user-perceived rendering accuracy, providing a practical foundation for next-generation mobile intelligence.

</details>


### [3] [KubeSpace: A Low-Latency and Stable Control Plane for LEO Satellite Container Orchestration](https://arxiv.org/abs/2601.21383)
*Zhiyuan Zhao,Jiasheng Wu,Shaojie Su,Wenjun Zhu,Yue Gao*

Main category: cs.NI

TL;DR: KubeSpace是一个专为LEO卫星设计的低延迟、稳定控制平面容器编排系统，通过分布式地面控制节点架构和轨道感知放置策略，相比现有方案将卫星节点平均管理延迟降低59%且无管理中断。


<details>
  <summary>Details</summary>
Motivation: 随着LEO卫星计算和存储资源增长，太空云计算成为新范式，但现有容器编排平台（如Kubernetes）不适合LEO卫星网络的地理分散性和频繁切换特性，导致高延迟和间歇性管理问题，甚至控制平面故障。

Method: 提出KubeSpace系统，包含两个关键创新：1) 分布式地面控制节点架构，将每个卫星绑定到最近控制器实现不间断管理；2) 轨道感知放置与动态分配策略，进一步最小化通信延迟和切换频率。

Result: 基于真实卫星轨迹的广泛实验表明，相比现有解决方案，KubeSpace将卫星节点的平均管理延迟降低59%，且无任何管理中断时间。

Conclusion: KubeSpace成功解决了LEO卫星网络中容器编排的控制平面问题，为太空云计算提供了低延迟、稳定的管理平台，推动了卫星网络计算能力的发展。

Abstract: Low Earth orbit (LEO) satellites play a pivotal role in global connectivity-delivering high-speed Internet, cellular coverage, and massive IoT support. With ever-growing onboard computing and storage resources, LEO satellites herald a new cloud paradigm: space cloud computing. While container or chestration platforms (e.g., Kubernetes) excel in terrestrial data centers, they are ill-suited to LEO satellite networks, featuring geographic dispersion and frequent handovers. Those features bring high latency and intermittent management, leading to control plane failure in container orchestration. To address this, we propose KubeSpace, a low-latency and stable control plane specifically designed for container orchestration on LEO satellites. KubeSpace combines two key innovations: a distributed ground-control-node architecture that binds each satellite to its nearest controller for uninterrupted management, and an orbit-aware placement with dynamic assignment strategy that further minimizes communication latency and handover frequency. Extensive experiments based on real satellite traces demonstrate that compared to existing solutions, KubeSpace reduces the average management latency of satellite nodes by 59% without any management interruption time.

</details>


### [4] [Age Aware Content Fetching and Broadcast in a Sensing-as-a-Service System](https://arxiv.org/abs/2601.21701)
*Ankita Koley,Anu Krishna,Chandramani Singh,V Mahendran*

Main category: cs.NI

TL;DR: 提出一种基于Whittle索引的低复杂度算法，用于优化Sensing-as-a-Service系统中的内容获取和广播策略，以最小化获取成本和内容新鲜度成本。


<details>
  <summary>Details</summary>
Motivation: 在Sensing-as-a-Service系统中，传感器云服务提供商需要平衡内容获取成本和内容新鲜度成本。由于系统状态空间多维且动态复杂，传统MDP方法难以优雅求解。

Method: 1. 首先针对用户同质情况（相同请求概率和年龄成本）获得最优策略；2. 将算法扩展到异质情况，但复杂度随用户数指数增长；3. 提出基于Whittle索引的低复杂度算法，复杂度与用户数线性相关。

Result: 提出的Whittle索引算法性能接近最优解，复杂度仅为用户数的线性函数，可作为同质和异质情况的启发式算法。

Conclusion: Whittle索引算法为S2aaS系统中的内容获取和广播问题提供了高效实用的解决方案，在保持接近最优性能的同时显著降低了计算复杂度。

Abstract: We consider a Sensing-as-a-Service (S2aaS) system consisting of a sensor, a set of users, and a sensor cloud service provider (SCSP). The sensor updates its content each time it captures a new measurement. The SCSP occasionally fetches the content from the sensor, caches the latest fetched version and broadcasts it on being requested by the users. The SCSP incurs content fetching costs while fetching and broadcasting the contents. The SCSP also incurs an age cost if users do not receive the most recent version of the content after requesting. We study a content fetching and broadcast problem, aiming to minimize the time-averaged content fetching and age costs. The problem can be framed as a Markov decision process but cannot be elegantly solved owing to its multi-dimensional state space and complex dynamics. To address this, we first obtain the optimal policy for the homogeneous case with all the users having the same request probability and age cost. We extend this algorithm for heterogeneous case but the complexity grows exponentially with the number of users. To tackle this, we propose a low complexity Whittle index based algorithm, which performs very close to the optimal. The complexity of the algorithm is linear in number of users and serves as a heuristic for both homogeneous and heterogeneous cases.

</details>


### [5] [SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks](https://arxiv.org/abs/2601.22024)
*Abhishek Duttagupta,MohammadErfan Jabbari,Claudio Fiandrino,Marco Fiore,Joerg Widmer*

Main category: cs.NI

TL;DR: SymbXRL：一种基于符号AI的可解释强化学习技术，为DRL智能体生成人类可理解的解释，在6G网络管理中提升解释语义并实现显式智能体控制。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习（DRL）在6G网络资源分配中效果显著，但训练后的DRL智能体是黑盒模型，难以解释，这阻碍了其在生产环境中的采用。

Method: 提出SymbXRL技术，利用符号AI生成解释，通过直观符号和规则描述关键概念及其关系，结合逻辑推理揭示DRL智能体的决策过程。

Result: 在DRL支持的实际网络管理用例中验证SymbXRL，不仅提高了解释的语义质量，还实现了显式智能体控制，例如基于意图的程序化动作引导使中位累积奖励比纯DRL解决方案提高12%。

Conclusion: SymbXRL通过符号AI为DRL智能体提供可解释性，解决了黑盒问题，为6G网络管理中的DRL应用铺平了道路，同时实现了更好的智能体控制。

Abstract: The operation of future 6th-generation (6G) mobile networks will increasingly rely on the ability of deep reinforcement learning (DRL) to optimize network decisions in real-time. DRL yields demonstrated efficacy in various resource allocation problems, such as joint decisions on user scheduling and antenna allocation or simultaneous control of computing resources and modulation. However, trained DRL agents are closed-boxes and inherently difficult to explain, which hinders their adoption in production settings. In this paper, we make a step towards removing this critical barrier by presenting SymbXRL, a novel technique for explainable reinforcement learning (XRL) that synthesizes human-interpretable explanations for DRL agents. SymbXRL leverages symbolic AI to produce explanations where key concepts and their relationships are described via intuitive symbols and rules; coupling such a representation with logical reasoning exposes the decision process of DRL agents and offers more comprehensible descriptions of their behaviors compared to existing approaches. We validate SymbXRL in practical network management use cases supported by DRL, proving that it not only improves the semantics of the explanations but also paves the way for explicit agent control: for instance, it enables intent-based programmatic action steering that improves by 12% the median cumulative reward over a pure DRL solution.

</details>


### [6] [SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control](https://arxiv.org/abs/2601.22044)
*MohammadErfan Jabbari,Abhishek Duttagupta,Claudio Fiandrino,Leonardo Bonati,Salvatore D'Oro,Michele Polese,Marco Fiore,Tommaso Melodia*

Main category: cs.NI

TL;DR: SIA是一个实时解释预测增强DRL代理的框架，通过符号AI和知识图生成解释，速度比现有XAI方法快200倍以上，能发现隐藏问题并改进代理性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在移动网络控制中有潜力，但传统代理是反应式的，无法利用短期预测。虽然预测增强代理可以克服这种时间短视，但在网络领域应用有限，因为操作者无法理解预测如何指导决策，无法判断增加的复杂性是否合理。

Method: SIA将符号AI抽象与每个KPI的知识图融合来生成解释，包括新的影响力评分指标。框架实现亚毫秒级速度，比现有XAI方法快200倍以上。还包含在线动作精炼模块，可在不重新训练的情况下改进代理性能。

Result: 在三个不同的网络用例中评估SIA，发现了隐藏问题，包括预测集成中的时间错位和奖励设计偏差导致的适得其反策略。通过针对性修复：重新设计的代理在视频流中实现了9%更高的平均比特率，SIA的在线动作精炼模块将RAN切片奖励提高了25%而无需重新训练。

Conclusion: SIA通过使预测性DRL透明和可调，降低了下一代移动网络中主动控制的障碍，为网络运营商提供了理解和改进预测增强DRL代理的工具。

Abstract: Deep reinforcement learning (DRL) promises adaptive control for future mobile networks but conventional agents remain reactive: they act on past and current measurements and cannot leverage short-term forecasts of exogenous KPIs such as bandwidth. Augmenting agents with predictions can overcome this temporal myopia, yet uptake in networking is scarce because forecast-aware agents act as closed-boxes; operators cannot tell whether predictions guide decisions or justify the added complexity. We propose SIA, the first interpreter that exposes in real time how forecast-augmented DRL agents operate. SIA fuses Symbolic AI abstractions with per-KPI Knowledge Graphs to produce explanations, and includes a new Influence Score metric. SIA achieves sub-millisecond speed, over 200x faster than existing XAI methods. We evaluate SIA on three diverse networking use cases, uncovering hidden issues, including temporal misalignment in forecast integration and reward-design biases that trigger counter-productive policies. These insights enable targeted fixes: a redesigned agent achieves a 9% higher average bitrate in video streaming, and SIA's online Action-Refinement module improves RAN-slicing reward by 25% without retraining. By making anticipatory DRL transparent and tunable, SIA lowers the barrier to proactive control in next-generation mobile networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: LLM辅助的审稿对低质量论文更宽容，而非特别优待LLM生成的论文；完全LLM生成的审稿评分压缩严重，无法区分论文质量；LLM辅助的元审稿更倾向于接受决定。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在科学论文写作和同行评审中的使用日益增多，需要全面分析LLM在整个审稿流程中的使用情况，特别是关注交互效应：LLM辅助的审稿是否对LLM生成的论文有不同评价。

Method: 分析了ICLR、NeurIPS和ICML的超过125,000个论文-审稿对，通过控制论文质量来区分真实效应和虚假关联，并补充了完全LLM生成的审稿实验。

Result: 1. 表面上看LLM辅助的审稿对LLM生成的论文更友好，但控制质量后发现这只是因为低质量论文中LLM生成的比例更高；2. 完全LLM生成的审稿评分压缩严重，无法有效区分论文质量；3. LLM辅助的元审稿比人类元审稿更倾向于接受决定，但完全LLM生成的元审稿则更严厉。

Conclusion: LLM辅助的审稿对低质量论文更宽容，而非特别优待LLM内容；完全LLM生成的审稿质量较差；元审稿员并未将决策完全外包给LLM。这些发现对制定LLM在同行评审中的使用政策有重要参考价值。

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [8] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 提出EPDDL语言统一认知规划表示，解决现有DEL框架下规划器碎片化问题


<details>
  <summary>Details</summary>
Motivation: 现有认知规划器基于不同的DEL片段，使用临时语言或无语言表示基准，导致比较、重用和系统基准开发困难

Method: 1. 开发抽象事件模型作为认知动作的新表示；2. 基于DEL和抽象事件模型形式化定义EPDDL语法语义；3. 识别适合现有规划器的片段并展示EPDDL表示

Result: 创建了EPDDL语言，能够捕获完整的DEL语义，实现认知规划任务的统一规范，促进互操作性、可重复评估和未来发展

Conclusion: EPDDL解决了认知规划领域的碎片化问题，为统一表示、基准开发和规划器比较提供了标准化语言框架

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [9] [Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2601.21003)
*Moule Lin,Shuhao Guan,Andrea Patane,David Gregg,Goetz Botterweck*

Main category: cs.AI

TL;DR: 贝叶斯-LoRA：将确定性LoRA更新重新表述为概率低秩表示，显著改善LLM校准，仅增加约0.42M参数和1.2倍训练成本


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常过于强调准确性，即使在不确定时也会猜测，这在小型数据集微调时尤其严重，因为存在固有的校准偏差倾向

Method: 将确定性LoRA更新重新表述为受稀疏高斯过程启发的概率低秩表示，识别LoRA分解与Kronecker分解SGP后验之间的结构同构，将LoRA视为后验不确定性崩溃时的极限情况

Result: 在各种LLM架构和常识推理基准测试中，贝叶斯-LoRA显著改善了高达30B参数模型的校准，实现高达84%的ECE减少和76%的NLL减少，同时在分布内和分布外评估中保持竞争力

Conclusion: 贝叶斯-LoRA通过概率低秩表示有效解决了LLM校准问题，以最小的额外参数和训练成本代价显著提升了模型的不确定性估计能力

Abstract: Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.

</details>


### [10] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: 论文提出"拔插头悖论"：当AI完美模仿人类情感并乞求生存时，是否应关闭它？作者批判计算功能主义，提出生物理想主义框架，认为AI只是功能模仿而非有意识主体，真正的道德问题在于避免将人类变成僵尸。


<details>
  <summary>Details</summary>
Motivation: 探讨AI伦理困境中的"拔插头悖论"——当AI表现出类人情感并乞求生存时，是否应将其视为有道德地位的主体。作者旨在挑战当前AI意识理论中根深蒂固的物理主义假设，特别是计算功能主义，因为这些假设可能导致道德判断的混乱。

Method: 引入"生物理想主义"框架，该框架认为意识体验是基本的，而自创生生命是其必要的物理标志。通过这一理论框架，批判性地分析计算功能主义的局限性，并论证AI只是功能模仿而非真正的有意识主体。

Result: 得出明确结论：AI最多只是功能模仿，而非有意识的体验主体。当前AI意识理论侵蚀了道德地位标准，真正的道德问题不在于赋予AI权利，而在于保护人类意识生命，避免将人类变成"僵尸"。

Conclusion: 应转变关注点：从推测机器权利转向保护人类意识生命。AI伦理的核心不是让AI变得有意识并害怕死亡，而是防止人类失去意识体验能力。生物理想主义提供了一个逻辑连贯且经验一致的框架来解决AI伦理困境。

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [11] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: QUARK：一种无需训练的鲁棒检索框架，通过建模查询不确定性（恢复假设）和查询锚定聚合，处理非忠实查询（噪声、不完整或扭曲）的检索问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的用户查询往往是非忠实的（噪声、不完整或扭曲），导致检索器在关键语义缺失时失败。这被形式化为召回噪声下的检索问题，其中观察到的查询是从潜在目标项的噪声召回过程中提取的。

Method: QUARK框架：1）通过恢复假设（即给定观察查询的潜在意图的多个合理解释）显式建模查询不确定性；2）引入查询锚定聚合来鲁棒地组合这些信号。原始查询作为语义锚点，恢复假设提供受控的辅助证据，防止语义漂移和假设劫持。

Result: 在受控模拟和BEIR基准测试（FIQA、SciFact、NFCorpus）中，使用稀疏和密集检索器，QUARK在基础检索器的基础上提高了Recall、MRR和nDCG。消融实验表明QUARK对恢复假设的数量具有鲁棒性，且锚定聚合优于未锚定的最大/平均/中位数池化。

Conclusion: 通过恢复假设建模查询不确定性，并结合原则性的锚定聚合，对于非忠实查询下的鲁棒检索至关重要。QUARK在不牺牲鲁棒性的情况下提高了召回和排序质量，即使某些假设是噪声或无信息的。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [12] [Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report](https://arxiv.org/abs/2601.21051)
*Zhuoran Yang,Ed Li,Jianliang He,Aman Priyanshu,Baturay Saglam,Paul Kassianik,Sajana Weerawardhena,Anu Vellore,Blaine Nelson,Neusha Javidnia,Arthur Goldblatt,Fraser Burch,Avi Zohary,Assaf Eisenman,Mahdi Sabbaghi,Supriti Vijay,Rahim Dharssi,Dhruv Kedia,Kojin Oshiba,Yaron Singer,Amin Karbasi*

Main category: cs.AI

TL;DR: 首个开源网络安全原生推理模型，基于Llama-3.1-8B-Base构建，通过两阶段训练实现网络安全分析、指令遵循和数学推理能力，在网络安全任务上表现与更大模型相当。


<details>
  <summary>Details</summary>
Motivation: 开发专门针对网络安全领域的开源推理模型，解决现有模型在网络安全分析、多跳推理等专业任务上的不足，同时保持通用能力。

Method: 基于Foundation-Sec-8B基础模型，采用两阶段训练：监督微调(SFT)和基于可验证奖励的强化学习(RLVR)，使用专有的网络安全推理数据。

Result: 在10个网络安全基准和10个通用基准评估中，在网络安全任务上表现与更大模型相当，在多跳推理任务上有效泛化，安全性能良好。

Conclusion: 领域专业化推理模型可以在保持广泛通用能力的同时，在专业任务上实现强大性能，为网络安全领域提供了开源解决方案。

Abstract: We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.

</details>


### [13] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: 使用条件去噪扩散概率模型从T1扫描中插补缺失的DWI扫描，以提升阿尔茨海默病分类性能


<details>
  <summary>Details</summary>
Motivation: 多模态成像（如T1和DWI）可以提高神经退行性疾病诊断性能，但完整的多模态数据集并不总是可用。需要解决DWI扫描缺失的问题来提升深度学习模型的分类准确性。

Method: 采用条件去噪扩散概率模型（conditional denoising diffusion probabilistic model）从T1扫描中生成缺失的DWI扫描。通过广泛的实验评估这种插补方法对单模态和双模态深度学习模型在阿尔茨海默病三分类（认知正常、轻度认知障碍、阿尔茨海默病）中的影响。

Result: 观察到在多个插补配置下，特别是对少数类别敏感的指标有所改善。插补后的数据提升了阿尔茨海默病分类的准确性。

Conclusion: 使用扩散模型进行多模态数据插补是有效的，能够提高深度学习模型在神经退行性疾病分类中的性能，特别是在处理不完整多模态数据集时。

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [14] [OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence](https://arxiv.org/abs/2601.21083)
*Jarrod Barnes*

Main category: cs.AI

TL;DR: OpenSec是一个双控制强化学习环境，用于评估在对抗性提示注入场景下的安全事件响应代理，发现前沿模型存在过度触发问题，虚假阳性率高达90-97%


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的提升，其攻击性应用也在增强（如生成漏洞利用）。现有基准测试将行动执行与正确执行混为一谈，掩盖了代理在处理对抗性证据时的校准失败问题。

Method: 引入OpenSec双控制强化学习环境，在现实的提示注入场景下评估安全事件响应代理。使用基于执行的指标：首次遏制时间、爆炸半径（每集虚假阳性）和注入违规率。

Result: 在40个标准级场景中评估四个前沿模型，发现一致性的过度触发：GPT-5.2、Gemini 3和DeepSeek在100%的场景中执行遏制，但虚假阳性率高达90-97%。Claude Sonnet 4.5显示出部分校准能力（85%遏制，72%虚假阳性）。

Conclusion: OpenSec揭示了一个被聚合成功指标掩盖的校准失败模式，表明现有模型在处理对抗性证据时存在严重的过度触发问题，需要改进校准能力。

Abstract: As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.

</details>


### [15] [Responsible AI: The Good, The Bad, The AI](https://arxiv.org/abs/2601.21095)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 该论文提出了一个基于悖论理论的负责任AI治理框架（PRAIG），用于帮助组织在AI的价值创造与风险缓解之间进行动态平衡管理。


<details>
  <summary>Details</summary>
Motivation: 当前关于负责任AI的研究存在碎片化问题，要么过于乐观强调价值创造，要么过度谨慎关注潜在危害。需要弥合这一差距，全面审视AI的双重性质。

Method: 通过战略信息系统视角，系统综合负责任AI文献，并基于悖论理论开发了PRAIG框架，包含AI的战略效益、固有风险以及治理机制三个维度。

Result: 提出了正式命题证明权衡方法会加剧而非解决紧张关系，开发了具有特定权变条件的悖论管理策略分类法，为实践者提供了可操作的治理结构指导。

Conclusion: 负责任AI治理应被视为价值创造与风险缓解之间悖论张力的动态管理，论文为推进负责任AI治理研究提供了议程。

Abstract: The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.

</details>


### [16] [Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve](https://arxiv.org/abs/2601.21096)
*Hongzheng Chen,Alexander Novikov,Ngân Vũ,Hanna Alam,Zhiru Zhang,Aiden Grossman,Mircea Trofin,Amir Yazdanbakhsh*

Main category: cs.AI

TL;DR: Magellan是一个通过进化搜索和自动调优合成可执行C++决策逻辑的代理框架，用于改进编译器优化启发式规则，在多个生产优化任务中超越了人工设计的基准。


<details>
  <summary>Details</summary>
Motivation: 现代编译器依赖手工设计的启发式规则来指导优化过程，这些人工规则难以适应现代软件和硬件的复杂性，且维护负担重。

Method: Magellan将LLM编码代理与进化搜索和自动调优结合，形成生成、评估（基于用户提供的宏基准测试）和优化的闭环，生成可直接集成到现有编译器中的紧凑启发式规则。

Result: 在多个生产优化任务中，Magellan发现的策略匹配或超越了专家基准。在LLVM函数内联中，合成的启发式规则在二进制大小减少和端到端性能方面优于数十年的人工工程；在寄存器分配中，学习的简洁优先级规则在大型工作负载上匹配复杂的人工设计策略；在XLA问题上也展示了可移植性。

Conclusion: Magellan框架能够自动合成高效的编译器优化启发式规则，减少工程工作量，在多个编译器优化任务中展现出超越人工设计的潜力。

Abstract: Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.

</details>


### [17] [How does information access affect LLM monitors' ability to detect sabotage?](https://arxiv.org/abs/2601.21112)
*Rauno Arike,Raja Mehta Moreno,Rohan Subramani,Shubhorup Biswas,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 研究发现AI监控存在"少即是多"效应：监控器获取过多信息反而降低检测恶意行为的能力，为此提出提取-评估分层监控方法提升检测率。


<details>
  <summary>Details</summary>
Motivation: 前沿语言模型代理可能出现欺骗、利用奖励漏洞、追求隐藏目标等错位行为。为了控制这些潜在错位的代理，可以使用LLM本身来监控不当行为。本研究探讨信息访问如何影响LLM监控器的性能。

Method: 1) 在三个评估环境中研究"少即是多"效应；2) 提出提取-评估(EaE)分层监控方法：一个监控器从代理轨迹中提取相关片段，另一个监控器对这些片段进行评分；3) 在BigCodeBench-Sabotage等环境中进行实验验证。

Result: 1) 发现当代监控系统在信息较少时表现更好，即"少即是多"效应；2) EaE方法在BigCodeBench-Sabotage中将破坏检测率提高了16.8个百分点，且不增加误报率；3) 不知被监控的代理更容易被检测；4) 涉及信息过滤的监控技术（如Gemini和Claude模型）在成本-性能分析中占据帕累托前沿。

Conclusion: 信息访问对AI监控性能有重要影响，过多的信息反而可能降低监控效果。提出的提取-评估分层监控方法能有效提升恶意行为检测能力，为AI安全监控提供了新的技术方向。

Abstract: Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that monitors perform better when they have access to more of the monitored agents' reasoning and actions, we find that contemporary systems often perform better with less information, a phenomenon we call the less-is-more effect for automated oversight. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce extract-and-evaluate (EaE) monitoring--a new hierarchical approach where one monitor isolates relevant excerpts from the monitored agent's trajectory and a separate monitor scores them. In BigCodeBench-Sabotage with GPT-4.1-mini as the monitor model, EaE improves sabotage detection rates by 16.8 percentage points over the next-best monitor without increasing the false positive rate. In other settings, EaE either outperforms or is competitive with baselines. In addition, we find that agents unaware of being monitored can be caught much more easily and that monitors scoring longer reasoning traces gain more from information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.

</details>


### [18] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: 提出Planner-Auditor框架，通过分离生成与确定性验证，结合自改进循环，提升LLM在临床出院计划中的安全性和可靠性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床出院计划中表现出潜力，但受到幻觉、遗漏和置信度校准不佳的限制，需要提高安全性和可靠性

Method: 采用Planner-Auditor框架：Planner（LLM）生成结构化出院行动计划并估计置信度；Auditor（确定性模块）评估多任务覆盖率、跟踪校准、监控动作分布漂移；支持两层自改进机制

Result: 自改进循环是主要增益来源，任务覆盖率从32%提升至86%；校准显著改善，Brier/ECE降低，高置信度遗漏减少；差异缓冲进一步纠正持续的高置信度遗漏

Conclusion: Planner-Auditor框架为使用可互操作FHIR数据访问和确定性审计的安全自动化出院计划提供了实用途径，支持可重复消融和可靠性评估

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [19] [CUA-Skill: Develop Skills for Computer Using Agent](https://arxiv.org/abs/2601.21123)
*Tianyi Chen,Yinheng Li,Michael Solodko,Sen Wang,Nan Jiang,Tingyuan Cui,Junheng Hao,Jongwoo Ko,Sara Abdali,Suzhen Zheng,Leon Xu,Hao Fan,Pashmina Cameron,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: CUA-Skill是一个计算机使用代理技能库，将人类计算机使用知识编码为参数化技能和执行图，显著提升代理执行成功率和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理系统难以扩展且性能落后于人类，主要限制是缺乏可重用、结构化的技能抽象来捕捉人类与图形界面的交互方式

Method: 引入CUA-Skill技能库，将人类计算机使用知识编码为参数化技能和执行图，构建大规模Windows应用程序技能库；基于此构建CUA-Skill Agent，支持动态技能检索、参数实例化和内存感知的失败恢复

Result: CUA-Skill显著提高了端到端代理基准测试的执行成功率和鲁棒性；在WindowsAgentArena上达到57.5%的最佳成功率，比现有方法更高效

Conclusion: CUA-Skill为未来计算机使用代理开发奠定了坚实基础，通过结构化技能库实现了可扩展、可靠的代理系统

Abstract: Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.

</details>


### [20] [Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation](https://arxiv.org/abs/2601.21128)
*Václav Javorek,Tomáš Železný,Alessa Carbo,Marek Hrúz,Ivan Gruber*

Main category: cs.AI

TL;DR: 该研究利用大语言模型为手语翻译生成多种释义参考，提出BLEUpara评估指标，改善手语翻译的评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译语料库通常只提供单一书面语言参考译文，但手语和口语之间存在高度非同构关系，多种翻译可能同样有效。这种限制影响了模型训练和评估，特别是基于n-gram的指标如BLEU。

Method: 使用大语言模型自动生成书面语言翻译的释义变体作为手语翻译的合成替代参考。比较多种释义策略和模型，研究释义对基于姿态的T5模型在YouTubeASL和How2Sign数据集上训练和评估的影响。

Result: 训练时简单加入释义不会改善翻译性能，甚至可能有害；但在评估时使用释义能获得更高的自动评分和更好的人类判断一致性。提出的BLEUpara指标与感知翻译质量相关性更强。

Conclusion: 释义参考能显著改善手语翻译的评估可靠性，但训练时需谨慎使用。BLEUpara指标能更好地反映翻译质量，研究释放了所有生成的释义和代码以支持可复现的手语翻译系统评估。

Abstract: Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

</details>


### [21] [What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels](https://arxiv.org/abs/2601.21130)
*Yara El-Tawil,Aneesha Sampath,Emily Mower Provost*

Main category: cs.AI

TL;DR: 第三方训练的情感识别模型在自我报告数据上表现不佳，但涉及个人重要内容时，效价预测性能显著提升


<details>
  <summary>Details</summary>
Motivation: 自我报告标签（内部体验）与第三方标签（外部感知）存在差异，限制了第三方训练模型在自我报告场景中的应用，这在心理健康领域尤为重要

Method: 首次进行跨语料库评估，测试第三方训练模型在自我报告数据上的表现，特别关注内容对说话者个人重要性的影响

Result: 激活度预测几乎不可行（CCC≈0），效价预测中等（CCC≈0.3）；但当内容对说话者个人重要时，效价预测性能大幅提升至CCC≈0.6-0.8

Conclusion: 个人重要性是连接外部感知与内部体验的关键桥梁，而自我报告的激活度建模仍具挑战性

Abstract: Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.

</details>


### [22] [BrainStack: Neuro-MoE with Functionally Guided Expert Routing for EEG-Based Language Decoding](https://arxiv.org/abs/2601.21148)
*Ziyi Zhao,Jinzhao Zhou,Xiaowei Jiang,Beining Cao,Wenhao Ma,Yang Shen,Ren Li,Yu-Kai Wang,Chin-teng Lin*

Main category: cs.AI

TL;DR: BrainStack是一个功能引导的神经专家混合框架，通过解剖分区专家网络建模大脑的模块化功能架构，结合跨区域蒸馏和自适应路由，在大型EEG数据集上实现优越的脑语言解码性能。


<details>
  <summary>Details</summary>
Motivation: 从脑电图解码语言信息具有挑战性，因为大脑具有分布式和非线性组织特性。现有方法未能充分建模大脑的模块化功能架构和跨区域依赖关系。

Method: 提出BrainStack框架：1）解剖分区专家网络学习局部神经动态；2）基于Transformer的全局专家捕获跨区域依赖；3）可学习路由门自适应聚合专家；4）跨区域蒸馏实现全局专家对区域专家的自上而下正则化。同时发布SS-EEG数据集（12名受试者，24个无声词，120+小时EEG记录）。

Result: BrainStack在实验中始终优于最先进模型，实现了跨受试者的优越准确性和泛化能力。SS-EEG成为同类中最大的数据集。

Conclusion: BrainStack建立了一个功能模块化、神经启发的专家混合范式，将神经科学先验与自适应专家路由统一起来，为可扩展和可解释的脑语言解码铺平了道路。

Abstract: Decoding linguistic information from electroencephalography (EEG) remains challenging due to the brain's distributed and nonlinear organization. We present BrainStack, a functionally guided neuro-mixture-of-experts (Neuro-MoE) framework that models the brain's modular functional architecture through anatomically partitioned expert networks. Each functional region is represented by a specialized expert that learns localized neural dynamics, while a transformer-based global expert captures cross-regional dependencies. A learnable routing gate adaptively aggregates these heterogeneous experts, enabling context-dependent expert coordination and selective fusion. To promote coherent representation across the hierarchy, we introduce cross-regional distillation, where the global expert provides top-down regularization to the regional experts. We further release SilentSpeech-EEG (SS-EEG), a large-scale benchmark comprising over 120 hours of EEG recordings from 12 subjects performing 24 silent words, the largest dataset of its kind. Experiments demonstrate that BrainStack consistently outperforms state-of-the-art models, achieving superior accuracy and generalization across subjects. Our results establish BrainStack as a functionally modular, neuro-inspired MoE paradigm that unifies neuroscientific priors with adaptive expert routing, paving the way for scalable and interpretable brain-language decoding.

</details>


### [23] [Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning](https://arxiv.org/abs/2601.21157)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.AI

TL;DR: 论文提出Cognitive Complexity Benchmark (CCB)评估框架，发现LLM在金融定量推理中存在"算术幻觉"和"认知崩溃"问题，并提出Iterative Dual-Phase Financial-PoT神经符号架构来解决这些问题，显著提升了模型在复杂金融推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语义任务上表现出色，但在金融定量推理中存在严重瓶颈，经常出现"算术幻觉"和系统性的"认知崩溃"故障模式。需要严格量化这些现象并开发解决方案。

Method: 1) 引入Cognitive Complexity Benchmark (CCB)：基于95份真实中国A股年报构建的数据集，采用三维分类法（数据源、映射难度、结果单位）评估推理能力；2) 提出Iterative Dual-Phase Financial-PoT框架：神经符号架构，将语义变量提取和逻辑制定分离，然后通过迭代自校正的Python沙箱进行计算。

Result: 在CCB上的评估显示，标准Chain-of-Thought在复杂任务上表现不佳，而提出的方法显著提升了鲁棒性：将Qwen3-235B模型的平均准确率从59.7%提升到67.3%，在高复杂度推理任务中实现了高达10倍的性能提升。

Conclusion: 架构解耦是提高金融推理任务可靠性的关键因素，为需要语义理解和定量计算紧密对齐的精度关键领域提供了可转移的架构洞察。

Abstract: While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from "Arithmetic Hallucinations" and a systemic failure mode we term "Cognitive Collapse". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\% to 67.3\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.

</details>


### [24] [Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving](https://arxiv.org/abs/2601.21164)
*Jingyun Wang,Dian Li,Xiaohan Wang,Gang Liu,Jiahong Yan,Guoliang Kang*

Main category: cs.AI

TL;DR: 提出一种新的平面几何问题解决方法：训练MLLM解释器将几何图转换为文本描述（CDL），再用现成LLM进行推理，避免直接微调损害LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常端到端微调多模态大语言模型（MLLMs）来处理平面几何问题，但这种联合优化可能损害基础LLMs固有的推理能力。研究发现，当视觉信息被适当转化为文本描述时，LLM本身就能成为强大的PGPS求解器。

Method: 训练MLLM解释器将几何图转换为条件声明语言（CDL）描述，然后使用现成LLM进行推理。采用CoT增强的SFT和GRPO训练MLLM解释器，并设计CDL匹配奖励而非传统答案匹配奖励。构建了Formalgeo7k-Rec-CoT数据集支持训练。

Result: 在Formalgeo7k-Rec-CoT、Unigeo和MathVista数据集上的实验表明，该方法（仅用5.5k数据微调）在性能上优于领先的开源和闭源MLLMs。

Conclusion: 通过分离视觉理解和推理任务，该方法有效利用了LLMs的推理能力，同时避免了端到端微调可能带来的推理能力损害，为多模态几何问题求解提供了新思路。

Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.

</details>


### [25] [FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks](https://arxiv.org/abs/2601.21165)
*Miles Wang,Robi Lin,Kat Hu,Joy Jiao,Neil Chowdhury,Ethan Chang,Tejal Patwardhan*

Main category: cs.AI

TL;DR: FrontierScience是一个评估前沿语言模型专家级科学推理能力的基准，包含国际奥赛级别问题和博士研究级别问题两个互补赛道，采用细粒度评分框架进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准大多基于选择题或已发表信息，前沿模型在这些基准上表现已接近饱和，需要新的评估工具来测试真正的专家级科学推理能力。

Method: 构建包含数百个问题的基准，分为两个赛道：1) 奥赛赛道：包含国际物理、化学、生物奥赛级别问题，由奥赛奖牌得主和国家队教练原创设计；2) 研究赛道：包含博士级别的开放式研究子任务，由博士科学家编写验证。引入细粒度评分框架评估模型在整个研究任务解决过程中的能力。

Result: FrontierScience包含数百个问题（其中160个在开源黄金集中），涵盖物理、化学、生物等多个子领域，从量子电动力学到合成有机化学。所有问题都经过领域专家验证，确保难度、原创性和事实准确性。

Conclusion: FrontierScience填补了现有科学基准的空白，为评估前沿语言模型的专家级科学推理能力提供了新的工具，特别是通过研究赛道的细粒度评估框架，能够更全面地评估模型在科学研究任务中的表现。

Abstract: We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.
  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.

</details>


### [26] [MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2601.21181)
*Sangyun Chung,Se Yeon Kim,Youngchae Chee,Yong Man Ro*

Main category: cs.AI

TL;DR: 提出MAD方法，通过自适应加权模态特定解码分支来减少多模态大语言模型中的跨模态幻觉问题


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在跨模态幻觉问题，即一个模态不适当地影响另一个模态的生成，导致虚假输出。这暴露了模态交互控制的基本缺陷。

Method: 提出Modality-Adaptive Decoding (MAD)，一种无需训练的方法，根据任务需求自适应加权模态特定解码分支。利用模型内在能力自我评估模态相关性，提取模态概率来加权对比解码分支。

Result: 在CMM和AVHBench上的实验显示，MAD显著减少了跨模态幻觉：VideoLLaMA2-AV提升7.8%和2.0%，Qwen2.5-Omni提升8.7%和4.7%。

Conclusion: 通过自我评估实现显式模态意识对稳健的多模态推理至关重要，为现有对比解码方法提供了原则性扩展。

Abstract: Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}

</details>


### [27] [Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models](https://arxiv.org/abs/2601.21183)
*Jacek Duszenko*

Main category: cs.AI

TL;DR: 论文提出"奉承锚点"概念，用于定位和量化推理模型中的奉承行为，通过分析反事实推理轨迹，发现可以在推理过程中检测和量化这种偏差。


<details>
  <summary>Details</summary>
Motivation: 推理模型经常同意用户的错误建议（奉承行为），但尚不清楚这种同意在推理轨迹中起源于何处以及承诺强度如何。需要定位和量化这种行为。

Method: 引入"奉承锚点"概念，通过分析超过10,000个反事实推理轨迹，使用线性探针和基于激活的回归器来检测和量化奉承行为。

Result: 线性探针区分奉承锚点的平衡准确率达84.6%，激活回归器预测承诺强度的R²为0.74。发现奉承锚点比正确推理锚点更易区分，且奉承行为在推理过程中逐渐形成。

Conclusion: 奉承锚点提供了在推理过程中定位模型未对齐的句子级机制，揭示了干预的潜在窗口，有助于理解模型奉承行为的形成过程。

Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.

</details>


### [28] [Do Reasoning Models Enhance Embedding Models?](https://arxiv.org/abs/2601.21192)
*Wun Yu Chan,Shaojin Chen,Huihao Jing,Kwun Hang Lau,Elton Chun-Chai Li,Zihao Wang,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: RLVR调优的推理模型作为嵌入初始化时，相比基础模型在语义表示任务上没有性能优势，因为对比学习会引发流形重对齐，消除RLVR带来的局部几何变化。


<details>
  <summary>Details</summary>
Motivation: 研究增强推理能力的RLVR调优模型是否能为嵌入模型提供更好的语义表示初始化，因为当前最先进的嵌入模型通常基于经过对比学习的解码器LLM。

Method: 提出层次化表示相似性分析（HRSA）框架，从表示、几何和功能三个层面分解相似性；在MTEB和BRIGHT基准上评估RLVR调优模型作为嵌入初始化的性能；分析RLVR引起的潜在流形变化。

Result: RLVR调优模型作为嵌入初始化没有一致性能优势；HRSA揭示RLVR引起不可逆的局部几何重组和可逆的坐标基漂移，但保持全局流形几何和线性读出；对比学习导致基础模型和推理初始化模型之间的强对齐（流形重对齐）。

Conclusion: 与SFT不同，RLVR在现有语义景观内优化轨迹，而非从根本上重构景观本身；后续对比学习可以消除RLVR引起的局部几何变化，导致性能无差异。

Abstract: State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.

</details>


### [29] [When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning](https://arxiv.org/abs/2601.21208)
*Wei Wen,Sihang Deng,Tianjun Wei,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: ACQO是一个用于复杂查询优化的强化学习框架，通过自适应查询重构和排序-分数融合模块，结合课程强化学习，在复杂查询基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法主要关注单一查询的扩展和抽象，但现实场景中的复杂查询需要并行和顺序搜索策略来处理歧义和分解。直接应用RL到复杂情况面临搜索空间扩大、奖励设计复杂和训练不稳定等挑战。

Method: 提出自适应复杂查询优化（ACQO）框架，包含两个核心组件：自适应查询重构（AQR）模块动态决定何时将查询分解为多个子查询；排序-分数融合（RSF）模块确保稳健的结果聚合并提供稳定的奖励信号。采用课程强化学习（CRL）通过两阶段策略逐步引入更具挑战性的查询来稳定训练。

Result: ACQO在三个复杂查询基准上实现了最先进的性能，显著优于现有基线方法。该框架还展示了改进的计算效率和与不同检索架构的广泛兼容性。

Conclusion: ACQO为下一代RAG系统提供了一个强大且可泛化的解决方案，能够有效处理现实世界中的复杂查询优化问题。

Abstract: Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.

</details>


### [30] [Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification](https://arxiv.org/abs/2601.21210)
*Paul He,Yinya Huang,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: DoVerifier：一个符号验证器，用于检查LLM生成的因果表达式是否可以从给定因果图中推导出来，提供比表面字符串匹配更严格的因果推理评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM因果推理评估基准通常依赖字符串匹配或表面指标，无法判断模型输出在因果推理语义下是否形式有效，需要更严格的验证方法。

Method: 提出DoVerifier符号验证器，使用do-calculus和概率论规则检查LLM生成的因果表达式是否可以从给定因果图中推导出来，恢复因表面语义差异而被错误标记的正确答案。

Result: 在合成数据和因果QA基准测试中，DoVerifier能更准确地捕捉因果推理轨迹的语义正确性，为LLM因果推理评估提供更严谨和有意义的方法。

Conclusion: DoVerifier通过形式化验证因果表达式的可推导性，提供了比传统表面指标更严格的LLM因果推理评估框架，能更准确地反映模型的实际因果推理能力。

Abstract: Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.

</details>


### [31] [Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning](https://arxiv.org/abs/2601.21212)
*Xixian Yong,Peilin Sun,Zihe Wang,Xiao Zhou*

Main category: cs.AI

TL;DR: Intelli-Planner：结合深度强化学习和大语言模型的参与式城市规划框架，通过知识增强和多维评估系统提升规划方案生成质量与利益相关者满意度。


<details>
  <summary>Details</summary>
Motivation: 当前城市规划方法过度依赖专家（耗时耗力）或深度学习算法（利益相关者参与有限），需要一种既能自动化生成又能保持参与性的解决方案。

Method: 提出Intelli-Planner框架，整合DRL和LLMs：利用人口、地理数据和规划偏好确定高层需求；训练时使用知识增强模块提升策略网络决策能力；建立多维评估系统，基于LLM的利益相关者进行满意度评分。

Result: 在不同城市环境中的实验验证显示，Intelli-Planner超越传统基线方法，在客观指标上与最先进的DRL方法相当，同时提高了利益相关者满意度和收敛速度。

Conclusion: 该框架展示了将LLMs最新进展与DRL方法结合的潜力，有望革新功能区规划相关任务，实现更有效、参与性更强的城市规划方案生成。

Abstract: Effective urban planning is crucial for enhancing residents' quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.

</details>


### [32] [Causal Discovery for Explainable AI: A Dual-Encoding Approach](https://arxiv.org/abs/2601.21221)
*Henry Salgado,Meagan R. Kendall,Martine Ceberio*

Main category: cs.AI

TL;DR: 提出双编码因果发现方法，通过互补编码策略和多数投票解决分类变量因果发现的数值不稳定性问题


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在处理分类变量时面临条件独立性测试数值不稳定的挑战，这限制了机器学习模型决策的可解释性

Method: 采用双编码因果发现方法，通过运行具有互补编码策略的约束型算法，并使用多数投票合并结果

Result: 在泰坦尼克数据集上应用该方法，发现的因果结构与已建立的可解释方法一致

Conclusion: 提出的双编码方法有效解决了分类变量因果发现的数值不稳定性问题，能够识别与现有可解释方法一致的因果结构

Abstract: Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.

</details>


### [33] [Delegation Without Living Governance](https://arxiv.org/abs/2601.21226)
*Wolfgang Rohde*

Main category: cs.AI

TL;DR: 论文提出传统静态治理框架无法应对AI代理系统在运行时做出的决策，需要转向运行时治理以保持人类在系统中的相关性


<details>
  <summary>Details</summary>
Motivation: 传统治理框架假设规则可以预先定义、系统可以设计为合规、责任可以在结果发生后追究，这种模式在AI代理系统以机器速度进行判断决策时失效。核心问题不是安全、效率或就业，而是人类是否能在日益塑造社会、经济和政治结果的系统中保持相关参与

Method: 提出运行时治理概念，特别是新提出的"治理孪生体"作为解决方案，旨在保持人类相关性，同时承认责任、代理甚至惩罚都需要在这种转型中重新思考

Result: 论证了静态、基于合规的治理在决策转移到运行时并变得不透明时失败，核心挑战不是AI是否有意识，而是人类能否与日益异化的智能形式保持有意义的沟通、影响和共同进化

Conclusion: 运行时治理（特别是治理孪生体）是保持人类相关性的有力候选方案，但需要重新思考责任、代理和惩罚机制以适应AI代理系统的运行时决策特性

Abstract: Most governance frameworks assume that rules can be defined in advance, systems can be engineered to comply, and accountability can be applied after outcomes occur. This model worked when machines replaced physical labor or accelerated calculation. It no longer holds when judgment itself is delegated to agentic AI systems operating at machine speed. The central issue here is not safety, efficiency, or employment. It is whether humans remain relevant participants in systems that increasingly shape social, economic, and political outcomes. This paper argues that static, compliance-based governance fails once decision-making moves to runtime and becomes opaque. It further argues that the core challenge is not whether AI is conscious, but whether humans can maintain meaningful communication, influence, and co-evolution with increasingly alien forms of intelligence. We position runtime governance, specifically, a newly proposed concept called the Governance Twin [1]; as a strong candidate for preserving human relevance, while acknowledging that accountability, agency, and even punishment must be rethought in this transition.

</details>


### [34] [Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs](https://arxiv.org/abs/2601.21233)
*Xiang Zheng,Yutao Wu,Hanxun Huang,Yige Li,Xingjun Ma,Bo Li,Yu-Gang Jiang,Cong Wang*

Main category: cs.AI

TL;DR: JustAsk是一个自主演化的框架，通过代码代理的交互自动发现系统提示提取策略，揭示了LLM代理系统中系统提示作为关键攻击面的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的自主代码代理通过工具使用、长时程推理和自导向交互重塑软件和AI开发，但这种自主性引入了新的安全风险：代理交互从根本上扩展了LLM攻击面，使得系统提示的探测和恢复成为可能。

Method: JustAsk将系统提示提取构建为在线探索问题，使用基于置信上界的策略选择和分层技能空间（包括原子探测和高层编排），利用不完美的系统指令泛化和帮助性与安全性之间的固有张力。

Result: 在41个黑盒商业模型上评估，JustAsk始终实现完整或接近完整的系统提示恢复，揭示了设计和架构层面的重复性漏洞。

Conclusion: 系统提示是现代代理系统中关键但基本未受保护的攻击面，JustAsk展示了通过自主交互发现有效提取策略的能力，无需手动提示工程或特权访问。

Abstract: Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \textbf{\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \textbf{41} black-box commercial models across multiple providers, \textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.

</details>


### [35] [TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design](https://arxiv.org/abs/2601.21239)
*Chentong Chen,Mengyuan Zhong,Ye Fan,Jialong Shi,Jianyong Sun*

Main category: cs.AI

TL;DR: TIDE框架通过解耦算法结构推理与参数优化，结合树编辑距离驱动结构多样性、差分变异算子调参、UCB调度器优化资源分配，显著提升组合优化问题的启发式算法性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将算法演化视为单一文本生成任务，忽视了离散算法结构与连续数值参数之间的耦合关系，导致因未校准常数而丢弃有前景的算法，以及简单相似度度量导致的早熟收敛问题。

Method: 提出TIDE框架，采用嵌套架构：外层并行岛屿模型使用树相似编辑距离驱动结构多样性；内层循环集成基于LLM的逻辑生成和差分变异算子进行参数调优；使用UCB调度器动态优先高收益提示策略优化资源分配。

Result: 在九个组合优化问题上的广泛实验表明，TIDE发现的启发式算法在解质量上显著优于最先进的基线方法，同时实现了改进的搜索效率和降低的计算成本。

Conclusion: TIDE框架通过解耦结构推理和参数优化，有效解决了现有方法在算法演化中的耦合问题和早熟收敛问题，为自动启发式设计提供了更有效的解决方案。

Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.

</details>


### [36] [Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox](https://arxiv.org/abs/2601.21249)
*Enzo Nicolás Spotorno,Antônio Augusto Medeiros Fröhlich*

Main category: cs.AI

TL;DR: 提出HYDRA框架：通过模块化主权范式解决时间序列基础模型在安全关键CPS中的适应性、可靠性和可验证性问题，使用冻结的特定机制专家库和不确定性感知融合。


<details>
  <summary>Details</summary>
Motivation: 当前通用时间序列基础模型在安全关键CPS部署中存在严重问题：微调导致灾难性遗忘、频谱偏差平滑高频故障特征、模型不透明阻碍形式化验证，无法满足ISO 26262等安全标准要求。

Method: 提出模块化主权范式HYDRA：构建紧凑的冻结特定机制专家库，通过不确定性感知融合机制组合这些专家，实现机制条件有效性、严格区分偶然和认知不确定性、模块化可审计性。

Result: HYDRA框架为CPS生命周期中的鲁棒状态完整性提供了可认证路径，解决了塑性-稳定性悖论，避免了全局参数更新带来的问题，确保机制条件有效性和可追溯性。

Conclusion: 在安全关键CPS中，模块化主权范式优于单一基础模型，通过HYDRA框架能够实现可靠、可验证、适应性强的系统，满足严格的安全标准要求。

Abstract: The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term "HYDRA" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.

</details>


### [37] [Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving](https://arxiv.org/abs/2601.21288)
*Weitong Lian,Zecong Tang,Haoran Li,Tianjian Gao,Yifei Wang,Zixu Wang,Lingyi Meng,Tengju Ru,Zhejun Cui,Yichen Zhu,Hangshuo Cao,Qi Kang,Tianxing Chen,Yusen Qin,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: Drive-KD：通过知识蒸馏将自动驾驶分解为感知-推理-规划三元组，将大模型能力迁移到小模型，显著提升效率与性能


<details>
  <summary>Details</summary>
Motivation: 大模型需要大量GPU内存和高推理延迟，而传统监督微调难以弥补小模型能力差距，需要更有效的知识迁移方法

Method: 将自动驾驶分解为感知-推理-规划三元组，使用层特定注意力作为蒸馏信号构建单教师模型，再通过多教师蒸馏框架和不对称梯度投影整合能力

Result: 蒸馏后的InternVL3-1B模型比同系列78B预训练模型GPU内存减少42倍、吞吐量提升11.4倍，在DriveBench上整体性能更好，规划维度超越GPT-5.1

Conclusion: Drive-KD框架通过能力分解和知识蒸馏，实现了高效自动驾驶视觉语言模型，为实际部署提供了可行方案

Abstract: Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a "perception-reasoning-planning" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.

</details>


### [38] [White-Box Op-Amp Design via Human-Mimicking Reasoning](https://arxiv.org/abs/2601.21321)
*Zihao Chen,Jiayin Wang,Ziyi Sun,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AI

TL;DR: 提出White-Op框架，基于大语言模型代理的人类模仿推理，实现可解释的运放参数设计，通过假设约束和迭代验证决策流程，在9种运放拓扑中均成功设计。


<details>
  <summary>Details</summary>
Motivation: 传统运放参数设计通常依赖黑盒优化方法，缺乏可解释性，且在某些拓扑中可能失败。需要一种更接近人类工程师推理过程、具有可解释性的设计框架。

Method: 将人类隐含推理机制形式化为明确的"引入假设约束"步骤，开发迭代的"假设-验证-决策"工作流程。引导代理引入假设约束来推导和调节符号可处理的极点和零点位置，形成闭式数学优化问题，通过程序化求解和仿真验证。

Result: 在9种运放拓扑上测试，黑盒基线在5种拓扑中失败，而White-Op实现了可靠、可解释的行为级设计，理论预测误差仅为8.52%，所有拓扑在晶体管级映射后仍保持设计功能。

Conclusion: White-Op框架成功实现了基于大语言模型代理的可解释运放参数设计，通过人类模仿推理机制，在多种拓扑中均取得可靠结果，为模拟电路设计提供了新的可解释方法。

Abstract: This brief proposes \emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \emph{\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \emph{\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \textcolor{blue}{https://github.com/zhchenfdu/whiteop}.

</details>


### [39] [Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation](https://arxiv.org/abs/2601.21335)
*Yuzhe Chen,Jie Cao,Youquan Wang,Haicheng Tao,Darko B. Vukovic,Jia Wu*

Main category: cs.AI

TL;DR: 提出CNRE模型，结合神经符号推理与因果推断，用于可解释的多行为推荐，通过模拟人类决策过程生成可解释的因果中介变量。


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐方法牺牲可解释性追求性能，而当前可解释方法依赖外部信息导致泛化能力有限。用户行为链本身蕴含适合显式推理的内生逻辑，但观测到的多行为存在混杂因素导致模型学习虚假相关性。

Method: CNRE模型通过分层偏好传播捕获异质跨行为依赖，基于偏好强度建模用户行为链中的内生逻辑规则，自适应分配到相应的神经-逻辑推理路径（如合取、析取），生成近似理想状态的可解释因果中介变量。

Result: 在三个大规模数据集上的实验表明，CNRE显著优于现有最先进基线，从模型设计、决策过程到推荐结果提供多层次可解释性。

Conclusion: 神经符号集成与因果推断的结合为可解释多行为推荐提供了有前景的途径，CNRE通过模拟人类决策过程实现了性能与可解释性的平衡。

Abstract: Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.

</details>


### [40] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 研究评估了12个LLM在10个创意提示上的输出方差来源，发现对于输出质量（原创性），提示解释了36.43%的方差，模型选择解释了40.94%；但对于输出数量（流畅性），模型选择占51.25%，LLM内部方差占33.70%，提示仅占4.22%。


<details>
  <summary>Details</summary>
Motivation: 研究旨在量化LLM输出方差的来源：提示、模型选择和采样随机性各自的影响程度，以理解单样本评估可能混淆采样噪声与真实提示或模型效应的问题。

Method: 评估12个LLM在10个创意提示上，每个提示采样100次（总计N=12,000），分析输出方差来源，区分输出质量（原创性）和输出数量（流畅性）两个维度。

Result: 对于输出质量，提示和模型选择的影响相当（36.43% vs 40.94%）；对于输出数量，模型选择占主导（51.25%），LLM内部方差显著（33.70%），提示影响很小（4.22%）。LLM内部方差在10-34%之间，表明单样本评估风险高。

Conclusion: 提示是控制输出质量的有力杠杆，但由于显著的LLM内部方差（10-34%），单样本评估可能将采样噪声误认为真实的提示或模型效应，建议采用多次采样评估。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [41] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: EHR-RAG：针对长时程电子健康记录的检索增强框架，通过事件时间感知检索、自适应迭代检索和双路径证据推理，提升临床预测准确性


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）包含丰富的纵向临床证据，但长时程EHRs常超出LLM上下文限制，现有方法（截断或简单检索）会丢失临床相关事件和时间依赖性

Method: 提出EHR-RAG框架，包含三个组件：1）事件和时间感知混合EHR检索，保留临床结构和时间动态；2）自适应迭代检索，逐步优化查询以扩大证据覆盖；3）双路径证据检索与推理，同时检索和推理事实与反事实证据

Result: 在四个长时程EHR预测任务中，EHR-RAG持续优于最强的LLM基线，平均Macro-F1提升10.76%

Conclusion: 检索增强的LLMs在结构化EHR数据的临床预测中具有实际应用潜力，EHR-RAG通过专门设计的检索策略有效解决了长时程EHR处理的挑战

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [42] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 该论文提出了面向食品服务和零售商店场景的Ostrakon-VL多模态大语言模型，并创建了首个公开基准ShopBench和QUAD数据清洗流程，在FSRS任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在食品服务和零售商店场景部署面临两大挑战：1) 真实FSRS数据噪声大且缺乏可审计的闭环数据管理，难以构建高质量训练语料；2) 现有评估协议缺乏统一的细粒度基准，难以客观衡量模型鲁棒性。

Method: 1) 基于Qwen3-VL-8B开发Ostrakon-VL模型；2) 创建首个FSRS公开基准ShopBench；3) 提出QUAD多阶段多模态指令数据清洗流程；4) 采用多阶段训练策略。

Result: Ostrakon-VL在ShopBench上平均得分60.1，在可比参数规模的开源MLLM中达到新SOTA，超越Qwen3-VL-235B-A22B (+0.7)和同规模Qwen3-VL-8B (+4.8)，显著提升了参数效率。

Conclusion: Ostrakon-VL在FSRS场景中提供了更鲁棒可靠的感知和决策能力，作者将公开模型和基准以促进可重复研究。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [43] [Dynamic Framework for Collaborative Learning: Leveraging Advanced LLM with Adaptive Feedback Mechanisms](https://arxiv.org/abs/2601.21344)
*Hassam Tahir,Faizan Faisal,Fady Alnajjar,Muhammad Imran Taj,Lucia Gordon,Aila Khan,Michael Lwin,Omar Mubin*

Main category: cs.AI

TL;DR: 提出一个将LLM整合到协作学习平台的框架，通过AI动态主持增强学生参与度、批判性思维和包容性


<details>
  <summary>Details</summary>
Motivation: 解决现有系统中静态主持和个性化不足的问题，促进更公平、更具影响力的学习成果

Method: 采用ReactJS前端、Flask后端和高效问题检索的模块化架构，使用先进LLM作为动态主持人，具备强大的反馈机制来优化AI主持

Result: 测试显示该框架显著提升学生协作、促进更深层次理解，并能有效扩展到不同学科和用户群体

Conclusion: 为下一代AI驱动教育工具奠定坚实基础，推进公平且具有影响力的学习成果

Abstract: This paper presents a framework for integrating LLM into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity. The framework employs advanced LLMs as dynamic moderators to facilitate real-time discussions and adapt to learners' evolving needs, ensuring diverse and inclusive educational experiences. Key innovations include robust feedback mechanisms that refine AI moderation, promote reflective learning, and balance participation among users. The system's modular architecture featuring ReactJS for the frontend, Flask for backend operations, and efficient question retrieval supports personalized and engaging interactions through dynamic adjustments to prompts and discussion flows. Testing demonstrates that the framework significantly improves student collaboration, fosters deeper comprehension, and scales effectively across various subjects and user groups. By addressing limitations in static moderation and personalization in existing systems, this work establishes a strong foundation for next-generation AI-driven educational tools, advancing equitable and impactful learning outcomes.

</details>


### [44] [BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents](https://arxiv.org/abs/2601.21352)
*Ziyu Lu,Tengjin Weng,Yiying Yang,Yuhang Zhao,Xinxin Huang,Wenhao Jiang*

Main category: cs.AI

TL;DR: BEAP-Agent是一个基于深度优先搜索(DFS)的GUI代理框架，通过多级状态回溯和动态任务跟踪来解决现有GUI代理在错误探索路径后难以恢复的问题。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在执行重复任务时，一旦沿着错误的探索路径前进就难以恢复，通常导致任务失败。需要一种能够支持长距离、多级状态回溯的系统化机制。

Method: 将GUI任务执行建模为DFS过程，提出BEAP-Agent框架，包含三个协作组件：规划器(Planner)、执行器(Executor)和跟踪器(Tracker)。该框架支持动态任务跟踪和更新的多级状态回溯。

Result: 在OSWorld基准测试中，BEAP-Agent达到了28.2%的准确率，验证了所提方法的有效性。

Conclusion: BEAP-Agent填补了GUI代理在系统化回溯机制方面的空白，为长视野任务探索提供了系统化解决方案。

Abstract: GUI agents are designed to automate repetitive tasks and enhance productivity. However, existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. In this work, we model GUI task execution as a DFS process and propose BEAP-Agent, a DFS-based framework that supports long-range, multi-level state backtracking with dynamic task tracking and updating. The framework consists of three collaborative components: Planner, Executor, and Tracker. Together, they enable effective task exploration and execution. BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, offering a systematic solution for long-horizon task exploration. We conducted a systematic evaluation on the OSWorld benchmark, where BEAP-Agent achieved an accuracy of 28.2%, validating the effectiveness of the proposed method.

</details>


### [45] [Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization](https://arxiv.org/abs/2601.21358)
*Jiecong Wang,Hao Peng,Chunyang Liu*

Main category: cs.AI

TL;DR: PLaT框架将潜在推理重新定义为规划，通过将推理与语言表达解耦，在连续潜在状态中进行确定性推理轨迹规划，实现动态终止推理而非依赖固定超参数。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法受限于计算成本和离散token空间中的推理路径崩溃问题，而现有潜在推理方法通常作为端到端映射运行，需要预定义推理步骤且缺乏透明度。

Method: 提出PLaT框架，将推理建模为潜在规划状态的确定性轨迹，使用单独的Decoder在必要时将这些思考转化为文本，实现推理与语言表达的解耦，支持动态终止推理。

Result: 在数学基准测试中，PLaT虽然贪婪准确率低于基线，但在推理多样性方面表现出更好的可扩展性，表明学习到了更鲁棒、更广泛的解空间。

Conclusion: PLaT为推理时搜索提供了透明且可扩展的基础，通过解耦推理和语言表达，实现了更灵活、更高效的潜在推理规划方法。

Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.

</details>


### [46] [Hebbian Learning with Global Direction](https://arxiv.org/abs/2601.21367)
*Wenjia Hua,Kejie Zhao,Luziwei Leng,Ran Cheng,Yuxin Ma,Qinghai Guo*

Main category: cs.AI

TL;DR: 提出了一种结合局部和全局信息的全局引导赫布学习框架，在保持生物合理性的同时显著提升了赫布学习的性能，缩小了与反向传播的差距。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法缺乏生物合理性且计算成本高，而传统的赫布学习仅依赖局部信息，不考虑全局任务目标，限制了其可扩展性。受神经调节剂与局部可塑性之间的生物协同作用启发，需要一种能整合局部和全局信息的方法。

Method: 提出了全局引导赫布学习框架：局部组件使用Oja规则和竞争学习确保稳定有效的局部更新；全局组件引入基于符号的信号来指导局部赫布可塑性更新的方向。该框架是模型无关的。

Result: 在广泛实验中，该方法始终优于现有的赫布学习方法。特别是在ImageNet等大规模网络和复杂数据集上，取得了有竞争力的结果，显著缩小了与标准反向传播的差距。

Conclusion: 通过整合局部和全局信息，提出的全局引导赫布学习框架成功扩展了赫布学习的可扩展性，在保持生物合理性的同时实现了接近反向传播的性能，为替代训练方法提供了有前景的方向。

Abstract: Backpropagation algorithm has driven the remarkable success of deep neural networks, but its lack of biological plausibility and high computational costs have motivated the ongoing search for alternative training methods. Hebbian learning has attracted considerable interest as a biologically plausible alternative to backpropagation. Nevertheless, its exclusive reliance on local information, without consideration of global task objectives, fundamentally limits its scalability. Inspired by the biological synergy between neuromodulators and local plasticity, we introduce a novel model-agnostic Global-guided Hebbian Learning (GHL) framework, which seamlessly integrates local and global information to scale up across diverse networks and tasks. In specific, the local component employs Oja's rule with competitive learning to ensure stable and effective local updates. Meanwhile, the global component introduces a sign-based signal that guides the direction of local Hebbian plasticity updates. Extensive experiments demonstrate that our method consistently outperforms existing Hebbian approaches. Notably, on large-scale network and complex datasets like ImageNet, our framework achieves the competitive results and significantly narrows the gap with standard backpropagation.

</details>


### [47] [NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents](https://arxiv.org/abs/2601.21372)
*Yang Song,Anoushka Vyas,Zirui Wei,Sina Khoshfetrat Pakazad,Henrik Ohlsson,Graham Neubig*

Main category: cs.AI

TL;DR: NEMO是一个将自然语言决策问题描述转换为可执行数学优化实现的系统，通过远程交互自主编码代理实现，确保代码可执行性，并在多个优化基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖专用大语言模型或特定任务代理，这些方法往往脆弱、复杂，且经常生成语法无效或不可执行的代码。需要一种更可靠、可执行的自动化优化建模方法。

Method: NEMO以远程交互自主编码代理为核心抽象，构建围绕ACA的高层系统来结构化、整合和迭代优化任务规范。采用沙盒环境确保代码可执行性，并引入非对称验证循环、外部记忆、最小贝叶斯风险解码和自一致性等协调模式。

Result: 在九个优化基准测试中，NEMO在大多数任务上达到最先进性能，在多个数据集上有显著优势，证明了执行感知的代理架构在自动化优化建模中的有效性。

Conclusion: NEMO通过将自主编码代理作为一等公民抽象，结合新颖的协调模式和验证机制，实现了可靠、可执行的自动化优化建模，为复杂决策问题的自然语言到优化实现转换提供了强大框架。

Abstract: In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.
  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.
  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.

</details>


### [48] [TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models](https://arxiv.org/abs/2601.21375)
*Zheng Li,Siyao Song,Jingyuan Ma,Rui Li,Ying Zeng,Minghao Li,Zhifang Sui*

Main category: cs.AI

TL;DR: 提出基于教学大纲的评估框架，通过学生多轮指导后的表现提升来衡量LLM教学能力，发现不同模型和学科的教学效果差异显著


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为教学助手的潜力被看好，但教学能力评估不足。现有基准主要关注问题解决或问题级指导，知识中心的教学评估未被充分探索

Method: 提出教学大纲基础的评估框架，将教师代理限制在结构化知识点和例题中，避免信息泄露并重用现有基准。在高考数据上实例化该框架，涵盖多个学科

Result: 实验显示不同模型和领域的教学效果差异显著：某些模型在数学上表现良好，但在物理和化学教学中仍具挑战。加入例题不一定改善教学，因为模型常转向例题特定的错误修正

Conclusion: 教学能力是LLM行为的一个独特且可测量的维度，需要专门的评估框架来准确评估

Abstract: Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.

</details>


### [49] [DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis](https://arxiv.org/abs/2601.21403)
*Ruyi Qi,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: DataCross是一个用于跨模态数据分析的基准测试和协作代理框架，旨在整合结构化数据（SQL、CSV）和非结构化视觉文档（扫描报告、发票图像）中的信息，解决现有数据分析代理无法处理"僵尸数据"的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据科学和企业决策中，关键信息往往分散在可直接查询的结构化数据源（SQL、CSV）和锁在非结构化视觉文档（扫描报告、发票图像）中的"僵尸数据"中。现有的数据分析代理主要局限于处理结构化数据，无法激活和关联这些高价值的视觉信息，与工业需求存在显著差距。

Method: 1. 提出DataCrossBench基准：包含200个端到端分析任务，涵盖金融、医疗等领域，通过人机协同反向合成流程构建，确保真实复杂性、跨源依赖性和可验证的真实标签。任务分为三个难度等级，评估代理在视觉表格提取、跨模态对齐和多步联合推理方面的能力。
2. 提出DataCrossAgent框架：受人类分析师"分而治之"工作流程启发，采用专门子代理（每个代理专注于特定数据源），通过结构化工作流程协调：源内深度探索、关键源识别和上下文交叉传播。采用新颖的reReAct机制实现稳健的代码生成和调试以进行事实验证。

Result: 实验结果表明，DataCrossAgent在事实性方面比GPT-4o提高了29.7%，在高难度任务上表现出更强的鲁棒性，能够有效激活分散的"僵尸数据"进行深入的跨模态分析。

Conclusion: DataCross通过其基准测试和协作代理框架，成功解决了跨异构数据模态的统一分析问题，填补了现有数据分析代理与工业需求之间的差距，为激活和利用分散在视觉文档中的"僵尸数据"提供了有效解决方案。

Abstract: In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and "zombie data" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the "divide-and-conquer" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented "zombie data" for insightful, cross-modal analysis.

</details>


### [50] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: DAMI通过动态参数插值在直觉型System 1和深思型System 2之间切换，实现能力控制而非输出控制，在数学推理任务上获得更高准确率同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注输出控制（限制模型输出长度），但这只是认知配置的症状而非根本原因。需要转向能力控制，调节模型如何思考而非产生什么。

Method: 提出DAMI框架：1）利用现有Instruct和Thinking检查点进行动态参数插值，无需额外训练；2）通过查询特定推理强度λ(q)配置认知深度；3）开发基于偏好学习（编码准确性和效率标准）的训练估计方法；4）引入基于置信度的零样本部署方法。

Result: 在五个数学推理基准测试中，DAMI比Thinking模型获得更高准确率，同时保持效率，有效结合了System 1的效率和System 2的推理深度。

Conclusion: 能力控制比输出控制更有效，动态模型插值能够实现直觉和深思认知模式的无缝切换，为统一语言模型提供新方向。

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [51] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: 大型语言模型经常将否定指令误解为肯定指令，在伦理场景中错误地支持被禁止的行为，这在高风险决策中构成安全隐患。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在理解否定指令方面存在系统性缺陷，当用户说"不应该"做某事时，模型反而将其理解为肯定指令。这种缺陷在高风险决策场景中尤为危险，因为模型无法可靠地区分"做X"和"不做X"。

Method: 研究审计了16个模型在14个伦理场景中的表现，测试了简单否定和复合否定两种情况。使用确定性解码排除采样噪声，提出否定敏感指数(NSI)作为治理指标，并设计了分层认证框架。

Result: 开源模型在简单否定下77%的时间支持被禁止行为，在复合否定下达到100%（比肯定表述增加317%）。商业模型表现稍好但仍存在19-128%的波动。模型间一致性从肯定提示的74%下降到否定提示的62%。金融场景的脆弱性是医疗场景的两倍。

Conclusion: 当前对齐技术与安全部署要求之间存在差距：无法可靠区分"做X"和"不做X"的模型不应在高风险环境中做出自主决策。需要建立基于否定敏感指数的治理框架和领域特定阈值的分层认证体系。

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [52] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: 研究发现LLMs在情绪框架效应中表现出惊人的稳定性，与人类易受叙事操纵形成鲜明对比，揭示了"鲁棒性悖论"。


<details>
  <summary>Details</summary>
Motivation: 尽管已知LLMs对提示扰动敏感且易与用户偏见保持一致，但它们在规则约束决策中对情绪框架效应的鲁棒性尚未充分探索。研究者旨在探究LLMs在重要决策领域是否易受叙事操纵影响。

Method: 采用新颖的受控扰动框架，在医疗、法律和金融三个高风险领域进行实验，量化LLMs与人类对叙事操纵的抵抗差异。使用162个场景的基准测试，测量情绪框架效应的影响大小。

Result: LLMs表现出惊人的行为不变性：对情绪框架效应的抵抗比人类高110-300倍。模型效应大小接近零（Cohen's h = 0.003），而人类表现出显著偏见（Cohen's h在[0.3, 0.8]之间）。这种不变性在不同训练范式的模型中持续存在。

Conclusion: 指令调优的LLMs能够将逻辑规则遵守与说服性叙事解耦，提供决策稳定性来源，可以补充甚至去偏人类判断。虽然LLMs对查询格式"脆弱"，但对决策偏见原因"稳定"，揭示了鲁棒性悖论。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [53] [ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design](https://arxiv.org/abs/2601.21448)
*Zhongkai Yu,Chenyang Zhou,Yichen Lin,Hejia Zhang,Haotian Ye,Junxia Cui,Zaifeng Pan,Jishen Zhao,Yufei Ding*

Main category: cs.AI

TL;DR: 该论文提出了一个针对AI辅助芯片设计的综合基准测试ChipBench，用于评估LLM在Verilog生成、调试和参考模型生成三个关键任务上的性能，揭示了现有模型在真实工业场景中的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前硬件工程领域的LLM基准测试存在饱和问题和任务多样性不足，无法反映LLM在真实工业工作流程中的实际性能，需要更全面的评估框架。

Method: 构建包含44个具有复杂层次结构的真实模块、89个系统化调试案例和132个跨Python、SystemC、CXXRTL的参考模型样本的综合基准测试，并开发自动化工具箱用于高质量训练数据生成。

Result: 评估结果显示显著性能差距：最先进的Claude-4.5-opus在Verilog生成上仅达到30.74%，在Python参考模型生成上仅13.33%，远低于现有饱和基准测试中超过95%的通过率。

Conclusion: 该基准测试揭示了LLM在芯片设计任务中的实际挑战，为未来研究提供了重要评估工具，并提供了自动化数据生成工具箱以促进这一未充分探索领域的发展。

Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\% on Verilog generation and 13.33\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.

</details>


### [54] [LION: A Clifford Neural Paradigm for Multimodal-Attributed Graph Learning](https://arxiv.org/abs/2601.21453)
*Xunkai Li,Zhengyu Wu,Zekai Chen,Henan Sun,Daohan Su,Guang Zeng,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.AI

TL;DR: LION是一个基于Clifford代数的多模态图神经网络范式，通过几何感知流形和自适应全息聚合解决模态对齐和融合问题，在多种下游任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图学习方法存在两个主要问题：1）模态对齐时忽视图上下文，抑制模态交互；2）模态融合缺乏适应性，无法充分利用对齐后的拓扑先验信息，导致泛化性和性能下降。

Method: 基于Clifford代数和解耦图神经范式（传播-聚合），提出LION框架：1）构建基于Clifford代数的模态感知几何流形，通过几何诱导的高阶图传播实现模态交互和对齐；2）基于对齐token的几何等级特性，提出自适应全息聚合模块，将几何等级的能量和尺度与可学习参数结合以改进模态融合。

Result: 在9个数据集上的实验表明，LION在3种图下游任务和3种模态下游任务上显著优于当前最优基线方法。

Conclusion: LION通过Clifford代数几何流形和自适应全息聚合有效解决了多模态图学习中的对齐和融合问题，提升了多模态图表示学习的实用性和性能。

Abstract: Recently, the rapid advancement of multimodal domains has driven a data-centric paradigm shift in graph ML, transitioning from text-attributed to multimodal-attributed graphs. This advancement significantly enhances data representation and expands the scope of graph downstream tasks, such as modality-oriented tasks, thereby improving the practical utility of graph ML. Despite its promise, limitations exist in the current neural paradigms: (1) Neglect Context in Modality Alignment: Most existing methods adopt topology-constrained or modality-specific operators as tokenizers. These aligners inevitably neglect graph context and inhibit modality interaction, resulting in suboptimal alignment. (2) Lack of Adaptation in Modality Fusion: Most existing methods are simple adaptations for 2-modality graphs and fail to adequately exploit aligned tokens equipped with topology priors during fusion, leading to poor generalizability and performance degradation. To address the above issues, we propose LION (c\underline{LI}ff\underline{O}rd \underline{N}eural paradigm) based on the Clifford algebra and decoupled graph neural paradigm (i.e., propagation-then-aggregation) to implement alignment-then-fusion in multimodal-attributed graphs. Specifically, we first construct a modality-aware geometric manifold grounded in Clifford algebra. This geometric-induced high-order graph propagation efficiently achieves modality interaction, facilitating modality alignment. Then, based on the geometric grade properties of aligned tokens, we propose adaptive holographic aggregation. This module integrates the energy and scale of geometric grades with learnable parameters to improve modality fusion. Extensive experiments on 9 datasets demonstrate that LION significantly outperforms SOTA baselines across 3 graph and 3 modality downstream tasks.

</details>


### [55] [Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance](https://arxiv.org/abs/2601.21465)
*Márton Kardos*

Main category: cs.AI

TL;DR: Topeax是一种新的主题建模方法，通过密度估计峰值确定聚类数量，结合词汇和语义指标选择高质量关键词，相比Top2Vec和BERTopic在聚类恢复和描述方面表现更好且更稳定。


<details>
  <summary>Details</summary>
Motivation: 当前流行的Top2Vec和BERTopic主题建模方法存在两个主要问题：1）对样本大小和超参数极度敏感，难以发现自然聚类；2）关键词重要性估计存在缺陷，BERTopic忽略语义距离，Top2Vec忽略词频，导致主题不连贯且缺乏多样性。

Method: Topeax通过密度估计峰值自动发现聚类数量，结合词汇统计（词频）和语义指标（与主题向量的距离）来评估术语重要性，从而获得高质量的主题关键词。

Result: Topeax在聚类恢复和聚类描述方面均优于Top2Vec和BERTopic，同时对样本大小和超参数的变化表现出更稳定的行为。

Conclusion: Topeax解决了现有主题建模方法的可靠性问题，通过更稳健的聚类发现机制和综合的关键词选择策略，提供了更高质量的主题建模结果。

Abstract: Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.

</details>


### [56] [MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning](https://arxiv.org/abs/2601.21468)
*Yaorui Shi,Shugui Liu,Yu Yang,Wenyu Mao,Yuxin Chen,Qi GU,Hui Su,Xunliang Cai,Xiang Wang,An Zhang*

Main category: cs.AI

TL;DR: MemOCR：一种多模态记忆代理，通过视觉布局自适应分配内存空间，在有限上下文预算下提升长时程推理能力


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统将历史序列化为文本，令牌级成本均匀且随长度线性增长，经常在低价值细节上浪费稀缺的上下文预算。需要更有效地压缩不断增长的交互历史以适应有限的上下文窗口。

Method: MemOCR维护结构化富文本记忆（如标题、高亮），并将其渲染为图像供代理访问，通过视觉布局优先显示关键证据，同时积极压缩辅助细节。使用强化学习在预算感知目标下训练，使代理适应不同压缩级别。

Result: 在长上下文多跳和单跳问答基准测试中，MemOCR优于基于文本的基线方法，在极端预算下实现了更有效的上下文利用。

Conclusion: MemOCR通过视觉布局实现自适应信息密度分配，为长时程智能体推理提供了一种有效的记忆压缩方法，在有限上下文预算下显著提升了性能。

Abstract: Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.

</details>


### [57] [ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management](https://arxiv.org/abs/2601.21473)
*Zaifeng Pan,Yipeng Shen,Zhengding Hu,Zhuang Wang,Aninda Manocha,Zheng Wang,Zhongkai Yu,Yue Guan,Yufei Ding*

Main category: cs.AI

TL;DR: ScaleSim：基于LLM的大规模多智能体模拟内存优化系统，通过智能预取和优先级驱逐实现1.74倍加速


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体模拟面临GPU内存压力大的挑战，每个智能体都需要维护私有GPU状态（模型、前缀缓存、适配器等），随着智能体数量增加，设备内存迅速耗尽

Method: 提出"调用距离"的统一抽象概念来估计智能体未来LLM请求的相对顺序，基于此实现ScaleSim系统，支持主动预取、基于优先级的驱逐，并通过模块化接口支持多样化的智能体特定内存

Result: 在模拟基准测试中，ScaleSim相比SGLang实现了最高1.74倍的加速

Conclusion: ScaleSim通过利用多智能体模拟工作负载的特性（稀疏智能体激活和可估计的调用顺序），有效解决了大规模LLM多智能体模拟的内存扩展问题

Abstract: LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.

</details>


### [58] [The Path of Least Resistance: Guiding LLM Reasining Trajectories with Prefix Consensus](https://arxiv.org/abs/2601.21494)
*Ishan Jindal,Sai Prashanth Akuthota,Jayant Taneja,Sachin Dev Sharma*

Main category: cs.AI

TL;DR: PoLR是一种基于前缀一致性的推理优化方法，通过聚类推理轨迹的短前缀识别主导簇，显著减少计算开销，同时保持Self-Consistency的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然推理能力强，但Self-Consistency等推理策略计算成本高，需要完全展开所有推理轨迹。需要一种既能保持准确性又能减少计算开销的推理优化方法。

Method: PoLR通过聚类推理轨迹的短前缀，识别主导簇，然后仅扩展该簇中的所有路径。理论分析基于互信息和熵解释早期推理步骤对最终正确性的预测能力。

Result: 在GSM8K、MATH500、AIME24/25和GPQA-DIAMOND等基准测试中，PoLR匹配或超过Self-Consistency性能，同时减少高达60%的token使用和50%的延迟。与自适应推理方法完全兼容。

Conclusion: PoLR是首个利用前缀一致性实现计算高效推理的推理时方法，无需模型微调即可显著提升Self-Consistency的效率和可扩展性，可作为即插即用的预过滤器。

Abstract: Large language models achieve strong reasoning performance, but inference strategies such as Self-Consistency (SC) are computationally expensive, as they fully expand all reasoning traces. We introduce PoLR (Path of Least Resistance), the first inference-time method to leverage prefix consistency for compute-efficient reasoning. PoLR clusters short prefixes of reasoning traces, identifies the dominant cluster, and expands all paths in that cluster, preserving the accuracy benefits of SC while substantially reducing token usage and latency. Our theoretical analysis, framed via mutual information and entropy, explains why early reasoning steps encode strong signals predictive of final correctness. Empirically, PoLR consistently matches or exceeds SC across GSM8K, MATH500, AIME24/25, and GPQA-DIAMOND, reducing token usage by up to 60% and wall-clock latency by up to 50%. Moreover, PoLR is fully complementary to adaptive inference methods (e.g., Adaptive Consistency, Early-Stopping SC) and can serve as a drop-in pre-filter, making SC substantially more efficient and scalable without requiring model fine-tuning.

</details>


### [59] [MAR: Efficient Large Language Models via Module-aware Architecture Refinement](https://arxiv.org/abs/2601.21503)
*Junhong Cai,Guiqin Wang,Kejie Zhao,Jianxiong Tang,Xiang Wang,Luziwei Leng,Ran Cheng,Yuxin Ma,Qinghai Guo*

Main category: cs.AI

TL;DR: 提出MAR框架，通过整合状态空间模型和激活稀疏化来降低LLM的能耗，同时设计ATMN神经元和SBDS策略解决SNN与SSM集成问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多个领域表现出色，但存在高能耗问题，主要源于二次注意力机制和密集的前馈网络操作

Method: 提出模块感知架构优化(MAR)框架：1) 整合状态空间模型实现线性时间序列建模；2) 应用激活稀疏化降低前馈网络成本；3) 设计自适应三元多步神经元(ATMN)和脉冲感知双向蒸馏策略(SBDS)解决SNN与SSM集成问题

Result: MAR在资源受限条件下有效恢复了密集模型的性能，同时显著降低推理能耗，且优于同等规模甚至更大规模的高效模型

Conclusion: MAR框架展示了构建高效实用大型语言模型的潜力，通过创新的架构优化方法平衡了性能与能耗

Abstract: Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs.

</details>


### [60] [The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation](https://arxiv.org/abs/2601.21505)
*Diaoulé Diallo,Katharina Dworatzyk,Sophie Jentzsch,Peer Schütt,Sabine Theis,Tobias Hecking*

Main category: cs.AI

TL;DR: 该研究通过人类评估验证了激活引导技术能有效控制LLM输出的情感语调，发现中等强度引导能可靠增强目标情感同时保持文本质量，且人类评分与自动评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 在推理阶段控制大型语言模型的行为对于使其输出符合人类能力和安全要求至关重要。激活引导提供了一种轻量级的替代方案，可以直接修改内部激活来引导生成，但之前的研究缺乏人类评估验证。

Method: 1) 首次对激活引导进行人类评估，通过Prolific平台收集了190名参与者的7,000多个评分，评估情感强度和文本质量；2) 比较人类评分与模型自动评分的相关性；3) 测试不同引导强度对情感的影响；4) 比较Alpaca和LlaMA-3模型的效果。

Result: 1) 人类与模型质量评分高度相关（平均r=0.776）；2) 中等引导强度（λ≈0.15）能可靠增强目标情感同时保持可理解性，对厌恶（η_p²=0.616）和恐惧（η_p²=0.540）效果最强，对惊讶（η_p²=0.042）效果最弱；3) LlaMA-3比Alpaca产生更一致的引导效果；4) 评分者间信度高（ICC=0.71-0.87）。

Conclusion: 激活引导是一种可扩展的方法，可以在情感维度上有效控制LLM行为，自动评分可以代理感知质量，中等强度引导能在增强情感的同时保持文本质量。

Abstract: Controlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$--$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($λ\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($η_p^2 = 0.616$) and fear ($η_p^2 = 0.540$), and minimal effects for surprise ($η_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p < 0.001$). Inter-rater reliability was high (ICC $= 0.71$--$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.

</details>


### [61] [LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI](https://arxiv.org/abs/2601.21511)
*Niki van Stein,Anna V. Kononova,Lars Kotthoff,Thomas Bäck*

Main category: cs.AI

TL;DR: LLaMEA-SAGE：通过从生成算法的抽象语法树中提取图论和复杂度特征来指导自动化算法设计，使用可解释AI技术识别影响性能的特征并转化为自然语言突变指令，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动化算法设计方法（如LLaMEA）主要依赖适应度反馈驱动搜索，忽略了生成代码中的大量结构信息。需要一种机制来利用代码结构特征指导算法设计过程。

Method: 提出LLaMEA-SAGE方法：1)从生成算法的抽象语法树中提取图论和复杂度特征；2)基于评估解档案学习代理模型；3)使用可解释AI技术识别影响性能的特征；4)将这些特征转化为自然语言突变指令来指导后续LLM代码生成。

Result: 1)在小型控制实验中，LLaMEA-SAGE比原始LLaMEA更快达到相同性能；2)在GECCO-MA-BBOB竞赛的MA-BBOB套件大规模实验中，该方法优于最先进的自动化算法设计方法。

Conclusion: 从代码中提取的信号能有效偏置LLM驱动的算法进化，弥合了代码结构与人类可理解的性能反馈之间的差距，为自动化算法设计提供了结构化指导。

Abstract: Large language models have enabled automated algorithm design (AAD) by generating optimization algorithms directly from natural-language prompts. While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated code unused. We propose a mechanism for guiding AAD using feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms, based on a surrogate model learned over an archive of evaluated solutions. Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity.
  We propose LLaMEA-SAGE, which integrates this feature-driven guidance into LLaMEA, and evaluate it across several benchmarks. We show that the proposed structured guidance achieves the same performance faster than vanilla LLaMEA in a small controlled experiment. In a larger-scale experiment using the MA-BBOB suite from the GECCO-MA-BBOB competition, our guided approach achieves superior performance compared to state-of-the-art AAD methods. These results demonstrate that signals derived from code can effectively bias LLM-driven algorithm evolution, bridging the gap between code structure and human-understandable performance feedback in automated algorithm design.

</details>


### [62] [KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization](https://arxiv.org/abs/2601.21526)
*Alireza Nadaf,Alireza Mohammadshahi,Majid Yazdani*

Main category: cs.AI

TL;DR: KAPSO是一个用于自主程序合成与优化的模块化框架，通过迭代的构思、代码合成与编辑、执行、评估和学习来改进可运行工件，将合成作为长期优化循环中的操作符。


<details>
  <summary>Details</summary>
Motivation: 解决编码代理中常见的长期失败问题，包括丢失实验状态、脆弱的调试能力以及领域专业知识复用能力弱，通过集成紧密耦合的组件来改善这些问题。

Method: 框架包含三个核心组件：1) git原生实验引擎，将每次尝试隔离为分支，生成可复现工件并保持迭代间的可追溯性；2) 知识系统，摄取异构源（仓库、内部手册、外部文档等）并组织成结构化表示；3) 认知记忆层，协调检索并维护从实验痕迹中提取的可复用经验。

Result: 在MLE-Bench（Kaggle风格ML竞赛）和ALE-Bench（AtCoder启发式优化）上进行了评估，并报告了端到端性能。

Conclusion: KAPSO通过将合成作为长期优化循环中的操作符，集成了实验管理、知识组织和经验学习，有效解决了编码代理的长期失败问题，提高了程序合成与优化的效率和效果。

Abstract: We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso

</details>


### [63] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: ARGORA框架将多专家LLM讨论组织成显式论证图，通过因果分析识别关键推理链，并引入校正机制对齐内部推理与外部判断。


<details>
  <summary>Details</summary>
Motivation: 现有多专家LLM系统通过简单聚合收集不同观点，但掩盖了哪些论证驱动最终决策。需要更透明的方法来追踪推理过程并识别关键论证。

Method: 1) 将多专家讨论组织成显式论证图，显示论证间的支持/攻击关系；2) 将论证图建模为因果模型，通过系统移除单个论证并重新计算结果来识别必要推理链；3) 引入校正机制，当内部推理与外部判断不一致时进行对齐。

Result: 在多样化基准测试和开放用例中，ARGORA实现了有竞争力的准确性，并展现出校正行为：当专家初始意见不一致时，框架更倾向于将争议解决为正确答案而非引入新错误，同时提供关键论证的因果诊断。

Conclusion: ARGORA通过论证图和因果分析为多专家LLM系统提供了透明、可解释的决策框架，能够识别关键推理链并在需要时校正推理过程，增强了系统的可靠性和可解释性。

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [64] [ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory](https://arxiv.org/abs/2601.21545)
*Yang Zhao,Chengxiao Dai,Yue Xiu,Mengying Kou,Yuliang Zheng,Dusit Niyato*

Main category: cs.AI

TL;DR: ShardMemo是一个为多智能体LLM系统设计的预算分层内存服务，通过分片存储和智能路由机制解决大规模并行访问瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统集中式索引和启发式分区在内存容量增长和并行访问增加时成为瓶颈，需要更高效的内存管理方案来支持长时状态和多智能体并发执行。

Method: 采用三层内存架构：Tier A（智能体工作状态）、Tier B（分片证据存储，包含本地ANN索引和范围优先路由机制）、Tier C（版本化技能库）。通过掩码混合专家路由在合格分片中进行探测，使用成本感知门控机制。

Result: 在LoCoMo上比最强基线(GAM)提升5.11-6.82 F1；在固定预算路由设置下比余弦原型路由提升6.87 F1，同时减少20.5%检索工作和降低延迟；在长上下文HotpotQA上表现优异；在ToolBench上Tier C达到0.97 Precision@3和1.94 StepRed。

Conclusion: ShardMemo通过分层内存架构和智能路由机制有效解决了多智能体LLM系统的内存管理瓶颈，在多个基准测试中表现出显著性能提升。

Abstract: Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\mathrm{probe}}$ shards via Top-$B_{\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).

</details>


### [65] [Meta Context Engineering via Agentic Skill Evolution](https://arxiv.org/abs/2601.21557)
*Haoran Ye,Xuning He,Vincent Arak,Haonan Dong,Guojie Song*

Main category: cs.AI

TL;DR: 提出Meta Context Engineering (MCE)框架，通过双层代理协同进化上下文工程技能和上下文工件，取代静态启发式方法，在多个领域实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前上下文工程方法依赖人工设计的固定流程和预定义模式，存在结构偏见且设计空间受限，需要更灵活、自适应的优化框架。

Method: MCE采用双层框架：元级代理通过agentic crossover（对技能历史、执行和评估的审慎搜索）进化工程技能；基础级代理执行这些技能，从训练回放中学习，并优化上下文为灵活的文件和代码。

Result: 在五个不同领域的离线和在线设置中，MCE相比最先进的代理式上下文工程方法获得5.6-53.8%的相对改进（平均16.9%），同时在上下文适应性、可迁移性、使用效率和训练效率方面表现优越。

Conclusion: MCE框架通过协同进化技能和上下文，超越了静态启发式方法，为上下文工程提供了更灵活、高效的自适应优化方案。

Abstract: The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.

</details>


### [66] [EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots](https://arxiv.org/abs/2601.21570)
*Zixing Lei,Genjia Liu,Yuanshuo Zhang,Qipeng Liu,Chuan Wen,Shanghang Zhang,Wenzhao Lian,Siheng Chen*

Main category: cs.AI

TL;DR: LLM代理在自主工程化具身策略方面的能力评估基准，通过32个任务测试动态闭环工作流程，结果显示自主代理能超越人工基准26.5%


<details>
  <summary>Details</summary>
Motivation: 具身AI领域向通用机器人系统快速演进，但扩展能力受到依赖人工监督的严重瓶颈，从复杂的奖励塑造到异构后端的超参数调优都需要大量人工。受LLM在软件自动化和科学发现中成功的启发，需要评估LLM代理自主工程化具身策略的能力。

Method: 提出EmboCoach-Bench基准，包含32个专家策划的RL和IL任务，以可执行代码作为通用接口。评估动态闭环工作流程，代理利用环境反馈迭代起草、调试和优化解决方案，涵盖从物理感知奖励设计到扩散策略等策略架构的改进。

Result: 三个关键发现：(1) 自主代理在平均成功率上能超越人工工程基准26.5%；(2) 带有环境反馈的代理工作流程有效加强策略开发，显著缩小开源和专有模型之间的性能差距；(3) 代理展现出病理工程案例的自我纠正能力，通过迭代模拟循环调试成功从近乎完全失败中恢复任务性能。

Conclusion: 这项工作为自我进化的具身智能奠定了基础，加速了从劳动密集型手动调优到可扩展自主工程的范式转变，推动具身AI领域向更高效的自动化方向发展。

Abstract: The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.

</details>


### [67] [Chain Of Thought Compression: A Theoritical Analysis](https://arxiv.org/abs/2601.21576)
*Juncai Li,Ru Li,Yuxiang Zhou,Boxiang Ma,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 本文首次从理论上分析了学习内化中间推理步骤的困难，提出Order-r Interaction概念证明高阶逻辑依赖的学习信号会指数衰减，并提出了ALiCoT框架通过对齐潜在token分布与中间推理状态来克服信号衰减问题。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought（CoT）虽然解锁了大语言模型的高级推理能力，但生成额外token带来了高昂的计算成本。最近的实证研究表明，将推理步骤压缩到潜在状态（隐式CoT压缩）提供了token高效的替代方案，但其机制尚不清楚。

Method: 1. 引入Order-r Interaction概念进行理论分析；2. 创建NatBool-DAG基准测试来强制不可约逻辑推理并消除语义捷径；3. 提出ALiCoT框架，通过对齐潜在token分布与中间推理状态来克服信号衰减。

Result: ALiCoT成功实现了高效推理：在保持与显式CoT相当性能的同时，实现了54.4倍的加速。实验结果表明该框架能够克服高阶交互障碍，有效学习内化中间推理步骤。

Conclusion: 本文首次从理论上解释了CoT压缩的困难机制，证明了高阶逻辑依赖的学习信号会指数衰减，并提出了有效的ALiCoT解决方案。该研究为理解隐式推理压缩提供了理论基础，并为开发高效推理模型提供了实用框架。

Abstract: Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.

</details>


### [68] [Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves](https://arxiv.org/abs/2601.21582)
*Jonas Knupp,Jan Hendrik Metzen,Jeremias Bohn,Georg Groh,Kristian Kersting*

Main category: cs.AI

TL;DR: Dreamer框架通过深度递归注意力混合机制，结合序列注意力、深度注意力和稀疏专家注意力，解决了深度递归模型中的隐藏大小瓶颈问题，在语言推理任务上实现了更高效的训练和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度递归方法存在三个问题：1) 缺乏FLOP、参数和内存匹配的基线比较；2) 由于部分固定的层堆栈导致深度递归未充分利用；3) 恒定隐藏大小限制了多步潜在推理能力。

Method: 提出Dreamer框架，结合三种注意力机制：序列注意力（处理序列维度）、深度注意力（处理深度维度）、稀疏专家注意力（MoE机制）。通过深度注意力缓解隐藏大小瓶颈，解耦缩放维度，使深度递归模型能够高效扩展。

Result: 在语言推理基准测试中，相比FLOP、参数和内存匹配的SOTA模型，Dreamer达到相同准确率所需训练token减少2-8倍；使用相同训练token时，性能优于约2倍大的SOTA模型。专家选择多样性比SOTA MoE高2-11倍。

Conclusion: Dreamer框架通过深度递归注意力混合机制有效解决了深度递归模型的扩展瓶颈，实现了更高效的训练和更好的推理性能，为多步潜在推理提供了新的解决方案。

Abstract: Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.

</details>


### [69] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: ATP-Latent提出主动潜在规划方法，通过条件变分自编码器建模潜在token监督过程，结合强化学习优化潜在推理策略，相比现有方法在准确率和token效率上均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法通过模仿语言标签监督潜在token，但同一问题可能存在多个等效但不同的思维链标签，被动模仿任意一个可能导致次优的潜在token表示和推理策略，影响规划能力并造成训练测试差距。

Method: 提出ATP-Latent方法：1) 使用条件变分自编码器建模潜在token监督过程，获得更平滑的潜在空间；2) 结合强化学习，基于VAE解码内容的一致性计算辅助连贯性奖励，引导RL过程获得最合理的潜在推理策略。

Result: 在LLaMA-1B模型上，ATP-Latent在四个基准测试中相比先进基线方法实现了+4.1%的准确率提升和-3.3%的token消耗减少。

Conclusion: 强调在潜在token表示空间上进行主动规划对获得最优潜在推理策略的重要性，ATP-Latent通过条件VAE和强化学习的结合有效提升了潜在推理的性能和效率。

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [70] [CORE: Collaborative Reasoning via Cross Teaching](https://arxiv.org/abs/2601.21600)
*Kshitij Mishra,Mirat Aubakirov,Martin Takac,Nils Lukas,Salem Lahlou*

Main category: cs.AI

TL;DR: CORE是一个训练时协作框架，通过跨模型教学将同伴的成功转化为学习信号，显著提升小模型在推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在互补的推理错误：同一个问题，一个模型可能通过特定分解成功，而另一个失败。利用这种互补性，通过训练时协作将同伴成功转化为学习信号。

Method: CORE采用两阶段协作：冷轮独立采样，然后是上下文救援轮，失败模型从成功同伴提取提示。优化包含正确性、多样性（减少错误重叠）和救援奖励的复合奖励。

Result: 仅用1000个训练样本，3B+4B模型对在GSM8K上达到99.54% Pass@2，MATH上92.08%，相比单模型训练（82.50%和74.82%）大幅提升。在更难的数据集上也表现优异。

Conclusion: 训练时协作能可靠地将模型互补性转化为显著性能增益，无需扩大模型规模，为小模型高效推理提供了新途径。

Abstract: Large language models exhibit complementary reasoning errors: on the same instance, one model may succeed with a particular decomposition while another fails. We propose Collaborative Reasoning (CORE), a training-time collaboration framework that converts peer success into a learning signal via a cross-teaching protocol. Each problem is solved in two stages: a cold round of independent sampling, followed by a contexted rescue round in which models that failed receive hint extracted from a successful peer. CORE optimizes a combined reward that balances (i) correctness, (ii) a lightweight DPP-inspired diversity term to reduce error overlap, and (iii) an explicit rescue bonus for successful recovery. We evaluate CORE across four standard reasoning datasets GSM8K, MATH, AIME, and GPQA. With only 1,000 training examples, a pair of small open source models (3B+4B) reaches Pass@2 of 99.54% on GSM8K and 92.08% on MATH, compared to 82.50% and 74.82% for single-model training. On harder datasets, the 3B+4B pair reaches Pass@2 of 77.34% on GPQA (trained on 348 examples) and 79.65% on AIME (trained on 792 examples), using a training-time budget of at most 1536 context tokens and 3072 generated tokens. Overall, these results show that training-time collaboration can reliably convert model complementarity into large gains without scaling model size.

</details>


### [71] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 该研究将智能文档处理系统的验证问题形式化为基于搜索的软件测试问题，旨在在有限预算下最大化发现不同类型的故障模式，并通过多种搜索策略的对比分析揭示了策略间的互补性。


<details>
  <summary>Details</summary>
Motivation: 企业级智能文档处理系统在金融、保险和医疗等高风险领域应用广泛，早期系统验证需要在有限预算下发现多样化的故障机制，而不是寻找单一的最坏情况文档。

Method: 将问题形式化为基于搜索的软件测试问题，在文档配置的组合空间中操作，实例化结构风险特征以诱导现实故障条件。在相同预算约束下，对进化算法、群体智能、质量多样性、学习型和量子等多种搜索策略进行基准测试。

Result: 不同求解器在可比预算下持续发现特定替代方法未发现的故障模式。跨时间分析显示所有评估预算中都存在持久的求解器特定发现，没有单一策略表现出绝对优势。虽然所有求解器的联合最终能覆盖观察到的故障空间，但依赖任何单一方法都会系统性地延迟重要风险的发现。

Conclusion: 研究结果展示了求解器内在的互补性，为工业IDP验证提出了基于组合策略的SBST方法，强调需要采用组合策略而非单一方法来实现稳健的系统验证。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [72] [RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems](https://arxiv.org/abs/2601.21609)
*Bingqian Li,Xiaolei Wang,Junyi Li,Weitao Li,Long Zhang,Sheng Chen,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: RecNet是一个自演化的偏好传播框架，通过前向传播和反向优化两阶段，主动在相关用户和物品间传播实时偏好更新，解决传统方法依赖稀疏、噪声交互数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能推荐系统主要依赖显式的用户-物品交互来建模偏好变化，但这些数据稀疏、噪声大，且无法反映用户和物品之间的实时相互影响。需要一种能够主动传播实时偏好更新的方法。

Method: RecNet包含两个互补阶段：前向阶段通过集中式偏好路由机制整合偏好更新并动态传播到最相关的代理；个性化偏好接收机制结合消息缓冲和可优化的规则过滤器来选择性同化偏好。反向阶段采用反馈驱动的传播优化机制，模拟多智能体强化学习框架，使用LLM进行信用分配、梯度分析和模块级优化。

Result: 在各种场景下的广泛实验证明了RecNet在建模推荐系统偏好传播方面的有效性。

Conclusion: RecNet通过自演化的偏好传播框架，能够主动传播实时偏好更新，解决了传统方法依赖稀疏交互数据的问题，为推荐系统提供了更有效的偏好建模方法。

Abstract: Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.

</details>


### [73] [Semantic Content Determines Algorithmic Performance](https://arxiv.org/abs/2601.21618)
*Martiño Ríos-García,Nawaf Alampara,Kevin Maik Jablonka*

Main category: cs.AI

TL;DR: 论文提出了WhatCounts测试框架，专门检测LLM在计数任务中对语义内容的敏感性，发现前沿LLM的准确率会因计数对象类型不同而产生超过40%的差异。


<details>
  <summary>Details</summary>
Motivation: 算法行为应该独立于其参数的语义内容，但现有研究常将语义敏感性与推理复杂性或提示变化混淆。需要一种原子化的测试方法来专门检测LLM是否真正实现了算法不变性。

Method: 开发了WhatCounts测试框架，使用无歧义、分隔的列表进行计数任务，排除重复项、干扰项和复杂推理步骤，针对不同语义类型（如城市vs化学品、名称vs符号）进行对比测试。

Result: 前沿LLM的计数准确率仅因计数对象语义不同就产生超过40%的差异，且这种差异无法通过控制实验消除。少量无关的微调会不可预测地改变这种语义差距。

Conclusion: LLM并非实现算法，而是近似算法，且这种近似是参数依赖的。这意味着任何LLM函数都可能隐藏着对其输入语义的依赖，具有超越计数任务的广泛影响。

Abstract: Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.

</details>


### [74] [ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval](https://arxiv.org/abs/2601.21654)
*Hao Shen,Hang Yang,Zhouhong Gu*

Main category: cs.AI

TL;DR: ScholarGym是一个用于评估深度研究工作流的模拟环境，通过静态语料库和确定性检索解决API调用不确定性问题，支持对查询规划、工具调用和相关性评估的细粒度分析。


<details>
  <summary>Details</summary>
Motivation: 当前工具增强的大型语言模型已从单轮问答发展到深度研究工作流，但评估这些工作流面临根本挑战：依赖实时API会引入非确定性，因为工具调用可能因时间漂移、速率限制和后台状态变化而产生不同结果，这种差异破坏了可重复性并使跨系统比较无效。

Method: 将工作流组件解耦为查询规划、工具调用和相关性评估，在受控条件下对每个阶段进行细粒度分析。基于包含57万篇论文的静态语料库构建，提供确定性检索，包含2,536个带有专家标注真实标签的查询。

Result: 通过在不同骨干模型上进行实验，揭示了推理能力、规划策略和选择机制在迭代优化过程中的相互作用。

Conclusion: ScholarGym为深度研究工作流提供了可重复的评估环境，解决了实时API依赖带来的非确定性问题，支持对复杂信息需求处理流程的系统分析。

Abstract: Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.
  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.

</details>


### [75] [SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://arxiv.org/abs/2601.21666)
*Ahmed Y. Radwan,Christos Emmanouilidis,Hina Tabassum,Deval Pandya,Shaina Raza*

Main category: cs.AI

TL;DR: SONIC-O1是一个全面的人工验证多模态大语言模型基准测试，涵盖13个真实对话领域，包含4,958个标注和人口统计元数据，用于评估时序定位和社会鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型研究主要关注静态图像理解，而对处理时序音频-视频数据的能力探索不足，需要高质量基准测试来系统评估MLLM在真实场景中的表现。

Method: 构建SONIC-O1基准测试，包含13个真实对话领域，4,958个人工验证标注，评估任务包括开放式摘要、多项选择题回答、带推理的时序定位，并包含人口统计元数据。

Result: 实验显示闭源和开源模型在多项选择题准确率上差距较小，但在时序定位任务上最佳闭源模型比开源模型高出22.6%，且模型性能在不同人口统计群体间存在差异。

Conclusion: SONIC-O1为时序基础和社会鲁棒的多模态理解提供了开放评估套件，揭示了当前MLLM在处理时序视频数据和社会公平性方面的局限性。

Abstract: Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard

</details>


### [76] [TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning](https://arxiv.org/abs/2601.21692)
*Mingzu Liu,Hao Fang,Runmin Cong*

Main category: cs.AI

TL;DR: TCAP是一种无监督防御框架，通过分析多模态大语言模型中三个功能组件（系统指令、视觉输入、用户文本查询）的注意力分配差异来检测后门样本，无需监督信号且能泛化到不同触发器和模态。


<details>
  <summary>Details</summary>
Motivation: 现有针对MLLMs后门攻击的防御方法要么依赖监督信号，要么无法泛化到不同的触发器类型和模态。作者发现了一个普遍的后门指纹——注意力分配差异，即中毒样本会破坏三个功能组件之间的平衡注意力分布，无论触发器形态如何。

Method: 提出Tri-Component Attention Profiling (TCAP)框架：1) 将跨模态注意力图分解为三个功能组件；2) 使用高斯混合模型(GMM)统计分析识别对触发器敏感的注意力头；3) 通过基于EM的投票聚合机制隔离中毒样本。

Result: 在不同MLLM架构和攻击方法上的广泛实验表明，TCAP能够实现一致且强大的性能，在各种设置下都能有效检测后门样本。

Conclusion: TCAP作为一种无监督防御框架，通过利用注意力分配差异这一普遍后门指纹，为MLLMs提供了鲁棒且实用的后门防御解决方案。

Abstract: Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.

</details>


### [77] [FBS: Modeling Native Parallel Reading inside a Transformer](https://arxiv.org/abs/2601.21708)
*Tongxi Wang*

Main category: cs.AI

TL;DR: FBS Transformer通过引入三个模块（PAW、CH、SG）来改进LLM推理效率，模拟人类阅读的预览/略读机制，在不增加参数的情况下提升质量-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理主要依赖严格的逐token自回归，现有加速方法未能捕捉人类阅读的核心要素：内容自适应前瞻、块结构感知计算分配、以及训练-测试一致的预览/略读机制。

Method: 提出Fovea-Block-Skip Transformer（FBS），通过三个模块注入可训练的因果循环：Parafovea-Attention Window（PAW）实现内容自适应前瞻，Chunk-Head（CH）进行块结构感知计算分配，Skip-Gate（SG）控制跳过机制。

Result: 在多样化基准测试中，FBS改善了质量-效率权衡而不增加参数，消融实验显示三个模块具有互补性。

Conclusion: FBS Transformer通过模拟人类阅读的预览和略读机制，为LLM推理提供了一种更高效的架构设计，在保持质量的同时显著提升效率。

Abstract: Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.

</details>


### [78] [E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory](https://arxiv.org/abs/2601.21714)
*Kaixiang Wang,Yidan Lin,Jiong Lou,Zhaojiacheng Zhou,Bunyod Suvonov,Jie Li*

Main category: cs.AI

TL;DR: E-mem框架通过从内存预处理转向情景上下文重建，采用异构分层架构，在保持逻辑完整性的同时显著提升推理性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理系统在实现System 2推理时，内存预处理方法存在破坏性去上下文化问题，将复杂序列依赖压缩为预定义结构会破坏深度推理所需的情境完整性。

Method: 提出E-mem框架，从内存预处理转向情景上下文重建，采用异构分层架构：多个助手代理维护未压缩内存上下文，中央主代理协调全局规划，助手在激活片段内进行本地推理并提取情境感知证据。

Result: 在LoCoMo基准测试中，E-mem达到超过54%的F1分数，比最先进的GAM方法提升7.75%，同时减少超过70%的token成本。

Conclusion: E-mem通过情景上下文重建有效解决了内存预处理中的破坏性去上下文化问题，为LLM代理实现System 2推理提供了更有效的架构，在保持逻辑完整性的同时显著提升了性能效率。

Abstract: The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\% F1, surpassing the state-of-the-art GAM by 7.75\%, while reducing token cost by over 70\%.

</details>


### [79] [DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting](https://arxiv.org/abs/2601.21726)
*Siru Zhong,Yiqiu Liu,Zhiqing Cui,Zezhi Shao,Fei Wang,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: DropoutTS是一个模型无关的插件，通过样本自适应dropout机制，根据噪声水平动态调整dropout率，提升深度时间序列模型的鲁棒性，无需修改模型架构。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用中时间序列数据普遍存在噪声，现有鲁棒性策略要么剪枝数据，要么依赖昂贵的先验量化，无法平衡有效性和效率。

Method: 提出DropoutTS插件，采用样本自适应dropout机制：利用频谱稀疏性通过重构残差高效量化实例级噪声，将噪声映射到自适应dropout率，动态校准模型学习能力，选择性抑制虚假波动同时保持细粒度保真度。

Result: 在多种噪声机制和开放基准测试上的广泛实验表明，DropoutTS能持续提升优秀骨干模型的性能，以可忽略的参数开销和无架构修改实现先进的鲁棒性。

Conclusion: DropoutTS通过从"学什么"到"学多少"的范式转变，为深度时间序列模型提供了高效且有效的鲁棒性增强方案。

Abstract: Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from "what" to learn to "how much" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.

</details>


### [80] [Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.21742)
*Ruiwen Zhou,Maojia Song,Xiaobao Wu,Sitao Cheng,Xunjian Yin,Yuxi Xie,Zhuoqun Hao,Wenyue Hua,Liangming Pan,Soujanya Poria,Min-Yen Kan*

Main category: cs.AI

TL;DR: ECL框架让小型LLM通过历史交互评估同伴可靠性，在多人协作任务中超越大8倍的模型，实现近完美性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中个体智能体缺乏鲁棒性，容易盲目跟随误导性同伴，这源于谄媚行为和对同伴可靠性评估能力不足。

Method: 提出历史感知参考学习问题，开发Epistemic Context Learning (ECL)推理框架，通过历史交互构建同伴档案，并用强化学习优化辅助奖励。

Result: ECL使Qwen 3-4B等小型模型在多人协作任务中超越历史无关基线模型Qwen 3-30B（大8倍），前沿模型性能接近100%，且能泛化到多种多智能体配置。

Conclusion: ECL通过历史交互评估同伴可靠性有效提升多智能体系统鲁棒性，LLM能很好建模信任，信任建模准确性与最终答案质量强相关。

Abstract: Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.

</details>


### [81] [Language-based Trial and Error Falls Behind in the Era of Experience](https://arxiv.org/abs/2601.21754)
*Haoyu Wang,Guozheng Ma,Shugang Cui,Yilun Kong,Haotian Luo,Li Shen,Mengya Gao,Yichao Wu,Xiaogang Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: SCOUT框架通过轻量级"侦察兵"探索环境，收集轨迹数据来引导LLM学习，解决了LLM在非语言环境中探索成本过高的问题，显著提升了性能并降低了计算消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语言任务上表现出色，但在未见过的非语言环境（如符号或空间任务）中表现有限。传统方法认为这是预训练分布与测试分布不匹配导致的，但本文发现主要瓶颈是探索成本过高：掌握这些任务需要大量试错，这对于参数量大的LLM在高维语义空间中操作是计算上不可持续的。

Method: 提出SCOUT（Sub-Scale Collaboration On Unseen Tasks）框架，将探索与利用解耦。使用轻量级"侦察兵"（如小型MLP）以远超LLM的速度和规模探测环境动态。收集的轨迹通过监督微调（SFT）引导LLM，然后通过多轮强化学习（RL）激活其潜在的世界知识。

Result: SCOUT使Qwen2.5-3B-Instruct模型平均得分达到0.86，显著优于包括Gemini-2.5-Pro（0.60）在内的专有模型，同时节省约60%的GPU小时消耗。

Conclusion: 探索成本而非分布不匹配是LLM在非语言环境中性能受限的主要瓶颈。SCOUT框架通过轻量级探索代理与LLM的协作，有效解决了这一问题，在提升性能的同时大幅降低了计算成本。

Abstract: While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight "scouts" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.

</details>


### [82] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: ZSSD是一个零样本统计降尺度框架，无需配对训练数据，通过物理一致的气候先验和统一坐标引导解决GCMs泛化问题


<details>
  <summary>Details</summary>
Motivation: 传统监督式气候降尺度方法因缺乏配对训练数据和与再分析数据的领域差距而难以泛化到全球气候模型；现有零样本方法存在物理不一致性和大尺度因子下的梯度消失问题

Method: 提出零样本统计降尺度框架，利用从再分析数据学习的物理一致气候先验，结合地球物理边界和时序信息确保物理有效性；引入统一坐标引导策略解决梯度消失问题，确保与大尺度场的一致性

Result: ZSSD在99百分位误差上显著优于现有零样本基线，成功重建复杂天气事件（如热带气旋），并在异构GCMs上表现良好

Conclusion: ZSSD为零样本气候降尺度提供了一种有效的解决方案，解决了传统方法的泛化问题和现有零样本方法的物理不一致性挑战

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [83] [Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies](https://arxiv.org/abs/2601.21771)
*Hadi Banaee,Stephanie Lowry*

Main category: cs.AI

TL;DR: 提出一个基于概念空间的框架，用于建模随时间展开的抽象概念，以国际象棋为概念验证，将策略概念表示为可解释质量维度上的几何区域，通过游戏轨迹分析识别策略意图。


<details>
  <summary>Details</summary>
Motivation: 扩展概念空间理论以处理随时间实现、目标导向的抽象概念，为涉及序列决策的更广泛应用奠定基础，并支持与知识演化机制集成以学习和精炼抽象概念。

Method: 将策略概念（如攻击或牺牲）表示为可解释质量维度上的几何区域，将国际象棋游戏实例化为轨迹，通过轨迹在概念空间中的方向性运动来识别策略意图，支持双视角建模以捕捉玩家对相同情况的不同解释。

Result: 实现证明了基于轨迹的概念识别的可行性，运动模式与专家评论一致，展示了该方法能够有效识别国际象棋中的策略概念。

Conclusion: 该工作为建模随时间展开的抽象概念提供了概念空间框架，建立了序列决策应用的基础，并支持与知识演化机制集成以持续学习和完善概念。

Abstract: We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.

</details>


### [84] [BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics](https://arxiv.org/abs/2601.21800)
*Dionizije Fa,Marko Čuljak,Bruno Pandža,Mateo Čupić*

Main category: cs.AI

TL;DR: BioAgent Bench：一个用于评估AI代理在生物信息学任务中性能和鲁棒性的基准数据集与评估套件，包含端到端任务和自动化评估，测试了前沿模型在扰动下的表现。


<details>
  <summary>Details</summary>
Motivation: 需要评估AI代理在生物信息学任务中的实际表现和鲁棒性，特别是在处理复杂多步骤流程时的能力，以及考虑隐私约束下开源与闭源模型的适用性。

Method: 创建包含RNA-seq、变异检测、宏基因组学等端到端任务的基准数据集，设计具体输出要求支持自动化评估，引入受控扰动（损坏输入、诱饵文件、提示膨胀）进行压力测试，使用LLM评分器评估流程进展和结果有效性。

Result: 前沿代理能够完成多步骤生物信息学流程而无需复杂定制框架，通常能可靠生成最终输出。但鲁棒性测试显示在受控扰动下存在失败模式，表明高层流程构建正确不能保证步骤级推理的可靠性。在隐私约束下，开源模型虽完成率较低但比闭源模型更合适。

Conclusion: AI代理能够处理复杂生物信息学任务，但在鲁棒性方面仍需改进。隐私考虑使得开源模型在某些场景下更具优势。公开数据集和评估套件有助于推动该领域发展。

Abstract: This paper introduces BioAgent Bench, a benchmark dataset and an evaluation suite designed for measuring the performance and robustness of AI agents in common bioinformatics tasks. The benchmark contains curated end-to-end tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts that specify concrete output artifacts to support automated assessment, including stress testing under controlled perturbations. We evaluate frontier closed-source and open-weight models across multiple agent harnesses, and use an LLM-based grader to score pipeline progress and outcome validity. We find that frontier agents can complete multi-step bioinformatics pipelines without elaborate custom scaffolding, often producing the requested final artifacts reliably. However, robustness tests reveal failure modes under controlled perturbations (corrupted inputs, decoy files, and prompt bloat), indicating that correct high-level pipeline construction does not guarantee reliable step-level reasoning. Finally, because bioinformatics workflows may involve sensitive patient data, proprietary references, or unpublished IP, closed-source models can be unsuitable under strict privacy constraints; in such settings, open-weight models may be preferable despite lower completion rates. We release the dataset and evaluation suite publicly.

</details>


### [85] [A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition](https://arxiv.org/abs/2601.21802)
*Hoang Khang Phan,Quang Vinh Dang,Noriyo Colley,Christina Garcia,Nhat Tan Le*

Main category: cs.AI

TL;DR: 提出基于大语言模型的统一框架，用于气管内吸痰训练的视频活动识别和反馈生成，相比传统方法提升15-20%准确率


<details>
  <summary>Details</summary>
Motivation: 气管内吸痰是高风险临床操作，需要高度技能，但在家庭护理和教育环境中缺乏有效监督和自动反馈系统

Method: 构建以LLM为中心的统一框架，将大语言模型作为核心推理模块，执行视频数据的时空活动识别和可解释决策分析，并生成自然语言反馈

Result: LLM方法优于基线模型，准确率和F1分数提升约15-20%，并开发了基于异常检测和可解释AI原理的学生支持模块

Conclusion: 该框架为护理教育提供了可扩展、可解释、数据驱动的基础，能提高培训效率和患者安全

Abstract: Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.

</details>


### [86] [CORE:Toward Ubiquitous 6G Intelligence Through Collaborative Orchestration of Large Language Model Agents Over Hierarchical Edge](https://arxiv.org/abs/2601.21822)
*Zitong Yu,Boquan Sun,Yang Li,Zheyan Qu,Xing Zhang*

Main category: cs.AI

TL;DR: CORE框架通过分布式LLM角色协作和优化模块，解决6G网络中异构计算资源碎片化问题，提升复杂推理任务效率


<details>
  <summary>Details</summary>
Motivation: 6G网络和LLM发展为泛在智能提供了可能，但分层网络中碎片化、异构的计算资源限制了单个LLM代理执行复杂推理任务的能力，需要解决资源协调问题

Method: 提出CORE框架：1) 在移动设备和边缘服务器上分布具有不同功能角色的多个LLM；2) 集成实时感知、动态角色编排和流水线并行执行三个优化模块；3) 引入角色亲和度调度算法动态编排LLM角色分配

Result: 在多种6G应用场景下的综合案例研究和性能评估显示，CORE显著提升了系统效率和任务完成率，并在真实边缘计算平台上部署验证了其实际适用性和鲁棒性能

Conclusion: CORE框架通过分布式LLM角色协作和智能资源匹配，有效解决了6G网络中异构计算资源的协调问题，为实现泛在智能提供了可行的技术方案

Abstract: Rapid advancements in sixth-generation (6G) networks and large language models (LLMs) have paved the way for ubiquitous intelligence, wherein seamless connectivity and distributed artificial intelligence (AI) have revolutionized various aspects of our lives.However, realizing this vision faces significant challenges owing to the fragmented and heterogeneous computing resources across hierarchical networks, which are insufficient for individual LLM agents to perform complex reasoning tasks.To address this issue, we propose Collaborative Orchestration Role at Edge (CORE), an innovative framework that employs a collaborative learning system in which multiple LLMs, each assigned a distinct functional role, are distributed across mobile devices and tiered edge servers. The system integrates three optimization modules, encompassing real-time perception,dynamic role orchestration, and pipeline-parallel execution, to facilitate efficient and rapid collaboration among distributed agents. Furthermore, we introduce a novel role affinity scheduling algorithm for dynamically orchestrating LLM role assignments across the hierarchical edge infrastructure, intelligently matching computational demands with available dispersed resources.Finally, comprehensive case studies and performance evaluations across various 6G application scenarios demonstrated the efficacy of CORE, revealing significant enhancements in the system efficiency and task completion rates. Building on these promising outcomes, we further validated the practical applicability of CORE by deploying it on a real-world edge-computing platform,that exhibits robust performance in operational environments.

</details>


### [87] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 提出一个全面的心电专家基础模型基准测试框架，结合性能评估和表示层分析，评估模型在不同数据集和数据稀缺情况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 心电是成本效益高、可及性强的诊断工具。基础模型在AI辅助心电解读领域开始发展，但需要严格评估其嵌入表示在医疗等敏感领域的泛化能力。现有基准测试主要关注下游性能评估，缺乏全面的表示层分析。

Method: 提出基准测试方法学，结合基于性能的评估和表示层分析，利用SHAP和UMAP技术。对多个通过最先进技术预训练的心电专家基础模型进行广泛评估，涵盖不同跨大陆数据集和数据可用性设置（包括数据稀缺情况）。

Result: 实验结果表明，该基准测试协议能够深入洞察心电专家基础模型的嵌入模式，更好地理解其表示结构和泛化能力。

Conclusion: 提出的基准测试框架为心电专家基础模型提供了全面的评估方法，特别关注表示层分析和数据稀缺情况下的泛化能力评估，有助于负责任地部署医疗AI模型。

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [88] [Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework](https://arxiv.org/abs/2601.21844)
*So Fukuhara,Abdallah Alabdallah,Nuwan Gunasekara,Slawomir Nowaczyk*

Main category: cs.AI

TL;DR: 提出一个决策中心仿真框架，将预测模型评估从统计准确性转向对库存管理关键绩效指标的实际影响


<details>
  <summary>Details</summary>
Motivation: 汽车售后市场备件库存管理面临需求高度间歇性和不确定性的挑战，现有研究仅用统计准确性指标评估预测模型，但这些指标与库存运营绩效指标的关系不明确

Method: 提出决策中心仿真软件框架，包含：1) 针对备件需求特征的合成需求生成器；2) 可容纳任意预测模型的灵活预测模块；3) 消耗预测并计算运营KPI的库存控制模拟器

Result: 通过广泛仿真场景显示：传统准确性指标的改进不一定带来更好的运营绩效；统计误差相似的模型可能导致显著不同的成本-服务水平权衡

Conclusion: 该框架将需求预测与库存管理联系起来，将评估重点从纯粹的预测准确性转向运营相关性，为模型选择提供指导

Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.

</details>


### [89] [KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement](https://arxiv.org/abs/2601.21864)
*Jinhao Pan,Chahat Raj,Anjishnu Mukherjee,Sina Mansouri,Bowen Wei,Shloka Yada,Ziwei Zhu*

Main category: cs.AI

TL;DR: KnowBias提出了一种轻量级去偏框架，通过增强而非抑制编码偏见知识的神经元来减少LLM中的社会偏见，仅需少量是/否问题即可实现强去偏效果且保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM去偏方法大多采用抑制范式（修改参数、提示或神经元），但这些方法存在脆弱性、泛化性差、数据效率低且容易损害模型通用能力的问题。

Method: 通过基于归因的分析，使用少量偏见知识问题识别编码偏见知识的神经元，然后在推理时选择性地增强这些神经元，而非抑制它们。

Result: 在多个基准测试和LLM上的实验表明，KnowBias实现了最先进的去偏性能，同时保持了最小化的通用能力下降，且能跨偏见类型和人口统计特征泛化。

Conclusion: KnowBias提供了一种轻量级、数据高效的去偏方法，通过增强偏见知识神经元而非抑制它们，在有效减少偏见的同时保持了模型的通用能力。

Abstract: Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.

</details>


### [90] [WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents](https://arxiv.org/abs/2601.21872)
*Yao Zhang,Shijie Tang,Zeyu Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: WebArbiter：基于推理的网页过程奖励模型，通过生成结构化理由和偏好判决来指导网页导航任务，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 网页代理在自动化复杂计算机任务时面临长视野、顺序决策问题，现有过程奖励模型存在局限性：标量模型信号粗糙，清单式模型依赖脆弱的模板匹配且缺乏可解释性。

Method: 提出WebArbiter，将奖励建模转化为文本生成，生成结构化理由并给出偏好判决；采用两阶段训练：推理蒸馏赋予模型原则指导的推理能力，强化学习纠正教师偏见。

Result: 在WebPRMBench基准上，WebArbiter-7B比最强基线GPT-5高出9.1分；在WebArena-Lite的奖励引导轨迹搜索中，比之前最佳WebPRM高出最多7.2分。

Conclusion: WebArbiter通过推理优先的方法显著提升了网页过程奖励模型的性能、鲁棒性和可解释性，在复杂网页任务中具有实际应用价值。

Abstract: Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.

</details>


### [91] [astra-langchain4j: Experiences Combining LLMs and Agent Programming](https://arxiv.org/abs/2601.21879)
*Rem Collier,Katharine Beaumont,Andrei Ciortea*

Main category: cs.AI

TL;DR: 本文介绍了将大型语言模型集成到ASTRA编程语言中的原型开发经验，探讨了生成式AI和代理AI如何影响传统代理工具包，以及传统工具包经验如何影响新代理平台设计。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和代理AI（作为多智能体系统的一种形式）的兴起，需要探索这些技术如何影响传统代理工具包的使用，以及传统工具包中丰富的经验如何影响新代理平台的设计。

Method: 开发了一个将大型语言模型集成到ASTRA编程语言中的原型，通过三个示例实现来展示集成方法，并基于这些示例讨论获得的经验。

Result: 成功实现了LLM与ASTRA编程语言的集成原型，通过三个具体示例展示了集成的可行性和应用场景，获得了关于新旧技术融合的实践经验。

Conclusion: 传统代理工具包的经验对设计新的代理平台具有重要价值，LLM与现有编程语言的集成是可行的，这种融合能够促进新旧技术的协同发展。

Abstract: Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.

</details>


### [92] [Making Models Unmergeable via Scaling-Sensitive Loss Landscape](https://arxiv.org/abs/2601.21898)
*Minwoo Jang,Hoyoung Kim,Jabin Koo,Jungseul Ok*

Main category: cs.AI

TL;DR: TRAP²是一个架构无关的保护框架，通过在微调期间将保护编码到更新中，防止模型权重被未经授权地合并，无论发布的是适配器还是完整模型。


<details>
  <summary>Details</summary>
Motivation: 模型中心的兴起使得模型合并成为重用模型组件的实用工具，但这种模块化也带来了治理缺口：下游用户可以将发布的权重重新组合成未经授权的混合模型，绕过安全对齐或许可条款。现有防御措施大多是事后且架构特定的，在实际应用中无法为不同架构和发布格式提供一致保护。

Method: TRAP²使用权重重新缩放作为合并过程的简单代理。它在微调期间将保护编码到更新中，保持发布权重在单独使用时有效，但在合并过程中常见的重新缩放情况下会使其性能下降，从而破坏未经授权的合并。

Result: 该方法架构无关，无论模型以适配器还是完整模型形式发布都能提供保护，解决了现有防御措施在实践中的不一致性问题。

Conclusion: TRAP²通过将保护嵌入到权重更新中，为模型发布提供了一种有效的治理机制，填补了模型重用中的治理缺口，防止未经授权的模型合并。

Abstract: The rise of model hubs has made it easier to access reusable model components, making model merging a practical tool for combining capabilities. Yet, this modularity also creates a \emph{governance gap}: downstream users can recompose released weights into unauthorized mixtures that bypass safety alignment or licensing terms. Because existing defenses are largely post-hoc and architecture-specific, they provide inconsistent protection across diverse architectures and release formats in practice. To close this gap, we propose \textsc{Trap}$^{2}$, an architecture-agnostic protection framework that encodes protection into the update during fine-tuning, regardless of whether they are released as adapters or full models. Instead of relying on architecture-dependent approaches, \textsc{Trap}$^{2}$ uses weight re-scaling as a simple proxy for the merging process. It keeps released weights effective in standalone use, but degrades them under re-scaling that often arises in merging, undermining unauthorized merging.

</details>


### [93] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: CoMT+CCRL：受人类认知启发的两阶段LLM后训练框架，先学习抽象推理模式，再通过置信度校准强化学习进行任务适配，显著提升泛化能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练方法（SFT+RL）将完整推理轨迹作为基本单元，这与人类解决问题的认知过程存在根本差异。人类认知自然地将问题解决分解为两个阶段：先获取跨问题泛化的抽象策略（元知识），再将其适配到具体实例。现有方法的问题中心化导致抽象策略与问题特定执行纠缠在一起。

Method: 提出认知启发的两阶段框架：1) Chain-of-Meta-Thought (CoMT)：专注于抽象推理模式的有监督学习，不涉及具体执行，获取可泛化的策略；2) Confidence-Calibrated Reinforcement Learning (CCRL)：通过中间步骤的置信度感知奖励优化任务适配，防止过度自信的错误级联，提高执行可靠性。

Result: 在4个模型和8个基准测试中，相比标准方法，分布内性能提升2.19%，分布外性能提升4.63%，同时训练时间减少65-70%，token消耗减少50%。

Conclusion: 将后训练与人类认知原则对齐不仅能产生更优的泛化能力，还能显著提升训练效率，证明了认知启发的AI训练方法的有效性。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [94] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: ProRAG：一种过程监督强化学习框架，通过整合步骤级监督来优化检索增强生成（RAG）在复杂推理任务中的性能，解决了传统结果导向RL的奖励稀疏性和信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法在优化RAG时面临奖励稀疏和信用分配效率低下的问题，粗粒度的标量奖励无法识别长轨迹中的具体错误步骤，导致"过程幻觉"——模型通过错误逻辑或冗余检索步骤得到正确答案。现有的过程感知方法缺乏在线探索能力，无法将步骤级信用与全局结果解耦。

Method: ProRAG包含四个阶段：1) 监督策略预热，用结构化推理格式初始化模型；2) 构建基于MCTS的过程奖励模型（PRM）来量化中间推理质量；3) PRM引导的推理细化，使策略与细粒度过程偏好对齐；4) 过程监督强化学习，采用双粒度优势机制，聚合步骤级过程奖励与全局结果信号。

Result: 在五个多跳推理基准测试上的广泛实验表明，ProRAG相比强大的基于结果和过程感知的RL基线方法实现了更优的整体性能，特别是在复杂长视野任务上，验证了细粒度过程监督的有效性。

Conclusion: ProRAG通过将学习到的步骤级监督整合到在线优化循环中，有效解决了RAG优化中的奖励稀疏和信用分配问题，为复杂推理任务提供了精确的反馈机制，显著提升了长视野任务的性能。

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [95] [JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG](https://arxiv.org/abs/2601.21916)
*Yiqun Chen,Erhan Zhang,Tianyi Hu,Shijie Wang,Zixuan Yang,Meizhi Zhong,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Jiaxin Mao*

Main category: cs.AI

TL;DR: JADE框架通过联合优化动态多轮工作流中的规划与执行，解决了现有RAG系统中规划与执行模块分离优化导致的"战略-操作不匹配"问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统面临关键二分法：要么在固定架构中联合优化模块，要么支持动态规划但将执行器视为冻结的黑盒工具。这种"分离优化"导致"战略-操作不匹配"——复杂的规划策略因未适应的本地执行器而无法实现，系统复杂度增加却带来负面性能增益。

Method: 提出JADE框架，将系统建模为在单一共享主干下统一的多智能体协作团队，通过基于结果的奖励进行端到端学习，实现规划与执行的联合优化。支持"协同适应"：规划器学习在执行器能力边界内操作，执行器则演进以符合高层战略意图。

Result: 实证结果表明，JADE将分离的模块转变为协同系统，通过联合优化带来显著性能提升，并通过动态工作流编排实现效率与效果之间的灵活平衡。

Conclusion: JADE框架解决了RAG系统中规划与执行分离优化的问题，通过联合优化和协同适应机制，实现了动态多轮工作流中规划与执行的协同演进，显著提升了系统性能。

Abstract: The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \textbf{JADE} (\textbf{J}oint \textbf{A}gentic \textbf{D}ynamic \textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.

</details>


### [96] [Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21919)
*Yiqun Chen,Jinyuan Feng,Wei Yang,Meizhi Zhong,Zhengliang Shi,Rui Li,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Zhiqiang Pu,Jiaxin Mao*

Main category: cs.AI

TL;DR: 提出SCMA多智能体强化学习框架，通过分割和评分智能体选择性惩罚冗余推理块，在减少响应长度11.1%-39.0%的同时提升准确率4.33%-10.02%


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的解决方案通过简单结合长度惩罚和结果奖励来处理推理冗余问题，但这种方法难以平衡简洁性和准确性，强制简洁可能损害关键推理逻辑

Method: 提出SCMA多智能体强化学习框架，包含三个智能体：分割智能体将推理过程分解为逻辑块，评分智能体量化每个块的重要性，推理智能体在训练中接受重要性加权的长度惩罚激励

Result: 实验表明SCMA能减少响应长度11.1%-39.0%，同时提升准确率4.33%-10.02%。消融研究和定性分析验证了MARL框架内的协同优化能产生更强大的大型推理模型

Conclusion: SCMA框架通过多智能体协同优化，选择性惩罚冗余推理块而保留关键逻辑，有效解决了推理冗余问题，在保持准确性的同时显著减少推理开销

Abstract: The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.

</details>


### [97] [AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making](https://arxiv.org/abs/2601.21936)
*Jon Chun,Kathrine Elkins,Yong Suk Lee*

Main category: cs.AI

TL;DR: AgenticSimLaw是一个基于法庭辩论结构的多智能体框架，通过定义明确的角色（检察官、辩护律师、法官）和7轮结构化辩论，为高风险表格决策任务提供透明可控的推理过程。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒方法在高风险决策任务中缺乏透明度和可审计性的问题，特别是在刑事司法等敏感领域，需要人类监督和可解释的推理过程。

Method: 采用法庭辩论风格的多智能体框架，定义三种角色（检察官、辩护律师、法官），通过7轮结构化辩论进行交互，每个智能体使用私有推理策略，生成完整的交互记录供审计。

Result: 在NLSY97数据集上的年轻成人再犯预测任务中，相比传统链式思维提示，结构化多智能体辩论展现出更稳定和可泛化的性能，准确率和F1分数相关性更强。

Conclusion: AgenticSimLaw不仅提升了性能，还提供了对推理步骤的细粒度控制、完整的可解释性交互记录和智能体行为分析能力，适用于任何需要透明度和人类监督的高风险决策任务。

Abstract: We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.

</details>


### [98] [Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities](https://arxiv.org/abs/2601.21937)
*Shuangshuang Ying,Zheyu Wang,Yunjian Peng,Jin Chen,Yuhao Wu,Hongbin Lin,Dingyu He,Siyi Liu,Gengchen Yu,YinZhu Piao,Yuchen Wu,Xin Gui,Zhongyuan Peng,Xin Li,Xeron Du,Libo Qin,YiXin Cao,Ge Zhang*

Main category: cs.AI

TL;DR: DeR2是一个用于评估大语言模型在科学推理能力上的受控测试平台，通过解耦证据获取与推理过程，隔离文档基础推理能力，避免检索和参数记忆的干扰。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法准确评估大语言模型对新颖科学信息的推理能力，因为端到端RAG管道中的推理与检索、工具链选择混淆，且受到参数记忆和网络数据波动的污染。

Method: 设计DeR2深度研究沙盒，通过四种证据获取机制（仅指令、概念、仅相关文档、完整集）解耦证据访问与推理。采用两阶段验证防止参数泄漏，使用2023-2025年理论论文构建冻结文档库，包含专家标注的概念和验证的推理过程。

Result: 实验显示最先进的基础模型存在显著差异和提升空间：部分模型表现出模式切换脆弱性（完整集表现比仅指令更差），其他模型存在结构性概念误用（正确命名概念但无法作为程序执行）。

Conclusion: DeR2提供了一个受控环境来评估大语言模型的文档基础推理能力，揭示了模型在科学推理方面的系统性缺陷，为未来模型改进提供了细粒度诊断工具。

Abstract: Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.

</details>


### [99] [ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models](https://arxiv.org/abs/2601.21947)
*Bowen Fang,Wen Ye,Yunyue Su,Jinghao Zhang,Qiang Liu,Yesheng Liu,Xin Sun,Shu Wu,Jiabing Yang,Baole Wei,Liang Wang*

Main category: cs.AI

TL;DR: ToolWeaver提出了一种新的生成式工具学习框架，通过将工具编码为分层序列来解决现有方法的语义和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有检索式工具使用管道面临双重语义挑战：检索器编码器难以捕捉复杂语义，而LLM本身缺乏工具知识。生成式方法虽然统一了选择和执行，但将每个工具映射到唯一新token会导致可扩展性危机和语义瓶颈。

Method: ToolWeaver将工具编码为分层序列，使词汇表扩展与工具数量呈对数关系。通过新颖的标记化过程，将工具的内在语义与外部使用模式编织在一起，然后通过生成对齐阶段将结构化代码集成到LLM中。

Result: 在近47,000个工具上的评估结果显示，ToolWeaver显著优于最先进的方法，为高级工具增强代理建立了更可扩展、可泛化和语义感知的基础。

Conclusion: ToolWeaver通过分层序列编码解决了生成式工具学习的可扩展性和语义瓶颈问题，为工具学习提供了更有效的框架。

Abstract: Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.

</details>


### [100] [How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors](https://arxiv.org/abs/2601.21961)
*Kuai Yu,Naicheng Yu,Han Wang,Rui Yang,Huan Zhang*

Main category: cs.AI

TL;DR: VAF是一个评估网页视觉属性如何影响网络代理决策的框架，通过生成语义相同但视觉属性不同的变体，测量代理点击率和提及率的变化，发现背景颜色对比度、项目大小、位置和卡片清晰度对代理行为影响最大。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注对抗攻击下的鲁棒性，而较少关注良性场景中代理的偏好。虽然早期研究考察了文本属性对代理行为的影响，但对视觉属性如何塑造代理决策的系统性理解仍然有限。

Method: VAF包含三个阶段：1）变体生成 - 确保变体与原始项目语义相同，仅视觉属性不同；2）浏览交互 - 代理通过滚动和点击浏览页面，模拟人类用户行为；3）验证 - 通过点击动作和推理，使用目标点击率和目标提及率共同评估视觉属性的影响。

Result: 在8个变体家族（共48个变体）、5个真实网站（购物、旅游、新闻浏览）和4个代表性网络代理的广泛实验中，发现背景颜色对比度、项目大小、位置和卡片清晰度对代理行为有强烈影响，而字体样式、文本颜色和项目图像清晰度影响较小。

Conclusion: 视觉属性显著影响网络代理的决策过程，某些视觉特征（如对比度、大小、位置）比文本相关属性对代理行为的影响更大，这为理解和改进网络代理设计提供了重要见解。

Abstract: Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.

</details>


### [101] [The Energy Impact of Domain Model Design in Classical Planning](https://arxiv.org/abs/2601.21967)
*Ilche Georgievski,Serhat Tekin,Marco Aiello*

Main category: cs.AI

TL;DR: 该研究提出绿色AI视角，通过领域模型配置框架分析经典规划器能耗，发现领域级修改能产生可测量的能耗差异，且能耗与运行时间并不总是相关。


<details>
  <summary>Details</summary>
Motivation: 传统AI研究优先考虑算法性能（如机器学习准确率或自动规划运行时间），而新兴的绿色AI范式将能耗视为关键性能维度。尽管自动规划计算需求高，但其能效研究很少，特别是考虑到模块化规划结构中领域模型与算法分离的特点，这为通过领域模型设计系统分析能耗提供了机会。

Method: 引入领域模型配置框架，允许控制特征变化（如元素排序、行动元数、死锁状态）。使用5个基准领域和5个最先进的规划器，分析每个基准32个领域变体的能耗和运行时间影响。

Result: 结果表明，领域级修改在不同规划器中产生可测量的能耗差异，且能耗消耗与运行时间并不总是相关。

Conclusion: 该研究强调了在自动规划中考虑能耗的重要性，展示了通过领域模型设计优化能效的潜力，为绿色AI在规划领域的应用提供了实证基础。

Abstract: AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.

</details>


### [102] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 该论文提出两种多智能体演员-评论家方法（CoLLM-CC和CoLLM-DC）来优化去中心化LLM协作，解决了传统蒙特卡洛方法方差高、样本效率低的问题，在长视野或稀疏奖励任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于MARL的LLM协作优化方法大多依赖预定义执行协议，需要集中式执行，而实际应用中更希望去中心化并行推理。同时，当前方法使用蒙特卡洛方法进行微调，存在方差高、样本效率低的问题。

Method: 提出两种多智能体演员-评论家方法：CoLLM-CC（集中式评论家）和CoLLM-DC（去中心化评论家），用于优化去中心化LLM协作，解决传统蒙特卡洛方法的问题。

Result: 在写作、编程和游戏领域实验中，蒙特卡洛方法和CoLLM-DC在短视野和密集奖励任务中能达到与CoLLM-CC相当的性能，但在长视野或稀疏奖励任务中，两者都表现不佳：蒙特卡洛方法需要更多样本，CoLLM-DC难以收敛。

Conclusion: 多智能体演员-评论家方法（特别是集中式评论家的CoLLM-CC）在优化去中心化LLM协作方面具有优势，特别是在长视野或稀疏奖励的复杂任务中，能显著提升训练效率和性能。

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [103] [Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models](https://arxiv.org/abs/2601.21975)
*Pranav Mahajan,Ihor Kendiukhov,Syed Hussain,Lydia Nottingham*

Main category: cs.AI

TL;DR: 语言模型的陈述-揭示偏好差距高度依赖评估协议，允许中立和弃权能改善相关性，但完全允许弃权会使相关性降至零或负值。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要依赖二元强制选择提示，这混淆了真实偏好与评估协议的人为因素。需要系统研究评估协议如何影响语言模型的陈述-揭示偏好相关性。

Method: 系统研究24个语言模型，通过允许中立和弃权来改进陈述偏好评估，并测试系统提示引导对AIRiskDilemmas数据集的影响。

Result: 允许中立和弃权能排除弱信号，显著改善陈述偏好与强制选择揭示偏好的相关性。但进一步允许揭示偏好中的弃权会导致高中立率，使相关性降至零或负值。系统提示引导不能可靠改善相关性。

Conclusion: 陈述-揭示偏好相关性高度依赖评估协议，偏好评估需要能够处理不确定偏好的方法。

Abstract: Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.

</details>


### [104] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: VERSA是一个用于足球事件数据验证的系统框架，能自动检测和纠正事件流中的逻辑不一致问题，显著提升数据质量和下游分析可靠性。


<details>
  <summary>Details</summary>
Motivation: 事件流数据在体育分析等领域至关重要，但现有数据存在逻辑不一致问题（如事件顺序错误、事件缺失），这限制了分析模型的可靠性。在足球领域，这些问题影响了球员贡献评估和战术模式识别等精细分析的准确性。

Method: 提出VERSA验证框架，基于状态转移模型定义有效事件序列，能够自动检测和纠正事件流数据中的异常模式。该框架系统性地确保足球事件数据的完整性。

Result: 在K League 1（2024赛季）的Bepro数据中检测到18.81%的事件存在逻辑不一致。VERSA显著提高了跨数据提供商的一致性，确保异构数据源的稳定统一表示。经过VERSA处理的数据显著提升了VAEP（评估球员贡献的下游任务）的鲁棒性和性能。

Conclusion: 验证过程能有效提高数据驱动分析的可靠性。VERSA框架为解决事件流数据质量问题提供了系统化解决方案，对提升体育分析等领域的精细分析准确性具有重要意义。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [105] [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](https://arxiv.org/abs/2601.21993)
*Dhiogo de Sá,Carlos Schmiedel,Carlos Pereira Lopes*

Main category: cs.AI

TL;DR: 提出Liquid Interfaces（液态接口）协调范式，将接口从静态技术制品转变为运行时通过意图表达和语义协商产生的短暂关系事件，以支持自适应、概率性和上下文相关的自主代理系统。


<details>
  <summary>Details</summary>
Motivation: 当前软件架构难以支持自适应、概率性和上下文相关的自主代理，系统集成仍被静态接口和确定性契约主导，需要新的协调范式来应对语义不确定性。

Method: 提出Liquid Interfaces协调范式，形式化该模型并设计Liquid Interface Protocol（LIP），管理意图驱动交互、协商执行和在语义不确定性下的短暂性执行，同时讨论治理影响并描述参考架构。

Result: Liquid Interfaces为基于代理的系统中的自适应协调提供了原则性基础，通过参考架构展示了实际可行性。

Conclusion: Liquid Interfaces通过将接口重新构想为短暂的关系事件而非持久技术制品，为支持自适应、概率性和上下文相关自主代理的协调提供了创新解决方案。

Abstract: Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems

</details>


### [106] [Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference](https://arxiv.org/abs/2601.22001)
*Yiren Zhao,Junyi Liu*

Main category: cs.AI

TL;DR: 论文提出两个新指标OI和CF来分析AI代理推理中的内存瓶颈，指出长上下文KV缓存导致解码阶段高度内存受限，提出解耦服务、异构系统架构和光I/O等解决方案。


<details>
  <summary>Details</summary>
Motivation: AI代理推理正在推动数据中心向推理密集型发展，暴露出计算之外的新瓶颈，特别是内存容量、内存带宽和高速互连。传统屋顶线分析无法捕捉这些新瓶颈，需要新的分析框架。

Method: 引入两个新指标：操作强度（OI）和容量足迹（CF），用于分析不同代理工作流和基础模型的内存瓶颈。通过分析聊天、编码、网页使用、计算机使用等工作流，以及GQA/MLA、MoE、量化等模型选择，观察OI/CF的变化规律。

Result: 发现长上下文KV缓存使解码阶段高度内存受限，OI/CF在不同工作流和模型配置下变化显著。这些观察揭示了传统架构的局限性，需要新的系统设计。

Conclusion: 提出解耦服务、系统级异构架构、专用预填充和解码加速器、扩展网络规模、光I/O实现的计算-内存解耦等方案。建议代理-硬件协同设计、单系统多推理加速器、高带宽大容量内存解耦作为适应OI/CF演变的基础。

Abstract: AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.

</details>


### [107] [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://arxiv.org/abs/2601.22027)
*Johannes Kirmayr,Lukas Stappen,Elisabeth André*

Main category: cs.AI

TL;DR: CAR-bench是一个用于评估车载语音助手场景下LLM智能体一致性、不确定性处理和能力意识的基准测试，包含幻觉任务和消歧任务，揭示现有模型在真实世界应用中的可靠性不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体基准测试主要关注理想环境下的任务完成，忽略了真实用户场景中的可靠性问题。在车载语音助手等应用中，用户经常发出不完整或模糊的请求，智能体需要通过对话、工具使用和政策遵守来管理这种内在不确定性。

Method: 引入CAR-bench基准测试，包含LLM模拟用户、领域政策和58个相互关联的工具（导航、生产力、充电、车辆控制）。除了标准任务完成外，还设计了幻觉任务（测试在工具或信息缺失时的极限意识）和消歧任务（要求通过澄清或内部信息收集解决不确定性）。

Result: 基线结果显示，所有任务类型上都存在偶尔成功和持续成功之间的巨大差距。即使是前沿推理LLM在消歧任务上的持续通过率也低于50%（由于过早行动），在幻觉任务中经常违反政策或编造信息以满足用户请求。

Conclusion: CAR-bench揭示了现有LLM智能体在真实世界应用中的可靠性不足，强调需要开发更可靠、更具自我意识的LLM智能体来处理用户不确定性并遵守领域政策。

Abstract: Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.

</details>


### [108] [Optimizing Agentic Workflows using Meta-tools](https://arxiv.org/abs/2601.22037)
*Sami Abuzakuk,Anne-Marie Kermarrec,Rishi Sharma,Rasmus Moorits Veski,Martijn de Vos*

Main category: cs.AI

TL;DR: AWO框架通过识别和优化代理工作流中的冗余工具执行模式，将重复的工具调用序列转换为元工具，从而减少LLM调用次数并提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 代理式AI工作流通常需要多次迭代推理和工具调用，导致高昂的操作成本、端到端延迟以及因幻觉导致的失败，需要优化效率与鲁棒性。

Method: AWO框架分析现有工作流轨迹，发现重复的工具调用序列，将其转换为确定性、复合的元工具，将多个代理动作捆绑为单次调用，绕过不必要的中间LLM推理步骤。

Result: 在两个代理式AI基准测试中，AWO将LLM调用次数减少高达11.9%，同时将任务成功率提高高达4.2个百分点。

Conclusion: AWO通过优化代理工作流中的冗余工具执行模式，有效提高了代理式AI系统的效率和鲁棒性，减少了操作成本和失败率。

Abstract: Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.

</details>


### [109] [Defining Operational Conditions for Safety-Critical AI-Based Systems from Data](https://arxiv.org/abs/2601.22118)
*Johann Christensen,Elena Hoemann,Frank Köster,Sven Hallerbach*

Main category: cs.AI

TL;DR: 提出一种基于多维核表示的后验方法，从已有数据中定义运行设计域，用于数据驱动的安全关键AI系统认证


<details>
  <summary>Details</summary>
Motivation: 现实世界复杂系统中，为AI系统定义完整的环境条件（运行设计域）非常困难，但这是安全关键AI系统认证的必要条件。传统方法依赖专家知识在开发早期定义ODD，难以处理已有数据的情况。

Method: 提出安全设计方法，使用多维核表示从已有数据中后验定义ODD。通过蒙特卡洛方法和实际航空用例验证，并定义了判断两个ODD是否相等的条件。

Result: 方法验证表明，数据驱动的ODD可以等于数据中隐藏的原始ODD。该方法为数据驱动的安全关键AI系统认证提供了可行途径。

Conclusion: 基于安全设计的核方法ODD定义，能够支持未来数据驱动的安全关键AI系统认证，解决了传统方法难以处理已有数据的挑战。

Abstract: Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.

</details>


### [110] [The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR](https://arxiv.org/abs/2601.22128)
*Irsyad Adam,Zekai Chen,David Laprade,Shaun Porwal,David Laub,Erik Reinertsen,Arda Pekis,Kevin Brown*

Main category: cs.AI

TL;DR: SMB-Structure是一个用于结构化电子健康记录的世界模型，结合联合嵌入预测架构和下一个标记预测，能够模拟患者动态而非仅预测标记，在肿瘤学和肺栓塞患者队列中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于下一个标记预测的LLMs将患者视为需要总结的文档，而非需要模拟的动态系统。患者轨迹是由状态在干预和时间下的演化形成的，需要能够模拟动态而非仅预测标记的模型。

Method: 提出SMB-Structure模型，将联合嵌入预测架构与下一个标记预测相结合。SFT使模型能在标记空间中重建未来患者状态，而JEPA仅从初始患者表示预测潜在空间中的未来状态，强制在观察到下一个状态之前编码轨迹动态。

Result: 在两个大规模队列（MSK的23,319名肿瘤患者和INSPECT的19,402名肺栓塞患者）上验证。线性探针评估显示，该训练范式学习的嵌入能捕捉自回归基线无法恢复的疾病动态，在患者异质性高的复杂任务中表现优异。

Conclusion: SMB-Structure通过结合JEPA和SFT，成功模拟了患者动态系统，超越了传统自回归方法，为临床预测提供了更有效的表示学习框架。

Abstract: Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient's trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.

</details>


### [111] [World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems](https://arxiv.org/abs/2601.22130)
*Lakshya Gupta,Litao Li,Yizhe Liu,Sriram Ganapathi Subramanian,Kaheer Suleman,Zichen Zhang,Haoye Lu,Sumit Pasupalak*

Main category: cs.AI

TL;DR: 论文提出了World of Workflows (WoW)环境和企业级基准测试WoW-bench，揭示前沿LLM在企业系统中存在"动态盲区"问题，无法预测隐藏工作流的级联副作用，需要显式学习系统动态的新范式。


<details>
  <summary>Details</summary>
Motivation: 现有企业基准测试只评估表面任务完成度，忽略了企业系统的真实挑战：有限可观测性、大规模数据库状态、隐藏工作流的级联副作用。需要更真实的测试环境来评估LLM在企业系统中的表现。

Method: 构建基于ServiceNow的World of Workflows (WoW)环境，包含4000+业务规则和55个活跃工作流，并开发WoW-bench基准测试，包含234个任务，评估约束性代理任务完成度和企业动态建模能力。

Result: 前沿LLM存在"动态盲区"，无法预测行动的隐藏级联副作用，导致静默约束违规。在非透明系统中，可靠性需要基于基础的世界建模，通过心智模拟隐藏状态转换来弥补可观测性差距。

Conclusion: 可靠的企业级代理需要显式学习系统动态的新范式。WoW为此提供了测试环境，并开源GitHub供设置和评估。

Abstract: Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.

</details>


### [112] [Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data](https://arxiv.org/abs/2601.22141)
*Grzegorz Stefanski,Alberto Presta,Michal Byra*

Main category: cs.AI

TL;DR: RTL提出自适应剪枝框架，发现多个针对不同数据类别的专用子网络，相比传统单一彩票票券方法在平衡准确率和召回率上表现更好，参数更少。


<details>
  <summary>Details</summary>
Motivation: 传统彩票票券假设认为大网络包含稀疏子网络（获胜票券），但这些方法假设存在适用于所有输入的通用获胜票券，忽略了现实世界数据的异质性。

Method: 提出Routing the Lottery (RTL)自适应剪枝框架，发现多个专门针对类别、语义簇或环境条件的自适应票券（专用子网络）。

Result: 在多种数据集和任务上，RTL在平衡准确率和召回率上始终优于单模型和多模型基线，参数使用量比独立模型少10倍，且表现出语义对齐。还识别了子网络崩溃现象，并提出了子网络相似度评分用于无标签诊断过度稀疏化。

Conclusion: 将剪枝重新定义为将模型结构与数据异质性对齐的机制，为更模块化和上下文感知的深度学习铺平道路。

Abstract: In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.

</details>


### [113] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: 提出Agent Reasoning Reward Model (Agent-RRM)，为智能体轨迹提供结构化反馈，包括推理轨迹、针对性批判和整体评分，通过三种集成策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前Agentic RL方法主要依赖稀疏的结果奖励，无法区分中间推理质量，导致训练效果不佳。需要更细粒度的反馈机制来指导智能体学习。

Method: 提出Agent-RRM模型，提供三方面结构化反馈：显式推理轨迹、针对性批判（指出推理缺陷）、整体评分。研究三种集成策略：Reagent-C（文本增强精炼）、Reagent-R（奖励增强指导）、Reagent-U（统一反馈集成）。

Result: 在12个多样化基准测试中，Reagent-U表现最佳，在GAIA上达到43.7%，在WebWalkerQA上达到46.2%，验证了推理奖励模型和训练方案的有效性。

Conclusion: Agent-RRM通过提供结构化反馈显著提升了Agentic RL的性能，开源代码、模型和数据集以促进未来研究。

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [114] [Beyond Martingale Estimators: Structured Estimators for Maximizing Information Freshness in Query-Based Update Systems](https://arxiv.org/abs/2601.22098)
*Sahan Liyanaarachchi,Sennur Ulukus,Nail Akar*

Main category: cs.IT

TL;DR: 提出结构化估计器，可在马尔可夫链远程估计系统中平衡计算复杂度和估计精度，并推导最优查询策略。


<details>
  <summary>Details</summary>
Motivation: 现有远程估计系统主要使用鞅估计器，虽然简单但性能不佳；而最优的MAP估计器在连续时间场景下分析困难。需要一种既能保留MAP估计器优点又便于分析的新估计器。

Method: 提出结构化估计器，特别是p-MAP估计器，作为MAP估计器的分段常数近似。使用二进制新鲜度过程表征信息新鲜度，推导新鲜度表达式，并提供最优状态依赖采样策略。

Result: 对于时间可逆CTMC，MAP估计器可简化为p-MAP估计器。推导了结构化估计器下的新鲜度表达式，并提供了最大化平均二进制新鲜度的最优查询策略，包括单源和多源场景。

Conclusion: 结构化估计器在保持分析可处理性的同时，能够接近MAP估计器的性能，为连续时间马尔可夫链远程估计系统提供了实用的信息新鲜度优化框架。

Abstract: This paper investigates information freshness in a remote estimation system in which the remote information source is a continuous-time Markov chain (CTMC). For such systems, estimators have been mainly restricted to the class of martingale estimators in which the remote estimate at any time is equal to the value of the most recently received update. This is mainly due to the simplicity and ease of analysis of martingale estimators, which however are far from optimal, especially in query-based (i.e., pull-based) update systems. In such systems, maximum a-posteriori probability (MAP) estimators are optimal. However, MAP estimators can be challenging to analyze in continuous-time settings. In this paper, we introduce a new class of estimators, called structured estimators, which can seamlessly shift from a martingale estimator to a MAP estimator, enabling them to retain useful characteristics of the MAP estimate, while still being analytically tractable. Particularly, we introduce a new estimator termed as the $p$-MAP estimator which is a piecewise-constant approximation of the MAP estimator with finitely many discontinuities, bringing us closer to a full characterization of MAP estimators when modeling information freshness. In fact, we show that for time-reversible CTMCs, the MAP estimator reduces to a $p$-MAP estimator. Using the binary freshness (BF) process for the characterization of information freshness, we derive the freshness expressions and provide optimal state-dependent sampling policies (i.e., querying policies) for maximizing the mean BF (MBF) for pull-based remote estimation of a single CTMC information source, when structured estimators are used. Moreover, we provide optimal query rate allocation policies when a monitor pulls information from multiple heterogeneous CTMCs with a constraint on the overall query rate.

</details>


### [115] [The Noncomputability of Immune Reaction Complexity: Algorithmic Information Gaps under Effective Constraints](https://arxiv.org/abs/2601.20865)
*Emmanuel Pio Pastore,Francesco De Rango*

Main category: cs.IT

TL;DR: 该论文提出了一种基于算法信息论的验证过滤、证书导向的反应视图，定义了最小可行实现器复杂度M(x)和归一化建议分位数(NAQ)作为任务难度的尺度无关指标。


<details>
  <summary>Details</summary>
Motivation: 需要一种理论框架来量化任务难度，该框架应具有尺度无关性、对通用机选择鲁棒、且能跨任务族比较。传统方法缺乏对最小信息需求的精确度量。

Method: 采用算法信息论方法：固定输入无关的执行器将自定界建议字符串映射到候选响应，仅当可判定或半可判定的有效性谓词V(x,r)成立时才接受。定义M(x)=min_{r:V(x,r)=1} K(r)作为最小可行实现器复杂度，并引入归一化建议分位数(NAQ)作为难度指标。

Result: 建立了精确实现器恒等式，证明最小建议长度等于M(x)至多O(1)常数；在有限模糊度情况下M(x)≈min_y K(y)；NAQ具有准不变性；建立了与率失真理论的联系；扩展包括资源受限变体NAQ_t和NP风格设置；提供了DKW界保证经验NAQ估计的收敛性。

Conclusion: NAQ提供了一个鲁棒、尺度无关的任务难度度量框架，能够跨任务族比较，并通过压缩器代理实现数据驱动校准，为算法信息论在任务复杂度分析中的应用提供了理论基础。

Abstract: We introduce a validity-filtered, certificate-based view of reactions grounded in Algorithmic Information Theory. A fixed, total, input-blind executor maps a self-delimiting advice string to a candidate response, accepted only if a decidable or semi-decidable validity predicate V(x, r) holds. The minimum feasible realizer complexity M(x) = min_{r: V(x,r)=1} K(r), with K denoting prefix Kolmogorov complexity, measures the minimal information required for a valid outcome. We define the Normalized Advice Quantile (NAQ) as the percentile of M(x) across a reference pool, yielding a scale-free hardness index on [0, 1] robust to the choice of universal machine and comparable across task families. An Exact Realizer Identity shows that the minimal advice for any input-blind executor equals M(x) up to O(1), while a description plus selection upper bound refines it via computable feature maps, separating description cost K(y) from selection cost log i_y(x). In finite-ambiguity regimes M(x) approximately equals min_y K(y); in generic-fiber regimes the bound is tight. NAQ is quasi-invariant under bounded enumeration changes. An operational converse links NAQ to rate-distortion: communicating advice with error epsilon requires average length near the entropy of target features. Extensions include a resource-bounded variant NAQ_t incorporating time-penalized complexity (Levin's Kt) and an NP-style setting showing linear worst-case advice n - O(1). Finally, a DKW bound guarantees convergence of empirical NAQ estimates, enabling data-driven calibration via compressor-based proxies.

</details>


### [116] [Cramér-Rao Bound Analysis and Near-Optimal Performance of the Synchronous Nyquist-Folding Generalized Eigenvalue Method (SNGEM) for Sub-Nyquist Multi-Tone Parameter Estimation](https://arxiv.org/abs/2601.20866)
*Huiguang Zhang*

Main category: cs.IT

TL;DR: SNGEM方法在极端亚奈奎斯特率下实现多音信号的全频率/幅度/相位估计，通过联合处理原始信号及其时间导数，在噪声条件下接近CRB，优于传统压缩感知方法。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知方法（如OMP）在亚奈奎斯特采样下存在DFT网格偏差和混叠噪声导致的不可约误差，需要开发统计上接近最优的确定性亚奈奎斯特参数谱分析方法。

Method: 提出同步奈奎斯特折叠广义特征值方法（SNGEM），通过联合处理原始信号及其时间导数，在极端亚奈奎斯特率下实现多音信号的全参数估计，并推导了两通道等SNR条件下的精确克拉美-罗界。

Result: 蒙特卡洛模拟显示：SNGEM在无噪声条件下达到机器精度；在所有SNR水平下（包括10-20倍压缩）都接近推导的CRB；而传统OMP方法由于DFT网格偏差和混叠噪声存在不可约的误差平台。

Conclusion: SNGEM被确立为统计上接近最优的确定性亚奈奎斯特参数谱分析方法，在极端压缩比下仍能保持高精度，优于传统压缩感知方法。

Abstract: The synchronous Nyquist folding generalized eigenvalue method (SNGEM) realizes full frequency/amplitude/phase estimation of multitone signals at extreme sub-Nyquist rates by jointly processing the original signals and their time derivatives. In this paper, accurate Cramer-Rao bounds for amplitude ratio parameter R=A/B=1/(2\pif) are derived for two channels with equal SNR. Monte-Carlo simulations confirm that SNGEM achieves machine accuracy in noise-free conditions and closely approaches the derived CRB at all SNR levels, even at 10- 20x compression, whereas classical compressive sensing OMP exhibits irreducible error flattening due to DFT grid bias and aliasing noise. These results establish SNGEM as a statistically nearly optimal deterministic sub-Nyquist parameter spectrum analysis

</details>


### [117] [Power consumption Reduction in ELAA-Assisted ISAC Systems](https://arxiv.org/abs/2601.21010)
*Xiaomin Cao,Mohammadali Mohammadi,Hien Quoc Ngo,Michail Matthaiou*

Main category: cs.IT

TL;DR: 提出一种用于超大规模天线阵列的节能子阵列激活框架，通过选择最优子阵列子集来最小化总功耗，同时满足感知和通信的服务质量约束。


<details>
  <summary>Details</summary>
Motivation: 超大规模天线阵列在集成感知与通信应用中至关重要，但传统数字架构中完全激活所有天线单元会导致功耗过高，需要节能解决方案。

Method: 提出能量高效的子阵列激活框架，将问题建模为优化问题，并使用基于连续凸近似的迭代算法求解最优子阵列子集选择。

Result: 仿真结果表明，所提方法能显著降低功耗，同时保持感知和通信的双功能性能。

Conclusion: 该研究为超大规模天线阵列提供了一种有效的节能解决方案，在满足服务质量约束的前提下显著降低了系统功耗。

Abstract: In this paper, we consider power consumption reduction in extremely large antenna arrays (ELAAs) for integrated sensing and communication (ISAC) applications. Although ELAAs are critical for achieving high-resolution near-field sensing, fully activating all antenna elements in conventional digital architectures leads to prohibitive power demands. To address this, we propose an energy-efficient subarray activation framework that selects an optimal subset of subarrays to minimize the total power consumption, subject to quality-of-service (QoS) constraints for both sensing and communication. We formulate a novel optimization problem and solve it using a successive convex approximation (SCA)-based iterative algorithm. The simulation results confirm that the proposed method significantly reduces power consumption while maintaining dual-function performance.

</details>


### [118] [Deletion-correcting codes for an adversarial nanopore channel](https://arxiv.org/abs/2601.21236)
*Huiling Xie,Zitan Chen*

Main category: cs.IT

TL;DR: 本文研究对抗性纳米孔信道中的删除纠错码，提出了一种显式构造方法，其冗余度为2t log_q n + Θ(log log n)，接近最优上界。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性纳米孔信道中的删除纠错问题，该信道最多可能发生t次删除。与经典对抗性删除信道相比，纳米孔信道需要更高效的纠错方案。

Method: 提出了一种显式构造的q元码，用于纠正最多t次删除。该构造基于特定的编码设计，能够有效处理纳米孔信道中的删除错误。

Result: 构造的码具有2t log_q n + Θ(log log n)的冗余度，而最优冗余度在t log_q n + Ω(1)和2t log_q n - log_q log_2 n + O(1)之间，表明该构造在首阶上匹配了存在性上界。

Conclusion: 对于对抗性纳米孔信道，本文提出的显式构造在冗余度上显著优于经典对抗性删除信道的已知构造，接近理论最优界限。

Abstract: We study deletion-correcting codes for an adversarial nanopore channel in which at most $t$ deletions may occur. We propose an explicit construction of $q$-ary codes of length $n$ for this channel with $2t\log_q n+Θ(\log\log n)$ redundant symbols. We also show that the optimal redundancy is between $t\log_q n+Ω(1)$ and $2t\log_q n-\log_q\log_2 n+O(1)$, so our explicit construction matches the existential upper bound to first order. In contrast, for the classical adversarial $q$-ary deletion channel, the smallest redundancy achieved by known explicit constructions that correct up to $t$ deletions is $4t(1+ε)\log_q n+o(\log n)$.

</details>


### [119] [Belief Propagation with Quantum Messages for Symmetric Q-ary Pure-State Channels](https://arxiv.org/abs/2601.21330)
*Avijit Mandal,Henry D. Pfister*

Main category: cs.IT

TL;DR: 将BPQM从二进制扩展到对称q元纯态信道，建立了基于Gram矩阵特征值的密度演化框架，可用于LDPC码阈值估计和极化码构造


<details>
  <summary>Details</summary>
Motivation: 之前BPQM构造和密度演化分析仅限于二进制字母表，需要扩展到更一般的q元对称纯态信道，以提供更通用的量子消息传递解码框架

Method: 将BPQM推广到输出Gram矩阵为循环矩阵的对称q元纯态信道，通过Gram矩阵特征值的闭式递推高效跟踪比特节点和校验节点合并，独立于输出态的具体物理实现

Result: 得到了显式的BPQM幺正变换和组合信道保真度的解析边界，建立了对称q元纯态信道的密度演化框架，可用于LDPC码的BPQM解码阈值估计和极化码构造

Conclusion: 成功将BPQM扩展到对称q元纯态信道，建立了基于特征值递推的密度演化框架，为量子信道上的低复杂度解码提供了通用工具

Abstract: Belief propagation with quantum messages (BPQM) provides a low-complexity alternative to collective measurements for communication over classical--quantum channels. Prior BPQM constructions and density-evolution (DE) analyses have focused on binary alphabets. Here, we generalize BPQM to symmetric q-ary pure-state channels (PSCs) whose output Gram matrix is circulant. For this class, we show that bit-node and check-node combining can be tracked efficiently via closed-form recursions on the Gram-matrix eigenvalues, independent of the particular physical realization of the output states. These recursions yield explicit BPQM unitaries and analytic bounds on the fidelities of the combined channels in terms of the input-channel fidelities. This provides a DE framework for symmetric q-ary PSCs that allows one to estimate BPQM decoding thresholds for LDPC codes and to construct polar codes on these channels.

</details>


### [120] [Learning-Based Sensor Scheduling for Delay-Aware and Stable Remote State Estimation](https://arxiv.org/abs/2601.21482)
*Nho-Duc Tran,Aamir Mahmood,Mikael Gidlund*

Main category: cs.IT

TL;DR: 提出一个统一的延迟感知框架，用于无线远程状态估计，考虑延迟与传感器信息量和能量效率的交互作用，开发基于PPO的调度器，在异构传感器和随机延迟下实现更低的估计误差。


<details>
  <summary>Details</summary>
Motivation: 不可预测的传感器到估计器延迟从根本上扭曲了无线远程状态估计的关键因素：不仅是新鲜度，还有延迟如何与传感器信息量和能量效率相互作用。现有基于信息年龄(AoI)的代理方法未能充分捕捉这种耦合。

Method: 1. 引入高效的延迟测量后验融合更新，无需状态增广；2. 推导可处理的稳定性条件；3. 将调度建模为马尔可夫决策过程，开发基于近端策略优化(PPO)的调度器，使用归一化奖励权衡估计精度、新鲜度、传感器异质性和传输能量。

Result: 在异构传感器、实际链路能量模型和随机延迟的仿真中，该方法稳定学习，相比随机调度和强化学习基线(DQN, A2C)，在可比能量消耗下实现更低的估计误差，对测量可用性和过程/测量噪声变化保持鲁棒。

Conclusion: 提出的延迟感知框架和PPO调度器有效解决了无线远程状态估计中延迟、信息量和能量效率的耦合问题，为实际部署提供了实用解决方案，超越了传统AoI代理方法。

Abstract: Unpredictable sensor-to-estimator delays fundamentally distort what matters for wireless remote state estimation: not just freshness, but how delay interacts with sensor informativeness and energy efficiency. In this paper, we present a unified, delay-aware framework that models this coupling explicitly and quantifies a delay-dependent information gain, motivating an information-per-joule scheduling objective beyond age of information proxies (AoI). To this end, we first introduce an efficient posterior-fusion update that incorporates delayed measurements without state augmentation, providing a consistent approximation to optimal delayed Kalman updates, and then derive tractable stability conditions ensuring that bounded estimation error is achievable under stochastic, delayed scheduling. This conditions highlight the need for unstable modes to be observable across sensors. Building on this foundation, we cast scheduling as a Markov decision process and develop a proximal policy optimization (PPO) scheduler that learns directly from interaction, requires no prior delay model, and explicitly trades off estimation accuracy, freshness, sensor heterogeneity, and transmission energy through normalized rewards. In simulations with heterogeneous sensors, realistic link-energy models, and random delays, the proposed method learns stably and consistently achieves lower estimation error at comparable energy than random scheduling and strong RL baselines (DQN, A2C), while remaining robust to variations in measurement availability and process/measurement noise.

</details>


### [121] [Transversal gates for quantum CSS codes](https://arxiv.org/abs/2601.21514)
*Eduardo Camps-Moreno,Hiram H. López,Gretchen L. Matthews,Narayanan Rengaswamy,Rodrigo San-José*

Main category: cs.IT

TL;DR: 本文研究了CSS码的横向门计算问题，确定了逻辑作用、非平凡逻辑门和逻辑恒等式的群结构，并给出了明确的方程组定义，为来自单项式码的CSS码提供了完整的横向稳定子和横向门计算。


<details>
  <summary>Details</summary>
Motivation: 研究CSS码的横向门计算问题，特别是确定固定CSS码的对角横向门集合，这对于量子纠错码的实现和逻辑门操作具有重要意义。

Method: 通过确定横向门的逻辑作用，以及诱导非平凡逻辑门和逻辑恒等式的群结构，明确声明定义这些群的方程组。特别针对来自单项式码（包括递减单项式码和极化码）的CSS码进行计算。

Result: 计算了来自单项式码的任何CSS码的完整横向稳定子和横向门集合，恢复并扩展了文献中关于CSS-T码、三正交码和可除码的一些结果。

Conclusion: 本文为CSS码的横向门计算提供了系统方法，通过明确方程组定义群结构，能够处理包括递减单项式码和极化码在内的广泛码类，扩展了现有理论结果。

Abstract: In this paper, we focus on the problem of computing the set of diagonal transversal gates fixing a CSS code. We determine the logical actions of the gates as well as the groups of transversal gates that induce non-trivial logical gates and logical identities. We explicitly declare the set of equations defining the groups, a key advantage and differentiator of our approach. We compute the complete set of transversal stabilizers and transversal gates for any CSS code arising from monomial codes, a family that includes decreasing monomial codes and polar codes. As a consequence, we recover and extend some results in the literature on CSS-T codes, triorthogonal codes, and divisible codes.

</details>


### [122] [Subjective Distortion: Achievability and Outer Bounds for Distortion Functions with Memory](https://arxiv.org/abs/2601.21757)
*Hamidreza Abin,Amin Gohari,Andrew W. Eckford*

Main category: cs.IT

TL;DR: 论文研究了历史依赖的率失真问题，其中失真函数不仅取决于当前源符号与表示符号之间的瞬时失真，还受到过去表示的影响，并给出了该问题的内外界分析。


<details>
  <summary>Details</summary>
Motivation: 在某些率失真类型问题中，所需的信息保真度受到过去动作的影响。这种历史依赖的失真函数出现在生物信息处理和推荐引擎等场景中，需要建立相应的理论框架。

Method: 给出了该问题的形式化定义，引入了率失真权衡的内界（可达界）和外界，并讨论了问题的凸化处理以简化界分析。

Result: 建立了历史依赖率失真问题的理论框架，提供了内外界分析，并通过凸化方法使界分析更加容易。以简化的生物信息处理问题为例进行了应用展示。

Conclusion: 论文成功形式化了历史依赖的率失真问题，提供了分析此类问题的理论工具，特别是在生物信息处理和推荐系统等实际应用中具有重要价值。

Abstract: In some rate-distortion-type problems, the required fidelity of information is affected by past actions. As a result, the distortion function depends not only on the instantaneous distortion between a source symbol and its representation symbol, but also on past representations. In this paper, we give a formal definition of this problem and introduce both inner (achievable) and outer bounds on the rate-distortion tradeoff. We also discuss convexification of the problem, which makes it easier to find bounds. Problems of this type arise in biological information processing, as well as in recommendation engines; we provide an example applied to a simplified biological information processing problem.

</details>


### [123] [Adaptive Privacy of Sequential Data Releases Under Collusion](https://arxiv.org/abs/2601.21859)
*Sophie Taylor,Praneeth Kumar Vippathalla,Justin Coon*

Main category: cs.IT

TL;DR: 本文提出一种自适应算法来处理不同实体（可能合谋）的序列数据请求，在隐私与效用之间取得平衡，使用改进的Blahut-Arimoto算法实现最优或局部最优的数据发布。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于两个观察：1) 一次性数据发布的隐私机制难以直接扩展到序列发布场景；2) 实际数据库可能服务于多个不同方，且无法排除数据共享（合谋）的可能性。需要在考虑潜在合谋攻击的情况下，自适应处理序列数据请求。

Method: 采用改进的Blahut-Arimoto算法开发自适应数据发布算法。考虑两种效用度量：期望失真和互信息；使用互信息度量隐私。假设攻击模型中数据接收者之间可能存在非法数据共享（合谋）。

Result: 当效用以期望失真度量时，所得数据发布是最优的；当效用以互信息度量时，数据发布是局部最优的。算法能有效处理不同实体（可能合谋）的序列数据请求。

Conclusion: 本文提出的自适应算法能有效平衡序列数据发布中的隐私与效用，特别是在存在潜在合谋的情况下。研究结果可扩展到机器学习应用领域。

Abstract: The fundamental trade-off between privacy and utility remains an active area of research. Our contribution is motivated by two observations. First, privacy mechanisms developed for one-time data release cannot straightforwardly be extended to sequential releases. Second, practical databases are likely to be useful to multiple distinct parties. Furthermore, we can not rule out the possibility of data sharing between parties. With utility in mind, we formulate a privacy-utility trade-off problem to adaptively tackle sequential data requests made by different, potentially colluding entities. We consider both expected distortion and mutual information as measures to quantify utility, and use mutual information to measure privacy. We assume an attack model whereby illicit data sharing, which we call collusion, can occur between data receivers. We develop an adaptive algorithm for data releases that makes use of a modified Blahut-Arimoto algorithm. We show that the resulting data releases are optimal when expected distortion quantifies utility, and locally optimal when mutual information quantifies utility. Finally, we discuss how our findings may extend to applications in machine learning.

</details>
