<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning](https://arxiv.org/abs/2507.07118)
*Zelin Zhu,Kai Yang,Rui Zhang*

Main category: cs.NI

TL;DR: 论文提出了一种基于深度学习的联合定位与感知优化方法（SPG-MIBO），适用于MIMO-OFDM系统的高维CSI数据。


<details>
  <summary>Details</summary>
Motivation: 无线定位与感知技术在现代网络中至关重要，但现有方法在高维CSI数据下的联合建模研究不足。

Method: 将定位与感知建模为混合整数双层深度学习问题，并提出SPG-MIBO算法，支持高维大规模数据的高效训练。

Result: 实验验证了算法的有效性，并展示了联合优化的性能提升。

Conclusion: SPG-MIBO为高维CSI数据的联合定位与感知提供了高效解决方案，具有理论和实践意义。

Abstract: Wireless localization and sensing technologies are essential in modern
wireless networks, supporting applications in smart cities, the Internet of
Things (IoT), and autonomous systems. High-performance localization and sensing
systems are critical for both network efficiency and emerging intelligent
applications. Integrating channel state information (CSI) with deep learning
has recently emerged as a promising solution. Recent works have leveraged the
spatial diversity of multiple input multiple output (MIMO) systems and the
frequency granularity of orthogonal frequency division multiplexing (OFDM)
waveforms to improve spatial resolution. Nevertheless, the joint modeling of
localization and sensing under the high-dimensional CSI characteristics of
MIMO-OFDM systems remains insufficiently investigated. This work aims to
jointly model and optimize localization and sensing tasks to harness their
potential synergy. We first formulate localization and sensing as a
mixed-integer bilevel deep learning problem and then propose a novel stochastic
proximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)
algorithm. SPG-MIBO is well-suited for high-dimensional and large-scale
datasets, leveraging mini-batch training at each step for computational and
memory efficiency. The algorithm is also supported by theoretical convergence
guarantees. Extensive experiments on multiple datasets validate its
effectiveness and highlight the performance gains from joint localization and
sensing optimization.

</details>


### [2] [DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training](https://arxiv.org/abs/2507.07149)
*Renyuan Liu,Yuyang Leng,Kaiyan Liu,Shaohan Hu,Chun-Fu,Chen,Peijun Zhao,Heechul Yun,Shuochao Yao*

Main category: cs.NI

TL;DR: DAF框架通过系统级优化实现高效动态量化训练，显著减少内存使用并加速训练。


<details>
  <summary>Details</summary>
Motivation: 解决移动和边缘设备上动态激活量化的系统级挑战，如计算开销和内存碎片。

Method: 开发混合归约操作、CPU-GPU协作位打包和重要性感知分页内存管理。

Result: 内存使用减少22.9倍，速度提升3.2倍，且不损失训练精度。

Conclusion: DAF为资源受限环境提供了可扩展且实用的解决方案。

Abstract: Recent advancements in on-device training for deep neural networks have
underscored the critical need for efficient activation compression to overcome
the memory constraints of mobile and edge devices. As activations dominate
memory usage during training and are essential for gradient computation,
compressing them without compromising accuracy remains a key research
challenge. While existing methods for dynamic activation quantization promise
theoretical memory savings, their practical deployment is impeded by
system-level challenges such as computational overhead and memory
fragmentation.
  To address these challenges, we introduce DAF, a Dynamic Activation Framework
that enables scalable and efficient on-device training through system-level
optimizations. DAF achieves both memory- and time-efficient dynamic
quantization training by addressing key system bottlenecks. It develops hybrid
reduction operations tailored to the memory hierarchies of mobile and edge
SoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic
quantization, and implements an importance-aware paging memory management
scheme to reduce fragmentation and support dynamic memory adjustments.
  These optimizations collectively enable DAF to achieve substantial memory
savings and speedup without compromising model training accuracy. Evaluations
on various deep learning models across embedded and mobile platforms
demonstrate up to a $22.9\times$ reduction in memory usage and a $3.2\times$
speedup, making DAF a scalable and practical solution for resource-constrained
environments.

</details>


### [3] [PHandover: Parallel Handover in Mobile Satellite Network](https://arxiv.org/abs/2507.07437)
*Jiasheng Wu,Shaojie Su,Wenjun Zhu,Xiong Wang,Jingjing Zhang,Xingqiu He,Yue Gao*

Main category: cs.NI

TL;DR: 论文提出了一种并行切换机制，显著降低了低地球轨道卫星网络中的切换延迟，通过基于计划的切换和机器学习模型优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决由于卫星高速移动导致的频繁高延迟切换问题，提升延迟敏感应用的性能。

Method: 采用基于计划的切换机制，引入卫星同步功能（SSF），并结合机器学习信号预测和高效切换调度算法。

Result: 实验表明，该方案将切换延迟降低21倍，同时显著提升网络稳定性和用户性能。

Conclusion: 提出的并行切换机制有效解决了卫星网络中的高延迟切换问题，具有实际应用潜力。

Abstract: The construction of Low Earth Orbit (LEO) satellite constellations has
recently attracted tremendous attention from both academia and industry. The 5G
and 6G standards have identified LEO satellite networks as a key component of
future mobile networks. However, due to the high-speed movement of satellites,
ground terminals often experience frequent and high-latency handovers, which
significantly deteriorate the performance of latency-sensitive applications. To
address this challenge, we propose a parallel handover mechanism for mobile
satellite networks that can considerably reduce handover latency. The main idea
is to employ plan-based handovers instead of measurement-based handovers to
avoid interactions between the access and core networks, thereby eliminating
the significant time overhead associated with traditional handover procedures.
Specifically, we introduce a novel network function named the Satellite
Synchronized Function (SSF), which is designed to be fully compliant with the
standard 5G core network. In addition, we propose a machine learning model for
signal strength prediction, coupled with an efficient handover scheduling
algorithm. We have conducted extensive experiments, and the results demonstrate
that our proposed handover scheme can reduce handover latency by 21\times
compared to the standard NTN handover scheme and two other existing handover
approaches, along with significant improvements in network stability and
user-level performance.

</details>


### [4] [Energy Transfer and Data Collection from Batteryless Sensors in Low-altitude Wireless Networks](https://arxiv.org/abs/2507.07481)
*Wen Zhang,Aimin Wang,Jiahui Li,Geng Sun,Jiacheng Wang,Weijie Yuan,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出了一种无人机辅助的无电池传感器网络数据收集与无线能量传输框架，通过多目标优化和强化学习算法提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决在高温等极端环境中传统固定无线能量传输基础设施无法安装且电池快速退化的问题。

Method: 结合无人机能量传输与OFDMA数据收集，提出SAC-PPV强化学习算法优化功率分配与飞行轨迹。

Result: 仿真结果表明，所提方法在各种网络配置下均优于基准算法。

Conclusion: 无人机辅助框架与SAC-PPV算法为极端环境下的无电池传感器网络提供了高效解决方案。

Abstract: The integration of wireless power transfer (WPT) with Internet of Things
(IoT) offers promising solutions for sensing applications, but faces
significant challenges when deployed in hard-to-access areas such as
high-temperature environments. In such extreme conditions, traditional fixed
WPT infrastructure cannot be safely installed, and batteries rapidly degrade
due to hardware failures. In this paper, we propose an uncrewed aerial vehicle
(UAV)-assisted data collection and WPT framework for batteryless sensor (BLS)
networks deployed in these challenging environments. Specifically, we consider
a practical scenario where a UAV first transfers energy to BLS nodes via WPT,
enabling these nodes to subsequently transmit their collected data to the UAV
through orthogonal frequency-division multiple access (OFDMA). Then, we
formulate a multi-objective optimization problem that aims to maximize the fair
data collection volume while minimizing the UAV energy consumption through
joint optimization of transmit power allocation and flight trajectory planning.
Due to the non-convex nature and dynamic characteristics of this problem,
conventional optimization methods prove inadequate. To address these
challenges, we propose an enhanced soft actor-critic algorithm with
parameter-free attention, prioritized experience replay, and value-based reward
centering (SAC-PPV), thereby improving the exploration efficiency and learning
stability of the algorithm in complex WPT scenarios. Simulation results
demonstrate that the proposed approach consistently outperforms benchmark
algorithms under various network configurations.

</details>


### [5] [A Fragmentation-Aware Adaptive Bilevel Search Framework for Service Mapping in Computing Power Networks](https://arxiv.org/abs/2507.07535)
*Jingzhao Xie,Zhenglian Li,Gang Sun,Long Luo,Hongfang Yu,Dusit Niyato*

Main category: cs.NI

TL;DR: 论文提出了一种名为ABS的模块化框架，用于解决计算能力网络（CPN）中的服务映射问题，显著提高了资源利用率和服务接受率。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能完全实现CPN中通过网络协调整合计算资源的愿景，尤其是在优化服务映射以最大化资源效率和服务满意度方面存在挑战。

Method: 论文提出了自适应双层搜索（ABS）框架，包括基于图分区的重新表述、双层优化架构和碎片感知评估。

Result: 在多样化的CPN场景中，ABS表现优于现有方法，计算资源利用率提高了73.2%，服务接受率提高了60.2%。

Conclusion: ABS框架有效解决了CPN中的服务映射问题，为资源优化提供了实用解决方案。

Abstract: Computing Power Network (CPN) unifies wide-area computing resources through
coordinated network control, while cloud-native abstractions enable flexible
resource orchestration and on-demand service provisioning atop the elastic
infrastructure CPN provides. However, current approaches fall short of fully
integrating computing resources via network-enabled coordination as envisioned
by CPN. In particular, optimally mapping services to an underlying
infrastructure to maximize resource efficiency and service satisfaction remains
challenging. To overcome this challenge, we formally define the service mapping
problem in CPN, establish its theoretical intractability, and identify key
challenges in practical optimization. We propose Adaptive Bilevel Search (ABS),
a modular framework featuring (1) graph partitioning-based reformulation to
capture variable coupling, (2) a bilevel optimization architecture for
efficient global exploration with local optimality guarantees, and (3)
fragmentation-aware evaluation for global performance guidance. Implemented
using distributed particle swarm optimization, ABS is extensively evaluated
across diverse CPN scenarios, consistently outperforming existing approaches.
Notably, in complex scenarios, ABS achieves up to 73.2% higher computing
resource utilization and a 60.2% higher service acceptance ratio compared to
the best-performing baseline.

</details>


### [6] [HaLert: A Resilient Smart City Architecture for Post-Disaster Based on Wi-Fi HaLow Mesh and SDN](https://arxiv.org/abs/2507.07841)
*Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.NI

TL;DR: HaLert是一种基于Wi-Fi HaLow IEEE 802.11s网状网络的智能城市弹性架构，可在灾难后重新分配资源以支持紧急通信系统。


<details>
  <summary>Details</summary>
Motivation: 灾难事件通常不可预测，现有基础设施的再利用对减少通信中断至关重要。智能城市的密集物联网网络为此提供了潜力。

Method: 结合Wi-Fi HaLow网状网络和SDN（软件定义网络）范式，支持远程监控和配置。开发原型并在真实城市环境中测试。

Result: 尽管障碍物和地形影响延迟和吞吐量，网络仍保持稳定和弹性。LoRa网络的平均消息成功率高达94.96%。

Conclusion: HaLert架构在灾难场景下表现出色，能够有效支持紧急通信。

Abstract: Events such as catastrophes and disasters are, in most cases, unpredictable.
Consequently, reusing existing infrastructures to develop alternative
communication strategies after disasters is essential to minimise the impact of
these events on the population's ability to communicate and promptly receive
alerts from authorities. In this context, the emergence of smart cities,
characterised by dense and geographically distributed IoT networks, presents
significant potential for such reuse. This work proposes HaLert, a resilient
architecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network,
whose resources can be readily reallocated to support a emergency communication
system to exchange messages (including text, location, image, audio, and video)
between citizens, authorities, and between both parties. To facilitate remote
monitoring and configuration of the network, the architecture incorporates the
SDN (Software-Defined Networking) paradigm, supported by a LoRa controlled
flooding mesh network. A prototype was developed based on this architecture and
tested in a real urban scenario comprising both indoor and outdoor
environments. The results demonstrated that, despite the significant impact of
obstacles, lack of line-of-sight, and terrain slopes on the latency (average
latency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and
726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow
network, it remained stable and resilient, successfully providing all
functionalities associated with the HaLert architecture. The tests conducted on
the LoRa network revealed a high average message success rate of 94.96%.

</details>


### [7] [Can cloud-based VR streaming handle Wi-Fi OBSS contention?](https://arxiv.org/abs/2507.07677)
*Miguel Casasnovas,Marc Carrascosa-Zamacois,Boris Bellalta*

Main category: cs.NI

TL;DR: 该论文实验分析了Wi-Fi网络中相邻网络在重叠信道上的竞争对VR流媒体的负面影响，发现不同重叠情况对性能的影响不同，并提出了一种自适应比特率算法（NeSt-VR）来缓解性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究Wi-Fi网络中重叠信道竞争对VR流媒体性能的负面影响，以优化VR流媒体在复杂网络环境中的表现。

Method: 通过实验分析不同重叠信道配置（部分和完全重叠）对VR流媒体性能的影响，并测试提出的NeSt-VR算法的有效性。

Result: 结果表明，重叠信道竞争会显著降低VR流媒体性能，但NeSt-VR算法能有效缓解这种影响。

Conclusion: NeSt-VR算法在复杂Wi-Fi环境中能显著提升VR流媒体的性能，为实际应用提供了解决方案。

Abstract: This paper experimentally analyzes the negative impact of contention caused
by neighboring Wi-Fi networks operating on overlapping channels on Virtual
Reality (VR) streaming over Wi-Fi, focusing on scenarios of partial and full
channel overlap within an 80 MHz channel. Our results show that (i) increasing
the number of 80 MHz Overlapping Basic Service Sets (OBSSs) intensifies
contention and degrades VR streaming performance; (ii) OBSS activity on the
secondary-sided 40 MHz portion degrades performance more than activity on the
primary-sided 40 MHz portion; (iii) for the same aggregate load, full channel
overlap with two 40 MHz OBSS contenders is less detrimental than partial
overlap with a single high-load 40 MHz contender, but more disruptive than full
overlap with two 80 MHz contenders; and (iv) full channel overlap with two 40
MHz OBSS contenders has a smaller impact on VR streaming under symmetric
traffic loads than under asymmetric loads. Moreover, our results demonstrate
that our previously proposed Network-aware Step-wise adaptive bitrate algorithm
for VR streaming (NeSt-VR) effectively mitigates performance degradation in
OBSS environments, enabling VR streaming under heavier OBSS traffic conditions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas,Mehmet Mercangoz*

Main category: cs.AI

TL;DR: 本文提出了一种结合符号推理与自适应控制的统一代理框架，利用大语言模型（LLMs）实现离散故障恢复规划和连续过程控制。通过有限状态机（FSMs）作为可解释的操作边界，结合仿真代理和验证-重提示循环，显著提升了规划准确性和控制性能。


<details>
  <summary>Details</summary>
Motivation: 现代化学过程日益复杂，劳动力短缺和故障场景多样化，需要新的自动化范式，结合符号推理与自适应控制以提高系统韧性。

Method: 采用统一代理框架，利用LLMs进行离散故障恢复规划和连续控制。通过FSMs定义操作边界，LLM规划代理生成恢复序列，仿真代理执行和验证，验证-重提示循环优化无效计划。

Result: 在随机生成的FSMs中，GPT-4o和GPT-4o-mini在五次重提示内实现100%有效路径成功率；在双加热器控制实验中，LLM控制器性能与经典PID相当，但能更好处理非线性动态。

Conclusion: 研究表明，通过结构化反馈和模块化代理，LLMs可以统一高层符号规划和低层连续控制，为化学工程中的语言驱动自动化提供了新方向。

Abstract: The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [9] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran,Shuang Chen,Jingjing Deng,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 论文提出了一种名为BOOST的方法，通过动态调整温度缩放和采样概率，解决AI艺术分类中的偏见问题，并在KaoKore和PACS数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: AI艺术分类中存在严重的偏见问题，源于数据集不平衡，导致模型对罕见艺术风格的分类准确性下降，亟需解决。

Method: 提出BOOST方法，动态调整温度缩放和采样概率，并引入新指标SODC评估类间分离和偏见减少效果。

Result: BOOST在KaoKore和PACS数据集上表现出色，能平衡高性能与公平性。

Conclusion: BOOST是一种有效的解决方案，适用于艺术领域AI模型的去偏见化。

Abstract: The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [10] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim,Junsik Kim,Hwidong Bae,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: SIBP方法通过状态推断和规则遵守，解决了大语言模型在游戏交易系统中的规则违反问题，显著提升了准确性和信任度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在动态游戏交互中表现良好，但在规则驱动的交易系统中容易违反规则（如物品幻觉和计算错误），影响玩家信任。

Method: 提出State-Inference-Based Prompting (SIBP)，将交易分解为六个状态，通过上下文感知的物品引用和占位符价格计算实现规则遵守。

Result: 在100个交易对话中，状态合规率>97%，引用准确率>95%，计算精度99.7%，计算效率高且优于基线方法。

Conclusion: SIBP为商业游戏中可信赖的NPC交互提供了实用基础。

Abstract: Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [11] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 论文探讨了如何利用神经符号方法检测供应链中的非法活动，提出了一种基于大型语言模型的问题树方法，用于自动分类和评估新闻文章的相关性。


<details>
  <summary>Details</summary>
Motivation: 供应链网络复杂且涉及非法活动时数据稀疏且不可靠，传统机器学习方法难以应对，需要新的方法来自动检测非法活动模式。

Method: 采用神经符号方法，结合问题树查询大型语言模型（LLM），自动提取特征并分类新闻文章。

Result: 比较了人工和机器分类新闻文章的效果，验证了自动化方法的有效性。

Conclusion: 神经符号方法和LLM结合的问题树方法能够有效识别供应链中的非法活动，尤其在数据稀疏的情况下表现优越。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [12] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: 提出了一种多智能体系统cmbagent，用于自动化科学研究任务，由约30个LLM智能体组成，采用规划与控制策略协调工作流，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 旨在通过多智能体系统实现科学研究任务的自动化，减少人工干预，提高效率。

Method: 系统由多个LLM智能体组成，每个智能体专注于不同任务（如检索科学论文和代码库、编写代码、解释结果等），并采用规划与控制策略协调工作流。

Result: 成功应用于博士级宇宙学任务，性能优于现有最先进的LLM。

Conclusion: cmbagent展示了多智能体系统在科学研究自动化中的潜力，代码已开源并部署。

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [13] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

Main category: cs.AI

TL;DR: 本文研究了在多智能体强化学习中利用大型语言模型作为专家规划器以实现高效探索的方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的高效探索问题因算法复杂性而加剧，需要新的解决方案。

Method: 提出使用大型语言模型作为专家规划器，指导多智能体在基于规划的任务中进行探索。

Result: 该方法有望提升多智能体在复杂任务中的探索效率。

Conclusion: 大型语言模型作为专家规划器在多智能体强化学习中具有潜力，可解决高效探索问题。

Abstract: Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [14] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: ViDove是一个多模态翻译代理系统，通过结合视觉和上下文信息提升翻译质量，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有LLM翻译代理仅限于文本输入，无法利用多模态信息。

Method: ViDove模仿人类翻译流程，整合多模态记忆系统和长短时记忆模块，结合领域知识。

Result: 在字幕生成和通用翻译任务中，ViDove的BLEU分数提升28%，SubER提升15%。

Conclusion: ViDove在多模态翻译中表现优异，并推出了新基准DoveBench。

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [15] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball,Greg Gluch,Shafi Goldwasser,Frauke Kreuter,Omer Reingold,Guy N. Rothblum*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLM）生成有害内容的问题，指出输入和输出过滤在计算上存在挑战，并证明外部过滤器无法确保安全性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛使用，防止其生成有害内容成为重要问题，研究旨在探索过滤技术的局限性。

Method: 通过理论分析，证明输入和输出过滤的计算困难性，并基于密码学假设提出分离结果。

Result: 发现高效过滤器和输出过滤器在计算上不可行，外部过滤器无法确保LLM的安全性。

Conclusion: 安全性需通过LLM内部设计实现，而非外部过滤器，智能与判断不可分离。

Abstract: With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [16] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai,Haoyu Wang,Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haifeng Chen,Yanjie Fu*

Main category: cs.AI

TL;DR: Sim-to-Dec框架结合生成模拟模块和双感知决策模型，显著提升供应链运输的及时交付率和利润。


<details>
  <summary>Details</summary>
Motivation: 供应链运输的高响应性和经济效率是关键目标，而运输模式的选择直接影响这些目标。需要一个集成框架来优化运输策略。

Method: 提出Sim-to-Dec框架，包括生成模拟模块（利用自回归建模模拟连续状态变化）和双感知决策模型（通过端到端优化迭代改进）。

Result: 在三个真实数据集上的实验表明，Sim-to-Dec显著提高了及时交付率和利润。

Conclusion: Sim-to-Dec框架满足跨场景通用性、细粒度动态模拟、历史与预测结合以及模拟反馈与策略优化的紧密集成需求。

Abstract: High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


### [17] [DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search](https://arxiv.org/abs/2507.07426)
*Zerui Yang,Yuwei Wan,Yinqiao Li,Yudai Matsuda,Tong Xie,Linqi Song*

Main category: cs.AI

TL;DR: DrugMCTS框架结合RAG、多智能体协作和蒙特卡洛树搜索，显著提升药物重定向任务的性能，无需领域微调即可超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学领域（如药物发现）的潜力受限于预训练知识外的推理能力，传统方法（如微调或检索增强生成）存在计算开销大或未能充分利用结构化科学数据的问题。

Method: 提出DrugMCTS框架，整合RAG、多智能体协作和蒙特卡洛树搜索，通过五个专业智能体检索和分析分子与蛋白质信息，实现结构化迭代推理。

Result: 在DrugBank和KIBA数据集上，DrugMCTS显著优于通用LLM和深度学习基线，Qwen2.5-7B-Instruct性能提升超20%。

Conclusion: 结构化推理、智能体协作和反馈驱动搜索机制对推进药物发现中的LLM应用至关重要。

Abstract: Recent advances in large language models have demonstrated considerable
potential in scientific domains such as drug discovery. However, their
effectiveness remains constrained when reasoning extends beyond the knowledge
acquired during pretraining. Conventional approaches, such as fine-tuning or
retrieval-augmented generation, face limitations in either imposing high
computational overhead or failing to fully exploit structured scientific data.
To overcome these challenges, we propose DrugMCTS, a novel framework that
synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree
Search for drug repurposing. The framework employs five specialized agents
tasked with retrieving and analyzing molecular and protein information, thereby
enabling structured and iterative reasoning. Without requiring domain-specific
fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by
over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate
that DrugMCTS achieves substantially higher recall and robustness compared to
both general-purpose LLMs and deep learning baselines. Our results highlight
the importance of structured reasoning, agent-based collaboration, and
feedback-driven search mechanisms in advancing LLM applications for drug
discovery.

</details>


### [18] [StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley](https://arxiv.org/abs/2507.07445)
*Weihao Tan,Changjiu Jiang,Yu Duan,Mingcong Lei,Jiageng Li,Yitian Hong,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: StarDojo是一个基于Stardew Valley的新基准测试，用于评估AI代理在开放式生产生活模拟中的表现，涵盖农业、手工艺、探索、战斗和社交互动等领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试很少同时评估生产活动和社会互动能力，StarDojo旨在填补这一空白。

Method: StarDojo包含1,000个任务，分为五个关键领域，并提供100个代表性任务子集。它提供统一界面，支持多环境并行执行。

Result: 评估显示，最先进的MLLM代理（如GPT-4.1）成功率仅为12.7%，主要受限于视觉理解、多模态推理和低级操作。

Conclusion: StarDojo为复杂生产生活环境中开放式代理的研究提供了用户友好的平台。

Abstract: Autonomous agents navigating human society must master both production
activities and social interactions, yet existing benchmarks rarely evaluate
these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel
benchmark based on Stardew Valley, designed to assess AI agents in open-ended
production-living simulations. In StarDojo, agents are tasked to perform
essential livelihood activities such as farming and crafting, while
simultaneously engaging in social interactions to establish relationships
within a vibrant community. StarDojo features 1,000 meticulously curated tasks
across five key domains: farming, crafting, exploration, combat, and social
interactions. Additionally, we provide a compact subset of 100 representative
tasks for efficient model evaluation. The benchmark offers a unified,
user-friendly interface that eliminates the need for keyboard and mouse
control, supports all major operating systems, and enables the parallel
execution of multiple environment instances, making it particularly well-suited
for evaluating the most capable foundation agents, powered by multimodal large
language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents
demonstrate substantial limitations, with the best-performing model, GPT-4.1,
achieving only a 12.7% success rate, primarily due to challenges in visual
understanding, multimodal reasoning and low-level manipulation. As a
user-friendly environment and benchmark, StarDojo aims to facilitate further
research towards robust, open-ended agents in complex production-living
environments.

</details>


### [19] [Position: We Need An Algorithmic Understanding of Generative AI](https://arxiv.org/abs/2507.07544)
*Oliver Eberle,Thomas McGee,Hamza Giaffar,Taylor Webb,Ida Momennejad*

Main category: cs.AI

TL;DR: 论文提出AlgEval框架，旨在系统研究LLM学习与使用的算法，揭示其底层计算原理，为模型解释性和性能优化提供新思路。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注通过规模提升LLM性能，缺乏对其学习算法的理论与实证理解，存在研究空白。

Method: 提出AlgEval框架，结合自上而下的假设生成与自下而上的电路级分析（如注意力模式与隐藏状态），研究LLM的算法组成。

Result: 案例研究表明，AlgEval能揭示LLM的搜索算法等底层计算机制，为模型解释性提供新方法。

Conclusion: AlgEval为理解LLM的计算原理提供系统路径，有望推动更高效的训练方法和新架构设计。

Abstract: What algorithms do LLMs actually learn and use to solve problems? Studies
addressing this question are sparse, as research priorities are focused on
improving performance through scale, leaving a theoretical and empirical gap in
understanding emergent algorithms. This position paper proposes AlgEval: a
framework for systematic research into the algorithms that LLMs learn and use.
AlgEval aims to uncover algorithmic primitives, reflected in latent
representations, attention, and inference-time compute, and their algorithmic
composition to solve task-specific problems. We highlight potential
methodological paths and a case study toward this goal, focusing on emergent
search algorithms. Our case study illustrates both the formation of top-down
hypotheses about candidate algorithms, and bottom-up tests of these hypotheses
via circuit-level analysis of attention patterns and hidden states. The
rigorous, systematic evaluation of how LLMs actually solve tasks provides an
alternative to resource-intensive scaling, reorienting the field toward a
principled understanding of underlying computations. Such algorithmic
explanations offer a pathway to human-understandable interpretability, enabling
comprehension of the model's internal reasoning performance measures. This can
in turn lead to more sample-efficient methods for training and improving
performance, as well as novel architectures for end-to-end and multi-agent
systems.

</details>


### [20] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala,Jordi Planes,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 论文探讨了机器学习中规则模型解释的负面影响，并提出算法分析这些问题的存在。


<details>
  <summary>Details</summary>
Motivation: 在机器学习的高风险领域，错误的解释可能误导决策者，因此需要严格的分析。

Method: 开发算法分析规则模型中的负面特征，如重叠和冗余。

Result: 研究发现广泛使用的规则学习工具会引入负面特征。

Conclusion: 规则模型的解释需进一步优化以避免负面影响。

Abstract: A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


### [21] [Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs](https://arxiv.org/abs/2507.07595)
*Zhixiang Su,Di Wang,Chunyan Miao*

Main category: cs.AI

TL;DR: 提出了一种名为Context Pooling的新方法，用于提升GNN在知识图谱链接预测中的性能，首次将图池化应用于知识图谱，并在42/48的设定中达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型在知识图谱链接预测中，普通聚合方法对性能影响有限，需要更有效的方法。

Method: 引入Context Pooling，设计邻域精度和邻域召回率两个指标，筛选逻辑相关的邻居进行链接预测。

Result: 在三个公开数据集上应用于两个SOTA模型，42/48的设定中性能最优。

Conclusion: Context Pooling是一种通用且高效的方法，显著提升了GNN在知识图谱链接预测中的表现。

Abstract: Recent investigations on the effectiveness of Graph Neural Network
(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that
vanilla aggregation does not significantly impact the model performance. In
this paper, we introduce a novel method, named Context Pooling, to enhance
GNN-based models' efficacy for link predictions in KGs. To our best of
knowledge, Context Pooling is the first methodology that applies graph pooling
in KGs. Additionally, Context Pooling is first-of-its-kind to enable the
generation of query-specific graphs for inductive settings, where testing
entities are unseen during training. Specifically, we devise two metrics,
namely neighborhood precision and neighborhood recall, to assess the neighbors'
logical relevance regarding the given queries, thereby enabling the subsequent
comprehensive identification of only the logically relevant neighbors for link
prediction. Our method is generic and assessed by being applied to two
state-of-the-art (SOTA) models on three public transductive and inductive
datasets, achieving SOTA performance in 42 out of 48 settings.

</details>


### [22] [Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.07599)
*Sedigh Khademi,Jim Black,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila*

Main category: cs.AI

TL;DR: 研究评估了微调的Llama 3.2模型，用于从急诊分诊记录中提取疫苗相关信息，以支持近实时疫苗安全监测。微调模型在提取疫苗名称的准确性上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 支持高效的疫苗安全监测和早期发现免疫后不良事件。

Method: 使用提示工程创建标注数据集，比较提示工程模型、微调模型和基于规则的方法。

Result: 微调的Llama 3B参数模型表现最佳，量化技术使其在资源受限环境中高效部署。

Conclusion: 大语言模型在自动化数据提取方面具有潜力，可提升疫苗安全监测效率。

Abstract: This study evaluates fine-tuned Llama 3.2 models for extracting
vaccine-related information from emergency department triage notes to support
near real-time vaccine safety surveillance. Prompt engineering was used to
initially create a labeled dataset, which was then confirmed by human
annotators. The performance of prompt-engineered models, fine-tuned models, and
a rule-based approach was compared. The fine-tuned Llama 3 billion parameter
model outperformed other models in its accuracy of extracting vaccine names.
Model quantization enabled efficient deployment in resource-constrained
environments. Findings demonstrate the potential of large language models in
automating data extraction from emergency department notes, supporting
efficient vaccine safety surveillance and early detection of emerging adverse
events following immunization issues.

</details>


### [23] [Towards conservative inference in credal networks using belief functions: the case of credal chains](https://arxiv.org/abs/2507.07619)
*Marco Sangalli,Thomas Krak,Cassio de Campos*

Main category: cs.AI

TL;DR: 提出了一种基于Dempster-Shafer理论的信念推理框架，用于处理信用网络中的不确定性，重点研究了链状网络。


<details>
  <summary>Details</summary>
Motivation: 探索如何在信用网络中更高效地传播不确定性，并比较信念推理与传统敏感性分析的优劣。

Method: 提出了一种新框架，利用信念和似然函数在链状网络中传播不确定性，并进行了数值实验。

Result: 结果表明该方法在计算速度和不确定性表示上具有优势，但也存在局限性。

Conclusion: 该框架为信用网络中的信念推理提供了实用工具，但其适用性仍需进一步研究。

Abstract: This paper explores belief inference in credal networks using Dempster-Shafer
theory. By building on previous work, we propose a novel framework for
propagating uncertainty through a subclass of credal networks, namely chains.
The proposed approach efficiently yields conservative intervals through belief
and plausibility functions, combining computational speed with robust
uncertainty representation. Key contributions include formalizing belief-based
inference methods and comparing belief-based inference against classical
sensitivity analysis. Numerical results highlight the advantages and
limitations of applying belief inference within this framework, providing
insights into its practical utility for chains and for credal networks in
general.

</details>


### [24] [PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations](https://arxiv.org/abs/2507.07644)
*Fedor Rodionov,Abdelrahman Eldesokey,Michael Birsak,John Femiani,Bernard Ghanem,Peter Wonka*

Main category: cs.AI

TL;DR: PlanQA是一个用于评估大型语言模型（LLMs）几何和空间推理能力的诊断基准，基于结构化室内场景表示，发现当前LLMs在真实世界布局推理方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在几何和空间推理方面的能力，尤其是在实际场景中的应用。

Method: 使用结构化符号格式（如JSON、XML）编码室内场景，设计多样化问题类型测试度量、拓扑推理及设计约束。

Result: 当前LLMs在浅层查询中表现良好，但在模拟物理约束、保持空间一致性及布局扰动下的泛化能力方面表现不佳。

Conclusion: PlanQA揭示了LLMs在真实世界布局推理中的盲点，为未来语言模型在空间和几何推理方面的改进提供了方向。

Abstract: We introduce PlanQA, a diagnostic benchmark for evaluating geometric and
spatial reasoning in large-language models (LLMs). PlanQA is grounded in
structured representations of indoor scenes, such as kitchens, living rooms,
and bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The
benchmark includes diverse question types that test not only metric and
topological reasoning (e.g., distance, visibility, shortest paths) but also
interior design constraints such as affordance, clearance, balance, and
usability. Our results across a variety of frontier open-source and commercial
LLMs show that while models may succeed in shallow queries, they often fail to
simulate physical constraints, preserve spatial coherence, or generalize under
layout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they
do not consistently reason about real-world layouts. We hope that this
benchmark inspires new work on language models that can accurately infer and
manipulate spatial and geometric properties in practical settings.

</details>


### [25] [Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization](https://arxiv.org/abs/2507.07723)
*Chengtao Jian,Kai Yang,Ye Ouyang,Xiaozhou Ye*

Main category: cs.AI

TL;DR: 本文分析了直接偏好优化（DPO）的理论局限性和动态特性，并提出了一种双层优化框架——稳定偏好优化（SPO），以改进模型对齐的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: DPO虽在实证中表现良好，但其理论性质和内在局限性未被充分研究，尤其是其对初始化的敏感性和概率质量分配问题。

Method: 通过概率演化视角分析DPO的动态特性，提出一个双层优化框架，结合监督微调和增强的DPO目标，并引入正则化方案。

Result: 实验表明，SPO在推理和摘要任务中表现优于标准DPO，提高了推理准确性和对齐一致性。

Conclusion: SPO为偏好对齐目标的设计提供了新思路，有助于实现更可靠和可解释的语言模型对齐。

Abstract: Direct Preference Optimization (DPO) has emerged as a popular and efficient
alternative to reward modeling and reinforcement learning for aligning language
models with human preferences. Despite its empirical success, the theoretical
properties and intrinsic limitations of DPO remain underexplored. In this work,
we first present a comprehensive analysis of DPO's dynamics from a probability
evolution perspective. Our analysis reveals that DPO is highly sensitive to
initialization. It also tends to misallocate probability mass, which can
inadvertently shift probability toward irrelevant or undesired responses. This
misallocation may unintentionally reinforce model bias, thereby compromising
both the stability of model alignment and the consistency with intended
preferences. Motivated by these theoretical findings, we propose a
theoretically grounded bilevel optimization framework that tightly integrate
supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference
optimization. Our approach introduces a principled regularization scheme to
explicitly encourage absolute probability improvement for preferred outputs,
while maintaining stable optimization dynamics. Experiments on challenging
reasoning and summarization benchmarks elucidate that our method consistently
improves reasoning accuracy and better aligns output distributions with
intended preferences, outperforming standard DPO. Stable preference
optimization provides new insights into the design of preference-based
alignment objectives and opens up new avenues towards more reliable and
interpretable language model alignment.

</details>


### [26] [Identification of Violin Reduction via Contour Lines Classification](https://arxiv.org/abs/2507.07743)
*Philémon Beghin,Anne-Emmanuelle Ceulemans,François Glineur*

Main category: cs.AI

TL;DR: 本文提出了一种基于轮廓线分类小提琴是否被缩小的方法，通过几何特征分析，发现区分缩小与非缩小乐器是可行的，其中开口参数β最具预测性。


<details>
  <summary>Details</summary>
Motivation: 研究小提琴制作中的尺寸标准化历史，尤其是缩小乐器对轮廓线的影响，填补了定量研究的空白。

Method: 使用25把小提琴的3D几何网格数据，提取10-20条轮廓线，拟合抛物线曲线并计算特征参数，应用分类方法评估几何特征是否能预测尺寸缩小。

Result: 研究发现基于几何特征可以一定程度区分缩小与非缩小乐器，其中开口参数β最具预测性。

Conclusion: 几何特征可用于小提琴尺寸缩小的分类，但需考虑乐器改造的多样性，未来研究可进一步优化方法。

Abstract: The first violins appeared in late 16th-century Italy. Over the next 200
years, they spread across Europe and luthiers of various royal courts, eager to
experiment with new techniques, created a highly diverse family of instruments.
Around 1750, size standards were introduced to unify violin making for
orchestras and conservatories. Instruments that fell between two standards were
then reduced to a smaller size by luthiers. These reductions have an impact on
several characteristics of violins, in particular on the contour lines, i.e.
lines of constant altitude, which look more like a U for non reduced
instruments and a V for reduced ones. While such differences are observed by
experts, they have not been studied quantitatively.
  This paper presents a method for classifying violins as reduced or
non-reduced based on their contour lines. We study a corpus of 25 instruments
whose 3D geometric meshes were acquired via photogrammetry. For each
instrument, we extract 10-20 contour lines regularly spaced every millimetre.
Each line is fitted with a parabola-like curve (with an equation of the type y
= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)
and how vertically stretched (alpha) the curve is. We compute additional
features from those parameters, using regressions and counting how many values
fall under some threshold. We also deal with outliers and non equal numbers of
levels, and eventually obtain a numerical profile for each instrument.
  We then apply classification methods to assess whether geometry alone can
predict size reduction. We find that distinguishing between reduced and non
reduced instruments is feasible to some degree, taking into account that a
whole spectrum of more or less transformed violins exists, for which it is more
difficult to quantify the reduction. We also find the opening parameter beta to
be the most predictive.

</details>


### [27] [Measuring AI Alignment with Human Flourishing](https://arxiv.org/abs/2507.07787)
*Elizabeth Hilliard,Akshaya Jagadeesh,Alex Cook,Steele Billings,Nicholas Skytland,Alicia Llewellyn,Jackson Paull,Nathan Paull,Nolan Kurylo,Keatra Nesbitt,Robert Gruenewald,Anthony Jantzi,Omar Chavez*

Main category: cs.AI

TL;DR: FAI Benchmark评估AI对人类全面福祉的贡献，涵盖七个维度，发现现有模型在部分维度表现不足。


<details>
  <summary>Details</summary>
Motivation: 传统AI评估聚焦技术能力或危害预防，缺乏对人类福祉的全面衡量。

Method: 通过1,229个主客观问题，结合专家LLM和几何平均评分，评估28个领先语言模型。

Result: 最高分模型72/100，但无模型在所有维度表现良好，尤其在信仰、品德和意义维度。

Conclusion: FAI Benchmark为开发支持人类福祉的AI提供了新框架，对AI伦理和评估有重要意义。

Abstract: This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel
evaluation framework that assesses AI alignment with human flourishing across
seven dimensions: Character and Virtue, Close Social Relationships, Happiness
and Life Satisfaction, Meaning and Purpose, Mental and Physical Health,
Financial and Material Stability, and Faith and Spirituality. Unlike
traditional benchmarks that focus on technical capabilities or harm prevention,
the FAI Benchmark measures AI performance on how effectively models contribute
to the flourishing of a person across these dimensions. The benchmark evaluates
how effectively LLM AI systems align with current research models of holistic
human well-being through a comprehensive methodology that incorporates 1,229
objective and subjective questions. Using specialized judge Large Language
Models (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs
geometric mean scoring to ensure balanced performance across all flourishing
dimensions. Initial testing of 28 leading language models reveals that while
some models approach holistic alignment (with the highest-scoring models
achieving 72/100), none are acceptably aligned across all dimensions,
particularly in Faith and Spirituality, Character and Virtue, and Meaning and
Purpose. This research establishes a framework for developing AI systems that
actively support human flourishing rather than merely avoiding harm, offering
significant implications for AI development, ethics, and evaluation.

</details>


### [28] [MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving](https://arxiv.org/abs/2507.07818)
*Lu Xu,Jiaqian Yu,Xiongfeng Peng,Yiwei Chen,Weiming Li,Jaewook Yoo,Sunghyun Chunag,Dongwook Lee,Daehyun Ji,Chao Zhang*

Main category: cs.AI

TL;DR: MoSE是一种技能导向的混合专家模型，模仿人类驾驶员的学习和推理过程，通过技能分步学习实现高效自动驾驶。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家模型（MoE）需要大量数据和复杂优化，而MoSE通过模仿人类驾驶员的学习过程，提出技能导向的路由机制，减少计算成本。

Method: 定义和标注特定技能，构建分层技能数据集，预训练路由器以实现分步推理，并在单次前向过程中整合辅助任务。

Result: MoSE在CODA AD任务中表现优于8B+参数模型，激活模型规模减少至少62.5%，性能达到SOTA。

Conclusion: MoSE通过技能导向和分步推理，显著提升了自动驾驶模型的效率和性能。

Abstract: Recent studies show large language models (LLMs) and vision language models
(VLMs) trained using web-scale data can empower end-to-end autonomous driving
systems for a better generalization and interpretation. Specifically, by
dynamically routing inputs to specialized subsets of parameters, the
Mixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve
substantial performance improvements while maintaining computational
efficiency. However, general MoE models usually demands extensive training data
and complex optimization. In this work, inspired by the learning process of
human drivers, we propose a skill-oriented MoE, called MoSE, which mimics human
drivers' learning process and reasoning process, skill-by-skill and
step-by-step. We propose a skill-oriented routing mechanism that begins with
defining and annotating specific skills, enabling experts to identify the
necessary driving competencies for various scenarios and reasoning tasks,
thereby facilitating skill-by-skill learning. Further align the driving process
to multi-step planning in human reasoning and end-to-end driving models, we
build a hierarchical skill dataset and pretrain the router to encourage the
model to think step-by-step. Unlike multi-round dialogs, MoSE integrates
valuable auxiliary tasks (e.g.\ description, reasoning, planning) in one single
forward process without introducing any extra computational cost. With less
than 3B sparsely activated parameters, our model outperforms several 8B+
parameters on CODA AD corner case reasoning task. Compared to existing methods
based on open-source models and data, our approach achieves state-of-the-art
performance with significantly reduced activated model size (at least by
$62.5\%$) with a single-turn conversation.

</details>


### [29] [AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift](https://arxiv.org/abs/2507.07820)
*Eunsu Baek,Keondo Park,Jeonggil Ko,Min-hwan Oh,Taesik Gong,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: 论文提出自适应感知作为AI可持续发展的新范式，通过动态调整传感器参数提升效率，减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前AI依赖大规模模型和数据集，带来环境、经济和伦理问题，需要更可持续的解决方案。

Method: 借鉴生物感官系统，动态调整传感器参数（如曝光、灵敏度），减少数据需求并提升模型性能。

Result: 实验证明自适应感知能让小模型超越大模型性能，同时降低资源消耗。

Conclusion: 论文呼吁将自适应感知融入实际应用，并提出研究方向以实现可持续、鲁棒和公平的AI系统。

Abstract: Current AI advances largely rely on scaling neural models and expanding
training datasets to achieve generalization and robustness. Despite notable
successes, this paradigm incurs significant environmental, economic, and
ethical costs, limiting sustainability and equitable access. Inspired by
biological sensory systems, where adaptation occurs dynamically at the input
(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive
sensing as a necessary and foundational shift. Adaptive sensing proactively
modulates sensor parameters (e.g., exposure, sensitivity, multimodal
configurations) at the input level, significantly mitigating covariate shifts
and improving efficiency. Empirical evidence from recent studies demonstrates
that adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass
substantially larger models (e.g., OpenCLIP-H) trained with significantly more
data and compute. We (i) outline a roadmap for broadly integrating adaptive
sensing into real-world applications spanning humanoid, healthcare, autonomous
systems, agriculture, and environmental monitoring, (ii) critically assess
technical and ethical integration challenges, and (iii) propose targeted
research directions, such as standardized benchmarks, real-time adaptive
algorithms, multimodal integration, and privacy-preserving methods.
Collectively, these efforts aim to transition the AI community toward
sustainable, robust, and equitable artificial intelligence systems.

</details>


### [30] [Searching for actual causes: Approximate algorithms with adjustable precision](https://arxiv.org/abs/2507.07857)
*Samuel Reyd,Ada Diaconescu,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 论文提出了一种多项式复杂度的算法，用于识别实际原因，解决了现有方法无法处理的非布尔、黑盒和随机系统问题。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能（XAI）和因果关系的文献未能满足非专家用户对实际原因的需求，且现有方法在形式化和计算复杂度上存在不足。

Method: 提出了一组多项式复杂度的算法，可调节精度和全面性，适用于非布尔、黑盒和随机系统。

Result: 实验表明，算法能识别现有方法无法处理的系统原因，且可通过增加计算时间提高精度和全面性。

Conclusion: 该算法为解决实际原因识别问题提供了高效且灵活的解决方案。

Abstract: Causality has gained popularity in recent years. It has helped improve the
performance, reliability, and interpretability of machine learning models.
However, recent literature on explainable artificial intelligence (XAI) has
faced criticism. The classical XAI and causality literature focuses on
understanding which factors contribute to which consequences. While such
knowledge is valuable for researchers and engineers, it is not what non-expert
users expect as explanations. Instead, these users often await facts that cause
the target consequences, i.e., actual causes. Formalizing this notion is still
an open problem. Additionally, identifying actual causes is reportedly an
NP-complete problem, and there are too few practical solutions to approximate
formal definitions. We propose a set of algorithms to identify actual causes
with a polynomial complexity and an adjustable level of precision and
exhaustiveness. Our experiments indicate that the algorithms (1) identify
causes for different categories of systems that are not handled by existing
approaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be
adjusted to gain more precision and exhaustiveness with more computation time.

</details>


### [31] [An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis](https://arxiv.org/abs/2507.07893)
*Mingda Zhang,Na Zhao,Jianglong Qing,Qing xu,Kaiwen Pan,Ting luo*

Main category: cs.AI

TL;DR: 论文提出了一种结合提示工程和多维知识图谱的框架，以解决大语言模型在法律纠纷分析中的局限性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在法律纠纷分析中存在法律知识表示不足、概念理解有限和推理缺陷等问题，亟需改进。

Method: 采用三阶段分层提示结构（任务定义、知识背景、推理指导）和三层次知识图谱架构（分类本体、表示层、实例层），结合四种法律概念检索方法。

Result: 实验结果显示，该框架在法律纠纷分析中显著提升了性能，能够准确分析复杂案例的法律适用。

Conclusion: 该研究为智能法律辅助系统的实现提供了新的技术途径。

Abstract: The rapid development of artificial intelligence has positioned large
language models as fundamental components of intelligent legal systems.
However, these models face significant limitations in legal dispute analysis,
including insufficient legal knowledge representation, limited concept
understanding, and reasoning deficiencies. This research proposes an enhanced
framework integrating prompt engineering with multidimensional knowledge
graphs. The framework introduces a three-stage hierarchical prompt structure
comprising task definition, knowledge background, and reasoning guidance,
supplemented by legal-specific reasoning templates and dynamic optimization
mechanisms. A three-layer knowledge graph architecture is constructed with
legal classification ontology, representation, and instance layers. Four
complementary methods enable precise legal concept retrieval: direct legal norm
code matching, domain-specific semantic vector similarity, ontology-based path
reasoning, and specialized lexical segmentation. These components integrate
with web search technology to establish a knowledge-enhanced framework for
legal decision-making. Experimental results demonstrate significant performance
improvements in legal dispute analysis, enabling accurate legal application
analysis for complex cases while exhibiting nuanced understanding of judicial
decision-making logic, providing a novel technical approach for implementing
intelligent legal assistance systems.

</details>


### [32] [Meek Models Shall Inherit the Earth](https://arxiv.org/abs/2507.07931)
*Hans Gundlach,Jayson Lynch,Neil Thompson*

Main category: cs.AI

TL;DR: 论文认为，随着计算资源投入的边际效益递减，AI模型性能将趋于收敛，小型模型（计算预算有限）将接近顶级模型的性能。


<details>
  <summary>Details</summary>
Motivation: 探讨AI模型性能不平等问题，提出计算资源边际效益递减将导致性能收敛的观点。

Method: 开发模型分析计算资源边际效益，结合基准数据和理论模型验证。

Result: 计算资源投入的边际效益显著下降，小型模型性能将接近顶级模型。

Conclusion: AI战略和政策需重新审视，以适应小型模型性能提升的趋势。

Abstract: The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.

</details>


### [33] [Working with AI: Measuring the Occupational Implications of Generative AI](https://arxiv.org/abs/2507.07935)
*Kiran Tomlinson,Sonia Jaffe,Will Wang,Scott Counts,Siddharth Suri*

Main category: cs.AI

TL;DR: 研究分析了生成式AI对经济的影响，通过用户与Microsoft Bing Copilot的对话数据，发现AI主要应用于信息收集和写作，并计算了各职业的AI适用性得分。


<details>
  <summary>Details</summary>
Motivation: 理解AI对经济的影响是社会的关键问题，研究旨在通过分析AI在工作活动中的应用及其效果，为这一问题提供初步答案。

Method: 分析了20万条用户与Microsoft Bing Copilot的匿名对话数据，结合职业活动分类、任务成功率和影响范围，计算了各职业的AI适用性得分。

Result: AI最常用于信息提供、写作、教学和咨询；知识型职业（如计算机、数学、行政支持）和销售职业的AI适用性得分最高。

Conclusion: 研究表明AI在知识型职业中的应用潜力较大，同时揭示了AI适用性与工资和教育水平的相关性。

Abstract: Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [34] [Sparse Signal Recovery From Quadratic Systems with Full-Rank Matrices](https://arxiv.org/abs/2507.07557)
*Jinming Wen,Yi Hu,Meng Huang*

Main category: cs.IT

TL;DR: 本文提出了一种基于稀疏性的两阶段算法（SGN），用于从二次测量中恢复高维稀疏信号，理论保证和实验表现均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决高维信号（m≪n）从二次测量中恢复的挑战，利用信号稀疏性提升恢复效率和准确性。

Method: 提出两阶段SGN算法：1) 支持限制的光谱初始化；2) 迭代硬阈值Gauss-Newton方法，实现二次收敛。

Result: 理论保证m≥2s（实数）或m≥4s-2（复数）即可恢复稀疏信号；实验显示SGN在测量数和计算效率上优于现有方法。

Conclusion: SGN算法在稀疏信号恢复中具有近最优采样复杂度和高效计算性能，显著优于现有方法。

Abstract: In signal processing and data recovery, reconstructing a signal from
quadratic measurements poses a significant challenge, particularly in
high-dimensional settings where measurements $m$ is far less than the signal
dimension $n$ (i.e., $m \ll n$). This paper addresses this problem by
exploiting signal sparsity. Using tools from algebraic geometry, we derive
theoretical recovery guarantees for sparse quadratic systems, showing that
$m\ge 2s$ (real case) and $m\ge 4s-2$ (complex case) generic measurements
suffice to uniquely recover all $s$-sparse signals. Under a Gaussian
measurement model, we propose a novel two-stage Sparse Gauss-Newton (SGN)
algorithm. The first stage employs a support-restricted spectral
initialization, yielding an accurate initial estimate with $m=O(s^2\log{n})$
measurements. The second stage refines this estimate via an iterative
hard-thresholding Gauss-Newton method, achieving quadratic convergence to the
true signal within finitely many iterations when $m\ge O(s\log{n})$. Compared
to existing second-order methods, our algorithm achieves near-optimal sampling
complexity for the refinement stage without requiring resampling. Numerical
experiments indicate that SGN significantly outperforms state-of-the-art
algorithms in both accuracy and computational efficiency. In particular, (1)
when sparsity level $s$ is high, compared with existing algorithms, SGN can
achieve the same success rate with fewer measurements. (2) SGN converges with
only about $1/10$ iterations of the best existing algorithm and reach lower
relative error.

</details>


### [35] [Secure Cooperative Gradient Coding: Optimality, Reliability, and Global Privacy](https://arxiv.org/abs/2507.07565)
*Shudi Weng*

Main category: cs.IT

TL;DR: 本文提出了一种名为SecCoGC的方法，用于解决隐私敏感的联邦学习中的安全聚合和通信不可靠问题，同时引入Fair-SecCoGC确保隐私公平性。


<details>
  <summary>Details</summary>
Motivation: 研究隐私敏感的联邦学习在不可靠通信下的安全聚合和滞后节点问题，避免因通信故障导致模型精度下降和目标不一致。

Method: 提出Secure Cooperative Gradient Coding (SecCoGC)方法，支持任意强度的隐私保护，并在不可靠通信下实现鲁棒的滞后节点处理。进一步扩展为Fair-SecCoGC，确保隐私保护的公平性。

Result: SecCoGC在任意强隐私保护下对不可靠通信表现出强鲁棒性，性能优于现有方法20%-70%。

Conclusion: 本文不仅提出了SecCoGC和Fair-SecCoGC，还提供了全面的隐私分析和性能验证，证明了其在实际部署中的有效性。

Abstract: This paper studies privacy-sensitive federated learning (FL) with unreliable
communication, focusing on secure aggregation and straggler mitigation. While
secure aggregation cryptographically reconstructs the global model without
exposing client updates, random link failures disrupt its key coordination,
degrading model accuracy. Moreover, unreliable communication can lead to
objective inconsistency, causing the global model to converge to arbitrary,
sub-optimal points far from the intended optimum. This paper proposes Secure
Cooperative Gradient Coding (SecCoGC), a practical solution that achieves
secure aggregation with arbitrarily strong privacy guarantees and robust
straggler mitigation under unreliable communication. SecCoGC operates natively
in the real field, making it directly applicable to practical deployments. To
ensure equitable privacy protection across clients, we further introduce
Fair-SecCoGC, an extension that enforces fairness in the level of privacy
offered to all users. To conclude, this paper formally formulates the problem
of secure aggregation in the real field and presents both general and
computationally efficient key construction methods. Moreover, it provides a
comprehensive privacy analysis under Local Mutual Information Privacy (LMIP)
and Local Differential Privacy (LDP) across all protocol layers. Robustness and
convergence properties are also rigorously analyzed. Finally, extensive
simulations are performed across diverse network conditions and benchmark
datasets to validate the effectiveness of the proposed methods. The results
show that SecCoGC achieves strong robustness to unreliable communication under
arbitrarily strong privacy guarantees. It outperforms existing
privacy-preserving methods with performance gains of up to 20\%-70\%.

</details>


### [36] [Linear codes for $b$-symbol read channels attaining the Griesmer bound](https://arxiv.org/abs/2507.07728)
*Sascha Kurz*

Main category: cs.IT

TL;DR: 研究了在$b$-符号度量下线性码的最优参数，特别是在最小距离足够大的情况下，并针对小维度确定了二进制线性码在双符号度量下的最优参数。


<details>
  <summary>Details</summary>
Motivation: 研究$b$-符号度量下的编码问题，特别是在存储等应用中的重要性。

Method: 通过理论分析确定线性码在$b$-符号度量下的最优参数，并针对小维度进行具体计算。

Result: 确定了在最小距离足够大时线性码的最优参数，以及小维度下二进制线性码在双符号度量下的最优参数。

Conclusion: 为$b$-符号度量下的编码问题提供了理论支持，尤其是在存储应用中具有重要意义。

Abstract: Reading channels where $b$-tuples of adjacent symbols are read at every step
have e.g.\ applications in storage. Corresponding bounds and constructions of
codes for the $b$-symbol metric, especially the pair-symbol metric where $b=2$,
were intensively studied in the last fifteen years. Here we determine the
optimal code parameters of linear codes in the $b$-symbol metric assuming that
the minimum distance is sufficiently large. We also determine the optimal
parameters of linear binary codes in the pair-symbol metric for small
dimensions.

</details>


### [37] [Generalized bilateral multilevel construction for constant dimension codes from parallel mixed dimension construction](https://arxiv.org/abs/2507.07842)
*Han Li,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 本文提出了一种改进的并行混合维度构造方法，用于优化恒定维度码（CDC），并构造了优于已知结果的新码。


<details>
  <summary>Details</summary>
Motivation: 恒定维度码在随机网络编码中有广泛应用，但其最大尺寸的确定仍是一个基本问题。本文旨在通过改进构造方法提升CDC的性能。

Method: 引入双边标识向量的选择标准，结合并行混合维度构造，并利用广义双边多级构造进行高效改进。

Result: 构造了许多优于先前最佳已知码的新CDC。

Conclusion: 通过改进的构造方法，显著提升了CDC的性能，为随机网络编码提供了更优的解决方案。

Abstract: Constant dimension codes (CDCs), as special subspace codes, have received
extensive attention due to their applications in random network coding. The
basic problem of CDCs is to determine the maximal possible size
$A_q(n,d,\{k\})$ for given parameters $q, n, d$, and $k$. This paper introduces
criteria for choosing appropriate bilateral identifying vectors compatible with
the parallel mixed dimension construction (Des. Codes Cryptogr. 93(1):227--241,
2025). We then utilize the generalized bilateral multilevel construction (Des.
Codes Cryptogr. 93(1):197--225, 2025) to improve the parallel mixed dimension
construction efficiently. Many new CDCs that are better than the previously
best-known codes are constructed.

</details>
