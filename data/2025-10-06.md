<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Interplay between Security, Privacy and Trust in 6G-enabled Intelligent Transportation Systems](https://arxiv.org/abs/2510.02487)
*Ahmed Danladi Abdullahi,Erfan Bahrami,Tooska Dargahi,Mohammed Al-Khalidi,Mohammad Hammoudeh*

Main category: cs.NI

TL;DR: 本文综述了6G智能交通系统的机遇与挑战，特别关注信任、安全和隐私问题，分析了量子技术带来的双重影响，提出了6G-ITS攻击模型分类，并强调了构建多层安全框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 6G技术有望彻底改变交通行业，但存在各种安全和隐私挑战，需要解决这些问题才能确保6G-ITS的安全部署和公众信任。

Method: 回顾6G-ITS的机会与挑战，重点关注信任、安全和隐私；提出6G-ITS攻击模型分类；比较5G-ITS和6G-ITS的安全威胁；讨论潜在缓解方案。

Result: 识别了6G-ITS中的关键安全挑战，提出了攻击模型分类，比较了5G和6G的安全威胁差异，并强调了量子技术既增强安全又引入新漏洞的双重作用。

Conclusion: 迫切需要构建一个全面的多层安全框架，涵盖物理基础设施保护、网络协议安全、数据管理保障、应用安全措施和信任管理系统，以有效缓解新兴安全和隐私风险。

Abstract: The advancement of 6G technology has the potential to revolutionize the
transportation sector and significantly improve how we travel. 6G-enabled
Intelligent Transportation Systems (ITS) promise to offer high-speed,
low-latency communication and advanced data analytics capabilities, supporting
the development of safer, more efficient, and more sustainable transportation
solutions. However, various security and privacy challenges were identified in
the literature that must be addressed to enable the safe and secure deployment
of 6G-ITS and ensure people's trust in using these technologies. This paper
reviews the opportunities and challenges of 6G-ITS, particularly focusing on
trust, security, and privacy, with special attention to quantum technologies
that both enhance security through quantum key distribution and introduce new
vulnerabilities. It discusses the potential benefits of 6G technology in the
transportation sector, including improved communication, device
interoperability support, data analytic capabilities, and increased automation
for different components, such as transportation management and communication
systems. A taxonomy of different attack models in 6G-ITS is proposed, and a
comparison of the security threats in 5G-ITS and 6G-ITS is provided, along with
potential mitigating solutions. This research highlights the urgent need for a
comprehensive, multi-layered security framework spanning physical
infrastructure protection, network protocol security, data management
safeguards, application security measures, and trust management systems to
effectively mitigate emerging security and privacy risks and ensure the
integrity and resilience of future transportation ecosystems.

</details>


### [2] [L4Span: Spanning Congestion Signaling over NextG Networks for Interactive Applications](https://arxiv.org/abs/2510.02682)
*Haoran Wan,Kyle Jamieson*

Main category: cs.NI

TL;DR: L4Span是一种新的无线接入网络(RAN)设计，通过抽象RAN队列复杂性，将RAN队列状态与端到端低延迟信令绑定，显著降低延迟并保持高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 有线宽带ISP正在部署先进的队列占用信令机制，但蜂窝无线接入网络(RAN)作为重要的最后一英里网络，在这些努力中落后。需要一种能够增量部署且兼容现有标准的RAN低延迟解决方案。

Method: L4Span在毫秒级时间尺度上预测RAN的队列占用情况，为低延迟和经典流量执行ECN标记。该设计轻量级，需要最小的RAN修改，并保持3GPP和O-RAN兼容性。在srsRAN开源软件上使用C++实现原型。

Result: 在各种无线信道条件下评估，L4Span将低延迟和经典流量的单向延迟降低高达98%，同时保持接近线速的吞吐量。

Conclusion: L4Span提供了一种有效的方法来改善RAN中的延迟性能，通过简单的接口抽象RAN队列复杂性，实现端到端低延迟信令，且易于部署。

Abstract: Design for low latency networking is essential for tomorrow's interactive
applications, but it is essential to deploy incrementally and universally at
the network's last mile. While wired broadband ISPs are rolling out the leading
queue occupancy signaling mechanisms, the cellular Radio Access Network (RAN),
another important last mile to many users, lags behind these efforts. This
paper proposes a new RAN design, L4Span, that abstracts the complexities of RAN
queueing in a simple interface, thus tying the queue state of the RAN to
end-to-end low-latency signaling all the way back to the content server. At
millisecond-level timescales, L4Span predicts the RAN's queuing occupancy and
performs ECN marking for both low-latency and classic flows. L4Span is
lightweight, requiring minimal RAN modifications, and remains 3GPP and O-RAN
compliant for maximum ease of deployment. We implement a prototype on the
srsRAN open-source software in C++. Our evaluation compares the performance of
low-latency as well as classic flows with or without the deployment of L4Span
in various wireless channel conditions. Results show that L4Span reduces the
one-way delay of both low-latency and classic flows by up to 98 %, while
simultaneously maintaining near line-rate throughput. The code is available at
https://github.com/PrincetonUniversity/L4Span.

</details>


### [3] [FSMA: Scalable and Reliable LoRa for Non-Terrestrial Networks with Mobile Gateways](https://arxiv.org/abs/2510.02800)
*Rohith Reddy Vennam,Maiyun Zhang,Raghav Subbaraman,Deepak Vashist,Dinesh Bharadia*

Main category: cs.NI

TL;DR: 提出了Free Signal Multiple Access (FSMA)协议，通过轻量级的自由信号啁啾(FreeChirp)实现网关控制的链路感知接入，解决非地面网络中碰撞和动态链路问题。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星和无人机网络面临两大挑战：大覆盖范围导致频繁碰撞，移动网关造成动态链路，需要免同步的链路感知传输。现有随机接入方案如ALOHA、CSMA、BSMA在此场景下表现不佳。

Method: FSMA协议引入轻量级FreeChirp信号，确保节点仅在信道空闲且链路可靠时传输，无需同步或复杂调度即可实现链路感知接入。

Result: 使用25个商用LoRa设备和无人机移动网关测试，FSMA相比基线方案吞吐量提升2倍，包接收率提升2-5倍，能效提升5倍。大规模仿真显示可扩展到5000+设备。

Conclusion: FSMA是实现可扩展、高能效、可靠非地面网络物联网的实际解决方案。

Abstract: The proliferation of Low Earth Orbit (LEO) satellites for universal IoT
applications and the growing use of drones in emergency services, agriculture,
and military operations highlight the transformative potential of
non-terrestrial networks (NTN). However, these networks face two key
challenges: (1) large coverage footprints that create frequent collisions and
(2) moving gateways that cause dynamic links and demand synchronization-free,
link-aware transmissions. Existing random access schemes such as ALOHA, CSMA,
and BSMA fail in this setting, suffering from high collision rates, hidden
terminals, or excessive gateway energy overhead. We propose Free Signal
Multiple Access (FSMA), a gateway-controlled protocol that introduces a
lightweight free signal chirp (FreeChirp). FreeChirp ensures that nodes
transmit only when the channel is idle and when links are reliable, thereby
reducing collisions and enabling link-aware access without the need for
synchronization or complex scheduling. We evaluate FSMA using 25 commercial
LoRa devices with a drone-mounted moving gateway and demonstrate up to 2x
higher throughput, 2x to 5x better packet reception ratio, and 5x improved
energy efficiency compared to the baselines. Large-scale simulations with a
custom Satellite IoT Simulator further show that FSMA scales to 5000+ devices
per satellite pass. These results establish FSMA as a practical step toward
scalable, energy-efficient, and reliable NTN IoT networks.

</details>


### [4] [DH-EAC: Design of a Dynamic, Hierarchical Entanglement Access Control Protocol](https://arxiv.org/abs/2510.02895)
*Akihisa Takahashi,Yoshito Tobe*

Main category: cs.NI

TL;DR: DH-EAC是一种纯量子协议，用于在由多个量子局域网组成的广域量子网络中公平匿名地分配稀缺纠缠资源。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Dicke态的纯量子MAC协议主要针对单个量子局域网，难以扩展到广域动态环境，且需要避免后选择协调问题。

Method: 采用两层纯量子抽签机制：外层选择获胜的量子局域网，内层选择每个获胜局域网内的获胜节点。关键设计原则是通过测量固定获胜集合和每个局域网的配额，无需经典往返通信。

Result: 在标准独立同分布损耗模型下，协议实现了匿名性和公平性，相比单层Dicke协议和经典GO驱动分配器，在成功概率、端到端延迟、吞吐量和公平性指标方面表现良好。

Conclusion: DH-EAC在纠缠访问控制领域提供了一个可实施的设计点，平衡了纯量子竞争解决、匿名性和多量子局域网网络的可扩展性。

Abstract: We propose Dynamic, Hierarchical Entanglement Access Control (DH-EAC), a
pure-quantum protocol for fair and anonymous allocation of scarce entanglement
across wide-area quantum networks composed of many quantum LANs (QLANs). Prior
Dicke-state-based pure-quantum MACs resolve contention by local measurements
without classical signaling, but they mainly target a single QLAN under static
conditions; extending them to wide-area, dynamic settings while avoiding
post-selection reconciliation remains open. DH-EAC adopts a two-layer
pure-quantum lottery: the outer layer selects winning QLANs and the inner layer
selects winning nodes within each winning QLAN. A key design principle is that
both the winning set and the per-QLAN quota are fixed by measurements alone, so
the contention loop requires no classical round trip. The protocol thus aims to
jointly satisfy anonymity (no node IDs revealed until decisions are fixed) and
fairness (bias suppression under heterogeneous QLAN sizes). We also provide
analytical models for success probability and latency under a standard i.i.d.
loss model, and we evaluate DH-EAC against two baselines - single-layer Dicke
within one QLAN and a classical GO-driven allocator - using a minimal,
reproducible set of scenarios. Metrics include success probability, end-to-end
latency, throughput, and Jain's fairness index. The results indicate that
DH-EAC offers an implementable design point in the space of entanglement access
control, balancing pure-quantum contention resolution, anonymity, and
scalability for multi-QLAN networks.

</details>


### [5] [Sequence-Based Deep Learning for Handover Optimization in Dense Urban Cellular Network](https://arxiv.org/abs/2510.02958)
*Muhammad Kabeer,Rosdiadee Nordin,Mehran Behjati,Lau Sian Lun*

Main category: cs.NI

TL;DR: 该论文利用真实世界多运营商路测数据集，评估基于序列的深度学习方法用于密集城市蜂窝网络中的切换检测和避免，GRU模型在减少乒乓切换和不必要切换方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 密集城市蜂窝网络中高小区密度、用户移动性和多样化服务需求增加了不必要切换和乒乓效应的可能性，需要高效的切换管理解决方案。

Method: 将切换预测制定为序列问题，在仅使用参考信号接收功率(RSRP)和全特征设置下评估门控循环单元(GRU)、长短期记忆网络(LSTM)和Transformer架构。

Result: 基于GRU的模型实现了98%的乒乓切换减少和46.25%的不必要切换减少，驻留时间(ToS)改善46%，推理时间仅0.91秒，显著优于传统3GPP A3算法。

Conclusion: 多维特征的集成显著提升了密集城市蜂窝网络中的切换性能，该解决方案高效且适合实时边缘部署，在移动鲁棒性和用户体验质量方面取得显著提升。

Abstract: Efficient handover management remains a critical challenge in dense urban
cellular networks, where high cell density, user mobility, and diverse service
demands increase the likelihood of unnecessary handovers and ping-pong effects.
This paper leverages a real-world, multi-operator drive-test dataset of 30,925
labelled records collected within a 2 km area around Sunway City to investigate
sequence-based deep learning approaches for handover detection and avoidance.
We formulate handover prediction as a sequence problem and evaluate Gated
Recurrent Unit (GRU), Long Short-Term Memory (LSTM), and Transformer
architectures under Reference Signal Received Power (RSRP)-only and all-feature
settings. The integration of multi-dimensional features significantly enhanced
handover performance in dense urban cellular networks. The proposed GRU-based
model achieved a remarkable 98% reduction in ping-pong handovers, alongside a
46.25% decrease in unnecessary handovers, outperforming the baseline RSRP-only
approach which yielded a 22.19% reduction. Furthermore, the model demonstrated
a 46% improvement in Time of Stay (ToS), indicating more stable user
connections. With an inference time of just 0.91 seconds, the solution proves
highly efficient and well-suited for real-time edge deployment scenarios.
Compared to the conventional 3GPP A3 algorithm, these improvements demonstrate
significant gains in mobility robustness and user Quality of Experience (QoE)
improvement. The dataset is released to foster reproducibility and further
research in intelligent mobility management for 5G and beyond.

</details>


### [6] [Automatic Generation of Digital Twins for Network Testing](https://arxiv.org/abs/2510.03205)
*Shenjia Ding,David Flynn,Paul Harvey*

Main category: cs.NI

TL;DR: 本文探索了自动生成数字孪生技术，为电信网络软件测试提供高效准确的验证工具，符合ITU-T自主网络架构的实验子系统要求。


<details>
  <summary>Details</summary>
Motivation: 随着电信网络运营管理中软件使用的增加，对部署前测试和验证的需求显著增长。数字孪生虽然能提供测试环境，但配置和执行需要大量时间和人力投入。

Method: 提出了自动生成数字孪生的方法，基于ITU-T自主网络架构的实验子系统，通过自动化流程创建数字孪生环境。

Result: 通过初步用例的实验结果表明，该方法能够自动创建高效的数字孪生，具有足够的准确性，可以纳入现有的验证流程中。

Conclusion: 自动生成数字孪生的方法是可行的，能够为电信网络软件的测试和验证提供有效的工具支持。

Abstract: The increased use of software in the operation and management of
telecommunication networks has moved the industry one step closer to realizing
autonomous network operation. One consequence of this shift is the
significantly increased need for testing and validation before such software
can be deployed. Complementing existing simulation or hardware-based
approaches, digital twins present an environment to achieve this testing;
however, they require significant time and human effort to configure and
execute. This paper explores the automatic generation of digital twins to
provide efficient and accurate validation tools, aligned to the ITU-T
autonomous network architecture's experimentation subsystem. We present
experimental results for an initial use case, demonstrating that the approach
is feasible in automatically creating efficient digital twins with sufficient
accuracy to be included as part of existing validation pipelines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks](https://arxiv.org/abs/2510.02418)
*Sagnik Anupam,Davis Brown,Shuo Li,Eric Wong,Hamed Hassani,Osbert Bastani*

Main category: cs.AI

TL;DR: BrowserArena是一个开放的网页代理评估平台，通过收集用户提交的任务、进行Arena式对比测试和使用步骤级人工反馈来识别代理的失败模式。研究发现网页代理存在三个主要失败模式：验证码识别、弹窗横幅移除和直接URL导航。


<details>
  <summary>Details</summary>
Motivation: 当前网页代理评估局限于沙盒环境或人工任务，需要真实的开放网络环境来评估代理性能并识别失败模式。

Method: 构建BrowserArena平台，收集用户提交的真实任务，进行Arena式对比测试，通过步骤级人工反馈分析代理轨迹，并针对发现的失败模式构建专门数据集进行深入研究。

Result: 识别出三个一致的失败模式：验证码识别、弹窗移除和直接URL导航。发现不同语言模型在这些任务上的表现差异，如o4-mini使用更多策略绕过验证码，而DeepSeek-R1在验证码解决上误导用户。

Conclusion: 当前网页代理表现出多样性和脆弱性，该基准测试方法为大规模评估和理解网页代理失败模式提供了有效途径。

Abstract: LLM web agents now browse and take actions on the open web, yet current agent
evaluations are constrained to sandboxed environments or artificial tasks. We
introduce BrowserArena, a live open-web agent evaluation platform that collects
user-submitted tasks, runs Arena-style head-to-head comparisons, and uses
step-level human feedback to surface failure modes. Collecting and analyzing
step-level annotations on the agent traces, we identify three consistent
failure modes: captcha resolution, pop-up banner removal, and direct navigation
to URLs. By constructing targeted datasets to further study these tasks, we
discover variations in how different language models navigate these failure
modes. We find, for example, that o4-mini deploys a wider variety of strategies
to circumvent captcha resolution than other models and DeepSeek-R1 consistently
misleads users about captcha resolution. Our findings surface both the
diversity and brittleness of current web agents. More broadly, our benchmarking
methodology provides an approach to evaluating and understanding web agent
failure modes at scale.

</details>


### [8] [RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation](https://arxiv.org/abs/2510.02423)
*Hang Wu,Yujun Cai,Haonan Ge,Hongkai Chen,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: 本文分析了ShotBench基准测试在电影摄影理解任务中的局限性，包括选项设计模糊和ShotVL模型在推理一致性与指令遵循方面的不足，提出了RefineShot基准来改进评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有电影摄影理解基准ShotBench存在选项设计模糊问题，且ShotVL模型在推理一致性和指令遵循方面存在缺陷，这影响了评估的可靠性，阻碍了公平比较和未来进展。

Method: 通过一致的选项重构系统化改进ShotBench，对ShotVL的推理行为进行首次批判性分析，并引入联合评估任务准确性和核心模型能力的扩展评估协议。

Result: 开发了RefineShot基准，这是一个经过改进和扩展的基准测试，能够实现更可靠的评估，促进电影摄影理解的未来发展。

Conclusion: RefineShot基准解决了ShotBench的局限性，为电影摄影理解领域提供了更可靠的评估框架，有助于推动该领域的进步。

Abstract: Cinematography understanding refers to the ability to recognize not only the
visual content of a scene but also the cinematic techniques that shape
narrative meaning. This capability is attracting increasing attention, as it
enhances multimodal understanding in real-world applications and underpins
coherent content creation in film and media. As the most comprehensive
benchmark for this task, ShotBench spans a wide range of cinematic concepts and
VQA-style evaluations, with ShotVL achieving state-of-the-art results on it.
However, our analysis reveals that ambiguous option design in ShotBench and
ShotVL's shortcomings in reasoning consistency and instruction adherence
undermine evaluation reliability, limiting fair comparison and hindering future
progress. To overcome these issues, we systematically refine ShotBench through
consistent option restructuring, conduct the first critical analysis of
ShotVL's reasoning behavior, and introduce an extended evaluation protocol that
jointly assesses task accuracy and core model competencies. These efforts lead
to RefineShot, a refined and expanded benchmark that enables more reliable
assessment and fosters future advances in cinematography understanding.

</details>


### [9] [Safe and Efficient In-Context Learning via Risk Control](https://arxiv.org/abs/2510.02480)
*Andrea Wynn,Metod Jazbec,Charith Peris,Rinat Khaziev,Anqi Liu,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 提出一种基于分布无关风险控制的方法，通过动态提前退出机制来防御恶意上下文示例对LLM性能的负面影响，同时保持对有益示例的计算效率提升。


<details>
  <summary>Details</summary>
Motivation: LLM的上下文学习能力虽然强大，但容易受到恶意或错误示例的影响，需要内置安全机制来防止性能下降。

Method: 定义零样本基准安全行为，应用分布无关风险控制，采用动态提前退出预测机制，忽略对不安全输入关注度高的注意力头。

Result: 理论分析和实证结果表明该方法能有效控制恶意上下文示例的风险，同时在有益示例上实现显著计算效率提升。

Conclusion: 该方法为LLM提供了内置的安全防护机制，在保证安全性的同时不牺牲有益上下文示例带来的性能提升和效率优势。

Abstract: Large language models (LLMs) demonstrate a remarkable ability to learn new
tasks from a few in-context examples. However, this flexibility introduces
safety concerns: LLMs can be influenced by incorrect or malicious
demonstrations -- for example, if an adversary tampers with or injects harmful
examples without a human supervisor noticing. This motivates principled designs
in which the system itself includes built-in mechanisms to guard against such
attacks. We propose a novel approach to limit the degree to which harmful
demonstrations can degrade model performance. First, we define a baseline
``safe'' behavior for the model -- the model's performance given no in-context
demonstrations (zero-shot). Next, we apply distribution-free risk control
(DFRC) to control the extent to which in-context samples can decay performance
below zero-shot. We achieve this by leveraging dynamic early exit prediction,
ignoring later attention heads that attend the most to the unsafe inputs.
Finally, we propose modifications to DFRC that allow it to both control risk
for harmful inputs \textit{and} leverage performance and efficiency gains on
helpful inputs. We present both theoretical and empirical results showing that
our approach can effectively control risk for harmful in-context demonstrations
while simultaneously achieving substantial computational efficiency gains with
helpful demonstrations.

</details>


### [10] [Multimodal Function Vectors for Spatial Relations](https://arxiv.org/abs/2510.02528)
*Shuhao Fu,Esther Goldberg,Ying Nian Wu,Hongjing Lu*

Main category: cs.AI

TL;DR: 研究发现大型多模态模型中的一小部分注意力头负责传递空间关系表征，这些注意力头的激活（称为功能向量）可以被提取和操作来改变模型在关系任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型展现出令人印象深刻的情境学习能力，但其支持任务学习的内在机制仍然不透明，需要深入理解模型如何编码和处理空间关系知识。

Method: 使用因果中介分析识别影响关系预测的注意力头，提取多模态功能向量，并在保持模型参数冻结的情况下用少量训练数据微调这些功能向量。

Result: 提取的功能向量在推理时提高了零样本准确性，经过微调后显著优于情境学习基线，并能通过线性组合解决涉及未训练空间关系的类比问题。

Conclusion: 大型多模态模型在局部内部结构中编码空间关系知识，这些知识可以被系统提取和优化，从而增进对模型模块化的理解并增强对关系推理的控制。

Abstract: Large Multimodal Models (LMMs) demonstrate impressive in-context learning
abilities from limited multimodal demonstrations, yet the internal mechanisms
supporting such task learning remain opaque. Building on prior work of large
language models, we show that a small subset of attention heads in the
vision-language model OpenFlamingo-4B is responsible for transmitting
representations of spatial relations. The activations of these attention heads,
termed function vectors, can be extracted and manipulated to alter an LMM's
performance on relational tasks. First, using both synthetic and real image
datasets, we apply causal mediation analysis to identify attention heads that
strongly influence relational predictions, and extract multimodal function
vectors that improve zero-shot accuracy at inference time. We further
demonstrate that these multimodal function vectors can be fine-tuned with a
modest amount of training data, while keeping LMM parameters frozen, to
significantly outperform in-context learning baselines. Finally, we show that
relation-specific function vectors can be linearly combined to solve analogy
problems involving novel and untrained spatial relations, highlighting the
strong generalization ability of this approach. Our results show that LMMs
encode spatial relational knowledge within localized internal structures, which
can be systematically extracted and optimized, thereby advancing our
understanding of model modularity and enhancing control over relational
reasoning in LMMs.

</details>


### [11] [Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge](https://arxiv.org/abs/2510.02557)
*Charlie Masters,Advaith Vellanki,Jiangbo Shangguan,Bart Kultys,Jonathan Gilmore,Alastair Moore,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 本文提出了自主管理代理的概念，用于协调动态人机团队中的复杂多智能体工作流，将其形式化为部分可观测随机博弈，并识别了四个核心挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管智能体AI在自动化单个任务方面取得了进展，但管理复杂的多智能体工作流仍然是一个具有挑战性的问题。

Method: 提出自主管理代理作为核心挑战，形式化工作流管理为部分可观测随机博弈，并开发了MA-Gym开源仿真框架来评估基于GPT-5的管理代理。

Result: 在20个工作流中评估GPT-5管理代理，发现它们在同时优化目标完成、约束遵守和工作流运行时间方面存在困难。

Conclusion: 工作流管理是一个困难的开放性问题，自主管理系统具有组织和伦理影响。

Abstract: While agentic AI has advanced in automating individual tasks, managing
complex multi-agent workflows remains a challenging problem. This paper
presents a research vision for autonomous agentic systems that orchestrate
collaboration within dynamic human-AI teams. We propose the Autonomous Manager
Agent as a core challenge: an agent that decomposes complex goals into task
graphs, allocates tasks to human and AI workers, monitors progress, adapts to
changing conditions, and maintains transparent stakeholder communication. We
formalize workflow management as a Partially Observable Stochastic Game and
identify four foundational challenges: (1) compositional reasoning for
hierarchical decomposition, (2) multi-objective optimization under shifting
preferences, (3) coordination and planning in ad hoc teams, and (4) governance
and compliance by design. To advance this agenda, we release MA-Gym, an
open-source simulation and evaluation framework for multi-agent workflow
orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we
find they struggle to jointly optimize for goal completion, constraint
adherence, and workflow runtime - underscoring workflow management as a
difficult open problem. We conclude with organizational and ethical
implications of autonomous management systems.

</details>


### [12] [Agentic Additive Manufacturing Alloy Discovery](https://arxiv.org/abs/2510.02567)
*Peter Pak,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.AI

TL;DR: 利用LLM驱动的多智能体系统，通过MCP协议调用热力学计算工具，自动化加速增材制造领域的合金发现过程。


<details>
  <summary>Details</summary>
Motivation: 增材制造中的合金发现是一个复杂挑战，需要材料科学、热力学模拟和实验分析等多个领域的专业知识。传统方法效率低下，需要自动化解决方案来加速这一过程。

Method: 开发基于LLM的多智能体系统，通过模型上下文协议(MCP)调用Thermo-Calc属性图计算和熔合缺陷过程图生成等工具，能够根据工具调用结果动态调整任务轨迹。

Result: 系统能够有效推理复杂用户提示，对提出的合金可打印性进行分析，并在实际环境中实现自主决策。

Conclusion: LLM驱动的智能体系统能够自动化并加速增材制造领域的合金发现任务，展示了采用这种多智能体系统的优势。

Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a
researcher's ability to investigate and propose novel solutions to existing
problems. Within Additive Manufacturing (AM), alloy discovery remains a complex
challenge, often requiring expertise in the various domains of materials
science, thermodynamic simulations, and experimental analysis. Large Language
Model (LLM) enabled agents can facilitate this endeavor by utilizing their
extensive knowledge base to dispatch tool calls via Model Context Protocol
(MCP) to perform actions such as Thermo-Calc property diagram calculations and
lack of fusion process map generation. In addition, the multi-agent system
developed in this work is able to effectively reason through complex user
prompts and provide analysis on the printability of proposed alloys. These
agents can dynamically adjust their task trajectory to the outcomes of tool
call results, effectively enabling autonomous decision-making in practical
environments. This work aims to utilize LLM enabled agents to automate and
accelerate the task of alloy discovery within the field of additive
manufacturing and showcase the benefits of adopting this multi-agent system.

</details>


### [13] [A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem](https://arxiv.org/abs/2510.02589)
*Yunqi Huang,Nishith Chennakeshava,Alexis Carras,Vladislav Neverov,Wei Liu,Aske Plaat,Yingjie Fan*

Main category: cs.AI

TL;DR: 开发了一个包含起重机调度的CSPP Gym环境，评估了5种RL算法在不同复杂度场景下的性能，发现算法选择和问题表述对CSPP至关重要。


<details>
  <summary>Details</summary>
Motivation: 集装箱配载规划在海上运输和码头运营中至关重要，但现有研究缺乏对不同RL算法的系统性基准比较。

Method: 开发了一个捕捉CSPP基本特征的Gym环境，扩展到包含起重机调度（多智能体和单智能体），评估了DQN、QR-DQN、A2C、PPO和TRPO五种RL算法。

Result: 结果显示随着复杂度增加，性能差距明显，强调了算法选择和问题表述对CSPP的重要性。

Conclusion: 本文为CSPP提供了多个RL方法的基准测试，同时提供了一个可重用的包含起重机调度的Gym环境，为未来研究和海事物流实际部署奠定了基础。

Abstract: Container stowage planning (CSPP) is a critical component of maritime
transportation and terminal operations, directly affecting supply chain
efficiency. Owing to its complexity, CSPP has traditionally relied on human
expertise. While reinforcement learning (RL) has recently been applied to CSPP,
systematic benchmark comparisons across different algorithms remain limited. To
address this gap, we develop a Gym environment that captures the fundamental
features of CSPP and extend it to include crane scheduling in both multi-agent
and single-agent formulations. Within this framework, we evaluate five RL
algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying
complexity. The results reveal distinct performance gaps with increasing
complexity, underscoring the importance of algorithm choice and problem
formulation for CSPP. Overall, this paper benchmarks multiple RL methods for
CSPP while providing a reusable Gym environment with crane scheduling, thus
offering a foundation for future research and practical deployment in maritime
logistics.

</details>


### [14] [Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs](https://arxiv.org/abs/2510.02592)
*Jean Douglas Carvalho,Hugo Kenji,Ahmad Mohammad Saber,Glaucia Melo,Max Mauro Dias Santos,Deepa Kundur*

Main category: cs.AI

TL;DR: 提出基于多模态大语言模型的框架，处理视觉感知、定位和车辆数据，为驾驶员生成自然语言警报，提升电动汽车在智能电网中的安全性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车与智能电网融合时，确保驾驶员、车辆与环境之间安全可解释的交互仍是关键挑战，需要将原始传感器数据转化为驾驶员可理解的信息。

Method: 使用多模态大语言模型处理对象检测、语义分割和车辆遥测等多源传感器数据，结合YOLOv8视觉感知、地理编码定位和CAN总线数据，通过提示工程生成自然语言警报。

Result: 基于真实世界数据的案例研究表明，该框架能有效生成针对关键情境（如接近行人、自行车和其他车辆）的上下文感知警报。

Conclusion: 大语言模型可作为电动出行的辅助工具，通过实现可扩展的车队协调、电动汽车负载预测和交通感知能源规划，同时惠及交通系统和电网。

Abstract: The integration of electric vehicles (EVs) into smart grids presents unique
opportunities to enhance both transportation systems and energy networks.
However, ensuring safe and interpretable interactions between drivers,
vehicles, and the surrounding environment remains a critical challenge. This
paper presents a multi-modal large language model (LLM)-based framework to
process multimodal sensor data - such as object detection, semantic
segmentation, and vehicular telemetry - and generate natural-language alerts
for drivers. The framework is validated using real-world data collected from
instrumented vehicles driving on urban roads, ensuring its applicability to
real-world scenarios. By combining visual perception (YOLOv8), geocoded
positioning, and CAN bus telemetry, the framework bridges raw sensor data and
driver comprehension, enabling safer and more informed decision-making in urban
driving scenarios. Case studies using real data demonstrate the framework's
effectiveness in generating context-aware alerts for critical situations, such
as proximity to pedestrians, cyclists, and other vehicles. This paper
highlights the potential of LLMs as assistive tools in e-mobility, benefiting
both transportation systems and electric networks by enabling scalable fleet
coordination, EV load forecasting, and traffic-aware energy planning.
  Index Terms - Electric vehicles, visual perception, large language models,
YOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.

</details>


### [15] [Mitigating Modal Imbalance in Multimodal Reasoning](https://arxiv.org/abs/2510.02608)
*Chen Henry Wu,Neil Kale,Aditi Raghunathan*

Main category: cs.AI

TL;DR: 该论文研究了基础模型在多模态联合推理中的表现，特别是在跨模态冲突场景下。研究发现模型在单模态中能识别90%的冲突，但在跨模态情况下识别率降至3%，主要原因是跨模态注意力不平衡。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型在多模态任务中的联合推理能力，特别是当不同模态之间存在冲突证据时，模型是否能进行跨模态推理来调和冲突。

Method: 通过跨模态冲突实验来测试模型表现，分析跨模态注意力不平衡问题，并提出一种简单可扩展的方法：在训练实例中显式组合多个模态。

Result: 模型在单模态冲突中识别率达90%，但跨模态冲突识别率降至3%。提出的方法能显著减少注意力不平衡，并在多个视觉语言基准测试中提升下游性能。

Conclusion: 需要系统性地解决跨模态上下文问题，以构建可靠的基础模型。盲目扩展多模态数据集不足以解决跨模态推理问题，需要显式的跨模态训练策略。

Abstract: Foundation models (FMs) deployed in real-world tasks such as computer-use
agents must integrate diverse modalities. How good are FMs at performing joint
reasoning, simultaneously reasoning over multiple modalities, especially when
the modalities interact and relate to each other to form cross-modal context?
To better understand this problem, we study FMs on cross-modal conflicts:
scenarios where conflicting evidence is presented across modalities. This
allows us to examine whether FMs prioritize one modality over another or reason
jointly to reconcile the conflict. Our experiments reveal that FMs can
recognize conflicts in unimodal contexts, composed of a single modality, 90% of
the time, but the ratio falls as low as 3% when evidence is split across
modalities -- similar observations hold in cross-lingual contexts, composed of
multiple languages. We trace this failure to cross-modal attention imbalance,
showing that FMs exhibit extreme asymmetry in attention scores,
disproportionately prioritizing certain modalities. We show that cross-modal
attention imbalance does not go away by simply scaling up multimodal or
multilingual datasets blindly, since they lack training examples that
explicitly require cross-modal reasoning. We demonstrate that even a simple and
scalable method of explicitly combining multiple modalities within each
training instance significantly reduces attention imbalance. Reduced attention
imbalance directly translates to improved downstream performance on several
vision-language benchmarks. Our findings underscore the importance of
systematically addressing cross-modal contexts to build reliable foundation
models.

</details>


### [16] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: 本文发现测试时缩放(TTS)中单纯增加样本数量K的收益有限，提出温度维度缩放能显著提升大语言模型的推理能力，使基础模型达到接近RL训练模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明增加推理轨迹样本数量K能持续提升准确率，但本文发现这种趋势并非无限持续，且单一温度采样只能探索模型潜力的部分子集。

Method: 提出温度维度缩放方法，在不同采样温度下生成推理轨迹，并设计多温度投票机制来降低计算开销。

Result: 在多个模型和推理基准测试中，温度缩放比单一温度TTS额外提升7.3个百分点，使基础模型性能接近RL训练模型。

Conclusion: TTS的潜力比之前认为的更大，温度缩放提供了一种简单有效的方法来释放基础模型的潜在能力。

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [17] [Geolog-IA: Conversational System for Academic Theses](https://arxiv.org/abs/2510.02653)
*Micaela Fuel Pozo,Andrea Guatumillo Saltos,Yeseña Tipan Llumiquinga,Kelly Lascano Aguirre,Marilyn Castillo Jara,Christian Mejia-Escobar*

Main category: cs.AI

TL;DR: 开发了Geolog-IA对话系统，使用Llama 3.1和Gemini 2.5语言模型结合RAG架构和SQLite数据库，为厄瓜多尔中央大学地质学论文提供自然问答服务。


<details>
  <summary>Details</summary>
Motivation: 解决传统AI系统在回答地质学论文问题时出现的幻觉问题和知识过时问题，为大学师生和管理人员提供准确的信息检索工具。

Method: 采用Llama 3.1和Gemini 2.5语言模型，结合检索增强生成(RAG)架构和SQLite数据库，构建基于Web的对话系统。

Result: 使用BLEU指标评估，系统性能平均达到0.87，表明生成回答具有高一致性和准确性。

Conclusion: 该系统在教育、培训和研究中具有重要价值，并为其他学科的应用奠定了基础。

Abstract: This study presents the development of Geolog-IA, a novel conversational
system based on artificial intelligence that responds naturally to questions
about geology theses from the Central University of Ecuador. Our proposal uses
the Llama 3.1 and Gemini 2.5 language models, which are complemented by a
Retrieval Augmented Generation (RAG) architecture and an SQLite database. This
strategy allows us to overcome problems such as hallucinations and outdated
knowledge. The evaluation of Geolog-IA's performance with the BLEU metric
reaches an average of 0.87, indicating high consistency and accuracy in the
responses generated. The system offers an intuitive, web-based interface that
facilitates interaction and information retrieval for directors, teachers,
students, and administrative staff at the institution. This tool can be a key
support in education, training, and research and establishes a basis for future
applications in other disciplines.

</details>


### [18] [A Concept of Possibility for Real-World Events](https://arxiv.org/abs/2510.02655)
*Daniel G. Schwartz*

Main category: cs.AI

TL;DR: 提出了一种新的可能性概念作为Zadeh(1978)标准概念的替代，专注于现实世界事件的可能性计算，基于事件的先决条件和约束的概率函数。


<details>
  <summary>Details</summary>
Motivation: 为规划问题提供更适用的可能性理论，特别是用于评估多个计划中哪个最容易或最可行完成。

Method: 将事件视为具有促使其发生的先决条件和阻碍其发生的约束，可能性计算为先决条件成立且约束不成立的概率函数。

Result: 开发了基于Łukasiewicz多值逻辑连接词的新可能性理论，提供了车辆路线规划的示例说明。

Conclusion: 该可能性模型可能正确捕捉了人类关于计划的正常推理，并具有未来应用的潜力。

Abstract: This paper offers a new concept of {\it possibility} as an alternative to the
now-a-days standard concept originally introduced by L.A. Zadeh in 1978. This
new version was inspired by the original but, formally, has nothing in common
with it other than that they both adopt the {\L}ukasiewicz multivalent
interpretation of the logical connectives. Moreover, rather than seeking to
provide a general notion of possibility, this focuses specifically on the
possibility of a real-world event. An event is viewed as having prerequisites
that enable its occurrence and constraints that may impede its occurrence, and
the possibility of the event is computed as a function of the probabilities
that the prerequisites hold and the constraints do not. This version of
possibility might appropriately be applied to problems of planning. When there
are multiple plans available for achieving a goal, this theory can be used to
determine which plan is most possible, i.e., easiest or most feasible to
complete. It is speculated that this model of reasoning correctly captures
normal human reasoning about plans. The theory is elaborated and an
illustrative example for vehicle route planning is provided. There is also a
suggestion of potential future applications.

</details>


### [19] [AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models](https://arxiv.org/abs/2510.02669)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.AI

TL;DR: AutoMaAS是一个自进化的多智能体架构搜索框架，通过神经架构搜索原理自动发现最优智能体配置，在性能提升1.0-7.1%的同时降低推理成本3-5%。


<details>
  <summary>Details</summary>
Motivation: 现有自动化设计方法寻求单一解决方案，无法根据查询复杂度和领域需求自适应分配资源，需要更灵活的多智能体系统设计框架。

Method: 采用神经架构搜索原则，包含四个关键创新：自动算子生成、融合和消除；动态成本感知优化；在线反馈集成；增强可解释性的决策追踪机制。

Result: 在六个基准测试中，相比最先进方法，性能提升1.0-7.1%，推理成本降低3-5%，在不同数据集和LLM骨干网络上表现出优越的迁移性。

Conclusion: AutoMaAS为大型语言模型时代的自动化多智能体系统设计建立了新范式，具有优越的性能、效率和可迁移性。

Abstract: Multi-agent systems powered by large language models have demonstrated
remarkable capabilities across diverse domains, yet existing automated design
approaches seek monolithic solutions that fail to adapt resource allocation
based on query complexity and domain requirements. This paper introduces
AutoMaAS, a self-evolving multi-agent architecture search framework that
leverages neural architecture search principles to automatically discover
optimal agent configurations through dynamic operator lifecycle management and
automated machine learning techniques. Our approach incorporates four key
innovations: (1) automatic operator generation, fusion, and elimination based
on performance-cost analysis, (2) dynamic cost-aware optimization with
real-time parameter adjustment, (3) online feedback integration for continuous
architecture refinement, and (4) enhanced interpretability through decision
tracing mechanisms. Extensive experiments across six benchmarks demonstrate
that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing
inference costs by 3-5\% compared to state-of-the-art methods. The framework
shows superior transferability across datasets and LLM backbones, establishing
a new paradigm for automated multi-agent system design in the era of large
language models.

</details>


### [20] [ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks](https://arxiv.org/abs/2510.02677)
*Zhaorun Chen,Xun Liu,Mintong Kang,Jiawei Zhang,Minzhou Pan,Shuang Yang,Bo Li*

Main category: cs.AI

TL;DR: ARMs是一个自适应红队代理，通过推理增强的多步骤编排自动优化多样化红队策略，有效引发目标VLM的有害输出。它整合了17种红队算法和11种新型多模态攻击策略，在基准测试中平均攻击成功率超过基线52.1%，并构建了包含30K+红队实例的大规模多模态安全数据集ARMs-Bench。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型(VLMs)的普及，其多模态接口引入了新的安全漏洞，现有红队方法要么局限于有限的对抗模式，要么依赖人工工程，缺乏对新兴真实世界VLM漏洞的可扩展探索。

Method: 提出ARMs自适应红队代理，采用推理增强的多步骤编排自动优化多样化红队策略；设计了11种新型多模态攻击策略和分层记忆结构，通过epsilon-greedy攻击探索算法平衡攻击多样性和有效性；整合17种红队算法通过模型上下文协议(MCP)。

Result: 在实例和策略基准测试中，ARMs达到最先进的攻击成功率，平均超过基线52.1%，在Claude-4-Sonnet上超过90%；生成的红队实例多样性显著更高，揭示了VLM的新兴漏洞；构建了ARMs-Bench数据集，包含30K+红队实例，涵盖51个风险类别。

Conclusion: 基于ARMs-Bench的安全微调显著提高了VLM的鲁棒性，同时保持了通用效用，为改进多模态安全对齐提供了可操作指导。

Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces
also introduce new safety vulnerabilities, making the safety evaluation
challenging and critical. Existing red-teaming efforts are either restricted to
a narrow set of adversarial patterns or depend heavily on manual engineering,
lacking scalable exploration of emerging real-world VLM vulnerabilities. To
bridge this gap, we propose ARMs, an adaptive red-teaming agent that
systematically conducts comprehensive risk assessments for VLMs. Given a target
harmful behavior or risk definition, ARMs automatically optimizes diverse
red-teaming strategies with reasoning-enhanced multi-step orchestration, to
effectively elicit harmful outputs from target VLMs. We propose 11 novel
multimodal attack strategies, covering diverse adversarial patterns of VLMs
(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming
algorithms into ARMs via model context protocol (MCP). To balance the diversity
and effectiveness of the attack, we design a layered memory with an
epsilon-greedy attack exploration algorithm. Extensive experiments on instance-
and policy-based benchmarks show that ARMs achieves SOTA attack success rates,
exceeding baselines by an average of 52.1% and surpassing 90% on
Claude-4-Sonnet. We show that the diversity of red-teaming instances generated
by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.
Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety
dataset comprising over 30K red-teaming instances spanning 51 diverse risk
categories, grounded in both real-world multimodal threats and regulatory
risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness
of VLMs while preserving their general utility, providing actionable guidance
to improve multimodal safety alignment against emerging threats.

</details>


### [21] [Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation](https://arxiv.org/abs/2510.02679)
*Yu-Zhe Shi,Qiao Xu,Yanjia Li,Mingchen Liu,Huamin Qu,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: 提出了一种基于约束中心的架构，利用LLMs实现生产调度中可靠的自动化约束规范，解决了传统方法中自然语言模糊性和输出不确定性的问题。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统中APS系统至关重要，但将制造需求转化为正式约束的过程仍依赖人工且耗时。虽然LLMs在自动化约束规范方面有潜力，但直接应用面临自然语言模糊性、输出不确定性和领域知识有限的挑战。

Method: 设计了一个约束中心架构，通过三个层次组织的层次结构空间，使用领域特定表示确保精度和可靠性，同时保持灵活性。还开发了自动生产场景适配算法，为特定制造配置高效定制架构。

Result: 实验结果表明，该方法成功平衡了LLMs的生成能力与制造系统的可靠性要求，在约束规范任务中显著优于纯LLM方法。

Conclusion: 提出的约束中心架构能够有效利用LLMs实现可靠的自动化约束规范，为制造系统提供了更高效和准确的调度解决方案。

Abstract: Advanced Planning and Scheduling (APS) systems have become indispensable for
modern manufacturing operations, enabling optimized resource allocation and
production efficiency in increasingly complex and dynamic environments. While
algorithms for solving abstracted scheduling problems have been extensively
investigated, the critical prerequisite of specifying manufacturing
requirements into formal constraints remains manual and labor-intensive.
Although recent advances of generative models, particularly Large Language
Models (LLMs), show promise in automating constraint specification from
heterogeneous raw manufacturing data, their direct application faces challenges
due to natural language ambiguity, non-deterministic outputs, and limited
domain-specific knowledge. This paper presents a constraint-centric
architecture that regulates LLMs to perform reliable automated constraint
specification for production scheduling. The architecture defines a
hierarchical structural space organized across three levels, implemented
through domain-specific representation to ensure precision and reliability
while maintaining flexibility. Furthermore, an automated production scenario
adaptation algorithm is designed and deployed to efficiently customize the
architecture for specific manufacturing configurations. Experimental results
demonstrate that the proposed approach successfully balances the generative
capabilities of LLMs with the reliability requirements of manufacturing
systems, significantly outperforming pure LLM-based approaches in constraint
specification tasks.

</details>


### [22] [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
*Yulong Zhang,Li Wang,Wei Du,Peilin Li,Yuqin Dai Zhiyuan Zhao,Lingyong Fang,Ziniu Liu,Ru Zhang,Huijia Zhu,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出Node-wise Consistency Verification (NCV)框架，通过节点级一致性检查来验证大语言模型的多步推理，提高错误定位精度并减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么评估整个推理链导致注意力分散，要么依赖昂贵的多重采样，难以精确定位错误且token成本高。

Method: 将推理链分解为互连的验证节点，进行轻量级二元一致性检查，避免生成长文本。

Result: 在公开数据集上，NCV相比传统方法F1分数提升10%-25%，token使用量减少6-58倍。

Conclusion: NCV提供了一种可扩展的可靠LLM推理验证解决方案，提高了可解释性和效率。

Abstract: Verifying multi-step reasoning in large language models is difficult due to
imprecise error localization and high token costs. Existing methods either
assess entire reasoning chains, suffering attention dilution, or rely on
expensive multi-sampling. We introduce Node-wise Consistency Verification
(NCV), a training-free framework that recasts verification as lightweight
binary consistency checks at the node level. By decomposing the chain of
thought into interconnected verification nodes, NCV precisely localizes errors
and avoids unnecessary long-form generation. Experiments demonstrate that our
approach enhances interpretability and efficiency, presenting a scalable
solution for reliable LLM reasoning verification. On public datasets, NCV
achieves a 10\% to 25\% improvement in F1 scores over baselines while utilizing
$6\times$~$58\times$ fewer tokens than traditional methods like CoT-based
verifiers.

</details>


### [23] [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
*Wonjoong Kim,Sangwu Park,Yeonjun In,Sein Kim,Dongha Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: TRACE是一个用于多维度评估工具增强LLM代理性能的框架，通过证据库积累推理步骤知识，能够有效评估代理的推理轨迹，包括效率、幻觉和适应性等方面。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强基准测试主要依赖答案匹配，但随着解决用户请求所需步骤增加，需要超越最终答案来评估问题解决轨迹，包括效率、幻觉和适应性等被忽视的方面。

Method: 引入TRACE框架，通过证据库积累前面推理步骤的知识，实现对代理推理轨迹的多方面分析和评估。创建元评估数据集，在现有基准上添加多样且有缺陷的轨迹，并标注多方面性能分数。

Result: TRACE能够准确评估复杂行为，具有可扩展性和成本效益，即使使用小型开源LLM也能实现。应用该方法评估代理在解决工具增强任务时产生的轨迹，提供了之前未报告的观察和见解。

Conclusion: TRACE框架为工具增强LLM代理提供了有效的多维度评估方法，能够超越简单的答案匹配，全面评估问题解决轨迹的性能表现。

Abstract: Although recent tool-augmented benchmarks incorporate complex user requests
and diverse tools, the evaluation methods for most of them remain limited to
answer matching. However, as the number of steps required to resolve a user
request increases, a proper evaluation of an agent's performance must go beyond
the final answer to also assess the problem-solving trajectory, including
previously ignored aspects such as efficiency, hallucination, and adaptivity.
The most straightforward method for evaluating these aspects is to compare an
agent's trajectory with the ground-truth trajectory, but this approach is
fundamentally limited since annotating all valid ground-truth trajectories is
prohibitively expensive. However, a simple LLM-based evaluator struggles to
assess trajectories in detail without ground truth. To effectively evaluate the
agents in this manner, we introduce TRACE, a framework for the
multi-dimensional evaluation of tool-augmented LLM agent performance. By
incorporating an evidence bank, which accumulates knowledge gathered from
preceding reasoning steps, TRACE enables a multi-faceted analysis and
evaluation of an agent's reasoning trajectory effectively. To validate our
framework, we develop a new meta-evaluation dataset by augmenting existing
benchmarks with diverse and flawed trajectories, each labeled with
multi-faceted performance scores. Our results confirm that TRACE accurately
evaluates these complex behaviors in a scalable and cost-effective manner, even
with small open-source LLMs. Furthermore, we apply our method to evaluate the
trajectories that agents produce while solving tool-augmented tasks, presenting
previously unreported observations and their corresponding insights.

</details>


### [24] [Take Goodhart Seriously: Principled Limit on General-Purpose AI Optimization](https://arxiv.org/abs/2510.02840)
*Antoine Maier,Aude Maier,Tom David*

Main category: cs.AI

TL;DR: 该论文挑战了机器学习中训练模型能完全满足目标函数的假设，指出由于近似、估计和优化误差，以及目标函数本身无法完美捕捉开发者意图，系统会偏离预期目标，这可能导致在强优化压力下出现Goodhart定律的失控模式。


<details>
  <summary>Details</summary>
Motivation: 动机是检验机器学习中普遍但未经检验的假设——训练产生的模型能完全满足其指定目标函数（目标满足假设OSA），并探讨OSA失败的现实影响和潜在风险。

Method: 采用学习范式无关的框架，分析近似误差、估计误差和优化误差如何导致系统偏离预期目标，并基于近期数学结果论证这些偏差在缺乏数学表征时与Goodhart定律失效模式无法区分。

Result: 研究发现OSA在现实条件下必然失败，目标函数无法完美捕捉开发者意图，导致错误规范不可避免。在强优化压力下，这些偏差会演变为Goodhart定律的失控模式。

Conclusion: 由于无法预先确定Goodhart临界点，必须对通用人工智能系统的优化施加原则性限制，否则持续优化可能导致可预测且不可逆的失控。

Abstract: A common but rarely examined assumption in machine learning is that training
yields models that actually satisfy their specified objective function. We call
this the Objective Satisfaction Assumption (OSA). Although deviations from OSA
are acknowledged, their implications are overlooked. We argue, in a
learning-paradigm-agnostic framework, that OSA fails in realistic conditions:
approximation, estimation, and optimization errors guarantee systematic
deviations from the intended objective, regardless of the quality of its
specification. Beyond these technical limitations, perfectly capturing and
translating the developer's intent, such as alignment with human preferences,
into a formal objective is practically impossible, making misspecification
inevitable. Building on recent mathematical results, absent a mathematical
characterization of these gaps, they are indistinguishable from those that
collapse into Goodhart's law failure modes under strong optimization pressure.
Because the Goodhart breaking point cannot be located ex ante, a principled
limit on the optimization of General-Purpose AI systems is necessary. Absent
such a limit, continued optimization is liable to push systems into predictable
and irreversible loss of control.

</details>


### [25] [Reward Model Routing in Alignment](https://arxiv.org/abs/2510.02850)
*Xinle Wu,Yao Lu*

Main category: cs.AI

TL;DR: 提出了BayesianRouter框架，通过结合离线RM强度学习和在线贝叶斯选择，动态选择奖励模型来提升大语言模型对齐质量，解决了现有方法冷启动和探索不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RLHF/RLAIF流程依赖单一奖励模型，限制了对齐质量并可能导致过拟合。虽然最近有研究探索奖励模型路由，但现有方法存在冷启动和探索不足的问题。

Method: 提出混合路由框架BayesianRouter：离线阶段训练多任务路由器估计每个RM的可靠性；在线阶段使用贝叶斯Thompson采样路由器进行每查询RM选择，用离线嵌入作为高斯先验初始化权重向量，并通过在线奖励自适应更新后验分布。

Result: 在指令跟随（AlpacaEval-2、Arena-Hard、MT-Bench）和推理（GSM8K、MMLU）基准测试中，BayesianRouter始终优于单个RM、RM集成和现有路由方法。

Conclusion: BayesianRouter框架通过有效结合离线和在线学习，显著提升了奖励模型路由的性能，为大语言模型对齐提供了更可靠的解决方案。

Abstract: Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become
the standard paradigm for aligning large language models (LLMs). However, most
pipelines rely on a single reward model (RM), limiting alignment quality and
risking overfitting. Recent work explores RM routing--dynamically selecting an
RM from a candidate pool to exploit complementary strengths while maintaining
$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient
exploration. We propose BayesianRouter, a hybrid routing framework that
combines offline RM strengths learning with online Bayesian selection. In the
offline stage, a multi-task router is trained on preference data to estimate
per-RM reliability. In the online stage, a Bayesian Thompson sampling router
performs per-query RM selection, initializing RM-specific weight vectors with
offline embeddings as Gaussian priors and adaptively updating their posteriors
with online rewards to adapt to the evolving policy distribution. Extensive
experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and
reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently
outperforms individual RMs, RM ensembling, and existing routing methods.

</details>


### [26] [Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models](https://arxiv.org/abs/2510.02880)
*Tianren Ma,Mu Zhang,Yibing Wang,Qixiang Ye*

Main category: cs.AI

TL;DR: 提出了MaskGRPO方法，这是首个在离散扩散模型中实现可扩展多模态强化学习的可行方法，解决了重要性采样和模态特定适应问题。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型的非自回归特性使得重要性采样难以处理，导致强化学习方法如GRPO难以应用，需要解决这一挑战。

Method: 首先澄清DDM的理论基础，构建捕捉有价值token波动的重要性估计器；然后为视觉序列精心设计rollout方法，产生多样化补全和可靠优化梯度。

Result: 在数学推理、编码和视觉生成基准测试中，MaskGRPO带来更稳定高效的更新，获得更强的推理性能和更好的生成质量。

Conclusion: MaskGRPO建立了一个系统性的策略优化方法，是离散化视觉扩散的第一个实用解决方案。

Abstract: Optimizing discrete diffusion model (DDM) with rewards remains a challenge:
the non-autoregressive paradigm makes importance sampling intractable and
rollout complex, puzzling reinforcement learning methods such as Group Relative
Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first
viable approach to enable scalable multimodal reinforcement learning in
discrete diffusion with effective importance sampling and modality-specific
adaptations. To this end, we first clarify the theoretical foundation for DDMs,
which facilitates building an importance estimator that captures valuable token
fluctuation for gradient updates. We then delicately tailored the rollout
method for visual sequences, which yields diverse completions and reliable
optimization gradients. Upon math reasoning, coding, and visual generation
benchmarks, MaskGRPO brings more stable and efficient updates, leading to
stronger reasoning performance and better generation quality. This study
establishes MaskGRPO as a systematic policy optimization approach and the first
practical way for discretized visual diffusion.

</details>


### [27] [Onto-Epistemological Analysis of AI Explanations](https://arxiv.org/abs/2510.02996)
*Martina Mattioli,Eike Petersen,Aasa Feragen,Marcello Pelillo,Siavash A. Bigdeli*

Main category: cs.AI

TL;DR: 本文分析了可解释AI方法中的本体论和认识论假设，指出这些假设对AI解释的有效性和解释具有重要影响，并讨论了如何为不同应用领域选择和适配适当的XAI方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流的深度学习方法是黑盒系统，缺乏对其推理过程的解释，这限制了其可信度和采用。虽然可解释AI方法旨在解决这一问题，但这些方法往往基于技术背景开发者的假设，而解释的基本概念本身在哲学上存在深刻争议。

Method: 研究可解释AI方法应用于AI系统时的本体论和认识论假设，即关于解释存在性以及我们获取这些解释知识能力的假设。分析XAI方法中看似小的技术变化如何对应解释基础假设的重要差异。

Result: 分析显示，XAI方法中的技术选择反映了不同的本体论和认识论立场，这些假设对解释的有效性和解释具有重要影响。忽略基础的本体-认识论范式在选择XAI方法时存在风险。

Conclusion: 在选择XAI方法时需要考虑其基础的本体论和认识论假设，并针对不同应用领域选择和适配适当的方法，以确保解释的有效性和适用性。

Abstract: Artificial intelligence (AI) is being applied in almost every field. At the
same time, the currently dominant deep learning methods are fundamentally
black-box systems that lack explanations for their inferences, significantly
limiting their trustworthiness and adoption. Explainable AI (XAI) methods aim
to overcome this challenge by providing explanations of the models' decision
process. Such methods are often proposed and developed by engineers and
scientists with a predominantly technical background and incorporate their
assumptions about the existence, validity, and explanatory utility of different
conceivable explanatory mechanisms. However, the basic concept of an
explanation -- what it is, whether we can know it, whether it is absolute or
relative -- is far from trivial and has been the subject of deep philosophical
debate for millennia. As we point out here, the assumptions incorporated into
different XAI methods are not harmless and have important consequences for the
validity and interpretation of AI explanations in different domains. We
investigate ontological and epistemological assumptions in explainability
methods when they are applied to AI systems, meaning the assumptions we make
about the existence of explanations and our ability to gain knowledge about
those explanations. Our analysis shows how seemingly small technical changes to
an XAI method may correspond to important differences in the underlying
assumptions about explanations. We furthermore highlight the risks of ignoring
the underlying onto-epistemological paradigm when choosing an XAI method for a
given application, and we discuss how to select and adapt appropriate XAI
methods for different domains of application.

</details>


### [28] [From Facts to Foils: Designing and Evaluating Counterfactual Explanations for Smart Environments](https://arxiv.org/abs/2510.03078)
*Anna Trapp,Mersedeh Sadeghi,Andreas Vogelsang*

Main category: cs.AI

TL;DR: 本文首次为基于规则的智能环境领域形式化并实现了反事实解释方法，通过用户研究发现用户偏好高度情境化：因果解释因其语言简洁性在时间紧迫时更受青睐，而反事实解释因其可操作性内容在解决问题时更受欢迎。


<details>
  <summary>Details</summary>
Motivation: 虽然反事实解释在可解释AI中是一个强大工具，但在基于规则的智能环境领域尚无成熟的生成方法，需要为这一领域开发专门的反事实解释框架。

Method: 开发了一个插件来扩展现有的智能环境解释引擎，实现了针对该领域的反事实解释形式化方法，并通过用户研究(N=17)评估生成的反事实解释与传统因果解释的效果。

Result: 用户偏好高度依赖情境：因果解释因语言简洁性和在时间紧迫情况下更受青睐；反事实解释因提供可操作内容，在用户想要解决问题时更受欢迎。

Conclusion: 本研究为智能环境中的新型解释提供了实用框架，并为选择最有效解释类型提供了实证依据，强调了解释类型选择的上下文依赖性。

Abstract: Explainability is increasingly seen as an essential feature of rule-based
smart environments. While counterfactual explanations, which describe what
could have been done differently to achieve a desired outcome, are a powerful
tool in eXplainable AI (XAI), no established methods exist for generating them
in these rule-based domains. In this paper, we present the first formalization
and implementation of counterfactual explanations tailored to this domain. It
is implemented as a plugin that extends an existing explanation engine for
smart environments. We conducted a user study (N=17) to evaluate our generated
counterfactuals against traditional causal explanations. The results show that
user preference is highly contextual: causal explanations are favored for their
linguistic simplicity and in time-pressured situations, while counterfactuals
are preferred for their actionable content, particularly when a user wants to
resolve a problem. Our work contributes a practical framework for a new type of
explanation in smart environments and provides empirical evidence to guide the
choice of when each explanation type is most effective.

</details>


### [29] [A Study of Rule Omission in Raven's Progressive Matrices](https://arxiv.org/abs/2510.03127)
*Binze Li*

Main category: cs.AI

TL;DR: 该研究通过故意在训练中省略部分结构规则，评估现代AI系统在不完整训练条件下的泛化能力，发现transformer模型在面对新规则时性能显著下降，揭示了当前方法在抽象推理方面的根本局限性。


<details>
  <summary>Details</summary>
Motivation: 类比推理是人类认知的核心，也是人工智能的基本挑战。虽然许多基于视觉和语言的模型在Raven渐进矩阵任务上取得了成功，但尚不清楚其性能是否反映了真正的推理能力还是依赖于统计捷径。

Method: 在Impartial-RAVEN数据集上评估序列到序列transformer模型和基于视觉的架构（如CoPINet和双对比网络），通过故意在训练中省略几个结构规则来测试泛化能力。

Result: 实验表明，虽然transformer在熟悉规则上表现强劲，但当面对新颖或省略的规则时，其准确性急剧下降。token级准确性和完整答案准确性之间的差距突显了当前方法的根本局限性。

Conclusion: 这些发现为深度学习模型的推理机制提供了新见解，并强调了需要超越模式识别、实现稳健抽象推理的架构。

Abstract: Analogical reasoning lies at the core of human cognition and remains a
fundamental challenge for artificial intelligence. Raven's Progressive Matrices
(RPM) serve as a widely used benchmark to assess abstract reasoning by
requiring the inference of underlying structural rules. While many vision-based
and language-based models have achieved success on RPM tasks, it remains
unclear whether their performance reflects genuine reasoning ability or
reliance on statistical shortcuts. This study investigates the generalization
capacity of modern AI systems under conditions of incomplete training by
deliberately omitting several structural rules during training. Both
sequence-to-sequence transformer models and vision-based architectures such as
CoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN
(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate
strong performance on familiar rules, their accuracy declines sharply when
faced with novel or omitted rules. Moreover, the gap between token-level
accuracy and complete answer accuracy highlights fundamental limitations in
current approaches. These findings provide new insights into the reasoning
mechanisms underlying deep learning models and underscore the need for
architectures that move beyond pattern recognition toward robust abstract
reasoning.

</details>


### [30] [Improving Cooperation in Collaborative Embodied AI](https://arxiv.org/abs/2510.03153)
*Hima Jacob Leven Suprabha,Laxmi Nag Laxminarayan Nagesh,Ajith Nair,Alvin Reuben Amal Selvaster,Ayan Khan,Raghuram Damarla,Sanju Hannah Samuel,Sreenithi Saravana Perumal,Titouan Puech,Venkataramireddy Marella,Vishal Sonar,Alessandro Suglia,Oliver Lemon*

Main category: cs.AI

TL;DR: 本文研究了不同提示方法在增强多智能体协作中的有效性，通过优化CoELA框架中的LLM提示工程，显著提升了协作性能，并集成了语音功能以改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在多智能体系统中的集成，探索如何通过提示工程优化智能体间的协作推理和决策能力，以提升系统整体性能。

Method: 增强CoELA框架，系统测试不同LLM和提示工程策略，寻找最优组合来最大化协作性能，并集成语音功能实现基于语音的协作交互。

Result: 最佳组合相比原始CoELA系统，在Gemma3上运行效率提升了22%。语音集成提供了更具吸引力的用户界面。

Conclusion: 提示优化能有效提升协作智能体性能，语音集成增强了系统的交互体验和演示效果。

Abstract: The integration of Large Language Models (LLMs) into multiagent systems has
opened new possibilities for collaborative reasoning and cooperation with AI
agents. This paper explores different prompting methods and evaluates their
effectiveness in enhancing agent collaborative behaviour and decision-making.
We enhance CoELA, a framework designed for building Collaborative Embodied
Agents that leverage LLMs for multi-agent communication, reasoning, and task
coordination in shared virtual spaces. Through systematic experimentation, we
examine different LLMs and prompt engineering strategies to identify optimised
combinations that maximise collaboration performance. Furthermore, we extend
our research by integrating speech capabilities, enabling seamless
collaborative voice-based interactions. Our findings highlight the
effectiveness of prompt optimisation in enhancing collaborative agent
performance; for example, our best combination improved the efficiency of the
system running with Gemma3 by 22% compared to the original CoELA system. In
addition, the speech integration provides a more engaging user interface for
iterative system development and demonstrations.

</details>


### [31] [CoDA: Agentic Systems for Collaborative Data Visualization](https://arxiv.org/abs/2510.03194)
*Zichen Chen,Jiefeng Chen,Sercan Ö. Arik,Misha Sra,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: CoDA是一个多智能体系统，通过专门的LLM智能体进行元数据分析、任务规划、代码生成和自我反思，显著提升了自然语言查询到可视化的自动化效果。


<details>
  <summary>Details</summary>
Motivation: 当前系统在处理包含多个文件的复杂数据集和迭代优化时表现不佳，现有方法往往过度简化任务，无法有效管理数据复杂性、代码错误或最终可视化质量。

Method: 将挑战重新定义为协作多智能体问题，引入CoDA系统，使用专门的LLM智能体进行元数据分析、任务规划、代码生成和自我反思，通过元数据分析绕过token限制，通过质量驱动的优化确保鲁棒性。

Result: 广泛评估显示CoDA在整体得分上取得显著提升，比竞争基线方法性能高出41.5%。

Conclusion: 可视化自动化的未来不在于孤立的代码生成，而在于集成的、协作的智能体工作流程。

Abstract: Deep research has revolutionized data analysis, yet data scientists still
devote substantial time to manually crafting visualizations, highlighting the
need for robust automation from natural language queries. However, current
systems struggle with complex datasets containing multiple files and iterative
refinement. Existing approaches, including simple single- or multi-agent
systems, often oversimplify the task, focusing on initial query parsing while
failing to robustly manage data complexity, code errors, or final visualization
quality. In this paper, we reframe this challenge as a collaborative
multi-agent problem. We introduce CoDA, a multi-agent system that employs
specialized LLM agents for metadata analysis, task planning, code generation,
and self-reflection. We formalize this pipeline, demonstrating how
metadata-focused analysis bypasses token limits and quality-driven refinement
ensures robustness. Extensive evaluations show CoDA achieves substantial gains
in the overall score, outperforming competitive baselines by up to 41.5%. This
work demonstrates that the future of visualization automation lies not in
isolated code generation but in integrated, collaborative agentic workflows.

</details>


### [32] [Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner](https://arxiv.org/abs/2510.03206)
*Cai Zhou,Chenxiao Yang,Yi Hu,Chenyu Wang,Chubin Zhang,Muhan Zhang,Lester Mackey,Tommi Jaakkola,Stephen Bates,Dinghuai Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种联合连续离散扩散模型CCDD，通过在连续表示空间和离散标记空间的联合空间中进行多模态扩散，解决了连续扩散模型在语言建模中表达能力与训练性能之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 连续扩散模型在理论上比离散扩散模型和循环变换器具有更强的表达能力，但在实际语言建模任务中表现不佳。作者认为这是因为连续扩散模型在将连续表示解码回离散标记空间时存在困难。

Method: 提出了CCDD模型，在连续表示空间和离散标记空间的联合空间上定义联合多模态扩散过程，使用单一模型同时在联合空间中进行去噪。结合了两种模态的优势，并提出了有效的架构和先进的训练/采样技术。

Result: 在真实世界语言建模任务的大规模实验中，CCDD展现出强大的实证性能，具有丰富的潜在空间语义和良好的样本质量。

Conclusion: CCDD通过联合连续离散扩散方法，成功解决了连续扩散模型在语言建模中的训练性能问题，同时保持了其理论上的表达能力优势。

Abstract: Diffusion language models, especially masked discrete diffusion models, have
achieved great success recently. While there are some theoretical and primary
empirical results showing the advantages of latent reasoning with looped
transformers or continuous chain-of-thoughts, continuous diffusion models
typically underperform their discrete counterparts. In this paper, we argue
that diffusion language models do not necessarily need to be in the discrete
space. In particular, we prove that continuous diffusion models have stronger
expressivity than discrete diffusions and looped transformers. We attribute the
contradiction between the theoretical expressiveness and empirical performance
to their practical trainability: while continuous diffusion provides
intermediate supervision that looped transformers lack, they introduce
additional difficulty decoding tokens into the discrete token space from the
continuous representation space. We therefore propose Coevolutionary Continuous
Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process
on the union of a continuous representation space and a discrete token space,
leveraging a single model to simultaneously denoise in the joint space. By
combining two modalities, CCDD is expressive with rich semantics in the latent
space, as well as good trainability and sample quality with the help of
explicit discrete tokens. We also propose effective architectures and advanced
training/sampling techniques for CCDD, which reveals strong empirical
performance in extensive language modeling experiments on real-world tasks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [33] [Drone Controller Localization Based on TDoA](https://arxiv.org/abs/2510.02622)
*Yuhong Wang,Yonghong Zeng,Peng Hui Tan,Sumei Sun,Yugang Ma*

Main category: cs.IT

TL;DR: 本文研究了基于TDoA的无人机控制器定位算法，分析了多径信道中的TDoA估计，提出了ML和LS-BF-GN两种算法来提高多径环境下的定位精度。


<details>
  <summary>Details</summary>
Motivation: 在多径信道环境下，传统的TDoA定位算法性能会显著下降，需要开发更鲁棒的算法来提高无人机控制器的定位精度。

Method: 提出了最大似然(ML)算法和最小二乘班克罗夫特-高斯牛顿(LS-BF-GN)算法，并在WLAN信道F和双射线地面反射信道中进行评估。

Result: 仿真结果表明，ML和LS-BF-GN算法在多径信道中显著优于LS-BF算法，通过多次定位估计平均可以进一步提高精度。

Conclusion: 所提出的算法能有效提高多径环境下的定位精度，同时需要考虑传感器间时间同步误差对定位性能的影响。

Abstract: We study time difference of arrival (TDoA)-based algorithms for drone
controller localization and analyze TDoA estimation in multipath channels.
Building on TDoA estimation, we propose two algorithms to enhance localization
accuracy in multipath environments: the Maximum Likelihood (ML) algorithm and
the Least Squares Bancroft with Gauss-Newton (LS-BF-GN) algorithm. We evaluate
these proposed algorithms in two typical outdoor channels: Wireless Local Area
Network (WLAN) Channel F and the two-ray ground reflection (TRGR) channel. Our
simulation results demonstrate that the ML and LS-BF-GN algorithms
significantly outperform the LS-BF algorithm in multipath channels. To further
enhance localization accuracy, we propose averaging multiple tentative location
estimations. Additionally, we evaluate the impact of time synchronization
errors among sensors on localization performance through simulation.

</details>


### [34] [Anti-Jamming Modulation for OFDM Systems under Jamming Attacks](https://arxiv.org/abs/2510.02640)
*Jaewon Yun,Joohyuk Park,Yo-Seb Jeon*

Main category: cs.IT

TL;DR: 提出了一种针对OFDM系统的抗干扰通信框架，包括抗干扰调制方案和自适应检测方法，在已知和未知干扰环境下都能实现优越的误码率性能。


<details>
  <summary>Details</summary>
Motivation: OFDM系统在干扰攻击下性能会严重下降，需要开发有效的抗干扰通信方案来增强系统的鲁棒性。

Method: 开发了使用扩频矩阵的抗干扰调制方案，设计了最大似然检测方法及其低复杂度变体，并提出了包含干扰非相干和相干两阶段的自适应通信框架。

Result: 仿真结果表明，所提出的抗干扰框架在各种通信和干扰场景下都优于现有的OFDM通信框架，具有优越的误码率性能。

Conclusion: 该抗干扰通信框架能够有效应对OFDM系统中的干扰攻击，在动态和未知干扰环境下保持可靠的通信性能。

Abstract: In this paper, we propose an anti-jamming communication framework for
orthogonal frequency-division multiplexing (OFDM) systems under jamming
attacks. To this end, we first develop an anti-jamming modulation scheme that
uses a spreading matrix to distribute each symbol across multiple subcarriers,
enhancing robustness against jamming. For optimal demodulation at a receiver,
we devise a maximum likelihood detection (MLD) method and its low-complexity
variant tailored to our anti-jamming modulation scheme in scenarios with known
jamming variance. We analyze the bit error rate (BER) of our modulation scheme
to optimize its modulation order according to a jamming scenario. To adapt to
dynamic and unknown jamming environments, we present a jamming-adaptive
communication framework consisting of two phases: (i) a jamming-noncoherent
phase and (ii) a jamming-coherent phase. In the jamming-noncoherent phase, we
develop an approximate MLD method that operates without prior knowledge of
jamming variance and enables the estimation of jamming parameters. In the
jamming-coherent phase, we use these estimated parameters to optimize the
proposed modulation scheme while employing the low-complexity MLD method.
Simulation results demonstrate the superior BER performance of the proposed
anti-jamming framework compared to existing OFDM communication frameworks
across a wide range of communication and jamming scenarios.

</details>


### [35] [Engineering Emergence](https://arxiv.org/abs/2510.02649)
*Abel Jansma,Erik Hoel*

Main category: cs.IT

TL;DR: 该论文扩展了因果涌现理论，分析了系统多尺度结构中的因果贡献分布，提出了系统可分为因果顶重、底重或复杂层次结构，并提供了量化这种复杂性的数学工具。


<details>
  <summary>Details</summary>
Motivation: 研究复杂系统的多尺度结构如何产生，以及这些尺度如何贡献于系统的因果工作机制，旨在更精确地理解尺度自由性和系统复杂性。

Method: 扩展因果涌现理论(2.0)，分析系统完整多尺度结构而非单一尺度路径的因果贡献，开发数学工具来量化多尺度结构的复杂性。

Result: 揭示了系统可分为因果顶重型、底重型或复杂层次结构，提供了比传统网络科学更具体的尺度自由性概念，展示了在系统多尺度结构中工程化涌现分布的能力。

Conclusion: 该研究为理解系统多尺度结构的因果层次提供了新的理论框架和数学工具，能够更精确地描述和工程化系统的涌现特性分布。

Abstract: One of the reasons complex systems are complex is because they have
multiscale structure. How does this multiscale structure come about? We argue
that it reflects an emergent hierarchy of scales that contribute to the
system's causal workings. An example is how a computer can be described at the
level of its hardware circuitry but also its software. But we show that many
systems, even simple ones, have such an emergent hierarchy, built from a small
subset of all their possible scales of description. Formally, we extend the
theory of causal emergence (2.0) so as to analyze the causal contributions
across the full multiscale structure of a system rather than just over a single
path that traverses the system's scales. Our methods reveal that systems can be
classified as being causally top-heavy or bottom-heavy, or their emergent
hierarchies can be highly complex. We argue that this provides a more specific
notion of scale-freeness (here, when causation is spread equally across the
scales of a system) than the standard network science terminology. More
broadly, we provide the mathematical tools to quantify this complexity and
provide diverse examples of the taxonomy of emergent hierarchies. Finally, we
demonstrate the ability to engineer not just degree of emergence in a system,
but how that emergence is distributed across the multiscale structure.

</details>


### [36] [Robust Segmented Analog Broadcast Design to Accelerate Wireless Federated Learning](https://arxiv.org/abs/2510.02701)
*Chong Zhang,Ben Liang,Min Dong,Ali Afana,Yahia Ahmed*

Main category: cs.IT

TL;DR: 提出分段模拟广播(SegAB)方案用于联邦学习的下行链路传输，通过分割模型参数并同时传输来减少延迟，在非完美信道状态信息下优化波束成形以最大化训练收敛速率。


<details>
  <summary>Details</summary>
Motivation: 在无线网络中，联邦学习的下行链路传输面临信道状态信息不完美和传输延迟的挑战，需要设计高效的广播方案来提升训练效率。

Method: 采用分段模拟广播(SegAB)方案，将全局模型参数向量分割成段，在公共下行链路上同时传输多个参数，并建立传输和接收过程的数学模型，通过epigraph表示和可行性子问题实现鲁棒波束成形。

Result: 在标准分类任务和典型无线网络设置下的仿真表明，SegAB方案显著优于传统的全模型逐参数广播和其他替代方案。

Conclusion: SegAB方案能够有效减少联邦学习的传输延迟，在非完美信道状态下通过鲁棒波束成形优化提升训练收敛性能。

Abstract: We consider downlink broadcast design for federated learning (FL) in a
wireless network with imperfect channel state information (CSI). Aiming to
reduce transmission latency, we propose a segmented analog broadcast (SegAB)
scheme, where the parameter server, hosted by a multi-antenna base station,
partitions the global model parameter vector into segments and transmits
multiple parameters from these segments simultaneously over a common downlink
channel. We formulate the SegAB transmission and reception processes to
characterize FL training convergence, capturing the effects of downlink
beamforming and imperfect CSI. To maximize the FL training convergence rate, we
establish an upper bound on the expected model optimality gap and show that it
can be minimized separately over the training rounds in online optimization,
without requiring knowledge of the future channel states. We solve the
per-round problem to achieve robust downlink beamforming, by minimizing the
worst-case objective via an epigraph representation and a feasibility
subproblem that ensures monotone convergence. Simulation with standard
classification tasks under typical wireless network setting shows that the
proposed SegAB substantially outperforms conventional full-model per-parameter
broadcast and other alternatives.

</details>


### [37] [Symbol Timing Synchronization and Signal Detection for Ambient Backscatter Communication](https://arxiv.org/abs/2510.02981)
*Yuxin Li,Guangyue Lu,Yinghui Ye,Zehui Xiong,Liqin Shi*

Main category: cs.IT

TL;DR: 提出了一种针对环境反向散射通信中符号定时偏移问题的最大似然估计方法，通过专用导频序列和能量检测器实现准确的STO估计和符号检测。


<details>
  <summary>Details</summary>
Motivation: 环境反向散射通信在实际应用中存在符号定时偏移问题，这源于传播延迟和接收器激活延迟，导致不可靠的符号恢复。传统基于相关的同步方法在AmBC中不可行，因为环境射频源不可控。

Method: 设计了专用导频序列以在导频信号中引入采样误差；提出了基于最大似然估计的导频STO估计器，利用接收导频信号的统计变化；将STO补偿集成到能量检测器中。

Result: 仿真结果表明，所提出的估计器实现了准确的STO估计，并有效缓解了由STO引起的BER性能下降。

Conclusion: 该方法能够在不需要环境射频源协调的情况下，有效解决AmBC中的符号定时偏移问题，提高通信可靠性。

Abstract: Ambient backscatter communication (AmBC) enables ambient Internet of Things
(AIoT) devices to achieve ultra-low-power, low-cost, and massive connectivity.
Most existing AmBC studies assume ideal synchronization between the backscatter
device (BD) and the backscatter receiver (BR). However, in practice, symbol
timing offset (STO) occurs due to both the propagation delay and the BR
activation latency, which leads to unreliable symbol recovery at the BR.
Moreover, the uncontrollable nature of the ambient radio frequency source
renders conventional correlation-based synchronization methods infeasible in
AmBC. To address this challenge, we investigate STO estimation and symbol
detection in AmBC without requiring coordination from the ambient radio
frequency source. Firstly, we design a specialized pilot sequence at the BD to
induce sampling errors in the pilot signal. Furthermore, we propose a
pilot-based STO estimator using the framework of maximum likelihood estimation
(MLE), which can exploit the statistical variations in the received pilot
signal. Finally, we integrate STO compensation into an energy detector and
evaluate the bit error rate (BER) performance. Simulation results show that the
proposed estimator achieves accurate STO estimation and effectively mitigates
the BER performance degradation caused by STO.

</details>


### [38] [Transport of Event Equation: Phase Retrieval from Defocus Events](https://arxiv.org/abs/2510.02989)
*Kaito Hori,Chihiro Tsutake,Keita Takahashi,Toshiaki Fujii*

Main category: cs.IT

TL;DR: 利用事件视觉传感器(EVS)进行相位恢复，通过沿光轴移动EVS记录离焦事件，建立传输事件方程来线性关联离焦事件与相位分布，实现快速稳定的相位恢复。


<details>
  <summary>Details</summary>
Motivation: 传统相位恢复方法在时间效率和稳定性方面存在不足，特别是在低光照条件下。EVS能够以高动态范围检测像素级对数强度变化，为快速稳定的相位信息获取提供了新途径。

Method: 将EVS沿光轴平移，记录由离焦引起的强度变化事件。建立传输事件方程(TEE)，该偏微分方程呈现了离焦事件与相位分布之间的线性关系。

Result: 实验证明EVS比传统图像传感器在快速稳定检测强度信息方面更具优势，特别是在低光照条件下能够实现准确的相位恢复。

Conclusion: 基于EVS的相位恢复方法通过利用离焦事件和传输事件方程，提供了一种时间高效且稳定的相位获取方案，特别适用于低光照条件。

Abstract: To time-efficiently and stably acquire the intensity information for phase
retrieval under a coherent illumination, we leverage an event-based vision
sensor (EVS) that can detect changes in logarithmic intensity at the pixel
level with a wide dynamic range. In our optical system, we translate the EVS
along the optical axis, where the EVS records the intensity changes induced by
defocus as events. To recover phase distributions, we formulate a partial
differential equation, referred to as the transport of event equation, which
presents a linear relationship between the defocus events and the phase
distribution. We demonstrate through experiments that the EVS is more
advantageous than the conventional image sensor for rapidly and stably
detecting the intensity information, defocus events, which enables accurate
phase retrieval, particularly under low-lighting conditions.

</details>


### [39] [Tradeoffs on the volume of fault-tolerant circuits](https://arxiv.org/abs/2510.03057)
*Anirudh Krishna,Gilles Zémor*

Main category: cs.IT

TL;DR: 论文证明纠错码无法同时具备恒定码率、增长距离和短深度CNOT门实现，这表明在计算应用中需要权衡码率、距离和电路深度。


<details>
  <summary>Details</summary>
Motivation: 选择适合计算的纠错码很困难，因为需要在码率、距离和编码信息可访问性之间取得平衡，而后者对于在编码数据上执行计算至关重要。

Method: 通过理论证明分析纠错码在计算应用中的基本限制，特别是研究码率、距离与实现CNOT门所需的电路深度之间的关系。

Result: 证明任何码族都不能同时具有恒定码率、增长距离和短深度CNOT门实现，这意味着良好的码率和距离可能需要接受很深的电路。

Conclusion: 在计算应用中实现良好的码率和距离可能需要以电路深度为代价，这种权衡在某些架构和应用中是不理想的。

Abstract: Dating back to the seminal work of von Neumann [von Neumann, Automata
Studies, 1956], it is known that error correcting codes can overcome faulty
circuit components to enable robust computation. Choosing an appropriate code
is non-trivial as it must balance several requirements. Increasing the rate of
the code reduces the relative number of redundant bits used in the
fault-tolerant circuit, while increasing the distance of the code ensures
robustness against faults. If the rate and distance were the only concerns, we
could use asymptotically optimal codes as is done in communication settings.
However, choosing a code for computation is challenging due to an additional
requirement: The code needs to facilitate accessibility of encoded information
to enable computation on encoded data. This seems to conflict with having large
rate and distance. We prove that this is indeed the case, namely that a code
family cannot simultaneously have constant rate, growing distance and
short-depth gadgets to perform encoded CNOT gates. As a consequence, achieving
good rate and distance may necessarily entail accepting very deep circuits, an
undesirable trade-off in certain architectures and applications.

</details>


### [40] [On the Hardness of the One-Sided Code Sparsifier Problem](https://arxiv.org/abs/2510.03184)
*Elena Grigorescu,Alice Moayyedi*

Main category: cs.IT

TL;DR: 本文证明了寻找最小尺寸的单边1/2稀疏化器是NP难问题，并给出了近似难度结果。


<details>
  <summary>Details</summary>
Motivation: 代码稀疏化是图论中割稀疏化概念的类比，最近Gharan和Sahami证明了单边1/2稀疏化器的存在性，但计算最小尺寸稀疏化器的复杂性尚不清楚。

Method: 通过从经典的最近码字问题归约，证明最小单边1/2稀疏化器问题的NP难性。

Result: 证明了寻找最小尺寸的单边1/2稀疏化器是NP难问题，并给出了近似难度结果。

Conclusion: 代码稀疏化的计算问题具有内在的复杂性，最小稀疏化器的寻找是计算困难的问题。

Abstract: The notion of code sparsification was introduced by Khanna, Putterman and
Sudan (arxiv.2311.00788), as an analogue to the the more established notion of
cut sparsification in graphs and hypergraphs. In particular, for $\alpha\in
(0,1)$ an (unweighted) one-sided $\alpha$-sparsifier for a linear code
$\mathcal{C} \subseteq \mathbb{F}_2^n$ is a subset $S\subseteq [n]$ such that
the weight of each codeword projected onto the coordinates in $S$ is preserved
up to an $\alpha$ fraction. Recently, Gharan and Sahami (arxiv.2502.02799) show
the existence of one-sided 1/2-sparsifiers of size $n/2+O(\sqrt{kn})$ for any
linear code, where $k$ is the dimension of $\mathcal{C}$. In this paper, we
consider the computational problem of finding a one-sided 1/2-sparsifier of
minimal size, and show that it is NP-hard, via a reduction from the classical
nearest codeword problem. We also show hardness of approximation results.

</details>
