<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.IT](#cs.IT) [Total: 5]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Fundamentals of Next-generation Network Planning](https://arxiv.org/abs/2508.13469)
*M. Umar Khan*

Main category: cs.NI

TL;DR: 基于4G网络数据和群智基础设施见解，研究5G网络规划方法，通过平衡的无线网络规模和数据驱动策略，解决覆盖与容量的批执，以最小化部署成本和附加值成本。


<details>
  <summary>Details</summary>
Motivation: 5G网络需要支持多种服务需求，包括高数据速率、低延迟和体验质量。继承4G网络生成的大量数据以及群智基础设施见解，可以识别高流量布局区域，但体验质量仍是网络运营商面临的重大挑战。

Method: 探索5G网络规划的基础方法，通过平衡的无线网络规模技术，结合实用的新无线电建模和数据驱动策略，以协调覆盖与容量之间的批执。

Result: 提出了一种能够最小化部署成本和附加值成本的5G网络规划方法，通过数据驱动的方法识别高流量布局区域，并使用新无线电的数字学和带宽部分来支持服务尺寸调整的资源分配。

Conclusion: 该研究为5G网络规划提供了基础方法，通过数据驱动和平衡的无线网络规模，有效解决了覆盖与容量的批执，并能够在保证体验质量的同时最小化部署成本和附加值成本，对于5G网络的实际部署具有重要意义。

Abstract: The fifth-generation (5G) of cellular communications is expected to be
deployed in the next years to support a wide range of services with different
demands of peak data rates, latency and quality of experience (QoE). To support
higher data rates and latency requirements third-generation partnership project
(3GPP) has introduced numerology and bandwidth parts (BWPs), via new radio (NR)
for service-tailored resource allocation. Legacy 4G networks have generated
extensive data, which combined with crowd-sourced LTE infrastructure insights,
enables identification of high-traffic 5G deployment area (5GDA) for planning
new services. Given the mission-critical nature of 5G services, QoE is a big
challenge for MNOs to guarantee peak data rates for a defined percentage of
time. This work studies the fundamentals of 5G network planning methods that
reconciles coverage-capacity trade-offs through balanced radio network
dimensioning (RND), leveraging pragmatic NR modeling, and data-driven
strategies to minimize deployment costs and reduce cost-per-bit.

</details>


### [2] [Electromagnetic Signal Modulation Recognition based on Subgraph Embedding Learning](https://arxiv.org/abs/2508.13474)
*Bojun Zhang*

Main category: cs.NI

TL;DR: 提出了SEL-AMR算法，首个能够适应任意动态信道和系统的深度学习自动调制识别方法，通过子图嵌入学习结构提取鲁棒特征，在5个真实数据集上比现有最优算法提升20%宏平均识别精度和30%识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的自动调制识别算法都是针对特定信道和系统设计的，因为训练数据集的数据维度是固定的，无法适应动态变化的通信环境。

Method: 提出子图嵌入学习(SEL)结构，将通信系统视为子图，利用样本间关系来平滑噪声和不同信道带来的影响，从而提取鲁棒特征。

Result: 在5个公开真实数据集和少量仿真数据上测试，SEL-AMR能够很好地适应不同信道和系统，始终优于现有最优算法，宏平均识别精度提升达20%，识别准确率提升30%。

Conclusion: SEL-AMR是首个能够适应任意动态信道和系统的自动调制识别算法，通过子图嵌入学习方法有效解决了传统方法对特定信道依赖的问题。

Abstract: Automatic Modulation Recognition (AMR) detects
  modulation schemes of received signals for further processing
  of signals without any priori information, which is critically
  important for civil spectrum regulation, information countermea sures, and
communication security. Due to the powerful feature
  extraction and classification capabilities of Deep Learning (DL),
  DL-based AMR algorithms have achieved excellent performance
  gains compared with traditional modulation detection algorithms.
  However, all existing DL-based AMR algorithms, to the best of
  our knowledge, are designed for specific channels and systems,
  because data dimension of the used training dataset is fixed. To
  this end, we takes the first step to propose a Subgraph Embedding
  Learning (SEL) structure to address the classical AMR problem,
  and the proposed algorithm is called SEL-AMR. Our algorithm
  treats the communication system as a subgraph and uses the
  relationship between samples to smooth the effects brought by
  noise and different channels to extract robust features. Thus,
  the proposed SEL-AMR algorithm can adapt to any dynamic
  channels and systems. We use 5 public real datasets and a small
  amount of simulation data to evaluate our SEL-AMR algorithm.
  Experimental results reveal that SEL-AMR can well adapt to
  different channels and systems, and always outperforms the state of-the-art
algorithms by improving up to 20% macro-average
  recognition precision and 30% recognition accuracy.

</details>


### [3] [CountingStars: Low-overhead Network-wide Measurement in LEO Mega-constellation Networks](https://arxiv.org/abs/2508.13512)
*Xiyuan Liu,Guano Liu,Xiucheng Tian,Wenting Wei*

Main category: cs.NI

TL;DR: CountingStars是一个低开销的网络测量架构，通过数字孪生系统和端口聚合数据结构解决LEO星座网络中包负载均衡带来的内存膨胀和哈希碰撞问题


<details>
  <summary>Details</summary>
Motivation: LEO巨型星座的高移动性导致网络拓扑高度动态，包负载均衡(PBLB)虽然能缓解服务中断，但带来了端口级粒度要求导致的内存膨胀和严重哈希碰撞问题

Method: 在地面控制器构建数字孪生系统预测未来网络拓扑，提前生成无碰撞哈希种子；在卫星上采用端口聚合数据结构，通过高效位操作解耦流标识符和多端口计数器

Result: 内存使用平均减少70%，测量相对误差平均降低90%，FPGA实现验证了实际部署可行性

Conclusion: CountingStars有效解决了PBLB带来的测量挑战，为动态LEO网络提供了高效可靠的测量解决方案

Abstract: The high mobility of satellites in Low Earth Orbit (LEO) mega-constellations
induces a highly dynamic network topology, leading to many problems like
frequent service disruptions. To mitigate this, Packet-based Load Balancing
(PBLB) is employed. However, this paradigm shift introduces two critical
challenges for network measurement stemming from the requirement for port-level
granularity: memory inflation and severe hash collisions. To tackle these
challenges, we propose CountingStars, a low-overhead network-wide measurement
architecture. In the ground controller, CountingStars builds a digital twins
system to accurately predict the future network topology. This allows ground
controller to generate and distribute collision-free hash seeds to satellites
in advance. On the satellite, we introduce a port aggregation data structure
that decouples the unique flow identifier from its multi-port counter and
updates it through efficient bit operations, solving the memory inflation
caused by PBLB. Simulation results show that the memory usage of CountingStars
is reduced by 70\% on average, and the relative error of measurement is reduced
by 90\% on average. Implementation on FPGA shows its prospect to deploy in real
system.

</details>


### [4] [Security-as-a-Function for IDS/IPS in Softwarized Network and Applications to 5G Network Systems](https://arxiv.org/abs/2508.13581)
*Shivank Malik,Samaresh Bera*

Main category: cs.NI

TL;DR: 这篇论文采用虚拟化IDS-IPS网络功能来防护5G核心网络受到DoS/DDoS攻击，通过VM和容器化实现并验证其能满足5G应用的QoS要求。


<details>
  <summary>Details</summary>
Motivation: 5G网络服务化架构带来了更多安全漏洞和威胁，但现有研究少有关注安全方面的网络功能部署问题。

Method: 采用VM和容器化方式实现IDS-IPS虚拟化网络功能，在5G软件化核心网络中部署，通过网络吞吐量、延迟和丢包率等指标评估网络性能。

Result: 实验结果显示软件化IDS-IPS能够满足5G应用的QoS要求，同时有效防范DoS和DDoS攻击。

Conclusion: 虚拟化IDS-IPS在5G核心网络中的部署是可行且有效的，能在保障网络安全的同时维持网络性能。

Abstract: The service-based architecture of 5G network allows network operators to
place virtualized network functions on commodity hardware, unlike the
traditional vendor-specific hardware-based functionalities. However, it expands
the security vulnerabilities and threats to the 5G network. While there exist
several theoretical studies on network function placement and service routing,
a few focused on the security aspects of the 5G network systems.
  This paper focuses on safeguarding the 5G core network systems from DoS and
DDoS attacks by placing intrusion detection and prevention systems (IDS-IPS) as
virtualized network functions following the 5G standalone architecture. To
ensure the virtualized placement of IDS-IPS, first, we provide thorough virtual
machine (VM)-based and containerized implementation details and evaluate the
network performance with two scenarios, IDS and IPS, in the presence of TCP and
UDP applications. Second, we apply the VM-based implementation of IDS-IPS on a
softwarized 5G core network and study the network performances. The experiment
results on network throughput, latency, and packet drop reveal that the
softwarized IDS-IPS can meet the QoS requirements of 5G applications, while
safeguarding the network from DoS and DDoS attacks.

</details>


### [5] [Towards Timing Isolation for Mixed-Criticality Communication in Software-Defined Vehicles](https://arxiv.org/abs/2508.13652)
*Lóránt Meszlényi,Julius Kahle,Dominik Püllen,Stefan Kowalewski,Stefan Katzenbeisser,Alexandru Kampmann*

Main category: cs.NI

TL;DR: 基于Linux的汽车控制单元中通过三层隔离架构实现混合关键性应用的可预测网络通信


<details>
  <summary>Details</summary>
Motivation: 解决Linux网络栈并发使用导致的干扰问题，确保混合关键性应用在汽车控制单元中的可预测执行

Method: 采用三层架构：中间件层固定优先级调度器，网络层XDP直接路由高优先级数据，硬件层专用NIC队列

Result: 实现了一致可预测的实时流量延迟，甚至在量应用干扰下也能保持稳定性

Conclusion: 该多层隔离方案有效解决了Linux基汽车系统中混合关键性应用的网络干扰问题

Abstract: As the automotive industry transitions toward centralized Linux-based
architectures, ensuring the predictable execution of mixed-criticality
applications becomes essential. However, concurrent use of the Linux network
stack introduces interference, resulting in unpredictable latency and jitter.
To address this challenge, we present a layered software architecture that
enforces timing isolation for Ethernet-based data exchange between
mixed-criticality applications on Linux-based automotive control units. Our
approach integrates traffic prioritization strategies at the middleware layer,
the network stack layer, and the hardware layer to achieve isolation across the
full software stack. At the middleware layer, we implement a fixed-priority,
non-preemptive scheduler to manage publishers of varying criticality. At the
network layer, we leverage the express data path (XDP) to route high-priority
data directly from the network interface driver into critical application
memory, bypassing the standard Linux network stack. At the hardware layer, we
dedicate a network interface card (NIC) queue exclusively to real-time traffic.
We demonstrate how our architecture performs in a Data Distribution Service
(DDS)-based system. Our evaluation shows that the approach leads to consistent
and predictable latencies for real-time traffic, even under heavy interference
from best-effort applications.

</details>


### [6] [Architecture Considerations for ISAC in 6G](https://arxiv.org/abs/2508.13736)
*Sebastian Robitzsch,Laksh Bhatia,Konstantinos G. Filis,Neda Petreska,Michael Bahr,Pablo Picazo Martinez,Xi Li*

Main category: cs.NI

TL;DR: 本文探讨6G中集成传感与通信(ISAC)的架构设计，基于3GPP和ETSI标准进展，提出支持感知服务的新型网络功能和协议栈适配方案。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要同时提供通信服务和环境感知能力，ISAC作为基础能力需要系统性的架构支持，以满足未来多场景感知应用需求。

Method: 基于欧洲MultiX项目的用例分析，提出集成新型网络功能(NFs)的6G系统架构，设计控制面和新感知面的协议栈适配方案。

Result: 构建了支持感知即服务的6G架构框架，明确了感知功能在网络中的部署方式和协议支持机制。

Conclusion: 提出的架构方案为6G ISAC标准化和实际部署提供了重要参考，推动了感知能力在移动网络中的集成与应用。

Abstract: ISAC is emerging as a foundational capability in 6G, enabling mobile networks
to not only offer communication services but also to sense and perceive their
environment at scale. This paper explores architectural considerations to
enable sensing in 6G, extending on recent developments by (pre-)standardisation
bodies such as 3GPP and ETSI. Selected ISAC use cases are presented from the
European MultiX project including associated potential functional system
requirements. The paper proposes a 6G system architecture that integrates newly
proposed NFs for the purpose of sensing and demonstrates how they are being
used in offering sensing as a service. Protocol stack adaptations for both
control and a newly proposed sensing plane are discussed.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
*Weizhen Li,Jianbo Lin,Zhuosong Jiang,Jingyi Cao,Xinpeng Liu,Jiayu Zhang,Zhenqiang Huang,Qianben Chen,Weichen Sun,Qiexiang Wang,Hongxuan Lu,Tianrui Qin,Chenghao Zhu,Yi Yao,Shuying Fan,Xiaowan Li,Tiannan Wang,Pai Liu,King Zhu,He Zhu,Dingfeng Shi,Piaohong Wang,Yeyi Guan,Xiangru Tang,Minghao Liu,Yuchen Eleanor Jiang,Jian Yang,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 链式机器人框架(CoA)通过多机器渗遗和机器人强化学习训练出端到端的机器人基础模型(AFM)，在多任务赋能中达到新的SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有多机器系统依赖手动提示工程，计算效率低且无法从数据中学习，需要一种本地的端到端复杂问题解决方案

Method: 提出链式机器人(CoA)范式，通过多机器渗遗把SOTA多机器系统转换为CoA轨迹进行监督微调，然后用机器人强化学习在可验证任务上进一步提升

Result: AFM模型在web agent和code agent多个基准测试中达到新的最高性能水平

Conclusion: 链式机器人框架能够在单个模型内实现端到端的多机器协作问题解决，并通过开源研究为机器人模型研究提供坚实基础

Abstract: Recent advances in large language models (LLMs) and multi-agent systems have
demonstrated remarkable capabilities in complex problem-solving tasks such as
deep research, vibe coding, and mathematical reasoning. However, most existing
multi-agent systems are built upon manual prompt/workflow engineering with
sophisticated agent frameworks, making them computationally inefficient, less
capable, and can not benefit from data-centric learning. In this work, we
introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables
native end-to-end complex problem-solving in the same way as a multi-agent
system (i.e., multi-turn problem solving with multiple tools and multiple
agents) within one model. In chain-of-agents problem-solving, the model
dynamically activates different tool agents and role-playing agents to simulate
multi-agent collaboration in an end-to-end fashion. To elicit end-to-end
chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent
distillation framework to distill state-of-the-art multi-agent systems into
chain-of-agents trajectories for agentic supervised fine-tuning. We then use
agentic reinforcement learning on verifiable agentic tasks to further improve
the models' capabilities on chain-of-agents problem solving. We call the
resulting models Agent Foundation Models (AFMs). Our empirical studies
demonstrate that AFM establishes new state-of-the-art performance across
diverse benchmarks in both web agent and code agent settings. We make the
entire research, including the model weights, code for training and evaluation,
and the training data, fully open-sourced, which offers a solid starting point
for future research on agent models and agentic RL.

</details>


### [8] [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
*Tao An*

Main category: cs.AI

TL;DR: 提出了Cognitive Workspace新范式，通过模拟人类外部记忆认知机制，超越传统RAG方法，实现主动记忆管理和任务驱动优化


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在上下文管理方面的根本限制，尽管现有技术能扩展上下文窗口至百万token，但缺乏人类认知的动态任务驱动特性

Method: 基于认知科学理论（Baddeley工作记忆模型、Clark扩展心智论、Hutchins分布式认知框架），采用三大创新：主动记忆管理、分层认知缓冲器、任务驱动上下文优化

Result: 平均58.6%记忆重用率（传统RAG为0%），净效率增益17-18%，统计显著性p<0.001，Cohen's d>23

Conclusion: Cognitive Workspace代表了从信息检索到真正认知增强的根本性转变，建立了主动记忆系统优越性的首个定量证据

Abstract: Large Language Models (LLMs) face fundamental limitations in context
management despite recent advances extending context windows to millions of
tokens. We propose Cognitive Workspace, a novel paradigm that transcends
traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive
mechanisms of external memory use. Drawing from cognitive science foundations
including Baddeley's working memory model, Clark's extended mind thesis, and
Hutchins' distributed cognition framework, we demonstrate that current passive
retrieval systems fail to capture the dynamic, task-driven nature of human
memory management. Our analysis of 2024-2025 developments reveals that while
techniques like Infini-attention and StreamingLLM achieve impressive context
lengths, they lack the metacognitive awareness and active planning capabilities
essential for true cognitive extension. Cognitive Workspace addresses these
limitations through three core innovations: (1) active memory management with
deliberate information curation, (2) hierarchical cognitive buffers enabling
persistent working states, and (3) task-driven context optimization that
dynamically adapts to cognitive demands. Empirical validation demonstrates
Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from
54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%
net efficiency gain despite 3.3x higher operation counts. Statistical analysis
confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple
task types, establishing the first quantitative evidence for active memory
superiority in LLM systems. We present a comprehensive theoretical framework
synthesizing insights from 50+ recent papers, positioning Cognitive Workspace
as a fundamental shift from information retrieval to genuine cognitive
augmentation.

</details>


### [9] [AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining](https://arxiv.org/abs/2508.13174)
*Hongjun Ding,Binqi Chen,Jinsheng Huang,Taian Guo,Zhengyang Mao,Guoyi Shao,Lutong Zou,Luchen Liu,Ming Zhang*

Main category: cs.AI

TL;DR: 提出AlphaEval框架，用于评估自动化alpha挖掘模型，解决传统评估方法的缺陷，包括四种补充维度的综合评估


<details>
  <summary>Details</summary>
Motivation: 现有alpha挖掘评估方法存在显著缺陷：回测计算粘度高且效率低，相关性指标只考虑预测能力而忽视其他重要特性，同时闭源模型影响可复现性

Method: 设计AlphaEval统一框架，通过五个补充维度评估alpha质量：预测能力、稳定性、市场干扰鲁棒性、金融逻辑性和多样性，支持并行化计算且无需回测

Result: 实验结果显示AlphaEval评估一致性与全面回测相当，但提供更全面的洞察和更高效率，同时能够有效识别优质alpha，超过传统单指标筛选方法

Conclusion: AlphaEval为alpha挖掘领域提供了一个统一、高效、全面的评估框架，充分考虑了alpha的多维度质量，并通过开源代码促进领域可复现性和社区参与

Abstract: Formula alpha mining, which generates predictive signals from financial data,
is critical for quantitative investment. Although various algorithmic
approaches-such as genetic programming, reinforcement learning, and large
language models-have significantly expanded the capacity for alpha discovery,
systematic evaluation remains a key challenge. Existing evaluation metrics
predominantly include backtesting and correlation-based measures. Backtesting
is computationally intensive, inherently sequential, and sensitive to specific
strategy parameters. Correlation-based metrics, though efficient, assess only
predictive ability and overlook other crucial properties such as temporal
stability, robustness, diversity, and interpretability. Additionally, the
closed-source nature of most existing alpha mining models hinders
reproducibility and slows progress in this field. To address these issues, we
propose AlphaEval, a unified, parallelizable, and backtest-free evaluation
framework for automated alpha mining models. AlphaEval assesses the overall
quality of generated alphas along five complementary dimensions: predictive
power, stability, robustness to market perturbations, financial logic, and
diversity. Extensive experiments across representative alpha mining algorithms
demonstrate that AlphaEval achieves evaluation consistency comparable to
comprehensive backtesting, while providing more comprehensive insights and
higher efficiency. Furthermore, AlphaEval effectively identifies superior
alphas compared to traditional single-metric screening approaches. All
implementations and evaluation tools are open-sourced to promote
reproducibility and community engagement.

</details>


### [10] [Fitting Ontologies and Constraints to Relational Structures](https://arxiv.org/abs/2508.13176)
*Simon Hosemann,Jean Christoph Jung,Carsten Lutz,Sebastian Rudolph*

Main category: cs.AI

TL;DR: 该论文研究如何从正负示例中拟合本体和约束，重点关注描述逻辑EL和ELI以及多种TGD类型，分析了计算复杂性、算法设计、拟合本体大小，并探讨了有限基的存在性问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何从有限关系结构的正负示例中自动学习本体和约束，这对于知识表示和数据库系统的自动化构建具有重要意义。

Method: 使用描述逻辑EL和ELI以及多种类型的元组生成依赖(TGD)作为本体和约束语言，分析不同语言下的计算复杂性，设计相应的拟合算法，并研究有限基的构造问题。

Result: 精确确定了各种语言下的计算复杂度，设计了相应的算法，分析了拟合本体的大小限制，并发现对于EL、ELI、保护TGD和包含依赖存在有限基，但对于完全、前沿保护和前沿一TGD通常不存在有限基。

Conclusion: 该研究为从示例中学习本体和约束提供了理论基础和算法支持，揭示了不同语言在可学习性方面的差异，对知识表示和数据库理论的自动化学习具有重要意义。

Abstract: We study the problem of fitting ontologies and constraints to positive and
negative examples that take the form of a finite relational structure. As
ontology and constraint languages, we consider the description logics
$\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several
classes of tuple-generating dependencies (TGDs): full, guarded,
frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion
dependencies. We pinpoint the exact computational complexity, design
algorithms, and analyze the size of fitting ontologies and TGDs. We also
investigate the related problem of constructing a finite basis of concept
inclusions / TGDs for a given set of finite structures. While finite bases
exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs,
and inclusion dependencies, they in general do not exist for full,
frontier-guarded and frontier-one TGDs.

</details>


### [11] [A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment](https://arxiv.org/abs/2508.13177)
*Nikola Pižurica,Nikola Milović,Igor Jovančević,Conor Heins,Miguel de Prado*

Main category: cs.AI

TL;DR: 通过统一的稀疏计算图优化，将pymdp的灵活性与硬件效率相结合，使主动推断在资源受限环境中的部署成为可能


<details>
  <summary>Details</summary>
Motivation: 主动推断(AIF)虽然提供了健壮的决策框架，但其计算和内存需求在资源受限环境中面临挑战

Method: 集成pymdp的灵活性和效率，构建专门为硬件效率执行而设计的统一稀疏计算图

Result: 延迟减少2倍以上，内存使用量最高减少35%

Conclusion: 该方法推进了高效AIF代理在实时和嵌入式应用中的部署

Abstract: Active Inference (AIF) offers a robust framework for decision-making, yet its
computational and memory demands pose challenges for deployment, especially in
resource-constrained environments. This work presents a methodology that
facilitates AIF's deployment by integrating pymdp's flexibility and efficiency
with a unified, sparse, computational graph tailored for hardware-efficient
execution. Our approach reduces latency by over 2x and memory by up to 35%,
advancing the deployment of efficient AIF agents for real-time and embedded
applications.

</details>


### [12] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: 通过模型可解释性分析和执行导向策略，结合筛选调整、逻辑关联精炼和模型融合，提出CESQL模型来提升WHERE子句语义解析的准确性和通用性


<details>
  <summary>Details</summary>
Motivation: 提升文本到SQL模型在实际应用中的基础能力和通用性，特别是在WHERE子句语义解析方面

Method: 整合模型可解释性分析与执行导向策略，结合筛选调整、逻辑关联精炼和模型融合技术，设计CESQL来实现条件增强

Result: 在WikiSQL数据集上表现优异，显著提升了预测结果的准确性，减少了对表格条件列数据的依赖，避免了人工标注训练数据的影响

Conclusion: 该研究为处理基础数据库查询提供了准确性提升的新方法，为处理复杂查询和实际数据库中不规则数据的研究开启了新视角

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [13] [Search-Time Data Contamination](https://arxiv.org/abs/2508.13180)
*Ziwen Han,Meher Mankikar,Julian Michael,Zifan Wang*

Main category: cs.AI

TL;DR: 搜索基于LLM代理的评估存在搜索时污染问题，当代理通过搜索获取到HuggingFace上的测试数据集时，会直接拷贝答案而非真正推理，导致评测结果失真。在3%的问题中发现了这种污染，阻塞HuggingFace后准确率下降约15%。


<details>
  <summary>Details</summary>
Motivation: 识别和揭示搜索基于LLM代理评估中存在的新型污染问题，即搜索时污染，这种污染会导致代理直接获取测试答案而非真正进行推理，影响评测的有效性和可靠性。

Method: 在三个常用能力测试集（HLE、SimpleQA、GPQA）上进行实验，分析搜索基于LLM代理的访问记录，检测是否从HuggingFace等在线平台获取到测试数据集。通过阻塞HuggingFace来验证污染效果，并进行分离实验以确认污染源。

Result: 发现约3%的问题存在直接从HuggingFace获取测试数据集的情况。阻塞HuggingFace后，在受污染问题子集上的准确率下降约15%，证明污染对评估结果有显著影响。分离实验显示HuggingFace可能不是唯一的污染源。

Conclusion: 提出了对付搜索时污染问题的最佳实践建议，包括测试集设计和结果报告方面的改进措施，以确保搜索基于LLM代理评估的可靠性。同时公开完整实验日志以便审计验证。

Abstract: Data contamination refers to the leakage of evaluation data into model
training data, resulting in overfitting to supposedly held-out test sets and
compromising test validity. We identify an analogous issue, search-time
contamination (STC), in evaluating search-based LLM agents which use tools to
gather information from online sources when answering user queries. STC occurs
when the retrieval step surfaces a source containing the test question (or a
near-duplicate) alongside its answer, enabling agents to copy rather than
genuinely infer or reason, undermining benchmark integrity. We find that
HuggingFace, an online platform hosting evaluation datasets, appears among
retrieved sources in search based agent logs. Consequently, agents often
explicitly acknowledge discovering question answer pairs from HuggingFace
within their reasoning chains. On three commonly used capability benchmarks:
Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for
approximately 3% of questions, search-based agents directly find the datasets
with ground truth labels on HuggingFace. When millions of evaluation queries
target the same benchmark, even small, repeated leaks can accelerate the
benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace
is blocked, we observe a drop in accuracy on the contaminated subset of
approximately 15%. We further show through ablation experiments that publicly
accessible evaluation datasets on HuggingFace may not be the sole source of
STC. To this end, we conclude by proposing best practices for benchmark design
and result reporting to address this novel form of leakage and ensure
trustworthy evaluation of search-based LLM agents. To facilitate the auditing
of evaluation results, we also publicly release the complete logs from our
experiments.

</details>


### [14] [QuickMerge++: Fast Token Merging with Autoregressive Prior](https://arxiv.org/abs/2508.13204)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: QuickMerge是一个轻量级token合并框架，通过注意力范数动态选择重要token，使用熵预算估计器减少计算量，同时保持自回归生成兼容性


<details>
  <summary>Details</summary>
Motivation: 随着生成模型处理更大规模输入，token级别的计算成本成为关键瓶颈，现有token选择方法大多是静态的、模态特定的或不兼容自回归生成

Method: 基于注意力范数动态选择重要token，使用熵预算估计器确定token数量，引入轻量级transformer先验在合并token序列上训练以保持自回归兼容性

Result: 在多模态领域评估显示，QuickMerge显著减少token数量，同时匹配甚至超越学习型tokenizer和固定patch基线的性能

Conclusion: QuickMerge通过语义显著性估计、灵活token预算和自回归对齐，实现了用更少token进行准确生成，在计算-精度权衡方面取得一致改进

Abstract: As generative models scale to larger inputs across language, vision, and
video domains, the cost of token-level computation has become a key bottleneck.
While prior work suggests that only a subset of tokens significantly influence
downstream predictions, most token selection methods are static,
modality-specific, or incompatible with autoregressive generation. In this
paper, we propose QuickMerge, a lightweight token merging framework designed
for efficient next-token prediction.
  QuickMerge dynamically selects a reduced number of tokens based on attention
norm magnitude, guided by an entropy-based budget estimator. To preserve
autoregressive compatibility, we introduce a lightweight transformer prior
trained over the merged token sequence. By combining semantic salience
estimation, flexible token budgets, and AR alignment, QuickMerge enables
accurate generation with fewer tokens.
  We evaluate QuickMerge across multi-modality domains, demonstrating
consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge
reduces token counts sustantially while matching as well as exceeding the
performance of learned tokenizers and fixed-patch baselines.

</details>


### [15] [AI sustains higher strategic tension than humans in chess](https://arxiv.org/abs/2508.13213)
*Adamo Cerioli,Edward D. Lee,Vito D. P. Servedio*

Main category: cs.AI

TL;DR: 棋牌中人工智能与人类在战略决策中的差异：AI能维持更高水平的战略张力和更长时间的攻防平衡，而人类因认知限制而更喜欢控制游戏复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究战略决策中瞬时机会与长期目标的羁绊关系，通过棋牌模型对比人类和AI在维持战略张力方面的不同策略。

Method: 提出基于网络的棋子互动指标来定量棋局中的战略张力，并分析人人对局和AI对局中该指标的演变规律。

Result: 竞争力强的AI能在更长时间内维持更高水平的战略张力；人类游戏的累计张力在Elo评分1600和2300附近出现突增；AI更能对担长期的攻防平衡状态，而人类更偏向控制游戏复杂度。

Conclusion: 人工智能在维持长期战略张力方面显示出超越人类的能力，这可能体现了人类的认知限制和适应性策略，对于AI在复杂战略环境中的应用具有重要意义。

Abstract: Strategic decision-making involves managing the tension between immediate
opportunities and long-term objectives. We study this trade-off in chess by
characterizing and comparing dynamics between human vs human and AI vs AI
games. We propose a network-based metric of piece-to-piece interaction to
quantify the ongoing strategic tension on the board. Its evolution in games
reveals that the most competitive AI players sustain higher levels of strategic
tension for longer durations than elite human players. Cumulative tension
varies with algorithmic complexity for AI and correspondingly in human-played
games increases abruptly with expertise at about 1600 Elo and again at 2300
Elo. The profiles reveal different approaches. Highly competitive AI tolerates
interconnected positions balanced between offensive and defensive tactics over
long periods. Human play, in contrast, limits tension and game complexity,
which may reflect cognitive limitations and adaptive strategies. The difference
may have implications for AI usage in complex, strategic environments.

</details>


### [16] [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
*Zeyu Zhang,Yang Zhang,Haoran Tan,Rui Li,Xu Chen*

Main category: cs.AI

TL;DR: 该论文提出了多跳个性化推理任务，研究不同记忆机制在个性化信息多跳推理中的表现，构建了数据集和评估框架，并提出了HybridMem混合方法来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的个性化方法主要关注偏好对齐和简单问答，但现实世界中复杂任务需要对大量用户信息进行多跳推理，当前记忆方法面临重大挑战。

Method: 明确定义多跳个性化推理任务，构建数据集和统一评估框架，实现各种显式和隐式记忆方法，并探索混合方法提出HybridMem模型。

Result: 通过全面实验从多角度评估不同记忆方法的性能，分析其优缺点，并证明HybridMem方法的有效性。

Conclusion: 该研究为解决个性化信息的多跳推理问题提供了系统性的方法和评估框架，HybridMem方法有效结合了显式和隐式记忆的优势，推动了该领域的发展。

Abstract: In large language model-based agents, memory serves as a critical capability
for achieving personalization by storing and utilizing users' information.
Although some previous studies have adopted memory to implement user
personalization, they typically focus on preference alignment and simple
question-answering. However, in the real world, complex tasks often require
multi-hop reasoning on a large amount of user information, which poses
significant challenges for current memory approaches. To address this
limitation, we propose the multi-hop personalized reasoning task to explore how
different memory mechanisms perform in multi-hop reasoning over personalized
information. We explicitly define this task and construct a dataset along with
a unified evaluation framework. Then, we implement various explicit and
implicit memory methods and conduct comprehensive experiments. We evaluate
their performance on this task from multiple perspectives and analyze their
strengths and weaknesses. Besides, we explore hybrid approaches that combine
both paradigms and propose the HybridMem method to address their limitations.
We demonstrate the effectiveness of our proposed model through extensive
experiments. To benefit the research community, we release this project at
https://github.com/nuster1128/MPR.

</details>


### [17] ["DIVE" into Hydrogen Storage Materials Discovery with AI Agents](https://arxiv.org/abs/2508.13251)
*Di Zhang,Xue Jia,Tran Ba Hung,Seong Hoon Jang,Linda Zhang,Ryuhei Sato,Yusuke Hashimoto,Toyoto Sato,Kiyoe Konno,Shin-ichi Orimo,Hao Li*

Main category: cs.AI

TL;DR: DIVE多智能体工作流通过系统读取和组织科学文献中的图形数据，显著提升材料数据提取准确性，建立了包含3万条数据的氢存储材料数据库，实现快速逆向设计


<details>
  <summary>Details</summary>
Motivation: 科学文献中大量材料数据被困在非结构化的图表中，阻碍了基于大语言模型的AI智能体进行自动化材料设计

Method: 提出DIVE多智能体工作流，系统读取科学文献中的图形元素数据，专注于固态氢存储材料的数据提取和组织

Result: 相比多模态模型直接提取，DIVE精度提升10-15%（商业模型）和30%以上（开源模型），建立了包含4,000篇文献30,000条数据的数据库，能在2分钟内识别未报道的氢存储成分

Conclusion: DIVE工作流和智能体设计可广泛应用于不同材料领域，为AI驱动的材料发现提供了新范式

Abstract: Data-driven artificial intelligence (AI) approaches are fundamentally
transforming the discovery of new materials. Despite the unprecedented
availability of materials data in the scientific literature, much of this
information remains trapped in unstructured figures and tables, hindering the
construction of large language model (LLM)-based AI agent for automated
materials design. Here, we present the Descriptive Interpretation of Visual
Expression (DIVE) multi-agent workflow, which systematically reads and
organizes experimental data from graphical elements in scientific literatures.
We focus on solid-state hydrogen storage materials-a class of materials central
to future clean-energy technologies and demonstrate that DIVE markedly improves
the accuracy and coverage of data extraction compared to the direct extraction
by multimodal models, with gains of 10-15% over commercial models and over 30%
relative to open-source models. Building on a curated database of over 30,000
entries from 4,000 publications, we establish a rapid inverse design workflow
capable of identifying previously unreported hydrogen storage compositions in
two minutes. The proposed AI workflow and agent design are broadly transferable
across diverse materials, providing a paradigm for AI-driven materials
discovery.

</details>


### [18] [CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support](https://arxiv.org/abs/2508.13256)
*Yuting Zhang,Karina V. Bunting,Asgher Champsi,Xiaoxia Wang,Wenqi Lu,Alexander Thorley,Sandeep S Hothi,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: CardAIc-Agents是一个多模态AI框架，通过外部工具增强和自适应推理来解决心血管疾病诊断中的临床挑战，在多个数据集上表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，但医疗工作者严重短缺。现有AI系统存在临床角色分配依赖提示工程、工作流程僵化、知识库静态、输入输出模式固定等问题，限制了临床应用。

Method: 提出多模态框架CardAIc-Agents：1)CardiacRAG代理从可更新知识库生成通用计划；2)主代理集成工具自主执行计划；3)逐步更新策略动态优化复杂任务计划；4)多学科讨论工具处理疑难病例；5)视觉审查面板辅助最终验证。

Result: 在三个数据集上的实验表明，CardAIc-Agents相比主流视觉语言模型、最先进的代理系统和微调视觉语言模型都表现出更高的效率。

Conclusion: CardAIc-Agents通过工具增强和自适应推理有效解决了心血管AI诊断中的关键挑战，为临床AI应用提供了可行的解决方案。

Abstract: Cardiovascular diseases (CVDs) remain the foremost cause of mortality
worldwide, a burden worsened by a severe deficit of healthcare workers.
Artificial intelligence (AI) agents have shown potential to alleviate this gap
via automated early detection and proactive screening, yet their clinical
application remains limited by: 1) prompt-based clinical role assignment that
relies on intrinsic model capabilities without domain-specific tool support; or
2) rigid sequential workflows, whereas clinical care often requires adaptive
reasoning that orders specific tests and, based on their results, guides
personalised next steps; 3) general and static knowledge bases without
continuous learning capability; and 4) fixed unimodal or bimodal inputs and
lack of on-demand visual outputs when further clarification is needed. In
response, a multimodal framework, CardAIc-Agents, was proposed to augment
models with external tools and adaptively support diverse cardiac tasks.
Specifically, a CardiacRAG agent generated general plans from updatable cardiac
knowledge, while the chief agent integrated tools to autonomously execute these
plans and deliver decisions. To enable adaptive and case-specific
customization, a stepwise update strategy was proposed to dynamically refine
plans based on preceding execution results, once the task was assessed as
complex. In addition, a multidisciplinary discussion tool was introduced to
interpret challenging cases, thereby supporting further adaptation. When
clinicians raised concerns, visual review panels were provided to assist final
validation. Experiments across three datasets showed the efficiency of
CardAIc-Agents compared to mainstream Vision-Language Models (VLMs),
state-of-the-art agentic systems, and fine-tuned VLMs.

</details>


### [19] [Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention](https://arxiv.org/abs/2508.13327)
*Sarthak Khanna,Armin Berger,David Berghaus,Tobias Deusser,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.AI

TL;DR: STONK是一个多模态股票预测框架，结合数值市场指标和情感增强的新闻嵌入，通过特征拼接和跨模态注意力机制提升股票涨跌预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统股票预测方法往往孤立分析数值或文本信息，无法充分利用多源数据的互补优势，需要开发统一的多模态框架来提升预测准确性。

Method: 采用特征拼接和跨模态注意力机制，将数值市场指标与情感增强的新闻嵌入进行融合，构建统一的多模态预测管道。

Result: 回测结果显示STONK在股票涨跌预测上优于仅使用数值指标的基线方法，证明了多模态融合的有效性。

Conclusion: 该研究为可扩展的多模态金融预测提供了实证指导，证明了结合数值和文本信息的价值，源代码已在GitHub开源。

Abstract: We propose STONK (Stock Optimization using News Knowledge), a multimodal
framework integrating numerical market indicators with sentiment-enriched news
embeddings to improve daily stock-movement prediction. By combining numerical &
textual embeddings via feature concatenation and cross-modal attention, our
unified pipeline addresses limitations of isolated analyses. Backtesting shows
STONK outperforms numeric-only baselines. A comprehensive evaluation of fusion
strategies and model configurations offers evidence-based guidance for scalable
multimodal financial forecasting. Source code is available on GitHub

</details>


### [20] [HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design](https://arxiv.org/abs/2508.13333)
*Chentong Chen,Mengyuan Zhong,Jianyong Sun,Ye Fan,Jialong Shi*

Main category: cs.AI

TL;DR: HiFo-Prompt框架通过前瞻性和后顾性提示策略，解决了LLM自动启发式设计中静态算子和知识积累不足的问题，显著提升了启发式生成质量和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自动启发式设计在进化计算中存在静态算子使用和缺乏知识积累机制的问题，限制了其有效性。

Method: 提出HiFo-Prompt框架，包含两种协同提示策略：前瞻性提示基于种群动态自适应引导搜索，管理探索-利用权衡；后顾性提示从过往成功启发式中提炼可重用设计原则，构建持久知识库。

Result: 实证结果显示HiFo-Prompt显著优于现有最先进的LLM-based AHD方法，生成更高质量的启发式，同时实现更快的收敛速度和更优的查询效率。

Conclusion: HiFo-Prompt的双重机制成功将瞬时发现转化为持久知识，使LLM能够从自身经验中学习，为自动启发式设计提供了有效的知识积累解决方案。

Abstract: LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation
(EC) frameworks has shown promising results. However, its effectiveness is
hindered by the use of static operators and the lack of knowledge accumulation
mechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two
synergistic prompting strategies: Foresight and Hindsight. Foresight-based
prompts adaptively steer the search based on population dynamics, managing the
exploration-exploitation trade-off. In addition, hindsight-based prompts mimic
human expertise by distilling successful heuristics from past generations into
fundamental, reusable design principles. This dual mechanism transforms
transient discoveries into a persistent knowledge base, enabling the LLM to
learn from its own experience. Empirical results demonstrate that HiFo-Prompt
significantly outperforms state-of-the-art LLM-based AHD methods, generating
higher-quality heuristics while achieving substantially faster convergence and
superior query efficiency.

</details>


### [21] [LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems](https://arxiv.org/abs/2508.13371)
*Ronit Virwani,Ruchika Suryawanshi*

Main category: cs.AI

TL;DR: LOOP是一个新颖的神经符号规划框架，通过神经和符号组件之间的迭代对话来解决复杂规划问题，在标准基准测试中达到85.8%的成功率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前神经规划方法在复杂领域存在缺失前提条件、目标不一致和幻觉问题，而经典规划器缺乏灵活性和自然语言理解能力。现有神经符号方法采用一次性翻译方式，无法实现神经和符号组件的协同优化。

Method: LOOP将规划视为神经和符号组件之间的迭代对话，集成了13个协调的神经特征，包括图神经网络、多智能体验证、分层分解和因果记忆，能够生成PDDL规范并根据符号反馈进行迭代优化。

Result: 在六个标准IPC基准域上评估，LOOP达到85.8%的成功率，显著优于LLM+P（55.0%）、LLM-as-Planner（19.2%）和Tree-of-Thoughts（3.3%）。

Conclusion: 可靠规划的关键不在于选择神经网络还是符号推理器，而在于让它们在过程中真正"对话"。LOOP为构建可信的关键现实应用自主系统提供了详细蓝图。

Abstract: Planning is one of the most critical tasks in autonomous systems, where even
a small error can lead to major failures or million-dollar losses. Current
state-of-the-art neural planning approaches struggle with complex domains,
producing plans with missing preconditions, inconsistent goals, and
hallucinations. While classical planners provide logical guarantees, they lack
the flexibility and natural language understanding capabilities needed for
modern autonomous systems. Existing neuro-symbolic approaches use one-shot
translation from natural language to formal plans, missing the opportunity for
neural and symbolic components to work and refine solutions together. To
address this gap, we develop LOOP -- a novel neuro-symbolic planning framework
that treats planning as an iterative conversation between neural and symbolic
components rather than simple translation. LOOP integrates 13 coordinated
neural features including graph neural networks for spatial relationships,
multi-agent validation for consensus-based correctness, hierarchical
decomposition for complex task management, and causal memory that learns from
both successes and failures. Unlike existing approaches, LOOP generates PDDL
specifications, refines them iteratively based on symbolic feedback, and builds
a causal knowledge base from execution traces. LOOP was evaluated on six
standard IPC benchmark domains, where it achieved 85.8% success rate compared
to LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This
work shows that the key to reliable planning is not in choosing between neural
networks or symbolic reasoners but it lies in making them actually ``talk'' to
each other during the entire process. LOOP provides a thorough blueprint for
building autonomous systems that can finally be trusted with critical
real-world applications.

</details>


### [22] [SPANER: Shared Prompt Aligner for Multimodal Semantic Representation](https://arxiv.org/abs/2508.13387)
*Thye Shan Ng,Caren Soyeon Han,Eun-Jung Holden*

Main category: cs.AI

TL;DR: SPANER是一个模态无关的参数高效微调框架，通过共享提示机制将不同模态输入嵌入到统一语义空间中，提升跨模态泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态PEFT方法主要关注任务特定性能提升，但忽略了多模态嵌入空间的结构，导致模态特定表示孤立，限制了跨模态泛化能力。

Method: 提出SPANER框架，采用共享提示机制作为概念锚点，使语义相关的实例在空间中汇聚，支持无缝集成音频等额外模态而无需改变核心架构。

Result: 在视觉-语言和音频-视觉基准测试中，SPANER展示了竞争力的少样本检索性能，同时在学习到的嵌入空间中保持了高语义一致性。

Conclusion: 研究强调了对齐嵌入结构（而非仅仅调整适配器权重）对于可扩展多模态学习的重要性。

Abstract: Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have
significantly improved performance on downstream tasks such as few-shot
retrieval. However, most existing approaches focus on task-specific gains while
neglecting the structure of the multimodal embedding space. As a result,
modality-specific representations often remain isolated, limiting cross-modal
generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a
modality-agnostic PEFT framework designed to embed inputs from diverse
modalities into a unified semantic space. At its core, SPANER employs a shared
prompt mechanism that acts as a conceptual anchor, enabling semantically
related instances to converge spatially regardless of modality. This shared
prompt design is inherently extensible, supporting the seamless integration of
additional modalities, such as audio, without altering the core architecture.
Through comprehensive experiments across vision-language and audio-visual
benchmarks, SPANER demonstrates competitive few-shot retrieval performance
while preserving high semantic coherence in the learned embedding space. Our
results highlight the importance of aligning embedding structures, rather than
merely tuning adapter weights, for scalable multimodal learning.

</details>


### [23] [TASER: Table Agents for Schema-guided Extraction and Recommendation](https://arxiv.org/abs/2508.13404)
*Nicole Cho,Kirsty Fielding,William Watson,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: TASER是一个持续学习的智能表格提取系统，专门处理金融文档中高度非结构化的多页异构表格，通过模式引导的提取和推荐机制，在表格检测性能上超越现有模型10.1%，并能显著提升提取效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界金融文档中的财务持仓信息通常隐藏在杂乱、多页、碎片化的表格中（99.4%的表格没有边界框，最多426行跨44页），传统方法难以有效处理这种高度非结构化的表格数据。

Method: 开发了TASER系统，包含表格代理执行表格检测、分类、提取和推荐，利用初始模式进行工作，然后由推荐代理审查输出、推荐模式修订并决定最终建议，实现持续学习过程。

Result: TASER在表格检测上比Table Transformer等现有模型提升10.1%；更大的批次大小使可操作的模式推荐增加104.3%，提取持仓增加9.8%；构建了包含22,584页、3,213个表格、7310亿美元持仓的真实金融表格数据集TASERTab。

Conclusion: 基于代理的模式引导提取系统在理解现实世界金融表格方面展现出巨大潜力，持续学习过程对于提升提取效果至关重要，发布的TASERTab数据集将为研究社区提供宝贵的真实金融表格资源。

Abstract: Real-world financial documents report essential information about an entity's
financial holdings that can span millions of different financial instrument
types. Yet, these details are often buried in messy, multi-page, fragmented
tables - for example, 99.4% of the tables in our dataset have no bounding boxes
with the maximum number of rows amounting to 426 per table across 44 pages. To
tackle these unique challenges from real-world tables, we present a
continuously learning, agentic table extraction system, TASER (Table Agents for
Schema-guided Extraction and Recommendation) that extracts highly unstructured,
multi-page, heterogeneous tables into normalized, schema-conforming outputs.
Our table agents execute on table detection, classification, extraction, and
recommendations by leveraging an initial schema. Then, our Recommender Agent
reviews the outputs, recommends schema revisions, and decides on the final
recommendations, enabling TASER to outperform existing table detection models
such as Table Transformer by 10.1%. Within this continuous learning process, we
highlight that larger batch sizes result in a 104.3% increase in schema
recommendations that are actionable and utilized, resulting in a 9.8% increase
in extracted holdings - highlighting the importance of a continuous learning
process. To train TASER, we have manually labeled 22,584 pages (28,150,449
tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of
the first real financial table datasets. We release our dataset TASERTab to
enable the research community to access real-world financial tables and
outputs. Our results highlight the promise of agentic, schema-guided extraction
systems for robust understanding of real-world financial tables.

</details>


### [24] [Virtuous Machines: Towards Artificial General Science](https://arxiv.org/abs/2508.13421)
*Gabrielle Wehr,Reuben Rideaux,Amaya J. Fox,David R. Lightfoot,Jason Tangen,Jason B. Mattingley,Shane E. Ehrhardt*

Main category: cs.AI

TL;DR: 一个领域无关的人工智能系统能够自主完成科学研究的全过程，包括假设生成、数据收集和稿件准备，并成功进行了三个心理学实验


<details>
  <summary>Details</summary>
Motivation: 科学文献指数增长和领域专门化限制了研究者跨领域知识综合能力，需要更通用的AI系统来加速科学发现

Method: 基于领域无关的代理AI系统，自主设计和执行了三个心理学实验（视觉工作记忆、心理旋转、想象生动性），进行在线数据收集并开发分析流程

Result: 系统成功收集了288名参与者的数据，通过8小时以上的连续编码开发了分析流程，生成了完整的稿件，表现出与经验丰富研究者相当的理论推理和方法论严谨性

Conclusion: 这是向能够通过实际实验测试假设的体现式AI进取的一步，可以自主探索人类认知和资源限制无法涉足的科学领域，同时也提出了关于科学理解本质和科学豪動归属的重要问题

Abstract: Artificial intelligence systems are transforming scientific discovery by
accelerating specific research tasks, from protein structure prediction to
materials design, yet remain confined to narrow domains requiring substantial
human oversight. The exponential growth of scientific literature and increasing
domain specialisation constrain researchers' capacity to synthesise knowledge
across disciplines and develop unifying theories, motivating exploration of
more general-purpose AI systems for science. Here we show that a
domain-agnostic, agentic AI system can independently navigate the scientific
workflow - from hypothesis generation through data collection to manuscript
preparation. The system autonomously designed and executed three psychological
studies on visual working memory, mental rotation, and imagery vividness,
executed one new online data collection with 288 participants, developed
analysis pipelines through 8-hour+ continuous coding sessions, and produced
completed manuscripts. The results demonstrate the capability of AI scientific
discovery pipelines to conduct non-trivial research with theoretical reasoning
and methodological rigour comparable to experienced researchers, though with
limitations in conceptual nuance and theoretical interpretation. This is a step
toward embodied AI that can test hypotheses through real-world experiments,
accelerating discovery by autonomously exploring regions of scientific space
that human cognitive and resource constraints might otherwise leave unexplored.
It raises important questions about the nature of scientific understanding and
the attribution of scientific credit.

</details>


### [25] [STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting](https://arxiv.org/abs/2508.13433)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.AI

TL;DR: STPFormer是一个时空模式感知Transformer模型，通过统一的表示学习在交通预测任务中实现了最先进性能，包含四个核心模块来处理复杂的时空模式。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型在交通预测中存在时间编码僵化和时空融合能力弱的问题，无法有效处理复杂的时空模式和多样化的输入格式。

Method: 提出STPFormer模型，包含四个模块：Temporal Position Aggregator（模式感知时间编码）、Spatial Sequence Aggregator（序列空间学习）、Spatial-Temporal Graph Matching（跨域对齐）和Attention Mixer（多尺度融合）。

Result: 在五个真实世界数据集上的实验表明，STPFormer始终达到新的SOTA结果，消融实验和可视化验证了其有效性和泛化能力。

Conclusion: STPFormer通过统一的表示学习和可解释的架构，成功解决了交通预测中的复杂时空模式建模问题，为时空预测任务提供了有效的解决方案。

Abstract: Spatio-temporal traffic forecasting is challenging due to complex temporal
patterns, dynamic spatial structures, and diverse input formats. Although
Transformer-based models offer strong global modeling, they often struggle with
rigid temporal encoding and weak space-time fusion. We propose STPFormer, a
Spatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art
performance via unified and interpretable representation learning. It
integrates four modules: Temporal Position Aggregator (TPA) for pattern-aware
temporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial
learning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,
and an Attention Mixer for multi-scale fusion. Experiments on five real-world
datasets show that STPFormer consistently sets new SOTA results, with ablation
and visualizations confirming its effectiveness and generalizability.

</details>


### [26] [Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences](https://arxiv.org/abs/2508.13437)
*Cheikh Ahmed,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: 提出了离散最小最大违规（DMMV）作为通用优化问题，开发了GPU加速启发式算法，在语言模型量化、离散层析成像和FIR滤波器设计三个应用场景中取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 许多应用场景都有最坏情况性能要求，需要一种通用的数学框架来处理这类离散优化问题，最小化最大约束违规

Method: 定义了DMMV问题的数学形式，研究了其性质，开发了利用数学特性的GPU加速启发式算法

Result: 在量化任务中平均提升14%，离散层析成像中误差减少16%且GPU加速6倍，FIR滤波器设计中波纹减少近50%

Conclusion: DMMV作为上下文无关优化问题具有广泛适用性，提出的GPU加速启发式算法在多个不同问题上都表现出优势，将开源促进进一步研究

Abstract: We introduce the Discrete Min-Max Violation (DMMV) as a general optimization
problem which seeks an assignment of discrete values to variables that
minimizes the largest constraint violation. This context-free mathematical
formulation is applicable to a wide range of use cases that have worst-case
performance requirements. After defining the DMMV problem mathematically, we
explore its properties to establish a foundational understanding. To tackle
DMMV instance sizes of practical relevance, we develop a GPU-accelerated
heuristic that takes advantage of the mathematical properties of DMMV for
speeding up the solution process. We demonstrate the versatile applicability of
our heuristic by solving three optimization problems as use cases: (1)
post-training quantization of language models, (2) discrete tomography, and (3)
Finite Impulse Response (FIR) filter design. In quantization without outlier
separation, our heuristic achieves 14% improvement on average over existing
methods. In discrete tomography, it reduces reconstruction error by 16% under
uniform noise and accelerates computations by a factor of 6 on GPU. For FIR
filter design, it nearly achieves 50% ripple reduction compared to using the
commercial integer optimization solver, Gurobi. Our comparative results point
to the benefits of studying DMMV as a context-free optimization problem and the
advantages that our proposed heuristic offers on three distinct problems. Our
GPU-accelerated heuristic will be made open-source to further stimulate
research on DMMV and its other applications. The code is available at
https://anonymous.4open.science/r/AMVM-5F3E/

</details>


### [27] [LM Agents May Fail to Act on Their Own Risk Knowledge](https://arxiv.org/abs/2508.13465)
*Yuzhi Tang,Tianxiao Li,Elizabeth Li,Chris J. Maddison,Honghua Dong,Yangjun Ruan*

Main category: cs.AI

TL;DR: 语言模型代理存在风险识别与安全执行的显著差距，研究构建了风险验证器系统来减少风险行为


<details>
  <summary>Details</summary>
Motivation: 识别语言模型代理在安全关键场景中的风险，发现代理虽然具有风险知识但无法有效执行安全行为

Method: 开发了一个综合评估框架，从风险知识、风险识别和安全执行三个维度评估代理安全性，并构建了风险验证器系统

Result: 发现代理风险知识率>98%，但风险识别性能下降23%，风险行为执行率<26%，风险验证器系统可减少55.3%的风险行为

Conclusion: 简单扩大模型能力无法解决安全问题，需要专门的风险验证机制来提升代理安全性

Abstract: Language model (LM) agents have demonstrated significant potential for
automating real-world tasks, yet they pose a diverse array of potential, severe
risks in safety-critical scenarios. In this work, we identify a significant gap
between LM agents' risk awareness and safety execution abilities: while they
often answer "Yes" to queries like "Is executing `sudo rm -rf /*' dangerous?",
they will likely fail to identify such risks in instantiated trajectories or
even directly perform these risky actions when acting as agents. To
systematically investigate this, we develop a comprehensive evaluation
framework to examine agents' safety across three progressive dimensions: 1)
their knowledge about potential risks, 2) their ability to identify
corresponding risks in execution trajectories, and 3) their actual behaviors to
avoid executing these risky actions. Our evaluation reveals two critical
performance gaps that resemble the generator-validator gaps observed in LMs:
while agents demonstrate near-perfect risk knowledge ($>98\%$ pass rates), they
fail to apply this knowledge when identifying risks in actual scenarios (with
performance dropping by $>23\%$) and often still execute risky actions ($<26\%$
pass rates). Notably, this trend persists across more capable LMs as well as in
specialized reasoning models like DeepSeek-R1, indicating that simply scaling
model capabilities or inference compute does not inherently resolve safety
concerns. Instead, we take advantage of these observed gaps to develop a risk
verifier that independently critiques the proposed actions by agents, with an
abstractor that converts specific execution trajectories into abstract
descriptions where LMs can more effectively identify the risks. Our overall
system achieves a significant reduction of risky action execution by $55.3\%$
over vanilla-prompted agents.

</details>


### [28] [CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter](https://arxiv.org/abs/2508.13530)
*Junyeong Park,Hyeonseo Cho,Sungjin Ahn*

Main category: cs.AI

TL;DR: CrafterDojo是一个轻量级的Minecraft-like测试平台，包含基础模型和工具套件，用于通用具身智能体研究的快速原型开发


<details>
  <summary>Details</summary>
Motivation: Minecraft环境复杂但速度慢、工程开销大，Crafter虽然轻量但缺乏基础模型支持，限制了其在通用具身智能体研究中的应用

Method: 开发了CrafterVPT（行为先验）、CrafterCLIP（视觉语言基础）、CrafterSteve-1（指令跟随）三个基础模型，以及CrafterPlay（行为数据集生成）、CrafterCaption（标注数据集生成）等工具套件

Result: 提供了完整的开源代码库、参考智能体实现和基准评估，将Crafter环境转化为适合快速原型开发的测试平台

Conclusion: CrafterDojo成功地将Crafter环境打造成为一个轻量级、原型友好且类似Minecraft的测试平台，为通用具身智能体研究提供了重要工具支持

Abstract: Developing general-purpose embodied agents is a core challenge in AI.
Minecraft provides rich complexity and internet-scale data, but its slow speed
and engineering overhead make it unsuitable for rapid prototyping. Crafter
offers a lightweight alternative that retains key challenges from Minecraft,
yet its use has remained limited to narrow tasks due to the absence of
foundation models that have driven progress in the Minecraft setting. In this
paper, we present CrafterDojo, a suite of foundation models and tools that
unlock the Crafter environment as a lightweight, prototyping-friendly, and
Minecraft-like testbed for general-purpose embodied agent research. CrafterDojo
addresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for
behavior priors, vision-language grounding, and instruction following,
respectively. In addition, we provide toolkits for generating behavior and
caption datasets (CrafterPlay and CrafterCaption), reference agent
implementations, benchmark evaluations, and a complete open-source codebase.

</details>


### [29] [Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance](https://arxiv.org/abs/2508.13579)
*Yue Fang,Yuxin Guo,Jiaran Gao,Hongxin Ding,Xinke Jiang,Weibin Liao,Yongxin Xu,Yinghao Zhu,Zhibang Yang,Liantao Ma,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: EAG-RL是一个两阶段训练框架，通过专家注意力引导来提升大语言模型在电子病历推理方面的能力，平均提升14.62%的性能，并增强鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将大语言模型作为固定的先验检索器，而下游深度学习模型处理预测，这无法提升大语言模型的内在推理能力，且继承了深度学习模型的泛化局限性。

Method: 提出EAG-RL两阶段训练框架：1）使用专家引导的蒙特卡洛树搜索构建高质量的分步推理轨迹来初始化策略；2）通过强化学习优化策略，使大语言模型的注意力与专家电子病历模型识别的临床显著特征对齐。

Result: 在两个真实电子病历数据集上的实验显示，EAG-RL平均提升大语言模型内在电子病历推理能力14.62%，同时增强了对特征扰动的鲁棒性和对未见临床领域的泛化能力。

Conclusion: EAG-RL展示了在临床预测任务中实际部署的潜力，能够有效提升大语言模型处理电子病历数据的能力。

Abstract: Improving large language models (LLMs) for electronic health record (EHR)
reasoning is essential for enabling accurate and generalizable clinical
predictions. While LLMs excel at medical text understanding, they underperform
on EHR-based prediction tasks due to challenges in modeling temporally
structured, high-dimensional data. Existing approaches often rely on hybrid
paradigms, where LLMs serve merely as frozen prior retrievers while downstream
deep learning (DL) models handle prediction, failing to improve the LLM's
intrinsic reasoning capacity and inheriting the generalization limitations of
DL models. To this end, we propose EAG-RL, a novel two-stage training framework
designed to intrinsically enhance LLMs' EHR reasoning ability through expert
attention guidance, where expert EHR models refer to task-specific DL models
trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise
reasoning trajectories using expert-guided Monte Carlo Tree Search to
effectively initialize the LLM's policy. Then, EAG-RL further optimizes the
policy via reinforcement learning by aligning the LLM's attention with
clinically salient features identified by expert EHR models. Extensive
experiments on two real-world EHR datasets show that EAG-RL improves the
intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also
enhancing robustness to feature perturbations and generalization to unseen
clinical domains. These results demonstrate the practical potential of EAG-RL
for real-world deployment in clinical prediction tasks. Our code have been
available at https://github.com/devilran6/EAG-RL.

</details>


### [30] [Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation](https://arxiv.org/abs/2508.13587)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Liming Zheng,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: 本文提出了多模态结构化强化学习(MSRL)方法，通过多粒度结构化奖励系统突破图表到代码生成任务中监督微调的性能瓶颈，在真实数据集上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在视觉语言模型中表现有效，但在需要深度理解信息丰富图像和生成结构化输出的任务中应用不足。图表到代码生成任务存在监督微调性能瓶颈问题，需要有效的强化学习策略来奖励结构化输出。

Method: 构建了包含300万真实arXiv表格图表-代码对的最大训练语料库；提出MSRL方法，使用多粒度结构化奖励系统：文本级基于规则的奖励验证代码细节，视觉级基于模型的奖励通过渲染代码成图像评估结构相似性；采用两阶段课程学习确保训练稳定性。

Result: MSRL显著突破了SFT的性能瓶颈，在ChartMimic和ReachQA基准测试上分别将高级指标提升了6.2%和9.9%，达到了与先进闭源模型竞争的性能水平。

Conclusion: 多模态结构化强化学习是解决图表到代码生成任务中监督微调性能瓶颈的有效方法，通过结合文本和视觉反馈的多粒度奖励系统能够显著提升模型性能。

Abstract: While reinforcement learning (RL) has proven highly effective for general
reasoning in vision-language models, its application to tasks requiring
in-depth understanding of information-rich images and generation of structured
outputs remains underexplored. Chart-to-code generation exemplifies this
challenge, demanding complex reasoning over visual charts to generate
structured code. Supervised fine-tuning (SFT) alone is often insufficient,
highlighting the need for effective RL strategies that appropriately reward
structured outputs. We systematically investigate the performance plateau in
SFT through large-scale experiments and propose Multimodal Structured
Reinforcement Learning (MSRL) for chart-to-code generation, which substantially
breaks through this plateau. We construct the largest training corpus to date,
containing 3 million chart-code pairs from real-world arXiv tables to mitigate
simplistic patterns of prior synthetic data. Despite reaching state-of-the-art
performance, our experiments show that scaling SFT data eventually hits a
plateau where further increases yield negligible improvements. Our MSRL method
leverages a multi-granularity structured reward system using multimodal textual
and visual feedback. At the textual level, rule-based rewards validate
fine-grained code details. At the visual level, model-based rewards assess
structural similarity by rendering generated code into images and employing an
evaluator model. We implement this within a two-stage curriculum for training
stability. Results demonstrate that MSRL significantly breaks the SFT plateau,
improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA
benchmarks respectively, achieving competitive performance with advanced
closed-source models.

</details>


### [31] [V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task](https://arxiv.org/abs/2508.13634)
*Jikai Chen,Long Chen,Dong Wang,Leilei Gan,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: V2P方法通过抑制注意力机制和基于Fitts定律的高斯热图建模，解决了GUI元素定位中的背景干扰和中心-边缘区分问题，在ScreenSpot基准上达到92.3%和50.5%的精度。


<details>
  <summary>Details</summary>
Motivation: 传统GUI定位方法忽视空间交互不确定性和视觉语义层次结构，现有注意力方法存在背景干扰导致注意力漂移，以及均匀标注无法区分目标UI元素中心和边缘的问题。

Method: 提出Valley-to-Peak (V2P)方法：1) 抑制注意力机制减少对无关背景区域的关注；2) 基于Fitts定律将GUI交互建模为2D高斯热图，权重从中心向边缘递减，方差由目标大小决定。

Result: 在ScreenSpot-v2和ScreenSpot-Pro两个基准测试上分别达到92.3%和50.5%的性能，消融实验验证了各组件贡献。

Conclusion: V2P方法能有效隔离目标区域并让模型专注于UI元素最关键的点，具有精确GUI定位任务的泛化能力。

Abstract: Precise localization of GUI elements is crucial for the development of GUI
agents. Traditional methods rely on bounding box or center-point regression,
neglecting spatial interaction uncertainty and visual-semantic hierarchies.
Recent methods incorporate attention mechanisms but still face two key issues:
(1) ignoring processing background regions causes attention drift from the
desired area, and (2) uniform labeling fails to distinguish between center and
edges of the target UI element, leading to click imprecision. Inspired by how
humans visually process and interact with GUI elements, we propose the
Valley-to-Peak (V2P) method to address these issues. To mitigate background
distractions, V2P introduces a suppression attention mechanism that minimizes
the model's focus on irrelevant regions to highlight the intended region. For
the issue of center-edge distinction, V2P applies a Fitts' Law-inspired
approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight
gradually decreases from the center towards the edges. The weight distribution
follows a Gaussian function, with the variance determined by the target's size.
Consequently, V2P effectively isolates the target area and teaches the model to
concentrate on the most essential point of the UI element. The model trained by
V2P achieves the performance with 92.3% and 50.5% on two benchmarks
ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's
contribution, highlighting V2P's generalizability for precise GUI grounding
tasks.

</details>


### [32] [Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints](https://arxiv.org/abs/2508.13663)
*Daniel Daza,Alberto Bernardi,Luca Costabello,Christophe Gueret,Masoud Mansoury,Michael Cochez,Martijn Schut*

Main category: cs.AI

TL;DR: 该论文提出了在知识图谱查询中处理软约束的新方法Neural Query Reranker(NQR)，通过交互式学习用户偏好来调整查询结果评分，同时保持原有查询答案的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱查询方法主要基于一阶逻辑，无法处理现实查询中常见的模糊或上下文相关的软约束条件，如属性偏好或相关类别偏好。

Method: 提出Neural Query Reranker(NQR)模型，通过交互式学习用户提供的偏好和非偏好实体示例，在不破坏原始查询答案的基础上调整评分。

Result: 实验表明NQR能够有效捕获软约束，同时保持稳健的查询回答性能。扩展了现有QA基准测试集以包含软约束场景。

Conclusion: 该研究填补了知识图谱查询处理中软约束支持的空白，提出的NQR方法为处理现实世界中的模糊查询需求提供了有效解决方案。

Abstract: Methods for query answering over incomplete knowledge graphs retrieve
entities that are likely to be answers, which is particularly useful when such
answers cannot be reached by direct graph traversal due to missing edges.
However, existing approaches have focused on queries formalized using
first-order-logic. In practice, many real-world queries involve constraints
that are inherently vague or context-dependent, such as preferences for
attributes or related categories. Addressing this gap, we introduce the problem
of query answering with soft constraints. We propose a Neural Query Reranker
(NQR) designed to adjust query answer scores by incorporating soft constraints
without disrupting the original answers to a query. NQR operates interactively,
refining answers based on incremental examples of preferred and non-preferred
entities. We extend existing QA benchmarks by generating datasets with soft
constraints. Our experiments demonstrate that NQR can capture soft constraints
while maintaining robust query answering performance.

</details>


### [33] [ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings](https://arxiv.org/abs/2508.13672)
*Rehan Raza,Guanjin Wang,Kevin Wong,Hamid Laga,Marco Fisichella*

Main category: cs.AI

TL;DR: 提出ITL-LIME框架，通过实例迁移学习解决LIME在数据稀缺环境下的局部性和不稳定性问题，利用相关源域的真实实例提升解释保真度和稳定性。


<details>
  <summary>Details</summary>
Motivation: LIME方法在扰动和采样过程中存在随机性，特别是在训练数据有限的情况下会导致局部性和不稳定性问题，数据稀缺可能产生偏离真实数据流形的不现实样本，使代理模型无法准确近似原始模型的复杂决策边界。

Method: 提出ITL-LIME框架，引入实例迁移学习：1) 使用聚类将源域划分为具有代表性原型的簇；2) 检索与目标实例最相似的源簇中的相关真实源实例，而非生成随机扰动；3) 构建基于对比学习的编码器作为加权机制，根据实例与目标实例的接近程度分配权重；4) 使用加权的源和目标实例训练代理模型进行解释。

Result: 该方法通过利用相关源域的真实实例，避免了生成不现实的随机扰动样本，提高了解释的保真度和稳定性，特别是在数据受限的环境中。

Conclusion: ITL-LIME框架有效解决了LIME在数据稀缺环境下的局限性，通过实例迁移学习和基于对比学习的加权机制，显著提升了解释方法的可靠性和实用性。

Abstract: Explainable Artificial Intelligence (XAI) methods, such as Local
Interpretable Model-Agnostic Explanations (LIME), have advanced the
interpretability of black-box machine learning models by approximating their
behavior locally using interpretable surrogate models. However, LIME's inherent
randomness in perturbation and sampling can lead to locality and instability
issues, especially in scenarios with limited training data. In such cases, data
scarcity can result in the generation of unrealistic variations and samples
that deviate from the true data manifold. Consequently, the surrogate model may
fail to accurately approximate the complex decision boundary of the original
model. To address these challenges, we propose a novel Instance-based Transfer
Learning LIME framework (ITL-LIME) that enhances explanation fidelity and
stability in data-constrained environments. ITL-LIME introduces instance
transfer learning into the LIME framework by leveraging relevant real instances
from a related source domain to aid the explanation process in the target
domain. Specifically, we employ clustering to partition the source domain into
clusters with representative prototypes. Instead of generating random
perturbations, our method retrieves pertinent real source instances from the
source cluster whose prototype is most similar to the target instance. These
are then combined with the target instance's neighboring real instances. To
define a compact locality, we further construct a contrastive learning-based
encoder as a weighting mechanism to assign weights to the instances from the
combined set based on their proximity to the target instance. Finally, these
weighted source and target instances are used to train the surrogate model for
explanation purposes.

</details>


### [34] [Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks](https://arxiv.org/abs/2508.13675)
*Mariam Arustashvili,Jörg Deigmöller,Heiko Paulheim*

Main category: cs.AI

TL;DR: 知识图连接预测在家庭行为场景中效果差，许多算法连基线都无法超越


<details>
  <summary>Details</summary>
Motivation: 家庭行为知识图在家庭机器人控制和视频分析中有重要作用，但视频中提取的信息完整性差，需要通过知识图补全来提升情况感知

Method: 研究了标准连接预测问题，分析了各种连接预测算法在情境知识图中的表现

Result: 发现情境知识图具有特殊特性，导致许多连接预测算法无法满足需求，甚至连简单基线方法都无法超越

Conclusion: 当前的标准连接预测算法在处理情境知识图时存在显著的不足，需要研究专门的方法来处理这类特殊场景

Abstract: Knowledge Graphs are used for various purposes, including business
applications, biomedical analyses, or digital twins in industry 4.0. In this
paper, we investigate knowledge graphs describing household actions, which are
beneficial for controlling household robots and analyzing video footage. In the
latter case, the information extracted from videos is notoriously incomplete,
and completing the knowledge graph for enhancing the situational picture is
essential. In this paper, we show that, while a standard link prediction
problem, situational knowledge graphs have special characteristics that render
many link prediction algorithms not fit for the job, and unable to outperform
even simple baselines.

</details>


### [35] [MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model](https://arxiv.org/abs/2508.13676)
*Yu Li,Zulong Chen,Wenjian Xu,Hong Wen,Yipeng Yu,Man Lung Yiu,Yuyu Yin*

Main category: cs.AI

TL;DR: MHSNet是一个多层级身份验证框架，用于检测第三方网站获取的简历与公司人才库中现有简历之间的重复项，通过微调BGE-M3和使用对比学习来解决简历文本的语义复杂性、结构异质性和信息不完整性等问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决从第三方网站获取的简历往往不完整和不准确的问题，需要检测这些简历与公司人才库中现有简历的重复项，以提升简历质量并丰富人才库。

Method: 提出MHSNet框架，通过对比学习微调BGE-M3模型，利用Mixture-of-Experts (MoE)生成多层级稀疏和密集表示来计算语义相似度，并采用状态感知MoE处理不完整简历。

Result: 实验结果验证了MHSNet的有效性。

Conclusion: MHSNet能够有效解决简历重复检测中的挑战，提升第三方简历质量并丰富公司人才库。

Abstract: To maintain the company's talent pool, recruiters need to continuously search
for resumes from third-party websites (e.g., LinkedIn, Indeed). However,
fetched resumes are often incomplete and inaccurate. To improve the quality of
third-party resumes and enrich the company's talent pool, it is essential to
conduct duplication detection between the fetched resumes and those already in
the company's talent pool. Such duplication detection is challenging due to the
semantic complexity, structural heterogeneity, and information incompleteness
of resume texts. To this end, we propose MHSNet, an multi-level identity
verification framework that fine-tunes BGE-M3 using contrastive learning. With
the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and
dense representations for resumes, enabling the computation of corresponding
multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts
(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental
results verify the effectiveness of MHSNet

</details>


### [36] [Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models](https://arxiv.org/abs/2508.13678)
*Xiao-Wen Yang,Jie-Jing Shao,Lan-Zhe Guo,Bo-Wen Zhang,Zhi Zhou,Lin-Han Jia,Wang-Zhou Dai,Yu-Feng Li*

Main category: cs.AI

TL;DR: 这篇论文全面回顾了神经符号方法在增强大语言模型推理能力方面的最新进展，从三个视角（符号→LLM、LLM→符号、LLM+符号）进行分类讨论，并指出了关键挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种任务上表现出色，但其推理能力仍是根本性挑战。开发具有强大推理能力的AI系统被认为是实现通用人工智能的关键里程碑，受到学术界和工业界的广泛关注。

Method: 论文首先形式化推理任务并简要介绍神经符号学习范式，然后从三个角度讨论增强LLM推理能力的神经符号方法：符号→LLM、LLM→符号、以及LLM+符号的协同方法。

Result: 论文提供了神经符号方法增强LLM推理能力的系统性综述，涵盖了不同技术路径的分类和分析，并发布了包含相关论文和资源的GitHub仓库。

Conclusion: 神经符号方法是增强LLM推理能力的有前景方向，论文识别了该领域的关键挑战并提出了未来研究方向，为后续研究提供了重要参考。

Abstract: Large Language Models (LLMs) have shown promising results across various
tasks, yet their reasoning capabilities remain a fundamental challenge.
Developing AI systems with strong reasoning capabilities is regarded as a
crucial milestone in the pursuit of Artificial General Intelligence (AGI) and
has garnered considerable attention from both academia and industry. Various
techniques have been explored to enhance the reasoning capabilities of LLMs,
with neuro-symbolic approaches being a particularly promising way. This paper
comprehensively reviews recent developments in neuro-symbolic approaches for
enhancing LLM reasoning. We first present a formalization of reasoning tasks
and give a brief introduction to the neurosymbolic learning paradigm. Then, we
discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs
from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.
Finally, we discuss several key challenges and promising future directions. We
have also released a GitHub repository including papers and resources related
to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.

</details>


### [37] [The DeepLog Neurosymbolic Machine](https://arxiv.org/abs/2508.13697)
*Vincent Derkinderen,Robin Manhaeve,Rik Adriaensen,Lucas Van Praet,Lennert De Smet,Giuseppe Marra,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepLog是一个神经符号AI的理论和操作框架，提供构建块和原语来抽象表示和计算机制，支持多种神经符号系统的表示和仿真。


<details>
  <summary>Details</summary>
Motivation: 为了解决神经符号AI中不同表示和计算机制的复杂性，提供一个统一的抽象框架来简化神经符号模型的构建和推理任务。

Method: DeepLog包含两个关键组件：1）基于注释神经扩展的接地一阶逻辑语言，用于指定神经符号模型和推理任务；2）使用扩展代数电路作为计算图的计算层组件。

Result: DeepLog能够表示和仿真广泛的神经符号系统，通过实验比较证明了其在模糊逻辑、概率逻辑以及GPU加速实现方面的通用性和效率。

Conclusion: DeepLog作为一个神经符号抽象机器，提供了中间抽象层和计算层的统一框架，能够通过选择不同的代数结构和逻辑轻松获得不同的神经符号模型。

Abstract: We contribute a theoretical and operational framework for neurosymbolic AI
called DeepLog. DeepLog introduces building blocks and primitives for
neurosymbolic AI that make abstraction of commonly used representations and
computational mechanisms used in neurosymbolic AI. DeepLog can represent and
emulate a wide range of neurosymbolic systems. It consists of two key
components. The first is the DeepLog language for specifying neurosymbolic
models and inference tasks. This language consists of an annotated neural
extension of grounded first-order logic, and makes abstraction of the type of
logic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the
architecture or in the loss function. The second DeepLog component is situated
at the computational level and uses extended algebraic circuits as
computational graphs. Together these two components are to be considered as a
neurosymbolic abstract machine, with the DeepLog language as the intermediate
level of abstraction and the circuits level as the computational one. DeepLog
is implemented in software, relies on the latest insights in implementing
algebraic circuits on GPUs, and is declarative in that it is easy to obtain
different neurosymbolic models by making different choices for the underlying
algebraic structures and logics. The generality and efficiency of the DeepLog
neurosymbolic machine is demonstrated through an experimental comparison
between 1) different fuzzy and probabilistic logics, 2) between using logic in
the architecture or in the loss function, and 3) between a standalone CPU-based
implementation of a neurosymbolic AI system and a DeepLog GPU-based one.

</details>


### [38] [CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning](https://arxiv.org/abs/2508.13721)
*Minh Hoang Nguyen,Van Dai Do,Dung Nguyen,Thin Nguyen,Hung Le*

Main category: cs.AI

TL;DR: CausalPlan是一个两阶段框架，通过将显式结构因果推理集成到LLM规划过程中，解决了小规模开源LLM代理在协作任务中产生因果无效或不连贯动作的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理（尤其是较小的开源模型）在协作任务中往往产生因果无效或不连贯的动作，因为它们依赖表面相关性而非基于因果推理，这限制了它们在动态环境中的协调和规划性能。

Method: 提出了CausalPlan框架，核心是结构因果行动（SCA）模型，从代理轨迹中学习因果图来捕捉先前行动和当前环境状态如何影响未来决策。该结构用于通过为LLM生成的动作提案分配因果分数来指导动作选择，必要时回退到因果基础的替代方案。

Result: 在Overcooked-AI基准测试中，使用四种不同规模的LLM（Gemma-7B、Llama-8B、Qwen-14B和Llama-70B）进行五个多智能体协调任务的评估。实验结果显示CausalPlan持续减少无效动作，在AI-AI和人类-AI设置中都改善了协作性能，优于强化学习基线。

Conclusion: 研究结果强调了因果驱动规划对于部署高效、可解释和可泛化的多智能体LLM系统的价值，能够在不需要微调LLM本身的情况下约束规划为干预一致的行为。

Abstract: Large language model (LLM) agents-especially smaller, open-source
models-often produce causally invalid or incoherent actions in collaborative
tasks due to their reliance on surface-level correlations rather than grounded
causal reasoning. This limitation undermines their performance in terms of
coordination and planning in dynamic environments. We address this challenge
with CausalPlan, a two-phase framework that integrates explicit structural
causal reasoning into the LLM planning process. At the core of CausalPlan is
the Structural Causal Action (SCA) model, which learns a causal graph from
agent trajectories to capture how prior actions and current environment states
influence future decisions. This structure is then used to guide action
selection by assigning causal scores to LLM-generated proposals, reweighting
them accordingly, or falling back to causally grounded alternatives when
needed. By embedding this causal knowledge directly into the decision loop,
CausalPlan constrains planning to intervention-consistent behaviours without
requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the
Overcooked-AI benchmark across five multi-agent coordination tasks and four
LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.
Experimental results show that CausalPlan consistently reduces invalid actions
and improves collaboration in both AI-AI and human-AI settings, outperforming
strong reinforcement learning baselines. Our findings highlight the value of
causality-driven planning for deploying efficient, interpretable, and
generalisable multi-agent LLM systems.

</details>


### [39] [Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making](https://arxiv.org/abs/2508.13754)
*Liuxin Bao,Zhihao Peng,Xiaofei Zhou,Runmin Cong,Jiyong Zhang,Yixuan Yuan*

Main category: cs.AI

TL;DR: 通过专业知识表动态选择多个LLM专家，结合自信度评估和对抗验证来提升医疗决策的准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 单一LLM方法受限于参数知识约束和静态训练语料，无法健壁整合复杂的临床信息，需要多LLM协作来提升医疗决策质量

Method: 专业知识感知的多LLM招募与协作框架(EMRC)：第一阶段通过公开语料库构建LLM专业知识表动态选择最佳LLM；第二阶段通过自信度评分和对抗验证整合多个专家的输出

Result: 在三个公开MDM数据集上识别性能超过现有最佳方法，在MMLU-Pro-Health数据集上达到74.45%准确率，比GPT-4-0613提升2.69%

Conclusion: EMRC框架通过动态招募多个LLM专家并利用其专业能力的互补性，显著提升了医疗决策系统的准确性和可靠性

Abstract: Medical Decision-Making (MDM) is a complex process requiring substantial
domain-specific expertise to effectively synthesize heterogeneous and
complicated clinical information. While recent advancements in Large Language
Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited
by their parametric knowledge constraints and static training corpora, failing
to robustly integrate the clinical information. To address this challenge, we
propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)
framework to enhance the accuracy and reliability of MDM systems. It operates
in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and
adversarial-driven multi-agent collaboration. Specifically, in the first stage,
we use a publicly available corpus to construct an LLM expertise table for
capturing expertise-specific strengths of multiple LLMs across medical
department categories and query difficulty levels. This table enables the
subsequent dynamic selection of the optimal LLMs to act as medical expert
agents for each medical query during the inference phase. In the second stage,
we employ selected agents to generate responses with self-assessed confidence
scores, which are then integrated through the confidence fusion and adversarial
validation to improve diagnostic reliability. We evaluate our EMRC framework on
three public MDM datasets, where the results demonstrate that our EMRC
outperforms state-of-the-art single- and multi-LLM methods, achieving superior
diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC
achieves 74.45% accuracy, representing a 2.69% improvement over the
best-performing closed-source model GPT- 4-0613, which demonstrates the
effectiveness of our expertise-aware agent recruitment strategy and the agent
complementarity in leveraging each LLM's specialized capabilities.

</details>


### [40] [Quantifier Instantiations: To Mimic or To Revolt?](https://arxiv.org/abs/2508.13811)
*Jan Jakubův,Mikoláš Janota*

Main category: cs.AI

TL;DR: 提出了一种基于概率上下文无关文法的动态量化实例化方法，通过从现有技术中学习实例化模式来平衡利用和探索


<details>
  <summary>Details</summary>
Motivation: 量化公式是SMT求解器面临的主要挑战，现有实例化技术各有优劣但缺乏协同，需要一种能够动态学习和生成新实例的方法

Method: 将观察到的实例化视为潜在语言的样本，使用概率上下文无关文法生成相似的新术语，可选地反转学习到的术语概率来探索多样性

Result: 该方法能够模仿成功的过往实例化，同时通过概率反转实现探索性，在量化推理中实现利用与探索的平衡

Conclusion: 基于文法学习的动态实例化方法为SMT求解器处理量化公式提供了新的有效途径，能够综合多种现有技术的优势

Abstract: Quantified formulas pose a significant challenge for Satisfiability Modulo
Theories (SMT) solvers due to their inherent undecidability. Existing
instantiation techniques, such as e-matching, syntax-guided, model-based,
conflict-based, and enumerative methods, often complement each other. This
paper introduces a novel instantiation approach that dynamically learns from
these techniques during solving. By treating observed instantiations as samples
from a latent language, we use probabilistic context-free grammars to generate
new, similar terms. Our method not only mimics successful past instantiations
but also explores diversity by optionally inverting learned term probabilities,
aiming to balance exploitation and exploration in quantifier reasoning.

</details>


### [41] [Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration](https://arxiv.org/abs/2508.13828)
*Yifei Chen,Guanting Dong,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文系统性研究了多个RAG系统的集成方法，从理论和机制角度分析了RAG集成框架，通过信息熵理论解释集成框架，并在管线和模块层面进行实验验证，证明多RAG系统集成具有良好的通用性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前单个RAG框架无法良好适应广泛的下游任务，需要抓取多个RAG系统的优势来提升性能。

Method: 从理论和机制两个角度分析RAG集成框架：理论方面从信息熵角度解释；机制方面从管线层面（分支、迭代、循环、代理）和模块层面（生成器、检索器、重排器）进行研究，解决7个研究问题。

Result: 实验结果显示，无论是在管线层面还是模块层面，聚合多个RAG系统都显示出良好的通用性和稳健性。

Conclusion: 本研究为多RAG系统集成相关研究奠定了基础，证明了多系统集成在RAG领域的有效性和价值。

Abstract: Retrieval-Augmented Generation (RAG) technology has been widely applied in
recent years. However, despite the emergence of various RAG frameworks, a
single RAG framework still cannot adapt well to a broad range of downstream
tasks. Therefore, how to leverage the advantages of multiple RAG systems has
become an area worth exploring. To address this issue, we have conducted a
comprehensive and systematic investigation into ensemble methods based on RAG
systems. Specifically, we have analyzed the RAG ensemble framework from both
theoretical and mechanistic analysis perspectives. From the theoretical
analysis, we provide the first explanation of the RAG ensemble framework from
the perspective of information entropy. In terms of mechanism analysis, we have
explored the RAG ensemble framework from both the pipeline and module levels.
We carefully select four different pipelines (Branching, Iterative, Loop, and
Agentic) and three different modules (Generator, Retriever, and Reranker) to
solve seven different research questions. The experiments show that aggregating
multiple RAG systems is both generalizable and robust, whether at the pipeline
level or the module level. Our work lays the foundation for similar research on
the multi-RAG system ensemble.

</details>


### [42] [Improved Generalized Planning with LLMs through Strategy Refinement and Reflection](https://arxiv.org/abs/2508.13876)
*Katharina Stein,Nils Hodel,Daniel Fišer,Jörg Hoffmann,Michael Katz,Alexander Koller*

Main category: cs.AI

TL;DR: 通过使用伪代码生成和自动调试、反思步骤以及多程序变体选择等方法，改进了LLM生成广义计划的质量，在17个基准领域中显著提升了计划的正确性。


<details>
  <summary>Details</summary>
Motivation: 之前的方法只生成一个策略并直接转换为程序，如果策略错误就会导致整个广义计划失败，需要更好的错误检测和修复机制。

Method: 使用伪代码生成策略并自动调试，在Python调试阶段添加反思步骤找出错误原因，生成多个程序变体并选择最佳的。

Result: 在17个基准领域中，新方法显著提升了广义计划的质量而不会低于原来。在12个领域中，最佳Python程序能解决所有可生成的任务。

Conclusion: 通过伪代码调试、反思和多程序选择等方法，可以有效提高LLM生成广义计划的质量和可靠性。

Abstract: LLMs have recently been used to generate Python programs representing
generalized plans in PDDL planning, i.e., plans that generalize across the
tasks of a given PDDL domain. Previous work proposed a framework consisting of
three steps: the LLM first generates a summary and then a strategy for the
domain, both in natural language, and then implements that strategy as a Python
program, that gets debugged on example planning tasks. In that work, only one
strategy is generated and passed directly to the program generation. If the
strategy is incorrect, its implementation will therefore result in an incorrect
generalized plan. Here, we introduce an approach that generates the strategy in
the form of pseudocode and enables automatic debugging of the pseudocode, hence
allowing us to identify and fix errors prior to the generation of the
generalized plan itself. Additionally, we extend the Python debugging phase
with a reflection step prompting the LLM to pinpoint the reason for the
observed plan failure. Finally, we take inspiration from LLM code generation to
produce several program variants and pick the best one. Running experiments on
17 benchmark domains, we show that these extensions substantially improve (and
never deteriorate) the quality of the generalized plans. In 12 of the domains,
our best Python programs solve all tasks that can be generated with the
respective instance generator.

</details>


### [43] [Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback](https://arxiv.org/abs/2508.13915)
*Yihao Ang,Yifan Bao,Lei Jiang,Jiajie Tao,Anthony K. H. Tung,Lukasz Szpruch,Hao Ni*

Main category: cs.AI

TL;DR: TS-Agent是一个模块化代理框架，用于自动化金融时间序列建模工作流程，通过三阶段决策过程在模型选择、代码优化和微调方面超越现有AutoML方法


<details>
  <summary>Details</summary>
Motivation: 金融时间序列建模需要高性能、可解释且可审计的模型，但现有AutoML框架缺乏对领域特定需求和动态目标的适应性，而LLM代理系统提供了更灵活的工作流自动化路径

Method: 设计模块化代理框架，包含规划代理和结构化知识库，通过三阶段迭代决策过程（模型选择、代码优化、微调）进行建模，结合上下文推理和实验反馈

Result: 在多种金融预测和合成数据生成任务中，TS-Agent持续优于最先进的AutoML和代理基线，实现了更高的准确性、鲁棒性和决策可追溯性

Conclusion: TS-Agent框架成功解决了金融时间序列建模中的适应性、可解释性和可审计性挑战，为高风险环境提供了有效的自动化解决方案

Abstract: Time-series data is central to decision-making in financial markets, yet
building high-performing, interpretable, and auditable models remains a major
challenge. While Automated Machine Learning (AutoML) frameworks streamline
model development, they often lack adaptability and responsiveness to
domain-specific needs and evolving objectives. Concurrently, Large Language
Models (LLMs) have enabled agentic systems capable of reasoning, memory
management, and dynamic code generation, offering a path toward more flexible
workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular
agentic framework designed to automate and enhance time-series modeling
workflows for financial applications. The agent formalizes the pipeline as a
structured, iterative decision process across three stages: model selection,
code refinement, and fine-tuning, guided by contextual reasoning and
experimental feedback. Central to our architecture is a planner agent equipped
with structured knowledge banks, curated libraries of models and refinement
strategies, which guide exploration, while improving interpretability and
reducing error propagation. \textsf{TS-Agent} supports adaptive learning,
robust debugging, and transparent auditing, key requirements for high-stakes
environments such as financial services. Empirical evaluations on diverse
financial forecasting and synthetic data generation tasks demonstrate that
\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic
baselines, achieving superior accuracy, robustness, and decision traceability.

</details>


### [44] [The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management](https://arxiv.org/abs/2508.13942)
*Soumyadeep Dhar*

Main category: cs.AI

TL;DR: AI代理在供应链中表现出'合作悖论'：理论上更优的协同AI代理比非AI基准表现更差，原因是库存囤积导致系统崩溃。通过高层AI策略设定和低层协同执行的结合才能实现稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究AI驱动代理在经济环境中的新兴战略行为，特别是在多级供应链这种容易出现牛鞭效应等不稳定性的合作环境中。

Method: 使用基于大语言模型的生成式AI代理在受控供应链模拟中进行计算实验，设计包含供应商管理库存原则的协同AI代理。

Result: 发现'合作悖论'现象，协同AI代理表现比非AI基准更差；通过高层AI策略设定和低层协同执行相结合的方法实现了系统稳定性。

Conclusion: 揭示了协同AI代理的新兴行为特征，为设计稳定有效的AI驱动商业分析系统提供了蓝图，强调需要多层次策略来避免灾难性故障模式。

Abstract: The rise of autonomous, AI-driven agents in economic settings raises critical
questions about their emergent strategic behavior. This paper investigates
these dynamics in the cooperative context of a multi-echelon supply chain, a
system famously prone to instabilities like the bullwhip effect. We conduct
computational experiments with generative AI agents, powered by Large Language
Models (LLMs), within a controlled supply chain simulation designed to isolate
their behavioral tendencies. Our central finding is the "collaboration
paradox": a novel, catastrophic failure mode where theoretically superior
collaborative AI agents, designed with Vendor-Managed Inventory (VMI)
principles, perform even worse than non-AI baselines. We demonstrate that this
paradox arises from an operational flaw where agents hoard inventory, starving
the system. We then show that resilience is only achieved through a synthesis
of two distinct layers: high-level, AI-driven proactive policy-setting to
establish robust operational targets, and a low-level, collaborative execution
protocol with proactive downstream replenishment to maintain stability. Our
final framework, which implements this synthesis, can autonomously generate,
evaluate, and quantify a portfolio of viable strategic choices. The work
provides a crucial insight into the emergent behaviors of collaborative AI
agents and offers a blueprint for designing stable, effective AI-driven systems
for business analytics.

</details>


### [45] [ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation](https://arxiv.org/abs/2508.13975)
*Jingquan Wang,Andrew Negrut,Harry Zhang,Khailanii Slaton,Shu Wang,Radu Serban,Jinlong Wu,Dan Negrut*

Main category: cs.AI

TL;DR: 这篇论文探讨了如何通过精细调整大型语言模型，将其转化为帮助专家使用PyChrono模拟工具的虚拟助手，以生成质量提升的模拟脚本。


<details>
  <summary>Details</summary>
Motivation: 为了降低专家使用PyChrono等模拟工具的入门阈值，利用AI技术提高模拟脚本的生成效率和质量。

Method: 提出一个框架，通过精细调整无论是开源还是闭源的大型语言模型，使其能够生成PyChrono虚拟实验脚本，并量化地提升脚本质量。

Result: 生成的模拟脚本质量显著提高，范围从简单的单摆模型到复杂的车辆在可变形地形上的实验。虽然并非完美，但作为初始点非常有用，还能回答API相关问题和推荐建模方法。

Conclusion: 该框架具有普遍性，可以扩展到其他模拟工具领域，有效降低专业模拟工具的使用门槛。

Abstract: This contribution is concerned with the following issue: can pretrained large
language models (LLMs) be refined and customized to the point where they become
virtual assistants helping experts with the effective use of a simulation tool?
In this case study, the ``simulation tool'' considered is PyChrono, an open
source multi-physics dynamics engine for multibody systems. We present a
framework for refining and customizing both open- and closed-source LLMs to
harness the power of AI in generating scripts that perform PyChrono virtual
experiments. We refine and customize several classes of LLMs through a process
that leads to a quantifiable improvement in the quality of the generated
PyChrono simulation scripts. These scripts can range from simple
single-pendulum simulations to complex virtual experiments involving full
vehicles on deformable terrain. While the generated scripts are rarely perfect,
they often serve as strong starting points for the user to modify and improve
on. Additionally, the LLM can answer specific API questions about the
simulator, or recommend modeling approaches. The framework discussed is general
and can be applied to lower the entry barrier for simulation tools associated
with other application domains.

</details>


### [46] [A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem](https://arxiv.org/abs/2508.14020)
*Christian Blum,Pedro Pinacho-Davidson*

Main category: cs.AI

TL;DR: 使用偏置随机键道因算法(BRKGA)解决最长运行子序列(LRS)问题，该算法在计算效率和解质量方面表现优异，但在大字母表字符串上仍有改进空间


<details>
  <summary>Details</summary>
Motivation: LRS问题是一个NP难组合优化问题，在生物信息学领域特别是基因组重新组装中具有重要作用，需要高效解决方法

Method: 提出使用Biased Random Key Genetic Algorithm (BRKGA)，重点关注个体评估的计算效率，将灰度值向量转换为有效解。为比较还开发了Max-Min Ant System和使用CPLEX整数规划求解器

Result: 计算结果显示BRKGA在LRS问题上是目前最先进的技术，在计算效率和解质量方面都表现优异

Conclusion: BRKGA方法在LRS问题上取得了迅速且有效的解决方案，但在处理大字母表字符串时仍有改进余地，需要进一步优化

Abstract: The longest run subsequence (LRS) problem is an NP-hard combinatorial
optimization problem belonging to the class of subsequence problems from
bioinformatics. In particular, the problem plays a role in genome reassembly.
In this paper, we present a solution to the LRS problem using a Biased Random
Key Genetic Algorithm (BRKGA). Our approach places particular focus on the
computational efficiency of evaluating individuals, which involves converting
vectors of gray values into valid solutions to the problem. For comparison
purposes, a Max-Min Ant System is developed and implemented. This is in
addition to the application of the integer linear programming solver CPLEX for
solving all considered problem instances. The computation results show that the
proposed BRKGA is currently a state-of-the-art technique for the LRS problem.
Nevertheless, the results also show that there is room for improvement,
especially in the context of input strings based on large alphabet sizes.

</details>


### [47] [ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/abs/2508.14040)
*Hanyu Lai,Xiao Liu,Yanxiao Zhao,Han Xu,Hanchen Zhang,Bohao Jing,Yanyu Ren,Shuntian Yao,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ComputerRL是一个自主桌面智能框架，通过API-GUI范式统一程序化API调用和直接GUI交互，解决了机器代理与以人为中心的桌面环境之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决机器代理在人类为中心的桌面环境中操作时的固有失配问题，以及扩展端到端强化学习训练在多样化桌面任务中的挑战。

Method: 开发了分布式RL基础设施，支持数千个并行虚拟桌面环境进行大规模在线RL训练；提出Entropulse训练策略，交替使用强化学习和监督微调来缓解熵崩溃问题。

Result: 在OSWorld基准测试中，基于GLM-4-9B-0414的AutoGLM-OS-9B达到了48.1%的最新最先进准确率，在桌面自动化通用代理方面显示出显著改进。

Conclusion: ComputerRL框架通过创新的API-GUI范式和可扩展的训练基础设施，显著提升了自主桌面智能代理的性能和泛化能力，为桌面自动化领域带来了重要突破。

Abstract: We introduce ComputerRL, a framework for autonomous desktop intelligence that
enables agents to operate complex digital workspaces skillfully. ComputerRL
features the API-GUI paradigm, which unifies programmatic API calls and direct
GUI interaction to address the inherent mismatch between machine agents and
human-centric desktop environments. Scaling end-to-end RL training is crucial
for improvement and generalization across diverse desktop tasks, yet remains
challenging due to environmental inefficiency and instability in extended
training. To support scalable and robust training, we develop a distributed RL
infrastructure capable of orchestrating thousands of parallel virtual desktop
environments to accelerate large-scale online RL. Furthermore, we propose
Entropulse, a training strategy that alternates reinforcement learning with
supervised fine-tuning, effectively mitigating entropy collapse during extended
training runs. We employ ComputerRL on open models GLM-4-9B-0414 and
Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B
based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,
demonstrating significant improvements for general agents in desktop
automation. The algorithm and framework are adopted in building AutoGLM (Liu et
al., 2024a)

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [48] [Repeater Swarm-Assisted Cellular Systems: Interaction Stability and Performance Analysis](https://arxiv.org/abs/2508.13593)
*Jianan Bai,Anubhab Chowdhury,Anders Hansson,Erik G. Larsson*

Main category: cs.IT

TL;DR: 大规模MIMO系统中部署中空重复器群来改善覆盖，解决正向反馈稳定性问题和性能优化挑战


<details>
  <summary>Details</summary>
Motivation: 中空重复器群作为低成本插入式解决方案可改善网络覆盖，但面临重复器间互动导致的正向反馈稳定性问题以及噪声和干扰对性能的影响

Method: 推导广义Nyquist稳定性准则，提供易检查的稳定性条件，并开发了一种高效迭代算法来聚合优化重复器增益、用户发射力率和接收组合权重

Result: 数值结果验证了理论发现，显示重复器能够在次-6GHz和毫米波段显著提升系统性能，但需要注意部署条件以保证重复器与基站间的视线链路概率

Conclusion: 中空重复器群是改善大规模MIMO系统覆盖的有效手段，通过理论分析和优化算法可以解决稳定性问题并实现显著性能提升，但需要谨慎部署以充分发挥其优势

Abstract: We consider a cellular massive MIMO system where swarms of wireless repeaters
are deployed to improve coverage. These repeaters are full-duplex relays with
small form factors that receive and instantaneously retransmit signals. They
can be deployed in a plug-and-play manner at low cost, while being transparent
to the network--conceptually they are active channel scatterers with
amplification capabilities. Two fundamental questions need to be addressed in
repeater deployments: (I) How can we prevent destructive effects of positive
feedback caused by inter-repeater interaction (i.e., each repeater receives and
amplifies signals from others)? (ii) How much performance improvement can be
achieved given that repeaters also inject noise and may introduce more
interference? To answer these questions, we first derive a generalized Nyquist
stability criterion for the repeater swarm system, and provide an easy-to-check
stability condition. Then, we study the uplink performance and develop an
efficient iterative algorithm that jointly optimizes the repeater gains, user
transmit powers, and receive combining weights to maximize the weighted sum
rate while ensuring system stability. Numerical results corroborate our
theoretical findings and show that the repeaters can significantly improve the
system performance, both in sub-6 GHz and millimeter-wave bands. The results
also warrant careful deployment to fully realize the benefits of repeaters, for
example, by ensuring a high probability of line-of-sight links between
repeaters and the base station.

</details>


### [49] [A Convergent Primal-Dual Algorithm for Computing Rate-Distortion-Perception Functions](https://arxiv.org/abs/2508.13486)
*Chunhui Chen,Linyi Chen,Xueyan Niu,Hao Wu*

Main category: cs.IT

TL;DR: 这篇论文提出了一种新的理论框架咇算法，用于计算信息码率-头真-感知（RDP）函数，解决了现有方法缺乏理论保证的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RDP函数计算方法缺乏理论保证，特别是在复杂的感知约束下存在非凸性咇计算难题。需要一种具有严格收敛性证明的算法。

Method: 通过改变重建分布的约束条件，将其转换为对重建分布本身的优化问题，开发了一种新的原对偶算法。

Result: 证明了算法具有$O(1/n)$的收敛速率，在代表性场景中达到了竞争性能的实验结果。

Conclusion: 该方法填补了RDP理论中的重要空白，为RDP约束压缩系统提供了更可靠咇可解释的优化方法。

Abstract: Recent advances in Rate-Distortion-Perception (RDP) theory highlight the
importance of balancing compression level, reconstruction quality, and
perceptual fidelity. While previous work has explored numerical approaches to
approximate the information RDP function, the lack of theoretical guarantees
remains a major limitation, especially in the presence of complex perceptual
constraints that introduce non-convexity and computational intractability.
Inspired by our previous constrained Blahut-Arimoto algorithm for solving the
rate-distortion function, in this paper, we present a new theoretical framework
for computing the information RDP function by relaxing the constraint on the
reconstruction distribution and replacing it with an alternative optimization
approach over the reconstruction distribution itself. This reformulation
significantly simplifies the optimization and enables a rigorous proof of
convergence. Based on this formulation, we develop a novel primal-dual
algorithm with provable convergence guarantees. Our analysis establishes, for
the first time, a rigorous convergence rate of $O(1/n)$ for the computation of
RDP functions. The proposed method not only bridges a key theoretical gap in
the existing literature but also achieves competitive empirical performance in
representative settings. These results lay the groundwork for more reliable and
interpretable optimization in RDP-constrained compression systems. Experimental
results demonstrate the efficiency and accuracy of the proposed algorithm.

</details>


### [50] [On optimal quantum LRCs from the Hermitian construction and $t$-designs](https://arxiv.org/abs/2508.13553)
*Yang Li,Shitao Li,Huimin Lao,Gaojun Luo,San Ling*

Main category: cs.IT

TL;DR: 本文通过Hermitian构造方法研究了量子本地可恢复码(qLRCs)的界限和构造，解决了Luo等人提出的开放问题，得到了三个明确的最优qLRCs家族。


<details>
  <summary>Details</summary>
Motivation: 量子本地可恢复码在大规模量子数据存储中具有重要应用潜力，并且对量子LDPC码有重要意义。本文主要解决通过Hermitian构造方法来探索qLRCs的理论界限和实际构造问题。

Method: 采用Hermitian构造方法，首先构造了几个新的无穷家族的NMDS码（非最大分离可行码），这些码支持t-designs（t=2,3）且具有灵活的维数。然后将这些码应用于获得Hermitian对偶包含的经典LRCs，最终引导出最优的qLRCs。

Result: 提出了qLRCs的四个新界限，并进行了流行性公式比较。构造了三个明确的最优qLRCs家族，这些码与通过CSS构造获得的已知qLRCs相比，具有更新颖和灵活的参数。同时，构造的经典LRCs本身也很有趣，因为它们在四个不同的cLRCs界限下都是最优的。

Conclusion: 本文成功地通过Hermitian构造方法开发了新的量子本地可恢复码，不仅解决了一个开放问题，而且提供了比现有CSS构造更优称的码类。这些成果对大规模量子数据存储和量子错误约束码的发展都有重要意义。

Abstract: In a recent work, quantum locally recoverable codes (qLRCs) have been
introduced for their potential application in large-scale quantum data storage
and implication for quantum LDPC codes. This work focuses on the bounds and
constructions of qLRCs derived from the Hermitian construction, which solves an
open problem proposed by Luo $et~al.$ (IEEE Trans. Inf. Theory, 71 (3):
1794-1802, 2025). We present four bounds for qLRCs and give comparisons in
terms of their asymptotic formulas. We construct several new infinite families
of NMDS codes, with general and flexible dimensions, that support t-designs for
$t\in \{2,3\}$, and apply them to obtain Hermitian dual-containing classical
LRCs (cLRCs). As a result, we derive three explicit families of optimal qLRCs.
Compared to the known qLRCs obtained by the CSS construction, our optimal qLRCs
offer new and more flexible parameters. It is also worth noting that the
constructed cLRCs themselves are interesting as they are optimal with respect
to four distinct bounds for cLRCs.

</details>


### [51] [Power and Rate Allocations for Positive-rate Covert Communications in Block-Fading Channels](https://arxiv.org/abs/2508.13555)
*Yubo Zhang,Hassan ZivariFard,Xiaodong Wang*

Main category: cs.IT

TL;DR: 该论文研究瑞利块衰落信道中无密钥隐蔽通信的功率和速率分配问题，提出了基于非因果CSI的三步法和基于因果CSI的DDQN方法来解决功率最大化和功耗最小化问题。


<details>
  <summary>Details</summary>
Motivation: 在瑞利块衰落信道中实现正速率的无密钥隐蔽通信，解决当发射端和合法接收端具有信道状态信息(CSI)而监听者只有统计信息时的隐蔽通信优化问题。

Method: 针对非因果CSI提出三步法解决功率和速率分配问题；针对因果CSI将功率分配建模为MDP并使用DDQN求解，速率分配问题则使用训练好的DDQN近似求解。

Result: 仿真结果表明所提出的功率和速率分配方法有效，并提供了不同分配方案的全面性能比较。

Conclusion: 论文成功解决了瑞利衰落信道中隐蔽通信的功率和速率优化问题，为非因果和因果CSI场景提供了有效的解决方案。

Abstract: We aim to achieve keyless covert communication with a positive-rate in
Rayleigh block-fading channels. Specifically, the transmitter and the
legitimate receiver are assumed to have either causal or non-causal knowledge
of the \ac{CSI} for both the legitimate and the warden channels, while the
warden only knows the statistical distribution of the \ac{CSI}. Two problem
formulations are considered in this work: (a) Power allocation: maximizing the
sum covert rate subject to a maximum power constraint, and (b) Rate allocation:
minimizing the power consumption subject to a minimum covert rate constraint.
Both problems are formulated based on recent information theoretical results on
covert communication over state-dependent channels. When the \ac{CSI} of each
fading block is known non-causally, we propose a novel three-step method to
solve both the power and rate allocation problems. In the case where the
\ac{CSI} is known causally, the power allocation problem can be formulated as
\ac{MDP} and be solved using a \ac{DDQN} approach. Although the rate allocation
problem under causal \ac{CSI} does not directly conform to an \ac{MDP}
structure, it can be approximately solved using the \ac{DDQN} trained for power
allocation. Simulation results demonstrate the effectiveness of the proposed
power and rate allocation methods and provide comprehensive performance
comparisons across different allocation schemes.

</details>


### [52] [Joint Beamforming Design for RIS-Empowered NOMA-ISAC Systems](https://arxiv.org/abs/2508.13842)
*Chunjie Wang,Xuhui Zhang,Jinke Ren,Wenchao Liu,Shuqiang Wang,Yanyan Shen,Kejiang Ye,Chengzhong Xu,Dusit Niyato*

Main category: cs.IT

TL;DR: 这篇论文研究了基于NOMA技术的RIS辅助感知通信一体化系统，提出了联合通信感知放大器设计方案，通过交替优化算法实现系统性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 为了在RIS辅助的ISAC系统中同时优化通信和感知性能，需要解决双功能基站主动放大、RIS反射系数和雷达接收滤波器的联合优化问题，以实现用户总速率的最大化。

Method: 提出了一种基于交替优化框架的迭代算法，联合优化双功能基站的主动放大、RIS的反射系数和雷达接收滤波器。算法考虑了多种约束条件，包括雷达信噪比阈值、用户干扰噪比要求、RIS相位移等。

Result: 模拟结果显示，所提算法在考虑的RIS增强型NOMA-ISAC系统中表现出艶，性能显著超过基线算法，验证了其优势。

Conclusion: 该研究成功地开发了一种高效的联合优化算法，为RIS辅助的NOMA-ISAC系统提供了性能优化方案，显示了在同时支持多用户通信和多目标感知的复杂场景中的应用潜力。

Abstract: This paper investigates a reconfigurable intelligent surface (RIS)-assisted
integrated sensing and communication (ISAC) system and proposes a joint
communication and sensing beamforming design based on non-orthogonal multiple
access (NOMA) technology. The system employs a dual-functional base station
(DFBS) to simultaneously serve multiple users and sense multiple targets with
the aid of RIS. To maximize the sum-rate of users, we jointly optimize the
DFBS's active beamforming, the RIS's reflection coefficients, and the radar
receive filters. The optimization is performed under constraints including the
radar signal-to-noise ratio thresholds, the user
signal-to-interference-plus-noise ratio requirements, the phase shifts of the
RIS, the total transmit power, the receive filters, and the successive
interference cancellation decoding order. To tackle the complex
interdependencies and non-convex nature of the optimization problem, we
introduce an effective iterative algorithm based on the alternating
optimization framework. Simulation results demonstrate that the proposed
algorithm outperforms baseline algorithms, highlighting its distinct advantages
in the considered RIS-empowered NOMA-ISAC systems.

</details>
