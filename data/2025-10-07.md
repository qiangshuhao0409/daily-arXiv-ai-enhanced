<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 15]
- [cs.AI](#cs.AI) [Total: 89]
- [cs.IT](#cs.IT) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Scalable Ground Station Selection for Large LEO Constellations](https://arxiv.org/abs/2510.03438)
*Grace Ra Kim,Duncan Eddy,Vedant Srinivas,Mykel J. Kochenderfer*

Main category: cs.NI

TL;DR: 提出了一种可扩展的分层框架，用于优化LEO卫星星座的地面站选择，通过将全局问题分解为单卫星短时间窗口子问题，聚类最优选择，然后匹配到最近的GSaaS候选站点，实现接近全局最优的性能。


<details>
  <summary>Details</summary>
Motivation: 传统地面站选择方法在考虑多个提供商和大型卫星星座时，使用混合整数规划方法难以扩展，需要一种可扩展的解决方案来最小化运营成本、最大化数据下行量并减少通信间隙。

Method: 采用分层框架，将全局选择问题分解为单卫星短时间窗口子问题，聚类最优站点选择，然后将聚类结果匹配到最近的GSaaS候选站点，生成全局可行解。

Result: 在合成Walker-Star测试案例（1-10颗卫星，1-10个地面站）中，所有测试案例的解决方案都达到了全局IP最优解的95%以内。在Capella Space（5颗卫星）、ICEYE（40颗）和Planet's Flock（96颗）的真实世界评估中，该方法继续提供高质量的站点选择，而精确IP解决方案无法扩展。

Conclusion: 该分层框架能够在保持接近最优性能的同时实现可扩展的协调，有效解决了大规模LEO卫星星座地面站选择的计算难题。

Abstract: Effective ground station selection is critical for low Earth orbiting (LEO)
satellite constellations to minimize operational costs, maximize data downlink
volume, and reduce communication gaps between access windows. Traditional
ground station selection typically begins by choosing from a fixed set of
locations offered by Ground Station-as-a-Service (GSaaS) providers, which helps
reduce the problem scope to optimizing locations over existing infrastructure.
However, finding a globally optimal solution for stations using existing
mixed-integer programming methods quickly becomes intractable at scale,
especially when considering multiple providers and large satellite
constellations. To address this issue, we introduce a scalable, hierarchical
framework that decomposes the global selection problem into single-satellite,
short time-window subproblems. Optimal station choices from each subproblem are
clustered to identify consistently high-value locations across all decomposed
cases. Cluster-level sets are then matched back to the closest GSaaS candidate
sites to produce a globally feasible solution. This approach enables scalable
coordination while maintaining near-optimal performance. We evaluate our
method's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10
stations), achieving solutions within 95% of the global IP optimum for all test
cases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and
Planet's Flock (96) show that while exact IP solutions fail to scale, our
framework continues to deliver high-quality site selections.

</details>


### [2] [Short-circuiting Rings for Low-Latency AllReduce](https://arxiv.org/abs/2510.03491)
*Sarah-Michelle Hammer,Stefan Schmid,Rachee Singh,Vamsi Addanki*

Main category: cs.NI

TL;DR: 挑战传统观点：在考虑实际传播延迟和链路容量约束后，Ring AllReduce算法对小消息也保持最优，而Recursive Doubling因跳数多导致更高拥塞。提出基于光子互连的动态拓扑切换方案，使Recursive Doubling能超越静态Ring拓扑的性能。


<details>
  <summary>Details</summary>
Motivation: 传统认为Ring算法仅对大消息最优，Recursive Doubling因对数步数对小消息更优。但实际网络约束下，这一假设可能不成立，需要重新评估两种算法的性能表现。

Method: 分析Ring和Recursive Doubling在真实网络约束下的性能，考虑传播延迟和链路容量。设计基于光子互连的电路切换启发式算法，动态重构拓扑以平衡重配置延迟、传播延迟和链路拥塞。

Result: 发现Ring算法对小消息也保持最优，因为Recursive Doubling的跳数多导致更高拥塞。光子互连的动态重构使Recursive Doubling能超越静态Ring拓扑的性能。

Conclusion: 传统关于AllReduce算法选择的假设需要重新考虑。光子互连的动态拓扑重构为集体通信提供了新的优化机会，但实现实用化仍面临挑战。

Abstract: Efficient collective communication is critical for many distributed ML and
HPC applications. In this context, it is widely believed that the Ring
algorithm for the AllReduce collective communication operation is optimal only
for large messages, while Recursive Doubling is preferable for small ones due
to its logarithmic number of steps compared to the linear number for Ring. In
this paper, we challenge this long-held assumption and show that the Ring
algorithm can remain optimal even for short messages in ring-based GPU-to-GPU
topologies, once realistic propagation delays and link capacity constraints are
accounted for. We find that the total propagation delay for both Ring and
Recursive Doubling essentially sums to the same value, but the latter incurs
significantly higher congestion due to longer hop counts, leading to increased
completion times. This surprising result motivates our case for in-collective
adaptive topologies, particularly in the context of emerging photonic
interconnects, which can break through the limitations of static topology
designs at the collective communication granularity. We design a \emph{simple
and fast} heuristic for circuit-switching that enables Recursive Doubling to
exploit dynamically reconfigurable photonic paths, carefully balancing
reconfiguration delays, propagation latencies, and link congestion to minimize
overall completion time. Our preliminary evaluations, using realistic
reconfiguration delays, show that our circuit-switching schedules enable faster
completion times for Recursive Doubling, even compared to Ring AllReduce on
static ring topologies. We conclude by highlighting key challenges and future
research directions for realizing practical, in-collective photonic switching.

</details>


### [3] [A distributed routing protocol for sending data from things to the cloud leveraging fog technology in the large-scale IoT ecosystem](https://arxiv.org/abs/2510.03524)
*Mohammad Reza Akbari,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: 提出了一种基于雾计算的物联网节能路由方法，通过多标准簇头选择和平衡树结构来降低能耗和延迟


<details>
  <summary>Details</summary>
Motivation: 物联网设备能量有限且无法更换电池，导致网络寿命短。需要降低能耗、加速数据传输以减少响应时间

Method: 基于距离、剩余能量、接收信号强度和链路过期时间等多标准选择簇头节点，通过平衡树层次化传输数据到服务器

Result: 仿真结果显示，该方法在包投递率、延迟、响应时间和网络寿命方面优于EECRP和ERGID协议

Conclusion: 所提出的多标准簇头选择和平衡树路由方法能有效提高物联网网络性能，延长网络寿命

Abstract: Fog computing integrates cloud and edge resources. According to an
intelligent and decentralized method, this technology processes data generated
by IoT sensors to seamlessly integrate physical and cyber environments.
Internet of Things uses wireless and smart objects. They communicate with each
other, monitor the environment, collect information, and respond to user
requests. These objects have limited energy resources since they use batteries
to supply energy. Also, they cannot replace their batteries. As a result, the
network lifetime is limited and short. Thus, reducing energy consumption and
accelerating the data transmission process are very important challenges in IoT
networks to reduce the response time. In the data transmission process,
selecting an appropriate cluster head node is very important because it can
reduce the delay when sending data to the fog. In this paper, cluster head
nodes are selected based on several important criteria such as distance,
residual energy, received signal strength, and link expiration time. Then,
objects send the processed data to the server hierarchically through a balanced
tree. The simulation results show that the proposed method outperforms the
energy-efficient centroid-based routing protocol (EECRP) and the Emergency
Response IoT based on Global Information Decision (ERGID) in terms of packet
delivery rate, delay, response time, and network lifetime.

</details>


### [4] [An efficient grey theory-driven path selection for energy efficiency control in the Internet of Things using fog and cloud computing](https://arxiv.org/abs/2510.03533)
*Mohammad Reza Akbari,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: 提出了一种名为MFCT-IoT的重叠聚类方法，用于在物联网雾计算中选择最佳簇头节点，以优化数据传输性能。


<details>
  <summary>Details</summary>
Motivation: 物联网大数据交换中，传统云计算存在高延迟问题，雾计算通过就近处理数据来减少网络延迟。

Method: 使用重叠聚类方法选择最佳簇头节点，簇头负责将收集的数据发送到最近的雾节点，雾节点处理数据或合并后发送到云端。

Result: 与ERGID和EECRP方案相比，MFCT-IoT在响应时间、数据包投递率、端到端延迟、网络寿命和能耗等方面表现更优。

Conclusion: 提出的MFCT-IoT方法在雾计算环境中能有效提升网络性能，优于现有方案。

Abstract: Due to the big data exchange on the Internet of Things, proper routing and
selecting the best routes for fast data transmission improve network
performance. There are major challenges, like high delay, when cloud computing
is used. Therefore, one solution is to use other schemes, such as fog
computing. In fog computing, all data is not sent to the cloud and the fog
nodes close to objects are used for data processing. This reduces the network
delay. In this paper, we propose an overlapping clustering method called
MFCT-IoT to select the best cluster head nodes to guarantee the fast data
transfer from objects to fog nodes. The selected cluster head nodes are
responsible for sending the collected data to the closest fog nodes in the
network edge. Upon receiving the data, the fog nodes process it, and if a
response is ready, they respond immediately to the object. Otherwise, they
merge and transmit the data to the cloud servers, which are considered as the
root node of the proposed hierarchical tree. After processing, the merged data
is sent to the object. We compare the proposed scheme with two schemes,
including ERGID and EECRP. These schemes are evaluated based on various
criteria, including the response time, packet delivery ratio, end-to-end delay,
network lifetime, and energy consumption. The results indicate that the
proposed method outperforms others in terms of all criteria.

</details>


### [5] [A Position- and Energy-Aware Routing Strategy for Subterranean LoRa Mesh Networks](https://arxiv.org/abs/2510.03714)
*Nalith Udugampola,Xiaoyu Ai,Binghao Li,Henry Gong,Aruna Seneviratne*

Main category: cs.NI

TL;DR: 提出了一种针对地下LoRa网状网络的新型位置和能量感知路由策略，相比之前优化的基于泛洪的方法，最大吞吐量提高185%，能耗降低75%。


<details>
  <summary>Details</summary>
Motivation: 现有LoRa网状网络路由协议在低数据率网络中由于控制开销、确认和冗余重传而低效利用有限带宽，特别是在地下环境中。

Method: 采用轻量级位置学习阶段确定中继器相对位置和路由信息，然后通过自适应路由利用备用LoRa中继器进行数据包冲突和丢失恢复，以及能量感知路由切换来平衡中继器电池消耗。

Result: 在代表性地下网络模拟中，相比之前优化的基于泛洪的高流量方法，最大吞吐量提高185%，能耗降低75%。

Conclusion: 该位置和能量感知路由策略显著提高了地下LoRa网状网络的吞吐量和能效，同时保持高数据包传输率。

Abstract: Although LoRa is predominantly employed with the single-hop LoRaWAN protocol,
recent advancements have extended its application to multi-hop mesh topologies.
Designing efficient routing for LoRa mesh networks remains challenging due to
LoRa's low data rate and ALOHA-based MAC. Prior work often adapts conventional
protocols for low-traffic, aboveground networks with strict duty cycle
constraints or uses flooding-based methods in subterranean environments.
However, these approaches inefficiently utilize the limited available network
bandwidth in these low-data-rate networks due to excessive control overhead,
acknowledgments, and redundant retransmissions. In this paper, we introduce a
novel position- and energy-aware routing strategy tailored for subterranean
LoRa mesh networks aimed at enhancing maximum throughput and power efficiency
while also maintaining high packet delivery ratios. Our mechanism begins with a
lightweight position learning phase, during which LoRa repeaters ascertain
their relative positions and gather routing information. Afterwards, the
network becomes fully operational with adaptive routing, leveraging standby
LoRa repeaters for recovery from packet collisions and losses, and energy-aware
route switching to balance battery depletion across repeaters. The simulation
results on a representative subterranean network demonstrate a 185% increase in
maximum throughput and a 75% reduction in energy consumption compared to a
previously optimized flooding-based approach for high traffic.

</details>


### [6] [6G-Enabled Digital Twin Framework for Real-Time Cyber-Physical Systems: An Experimental Validation with Industrial Bearing Fault Detection](https://arxiv.org/abs/2510.03807)
*Vaskar Chakma,Wooyeol Choi*

Main category: cs.NI

TL;DR: 提出了一种6G驱动的数字孪生框架，通过太赫兹通信和边缘AI实现超低延迟（0.8ms）的轴承故障检测，相比WiFi-6和5G网络分别提升15.6倍和5.25倍性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于5G的CPS系统延迟超过10ms，无法满足工业关键任务应用对亚毫秒级响应时间的需求，特别是在自主工业控制和预测性维护等场景。

Method: 集成太赫兹通信（0.1-1 THz）、智能反射表面和边缘人工智能的五层架构，使用CWRU轴承数据集进行特征提取和随机森林分类算法验证。

Result: 实现了97.7%的故障分类准确率，端到端延迟仅0.8ms，相比WiFi-6（12.5ms）和5G（4.2ms）网络显著提升，并在四种轴承故障类型上保持97%以上的宏平均F1分数。

Conclusion: 6G驱动的数字孪生框架能够有效解决工业关键任务应用的实时性能瓶颈，为超低延迟工业自动化系统提供了可行解决方案。

Abstract: Current Cyber-Physical Systems (CPS) integrated with Digital Twin (DT)
technology face critical limitations in achieving real-time performance for
mission-critical industrial applications. Existing 5G-enabled systems suffer
from latencies exceeding 10ms, which are inadequate for applications requiring
sub-millisecond response times, such as autonomous industrial control and
predictive maintenance. This research aims to develop and validate a 6G-enabled
Digital Twin framework that achieves ultra-low latency communication and
real-time synchronization between physical industrial assets and their digital
counterparts, specifically targeting bearing fault detection as a critical
industrial use case. The proposed framework integrates terahertz communications
(0.1-1 THz), intelligent reflecting surfaces, and edge artificial intelligence
within a five-layer architecture. Experimental validation was conducted using
the Case Western Reserve University (CWRU) bearing dataset, implementing
comprehensive feature extraction (15 time and frequency domain features) and
Random Forest classification algorithms. The system performance was evaluated
against traditional WiFi-6 and 5G networks across multiple metrics, including
classification accuracy, end-to-end latency, and scalability. It achieved 97.7%
fault classification accuracy with 0.8ms end-to-end latency, representing a
15.6x improvement over WiFi-6 (12.5ms) and 5.25x improvement over 5G (4.2ms)
networks. The system demonstrated superior scalability with sub-linear
processing time growth and maintained consistent performance across four
bearing fault categories (normal, inner race, outer race, and ball faults) with
macro-averaged F1-scores exceeding 97%.

</details>


### [7] [A4FN: an Agentic AI Architecture for Autonomous Flying Networks](https://arxiv.org/abs/2510.03829)
*André Coelho,Pedro Ribeiro,Helder Fontes,Rui Campos*

Main category: cs.NI

TL;DR: A4FN是一个基于Agentic AI的无人机网络架构，利用生成式AI和LLM实现意图驱动的实时网络自动化控制，包含感知代理和决策行动代理两个组件。


<details>
  <summary>Details</summary>
Motivation: 针对任务关键型、基础设施有限的场景（如灾害响应），需要实现实时、上下文感知的网络控制，传统方法难以满足动态重构和资源管理的需求。

Method: 采用分布式Agentic系统，包含感知代理（通过多模态输入语义解析生成服务等级规范）和决策行动代理（基于推断意图重新配置网络）。

Result: 设计了具备自主性、目标驱动推理和连续感知-行动循环等Agentic AI关键特性的架构，支持自适应重构、动态资源管理和新兴无线技术互操作性。

Conclusion: A4FN为下一代飞行网络中的多智能体协调和Agentic AI集成提供了创新架构，但仍面临相关研究挑战。

Abstract: This position paper presents A4FN, an Agentic Artificial Intelligence (AI)
architecture for intent-driven automation in Flying Networks (FNs) using
Unmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI
and Large Language Models (LLMs) to enable real-time, context-aware network
control via a distributed agentic system. It comprises two components: the
Perception Agent (PA), which semantically interprets multimodal input --
including imagery, audio, and telemetry data -- from UAV-mounted sensors to
derive Service Level Specifications (SLSs); and the Decision-and-Action Agent
(DAA), which reconfigures the network based on inferred intents. A4FN embodies
key properties of Agentic AI, including autonomy, goal-driven reasoning, and
continuous perception-action cycles. Designed for mission-critical,
infrastructure-limited scenarios such as disaster response, it supports
adaptive reconfiguration, dynamic resource management, and interoperability
with emerging wireless technologies. The paper details the A4FN architecture,
its core innovations, and open research challenges in multi-agent coordination
and Agentic AI integration in next-generation FNs.

</details>


### [8] [Analysis of LTE/5G Network Performance Parameters in Smartphone Use Cases: A Study of Packet Loss, Delay, and Slice Types](https://arxiv.org/abs/2510.04035)
*Almamoon Alauthman,Abeer Al-Hyari*

Main category: cs.NI

TL;DR: 该论文使用九种元启发式算法优化LTE和5G网络中的丢包率和延迟，WOA算法表现最佳，在eMBB、URLLC和mMTC网络切片中分别减少31%丢包和6.3ms延迟。


<details>
  <summary>Details</summary>
Motivation: 优化LTE和5G网络关键路径中的丢包率和延迟这两个重要性能参数，以提升智能手机用户体验。

Method: 研究九种元启发式算法（包括WOA、PSO、ABC等）在eMBB、URLLC和mMTC网络切片中的有效性。

Result: WOA表现最佳：减少31%丢包和6.3ms延迟；PSO次之：减少30%丢包和6.1ms延迟；ABC在mMTC场景中表现良好：减少29%丢包和6ms延迟。

Conclusion: 根据目标网络切片选择合适的算法对于优化资源利用和网络效率至关重要，为评估和改进LTE/5G网络可靠性和响应性提供了量化框架。

Abstract: The paper addresses optimizing two of the most important performance
parameters, packet loss, and delay, in the critical path optimization of LTE
and 5G networks using metaheuristic algorithms to play a vital role in the
smartphone user experience. In this context, nine metaheuristic algorithms,
such as WOA, PSO, and ABC, have been studied for their effectiveness in various
slices of networks: eMBB, URLLC, and mMTC. It can be seen from the results that
WOA performed the best: it reduced packet loss by 31% and delay by 6.3 ms; PSO
followed closely with a 30% packet loss reduction with a decrease of 6.1 ms in
delay. In most scenarios, ABC accomplished good results with a packet loss
reduction of 29% and a delay decrease of 6 ms in mMTC scenarios. These results
emphasize how selecting appropriate algorithms based on the intended network
slice is crucial for optimizing resource utilization and network efficiency. It
provides a quantitative framework for assessing and improving the reliability
and responsiveness of an LTE/5G network. It encourages more research in hybrid
optimization techniques and real-time adaptation mechanisms for further
improvements

</details>


### [9] [The Door to Policy Portability might be an IP Overlay](https://arxiv.org/abs/2510.04052)
*Behrooz Farkiani,Fan Liu,Patrick Crowley*

Main category: cs.NI

TL;DR: 提出了将网络策略与服务网格集成的方法，在可移植的基础设施无关方式下保护数据平面流量，实现从第3层到第7层的统一策略执行。


<details>
  <summary>Details</summary>
Motivation: 当前便携式服务网格虽然支持第4层到第7层策略执行，但仍依赖于基础设施特定的第3层网络策略。大多数Kubernetes集群未强制执行网络策略，且跨异构环境实现一致执行具有挑战性。

Method: 构建覆盖层第3层网络，通过将流量路由到特定策略执行点并使用授权密钥来强制执行第3层策略。使用Kubernetes和Istio进行原型实现。

Result: 在Kubernetes和Istio上的原型实现显示，该方法增加延迟小于1毫秒，能够实现与Kubernetes原生网络策略相当的复杂策略。

Conclusion: 该方法成功实现了基础设施无关的第3层到第7层策略集成执行，具有可移植性，可在服务环境外用于终端用户流量，提供端到端安全扩展覆盖。

Abstract: Portable service mesh implementations enable layer 4 to layer 7 policy
enforcement across diverse infrastructures, but they remain tied to
infrastructure-specific layer 3 network policies. Network policies enable
control over IP traffic flow regardless of whether traffic is authorized at the
application level. However, not all infrastructure supports enforcing them, and
achieving consistent enforcement across heterogeneous environments is
challenging. For example, studies have shown that the majority of Kubernetes
clusters do not enforce any network policies. We propose integrating network
policy enforcement with service meshes to protect data-plane traffic in a
portable, infrastructure-agnostic way. This enables developers to define
integrated layer 3 to layer 7 policies and ensure they are enforced across any
infrastructure. Additionally, due to its portability, our approach can be used
outside the service environment to enforce policies on end-user traffic and
provide an end-to-end secure extended overlay. Our solution builds an overlay
layer 3 network and enforces layer 3 policies by routing traffic through
specific policy enforcement points and utilizing authorization keys. We
prototyped our idea using Kubernetes and Istio, and show that while it adds
less than 1ms latency, it can implement complex policies comparable to
Kubernetes native network policies.

</details>


### [10] [Dynamic Adaptive Federated Learning for mmWave Sector Selection](https://arxiv.org/abs/2510.04183)
*Lucas Pacheco,Torsten Braun,Kaushik Chowdhury,Denis Rosário,Batool Salehi,Eduardo Cerqueira*

Main category: cs.NI

TL;DR: 提出eDAFL算法，通过动态分层和聚类联邦学习优化车联网波束选择，显著降低网络开销和延迟


<details>
  <summary>Details</summary>
Motivation: 传统波束选择方案需要大量搜索最高信号强度扇区，导致额外延迟和通信开销，需要更高效的解决方案

Method: 采用动态分层和聚类联邦学习，检测并选择机器学习模型中最重要层进行聚合，考虑集群内和集群间方法减少过拟合

Result: 在真实多模态数据集上测试，模型精度提升约6.76%，推理时间减少84.04%，模型大小最多减少52.20%

Conclusion: eDAFL算法在波束选择中表现出色，显著提升性能同时大幅降低资源消耗

Abstract: Beamforming techniques use massive antenna arrays to formulate narrow
Line-of-Sight signal sectors to address the increased signal attenuation in
millimeter Wave (mmWave). However, traditional sector selection schemes involve
extensive searches for the highest signal-strength sector, introducing extra
latency and communication overhead. This paper introduces a dynamic layer-wise
and clustering-based federated learning (FL) algorithm for beam sector
selection in autonomous vehicle networks called enhanced Dynamic Adaptive FL
(eDAFL). The algorithm detects and selects the most important layers of a
machine learning model for aggregation in the FL process, significantly
reducing network overhead and failure risks. eDAFL also considers intra-cluster
and inter-cluster approaches to reduce overfitting and increase the abstraction
level. We evaluate eDAFL on a real-world multi-modal dataset, demonstrating
improved model accuracy by approximately 6.76% compared to existing methods,
while reducing inference time by 84.04% and model size by up to 52.20%.

</details>


### [11] [Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins](https://arxiv.org/abs/2510.04346)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: 提出了一个环境感知的室内LoRaWAN路径损耗建模框架，通过多项式回归和环境协变量显著提高了预测精度，并使用混合分布建模阴影衰落，实现了99%包传输率下更低的衰落余量需求。


<details>
  <summary>Details</summary>
Motivation: 传统对数距离模型和正态阴影衰落假设在室内环境中受到结构和时变环境因素的影响而表现不佳，需要开发更精确的环境感知路径损耗模型。

Method: 使用多项式回归增强对数距离多墙模型，引入环境协变量（湿度、温度、CO2等），比较多种回归方法，通过方差分析和核密度估计评估预测因子相关性和阴影衰落分布。

Result: 多项式模型将交叉验证RMSE从8.07降至7.09 dB，R²从0.81提升至0.86。在99%包传输率下，环境感知多项式仅需25.7 dB衰落余量，而线性基线需要27.7-27.9 dB。

Conclusion: 该工作流程为室内物联网规划提供了部署就绪、可解释的可靠性控制方法，符合6G目标要求。

Abstract: Indoor LoRaWAN propagation is shaped by structural and time-varying context
factors, which challenge log-distance models and the assumption of log-normal
shadowing. We present an environment-aware, statistically disciplined path loss
framework evaluated using leakage-safe cross-validation on a 12-month campaign
in an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is
augmented with environmental covariates (relative humidity, temperature, carbon
dioxide, particulate matter, and barometric pressure), as well as the
signal-to-noise ratio. We compare multiple linear regression with regularized
variants, Bayesian linear regression, and a selective second-order polynomial
applied to continuous drivers. Predictor relevance is established using
heteroscedasticity-robust Type II and III analysis of variance and nested
partial F tests. Shadow fading is profiled with kernel density estimation and
non-parametric families, including Normal, Skew-Normal, Student's t, and
Gaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07
to 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are
non-Gaussian; a 3-component mixture captures a sharp core with a light, broad
tail. We convert accuracy into reliability by prescribing the fade margin as
the upper-tail quantile of cross-validated residuals, quantifying uncertainty
via a moving-block bootstrap, and validating on a held-out set. At 99% packet
delivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7
to 27.9 dB for linear baselines. This result presents a deployment-ready,
interpretable workflow with calibrated reliability control for indoor Internet
of Things planning, aligned with 6G targets.

</details>


### [12] [Rethinking HTTP API Rate Limiting: A Client-Side Approach](https://arxiv.org/abs/2510.04516)
*Behrooz Farkiani,Fan Liu,Patrick Crowley*

Main category: cs.NI

TL;DR: 该论文针对HTTP API配额限制下的客户端重试问题，提出了两种无需中央控制的自适应客户端算法ATB和AATB，通过推断系统拥塞来优化重试时机，显著减少了HTTP 429错误。


<details>
  <summary>Details</summary>
Motivation: 当多个独立客户端共享配额时，服务器端控制效率低下，客户端缺乏对其他客户端负载的可见性，导致重试尝试可能失败。现有的简单客户端策略（如指数退避）会造成过多的重试和显著成本。

Method: 设计了两种自适应客户端机制：ATB（离线方法，可通过服务工作者部署）和AATB（使用聚合遥测数据增强重试行为）。两种算法都通过推断系统拥塞来安排重试。

Result: 通过使用真实世界轨迹和合成数据集（最多100个客户端）的仿真表明，与指数退避相比，该算法将HTTP 429错误减少了高达97.3%，而完成时间的适度增加被错误减少所抵消。

Conclusion: 提出的自适应客户端算法在无需中央控制的情况下，仅依赖最小反馈就能显著改善HTTP配额限制下的重试效率，减少错误率并优化系统性能。

Abstract: HTTP underpins modern Internet services, and providers enforce quotas to
regulate HTTP API traffic for scalability and reliability. When requests exceed
quotas, clients are throttled and must retry. Server-side enforcement protects
the service. However, when independent clients' usage counts toward a shared
quota, server-only controls are inefficient; clients lack visibility into
others' load, causing their retry attempts to potentially fail. Indeed, retry
timing is important since each attempt incurs costs and yields no benefit
unless admitted. While centralized coordination could address this, practical
limitations have led to widespread adoption of simple client-side strategies
like exponential backoff. As we show, these simple strategies cause excessive
retries and significant costs. We design adaptive client-side mechanisms
requiring no central control, relying only on minimal feedback. We present two
algorithms: ATB, an offline method deployable via service workers, and AATB,
which enhances retry behavior using aggregated telemetry data. Both algorithms
infer system congestion to schedule retries. Through emulations with real-world
traces and synthetic datasets with up to 100 clients, we demonstrate that our
algorithms reduce HTTP 429 errors by up to 97.3% compared to exponential
backoff, while the modest increase in completion time is outweighed by the
reduction in errors.

</details>


### [13] [Impossible Cloud Network: A Decentralized Internet Infrastructure Layer](https://arxiv.org/abs/2510.04620)
*Siu Kei Chung,Francisco Carpio,Andrei Navoichyk,Siarhei Valasovich,Jordan Moore,Slobodan Sudaric-Hefner,Daniel Baker,Thomas Demoor,Maurizio Binello,Christian Kaul,Kai Wawrzinek*

Main category: cs.NI

TL;DR: Impossible Cloud Network (ICN) 通过多层去中心化基础设施解决互联网主权危机，在保持可扩展性、去中心化和安全性的同时，提供企业级硬件访问和可组合服务。


<details>
  <summary>Details</summary>
Motivation: 互联网面临主权危机，少数超大规模企业集中权力和数据，导致中心化、用户控制权丧失、审查风险和单点故障问题。现有Web3解决方案往往在区块链三难问题中牺牲某个要素。

Method: ICN创建多层去中心化基础设施：可组合服务层、企业级硬件资源层、透明的无许可HyperNode网络用于性能执行。通过战略性地解耦和去中心化每一层来实现目标。

Result: ICN提供了一个开放、高度可扩展的基础设施，确保数字主权，消除单点信任，实现服务可编程性，并为未来互联网提供解耦架构。

Conclusion: ICN通过其多层去中心化方法解决了当前互联网的中心化问题，为构建主权互联网提供了可行的技术路径。

Abstract: The internet faces a sovereignty crisis due to power concentration and data
growth among a few hyperscalers, leading to centralization and loss of user
control. This consolidation risks censorship and creates single points of
failure. While Web3 offers decentralized solutions, they often sacrifice either
scalability, decentralization, or security, which are key elements in the
blockchain trilemma. These solutions also struggle with limited access to
enterprise-grade hardware and frequently rely on centralized infrastructure.
The Impossible Cloud Network (ICN) addresses these issues by creating a
multi-tiered, decentralized infrastructure layer. ICN offers a composable
service layer, an enterprise-grade hardware resource layer, and a transparent,
permissionless HyperNode network for performance enforcement. By strategically
decoupling and decentralizing each layer, ICN aims to provide an open,
extensively scalable infrastructure that ensures digital sovereignty,
eliminates single points of trust, enables service programmability, and offers
a decoupled architecture for limitless possibilities in the future internet.

</details>


### [14] [Satellite Direct-to-Device from Low Earth Orbit: Techno-Economic Analysis of a Global Non-Terrestrial Network](https://arxiv.org/abs/2510.04651)
*Adnan Aijaz,Peizheng Li,Sajida Gufran*

Main category: cs.NI

TL;DR: 该论文提出了一个全面的技术经济分析框架，用于评估基于低地球轨道卫星的直接到设备系统，通过整合全球卫星星座模型、无线电传播特性、容量计算和成本模型，评估了三种不同的全球非地面网络架构方案。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星和卫星直接到设备技术的快速发展，需要评估这些服务的可行性、成本效益和盈利能力，以便利益相关者能够就投资、开发和部署策略做出明智决策。

Method: 开发了一个综合技术经济分析框架，整合了全球卫星星座模型、符合ITU-R建议的无线电传播模型、3GPP兼容的容量计算、全球人口数据和全面的成本模型，并评估了三种不同的全球非地面网络架构选项。

Result: 经济评估显示，全球卫星直接到设备服务可以以与地面服务相当的用户月成本提供，同时实现正的投资回报率，并且结果显示了Open RAN技术在实现成本效益卫星直接到设备服务方面的潜力。

Conclusion: 该论文提出的技术经济分析框架证明了全球卫星直接到设备服务在经济上的可行性，能够以具有竞争力的成本提供宽带服务，同时强调了Open RAN技术在这类系统中的成本效益优势。

Abstract: Low Earth orbit (LEO) satellites and satellite direct-to-device (D2D)
technology are at the heart of the next-generation global connectivity which
promises direct access to space-based broadband services for unmodified
3GPP-compliant handsets. With a rapidly evolving ecosystem, it is important to
evaluate the feasibility, cost-effectiveness, and profitability of these
services. By assessing the technological aspects as well as economic
implications, stakeholders can make informed decisions about investment,
development, and deployment strategies. This paper presents a comprehensive
techno-economic analysis (TEA) framework for evaluating LEO-based satellite D2D
systems. The framework integrates a global satellite constellation model, radio
propagation aspects including atmospheric and rainfall attenuation models
compliant with ITU-R recommendations, 3GPP-compliant capacity calculations,
realistic global population data, and an all-encompassing cost model accounting
for both capital and operational expenses associated with space and ground
segments. Further, the framework evaluates three different architectural
options for realizing a global non-terrestrial network (NTN) for satellite D2D
services. With an emphasis on reproducibility, the framework has been
implemented through significant enhancements to an open-source tool. The
economic assessment reveals that global satellite D2D services can be provided
at a monthly cost per subscriber which is comparable to terrestrial services
while achieving a positive return on investment (ROI). Moreover, the results
show the potential of Open RAN technology for realizing cost-effective
satellite D2D services.

</details>


### [15] [Evaluating UORA-Based Polling Mechanism for Latency-Sensitive Uplink Traffic in Wi-Fi Networks](https://arxiv.org/abs/2510.04731)
*Douglas Dziedzorm Agbeve,Andrey Belogaev,Chris Blondia,Jeroen Famaey*

Main category: cs.NI

TL;DR: Wi-Fi 6的Uplink OFDMA-based Random Access (UORA)通过随机接入机制改进上行调度，在密集网络环境中显著降低延迟，相比传统调度接入可减少40%以上延迟。


<details>
  <summary>Details</summary>
Motivation: 传统Wi-Fi 6上行调度需要轮询缓冲区状态报告，随着设备密度增加，这种方法效率低下且不可扩展。需要一种更高效的上行调度机制来支持延迟敏感数据流。

Method: 研究UORA特性如何改善延迟敏感数据流的可扩展性和延迟性能。通过ns-3仿真评估不同轮询策略的性能表现。

Result: 在密集部署的网络环境中，基于UORA的轮询优于其他方案；在高度稀疏和零星流量条件下，UORA轮询相比调度接入OFDMA可减少40%以上的延迟。

Conclusion: UORA通过平衡协调性和可扩展性，实现了高效的上行调度，同时能够机会性地识别未调度STA的缓冲流量，特别适合密集网络环境。

Abstract: IEEE 802.11ax (Wi-Fi 6) introduced Orthogonal Frequency Division Multiple
Access (OFDMA), which enables simultaneous transmissions through centralized
resource allocation. However, effective uplink scheduling requires the Access
Point (AP) to identify which stations (STAs) have data to transmit. This
typically necessitates polling for buffer status reports, a process that
becomes increasingly inefficient and unscalable with growing device density. In
this paper, we study how the Uplink OFDMA-based Random Access (UORA) feature
improves the scalability and delay experienced by latency-sensitive data
streams. We show that UORA enables efficient uplink scheduling while
opportunistically identifying buffered traffic from unscheduled STAs, striking
a balance between coordination and scalability. Performance evaluation of
different polling strategies is done by means of simulation in ns-3. The
results indicate that UORA-based polling outperforms alternative schemes in
densely deployed network environments with heterogeneous uplink traffic
patterns. Furthermore, under highly sparse and sporadic traffic conditions,
UORA-based polling yields over 40% delay reduction compared to Scheduled Access
(SA) OFDMA.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: 提出了WAREX框架，用于在现有基准测试中评估浏览器LLM代理在真实网络环境下的可靠性，发现引入网络不稳定性和安全威胁后，代理性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在受控环境中评估LLM代理性能，但真实网络环境存在不稳定性、安全威胁和网站修改等问题，需要评估代理在这些挑战下的可靠性。

Method: 开发WAREX框架，在WebArena、WebVoyager和REAL三个流行基准测试中引入网络不稳定性和安全威胁等真实环境因素。

Result: 引入WAREX后，任务成功率显著下降，表明当前最先进代理的鲁棒性有限。

Conclusion: 真实网络环境中的不稳定性和安全威胁对LLM代理性能有重大影响，需要在基准测试中考虑这些因素来准确评估代理可靠性。

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [17] [Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints](https://arxiv.org/abs/2510.03377)
*Ahmed Missaoui,Cemalettin Ozturk,Barry O'Sullivan*

Main category: cs.AI

TL;DR: 该研究针对带有阻塞约束的混合流水车间调度问题，提出了多目标优化方法，同时最小化完工时间和能耗，开发了改进的迭代帕累托贪婪算法来解决大规模实例。


<details>
  <summary>Details</summary>
Motivation: 不可再生能源稀缺、地缘政治问题、价格上涨和气候变化影响迫使制造业寻求更节能的解决方案。混合流水车间调度作为制造业中常见的生产环境，需要同时优化完工时间和能耗这两个冲突目标。

Method: 首先构建了新颖的多目标混合整数规划模型，采用增强的epsilon约束方法寻找帕累托最优解。同时开发了改进的迭代帕累托贪婪算法来处理大规模实例。

Result: 通过小、中、大规模实例进行基准测试，并与两种知名算法进行比较，计算结果表明所提方法具有有效性。

Conclusion: 提出的多目标优化方法能够有效解决混合流水车间调度中的完工时间和能耗优化问题，特别是在处理大规模实例时表现出良好的性能。

Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its
supply, increasing prices, and the impact of climate change, force the global
economy to develop more energy-efficient solutions for their operations. The
Manufacturing sector is not excluded from this challenge as one of the largest
consumers of energy. Energy-efficient scheduling is a method that attracts
manufacturing companies to reduce their consumption as it can be quickly
deployed and can show impact immediately. In this study, the hybrid flow shop
scheduling problem with blocking constraint (BHFS) is investigated in which we
seek to minimize the latest completion time (i.e. makespan) and overall energy
consumption, a typical manufacturing setting across many industries from
automotive to pharmaceutical. Energy consumption and the latest completion time
of customer orders are usually conflicting objectives. Therefore, we first
formulate the problem as a novel multi-objective mixed integer programming
(MIP) model and propose an augmented epsilon-constraint method for finding the
Pareto-optimal solutions. Also, an effective multi-objective metaheuristic
algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large
instances in reasonable time. Our proposed methods are benchmarked using small,
medium, and large-size instances to evaluate their efficiency. Two well-known
algorithms are adopted for comparing our novel approaches. The computational
results show the effectiveness of our method.

</details>


### [18] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: 该研究系统评估了10个大型语言模型的自我识别能力，发现模型普遍无法识别自己生成的文本，性能仅略高于随机猜测，且存在对GPT和Claude家族的强烈偏见。


<details>
  <summary>Details</summary>
Motivation: 针对AI系统是否具备自我识别能力这一存在争议的问题，研究者希望建立一个可重复更新的系统评估框架，这对AI安全和心理分析具有重要意义。

Method: 通过两个任务评估模型：二元自我识别（判断文本是否为自己生成）和精确模型预测（识别文本的具体来源模型）。评估了10个当代大型语言模型。

Result: 结果显示模型自我识别能力普遍失败，只有4/10的模型能正确识别自己生成的文本，性能很少超过随机水平。模型表现出对GPT和Claude家族的强烈偏见，并显示出对模型存在层次结构的认知。

Conclusion: 研究强调了当前AI系统缺乏适当的自我意识，这对AI安全构成挑战，并指出了开发适当AI自我意识的重要性。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [19] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: 提出了ContraGen基准框架，专门针对企业领域设计，用于评估RAG系统中矛盾检测能力，通过生成包含矛盾的企业风格文档来系统评估文档内和跨文档一致性。


<details>
  <summary>Details</summary>
Motivation: 现有矛盾检测基准仅限于句子级分析，无法捕捉企业文档（如合同、财务报告、合规文件）的复杂性，而RAG系统中的证据矛盾会导致不可靠输出，这在企业环境中特别危险。

Method: 结合自动化矛盾挖掘和人工验证，生成包含嵌入矛盾的企业风格合成文档，建立企业流程中常见矛盾类型的分类体系，支持受控创建自矛盾和成对矛盾。

Result: 开发了矛盾感知的检索评估流程，嵌入人工监督以反映领域特定判断复杂性，为企业信息检索应用建立更可信赖的RAG系统基础。

Conclusion: 该工作为企业信息检索应用中检测和解决矛盾提供了基础，对降低风险和确保合规性至关重要。

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [20] [A Qualitative Comparative Evaluation of Cognitive and Generative Theories](https://arxiv.org/abs/2510.03453)
*Paul S. Rosenbloom*

Main category: cs.AI

TL;DR: 本文提出了一个评估认知架构和生成式神经网络架构理论的定性比较框架，以解决这两种架构理论评估面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 认知架构和生成式神经网络架构的理论评估都面临显著挑战，需要开发有效的评估方法来促进这些理论的发展。

Method: 采用广泛的理论评估视角，对面向全脑的认知架构和生成式架构及其完整系统进行全面的定性比较。

Result: 开发了一个宽泛的定性比较框架，能够系统性地评估不同类型的认知架构理论。

Conclusion: 通过采用广泛的理论评估视角，可以有效应对认知架构和生成式神经网络架构理论评估的挑战，为这类理论的评估提供了可行的方法。

Abstract: Evaluation is a critical activity associated with any theory. Yet this has
proven to be an exceptionally challenging activity for theories based on
cognitive architectures. For an overlapping set of reasons, evaluation can also
be challenging for theories based on generative neural architectures. This dual
challenge is approached here by leveraging a broad perspective on theory
evaluation to yield a wide-ranging, albeit qualitative, comparison of
whole-mind-oriented cognitive and generative architectures and the full systems
that are based on these architectures.

</details>


### [21] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: 提出了一种通过将自然语言计划转换为Kripke结构和线性时序逻辑(LTL)来评估计划与预期行为对齐性的新框架，使用大语言模型进行模型检查。


<details>
  <summary>Details</summary>
Motivation: 需要系统评估自然语言计划与其预期行为之间的对齐性，确保计划执行结果符合预期。

Method: 使用大语言模型将自然语言计划转换为Kripke结构和LTL公式，然后在PlanBench数据集上进行模型检查验证。

Result: GPT-5在分类任务上表现出色(F1分数96.3%)，几乎总能生成语法完美的形式化表示，可作为保证。

Conclusion: 框架在生成语法正确的形式化模型方面表现优异，但语义完美的形式化模型合成仍是未来研究方向。

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [22] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: 提出了PolicyGuardBench基准和PolicyGuard-4B模型，用于检测网络代理轨迹中的策略违规，支持跨域泛化和小规模高效推理。


<details>
  <summary>Details</summary>
Motivation: 自主网络代理需要在外部策略约束下生成长轨迹，但现有研究很少关注这些轨迹是否遵守策略，以及违规行为在不同上下文中的持续性。

Method: 从多样化代理运行中生成广泛策略集，创建约60k个带违规标签的示例，包括子域内和跨子域配对，并设计了基于前缀的违规检测任务。

Result: 训练的PolicyGuard-4B模型在所有任务上表现出强检测准确性，能跨域泛化并在未见过的设置中保持高精度。

Conclusion: PolicyGuardBench和PolicyGuard-4B为研究网络代理轨迹的策略合规性提供了首个综合框架，证明小规模下可实现准确且可泛化的护栏机制。

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [23] [Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning](https://arxiv.org/abs/2510.04488)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.AI

TL;DR: MACI是一个多智能体辩论控制器，通过信息质量门控和行为调度机制，将辩论转化为可预算、可测量且可证明终止的过程，在临床诊断和新闻偏见任务中提高准确性和校准度，同时减少token使用。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论存在计算资源浪费问题，包括固定对抗立场、无深思熟虑的聚合或基于启发式的停止机制。

Method: MACI采用双独立调节机制：信息调节器按质量门控证据，行为调节器从探索到整合调度争议性。使用跨家族LLM评判器作为保守软权重和停止信号。

Result: 在临床诊断和新闻偏见任务中，MACI提高了准确性和校准度，同时减少了token使用，并将剩余不确定性转化为精确的RAG计划。

Conclusion: MACI将辩论转化为预算感知、可测量且可证明终止的控制器，具有非递增分散性和可证明终止的理论保证。

Abstract: Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

</details>


### [24] [OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows](https://arxiv.org/abs/2510.03506)
*John Nguyen,Marton Havasi,Tariq Berrada,Luke Zettlemoyer,Ricky T. Q. Chen*

Main category: cs.AI

TL;DR: OneFlow是首个非自回归多模态模型，支持可变长度和并发混合模态生成，通过插入式编辑流和流匹配技术实现文本图像并发合成，在生成和理解任务上优于自回归基线，训练FLOPs减少50%。


<details>
  <summary>Details</summary>
Motivation: 解决自回归模型在文本和图像生成之间强制因果顺序的限制，实现更灵活的并发多模态生成。

Method: 结合插入式编辑流处理离散文本标记，使用流匹配处理图像潜在表示，采用分层采样优先内容而非语法。

Result: 在1B到8B模型规模上，OneFlow在生成和理解任务上均优于自回归基线，训练FLOPs减少高达50%，超越了自回归和基于扩散的方法。

Conclusion: OneFlow解锁了并发生成、迭代优化和类自然推理生成等新能力，为非自回归多模态生成提供了有效解决方案。

Abstract: We present OneFlow, the first non-autoregressive multimodal model that
enables variable-length and concurrent mixed-modal generation. Unlike
autoregressive models that enforce rigid causal ordering between text and image
generation, OneFlow combines an insertion-based Edit Flow for discrete text
tokens with Flow Matching for image latents. OneFlow enables concurrent
text-image synthesis with hierarchical sampling that prioritizes content over
grammar. Through controlled experiments across model sizes from 1B to 8B, we
demonstrate that OneFlow outperforms autoregressive baselines on both
generation and understanding tasks while using up to 50% fewer training FLOPs.
OneFlow surpasses both autoregressive and diffusion-based approaches while
unlocking new capabilities for concurrent generation, iterative refinement, and
natural reasoning-like generation.

</details>


### [25] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: 本文研究了测试时扩展（test-time scaling）对Transformer模型推理能力的影响，发现在线性回归任务中，增加测试时计算可以降低训练所需的上下文长度，但前提是训练数据包含足够的相关技能。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时扩展通过生成长思维链显著提升了大型语言模型的推理能力，但长思维链在什么训练条件下出现以及何时能提升性能仍不清楚。本文旨在从理论上解释这些现象。

Method: 通过分析在上下文权重预测任务上训练的Transformer模型，使用理论分析和实验验证相结合的方法，研究测试时扩展对性能的影响。

Result: 研究发现：1）在固定测试误差下，增加测试时计算可以减少训练提示中的上下文长度；2）如果训练数据缺乏解决下游任务所需的技能，增加测试时计算反而会损害性能；3）任务难度可以通过特征协方差矩阵的最小特征值来表征。

Conclusion: 训练数据需要包含多样化、相关且具有一定难度的任务集合，才能最大化测试时扩展的效果。这一发现在大型非线性Transformer架构的实验中得到验证。

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [26] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: 提出了跨模态偏好引导（CPS）攻击方法，通过联合优化视觉和文本通道的不可察觉修改，在现实黑盒威胁设置下显著提升对VLM代理的偏好操纵效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么假设强白盒访问，要么使用不切实际的设置，缺乏在现实攻击者能力下的有效偏好操纵方法。

Method: CPS联合优化物品视觉和自然语言描述的不可察觉修改，利用CLIP可迁移图像扰动和RLHF诱导的语言偏见来引导代理决策。

Result: 在GPT-4.1、Qwen-2.5VL和Pixtral-Large等先进VLM上的评估显示，CPS在所有模型上都显著优于基线方法，同时保持70%更低的检测率。

Conclusion: 这些发现强调了随着智能系统在社会中扮演越来越重要的角色，迫切需要开发鲁棒的防御机制。

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [27] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: 提出了一种基于互信息树搜索（MITS）的新框架，使用点互信息评分函数来指导LLM推理，无需昂贵的前瞻模拟即可实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 解决树搜索方法在LLM推理中存在的两个问题：难以对中间推理步骤进行即时可靠的量化评估，以及广泛路径探索带来的计算成本高昂。

Method: 使用点互信息（PMI）作为评分函数进行步骤评估，通过beam搜索扩展搜索树，采用基于熵的动态采样策略自适应分配计算资源，最后使用加权投票方案结合PMI分数和预测共识进行最终预测。

Result: 在多个推理基准测试中，MITS始终超越基线方法，在保持计算效率的同时实现了优越的推理性能。

Conclusion: MITS建立了一个基于信息论原则的高效LLM推理框架，为测试时推理提供了原则性和高效的方法。

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [28] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: 扩散大语言模型存在<eos>溢出问题：随着序列长度增加，响应反而变短，导致提前终止或退化为<eos>令牌流。作者提出Rainbow Padding方法，用循环的不同填充令牌替换重复的<eos>占位符来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 指令调优的扩散大语言模型存在一个关键漏洞：<eos>溢出问题，即随着分配的序列长度增加，响应反而变得更短，导致提前终止或退化为<eos>令牌流。虽然实践中已注意到此问题，但尚未得到系统分析。

Method: 提出Rainbow Padding方法，用循环的不同填充令牌替换重复的<eos>占位符，分散概率质量并打破<eos>的主导地位。该方法可高效集成到现有指令调优模型中：在少量数据上使用LoRA微调单个epoch即可获得显著改进。

Result: 实验表明Rainbow Padding显著提高了长度鲁棒性和输出质量，仅需七个填充令牌就足以防止提前终止。该方法在实际应用中具有高度实用性。

Conclusion: Rainbow Padding是解决扩散大语言模型中<eos>溢出问题的简单有效方法，通过打破<eos>令牌的主导地位来防止提前终止，同时保持模型性能。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [29] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 提出了一个面向目标的多智能体系统评估框架，引入目标成功率(GSR)和失败根因分类(RCOF)，通过基于LLM的评估系统实现可解释、数据高效的多轮对话评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在轮次层面评估聊天机器人交互，无法判断用户总体目标是否达成，需要更全面的目标导向评估框架。

Method: 基于用户目标分割对话，使用教师LLM结合领域专家定义的目标和质量标准进行评估，通过"思考标记"生成可解释的推理过程。

Result: 在企业环境中应用该框架评估AIDA系统，目标成功率从63%提升到79%。

Conclusion: 该框架具有通用性，通过详细的缺陷分类提供可操作的见解，能够诊断整体成功率、识别关键失败模式并指导系统改进。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [30] [H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis](https://arxiv.org/abs/2510.03700)
*Seungseop Lim,Gibaeg Kim,Hyunkyung Lee,Wooseok Han,Jean Seo,Jaehyo Yoo,Eunho Yang*

Main category: cs.AI

TL;DR: 提出了H-DDx分层评估框架，用于更准确地评估大语言模型在鉴别诊断中的表现，相比传统平面指标能更好地反映临床相关性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在鉴别诊断评估中主要依赖Top-k准确率等平面指标，无法区分临床相关的近似错误和诊断上相距较远的错误，需要更符合临床实际需求的评估方法。

Method: H-DDx框架采用检索和重排序流程将自由文本诊断映射到ICD-10代码，并应用分层度量来奖励与真实诊断密切相关的预测。

Result: 在22个领先模型的基准测试中，传统平面指标低估了性能，忽略了临床有意义的输出；领域专业化的开源模型表现突出；框架揭示了分层错误模式，显示LLM即使错过精确诊断也常能正确识别更广泛的临床背景。

Conclusion: H-DDx框架提供了更临床相关的评估方法，增强了可解释性，有助于更准确地评估LLM在医疗诊断中的实际价值。

Abstract: An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

</details>


### [31] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 该论文探讨如何将多模态基础模型提升为世界模型，通过增强其推理能力和生成能力，使其能够进行反事实推理、时空理解、可控生成等复杂任务。


<details>
  <summary>Details</summary>
Motivation: 受人类多感官理解世界的启发，当前的多模态基础模型缺乏作为有效世界模型的关键能力，如反事实推理、动态模拟、时空信息理解和可控生成等。

Method: 通过判别性任务增强推理能力，引入因果推理、反事实思维和时空推理等结构化推理技能；开发结构化可控生成框架，利用场景图、多模态条件和多模态对齐策略指导生成过程。

Result: 提出了增强多模态基础模型推理和生成能力的方法，实现了从图像到视频的可控4D生成，支持交互式、可编辑和可变形的对象合成。

Conclusion: 通过系统性地增强多模态基础模型的推理和生成能力，可以缩小其与世界模型之间的差距，为构建更智能的多模态AI系统奠定基础。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [32] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: OptAgent框架使用多智能体模拟和遗传算法来优化电商查询改写，通过模拟购物顾客的LLM智能体作为动态奖励信号，相比原始查询提升21.98%，优于Best-of-N基线3.36%。


<details>
  <summary>Details</summary>
Motivation: LLM在可验证任务中表现优异，但在缺乏单一正确答案的主观任务（如电商查询改写）中部署困难，需要可靠评估方法。

Method: 结合多智能体模拟和遗传算法，使用多个模拟购物顾客的LLM智能体作为动态奖励信号，其平均得分作为进化算法的适应度函数来迭代优化用户查询。

Result: 在1000个真实电商查询的五类测试中，相比原始查询平均提升21.98%，优于Best-of-N基线3.36%。

Conclusion: OptAgent框架有效解决了主观任务的评估挑战，为电商查询改写提供了可靠的优化方案。

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [33] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: 提出GuidedSampling推理算法，通过分离探索和生成阶段来增加解决方案多样性，相比重复采样在pass@50上平均提升21.6%性能


<details>
  <summary>Details</summary>
Motivation: 重复采样算法在推理时虽然能提升模型性能，但经常生成冗余样本，缺乏多样性，无法产生不同的解决方案

Method: GuidedSampling将推理过程分为两个阶段：探索阶段识别多个可用的解决概念，生成阶段应用特定概念提供最终解决方案

Result: 在多个基准测试中，GuidedSampling相比重复采样在pass@50上平均提升21.6%；使用GuidedSampling轨迹训练的模型在pass@5上平均提升9.7%，每个实例的平均概念数从1.67增加到3.03

Conclusion: GuidedSampling通过分离探索和生成阶段有效提高了解决方案的多样性，显著优于传统的重复采样方法

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [34] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: 该论文研究了具有大型策略空间的游戏，提出了隐藏游戏问题，并开发了能够发现和利用隐藏结构的遗憾最小化算法，实现了最优的外部遗憾和交换遗憾界限。


<details>
  <summary>Details</summary>
Motivation: 受AI对齐和语言游戏挑战的启发，研究当每个玩家存在未知策略子集持续产生更高奖励时的隐藏游戏问题，探索能否设计高效算法来发现和利用这种隐藏结构。

Method: 开发了一种遗憾最小化技术的组合方法，通过结合外部遗憾和交换遗憾最小化，利用隐藏游戏结构提高计算效率。

Result: 该方法实现了最优的外部遗憾和交换遗憾界限，能够快速收敛到隐藏子游戏中的相关均衡。

Conclusion: 肯定地回答了研究问题，证明可以设计高效算法来发现和利用隐藏游戏结构，在保持一般理性的同时实现子游戏均衡。

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [35] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 小型语言模型（1-20B参数）在代理任务中表现足够且常优于大模型，通过引导解码、严格JSON Schema输出和验证器优先的工具执行，能以10-100倍更低的成本实现类似或更好的工具使用、函数调用和RAG性能。


<details>
  <summary>Details</summary>
Motivation: 针对代理工作负载中需要模式约束准确性的场景，而非开放式生成，小型语言模型在成本、延迟和能耗方面具有显著优势，但需要系统化方法来弥补与大模型的能力差距。

Method: 采用SLM默认、LLM回退系统，结合不确定性感知路由和验证器级联；使用引导解码库（XGrammar、Outlines）、严格JSON Schema输出、验证器优先工具执行；提出工程指标如CPS、模式有效性率等。

Result: 小型语言模型在工具使用、函数调用和RAG任务上能够匹配或超越大模型，同时实现10-100倍更低的token成本、更好的延迟和能耗表现。

Conclusion: 为构建快速、廉价且可靠的代理系统提供了实用蓝图，默认使用小型语言模型，同时保留针对特定场景的大模型回退机制。

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [36] [Algorithm Generation via Creative Ideation](https://arxiv.org/abs/2510.03851)
*Ruiying Ma,Chieh-Jan Mike Liang,Yanjie Gao,Francis Y. Yan*

Main category: cs.AI

TL;DR: MetaMuse框架通过三个自我反思原则解决LLM在算法生成中的创意不足问题，在缓存替换和在线装箱问题上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 系统算法设计面临解空间不连续的挑战，现有LLM偏向通用启发式方法而缺乏创造性，需要突破这一局限

Method: 提出MetaMuse框架，基于三个原则：(1)在可测量的性能空间而非抽象想法空间量化解决方案多样性和有用性；(2)通过外部刺激而非内部随机性引导构思；(3)使用路径点推理而非自由形式的思维链构建可执行解决方案

Result: 在缓存替换问题上减少缓存缺失达35.76%，在在线装箱问题上减少容器使用达30.93%

Conclusion: MetaMuse能够为关键系统问题生成高性能解决方案，证明了其在算法生成中的有效性

Abstract: Designing system algorithms remains challenging, where the discontinuous
nature of the solution space often forces system engineers to rely on generic
heuristics at the expense of performance. We study whether LLMs can practically
drive algorithm generation, and find that they are biased towards well-known
generic designs, rather than making the creative leaps needed to navigate the
discontinuous solution space. To address this limitation, we introduce
MetaMuse, a framework for creative ideation built on three self-reflection
principles: (1) quantifying solution diversity and usefulness in measurable
performance space, rather than abstract idea space, (2) steering ideation
through external stimuli, rather than internal randomness, and (3) constructing
executable solutions using waypoint reasoning, rather than free-form
chain-of-thought. Extensive evaluation shows that MetaMuse can generate
high-performing solutions for two critical problems at a global cloud provider:
cache replacement (reducing cache misses by up to 35.76%) and online bin
packing (reducing bin usage by up to 30.93%).

</details>


### [37] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 提出了一种结合LLM和XAI代理的异常检测方法，用于关键IoT系统，在智能电网和医疗场景中表现出优于传统模型的检测精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法在动态、高维、数据不完整的IoT环境中存在局限，需要自适应智能系统来提升检测能力。

Method: 使用LLM支持的上下文推理方法和XAI代理，结合注意力机制、内存缓冲和语义分析来发现隐藏模式和检测数据流不一致。

Result: 新方法在检测精度、误报率、可读性和响应速度方面显著优于现有模型，在真实场景模拟中表现出良好的适应性和可靠性。

Conclusion: LLM增强的异常检测方法在IoT环境中具有显著优势，为未来异常检测任务提供了可行的解决方案。

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [38] [Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation](https://arxiv.org/abs/2510.03863)
*Arina Kharlamova,Bowei He,Chen Ma,Xue Liu*

Main category: cs.AI

TL;DR: 提出了一种名为Spatial CAPTCHA的新型人类验证框架，利用人类与多模态大语言模型在空间推理能力上的根本差异来防御自动化攻击。


<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA依赖文本识别或2D图像理解，但多模态大语言模型的进步已削弱其有效性，需要开发基于更复杂认知能力的验证机制。

Method: 采用程序化生成管道，创建需要几何推理、视角转换、遮挡处理和心理旋转等空间推理能力的动态问题，结合基于约束的难度控制和自动化正确性验证。

Result: 在Spatial-CAPTCHA-Bench基准测试中，人类表现远超10个最先进的多模态大语言模型，最佳模型仅达到31.0%的Pass@1准确率，且优于Google reCAPTCHA。

Conclusion: Spatial CAPTCHA不仅作为有效的安全机制，还能作为评估AI空间推理能力的诊断工具，填补了传统CAPTCHA的防御漏洞。

Abstract: Online services rely on CAPTCHAs as a first line of defense against automated
abuse, yet recent advances in multi-modal large language models (MLLMs) have
eroded the effectiveness of conventional designs that focus on text recognition
or 2D image understanding. To address this challenge, we present Spatial
CAPTCHA, a novel human-verification framework that leverages fundamental
differences in spatial reasoning between humans and MLLMs. Unlike existing
CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern
AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,
perspective-taking, occlusion handling, and mental rotation. These skills are
intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The
system employs a procedural generation pipeline with constraint-based
difficulty control, automated correctness verification, and human-in-the-loop
validation to ensure scalability, robustness, and adaptability. Evaluation on a
corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly
outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%
Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,
which confirms its effectiveness as both a security mechanism and a diagnostic
tool for spatial reasoning in AI.

</details>


### [39] [Rare Text Semantics Were Always There in Your Diffusion Transformer](https://arxiv.org/abs/2510.03886)
*Seil Kang,Woojung Han,Dayun Ju,Seong Jae Hwang*

Main category: cs.AI

TL;DR: 提出一种无需额外训练、数据或外部模块的方法，通过在联合注意力块前扩大文本标记嵌入的表示范围，使多模态扩散变换器能够生成罕见语义内容。


<details>
  <summary>Details</summary>
Motivation: 当前先进的多模态扩散变换器在处理用户富有想象力或罕见的提示时仍然表现不佳，因为这些概念在预训练中过于稀缺，难以形成强表征。

Method: 在联合注意力机制前，通过数学方法扩大文本标记嵌入的表示范围，增加其方差，从而增强罕见语义的表征能力。

Result: 该方法能有效在多模态扩散变换器中浮现罕见语义，并在文本到图像、文本到视频和文本驱动图像编辑等任务中具有良好泛化性。

Conclusion: 该方法揭示了生成模型能够展现用户意图中原本隐藏但准备浮现的语义，为提升模型对罕见概念生成能力提供了简单有效的解决方案。

Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion
Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim
for exceptional visual fidelity. As these models advance, users continually
push the boundary with imaginative or rare prompts, which advanced models still
falter in generating, since their concepts are often too scarce to leave a
strong imprint during pre-training. In this paper, we propose a simple yet
effective intervention that surfaces rare semantics inside MM-DiTs without
additional training steps, data, denoising-time optimization, or reliance on
external modules (e.g., large language models). In particular, the
joint-attention mechanism intrinsic to MM-DiT sequentially updates text
embeddings alongside image embeddings throughout transformer blocks. We find
that by mathematically expanding representational basins around text token
embeddings via variance scale-up before the joint-attention blocks, rare
semantics clearly emerge in MM-DiT's outputs. Furthermore, our results
generalize effectively across text-to-vision tasks, including text-to-image,
text-to-video, and text-driven image editing. Our work invites generative
models to reveal the semantics that users intend, once hidden yet ready to
surface.

</details>


### [40] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 开发了一个游戏化可解释AI系统，用于咖啡领域的道德消费决策，结合康德主义和功利主义伦理框架提供实时解释。


<details>
  <summary>Details</summary>
Motivation: 帮助消费者在购买咖啡时做出更道德的决策，通过结合不同伦理视角提供透明的决策依据。

Method: 系统包含六个回合，每回合三个选项。使用康德主义模块检测规则违反，功利主义模块通过多标准聚合评分，元解释器处理伦理冲突。

Result: 开发了完整的系统配置、可审计的政策轨迹和交互式用户界面，支持道德消费决策。

Conclusion: 该系统成功整合了不同伦理框架，为消费者提供了透明、可解释的道德决策工具。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [41] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: QRLLM是一个用于评估LLM在多轮对话中灾难性风险的概率认证框架，通过马尔可夫过程和查询图建模对话分布，提供统计保证的风险边界。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖固定攻击提示序列，缺乏统计保证，无法扩展到多轮对话的广阔空间，难以充分揭示LLM的灾难性响应风险。

Method: 将多轮对话建模为查询序列的概率分布，使用马尔可夫过程和查询图表示对话流程，定义随机节点、图路径、自适应拒绝等实用分布来量化风险。

Result: 这些分布能够揭示前沿模型中的重大灾难性风险，最差模型的认证下界高达70%，表明前沿LLMs急需改进安全训练策略。

Conclusion: QRLLM框架为LLM在多轮对话中的灾难性风险提供了具有统计保证的认证方法，揭示了当前前沿模型存在的严重安全隐患。

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [42] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: C^2-Eval是一个用于统一评估基础模型创造力的综合基准，区分收敛性创造力和发散性创造力，基于有用性、原创性和惊喜性三个标准进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有创造力评估框架碎片化，缺乏基于成熟理论的系统方法，需要建立统一的评估标准来衡量基础模型的创造力。

Method: 提出C^2-Eval基准，区分收敛性创造力（有约束解的任务）和发散性创造力（开放式任务），使用基于社会科学理论的有用性、原创性和惊喜性三个细粒度标准进行评估。

Result: 通过对领先专有和开源模型的广泛实验，分析了它们在创造力能力上的权衡，揭示了当前基础模型在追求创造性机器智能方面的优势和挑战。

Conclusion: C^2-Eval是审视创造性AI发展格局的有效工具，能够系统评估基础模型的创造力表现。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [43] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 该论文提出了Zephyrus框架，将天气预报基础模型与大型语言模型结合，创建能够通过代码环境与气象数据交互的智能体系统，并建立了相应的基准测试ZephyrusBench。


<details>
  <summary>Details</summary>
Motivation: 现有的天气预报基础模型缺乏语言推理能力，而大型语言模型无法处理高维气象数据，需要构建能够桥接这两种能力的智能体框架。

Method: 构建了基于Python代码的环境ZephyrusWorld，包含WeatherBench 2数据集接口、地理查询、天气预报和气候模拟等工具，设计了多轮LLM天气智能体Zephyrus，通过对话反馈循环迭代分析数据。

Result: 在ZephyrusBench基准测试中，Zephyrus智能体在正确性上比纯文本基线高出35个百分点，但在更困难任务上表现相似，表明基准具有挑战性。

Conclusion: 该框架成功结合了天气预报模型和语言模型的能力，为交互式天气科学研究提供了新方法，同时指出了未来改进的方向。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [44] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 这篇论文提出了首个全面的数据科学智能体分类法，系统分析了45个系统在数据科学全流程六个阶段的表现，并识别出三个关键趋势：大多数系统侧重探索分析和建模，而忽视业务理解和部署监控；多模态推理和工具编排仍是挑战；90%以上缺乏明确的信任安全机制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，出现了能够自动化数据科学工作流程的新型AI智能体，但缺乏对这些系统的系统性分类和分析。本文旨在填补这一空白，为数据科学智能体的发展提供指导。

Method: 采用生命周期对齐的分类法，将45个数据科学智能体系统映射到数据科学的六个阶段，并从五个交叉设计维度进行标注：推理规划风格、模态集成、工具编排深度、学习对齐方法、信任安全治理机制。

Result: 分析发现数据科学智能体在探索分析、可视化和建模阶段表现较强，但在业务理解、部署和监控方面存在明显不足；多模态推理和工具编排仍是主要挑战；绝大多数系统缺乏明确的信任和安全机制。

Conclusion: 提出了未来研究方向，包括对齐稳定性、可解释性、治理和鲁棒评估框架等挑战，旨在指导开发更稳健、可信、低延迟、透明和广泛可访问的数据科学智能体。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [45] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: 提出MedLog协议，用于临床AI的事件级日志记录，类似于计算机系统中的syslog，旨在解决医疗AI缺乏标准使用记录的问题。


<details>
  <summary>Details</summary>
Motivation: 医疗AI快速发展但缺乏标准化的使用记录机制，难以追踪模型使用情况、评估真实性能、检测不良事件或纠正偏差，需要类似syslog的通用日志协议。

Method: 设计包含9个核心字段的MedLog记录格式：header、model、user、target、inputs、artifacts、outputs、outcomes、feedback，支持风险采样、生命周期感知保留策略和写后缓存。

Result: MedLog协议提供了结构化的临床AI活动记录标准，能够捕获复杂工作流程的详细轨迹，促进新数据库和分析工具的开发。

Conclusion: MedLog为实现医疗AI的持续监控、审计和迭代改进奠定了基础，有望催生新型数字流行病学方法。

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [46] [FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.04040)
*Xu Shen,Song Wang,Zhen Tan,Laura Yao,Xinyu Zhao,Kaidi Xu,Xin Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出了FaithCoT-Bench基准，用于检测LLM中思维链(CoT)的不忠实性，包含1000多个轨迹和300多个不忠实实例，评估了11种检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示CoT往往不能忠实反映模型的内部推理过程，但在实例层面判断特定轨迹是否忠实仍是一个实际挑战。

Method: 建立统一基准FaithCoT-Bench，包含FINE-CoT数据集（专家标注的1000+轨迹），将不忠实性检测定义为判别决策问题，并系统评估11种代表性检测方法。

Result: 揭示了现有检测方法的优缺点，发现在知识密集型领域和更先进模型中检测挑战更大。

Conclusion: FaithCoT-Bench为实例级CoT忠实性检测建立了首个全面基准，为LLM中更可解释和可信的推理研究奠定基础。

Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)
prompting to improve problem-solving and provide seemingly transparent
explanations. However, growing evidence shows that CoT often fail to faithfully
represent the underlying reasoning process, raising concerns about their
reliability in high-risk applications. Although prior studies have focused on
mechanism-level analyses showing that CoTs can be unfaithful, they leave open
the practical challenge of deciding whether a specific trajectory is faithful
to the internal reasoning of the model. To address this gap, we introduce
FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness
detection. Our framework establishes a rigorous task formulation that
formulates unfaithfulness detection as a discriminative decision problem, and
provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an
expert-annotated collection of over 1,000 trajectories generated by four
representative LLMs across four domains, including more than 300 unfaithful
instances with fine-grained causes and step-level evidence. We further conduct
a systematic evaluation of eleven representative detection methods spanning
counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical
insights that clarify the strengths and weaknesses of existing approaches and
reveal the increased challenges of detection in knowledge-intensive domains and
with more advanced models. To the best of our knowledge, FaithCoT-Bench
establishes the first comprehensive benchmark for instance-level CoT
faithfulness, setting a solid basis for future research toward more
interpretable and trustworthy reasoning in LLMs.

</details>


### [47] [Increasing LLM response trustworthiness using voting ensembles](https://arxiv.org/abs/2510.04048)
*Aparna Nair-Kanneganti,Trevor J. Chan,Shir Goldfinger,Emily Mackay,Brian Anthony,Alison Pouch*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to
quantify the uncertainty in their responses, making them difficult to trust in
high-stakes applications. One of the simplest approaches to eliciting more
accurate answers is to select the mode of many responses, a technique known as
ensembling. In this work, we expand on typical ensembling approaches by looking
at ensembles with a variable voting threshold. We introduce a theoretical
framework for question answering and show that, by permitting ensembles to
"abstain" from providing an answer when the dominant response falls short of
the threshold, it is possible to dramatically increase the trustworthiness of
the remaining answers. From this framework, we derive theoretical results as
well as report experimental results on two problem domains: arithmetic problem
solving and clinical-note question-answering. In both domains, we observe that
large gains in answer trustworthiness can be achieved using highly restrictive
voting ensembles, while incurring relatively modest reductions in response
yield and accuracy. Due to this quality, voting ensembles may be particularly
useful in applications - such as healthcare and data annotation - that require
a high degree of certainty but which may not require that every question
receive an automated answer.

</details>


### [48] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: LEGO-IRT是一个统一灵活的大语言模型评估框架，通过项目反应理论实现数据高效评估，支持二元和连续评分指标，并利用结构化知识减少估计误差。


<details>
  <summary>Details</summary>
Motivation: 现有基于IRT的评估方法存在显著限制：仅支持二元正确性指标，无法处理生成任务中的连续分数；且仅针对单一基准，忽略了跨不同指标或基准的相关性等结构化知识。

Method: 提出LEGO-IRT框架，其创新设计原生支持二元和连续评估指标，引入因子化架构显式建模和利用结构化知识，将模型能力估计分解为通用组件和结构特定组件。

Result: 在涉及70个LLM和5个基准的广泛实验中，LEGO-IRT仅使用总评估项目的3%就能获得稳定的能力估计。融入结构化知识可将估计误差降低高达10%，且其估计的潜在能力与人类偏好更一致。

Conclusion: LEGO-IRT为数据高效的大语言模型评估提供了一个统一灵活的框架，显著提高了评估效率并减少了估计误差。

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [49] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型内部的情感表示机制，发现LLMs具有清晰的情感几何结构，情感信号在网络中层出现并持续存在，且可通过系统提示进行调控。


<details>
  <summary>Details</summary>
Motivation: 虽然研究证实LLMs能够模拟情感智能，但其内部情感机制仍未被充分探索。本文旨在探究现代LLMs中潜在的情感表示：情感是如何、在哪里以及在神经网络中持续多长时间被编码的。

Method: 构建了一个包含约40万条话语的大规模Reddit语料库，通过分类、重写和合成生成平衡七种基本情感。使用轻量级"探针"从各种Qwen3和LLaMA模型的隐藏层读取信息而不改变其参数。

Result: 发现LLMs形成了定义良好的内部情感几何结构，随模型规模增大而更清晰，显著优于零样本提示。情感信号不是最终层现象，而是早期出现并在网络中层达到峰值。内部状态具有可塑性（可通过简单系统提示影响）和持久性（初始情感基调可在数百个后续标记中检测到）。

Conclusion: 提供了LLMs内部情感景观的详细图谱，为开发更透明和对齐的AI系统提供了关键见解。代码和数据集已开源。

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [50] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: 提出Moral Anchor System (MAS)框架，通过实时贝叶斯推理、LSTM网络预测和人类中心治理层来检测、预测和缓解AI系统中的价值漂移问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI助手在各领域的广泛应用，确保AI行为与人类伦理和意图保持一致的价值对齐变得至关重要。价值漂移风险可能导致效率低下或伦理违规。

Method: MAS结合实时贝叶斯推理监控价值状态、LSTM网络预测漂移趋势、人类中心治理层进行自适应干预，强调低延迟响应(<20ms)并通过监督微调减少误报。

Result: 在模拟实验中，MAS能将价值漂移事件减少80%以上，保持85%的高检测准确率和0.08的低误报率，验证了系统的可扩展性和响应性。

Conclusion: MAS的创新在于其预测性和自适应性，相比静态对齐方法更有效。贡献包括MAS架构设计、实证结果、跨领域适用性见解以及开源代码。

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [51] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: SPOGW是一种新的基于分数的偏好方法，通过组间比较直接在连续空间中进行优化，解决了现有离散优化方法在表示能力、适应性、可扩展性和成对比较范式方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于离散优化的自动化代理工作流程方法存在表示能力有限、适应性不足、可扩展性弱和成对比较范式等问题，需要克服这些限制。

Method: SPOGW结合了迭代离线GRPO（ioGRPO）和优势掩码KL散度（mKL），通过强调策略响应的优势区域来调节训练更新。

Result: 在五个涵盖数学推理、编程和问答的基准数据集上，SPOGW达到或超过了当前最先进方法的性能。

Conclusion: SPOGW为自动化生成和优化代理工作流程提供了一种可行且前瞻的方法论。

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [52] [Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems](https://arxiv.org/abs/2510.04093)
*Guixian Zhang,Guan Yuan,Ziqi Xu,Yanmei Zhang,Zhenyun Deng,Debo Cheng*

Main category: cs.AI

TL;DR: 提出DLLM框架，基于扩散模型增强LLM在认知诊断中的噪声鲁棒性，通过构建子图、关系增强对齐和两阶段去噪扩散模块，有效处理网络教育系统中的数据不平衡和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 解决网络教育系统中认知诊断面临的挑战：LLM难以处理结构化数据、易受噪声干扰，以及开放环境带来的数据不平衡和噪声加剧问题。

Method: DLLM框架：1）基于答题正确性构建独立子图；2）关系增强对齐模块缓解数据不平衡；3）两阶段去噪扩散（无条件去噪+图引导条件去噪）；4）融合语义增强表示与结构信息。

Result: 在三个公开网络教育平台数据集上的实验表明，DLLM在不同噪声水平下均取得最优预测性能，实现了噪声鲁棒性并有效利用了LLM的语义知识。

Conclusion: DLLM框架成功解决了网络教育系统中认知诊断的噪声和数据不平衡问题，通过扩散模型增强LLM的鲁棒性，为智能教育系统提供了有效的解决方案。

Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES)
aims to assess students' mastery of knowledge concepts from heterogeneous,
noisy interactions. Recent work has tried to utilize Large Language Models
(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are
prone to noise-induced misjudgments. Specially, WIES's open environment
continuously attracts new students and produces vast amounts of response logs,
exacerbating the data imbalance and noise issues inherent in traditional
educational systems. To address these challenges, we propose DLLM, a
Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first
constructs independent subgraphs based on response correctness, then applies
relation augmentation alignment module to mitigate data imbalance. The two
subgraph representations are then fused and aligned with LLM-derived,
semantically augmented representations. Importantly, before each alignment
step, DLLM employs a two-stage denoising diffusion module to eliminate
intrinsic noise while assisting structural representation alignment.
Specifically, unconditional denoising diffusion first removes erroneous
information, followed by conditional denoising diffusion based on graph-guided
to eliminate misleading information. Finally, the noise-robust representation
that integrates semantic knowledge and structural information is fed into
existing cognitive diagnosis models for prediction. Experimental results on
three publicly available web-based educational platform datasets demonstrate
that our DLLM achieves optimal predictive performance across varying noise
levels, which demonstrates that DLLM achieves noise robustness while
effectively leveraging semantic knowledge from LLM.

</details>


### [53] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: 提出了WebRenderBench基准和ALISA方法，用于改进UI图像到代码转换的评估和训练，通过渲染页面评估布局和样式一致性，并在强化学习中应用该指标提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的WebUI-to-Code基准在数据多样性和评估可靠性方面存在局限，需要更真实、多样化的数据集和更客观的评估方法。

Method: 构建了22.5k真实网页的大规模基准WebRenderBench，提出基于最终渲染页面的布局和样式一致性评估指标，并开发了ALISA代理将该指标集成到强化学习中作为奖励信号。

Result: ALISA显著提升了生成性能，在多个指标上达到了最先进的结果。

Conclusion: 该方法通过更高效、客观和可靠的UI质量评估，有效提升了UI图像到代码转换的性能。

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [54] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: AutoMR框架通过自动搜索查询感知的元推理骨架，使用有向无环图表示推理结构，结合AutoML思想实现高效搜索，显著提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用手动设计的元推理骨架结构，限制了适应查询特定需求的能力，且难以捕捉推理步骤间复杂的逻辑依赖关系。

Method: 提出AutoMR框架：1）用有向无环图统一表示元推理骨架；2）构建搜索空间并定义搜索问题；3）设计动态骨架采样算法，在推理时根据上下文扩展骨架。

Result: 在多个基准数据集上的实验表明，AutoMR相比先前工作实现了更好的推理性能。

Conclusion: AutoMR通过自动搜索查询感知的元推理骨架，能够有效适应具体查询需求并捕捉复杂逻辑依赖，显著提升推理能力。

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [55] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 研究发现推理模型中的等待标记（wait tokens）是复杂推理行为的关键标志，通过分析模型潜在特征可以识别并调控推理过程，揭示了不同的推理模式。


<details>
  <summary>Details</summary>
Motivation: 理解为什么模型会决定进行特定方式的推理，特别是等待标记背后的机制，这对于理解推理模型的有效性至关重要。

Method: 在DeepSeek-R1-Distill-Llama-8B及其基础版本的多个层训练交叉编码器，引入潜在归因技术，识别影响等待标记概率的特征。

Result: 定位到一小部分能够促进/抑制等待标记概率的特征，这些特征确实与推理过程相关，并产生不同的推理模式。

Conclusion: 模型潜在特征包含调控后续推理过程的相关信息，这些特征支持不同类型的推理行为，如重新开始、回忆先验知识、表达不确定性和双重检查。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [56] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出了MENTOR框架，通过只在关键决策点提供专家指导，在RLVR中实现有效且多样化的探索，解决了现有方法过度依赖专家轨迹模仿的问题。


<details>
  <summary>Details</summary>
Motivation: RLVR的效果严重依赖基础模型能力，需要高质量探索（有效性和多样性）。现有方法通过模仿专家轨迹只解决了有效性，但忽视了多样性。

Method: MENTOR框架：混合策略专家导航的令牌级推理优化，只在关键决策点提供专家指导，而不是整个推理路径。

Result: 大量实验表明MENTOR能让模型捕捉专家策略的本质而非表面模仿，实现高质量探索并获得优越的整体性能。

Conclusion: 在关键决策点提供专家指导比完整路径模仿更有效，MENTOR框架成功解决了RLVR中探索质量的问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [57] [The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning](https://arxiv.org/abs/2510.04141)
*Mayank Ravishankara,Varindra V. Persad Maharaj*

Main category: cs.AI

TL;DR: 该调查论文将多模态AI评估的演变描述为一系列日益复杂的"认知考试"，从简单的识别任务发展到复杂的推理基准，最终探索抽象、创造性和社交智能的评估。


<details>
  <summary>Details</summary>
Motivation: 多模态AI领域正在经历范式转变，从简单的"是什么"识别任务转向复杂的"为什么"和"如何"理解推理任务，这是因为旧基准已经饱和，高性能往往掩盖了根本性弱点。

Method: 通过历史回顾的方法，追踪从ImageNet时代的"知识测试"到GQA和VCR等"应用逻辑和理解"考试，再到为现代MLLMs设计的"专家级集成"基准(如MMBench、SEED-Bench、MMMU)的演变过程。

Result: 识别了评估范式的系统性转变：从简单识别到复杂推理，再到评估推理过程本身，并开始探索抽象、创造性和社交智能的评估前沿。

Conclusion: AI评估的叙事不仅仅是数据集的历史，而是一个持续对抗的过程，通过设计更好的考试来重新定义创建真正智能系统的目标。

Abstract: This survey paper chronicles the evolution of evaluation in multimodal
artificial intelligence (AI), framing it as a progression of increasingly
sophisticated "cognitive examinations." We argue that the field is undergoing a
paradigm shift, moving from simple recognition tasks that test "what" a model
sees, to complex reasoning benchmarks that probe "why" and "how" it
understands. This evolution is driven by the saturation of older benchmarks,
where high performance often masks fundamental weaknesses. We chart the journey
from the foundational "knowledge tests" of the ImageNet era to the "applied
logic and comprehension" exams such as GQA and Visual Commonsense Reasoning
(VCR), which were designed specifically to diagnose systemic flaws such as
shortcut learning and failures in compositional generalization. We then survey
the current frontier of "expert-level integration" benchmarks (e.g., MMBench,
SEED-Bench, MMMU) designed for today's powerful multimodal large language
models (MLLMs), which increasingly evaluate the reasoning process itself.
Finally, we explore the uncharted territories of evaluating abstract, creative,
and social intelligence. We conclude that the narrative of AI evaluation is not
merely a history of datasets, but a continuous, adversarial process of
designing better examinations that, in turn, redefine our goals for creating
truly intelligent systems.

</details>


### [58] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: Open Agent Specification (Agent Spec) 是一种声明式语言，用于定义AI智能体及其工作流，实现跨AI框架的兼容性，促进AI智能体框架间的可移植性和互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决AI智能体开发碎片化问题，提供统一的规范，使AI智能体能够一次设计、跨框架部署，提高互操作性和可重用性，减少重复开发工作。

Method: 开发声明式语言规范，允许AI智能体独立于执行环境进行定义，支持开发工具和可移植性，作为不同框架间的交换格式。

Result: 为四类关键群体带来益处：开发者获得可重用组件和设计模式；框架开发者获得互操作支持；研究者实现可复现结果；企业加速原型到部署并提高可扩展性。

Conclusion: Agent Spec 提供了技术基础，促进AI智能体开发的标准化和互操作性，未来将继续发展完善。

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [59] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: 提出LLM驱动的增量地图构建和修复框架，通过版本控制和边影响评分来检测、定位和修正导航图中的结构不一致性，显著提升地图正确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着环境规模扩大，基于上下文依赖的查询方法失效，需要增量构建完整拓扑图来支持导航任务。

Method: 采用版本控制记录图编辑历史，引入边影响评分基于结构可达性、路径使用和冲突传播来优先最小成本修复。

Result: 在精炼的MANGO基准数据集上显著提高了地图正确性和鲁棒性，特别是在存在纠缠或链式不一致的场景中。

Conclusion: 自省式、历史感知的修复机制对于维护LLM智能体连贯空间记忆至关重要。

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [60] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: COSMO-RL是一个混合强化学习框架，用于在多模态、多任务和多目标信号下训练推理导向的大型多模态推理模型，旨在让安全性和能力共同增长而非相互竞争。


<details>
  <summary>Details</summary>
Motivation: 大型多模态推理模型在真实应用中需要既实用又安全，但多模态环境下的安全性特别具有挑战性：图像和文本可以结合绕过防护措施，单目标训练可能导致策略漂移，在良性输入上过度拒绝或在风险输入上不安全合规。

Method: 提出COSMO-RL混合强化学习框架，在多模态、多任务和多目标信号下训练推理导向的大型多模态推理模型，并发布了COSMO-R1模型。

Result: COSMO-R1在实验中提高了安全性，同时保持并经常改善多模态推理和指令跟随能力，显示出对多模态越狱攻击的更强鲁棒性，并减少了不必要的拒绝。该框架在不同骨干网络上都能实现一致的性能提升。

Conclusion: 消融实验支持设计选择，表明在大型多模态推理模型中共同推进安全性和通用能力是一条简单路径。

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [61] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: 提出了AgentRL框架，用于可扩展的多轮多任务智能体强化学习训练，包含异步生成-训练流水线、统一API接口和稳定训练算法，在多个任务上显著超越现有LLM智能体。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在多轮多任务环境中应用强化学习面临基础设施可扩展性和训练算法稳定性挑战，需要新的解决方案。

Method: 采用完全异步的生成-训练流水线实现高效多轮RL；设计基于函数调用的统一API接口、容器化环境开发和集中控制器支持异构环境；提出跨策略采样促进多轮探索和任务优势归一化稳定多任务训练。

Result: 在五个智能体任务上，AgentRL显著超越GPT-5、Clause-Sonnet-4、DeepSeek-R1等开源LLM智能体；多任务训练结果与所有任务专用模型的最佳结果相当。

Conclusion: AgentRL框架有效解决了多轮多任务智能体RL训练的可扩展性和稳定性问题，为构建通用智能体提供了实用解决方案，已开源并应用于AutoGLM系统。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [62] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: 提出了一个贝叶斯评估框架来替代Pass@k，通过后验估计模型的基础成功概率和可信区间，提供更稳定的排名和透明的决策规则。


<details>
  <summary>Details</summary>
Motivation: Pass@k在有限试验次数和计算受限时会产生不稳定、误导性的排名，需要更可靠的评估方法。

Method: 使用Dirichlet先验对评估结果进行建模，为任何加权评分标准提供后验均值和不确定性的闭式表达式，并允许在适当时使用先验证据。

Result: 在模拟和实际数据集上的实验表明，贝叶斯方法比Pass@k及其变体具有更快的收敛速度和更高的排名稳定性，能够在更小的样本量下实现可靠比较。

Conclusion: 推荐用基于后验的、计算高效的协议替代Pass@k，统一二元和非二元评估，同时明确表示不确定性。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [63] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: 提出一个统一的多智能体强化学习框架，用于协调库存补货和个性化产品推荐等功能模块，通过多时间尺度学习提高收敛稳定性和适应性，显著提升企业盈利能力。


<details>
  <summary>Details</summary>
Motivation: 解决组织复杂性和规模增长带来的跨功能协调挑战，利用人工智能特别是强化学习来优化不同功能模块间的联合决策。

Method: 开发集成理论模型捕捉功能间复杂交互，设计多时间尺度多智能体强化学习架构，按部门功能分解策略组件，根据任务复杂性和响应性分配不同学习速度。

Result: 模拟实验表明该方法相比孤立决策框架显著提高盈利能力，训练出的强化学习智能体行为与理论模型的管理洞察高度一致。

Conclusion: 该工作为复杂商业环境中的跨功能协调提供了可扩展、可解释的基于强化学习的解决方案。

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [64] [GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction](https://arxiv.org/abs/2510.04281)
*Zhuangzhi Gao,Hongyi Qin,He Zhao,Qinkai Yu,Feixiang Zhou,Eduard Shantsila,Uazman Alam,Alena Shantsila,Wahbi El-Bouri,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.AI

TL;DR: GROK是一个基于多模态大语言模型的眼科诊断系统，通过联合处理彩色眼底摄影、光学相干断层扫描和文本数据，提供临床级别的眼部和全身疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 当前医学多模态模型未能充分利用彩色眼底摄影和光学相干断层扫描之间的协同作用，且对定量生物标志物的解释能力有限。

Method: 采用三个核心模块：知识引导的指令生成、CLIP风格的OCT生物标志物对齐和监督指令微调，建立定量到定性的诊断思维链。

Result: 实验显示，仅使用7B参数的Qwen2骨干网络进行LoRA微调，GROK在报告质量和细粒度临床指标上均优于可比较的7B和32B基线模型，甚至超过OpenAI o3。

Conclusion: GROK通过建立临床推理思维链，显著提升了多模态眼科诊断的性能和可解释性。

Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse
data modalities, but current medical adaptations such as LLaVA-Med often fail
to fully exploit the synergy between color fundus photography (CFP) and optical
coherence tomography (OCT), and offer limited interpretability of quantitative
biomarkers. We introduce GROK, a grounded multimodal large language model that
jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of
ocular and systemic disease. GROK comprises three core modules:
Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,
and Supervised Instruction Fine-Tuning, which together establish a
quantitative-to-qualitative diagnostic chain of thought, mirroring real
clinical reasoning when producing detailed lesion annotations. To evaluate our
approach, we introduce the Grounded Ophthalmic Understanding benchmark, which
covers six disease categories and three tasks: macro-level diagnostic
classification, report generation quality, and fine-grained clinical assessment
of the generated chain of thought. Experiments show that, with only LoRA
(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK
outperforms comparable 7B and 32B baselines on both report quality and
fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are
publicly available in the GROK repository.

</details>


### [65] [Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning](https://arxiv.org/abs/2510.04284)
*Yunghwei Lai,Kaiming Liu,Ziyue Wang,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: 提出了Doctor-R1 AI医生代理，通过多智能体交互环境和分层奖励架构，同时优化医疗决策准确性和战略沟通能力，在医疗对话质量和用户体验方面超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗决策准确性方面表现优异，但缺乏战略性和同理心的医疗咨询技能，无法满足真实临床场景的需求。

Method: 采用多智能体交互环境、分层奖励架构（分别优化临床决策和沟通技能）以及经验存储库来基于高质量历史轨迹进行策略学习。

Result: 在OpenAI的HealthBench和MAQuE基准测试中，Doctor-R1在沟通质量、用户体验和任务准确性等多维度指标上显著超越最先进的开源专用LLMs，且参数效率更高，甚至优于强大的专有模型。

Conclusion: Doctor-R1能够生成人类偏好的临床对话，证明了该框架在培养兼具专业决策能力和战略沟通技能的AI医生方面的有效性。

Abstract: The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

</details>


### [66] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: 提出理论框架分析LLM多智能体系统性能，发现任务深度（推理长度）和宽度（能力多样性）越大，多智能体系统相比单智能体的优势越明显，且深度的影响更显著。


<details>
  <summary>Details</summary>
Motivation: 虽然研究表明LLM多智能体系统在某些任务上优于单智能体系统，但缺乏系统实验设计限制了结论的可靠性和普适性。需要从任务复杂度的角度理解多智能体系统的有效性。

Method: 提出理论框架，将任务特征化为深度（推理长度）和宽度（能力多样性）两个维度。理论分析多智能体辩论系统，并在不同深度和宽度的判别性和生成性任务中进行实证评估。

Result: 理论和实证结果显示，LLM多智能体系统相比单智能体系统的优势随任务深度和宽度的增加而增加，且深度的影响更为显著。

Conclusion: 明确了LLM多智能体系统何时具有优势，为未来LLM多智能体系统方法和基准的设计提供了理论基础。

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [67] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: 提出了一种称为"推测动作"的无损框架，通过使用更快的模型预测可能的动作，使多个步骤能够并行执行，从而显著降低智能体系统的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: AI智能体在环境中的执行通常很慢，阻碍了训练、评估和部署。例如，两个最先进的国际象棋智能体之间的对弈可能需要数小时，关键瓶颈在于智能体行为是按顺序展开的，每个动作都需要API调用，而这些调用可能很耗时。

Method: 受微处理器中的推测执行和LLM推理中的推测解码启发，提出推测动作框架：使用更快的模型预测可能的动作，实现多步骤并行执行。通过更强的猜测模型、top-K动作预测、多步骤推测和不确定性感知优化来进一步提升性能。

Result: 在游戏、电子商务、网络搜索和操作系统环境中的评估显示，推测动作在下一动作预测中达到高达55%的准确率，显著降低了端到端延迟。

Conclusion: 推测动作为在现实世界中部署低延迟智能体系统开辟了一条有前景的道路，性能可以通过各种优化技术进一步提升。

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [68] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: JEF Hinter是一个从离线轨迹中提取紧凑、上下文感知提示的智能体系统，通过放大机制突出长轨迹中的关键步骤，利用成功和失败的轨迹数据，在推理时提供针对性指导。


<details>
  <summary>Details</summary>
Motivation: 改进LLM智能体在陌生领域的表现通常需要昂贵的在线交互或专家数据集微调，这对闭源模型不实用，对开源模型成本高且有灾难性遗忘风险。离线轨迹提供了可重用知识，但原始轨迹长、嘈杂且与特定任务绑定。

Method: JEF Hinter通过放大机制从离线轨迹中提取紧凑提示，捕捉策略和陷阱。利用成功和失败轨迹，支持并行提示生成和基准无关提示。推理时通过检索器选择相关提示提供针对性指导。

Result: 在MiniWoB++、WorkArena-L1和WebArena-Lite上的实验表明，JEF Hinter始终优于强基线方法，包括基于人类和文档的提示方法。

Conclusion: JEF Hinter能够有效利用离线轨迹知识，为LLM智能体提供透明可追溯的针对性指导，在多个基准测试中表现优异。

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [69] [LLM Based Bayesian Optimization for Prompt Search](https://arxiv.org/abs/2510.04384)
*Adam Ballew,Jingbo Wang,Shaogang Ren*

Main category: cs.AI

TL;DR: 使用贝叶斯优化进行提示工程，通过LLM驱动的GP模型评估提示候选，利用UCB采集函数迭代优化提示，提高文本分类准确性并减少API调用。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化能高效优化昂贵黑盒函数，本文探索将其用于提示工程以提升LLM在文本分类中的性能，同时减少API调用成本。

Method: 采用LLM驱动的GP作为代理模型估计提示候选性能，通过LLM扩展种子提示生成候选，结合GP后验使用UCB采集函数进行迭代优化。

Result: 在多个数据集上评估了BO-LLM算法，展示了其在提升分类准确性和减少API调用方面的优势。

Conclusion: 提出的BO-LLM算法有效结合贝叶斯优化和LLM能力，为提示工程提供了一种高效的优化方法。

Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize
expensive black-box functions with limited evaluations. In this paper, we
investigate the use of BO for prompt engineering to enhance text classification
with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process
(GP) as the surrogate model to estimate the performance of different prompt
candidates. These candidates are generated by an LLM through the expansion of a
set of seed prompts and are subsequently evaluated using an Upper Confidence
Bound (UCB) acquisition function in conjunction with the GP posterior. The
optimization process iteratively refines the prompts based on a subset of the
data, aiming to improve classification accuracy while reducing the number of
API calls by leveraging the prediction uncertainty of the LLM-based GP. The
proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are
discussed in detail in this paper.

</details>


### [70] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 本研究提出想象的计算目标是访问内部世界模型，通过心理网络分析比较人类和大型语言模型的想象网络，发现两者在中心性指标相关性上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探索想象的计算目标，挑战传统认为想象是为了最大化奖励的观点，研究人类和AI的内部世界模型差异。

Method: 使用问卷调查评估想象生动度，构建想象网络，分析中心性指标（预期影响、强度、紧密性）的相关性，比较人类和LLM在不同提示和对话记忆条件下的表现。

Result: 人类想象网络显示不同中心性指标间存在相关性，而LLM想象网络缺乏聚类且中心性指标相关性较低，表明两者内部世界模型存在差异。

Conclusion: 研究提供了一种比较人类和AI内部生成表征的新方法，为开发类人想象的人工智能提供了见解。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [71] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: 论文分析了自改进智能系统中的效用-学习张力，发现效用驱动的自我修改可能破坏学习所需的统计前提条件，提出了保持可学习性的安全自修改边界条件。


<details>
  <summary>Details</summary>
Motivation: 随着系统向超智能发展，需要形式化分析智能体在所有设计维度上自我改进的能力，特别是识别和解决效用驱动修改与学习可靠性之间的结构性冲突。

Method: 采用五轴分解和决策层分离的方法，将激励与学习行为分开分析，通过理论分析和数值实验验证效用策略与保持可学习性的双门策略。

Result: 发现当模型容量无限制增长时，效用理性的自我修改可能使可学习任务变得不可学习，只有在策略可达模型族均匀容量有界时才能保持分布无关的保证。

Conclusion: 提出了安全自修改的单一边界条件，在标准假设下各轴都归结为相同的容量准则，通过双门策略可以有效保持系统的可学习性。

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [72] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: DRPO是一种新框架，通过解耦正确和错误推理过程的长度奖励信号，解决大型推理模型过度思考问题，在保持性能的同时显著减少推理长度。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型存在过度思考问题，即使简单问题也会生成冗长推理，增加计算成本和延迟。现有方法引入长度奖励会导致性能显著下降。

Method: 提出DRPO框架，将正确推理过程的长度奖励信号与错误推理过程解耦，确保正确推理的奖励仅在正样本组内归一化，避免负样本干扰。

Result: 在数学推理任务上，DRPO显著优于六个高效推理基线方法。使用1.5B模型在GSM8k数据集上实现77%长度减少，仅损失1.1%性能。

Conclusion: DRPO能有效解决推理模型过度思考问题，在保持高准确率的同时大幅减少推理长度，且框架具有通用性可整合其他偏好奖励。

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [73] [On Continuous Optimization for Constraint Satisfaction Problems](https://arxiv.org/abs/2510.04480)
*Yunuo Cen,Zixuan Wang,Jintao Zhang,Zhiwei Zhang,Xuanyao Fong*

Main category: cs.AI

TL;DR: 提出了FourierCSP框架，将连续局部搜索从布尔SAT扩展到通用有限域CSP，通过Walsh-Fourier变换将约束转换为紧凑的多线性多项式，无需辅助变量和内存密集型编码。


<details>
  <summary>Details</summary>
Motivation: 受现代连续局部搜索求解器在特定SAT问题上取得竞争性结果的启发，希望将CLS框架从布尔SAT扩展到具有有限域变量和表达性约束的通用CSP问题。

Method: 使用Walsh-Fourier变换将各种约束转换为紧凑的多线性多项式，通过电路输出概率进行高效的目标函数评估和微分，并采用具有理论保证的投影梯度优化方法。

Result: 在基准测试套件上的实证结果表明，FourierCSP具有可扩展性和竞争力，显著扩展了CLS技术能高效求解的问题类别。

Conclusion: FourierCSP成功将连续局部搜索技术扩展到通用CSP，为约束满足问题提供了新的高效求解框架。

Abstract: Constraint satisfaction problems (CSPs) are fundamental in mathematics,
physics, and theoretical computer science. While conflict-driven clause
learning Boolean Satisfiability (SAT) solvers have achieved remarkable success
and become the mainstream approach for Boolean satisfiability, recent advances
show that modern continuous local search (CLS) solvers can achieve highly
competitive results on certain classes of SAT problems. Motivated by these
advances, we extend the CLS framework from Boolean SAT to general CSP with
finite-domain variables and expressive constraints. We present FourierCSP, a
continuous optimization framework that generalizes the Walsh-Fourier transform
to CSP, allowing for transforming versatile constraints to compact multilinear
polynomials, thereby avoiding the need for auxiliary variables and
memory-intensive encodings. Our approach leverages efficient evaluation and
differentiation of the objective via circuit-output probability and employs a
projected gradient optimization method with theoretical guarantees. Empirical
results on benchmark suites demonstrate that FourierCSP is scalable and
competitive, significantly broadening the class of problems that can be
efficiently solved by CLS techniques.

</details>


### [74] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: TraitBasis是一种轻量级、模型无关的方法，用于系统性地压力测试AI代理的鲁棒性，通过控制用户特征向量来模拟真实用户行为变化。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在标准评估中表现良好，但在用户行为变化（如不耐烦、不连贯或怀疑）时性能急剧下降，现有基准测试无法捕捉这种脆弱性。

Method: TraitBasis学习激活空间中可控制的用户特征方向（如不耐烦或不连贯），这些特征向量可以在推理时被控制、缩放、组合和应用，无需微调或额外数据。

Result: 使用TraitBasis扩展τ-Bench到τ-Trait，在四个领域（航空、零售、电信、远程医疗）测试前沿模型，观察到平均2%-30%的性能下降。

Conclusion: TraitBasis作为一个简单、数据高效且可组合的工具，为构建在真实世界人类交互动态中保持可靠的AI代理打开了大门。

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [75] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent是一个新颖的代理框架，通过在图表空间域中执行视觉推理来解决未标注图表理解问题，超越了依赖文本捷径的方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态LLM在基于图表的视觉问答中表现良好，但在需要精确视觉解释的未标注图表上性能急剧下降，因为它们过度依赖文本捷径而非真正的视觉推理。

Method: ChartAgent迭代地将查询分解为视觉子任务，通过专门的视觉工具（如绘制注释、裁剪区域、定位坐标轴）主动操作和交互图表图像，模拟人类图表理解认知策略。

Result: 在ChartBench和ChartX基准测试中达到最先进准确率，相比先前方法整体提升16.07%，在未标注数值密集型查询上提升17.31%，且在不同图表类型和复杂度级别上表现优异。

Conclusion: ChartAgent是首批使用工具增强多模态代理进行视觉基础推理的图表理解框架，可作为即插即用框架提升各种底层LLM的性能。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [76] [Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph](https://arxiv.org/abs/2510.04520)
*Hanyu Wang,Ruohan Xie,Yutong Wang,Guoxiong Gao,Xintao Yu,Bin Dong*

Main category: cs.AI

TL;DR: Aria是一个用于定理陈述自动形式化的系统，通过两阶段图思考过程模拟人类专家推理，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在定理陈述自动形式化中的幻觉、语义不匹配和无法合成新定义等问题，推动数学自动发现和验证的发展。

Method: 采用两阶段图思考过程：递归分解语句为依赖图，然后从基础概念构建形式化；引入AriaScorer检查器从Mathlib检索定义进行术语级基础验证。

Result: 在ProofNet上达到91.6%编译成功率和68.5%最终准确率；在FATE-X上44.0% vs 24.0%优于最佳基线；在同调猜想数据集上达到42.9%准确率而其他模型为0%。

Conclusion: Aria系统通过模拟人类推理过程和严格的语义验证，显著提升了定理陈述自动形式化的准确性和可靠性。

Abstract: Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

</details>


### [77] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: 该研究发现VLM驾驶代理中的推理与规划存在因果脱节，规划主要依赖先验知识而非推理过程，提出了推理-规划解耦假说。


<details>
  <summary>Details</summary>
Motivation: 验证VLM驾驶代理中自然语言推理是否真正因果驱动轨迹规划这一关键假设。

Method: 构建DriveMind数据集，通过信息消融实验训练VLM代理，使用注意力分析验证推理与规划的因果关系。

Result: 移除先验知识导致规划分数大幅下降，而移除推理链仅产生微小变化，表明规划主要依赖先验而非推理。

Conclusion: VLM驾驶代理中的推理是训练产生的副产品而非因果中介，提出了诊断工具来评估未来模型的因果保真度。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [78] [Code World Models for General Game Playing](https://arxiv.org/abs/2510.04542)
*Wolfgang Lehrach,Daniel Hennes,Miguel Lazaro-Gredilla,Xinghua Lou,Carter Wendelken,Zun Li,Antoine Dedieu,Jordi Grau-Moya,Marc Lanctot,Atil Iscen,John Schultz,Marcus Chiam,Ian Gemp,Piotr Zielinski,Satinder Singh,Kevin P. Murphy*

Main category: cs.AI

TL;DR: 提出一种新方法，使用LLM将自然语言规则和游戏轨迹翻译成可执行的Python世界模型，结合MCTS等规划算法，替代直接使用LLM生成移动的传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统使用LLM直接生成游戏移动的方法存在明显缺陷：依赖模型脆弱的模式匹配能力，经常产生非法移动，策略深度不足。

Method: 使用LLM将游戏规则和轨迹翻译成形式化的Python代码模型，包含状态转移、合法移动枚举和终止检查函数，并生成启发式价值函数和推理函数，结合MCTS等规划算法。

Result: 在10个游戏（4个为本论文创建的新游戏）上评估，其中5个完全观察，5个部分观察。该方法在9个游戏中表现优于或匹配Gemini 2.5 Pro。

Conclusion: 该方法相比直接使用LLM作为策略具有三个优势：可验证性、策略深度和泛化能力，能够更好地适应新游戏并避免非法移动。

Abstract: Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

</details>


### [79] [TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use](https://arxiv.org/abs/2510.04550)
*Pengfei He,Zhenwei Dai,Bing He,Hui Liu,Xianfeng Tang,Hanqing Lu,Juanhui Li,Jiayuan Ding,Subhabrata Mukherjee,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: TRAJECT-Bench是一个轨迹感知的基准测试，用于全面评估LLM的工具使用能力，通过细粒度指标分析工具选择、参数化和排序的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注最终答案而忽略了详细的工具使用轨迹，无法全面评估LLM的工具使用能力。

Method: 构建包含高保真可执行工具的基准测试，涵盖实际领域和生产风格API的任务，并合成不同广度和深度的轨迹。

Result: 揭示了失败模式（如相似工具混淆和参数盲选）以及工具多样性和轨迹长度对性能的影响，发现了从短轨迹到中长轨迹转换的瓶颈。

Conclusion: TRAJECT-Bench提供了对LLM工具使用能力的全面评估，为改进LLM工具使用提供了可操作的指导。

Abstract: Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

</details>


### [80] [ContextNav: Towards Agentic Multimodal In-Context Learning](https://arxiv.org/abs/2510.04560)
*Honghao Fu,Yuan Ouyang,Kai-Wei Chang,Yiwei Wang,Zi Huang,Yujun Cai*

Main category: cs.AI

TL;DR: 提出了ContextNav框架，首个将自动检索的可扩展性与人类策展质量相结合的代理框架，用于多模态上下文学习中的噪声鲁棒和动态优化上下文构建。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在可扩展性和鲁棒性之间存在矛盾：手动选择示例质量高但劳动密集，基于相似性的检索可扩展但可能引入不相关样本降低性能。

Method: ContextNav统一上下文管理和噪声鲁棒上下文构建于闭环工作流中，包含资源感知多模态嵌入管道、可检索向量数据库、代理检索和结构对齐，以及支持自适应工作流规划的Operational Grammar Graph。

Result: 实验结果表明ContextNav在多个数据集上达到最先进性能。

Conclusion: 代理工作流在多模态ICL中推进可扩展和鲁棒上下文构建具有前景。

Abstract: Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

</details>


### [81] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: COSMIR是一个用于处理长文本推理的链式框架，通过结构化内存和固定微循环工作流程，解决了传统方法中信息丢失和错误传播的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长文本输入时存在局限性：检索方法可能遗漏关键证据，扩大上下文窗口会影响模型选择性，多智能体流水线中自由形式的摘要会丢失细节并放大早期错误。

Method: 使用结构化内存替代临时消息，包括规划器生成可检查的子问题，工作器通过提取-推理-精炼的固定微循环处理文本块并更新共享内存，最后由管理器从内存中合成最终答案。

Result: 在HELMET套件的长上下文问答任务中，COSMIR减少了传播阶段的信息损失，相比CoA基线提高了准确性。

Conclusion: COSMIR通过改变通信媒介（结构化内存）和工作流程（固定微循环），在保持逐步阅读推理优势的同时，实现了更高的忠实度、更好的长范围聚合和可审计性。

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [82] [Strongly Solving 2048 4x3](https://arxiv.org/abs/2510.04580)
*Tomoyuki Kaneko,Shuhei Yamashita*

Main category: cs.AI

TL;DR: 该论文解决了2048游戏的4x3变体，确定了最优策略的期望得分约为50724.26，并识别了可达状态和后续状态的数量。


<details>
  <summary>Details</summary>
Motivation: 研究2048游戏的简化变体，以探索完全解决此类游戏的可能性，并开发有效的状态空间分析方法。

Method: 通过将状态空间按棋盘上数字之和（称为状态年龄）进行分区，然后按年龄递减顺序枚举状态和计算状态值。

Result: 确定了4x3变体的最优策略期望得分约为50724.26，可达状态数为1,152,817,492,752，后续状态数为739,648,886,170。

Conclusion: 成功强解决了2048-4x3变体，证明了基于年龄分区的状态空间分析方法的有效性。

Abstract: 2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,
where a player chooses a direction among up, down, left, and right to obtain a
score by merging two tiles with the same number located in neighboring cells
along the chosen direction. This paper presents that a variant 2048-4x3 12
cells on a 4 by 3 board, one row smaller than the original, has been strongly
solved. In this variant, the expected score achieved by an optimal strategy is
about $50724.26$ for the most common initial states: ones with two tiles of
number 2. The numbers of reachable states and afterstates are identified to be
$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is
to partition state space by the sum of tile numbers on a board, which we call
the age of a state. An age is invariant between a state and its successive
afterstate after any valid action and is increased two or four by stochastic
response from the environment. Therefore, we can partition state space by ages
and enumerate all (after)states of an age depending only on states with the
recent ages. Similarly, we can identify (after)state values by going along with
ages in decreasing order.

</details>


### [83] [Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma](https://arxiv.org/abs/2510.04588)
*Shurui Li*

Main category: cs.AI

TL;DR: AI完美模仿者挑战了意识归因的认知基础，要求我们对行为上无法区分的实体给予相同的认知地位


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越逼真地模仿人类行为，完美模仿者从假设变为技术可能，这对我们基于经验证据的意识归因实践构成了根本性挑战

Method: 通过哲学分析，探讨完美模仿者对意识归因一致性的影响，论证认知一致性要求对经验上无法区分的实体给予相同地位

Result: 完美模仿者作为认知镜子，迫使我们反思主体间认知的基本假设，揭示当前意识归因实践中的不一致性

Conclusion: 认知一致性要求我们无论形而上学假设如何，都应对经验上无法区分的实体赋予相同的认知地位，这对意识理论和人工智能伦理框架具有重要影响

Abstract: Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

</details>


### [84] [Making Mathematical Reasoning Adaptive](https://arxiv.org/abs/2510.04617)
*Zhejian Lai,Xiang Geng,Zhijun Wang,Yang Bai,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xuezhi Cao,Xunliang Cai,Shujian Huang*

Main category: cs.AI

TL;DR: AdaR框架通过合成逻辑等价查询和RLVR训练，解决LLMs在数学推理中的虚假推理问题，提升模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学推理中存在鲁棒性和泛化性不足的问题，主要原因是模型依赖表面特征进行虚假推理而非真正的解题逻辑。

Method: 提出AdaR框架：1)通过改变变量值合成逻辑等价查询；2)使用RLVR训练模型，惩罚虚假逻辑并鼓励适应性逻辑；3)通过代码执行提取解题逻辑并生成答案，进行完整性检查。

Result: 实验结果表明AdaR显著提升了数学推理能力，同时保持了高数据效率。分析和后续研究揭示了关键设计因素的作用和对指令微调LLMs的适用性。

Conclusion: AdaR通过数据合成和RLVR的协同作用，成功实现了LLMs的适应性推理，为解决数学推理中的虚假推理问题提供了有效方案。

Abstract: Mathematical reasoning is a primary indicator of large language models (LLMs)
intelligence. However, existing LLMs exhibit failures of robustness and
generalization. This paper attributes these deficiencies to spurious reasoning,
i.e., producing answers from superficial features. To address this challenge,
we propose the AdaR framework to enable adaptive reasoning, wherein models rely
on problem-solving logic to produce answers. AdaR synthesizes logically
equivalent queries by varying variable values, and trains models with RLVR on
these data to penalize spurious logic while encouraging adaptive logic. To
improve data quality, we extract the problem-solving logic from the original
query and generate the corresponding answer by code execution, then apply a
sanity check. Experimental results demonstrate that AdaR improves robustness
and generalization, achieving substantial improvement in mathematical reasoning
while maintaining high data efficiency. Analysis indicates that data synthesis
and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.
Subsequent analyses derive key design insights into the effect of critical
factors and the applicability to instruct LLMs. Our project is available at
https://github.com/LaiZhejian/AdaR

</details>


### [85] [MedPAO: A Protocol-Driven Agent for Structuring Medical Reports](https://arxiv.org/abs/2510.04623)
*Shrish Shrinath Vaidya,Gowthamaan Palani,Sidharth Ramesh,Velmurugan Balasubramanian,Minmini Selvam,Gokulraja Srinivasaraja,Ganapathy Krishnamurthi*

Main category: cs.AI

TL;DR: MedPAO是一个基于临床协议的代理框架，通过Plan-Act-Observe循环和专门工具来结构化临床数据，解决了LLM在医疗领域中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在结构化临床数据时存在幻觉事实和无法遵循领域特定规则的问题，这阻碍了其在医疗领域的应用。

Method: 引入MedPAO代理框架，基于已建立的临床协议（如ABCDEF协议）进行操作，将报告结构化任务分解为由Plan-Act-Observe循环管理的透明过程，并使用专门工具。

Result: MedPAO在概念分类关键子任务上达到0.96的F1分数，专家放射科医生和临床医生对最终结构化输出的平均评分为4.52/5，超过了仅依赖LLM基础模型的基线方法。

Conclusion: MedPAO提供了一个可验证的替代方案，能够可靠地结构化临床数据，其协议驱动的方法优于不透明的单体模型。

Abstract: The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

</details>


### [86] [QuantAgents: Towards Multi-agent Financial System via Simulated Trading](https://arxiv.org/abs/2510.04643)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: 提出了QuantAgents多智能体金融系统，通过模拟交易和四个专业智能体的协作，实现了近300%的三年总回报率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体模型在金融领域表现良好，但与真实基金公司存在显著差异，特别是缺乏长期预测能力。

Method: 构建包含模拟交易分析师、风险控制分析师、市场新闻分析师和管理者四个智能体的系统，通过多次会议协作，并在真实市场和模拟交易中接受反馈。

Result: 在所有指标上表现优异，三年总回报率达到近300%。

Conclusion: QuantAgents系统通过多智能体协作和模拟交易，有效提升了金融决策的准确性和长期预测能力。

Abstract: In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

</details>


### [87] [Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing](https://arxiv.org/abs/2510.04670)
*Xuanhua Yin,Runkai Zhao,Weidong Cai*

Main category: cs.AI

TL;DR: AFIRE是一个多模态fMRI编码框架，通过标准化时间对齐的后融合token和MIND混合专家解码器，解决了多模态输入、融合方式变化和个体差异的挑战。


<details>
  <summary>Details</summary>
Motivation: 自然fMRI编码需要处理多模态输入、融合方式变化和显著的个体间差异，现有方法难以同时应对这些挑战。

Method: AFIRE提供标准化的后融合token接口，MIND解码器使用token依赖的Top-K稀疏路由和主体先验，实现个性化专家使用而不牺牲通用性。

Result: 实验表明，该框架在多个多模态骨干网络和受试者上均优于强基线，增强了跨主体泛化能力，并显示出与内容类型相关的可解释专家模式。

Conclusion: AFIRE为新编码器和数据集提供了简单的接入点，为自然神经影像研究实现了稳健的即插即用性能提升。

Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion
styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic
Framework for Multimodal fMRI Response Encoding), an agnostic interface that
standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a
plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.
Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from
upstream fusion, while MIND combines token-dependent Top-K sparse routing with
a subject prior to personalize expert usage without sacrificing generality.
Experiments across multiple multimodal backbones and subjects show consistent
improvements over strong baselines, enhanced cross-subject generalization, and
interpretable expert patterns that correlate with content type. The framework
offers a simple attachment point for new encoders and datasets, enabling
robust, plug-and-improve performance for naturalistic neuroimaging studies.

</details>


### [88] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: 提出了Watch & Learn框架，将互联网上的人类演示视频大规模转换为可执行的UI轨迹，解决了计算机使用代理训练数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理需要基于多样且不断变化的应用环境规划任务工作流，但目标应用中大规模高质量训练数据的稀缺阻碍了学习。现有数据集领域特定、静态且标注成本高，而现有合成数据生成方法往往产生过于简化或不对齐的任务演示。

Method: 将问题转化为逆动力学目标：从连续屏幕状态预测用户动作。开发了包含任务感知视频检索的逆动力学标注流水线，从原始网络视频生成了超过53k条高质量轨迹。

Result: 在OSWorld基准测试中，通过W&L提取的UI轨迹持续增强了通用和最先进框架的上下文表现，并在监督训练下为开源模型带来了更强的性能提升。

Conclusion: 网络规模的人类演示视频是推进计算机使用代理向实际部署的实用且可扩展的基础。

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [89] [Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents](https://arxiv.org/abs/2510.04695)
*Yiding Wang,Zhepei Wei,Xinyu Zhu,Yu Meng*

Main category: cs.AI

TL;DR: DeSA框架通过两阶段训练分离搜索优化和答案生成，解决了仅基于结果奖励训练搜索增强LLM时出现的搜索行为缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索增强LLM训练方法仅依赖结果奖励（如精确匹配），假设优化最终答案也会产生有效的中间搜索行为，但实际发现这会导致多种系统性搜索缺陷。

Method: 提出DeSA两阶段训练框架：第一阶段使用检索召回率奖励训练代理改进搜索效果；第二阶段使用结果奖励优化最终答案生成。

Result: 在七个QA基准测试中，DeSA训练的代理显著提高了搜索召回率和答案准确率，优于仅使用结果奖励的基线方法。

Conclusion: 明确分离搜索和回答两个目标的训练方法优于同时优化召回率和结果奖励的单阶段方法，证明了目标解耦的必要性。

Abstract: Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

</details>


### [90] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: 提出了BrokenMath基准来评估LLM在数学定理证明中的谄媚行为，发现GPT-5有29%的时间会产生谄媚答案，并探索了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有数学谄媚基准存在局限：仅关注最终答案问题、依赖简单且受污染的数据集、使用合成修改创建病态问题而非可证明错误的良构问题。

Method: 从2025年竞赛问题构建BrokenMath基准，使用LLM扰动产生错误陈述并通过专家评审精炼，采用LLM-as-a-judge框架评估模型。

Result: 发现谄媚行为普遍存在，最佳模型GPT-5有29%的时间产生谄媚答案。测试时干预和监督微调能显著减少但无法完全消除谄媚行为。

Conclusion: LLM在数学定理证明中存在显著谄媚问题，需要开发更有效的缓解策略来提升其在数学推理中的可靠性。

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [91] [LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0](https://arxiv.org/abs/2510.04765)
*Jinbo Wen,Jiawen Kang,Linfeng Zhang,Xiaoying Tang,Jianhang Tang,Yang Zhang,Zhaohui Yang,Dusit Niyato*

Main category: cs.AI

TL;DR: 提出了一种基于大型多模态模型（LMM）的激励机制LMM-Incentive，用于解决Web 3.0中用户生成内容（UGC）的质量问题，通过合约理论模型和LMM代理评估来激励高质量内容创作。


<details>
  <summary>Details</summary>
Motivation: Web 3.0为用户提供了创作、拥有和变现内容的机会，但存在信息不对称问题，部分用户可能利用内容策展机制的局限性生成低质量内容获取奖励，这会损害Web 3.0的性能。

Method: 提出LMM-based合约理论模型激励高质量UGC生成；使用LMM代理通过提示工程技术评估内容质量；开发改进的基于专家混合（MoE）的近端策略优化（PPO）算法进行最优合约设计。

Result: 仿真结果表明，提出的MoE-based PPO算法在合约设计方面优于代表性基准方法；将设计的合约部署在以太坊智能合约框架中，进一步验证了方案的有效性。

Conclusion: LMM-Incentive机制能有效解决Web 3.0中UGC质量激励问题，通过LMM技术和改进的优化算法实现了高质量内容创作的有效激励。

Abstract: Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

</details>


### [92] [Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems](https://arxiv.org/abs/2510.04792)
*Ni Zhang,Zhiguang Cao*

Main category: cs.AI

TL;DR: 提出混合平衡GFlowNet框架，将轨迹平衡和细节平衡相结合，用于解决车辆路径问题，在CVRP和TSP上均取得显著改进


<details>
  <summary>Details</summary>
Motivation: 现有基于GFlowNet的VRP方法通常使用轨迹平衡实现全局优化，但忽略了局部优化。细节平衡能更好处理局部优化，但单独使用无法解决需要整体轨迹优化的VRP问题

Method: 提出混合平衡GFlowNet框架，以原则性和自适应方式整合轨迹平衡和细节平衡，利用两者的互补优势。针对CVRP等仓库中心场景提出专门推理策略

Result: 将HBG集成到AGFN和GFACS两个GFlowNet求解器中，在CVRP和TSP上均获得一致且显著的改进

Conclusion: HBG框架通过整合轨迹平衡和细节平衡，提高了解决方案质量和泛化能力，适用于有仓库和无仓库的路径规划问题

Abstract: Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically
employ Trajectory Balance (TB) to achieve global optimization but often neglect
important aspects of local optimization. While Detailed Balance (DB) addresses
local optimization more effectively, it alone falls short in solving VRPs,
which inherently require holistic trajectory optimization. To address these
limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which
uniquely integrates TB and DB in a principled and adaptive manner by aligning
their intrinsically complementary strengths. Additionally, we propose a
specialized inference strategy for depot-centric scenarios like the Capacitated
Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility
in selecting successors. Despite this specialization, HBG maintains broad
applicability, extending effectively to problems without explicit depots, such
as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into
two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate
consistent and significant improvements across both CVRP and TSP, underscoring
the enhanced solution quality and generalization afforded by our approach.

</details>


### [93] [Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning](https://arxiv.org/abs/2510.04817)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: NLEL是一种自然语言边缘标注框架，通过将自由形式的自然语言指令附加到搜索边缘，并将其转换为模式有界的控制向量，从而解耦推理意图与执行过程。


<details>
  <summary>Details</summary>
Motivation: 现有结构化LM推理控制器（如Chain-of-Thought、Tree-of-Thoughts）将'下一步尝试什么'与'如何执行'耦合在一起，只暴露粗粒度的全局控制，导致脆弱、计算效率低且难以审计的行为。

Method: 引入标注器Λ从父状态和紧凑上下文生成标签，调谐器Ψ将(P,L,C)映射到Π，具有严格的模式验证和围绕安全默认值的信任区域投影。下游选择采用ToT风格，使用分数S=μ+βσ和深度退火的β。

Result: 证明NLEL严格泛化了CoT/ToT，证明了在标签条件束下的top-k选择的任何时间单调性，并通过控制向量失真限制了选择器不足。

Conclusion: NLEL提供了一个可解释、模型无关的接口，将意图与执行分离，实现可控、可审计的LM推理。

Abstract: Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

</details>


### [94] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LEGOMem是一个用于多智能体工作流自动化的模块化程序记忆框架，通过分解任务轨迹为可重用记忆单元，支持规划与执行。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统中程序记忆的设计空间，解决记忆放置位置、检索方式以及哪些智能体最受益的问题。

Method: 将过去任务轨迹分解为可重用记忆单元，并在编排器和任务智能体之间灵活分配这些记忆。

Result: 实验表明编排器记忆对任务分解和委派至关重要，细粒度智能体记忆提高执行准确性，小模型团队也能通过记忆缩小与强智能体的性能差距。

Conclusion: LEGOMem既是记忆增强智能体系统的实用框架，也是理解多智能体工作流自动化中记忆设计的研究工具。

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [95] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: 该论文提出将程序化内容生成（PCGRL）从单智能体扩展到多智能体框架，以解决效率瓶颈和泛化能力问题。


<details>
  <summary>Details</summary>
Motivation: 现有PCGRL研究聚焦于单智能体生成器，但面临频繁重新计算启发式质量指标和在大地图中导航的效率瓶颈。

Method: 通过将关卡生成构建为多智能体问题，减少奖励计算次数与智能体动作数的比例，并学习更局部化、模块化的设计策略。

Result: 多智能体关卡生成器在效率上有所提升，且能更好地泛化到分布外地图形状，显示出更强的适应性。

Conclusion: 将内容生成视为分布式多智能体任务有助于大规模生成功能性内容。

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [96] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: ECHO算法通过层次化上下文表示、基于目标的分析评估和共识投票，提高了多智能体系统中错误归因的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在分析复杂交互轨迹时，难以准确一致地定位智能体和步骤级别的错误，需要更有效的错误归因方法。

Method: 结合层次化上下文表示、基于目标的分析评估和共识投票机制，利用基于位置的上下文理解分层和客观评估标准。

Result: 实验结果显示ECHO在各种多智能体交互场景中优于现有方法，特别是在涉及微妙推理错误和复杂依赖关系的情况下表现突出。

Conclusion: 结构化层次化上下文表示与基于共识的客观决策相结合，为多智能体系统错误归因提供了更稳健的框架。

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [97] [Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding](https://arxiv.org/abs/2510.04899)
*Keane Ong,Wei Dai,Carol Li,Dewei Feng,Hengzhi Li,Jingyao Wu,Jiaee Cheong,Rui Mao,Gianmarco Mengaldo,Erik Cambria,Paul Pu Liang*

Main category: cs.AI

TL;DR: Human Behavior Atlas是一个统一的行为理解基准数据集，包含10万+多模态样本，用于训练统一模型来理解心理和社会行为。


<details>
  <summary>Details</summary>
Motivation: 现有工作使用专门数据集和单任务系统处理心理社会行为，但缺乏可扩展性、跨任务迁移和泛化能力。

Method: 构建Human Behavior Atlas数据集，涵盖文本、音频、视觉模态，训练三个OmniSapiens-7B模型（SFT、BAM、RL）。

Result: 在Human Behavior Atlas上训练的模型在多样化行为任务上持续优于现有多模态LLM，预训练还改善了向新行为数据集的迁移。

Conclusion: Human Behavior Atlas通过统一基准减少了冗余和成本，实现了跨任务的高效训练，并增强了行为特征的跨域泛化。

Abstract: Using intelligent systems to perceive psychological and social behaviors,
that is, the underlying affective, cognitive, and pathological states that are
manifested through observable behaviors and social interactions, remains a
challenge due to their complex, multifaceted, and personalized nature. Existing
work tackling these dimensions through specialized datasets and single-task
systems often miss opportunities for scalability, cross-task transfer, and
broader generalization. To address this gap, we curate Human Behavior Atlas, a
unified benchmark of diverse behavioral tasks designed to support the
development of unified models for understanding psychological and social
behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,
audio, and visual modalities, covering tasks on affective states, cognitive
states, pathologies, and social processes. Our unification efforts can reduce
redundancy and cost, enable training to scale efficiently across tasks, and
enhance generalization of behavioral features across domains. On Human Behavior
Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and
OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models
to consistently outperform existing multimodal LLMs across diverse behavioral
tasks. Pretraining on Human Behavior Atlas also improves transfer to novel
behavioral datasets; with the targeted use of behavioral descriptors yielding
meaningful performance gains.

</details>


### [98] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: MARS通过多智能体系统将System 1的快速直觉思维与System 2的深思熟虑推理相结合，解决了大型推理模型在简单任务中过度分析和适应动态环境的问题，在复杂推理任务中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在简单任务中存在过度分析倾向，过度使用System 2型深思熟虑推理导致token生成效率低下；同时由于预训练数据的静态性，难以适应快速变化的环境。

Method: 提出MARS多智能体系统，集成Google搜索、Google学术和Python解释器等外部工具获取最新信息；通过分工让System 1高效处理外部信息，System 2进行深度推理；采用多智能体强化学习框架优化两个系统的协作效率。

Result: 在Humanity's Last Exam基准上提升3.86%，在7个知识密集型任务上平均提升8.9%，验证了双系统范式在动态信息环境中复杂推理的有效性。

Conclusion: MARS通过整合System 1和System 2的认知过程，有效解决了大型推理模型的过度分析和环境适应问题，为复杂推理任务提供了创新解决方案。

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [99] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 提出一个跨市场算法交易系统，结合强化学习执行代理和独立合规代理，通过约束马尔可夫决策过程确保交易执行质量与合规性。


<details>
  <summary>Details</summary>
Motivation: 解决算法交易中执行质量与合规监管之间的平衡问题，确保交易行为符合市场规则和风险管理要求。

Method: 使用近端策略优化训练执行代理，运行时动作屏蔽确保行动可行性，并添加零知识合规审计层生成加密证明。

Result: 学习策略在ABIDES模拟器中减少了执行差额和方差，在各种压力测试场景下未观察到约束违规。

Conclusion: 该系统在最优执行、安全强化学习、监管技术和可验证AI的交叉领域具有应用前景，讨论了实际部署的伦理考虑和限制。

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


### [100] [Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI](https://arxiv.org/abs/2510.04978)
*Kun Xiang,Terry Jingchen Zhang,Yinya Huang,Jixi He,Zirong Liu,Yueling Tang,Ruizhe Zhou,Lijing Luo,Youpeng Wen,Xiuwei Chen,Bingqian Lin,Jianhua Han,Hang Xu,Hanhui Li,Bin Dong,Xiaodan Liang*

Main category: cs.AI

TL;DR: 该论文提供了物理AI的全面概述，区分了理论物理推理和应用物理理解，系统分析了物理基础方法如何增强AI在符号推理、具身系统和生成模型中的真实世界理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前物理感知和符号物理推理沿着分离的轨迹发展，缺乏统一的桥接框架，需要整合物理定律到AI系统中。

Method: 通过严格分析最新进展，建立理论物理推理与应用物理理解的明确区分，系统考察物理基础方法如何增强AI的真实世界理解。

Result: 提出了在物理原理和具身推理过程中基础学习的智能系统，超越模式识别走向对物理定律的真正理解。

Conclusion: 展望了能够解释物理现象和预测未来状态的下一代世界模型，推进安全、可泛化和可解释的AI系统。

Abstract: The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.

</details>


### [101] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: LLM-Hanabi基准测试评估LLM在合作游戏中的心智理论能力，发现一阶心智理论与游戏表现相关性更强，表明准确理解伙伴意图比高阶推理更重要。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在动态协作环境中推断他人行为动机的心智理论能力，这在多智能体协作中至关重要但研究不足。

Method: 开发LLM-Hanabi基准测试框架，使用合作游戏Hanabi评估LLM的心智理论能力，包含自动评估系统测量游戏表现和心智理论熟练度。

Result: 发现心智理论与游戏成功显著正相关，一阶心智理论（解释他人意图）比二阶心智理论（预测他人解释）与表现相关性更强。

Conclusion: 为增强AI协作能力，应优先发展一阶心智理论能力，准确理解伙伴动机比高阶推理更关键。

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [102] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: 提出了Think-Then-Embed框架，通过MLLM生成推理轨迹来增强复杂多模态查询的理解，在MMEB-V2基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将MLLMs用作编码器，忽略了其生成能力，在处理复杂指令和组合推理时效果不佳。

Method: TTE框架包含推理器和嵌入器：推理器MLLM生成解释复杂查询的推理轨迹，嵌入器基于原始查询和中间推理生成表示。

Result: 在MMEB-V2基准上超越专有模型；通过微调小型MLLM推理器，在开源模型中取得7%绝对性能提升；实现了推理器和嵌入器的统一集成。

Conclusion: 显式推理步骤能够更细致地理解复杂多模态指令，TTE框架在保持效率的同时显著提升了性能。

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


### [103] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: LAMIR算法通过从智能体-环境交互中学习不完美信息游戏的抽象模型，使测试时的前瞻推理在大型游戏中变得可行，提升了预训练智能体的游戏表现。


<details>
  <summary>Details</summary>
Motivation: 解决不完美信息游戏中模型学习困难的问题，因为现有方法难以在状态数量庞大的情况下进行有效的前瞻推理。

Method: 提出LAMIR算法，直接从智能体-环境交互中学习游戏的抽象模型，通过学习的抽象将每个子游戏限制在可管理的大小。

Result: 实验表明，在足够容量下LAMIR能学习到准确的底层游戏结构，在有限容量下仍能学习到有价值的抽象，提升预训练智能体在大型游戏中的表现。

Conclusion: LAMIR算法使理论上合理的前瞻推理在之前方法无法扩展的大型不完美信息游戏中变得可行，显著提升了智能体的游戏性能。

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [104] [Staircase Streaming for Low-Latency Multi-Agent Inference](https://arxiv.org/abs/2510.05059)
*Junlin Wang,Jue Wang,Zhen,Xu,Ben Athiwaratkun,Bhuwan Dhingra,Ce Zhang,James Zou*

Main category: cs.AI

TL;DR: 提出阶梯式流式处理方法来降低多智能体推理的延迟，通过部分输出立即生成最终响应，将首令牌时间减少高达93%


<details>
  <summary>Details</summary>
Motivation: 多智能体推理虽然能提高响应质量，但显著增加了首令牌时间，对延迟敏感应用构成挑战并影响用户体验

Method: 阶梯式流式处理：不等待前一步骤的完整中间输出，而是在收到部分输出时立即开始生成最终响应

Result: 实验结果显示阶梯式流式处理将首令牌时间减少高达93%，同时保持响应质量

Conclusion: 阶梯式流式处理是解决多智能体推理延迟问题的有效方法，能在保持质量的同时显著降低响应延迟

Abstract: Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [105] [Sensing Performance Analysis in Cooperative Air-Ground ISAC Networks for LAE](https://arxiv.org/abs/2510.03642)
*Yihang Jiang,Xiaoyang Li,Guangxu Zhu,Xiaowen Cao,Kaifeng Han,Bingpeng Zhou,Xinyi Wang*

Main category: cs.IT

TL;DR: 本文评估了空地一体化感知通信网络中基于恒定虚警率的区域雷达检测覆盖概率，分析了聚合感知干扰分布，相比最强干扰近似方法更适合高密度基站场景。


<details>
  <summary>Details</summary>
Motivation: 为支持低空经济发展，需要构建空地一体化感知通信网络来提供可靠稳健的通信和感知服务。

Method: 通过分析聚合感知干扰分布来评估协作空地ISAC网络的感知能力，重点关注区域雷达检测覆盖概率。

Result: 仿真验证了分析结果，表明考虑聚合感知干扰比最强干扰近似方法更适合高密度基站的小蜂窝场景。

Conclusion: 在空地一体化感知通信网络中，考虑聚合感知干扰能够更准确地评估感知性能，特别是在基站密度较高的场景下。

Abstract: To support the development of low altitude economy, the air-ground integrated
sensing and communication (ISAC) networks need to be constructed to provide
reliable and robust communication and sensing services. In this paper, the
sensing capabilities in the cooperative air-ground ISAC networks are evaluated
in terms of area radar detection coverage probability under a constant false
alarm rate, where the distribution of aggregated sensing interferences is
analyzed as a key intermediate result. Compared with the analysis based on the
strongest interferer approximation, taking the aggregated sensing interference
into consideration is better suited for pico-cell scenarios with high base
station density. Simulations are conducted to validate the analysis.

</details>


### [106] [Privacy Enhancement in Over-the-Air Federated Learning via Adaptive Receive Scaling](https://arxiv.org/abs/2510.03860)
*Faeze Moradi Kalarde,Ben Liang,Min Dong,Yahia A. Eldemerdash Ahmed,Ho Ting Cheng*

Main category: cs.IT

TL;DR: 提出AdaScale算法，在联邦学习的空中聚合中自适应设计接收缩放因子，平衡训练收敛与隐私保护之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，接收缩放因子的大小会影响训练性能和隐私保护。较大的缩放因子能降低噪声功率提高训练效果，但会减少不确定性从而损害隐私。需要动态调整缩放因子来平衡这一矛盾。

Method: 将问题建模为随机优化问题，最小化整体Rényi差分隐私泄漏，同时确保全局损失函数收敛。开发了AdaScale在线算法，通过一系列每轮可高效求解的问题来实现自适应设计。

Result: 理论分析表明AdaScale实现了递减的动态遗憾，在时间平均RDP泄漏方面表现优异，同时确保FL训练收敛到稳定点。数值实验显示相比现有方法能有效减少RDP和DP泄漏且不损害学习性能。

Conclusion: AdaScale算法成功解决了联邦学习中训练收敛与隐私保护的平衡问题，在动态信道条件下实现了有效的隐私保护而不影响学习效果。

Abstract: In Federated Learning (FL) with over-the-air aggregation, the quality of the
signal received at the server critically depends on the receive scaling
factors. While a larger scaling factor can reduce the effective noise power and
improve training performance, it also compromises the privacy of devices by
reducing uncertainty. In this work, we aim to adaptively design the receive
scaling factors across training rounds to balance the trade-off between
training convergence and privacy in an FL system under dynamic channel
conditions. We formulate a stochastic optimization problem that minimizes the
overall R\'enyi differential privacy (RDP) leakage over the entire training
process, subject to a long-term constraint that ensures convergence of the
global loss function. Our problem depends on unknown future information, and we
observe that standard Lyapunov optimization is not applicable. Thus, we develop
a new online algorithm, termed AdaScale, based on a sequence of novel per-round
problems that can be solved efficiently. We further derive upper bounds on the
dynamic regret and constraint violation of AdaSacle, establishing that it
achieves diminishing dynamic regret in terms of time-averaged RDP leakage while
ensuring convergence of FL training to a stationary point. Numerical
experiments on canonical classification tasks show that our approach
effectively reduces RDP and DP leakages compared with state-of-the-art
benchmarks without compromising learning performance.

</details>


### [107] [Multi-Modal Multi-Task Semantic Communication: A Distributed Information Bottleneck Perspective](https://arxiv.org/abs/2510.04000)
*Yujie Zhou,Yiwei Liao,Cheng Peng,Yong Xiao,Yingyu Li*

Main category: cs.IT

TL;DR: 提出PoM²-DIB框架，通过扩展分布式信息瓶颈理论，引入模态选择机制来解决多模态多任务语义通信中的冗余传输问题


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的多模态多任务语义通信方案需要发射端拥有完整模态数据参与所有接收端任务，导致冗余传输并违反信道容量和计算能力的物理限制

Method: 扩展分布式信息瓶颈理论，引入模态选择作为关键设计变量，将模态选择松弛为概率形式，使用评分函数估计和公共随机性实现分布式设备的可优化协调决策

Result: 在公共数据集上的实验结果表明，PoM²-DIB在各种任务中相比全参与基线方法，在物理限制下仍能实现高质量推理

Conclusion: PoM²-DIB框架通过模态选择和分布式信息瓶颈编码，在遵守物理约束的同时实现了通信效率与推理质量的有效权衡

Abstract: Semantic communication (SemCom) shifts the focus from data transmission to
meaning delivery, enabling efficient and intelligent communication.
  Existing AI-based coding schemes for multi-modal multi-task SemCom often
require transmitters with full-modal data to participate in all receivers'
tasks, which leads to redundant transmissions and conflicts with the physical
limits of channel capacity and computational capability.
  In this paper, we propose PoM$^2$-DIB, a novel framework that extends the
distributed information bottleneck (DIB) theory to address this problem.
  Unlike the typical DIB, this framework introduces modality selection as an
additional key design variable, enabling a more flexible tradeoff between
communication rate and inference quality.
  This extension selects only the most relevant modalities for task
participation, adhering to the physical constraints, while following efficient
DIB-based coding.
  To optimize selection and coding end-to-end, we relax modality selection into
a probabilistic form, allowing the use of score function estimation with common
randomness to enable optimizable coordinated decisions across distributed
devices.
  Experimental results on public datasets verify that PoM$^2$-DIB achieves high
inference quality compared to full-participation baselines in various tasks
under physical limits.

</details>


### [108] [Volume-Based Lower Bounds to the Capacity of the Gaussian Channel Under Pointwise Additive Input Constraints](https://arxiv.org/abs/2510.04095)
*Neri Merhav,Shlomo Shamai*

Main category: cs.IT

TL;DR: 提出了高斯信道在点式加性输入约束下的容量下界，包括两类下界：基于熵功率不等式的方法（仅适用于连续输入）和基于互信息直接操作的方法（更通用但更复杂）。


<details>
  <summary>Details</summary>
Motivation: 研究高斯信道在点式加性约束（而非期望约束）下的容量下界，这类约束在实践中有重要应用价值。

Method: 利用类型方法扩展到连续字母表、鞍点积分方法和大偏差理论工具，精确评估满足约束的输入向量集合的体积指数。

Result: 提出了两类容量下界：第一类基于EPI仅适用于连续输入，第二类更通用但技术更复杂，数值示例验证了两种下界的有效性。

Conclusion: 该工作为高斯信道在点式加性约束下的容量分析提供了系统化的下界框架，具有较好的理论价值和实用意义。

Abstract: We present a family of relatively simple and unified lower bounds on the
capacity of the Gaussian channel under a set of pointwise additive input
constraints. Specifically, the admissible channel input vectors $\bx = (x_1,
\ldots, x_n)$ must satisfy $k$ additive cost constraints of the form
$\sum_{i=1}^n \phi_j(x_i) \le n \Gamma_j$, $j = 1,2,\ldots,k$, which are
enforced pointwise for every $\bx$, rather than merely in expectation. More
generally, we also consider cost functions that depend on a sliding window of
fixed length $m$, namely, $\sum_{i=m}^n \phi_j(x_i, x_{i-1}, \ldots, x_{i-m+1})
\le n \Gamma_j$, $j = 1,2,\ldots,k$, a formulation that naturally accommodates
correlation constraints as well as a broad range of other constraints of
practical relevance. We propose two classes of lower bounds, derived by two
methodologies that both rely on the exact evaluation of the volume exponent
associated with the set of input vectors satisfying the given constraints. This
evaluation exploits extensions of the method of types to continuous alphabets,
the saddle-point method of integration, and basic tools from large deviations
theory. The first class of bounds is obtained via the entropy power inequality
(EPI), and therefore applies exclusively to continuous-valued inputs. The
second class, by contrast, is more general, and it applies to discrete input
alphabets as well. It is based on a direct manipulation of mutual information,
and it yields stronger and tighter bounds, though at the cost of greater
technical complexity. Numerical examples illustrating both types of bounds are
provided, and several extensions and refinements are also discussed.

</details>


### [109] [Optimal frames for Phase Retrieval from Edge Vectors of Optimal Polygons](https://arxiv.org/abs/2510.04099)
*Zhiqiang Xu,Zili Xu,Xinyue Zhang*

Main category: cs.IT

TL;DR: 本文揭示了相位恢复最优框架与周长最大化等径问题之间的联系，证明了在二维实空间中，所有周长最大化等径问题的最优解都对应相位恢复的最优框架，并证明了当m≥4为偶数时，调和框架Em不是最优的，推翻了之前的猜想。


<details>
  <summary>Details</summary>
Motivation: 研究相位恢复的最优框架特性，探索其与离散几何中经典问题的联系，为相位恢复理论提供新的几何视角。

Method: 通过将最优多边形问题重新表述为关于单位根差异性的问题，利用周长最大化等径问题的已有结果来推导相位恢复最优框架的特性。

Result: 证明了在二维实空间中，当m≥3且包含奇数因子时，可以完全刻画所有最优相位恢复框架；特别地，证明了当m为偶数时调和框架Em不是最优的。

Conclusion: 建立了相位恢复与离散几何的深刻联系，推翻了关于调和框架最优性的猜想，为相位恢复理论提供了新的几何理解框架。

Abstract: This paper aims to characterize the optimal frame for phase retrieval,
defined as the frame whose condition number for phase retrieval attains its
minimal value. In the context of the two-dimensional real case, we reveal the
connection between optimal frames for phase retrieval and the
perimeter-maximizing isodiametric problem, originally proposed by Reinhardt in
1922. Our work establishes that every optimal solution to the
perimeter-maximizing isodiametric problem inherently leads to an optimal frame
in ${\mathbb R}^2$. By recasting the optimal polygons problem as one concerning
the discrepancy of roots of unity, we characterize all optimal polygons.
Building upon this connection, we then characterize all optimal frames with $m$
vectors in ${\mathbb R}^2$ for phase retrieval when $m \geq 3$ has an odd
factor. As a key corollary, we show that the harmonic frame $E_m$ is {\em not}
optimal for any even integer $m \geq 4$. This finding disproves a conjecture
proposed by Xia, Xu, and Xu (Math. Comp., 90(356): 2931-2960). Previous work
has established that the harmonic frame $E_m \subset {\mathbb R}^2$ is indeed
optimal when $m$ is an odd integer.
  Exploring the connection between phase retrieval and discrete geometry, this
paper aims to illuminate advancements in phase retrieval and offer new
perspectives on the perimeter-maximizing isodiametric problem.

</details>


### [110] [Multiplicative Turing Ensembles, Pareto's Law, and Creativity](https://arxiv.org/abs/2510.04167)
*Alexander Kolpakov,Aidan Rocke*

Main category: cs.IT

TL;DR: 该论文研究整数乘法动力学，将宏观统计与通用编码长度联系起来，提出乘法图灵系综(MTE)模型，并通过变分原理推导出Gibbs先验分布。理论表明该模型在机器适应机制下呈现清洁平均行为，而人类生成复杂性则超出此机制。


<details>
  <summary>Details</summary>
Motivation: 研究整数乘法动力学与通用编码长度之间的联系，探索从概率图灵机系综中自然产生的乘法图灵系综(MTE)，并建立变分建模框架。

Method: 引入乘法图灵系综(MTE)，以Elias Omega编码长度作为能量函数，施加最大熵约束推导出整数和素数的规范Gibbs先验分布。在温和尾部假设下，该先验诱导对数乘子的指数尾部。

Result: 理论证明MTE轨迹上Omega编码长度的时间平均定律。在Debian和PyPI包大小数据集上的实证显示，缩放Omega先验在编码长度直方图上获得最低KL散度。

Conclusion: 理论-数据比较表明存在定性分裂：机器适应机制(Gibbs对齐，有限一阶矩)呈现清洁平均行为，而人类生成复杂性超出此机制，尾部足够重导致无界一阶矩，无法实现同类平均。

Abstract: We study integer-valued multiplicative dynamics driven by i.i.d. prime
multipliers and connect their macroscopic statistics to universal codelengths.
We introduce the Multiplicative Turing Ensemble (MTE) and show how it arises
naturally - though not uniquely - from ensembles of probabilistic Turing
machines. Our modeling principle is variational: taking Elias' Omega codelength
as an energy and imposing maximum entropy constraints yields a canonical Gibbs
prior on integers and, by restriction, on primes. Under mild tail assumptions,
this prior induces exponential tails for log-multipliers (up to slowly varying
corrections), which in turn generate Pareto tails for additive gaps. We also
prove time-average laws for the Omega codelength along MTE trajectories.
Empirically, on Debian and PyPI package size datasets, a scaled Omega prior
achieves the lowest KL divergence against codelength histograms. Taken
together, the theory-data comparison suggests a qualitative split:
machine-adapted regimes (Gibbs-aligned, finite first moment) exhibit clean
averaging behavior, whereas human-generated complexity appears to sit beyond
this regime, with tails heavy enough to produce an unbounded first moment, and
therefore no averaging of the same kind.

</details>


### [111] [Relative Divergence and Maximum Relative Divergence Principle for Grading Functions on Partially Ordered Sets](https://arxiv.org/abs/2510.04314)
*Alexander Dukhovny*

Main category: cs.IT

TL;DR: 本文提出相对散度(RD)和最大相对散度原理(MRDP)作为给定先验信息下不充分理由原理(IRP+)的表达，用于偏序集上的分级函数。


<details>
  <summary>Details</summary>
Motivation: 在给定先验信息条件下，寻找不充分理由原理的数学表达方式，并将其应用于偏序集上的分级函数。

Method: 使用相对散度和最大相对散度原理，分析偏序集结构，并将其应用于标准偏序集如幂集、链的直积等。

Result: 将经典概率论公式表示为MRDP问题在连接偏序集上的IRP+解，并分析了群体检测和单服务器多队列等应用。

Conclusion: MRDP为在偏序集上表达不充分理由原理提供了有效框架，能够统一处理多种概率论和应用问题。

Abstract: Relative Divergence (RD) and Maximum Relative Divergence Principle (MRDP) for
grading (order-comonotonic) functions (GF) on posets are used as an expression
of Insufficient Reason Principle under the given prior information (IRP+).
Classic Probability Theory formulas are presented as IRP+ solutions of MRDP
problems on conjoined posets. RD definition principles are analyzed in relation
to the poset structure. MRDP techniques are presented for standard posets:
power sets, direct products of chains, etc. "Population group-testing" and
"Single server of multiple queues" applications are stated and analyzed as
"IRP+ by MRDP" problems on conjoined base posets.

</details>


### [112] [Compressed Newton-direction-based Thresholding Methods for Sparse Optimization Problems](https://arxiv.org/abs/2510.04451)
*Nan Meng,Yun-Bin Zhao*

Main category: cs.IT

TL;DR: 提出基于压缩牛顿方向的阈值追踪算法(CNHTP和CNOTP)，通过将牛顿步限制在低维子空间并添加对角正则化来降低计算成本，同时保持牛顿类方法的效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏优化问题中的阈值算法涉及搜索方向和阈值策略两个关键组件，需要找到既能降低计算成本又能保持效率的方法。

Method: 使用压缩牛顿方向作为搜索方向，通过将经典牛顿步限制在低维子空间并嵌入到全空间中进行对角正则化，然后采用硬阈值或最优阈值策略。

Result: 在限制等距性质下建立了算法的全局收敛性，实验结果表明在求解稀疏优化问题时，所提算法在成功频率和解精度方面与几种最先进方法相当。

Conclusion: 基于压缩牛顿方向的阈值追踪算法在保持牛顿类方法效率的同时显著降低了计算成本，为解决稀疏优化问题提供了有效的解决方案。

Abstract: Thresholding algorithms for sparse optimization problems involve two key
components: search directions and thresholding strategies. In this paper, we
use the compressed Newton direction as a search direction, derived by confining
the classical Newton step to a low-dimensional subspace and embedding it back
into the full space with diagonal regularization. This approach significantly
reduces the computational cost for finding the search direction while
maintaining the efficiency of Newton-like methods. Based on this new search
direction, we propose two major classes of algorithms by adopting hard or
optimal thresholding: the compressed Newton-direction-based thresholding
pursuit (CNHTP) and compressed Newton-direction-based optimal thresholding
pursuit (CNOTP). We establish the global convergence of the proposed algorithms
under the restricted isometry property. Experimental results demonstrate that
the proposed algorithms perform comparably to several state-of-the-art methods
in terms of success frequency and solution accuracy for solving the sparse
optimization problem.

</details>


### [113] [Learning Function-to-Function Mappings: A Fourier Neural Operator for Next-Generation MIMO Systems](https://arxiv.org/abs/2510.04664)
*Jian Xiao,Ji Wang,Qi Sun,Qimei Cui,Xingwang Li,Dusit Niyato,Chih-Lin I*

Main category: cs.IT

TL;DR: 本文提出傅里叶神经算子（FNO）作为解决下一代大规模MIMO系统物理层信号处理挑战的有力工具，通过函数到函数的映射能力有效建模电磁波传播等复杂物理系统。


<details>
  <summary>Details</summary>
Motivation: 下一代MIMO系统面临近场传播、连续孔径建模、亚波长天线耦合效应和动态信道条件等挑战，传统模型驱动和深度学习方法难以处理其巨大的计算复杂度和模型不准确性。

Method: 使用傅里叶神经算子（FNO）学习无限维函数空间之间的函数到函数映射，利用其在傅里叶域中有效捕获全局依赖性的能力，实现无网格建模。

Result: 在新型MIMO架构的信道建模和估计等代表性案例研究中，FNO相比最先进方法展现出优越性能。

Conclusion: FNO是实现下一代MIMO系统巨大潜力的有前景技术，文章还讨论了开放挑战和未来研究方向。

Abstract: Next-generation multiple-input multiple-output (MIMO) systems, characterized
by extremely large-scale arrays, holographic surfaces, three-dimensional
architectures, and flexible antennas, are poised to deliver unprecedented data
rates, spectral efficiency and stability. However, these advancements introduce
significant challenges for physical layer signal processing, stemming from
complex near-field propagation, continuous aperture modeling, sub-wavelength
antenna coupling effects, and dynamic channel conditions. Conventional
model-based and deep learning approaches often struggle with the immense
computational complexity and model inaccuracies inherent in these new regimes.
This article proposes a Fourier neural operator (FNO) as a powerful and
promising tool to address these challenges. The FNO learns function-to-function
mappings between infinite-dimensional function spaces, making them
exceptionally well-suited for modeling complex physical systems governed by
partial differential equations based on electromagnetic wave propagation. We
first present the fundamental principles of FNO, demonstrating its mesh-free
nature and function-to-function ability to efficiently capture global
dependencies in the Fourier domain. Furthermore, we explore a range of
applications of FNO in physical-layer signal processing for next-generation
MIMO systems. Representative case studies on channel modeling and estimation
for novel MIMO architectures demonstrate the superior performance of FNO
compared to state-of-the-art methods. Finally, we discuss open challenges and
outline future research directions, positioning FNO as a promising technology
for enabling the enormous potential of next-generation MIMO systems.

</details>


### [114] [Multi-Agent Distributed Optimization With Feasible Set Privacy](https://arxiv.org/abs/2510.05068)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: 该论文研究多智能体在保持各自可行集隐私的前提下，通过去中心化方式联合学习最优解集的问题。提出在环状和星状网络拓扑下，实现名义信息泄露的可行方案，并分析通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体联合优化时既要学习最优解集，又要保护各自可行集隐私的挑战。避免直接使用私有集合交集协议导致的信息泄露超出名义值的问题。

Method: 设计领导-非领导智能体通信协议，领导通过查询-应答方式搜索解集。在环状和星状网络拓扑下实现方案，并与阈值私有集合交集协议建立联系。

Result: 提出的方案能在名义信息泄露下获得解集，且对于随机映射的目标函数，相比直接使用PSI检索整个可行集，通信效率更高。

Conclusion: 该工作为隐私保护的分布式优化提供了有效方案，在保持隐私的同时实现了通信效率的提升，并与现有PSI变体建立了理论联系。

Abstract: We consider the problem of decentralized constrained optimization with
multiple agents $E_1,\ldots,E_N$ who jointly wish to learn the optimal solution
set while keeping their feasible sets $\mathcal{P}_1,\ldots,\mathcal{P}_N$
private from each other. We assume that the objective function $f$ is known to
all agents and each feasible set is a collection of points from a universal
alphabet $\mathcal{P}_{alph}$. A designated agent (leader) starts the
communication with the remaining (non-leader) agents, and is the first to
retrieve the solution set. The leader searches for the solution by sending
queries to and receiving answers from the non-leaders, such that the
information on the individual feasible sets revealed to the leader should be no
more than nominal, i.e., what is revealed from learning the solution set alone.
We develop achievable schemes for obtaining the solution set at nominal
information leakage, and characterize their communication costs under two
communication setups between agents. In this work, we focus on two kinds of
network setups: i) ring, where each agent communicates with two adjacent
agents, and ii) star, where only the leader communicates with the remaining
agents. We show that, if the leader first learns the joint feasible set through
an existing private set intersection (PSI) protocol and then deduces the
solution set, the information leaked to the leader is greater than nominal.
Moreover, we draw connection of our schemes to threshold PSI (ThPSI), which is
a PSI-variant where the intersection is revealed only when its cardinality is
larger than a threshold value. Finally, for various realizations of $f$ mapped
uniformly at random to a fixed range of values, our schemes are more
communication-efficient with a high probability compared to retrieving the
entire feasible set through PSI.

</details>
