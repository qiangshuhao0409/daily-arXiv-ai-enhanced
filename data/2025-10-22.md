<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 47]
- [cs.IT](#cs.IT) [Total: 3]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A New Broadcast Model for Several Network Topologies](https://arxiv.org/abs/2510.18058)
*Hongbo Lu,Junsung Hwang,Bernard Tenreiro,Nabila Jaman Tripti,Darren Hamilton,Yuefan Deng*

Main category: cs.NI

TL;DR: BBS是一种优化的广播算法，通过平衡饱和机制最大化节点利用率，在各种网络拓扑中显著提升通信效率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决大规模系统（如超级计算机）中广播操作面临的拓扑约束、带宽限制和同步开销等挑战，优化通信效率。

Method: 采用平衡饱和广播机制，通过精确的通信周期实现可重复、流线化的逐步广播框架，确保节点在整个广播过程中保持持续活动。

Result: 在各种拓扑结构的模拟中，BBS算法始终优于常见的通用广播算法，且优势显著。

Conclusion: BBS是一个通用且鲁棒的广播框架，有潜力重新定义跨网络拓扑的广播策略。

Abstract: We present Broadcast by Balanced Saturation (BBS), a general broadcast
algorithm designed to optimize communication efficiency across diverse network
topologies. BBS maximizes node utilization, addressing challenges in broadcast
operations such as topology constraints, bandwidth limitations, and
synchronization overhead, particularly in large-scale systems like
supercomputers. The algorithm ensures sustained activity with nodes throughout
the broadcast, thereby enhancing data propagation and significantly reducing
latency. Through a precise communication cycle, BBS provides a repeatable,
streamlined, stepwise broadcasting framework. Simulation results across various
topologies demonstrate that the BBS algorithm consistently outperforms common
general broadcast algorithms, often by a substantial margin. These findings
suggest that BBS is a versatile and robust framework with the potential to
redefine broadcast strategies across network topologies.

</details>


### [2] [Revisiting RFID Missing Tag Identification](https://arxiv.org/abs/2510.18285)
*Kanghuai Liu,Lin Chen,Jihong Yu,Junyi Huang,Shiyuan Liu*

Main category: cs.NI

TL;DR: 本文重新研究RFID网络中的缺失标签识别问题，通过理论分析和算法设计，显著提升了识别效率。


<details>
  <summary>Details</summary>
Motivation: 现有缺失标签识别算法的执行时间仍有优化空间，需要建立理论性能极限并开发更高效的算法。

Method: 使用基于标签伪ID子集构建的碰撞分区树(CPT)数据结构，实现更平衡的树结构，减少解析时间。

Result: 新算法执行时间为Θ(loglogN/logN·N + (1-α)²(1-δ)²/ε²)，比现有最佳算法提升高达logN倍。

Conclusion: 提出的CPT树结构有效降低了缺失标签识别的时间复杂度，为RFID网络提供了更高效的识别方案。

Abstract: We revisit the problem of missing tag identification in RFID networks by
making three contributions. Firstly, we quantitatively compare and gauge the
existing propositions spanning over a decade on missing tag identification. We
show that the expected execution time of the best solution in the literature is
$\Theta \left(N+\frac{(1-\alpha)^2(1-\delta)^2}{ \epsilon^2}\right)$, where
$\delta$ and $\epsilon$ are parameters quantifying the required identification
accuracy, $N$ denotes the number of tags in the system, among which $\alpha N$
tags are missing. Secondly, we analytically establish the expected execution
time lower-bound for any missing tag identification algorithm as
$\Theta\left(\frac{N}{\log N}+\frac{(1-\delta)^2(1-\alpha)^2}{\epsilon^2 \log
\frac{(1-\delta)(1-\alpha)}{\epsilon}}\right)$, thus giving the theoretical
performance limit. Thirdly, we develop a novel missing tag identification
algorithm by leveraging a tree structure with the expected execution time of
$\Theta \left(\frac{\log\log N}{\log N}N+\frac{(1-\alpha)^2(1-\delta)^2}{
\epsilon^2}\right)$, reducing the time overhead by a factor of up to $\log N$
over the best algorithm in the literature. The key technicality in our design
is a novel data structure termed as collision-partition tree (CPT), built on a
subset of bits in tag pseudo-IDs, leading to more balanced tree structure and
reducing the time complexity in parsing the entire tree.

</details>


### [3] [On AI Verification in Open RAN](https://arxiv.org/abs/2510.18417)
*Rahul Soundrarajan,Claudio Fiandrino,Michele Polese,Salvatore D'Oro,Leonardo Bonati,Tommaso Melodia*

Main category: cs.NI

TL;DR: 提出基于可解释模型的轻量级验证方法，用于验证Open RAN中深度强化学习代理的行为，通过决策树验证器在运行时执行一致性检查


<details>
  <summary>Details</summary>
Motivation: 虽然可解释AI有助于缓解AI模型的不透明性，但仅靠可解释性不能保证可靠的网络运营，需要验证AI行为

Method: 使用基于决策树的验证器在运行时执行近实时一致性检查，避免计算昂贵的现有验证器

Result: 提出了可扩展的架构集成，并展示了基于决策树的切片验证器的可行性

Conclusion: 为确保Open RAN中可信AI的采用，需要解决未来的挑战

Abstract: Open RAN introduces a flexible, cloud-based architecture for the Radio Access
Network (RAN), enabling Artificial Intelligence (AI)/Machine Learning
(ML)-driven automation across heterogeneous, multi-vendor deployments. While
EXplainable Artificial Intelligence (XAI) helps mitigate the opacity of AI
models, explainability alone does not guarantee reliable network operations. In
this article, we propose a lightweight verification approach based on
interpretable models to validate the behavior of Deep Reinforcement Learning
(DRL) agents for RAN slicing and scheduling in Open RAN. Specifically, we use
Decision Tree (DT)-based verifiers to perform near-real-time consistency checks
at runtime, which would be otherwise unfeasible with computationally expensive
state-of-the-art verifiers. We analyze the landscape of XAI and AI
verification, propose a scalable architectural integration, and demonstrate
feasibility with a DT-based slice-verifier. We also outline future challenges
to ensure trustworthy AI adoption in Open RAN.

</details>


### [4] [JAUNT: Joint Alignment of User Intent and Network State for QoE-centric LLM Tool Routing](https://arxiv.org/abs/2510.18550)
*Enhan Li,Hongyang Du*

Main category: cs.NI

TL;DR: JAUNT框架通过联合对齐用户意图和网络状态，在QoE中心化的工具路由中显著提升用户体验质量


<details>
  <summary>Details</summary>
Motivation: 当前工具路由机制仅考虑用户查询与工具的功能匹配，但用户意图可能模糊，实际体验质量还取决于网络延迟、服务器可用性等外部因素

Method: 引入双视图对齐策略，解释用户意图的同时使用LLM代理构建网络配置文件，将数值性能指标映射到语义空间以指导路由

Result: 实验结果显示JAUNT相比多个基线方法显著改善了QoE，证明了意图和网络状态对齐对于可扩展LLM服务编排的重要性

Conclusion: JAUNT框架通过联合考虑用户意图和网络状态，为QoE中心化的工具路由提供了有效解决方案

Abstract: Large Language Models (LLMs) increasingly rely on emerging protocols such as
the Model Context Protocol (MCP) to invoke external tools and services.
However, current tool routing mechanisms remain fragile because they only
consider functional matching between users' queries and tools. In practice,
user intent expressed through queries can be vague or underspecified, and the
actual Quality of Experience (QoE) also depends on external factors such as
link latency and server availability that are not captured by semantics alone.
To address this challenge, we propose JAUNT, a framework for Joint Alignment of
User intent and Network state in QoE-centric Tool routing. JAUNT introduces a
dual-view alignment strategy that interprets user intent while employing LLM
agents to construct network profiles, mapping numerical performance indicators
into the semantic space to guide routing. We further design a benchmark that
integrates diverse user request patterns with heterogeneous network states,
enabling systematic evaluation of QoE outcomes. Experimental results show that
JAUNT significantly improves QoE compared with several baselines, demonstrating
the importance of aligning both intent and network state for scalable LLM
service orchestration.

</details>


### [5] [Formal Methods for Mobile Ad Hoc Networks: A Survey](https://arxiv.org/abs/2510.18730)
*Wan Fokkink,Rob van Glabbeek*

Main category: cs.NI

TL;DR: 这篇论文是关于移动自组织网络(MANET)协议形式化分析技术的综述，重点关注路由协议的功能正确性、实时性和安全性分析。


<details>
  <summary>Details</summary>
Motivation: 由于MANET中通信是无线且节点可独立移动，正确分析其协议的功能正确性、性能和安全性具有挑战性。该领域研究较为分散，需要系统性的综述来提供全面概览。

Method: 采用文献综述方法，系统梳理了应用于MANET协议分析的各种形式化规范和分析技术，特别关注专门针对MANET的形式化框架和性能分析所依赖的移动性模型。

Result: 提供了对MANET协议形式化分析领域的全面和连贯概览，涵盖了功能正确性、实时属性和安全性等多个方面，以及各种严格形式化方法在不同MANET协议分析中的应用。

Conclusion: 该综述为这个分散的研究领域提供了系统性的整理，展示了多种严格形式化方法在分析各类MANET协议不同方面的应用价值。

Abstract: In a mobile ad hoc network (MANET), communication is wireless and nodes can
move independently. Properly analyzing the functional correctness, performance,
and security of MANET protocols is a challenging task. A wide range of formal
specification and analysis techniques have been employed in the analysis of
MANET protocols. This survey presents an overview of rigorous formal analysis
techniques and their applications, with a focus on MANET routing protocols.
Next to functional correctness, also real-time properties and security are
considered. Moreover, an overview is given of formal frameworks that target
MANETs specifically, as well as mobility models that underlie performance
analyses of MANET protocols. The aim is to give a comprehensive and coherent
overview of this rather scattered field, in which a variety of rigorous formal
methods have been applied to analyze different aspects of a wide range of MANET
protocols.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures](https://arxiv.org/abs/2510.17902)
*Al Kari*

Main category: cs.AI

TL;DR: CAST框架通过激活空间映射实现不同架构LLM间LoRA适配器的零样本迁移，解决了模型架构锁定问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM微调后行为被锁定在特定架构中的问题，现有权重空间迁移方法脆弱且间接。

Method: 学习双向投影头在激活流形间建立非线性映射，将目标模型激活转换为源模型空间，应用冻结的LoRA核，再转换回目标空间。

Result: 在Llama-2和Mistral等异构模型间迁移LoRA适配器，达到重新训练LoRA性能的85-95%，优于现有权重空间迁移方法。

Conclusion: CAST实现了真正的零样本LoRA迁移，建立了模型互操作性的新标准，有效解耦学习技能与源架构的绑定。

Abstract: The proliferation of Large Language Model (LLM) architectures presents a
fundamental challenge: valuable, task-specific behaviors learned through
fine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped
within their source model's architecture, herein referred to architectural
lock-in. Existing transfer methods attempt to bridge this gap by aligning the
static weight spaces of models, a brittle and indirect approach that relies on
tenuous correlations between parameter geometries. This paper introduces a
fundamentally different and more direct paradigm: the Cartridge Activation
Space Transfer (CAST), a novel framework that liberates LoRA-encoded behaviors
by learning a direct, nonlinear mapping between the activation manifolds, the
geometric structures formed by the model's internal neuron activations, of two
distinct LLM architectures. CAST treats a pre-trained LoRA as a frozen
"behavioral kernel." It learns a set of lightweight, bidirectional projection
heads that translate the target model's activation stream into the source
model's latent space, apply the frozen kernel, and project the result back.
This process, trained on a general text corpus without any task-specific data,
effectively decouples the learned skill from the source architecture. We
demonstrate that CAST enables true "zero-shot" translation of any standard LoRA
adapter. Our experiments, including transfers between heterogeneous model
families like Llama-2 and Mistral, show that CAST-translated adapters achieve
85-95\% of the performance of a LoRA fully retrained on the target model,
quantitatively outperforming current weight-space transfer techniques and
establishing a new state-of-the-art in model interoperability.

</details>


### [7] [Beyond More Context: Retrieval Diversity Boosts Multi-Turn Intent Understanding](https://arxiv.org/abs/2510.17940)
*Zhiming Lin*

Main category: cs.AI

TL;DR: 提出了一个多样性感知检索框架，通过平衡意图覆盖和语言多样性来选择上下文示例，在固定token预算下显著提升LLM意图理解性能


<details>
  <summary>Details</summary>
Motivation: 面向任务的聊天机器人在实际部署中面临严格的token预算和噪声上下文，现有检索流程强调相关性但忽略了集合层面的多样性和混淆因素

Method: 构建多样性感知检索框架，选择上下文示例来平衡意图覆盖和语言多样性，并与标准LLM解码器集成；评估采用预算匹配提示和随机化位置

Result: 在MultiWOZ 2.4和SGD数据集上，在相同token预算下实现了联合目标准确率的显著提升，超越了强LLM/DST基线

Conclusion: 研究分离并验证了检索中内容多样性的影响，为构建准确、预算受限的多轮意图系统提供了简单可部署的选择原则

Abstract: Multi turn intent understanding is central to task oriented chatbots, yet
real deployments face tight token budgets and noisy contexts, and most
retrieval pipelines emphasize relevance while overlooking set level diversity
and confounds such as more context or exemplar order. We ask whether retrieval
diversity, rather than longer prompts, systematically improves LLM intent
understanding under fixed budgets. We present a diversity aware retrieval
framework that selects in context exemplars to balance intent coverage and
linguistic variety, and integrates this selection with standard LLM decoders;
the evaluation enforces budget matched prompts and randomized positions, and
includes sensitivity analyses over exemplar count, diversity strength, and
backbone size. On MultiWOZ 2.4 and SGD, the approach achieves strong gains in
Joint Goal Accuracy under equal token budgets, surpassing strong LLM/DST
baselines, with consistent improvements across K from 4 to 7 and moderate
latency. Overall, the study isolates and validates the impact of content
diversity in retrieval and offers a simple, deployable selection principle for
building accurate, budget constrained multi turn intent systems.

</details>


### [8] [FABRIC: Framework for Agent-Based Realistic Intelligence Creation](https://arxiv.org/abs/2510.17995)
*Abhigya Verma,Seganrasan Subramanian,Nandhakumar Kandasamy,Naman Gupta*

Main category: cs.AI

TL;DR: 提出了一个仅使用LLM合成智能体数据的统一框架，无需人工监督，能够生成包含任务规范、工具定义、策略伪代码、自然语言交互和执行轨迹的完整交互记录。


<details>
  <summary>Details</summary>
Motivation: 收集智能体数据需要人工标注，成本高、耗时长且难以扩展，因此需要一种可扩展的合成数据生成方法。

Method: 采用模块化流水线生成符合严格语法和语义约束的交互记录，支持单任务、多任务和多轮交互，并集成了约束生成格式、JSON模式验证和基于评判的过滤机制。

Result: 框架能够生成机器可解析且输入、输出和工具调用之间忠实对齐的高质量合成数据，支持构建反映完整工具使用能力的数据集。

Conclusion: 该框架为手动收集提供了可重现的LLM-only替代方案，促进了能够进行稳健工具使用的智能体LLM的发展。

Abstract: Large language models (LLMs) are increasingly deployed as agents, expected to
decompose goals, invoke tools, and verify results in dynamic environments.
Realizing these capabilities requires access to agentic data-structured
interaction records that couple user intents with tool specifications,
argument-grounded calls, and verifiable execution traces. However, collecting
such data from human annotators is costly, time-consuming, and difficult to
scale. We present a unified framework for synthesizing agentic data using only
LLMs, without any human-in-the-loop supervision. This framework decomposes
generation into modular pipelines that produce complete interaction records
spanning task specifications, tool definitions, policy pseudocode, natural
language exchanges, and execution traces. Records conform to strict syntactic
and semantic constraints, ensuring machine-parseability and faithful alignment
across inputs, outputs, and tool calls. Beyond single tasks, there is support
for both multi-task and multi-turn agent interactions, enabling the
construction of datasets that reflect the full spectrum of tool-use
competencies. To ensure quality and consistency, the framework integrates
constrained generation formats, JSON-schema validation, and judge-based
filtering. This paper formalizes the schema for agentic records, details the
prompt design principles that guide generation, and introduces scalable
pipelines for high-quality synthetic data. By providing a reproducible,
LLM-only alternative to manual collection, hence advancing the development of
agentic LLMs capable of robust tool use.

</details>


### [9] [OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning](https://arxiv.org/abs/2510.18032)
*Zhenyu Bi,Meng Lu,Yang Li,Swastik Roy,Weijie Guan,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: 提出了一种多智能体口头强化学习算法，通过动态构建和优化多智能体协作结构来增强复杂推理能力，在多种推理任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统要么采用预定义结构，要么依赖多数投票或圆桌辩论，这会压制正确但非主导智能体的贡献，且现有图网络方法只优化智能体性能而忽视交互质量。

Method: 提出多智能体口头强化学习算法，定义动作空间和反馈机制来评估辩论过程中的沟通鲁棒性和连贯性，最终通过所有智能体的多数投票做出决策。

Result: 在数学推理、创意写作、科学推理和数值排序等任务上，该方法显著优于单智能体提示方法和最先进的多智能体框架。

Conclusion: 有效的智能体沟通对多智能体推理至关重要，辩论质量起着重要作用，所提出的动态协作结构优化方法能够显著提升多智能体系统的推理性能。

Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities in
mathematical and scientific tasks. To enhance complex reasoning, multi-agent
systems have been proposed to harness the collective intelligence of LLM
agents. However, existing collaboration structures are either predefined or
rely on majority voting or round-table debates, which can suppress correct but
less dominant agent contributions. Recent approaches model multi-agent systems
as graph networks but optimize purely for agent performance, neglecting the
quality of interactions. We hypothesize that effective agent communication is
crucial for multi-agent reasoning and that debating quality plays a significant
role. To address this, we propose $\ours$, a multi-agent verbal reinforcement
learning algorithm that dynamically constructs and refines multi-agent
collaboration structures. Our method defines action spaces and a feedback
mechanism that evaluates communication robustness and coherence throughout the
debate. The final decision is achieved through a majority vote over all the
agents. We assess $\ours$ on various reasoning tasks, including mathematical
reasoning, creative writing, scientific reasoning, and numerical sorting.
Results demonstrate that our approach significantly outperforms single-agent
prompting methods and state-of-the-art multi-agent frameworks on diverse tasks.

</details>


### [10] [Subject-Event Ontology Without Global Time: Foundations and Execution Semantics](https://arxiv.org/abs/2510.18040)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 提出了一种基于主体-事件的本体论形式化方法，用于建模复杂动态系统，不依赖全局时间，通过声明式数据流机制确保确定性。


<details>
  <summary>Details</summary>
Motivation: 为分布式系统、微服务架构、DLT平台和多视角场景提供不依赖全局时间的建模方法，解决不同主体间事实冲突的问题。

Method: 基于九个公理(A1-A9)的形式化方法，包括事件作为固定行为、因果顺序通过happens-before关系、模型作为认知过滤器等核心原则，并通过boldsea系统实现。

Result: 开发了boldsea系统作为可执行本体的工作流引擎，理论构造在BSL语言中实现，确保历史单调性、因果无环性和可追溯性。

Conclusion: 该形式化方法适用于各种分布式场景，能够处理多主体间的冲突事实，为复杂动态系统提供可靠的理论基础和实践工具。

Abstract: A formalization of a subject-event ontology is proposed for modeling complex
dynamic systems without reliance on global time. Key principles: (1) event as
an act of fixation - a subject discerns and fixes changes according to models
(conceptual templates) available to them; (2) causal order via happens-before -
the order of events is defined by explicit dependencies, not timestamps; (3)
making the ontology executable via a declarative dataflow mechanism, ensuring
determinism; (4) models as epistemic filters - a subject can only fix what
falls under its known concepts and properties; (5) presumption of truth - the
declarative content of an event is available for computation from the moment of
fixation, without external verification. The formalization includes nine axioms
(A1-A9), ensuring the correctness of executable ontologies: monotonicity of
history (I1), acyclicity of causality (I2), traceability (I3). Special
attention is given to the model-based approach (A9): event validation via
schemas, actor authorization, automatic construction of causal chains (W3)
without global time. Practical applicability is demonstrated on the boldsea
system - a workflow engine for executable ontologies, where the theoretical
constructs are implemented in BSL (Boldsea Semantic Language). The
formalization is applicable to distributed systems, microservice architectures,
DLT platforms, and multiperspectivity scenarios (conflicting facts from
different subjects).

</details>


### [11] [CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows](https://arxiv.org/abs/2510.18043)
*Joong Ho Choi,Jiayang Zhao,Jeel Shah,Ritvika Sonawane,Vedant Singh,Avani Appalla,Will Flanagan,Filipe Condessa*

Main category: cs.AI

TL;DR: CompactPrompt是一个端到端提示压缩管道，通过合并硬提示压缩和轻量级文件级数据压缩，在保持输出质量的同时将总token使用量和推理成本降低高达60%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代理工作流中运行时会产生高昂成本，因为需要链接冗长提示和处理丰富数据流。

Method: 使用自信息评分和基于依赖的短语分组修剪低信息token，同时对文档应用n-gram缩写和数值列均匀量化。

Result: 在TAT-QA和FinQA基准测试中，总token使用量和推理成本降低高达60%，输出质量保持稳定（Claude-3.5-Sonnet和GPT-4.1-Mini的准确率下降小于5%）。

Conclusion: CompactPrompt为更精简的生成式AI管道奠定了基础，能够可视化实时压缩决策并量化成本性能权衡。

Abstract: Large Language Models (LLMs) deliver powerful reasoning and generation
capabilities but incur substantial run-time costs when operating in agentic
workflows that chain together lengthy prompts and process rich data streams. We
introduce CompactPrompt, an end-to-end pipeline that merges hard prompt
compression with lightweight file-level data compression. CompactPrompt first
prunes low-information tokens from prompts using self-information scoring and
dependency-based phrase grouping. In parallel, it applies n-gram abbreviation
to recurrent textual patterns in attached documents and uniform quantization to
numerical columns, yielding compact yet semantically faithful representations.
Integrated into standard LLM agents, CompactPrompt reduces total token usage
and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA,
while preserving output quality (Results in less than 5% accuracy drop for
Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time
compression decisions and quantify cost-performance trade-offs, laying the
groundwork for leaner generative AI pipelines.

</details>


### [12] [Planned Diffusion](https://arxiv.org/abs/2510.18087)
*Daniel Israel,Tian Jin,Ellie Cheng,Guy Van den Broeck,Aditya Grover,Suvinay Subramanian,Michael Carbin*

Main category: cs.AI

TL;DR: Planned diffusion是一种结合自回归和扩散模型的混合方法，通过两阶段生成：先自回归创建短计划分解输出为独立跨度，然后用扩散模型并行生成这些跨度，在质量和速度之间实现帕累托最优权衡。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型推理中生成速度与输出质量之间的权衡问题，自回归模型质量高但序列生成慢，扩散模型可并行生成但需要多次迭代才能达到相同质量。

Method: 两阶段混合方法：第一阶段使用自回归模型创建短计划，将输出分解为较小的独立跨度；第二阶段使用扩散模型同时生成这些跨度。

Result: 在AlpacaEval的805个指令跟随提示上，实现了帕累托最优的质量-延迟权衡，相比自回归生成获得1.27x到1.81x的加速，仅损失0.87%到5.4%的胜率。

Conclusion: Planned diffusion扩展了速度-质量的帕累托前沿，为更快、高质量的文本生成提供了实用路径，其规划机制最小且可靠，可通过简单运行时旋钮灵活控制质量-延迟权衡。

Abstract: A central challenge in large language model inference is the trade-off
between generation speed and output quality. Autoregressive models produce
high-quality text but generate tokens sequentially. Diffusion models can
generate tokens in parallel but often need many iterations to match the same
quality. We propose planned diffusion, a hybrid method that combines the
strengths of both paradigms. Planned diffusion works in two stages: first, the
model creates a short autoregressive plan that breaks the output into smaller,
independent spans. Second, the model generates these spans simultaneously using
diffusion. This approach expands the speed-quality Pareto frontier and provides
a practical path to faster, high-quality text generation. On AlpacaEval, a
suite of 805 instruction-following prompts, planned diffusion achieves
Pareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x
speedup over autoregressive generation with only 0.87\% to 5.4\% drop in win
rate, respectively. Our sensitivity analysis shows that the planning mechanism
of planned diffusion is minimal and reliable, and simple runtime knobs exist to
provide flexible control of the quality-latency trade-off.

</details>


### [13] [SMaRT: Select, Mix, and ReinvenT -- A Strategy Fusion Framework for LLM-Driven Reasoning and Planning](https://arxiv.org/abs/2510.18095)
*Nikhil Verma,Manasa Bharadwaj,Wonjun Jang,Harmanpreet Singh,Yixiao Wang,Homa Fashandi,Chul Lee*

Main category: cs.AI

TL;DR: SMaRT框架通过融合多种推理策略来提升LLM性能，在推理、规划和决策任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一策略提示，无法充分发挥不同推理策略的协同效应，需要融合多种策略来最大化性能和鲁棒性

Method: 提出Select, Mix, and ReinvenT (SMaRT)框架，将LLM作为智能集成器而非仅仅评估器，实现跨策略校准和策略融合

Result: 在推理、规划和顺序决策基准测试中，SMaRT在解决方案质量、约束遵循和性能指标方面持续优于最先进的基线方法

Conclusion: 这项工作通过开创跨策略校准新范式，重新定义了LLM驱动的决策制定，为推理系统解锁了更优结果并推进了自精化方法的边界

Abstract: Large Language Models (LLMs) have redefined complex task automation with
exceptional generalization capabilities. Despite these advancements,
state-of-the-art methods rely on single-strategy prompting, missing the synergy
of diverse reasoning approaches. No single strategy excels universally,
highlighting the need for frameworks that fuse strategies to maximize
performance and ensure robustness. We introduce the Select, Mix, and ReinvenT
(SMaRT) framework, an innovative strategy fusion approach designed to overcome
this constraint by creating balanced and efficient solutions through the
seamless integration of diverse reasoning strategies. Unlike existing methods,
which employ LLMs merely as evaluators, SMaRT uses them as intelligent
integrators, unlocking the "best of all worlds" across tasks. Extensive
empirical evaluations across benchmarks in reasoning, planning, and sequential
decision-making highlight the robustness and adaptability of SMaRT. The
framework consistently outperforms state-of-the-art baselines in solution
quality, constraint adherence, and performance metrics. This work redefines
LLM-driven decision-making by pioneering a new paradigm in cross-strategy
calibration, unlocking superior outcomes for reasoning systems and advancing
the boundaries of self-refining methodologies.

</details>


### [14] [Measuring Reasoning in LLMs: a New Dialectical Angle](https://arxiv.org/abs/2510.18134)
*Soheil Abbasloo*

Main category: cs.AI

TL;DR: 提出SIEV框架，基于辩证法评估语言模型的推理过程，发现即使基准测试饱和的模型仍存在显著推理缺陷


<details>
  <summary>Details</summary>
Motivation: 当前评估只关注答案正确性，但无法揭示推理过程。推理应是动态的思想交互过程，而非静态步骤链

Method: 基于辩证法（正题-反题-合题）构建SIEV框架，评估模型解决矛盾、整合观点和综合高阶推理的能力

Result: GPT-5-chat在GSM基准上使用SIEV评估时得分下降40多分，揭示现有模型存在重大推理缺陷

Conclusion: 采用过程导向、哲学基础的方法能对LLM推理进行更深入、严谨和区分性的评估

Abstract: What does it truly mean for a language model to "reason"? Most current
evaluations and benchmarks reward models' correct standalone answers--but
correctness alone reveals little about the process that produced them. In this
work, we explore a different perspective: reasoning is not a static chain of
steps, but a dynamic trajectory where ideas interact, clash, and evolve into
deeper insights. To capture this dynamic, we draw on a well-established
philosophical tradition: \textit{dialectics}, where reasoning unfolds through
thesis, antithesis, and synthesis. Building on this, we present SIEV, a
structured framework that evaluates reasoning of LLMs through dialectics.
Unlike conventional evaluations, SIEV assesses not only the conclusion a model
reaches, but how it gets there: its ability to resolve tension, integrate
distinct ideas, and synthesize higher-order reasoning. This lens uncovers
significant reasoning gaps in state-of-the-art models even under saturated
benchmarks like GSM and MMLU. For instance, GPT-5-chat, a recent model, loses
over 40 points (out of 100) when evaluated with SIEV on GSM. Our findings
highlight that adopting a process-oriented, philosophically grounded approach
enables a deeper, more rigorous, and more discriminative assessment of LLM
reasoning.

</details>


### [15] [Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models](https://arxiv.org/abs/2510.18143)
*Huan Song,Deeksha Razdan,Yiyue Qian,Arijit Ghosh Chowdhury,Parth Patwa,Aman Chadha,Shinan Zhang,Sharlina Keshava,Hannah Marlowe*

Main category: cs.AI

TL;DR: PaDA-Agent是一个评估驱动的数据增强方法，通过发现验证数据中的失败模式来制定针对性数据增强策略，显著提升小语言模型在领域特定任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在部署成本和延迟方面具有优势，但在复杂领域特定任务上的准确性往往落后于大模型。虽然有监督微调可以弥补性能差距，但需要大量手动数据准备和迭代优化工作。

Method: 提出PaDA-Agent方法，通过评估从验证数据中发现失败模式，制定针对性数据增强策略，直接减少泛化差距。与仅关注模型训练错误的方法不同，该方法协调操作简化SLMs的数据增强过程。

Result: 实验结果显示，在Llama 3.2 1B Instruct模型微调中，相比最先进的基于LLM的数据增强方法取得了显著改进。

Conclusion: PaDA-Agent提供了一种有效的评估驱动数据增强方法，能够显著提升小语言模型在领域特定任务上的性能，同时减少手动数据准备的工作量。

Abstract: Small Language Models (SLMs) offer compelling advantages in deployment cost
and latency, but their accuracy often lags behind larger models, particularly
for complex domain-specific tasks. While supervised fine-tuning can help bridge
this performance gap, it requires substantial manual effort in data preparation
and iterative optimization. We present PaDA-Agent (Pattern-guided Data
Augmentation Agent), an evaluation-driven approach that streamlines the data
augmentation process for SLMs through coordinated operations. Unlike
state-of-the-art approaches that focus on model training errors only and
generating error-correcting samples, PaDA-Agent discovers failure patterns from
the validation data via evaluations and drafts targeted data augmentation
strategies aiming to directly reduce the generalization gap. Our experimental
results demonstrate significant improvements over state-of-the-art LLM-based
data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.

</details>


### [16] [Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety](https://arxiv.org/abs/2510.18154)
*Antonio-Gabriel Chacón Menke,Phan Xuan Tan,Eiji Kamioka*

Main category: cs.AI

TL;DR: 提出了一个句子级标注的数据集，用于在LLM推理过程中基于激活的安全行为监控，通过提取引导向量来检测和影响模型激活中的安全行为。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本推理步骤的安全监控方法可能遗漏微妙的有害模式，且可能被隐藏不安全推理的模型规避，需要更细粒度的安全监控技术。

Method: 构建包含推理序列和句子级安全行为标注的数据集，提取引导向量来检测和影响模型激活中的安全行为。

Result: 展示了该数据集的实用性，通过提取的表征能够检测和引导模型激活中的安全行为。

Conclusion: 激活级技术有潜力改善推理过程中的安全监督，填补了安全研究中精确识别推理链中特定行为发生时机的重要空白。

Abstract: Recent work has highlighted the importance of monitoring chain-of-thought
reasoning for AI safety; however, current approaches that analyze textual
reasoning steps can miss subtle harmful patterns and may be circumvented by
models that hide unsafe reasoning. We present a sentence-level labeled dataset
that enables activation-based monitoring of safety behaviors during LLM
reasoning. Our dataset contains reasoning sequences with sentence-level
annotations of safety behaviors such as expression of safety concerns or
speculation on user intent, which we use to extract steering vectors for
detecting and influencing these behaviors within model activations. The dataset
fills a key gap in safety research: while existing datasets label reasoning
holistically, effective application of steering vectors for safety monitoring
could be improved by identifying precisely when specific behaviors occur within
reasoning chains. We demonstrate the dataset's utility by extracting
representations that both detect and steer safety behaviors in model
activations, showcasing the potential of activation-level techniques for
improving safety oversight on reasoning.
  Content Warning: This paper discusses AI safety in the context of harmful
prompts and may contain references to potentially harmful content.

</details>


### [17] [LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior](https://arxiv.org/abs/2510.18155)
*Man-Lin Chu,Lucian Terhorst,Kadin Reed,Tom Ni,Weiwei Chen,Rongyu Lin*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体仿真框架，用于模拟消费者决策和社会动态，替代传统的基于规则的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的事后分析和基于规则的智能体模型难以捕捉人类行为和社会互动的复杂性，需要更有效的营销策略预测试工具。

Method: 利用大语言模型在沙盒环境中构建多智能体仿真框架，智能体能够交互、表达内部推理、形成习惯并进行购买决策，无需预定义规则。

Result: 在价格折扣营销场景中，系统提供了可操作的策略测试结果，并揭示了传统方法无法捕捉的新兴社会模式。

Conclusion: 该方法为营销人员提供了一个可扩展、低风险的预实施测试工具，减少了对耗时的事后评估的依赖，并降低了营销活动表现不佳的风险。

Abstract: Simulating consumer decision-making is vital for designing and evaluating
marketing strategies before costly real-world deployment. However, post-event
analyses and rule-based agent-based models (ABMs) struggle to capture the
complexity of human behavior and social interaction. We introduce an
LLM-powered multi-agent simulation framework that models consumer decisions and
social dynamics. Building on recent advances in large language model simulation
in a sandbox environment, our framework enables generative agents to interact,
express internal reasoning, form habits, and make purchasing decisions without
predefined rules. In a price-discount marketing scenario, the system delivers
actionable strategy-testing outcomes and reveals emergent social patterns
beyond the reach of conventional methods. This approach offers marketers a
scalable, low-risk tool for pre-implementation testing, reducing reliance on
time-intensive post-event evaluations and lowering the risk of underperforming
campaigns.

</details>


### [18] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 提出了Saber算法，一种无需训练的新型采样方法，用于提升扩散语言模型在代码生成任务中的推理速度和输出质量


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在代码生成任务中存在推理速度与输出质量之间的关键权衡，减少采样步骤会导致性能灾难性下降

Method: Saber算法基于两个关键洞察：1）随着代码上下文的建立可以自适应加速；2）需要回溯机制来反转生成的token

Result: 在多个主流代码生成基准测试中，Pass@1准确率平均提升1.9%，推理速度平均加速251.4%

Conclusion: 通过利用扩散语言模型的固有优势，显著缩小了与自回归模型在代码生成方面的性能差距

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [19] [AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI](https://arxiv.org/abs/2510.18170)
*Manik Rana,Calissa Man,Anotida Expected Msiiwa,Jeffrey Paine,Kevin Zhu,Sunishchal Dev,Vasu Sharma,Ahan M R*

Main category: cs.AI

TL;DR: 提出了AgentChangeBench基准，专门评估工具增强语言模型代理在对话中目标变化时的适应能力，包含四个互补指标来衡量效果、可靠性、效率和适应延迟。


<details>
  <summary>Details</summary>
Motivation: 现实世界多轮交互中目标变化很常见，但现有基准主要评估静态目标或一次性工具使用，缺乏对动态目标适应能力的测试。

Method: 构建包含2,835个任务序列和五个用户角色的基准框架，在三个企业领域测试代理对目标变化的适应能力，使用四个指标：任务成功率、工具使用效率、工具调用冗余率和目标变化恢复时间。

Result: 评估多个前沿模型发现显著差异：GPT-4o在航班预订任务中达到92.2%恢复率，而Gemini只有48.6%；零售任务参数有效性接近完美但冗余率超过80%，显示重大效率问题。

Conclusion: 高原始准确率不代表在动态目标下的鲁棒性，明确测量恢复时间和冗余率对于评估代理韧性至关重要，该基准为诊断和改进企业环境中代理韧性提供了可复现测试平台。

Abstract: Goal changes are a defining feature of real world multi-turn interactions,
yet current agent benchmarks primarily evaluate static objectives or one-shot
tool use. We introduce AgentChangeBench, a benchmark explicitly designed to
measure how tool augmented language model agents adapt to mid dialogue goal
shifts across three enterprise domains. Our framework formalizes evaluation
through four complementary metrics: Task Success Rate (TSR) for effectiveness,
Tool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for
wasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency.
AgentChangeBench comprises 2,835 task sequences and five user personas, each
designed to trigger realistic shift points in ongoing workflows. Using this
setup, we evaluate several frontier models and uncover sharp contrasts obscured
by traditional $\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\%$
recovery on airline booking shifts while Gemini collapses to $48.6\%$, and
retail tasks show near perfect parameter validity yet redundancy rates above
$80\%$, revealing major inefficiencies. These findings demonstrate that high
raw accuracy does not imply robustness under dynamic goals, and that explicit
measurement of recovery time and redundancy is essential. AgentChangeBench
establishes a reproducible testbed for diagnosing and improving agent
resilience in realistic enterprise settings.

</details>


### [20] [Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains](https://arxiv.org/abs/2510.18176)
*Soumya Rani Samineni,Durgesh Kalwar,Vardaan Gangal,Siddhant Bhambri,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 本文研究了基于可验证奖励的强化学习(RLVR)后训练对大型语言模型推理轨迹的影响，发现RL后训练提高了中间推理步骤的局部连贯性，但这种改进不一定能保证最终答案的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法通常统一处理所有token而不考虑token级优势，主要基于最终答案正确性评估性能，但声称RL后训练能改进推理轨迹。这促使我们研究RL后训练对未直接激励的中间token的影响。

Method: 使用GRPO算法和Qwen-2.5-0.5B模型在GSM8K数据集上进行实验，引入基于一阶逻辑的轨迹连贯性度量来捕捉推理步骤的一致性，区分轨迹有效性和轨迹连贯性。

Result: RL后训练总体上提高了轨迹连贯性，在基础模型失败但RL模型成功的问题上改进最显著。令人惊讶的是，RL增强了局部连贯性但不一定产生有效或正确的解决方案。

Conclusion: 改进的推理步骤局部连贯性不能保证最终答案正确性。声称RL改进推理的说法需要谨慎审视，因为这些改进可能基于轨迹连贯性的提升，而不一定能转化为完全有效的数学证明。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR)-based post-training of
Large Language Models (LLMs) has been shown to improve accuracy on reasoning
tasks and continues to attract significant attention. Existing RLVR methods,
however, typically treat all tokens uniformly without accounting for
token-level advantages. These methods primarily evaluate performance based on
final answer correctness or Pass@K accuracy, and yet make claims about RL
post-training leading to improved reasoning traces. This motivates our
investigation into the effect of RL post-training on intermediate tokens which
are not directly incentivized. To study this, we design an experimental setup
using the GRPO algorithm with Qwen-2.5-0.5B model on the GSM8K dataset. We
introduce trace coherence, a First-Order Logic (FOL)-based measure to capture
the consistency of reasoning steps by identifying errors in the traces. We
distinguish between trace validity and trace coherence, noting that the former
implies logical soundness while the latter measures local coherence via lack of
errors. Our results show that RL post-training overall improves trace coherence
with the most significant gains on problems where the base model fails but the
RL model succeeds. Surprisingly, RL enhances local coherence without
necessarily producing valid or correct solutions. This highlights a crucial
distinction: improved local coherence in reasoning steps does not guarantee
final answer correctness. We argue that claims of improved reasoning via RL
must be examined with care, as these may be based on improved trace coherence,
which may not translate into fully valid mathematical proofs.

</details>


### [21] [FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo](https://arxiv.org/abs/2510.18193)
*Keivan Shariatmadar,Ahmad Osman,Ramin Ray,Usman Dildar,Kisam Kim*

Main category: cs.AI

TL;DR: FST.ai 2.0是一个可解释的AI生态系统，用于支持跆拳道比赛和训练中的实时决策，通过姿态识别、不确定性建模和可视化解释来提升裁判决策的公平性和透明度。


<details>
  <summary>Details</summary>
Motivation: 解决奥林匹克和残奥会格斗运动中公平、透明和可解释决策的挑战，支持裁判、教练和运动员的实时决策。

Method: 集成基于图卷积网络(GCNs)的姿态动作识别、通过置信集进行认知不确定性建模、可视化解释叠加层，以及支持人机协作的交互式仪表板。

Result: 在比赛数据上的实验验证显示：决策审查时间减少85%，裁判对AI辅助决策的信任度达到93%。

Conclusion: 该框架为可信赖、数据驱动的裁判和运动员评估建立了透明且可扩展的流程，代表了体育领域向公平、负责和以人为本的AI迈出的一步。

Abstract: Fair, transparent, and explainable decision-making remains a critical
challenge in Olympic and Paralympic combat sports. This paper presents
\emph{FST.ai 2.0}, an explainable AI ecosystem designed to support referees,
coaches, and athletes in real time during Taekwondo competitions and training.
The system integrates {pose-based action recognition} using graph convolutional
networks (GCNs), {epistemic uncertainty modeling} through credal sets, and
{explainability overlays} for visual decision support. A set of {interactive
dashboards} enables human--AI collaboration in referee evaluation, athlete
performance analysis, and Para-Taekwondo classification. Beyond automated
scoring, FST.ai~2.0 incorporates modules for referee training, fairness
monitoring, and policy-level analytics within the World Taekwondo ecosystem.
Experimental validation on competition data demonstrates an {85\% reduction in
decision review time} and {93\% referee trust} in AI-assisted decisions. The
framework thus establishes a transparent and extensible pipeline for
trustworthy, data-driven officiating and athlete assessment. By bridging
real-time perception, explainable inference, and governance-aware design,
FST.ai~2.0 represents a step toward equitable, accountable, and human-aligned
AI in sports.

</details>


### [22] [A Definition of AGI](https://arxiv.org/abs/2510.18212)
*Dan Hendrycks,Dawn Song,Christian Szegedy,Honglak Lee,Yarin Gal,Erik Brynjolfsson,Sharon Li,Andy Zou,Lionel Levine,Bo Han,Jie Fu,Ziwei Liu,Jinwoo Shin,Kimin Lee,Mantas Mazeika,Long Phan,George Ingebretsen,Adam Khoja,Cihang Xie,Olawale Salaudeen,Matthias Hein,Kevin Zhao,Alexander Pan,David Duvenaud,Bo Li,Steve Omohundro,Gabriel Alfour,Max Tegmark,Kevin McGrew,Gary Marcus,Jaan Tallinn,Eric Schmidt,Yoshua Bengio*

Main category: cs.AI

TL;DR: 本文提出了一个可量化的AGI评估框架，基于Cattell-Horn-Carroll人类认知理论，将通用智能分解为10个核心认知领域，并揭示了当前AI系统存在"锯齿状"认知能力分布。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏对AGI的具体定义，当前专业AI与人类水平认知之间的差距难以衡量。本文旨在通过量化框架来解决这一问题。

Method: 基于Cattell-Horn-Carroll人类认知理论，将通用智能分解为10个核心认知领域，并采用已建立的人类心理测量工具来评估AI系统。

Result: 应用该框架发现当代AI模型具有高度"锯齿状"认知特征：在知识密集型领域表现良好，但在基础认知机制（特别是长期记忆存储）方面存在严重缺陷。GPT-4得分为27%，GPT-5为58%。

Conclusion: 该框架量化了AI向AGI的快速进展和剩余差距，为AGI发展提供了具体的衡量标准。

Abstract: The lack of a concrete definition for Artificial General Intelligence (AGI)
obscures the gap between today's specialized AI and human-level cognition. This
paper introduces a quantifiable framework to address this, defining AGI as
matching the cognitive versatility and proficiency of a well-educated adult. To
operationalize this, we ground our methodology in Cattell-Horn-Carroll theory,
the most empirically validated model of human cognition. The framework dissects
general intelligence into ten core cognitive domains-including reasoning,
memory, and perception-and adapts established human psychometric batteries to
evaluate AI systems. Application of this framework reveals a highly "jagged"
cognitive profile in contemporary models. While proficient in
knowledge-intensive domains, current AI systems have critical deficits in
foundational cognitive machinery, particularly long-term memory storage. The
resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify
both rapid progress and the substantial gap remaining before AGI.

</details>


### [23] [ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning](https://arxiv.org/abs/2510.18250)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Cancheng Zhang,Xiangdong Zhang,Mingquan Feng,Jingzhi Wang,Junchi Yan*

Main category: cs.AI

TL;DR: 提出ssToken方法，通过自调制损失差异和语义感知注意力机制进行token级数据选择，无需额外参考模型，在保持训练效率的同时提升SFT性能


<details>
  <summary>Details</summary>
Motivation: 现有token级选择方法需要额外参考模型且仅依赖损失信息，无法很好保留语义重要但损失不敏感的token

Method: 使用历史模型计算token损失差异作为自调制信号，结合注意力机制评估token语义重要性，两者互补进行token选择

Result: 自调制选择和语义感知选择单独使用均优于全数据微调，两者结合进一步超越现有token级选择方法

Conclusion: ssToken通过自调制和语义感知的token选择，在无需额外模型的情况下实现了更好的SFT性能

Abstract: Data quality plays a critical role in enhancing supervised fine-tuning (SFT)
for large language models (LLMs), and token-level data selection has emerged as
a promising direction for its fine-grained nature. Despite their strong
empirical performance, existing token-level selection methods share two key
limitations: (1) requiring training or accessing an additional reference model,
and (2) relying solely on loss information for token selection, which cannot
well preserve semantically important tokens that are not favored by loss-based
metrics. To address these challenges, we propose ssToken, a Self-modulated and
Semantic-aware Token Selection approach. ssToken leverages readily accessible
history models to compute the per-token loss difference with the current model,
which serves as a self-modulated signal that enables the model to adaptively
select tokens along its optimization trajectory, rather than relying on excess
loss from an offline-trained reference model as in prior works. We further
introduce a semantic-aware, attention-based token importance estimation metric,
orthogonal to loss-based selection and providing complementary semantic
information for more effective filtering. Extensive experiments across
different model families and scales demonstrate that both self-modulated
selection and semantic-aware selection alone outperform full-data fine-tuning,
while their integration--ssToken--achieves synergistic gains and further
surpasses prior token-level selection methods, delivering performance
improvements while maintaining training efficiency.

</details>


### [24] [Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](https://arxiv.org/abs/2510.18254)
*Sion Weatherhead,Flora Salim,Aaron Belbasis*

Main category: cs.AI

TL;DR: 本文测试了8个前沿大语言模型在开放式但规则约束任务中的自我反思能力，发现模型反思效果有限，经常重复相同的约束违反错误，表明当前LLM的'反思'缺乏人类目标驱动的主动监控机制。


<details>
  <summary>Details</summary>
Motivation: 研究当前大语言模型的'反思'能力是否在功能上等同于人类的反思推理，特别是在开放式但规则约束的任务中，而非仅在有明确正确信号的封闭任务中。

Method: 在需要生成有效科学测试项目并基于自身批评进行修订的简单现实任务上测试8个前沿模型，分析第一轮表现和反思后的改进情况。

Result: 第一轮表现很差（通常4个项目中零有效项目，平均约1个），反思后仅获得适度提升（也约1个）。关键发现是第二轮尝试经常重复相同的约束违反，表明'修正收益'主要来自偶然产生有效项目而非错误检测和原则性修复。

Conclusion: 当前LLM的'反思'缺乏功能性证据证明其具有帮助人类在第一次尝试时就尊重约束的主动、目标驱动监控机制。在模型本身实例化此类机制之前，可靠性能需要强制执行约束的外部结构。

Abstract: Humans do not just find mistakes after the fact -- we often catch them
mid-stream because 'reflection' is tied to the goal and its constraints.
Today's large language models produce reasoning tokens and 'reflective' text,
but is it functionally equivalent with human reflective reasoning? Prior work
on closed-ended tasks -- with clear, external 'correctness' signals -- can make
'reflection' look effective while masking limits in self-correction. We
therefore test eight frontier models on a simple, real-world task that is
open-ended yet rule-constrained, with auditable success criteria: to produce
valid scientific test items, then revise after considering their own critique.
First-pass performance is poor (often zero valid items out of 4 required; mean
$\approx$ 1), and reflection yields only modest gains (also $\approx$ 1).
Crucially, the second attempt frequently repeats the same violation of
constraint, indicating 'corrective gains' arise largely from chance production
of a valid item rather than error detection and principled,
constraint-sensitive repair. Performance before and after reflection
deteriorates as open-endedness increases, and models marketed for 'reasoning'
show no advantage. Our results suggest that current LLM 'reflection' lacks
functional evidence of the active, goal-driven monitoring that helps humans
respect constraints even on a first pass. Until such mechanisms are
instantiated in the model itself, reliable performance requires external
structure that enforces constraints.

</details>


### [25] [Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming](https://arxiv.org/abs/2510.18314)
*Zheng Zhang,Jiarui He,Yuchen Cai,Deheng Ye,Peilin Zhao,Ruili Feng,Hao Wang*

Main category: cs.AI

TL;DR: 提出Genesis框架，通过遗传算法和混合策略表示生成对抗性注入攻击，动态发现有效攻击策略并持续优化攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有网络代理攻击研究有限，手动或静态方法难以捕捉代理行为模式，无法适应多样化环境，需要持续发现和演进攻击策略。

Method: Genesis框架包含三个模块：攻击者（集成遗传算法和混合策略表示）、评分器（评估目标代理响应）、策略师（从交互日志动态发现有效策略并构建策略库）。

Result: 在各种网络任务上的广泛实验表明，该框架能发现新颖策略，始终优于现有攻击基线。

Conclusion: Genesis框架能有效发现网络代理的新型攻击策略，显著提升攻击效果，为网络代理安全研究提供新思路。

Abstract: As large language model (LLM) agents increasingly automate complex web tasks,
they boost productivity while simultaneously introducing new security risks.
However, relevant studies on web agent attacks remain limited. Existing
red-teaming approaches mainly rely on manually crafted attack strategies or
static models trained offline. Such methods fail to capture the underlying
behavioral patterns of web agents, making it difficult to generalize across
diverse environments. In web agent attacks, success requires the continuous
discovery and evolution of attack strategies. To this end, we propose Genesis,
a novel agentic framework composed of three modules: Attacker, Scorer, and
Strategist. The Attacker generates adversarial injections by integrating the
genetic algorithm with a hybrid strategy representation. The Scorer evaluates
the target web agent's responses to provide feedback. The Strategist
dynamically uncovers effective strategies from interaction logs and compiles
them into a continuously growing strategy library, which is then re-deployed to
enhance the Attacker's effectiveness. Extensive experiments across various web
tasks show that our framework discovers novel strategies and consistently
outperforms existing attack baselines.

</details>


### [26] [Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning](https://arxiv.org/abs/2510.18318)
*Aaron Bell,Amit Aides,Amr Helmy,Arbaaz Muslim,Aviad Barzilai,Aviv Slobodkin,Bolous Jaber,David Schottlander,George Leifman,Joydeep Paul,Mimi Sun,Nadav Sherman,Natalie Williams,Per Bjornsson,Roy Lee,Ruth Alcantara,Thomas Turnbull,Tomer Shekel,Vered Silverman,Yotam Gigi,Adam Boulanger,Alex Ottenwess,Ali Ahmadalipour,Anna Carter,Charles Elliott,David Andre,Elad Aharoni,Gia Jung,Hassler Thurston,Jacob Bien,Jamie McPike,Juliet Rothenberg,Kartik Hegde,Kel Markert,Kim Philipp Jablonski,Luc Houriez,Monica Bharel,Phing VanLee,Reuven Sayag,Sebastian Pilarski,Shelley Cazares,Shlomi Pasternak,Siduo Jiang,Stone Jiang,Thomas Colthurst,Yang Chen,Yehonathan Refael,Yochai Blau,Yuval Carny,Yael Maguire,Avinatan Hassidim,James Manyika,Tim Thelin,Genady Beryozkin,Gautam Prasad,Luke Barrington,Yossi Matias,Niv Efron,Shravya Shetty*

Main category: cs.AI

TL;DR: 提出了Earth AI，一个结合地理空间AI模型和智能推理的系统，通过三个基础模型（行星尺度图像、人口、环境）和Gemini驱动的推理引擎，解决地理空间数据分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 地理空间数据具有巨大潜力，但其庞大的数量、多样性以及不同的分辨率、时间尺度和稀疏性给深入分析和解释带来了重大挑战。

Method: 构建了三个关键领域的基础模型（行星尺度图像、人口、环境），并开发了Gemini驱动的智能推理引擎，能够联合推理多个基础模型、大型地理空间数据源和工具。

Result: 在严格基准测试中展示了基础模型的能力，验证了它们在地理空间推理中的互补价值，其协同作用解锁了卓越的预测能力。在真实危机场景的新基准测试中，代理能够提供关键及时的见解。

Conclusion: Earth AI系统有效弥合了原始地理空间数据与可操作理解之间的差距，为理解地球提供了新的深刻见解。

Abstract: Geospatial data offers immense potential for understanding our planet.
However, the sheer volume and diversity of this data along with its varied
resolutions, timescales, and sparsity pose significant challenges for thorough
analysis and interpretation. This paper introduces Earth AI, a family of
geospatial AI models and agentic reasoning that enables significant advances in
our ability to unlock novel and profound insights into our planet. This
approach is built upon foundation models across three key domains--Planet-scale
Imagery, Population, and Environment--and an intelligent Gemini-powered
reasoning engine. We present rigorous benchmarks showcasing the power and novel
capabilities of our foundation models and validate that when used together,
they provide complementary value for geospatial inference and their synergies
unlock superior predictive capabilities. To handle complex, multi-step queries,
we developed a Gemini-powered agent that jointly reasons over our multiple
foundation models along with large geospatial data sources and tools. On a new
benchmark of real-world crisis scenarios, our agent demonstrates the ability to
deliver critical and timely insights, effectively bridging the gap between raw
geospatial data and actionable understanding.

</details>


### [27] [ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.18342)
*Peng Tang,Xiaoxiao Yan,Xiaobin Hu,Yuning Cui,Donghao Luo,Jiangning Zhang,Pengcheng Xu,Jinlong Peng,Qingdong He,Feiyue Huang,Song Xue,Tobias Lasser*

Main category: cs.AI

TL;DR: 提出了ShortcutBreaker框架，通过低秩噪声瓶颈和全局扰动注意力解决多类无监督异常检测中的身份捷径问题，在多个数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 多类无监督异常检测需要统一模型检测多类异常，但现有Transformer架构存在身份捷径问题，即直接复制输入到输出，导致正常与异常样本的重建误差差异缩小，难以区分。

Method: 1. 基于矩阵秩不等式设计低秩噪声瓶颈，将高维特征投影到低秩潜在空间，防止平凡身份复制；2. 利用ViT的全局建模能力，引入全局扰动注意力防止解码器中的信息捷径。

Result: 在四个基准数据集上测试：MVTec-AD(99.8%)、ViSA(98.9%)、Real-IAD(90.6%)和Universal Medical(87.8%)的图像级AUROC，均优于现有MUAD方法。

Conclusion: ShortcutBreaker通过有效解决身份捷径问题，在多类无监督异常检测任务中实现了优异的性能，适用于工业和医疗等多种场景。

Abstract: Multi-class unsupervised anomaly detection (MUAD) has garnered growing
research interest, as it seeks to develop a unified model for anomaly detection
across multiple classes, i.e., eliminating the need to train separate models
for distinct objects and thereby saving substantial computational resources.
Under the MUAD setting, while advanced Transformer-based architectures have
brought significant performance improvements, identity shortcuts persist: they
directly copy inputs to outputs, narrowing the gap in reconstruction errors
between normal and abnormal cases, and thereby making the two harder to
distinguish. Therefore, we propose ShortcutBreaker, a novel unified
feature-reconstruction framework for MUAD tasks, featuring two key innovations
to address the issue of shortcuts. First, drawing on matrix rank inequality, we
design a low-rank noisy bottleneck (LRNB) to project highdimensional features
into a low-rank latent space, and theoretically demonstrate its capacity to
prevent trivial identity reproduction. Second, leveraging ViTs global modeling
capability instead of merely focusing on local features, we incorporate a
global perturbation attention to prevent information shortcuts in the decoders.
Extensive experiments are performed on four widely used anomaly detection
benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)
and one medical dataset (Universal Medical). The proposed method achieves a
remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four
datasets, respectively, consistently outperforming previous MUAD methods across
different scenarios.

</details>


### [28] [Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games](https://arxiv.org/abs/2510.18395)
*Runnan Qi,Yanan Ni,Lumin Jiang,Zongyuan Li,Kuihua Huang,Xian Guo*

Main category: cs.AI

TL;DR: 提出了MASMP框架，将状态机提示与记忆机制结合，用于实时策略游戏中的LLM智能体，解决了幻觉和决策碎片化问题，在星际争霸II中达到60%胜率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中的幻觉问题和决策碎片化挑战，统一结构化动作与长期战术连贯性。

Method: 结合自然语言驱动的状态机架构和轻量级记忆模块，通过提示引导LLM模拟有限状态机和行为树，跨决策周期保存战略变量。

Result: 在星际争霸II中对抗最强内置AI（Lv7）达到60%胜率，远超基线方法（0%），解决了"知行差距"问题。

Conclusion: 建立了结合神经和符号AI在复杂决策中的新范式，实现了可解释性和类似FSM的可靠性。

Abstract: This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel
framework for LLM agents in real-time strategy games. Addressing key challenges
like hallucinations and fragmented decision-making in existing approaches,
MASMP integrates state machine prompting with memory mechanisms to unify
structured actions with long-term tactical coherence. The framework features:
(1) a natural language-driven state machine architecture that guides LLMs to
emulate finite state machines and behavior trees through prompts, and (2) a
lightweight memory module preserving strategic variables (e.g., tactics,
priority units) across decision cycles. Experiments in StarCraft II demonstrate
MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly
outperforming baselines (0%). Case studies reveal the method retains LLMs'
semantic comprehension while resolving the "Knowing-Doing Gap" through strict
state-action mapping, achieving both interpretability and FSM-like reliability.
This work establishes a new paradigm for combining neural and symbolic AI in
complex decision-making.

</details>


### [29] [Heterogeneous Adversarial Play in Interactive Environments](https://arxiv.org/abs/2510.18407)
*Manjie Xu,Xinyi Yang,Jiayu Zhan,Wei Liang,Chi Zhang,Yixin Zhu*

Main category: cs.AI

TL;DR: 提出了Heterogeneous Adversarial Play (HAP)框架，通过对抗性自动课程学习实现教师-学生协同进化，解决传统自博弈在非对称学习场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自博弈方法在零和竞争环境中利用智能体对称性，但在具有内在不对称性的开放式学习场景中表现不足。人类教学系统展示了非对称教学框架的优势，需要将这种自适应教学机制操作化到人工智能系统中。

Method: HAP将教师-学生互动形式化为极小极大优化，任务生成教师和问题解决学生通过对抗动态共同进化。建立双向反馈系统，教师根据实时学习者表现指标持续重新校准任务复杂度。

Result: 在多任务学习领域的实验验证表明，该框架达到最先进基线的性能水平，同时生成的课程提高了人工智能体和人类受试者的学习效果。

Conclusion: HAP框架成功实现了非对称自适应教学机制，通过对抗性自动课程学习有效提升了学习效率，为开放式学习场景提供了新的解决方案。

Abstract: Self-play constitutes a fundamental paradigm for autonomous skill
acquisition, whereby agents iteratively enhance their capabilities through
self-directed environmental exploration. Conventional self-play frameworks
exploit agent symmetry within zero-sum competitive settings, yet this approach
proves inadequate for open-ended learning scenarios characterized by inherent
asymmetry. Human pedagogical systems exemplify asymmetric instructional
frameworks wherein educators systematically construct challenges calibrated to
individual learners' developmental trajectories. The principal challenge
resides in operationalizing these asymmetric, adaptive pedagogical mechanisms
within artificial systems capable of autonomously synthesizing appropriate
curricula without predetermined task hierarchies. Here we present Heterogeneous
Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework
that formalizes teacher-student interactions as a minimax optimization wherein
task-generating instructor and problem-solving learner co-evolve through
adversarial dynamics. In contrast to prevailing ACL methodologies that employ
static curricula or unidirectional task selection mechanisms, HAP establishes a
bidirectional feedback system wherein instructors continuously recalibrate task
complexity in response to real-time learner performance metrics. Experimental
validation across multi-task learning domains demonstrates that our framework
achieves performance parity with SOTA baselines while generating curricula that
enhance learning efficacy in both artificial agents and human subjects.

</details>


### [30] [Deep Learning-Based Control Optimization for Glass Bottle Forming](https://arxiv.org/abs/2510.18412)
*Mattia Pujatti,Andrea Di Luca,Nicola Peghini,Federico Monegaglia,Marco Cristoforetti*

Main category: cs.AI

TL;DR: 提出基于深度学习的控制算法，用于优化玻璃瓶制造中的成型过程，通过神经网络预测参数变化影响并确定最佳机器设置。


<details>
  <summary>Details</summary>
Motivation: 在玻璃瓶制造中，精确控制成型机器对于确保质量和减少缺陷至关重要，需要优化实时生产环境中的成型过程。

Method: 使用来自实际制造工厂的运行数据，通过神经网络预测参数变化的影响，并设计特定的反演机制来确定实现所需玻璃料滴特性的最佳机器设置。

Result: 在多个生产线的历史数据集上的实验结果显示，该方法产生了有希望的结果，表明具有提高过程稳定性、减少浪费和改进产品一致性的潜力。

Conclusion: 这些结果突显了深度学习在玻璃制造过程控制中的潜力，能够优化成型过程并改善生产质量。

Abstract: In glass bottle manufacturing, precise control of forming machines is
critical for ensuring quality and minimizing defects. This study presents a
deep learning-based control algorithm designed to optimize the forming process
in real production environments. Using real operational data from active
manufacturing plants, our neural network predicts the effects of parameter
changes based on the current production setup. Through a specifically designed
inversion mechanism, the algorithm identifies the optimal machine settings
required to achieve the desired glass gob characteristics. Experimental results
on historical datasets from multiple production lines show that the proposed
method yields promising outcomes, suggesting potential for enhanced process
stability, reduced waste, and improved product consistency. These results
highlight the potential of deep learning to process control in glass
manufacturing.

</details>


### [31] [Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents](https://arxiv.org/abs/2510.18424)
*Guangfu Guo,Xiaoqian Lu,Yue Feng*

Main category: cs.AI

TL;DR: 提出了Med-VRAgent框架，结合视觉引导、自奖励和蒙特卡洛树搜索来解决医学视觉语言模型的幻觉、模糊描述和逻辑不一致问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医学推理中表现良好但存在幻觉、模糊描述、逻辑不一致和定位能力差的问题。

Method: 基于视觉引导和自奖励范式，结合蒙特卡洛树搜索，使用PPO目标对VLM进行微调。

Result: 在多个医学VQA基准测试中优于现有方法。

Conclusion: Med-VRAgent框架有效提升了医学视觉推理能力，通过轨迹反馈和PPO微调进一步优化性能。

Abstract: Visual Language Models (VLMs) achieve promising results in medical reasoning
but struggle with hallucinations, vague descriptions, inconsistent logic and
poor localization. To address this, we propose a agent framework named Medical
Visual Reasoning Agent (\textbf{Med-VRAgent}). The approach is based on Visual
Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By
combining the Visual Guidance with tree search, Med-VRAgent improves the
medical visual reasoning capabilities of VLMs. We use the trajectories
collected by Med-VRAgent as feedback to further improve the performance by
fine-tuning the VLMs with the proximal policy optimization (PPO) objective.
Experiments on multiple medical VQA benchmarks demonstrate that our method
outperforms existing approaches.

</details>


### [32] [Automated urban waterlogging assessment and early warning through a mixture of foundation models](https://arxiv.org/abs/2510.18425)
*Chenxu Zhang,Fuxiang Huang,Lei Zhang*

Main category: cs.AI

TL;DR: UWAssess是一个基于基础模型的框架，自动从监控图像中识别积水区域并生成结构化评估报告，采用半监督微调和思维链提示策略解决标注数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧导致城市内涝威胁日益严重，现有监测方法依赖人工报告，无法提供及时全面的评估。

Method: 设计半监督微调策略和思维链提示策略，释放基础模型在数据稀缺下游任务中的潜力，构建多基础模型协作框架。

Result: 在视觉基准测试中感知性能显著提升，GPT评估证实能生成准确描述积水范围、深度、风险和影响的可靠文本报告。

Conclusion: 该框架实现了从感知到生成的监测模式转变，为智能可扩展系统奠定基础，支持城市管理、灾害响应和气候韧性。

Abstract: With climate change intensifying, urban waterlogging poses an increasingly
severe threat to global public safety and infrastructure. However, existing
monitoring approaches rely heavily on manual reporting and fail to provide
timely and comprehensive assessments. In this study, we present Urban
Waterlogging Assessment (UWAssess), a foundation model-driven framework that
automatically identifies waterlogged areas in surveillance images and generates
structured assessment reports. To address the scarcity of labeled data, we
design a semi-supervised fine-tuning strategy and a chain-of-thought (CoT)
prompting strategy to unleash the potential of the foundation model for
data-scarce downstream tasks. Evaluations on challenging visual benchmarks
demonstrate substantial improvements in perception performance. GPT-based
evaluations confirm the ability of UWAssess to generate reliable textual
reports that accurately describe waterlogging extent, depth, risk and impact.
This dual capability enables a shift of waterlogging monitoring from perception
to generation, while the collaborative framework of multiple foundation models
lays the groundwork for intelligent and scalable systems, supporting urban
management, disaster response and climate resilience.

</details>


### [33] [AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library](https://arxiv.org/abs/2510.18428)
*Minwei Kong,Ao Qu,Xiaotong Guo,Wenbin Ouyang,Chonghe Jiang,Han Zheng,Yining Ma,Dingyi Zhuang,Yuhan Tang,Junyi Li,Hai Wang,Cathy Wu,Jinhua Zhao*

Main category: cs.AI

TL;DR: AlphaOPT是一个自改进的经验库系统，让LLM能够从有限演示和求解器反馈中学习优化建模，无需标注推理轨迹或参数更新。


<details>
  <summary>Details</summary>
Motivation: 优化建模在工业决策中至关重要但难以自动化，现有LLM方法要么依赖脆弱的提示工程，要么需要昂贵的重新训练且泛化能力有限。

Method: 采用持续两阶段循环：库学习阶段从失败尝试中提取求解器验证的结构化见解；库演化阶段诊断检索偏差并精炼存储见解的适用条件。

Result: 实验显示AlphaOPT随数据增加持续改进（从100到300训练项，性能从65%提升到72%），在仅使用答案训练时在OptiBench数据集上超越最强基线7.7%。

Conclusion: AlphaOPT能够高效从有限演示中学习，通过更新库而非模型权重实现持续扩展，使知识显式化且可解释，支持人工检查和干预。

Abstract: Optimization modeling enables critical decisions across industries but
remains difficult to automate: informal language must be mapped to precise
mathematical formulations and executable solver code. Prior LLM approaches
either rely on brittle prompting or costly retraining with limited
generalization. We present AlphaOPT, a self-improving experience library that
enables an LLM to learn from limited demonstrations (even answers alone,
without gold-standard programs) and solver feedback - without annotated
reasoning traces or parameter updates. AlphaOPT operates in a continual
two-phase cycle: (i) a Library Learning phase that reflects on failed attempts,
extracting solver-verified, structured insights as {taxonomy, condition,
explanation, example}; and (ii) a Library Evolution phase that diagnoses
retrieval misalignments and refines the applicability conditions of stored
insights, improving transfer across tasks. This design (1) learns efficiently
from limited demonstrations without curated rationales, (2) expands continually
without costly retraining by updating the library rather than model weights,
and (3) makes knowledge explicit and interpretable for human inspection and
intervention. Experiments show that AlphaOPT steadily improves with more data
(65% to 72% from 100 to 300 training items) and surpasses the strongest
baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only
on answers. Code and data are available at:
https://github.com/Minw913/AlphaOPT.

</details>


### [34] [PlanU: Large Language Model Decision Making through Planning under Uncertainty](https://arxiv.org/abs/2510.18442)
*Ziwei Deng,Mian Deng,Chenjing Liang,Zeming Gao,Chennan Ma,Chenxing Lin,Haipeng Zhang,Songzhu Mei,Cheng Wang,Siqi Shen*

Main category: cs.AI

TL;DR: PlanU是一种基于LLM的规划方法，通过蒙特卡洛树搜索中的分位数分布来捕捉不确定性，提升LLM在不确定性环境中的决策能力。


<details>
  <summary>Details</summary>
Motivation: LLM在不确定性环境中的决策能力较弱，现有方法要么只处理LLM不确定性而忽略环境不确定性，要么无法处理需要与环境交互的多步决策任务。

Method: 在MCTS中将每个节点的回报建模为分位数分布，引入带有好奇心因子的上置信界分数来平衡探索与利用。

Result: 通过大量实验验证了PlanU在不确定性环境下LLM决策任务中的有效性。

Conclusion: PlanU成功解决了LLM决策中的不确定性挑战，在随机环境中表现出色。

Abstract: Large Language Models (LLMs) are increasingly being explored across a range
of decision-making tasks. However, LLMs sometimes struggle with decision-making
tasks under uncertainty that are relatively easy for humans, such as planning
actions in stochastic environments. The adoption of LLMs for decision-making is
impeded by uncertainty challenges, such as LLM uncertainty and environmental
uncertainty. LLM uncertainty arises from the stochastic sampling process
inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM
uncertainty through multiple reasoning chains or search trees. However, these
approaches overlook environmental uncertainty, which leads to poor performance
in environments with stochastic state transitions. Some recent LDM approaches
deal with uncertainty by forecasting the probability of unknown variables.
However, they are not designed for multi-step decision-making tasks that
require interaction with the environment. To address uncertainty in LLM
decision-making, we introduce PlanU, an LLM-based planning method that captures
uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of
each node in the MCTS as a quantile distribution, which uses a set of quantiles
to represent the return distribution. To balance exploration and exploitation
during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity
(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive
experiments, we demonstrate the effectiveness of PlanU in LLM-based
decision-making tasks under uncertainty.

</details>


### [35] [CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs](https://arxiv.org/abs/2510.18470)
*Shaobo Wang,Yongliang Miao,Yuancheng Liu,and Qianli Ma,Ning Liao,Linfeng Zhang*

Main category: cs.AI

TL;DR: CircuitSeer通过分析模型内部注意力机制识别核心推理电路，基于此选择高质量训练数据，仅用10%数据就能超越全数据集训练效果


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法依赖昂贵的外部模型或不透明的启发式方法，而大型语言模型的推理能力提升通常需要大量计算资源训练推理数据集

Method: 发现复杂推理任务会激活稀疏的专用注意力头形成核心推理电路，CircuitSeer通过量化数据对这些关键电路的影响来评估推理复杂度

Result: 在4个模型和9个数据集上的实验显示，使用CircuitSeer选择的10%数据微调Qwen2.5-Math-7B，平均Pass@1比全数据集训练提升1.4个百分点

Conclusion: 基于模型内部机制的数据选择方法CircuitSeer在效率和效果上都优于现有方法，为高效训练推理模型提供了新思路

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities, but scaling their performance often relies on massive reasoning
datasets that are computationally expensive to train on. Existing data
selection methods aim to curate smaller, high-quality subsets but often rely on
costly external models or opaque heuristics. In this work, we shift the focus
from external heuristics to the model's internal mechanisms. We find that
complex reasoning tasks consistently activate a sparse, specialized subset of
attention heads, forming core reasoning circuits. Building on this insight, we
propose CircuitSeer, a novel data selection method that quantifies the
reasoning complexity of data by measuring its influence on these crucial
circuits. Extensive experiments on 4 models and 9 datasets demonstrate
CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of
data selected by our method achieves a 1.4-point gain in average Pass@1 over
training on the full dataset, highlighting its efficiency and effectiveness.

</details>


### [36] [Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents](https://arxiv.org/abs/2510.18476)
*Feifan Xia,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 提出了一个用于多轮社交对话中LLM智能体的概率意图建模框架，通过维护对伙伴潜在意图的信念分布来实现自适应对话策略


<details>
  <summary>Details</summary>
Motivation: 在多轮社交对话中，LLM智能体需要理解并适应伙伴的潜在意图，但直接观察意图通常不可行，因此需要概率建模来处理不确定性

Method: 框架维护对伙伴潜在意图的信念分布，从上下文先验初始化，并在每轮对话后通过似然估计动态更新，为策略提供额外的上下文基础

Result: 在SOTOPIA环境中的初步实验显示：相比Qwen2.5-7B基线，总体得分在SOTOPIA-All上提升9.0%，在SOTOPIA-Hard上提升4.1%，甚至略微超过直接观察伙伴意图的oracle智能体

Conclusion: 概率意图建模有助于开发具有社会智能的LLM智能体，能够有效处理对话中的不确定性并实现自适应策略

Abstract: We present a probabilistic intent modeling framework for large language model
(LLM) agents in multi-turn social dialogue. The framework maintains a belief
distribution over a partner's latent intentions, initialized from contextual
priors and dynamically updated through likelihood estimation after each
utterance. The evolving distribution provides additional contextual grounding
for the policy, enabling adaptive dialogue strategies under uncertainty.
Preliminary experiments in the SOTOPIA environment show consistent
improvements: the proposed framework increases the Overall score by 9.0% on
SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and
slightly surpasses an oracle agent that directly observes partner intentions.
These early results suggest that probabilistic intent modeling can contribute
to the development of socially intelligent LLM agents.

</details>


### [37] [LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources](https://arxiv.org/abs/2510.18477)
*Haichao Ji,Zibo Wang,Yifei Zhu,Meng han,Dan Wang,Zhu Han*

Main category: cs.AI

TL;DR: LAFA是首个将基于LLM代理的数据分析与联邦分析(FA)相结合的系统，通过分层多代理架构将自然语言查询转换为优化的可执行FA工作流，在保护隐私的同时支持自然语言输入。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM代理的分析框架假设集中式数据访问，缺乏隐私保护；而联邦分析虽然支持隐私保护计算，但缺乏自然语言输入支持。需要结合两者的优势。

Method: 采用分层多代理架构：粗粒度规划器分解复杂查询为子查询，细粒度规划器将子查询映射为FA操作的有向无环图，优化器代理重写和合并多个DAG以消除冗余操作。

Result: 实验表明LAFA在实现更高执行计划成功率的同时，显著减少了资源密集的FA操作，优于基线提示策略。

Conclusion: 这项工作为在FA设置中支持自然语言输入的隐私保护、LLM驱动的分析建立了实用基础。

Abstract: Large Language Models (LLMs) have shown great promise in automating data
analytics tasks by interpreting natural language queries and generating
multi-operation execution plans. However, existing LLM-agent-based analytics
frameworks operate under the assumption of centralized data access, offering
little to no privacy protection. In contrast, federated analytics (FA) enables
privacy-preserving computation across distributed data sources, but lacks
support for natural language input and requires structured, machine-readable
queries. In this work, we present LAFA, the first system that integrates
LLM-agent-based data analytics with FA. LAFA introduces a hierarchical
multi-agent architecture that accepts natural language queries and transforms
them into optimized, executable FA workflows. A coarse-grained planner first
decomposes complex queries into sub-queries, while a fine-grained planner maps
each subquery into a Directed Acyclic Graph of FA operations using prior
structural knowledge. To improve execution efficiency, an optimizer agent
rewrites and merges multiple DAGs, eliminating redundant operations and
minimizing computational and communicational overhead. Our experiments
demonstrate that LAFA consistently outperforms baseline prompting strategies by
achieving higher execution plan success rates and reducing resource-intensive
FA operations by a substantial margin. This work establishes a practical
foundation for privacy-preserving, LLM-driven analytics that supports natural
language input in the FA setting.

</details>


### [38] [StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking](https://arxiv.org/abs/2510.18483)
*Haoran Zhang,Chenhao Zhu,Sicong Guo,Hanzhe Guo,Haiming Li,Donglin Yu*

Main category: cs.AI

TL;DR: StarBench是一个基于《崩坏：星穹铁道》的回合制RPG基准测试，评估视觉语言模型在像素到动作的多模态决策和主动信息寻求两个人类能力方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在真实客户端中实现人类式游戏（将原始截图映射到时间一致的低级动作，同时决定何时寻求指导）仍然是一个开放挑战。

Method: StarBench标准化了八个战斗任务的评估，包含两种模式：直接控制（仅接收截图并发出低级操作）和工具辅助控制（通过检测器和OCR提供文本化观察）。还包括ask-or-act诊断来测量代理何时选择请求指导。

Result: 结果显示在直接控制模式下感知到控制的保真度存在显著差距，而明智的信息寻求与成功率的提高相关。

Conclusion: StarBench为真实客户端游戏中的主动信息寻求和多模态决策提供了一个可复现的衡量标准。

Abstract: Human players do more than press buttons: they ground what they see on screen
into precise keyboard-mouse actions and, when stuck, they seek information
before trying again. We ask whether current vision-language models (VLMs) can
do the same. Despite encouraging results under simplified control or tool
scaffolds, human-like play in a real client - mapping raw screenshots to
temporally coherent low-level actions while deciding when to ask for guidance -
remains an open challenge. We introduce StarBench, a turn-based RPG benchmark
derived from Honkai: Star Rail that targets these two human-like competencies:
multimodal decision-making from pixels to actions and agentic information
seeking. StarBench standardizes evaluation across eight combat tasks and two
regimes with shared tasks and metrics: (i) direct control, where agents receive
only screenshots and must emit low-level primitives (click and keypress) with
no semantic hints; and (ii) tool-assisted control, where higher-level intents
can be mapped to primitives by detectors and OCR outputs provide optional
textualized observations to ease UI grounding. To mirror human practice,
StarBench also includes an ask-or-act diagnostic that measures whether and when
agents choose to request brief guidance before proceeding, and how that choice
affects subsequent performance. We report reference baselines for contemporary
VLMs and a human reference. Results expose sizable gaps in
perception-to-control fidelity in the direct regime, while showing that
judicious information seeking correlates with improved success, establishing
StarBench as a reproducible yardstick for agentic information seeking and
multimodal decision-making in real-client play.

</details>


### [39] [AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification](https://arxiv.org/abs/2510.18488)
*Ho Fai Leung,Xiaoyan Xi,Fei Zuo*

Main category: cs.AI

TL;DR: 研究发现现有GUI代理基准测试存在缺陷，通过改进AndroidControl为AndroidControl-Curated基准，使模型性能评估更准确，并开发了轻量级模型Magma-R1-3B，性能接近大型模型但参数少200倍。


<details>
  <summary>Details</summary>
Motivation: 解决现有虚拟助手依赖刚性API的问题，以及GUI代理因基准测试不准确而被低估性能的问题。

Method: 识别AndroidControl基准的缺陷，通过净化流程创建AndroidControl-Curated基准，并使用2.4k精选样本对3B参数模型进行后训练。

Result: 在改进的基准上，最先进模型的复杂任务成功率接近75%（提升15%），Magma-R1-3B模型性能与Qwen3-VL-235B相当但参数少200倍。

Conclusion: 设备端GUI代理实际性能优于预期，改进的基准能更准确评估模型能力，加速稳健虚拟助手的开发。

Abstract: On-device virtual assistants like Siri and Google Assistant are increasingly
pivotal, yet their capabilities are hamstrung by a reliance on rigid,
developer-dependent APIs. GUI agents offer a powerful, API-independent
alternative, but their adoption is hindered by the perception of poor
performance, as even the best models (e.g. Qwen3-VL-235B) scores are capped at
around 60% on benchmarks like AndroidControl, far from viability for real-world
use. Our research reveals that issue lies not only with the models but with the
benchmarks themselves. We identified notable shortcomings in AndroidControl,
including ambiguities and factual errors, which systematically underrates agent
capabilities. To address this critical oversight, we enhanced AndroidControl
into AndroidControl-Curated, a refined version of the benchmark improved
through a rigorous purification pipeline. On this enhanced benchmark,
state-of-the-art models achieve success rates nearing 75% on complex tasks (15%
improvement), reflecting that on-device GUI agents are actually closer to
practical deployment than previously thought. We introduce our new SOTA model,
Magma-R1- 3B, post-trained on just 2.4k curated samples using 60 hours of an
H20 GPU (approximately $60). Despite being 200 times smaller in parameters,
this model delivers performance comparable to Qwen3- VL-235B. We release both
AndroidControl-Curated benchmark and Magma-R1 model to the research community,
encouraging adoption of this enhanced benchmark to better reflect model
capabilities and accelerate the development of robust, on-device virtual
assistants.

</details>


### [40] [Crucible: Quantifying the Potential of Control Algorithms through LLM Agents](https://arxiv.org/abs/2510.18491)
*Lianchen Jia,Chaoyang Li,Qian Houde,Tianchi Huang,Jiangchuan Liu,Lifeng Sun*

Main category: cs.AI

TL;DR: 本文提出了Crucible框架，使用LLM驱动的多级专家模拟来评估算法的调优潜力，并定义了量化指标来系统评估不同算法的可调空间。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注算法在理想或默认配置下的性能，忽略了调优潜力这一关键维度。生产环境中的控制算法需要领域专家针对特定场景进行参数和逻辑调优。

Method: 采用LLM驱动的多级专家模拟方法，通过Crucible框架对算法进行调优，并定义形式化指标来量化评估调优潜力。

Result: 实验结果表明，Crucible能够系统量化不同算法的可调空间，在从经典控制任务到复杂计算机系统的广泛案例研究中都表现出有效性，并在实际部署中得到验证。

Conclusion: Crucible为算法分析和设计提供了新维度，最终能够带来性能提升，是评估算法调优潜力的有效工具。

Abstract: Control algorithms in production environments typically require domain
experts to tune their parameters and logic for specific scenarios. However,
existing research predominantly focuses on algorithmic performance under ideal
or default configurations, overlooking the critical aspect of Tuning Potential.
To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven,
multi-level expert simulation to turn algorithms and defines a formalized
metric to quantitatively evaluate their Tuning Potential. We demonstrate
Crucible's effectiveness across a wide spectrum of case studies, from classic
control tasks to complex computer systems, and validate its findings in a
real-world deployment. Our experimental results reveal that Crucible
systematically quantifies the tunable space across different algorithms.
Furthermore, Crucible provides a new dimension for algorithm analysis and
design, which ultimately leads to performance improvements. Our code is
available at https://github.com/thu-media/Crucible.

</details>


### [41] [Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models](https://arxiv.org/abs/2510.18526)
*Hanze Guo,Jing Yao,Xiao Zhou,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: COUPLE是一个基于反事实推理的框架，用于解决大语言模型与多元人类价值观对齐的挑战，通过结构因果模型处理价值观的复杂性和可引导性问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多元文化、社区和人口统计中的应用增加，需要超越平均原则（如HHH）与多元人类价值观对齐。现有方法在处理细粒度价值目标时面临两个挑战：价值观复杂性和可引导性。

Method: 提出COUPLE框架，使用结构因果模型（SCM）来表征特征间的复杂相互依赖关系和优先级，以及高层次价值维度与行为之间的因果关系。应用反事实推理生成符合任何期望价值目标的输出。

Result: 在两个具有不同价值系统的数据集上评估COUPLE，结果显示COUPLE在多种价值目标类型上优于其他基线方法。

Conclusion: COUPLE通过明确的因果建模提供了更好的可解释性，能够有效处理多元价值观对齐的复杂性和可引导性问题，在多样化价值目标上表现出优越性能。

Abstract: As large language models (LLMs) become increasingly integrated into
applications serving users across diverse cultures, communities and
demographics, it is critical to align LLMs with pluralistic human values beyond
average principles (e.g., HHH). In psychological and social value theories such
as Schwartz's Value Theory, pluralistic values are represented by multiple
value dimensions paired with various priorities. However, existing methods
encounter two challenges when aligning with such fine-grained value objectives:
1) they often treat multiple values as independent and equally important,
ignoring their interdependence and relative priorities (value complexity); 2)
they struggle to precisely control nuanced value priorities, especially those
underrepresented ones (value steerability). To handle these challenges, we
propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE
alignment. It introduces a structural causal model (SCM) to feature complex
interdependency and prioritization among features, as well as the causal
relationship between high-level value dimensions and behaviors. Moreover, it
applies counterfactual reasoning to generate outputs aligned with any desired
value objectives. Benefitting from explicit causal modeling, COUPLE also
provides better interpretability. We evaluate COUPLE on two datasets with
different value systems and demonstrate that COUPLE advances other baselines
across diverse types of value objectives.

</details>


### [42] [Physics-guided Emulators Reveal Resilience and Fragility under Operational Latencies and Outages](https://arxiv.org/abs/2510.18535)
*Sarth Dubey,Subimal Ghosh,Udit Bhatia*

Main category: cs.AI

TL;DR: 开发了一个操作就绪的GloFAS模拟器，结合长短时记忆网络和松弛水平衡约束，在数据延迟、缺失或不一致情况下保持稳定性能。


<details>
  <summary>Details</summary>
Motivation: 大多数降雨径流预测模型在理想数据条件下评估，强调准确性而非操作韧性。需要开发在真实操作环境中可靠的洪水预报系统。

Method: 使用五种架构覆盖从完整历史预报到数据延迟和中断的不同信息可用性场景，结合长短时记忆网络和松弛水平衡约束保持物理一致性。

Result: 在美国5000多个流域测试，包括印度受强烈调控的河流，模拟器能重现GloFAS水文核心，在信息质量下降时性能平稳退化。跨不同水文气候和管理体制的迁移产生物理一致但性能降低的结果。

Conclusion: 该框架将操作鲁棒性确立为水文机器学习可测量属性，推进了可靠实时预报系统的设计。

Abstract: Reliable hydrologic and flood forecasting requires models that remain stable
when input data are delayed, missing, or inconsistent. However, most advances
in rainfall-runoff prediction have been evaluated under ideal data conditions,
emphasizing accuracy rather than operational resilience. Here, we develop an
operationally ready emulator of the Global Flood Awareness System (GloFAS) that
couples long- and short-term memory networks with a relaxed water-balance
constraint to preserve physical coherence. Five architectures span a continuum
of information availability: from complete historical and forecast forcings to
scenarios with data latency and outages, allowing systematic evaluation of
robustness. Trained in minimally managed catchments across the United States
and tested in more than 5,000 basins, including heavily regulated rivers in
India, the emulator reproduces the hydrological core of GloFAS and degrades
smoothly as information quality declines. Transfer across contrasting
hydroclimatic and management regimes yields reduced yet physically consistent
performance, defining the limits of generalization under data scarcity and
human influence. The framework establishes operational robustness as a
measurable property of hydrological machine learning and advances the design of
reliable real-time forecasting systems.

</details>


### [43] [SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation](https://arxiv.org/abs/2510.18551)
*Yuncheng Hua,Sion Weatherhead,Mehdi Jafari,Hao Xue,Flora D. Salim*

Main category: cs.AI

TL;DR: SOCIA-Nabla是一个端到端的智能体框架，将模拟器构建视为文本计算图中的代码实例优化，通过LLM驱动的智能体节点和损失驱动循环实现代码合成、执行、评估和修复。


<details>
  <summary>Details</summary>
Motivation: 将脆弱的提示管道转换为可重现、约束感知的模拟器代码生成，实现跨领域和模拟粒度的扩展。

Method: 在文本计算图中嵌入专门的LLM驱动智能体作为节点，工作流管理器执行损失驱动循环：代码合成→执行→评估→代码修复，优化器执行文本梯度下降(TGD)。

Result: 在三个CPS任务（用户建模、口罩采用和个人移动性）中实现了最先进的整体准确率。

Conclusion: 通过统一多智能体编排与损失对齐的优化视角，SOCIA-Nabla能够生成可重现、约束感知的模拟器代码，并扩展到不同领域和模拟粒度。

Abstract: In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that
treats simulator construction asinstance optimization over code within a
textual computation graph. Specialized LLM-driven agents are embedded as graph
nodes, and a workflow manager executes a loss-driven loop: code synthesis ->
execution -> evaluation -> code repair. The optimizer performs Textual-Gradient
Descent (TGD), while human-in-the-loop interaction is reserved for task-spec
confirmation, minimizing expert effort and keeping the code itself as the
trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,
and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.
By unifying multi-agent orchestration with a loss-aligned optimization view,
SOCIA-Nabla converts brittle prompt pipelines into reproducible,
constraint-aware simulator code generation that scales across domains and
simulation granularities. This work is under review, and we will release the
code soon.

</details>


### [44] [Extracting alignment data in open models](https://arxiv.org/abs/2510.18554)
*Federico Barbero,Xiangming Gu,Christopher A. Choquette-Choo,Chawin Sitawarin,Matthew Jagielski,Itay Yona,Petar Veličković,Ilia Shumailov,Jamie Hayes*

Main category: cs.AI

TL;DR: 研究发现可以从经过后训练（如SFT或RL）的模型中提取大量对齐训练数据，这些数据可用于改进模型的长上下文推理、安全性、指令遵循和数学能力。


<details>
  <summary>Details</summary>
Motivation: 揭示模型对齐数据提取的风险，以及蒸馏实践可能带来的间接训练原数据集的影响。

Method: 使用高质量嵌入模型而非字符串匹配来识别语义相似的训练数据，发现近似字符串匹配会严重低估可提取的数据量。

Result: 模型容易重现后训练阶段使用的训练数据，这些数据可用于训练基础模型，恢复相当程度的原始性能。

Conclusion: 这项工作暴露了提取对齐数据的潜在风险，并引发了关于蒸馏实践下游影响的讨论。

Abstract: In this work, we show that it is possible to extract significant amounts of
alignment training data from a post-trained model -- useful to steer the model
to improve certain capabilities such as long-context reasoning, safety,
instruction following, and maths. While the majority of related work on
memorisation has focused on measuring success of training data extraction
through string matching, we argue that embedding models are better suited for
our specific goals. Distances measured through a high quality embedding model
can identify semantic similarities between strings that a different metric such
as edit distance will struggle to capture. In fact, in our investigation,
approximate string matching would have severely undercounted (by a conservative
estimate of $10\times$) the amount of data that can be extracted due to trivial
artifacts that deflate the metric. Interestingly, we find that models readily
regurgitate training data that was used in post-training phases such as SFT or
RL. We show that this data can be then used to train a base model, recovering a
meaningful amount of the original performance. We believe our work exposes a
possibly overlooked risk towards extracting alignment data. Finally, our work
opens up an interesting discussion on the downstream effects of distillation
practices: since models seem to be regurgitating aspects of their training set,
distillation can therefore be thought of as indirectly training on the model's
original dataset.

</details>


### [45] [QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework](https://arxiv.org/abs/2510.18569)
*Junhyeog Yun,Hyoun Jun Lee,Insu Jeon*

Main category: cs.AI

TL;DR: QuantEvolve是一个结合质量多样性优化和假设驱动策略生成的进化框架，用于自动化开发量化交易策略，能够在动态市场中保持策略多样性并适应市场变化。


<details>
  <summary>Details</summary>
Motivation: 动态市场中自动化开发量化交易策略具有挑战性，现有方法难以在探索广阔策略空间的同时保持策略多样性，无法适应变化的市场条件和个性化投资需求。

Method: 使用特征映射与投资者偏好对齐（策略类型、风险特征、换手率、收益特征），结合假设驱动的多智能体系统，通过迭代生成和评估来系统探索策略空间。

Result: 实证结果表明QuantEvolve优于传统基准方法，能够产生多样化、复杂的策略，适应市场机制变化和个性化投资需求。

Conclusion: QuantEvolve框架有效解决了量化交易策略开发中的多样性保持和适应性挑战，为未来研究提供了策略数据集支持。

Abstract: Automating quantitative trading strategy development in dynamic markets is
challenging, especially with increasing demand for personalized investment
solutions. Existing methods often fail to explore the vast strategy space while
preserving the diversity essential for robust performance across changing
market conditions. We present QuantEvolve, an evolutionary framework that
combines quality-diversity optimization with hypothesis-driven strategy
generation. QuantEvolve employs a feature map aligned with investor
preferences, such as strategy type, risk profile, turnover, and return
characteristics, to maintain a diverse set of effective strategies. It also
integrates a hypothesis-driven multi-agent system to systematically explore the
strategy space through iterative generation and evaluation. This approach
produces diverse, sophisticated strategies that adapt to both market regime
shifts and individual investment needs. Empirical results show that QuantEvolve
outperforms conventional baselines, validating its effectiveness. We release a
dataset of evolved strategies to support future research.

</details>


### [46] [VAR: Visual Attention Reasoning via Structured Search and Backtracking](https://arxiv.org/abs/2510.18619)
*Wei Cai,Jian Zhao,Yuchen Yuan,Tianle Zhang,Ming Zhu,Haichuan Tang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出VAR框架，通过结构化搜索解决多模态大语言模型的幻觉问题，将推理分解为可追溯的证据基础和搜索式思维链生成，包含回溯自校正机制。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs存在高幻觉倾向和脆弱的线性推理过程，在复杂任务中容易失败，需要更可靠的推理框架。

Method: 将基础推理重构为结构化搜索过程，包含两个关键阶段：可追溯证据基础和搜索式CoT生成，采用多面奖励函数进行语义和几何自验证。

Result: VAR-7B模型在幻觉和安全基准测试中创下新SOTA，显著优于开源模型，与领先专有系统性能相当。

Conclusion: VAR框架通过结构化搜索和自验证机制有效减少了MLLMs的幻觉问题，提供了更可靠的推理能力。

Abstract: Multimodal Large Language Models (MLLMs), despite their advances, are
hindered by their high hallucination tendency and heavy reliance on brittle,
linear reasoning processes, leading to failures in complex tasks. To address
these limitations, we introduce Visual Attention Reasoning (VAR), a novel
framework that recasts grounded reasoning as a structured search over a
reasoning trajectory space. VAR decomposes the reasoning process into two key
stages: traceable evidence grounding and search-based chain-of-thought (CoT)
generation, which incorporates a backtracking mechanism for self-correction.
The search is guided by a multi-faceted reward function with semantic and
geometric self-verification components, which penalize outputs that are not
faithfully grounded in the visual input. We provide a theoretical analysis for
our search strategy, validating its capability to find the correct solution
with high probability. Experimental results show that our 7B model, VAR-7B,
sets a new state-of-the-art on a comprehensive suite of hallucination and
safety benchmarks, significantly outperforming existing open-source models and
demonstrating competitive performance against leading proprietary systems.

</details>


### [47] [Leveraging Association Rules for Better Predictions and Better Explanations](https://arxiv.org/abs/2510.18628)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.AI

TL;DR: 提出了一种结合数据和知识的分类方法，通过数据挖掘获取关联规则来提升决策树和随机森林的预测性能，并改善解释任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法主要依赖数据，缺乏对领域知识的利用。本文旨在结合数据挖掘得到的关联规则来增强树基模型的预测能力和解释性。

Method: 从数据中挖掘关联规则（可能包含否定），将这些规则整合到决策树和随机森林中，用于提升分类性能并生成更泛化的溯因解释。

Result: 实验表明，该方法在两个树基模型上都能在预测性能和解释大小方面带来显著改进。

Conclusion: 结合数据挖掘得到的关联规则可以有效提升树基分类模型的预测性能和解释能力，为分类任务提供了更优的解决方案。

Abstract: We present a new approach to classification that combines data and knowledge.
In this approach, data mining is used to derive association rules (possibly
with negations) from data. Those rules are leveraged to increase the predictive
performance of tree-based models (decision trees and random forests) used for a
classification task. They are also used to improve the corresponding
explanation task through the generation of abductive explanations that are more
general than those derivable without taking such rules into account.
Experiments show that for the two tree-based models under consideration,
benefits can be offered by the approach in terms of predictive performance and
in terms of explanation sizes.

</details>


### [48] [Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises](https://arxiv.org/abs/2510.18631)
*Carlo Proietti,Antonio Yuste-Ginel*

Main category: cs.AI

TL;DR: 本文研究了形式论证中定性不确定性的建模，比较了抽象模型和结构化模型在表达能力方面的差异，提出了新的表达能力概念，并给出了正负两方面的表达能力结果。


<details>
  <summary>Details</summary>
Motivation: 在形式论证中建模定性不确定性对于实际应用和理论理解都至关重要，但目前大多数研究都集中在抽象模型上，需要研究这些抽象模型的合理实例化。

Method: 通过将论证的不确定性基于其组成部分（规则和前提）来构建结构化模型，引入了一个能够处理抽象和结构化形式体系的表达能力概念，并比较了抽象模型和结构化模型的表达能力。

Result: 提出了正负两方面的表达能力结果，影响了不完整抽象论证框架及其依赖扩展（抽象方面）和ASPIC+（结构化方面）。

Conclusion: 研究为形式论证中不确定性建模提供了理论基础，通过比较抽象和结构化模型的表达能力，推动了该领域的发展。

Abstract: Modelling qualitative uncertainty in formal argumentation is essential both
for practical applications and theoretical understanding. Yet, most of the
existing works focus on \textit{abstract} models for arguing with uncertainty.
Following a recent trend in the literature, we tackle the open question of
studying plausible instantiations of these abstract models. To do so, we ground
the uncertainty of arguments in their components, structured within rules and
premises. Our main technical contributions are: i) the introduction of a notion
of expressivity that can handle abstract and structured formalisms, and ii) the
presentation of both negative and positive expressivity results, comparing the
expressivity of abstract and structured models of argumentation with
uncertainty. These results affect incomplete abstract argumentation frameworks,
and their extension with dependencies, on the abstract side, and ASPIC+, on the
structured side.

</details>


### [49] [Query Decomposition for RAG: Balancing Exploration-Exploitation](https://arxiv.org/abs/2510.18633)
*Roxana Petcu,Kenton Murray,Daniel Khashabi,Evangelos Kanoulas,Maarten de Rijke,Dawn Lawrie,Kevin Duh*

Main category: cs.AI

TL;DR: 本文提出了一种基于多臂老虎机方法的检索增强生成系统，通过动态选择信息量最大的子查询来平衡检索的广度和噪声控制，显著提升了文档检索精度和长文本生成性能。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中在复杂用户请求下平衡检索广度（捕获所有相关材料）与检索限制（避免过度噪声和计算成本）的关键权衡问题。

Method: 将查询分解和文档检索建模为利用-探索设置，使用各种老虎机学习方法动态选择最有信息量的子查询，并利用排名信息和人工判断来估计文档相关性。

Result: 在文档级精度上获得35%的提升，α-nDCG指标提高15%，并在长文本生成的下游任务中表现更好。

Conclusion: 基于老虎机学习的动态子查询选择方法能有效提升RAG系统的检索效率和生成质量，证明了利用排名信息和相关性估计的重要性。

Abstract: Retrieval-augmented generation (RAG) systems address complex user requests by
decomposing them into subqueries, retrieving potentially relevant documents for
each, and then aggregating them to generate an answer. Efficiently selecting
informative documents requires balancing a key trade-off: (i) retrieving
broadly enough to capture all the relevant material, and (ii) limiting
retrieval to avoid excessive noise and computational cost. We formulate query
decomposition and document retrieval in an exploitation-exploration setting,
where retrieving one document at a time builds a belief about the utility of a
given sub-query and informs the decision to continue exploiting or exploring an
alternative. We experiment with a variety of bandit learning methods and
demonstrate their effectiveness in dynamically selecting the most informative
sub-queries. Our main finding is that estimating document relevance using rank
information and human judgments yields a 35% gain in document-level precision,
15% increase in {\alpha}-nDCG, and better performance on the downstream task of
long-form generation.

</details>


### [50] [Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval](https://arxiv.org/abs/2510.18659)
*Dong Yun,Marco Schouten,Dim Papadopoulos*

Main category: cs.AI

TL;DR: SherlockLLM是一个基于强化学习的对话检索框架，通过生成二进制问题序列来高效缩小搜索空间，无需大规模标注对话数据。


<details>
  <summary>Details</summary>
Motivation: 信息检索中的用户查询通常具有歧义性，而现有的对话式交互检索系统缺乏明确的策略来提出最具信息量的问题，导致效率低下。

Method: 使用强化学习训练一个代理，学习生成二进制问题序列的最优提问策略，无需依赖大规模标注对话数据。

Result: 在结构化任务中，性能与强基线方法相当，接近二进制搜索的理论最优值；在非结构化任务中，显著优于基线方法。

Conclusion: SherlockLLM是一个稳健高效的解决方案，能够学习到高度有效的信息寻求对话策略。

Abstract: User queries in information retrieval are often ambiguous, making it
challenging for systems to identify a user's target from a single query. While
recent dialogue-based interactive retrieval systems can clarify user intent,
they are inefficient as they often lack an explicit strategy to ask the most
informative questions. To address this limitation, we propose SherlockLLM, a
dialogue-driven retrieval framework that learns an optimal questioning strategy
via Reinforcement Learning (RL) and avoids the need for large-scale annotated
dialogue data. In our framework, an agent is trained to generate a sequence of
binary questions to efficiently narrow down the search space. To validate our
approach, we introduce a benchmark with both structured and unstructured tasks.
Experimental results show that SherlockLLM is a robust and efficient solution.
On the structured tasks, its performance matches strong baselines and
approaches the theoretical optimal defined by binary search. On the challenging
unstructured task, our agent significantly outperforms these baselines,
showcasing its ability to learn a highly effective information-seeking dialogue
policy.

</details>


### [51] [Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation](https://arxiv.org/abs/2510.18751)
*Patterson Hsieh,Jerry Yeh,Mao-Chi He,Wen-Han Hsieh,Elvis Hsieh*

Main category: cs.AI

TL;DR: ALGOS是一个用于有害藻华监测的分割与推理系统，结合遥感图像理解和严重程度估计，通过GeoSAM辅助人工评估和微调视觉语言模型，在分割和严重程度估计方面均表现出色。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了有害藻华的发生，特别是蓝藻，威胁水生生态系统和人类健康。传统监测方法劳动密集且覆盖范围有限，需要开发可扩展的AI驱动解决方案。

Method: 结合遥感图像理解和严重程度估计，集成GeoSAM辅助人工评估进行高质量分割掩码整理，并使用NASA的CAML数据集微调视觉语言模型进行严重程度预测。

Result: 实验表明ALGOS在分割和严重程度级别估计方面均实现了稳健性能。

Conclusion: 该系统为实现实用和自动化的蓝藻监测系统铺平了道路。

Abstract: Climate change is intensifying the occurrence of harmful algal bloom (HAB),
particularly cyanobacteria, which threaten aquatic ecosystems and human health
through oxygen depletion, toxin release, and disruption of marine biodiversity.
Traditional monitoring approaches, such as manual water sampling, remain
labor-intensive and limited in spatial and temporal coverage. Recent advances
in vision-language models (VLMs) for remote sensing have shown potential for
scalable AI-driven solutions, yet challenges remain in reasoning over imagery
and quantifying bloom severity. In this work, we introduce ALGae Observation
and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB
monitoring that combines remote sensing image understanding with severity
estimation. Our approach integrates GeoSAM-assisted human evaluation for
high-quality segmentation mask curation and fine-tunes vision language model on
severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)
from NASA. Experiments demonstrate that ALGOS achieves robust performance on
both segmentation and severity-level estimation, paving the way toward
practical and automated cyanobacterial monitoring systems.

</details>


### [52] [Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location](https://arxiv.org/abs/2510.18803)
*Shirin Tavakoli Kafiabad,Andrea Schiffauerova,Ashkan Ebadi*

Main category: cs.AI

TL;DR: 该研究比较了LDA、STM和BERTopic三种主题建模方法在分析加拿大NSERC研究提案中的应用，并开发了COFFEE算法来增强BERTopic的协变量分析能力，发现BERTopic在识别细粒度主题方面表现最佳，揭示了省级研究专业化和性别相关主题模式。


<details>
  <summary>Details</summary>
Motivation: 优化国家科学投资需要了解研究趋势演变及人口地理因素的影响，特别是在促进公平、多样性和包容性的背景下。

Method: 分析了2005-2022年NSERC资助的研究提案，比较了LDA、STM和BERTopic三种主题建模方法，并开发了COFFEE算法用于BERTopic的协变量效应估计。

Result: BERTopic在识别细粒度、连贯性和新兴主题方面表现最优，如人工智能的快速发展；协变量分析揭示了省级研究专业化和跨学科一致的性别相关主题模式。

Conclusion: 这些发现为资助机构制定更公平、更有影响力的资助策略提供了实证基础，有助于提升科学生态系统的有效性。

Abstract: Optimizing national scientific investment requires a clear understanding of
evolving research trends and the demographic and geographical forces shaping
them, particularly in light of commitments to equity, diversity, and inclusion.
This study addresses this need by analyzing 18 years (2005-2022) of research
proposals funded by the Natural Sciences and Engineering Research Council of
Canada (NSERC). We conducted a comprehensive comparative evaluation of three
topic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic
Modelling (STM), and BERTopic. We also introduced a novel algorithm, named
COFFEE, designed to enable robust covariate effect estimation for BERTopic.
This advancement addresses a significant gap, as BERTopic lacks a native
function for covariate analysis, unlike the probabilistic STM. Our findings
highlight that while all models effectively delineate core scientific domains,
BERTopic outperformed by consistently identifying more granular, coherent, and
emergent themes, such as the rapid expansion of artificial intelligence.
Additionally, the covariate analysis, powered by COFFEE, confirmed distinct
provincial research specializations and revealed consistent gender-based
thematic patterns across various scientific disciplines. These insights offer a
robust empirical foundation for funding organizations to formulate more
equitable and impactful funding strategies, thereby enhancing the effectiveness
of the scientific ecosystem.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [53] [Information Capacity of EEG: Theoretical and Computational Limits of Recoverable Neural Information](https://arxiv.org/abs/2510.17841)
*Ishir Rao*

Main category: cs.IT

TL;DR: EEG的信息容量有限，每个样本仅能传递数十比特关于低维神经活动的信息，信息量在64-128个电极时达到饱和，且与信噪比呈对数关系增长。


<details>
  <summary>Details</summary>
Motivation: 研究EEG的定量信息容量，了解脑电图在记录大脑动态时的信息传递能力限制。

Method: 结合信息论和合成前向建模，使用高斯信道理论和经验模拟来估计皮层源与EEG记录之间的互信息。

Result: 线性解码器几乎能捕获所有可线性恢复的方差，但恢复的互信息远低于分析信道容量，表明测量物理是主要限制因素。

Conclusion: 这些结果揭示了从EEG推断大脑状态或思维内容结构的内在上限，测量物理而非算法复杂性是主要限制。

Abstract: Electroencephalography (EEG) is widely used to study human brain dynamics,
yet its quantitative information capacity remains unclear. Here, we combine
information theory and synthetic forward modeling to estimate the mutual
information between latent cortical sources and EEG recordings. Using
Gaussian-channel theory and empirical simulations, we find that scalp EEG
conveys only tens of bits per sample about low-dimensional neural activity.
Information saturates with approximately 64-128 electrodes and scales
logarithmically with signal-to-noise ratio (SNR). Linear decoders capture
nearly all variance that is linearly recoverable, but the mutual information
they recover remains far below the analytic channel capacity, indicating that
measurement physics - not algorithmic complexity - is the dominant limitation.
These results outline the intrinsic ceiling on how much structure about brain
state or thought content can be inferred from EEG.

</details>


### [54] [Performance of Modified Fractional Frequency Reuse Algorithm in Random Ultra Dense Networks](https://arxiv.org/abs/2510.18440)
*Bach Hung Luu,Samuel Harry Gardner,Sinh Cong Lam,Trong Minh Hoang*

Main category: cs.IT

TL;DR: 提出一种改进的分数频率复用算法，使用服务基站与第二最近基站的信号强度功率比来分类小区边缘用户和中心用户，通过提高边缘用户传输功率来改善其性能。


<details>
  <summary>Details</summary>
Motivation: 在5G及超5G高密度基站网络中，传统基于SINR或距离的用户分类方法存在局限性，需要更有效的干扰管理方法来提升用户性能。

Method: 使用服务基站与第二最近基站的信号强度功率比作为用户分类标准，当功率比低于预设阈值时，将用户分类为小区边缘用户并分配更高传输功率。

Result: 仿真结果显示增加传输功率能有效提升边缘用户性能，但会降低典型用户性能；在障碍物密集环境中，频率复用算法能有效抑制小区间干扰。

Conclusion: 基于功率比的用户分类方法在高密度障碍物环境中特别有效，但需要在边缘用户性能提升和典型用户性能损失之间取得平衡。

Abstract: Mitigating intercell interference by employing fractional frequency reuse
algorithms is one of the important approaches to improving user performance in
5G and Beyond 5G cellular network systems, which typically have a high density
of Base Stations (BSs). While most frequency reuse algorithms are based on the
downlink Signal-to-Interference-plus-Noise Ratio (SINR) or the distance between
the user and its serving BS to classify Cell-Edge Users (CEUs) and Cell-Center
Users (CCUs), this paper discusses a modified algorithm that uses the power
ratio between the signal strengths from the serving BS and the second nearest
BS for user classification. Specifically, if the power ratio is below a
predefined threshold, the user is classified as a CEU and is served with higher
transmission power. Simulation results show that increasing transmission power
is necessary to enhance CEU performance, but it also degrades the performance
of typical users. The use of frequency reuse algorithms is particularly
feasible in environments with a high density of obstacles, where intercell
interference can be effectively suppressed.

</details>


### [55] [A Markov-Chain Characterization of Finite-State Dimension and a Generalization of Agafonov's Theorem](https://arxiv.org/abs/2510.18736)
*Laurent Bienvenu,Hugo Gimbert,Subin Pulari*

Main category: cs.IT

TL;DR: 本文扩展了有限状态维度的信息论特征，将其与马尔可夫链模拟中的Kullback-Leibler散度联系起来，并推广了Agafonov定理到任意序列。


<details>
  <summary>Details</summary>
Motivation: 有限状态维度量化了有限自动机感知的无限序列信息率，但现有研究主要关注Borel正规数。本文旨在将Schnorr-Stimm定理和Agafonov定理推广到更一般的序列。

Method: 通过马尔可夫链模拟序列，分析条件Kullback-Leibler散度与有限状态维度的关系，建立新的信息论特征。

Result: 证明了序列的有限状态维度可以通过马尔可夫链模拟中的条件KL散度来刻画，并建立了序列与其自动子序列有限状态维度之间的定量关系。

Conclusion: 提供了有限状态维度的新信息论特征，推广了Schnorr-Stimm和Agafonov定理，为理解序列的信息内容提供了更一般的框架。

Abstract: Finite-state dimension quantifies the asymptotic rate of information in an
infinite sequence as perceived by finite automata. For a fixed alphabet, the
infinite sequences that have maximal finite-state dimension are exactly those
that are Borel normal, i.e., in which all words of any given length appear with
the same frequency. A theorem of Schnorr and Stimm (1972) shows that a real
number is Borel normal if and only if, for every finite-state irreducible
Markov chain with fair transitions, when the chain is simulated using the
binary expansion of the given number, the empirical distribution of states
converges to its stationary distribution. In this paper we extend this
correspondence beyond normal numbers. We show that the finite-state dimension
of a sequence can be characterized in terms of the conditional Kullback-Leibler
divergence between the limiting distributions arising from the simulation of
Markov chains using the given sequence and their stationary distributions. This
provides a new information-theoretic characterization of finite-state dimension
which generalizes the Schnorr-Stimm result.
  As an application, we prove a generalization of Agafonov's theorem for normal
numbers. Agafonov's theorem states that a sequence is normal if and only if
every subsequence selected by a finite automaton is also normal. We extend this
to arbitrary sequences by establishing a tight quantitative relationship
between the finite-state dimension of a sequence and the finite-state
dimensions of its automatic subsequences.

</details>
