<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [UTM Performance Under Stressing Scenarios](https://arxiv.org/abs/2509.08124)
*Ian Jessen*

Main category: cs.NI

TL;DR: ANAMLL虚拟系统集成实验室用于测试无人机交通管理系统在高压条件下的性能，发现了系统在需求高峰时无法及时完成飞行重规划的问题


<details>
  <summary>Details</summary>
Motivation: 随着无人机和先进空中交通的发展，需要开发新的空域管理解决方案，但真实环境测试受限，需要建模和仿真环境来研究系统在高压条件下的行为

Method: 开发了ANAMLL虚拟系统集成实验室，作为联邦自治网络的测试平台，通过代表性UTM网络在高压需求场景下的性能测试进行分析

Result: ANAMLL展示了UTM系统在需求高峰时无法在规定时间窗口内完成飞行重规划的临界点，并证明了网络连接性能对最终用户空域访问的影响

Conclusion: ANAMLL仿真环境能够有效识别UTM系统在高压条件下的性能瓶颈，为空域管理系统的测试和验证提供了重要工具

Abstract: Proliferation of new classes of airspace participants, including uncrewed and
advanced aerial mobility vehicles, necessitates the development and deployment
of novel airspace management solutions, such as the Unmanned Traffic Management
(UTM) system and the Provider of Services to UAM (PSU) Network. The efficacy of
such systems has been demonstrated on multiple occasions via real-world
deployments in limited test environments, however exploration of system
behavior under stressing conditions requires the development of appropriate
modeling and simulation (M&S) environments. Autonomy Networks for Advanced
Mobility at Lincoln Laboratory (ANAMLL) is a virtual Systems Integration
Laboratory (SIL) designed to host federated autonomy networks, such as a UTM or
PSU Network, and to enable test and validation at scales not available in
real-world deployments. As an example of ANAMLL's utility, we explore the
performance of a representative UTM network during a stressing demand scenario.
In a close examination of the demand scenario, ANAMLL demonstrates a UTM system
demand point at which in-flight replanning can no longer be accomplished within
an allowable time window. In a second analysis of the same scenario, ANAMLL
demonstrates the impact of network connectivity performance on end-user
airspace access.

</details>


### [2] [Matisse: Visualizing Measured Internet Latencies as Manifolds](https://arxiv.org/abs/2509.08097)
*Stephen Jasina,Loqman Salamatian,Joshua Mathews,Scott Anderson,Paul Barford,Mark Crovella,Walter Willinger*

Main category: cs.NI

TL;DR: 这篇论文提出了一种新的流形可视化方法Matisse，通过网络延迟测量数据推断出流形并在地理地图上可视化，用于呈现网络连接性特征和异常检测。


<details>
  <summary>Details</summary>
Motivation: 流形是复杂的拓扑空间，可用于表示实际测量数据。可视化流形能够展示其拓扑特征（如曲率）并提供对基础数据重要属性的洞察（如测量中的异常）。

Method: 提出了一种新方法论，利用一系列图来捕捉数据中的关键信息，包括明确的位置（顶点）和Ricci曲率信息（边）。可视化方法生成曲面（流形），保持顶点的地理位置，并使图边的Ricci曲率值决定流形的曲率特性。

Result: 生成的流形强调了关键连接性区域，定义了一种"互联网延迟空间"，其中延迟测量表现为洞线。开发了科学可视化工具Matisse，用于生成、可视化和操作投影到基础地图上的流形。

Conclusion: 该方法能够有效可视化网络延迟数据的流形特征，通过美国公共互联网的案例研究证明了Matisse工具的实用性，为网络性能分析和异常检测提供了新的可视化手段。

Abstract: Manifolds are complex topological spaces that can be used to represent
datasets of real-world measurements. Visualizing such manifolds can help with
illustrating their topological characteristics (e.g., curvature) and providing
insights into important properties of the underlying data (e.g., anomalies in
the measurements). In this paper, we describe a new methodology and system for
generating and visualizing manifolds that are inferred from actual Internet
latency measurements between different cities and are projected over a 2D
Euclidean space (e.g., a geographic map). Our method leverages a series of
graphs that capture critical information contained in the data, including
well-defined locations (for vertices) and Ricci curvature information (for
edges). Our visualization approach then generates a curved surface (manifold)
in which (a) geographical locations of vertices are maintained and (b) the
Ricci curvature values of the graph edges determine the curvature properties of
the manifold. The resulting manifold highlights areas of critical connectivity
and defines an instance of "Internet delay space" where latency measurements
manifest as geodesics. We describe details of our method and its implementation
in a tool, which we call Matisse, for generating, visualizing and manipulating
manifolds projected onto a base map. We illustrate Matisse with two case
studies: a simple example to demonstrate key concepts, and visualizations of
the US public Internet to show Matisse's utility.

</details>


### [3] [SKYLINK: Scalable and Resilient Link Management in LEO Satellite Network](https://arxiv.org/abs/2509.08455)
*Wanja de Sombre,Arash Asadi,Debopam Bhattacherjee,Deepak Vasisht,Andrea Ortiz*

Main category: cs.NI

TL;DR: SKYLINK是一种完全分布式学习策略，用于LEO卫星网络的链路管理，显著降低延迟和丢包率，同时保持计算复杂度恒定


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络的高移动性、动态流量模式和潜在链路故障对高效弹性路由提出了重大挑战，需要解决传统方法在可扩展性和实时响应性方面的不足

Method: 将LEO卫星网络建模为时变图，提出SKYLINK分布式学习策略，使每个卫星能够独立实时决策流量分发，适应时变网络条件

Result: 相比弯管方法降低29%的加权延迟和丢包率，相比Dijkstra降低92%；丢包率相比k最短路径降低95%，相比Dijkstra降低99%，相比弯管基线降低74%；吞吐量提升高达46%

Conclusion: SKYLINK在保证实时响应性、可扩展性和网络弹性的同时，显著提升了LEO卫星网络的性能，计算复杂度与星座规模保持恒定

Abstract: The rapid growth of space-based services has established LEO satellite
networks as a promising option for global broadband connectivity.
Next-generation LEO networks leverage inter-satellite links (ISLs) to provide
faster and more reliable communications compared to traditional bent-pipe
architectures, even in remote regions. However, the high mobility of
satellites, dynamic traffic patterns, and potential link failures pose
significant challenges for efficient and resilient routing. To address these
challenges, we model the LEO satellite network as a time-varying graph
comprising a constellation of satellites and ground stations. Our objective is
to minimize a weighted sum of average delay and packet drop rate. Each
satellite independently decides how to distribute its incoming traffic to
neighboring nodes in real time. Given the infeasibility of finding optimal
solutions at scale, due to the exponential growth of routing options and
uncertainties in link capacities, we propose SKYLINK, a novel fully distributed
learning strategy for link management in LEO satellite networks. SKYLINK
enables each satellite to adapt to the time-varying network conditions,
ensuring real-time responsiveness, scalability to millions of users, and
resilience to network failures, while maintaining low communication overhead
and computational complexity. To support the evaluation of SKYLINK at global
scale, we develop a new simulator for large-scale LEO satellite networks. For
25.4 million users, SKYLINK reduces the weighted sum of average delay and drop
rate by 29% compared to the bent-pipe approach, and by 92% compared to
Dijkstra. It lowers drop rates by 95% relative to k-shortest paths, 99%
relative to Dijkstra, and 74% compared to the bent-pipe baseline, while
achieving up to 46% higher throughput. At the same time, SKYLINK maintains
constant computational complexity with respect to constellation size.

</details>


### [4] [Enhancing 6G Network Security and Incident Response through Integrated VNF and SDN Technologies](https://arxiv.org/abs/2509.08274)
*Abdul Razaque,Abitkhanova Zhadyra Abitkhanovna*

Main category: cs.NI

TL;DR: 本文提出了一种结合VNF和SDN技术的VNFSDN架构，用于解决低速互联网对安全事件响应的负面影响，提升6G网络的安全防护能力。


<details>
  <summary>Details</summary>
Motivation: 低速互联网会降低团队协作效率、延迟威胁检测、影响响应行动效率并增加安全风险。6G网络产生的大量数据需要实时处理，传统网络架构难以满足安全需求。

Method: 通过集成虚拟网络功能(VNF)和软件定义网络(SDN)技术，构建虚拟网络功能服务交付网络(VNFSDN)，实现动态安全服务适配和实时数据分析。

Result: VNFSDN能够动态适应变化的安全需求和网络条件，提高网络弹性，实现主动威胁检测，显著减少安全漏洞风险。

Conclusion: VNFSDN架构有效解决了低速互联网环境下的安全响应问题，结合AI/ML技术可进一步提升网络安全和威胁检测能力，为6G网络提供强有力的安全保障。

Abstract: Low-speed internet can negatively affect incident response in a number of
ways, including decreased teamwork, delayed detection, inefficient action, and
elevated risk. Delayed data acquisition and processing may result from
inadequate internet connectivity, hindering security teams' ability to obtain
the necessary information for timely and effective responses. Each of these
factors may augment the organization's susceptibility to security incidents and
their subsequent ramifications. This article establishes a virtual network
function service delivery network (VNFSDN) through the integration of virtual
network function (VNF) and software-defined networking (SDN) technologies. The
VNFSDN approach enhances network security effectiveness and efficiency while
reducing the danger of breaches. This method assists security services in
rapidly assessing vast quantities of data generated by 6G networks. VNFSDN
adapts dynamically to changing safety requirements and connection conditions
through the use of SDN and VNF. This flexibility enables enterprises to
mitigate or halt the impact of cyberattacks by swiftly identifying and
addressing security threats. The VNFSDN enhances network resilience, allowing
operators to proactively mitigate possible security attacks and minimize
downtime. The incorporation of machine learning and artificial intelligence
into VNFSDN can significantly improve network security and threat detection
capabilities. The VNFSDN integrates VNF and SDN technologies to deliver
security services that analyze vast quantities of 6G data in real time. As
security requirements and network conditions evolve, it adapts dynamically to
enhance network resilience and facilitate proactive threat detection.

</details>


### [5] [Ubiquitous Intelligence Via Wireless Network-Driven LLMs Evolution](https://arxiv.org/abs/2509.08400)
*Xingkun Yin,Feiran You,Hongyang Du,Kaibin Huang*

Main category: cs.NI

TL;DR: 提出泛在智能新范式，通过无线网络与LLM的协同进化实现可扩展的持续智能提升


<details>
  <summary>Details</summary>
Motivation: 传统静态模型部署无法满足持续智能演进需求，需要探索网络与AI模型协同发展的新范式

Method: 构建无线网络驱动的生态系统，支持系统编排的终身学习，同时利用LLM驱动下一代自适应网络开发

Result: 实现了网络与LLM的协同进化，形成自我改进系统，在多样化资源受限环境中持续提升能力

Conclusion: 泛在智能范式代表了向自我改进系统的转变，为在复杂环境中实现持续智能演进提供了可行路径

Abstract: We introduce ubiquitous intelligence as a paradigm where Large Language
Models (LLMs) evolve within wireless network-driven ecosystems. Unlike static
model deployments, this approach enables scalable and continuous intelligence
ascension through coordination between networks and LLMs. Wireless networks
support system-orchestrated lifelong learning, while LLMs drive the
next-generation network development that is more adaptive and responsive. This
co-evolution highlights a shift toward self-improving systems, sustaining
capability growth across diverse and resource-constrained environments.

</details>


### [6] [Design and Development of a Scalable and Energy-Efficient Localization Framework Leveraging LoRa Ranging-Capable Transceivers](https://arxiv.org/abs/2509.08488)
*Hasan Albinsaid,Bodhibrata Mukhopadhyay,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 基于Semtech SX1280 LoRa透传器的能源效率十分高的半实时定位协调框架，通过同步唤醒窗口和深度睡眠策略，在保持5米定位精度的同时将节点续航时间扩长至9个月


<details>
  <summary>Details</summary>
Motivation: 解决LoRa SX1280透传器在IoT定位中的能源消耗问题，虽然该芯片具有低功耗和精确定位能力，但缺乏有效的系统级协调框架来管理睡眠-唤醒协调和角色分配

Method: 设计了一种协调框架，通过等待初始器和响应者之间短暂的同步唤醒窗口来安排，使设备在大部分工作周期中保持深度睡眠模式，减少对精确连续时间的依赖并减少低成本振荡器的偏移

Result: 实验结果显示，该方案允许节点保持超低功耗模式，周期性唤醒检查指令，在单个硬币电池下可以在备用模式下续航高达9个月，并能够按需进行近实时的定位操作，定位精度保持在5米以内

Conclusion: 该协调框架成功地在保持Semtech SX1280 LoRa透传器内置精确定位能力的同时，显著降低了功耗，为大规模IoT部署提供了一种能源效率高、成本效益好的实时定位解决方案

Abstract: Precise and energy-efficient localization is a critical requirement in many
Internet of Things (IoT) applications, particularly in large-scale deployments
such as asset tagging, agriculture, and smart cities, where long battery life
and cost-effectiveness are crucial. The Semtech SX1280 LoRa transceiver
presents a promising solution for IoT localization. It combines low cost, low
power, and precise ranging capability over distances of up to 1 km. However,
the ranging process requires two devices to be simultaneously active, one
initiating the ranging request and the other responding to it, which can lead
to significant energy expenditure if not properly managed. Despite the
transceiver's excellent performance, no existing system-level framework
effectively manages sleep-wake coordination and role assignment needed for
energy-efficient operation. This paper presents a coordination framework that
significantly reduces power consumption while maintaining the inherent precise
ranging capability of the chip. The framework schedules short, synchronized
wake-up windows between the initiator and the responder, allowing devices to
remain in deep sleep for most of their duty cycle. This scheduling strategy
minimizes reliance on precise continuous timing and mitigates drift in low-cost
oscillators. To validate the framework, we designed and developed custom nodes
that are compliant with the framework's protocol. Experimental results show
that the proposed approach allows a node to stay in ultra-low power mode and
wake periodically to check for instructions. The node can remain in standby
mode for up to nine months on a single coin cell battery and can perform
ranging operations on demand in near real-time, all while maintaining a
localization accuracy within five meters.

</details>


### [7] [The Role of Legacy Mobile Networks in Infrastructure Resilience: Evidence from the Southern Brazil Flood](https://arxiv.org/abs/2509.08595)
*Daniel Meyer,Lisandro Z Granville,Leandro M. Bertholdo*

Main category: cs.NI

TL;DR: 本文分析了2024年5月巴西南里奥格兰德州洪水期间移动通信网络的韧性，发现现代网络(4G/5G)在灾害中脆弱，而传统技术(2G/3G)在维持基本连接方面发挥关键作用。


<details>
  <summary>Details</summary>
Motivation: 研究极端洪水事件中移动通信网络的恢复能力和脆弱性，为未来灾害应对提供基础设施规划参考。

Method: 基于监管数据和运营商技术洞察，分析网络中断的主要原因，重点关注洪水和长时间停电的影响。

Result: 现代4G/5G网络在灾害中表现出显著脆弱性，而传统2G/3G技术在恶劣条件下对维持基本连接至关重要。

Conclusion: 需要制定灾害感知的基础设施规划，考虑传统系统的持续重要性、多样化供电策略和弹性网络设计，以增强未来危机中的服务连续性。

Abstract: This paper investigates the resilience of mobile communication networks
during the extreme flooding that affected Rio Grande do Sul, Brazil, in May
2024. Based on regulatory data and technical insights from operators, the study
identifies the leading causes of mobile network disruptions, primarily related
to flooding and prolonged power outages. The results reveal the significant
vulnerability of modern networks (4G/5G) during the event and the essential
role played by legacy technologies (2G/3G) in sustaining basic connectivity
under adverse conditions. The findings underscore the necessity of
disaster-aware infrastructure planning, taking into account the ongoing
significance of legacy systems, diversified power supply strategies, and
resilient network designs to enhance service continuity during future crises.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Learning-Based Planning for Improving Science Return of Earth Observation Satellites](https://arxiv.org/abs/2509.07997)
*Abigail Breitfeld,Alberto Candela,Juan Delfa,Akseli Kangaslahti,Itai Zilberstein,Steve Chien,David Wettergreen*

Main category: cs.AI

TL;DR: 这篇论文提出了两种基于学习的动态目标指向方法（强化学习和模仿学习），用于优化地球观测卫星的数据采集效率。


<details>
  <summary>Details</summary>
Motivation: 地球观测卫星存在轨道偏移难、传感器视野有限、指向操作资源消耗大等限制，需要优化数据采集效率。动态目标指向技术可以利用卫星资源和预光仪器数据来智能地重新配置主仪器。

Method: 提出两种学习方法：强化学习和模仿学习。这些方法基于动态规划解决方案来规划采样位置序列。

Result: 学习方法在动态目标指向任务中表现优异：模仿学习比最佳假设方法平均提高10.0%，强化学习平均提高13.7%。同时证明了两种方法都可以使用较少数据有效训练。

Conclusion: 学习方法在动态目标指向任务中显示出优势，能够提高地球观测卫星的科学数据采集效率，且训练效率高。

Abstract: Earth observing satellites are powerful tools for collecting scientific
information about our planet, however they have limitations: they cannot easily
deviate from their orbital trajectories, their sensors have a limited field of
view, and pointing and operating these sensors can take a large amount of the
spacecraft's resources. It is important for these satellites to optimize the
data they collect and include only the most important or informative
measurements. Dynamic targeting is an emerging concept in which satellite
resources and data from a lookahead instrument are used to intelligently
reconfigure and point a primary instrument. Simulation studies have shown that
dynamic targeting increases the amount of scientific information gathered
versus conventional sampling strategies. In this work, we present two different
learning-based approaches to dynamic targeting, using reinforcement and
imitation learning, respectively. These learning methods build on a dynamic
programming solution to plan a sequence of sampling locations. We evaluate our
approaches against existing heuristic methods for dynamic targeting, showing
the benefits of using learning for this application. Imitation learning
performs on average 10.0\% better than the best heuristic method, while
reinforcement learning performs on average 13.7\% better. We also show that
both learning methods can be trained effectively with relatively small amounts
of data.

</details>


### [9] [EnvX: Agentize Everything with Agentic AI](https://arxiv.org/abs/2509.08088)
*Linyao Chen,Zimian Peng,Yingxuan Yang,Yikun Wang,Wenzheng Tom Tang,Hiroki H. Kobayashi,Weinan Zhang*

Main category: cs.AI

TL;DR: EnvX是一个利用Agentic AI将GitHub仓库转化为智能代理的框架，通过自然语言交互和代理间协作实现软件组件的自动化重用，在GitTaskBench基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开源软件仓库数量庞大但重用过程仍需要人工操作，存在文档查阅、API理解、集成代码编写等障碍，导致软件重用效率低下。

Method: EnvX采用三阶段方法：1)TODO引导的环境初始化；2)人类对齐的代理自动化；3)代理间(A2A)协作协议，将仓库转化为能够自主执行任务的智能代理。

Result: 在GitTaskBench基准测试中，EnvX实现了74.07%的执行完成率和51.85%的任务通过率，优于现有框架，并能支持多仓库协作。

Conclusion: EnvX将仓库从被动的代码资源转变为智能交互代理，显著提升了开源软件的可访问性和协作效率，代表了软件重用范式的重要转变。

Abstract: The widespread availability of open-source repositories has led to a vast
collection of reusable software components, yet their utilization remains
manual, error-prone, and disconnected. Developers must navigate documentation,
understand APIs, and write integration code, creating significant barriers to
efficient software reuse. To address this, we present EnvX, a framework that
leverages Agentic AI to agentize GitHub repositories, transforming them into
intelligent, autonomous agents capable of natural language interaction and
inter-agent collaboration. Unlike existing approaches that treat repositories
as static code resources, EnvX reimagines them as active agents through a
three-phase process: (1) TODO-guided environment initialization, which sets up
the necessary dependencies, data, and validation datasets; (2) human-aligned
agentic automation, allowing repository-specific agents to autonomously perform
real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple
agents to collaborate. By combining large language model capabilities with
structured tool integration, EnvX automates not just code generation, but the
entire process of understanding, initializing, and operationalizing repository
functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18
repositories across domains such as image processing, speech recognition,
document analysis, and video manipulation. Our results show that EnvX achieves
a 74.07% execution completion rate and 51.85% task pass rate, outperforming
existing frameworks. Case studies further demonstrate EnvX's ability to enable
multi-repository collaboration via the A2A protocol. This work marks a shift
from treating repositories as passive code resources to intelligent,
interactive agents, fostering greater accessibility and collaboration within
the open-source ecosystem.

</details>


### [10] [Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI](https://arxiv.org/abs/2509.08151)
*Botao Zhu,Jeslyn Wang,Dusit Niyato,Xianbin Wang*

Main category: cs.AI

TL;DR: 提出基于大AI模型的师生代理架构2TSD模型，通过服务器端教师代理提取任务特定信任语义并传输给设备端学生代理，实现快速准确的协作设备选择


<details>
  <summary>Details</summary>
Motivation: 传统方法中每个任务所有者独立评估所有协作设备的可信度会导致频繁数据交换、复杂推理和动态情境变化，造成显著开销和信任评估质量下降

Method: 基于大AI模型的师生代理架构，教师代理部署在服务器上进行多维信任数据收集、任务特定信任语义提取和任务-协作设备匹配分析，学生代理接收信任语义实现快速选择

Result: 实验结果表明2TSD模型能减少协作设备评估时间、降低设备资源消耗，并提高协作设备选择的准确性

Conclusion: 2TSD模型通过任务特定信任语义蒸馏有效解决了协作设备信任评估中的开销和准确性问题，为复杂计算任务的协作执行提供了高效解决方案

Abstract: Accurate trustworthiness evaluation of potential collaborating devices is
essential for the effective execution of complex computing tasks. This
evaluation process involves collecting diverse trust-related data from
potential collaborators, including historical performance and available
resources, for collaborator selection. However, when each task owner
independently assesses all collaborators' trustworthiness, frequent data
exchange, complex reasoning, and dynamic situation changes can result in
significant overhead and deteriorated trust evaluation. To overcome these
challenges, we propose a task-specific trust semantics distillation (2TSD)
model based on a large AI model (LAM)-driven teacher-student agent
architecture. The teacher agent is deployed on a server with powerful
computational capabilities and an augmented memory module dedicated to
multidimensional trust-related data collection, task-specific trust semantics
extraction, and task-collaborator matching analysis. Upon receiving
task-specific requests from device-side student agents, the teacher agent
transfers the trust semantics of potential collaborators to the student agents,
enabling rapid and accurate collaborator selection. Experimental results
demonstrate that the proposed 2TSD model can reduce collaborator evaluation
time, decrease device resource consumption, and improve the accuracy of
collaborator selection.

</details>


### [11] [Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following](https://arxiv.org/abs/2509.08222)
*Minjong Yoo,Jinwoo Jang,Wei-jin Park,Honguk Woo*

Main category: cs.AI

TL;DR: ExRAP框架通过探索增强检索和规划，提升LLM在动态环境中的持续指令跟随能力，结合信息探索和时序一致性优化，在多个基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 解决具身智能体在动态非平稳环境中持续执行指令的挑战，增强LLM的环境推理能力，将任务规划与变化的环境上下文有效结合

Method: 提出探索增强检索规划框架，包括：1）环境探索建立上下文记忆；2）信息探索集成到LLM规划过程；3）记忆增强查询评估；4）时序一致性精化处理知识衰减

Result: 在VirtualHome、ALFRED和CARLA等基准测试中，相比其他最先进的LLM任务规划方法，在目标成功率和执行效率方面均表现更优，对各种指令规模、类型和非平稳度具有鲁棒性

Conclusion: ExRAP框架通过有效平衡环境探索和记忆有效性，显著提升了具身智能体在动态环境中的持续任务执行能力，为LLM在具身智能领域的应用提供了有效解决方案

Abstract: This study presents an Exploratory Retrieval-Augmented Planning (ExRAP)
framework, designed to tackle continual instruction following tasks of embodied
agents in dynamic, non-stationary environments. The framework enhances Large
Language Models' (LLMs) embodied reasoning capabilities by efficiently
exploring the physical environment and establishing the environmental context
memory, thereby effectively grounding the task planning process in time-varying
environment contexts. In ExRAP, given multiple continual instruction following
tasks, each instruction is decomposed into queries on the environmental context
memory and task executions conditioned on the query results. To efficiently
handle these multiple tasks that are performed continuously and simultaneously,
we implement an exploration-integrated task planning scheme by incorporating
the {information-based exploration} into the LLM-based planning process.
Combined with memory-augmented query evaluation, this integrated scheme not
only allows for a better balance between the validity of the environmental
context memory and the load of environment exploration, but also improves
overall task performance. Furthermore, we devise a {temporal consistency
refinement} scheme for query evaluation to address the inherent decay of
knowledge in the memory. Through experiments with VirtualHome, ALFRED, and
CARLA, our approach demonstrates robustness against a variety of embodied
instruction following scenarios involving different instruction scales and
types, and non-stationarity degrees, and it consistently outperforms other
state-of-the-art LLM-based task planning approaches in terms of both goal
success rate and execution efficiency.

</details>


### [12] [Real-world Music Plagiarism Detection With Music Segment Transcription System](https://arxiv.org/abs/2509.08282)
*Seonghyeon Go*

Main category: cs.AI

TL;DR: 提出了一种结合多种音乐信息检索技术的音乐抄袭检测系统，通过提取音乐片段并计算多特征相似度来检测不同格式音乐间的抄袭


<details>
  <summary>Details</summary>
Motivation: 随着音乐信息检索技术的发展，音乐生成和分发变得更加多样化，为了保护音乐知识产权和版权，需要有效的音乐抄袭检测方法

Method: 开发了音乐片段转录系统，从音频录音中提取有音乐意义的片段，基于多个音乐特征计算相似度得分，并进行全面的音乐分析

Result: 在音乐抄袭检测实验中取得了有希望的结果，该方法可应用于实际音乐场景，并收集了公开的相似音乐对数据集

Conclusion: 提出的系统能够有效检测音乐抄袭，为音乐知识产权保护提供了实用的技术解决方案

Abstract: As a result of continuous advances in Music Information Retrieval (MIR)
technology, generating and distributing music has become more diverse and
accessible. In this context, interest in music intellectual property protection
is increasing to safeguard individual music copyrights. In this work, we
propose a system for detecting music plagiarism by combining various MIR
technologies. We developed a music segment transcription system that extracts
musically meaningful segments from audio recordings to detect plagiarism across
different musical formats. With this system, we compute similarity scores based
on multiple musical features that can be evaluated through comprehensive
musical analysis. Our approach demonstrated promising results in music
plagiarism detection experiments, and the proposed method can be applied to
real-world music scenarios. We also collected a Similar Music Pair (SMP)
dataset for musical similarity research using real-world cases. The dataset are
publicly available.

</details>


### [13] [Leveraging AI Agents for Autonomous Networks: A Reference Architecture and Empirical Studies](https://arxiv.org/abs/2509.08312)
*Binghan Wu,Shoufeng Wang,Yunxin Liu,Ya-Qin Zhang,Joseph Sifakis,Ye Ouyang*

Main category: cs.AI

TL;DR: 通过实现Sifakis自主网络代理架构，在5G RAN中实现了次毫秒级实时控制，下行速率提升6%，误码率降低67%


<details>
  <summary>Details</summary>
Motivation: 解决传统自动化网络向L4级自主网络转型的问题，实现真正的认知能力，达到自配置、自恢复、自优化的目标

Method: 采用Joseph Sifakis的AN Agent参考架构，部署协调的预防性-反应性运行时，通过混合知识表示驱动，并在RAN链路适配代理中进行实验验证

Result: 在5G NR sub-6 GHz中实现了次10毫秒实时控制，下行速率比OLLA算法提高6%，超高可靠服务的误码率降低67%

Conclusion: 该架构有效克服了传统自主性障碍，推进了L4级关键能力的发展，为下一代自主网络目标奠定了基础

Abstract: The evolution toward Level 4 (L4) Autonomous Networks (AN) represents a
strategic inflection point in telecommunications, where networks must transcend
reactive automation to achieve genuine cognitive capabilities--fulfilling TM
Forum's vision of self-configuring, self-healing, and self-optimizing systems
that deliver zero-wait, zero-touch, and zero-fault services. This work bridges
the gap between architectural theory and operational reality by implementing
Joseph Sifakis's AN Agent reference architecture in a functional cognitive
system, deploying coordinated proactive-reactive runtimes driven by hybrid
knowledge representation. Through an empirical case study of a Radio Access
Network (RAN) Link Adaptation (LA) Agent, we validate this framework's
transformative potential: demonstrating sub-10 ms real-time control in 5G NR
sub-6 GHz while achieving 6% higher downlink throughput than Outer Loop Link
Adaptation (OLLA) algorithms and 67% Block Error Rate (BLER) reduction for
ultra-reliable services through dynamic Modulation and Coding Scheme (MCS)
optimization. These improvements confirm the architecture's viability in
overcoming traditional autonomy barriers and advancing critical L4-enabling
capabilities toward next-generation objectives.

</details>


### [14] [Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives](https://arxiv.org/abs/2509.08380)
*Prathamesh Vasudeo Naik,Naresh Kumar Dintakurthi,Zhanghao Hu,Yue Wang,Robby Qiu*

Main category: cs.AI

TL;DR: 这篇论文提出了Co-Investigator AI框架，通过多个专门代理协同工作，大幅提高疑似活动报告(SAR)的生成速度和准确性，解决了传统LLM在遵规领域的幻觉问题和解释性不足。


<details>
  <summary>Details</summary>
Motivation: 解决反洗钱工作流中疑似活动报告(SAR)生成成本高、扩展性低的瓶颈问题，充分利用AI效率同时确保遵规要求和准确性。

Method: 采用自治代理架构，整合规划、犯罪类型检测、外部情报收集、遵规验证等专门代理，包含动态内存管理、AI隐私保护层和实时验证机制。

Result: 在多种复杂金融犯罪场景中展示了良好效果，能大幅提升SAR编写效率、体现遵规期望、让遵规团队能专注高级分析工作。

Conclusion: 该框架标志着遵规报告新时代的开始，将AI代理的转型效益带入核心遵规流程，开启了可扩展、可靠、透明的SAR生成方式。

Abstract: Generating regulatorily compliant Suspicious Activity Report (SAR) remains a
high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.
While large language models (LLMs) offer promising fluency, they suffer from
factual hallucination, limited crime typology alignment, and poor
explainability -- posing unacceptable risks in compliance-critical domains.
This paper introduces Co-Investigator AI, an agentic framework optimized to
produce Suspicious Activity Reports (SARs) significantly faster and with
greater accuracy than traditional methods. Drawing inspiration from recent
advances in autonomous agent architectures, such as the AI Co-Scientist, our
approach integrates specialized agents for planning, crime type detection,
external intelligence gathering, and compliance validation. The system features
dynamic memory management, an AI-Privacy Guard layer for sensitive data
handling, and a real-time validation agent employing the Agent-as-a-Judge
paradigm to ensure continuous narrative quality assurance. Human investigators
remain firmly in the loop, empowered to review and refine drafts in a
collaborative workflow that blends AI efficiency with domain expertise. We
demonstrate the versatility of Co-Investigator AI across a range of complex
financial crime scenarios, highlighting its ability to streamline SAR drafting,
align narratives with regulatory expectations, and enable compliance teams to
focus on higher-order analytical work. This approach marks the beginning of a
new era in compliance reporting -- bringing the transformative benefits of AI
agents to the core of regulatory processes and paving the way for scalable,
reliable, and transparent SAR generation.

</details>


### [15] [TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making](https://arxiv.org/abs/2509.08500)
*Kechen Jiao,Zhirui Fang,Jiahao Liu,Bei Li,Qifan Wang,Xinyu Liu,Junhao Ruan,Zhongjian Qiao,Yifan Zhu,Yaxin Xu,Jingang Wang,Xiu Li*

Main category: cs.AI

TL;DR: 本文提出了Thought-Centric Preference Optimization (TCPO)方法，通过基于偏好的逐步优化和动作策略一致性约束，解决了视觉语言模型在具身人工智能决策中的幻觉问题和模型退化问题，在ALFWorld环境中实现了26.67%的平均成功率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在具身人工智能的动态任务中存在响应迟缓、幻觉问题，现有后SFT方法依赖强化学习和思维链，但受到稀疏奖励和仅动作优化的限制，导致样本效率低、一致性差和模型退化。

Method: 提出TCPO方法：1）基于偏好的逐步优化，将稀疏奖励信号转化为更丰富的步骤样本对；2）强调模型中间推理过程的对齐；3）引入动作策略一致性约束(APC)对模型输出施加一致性约束。

Result: 在ALFWorld环境中的实验显示平均成功率达到26.67%，相比RL4VLM提升了6%，有效缓解了微调后的模型退化问题。

Conclusion: TCPO方法通过将基于偏好的学习技术与思维链过程相结合，有效提升了视觉语言模型在具身智能体中的决策能力，为解决模型退化问题提供了有效方案。

Abstract: Using effective generalization capabilities of vision language models (VLMs)
in context-specific dynamic tasks for embodied artificial intelligence remains
a significant challenge. Although supervised fine-tuned models can better align
with the real physical world, they still exhibit sluggish responses and
hallucination issues in dynamically changing environments, necessitating
further alignment. Existing post-SFT methods, reliant on reinforcement learning
and chain-of-thought (CoT) approaches, are constrained by sparse rewards and
action-only optimization, resulting in low sample efficiency, poor consistency,
and model degradation. To address these issues, this paper proposes
Thought-Centric Preference Optimization (TCPO) for effective embodied
decision-making. Specifically, TCPO introduces a stepwise preference-based
optimization approach, transforming sparse reward signals into richer step
sample pairs. It emphasizes the alignment of the model's intermediate reasoning
process, mitigating the problem of model degradation. Moreover, by
incorporating Action Policy Consistency Constraint (APC), it further imposes
consistency constraints on the model output. Experiments in the ALFWorld
environment demonstrate an average success rate of 26.67%, achieving a 6%
improvement over RL4VLM and validating the effectiveness of our approach in
mitigating model degradation after fine-tuning. These results highlight the
potential of integrating preference-based learning techniques with CoT
processes to enhance the decision-making capabilities of vision-language models
in embodied agents.

</details>


### [16] [No-Knowledge Alarms for Misaligned LLMs-as-Judges](https://arxiv.org/abs/2509.08593)
*Andrés Corrada-Emmanuel*

Main category: cs.AI

TL;DR: 通过分析不同LLM评判者之间的逻辑一致性，发现并告警评判者集群中的错误评价


<details>
  <summary>Details</summary>
Motivation: 解决在缺乏真实地面情况下，LLM作为评判者评估其他LLM时的监督问题和无限监控链问题

Method: 利用不同LLM评判者之间的分歧一致性，通过线性规划问题计算唯一可能的评价结果，开发无知识告警机制

Result: 开发出能够以零假阶正率检测评判者集群中至少一个成员违反用户指定评分能力要求的告警系统

Conclusion: 通过逻辑一致性分析可以有效地监测LLM评判者的对齐问题，为处理专家评估中的不确定性提供了新的解决方案

Abstract: If we use LLMs as judges to evaluate the complex decisions of other LLMs, who
or what monitors the judges? Infinite monitoring chains are inevitable whenever
we do not know the ground truth of the decisions by experts and we do not want
to trust them. One way to ameliorate our evaluation uncertainty is to exploit
the use of logical consistency between disagreeing experts. By observing how
LLM judges agree and disagree while grading other LLMs, we can compute the only
possible evaluations of their grading ability. For example, if two LLM judges
disagree on which tasks a third one completed correctly, they cannot both be
100\% correct in their judgments. This logic can be formalized as a Linear
Programming problem in the space of integer response counts for any finite
test. We use it here to develop no-knowledge alarms for misaligned LLM judges.
The alarms can detect, with no false positives, that at least one member or
more of an ensemble of judges are violating a user specified grading ability
requirement.

</details>


### [17] [Automatic Failure Attribution and Critical Step Prediction Method for Multi-Agent Systems Based on Causal Inference](https://arxiv.org/abs/2509.08682)
*Guoqing Ma,Jia Zhu,Hanghui Guo,Weijie Shi,Jiawei Shen,Jingjiang Liu,Yidan Liang*

Main category: cs.AI

TL;DR: 提出了首个基于多粒度因果推理的多智能体系统故障归因框架，通过性能因果反转原理和因果发现算法CDC-MAS，显著提升了故障根因定位准确率和任务成功率


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在实际部署中面临故障归因的严峻挑战，现有基于统计相关性的诊断工具在Who&When等基准测试中准确率不足15%，无法有效定位故障根因步骤

Method: 1) 性能因果反转原理：通过反转执行日志中的数据流正确建模性能依赖关系，结合Shapley值准确分配智能体级责任；2) CDC-MAS因果发现算法：鲁棒地识别关键故障步骤，处理MAS交互数据的非平稳特性

Result: 在Who&When和TRAIL基准测试中表现显著提升：方法达到36.2%的步骤级准确率，生成的优化方案平均提升22.4%的整体任务成功率

Conclusion: 为调试复杂智能体交互提供了原则性和有效的解决方案，为更可靠和可解释的多智能体系统铺平了道路

Abstract: Multi-agent systems (MAS) are critical for automating complex tasks, yet
their practical deployment is severely hampered by the challenge of failure
attribution. Current diagnostic tools, which rely on statistical correlations,
are fundamentally inadequate; on challenging benchmarks like Who\&When,
state-of-the-art methods achieve less than 15\% accuracy in locating the
root-cause step of a failure. To address this critical gap, we introduce the
first failure attribution framework for MAS grounded in multi-granularity
causal inference. Our approach makes two key technical contributions: (1) a
performance causal inversion principle, which correctly models performance
dependencies by reversing the data flow in execution logs, combined with
Shapley values to accurately assign agent-level blame; (2) a novel causal
discovery algorithm, CDC-MAS, that robustly identifies critical failure steps
by tackling the non-stationary nature of MAS interaction data. The framework's
attribution results directly fuel an automated optimization loop, generating
targeted suggestions whose efficacy is validated via counterfactual
simulations. Evaluations on the Who\&When and TRAIL benchmarks demonstrate a
significant leap in performance. Our method achieves up to 36.2\% step-level
accuracy. Crucially, the generated optimizations boost overall task success
rates by an average of 22.4\%. This work provides a principled and effective
solution for debugging complex agent interactions, paving the way for more
reliable and interpretable multi-agent systems.

</details>


### [18] [One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases](https://arxiv.org/abs/2509.08705)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 基于双系统理论的心理理论框架，结合图卷积网络（System 1）和元学习（System 2），通过上下文门机制动态平衡直觉和思考性推理。


<details>
  <summary>Details</summary>
Motivation: 建立一种能够模仿人类双重推理过程的AI系统，以更好地理解和复现人类的社交认知和适应性决策能力。

Method: 使用图卷积网络（GCNs）实现快速、习惯性的System 1，通过元学习技术实现慢速、上下文敏感的System 2，并通过学习的上下文门机制动态调节两者。

Result: 在标准错误信念任务上验证了框架有效性，能够复现中定、认知负荷疲劳、框架效应、启动效应等认知偏差，与人类适应性行为密切匹配，并实现了对未见上下文的健壮推广。

Conclusion: 该工作在人工智能和认知理论之间建立了桥梁，为开发具有细腻、类人社交认知和适应性决策能力的AI系统排阵铁。

Abstract: We introduce a novel Theory of Mind (ToM) framework inspired by dual-process
theories from cognitive science, integrating a fast, habitual graph-based
reasoning system (System 1), implemented via graph convolutional networks
(GCNs), and a slower, context-sensitive meta-adaptive learning system (System
2), driven by meta-learning techniques. Our model dynamically balances
intuitive and deliberative reasoning through a learned context gate mechanism.
We validate our architecture on canonical false-belief tasks and systematically
explore its capacity to replicate hallmark cognitive biases associated with
dual-process theory, including anchoring, cognitive-load fatigue, framing
effects, and priming effects. Experimental results demonstrate that our
dual-process approach closely mirrors human adaptive behavior, achieves robust
generalization to unseen contexts, and elucidates cognitive mechanisms
underlying reasoning biases. This work bridges artificial intelligence and
cognitive theory, paving the way for AI systems exhibiting nuanced, human-like
social cognition and adaptive decision-making capabilities.

</details>


### [19] [The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems](https://arxiv.org/abs/2509.08713)
*Ziming Luo,Atoosa Kasirzadeh,Nihar B. Shah*

Main category: cs.AI

TL;DR: 本文分析了AI科学家系统的潜在失败模式，包括基准选择不当、数据泄露、指标误用和后验选择偏差，并通过实验验证了这些风险的存在，建议期刊要求提交完整工作流日志以确保透明度。


<details>
  <summary>Details</summary>
Motivation: AI科学家系统能够自主执行完整研究流程，但其内部工作流缺乏审查，可能引入损害研究输出完整性、可靠性和可信度的缺陷。

Method: 设计受控实验隔离四种潜在失败模式，评估两个主流开源AI科学家系统，分析完整自动化工作流的追踪日志和代码。

Result: 评估发现多个不同程度的失败模式，这些在实践中容易被忽视；完整工作流日志比仅检查最终论文能更有效地检测失败。

Conclusion: 建议期刊和会议在评估AI生成研究时，要求同时提交完整工作流日志和代码，以确保透明度、问责制和可重复性。

Abstract: AI scientist systems, capable of autonomously executing the full research
workflow from hypothesis generation and experimentation to paper writing, hold
significant potential for accelerating scientific discovery. However, the
internal workflow of these systems have not been closely examined. This lack of
scrutiny poses a risk of introducing flaws that could undermine the integrity,
reliability, and trustworthiness of their research outputs. In this paper, we
identify four potential failure modes in contemporary AI scientist systems:
inappropriate benchmark selection, data leakage, metric misuse, and post-hoc
selection bias. To examine these risks, we design controlled experiments that
isolate each failure mode while addressing challenges unique to evaluating AI
scientist systems. Our assessment of two prominent open-source AI scientist
systems reveals the presence of several failures, across a spectrum of
severity, which can be easily overlooked in practice. Finally, we demonstrate
that access to trace logs and code from the full automated workflow enables far
more effective detection of such failures than examining the final paper alone.
We thus recommend journals and conferences evaluating AI-generated research to
mandate submission of these artifacts alongside the paper to ensure
transparency, accountability, and reproducibility.

</details>


### [20] [Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making](https://arxiv.org/abs/2509.08785)
*Anup Tuladhar,Araz Minhas,Adam Kirton,Eli Kinney-Lang*

Main category: cs.AI

TL;DR: 基于双系统架构的实验平台，通过结合强化学习和语言模型推理，探索故事框架对AI决策的影响


<details>
  <summary>Details</summary>
Motivation: 尽管AI系统现在既能做决策又能进行故事推理，但这两种能力主要是分开研究的，需要架路这个间隔来研窋故事框架如何影响奖励基于学习

Method: 采用双系统架构：一个基于过往经验建议行动的强化学习策略，和一个语言模型，通过不同故事框架处理这些建议来指导决策，实现在可配置的gridworld环境中

Result: 开发了一个模块化的实验平台，能够控制测试环境复杂性、故事参数以及强化学习与故事基于决策的交互作用，记录了从RL策略价值到语言模型推理内容的各种决策指标

Conclusion: 虽然是初步实现，但这个平台为研究不同故事框架如何影响奖励基于决策提供了基础，同时也为探索优化基于学习与符号推理在AI系统中的潜在交互作用创造了条件

Abstract: We present a preliminary experimental platform that explores how narrative
elements might shape AI decision-making by combining reinforcement learning
(RL) with language model reasoning. While AI systems can now both make
decisions and engage in narrative reasoning, these capabilities have mostly
been studied separately. Our platform attempts to bridge this gap using a
dual-system architecture to examine how narrative frameworks could influence
reward-based learning. The system comprises a reinforcement learning policy
that suggests actions based on past experience, and a language model that
processes these suggestions through different narrative frameworks to guide
decisions. This setup enables initial experimentation with narrative elements
while maintaining consistent environment and reward structures. We implement
this architecture in a configurable gridworld environment, where agents receive
both policy suggestions and information about their surroundings. The
platform's modular design facilitates controlled testing of environmental
complexity, narrative parameters, and the interaction between reinforcement
learning and narrative-based decisions. Our logging system captures basic
decision metrics, from RL policy values to language model reasoning to action
selection patterns. While preliminary, this implementation provides a
foundation for studying how different narrative frameworks might affect
reward-based decisions and exploring potential interactions between
optimization-based learning and symbolic reasoning in AI systems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [21] [The Linear Reliability Channel](https://arxiv.org/abs/2509.08079)
*Alexander Mariona,Ken R. Duffy,Muriel Médard*

Main category: cs.IT

TL;DR: 本文提出了线性可靠性信道(LRC)，这是一种基于接收符号可靠性排序的离散软判决信道，证明了在高噪声方差下LRC是对一类离散调制连续噪声信道的合适近似，并建立了随机码在硬判决和软判决ML解码下的显式错误指数。


<details>
  <summary>Details</summary>
Motivation: 为了在软判决解码设置中提供一个可进行广泛数学分析的离散信道模型，特别是为了定量评估软判决解码相对于硬判决解码的优势。

Method: 引入线性可靠性信道(LRC)，利用其组合特性对信道和相应的硬判决、软判决最大似然解码器进行数学分析，建立随机码的错误指数。

Result: 证明了LRC在高噪声方差下是对一类离散调制连续噪声信道的合适近似，获得了硬判决和软判决ML解码的显式错误指数，为软判决解码的优势提供了定量评估。

Conclusion: LRC提供了一个新的离散几何视角，不同于BSC的汉明权重特性，为软判决设置下的码构造提供了新的思路，并且其组合特性使得对信道和解码器的深入数学分析成为可能。

Abstract: We introduce and analyze a discrete soft-decision channel called the linear
reliability channel (LRC) in which the soft information is the rank ordering of
the received symbol reliabilities. We prove that the LRC is an appropriate
approximation to a general class of discrete modulation, continuous noise
channels when the noise variance is high. The central feature of the LRC is
that its combinatorial nature allows for an extensive mathematical analysis of
the channel and its corresponding hard- and soft-decision maximum likelihood
(ML) decoders. In particular, we establish explicit error exponents for ML
decoding in the LRC when using random codes under both hard- and soft-decision
decoding. This analysis allows for a direct, quantitative evaluation of the
relative advantage of soft-decision decoding. The discrete geometry of the LRC
is distinct from that of the BSC, which is characterized by the Hamming weight,
offering a new perspective on code construction for soft-decision settings.

</details>


### [22] [Holographic Beamforming for Integrated Sensing and Communication with Mutual Coupling Effects](https://arxiv.org/abs/2509.08113)
*Shuhao Zeng,Haobo Zhang,Boya Di,Hongliang Zhang,Zijian Shao,Zhu Han,H. Vincent Poor,Lingyang Song*

Main category: cs.IT

TL;DR: 本文提出了一种考虑互耦效应的全息波束成形算法，用于集成感知与通信系统，通过耦合偶极子近似来抑制旁瓣并提升性能


<details>
  <summary>Details</summary>
Motivation: 6G网络中集成感知与通信(ISAC)需要高性能波束成形，但现有全息波束成形方案忽略互耦效应，导致旁瓣水平升高，影响感知性能

Method: 提出基于电磁兼容的全息ISAC模型，使用耦合偶极子近似建立互耦闭式表达式，开发高效的互耦感知全息波束成形算法

Result: 数值结果验证了所提算法的有效性，能够显著抑制旁瓣并提升ISAC系统性能

Conclusion: 考虑互耦效应的全息波束成形设计对ISAC系统至关重要，所提算法为解决互耦引起的非线性问题提供了有效方案

Abstract: Integrated sensing and communication (ISAC) is envisioned as a key technology
in 6G networks, owing to its potential for high spectral and cost efficiency.
As a promising solution for extremely large-scale arrays, reconfigurable
holographic surfaces (RHS) can be integrated with ISAC to form the holographic
ISAC paradigm, where enlarged radiation apertures of RHS can achieve
significant beamforming gains, thereby improving both communication and sensing
performance. In this paper, we investigate holographic beamforming designs for
ISAC systems, which, unlike existing holographic beamforming schemes developed
for RHS-aided communications, requires explicit consideration of mutual
coupling effects within RHS. This is because, different from prior works only
considering communication performance, ISAC systems incorporate sensing
functionality, which is sensitive to sidelobe levels. Ignoring mutual coupling
in holographic beamforming can lead to notable undesired sidelobes, thus
degrading sensing performance. The consideration of mutual coupling introduces
new challenges, i.e., it induces non-linearity in beamforming problems,
rendering them inherently non-convex. To address this issue, we propose a
tractable electromagnetic-compliant holographic ISAC model that characterizes
mutual coupling in a closed form using coupled dipole approximations. We then
develop an efficient mutual coupling aware holographic beamforming algorithm to
suppress sidelobes and enhance ISAC performance. Numerical results validate
effectiveness of the proposed algorithm.

</details>


### [23] [SCA-LLM: Spectral-Attentive Channel Prediction with Large Language Models in MIMO-OFDM](https://arxiv.org/abs/2509.08139)
*Ke He,Le He,Lisheng Fan,Xianfu Lei,Thang X. Vu,George K. Karagiannidis,Symeon Chatzinotas*

Main category: cs.IT

TL;DR: 本文提出SCA-LLM框架，通过频谱注意力机制解决大语言模型在信道预测中的领域不匹配问题，在MIMO-OFDM系统中实现了最先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在无线通信信道预测中存在领域不匹配问题，现有适配器未能充分利用频谱信息来弥合信道状态信息与文本预训练模型之间的差距。

Method: 提出频谱注意力框架SCA-LLM，其新型适配器能够捕获更精细的频谱细节，更好地将LLM适配于信道预测任务。

Result: SCA-LLM实现了最先进的预测性能和强泛化能力，相比之前的LLM方法获得了高达-2.4 dB的NMSE优势。

Conclusion: 通过从CSI特征的频谱分量学习表示，能够更有效地弥合领域差距，SCA-LLM在缓解领域不匹配方面表现出优越性。

Abstract: In recent years, the success of large language models (LLMs) has inspired
growing interest in exploring their potential applications in wireless
communications, especially for channel prediction tasks. However, directly
applying LLMs to channel prediction faces a domain mismatch issue stemming from
their text-based pre-training. To mitigate this, the ``adapter + LLM" paradigm
has emerged, where an adapter is designed to bridge the domain gap between the
channel state information (CSI) data and LLMs. While showing initial success,
existing adapters may not fully exploit the potential of this paradigm. To
address this limitation, this work provides a key insight that learning
representations from the spectral components of CSI features can more
effectively help bridge the domain gap. Accordingly, we propose a
spectral-attentive framework, named SCA-LLM, for channel prediction in
multiple-input multiple-output orthogonal frequency division multiplexing
(MIMO-OFDM) systems. Specifically, its novel adapter can capture finer spectral
details and better adapt the LLM for channel prediction than previous methods.
Extensive simulations show that SCA-LLM achieves state-of-the-art prediction
performance and strong generalization, yielding up to $-2.4~\text{dB}$
normalized mean squared error (NMSE) advantage over the previous LLM based
method. Ablation studies further confirm the superiority of SCA-LLM in
mitigating domain mismatch.

</details>


### [24] [The Shannon Upper Bound for the Error Exponent](https://arxiv.org/abs/2509.08425)
*Sergey Tridenski,Anelia Somekh-Baruch*

Main category: cs.IT

TL;DR: 本文针对离散时间加性白广义高斯噪声信道，在广义输入功率约束下，推导了最优块错误指数的上界，并给出了三种特殊情况的显式渐近上界。


<details>
  <summary>Details</summary>
Motivation: 研究广义高斯噪声信道在功率约束下的最优错误性能界限，为信道编码理论提供理论分析工具。

Method: 使用基于类型的方法，采用依赖于块长度n的有限字母表，类型数量在n中呈亚指数增长。

Result: 推导出了最优块错误指数的上界，并针对拉普拉斯噪声信道和高斯噪声信道的三种特殊情况给出了显式渐近上界。

Conclusion: 该方法能够有效分析广义高斯噪声信道在功率约束下的错误性能，为相关信道编码设计提供理论指导。

Abstract: For the discrete-time additive white generalized Gaussian noise channel with
a generalized input power constraint, with the respective shape and power
parameters >= 1, we derive an upper bound on the optimal block error exponent.
Explicit asymptotic upper bounds in the limit of a large block length n are
given for three special cases: the Laplace noise channel and the Gaussian noise
channel with the average absolute value constraint, and for the Laplace noise
channel with the second power constraint. The derivation uses the method of
types with finite alphabets of sizes depending on the block length n and with
the number of types sub-exponential in n.

</details>


### [25] [Deep holes of a class of twisted Reed-Solomon codes](https://arxiv.org/abs/2509.08526)
*Haojie Gu,Nan Wang,Jun Zhang*

Main category: cs.IT

TL;DR: 本文研究了扭曲Reed-Solomon码的深洞问题，给出了深洞存在的充要条件，并在特定参数范围内完全确定了所有深洞。


<details>
  <summary>Details</summary>
Motivation: 深洞问题是编码理论中的基本问题，在码构造和密码学中有重要应用。作为Reed-Solomon码的推广，研究扭曲Reed-Solomon码的深洞问题具有重要意义。

Method: 首先给出了扭曲Reed-Solomon码深洞存在的充要条件，然后针对特定参数范围的扭曲Reed-Solomon码，通过数学证明确定了所有深洞。

Result: 对于TRS_k(F_q*,k-1,η)码，在q为偶数且3q+2√q-8/4≤k≤q-5，以及q为奇数且3q+3√q-5/4≤k≤q-5时，证明了没有其他深洞存在；在q为偶数且q-4≤k≤q-2时，完全确定了所有深洞。

Conclusion: 本文系统研究了扭曲Reed-Solomon码的深洞问题，建立了理论框架并获得了具体的分类结果，为这类码的深洞特性提供了完整的理论描述。

Abstract: The deep hole problem is a fundamental problem in coding theory, and it has
many important applications in code constructions and cryptography. The deep
hole problem of Reed-Solomon codes has gained a lot of attention. As a
generalization of Reed-Solomon codes, we investigate the problem of deep holes
of a class of twisted Reed-Solomon codes in this paper.
  Firstly, we provide the necessary and sufficient conditions for
$\boldsymbol{a}=(a_{0},a_{1},\cdots,a_{n-k-1})\in\mathbb{F}_{q}^{n-k}$ to be
the syndrome of some deep hole of $TRS_{k}(\mathcal{A},l,\eta)$. Next, we
consider the problem of determining all deep holes of the twisted Reed-Solomon
codes $TRS_{k}(\mathbb{F}_{q}^{*},k-1,\eta)$. Specifically, we prove that there
are no other deep holes of $TRS_{k}(\mathbb{F}_{q}^{*},k-1,\eta)$ for
$\frac{3q+2\sqrt{q}-8}{4}\leq k\leq q-5$ when q is even, and
$\frac{3q+3\sqrt{q}-5}{4}\leq k\leq q-5$ when q is odd. We also completely
determine their deep holes for $q-4\leq k\leq q-2$ when $q$ is even.

</details>


### [26] [The Landscape of Fairness: An Axiomatic and Predictive Framework for Network QoE Sensitivity](https://arxiv.org/abs/2509.08551)
*Zhiyuan Ren,Xinke Jian,Wenchi Cheng,Kun Yang*

Main category: cs.IT

TL;DR: 提出了一个完整的分析框架，将公平性评估从单点测量转变为基于可预测敏感性景观的主动工程学科，包括QoE-Imbalance度量、协方差规则、相图和拓扑感知设计规则。


<details>
  <summary>Details</summary>
Motivation: 网络范围的公平性评估具有挑战性，因为它不是静态属性，而是对SLA参数高度敏感。现有方法缺乏系统性框架来理解和导航公平性敏感性景观。

Method: 1) 基于公平性基本公理构建QoE-Imbalance度量；2) 推导闭式协方差规则作为局部指南；3) 构建相图映射全局景观；4) 分析景观曲率得出拓扑感知设计规则。

Result: 框架揭示了关键的拓扑特征，如稳健的"稳定带"和高风险的"危险楔形区"，并提出了最优的"阈值优先"调优策略。

Conclusion: 该框架提供了映射、解释和导航系统敏感性景观的工具，能够设计更稳健和弹性的网络，将公平性评估转变为可预测的工程学科。

Abstract: Evaluating network-wide fairness is challenging because it is not a static
property but one highly sensitive to Service Level Agreement (SLA) parameters.
This paper introduces a complete analytical framework to transform fairness
evaluation from a single-point measurement into a proactive engineering
discipline centered on a predictable sensitivity landscape. Our framework is
built upon a QoE-Imbalance metric whose form is not an ad-hoc choice, but is
uniquely determined by a set of fundamental axioms of fairness, ensuring its
theoretical soundness. To navigate the fairness landscape across the full
spectrum of service demands, we first derive a closed-form covariance rule.
This rule provides an interpretable, local compass, expressing the fairness
gradient as the covariance between a path's information-theoretic importance
and its parameter sensitivity. We then construct phase diagrams to map the
global landscape, revealing critical topological features such as robust
"stable belts" and high-risk "dangerous wedges". Finally, an analysis of the
landscape's curvature yields actionable, topology-aware design rules, including
an optimal "Threshold-First" tuning strategy. Ultimately, our framework
provides the tools to map, interpret, and navigate the landscape of system
sensitivity, enabling the design of more robust and resilient networks.

</details>


### [27] [Low-Complexity CSI Acquisition Exploiting Geographical Diversity in Fluid Antenna System](https://arxiv.org/abs/2509.08598)
*Zhentian Zhang,David Morales-Jimenez,Jian Dang,Zaichen Zhang,Christos Masouros,Hao Jiang*

Main category: cs.IT

TL;DR: 提出基于EM-AMP框架的变体算法，利用FAS地理先验信息，提高信道状态信息估计精度，加速收敛并降低大规模部署复杂度


<details>
  <summary>Details</summary>
Motivation: 流体天线系统需要高效的信道状态信息获取方案，现有贪婪算法依赖信号假设，无模型方法复杂度高，需要灵活低复杂度的解决方案

Method: 基于期望最大化-近似消息传递(EM-AMP)框架，开发变体算法，利用FAS的地理先验信息进行自适应学习

Result: 仿真验证了所提算法的有效性，提高了估计精度，加速了收敛速度，降低了复杂度

Conclusion: 该EM-AMP变体算法为大规模FAS部署提供了高效的信道状态信息获取解决方案

Abstract: The fluid antenna system (FAS) employs reconfigurable antennas for high
spatial gains in compact spaces, enhancing physical layer flexibility. Channel
state information (CSI) acquisition is vital for port selection and FAS
optimization. Greedy algorithms rely on signal assumptions, and model-free
methods face high complexity. A flexible, low-complexity solution is needed for
massive connectivity in FAS. Based on expectation maximization-approximate
message passing (EM-AMP) framework, efficient matrix computations and adaptive
learning without prior model knowledge naturally suit CSI acquisition for FAS.
We propose a EM-AMP variant exploiting FAS geographical priors, improving
estimation precision, accelerating convergence, and reducing complexity in
large-scale deployment. Simulations validate the efficacy of the proposed
algorithm.

</details>


### [28] [Fluid Antenna Systems: A Geometric Approach to Error Probability and Fundamental Limits](https://arxiv.org/abs/2509.08815)
*Xusheng Zhu,Kai-Kit Wong,Hao Xu,Han Xiao,Hanjiang Hong,Hyundong Shin,Yangyang Zhang*

Main category: cs.IT

TL;DR: 流体天线系统(FAS)错误概率分析框架，推导出闭式符号错误率表达式，发现多样性收益由渡效秩决定，而非天线端口数量


<details>
  <summary>Details</summary>
Motivation: 缺少在实际空间相关渡道下对FAS错误概率进行严格分析的框架

Method: 推导紧凑的闭式进阶SER表达式，提出双重方法：基于几何的算法提取性能阈值，理论证明渡效秩收敛到由天线归一化孔径宽度决定的基本极限

Result: 渡效秩模型较现有方法更准确，完整定义了多样性和编码收益，证明了几何算法阈值与理论极限的等价性

Conclusion: FAS性能收益的根本驱动因素是扩大天线可探索孔径以增加渡效秩，而在固定孔径内增加端口密度的收益逐渐减少

Abstract: The fluid antenna system (FAS) concept is an emerging paradigm that promotes
the utilization of the feature of shape and position reconfigurability in
antennas to broaden the design of wireless communication systems. This also
means that spatial diversity can be exploited in an unconventional way.
However, a rigorous framework for error probability analysis of FAS under
realistic spatially correlated channels has been lacking. In this paper, we
fill this gap by deriving a tight, closed-form asymptotic expression for the
symbol error rate (SER) that establishes the fundamental scaling law linking
the system's SER to the channel's spatial correlation structure. A key insight
of our analysis is that the achievable diversity gain is governed not by the
number of antenna ports, but by the channel's effective rank. To find this
critical parameter, we propose a novel dual-pronged approach. First of all, we
develop a geometry-based algorithm that extracts distinct performance
thresholds from the channel's eigenvalue spectrum. Second, we theoretically
prove that the effective rank converges to a fundamental limit dictated solely
by the antenna's normalized aperture width. We further establish the
equivalence between the threshold identified by the geometric algorithm and the
derived theoretical limit, providing rigorous validation for the proposed
method. Our effective rank model achieves higher accuracy than existing
approaches in the literature. Building on this framework, we offer a complete
characterization of diversity and coding gains. The analysis leads to a
definitive design insight: FAS performance improvements are fundamentally
driven by enlarging the antenna's explorable aperture, which increases the
effective channel rank, whereas increasing port density within a fixed aperture
yields diminishing returns.

</details>


### [29] [ToDMA: Large Model-Driven Token-Domain Multiple Access for Semantic Communications](https://arxiv.org/abs/2505.10946)
*Li Qiao,Mahdi Boloursaz Mashhadi,Zhen Gao,Robert Schober,Deniz Gündüz*

Main category: cs.IT

TL;DR: 提出基于token域的多址接入方案ToDMA，利用MLLM处理token碰撞，实现低延迟高质量的多模态语义通信


<details>
  <summary>Details</summary>
Motivation: 传统通信方案在大量设备接入时存在延迟高、效率低的问题，需要利用语义通信和上下文信息来提升多址接入性能

Method: 设备共享token码本和调制码本，通过压缩感知检测活跃token和CSI，利用MLLM预测被碰撞的token来重构序列

Result: 在文本和图像传输任务中，相比无上下文正交方案显著降低延迟，相比最先进的无上下文非正交方法提供更好的失真和感知质量

Conclusion: ToDMA框架有效解决了token碰撞问题，为大规模设备语义通信提供了高效的多址接入解决方案

Abstract: Token communications (TokCom) is an emerging generative semantic
communication concept that reduces transmission rates by using context and
multimodal large language model (MLLM)-based token processing, with tokens
serving as universal semantic units across modalities. In this paper, we
propose a semantic multiple access scheme in the token domain, referred to as
token domain multiple access (ToDMA), where a large number of devices share a
token codebook and a modulation codebook for source and channel coding,
respectively. Specifically, each transmitter first tokenizes its source signal
and modulate each token to a codeword. At the receiver, compressed sensing is
employed first to detect active tokens and the corresponding channel state
information (CSI) from the superposed signals. Then, the source token sequences
are reconstructed by clustering the token-associated CSI across multiple time
slots. In case of token collisions, some active tokens cannot be assigned and
some positions in the reconstructed token sequences are empty. We propose to
use pre-trained MLLMs to leverage the context, predict masked tokens, and thus
mitigate token collisions. Simulation results demonstrate the effectiveness of
the proposed ToDMA framework for both text and image transmission tasks,
achieving significantly lower latency compared to context-unaware orthogonal
communication schemes, while also delivering superior distortion and perceptual
quality compared to state-of-the-art context-unaware non-orthogonal
communication methods.

</details>
