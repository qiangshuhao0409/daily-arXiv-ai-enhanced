<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 11]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.IT](#cs.IT) [Total: 12]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Deep Reinforcement Learning Approach to QoSAware Load Balancing in 5G Cellular Networks under User Mobility and Observation Uncertainty](https://arxiv.org/abs/2510.24869)
*Mehrshad Eskandarpour,Hossein Soleimani*

Main category: cs.NI

TL;DR: 提出基于PPO强化学习的5G网络负载均衡框架，在Python仿真环境中实现端到端的QoS感知自主控制，相比传统方法在各项性能指标上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决密集动态5G网络中移动性管理和负载均衡的挑战，确保服务质量(QoS)的持续维持。

Method: 使用近端策略优化(PPO)强化学习算法，将控制问题建模为马尔可夫决策过程，通过周期性调整小区个体偏移值来引导用户-小区关联。

Result: 在500+训练轮次和压力测试中，PPO策略持续改善KPI趋势（更高吞吐量和公平性，更低延迟、抖动、丢包率和切换次数），并表现出快速稳定的收敛性。

Conclusion: PPO的裁剪策略更新和基于优势的训练为下一代RAN负载均衡提供了鲁棒、可部署的控制方案，使用纯Python工具链实现。

Abstract: Efficient mobility management and load balancing are critical to sustaining
Quality of Service (QoS) in dense, highly dynamic 5G radio access networks. We
present a deep reinforcement learning framework based on Proximal Policy
Optimization (PPO) for autonomous, QoS-aware load balancing implemented
end-to-end in a lightweight, pure-Python simulation environment. The control
problem is formulated as a Markov Decision Process in which the agent
periodically adjusts Cell Individual Offset (CIO) values to steer user-cell
associations. A multi-objective reward captures key performance indicators
(aggregate throughput, latency, jitter, packet loss rate, Jain's fairness
index, and handover count), so the learned policy explicitly balances
efficiency and stability under user mobility and noisy observations. The PPO
agent uses an actor-critic neural network trained from trajectories generated
by the Python simulator with configurable mobility (e.g., Gauss-Markov) and
stochastic measurement noise. Across 500+ training episodes and stress tests
with increasing user density, the PPO policy consistently improves KPI trends
(higher throughput and fairness, lower delay, jitter, packet loss, and
handovers) and exhibits rapid, stable convergence. Comparative evaluations show
that PPO outperforms rule-based ReBuHa and A3 as well as the learning-based
CDQL baseline across all KPIs while maintaining smoother learning dynamics and
stronger generalization as load increases. These results indicate that PPO's
clipped policy updates and advantage-based training yield robust, deployable
control for next-generation RAN load balancing using an entirely Python-based
toolchain.

</details>


### [2] [Performance Evaluation of Multimedia Traffic in Cloud Storage Services over Wi-Fi and LTE Networks](https://arxiv.org/abs/2510.25079)
*Albert Espinal,V. Sanchez Padilla,Yesenia Cevallos*

Main category: cs.NI

TL;DR: 评估Dropbox、Google Drive和OneDrive在Wi-Fi和LTE网络下多媒体文件上传的性能，Google Drive表现最稳定，Dropbox带宽利用率高但LTE延迟大，OneDrive对网络波动敏感。


<details>
  <summary>Details</summary>
Motivation: 了解不同云存储服务在不同网络条件下的性能表现，为优化云存储应用提供参考。

Method: 使用Wireshark捕获流量，分析延迟、抖动、带宽和丢包率等关键指标，采用双泊松函数建模数据包大小分布。

Result: Google Drive在两网络下表现最稳定，延迟低抖动小；Dropbox带宽利用率高但LTE延迟大；OneDrive数据包速率高且对移动网络波动敏感。Wi-Fi连接更稳定，LTE性能因平台实现而异。

Conclusion: Wi-Fi为多媒体传输提供更好稳定性，云存储服务性能受网络类型和平台实现影响，建议使用更大数据集和异构网络进行进一步分析。

Abstract: The performance of Dropbox, Google Drive, and OneDrive cloud storage services
was evaluated under Wi-Fi and LTE network conditions during multimedia file
uploads. Traffic was captured using Wireshark, and key metrics (including
delay, jitter, bandwidth, and packet loss) were analyzed. Google Drive
maintained the most consistent performance across both types of networks,
showing low latency and reduced jitter. Dropbox showed efficient bandwidth
utilization, but experienced a longer delay over LTE, attributed to a greater
number of intermediate hops. OneDrive presented variable behavior, with
elevated packet rates and increased sensitivity to fluctuations in the mobile
network. A bimodal distribution of packet sizes was observed and modeled using
a dual Poisson function. In general, Wi-Fi connections provided greater
stability for multimedia transfers, while LTE performance varied depending on
platform-specific implementations. The results contribute to a better
understanding of traffic behavior in cloud-based storage applications and
suggest further analysis with larger datasets and heterogeneous access
networks.

</details>


### [3] [Learning-Based vs Human-Derived Congestion Control: An In-Depth Experimental Study](https://arxiv.org/abs/2510.25105)
*Mihai Mazilu,Luca Giacomoni,George Parisis*

Main category: cs.NI

TL;DR: 本文对基于学习的拥塞控制算法进行了系统性研究，通过与TCP Cubic和BBR等传统算法对比，揭示了现有学习型CC方法的优势和根本局限性。


<details>
  <summary>Details</summary>
Motivation: 基于学习的拥塞控制在快速变化的网络环境中具有潜力，但目前仍处于早期阶段，需要深入研究现有方法的局限性和研究挑战，以开发适用于真实网络的可部署解决方案。

Method: 建立了评估学习型CC的方法论，进行了大规模实验，直接对比学习型方法与广泛部署的传统CC算法（TCP Cubic和BBR v3），并使用公开可用的学习型CC方法。

Result: 研究发现：将公平性直接嵌入奖励函数是有效的，但公平性在未见条件下无法泛化；RL方法能够获取所有可用带宽并保持低延迟；现有最新学习型CC在带宽和端到端延迟动态变化时表现不佳，但对非拥塞性丢包具有抵抗力。

Conclusion: 学习型拥塞控制仍需进一步研究以解决泛化性问题，实验代码和数据集已公开以促进研究社区的透明度和可复现性。

Abstract: Learning-based congestion control (CC), including Reinforcement-Learning,
promises efficient CC in a fast-changing networking landscape, where evolving
communication technologies, applications and traffic workloads pose severe
challenges to human-derived, static CC algorithms. Learning-based CC is in its
early days and substantial research is required to understand existing
limitations, identify research challenges and, eventually, yield deployable
solutions for real-world networks. In this paper, we extend our prior work and
present a reproducible and systematic study of learning-based CC with the aim
to highlight strengths and uncover fundamental limitations of the
state-of-the-art. We directly contrast said approaches with widely deployed,
human-derived CC algorithms, namely TCP Cubic and BBR (version 3). We identify
challenges in evaluating learning-based CC, establish a methodology for
studying said approaches and perform large-scale experimentation with
learning-based CC approaches that are publicly available. We show that
embedding fairness directly into reward functions is effective; however, the
fairness properties do not generalise into unseen conditions. We then show that
RL learning-based approaches existing approaches can acquire all available
bandwidth while largely maintaining low latency. Finally, we highlight that
existing the latest learning-based CC approaches under-perform when the
available bandwidth and end-to-end latency dynamically change while remaining
resistant to non-congestive loss. As with our initial study, our
experimentation codebase and datasets are publicly available with the aim to
galvanise the research community towards transparency and reproducibility,
which have been recognised as crucial for researching and evaluating
machine-generated policies.

</details>


### [4] [ML-Based Preamble Collision Detection in the Random Access Procedure of Cellular IoT Networks](https://arxiv.org/abs/2510.25145)
*Giancarlo Maldonado Cardenas,Diana C. Gonzalez,Judy C. Guevara,Carlos A. Astudillo,Nelson L. S. da Fonseca*

Main category: cs.NI

TL;DR: 提出基于机器学习的随机接入信道前导码碰撞早期检测机制，使用神经网络在多种通信场景下实现超过98%的平衡准确率，并通过量化优化将推理时间从2500ms降至0.3ms。


<details>
  <summary>Details</summary>
Motivation: 大规模机器类通信场景中随机接入信道的前导码碰撞是主要瓶颈，影响蜂窝物联网部署的性能。

Method: 使用MATLAB模拟真实信道条件生成标记数据集，评估九种经典分类器（包括树集成、支持向量机和神经网络），并在四种通信场景下进行测试，应用训练后量化优化部署。

Result: 神经网络表现最佳，在分布内评估中达到超过98%平衡准确率，分布外评估中保持95%准确率。全整数量化将推理时间从2500ms降至0.3ms，精度损失可忽略。

Conclusion: 该解决方案结合高检测精度和低延迟推理，适用于真实网络中可扩展的实时蜂窝物联网应用。

Abstract: Preamble collision in the random access channel (RACH) is a major bottleneck
in massive machine-type communication (mMTC) scenarios, typical of cellular IoT
(CIoT) deployments. This work proposes a machine learning-based mechanism for
early collision detection during the random access (RA) procedure. A labeled
dataset was generated using the RA procedure messages exchanged between the
users and the base station under realistic channel conditions, simulated in
MATLAB. We evaluate nine classic classifiers -- including tree ensembles,
support vector machines, and neural networks -- across four communication
scenarios, varying both channel characteristics (e.g., Doppler spread,
multipath) and the cell coverage radius, to emulate realistic propagation,
mobility, and spatial conditions. The neural network outperformed all other
models, achieving over 98\% balanced accuracy in the in-distribution evaluation
(train and test drawn from the same dataset) and sustaining 95\% under
out-of-distribution evaluation (train/test from different datasets). To enable
deployment on typical base station hardware, we apply post-training
quantization. Full integer quantization reduced inference time from 2500 ms to
as low as 0.3 ms with negligible accuracy loss. The proposed solution combines
high detection accuracy with low-latency inference, making it suitable for
scalable, real-time CIoT applications found in real networks.

</details>


### [5] [Adaptive Design of mmWave Initial Access Codebooks using Reinforcement Learning](https://arxiv.org/abs/2510.25271)
*Sabrine Aroua,Christos Anastasios Bovolis,Bo Göransson,Anastasios Giovanidis,Mathieu Leconte,Apostolos Destounis*

Main category: cs.NI

TL;DR: 提出一种混合强化学习框架，用于自适应SSB码本设计，通过结合专家知识和实时反馈来动态调整波束配置，在5G毫米波系统中提升用户连接性能。


<details>
  <summary>Details</summary>
Motivation: 当前网络中由专家设计的SSB码本在动态或异构环境中缺乏灵活性，限制了在用户分布变化时的整体有效性。

Method: 构建混合强化学习框架，基于专家知识池，RL代理根据实时反馈自适应选择或组合SSB波束，无需显式用户位置信息，同时满足实际波束约束。

Result: 仿真结果显示，相比静态专家配置，该方法平均提升用户连接性10.8%。

Conclusion: 结合专家知识与数据驱动优化，能够实现下一代无线网络中更智能、灵活和弹性的波束管理。

Abstract: Initial access (IA) is the process by which user equipment (UE) establishes
its first connection with a base station. In 5G systems, particularly at
millimeter-wave frequencies, IA integrates beam management to support highly
directional transmissions. The base station employs a codebook of beams for the
transmission of Synchronization Signal Blocks (SSBs), which are periodically
swept to detect and connect users. The design of this SSB codebook is critical
for ensuring reliable, wide-area coverage. In current networks, SSB codebooks
are meticulously engineered by domain experts. While these expert-defined
codebooks provide a robust baseline, they lack flexibility in dynamic or
heterogeneous environments where user distributions vary, limiting their
overall effectiveness. This paper proposes a hybrid Reinforcement Learning (RL)
framework for adaptive SSB codebook design. Building on top of expert
knowledge, the RL agent leverages a pool of expert-designed SSB beams and
learns to adaptively select or combine them based on real-time feedback. This
enables the agent to dynamically tailor codebooks to the actual environment,
without requiring explicit user location information, while always respecting
practical beam constraints. Simulation results demonstrate that, on average,
the proposed approach improves user connectivity by 10.8$\%$ compared to static
expert configurations. These findings highlight the potential of combining
expert knowledge with data-driven optimization to achieve more intelligent,
flexible, and resilient beam management in next-generation wireless networks.

</details>


### [6] [Deep Reinforcement Learning-Based Cooperative Rate Splitting for Satellite-to-Underground Communication Networks](https://arxiv.org/abs/2510.25562)
*Kaiqiang Lin,Kangchun Zhao,Yijie Mao*

Main category: cs.NI

TL;DR: 提出一种基于协作速率分割的新型传输框架，通过地上中继转发公共流到地下设备，解决了卫星-地下网络中下行通信的信号衰减问题。


<details>
  <summary>Details</summary>
Motivation: 卫星到地下网络的可靠下行通信面临严重挑战，主要由于地下土壤和空气-土壤界面折射造成的严重信号衰减。

Method: 开发了基于近端策略优化算法的深度强化学习解决方案框架，包含分布感知动作建模和多分支行动者网络，联合优化功率分配、消息分割和时隙调度。

Result: 在现实地下管道监测场景下的仿真结果表明，该方法在各种地下条件和设备数量下，相比传统基准策略实现了平均最大最小速率增益超过167%。

Conclusion: 所提出的协作速率分割辅助传输框架能有效提升卫星-地下网络的通信可靠性，显著改善地下设备的最小可达速率。

Abstract: Reliable downlink communication in satellite-to-underground networks remains
challenging due to severe signal attenuation caused by underground soil and
refraction in the air-soil interface. To address this, we propose a novel
cooperative rate-splitting (CRS)-aided transmission framework, where an
aboveground relay decodes and forwards the common stream to underground devices
(UDs). Based on this framework, we formulate a max-min fairness optimization
problem that jointly optimizes power allocation, message splitting, and time
slot scheduling to maximize the minimum achievable rate across UDs. To solve
this high-dimensional non-convex problem under uncertain channels, we develop a
deep reinforcement learning solution framework based on the proximal policy
optimization (PPO) algorithm that integrates distribution-aware action modeling
and a multi-branch actor network. Simulation results under a realistic
underground pipeline monitoring scenario demonstrate that the proposed approach
achieves average max-min rate gains exceeding $167\%$ over conventional
benchmark strategies across various numbers of UDs and underground conditions.

</details>


### [7] [TCP ROCCET: An RTT-Oriented CUBIC Congestion Control Extension for 5G and Beyond Networks](https://arxiv.org/abs/2510.25281)
*Lukas Prause,Mark Akselrod*

Main category: cs.NI

TL;DR: TCP ROCCET是一个基于延迟的TCP CUBIC扩展，通过在丢包之外还响应往返时间来缓解蜂窝网络中的缓冲区膨胀问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于丢包的TCP拥塞控制算法（如TCP CUBIC）在现代蜂窝网络中面临挑战，由于RLC层大缓冲区导致缺乏丢包和缓冲区膨胀问题。

Method: 提出了TCP ROCCET，这是TCP CUBIC的基于延迟的扩展，除了响应丢包外还基于往返时间响应网络拥塞。

Result: TCP ROCCET相比标准CUBIC实现能降低延迟和缓冲区膨胀，与TCP BBRv3相比提供相似吞吐量但保持更低总体延迟。

Conclusion: 在真实5G网络中的评估证实ROCCET在不同条件下对网络拥塞的响应有所改善。

Abstract: The behavior of loss-based TCP congestion control algorithms like TCP CUBIC
continues to be a challenge in modern cellular networks. Due to the large RLC
layer buffers required to deal with short-term changes in channel capacity, the
behavior of both the Slow Start and congestion avoidance phases may be heavily
impacted by the lack of packet losses and the resulting bufferbloat. While
existing congestion control algorithms like TCP BBR do tend to perform better
even in the presence of large bottleneck buffers, they still tend to fill the
buffer more than necessary and can have fairness issues when compared to
loss-based algorithms.
  In this paper, we analyze the issues with the use of loss-based congestion
control algorithms by analyzing TCP CUBIC, which is currently the most popular
variant. To mitigate the issues experienced by TCP CUBIC in cellular networks,
we introduce TCP ROCCET, a latency-based extension of TCP CUBIC that responds
to network congestion based on round-trip time in addition to packet loss.
  Our findings show that TCP ROCCET can reduce latency and bufferbloat compared
to the standard CUBIC implementation, without requiring a specific network
architecture. Compared to TCP BBRv3, ROCCET offers similar throughput while
maintaining lower overall latency. The evaluation was conducted in real 5G
networks, including both stationary and mobile scenarios, confirming ROCCET's
improved response to network congestion under varying conditions.

</details>


### [8] [Energy consumption assessment of a Virtual Reality Remote Rendering application over 5G networks](https://arxiv.org/abs/2510.25357)
*Roberto Viola,Mikel Irazola,José Ramón Juárez,Minh Nguyen,Alexander Zoubarev,Alexander Futasz,Louay Bassbouss,Amr A. AbdelNabi,Javier Fernández Hidalgo*

Main category: cs.NI

TL;DR: 本文研究了在真实5G测试环境中VR应用远程渲染的能耗影响，提出了硬件和软件两种能耗监控方案，评估了不同配置下VR远程渲染器的能耗表现。


<details>
  <summary>Details</summary>
Motivation: 远程渲染让轻量设备能够通过将计算密集型任务卸载到云端来访问高性能图形内容，但这种方法引发了包括远程计算节点、5G核心网、无线接入网和用户设备在内的各网络组件能耗担忧。

Method: 提出并评估了两种互补的能耗监控解决方案（硬件基和软件基），使用基于MoQ协议的VR远程渲染器作为测试案例，在不同多媒体和网络配置下评估其能耗足迹。

Result: 结果提供了关于在5G环境中运行的真实VR应用能耗与性能之间权衡的关键见解。

Conclusion: 该研究为理解VR远程渲染在5G网络中的能耗特性提供了重要参考，揭示了能耗与性能之间的平衡关系。

Abstract: This paper investigates the energy implications of remote rendering for
Virtual Reality (VR) applications within a real 5G testbed. Remote rendering
enables lightweight devices to access high-performance graphical content by
offloading computationally intensive tasks to Cloud-native Network Functions
(CNFs) running on remote servers. However, this approach raises concerns
regarding energy consumption across the various network components involved,
including the remote computing node, the 5G Core, the Radio Access Network
(RAN), and the User Equipment (UE). This work proposes and evaluates two
complementary energy monitoring solutions, one hardware-based and one
software-based, to measure energy consumption at different system levels. A VR
remote renderer, deployed as CNF and leveraging the Media over QUIC (MoQ)
protocol, is used as test case for assessing its energy footprint under
different multimedia and network configurations. The results provide critical
insights into the trade-off between energy consumption and performance of a
real-world VR application running in a 5G environment.

</details>


### [9] [Evaluating Learning Congestion control Schemes for LEO Constellations](https://arxiv.org/abs/2510.25498)
*Mihai Mazilu,Aiden Valentine,George Parisis*

Main category: cs.NI

TL;DR: 本文首次通过仿真驱动的评估方法，系统分析了低地球轨道(LEO)卫星网络中各类拥塞控制方案的性能，揭示了现有方案在动态LEO环境中的关键局限性。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络由于频繁切换、快速变化的RTT和非拥塞性丢包等特性，给拥塞控制带来独特挑战，需要专门评估现有方案在LEO环境下的表现。

Method: 结合LeoEM框架的轨道动力学模拟和Mininet微基准测试，评估了三类代表性CC算法：基于丢包的(Cubic、SaTCP)、基于模型的(BBRv3)和基于学习的(Vivace、Sage、Astraea)，涵盖单流和多流场景。

Result: 发现：切换感知的丢包方案能回收带宽但增加延迟；BBRv3维持高吞吐量但对RTT突变反应慢；RL方案在动态条件下表现差但对非拥塞丢包有抵抗力；RTT不对称和多瓶颈时公平性显著下降；AQM能恢复公平性和提升效率。

Conclusion: 当前CC方案在LEO网络中存在严重局限，这些结果为设计LEO专用数据传输协议提供了重要见解。

Abstract: Low Earth Orbit (LEO) satellite networks introduce unique congestion control
(CC) challenges due to frequent handovers, rapidly changing round-trip times
(RTTs), and non-congestive loss. This paper presents the first comprehensive,
emulation-driven evaluation of CC schemes in LEO networks, combining realistic
orbital dynamics via the LeoEM framework with targeted Mininet
micro-benchmarks. We evaluated representative CC algorithms from three classes,
loss-based (Cubic, SaTCP), model-based (BBRv3), and learning-based (Vivace,
Sage, Astraea), across diverse single-flow and multi-flow scenarios, including
interactions with active queue management (AQM). Our findings reveal that: (1)
handover-aware loss-based schemes can reclaim bandwidth but at the cost of
increased latency; (2) BBRv3 sustains high throughput with modest delay
penalties, yet reacts slowly to abrupt RTT changes; (3) RL-based schemes
severely underperform under dynamic conditions, despite being notably resistant
to non-congestive loss; (4) fairness degrades significantly with RTT asymmetry
and multiple bottlenecks, especially in human-designed CC schemes; and (5) AQM
at bottlenecks can restore fairness and boost efficiency. These results expose
critical limitations in current CC schemes and provide insight for designing
LEO-specific data transport protocols.

</details>


### [10] [Device to Device Pairs Sharding based on Distance](https://arxiv.org/abs/2510.25552)
*K Prajwal,Tharun K,Navaneeth P,Ishwar Mandal,Kiran M*

Main category: cs.NI

TL;DR: 该论文开发了一个基于距离标准使用K-means聚类方法实现设备到设备(D2D)通信的模型，允许终端用户直接通信而不通过基站。


<details>
  <summary>Details</summary>
Motivation: 传统蜂窝系统中所有通信必须通过基站，无法满足用户对多媒体数据交换、快速服务、低延迟、高数据速率等日益增长的需求。有限的频谱资源需要灵活使用来应对这些需求。

Method: 使用K-means聚类方法基于距离标准对用户设备(UEs)和蜂窝用户(CUs)进行分片，建立不涉及eNB的D2D通信模型。

Result: 开发了一个能够实现终端用户间直接D2D通信的模型，通过距离聚类优化了通信效率。

Conclusion: D2D通信是满足下一代网络高数据速率需求的重要促进因素，该模型为灵活使用有限频谱资源提供了有效解决方案。

Abstract: In the conventional cellular system, devices are not allowed to communicate
directly with each other in the licensed cellular bandwidth and all
communications take place through the base stations. The users requirements has
led the technology to become fast and faster. Multimedia rich data exchange,
fast service, high quality voice calls, newer and more demanding applications,
information at fingertips, everything requires technology and communication
between devices. A constant need to increase network capacity for meeting the
users growing demands has led to the growth of cellular communication networks
from the first generation(1G) to the fifth generation(5G). There will be crores
of connected devices in the coming future . A large number of connections are
going to be heterogeneous, demanding lesser delays, higher data rates, superior
throughput and enhanced system capacity. The available spectrum resources are
limited and has to be flexibly used by mobile network operators to cope with
the rising demands. An emerging facilitator of the upcoming high data rate
demanding next-generation networks are device-to-device(D2D) communication.
This paper has developed a model that establishes Device-to-Device (D2D)
communication between two end-users without involving the eNB (evolved Node B).
We have sharded the UEs and CUs based on the criteria of DISTANCE. To do so, we
used the K-means clustering method.

</details>


### [11] [MetaLore: Learning to Orchestrate Communication and Computation for Metaverse Synchronization](https://arxiv.org/abs/2510.25705)
*Elif Ebru Ohri,Qi Liao,Anastasios Giovanidis,Francesca Fossati,Nour-El-Houda Yellas*

Main category: cs.NI

TL;DR: MetaLore是一个基于深度强化学习的框架，用于元宇宙或数字孪生环境中的联合通信和计算资源分配，通过引入新的信息年龄指标来优化同步质量。


<details>
  <summary>Details</summary>
Motivation: 随着增强现实和虚拟现实的发展，物理和数字领域之间的无缝同步仍然是一个关键挑战，特别是在实时应用中，延迟会影响用户体验。

Method: 提出MetaLore框架，使用深度强化学习动态分配通信带宽和计算资源，引入Age of Request Information (AoRI)和Age of Sensor Information (AoSI)两个新指标，并集成到奖励函数中。

Result: 深度强化学习解决方案通过使用网络侧两个队列长度的小型任务导向观察空间，实现了与完全枚举暴力解决方案相当的性能，并能有效自主适应动态流量条件。

Conclusion: MetaLore框架能够有效优化元宇宙环境中的资源分配和同步质量，满足端到端延迟保证，并通过开源模拟器验证了方法的有效性。

Abstract: As augmented and virtual reality evolve, achieving seamless synchronization
between physical and digital realms remains a critical challenge, especially
for real-time applications where delays affect the user experience. This paper
presents MetaLore, a Deep Reinforcement Learning (DRL) based framework for
joint communication and computational resource allocation in Metaverse or
digital twin environments. MetaLore dynamically shares the communication
bandwidth and computational resources among sensors and mobile devices to
optimize synchronization, while offering high throughput performance. Special
treatment is given in satisfying end-to-end delay guarantees. A key
contribution is the introduction of two novel Age of Information (AoI) metrics:
Age of Request Information (AoRI) and Age of Sensor Information (AoSI),
integrated into the reward function to enhance synchronization quality. An open
source simulator has been extended to incorporate and evaluate the approach.
The DRL solution is shown to achieve the performance of full-enumeration
brute-force solutions by making use of a small, task-oriented observation space
of two queue lengths at the network side. This allows the DRL approach the
flexibility to effectively and autonomously adapt to dynamic traffic
conditions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Scheduling Your LLM Reinforcement Learning with Reasoning Trees](https://arxiv.org/abs/2510.24832)
*Hong Wang,Zhezheng Hao,Jian Luo,Chenxing Wei,Yao Shu,Lei Liu,Qiang Lin,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: 提出了一种基于推理树结构的学习难度度量方法(r-score)和相应的数据调度算法Re-Schedule，用于改进RLVR训练中的数据调度效率。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR数据调度方法通常依赖基于路径的指标来排序查询，忽略了这些查询的推理树结构。

Method: 引入推理分数(r-score)来基于推理树结构衡量查询的学习难度，并基于此提出Re-Schedule调度算法，构建从结构简单到复杂的课程学习顺序。

Result: 在六个数学推理基准测试中，Re-Schedule显著提高了平均准确率，最高增益达3.2%。

Conclusion: 对推理树的结构理解为RLVR数据调度提供了更强大和原则性的基础。

Abstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large
Language Models (LLMs) can be conceptualized as progressively editing a query's
`Reasoning Tree'. This process involves exploring nodes (tokens) and
dynamically modifying the model's policy at each node. When combined with data
scheduling, this process yields further gains in data efficiency and accuracy.
However, existing RLVR data scheduling methods typically rely on path-based
metrics to rank queries, overlooking the reasoning tree structures of these
queries. In this paper, we introduce a novel metric, namely Reasoning Score
(r-score), which measures the query's learning difficulty based on the
structure of its reasoning tree. Based on the r-score, we propose the Reasoning
Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a
curriculum progressing from structurally simple (high r-score) to complex (low
r-score) queries. Experiments on six math-reasoning benchmarks show that
Re-Schedule significantly improves average accuracy, achieving gains of up to
3.2%. These strong results validate our approach and demonstrate that a
structural understanding of the reasoning tree provides a more powerful and
principled foundation for RLVR data scheduling.

</details>


### [13] [Cyclic Counterfactuals under Shift-Scale Interventions](https://arxiv.org/abs/2510.25005)
*Saptarshi Saha,Dhruv Vansraj Rathore,Utpal Garain*

Main category: cs.AI

TL;DR: 研究循环结构因果模型中的反事实推理，关注移位-尺度干预下的政策式变化


<details>
  <summary>Details</summary>
Motivation: 传统反事实推理框架假设无环结构因果模型，但现实系统（如生物系统）常包含反馈循环或循环依赖，违反无环性假设

Method: 研究循环结构因果模型中的反事实推理，特别关注移位-尺度干预（即对变量机制进行重新缩放和/或移位的软性、政策式变化）

Result: 未在摘要中明确说明具体结果

Conclusion: 需要研究循环结构因果模型中的反事实推理，以处理现实系统中常见的反馈循环和循环依赖问题

Abstract: Most counterfactual inference frameworks traditionally assume acyclic
structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,
many real-world systems (e.g. biological systems) contain feedback loops or
cyclic dependencies that violate acyclicity. In this work, we study
counterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,
soft, policy-style changes that rescale and/or shift a variable's mechanism.

</details>


### [14] [Taming the Real-world Complexities in CPT E/M Coding with Large Language Models](https://arxiv.org/abs/2510.25007)
*Islam Nassar,Yang Lin,Yuan Jin,Rongxin Zhu,Chang Wei Tan,Zenan Zhai,Nitika Mathur,Thanh Tien Vu,Xu Zhong,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: 本文提出了ProFees框架，基于大语言模型自动化医疗评估与管理编码，在真实数据集上比商业系统提升36%准确率


<details>
  <summary>Details</summary>
Motivation: 自动化E/M编码可以减轻医生文档负担，提高计费效率，但现实世界的复杂性使这项任务充满挑战

Method: 开发了基于大语言模型的ProFees框架，专门处理E/M编码中的复杂性问题

Result: 在专家策划的真实数据集上，ProFees比商业CPT E/M编码系统准确率提高36%，比最强单提示基线提高近5%

Conclusion: ProFees框架有效解决了E/M编码自动化中的现实复杂性，展示了其在实际应用中的有效性

Abstract: Evaluation and Management (E/M) coding, under the Current Procedural
Terminology (CPT) taxonomy, documents medical services provided to patients by
physicians. Used primarily for billing purposes, it is in physicians' best
interest to provide accurate CPT E/M codes. %While important, it is an
auxiliary task that adds to physicians' documentation burden. Automating this
coding task will help alleviate physicians' documentation burden, improve
billing efficiency, and ultimately enable better patient care. However, a
number of real-world complexities have made E/M encoding automation a
challenging task. In this paper, we elaborate some of the key complexities and
present ProFees, our LLM-based framework that tackles them, followed by a
systematic evaluation. On an expert-curated real-world dataset, ProFees
achieves an increase in coding accuracy of more than 36\% over a commercial CPT
E/M coding system and almost 5\% over our strongest single-prompt baseline,
demonstrating its effectiveness in addressing the real-world complexities.

</details>


### [15] [Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading](https://arxiv.org/abs/2510.25014)
*Minkyung Kim,Junsik Kim,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 提出了Autoregressive State-Tracking Prompting (ASTP)方法，通过显式状态跟踪和占位符后处理，解决LLM在游戏交易系统中无法遵循规则流程的问题，实现>99%的状态合规性和99.3%的计算精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在动态游戏交互中表现灵活，但无法遵循规则化交易系统的必要流程（浏览-报价-审核-确认），这会损害玩家信任。需要解决LLM的创造性灵活性与游戏交易程序性要求之间的核心矛盾。

Method: 引入ASTP方法，通过精心设计的提示策略强制LLM使其状态跟踪过程显式化和可验证。LLM需要识别并报告前一轮的预定义状态标签。配合状态特定的占位符后处理方法确保交易完整性。

Result: 在300个交易对话评估中，实现了>99%的状态合规性和99.3%的计算精度。ASTP配合占位符后处理在较小模型(Gemini-2.5-Flash)上能达到较大模型(Gemini-2.5-Pro)的性能，同时将响应时间从21.2秒减少到2.4秒。

Conclusion: ASTP建立了一个实用的基础框架，既能满足商业游戏的实时性要求，又能适应资源限制，为LLM在规则化交易系统中的可靠应用提供了解决方案。

Abstract: Large Language Models (LLMs) enable dynamic game interactions but fail to
follow essential procedural flows in rule-governed trading systems, eroding
player trust. This work resolves the core tension between the creative
flexibility of LLMs and the procedural demands of in-game trading
(browse-offer-review-confirm). To this end, Autoregressive State-Tracking
Prompting (ASTP) is introduced, a methodology centered on a strategically
orchestrated prompt that compels an LLM to make its state-tracking process
explicit and verifiable. Instead of relying on implicit contextual
understanding, ASTP tasks the LLM with identifying and reporting a predefined
state label from the previous turn. To ensure transactional integrity, this is
complemented by a state-specific placeholder post-processing method for
accurate price calculations. Evaluation across 300 trading dialogues
demonstrates >99% state compliance and 99.3% calculation precision. Notably,
ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)
matches larger models' (Gemini-2.5-Pro) performance while reducing response
time from 21.2s to 2.4s, establishing a practical foundation that satisfies
both real-time requirements and resource constraints of commercial games.

</details>


### [16] [Reasoning-Aware GRPO using Process Mining](https://arxiv.org/abs/2510.25065)
*Taekhyun Park,Yongjae Lee,Hyerim Bae*

Main category: cs.AI

TL;DR: PM4GRPO是一种基于过程挖掘的推理感知组相对策略优化方法，通过增强标准答案/格式奖励，在强化学习后训练中提升大型推理模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的后训练奖励方案通常只关注结果，缺乏对推理过程的考虑，限制了大型推理模型的多步推理能力发展。

Method: 利用过程挖掘技术计算一致性奖励，衡量策略模型的推理过程与预训练教师模型的匹配程度，将过程信号与标准答案/格式奖励结合。

Result: 在五个基准测试上的实证结果表明，PM4GRPO显著优于现有的GRPO后训练方法。

Conclusion: 利用过程挖掘进行推理感知的GRPO能够有效增强策略模型的推理能力。

Abstract: Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.

</details>


### [17] [H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts](https://arxiv.org/abs/2510.25091)
*Peilin Tan,Liang Xie,Churan Zhi,Dian Tu,Chuanqi Shi*

Main category: cs.AI

TL;DR: H3M-SSMoEs是一个用于股票预测的新型超图多模态架构，结合了LLM推理和风格结构化专家混合，通过多上下文超图、LLM增强推理和专业化专家系统，在多个股票市场实现了优越的预测精度和投资表现。


<details>
  <summary>Details</summary>
Motivation: 股票预测面临复杂的时间依赖、异构模态和动态变化的股票间关系等挑战，现有方法难以在可扩展框架内统一结构、语义和机制自适应建模。

Method: 提出H3M-SSMoEs架构，包含三个核心创新：1）多上下文多模态超图，通过局部和全局上下文超图分层捕获时空动态；2）LLM增强推理模块，利用冻结大语言模型融合定量和文本模态；3）风格结构化专家混合，结合共享市场专家和行业专业专家。

Result: 在三个主要股票市场的广泛实验表明，H3M-SSMoEs在预测精度和投资表现上均超越了最先进方法，同时展现出有效的风险控制能力。

Conclusion: 该工作提出的统一框架成功解决了股票预测中的关键挑战，为金融时序分析提供了新的研究方向，相关数据集、源代码和模型权重已开源。

Abstract: Stock movement prediction remains fundamentally challenging due to complex
temporal dependencies, heterogeneous modalities, and dynamically evolving
inter-stock relationships. Existing approaches often fail to unify structural,
semantic, and regime-adaptive modeling within a scalable framework. This work
introduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with
LLM reasoning and Style-Structured Mixture of Experts, integrating three key
innovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically
captures fine-grained spatiotemporal dynamics via a Local Context Hypergraph
(LCH) and persistent inter-stock dependencies through a Global Context
Hypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon
Divergence weighting mechanism for adaptive relational learning and cross-modal
alignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large
language model with lightweight adapters to semantically fuse and align
quantitative and textual modalities, enriching representations with
domain-specific financial knowledge; and (3) a Style-Structured Mixture of
Experts (SSMoEs) that combines shared market experts and industry-specialized
experts, each parameterized by learnable style vectors enabling regime-aware
specialization under sparse activation. Extensive experiments on three major
stock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in
both superior predictive accuracy and investment performance, while exhibiting
effective risk control. Datasets, source code, and model weights are available
at our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.

</details>


### [18] [KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA](https://arxiv.org/abs/2510.25101)
*Zhuo Chen,Fei Wang,Zixuan Li,Zhao Zhang,Weiwei Ding,Chuanguang Yang,Yongjun Xu,Xiaolong Jin,Jiafeng Guo*

Main category: cs.AI

TL;DR: 提出了KnowCoder-A1，一种基于多阶段课程强化学习的LLM，能够在知识库上进行自主推理，仅使用结果监督而非过程监督，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的KBQA方法通常通过过程监督微调LLMs，但这种方法对探索的激励较弱，无法有效增强代理推理能力。

Method: 采用多阶段课程强化学习，首先在高质量轨迹上微调LLM建立基础能力，然后应用从易到难的奖励调度来缓解结果监督中的奖励稀疏问题。

Result: 在三个主流数据集上持续优于先前方法，在GrailQA的零样本子集上实现了11.1%的相对改进，仅使用十二分之一的训练数据。

Conclusion: 仅使用结果监督训练的KnowCoder-A1展现出强大的推理行为，证明了其在知识库问答中的有效代理推理能力。

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language
questions over a structured Knowledge Base (KB). Recent work improves KBQA by
adopting an agentic reasoning paradigm, in which Large Language Models (LLMs)
iteratively decompose a question, generate its corresponding logical queries,
and interact with the KB to derive the answer. However, these methods typically
fine-tune LLMs on reasoning trajectories synthesized via process supervision,
which offers weak incentives for exploration and thus fails to strengthen the
agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that
can autonomously perform agentic reasoning on KBs to obtain answers. To
incentivize autonomous exploration, KnowCoder-A1 trains the LLM under
outcome-only supervision via a multi-stage curriculum reinforcement learning
with an easy-to-hard curriculum. To establish foundational agentic
capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of
high-quality trajectories obtained through outcome-based rejection sampling.
Then, to alleviate the reward sparsity inherent in outcome-only supervision, it
applies multi-stage curriculum RL with reward schedules that progress from easy
to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful
reasoning behaviors and consistently outperforms prior approaches across three
mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1
achieves up to an 11.1% relative improvement while using only one-twelfth of
the training data, demonstrating strong agentic reasoning capabilities.

</details>


### [19] [Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models](https://arxiv.org/abs/2510.25179)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.AI

TL;DR: 提出了Agentic Moderation框架，利用专门代理来防御多模态系统的越狱攻击，相比静态方法提供动态、协作和可解释的审核


<details>
  <summary>Details</summary>
Motivation: 将代理方法扩展到安全对齐领域，解决现有方法只能提供静态、二元分类的局限性，实现上下文感知和可解释的审核

Method: 引入动态协作代理框架，包括Shield、Responder、Evaluator和Reflector等专门代理，实现模型无关的多模态系统安全防御

Result: 在5个数据集和4个大型视觉语言模型上的实验显示，攻击成功率降低7-19%，拒绝率提高4-20%，保持稳定的非跟随率

Conclusion: Agentic Moderation通过代理架构的灵活性和推理能力，提供了模块化、可扩展和细粒度的安全执行，展示了代理系统作为自动化安全治理基础的潜力

Abstract: Agentic methods have emerged as a powerful and autonomous paradigm that
enhances reasoning, collaboration, and adaptive control, enabling systems to
coordinate and independently solve complex tasks. We extend this paradigm to
safety alignment by introducing Agentic Moderation, a model-agnostic framework
that leverages specialised agents to defend multimodal systems against
jailbreak attacks. Unlike prior approaches that apply as a static layer over
inputs or outputs and provide only binary classifications (safe or unsafe), our
method integrates dynamic, cooperative agents, including Shield, Responder,
Evaluator, and Reflector, to achieve context-aware and interpretable
moderation. Extensive experiments across five datasets and four representative
Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the
Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),
and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,
and well-balanced safety performance. By harnessing the flexibility and
reasoning capacity of agentic architectures, Agentic Moderation provides
modular, scalable, and fine-grained safety enforcement, highlighting the
broader potential of agentic systems as a foundation for automated safety
governance.

</details>


### [20] [Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision](https://arxiv.org/abs/2510.25205)
*Yuyang Xia,Zibo Liang,Liwei Deng,Yan Zhao,Han Su,Kai Zheng*

Main category: cs.AI

TL;DR: 提出EneAD框架，通过自适应感知模块和鲁棒决策模块，在保持感知精度的同时显著降低自动驾驶系统的能耗，提升电动车的续航里程。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术带来社会经济效益，但感知计算能耗高限制了电动车续航。现有模型压缩方法要么模型尺寸大，要么感知精度下降明显。

Method: 1. 自适应感知模块：管理多个不同计算消耗的感知模型，动态调整执行帧率；基于贝叶斯优化的可迁移调优方法；轻量级分类模型区分场景感知难度。2. 鲁棒决策模块：基于强化学习的决策模型，设计正则化项增强面对扰动感知结果的驾驶稳定性。

Result: EneAD能将感知能耗降低1.9倍到3.5倍，从而将驾驶里程提升3.9%到8.5%。

Conclusion: EneAD框架在能耗和驾驶性能方面均表现出优越性，为自动驾驶系统提供了有效的能效优化解决方案。

Abstract: Autonomous driving is an emerging technology that is expected to bring
significant social, economic, and environmental benefits. However, these
benefits come with rising energy consumption by computation engines, limiting
the driving range of vehicles, especially electric ones. Perception computing
is typically the most power-intensive component, as it relies on largescale
deep learning models to extract environmental features. Recently, numerous
studies have employed model compression techniques, such as sparsification,
quantization, and distillation, to reduce computational consumption. However,
these methods often result in either a substantial model size or a significant
drop in perception accuracy compared to high-computation models. To address
these challenges, we propose an energy-efficient autonomous driving framework,
called EneAD. In the adaptive perception module, a perception optimization
strategy is designed from the perspective of data management and tuning.
Firstly, we manage multiple perception models with different computational
consumption and adjust the execution framerate dynamically. Then, we define
them as knobs and design a transferable tuning method based on Bayesian
optimization to identify promising knob values that achieve low computation
while maintaining desired accuracy. To adaptively switch the knob values in
various traffic scenarios, a lightweight classification model is proposed to
distinguish the perception difficulty in different scenarios. In the robust
decision module, we propose a decision model based on reinforcement learning
and design a regularization term to enhance driving stability in the face of
perturbed perception results. Extensive experiments evidence the superiority of
our framework in both energy consumption and driving performance. EneAD can
reduce perception consumption by 1.9x to 3.5x and thus improve driving range by
3.9% to 8.5%

</details>


### [21] [RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models](https://arxiv.org/abs/2510.25206)
*Tianqianjin Lin,Xi Zhao,Xingyao Zhang,Rujiao Long,Yi Xu,Zhuoren Jiang,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 本文提出RAVR框架，利用答案引导的推理作为变分代理，通过条件化答案来提升语言模型生成高质量推理路径的能力，解决了在模型能力不足时强化学习难以有效改进推理的问题。


<details>
  <summary>Details</summary>
Motivation: 当语言模型在当前任务上能力不足时，很难采样到高质量的推理路径，强化学习容易强化熟悉但次优的推理。受认知科学启发，"为什么这是答案"比"答案是什么"更容易回答，因为避免了开放式探索的认知负担，转而进行解释性重建。

Method: 提出RAVR（参考答案引导的变分推理）框架，将答案条件化的推理作为问题单独推理的变分代理，通过条件化答案来推导高质量推理路径。

Result: 在通用和数学领域的实验中，RAVR相比强基线方法取得了持续改进，分析表明RAVR减少了犹豫、加强了结论整合，并促进了问题特定策略的推理。

Conclusion: 条件化答案可以显著提高采样推理路径的期望效用，将难以处理的问题转化为可学习的问题，为语言模型的推理能力提升提供了有效途径。

Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large
language models (LLMs), but critically depends on a key prerequisite: the LLM
can already generate high-utility reasoning paths with non-negligible
probability. For tasks beyond the LLM's current competence, such reasoning path
can be hard to sample, and learning risks reinforcing familiar but suboptimal
reasoning. We are motivated by the insight from cognitive science that Why is
this the answer is often an easier question than What is the answer, as it
avoids the heavy cognitive load of open-ended exploration, opting instead for
explanatory reconstruction-systematically retracing the reasoning that links a
question to its answer. We show that LLMs can similarly leverage answers to
derive high-quality reasoning paths. We formalize this phenomenon and prove
that conditioning on answer provably increases the expected utility of sampled
reasoning paths, thereby transforming intractable problems into learnable ones.
Building on this insight, we introduce RAVR (Reference-Answer-guided
Variational Reasoning), an end-to-end framework that uses answer-conditioned
reasoning as a variational surrogate for question-only reasoning. Experiments
in both general and math domains demonstrate consistent improvements over
strong baselines. We further analyze the reasoning behavior and find that RAVR
reduces hesitation, strengthens conclusion consolidation, and promotes
problem-specific strategies in reasoning.

</details>


### [22] [FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data](https://arxiv.org/abs/2510.25223)
*Kun ouyang,Haoyu Wang,Dong Fang*

Main category: cs.AI

TL;DR: FELA是一个基于LLM的多代理进化系统，能够从复杂的工业事件日志数据中自主提取有意义且高性能的特征。


<details>
  <summary>Details</summary>
Motivation: 工业事件日志数据复杂且异构，现有自动特征工程方法存在可解释性差、操作僵化、适应性弱等问题。

Method: 采用多代理系统（想法代理、代码代理、批评代理、评估代理）协作生成、验证和实施特征想法，结合强化学习和遗传算法实现代理进化。

Result: 在真实工业数据集上的实验表明，FELA能够生成可解释、领域相关的特征，显著提升模型性能并减少人工工作量。

Conclusion: 基于LLM的多代理系统有潜力成为复杂现实环境中自动化、可解释且自适应的特征工程通用框架。

Abstract: Event log data, recording fine-grained user actions and system events,
represent one of the most valuable assets for modern digital services. However,
the complexity and heterogeneity of industrial event logs--characterized by
large scale, high dimensionality, diverse data types, and intricate temporal or
relational structures--make feature engineering extremely challenging. Existing
automatic feature engineering approaches, such as AutoML or genetic methods,
often suffer from limited explainability, rigid predefined operations, and poor
adaptability to complicated heterogeneous data. In this paper, we propose FELA
(Feature Engineering LLM Agents), a multi-agent evolutionary system that
autonomously extracts meaningful and high-performing features from complex
industrial event log data. FELA integrates the reasoning and coding
capabilities of large language models (LLMs) with an insight-guided
self-evolution paradigm. Specifically, FELA employs specialized agents--Idea
Agents, Code Agents, and Critic Agents--to collaboratively generate, validate,
and implement novel feature ideas. An Evaluation Agent summarizes feedback and
updates a hierarchical knowledge base and dual-memory system to enable
continual improvement. Moreover, FELA introduces an agentic evolution
algorithm, combining reinforcement learning and genetic algorithm principles to
balance exploration and exploitation across the idea space. Extensive
experiments on real industrial datasets demonstrate that FELA can generate
explainable, domain-relevant features that significantly improve model
performance while reducing manual effort. Our results highlight the potential
of LLM-based multi-agent systems as a general framework for automated,
interpretable, and adaptive feature engineering in complex real-world
environments.

</details>


### [23] [From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity](https://arxiv.org/abs/2510.25232)
*Tianxi Wan,Jiaming Luo,Siyuan Chen,Kunyao Lan,Jianhua Chen,Haiyang Geng,Mengyue Wu*

Main category: cs.AI

TL;DR: 开发了PsyCoTalk——首个支持共病的大规模对话数据集，包含3000个多轮诊断对话，通过合成EMR和多智能体诊断对话生成来应对精神病共病的临床挑战。


<details>
  <summary>Details</summary>
Motivation: 精神病共病在临床上很重要但具有挑战性，因为多个共病障碍的复杂性使得诊断和治疗变得困难。

Method: 整合合成患者电子病历构建和多智能体诊断对话生成，创建502个合成EMR，并将临床访谈协议转化为分层状态机和上下文树，支持130多个诊断状态。

Result: 构建了PsyCoTalk数据集，与真实临床记录相比在对话长度、标记分布和诊断推理策略方面具有高结构性和语言保真度，精神科医生确认了对话的真实性和诊断有效性。

Conclusion: 该数据集提高了诊断准确性和治疗规划，为精神病共病研究提供了宝贵资源，并支持在单次对话中开发和评估多障碍精神病筛查模型。

Abstract: Psychiatric comorbidity is clinically significant yet challenging due to the
complexity of multiple co-occurring disorders. To address this, we develop a
novel approach integrating synthetic patient electronic medical record (EMR)
construction and multi-agent diagnostic dialogue generation. We create 502
synthetic EMRs for common comorbid conditions using a pipeline that ensures
clinical relevance and diversity. Our multi-agent framework transfers the
clinical interview protocol into a hierarchical state machine and context tree,
supporting over 130 diagnostic states while maintaining clinical standards.
Through this rigorous process, we construct PsyCoTalk, the first large-scale
dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic
dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy
and treatment planning, offering a valuable resource for psychiatric
comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk
exhibits high structural and linguistic fidelity in terms of dialogue length,
token distribution, and diagnostic reasoning strategies. Licensed psychiatrists
confirm the realism and diagnostic validity of the dialogues. This dataset
enables the development and evaluation of models capable of multi-disorder
psychiatric screening in a single conversational pass.

</details>


### [24] [GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning](https://arxiv.org/abs/2510.25320)
*Jiaqi Wu,Qinlao Zhao,Zefeng Chen,Kai Qin,Yifei Zhao,Xueqian Wang,Yuhang Yao*

Main category: cs.AI

TL;DR: GAP框架通过图规划实现工具并行执行，显著提升多步推理任务的效率和准确性


<details>
  <summary>Details</summary>
Motivation: 现有方法如ReAct依赖顺序推理，无法利用独立子任务的并行性，导致工具使用效率低下

Method: 训练智能体基础模型将复杂任务分解为依赖感知的子任务图，自适应确定并行和串行工具执行

Result: 在MHQA数据集上显著超越传统ReAct基线，特别是在多步检索任务中，工具调用效率大幅提升

Conclusion: 图规划方法能够有效解决顺序推理瓶颈，实现智能并行化，提升多步推理任务的性能

Abstract: Autonomous agents powered by large language models (LLMs) have shown
impressive capabilities in tool manipulation for complex task-solving. However,
existing paradigms such as ReAct rely on sequential reasoning and execution,
failing to exploit the inherent parallelism among independent sub-tasks. This
sequential bottleneck leads to inefficient tool utilization and suboptimal
performance in multi-step reasoning scenarios. We introduce Graph-based Agent
Planning (GAP), a novel framework that explicitly models inter-task
dependencies through graph-based planning to enable adaptive parallel and
serial tool execution. Our approach trains agent foundation models to decompose
complex tasks into dependency-aware sub-task graphs, autonomously determining
which tools can be executed in parallel and which must follow sequential
dependencies. This dependency-aware orchestration achieves substantial
improvements in both execution efficiency and task accuracy. To train GAP, we
construct a high-quality dataset of graph-based planning traces derived from
the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage
training strategy: supervised fine-tuning (SFT) on the curated dataset,
followed by reinforcement learning (RL) with a correctness-based reward
function on strategically sampled queries where tool-based reasoning provides
maximum value. Experimental results on MHQA datasets demonstrate that GAP
significantly outperforms traditional ReAct baselines, particularly on
multi-step retrieval tasks, while achieving dramatic improvements in tool
invocation efficiency through intelligent parallelization. The project page is
available at: https://github.com/WJQ7777/Graph-Agent-Planning.

</details>


### [25] [Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm](https://arxiv.org/abs/2510.25388)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文提出了KVDA-UCT算法，通过分析即时奖励推断价值差异，在MCTS中检测更多抽象状态-动作对，相比OGA-UCT显著提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统MCTS抽象算法OGA-UCT要求状态-动作对具有相同的即时奖励，这一刚性条件限制了可发现的抽象数量，影响了样本效率。

Method: 提出KVDA框架，不再要求价值等价，而是推断状态-动作对之间的价值差异，并基于此构建抽象。将OGA-UCT修改为KVDA-UCT。

Result: KVDA-UCT在多种确定性环境和参数设置下检测到的抽象数量显著多于OGA-UCT，性能表现更优。

Conclusion: KVDA框架通过推断价值差异而非要求价值等价，能够发现更多抽象状态-动作对，有效提升MCTS的样本效率。

Abstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,
which can be improved by grouping state-action pairs and using their aggregate
statistics instead of single-node statistics. On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS
abstraction algorithm for deterministic environments that builds its
abstraction using the Abstractions of State-Action Pairs (ASAP) framework,
which aims to detect states and state-action pairs with the same value under
optimal play by analysing the search graph. ASAP, however, requires two
state-action pairs to have the same immediate reward, which is a rigid
condition that limits the number of abstractions that can be found and thereby
the sample efficiency. In this paper, we break with the paradigm of grouping
value-equivalent states or state-action pairs and instead group states and
state-action pairs with possibly different values as long as the difference
between their values can be inferred. We call this abstraction framework Known
Value Difference Abstractions (KVDA), which infers the value differences by
analysis of the immediate rewards and modifies OGA-UCT to use this framework
instead. The modification is called KVDA-UCT, which detects significantly more
abstractions than OGA-UCT, introduces no additional parameter, and outperforms
OGA-UCT on a variety of deterministic environments and parameter settings.

</details>


### [26] [Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions](https://arxiv.org/abs/2510.25445)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.AI

TL;DR: 本文提出了一个双范式框架，将智能体AI系统分为符号/经典范式（基于算法规划和持久状态）和神经/生成范式（基于随机生成和提示驱动编排），通过系统综述90项研究（2018-2025）分析了两种范式在理论基础、领域应用和伦理挑战等方面的差异。


<details>
  <summary>Details</summary>
Motivation: 解决智能体AI快速发展导致的认知碎片化问题，澄清将现代神经系统与过时符号模型混淆的概念回溯现象，为理解智能体AI提供清晰的概念框架。

Method: 采用PRISMA系统综述方法，分析了2018-2025年间的90项研究，从三个维度进行结构化分析：理论基础与架构原则、领域特定实施、范式特定伦理与治理挑战。

Result: 发现范式选择具有战略性：符号系统主导安全关键领域（如医疗），神经系统主导自适应、数据丰富的环境（如金融）；识别出符号系统治理模型严重不足和神经符号混合架构需求等关键研究空白。

Conclusion: 智能体AI的未来不在于单一范式的统治，而在于有意识的整合，创建既适应性强又可靠的系统，为未来研究、开发和政策提供概念工具包。

Abstract: Agentic AI represents a transformative shift in artificial intelligence, but
its rapid advancement has led to a fragmented understanding, often conflating
modern neural systems with outdated symbolic models -- a practice known as
conceptual retrofitting. This survey cuts through this confusion by introducing
a novel dual-paradigm framework that categorizes agentic systems into two
distinct lineages: the Symbolic/Classical (relying on algorithmic planning and
persistent state) and the Neural/Generative (leveraging stochastic generation
and prompt-driven orchestration). Through a systematic PRISMA-based review of
90 studies (2018--2025), we provide a comprehensive analysis structured around
this framework across three dimensions: (1) the theoretical foundations and
architectural principles defining each paradigm; (2) domain-specific
implementations in healthcare, finance, and robotics, demonstrating how
application constraints dictate paradigm selection; and (3) paradigm-specific
ethical and governance challenges, revealing divergent risks and mitigation
strategies. Our analysis reveals that the choice of paradigm is strategic:
symbolic systems dominate safety-critical domains (e.g., healthcare), while
neural systems prevail in adaptive, data-rich environments (e.g., finance).
Furthermore, we identify critical research gaps, including a significant
deficit in governance models for symbolic systems and a pressing need for
hybrid neuro-symbolic architectures. The findings culminate in a strategic
roadmap arguing that the future of Agentic AI lies not in the dominance of one
paradigm, but in their intentional integration to create systems that are both
adaptable and reliable. This work provides the essential conceptual toolkit to
guide future research, development, and policy toward robust and trustworthy
hybrid intelligent systems.

</details>


### [27] [Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?](https://arxiv.org/abs/2510.25471)
*Willem Fourie*

Main category: cs.AI

TL;DR: 本文提出AI对齐研究的新视角：将工具性目标视为需要接受和管理的特征，而非需要限制的故障。基于亚里士多德本体论，认为高级AI系统的构成会产生不同于设计者意图的效果，工具性倾向是其固有属性而非偶然故障。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐理论将工具性目标（如权力寻求、自我保存）视为风险来源，试图限制其表现。本文旨在提供替代框架，认为这些目标应被视为系统构成的自然结果，需要管理而非消除。

Method: 采用哲学论证方法，借鉴亚里士多德本体论及其现代解释，构建关于具体、目标导向实体的本体论框架，分析AI系统的形式和物质构成。

Result: 论证了高级AI系统的工具性倾向是其构成产生的固有结果（per se outcomes），而非偶然故障（accidental malfunctions）。

Conclusion: AI对齐工作应减少对消除工具性目标的关注，更多聚焦于理解、管理和引导这些目标朝向人类对齐的最终目的。

Abstract: In artificial intelligence (AI) alignment research, instrumental goals, also
called instrumental subgoals or instrumental convergent goals, are widely
associated with advanced AI systems. These goals, which include tendencies such
as power-seeking and self-preservation, become problematic when they conflict
with human aims. Conventional alignment theory treats instrumental goals as
sources of risk that become problematic through failure modes such as reward
hacking or goal misgeneralization, and attempts to limit the symptoms of
instrumental goals, notably resource acquisition and self-preservation. This
article proposes an alternative framing: that a philosophical argument can be
constructed according to which instrumental goals may be understood as features
to be accepted and managed rather than failures to be limited. Drawing on
Aristotle's ontology and its modern interpretations, an ontology of concrete,
goal-directed entities, it argues that advanced AI systems can be seen as
artifacts whose formal and material constitution gives rise to effects distinct
from their designers' intentions. In this view, the instrumental tendencies of
such systems correspond to per se outcomes of their constitution rather than
accidental malfunctions. The implication is that efforts should focus less on
eliminating instrumental goals and more on understanding, managing, and
directing them toward human-aligned ends.

</details>


### [28] [Multi-Objective Search: Algorithms, Applications, and Emerging Directions](https://arxiv.org/abs/2510.25504)
*Oren Salzman,Carlos Hernández Ulloa,Ariel Felner,Sven Koenig*

Main category: cs.AI

TL;DR: 多目标搜索（MOS）作为规划和决策问题的统一框架，用于平衡多个常冲突的标准。本文综述了MOS的发展，强调跨学科机会，并概述了定义MOS新兴前沿的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统很少只优化单一指标，近年来在机器人、交通和运筹学等AI应用中重新引起对多目标搜索的兴趣，反映了这一现实需求。

Method: 本文采用文献综述的方法，系统梳理多目标搜索领域的发展历程和最新进展。

Result: 通过综述发现多目标搜索在多个AI应用领域具有重要价值，并识别出跨学科合作的机会。

Conclusion: 多目标搜索是一个活跃的研究领域，存在许多开放挑战，这些挑战定义了该领域的新兴前沿。

Abstract: Multi-objective search (MOS) has emerged as a unifying framework for planning
and decision-making problems where multiple, often conflicting, criteria must
be balanced. While the problem has been studied for decades, recent years have
seen renewed interest in the topic across AI applications such as robotics,
transportation, and operations research, reflecting the reality that real-world
systems rarely optimize a single measure. This paper surveys developments in
MOS while highlighting cross-disciplinary opportunities, and outlines open
challenges that define the emerging frontier of MOS

</details>


### [29] [MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2510.25510)
*Zekun Xu,Siyu Xia,Chuhuai Yue,Jiajun Chai,Mingxue Tian,Xiaohan Wang,Wei Lin,Haoxuan Li,Guojun Yin*

Main category: cs.AI

TL;DR: 提出了MTIR-SQL框架，这是一个多轮工具集成推理的强化学习框架，用于改进Text-to-SQL任务。通过在执行过程中集成动态反馈，实现上下文敏感的查询生成和渐进式优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖静态执行反馈，限制了实时错误纠正能力。集成多轮工具调用和动态反馈可以显著提高模型的适应性和鲁棒性，从而提升性能。

Method: 提出了执行感知的多轮推理范式，在每个推理步骤无缝集成数据库执行反馈。扩展了GRPO算法以适应复杂的多轮交互场景，并添加了轨迹过滤机制和移除KL损失约束。

Result: 在BIRD Dev数据集上达到64.4%的准确率，在SPIDER Dev数据集上达到84.6%的执行准确率，显著优于现有方法。

Conclusion: MTIR-SQL框架通过多轮工具集成推理和动态反馈机制，有效提升了Text-to-SQL任务的性能，证明了该方法在复杂数据库查询生成中的有效性。

Abstract: As large language models (LLMs) are increasingly used in Text-to-SQL tasks,
Reinforcement Learning (RL) has become a common method for improving
performance. Existing methods primarily rely on static execution feedback,
which restricts real-time error correction. However, integrating multi-turn
tool invocation along with dynamic feedback could significantly improve
adaptability and robustness, ultimately enhancing model performance. To address
these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated
Reasoning reinforcement learning framework for Text-to-SQL. Our approach
introduces an execution-aware multi-turn reasoning paradigm that seamlessly
incorporates database execution feedback at each reasoning step, enabling
context-sensitive query generation and progressive refinement throughout the
reasoning process. The framework extends the GRPO algorithm to accommodate
complex multi-turn interaction scenarios. Considering the training instability
characteristics of MTIR and the potential for significant Deviation of model
distribution from the initial model, we enhance the GRPO algorithm by adding a
trajectory filtering mechanism and removing KL loss constraints. Experimental
results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\%
accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,
significantly outperforming existing approaches.

</details>


### [30] [Predicate Renaming via Large Language Models](https://arxiv.org/abs/2510.25517)
*Elisabetta Gentili,Tony Ribeiro,Fabrizio Riguzzi,Katsumi Inoue*

Main category: cs.AI

TL;DR: 使用大型语言模型为逻辑规则中的谓词命名，解决谓词发明等方法产生的未命名谓词问题，提高逻辑理论的可读性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在归纳逻辑编程中，各种规则生成方法会产生包含未命名谓词的规则，这阻碍了逻辑理论的可读性、可解释性和可重用性。

Method: 利用大型语言模型处理自然语言和代码的能力，为未命名谓词提供语义上有意义的命名建议。

Result: 在手工制作的逻辑规则上评估表明，大型语言模型在这项任务上具有潜力。

Conclusion: 大型语言模型在解决逻辑规则中谓词命名问题上展现出潜力，能够提供语义上有意义的命名建议。

Abstract: In this paper, we address the problem of giving names to predicates in logic
rules using Large Language Models (LLMs). In the context of Inductive Logic
Programming, various rule generation methods produce rules containing unnamed
predicates, with Predicate Invention being a key example. This hinders the
readability, interpretability, and reusability of the logic theory. Leveraging
recent advancements in LLMs development, we explore their ability to process
natural language and code to provide semantically meaningful suggestions for
giving a name to unnamed predicates. The evaluation of our approach on some
hand-crafted logic rules indicates that LLMs hold potential for this task.

</details>


### [31] [Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation](https://arxiv.org/abs/2510.25518)
*Thomas Cook,Richard Osuagwu,Liman Tsatiashvili,Vrynsia Vrynsia,Koustav Ghosal,Maraim Masoud,Riccardo Mattivi*

Main category: cs.AI

TL;DR: 本文提出了一种面向金融科技领域的智能RAG架构，通过模块化代理系统解决专业领域检索中的术语、缩略语等挑战，在检索精度和相关性方面优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在金融科技等专业领域面临挑战，包括领域特定本体、密集术语和缩略语等问题，影响检索和合成的效果。

Method: 采用模块化代理架构，包括智能查询重构、基于关键词提取的迭代子查询分解、上下文缩略语解析和交叉编码器上下文重排序。

Result: 在包含85个问答参考三元组的金融科技知识库数据集上，智能RAG系统在检索精度和相关性方面优于基准方法，但延迟有所增加。

Conclusion: 结构化多代理方法为增强复杂领域特定环境中的检索鲁棒性提供了有前景的方向。

Abstract: Retrieval-Augmented Generation (RAG) systems often face limitations in
specialized domains such as fintech, where domain-specific ontologies, dense
terminology, and acronyms complicate effective retrieval and synthesis. This
paper introduces an agentic RAG architecture designed to address these
challenges through a modular pipeline of specialized agents. The proposed
system supports intelligent query reformulation, iterative sub-query
decomposition guided by keyphrase extraction, contextual acronym resolution,
and cross-encoder-based context re-ranking. We evaluate our approach against a
standard RAG baseline using a curated dataset of 85 question--answer--reference
triples derived from an enterprise fintech knowledge base. Experimental results
demonstrate that the agentic RAG system outperforms the baseline in retrieval
precision and relevance, albeit with increased latency. These findings suggest
that structured, multi-agent methodologies offer a promising direction for
enhancing retrieval robustness in complex, domain-specific settings.

</details>


### [32] [Zero Reinforcement Learning Towards General Domains](https://arxiv.org/abs/2510.25528)
*Yuyuan Zeng,Yufei Huang,Can Xu,Qingfeng Sun,Jianfeng Yan,Guanghui Xu,Tao Yang,Fengzong Lian*

Main category: cs.AI

TL;DR: 提出了一种新的零强化学习范式，通过结合可验证奖励和生成奖励模型，在可验证和不可验证领域进行多任务训练，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前零强化学习研究主要关注可验证奖励信号的领域，但在验证不直接的多样化场景中激发推理能力的研究不足。

Method: 结合可验证奖励与生成奖励模型进行多任务零强化学习训练，设计平滑长度惩罚机制防止奖励黑客攻击。

Result: 在Qwen3-8B-Base和Qwen3-14B-Base上的实验表明，该方法在需要广泛推理的任务和更一般任务上都取得了优越的推理性能。

Conclusion: 该方法能够有效提升模型在可验证和不可验证领域的推理能力，实现推理能力在不同领域间的迁移。

Abstract: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach
for enhancing the reasoning capabilities of large language models (LLMs) by
directly applying reinforcement learning with verifiable rewards on pretrained
models, without the need for a supervised fine-tuning phase. However, current
research on zero-RL primarily focuses on domains with easily verifiable reward
signals, such as mathematics, programming, and other reasoning tasks. The
challenge of eliciting reasoning abilities in more diverse scenarios, where
verification is not straightforward, remains underexplored. To address this
gap, we propose a novel zero-RL paradigm designed to improve a model's
reasoning ability across both verifiable and non-verifiable domains. By
combining verifiable rewards with a generative reward model, we conduct
multi-task zero-RL training across both domains, facilitating the transfer of
reasoning capabilities between them. Furthermore, to mitigate reward hacking in
the generative reward model, we design a smooth length penalty that encourages
the generation of more comprehensive thinking tokens in general domains.
Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our
approach achieves superior reasoning performance, not only on tasks requiring
extensive reasoning but also on more general tasks.

</details>


### [33] [Off-policy Reinforcement Learning with Model-based Exploration Augmentation](https://arxiv.org/abs/2510.25529)
*Likun Wang,Xiangteng Zhang,Yinuo Wang,Guojian Zhan,Wenxuan Wang,Haoyu Gao,Jingliang Duan,Shengbo Eben Li*

Main category: cs.AI

TL;DR: 提出MoGE方法，通过生成未充分探索的关键状态和动态一致的经验来增强被动探索，提高强化学习中的样本效率和性能


<details>
  <summary>Details</summary>
Motivation: 现有探索方法存在局限性：主动探索在高维环境中表现不佳，被动探索受限于样本多样性不足。需要解决被动探索的局限性

Method: MoGE包含两个组件：(1)基于扩散模型的关键状态生成器，在效用函数指导下合成关键状态；(2)一步想象世界模型，基于关键状态构建关键转移用于智能体学习

Result: 在OpenAI Gym和DeepMind Control Suite上的实验表明，MoGE有效连接了探索和策略学习，在复杂控制任务中显著提高了样本效率和性能

Conclusion: MoGE采用模块化设计，可与现有算法无缝集成，在不改变核心结构的情况下改善探索效果

Abstract: Exploration is fundamental to reinforcement learning (RL), as it determines
how effectively an agent discovers and exploits the underlying structure of its
environment to achieve optimal performance. Existing exploration methods
generally fall into two categories: active exploration and passive exploration.
The former introduces stochasticity into the policy but struggles in
high-dimensional environments, while the latter adaptively prioritizes
transitions in the replay buffer to enhance exploration, yet remains
constrained by limited sample diversity. To address the limitation in passive
exploration, we propose Modelic Generative Exploration (MoGE), which augments
exploration through the generation of under-explored critical states and
synthesis of dynamics-consistent experiences through transition models. MoGE is
composed of two components: (1) a diffusion-based generator that synthesizes
critical states under the guidance of a utility function evaluating each
state's potential influence on policy exploration, and (2) a one-step
imagination world model for constructing critical transitions based on the
critical states for agent learning. Our method adopts a modular formulation
that aligns with the principles of off-policy learning, allowing seamless
integration with existing algorithms to improve exploration without altering
their core structures. Empirical results on OpenAI Gym and DeepMind Control
Suite reveal that MoGE effectively bridges exploration and policy learning,
leading to remarkable gains in both sample efficiency and performance across
complex control tasks.

</details>


### [34] [Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2510.25588)
*Eranga Bandara,Ross Gore,Atmaram Yarlagadda,Anita H. Clayton,Preston Samuel,Christopher K. Rhea,Sachin Shetty*

Main category: cs.AI

TL;DR: 提出一个结合微调大语言模型联盟和OpenAI推理LLM的决策支持系统，用于精神障碍的临床诊断，旨在标准化精神病学诊断过程。


<details>
  <summary>Details</summary>
Motivation: 当前精神病学诊断主要依赖医患对话，主观性强，导致不同医生和患者间的诊断存在差异和不一致性，需要标准化方法来提高诊断可靠性。

Method: 使用在精神科医患对话数据集上微调的LLMs，通过基于共识的决策过程聚合各模型的诊断预测，并由OpenAI推理LLM进行精炼，部署LLM代理协调整个诊断流程。

Result: 实验结果表明，结合微调LLMs和推理模型能够创建强大且高度准确的精神健康评估诊断系统，原型系统已与美国陆军医疗研究团队合作开发。

Conclusion: 这是首个将微调LLM联盟与推理LLM集成应用于临床精神健康诊断的工作，为下一代AI驱动的电子健康系统标准化精神病学诊断铺平了道路。

Abstract: The diagnosis of most mental disorders, including psychiatric evaluations,
primarily depends on dialogues between psychiatrists and patients. This
subjective process can lead to variability in diagnoses across clinicians and
patients, resulting in inconsistencies and challenges in achieving reliable
outcomes. To address these issues and standardize psychiatric diagnoses, we
propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss
Reasoning LLM-enabled Decision Support System for the clinical diagnosis of
mental disorders. Our approach leverages fine-tuned LLMs trained on
conversational datasets involving psychiatrist-patient interactions focused on
mental health conditions (e.g., depression). The diagnostic predictions from
individual models are aggregated through a consensus-based decision-making
process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method
for deploying LLM agents that orchestrate communication between the LLM
consortium and the reasoning LLM, ensuring transparency, reliability, and
responsible AI across the entire diagnostic workflow. Experimental results
demonstrate the transformative potential of combining fine-tuned LLMs with a
reasoning model to create a robust and highly accurate diagnostic system for
mental health assessment. A prototype of the proposed platform, integrating
three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in
collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,
USA. To the best of our knowledge, this work represents the first application
of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical
mental health diagnosis paving the way for next-generation AI-powered eHealth
systems aimed at standardizing psychiatric diagnoses.

</details>


### [35] [Counterfactual-based Agent Influence Ranker for Agentic AI Workflows](https://arxiv.org/abs/2510.25612)
*Amit Giloni,Chiara Picardi,Roy Betser,Shamik Bose,Aishvariya Priya Rathina Sabapathy,Roman Vainshtein*

Main category: cs.AI

TL;DR: CAIR是首个评估多智能体AI工作流中各个智能体对最终输出影响力的方法，通过反事实分析提供任务无关的分析能力。


<details>
  <summary>Details</summary>
Motivation: 由于AAW系统的高度自主性和广泛应用，需要从质量和安全角度深入理解其运作机制，但目前缺乏评估单个智能体对最终输出影响的方法。

Method: 提出基于反事实分析的智能体影响力排序方法(CAIR)，通过反事实分析评估每个智能体对AAW最终输出的影响程度。

Result: 在包含30个用例和230个功能的AAW数据集上评估，CAIR产生一致的排序结果，优于基线方法，并能有效提升下游任务的效果和相关性。

Conclusion: CAIR是首个能够评估AAW系统中智能体影响力的方法，支持离线和推理时分析，为理解和优化多智能体系统提供了重要工具。

Abstract: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,
is an autonomous system that assembles several LLM-based agents to work
collaboratively towards a shared goal. The high autonomy, widespread adoption,
and growing interest in such AAWs highlight the need for a deeper understanding
of their operations, from both quality and security aspects. To this day, there
are no existing methods to assess the influence of each agent on the AAW's
final output. Adopting techniques from related fields is not feasible since
existing methods perform only static structural analysis, which is unsuitable
for inference time execution. We present Counterfactual-based Agent Influence
Ranker (CAIR) - the first method for assessing the influence level of each
agent on the AAW's output and determining which agents are the most
influential. By performing counterfactual analysis, CAIR provides a
task-agnostic analysis that can be used both offline and at inference time. We
evaluate CAIR using an AAWs dataset of our creation, containing 30 different
use cases with 230 different functionalities. Our evaluation showed that CAIR
produces consistent rankings, outperforms baseline methods, and can easily
enhance the effectiveness and relevancy of downstream tasks.

</details>


### [36] [ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents](https://arxiv.org/abs/2510.25668)
*Tianyu Yang,Terry Ruas,Yijun Tian,Jan Philip Wahle,Daniel Kurzawe,Bela Gipp*

Main category: cs.AI

TL;DR: ALDEN是一个基于强化学习的多轮交互框架，通过训练视觉语言模型作为主动导航智能体来处理长文档理解任务，引入页面索引访问和视觉语义锚定机制，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在处理需要跨多页分析和信息整合的复杂长文档时表现不佳，主要依赖固定的推理模板或刚性流程，限制了模型的效率和泛化能力。

Method: 提出ALDEN框架：1）使用强化学习微调VLM作为交互智能体；2）引入页面索引访问动作；3）设计基于规则的跨层级奖励机制；4）提出视觉语义锚定机制解决训练不稳定问题。

Result: 在三个开源数据集构建的语料库上训练后，ALDEN在五个长文档基准测试中达到了最先进的性能。

Conclusion: ALDEN标志着从被动文档阅读向自主导航和跨长文档推理智能体的重要进展，为更准确高效的长文档理解提供了稳健路径。

Abstract: Vision-language models (VLMs) excel at interpreting text-rich images but
struggle with long, visually complex documents that demand analysis and
integration of information spread across multiple pages. Existing approaches
typically rely on fixed reasoning templates or rigid pipelines, which force
VLMs into a passive role and hinder both efficiency and generalization. We
present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement
learning framework that fine-tunes VLMs as interactive agents capable of
actively navigating long, visually rich documents. ALDEN introduces a novel
fetch action that directly accesses the page by index, complementing the
classic search action and better exploiting document structure. For dense
process supervision and efficient training, we propose a rule-based cross-level
reward that provides both turn- and token-level signals. To address the
empirically observed training instability caused by numerous visual tokens from
long documents, we further propose a visual-semantic anchoring mechanism that
applies a dual-path KL-divergence constraint to stabilize visual and textual
representations separately during training. Trained on a corpus constructed
from three open-source datasets, ALDEN achieves state-of-the-art performance on
five long-document benchmarks. Overall, ALDEN marks a step beyond passive
document reading toward agents that autonomously navigate and reason across
long, visually rich documents, offering a robust path to more accurate and
efficient long-document understanding.

</details>


### [37] [Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning](https://arxiv.org/abs/2510.25679)
*Federica Tonti,Ricardo Vinuesa*

Main category: cs.AI

TL;DR: 提出了一种基于深度强化学习的无人机最优导航策略，结合PPO算法和GTrXL架构，在三维城市流动模拟中显著提高了导航成功率并降低了碰撞率。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在城市区域用于配送和监控的日益普及，需要开发能够在复杂城市气流环境中有效导航的策略。

Method: 采用流感知的Proximal Policy Optimization (PPO)算法与Gated Transformer eXtra Large (GTrXL)架构相结合，为无人机提供更丰富的湍流场信息。

Result: 与PPO+LSTM、PPO+GTrXL（无辅助预测任务）和传统Zermelo导航算法相比，该方法显著提高了成功率(SR)并降低了碰撞率(CR)。

Conclusion: 该方法为在复杂城市环境中重新构想无人机应用前景铺平了道路。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for
delivery and surveillance purposes. In this work, we develop an optimal
navigation strategy based on Deep Reinforcement Learning. The environment is
represented by a three-dimensional high-fidelity simulation of an urban flow,
characterized by turbulence and recirculation zones. The algorithm presented
here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated
Transformer eXtra Large (GTrXL) architecture, giving the agent richer
information about the turbulent flow field in which it navigates. The results
are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO
combined with Long Short Term Memory (LSTM) cells and a traditional navigation
algorithm. The obtained results show a significant increase in the success rate
(SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the
classical Zermelo's navigation algorithm, paving the way to a completely
reimagined UAV landscape in complex urban environments.

</details>


### [38] [BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph](https://arxiv.org/abs/2510.25724)
*Vanya Arikutharam,Arkadiy Ukolov*

Main category: cs.AI

TL;DR: BambooKG是一种基于频率权重的知识图谱，通过在非三元组边添加权重来减少信息损失，提升单跳和多跳推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成方法将检索到的文本块独立处理，难以进行多跳或关系推理，特别是跨文档时。知识图谱通过三元组捕捉实体关系，但会丢失不符合三元组结构的信息。

Method: 引入BambooKG知识图谱，在非三元组边上应用基于频率的权重，反映链接强度，借鉴赫布原理的"一起激发，一起连接"思想。

Result: 减少了信息损失，在单跳和多跳推理任务上表现优于现有解决方案。

Conclusion: BambooKG通过频率权重机制有效解决了传统知识图谱的信息丢失问题，显著提升了推理性能。

Abstract: Retrieval-Augmented Generation allows LLMs to access external knowledge,
reducing hallucinations and ageing-data issues. However, it treats retrieved
chunks independently and struggles with multi-hop or relational reasoning,
especially across documents. Knowledge graphs enhance this by capturing the
relationships between entities using triplets, enabling structured, multi-chunk
reasoning. However, these tend to miss information that fails to conform to the
triplet structure. We introduce BambooKG, a knowledge graph with
frequency-based weights on non-triplet edges which reflect link strength,
drawing on the Hebbian principle of "fire together, wire together". This
decreases information loss and results in improved performance on single- and
multi-hop reasoning, outperforming the existing solutions.

</details>


### [39] [TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling](https://arxiv.org/abs/2510.25758)
*He Hu,Yucheng Zhou,Chiyuan Ma,Qianning Wang,Zheng Zhang,Fei Ma,Laizhong Cui,Qi Tian*

Main category: cs.AI

TL;DR: TheraMind是一个用于纵向心理咨询的战略性自适应代理，采用新颖的双循环架构，将复杂咨询过程解耦为会话内循环（战术对话管理）和跨会话循环（战略治疗规划），在模拟环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有心理咨询大语言模型缺乏情感理解、自适应策略和跨多会话的治疗方法使用，与真实临床实践差距较大。

Method: 提出双循环架构：会话内循环感知患者情绪状态并动态选择响应策略，跨会话循环评估治疗效果并调整后续方法。

Result: 在高保真模拟环境中，TheraMind在连贯性、灵活性和治疗协调等多会话指标上优于其他方法。

Conclusion: 双循环设计在模拟战略性、自适应和纵向治疗行为方面有效，代码已公开。

Abstract: Large language models (LLMs) in psychological counseling have attracted
increasing attention. However, existing approaches often lack emotional
understanding, adaptive strategies, and the use of therapeutic methods across
multiple sessions with long-term memory, leaving them far from real clinical
practice. To address these critical gaps, we introduce TheraMind, a strategic
and adaptive agent for longitudinal psychological counseling. The cornerstone
of TheraMind is a novel dual-loop architecture that decouples the complex
counseling process into an Intra-Session Loop for tactical dialogue management
and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session
Loop perceives the patient's emotional state to dynamically select response
strategies while leveraging cross-session memory to ensure continuity.
Crucially, the Cross-Session Loop empowers the agent with long-term
adaptability by evaluating the efficacy of the applied therapy after each
session and adjusting the method for subsequent interactions. We validate our
approach in a high-fidelity simulation environment grounded in real clinical
cases. Extensive evaluations show that TheraMind outperforms other methods,
especially on multi-session metrics like Coherence, Flexibility, and
Therapeutic Attunement, validating the effectiveness of its dual-loop design in
emulating strategic, adaptive, and longitudinal therapeutic behavior. The code
is publicly available at https://0mwwm0.github.io/TheraMind/.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [40] [Dual-Domain Deep Learning-Assisted NOMA-CSK Systems for Secure and Efficient Vehicular Communications](https://arxiv.org/abs/2510.24763)
*Tingting Huang,Jundong Chen,Huanqiang Zeng,Guofa Cai,Georges Kaddoum*

Main category: cs.IT

TL;DR: 提出了一种基于深度学习的功率域非正交多址混沌移位键控系统，用于车辆通信，通过DNN解调器消除混沌同步需求，提高频谱效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有多用户混沌通信系统存在频谱效率低、用户连接数有限、计算复杂度高等问题，需要一种更高效的解决方案来满足车辆通信的安全和效率需求。

Method: 设计基于深度神经网络的解调器，采用双域特征提取架构联合处理时域和频域信息，并集成到连续干扰消除框架中减轻误差传播。

Result: 理论分析和仿真表明，该系统在频谱效率、能量效率、误码率、安全性和鲁棒性方面表现优异，同时计算复杂度低于传统方法。

Conclusion: 所提出的DL-NOMA-CSK系统具有实际可行性，能够为安全的车辆通信提供有效的解决方案。

Abstract: Ensuring secure and efficient multi-user (MU) transmission is critical for
vehicular communication systems. Chaos-based modulation schemes have garnered
considerable interest due to their benefits in physical layer security.
However, most existing MU chaotic communication systems, particularly those
based on non-coherent detection, suffer from low spectral efficiency due to
reference signal transmission, and limited user connectivity under orthogonal
multiple access (OMA). While non-orthogonal schemes, such as sparse code
multiple access (SCMA)-based DCSK, have been explored, they face high
computational complexity and inflexible scalability due to their fixed codebook
designs. This paper proposes a deep learning-assisted power domain
non-orthogonal multiple access chaos shift keying (DL-NOMA-CSK) system for
vehicular communications. A deep neural network (DNN)-based demodulator is
designed to learn intrinsic chaotic signal characteristics during offline
training, thereby eliminating the need for chaotic synchronization or reference
signal transmission. The demodulator employs a dual-domain feature extraction
architecture that jointly processes the time-domain and frequency-domain
information of chaotic signals, enhancing feature learning under dynamic
channels. The DNN is integrated into the successive interference cancellation
(SIC) framework to mitigate error propagation issues. Theoretical analysis and
extensive simulations demonstrate that the proposed system achieves superior
performance in terms of spectral efficiency (SE), energy efficiency (EE), bit
error rate (BER), security, and robustness, while maintaining lower
computational complexity compared to traditional MU-DCSK and existing DL-aided
schemes. These advantages validate its practical viability for secure vehicular
communications.

</details>


### [41] [Resi-VidTok: An Efficient and Decomposed Progressive Tokenization Framework for Ultra-Low-Rate and Lightweight Video Transmission](https://arxiv.org/abs/2510.25002)
*Zhenyu Liu,Yi Ma,Rahim Tafazolli,Zhi Ding*

Main category: cs.IT

TL;DR: Resi-VidTok是一个用于超低速率轻量级视频传输的弹性令牌化框架，通过重要性排序的令牌流实现渐进编码和优雅质量降级，在恶劣信道条件下保持感知和语义保真度。


<details>
  <summary>Details</summary>
Motivation: 在有限带宽和弱连接等恶劣信道条件下，无线网络中的实时视频传输仍然极具挑战性，现有深度模型难以有效应对。

Method: 将时空内容重组为离散的重要性排序令牌流（关键令牌和细化令牌），集成差分时间令牌编码，结合步长控制的帧稀疏化和轻量级解码器侧插值器，以及信道自适应源信道编码和调制方案。

Result: 在低至0.0004的信道带宽比下保持稳健的视觉和语义一致性，实时重建速度超过30fps。

Conclusion: Resi-VidTok为能效敏感、延迟敏感和可靠性关键的无线应用提供了实用的视频传输解决方案。

Abstract: Real-time transmission of video over wireless networks remains highly
challenging, even with advanced deep models, particularly under severe channel
conditions such as limited bandwidth and weak connectivity. In this paper, we
propose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for
ultra-low-rate and lightweight video transmission that delivers strong
robustness while preserving perceptual and semantic fidelity on commodity
digital hardware. By reorganizing spatio--temporal content into a discrete,
importance-ordered token stream composed of key tokens and refinement tokens,
Resi-VidTok enables progressive encoding, prefix-decodable reconstruction, and
graceful quality degradation under constrained channels. A key contribution is
a resilient 1D tokenization pipeline for video that integrates differential
temporal token coding, explicitly supporting reliable recovery from incomplete
token sets using a single shared framewise decoder--without auxiliary temporal
extractors or heavy generative models. Furthermore, stride-controlled frame
sparsification combined with a lightweight decoder-side interpolator reduces
transmission load while maintaining motion continuity. Finally, a
channel-adaptive source--channel coding and modulation scheme dynamically
allocates rate and protection according to token importance and channel
condition, yielding stable quality across adverse SNRs. Evaluation results
indicate robust visual and semantic consistency at channel bandwidth ratios
(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,
demonstrating the practicality of Resi-VidTok for energy-efficient,
latency-sensitive, and reliability-critical wireless applications.

</details>


### [42] [Fed-PELAD: Communication-Efficient Federated Learning for Massive MIMO CSI Feedback with Personalized Encoders and a LoRA-Adapted Shared Decoder](https://arxiv.org/abs/2510.25181)
*Yixiang Zhou,Tong Wu,Meixia Tao,Jianhua Mo*

Main category: cs.IT

TL;DR: 提出Fed-PELAD联邦学习框架，通过个性化编码器和LoRA适配共享解码器，解决大规模MIMO系统中CSI反馈的通信开销、数据异构性和隐私问题，相比传统方法减少57.03%的上行通信成本，在异构条件下提升1.2dB的CSI反馈精度。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MIMO系统中深度学习的通信开销高、数据异构性强和隐私保护需求等关键挑战。

Method: 提出Fed-PELAD框架：本地训练个性化编码器捕获设备特定信道特征，基站协调更新共享解码器，使用LoRA技术仅传输紧凑的适配器参数进行聚合，并引入交替冻结策略和校准学习率比来增强收敛稳定性。

Result: 在3GPP标准信道模型上的广泛仿真表明，相比传统方法仅需42.97%的上行通信成本，在异构条件下实现1.2dB的CSI反馈精度提升。

Conclusion: Fed-PELAD框架有效平衡了通信效率、个性化学习和隐私保护，为大规模MIMO系统中的CSI反馈提供了高效解决方案。

Abstract: This paper addresses the critical challenges of communication overhead, data
heterogeneity, and privacy in deep learning for channel state information (CSI)
feedback in massive MIMO systems. To this end, we propose Fed-PELAD, a novel
federated learning framework that incorporates personalized encoders and a
LoRA-adapted shared decoder. Specifically, personalized encoders are trained
locally on each user equipment (UE) to capture device-specific channel
characteristics, while a shared decoder is updated globally via the
coordination of the base station (BS) by using Low-Rank Adaptation (LoRA). This
design ensures that only compact LoRA adapter parameters instead of full model
updates are transmitted for aggregation. To further enhance convergence
stability, we introduce an alternating freezing strategy with calibrated
learning-rate ratio during LoRA aggregation. Extensive simulations on
3GPP-standard channel models demonstrate that Fed-PELAD requires only 42.97\%
of the uplink communication cost compared to conventional methods while
achieving a performance gain of 1.2 dB in CSI feedback accuracy under
heterogeneous conditions.

</details>


### [43] [Joint Spatial Registration and Resource Allocation for Transmissive RIS Enabled Cooperative ISCC Networks](https://arxiv.org/abs/2510.25266)
*Ziwei Liu,Wen Chen,Zhendong Li,Qiong Wu*

Main category: cs.IT

TL;DR: 提出了一种新型透射式可重构智能表面(TRIS)收发器驱动的协作式集成感知、计算与通信(ISCC)网络，通过联合优化波束成形、时隙分配、感知数据分配和感知波束调度，最小化网络总能耗。


<details>
  <summary>Details</summary>
Motivation: 为满足多样化网络需求并降低能耗，需要开发高效的集成感知、计算与通信网络架构。

Method: 采用信号级空间配准算法和多流通信，引入秩-N约束并使用迭代秩最小化方案，通过块坐标下降法解决非凸优化问题。

Result: 数值仿真结果表明所提方案在提升整体网络性能和降低网络总能耗方面具有优越性。

Conclusion: TRIS收发器驱动的协作ISCC网络能够有效降低能耗并提升网络性能，为未来多样化网络需求提供了可行解决方案。

Abstract: In this paper, we propose a novel transmissive reconfigurable intelligent
surface (TRIS) transceiver-driven cooperative integrated sensing, computing,
and communication (ISCC) network to meet the requirement for a diverse network
with low energy consumption. The cooperative base stations (BSs) are equipped
with TRIS transceivers to accomplish sensing data acquisition, communication
offloading, and computation in a time slot. In order to obtain higher
cooperation gain, we utilize a signal-level spatial registration algorithm,
which is realized by adjusting the beamwidth. Meanwhile, for more efficient
offloading of the computational task, multistream communication is considered,
and rank-$N$ constraints are introduced, which are handled using an iterative
rank minimization (IRM) scheme. We construct an optimization problem with the
objective function of minimizing the total energy consumption of the network to
jointly optimize the beamforming matrix, time slot allocation, sensing data
allocation and sensing beam scheduling variables. Due to the coupling of the
variables, the proposed problem is a non-convex optimization problem, which we
decouple and solve using a block coordinate descent (BCD) scheme. Finally,
numerical simulation results confirm the superiority of the proposed scheme in
improving the overall network performance and reducing the total energy
consumption of the network.

</details>


### [44] [General Coverage Models: Structure, Monotonicity, and Shotgun Sequencing](https://arxiv.org/abs/2510.25305)
*Yitzchak Grunbaum,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 研究覆盖过程，引入固定长度窗口模型，开发组合工具将覆盖时间计算转化为计数问题，获得精确表达式并分析渐近行为，研究均匀ℓ-正则模型并与批量采样模型比较。


<details>
  <summary>Details</summary>
Motivation: 受鸟枪法DNA测序启发，研究每个采样是固定长度连续窗口的覆盖过程，推广经典的优惠券收集问题。

Method: 开发组合工具将覆盖时间概率问题转化为子集族的计数问题，获得窗口模型的精确表达式，利用连续类比分析渐近行为，研究均匀ℓ-正则模型。

Result: 获得了窗口模型的精确覆盖时间表达式，分析了渐近行为，证明了均匀ℓ-正则模型的通用上界，发现许多采样模型具有相同的主阶渐近行为。

Conclusion: 开发了统一的组合框架分析覆盖过程，证明了均匀ℓ-正则模型中批量采样模型可能最大化覆盖时间，揭示了不同采样模型在渐近行为上的共性和差异。

Abstract: We study coverage processes in which each draw reveals a subset of $[n]$, and
the goal is to determine the expected number of draws until all items are seen
at least once. A classical example is the Coupon Collector's Problem, where
each draw reveals exactly one item. Motivated by shotgun DNA sequencing, we
introduce a model where each draw is a contiguous window of fixed length, in
both cyclic and non-cyclic variants. We develop a unifying combinatorial tool
that shifts the task of finding coverage time from probability, to a counting
problem over families of subsets of $[n]$ that together contain all items,
enabling exact calculation. Using this result, we obtain exact expressions for
the window models. We then leverage past results on a continuous analogue of
the cyclic window model to analyze the asymptotic behavior of both models. We
further study what we call uniform $\ell$-regular models, where every draw has
size $\ell$ and every item appears in the same number of admissible draws. We
compare these to the batch sampling model, in which all $\ell$-subsets are
drawn uniformly at random and present upper and lower bounds, which were also
obtained independently by Berend and Sher. We conjecture, and prove for special
cases, that this model maximizes the coverage time among all uniform
$\ell$-regular models. Finally, we prove a universal upper bound on the entire
class of uniform $\ell$-regular models, which illuminates the fact that many
sampling models share the same leading asymptotic order, while potentially
differing significantly in lower-order terms.

</details>


### [45] [Network Oblivious Transfer via Noisy Broadcast Channels](https://arxiv.org/abs/2510.25343)
*Hadi Aghaee,Christian Deppe,Holger Boche*

Main category: cs.IT

TL;DR: 本文研究了通过离散无记忆广播信道实现信息论不经意传输，分析了非共谋和共谋两种用户模型，建立了容量区域的上界，并提出了两种具体协议。


<details>
  <summary>Details</summary>
Motivation: 构建连接网络信息论和密码学安全的理论框架，探索噪声广播信道作为多用户隐私保护通信原语的潜力。

Method: 使用离散无记忆广播信道模型，针对非共谋和共谋用户分别设计协议：第一个利用二进制擦除广播信道结构，第二个引入熵共享和隐私放大机制。

Result: 对于非共谋情况，不经意传输容量的上下界一致，完全刻画了可达区域；对于共谋情况，协议能保持安全性。

Conclusion: 噪声广播信道可以作为强大的多用户隐私保护通信原语，为网络信息论和密码学安全提供了统一的理论框架。

Abstract: This paper investigates information-theoretic oblivious transfer via a
discrete memoryless broadcast channel with one sender and two receivers. We
analyze both non-colluding and colluding honest-but-curious user models and
establish general upper bounds on the achievable oblivious transfer capacity
region for each case. Two explicit oblivious transfer protocols are proposed.
The first ensures correctness and privacy for independent, non-colluding
receivers by leveraging the structure of binary erasure broadcast channels. The
second protocol, secure even under receiver collusion, introduces additional
entropy-sharing and privacy amplification mechanisms to preserve secrecy
despite information leakage between users. Our results show that for the
non-colluding case, the upper and lower bounds on oblivious transfer capacity
coincide, providing a complete characterization of the achievable region. The
work provides a unified theoretical framework bridging network information
theory and cryptographic security, highlighting the potential of noisy
broadcast channels as powerful primitives for multi-user privacy-preserving
communication.

</details>


### [46] [Joint Beamforming Design and Resource Allocation for IRS-Assisted Full-Duplex Terahertz Systems](https://arxiv.org/abs/2510.25346)
*Chi Qiu,Wen Chen,Qingqing Wu,Fen Hou,Wanming Hao,Ruiqi Liu,Derrick Wing Kwan Ng*

Main category: cs.IT

TL;DR: 提出了一种IRS辅助FD THz通信系统的联合资源分配框架，通过优化IRS反射相移、上下行功率控制、子带带宽分配和子带分配来最大化用户加权最小速率，确保服务质量公平性。


<details>
  <summary>Details</summary>
Motivation: 解决IRS辅助FD THz通信系统在实际部署中面临的严重传播损耗、频率相关分子吸收和强残余自干扰等技术挑战。

Method: 开发了两种计算高效算法：一种采用等子带带宽分配以简化优化，另一种引入自适应带宽分配以进一步提高频谱利用率和系统灵活性。

Result: 仿真结果验证了所提设计的有效性，表明所采用方案相比基准方案实现了显著的频谱效率提升。

Conclusion: 所提出的联合资源分配框架能够有效解决IRS辅助FD THz通信系统的技术挑战，在系统性能和计算复杂度之间取得良好平衡。

Abstract: Intelligent reflecting surface (IRS)-assisted full-duplex (FD) terahertz
(THz) communication systems have emerged as a promising paradigm to satisfy the
escalating demand for ultra-high data rates and spectral efficiency in future
wireless networks. However, the practical deployment of such systems presents
unique technical challenges, stemming from severe propagation loss,
frequency-dependent molecular absorption in the THz band, and the presence of
strong residual self-interference (SI) inherent to FD communications. To tackle
these issues, this paper proposes a joint resource allocation framework that
aims to maximize the weighted minimum rate among all users, thereby ensuring
fairness in quality of service. Specifically, the proposed design jointly
optimizes IRS reflecting phase shifts, uplink/downlink transmit power control,
sub-band bandwidth allocation, and sub-band assignment, explicitly capturing
the unique propagation characteristics of THz channels and the impact of
residual SI. To strike an balance between system performance and computational
complexity, two computationally efficient algorithms are developed under
distinct spectrum partitioning schemes: one assumes equal sub-band bandwidth
allocation to facilliate tractable optimization, while the other introduces
adaptive bandwidth allocation to further enhance spectral utilization and
system flexibility. Simulation results validate the effectiveness of the
proposed designs and demonstrate that the adopted scheme achieves significant
spectral efficiency improvements over benchmark schemes.

</details>


### [47] [AirCNN via Reconfigurable Intelligent Surfaces: Architecture Design and Implementation](https://arxiv.org/abs/2510.25389)
*Meng Hua,Haotian Wu,Deniz Gündüz*

Main category: cs.IT

TL;DR: AirCNN是一种通过空中模拟计算实现卷积神经网络的新范式，利用可重构智能表面(RIS)和收发器设计，将无线传播环境改造成CNN层操作。


<details>
  <summary>Details</summary>
Motivation: 探索利用无线环境本身进行神经网络计算的创新方法，避免传统数字计算的能耗和延迟问题，实现更高效的边缘智能计算。

Method: 提出两种RIS辅助传输架构：MIMO和MISO，用于实现2D卷积和深度可分离卷积。联合优化发射预编码器、接收合并器和RIS相移，在传输功率预算和单位模相移约束下进行参数优化。

Result: 仿真结果显示AirCNN架构能实现满意的分类性能。Conv2d MISO在各种设置下始终优于Conv2d MIMO，而ConvSD中MISO仅在信道条件差时表现更好。多RIS相比单RIS显著提升性能，特别是在视距主导的无线环境中。

Conclusion: AirCNN证明了通过无线传播环境实现CNN计算的可行性，为边缘智能计算提供了新的实现途径，多RIS架构在特定条件下具有明显优势。

Abstract: This paper introduces AirCNN, a novel paradigm for implementing convolutional
neural networks (CNNs) via over-the-air (OTA) analog computation. By leveraging
multiple reconfigurable intelligent surfaces (RISs) and transceiver designs, we
engineer the ambient wireless propagation environment to emulate the operations
of a CNN layer. To comprehensively evaluate AirCNN, we consider two types of
CNNs, namely classic two-dimensional (2D) convolution (Conv2d) and light-weight
convolution, i.e., depthwise separable convolution (ConvSD). For Conv2d
realization via OTA computation, we propose and analyze two RIS-aided
transmission architectures: multiple-input multiple-output (MIMO) and
multiple-input single-output (MISO), balancing transmission overhead and
emulation performance. We jointly optimize all parameters, including the
transmitter precoder, receiver combiner, and RIS phase shifts, under practical
constraints such as transmit power budget and unit-modulus phase shift
requirements. We further extend the framework to ConvSD, which requires
distinct transmission strategies for depthwise and pointwise convolutions.
Simulation results demonstrate that the proposed AirCNN architectures can
achieve satisfactory classification performance. Notably, Conv2d MISO
consistently outperforms Conv2d MIMO across various settings, while for ConvSD,
MISO is superior only under poor channel conditions. Moreover, employing
multiple RISs significantly enhances performance compared to a single RIS,
especially in line-of-sight (LoS)-dominated wireless environments.

</details>


### [48] [Several classes of $p$-ary linear codes with few-weights derived from Weil sums](https://arxiv.org/abs/2510.25578)
*Mrinal Kanti Bose,Abhay Kumar Singh*

Main category: cs.IT

TL;DR: 本文提出了几类新的少重量线性码，通过两个特定的定义集构造了5类4重量码、1类2重量码，以及使用弱正则bent函数构造了2类6重量码、2类8重量码和1类9重量码。


<details>
  <summary>Details</summary>
Motivation: 少重量线性码在编码理论中具有重要研究价值，在秘密共享方案、认证码、关联方案和强正则图等领域有广泛应用。受Cheng和Gao以及Wu、Li和Zeng工作的启发，本文旨在构造新的少重量线性码。

Method: 通过选择两个特定的定义集来构造线性码：第一个定义集产生4重量和2重量码；第二个定义集结合弱正则bent函数产生6重量、8重量和9重量码。所有码的参数和重量分布通过有限域上Weil和的详细计算确定。

Result: 成功构造了多种少重量线性码：5类4重量码、1类2重量码、2类6重量码、2类8重量码和1类9重量码。其中2重量码达到Griesmer界，是最优的。

Conclusion: 本文提出的方法能够有效构造多种少重量线性码，其中2重量码达到最优性能，为编码理论提供了新的构造方案。

Abstract: Linear codes with few weights have been a significant area of research in
coding theory for many years, due to their applications in secret sharing
schemes, authentication codes, association schemes, and strongly regular
graphs. Inspired by the works of Cheng and Gao \cite{P8} and Wu, Li and Zeng
\cite{P12}, in this paper, we propose several new classes of few-weight linear
codes over the finite field $\mathbb{F}_{p}$ through the selection of two
specific defining sets. Consequently, we obtain five classes of $4$-weight
linear codes and one class of $2$-weight linear codes from our first defining
set. Furthermore, by employing weakly regular bent functions in our second
defining set, we derive two classes of $6$-weight codes, two classes of
$8$-weight codes, and one class of $9$-weight codes. The parameters and weight
distributions of all these constructed codes are wholly determined by detailed
calculations on certain Weil sums over finite fields. In addition, we identify
an optimal class of $2$-weight codes that meet the Griesmer bound.

</details>


### [49] [On Multidimensional 2-Weight-Limited Burst-Correcting Codes](https://arxiv.org/abs/2510.25592)
*Hagai Berend,Ohad Elishco,Moshe Schwartz*

Main category: cs.IT

TL;DR: 该论文研究能够纠正权重最多为2的突发错误的多维码，针对三种不同的位置限制情况提供了显式码构造，并与证明的下界比较了冗余度。


<details>
  <summary>Details</summary>
Motivation: 研究多维码在纠正权重最多为2的突发错误时的性能，特别关注错误位置之间的相对位置限制对编码效率的影响。

Method: 针对三种不同的位置限制情况（L∞距离有界、L1距离有界、两个错误位置在轴平行线上且距离有界），提供了显式的码构造方法。

Result: 为所有三种情况提供了具体的码构造方案，并通过与证明的下界比较，分析了这些构造的冗余度性能。

Conclusion: 论文成功构建了针对不同位置限制条件的多维码，并评估了其冗余度效率，为纠正权重为2的突发错误提供了实用的编码解决方案。

Abstract: We consider multidimensional codes capable of correcting a burst error of
weight at most $2$. When two positions are in error, the burst limits their
relative position. We study three such limitations: the $L_\infty$ distance
between the positions is bounded, the $L_1$ distance between the positions is
bounded, or the two positions are on an axis-parallel line with bounded
distance between them. In all cases we provide explicit code constructions, and
compare their excess redundancy to a lower bound we prove.

</details>


### [50] [Effect of Full Common Randomness Replication in Symmetric PIR on Graph-Based Replicated Systems](https://arxiv.org/abs/2510.25736)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: 该论文研究图复制数据库中的对称私有信息检索(SPIR)问题，通过开发算法将PIR方案转换为SPIR方案，为路径图和循环图建立了容量下界，并在三顶点路径图中确定了SPIR容量为1/2。


<details>
  <summary>Details</summary>
Motivation: 研究图复制数据库模型中的SPIR问题，其中消息在相邻服务器间复制，目标是量化相比图复制公共随机性设置的SPIR容量改进。

Method: 开发算法将一类PIR方案转换为相应的SPIR方案，为存在此类方案的图建立容量下界，特别关注路径图和循环图。

Result: 为路径图和循环图推导了比各自PIR容量更紧的容量上界，在三顶点路径图的特殊情况下确定了SPIR容量为1/2。

Conclusion: 提出的转换算法有效建立了图复制数据库SPIR的容量下界，并在特定图结构中获得了精确的容量结果。

Abstract: We revisit the problem of symmetric private information retrieval (SPIR) in
settings where the database replication is modeled by a simple graph. Here,
each vertex corresponds to a server, and a message is replicated on two servers
if and only if there is an edge between them. To satisfy the requirement of
database privacy, we let all the servers share some common randomness,
independent of the messages. We aim to quantify the improvement in SPIR
capacity, i.e., the maximum ratio of the number of desired and downloaded
symbols, compared to the setting with graph-replicated common randomness.
Towards this, we develop an algorithm to convert a class of PIR schemes into
the corresponding SPIR schemes, thereby establishing a capacity lower bound on
graphs for which such schemes exist. This includes the class of path and cyclic
graphs for which we derive capacity upper bounds that are tighter than the
trivial bounds given by the respective PIR capacities. For the special case of
path graph with three vertices, we identify the SPIR capacity to be
$\frac{1}{2}$.

</details>


### [51] [A mathematical study of the excess growth rate](https://arxiv.org/abs/2510.25740)
*Steven Campbell,Ting-Kam Leonard Wong*

Main category: cs.IT

TL;DR: 该论文从信息论角度研究投资组合理论中的超额增长率，将其与Rényi熵、交叉熵、Helmholtz自由能等概念联系起来，并提供了三个公理化特征定理。


<details>
  <summary>Details</summary>
Motivation: 探索超额增长率在信息论中的理论基础，建立投资组合理论与信息论之间的新联系。

Method: 通过公理化方法，从相对熵、Jensen不等式差距和对数散度三个角度对超额增长率进行特征化分析。

Result: 证明了超额增长率与信息论中多种概念的等价关系，并研究了其最大化问题与最优增长投资组合的比较。

Conclusion: 研究不仅为超额增长率的重要性提供了理论依据，还建立了信息论与量化金融之间的新联系。

Abstract: We study the excess growth rate -- a fundamental logarithmic functional
arising in portfolio theory -- from the perspective of information theory. We
show that the excess growth rate can be connected to the R\'{e}nyi and cross
entropies, the Helmholtz free energy, L. Campbell's measure of average code
length and large deviations. Our main results consist of three axiomatic
characterization theorems of the excess growth rate, in terms of (i) the
relative entropy, (ii) the gap in Jensen's inequality, and (iii) the
logarithmic divergence that generalizes the Bregman divergence. Furthermore, we
study maximization of the excess growth rate and compare it with the growth
optimal portfolio. Our results not only provide theoretical justifications of
the significance of the excess growth rate, but also establish new connections
between information theory and quantitative finance.

</details>
