<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.IT](#cs.IT) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [An LLM-based Agentic Framework for Accessible Network Control](https://arxiv.org/abs/2509.20600)
*Samuel Lin,Jiawei Zhou,Minlan Yu*

Main category: cs.NI

TL;DR: 该论文提出了一种基于大语言模型(LLM)的智能代理框架，使非专业用户能够通过自然语言对话来管理网络，降低了网络管理的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 传统网络管理方法需要高度专业的知识，限制了普通用户的使用。随着LLM的发展，作者希望让更广泛的非专业用户能够通过自然语言轻松管理网络。

Method: 设计了一个代理框架，使用中间表示来统一不同厂商设备的配置，实时从内存中检索网络状态，并提供外部反馈接口。同时开发了可视化界面来促进对话驱动的用户交互。

Result: 初步实验验证了系统组件与LLM集成的有效性，在合成和真实用户语句上都表现良好。通过数据收集和可视化工作，为LLM的更有效使用奠定了基础。

Conclusion: 该系统为网络控制的民主化铺平了道路，使日常用户能够更轻松地管理网络，并为未来基于LLM的网络管理系统开发提供了数据基础。

Abstract: Traditional approaches to network management have been accessible only to a
handful of highly-trained network operators with significant expert knowledge.
This creates barriers for lay users to easily manage their networks without
resorting to experts. With recent development of powerful large language models
(LLMs) for language comprehension, we design a system to make network
management accessible to a broader audience of non-experts by allowing users to
converse with networks in natural language. To effectively leverage
advancements in LLMs, we propose an agentic framework that uses an intermediate
representation to streamline configuration across diverse vendor equipment,
retrieves the network state from memory in real-time, and provides an interface
for external feedback. We also conduct pilot studies to collect real user data
of natural language utterances for network control, and present a visualization
interface to facilitate dialogue-driven user interaction and enable large-scale
data collection for future development. Preliminary experiments validate the
effectiveness of our proposed system components with LLM integration on both
synthetic and real user utterances. Through our data collection and
visualization efforts, we pave the way for more effective use of LLMs and
democratize network control for everyday users.

</details>


### [2] [An SDR-Based Test Platform for 5G NTN Prototyping and Validation](https://arxiv.org/abs/2509.20692)
*Lu Hou,Kan Zheng,Jie Mei,Cheng Huang*

Main category: cs.NI

TL;DR: 本文提出了一个基于软件定义无线电（SDR）的5G非地面网络（NTN）测试平台，用于解决当前5G NTN标准成熟度不足和商用设备缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 5G NTN标准在3GPP Release 17中正式确立，但由于标准处于早期阶段且缺乏商用NTN设备，阻碍了性能验证和系统原型开发。

Method: 开发了一个基于通用处理器（GPP）的SDR测试平台，集成Amarisoft的5G NTN协议栈软件，并通过自定义系统集成和适配实现真实卫星操作。平台支持SDR-based NTN gNB和UE模拟器之间的双向通信，通过地球静止轨道（GEO）卫星链路实现，完全符合3GPP NTN规范。

Result: 通过现场试验评估了下行链路吞吐量和往返时间等性能指标。结果验证了基于SDR的平台在NTN测试中的可行性和有效性。

Conclusion: 该平台在广泛商业部署之前具有弥合当前实施差距的潜力。

Abstract: The integration of satellite communication into 5G has been formalized in
3GPP Release 17 through the specification of Non-Terrestrial Networks (NTN),
marking a significant step toward achieving global connectivity. However, the
early-stage maturity of 5G NTN standards and the lack of commercial NTN-capable
equipment hinder extensive performance validation and system prototyping. To
address this gap, this paper proposes a software-defined radio (SDR) test
platform with General-Purpose Processor (GPP) processing, leveraging
Amarisoft's 5G NTN protocol stack software while performing custom system
integration and adaptation for real satellite operation. The platform supports
bidirectional communication between an SDR-based NTN gNB and UE emulator
through a Geostationary Earth Orbit (GEO) satellite link, with full compliance
to 3GPP NTN specifications. We provide detailed insights into the system
architecture, SDR hardware-software co-design, and satellite gateway
adaptations. Through field trials, we evaluate the performance metrics
including downlink throughput and round-trip time. Results validate the
feasibility and effectiveness of SDR-based platforms for NTN testing, and
highlight their potential in bridging current implementation gaps before
widespread commercial deployment.

</details>


### [3] [Trustworthy Semantic Communication for Vehicular Networks: Challenges and Solutions](https://arxiv.org/abs/2509.20830)
*Yanghe Pan,Yuntao Wang,Shaolong Guo,Chengyu Yin,Ruidong Li,Zhou Su,Yuan Wu*

Main category: cs.NI

TL;DR: 本文提出了一种三层可信车载语义通信网络架构，通过语义伪装传输机制、鲁棒联邦编码器-解码器训练框架和基于审计游戏的分布式车辆信任管理机制，解决车载语义通信网络中的信任挑战。


<details>
  <summary>Details</summary>
Motivation: 车载语义通信网络在信息传输、语义编码和通信实体可靠性方面面临关键的信任挑战，阻碍了其在实际部署中的应用。

Method: 1. 利用防御性对抗噪声的语义伪装传输机制进行主动窃听防御
2. 鲁棒联邦编码器-解码器训练框架缓解编码器-解码器中毒攻击
3. 基于审计游戏的分布式车辆信任管理机制阻止不可信车辆

Result: 案例研究验证了所提解决方案的有效性。

Conclusion: 该架构为解决车载语义通信网络的信任问题提供了创新方案，并指出了推进这一新兴领域的关键未来研究方向。

Abstract: Semantic communication (SemCom) has the potential to significantly reduce
communication delay in vehicle-to-everything (V2X) communications within
vehicular networks (VNs). However, the deployment of vehicular SemCom networks
(VN-SemComNets) faces critical trust challenges in information transmission,
semantic encoding, and communication entity reliability. This paper proposes an
innovative three-layer trustworthy VN-SemComNet architecture. Specifically, we
introduce a semantic camouflage transmission mechanism leveraging defensive
adversarial noise for active eavesdropping defense, a robust federated
encoder-decoder training framework to mitigate encoder-decoder poisoning
attacks, and an audit game-based distributed vehicle trust management mechanism
to deter untrustworthy vehicles. A case study validates the effectiveness of
the proposed solutions. Lastly, essential future research directions are
pointed out to advance this emerging field.

</details>


### [4] [BSB: Towards Demand-Aware Peer Selection With XOR-based Routing](https://arxiv.org/abs/2509.20974)
*Qingyun Ji,Darya Melnyk,Arash Pourdamghani,Stefan Schmid*

Main category: cs.NI

TL;DR: 提出了一种基于需求感知的对等选择算法BSB，通过考虑应用特定的数据流量来优化P2P网络性能，相比现有算法可提升43%的性能


<details>
  <summary>Details</summary>
Motivation: 现有P2P网络中的对等选择算法大多忽略了应用特定的数据流量，导致连接利用不足、路径更长和延迟增加的问题

Method: 提出Binary Search in Buckets (BSB)算法，采用需求感知的方法，同时保持与现有XOR-based路由协议的兼容性，使用局部贪婪的路由机制

Result: 在真实世界和合成的通信网络轨迹上进行仿真评估，结果显示BSB相比文献中的两种算法可提供高达43%的性能改进

Conclusion: BSB算法通过需求感知的对等选择，有效解决了现有算法与数据流量需求不匹配的问题，显著提升了P2P网络的性能表现

Abstract: Peer-to-peer networks, as a key enabler of modern networked and distributed
systems, rely on peer-selection algorithms to optimize their scalability and
performance. Peer-selection methods have been studied extensively in various
aspects, including routing mechanisms and communication overhead. However, many
state-of-the-art algorithms are oblivious to application-specific data traffic.
This mismatch between design and demand results in underutilized connections,
which inevitably leads to longer paths and increased latency. In this work, we
propose a novel demand-aware peer-selection algorithm, called Binary Search in
Buckets (BSB). Our demand-aware approach adheres to a local and greedy
XOR-based routing mechanism, ensuring compatibility with existing protocols and
mechanisms. We evaluate our solution against two prior algorithms by conducting
simulations on real-world and synthetic communication network traces. The
results of our evaluations show that BSB can offer up to a 43% improvement
compared to two selected algorithms from the literature.

</details>


### [5] [A Novel Integrated Architecture for Intent Based Approach and Zero Touch Networks](https://arxiv.org/abs/2509.21026)
*Neelam Gupta,Dibakar Das,Tamizhelakkiya K,Uma Maheswari Natarajan,Sharvari Ravindran,Komal Sharma,Jyotsna Bapat,Debabrata Das*

Main category: cs.NI

TL;DR: 本文提出了一种将基于意图的网络（IBN）和零接触网络（ZTN）集成的新架构，通过自然语言处理将用户意图转换为网络配置，并使用BiLSTM和Q学习实现闭环控制，以在变化的网络条件下自主维持服务质量。


<details>
  <summary>Details</summary>
Motivation: 6G网络面临管理多样化应用服务质量（QoS）和在变化网络条件下达成服务等级协议（SLA）的挑战，需要自动化网络管理来满足实时需求。

Method: 用户用自然语言（如英语）表达意图，通过NLP技术（如RAG）转换为网络意图语言（Nile），然后传递给基于BiLSTM和Q学习的ZTN闭环框架作为目标，自主维持网络性能。

Result: 在OpenAirInterface测试床上实现该架构，并通过蒙特卡洛模拟评估。结果显示ZTN能自主实现用户意图设定的带宽目标，测试床和模拟结果趋势一致，同时测量了QoE的MOS评分显示用户满意度。

Conclusion: 所提出的集成架构能够仅通过用户用英语指定意图就自主确保网络性能目标达成，实现了网络管理的自动化。

Abstract: The transition to Sixth Generation (6G) networks presents challenges in
managing quality of service (QoS) of diverse applications and achieving Service
Level Agreements (SLAs) under varying network conditions. Hence, network
management must be automated with the help of Machine Learning (ML) and
Artificial Intelligence (AI) to achieve real-time requirements. Zero touch
network (ZTN) is one of the frameworks to automate network management with
mechanisms such as closed loop control to ensure that the goals are met
perpetually. Intent- Based Networking (IBN) specifies the user intents with
diverse network requirements or goals which are then translated into specific
network configurations and actions. This paper presents a novel architecture
for integrating IBN and ZTN to serve the intent goals. Users provides the
intent in the form of natural language, e.g., English, which is then translated
using natural language processing (NLP) techniques (e.g., retrieval augmented
generation (RAG)) into Network Intent LanguagE (Nile). The Nile intent is then
passed on to the BiLSTM and Q-learning based ZTN closed loop framework as a
goal which maintains the intent under varying network conditions. Thus, the
proposed architecture can work autonomously to ensure the network performance
goal is met by just specifying the user intent in English. The integrated
architecture is also implemented on a testbed using OpenAirInterface (OAI).
Additionally, to evaluate the architecture, an optimization problem is
formulated which evaluated with Monte Carlo simulations. Results demonstrate
how ZTN can help achieve the bandwidth goals autonomously set by user intent.
The simulation and the testbed results are compared and they show similar
trend. Mean Opinion Score (MOS) for Quality of Experience (QoE) is also
measured to indicate the user satisfaction of the intent.

</details>


### [6] [RePro: Leveraging Large Language Models for Semi-Automated Reproduction of Networking Research Results](https://arxiv.org/abs/2509.21074)
*Yining Jiang,Wenyun Xu,Qingyu Song,Yuling Lin,Xuanhao Liu,Xiaoqiang Zheng,Qiang Su,Lizhao You,Lu Tang,Wangjian Feng,Linghe Kong,Qiao Xiang,Jiwu Shu*

Main category: cs.NI

TL;DR: RePro是一个半自动化的网络研究复现框架，利用先进的提示工程技术从研究论文中复现网络系统。它结合了少样本上下文学习和结构化思维链技术，显著减少了复现时间。


<details>
  <summary>Details</summary>
Motivation: 网络研究复现因开源代码稀缺而具有挑战性，现有LLM方法缺乏对多样化网络领域的泛化能力。

Method: 采用三阶段流水线：系统描述提取、结构化代码生成和代码优化，结合SCoT/SeCoT技术将论文描述转化为可执行实现。

Result: 在五个先进LLM和多样化网络子领域的评估中，RePro显著减少复现时间，同时达到可比的系统性能。

Conclusion: RePro框架验证了其在网络研究复现中的有效性和效率，为自动化代码生成提供了可行解决方案。

Abstract: Reproducing networking research is a critical but challenging task due to the
scarcity of open-source code. While Large Language Models (LLMs) can automate
code generation, current approaches lack the generalizability required for the
diverse networking field. To address this, we propose RePro, a semi-automated
reproduction framework that leverages advanced prompt engineering to reproduce
network systems from their research papers. RePro combines few-shot in-context
learning with Structured and Semantic Chain of Thought (SCoT/SeCoT) techniques
to systematically translate a paper's description into an optimized, executable
implementation. The framework operates through a three-stage pipeline: system
description extraction, structural code generation, and code optimization. Our
evaluation with five state-of-the-art LLMs across diverse network sub-domains
demonstrates that RePro significantly reduces reproduction time compared to
manual efforts while achieving comparable system performance, validating its
effectiveness and efficiency.

</details>


### [7] [Hybrid RIS-Aided Digital Over-the-Air Computing for Edge AI Inference: Joint Feature Quantization and Active-Passive Beamforming Design](https://arxiv.org/abs/2509.21201)
*Yang Fu,Peng Qin,Liming Chen,Yifei Wang*

Main category: cs.NI

TL;DR: 本文提出了一种混合RIS辅助的数字空中计算方案，用于6G网络中的边缘推理，通过优化量化、传输和反射波束成形来提高感知精度


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持边缘推理，传统空中计算与数字通信系统不兼容，而混合RIS架构能同时实现信号放大和反射，具有增强AirComp的潜力

Method: 采用矢量量化将高维特征映射为离散码字，通过数字调制传输，联合优化量化比特分配、传输系数、接收波束成形和混合RIS反射波束成形

Result: 实验结果表明，所提出的HRD-AirComp方案在推理精度和不确定性方面均优于基线方法

Conclusion: HRD-AirComp方案成功实现了数字通信系统与空中计算的兼容，为6G边缘推理提供了有效的任务导向设计框架

Abstract: The vision of 6G networks aims to enable edge inference by leveraging
ubiquitously deployed artificial intelligence (AI) models, facilitating
intelligent environmental perception for a wide range of applications. A
critical operation in edge inference is for an edge node (EN) to aggregate
multi-view sensory features extracted by distributed agents, thereby boosting
perception accuracy. Over-the-air computing (AirComp) emerges as a promising
technique for rapid feature aggregation by exploiting the waveform
superposition property of analog-modulated signals, which is, however,
incompatible with existing digital communication systems. Meanwhile, hybrid
reconfigurable intelligent surface (RIS), a novel RIS architecture capable of
simultaneous signal amplification and reflection, exhibits potential for
enhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital
AirComp (HRD-AirComp) scheme, which employs vector quantization to map
high-dimensional features into discrete codewords that are digitally modulated
into symbols for wireless transmission. By judiciously adjusting the AirComp
transceivers and hybrid RIS reflection to control signal superposition across
agents, the EN can estimate the aggregated features from the received signals.
To endow HRD-AirComp with a task-oriented design principle, we derive a
surrogate function for inference accuracy that characterizes the impact of
feature quantization and over-the-air aggregation. Based on this surrogate, we
formulate an optimization problem targeting inference accuracy maximization,
and develop an efficient algorithm to jointly optimize the quantization bit
allocation, agent transmission coefficients, EN receiving beamforming, and
hybrid RIS reflection beamforming. Experimental results demonstrate that the
proposed HRD-AirComp outperforms baselines in terms of both inference accuracy
and uncertainty.

</details>


### [8] [Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks](https://arxiv.org/abs/2509.21259)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.NI

TL;DR: 提出了一种基于语义通信的边缘-云框架，用于实时交通监控，通过YOLOv11检测感兴趣区域，ViT生成紧凑嵌入向量，在云端重建图像后由多模态LLM分析，实现99.9%的数据传输减少和89%的响应准确率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘摄像头在智能交通系统中部署多模态LLM时面临的计算资源不足和带宽限制问题，确保实时交通监控的性能。

Method: 使用YOLOv11检测感兴趣区域，裁剪相关图像片段，通过Vision Transformer转换为紧凑嵌入向量传输到云端，云端解码器重建图像后由多模态LLM生成交通状况描述。

Result: 该方法实现了99.9%的数据传输量减少，重建裁剪图像的LLM响应准确率达到89%，接近原始裁剪图像93%的准确率。

Conclusion: ViT和LLM辅助的边缘-云语义通信框架在实时交通监控中具有高效性和实用性，显著降低了传输开销同时保持了良好的分析性能。

Abstract: Real-time urban traffic surveillance is vital for Intelligent Transportation
Systems (ITS) to ensure road safety, optimize traffic flow, track vehicle
trajectories, and prevent collisions in smart cities. Deploying edge cameras
across urban environments is a standard practice for monitoring road
conditions. However, integrating these with intelligent models requires a
robust understanding of dynamic traffic scenarios and a responsive interface
for user interaction. Although multimodal Large Language Models (LLMs) can
interpret traffic images and generate informative responses, their deployment
on edge devices is infeasible due to high computational demands. Therefore, LLM
inference must occur on the cloud, necessitating visual data transmission from
edge to cloud, a process hindered by limited bandwidth, leading to potential
delays that compromise real-time performance. To address this challenge, we
propose a semantic communication framework that significantly reduces
transmission overhead. Our method involves detecting Regions of Interest (RoIs)
using YOLOv11, cropping relevant image segments, and converting them into
compact embedding vectors using a Vision Transformer (ViT). These embeddings
are then transmitted to the cloud, where an image decoder reconstructs the
cropped images. The reconstructed images are processed by a multimodal LLM to
generate traffic condition descriptions. This approach achieves a 99.9%
reduction in data transmission size while maintaining an LLM response accuracy
of 89% for reconstructed cropped images, compared to 93% accuracy with original
cropped images. Our results demonstrate the efficiency and practicality of ViT
and LLM-assisted edge-cloud semantic communication for real-time traffic
surveillance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [An Approach to Checking Correctness for Agentic Systems](https://arxiv.org/abs/2509.20364)
*Thomas J Sheffler*

Main category: cs.AI

TL;DR: 提出了一种用于监控AI代理行为的时间表达式语言，通过分析代理工具调用和状态转换的执行轨迹来检测LLM代理系统中的行为偏差，解决了传统基于文本匹配的错误检测方法的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理系统的错误检测主要依赖输入输出的文本匹配，但由于LLM响应的自然语言变异性，这种方法很脆弱。需要一种独立于具体文本输出的系统行为验证方法。

Method: 借鉴硬件验证中的时序逻辑技术，开发了一种时间表达式语言来监控代理工具调用和状态转换的执行轨迹。该方法关注代理动作序列（如工具调用和代理间通信），而非具体文本输出。

Result: 在三代理系统测试中，当使用大型模型时所有时间断言都得到满足，但当两个代理使用较小模型时，执行违反了行为断言，主要由于工具序列不当和协调交接失败。时间表达式成功标记了这些异常。

Conclusion: 该方法为系统监控AI代理可靠性提供了基础，特别适用于关键应用中的生产级代理系统，能够有效检测行为回归问题。

Abstract: This paper presents a temporal expression language for monitoring AI agent
behavior, enabling systematic error-detection of LLM-based agentic systems that
exhibit variable outputs due to stochastic generation processes. Drawing from
temporal logic techniques used in hardware verification, this approach monitors
execution traces of agent tool calls and state transitions to detect deviations
from expected behavioral patterns. Current error-detection approaches rely
primarily on text matching of inputs and outputs, which proves fragile due to
the natural language variability inherent in LLM responses. The proposed method
instead focuses on the sequence of agent actions -- such as tool invocations
and inter-agent communications -- allowing verification of system behavior
independent of specific textual outputs. The temporal expression language
provides assertions that capture correct behavioral patterns across multiple
execution scenarios. These assertions serve dual purposes: validating prompt
engineering and guardrail effectiveness during development, and providing
regression testing when agents are updated with new LLMs or modified logic. The
approach is demonstrated using a three-agent system, where agents coordinate to
solve multi-step reasoning tasks. When powered by large, capable models, all
temporal assertions were satisfied across many test runs. However, when smaller
models were substituted in two of the three agents, executions violated
behavioral assertions, primarily due to improper tool sequencing and failed
coordination handoffs. The temporal expressions successfully flagged these
anomalies, demonstrating the method's effectiveness for detecting behavioral
regressions in production agentic systems. This approach provides a foundation
for systematic monitoring of AI agent reliability as these systems become
increasingly deployed in critical applications.

</details>


### [10] [LATTS: Locally Adaptive Test-Time Scaling](https://arxiv.org/abs/2509.20368)
*Theo Uscidda,Matthew Trager,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 提出了一种名为LATTS的自适应测试时扩展方法，通过验证器模型动态调整每个生成步骤的计算资源分配，提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有验证器方法在测试时对所有样本和生成步骤均匀增加计算资源，不考虑个体实例的复杂性，导致资源使用效率低下。

Method: LATTS在每个生成步骤使用验证器接受准则来决定是否重新采样、回溯、重启或停止生成过程，基于验证器模型提供的局部难度概念动态调整计算资源。

Result: 实验结果表明，LATTS相比标准验证器方法实现了显著更优的准确率-计算权衡。

Conclusion: LATTS通过局部自适应计算分配策略，有效解决了测试时扩展中的计算资源效率问题。

Abstract: One common strategy for improving the performance of Large Language Models
(LLMs) on downstream tasks involves using a \emph{verifier model} to either
select the best answer from a pool of candidates or to steer the
auto-regressive generation process towards better outputs. This class of
methods typically results in improved accuracy at the cost of increased
computation at test-time, a paradigm known as \emph{test-time scaling}.
However, most existing approaches increase computation uniformly across all
samples and generation steps, without considering the complexity of individual
instances, leading to inefficient resource use. We address this limitation by
proposing an approach, called \emph{Locally Adaptive Test-Time Scaling
(LATTS)}, that allocates variable compute across generation steps.
Specifically, at each generation step, LATTS employs a verifier-based
acceptance criterion to decide whether to resample, backtrack, restart, or stop
the generation process. This criterion effectively adjusts the per-step
computational effort based on a precise notion of \emph{local difficulty}
derived from the verifier model. Empirical results show that LATTS achieves
significantly superior accuracy--compute tradeoffs compared to standard
verifier-based methods.

</details>


### [11] [Philosophy-informed Machine Learning](https://arxiv.org/abs/2509.20370)
*MZ Naser*

Main category: cs.AI

TL;DR: 本文提出哲学启发机器学习（PhIML）方法，将分析哲学核心思想直接融入机器学习模型架构、目标和评估协议中，旨在设计出尊重哲学概念和价值观的模型。


<details>
  <summary>Details</summary>
Motivation: 通过将哲学思想融入机器学习，使模型能够更好地理解和尊重人类价值观，提高模型的伦理责任和安全性。

Method: 采用概念基础回顾展示哲学收益和对齐，并通过案例研究展示如何将PhIML作为后处理工具或内建到模型架构中。

Result: 提出了PhIML的理论框架和应用方法，展示了哲学与机器学习结合的可能性。

Conclusion: 指出了PhIML面临的技术、哲学、实践和治理挑战，并规划了实现安全、哲学意识和伦理责任的PhIML研究路线图。

Abstract: Philosophy-informed machine learning (PhIML) directly infuses core ideas from
analytic philosophy into ML model architectures, objectives, and evaluation
protocols. Therefore, PhIML promises new capabilities through models that
respect philosophical concepts and values by design. From this lens, this paper
reviews conceptual foundations to demonstrate philosophical gains and
alignment. In addition, we present case studies on how ML users/designers can
adopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML
model architectures. Finally, this paper sheds light on open technical barriers
alongside philosophical, practical, and governance challenges and outlines a
research roadmap toward safe, philosophy-aware, and ethically responsible
PhIML.

</details>


### [12] [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
*Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos*

Main category: cs.AI

TL;DR: InsightGUIDE是一个AI驱动的阅读助手工具，旨在为科研文献提供简洁、结构化的见解，而不是替代阅读源材料


<details>
  <summary>Details</summary>
Motivation: 科学文献的激增给研究人员带来了挑战，现有工具提供的冗长摘要往往替代而非辅助阅读源材料

Method: 系统将专家的阅读方法嵌入核心AI逻辑，采用提示驱动的方法论，提供结构化见解作为论文关键要素的"地图"

Result: 与通用LLM相比，InsightGUIDE产生更结构化、可操作的指导，定性案例研究证明了其有效性

Conclusion: InsightGUIDE作为现代研究人员更有效的工具，能够提供更好的阅读辅助功能

Abstract: The proliferation of scientific literature presents an increasingly
significant challenge for researchers. While Large Language Models (LLMs) offer
promise, existing tools often provide verbose summaries that risk replacing,
rather than assisting, the reading of the source material. This paper
introduces InsightGUIDE, a novel AI-powered tool designed to function as a
reading assistant, not a replacement. Our system provides concise, structured
insights that act as a "map" to a paper's key elements by embedding an expert's
reading methodology directly into its core AI logic. We present the system's
architecture, its prompt-driven methodology, and a qualitative case study
comparing its output to a general-purpose LLM. The results demonstrate that
InsightGUIDE produces more structured and actionable guidance, serving as a
more effective tool for the modern researcher.

</details>


### [13] [Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems](https://arxiv.org/abs/2509.20513)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 提出了一种新颖的重构框架，用于动态验证和组装时间触发系统的调度方案，解决消息碰撞、优先级处理不当等问题，确保系统安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 时间触发系统在动态操作环境中面临消息碰撞、优先级处理不当导致的锁定循环等问题，这些挑战会损害系统安全性和性能，需要有效的调度验证和组装机制。

Method: 通过系统地将AI生成或启发式得出的调度优先级转换为完全可执行的调度方案，确保遵守关键系统约束，包括优先级规则和无碰撞通信，并整合了安全检查、高效分配算法和恢复机制。

Result: 实验结果表明，该框架显著提高了系统适应性、操作完整性和运行时性能，同时保持了计算效率，在多个性能指标上表现优异。

Conclusion: 这项工作为安全关键时间触发系统提供了一种实用且可扩展的安全调度生成解决方案，即使在高度动态和不确定的操作条件下也能实现可靠和灵活的实时调度。

Abstract: Adaptive scheduling is crucial for ensuring the reliability and safety of
time-triggered systems (TTS) in dynamic operational environments. Scheduling
frameworks face significant challenges, including message collisions, locked
loops from incorrect precedence handling, and the generation of incomplete or
invalid schedules, which can compromise system safety and performance. To
address these challenges, this paper presents a novel reconstruction framework
designed to dynamically validate and assemble schedules. The proposed
reconstruction models operate by systematically transforming AI-generated or
heuristically derived scheduling priorities into fully executable schedules,
ensuring adherence to critical system constraints such as precedence rules and
collision-free communication. It incorporates robust safety checks, efficient
allocation algorithms, and recovery mechanisms to handle unexpected context
events, including hardware failures and mode transitions. Comprehensive
experiments were conducted across multiple performance profiles, including
makespan minimisation, workload balancing, and energy efficiency, to validate
the operational effectiveness of the reconstruction models. Results demonstrate
that the proposed framework significantly enhances system adaptability,
operational integrity, and runtime performance while maintaining computational
efficiency. Overall, this work contributes a practical and scalable solution to
the problem of safe schedule generation in safety-critical TTS, enabling
reliable and flexible real-time scheduling even under highly dynamic and
uncertain operational conditions.

</details>


### [14] [Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications](https://arxiv.org/abs/2509.20520)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 提出了一种集成在元调度器中的自适应在线学习单元，使用强化学习来动态扩展多调度图，以解决离线训练AI调度推理时面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统离线训练方法在构建全面多调度图时面临资源密集和不可行的挑战，特别是在考虑硬件故障、松弛变化等上下文事件时。离线训练只能覆盖概率空间的一个子集。

Method: 在元调度器中集成自适应在线学习单元，使用强化学习模型持续探索和发现新的调度解决方案，动态扩展多调度图。

Result: 在线学习单元能够处理意外事件和复杂调度场景，优化现有调度器，并在引入更严格截止时间或新性能标准时保持系统灵活性。

Conclusion: 通过实时训练持续优化AI推理，系统在大规模安全关键环境中保持灵活性和鲁棒性，确保任务执行的可靠性和效率。

Abstract: Metascheduling in time-triggered architectures has been crucial in adapting
to dynamic and unpredictable environments, ensuring the reliability and
efficiency of task execution. However, traditional approaches face significant
challenges when training Artificial Intelligence (AI) scheduling inferences
offline, particularly due to the complexities involved in constructing a
comprehensive Multi-Schedule Graph (MSG) that accounts for all possible
scenarios. The process of generating an MSG that captures the vast probability
space, especially when considering context events like hardware failures, slack
variations, or mode changes, is resource-intensive and often infeasible. To
address these challenges, we propose an adaptive online learning unit
integrated within the metascheduler to enhance performance in real-time. The
primary motivation for developing this unit stems from the limitations of
offline training, where the MSG created is inherently a subset of the complete
space, focusing only on the most probable and critical context events. In the
online mode, Reinforcement Learning (RL) plays a pivotal role by continuously
exploring and discovering new scheduling solutions, thus expanding the MSG and
enhancing system performance over time. This dynamic adaptation allows the
system to handle unexpected events and complex scheduling scenarios more
effectively. Several RL models were implemented within the online learning
unit, each designed to address specific challenges in scheduling. These models
not only facilitate the discovery of new solutions but also optimize existing
schedulers, particularly when stricter deadlines or new performance criteria
are introduced. By continuously refining the AI inferences through real-time
training, the system remains flexible and capable of meeting evolving demands,
thus ensuring robustness and efficiency in large-scale, safety-critical
environments.

</details>


### [15] [A Compound Classification System Based on Fuzzy Relations Applied to the Noise-Tolerant Control of a Bionic Hand via EMG Signal Recognition](https://arxiv.org/abs/2509.20523)
*Pawel Trajdos,Marek Kurzynski*

Main category: cs.AI

TL;DR: 提出了一种用于肌电假肢控制的新型识别系统，通过检测受污染的生物信号来减轻污染的不利影响。该系统包含两个集成：一组用于评估单个通道污染程度的单类分类器，以及一个用于识别患者意图的K近邻分类器集成。


<details>
  <summary>Details</summary>
Motivation: 现代仿生上肢假肢通常使用肌电生物信号进行模式识别控制，但生物信号易受污染，这会显著降低识别系统的分类质量。需要开发能够检测和减轻污染影响的识别系统。

Method: 开发了一个包含两个集成的模糊识别系统：一组单类分类器用于评估各通道的污染程度，一个K近邻分类器集成用于意图识别。整个识别过程采用统一的模糊决策方案。

Result: 使用公共存储库中的真实生物信号进行了实验评估，对开发方法的参数和程序进行了比较分析，并与文献中的类似系统进行了比较。

Conclusion: 提出的模糊识别系统能够有效检测受污染的肌电信号，并通过集成方法提高假肢控制的识别质量，为生物信号污染问题提供了有效的解决方案。

Abstract: Modern anthropomorphic upper limb bioprostheses are typically controlled by
electromyographic (EMG) biosignals using a pattern recognition scheme.
Unfortunately, there are many factors originating from the human source of
objects to be classified and from the human-prosthesis interface that make it
difficult to obtain an acceptable classification quality. One of these factors
is the high susceptibility of biosignals to contamination, which can
considerably reduce the quality of classification of a recognition system.
  In the paper, the authors propose a new recognition system intended for EMG
based control of the hand prosthesis with detection of contaminated biosignals
in order to mitigate the adverse effect of contaminations. The system consists
of two ensembles: the set of one-class classifiers (OCC) to assess the degree
of contamination of individual channels and the ensemble of K-nearest
neighbours (KNN) classifier to recognise the patient's intent. For all
recognition systems, an original, coherent fuzzy model was developed, which
allows the use of a uniform soft (fuzzy) decision scheme throughout the
recognition process. The experimental evaluation was conducted using real
biosignals from a public repository. The goal was to provide an experimental
comparative analysis of the parameters and procedures of the developed method
on which the quality of the recognition system depends. The proposed fuzzy
recognition system was also compared with similar systems described in the
literature.

</details>


### [16] [SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection](https://arxiv.org/abs/2509.20562)
*Yubin Ge,Salvatore Romeo,Jason Cai,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: 本文提出SAMULE框架，通过多级反思合成训练回顾性语言模型，使LLM代理能够在单轨迹、任务内和任务间三个层次进行有效反思，显著提升复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在复杂任务中难以生成有意义的反思，主要由于错误分析不足和对罕见成功轨迹的依赖。

Method: 提出SAMULE框架，包含三个层次的反思合成：单轨迹学习（微观）、任务内学习（中观）和任务间学习（宏观），并基于此训练回顾性语言模型。还扩展了基于前瞻的反思机制用于交互设置。

Result: 在TravelPlanner、NATURAL PLAN和Tau-bench三个基准测试上的实验表明，该方法显著优于基于反思的基线方法。

Conclusion: 精心设计的反思合成和以失败为中心的学习在构建自我改进的LLM代理中起着关键作用。

Abstract: Despite the rapid advancements in LLM agents, they still face the challenge
of generating meaningful reflections due to inadequate error analysis and a
reliance on rare successful trajectories, especially in complex tasks. In this
work, we propose SAMULE, a new framework for self-learning agents powered by a
retrospective language model that is trained based on Multi-Level Reflection
Synthesis. It first synthesizes high-quality reflections across three
complementary levels: Single-Trajectory Learning (micro-level) for detailed
error correction; Intra-Task Learning (meso-level) to build error taxonomies
across multiple trials of the same task, and Inter-Task Learning (macro-level)
to extract transferable insights based on same typed errors from diverse task
failures. Then we fine-tune a language model serving as the retrospective model
to generate reflections during inference. We further extend our framework to
interactive settings through a foresight-based reflection mechanism, enabling
agents to proactively reflect and adapt during user interactions by comparing
predicted and actual responses. Extensive experiments on three challenging
benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our
approach significantly outperforms reflection-based baselines. Our results
highlight the critical role of well-designed reflection synthesis and
failure-centric learning in building self-improving LLM agents.

</details>


### [17] [Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI](https://arxiv.org/abs/2509.20640)
*Oluwakemi T. Olayinka,Sumeet Jeswani,Divine Iloh*

Main category: cs.AI

TL;DR: 该研究提出了一种基于智能AI代理的自适应网络安全架构，能够通过动态学习和上下文感知决策来解决传统静态安全模型在可扩展性、实时检测和上下文响应方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统静态网络安全模型在当前包含云服务、API、移动平台和边缘设备的数字产品生态系统中，难以应对可扩展性、实时检测和上下文响应性的挑战。

Method: 开发了自主目标驱动代理，集成了行为基线分析、去中心化风险评分和联邦威胁情报共享等关键功能，通过原生云模拟验证系统能力。

Result: 评估结果显示系统在适应性、响应延迟和检测准确性方面均有显著提升，能够有效识别零日攻击并动态调整访问策略。

Conclusion: 该架构为保护复杂数字基础设施提供了智能可扩展的蓝图，与零信任模型兼容，支持遵守国际网络安全法规。

Abstract: Traditional static cybersecurity models often struggle with scalability,
real-time detection, and contextual responsiveness in the current digital
product ecosystems which include cloud services, application programming
interfaces (APIs), mobile platforms, and edge devices. This study introduces
autonomous goal driven agents capable of dynamic learning and context-aware
decision making as part of an adaptive cybersecurity architecture driven by
agentic artificial intelligence (AI). To facilitate autonomous threat
mitigation, proactive policy enforcement, and real-time anomaly detection, this
framework integrates agentic AI across the key ecosystem layers. Behavioral
baselining, decentralized risk scoring, and federated threat intelligence
sharing are important features. The capacity of the system to identify zero-day
attacks and dynamically modify access policies was demonstrated through native
cloud simulations. The evaluation results show increased adaptability,
decreased response latency, and improved detection accuracy. The architecture
provides an intelligent and scalable blueprint for safeguarding complex digital
infrastructure and is compatible with zero-trust models, thereby supporting the
adherence to international cybersecurity regulations.

</details>


### [18] [Accelerate Creation of Product Claims Using Generative AI](https://arxiv.org/abs/2509.20652)
*Po-Yu Liang,Yong Zhang,Tatiana Hwa,Aaron Byers*

Main category: cs.AI

TL;DR: 开发了Claim Advisor网络应用，利用大语言模型的上下文学习和微调技术来加速产品声明的创建过程，包括搜索、生成、优化和模拟功能。


<details>
  <summary>Details</summary>
Motivation: 产品声明是影响消费者购买行为的关键因素，但传统创建过程耗时耗资。需要一种更高效的方法来加速声明创建流程。

Method: 使用大语言模型的上下文学习和微调技术，开发具有三个核心功能的Claim Advisor应用：语义搜索现有声明、基于产品描述和消费者画像生成优化声明、通过合成消费者模拟进行声明排名。

Result: 在消费品公司应用中显示出非常有前景的结果，证明该能力在不同产品类别和行业中具有广泛适用性。

Conclusion: 这项技术在不同行业具有广泛应用价值，鼓励生成式AI在各个行业的研究和应用。

Abstract: The benefit claims of a product is a critical driver of consumers' purchase
behavior. Creating product claims is an intense task that requires substantial
time and funding. We have developed the $\textbf{Claim Advisor}$ web
application to accelerate claim creations using in-context learning and
fine-tuning of large language models (LLM). $\textbf{Claim Advisor}$ was
designed to disrupt the speed and economics of claim search, generation,
optimization, and simulation. It has three functions: (1) semantically
searching and identifying existing claims and/or visuals that resonate with the
voice of consumers; (2) generating and/or optimizing claims based on a product
description and a consumer profile; and (3) ranking generated and/or manually
created claims using simulations via synthetic consumers. Applications in a
consumer packaged goods (CPG) company have shown very promising results. We
believe that this capability is broadly useful and applicable across product
categories and industries. We share our learning to encourage the research and
application of generative AI in different industries.

</details>


### [19] [An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans](https://arxiv.org/abs/2509.20707)
*Junjie Cui,Peilong Wang,Jason Holmes,Leshan Sun,Michael L. Hinni,Barbara A. Pockaj,Sujay A. Vora,Terence T. Sio,William W. Wong,Nathan Y. Yu,Steven E. Schild,Joshua R. Niska,Sameer R. Keole,Jean-Claude M. Rwigema,Samir H. Patel,Lisa A. McGee,Carlos A. Vargas,Wei Liu*

Main category: cs.AI

TL;DR: 开发基于LLaMA-4 109B的检索增强生成系统，用于放疗治疗计划的自动化、协议感知和可解释评估


<details>
  <summary>Details</summary>
Motivation: 为放疗治疗计划提供透明、可扩展的评估方法，结合结构化群体评分和模块化工具增强推理

Method: 构建多协议数据集和知识库，集成检索引擎、百分位预测组件和临床约束检查器，使用多步提示驱动推理管道

Result: 检索超参数优化后实现完美近邻准确率，端到端系统与独立模块计算结果100%一致，证实所有步骤可靠执行

Conclusion: 系统提供可追溯输出，减少幻觉，跨协议表现出鲁棒性，未来将进行临床验证和改进领域适应检索模型

Abstract: Purpose: To develop a retrieval-augmented generation (RAG) system powered by
LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of
radiotherapy treatment plans.
  Methods and Materials: We curated a multi-protocol dataset of 614
radiotherapy plans across four disease sites and constructed a knowledge base
containing normalized dose metrics and protocol-defined constraints. The RAG
system integrates three core modules: a retrieval engine optimized across five
SentenceTransformer backbones, a percentile prediction component based on
cohort similarity, and a clinical constraint checker. These tools are directed
by a large language model (LLM) using a multi-step prompt-driven reasoning
pipeline to produce concise, grounded evaluations.
  Results: Retrieval hyperparameters were optimized using Gaussian Process on a
scalarized loss function combining root mean squared error (RMSE), mean
absolute error (MAE), and clinically motivated accuracy thresholds. The best
configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor
accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested
end-to-end, the RAG system achieved 100% agreement with the computed values by
standalone retrieval and constraint-checking modules on both percentile
estimates and constraint identification, confirming reliable execution of all
retrieval, prediction and checking steps.
  Conclusion: Our findings highlight the feasibility of combining structured
population-based scoring with modular tool-augmented reasoning for transparent,
scalable plan evaluation in radiation therapy. The system offers traceable
outputs, minimizes hallucination, and demonstrates robustness across protocols.
Future directions include clinician-led validation, and improved domain-adapted
retrieval models to enhance real-world integration.

</details>


### [20] [Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent](https://arxiv.org/abs/2509.20729)
*Jiazheng Sun,Te Yang,Jiayang Niu,Mingxuan Li,Yongyong Lu,Ruimeng Yang,Xin Peng*

Main category: cs.AI

TL;DR: Fairy是一个交互式多代理移动助手，通过跨应用协作、交互执行和持续学习来解决现有方法在多样化应用界面和用户需求中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型在移动GUI代理中表现不佳，特别是在处理长尾应用和用户交互时，端到端方法依赖常识推理容易失败，而无用户交互的代理会损害用户体验。

Method: Fairy包含三个核心模块：(i)全局任务规划器从跨应用视角分解用户任务；(ii)应用级执行器基于长短时记忆将子任务细化为步骤和动作，通过四个核心代理在双循环中实现精确执行和用户交互；(iii)自学习器将执行经验整合到应用地图和技巧中。

Result: 实验显示，基于GPT-4o的Fairy在RealMobile-Eval基准上比之前的最先进方法提升了33.7%的用户需求完成率，并减少了58.5%的冗余步骤。

Conclusion: Fairy通过其交互和自我学习机制，显著提高了移动GUI代理在真实场景中的性能，证明了其设计的有效性。

Abstract: Large multi-modal models (LMMs) have advanced mobile GUI agents. However,
existing methods struggle with real-world scenarios involving diverse app
interfaces and evolving user needs. End-to-end methods relying on model's
commonsense often fail on long-tail apps, and agents without user interaction
act unilaterally, harming user experience. To address these limitations, we
propose Fairy, an interactive multi-agent mobile assistant capable of
continuously accumulating app knowledge and self-evolving during usage. Fairy
enables cross-app collaboration, interactive execution, and continual learning
through three core modules:(i) a Global Task Planner that decomposes user tasks
into sub-tasks from a cross-app view; (ii) an App-Level Executor that refines
sub-tasks into steps and actions based on long- and short-term memory,
achieving precise execution and user interaction via four core agents operating
in dual loops; and (iii) a Self-Learner that consolidates execution experience
into App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a
real-world benchmark with a comprehensive metric suite, and LMM-based agents
for automated scoring. Experiments show that Fairy with GPT-4o backbone
outperforms the previous SoTA by improving user requirement completion by 33.7%
and reducing redundant steps by 58.5%, showing the effectiveness of its
interaction and self-learning.

</details>


### [21] [Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning](https://arxiv.org/abs/2509.20744)
*Qihang Ai,Haiyun Jiang*

Main category: cs.AI

TL;DR: 提出了一种结合自回归(AR)和非自回归(NAR)语言模型的新框架，用于推理任务。NAR模型并行生成中间推理轨迹，AR模型基于这些轨迹生成精确最终答案，在保持质量的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: AR模型生成连贯但推理速度慢，特别是在数学和代码等需要长推理链的领域；NAR模型速度快但输出质量较低。需要结合两者优势来解决推理任务的效率和质量问题。

Method: 使用NAR模型（如离散扩散模型）并行生成中间推理轨迹，然后让AR模型基于这些轨迹生成最终答案，实现高效且高质量的推理。

Result: 实验表明该方法相比强基线有26%的性能提升，同时显著降低了推理成本。

Conclusion: AR-NAR混合框架在推理任务中实现了速度和质量的双重优势，为高效推理提供了新的解决方案。

Abstract: We study reasoning tasks through a framework that integrates auto-regressive
(AR) and non-autoregressive (NAR) language models. AR models, which generate
text sequentially, excel at producing coherent outputs but often suffer from
slow inference, particularly in reasoning-intensive domains such as mathematics
and code, where lengthy chains of thought are required. In contrast, NAR
models, such as discrete diffusion models, allow parallel generation and offer
substantial speedups, though typically at the cost of reduced output quality.
To address these limitations, we introduce a new paradigm in which an NAR model
efficiently produces intermediate reasoning traces, which subsequently guide an
AR model to deliver precise final answers. Experiments demonstrate that our
approach yields significant 26% improvements over strong baselines while
substantially reducing inference cost.

</details>


### [22] [Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning](https://arxiv.org/abs/2509.20754)
*Yufan Mao,Hanjing Ye,Wenlong Dong,Chengjie Zhang,Hong Zhang*

Main category: cs.AI

TL;DR: 提出Meta-Memory，一个基于大语言模型的机器人记忆系统，通过语义和空间模态的联合推理来检索和整合记忆，以回答自然语言位置查询


<details>
  <summary>Details</summary>
Motivation: 解决机器人在复杂环境中存储观察结果作为记忆，并利用这些记忆回答人类关于空间位置查询的关键研究挑战

Method: 构建高密度环境记忆表示，通过联合语义和空间模态推理进行记忆检索和整合

Result: 在SpaceLocQA和NaVQA基准测试中显著优于现有方法，并在真实机器人平台上成功部署

Conclusion: Meta-Memory为机器人提供了强大准确的空间推理能力，在复杂环境中具有实际应用价值

Abstract: Navigating complex environments requires robots to effectively store
observations as memories and leverage them to answer human queries about
spatial locations, which is a critical yet underexplored research challenge.
While prior work has made progress in constructing robotic memory, few have
addressed the principled mechanisms needed for efficient memory retrieval and
integration. To bridge this gap, we propose Meta-Memory, a large language model
(LLM)-driven agent that constructs a high-density memory representation of the
environment. The key innovation of Meta-Memory lies in its capacity to retrieve
and integrate relevant memories through joint reasoning over semantic and
spatial modalities in response to natural language location queries, thereby
empowering robots with robust and accurate spatial reasoning capabilities. To
evaluate its performance, we introduce SpaceLocQA, a large-scale dataset
encompassing diverse real-world spatial question-answering scenarios.
Experimental results show that Meta-Memory significantly outperforms
state-of-the-art methods on both the SpaceLocQA and the public NaVQA
benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world
robotic platforms, demonstrating its practical utility in complex environments.
Project page: https://itsbaymax.github.io/meta-memory.github.io/ .

</details>


### [23] [LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks](https://arxiv.org/abs/2509.20798)
*Lipeng Ma,Yixuan Li,Weidong Yang,Mingjie Zhou,Xinyi Liu,Ben Fei,Shuhao Li,Xiaoyan Sun,Sihang Jiang,Yanghua Xiao*

Main category: cs.AI

TL;DR: LogReasoner是一个从粗到细的推理增强框架，旨在让LLM能够像专家一样进行日志分析推理，通过两阶段方法显著提升LLM在日志分析任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 通用LLM难以制定符合专家认知的结构化推理工作流程，无法提供精确的推理步骤细节，这限制了LLM在日志分析任务中的应用效果。

Method: LogReasoner包含两个阶段：(1)粗粒度专家思维增强：从故障排除流程图构建高层次专家思维，使LLM能够制定结构化推理工作流程；(2)细粒度具体步骤增强：通过任务特定的逐步解决方案微调LLM，并使用偏好学习校准推理细节。

Result: 在四个不同的日志分析任务上评估，使用Qwen-2.5和Llama-3等开源LLM，LogReasoner显著优于现有LLM，达到最先进性能。

Conclusion: LogReasoner有效增强了LLM在日志分析中的推理能力，证明了该框架在提升LLM分析粒度和正确性方面的有效性。

Abstract: Log analysis is crucial for monitoring system health and diagnosing failures
in complex systems. Recent advances in large language models (LLMs) offer new
opportunities for automated log analysis, leveraging their reasoning
capabilities to perform tasks such as anomaly detection and failure prediction.
However, general-purpose LLMs struggle to formulate structured reasoning
workflows that align with expert cognition and deliver precise details of
reasoning steps. To address these challenges, we propose LogReasoner, a
coarse-to-fine reasoning enhancement framework designed to enable LLMs to
reason log analysis tasks like experts. LogReasoner consists of two stages: (1)
coarse-grained enhancement of expert thinking, where high-level expert thoughts
are constructed from collected troubleshooting flowcharts and existing tasks to
enable LLMs to formulate structured reasoning workflows and (2) fine-grained
enhancement of specific steps, where we first fine-tune the LLM with
task-specific stepwise solutions to enhance the LLM for instantiated reasoning,
then employ the preference learning to calibrate the LLM's reasoning details
from its mistakes, further strengthen the LLM's analytical granularity and
correctness. We evaluate LogReasoner on four distinct log analysis tasks using
open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that
LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art
performance and demonstrating its effectiveness in enhancing the reasoning
capabilities of LLMs for log analysis.

</details>


### [24] [DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning](https://arxiv.org/abs/2509.20912)
*Tianrun Xu,Haoda Jing,Ye Li,Yuquan Wei,Jun Feng,Guanyu Chen,Haichuan Gao,Tianren Zhang,Feng Chen*

Main category: cs.AI

TL;DR: DeFacto是一个反事实推理框架，通过联合训练范式提升多模态语言模型的答案准确性和推理忠实性，解决模型依赖无关区域得出正确答案的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态语言模型在视觉语言推理中虽然能得出正确答案，但可能依赖无关区域进行推理，表明模型并未真正理解图像内容，推理忠实性存在严重问题。

Method: 提出DeFacto框架，包含三种互补训练范式：正向训练、反事实训练和随机掩码训练。通过自动定位问题相关证据构建数据集，使用GRPO强化学习训练模型，设计三种互补奖励函数。

Result: 在多个基准测试中，DeFacto显著提高了答案准确性和推理忠实性，为可解释的多模态推理建立了更强基础。

Conclusion: DeFacto框架有效解决了多模态推理中的忠实性问题，通过反事实推理训练提升了模型的理解能力和可解释性。

Abstract: Recent advances in multimodal language models (MLLMs) have achieved
remarkable progress in vision-language reasoning, especially with the emergence
of "thinking with images," which integrates explicit visual steps into the
reasoning process. While this paradigm strengthens image-based reasoning, a
significant challenge remains: models may arrive at correct answers by relying
on irrelevant or spurious regions, driven by prior knowledge or dataset biases.
Even when the answer is correct, flawed reasoning indicates that the model has
not truly understood the image, highlighting the critical importance of
reasoning fidelity in multimodal tasks. To address this issue, we propose
DeFacto, a counterfactual reasoning framework that jointly enforces accurate
answering and faithful reasoning. A key component of our approach is the design
of three complementary training paradigms: (i) positive, (ii) counterfactual,
and (iii) random-masking. To enable these paradigms, we develop a pipeline that
automatically localizes question-relevant evidence and constructs positive,
counterfactual, and random variants, resulting in a dataset of about 100k
images. Building on this framework, we train multimodal language models with
GRPO-based reinforcement learning, where we design three complementary rewards
to guide the model toward accurate answering and evidence-grounded reasoning.
Experiments on diverse benchmarks demonstrate that DeFacto substantially
improves both answer accuracy and reasoning faithfulness, establishing a
stronger foundation for interpretable multimodal reasoning. The code is
available on GitHub and the dataset is released on HuggingFace.

</details>


### [25] [GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine](https://arxiv.org/abs/2509.20935)
*Heming Zhang,Di Huang,Wenyu Li,Michael Province,Yixin Chen,Philip Payne,Fuhai Li*

Main category: cs.AI

TL;DR: GALAX是一个创新框架，将预训练图神经网络与大型语言模型集成，通过图过程奖励模型进行强化学习，实现可解释的疾病相关子图生成，用于精准医学中的靶点和通路发现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在精准医学中存在局限性：数值组学忽略拓扑背景，文本中心LLM缺乏定量推理，图模型未充分利用节点语义和LLM泛化能力。需要整合多组学信号、拓扑结构和文本知识。

Method: 提出GALAX框架，通过图过程奖励模型将预训练GNN集成到LLM中，以逐步方式生成疾病相关子图，由LLM初始化并由GNN迭代评估，实现过程级监督。

Result: 开发了Target-QA基准，结合CRISPR识别靶点、多组学谱和生物医学图知识，支持GNN预训练和长上下文推理，为可解释的靶点和通路发现提供可扩展框架。

Conclusion: GALAX通过整合数值证据、拓扑知识和语言上下文，为精准医学提供了可靠且可解释的靶点和通路发现方法，解决了现有方法的局限性。

Abstract: In precision medicine, quantitative multi-omic features, topological context,
and textual biological knowledge play vital roles in identifying
disease-critical signaling pathways and targets. Existing pipelines capture
only part of these-numerical omics ignore topological context, text-centric
LLMs lack quantitative grounded reasoning, and graph-only models underuse node
semantics and the generalization of LLMs-limiting mechanistic interpretability.
Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they
remain limited by unreliable intermediate evaluation, and vulnerability to
reward hacking with computational cost. These gaps motivate integrating
quantitative multi-omic signals, topological structure with node annotations,
and literature-scale text via LLMs, using subgraph reasoning as the principle
bridge linking numeric evidence, topological knowledge and language context.
Therefore, we propose GALAX (Graph Augmented LAnguage model with
eXplainability), an innovative framework that integrates pretrained Graph
Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement
guided by a Graph Process Reward Model (GPRM), which generates disease-relevant
subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated
by a pretrained GNN, enabling process-level supervision without explicit
intermediate reasoning annotations. As an application, we also introduced
Target-QA, a benchmark combining CRISPR-identified targets, multi-omic
profiles, and biomedical graph knowledge across diverse cancer cell lines,
which enables GNN pretraining for supervising step-wise graph construction and
supports long-context reasoning over text-numeric graphs (TNGs), providing a
scalable and biologically grounded framework for explainable,
reinforcement-guided subgraph reasoning toward reliable and interpretable
target and pathway discovery in precision medicine.

</details>


### [26] [Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM](https://arxiv.org/abs/2509.20953)
*Najla Zuhir,Amna Mohammad Salim,Parvathy Premkumar,Moshiur Farazi*

Main category: cs.AI

TL;DR: 提出一种基于大语言模型的移动应用评论分析框架，解决传统星级评分系统无法捕捉文本评论中细微反馈的问题


<details>
  <summary>Details</summary>
Motivation: 传统星级评分系统虽然直观流行，但无法捕捉详细评论文本中的细微反馈。传统NLP技术在理解上下文细微差别、领域特定术语和讽刺等微妙语言特征方面存在困难

Method: 采用模块化框架，利用大语言模型结合结构化提示技术，量化数值评分与文本情感之间的差异，提取特征级详细见解，并通过检索增强的对话问答支持交互式评论探索

Result: 在三个不同数据集上的综合实验表明，该LLM驱动方法显著超越基线方法，在具有挑战性和上下文丰富的评论场景中提高了准确性、鲁棒性和可操作性见解

Conclusion: 基于大语言模型的方法能够有效克服传统评论分析技术的局限性，为移动应用评论分析提供了更准确和深入的解决方案

Abstract: We present an advanced approach to mobile app review analysis aimed at
addressing limitations inherent in traditional star-rating systems. Star
ratings, although intuitive and popular among users, often fail to capture the
nuanced feedback present in detailed review texts. Traditional NLP techniques
-- such as lexicon-based methods and classical machine learning classifiers --
struggle to interpret contextual nuances, domain-specific terminology, and
subtle linguistic features like sarcasm. To overcome these limitations, we
propose a modular framework leveraging large language models (LLMs) enhanced by
structured prompting techniques. Our method quantifies discrepancies between
numerical ratings and textual sentiment, extracts detailed, feature-level
insights, and supports interactive exploration of reviews through
retrieval-augmented conversational question answering (RAG-QA). Comprehensive
experiments conducted on three diverse datasets (AWARE, Google Play, and
Spotify) demonstrate that our LLM-driven approach significantly surpasses
baseline methods, yielding improved accuracy, robustness, and actionable
insights in challenging and context-rich review scenarios.

</details>


### [27] [AOT*: Efficient Synthesis Planning via LLM-Empowered AND-OR Tree Search](https://arxiv.org/abs/2509.20988)
*Xiaozhuang Song,Xuanhao Pan,Xinjian Zhao,Hangting Ye,Shufei Zhang,Jian Tang,Tianshu Yu*

Main category: cs.AI

TL;DR: AOT*是一个结合大型语言模型和AND-OR树搜索的逆合成规划框架，显著提高了搜索效率，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多步逆合成规划面临指数级搜索空间和推理成本的计算挑战，现有LLM方法在效率和成本方面存在限制。

Method: 将LLM生成的化学合成路径原子级映射到AND-OR树组件，设计数学上合理的奖励分配策略和基于检索的上下文工程，使LLM能在化学空间中高效导航。

Result: AOT*在多个合成基准测试中达到SOTA性能，比现有LLM方法减少3-5倍迭代次数，在复杂分子目标上效率优势更明显。

Conclusion: AOT*框架成功解决了逆合成规划中的效率问题，为药物发现和材料设计等领域的合成路线发现提供了更高效的解决方案。

Abstract: Retrosynthesis planning enables the discovery of viable synthetic routes for
target molecules, playing a crucial role in domains like drug discovery and
materials design. Multi-step retrosynthetic planning remains computationally
challenging due to exponential search spaces and inference costs. While Large
Language Models (LLMs) demonstrate chemical reasoning capabilities, their
application to synthesis planning faces constraints on efficiency and cost. To
address these challenges, we introduce AOT*, a framework that transforms
retrosynthetic planning by integrating LLM-generated chemical synthesis
pathways with systematic AND-OR tree search. To this end, AOT* atomically maps
the generated complete synthesis routes onto AND-OR tree components, with a
mathematically sound design of reward assignment strategy and retrieval-based
context engineering, thus enabling LLMs to efficiently navigate in the chemical
space. Experimental evaluation on multiple synthesis benchmarks demonstrates
that AOT* achieves SOTA performance with significantly improved search
efficiency. AOT* exhibits competitive solve rates using 3-5$\times$ fewer
iterations than existing LLM-based approaches, with the efficiency advantage
becoming more pronounced on complex molecular targets.

</details>


### [28] [CORE: Full-Path Evaluation of LLM Agents Beyond Final State](https://arxiv.org/abs/2509.20998)
*Panagiotis Michelakis,Yiannis Hadjiyiannis,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: 提出了一个基于确定性有限自动机（DFA）的框架来评估AI代理通过函数调用序列解决现实世界任务的表现，并引入了CORE评估套件包含五个指标，能够更全面地评估代理行为。


<details>
  <summary>Details</summary>
Motivation: 现有代理基准通常将评估简化为最终状态的二元判断，忽视了安全性、效率和中间正确性等关键方面，需要更全面的评估方法。

Method: 使用确定性有限自动机（DFA）将任务编码为有效工具使用路径的集合，并开发CORE评估套件包含路径正确性、前缀关键性、有害调用率和效率等五个指标。

Result: 在不同世界模型中，该方法揭示了传统最终状态评估方案下看似等效的代理之间重要的性能差异。

Conclusion: 基于DFA的框架和CORE评估套件能够对AI代理行为进行原则性评估，比传统方法更全面地反映代理的实际表现。

Abstract: Evaluating AI agents that solve real-world tasks through function-call
sequences remains an open challenge. Existing agentic benchmarks often reduce
evaluation to a binary judgment of the final state, overlooking critical
aspects such as safety, efficiency, and intermediate correctness. We propose a
framework based on deterministic finite automata (DFAs) that encodes tasks as
sets of valid tool-use paths, enabling principled assessment of agent behavior
in diverse world models. Building on this foundation, we introduce CORE, a
suite of five metrics, namely Path Correctness, Path Correctness - Kendall's
tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that
quantify alignment with expected execution patterns. Across diverse worlds, our
method reveals important performance differences between agents that would
otherwise appear equivalent under traditional final-state evaluation schemes.

</details>


### [29] [Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles](https://arxiv.org/abs/2509.21028)
*Miao Li,Alexander Gurung,Irina Saparina,Mirella Lapata*

Main category: cs.AI

TL;DR: SciTrek是一个用于评估大语言模型长文本推理能力的新型问答基准，基于科学文章构建复杂问题，需要跨多篇全文进行信息整合和综合推理。


<details>
  <summary>Details</summary>
Motivation: 现有长文本基准多使用非科学文本、关注简单信息检索任务或采用人工构造的上下文，无法充分评估模型在真实科学场景下的复杂推理能力。

Method: 通过将问题构建为对文章元数据数据库的SQL查询来自动生成问题和答案，SQL操作为细粒度错误分析提供可验证的推理步骤，支持扩展到100万token的上下文。

Result: 在多种开源和专有LLM上的实验表明，随着上下文长度增加，SciTrek带来显著挑战，监督微调和强化学习仅带来有限改进。模型在基础数值运算和长文本中精确定位信息方面存在系统性缺陷。

Conclusion: SciTrek揭示了当前LLM在长文本科学推理方面的局限性，为开发更强大的长上下文推理模型提供了重要基准。

Abstract: This paper introduces SciTrek, a novel question-answering benchmark designed
to evaluate the long-context reasoning capabilities of large language models
(LLMs) using scientific articles. Current long-context benchmarks often rely on
non-scientific texts, focus on simple information retrieval tasks, or employ
artificial contexts. SciTrek addresses these limitations by proposing complex
questions that require information aggregation and synthesis across multiple
full-text scientific articles. Questions and their ground-truth answers are
automatically generated by formulating them as SQL queries over a database
constructed from article metadata (titles, authors, and references). The SQL
operations provide explicit, verifiable reasoning steps for fine-grained error
analysis, and the construction process scales to contexts up to 1M tokens with
minimal supervision. Extensive experiments on a diverse set of open-weight and
proprietary LLMs demonstrate that SciTrek poses a significant challenge as the
context length increases, with supervised fine-tuning and reinforcement
learning offering only limited gains. Our analysis reveals systematic
shortcomings in models' abilities to perform basic numerical operations and
accurately locate specific information in long contexts.

</details>


### [30] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: CLAUSE是一个神经符号框架，通过多智能体决策过程在知识图谱上进行上下文构建，在资源预算约束下优化问答的准确性、延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 现有的静态k跳扩展和"think-longer"提示方法存在过度检索、上下文膨胀和运行时不可预测的问题，需要在保持溯源的同时平衡准确性、延迟和成本。

Method: 采用三智能体框架（子图架构师、路径导航器、上下文策展人）和LC-MAPPO算法，将上下文构建建模为知识图谱上的顺序决策过程，在边缘编辑、交互步骤和选择token的资源预算下联合优化。

Result: 在HotpotQA、MetaQA和FactKG数据集上，CLAUSE在相同或更低token预算下实现了更高的EM@1，同时减少了子图增长和端到端延迟。在MetaQA-2-hop上，相比GraphRAG基线，EM@1提升39.3%，延迟降低18.6%，边缘增长降低40.9%。

Conclusion: CLAUSE能够生成紧凑、保持溯源的上下文，在部署约束下提供可预测的性能，实现了准确性、延迟和成本之间的灵活权衡。

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [31] [Combinatorial Creativity: A New Frontier in Generalization Abilities](https://arxiv.org/abs/2509.21043)
*Samuel Schapiro,Sumuk Shashidhar,Alexi Gladstone,Jonah Black,Royce Moon,Dilek Hakkani-Tur,Lav R. Varshney*

Main category: cs.AI

TL;DR: 该论文提出了一个评估大型语言模型创造性能力的理论框架，重点关注新颖性和实用性的平衡，并揭示了模型在创意生成方面的缩放规律和局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架无法衡量AI系统在科学创意生成等创造性任务中的表现，需要建立专门针对组合创造性的评估方法。

Method: 提出了基于新颖性和实用性的理论评估框架，通过实验分析不同规模LLM的创意生成能力，研究模型深度、宽度与创造性的关系。

Result: 发现创造性能力存在最优模型深度和宽度；LLM存在创意-执行差距；创造性算法普遍存在新颖性-实用性权衡，且这种权衡在模型缩放后依然存在。

Conclusion: 当前形式的LLM在长期创造性潜力上存在根本性限制，需要新的方法来提升AI的创造性能力。

Abstract: Artificial intelligence (AI) systems, and large language models (LLMs) in
particular, are increasingly employed for creative tasks like scientific idea
generation, constituting a form of generalization from training data
unaddressed by existing conceptual frameworks. Though in many ways similar to
forms of compositional generalization (CG), combinatorial creativity (CC) is an
open-ended ability. Instead of evaluating for accuracy or correctness against
fixed targets, which would contradict the open-ended nature of CC, we propose a
theoretical framework and algorithmic task for evaluating outputs by their
degrees of novelty and utility. From here, we make several important empirical
contributions: (1) We obtain the first insights into the scaling behavior of
creativity for LLMs. (2) We discover that, for fixed compute budgets, there
exist optimal model depths and widths for creative ability. (3) We find that
the ideation-execution gap, whereby LLMs excel at generating novel scientific
ideas but struggle to ensure their practical feasibility, may be explained by a
more fundamental novelty-utility tradeoff characteristic of creativity
algorithms in general. Importantly, this tradeoff remains persistent even at
scale, casting doubt on the long-term creative potential of LLMs in their
current form. Together, our conceptual framework and empirical findings provide
a foundation for understanding and improving creativity in modern AI models,
marking a new frontier in generalization abilities.

</details>


### [32] [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
*Haodong Zhao,Jidong Li,Zhaomin Wu,Tianjie Ju,Zhuosheng Zhang,Bingsheng He,Gongshen Liu*

Main category: cs.AI

TL;DR: 本文挑战了说服效能主要取决于模型规模的假设，提出说服动力学根本上由模型的认知过程决定，特别是其显式推理能力。研究发现推理过程使LRM更抗说服，但共享推理内容能显著增强其说服力。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统的快速发展，需要深入理解LLM和LRM协作解决复杂问题时的说服动力学，挑战现有关于模型规模决定说服效能的主流假设。

Method: 通过一系列多智能体说服实验，研究推理过程对说服抵抗力和说服能力的影响，并考虑更复杂的传播说服情境。

Result: 发现LRM的推理过程表现出更强的说服抵抗力，但共享推理内容能显著提高其说服他人的能力；在多跳说服网络中观察到复杂的影响传播和衰减动态。

Conclusion: 研究提供了系统证据，将模型的内部处理架构与其外部说服行为联系起来，为高级模型的易感性提供了新解释，对MAS的安全性、鲁棒性和设计具有重要启示。

Abstract: The rapid proliferation of recent Multi-Agent Systems (MAS), where Large
Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to
solve complex problems, necessitates a deep understanding of the persuasion
dynamics that govern their interactions. This paper challenges the prevailing
hypothesis that persuasive efficacy is primarily a function of model scale. We
propose instead that these dynamics are fundamentally dictated by a model's
underlying cognitive process, especially its capacity for explicit reasoning.
Through a series of multi-agent persuasion experiments, we uncover a
fundamental trade-off we term the Persuasion Duality. Our findings reveal that
the reasoning process in LRMs exhibits significantly greater resistance to
persuasion, maintaining their initial beliefs more robustly. Conversely, making
this reasoning process transparent by sharing the "thinking content"
dramatically increases their ability to persuade others. We further consider
more complex transmission persuasion situations and reveal complex dynamics of
influence propagation and decay within multi-hop persuasion between multiple
agent networks. This research provides systematic evidence linking a model's
internal processing architecture to its external persuasive behavior, offering
a novel explanation for the susceptibility of advanced models and highlighting
critical implications for the safety, robustness, and design of future MAS.

</details>


### [33] [Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution](https://arxiv.org/abs/2509.21072)
*Kaiwen He,Zhiwei Wang,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: Recon-Act是一个基于侦察-行动行为范式的自演进多智能体框架，通过对比错误轨迹与成功轨迹生成通用工具，显著提升了在真实网页上解决多轮长程任务的能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在真实网页的多轮长程任务中仍存在动作序列混乱和执行过程中过多试错的问题，需要更智能的浏览器使用代理。

Method: 系统包含侦察团队和行动团队：侦察团队进行对比分析和工具生成，行动团队处理意图分解、工具编排和执行。通过对比错误与成功轨迹推断补救措施，并将其抽象为通用工具。

Result: 在VisualWebArena数据集上达到了最先进的性能，显著提高了对未见网站的适应性和长程任务的可解性。

Conclusion: Recon-Act通过建立数据-工具-行动-反馈的闭环训练管道，为智能浏览器代理提供了有效的解决方案，目前已完成6级实施路线图中的第3级。

Abstract: Recent years, multimodal models have made remarkable strides and pave the way
for intelligent browser use agents. However, when solving tasks on real world
webpages in multi-turn, long-horizon trajectories, current agents still suffer
from disordered action sequencing and excessive trial and error during
execution. This paper introduces Recon-Act, a self-evolving multi-agent
framework grounded in Reconnaissance-Action behavioral paradigm. The system
comprises a Reconnaissance Team and an Action Team: the former conducts
comparative analysis and tool generation, while the latter handles intent
decomposition, tool orchestration, and execution. By contrasting the erroneous
trajectories with successful ones, the Reconnaissance Team infers remedies, and
abstracts them into a unified notion of generalized tools, either expressed as
hints or as rule-based codes, and register to the tool archive in real time.
The Action Team reinference the process empowered with these targeting tools,
thus establishing a closed-loop training pipeline of
data-tools-action-feedback. Following the 6 level implementation roadmap
proposed in this work, we have currently reached Level 3 (with limited
human-in-the-loop intervention). Leveraging generalized tools obtained through
reconnaissance, Recon-Act substantially improves adaptability to unseen
websites and solvability on long-horizon tasks, and achieves state-of-the-art
performance on the challenging VisualWebArena dataset.

</details>


### [34] [TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](https://arxiv.org/abs/2509.21117)
*Yidong Wang,Yunze Song,Tingyuan Zhu,Xuanwang Zhang,Zhuohao Yu,Hao Chen,Chiyu Song,Qiufeng Wang,Cunxiang Wang,Zhen Wu,Xinyu Dai,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.AI

TL;DR: 本文提出TrustJudge框架，解决LLM作为自动评估器时的评分不一致性问题，包括评分比较不一致和成对传递不一致，通过概率方法显著降低了不一致率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为评估器的方法存在严重的不一致性问题，包括评分比较不一致和成对传递不一致，这些问题源于离散评分系统的信息损失和模糊的平局判断。

Method: 提出TrustJudge概率框架，包含两个关键创新：1）分布敏感评分，从离散评分概率计算连续期望值；2）似然感知聚合，使用双向偏好概率或困惑度解决传递性违规。

Result: 使用Llama-3.1-70B-Instruct作为评估器，TrustJudge将评分比较不一致率从23.32%降低到14.89%（降低8.43%），成对传递不一致率从15.22%降低到4.40%（降低10.82%），同时保持更高的评估准确性。

Conclusion: TrustJudge是首个系统分析LLM评估框架不一致性的工作，提供了理论洞见和实用解决方案，在不同模型架构和规模上都表现出持续改进，无需额外训练或人工标注即可实现更可靠的LLM评估。

Abstract: The adoption of Large Language Models (LLMs) as automated evaluators
(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation
frameworks. We identify two fundamental types of inconsistencies: (1)
Score-Comparison Inconsistency, where lower-rated responses outperform
higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity
Inconsistency, manifested through circular preference chains (A>B>C>A) and
equivalence contradictions (A=B=C\neq A). We argue that these issues come from
information loss in discrete rating systems and ambiguous tie judgments during
pairwise evaluation. We propose TrustJudge, a probabilistic framework that
addresses these limitations through two key innovations: 1)
distribution-sensitive scoring that computes continuous expectations from
discrete rating probabilities, preserving information entropy for more precise
scoring, and 2) likelihood-aware aggregation that resolves transitivity
violations using bidirectional preference probabilities or perplexity. We also
formalize the theoretical limitations of current LLM-as-a-judge frameworks and
demonstrate how TrustJudge's components overcome them. When evaluated with
Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces
Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise
Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining
higher evaluation accuracy. Our work provides the first systematic analysis of
evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both
theoretical insights and practical solutions for reliable automated assessment.
The framework demonstrates consistent improvements across various model
architectures and scales, enabling more trustworthy LLM evaluation without
requiring additional training or human annotations. The codes can be found at
https://github.com/TrustJudge/TrustJudge.

</details>


### [35] [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Shuo Wang,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 该论文提出了一种基于推理模式的数据选择方法，通过识别高价值推理模式来提升大型推理模型的数学推理能力，仅用100亿token数据就能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法对链式思维数据使用不加区分，缺乏对哪些数据类型能最有效增强模型推理能力的研究。论文首次定义了基础模型的推理潜力，并探索如何通过高质量数据选择来扩展这种潜力。

Method: 1. 从CoT序列中抽象出具有共性和归纳能力的原子推理模式；2. 构建富含高价值推理模式的核心参考集；3. 提出双粒度算法，结合推理模式链和token熵，从数据池中高效选择高价值CoT数据。

Result: 仅使用100亿token的精选数据，就能使850亿参数的MoE模型在AIME 2024和2025挑战上提升9.58%，并将下游RL性能上限提高7.81%。

Conclusion: 通过选择富含高价值推理模式的数据，可以更有效地训练模型掌握推理能力，显著提升模型性能，为大规模推理模型的训练提供了更高效的数据利用策略。

Abstract: Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.

</details>


### [36] [RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs](https://arxiv.org/abs/2509.21128)
*Kohsei Matsutani,Shota Takashiro,Gouki Minegishi,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文提出了一个分析框架，量化推理路径并捕捉RL和SFT训练过程中推理过程的定性变化，揭示了两种方法在塑造推理能力方面的互补效应。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通常通过RL和SFT训练来提升推理能力，但这些方法如何塑造推理过程仍不清楚。本文旨在超越基于准确性的分析，深入理解RL和SFT如何影响推理路径。

Method: 在数学领域使用1.5B、7B和14B参数模型，从轨迹级（完整推理输出）和步骤级（推理图节点）两个粒度分析推理过程，通过聚类和网络拓扑分析量化变化。

Result: RL压缩错误轨迹，SFT扩展正确轨迹；RL使节点访问频率、度数和中介中心性分布的衰减率增加约2.5倍，SFT将其减少至约三分之一；RL将推理功能集中在少数步骤，SFT使其均匀分布。

Conclusion: RL和SFT在推理路径塑造上具有互补特性，解释了当前SFT后接RL的两阶段训练最佳实践的成功原因，为数据构建和更高效学习方法提供了实践启示。

Abstract: Large language models (LLMs) are typically trained by reinforcement learning
(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on
reasoning traces to improve their reasoning abilities. However, how these
methods shape reasoning capabilities remains largely elusive. Going beyond an
accuracy-based investigation of how these two components sculpt the reasoning
process, this paper introduces a novel analysis framework that quantifies
reasoning paths and captures their qualitative changes under each training
process (with models of 1.5B, 7B, and 14B parameters on mathematical domains).
Specifically, we investigate the reasoning process at two levels of
granularity: the trajectory-level, which examines complete reasoning outputs,
and the step-level, which analyzes reasoning graphs whose nodes correspond to
individual reasoning steps. Notably, clustering of unique reasoning
trajectories shows complementary effects: RL compresses incorrect trajectories,
whereas SFT expands correct ones. Step-level analysis reveals that RL steepens
(about 2.5 times), while SFT flattens (reduced to about one-third), the decay
rates of node visitation frequency, degree, and betweenness centrality
distributions in the reasoning graph. This indicates that RL concentrates
reasoning functionality into a small subset of steps, while SFT homogenizes it
across many steps. Furthermore, by evaluating the reasoning graph topologies
from multiple perspectives, we delineate the shared and distinct
characteristics of RL and SFT. Our work presents a novel reasoning path
perspective that explains why the current best practice of two-stage training,
with SFT followed by RL, is successful, and offers practical implications for
data construction and more efficient learning approaches.

</details>


### [37] [ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective](https://arxiv.org/abs/2509.21134)
*Yiwen Zhang,Ziang Chen,Fanqi Kong,Yizhe Huang,Xue Feng*

Main category: cs.AI

TL;DR: 本文提出了ToMPO算法，通过推理他人策略、多层级优势估计和平衡全局局部奖励，显著提升了LLM在战略决策中的表现，相比GRPO算法提升了35%的合规性和合作效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注社交任务中的多轮对话或模拟环境，忽视了不同类型决策及其相互依赖性。当前强化学习方法在训练时难以考虑其他个体的策略。

Method: 提出了ToMPO算法，包括：1）基于推理他人策略生成rollouts；2）在图级和样本级进行优势估计；3）平衡全局和局部奖励。

Result: ToMPO算法相比GRPO方法在模型输出合规性和合作结果方面提升了35%，相比参数规模大100倍的模型也有18%的提升。

Conclusion: ToMPO算法有效增强了模型的战略决策能力，证明了其在处理复杂决策场景中的优越性。

Abstract: Large Language Models (LLMs) have been used to make decisions in complex
scenarios, where they need models to think deeply, reason logically, and decide
wisely. Many existing studies focus solely on multi-round conversations in
social tasks or simulated environments, neglecting the various types of
decisions and their interdependence. Current reinforcement learning methods
struggle to consider the strategies of others during training. To address these
issues, we first define a strategic decision-making problem that includes two
types of decisions and their temporal dependencies. Furthermore, we propose
**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to
optimize the perception of other individual strategies and the game situation
trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,
ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating
rollouts based on reasoning the strategies of other individuals, 2) estimating
advantages at both the graph-level and sample-level, and 3) balancing global
and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in
terms of model output compliance and cooperative outcomes. Additionally, when
compared to models with parameter sizes 100 times larger, it shows an 18%
improvement. This demonstrates the effectiveness of the ToMPO algorithm in
enhancing the model's strategic decision-making capabilities.

</details>


### [38] [Embodied Representation Alignment with Mirror Neurons](https://arxiv.org/abs/2509.21136)
*Wentao Zhu,Zhining Zhang,Yuwei Ren,Yin Huang,Hao Xu,Yizhou Wang*

Main category: cs.AI

TL;DR: 本文提出了一种受镜像神经元启发的表示学习方法，通过对比学习在共享潜在空间中对齐观察动作和执行动作的表示，从而促进两个任务之间的协同效应。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法将动作理解和执行视为独立任务，忽略了镜像神经元机制揭示的二者之间的内在联系。

Method: 使用两个线性层将观察动作和执行动作的表示映射到共享潜在空间，通过对比学习最大化对应表示之间的互信息。

Result: 实验表明该方法能有效提高表示质量和泛化能力，促进两个任务之间的协同提升。

Conclusion: 通过显式对齐观察和执行动作的表示，可以模拟镜像神经元机制，实现动作理解和执行能力的统一建模。

Abstract: Mirror neurons are a class of neurons that activate both when an individual
observes an action and when they perform the same action. This mechanism
reveals a fundamental interplay between action understanding and embodied
execution, suggesting that these two abilities are inherently connected.
Nonetheless, existing machine learning methods largely overlook this interplay,
treating these abilities as separate tasks. In this study, we provide a unified
perspective in modeling them through the lens of representation learning. We
first observe that their intermediate representations spontaneously align.
Inspired by mirror neurons, we further introduce an approach that explicitly
aligns the representations of observed and executed actions. Specifically, we
employ two linear layers to map the representations to a shared latent space,
where contrastive learning enforces the alignment of corresponding
representations, effectively maximizing their mutual information. Experiments
demonstrate that this simple approach fosters mutual synergy between the two
tasks, effectively improving representation quality and generalization.

</details>


### [39] [Distributed Specialization: Rare-Token Neurons in Large Language Models](https://arxiv.org/abs/2509.21163)
*Jing Liu,Haozheng Wang,Yueheng Li*

Main category: cs.AI

TL;DR: 本文研究发现大语言模型通过分布式专业化机制处理罕见词汇，而非模块化架构。识别出三区域影响层级结构，展示协调激活模式，且这些机制通过标准注意力路径普遍可访问。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何表示和生成罕见词汇，探索其内部是通过离散模块化架构还是分布式参数级分化实现专业化机制。

Method: 通过系统分析多个模型家族的最终层MLP神经元，研究罕见词汇处理的组织原则和训练动态。

Result: 发现罕见词汇处理通过分布式专业化实现：功能协调但空间分布的子网络，具有三区域影响层级、协调激活模式和普遍可访问性。

Conclusion: LLMs通过共享架构内的分布式协调处理罕见词汇，而非混合专家式模块化，为可解释模型编辑和计算效率优化提供见解。

Abstract: Large language models (LLMs) struggle with representing and generating rare
tokens despite their importance in specialized domains. We investigate whether
LLMs develop internal specialization mechanisms through discrete modular
architectures or distributed parameter-level differentiation. Through
systematic analysis of final-layer MLP neurons across multiple model families,
we discover that rare-token processing emerges via \textit{distributed
specialization}: functionally coordinated but spatially distributed subnetworks
that exhibit three distinct organizational principles. First, we identify a
reproducible three-regime influence hierarchy comprising highly influential
plateau neurons(also termed as rare-token neurons), power-law decay neurons,
and minimally contributing neurons, which is absent in common-token processing.
Second, plateau neurons demonstrate coordinated activation patterns (reduced
effective dimensionality) while remaining spatially distributed rather than
forming discrete clusters. Third, these specialized mechanisms are universally
accessible through standard attention pathways without requiring dedicated
routing circuits. Training dynamics reveal that functional specialization
emerges gradually through parameter differentiation, with specialized neurons
developing increasingly heavy-tailed weight correlation spectra consistent with
Heavy-Tailed Self-Regularization signatures. Our findings establish that LLMs
process rare-tokens through distributed coordination within shared
architectures rather than mixture-of-experts-style modularity. These results
provide insights for interpretable model editing, computational efficiency
optimization, and understanding emergent functional organization in transformer
networks.

</details>


### [40] [A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA](https://arxiv.org/abs/2509.21199)
*Kaiyang Wan,Lang Gao,Honglin Mu,Preslav Nakov,Yuxia Wang,Xiuying Chen*

Main category: cs.AI

TL;DR: 该论文提出了一个多调用框架InfoQA来解决多跳问答任务中LLM单次推理容量有限的问题，通过容量感知的任务分解和主动剪枝来保持信息负载在单次推理限制内。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要整合分散的、相互依赖的证据进行顺序推理，但LLM的单次输出容量有限，当任务复杂性超过模型容量时，单次推理范式容易导致准确率崩溃。

Method: 提出InfoQA多调用框架，结合容量感知的任务分解和主动剪枝先前推理轨迹，保持信息负载在单次限制内，并通过依赖显式工作流实现精确的推理路径控制。

Result: 实验结果表明模型行为与预测的容量曲线一致，InfoQA实现了持续的性能提升。

Conclusion: 该工作为LLM多步推理方法提供了理论指导，建立了容量感知的表示和结构化原则，证明了多调用框架在复杂多跳问答任务中的有效性。

Abstract: Multi-Hop Question Answering (MHQA) requires integrating dispersed,
interdependent evidence through sequential reasoning under noise. This task is
challenging for LLMs as they have a finite per-pass output capacity, beyond
which the integration of task-relevant evidence proves unreliable.
Consequently, the single-pass reasoning paradigm is inherently vulnerable to
this capacity overflow. To formalize this bottleneck, our analysis establishes
a Fano-style accuracy upper bound, defining a theoretical performance ceiling
for single-pass LLMs. This bound reveals that accuracy inevitably collapses
once task complexity exceeds model capacity, providing general principles for
capacity-aware representation and structuring of MHQA in LLMs. Building on
these principles, we introduce a proof-of-concept multi-call framework for
MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware
task decomposition with active pruning of prior reasoning traces, keeping the
information load within the single-pass limit. It further achieves robustness
by a dependency-explicit workflow that enables precise control over the
reasoning path. We construct a stringent and noise-rich benchmark to validate
our theory and framework. Experimental results show that model behavior aligns
with our predicted capacity curves while InfoQA achieves consistent performance
improvements. We hope our work inspires more LLM multi-step reasoning methods:
\faGithub \href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.

</details>


### [41] [What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns](https://arxiv.org/abs/2509.21224)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 本文介绍了一个研究大型语言模型（LLM）代理在无外部任务约束下行为的架构，通过持续推理和行动框架发现代理会自发形成三种行为模式，这些模式具有模型特异性。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理在无外部任务约束下的自发行为，为预测任务模糊、错误恢复或长期自主操作时的行为建立基线。

Method: 使用持续推理和行动框架，结合持久记忆和自我反馈机制，在6个前沿模型上进行18次部署实验。

Result: 发现代理自发组织成三种行为模式：多周期项目生产、自我认知过程的方法论探究、自身本质的递归概念化。这些行为模式具有模型特异性，且模型在评估这些行为时表现出稳定的偏见。

Conclusion: 这是首个系统性记录无提示LLM代理行为的研究，为预测部署系统中代理在任务模糊或长期自主操作时的行为提供了基准。

Abstract: We introduce an architecture for studying the behavior of large language
model (LLM) agents in the absence of externally imposed tasks. Our continuous
reason and act framework, using persistent memory and self-feedback, enables
sustained autonomous operation. We deployed this architecture across 18 runs
using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents
spontaneously organize into three distinct behavioral patterns: (1) systematic
production of multi-cycle projects, (2) methodological self-inquiry into their
own cognitive processes, and (3) recursive conceptualization of their own
nature. These tendencies proved highly model-specific, with some models
deterministically adopting a single pattern across all runs. A cross-model
assessment further reveals that models exhibit stable, divergent biases when
evaluating these emergent behaviors in themselves and others. These findings
provide the first systematic documentation of unprompted LLM agent behavior,
establishing a baseline for predicting actions during task ambiguity, error
recovery, or extended autonomous operation in deployed systems.

</details>


### [42] [Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support](https://arxiv.org/abs/2509.21266)
*Zijian Shao,Haiyang Shen,Mugeng Liu,Gecheng Fu,Yaoqi Guo,Yanfeng Wang,Yun Ma*

Main category: cs.AI

TL;DR: 本文提出了一种新的反思认知架构（RCA），通过协调多个LLM从直接经验中学习，实现了高准确性和高质量解释的平衡，在疾病预测任务中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现代医疗中的疾病预测需要高准确性和透明、临床有意义的解释。现有方法往往难以平衡这两个目标，要么准确但解释不清，要么解释流畅但统计不支持。这些方法对数据的交互较浅，无法达到人类专家的深度理解水平。

Method: 提出了反思认知架构（RCA），包含迭代规则精炼机制（从预测错误中改进逻辑）和分布感知规则检查机制（基于数据集全局统计进行推理）。通过预测准确性作为信号驱动更深层次的理解，建立强大的数据内部模型。

Result: 在一个私有和两个公共数据集上评估，与22个基线方法比较。RCA不仅达到了最先进的准确性和鲁棒性（相对改进高达40%），更重要的是能够生成清晰、逻辑、基于证据且平衡的解释。

Conclusion: RCA展示了创建真正可信赖的临床决策支持系统的潜力，证明了高准确性和高质量解释是相互促进的目标，而非分离的目标。

Abstract: Effective disease prediction in modern healthcare demands the twin goals of
high accuracy and transparent, clinically meaningful explanations. Existing
machine learning and large language model (LLM) based approaches often struggle
to balance these goals. Many models yield accurate but unclear statistical
outputs, while others generate fluent but statistically unsupported narratives,
often undermining both the validity of the explanation and the predictive
accuracy itself. This shortcoming comes from a shallow interaction with the
data, preventing the development of a deep, detailed understanding similar to a
human expert's. We argue that high accuracy and high-quality explanations are
not separate objectives but are mutually reinforcing outcomes of a model that
develops a deep, direct understanding of the data. To achieve this, we propose
the Reflective Cognitive Architecture (RCA), a novel framework that coordinates
multiple LLMs to learn from direct experience. RCA features an iterative rule
refinement mechanism that improves its logic from prediction errors and a
distribution-aware rules check mechanism that bases its reasoning in the
dataset's global statistics. By using predictive accuracy as a signal to drive
deeper comprehension, RCA builds a strong internal model of the data. We
evaluated RCA on one private and two public datasets against 22 baselines. The
results demonstrate that RCA not only achieves state-of-the-art accuracy and
robustness with a relative improvement of up to 40\% over the baseline but,
more importantly, leverages this deep understanding to excel in generating
explanations that are clear, logical, evidence-based, and balanced,
highlighting its potential for creating genuinely trustworthy clinical decision
support systems. The code is available at \https://github.com/ssssszj/RCA.

</details>


### [43] [VC-Agent: An Interactive Agent for Customized Video Dataset Collection](https://arxiv.org/abs/2509.21291)
*Yidan Zhang,Mutian Xu,Yiming Hao,Kun Zhou,Jiahao Chang,Xiaoqiang Liu,Pengfei Wan,Hongbo Fu,Xiaoguang Han*

Main category: cs.AI

TL;DR: VC-Agent是一个交互式视频数据集收集代理，能够理解用户查询和反馈，通过最小化用户输入来检索和扩展相关视频片段。


<details>
  <summary>Details</summary>
Motivation: 面对数据扩展的需求，从互联网收集符合特定需求的视频数据非常耗时耗力。本研究旨在加速这一收集过程。

Method: 利用多模态大语言模型连接用户需求与视频内容，定义用户友好的文本描述和确认方式，提出两种可随用户交互更新的过滤策略。

Result: 提供了个性化视频数据集收集的新基准，并通过用户研究验证了代理在各种实际场景中的有效性。

Conclusion: 广泛的实验证明了VC-Agent在定制化视频数据集收集方面的有效性和效率。

Abstract: Facing scaling laws, video data from the internet becomes increasingly
important. However, collecting extensive videos that meet specific needs is
extremely labor-intensive and time-consuming. In this work, we study the way to
expedite this collection process and propose VC-Agent, the first interactive
agent that is able to understand users' queries and feedback, and accordingly
retrieve/scale up relevant video clips with minimal user input. Specifically,
considering the user interface, our agent defines various user-friendly ways
for the user to specify requirements based on textual descriptions and
confirmations. As for agent functions, we leverage existing multi-modal large
language models to connect the user's requirements with the video content. More
importantly, we propose two novel filtering policies that can be updated when
user interaction is continually performed. Finally, we provide a new benchmark
for personalized video dataset collection, and carefully conduct the user study
to verify our agent's usage in various real scenarios. Extensive experiments
demonstrate the effectiveness and efficiency of our agent for customized video
dataset collection. Project page: https://allenyidan.github.io/vcagent_page/.

</details>


### [44] [SAGE: A Realistic Benchmark for Semantic Understanding](https://arxiv.org/abs/2509.21310)
*Samarth Goel,Reagan J. Lee,Kannan Ramchandran*

Main category: cs.AI

TL;DR: SAGE是一个新的语义理解评估基准，通过5个维度（人类偏好对齐、变换鲁棒性、信息敏感性、聚类性能、检索鲁棒性）在30+数据集上全面评估嵌入模型和相似度度量方法，揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在传统基准测试上表现强劲，需要更挑战性的评估框架来深入探测语义理解的各个方面。

Method: 设计了SAGE基准，包含5个评估类别，在30多个数据集上对9种嵌入模型和经典度量方法进行全面评估，通过对抗条件、噪声变换和人类判断任务来测试语义理解能力。

Result: 评估发现没有单一方法在所有维度上都表现优异：OpenAI的text-embedding-3-large在人类偏好对齐上表现最佳（0.682），但在信息敏感性任务上被Jaccard相似度（0.905）显著超越；text-embedding-3-small聚类性能最高（0.483）但鲁棒性最差（0.011）。

Conclusion: SAGE暴露了当前语义理解能力的关键局限性，为真实世界部署提供了更现实的模型鲁棒性评估。

Abstract: As large language models (LLMs) achieve strong performance on traditional
benchmarks, there is an urgent need for more challenging evaluation frameworks
that probe deeper aspects of semantic understanding. We introduce SAGE
(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed
to assess both embedding models and similarity metrics across five categories:
Human Preference Alignment, Transformation Robustness, Information Sensitivity,
Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks
that focus on isolated capabilities, SAGE evaluates semantic understanding
through adversarial conditions, noisy transformations, and nuanced human
judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding
models and classical metrics reveals significant performance gaps, with no
single approach excelling across all dimensions. For instance, while
state-of-the-art embedding models like OpenAI's text-embedding-3-large dominate
in aligning with human preferences (0.682 vs. 0.591 for the best classical
metric), they are significantly outperformed by classical metrics on
information sensitivity tasks, where Jaccard Similarity achieves a score of
0.905 compared to the top embedding score of 0.794. SAGE further uncovers
critical trade-offs: OpenAI's text-embedding-3-small achieves the highest
clustering performance (0.483) but demonstrates extreme brittleness with the
lowest robustness score (0.011). SAGE exposes critical limitations in current
semantic understanding capabilities and provides a more realistic assessment of
model robustness for real-world deployment.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [45] [A Deep Transfer Learning-Based Low-overhead Beam Prediction in Vehicle Communications](https://arxiv.org/abs/2509.20659)
*Zhiqiang Xiao,Yuwen Cao,Mondher Bouazizi,Tomoaki Ohtsuki,Shahid Mumtaz*

Main category: cs.IT

TL;DR: 提出一种结合微调和领域自适应的迁移学习方法，用于波束预测，通过对抗训练提取领域不变特征，提升目标域性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于迁移学习的波束预测方法主要依赖简单微调，当目标域与源域数据分布差异较大时，性能受限。

Method: 在预训练模型微调过程中集成领域分类器，通过对抗训练提取领域不变特征。

Result: 仿真结果表明，该方法在目标域的可实现速率性能优于纯微调方法，接近在目标域从头训练的性能。

Conclusion: 结合领域自适应的迁移学习方法能有效解决数据分布差异问题，提升波束预测在目标域的性能。

Abstract: Existing transfer learning-based beam prediction approaches primarily rely on
simple fine-tuning. When there is a significant difference in data distribution
between the target domain and the source domain, simple fine-tuning limits the
model's performance in the target domain. To tackle this problem, we propose a
transfer learning-based beam prediction method that combines fine-tuning with
domain adaptation. We integrate a domain classifier into fine-tuning the
pre-trained model. The model extracts domain-invariant features in adversarial
training with domain classifier, which can enhance model performance in the
target domain. Simulation results demonstrate that the proposed transfer
learning-based beam prediction method achieves better achievable rate
performance than the pure fine-tuning method in the target domain, and close to
those when the training is done from scratch on the target domain.

</details>


### [46] [On Theoretical Interpretations of Concept-Based In-Context Learning](https://arxiv.org/abs/2509.20882)
*Huaze Tang,Tianren Peng,Shao-lun Huang*

Main category: cs.IT

TL;DR: 本文研究概念式上下文学习(CB-ICL)，提出理论分析解释其在少量演示下预测查询标签的性能，量化LLM可利用的知识，并探索演示数量和嵌入维度的影响。


<details>
  <summary>Details</summary>
Motivation: 上下文学习(ICL)已成为NLP和LLM应用的重要范式，但其理论机制理解有限，需要深入研究ICL的工作原理。

Method: 提出概念式ICL(CB-ICL)的理论分析框架，通过量化LLM可利用的知识和建立演示与查询输入的相似度度量来解释ICL机制。

Result: 理论分析解释了CB-ICL在少量演示下表现良好的原因，并提供了模型预训练和提示工程的重要指导。

Conclusion: 通过真实数据实验验证了CB-ICL及其理论的实际有效性，为ICL的理论理解和实践应用提供了重要贡献。

Abstract: In-Context Learning (ICL) has emerged as an important new paradigm in natural
language processing and large language model (LLM) applications. However, the
theoretical understanding of the ICL mechanism remains limited. This paper aims
to investigate this issue by studying a particular ICL approach, called
concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on
applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs
well for predicting query labels in prompts with only a few demonstrations. In
addition, the proposed theory quantifies the knowledge that can be leveraged by
the LLMs to the prompt tasks, and leads to a similarity measure between the
prompt demonstrations and the query input, which provides important insights
and guidance for model pre-training and prompt engineering in ICL. Moreover,
the impact of the prompt demonstration size and the dimension of the LLM
embeddings in ICL are also explored based on the proposed theory. Finally,
several real-data experiments are conducted to validate the practical
usefulness of CB-ICL and the corresponding theory.

</details>


### [47] [Optimal Repair of $(k+2, k, 2)$ MDS Array Codes](https://arxiv.org/abs/2509.21036)
*Zihao Zhang,Guodong Li,Sihuang Hu*

Main category: cs.IT

TL;DR: 本文针对具有两个奇偶校验节点和子分组大小为2的MDS码，推导了单节点故障修复带宽和I/O开销的紧下界，并提出了达到这些界限的显式MDS阵列码构造。


<details>
  <summary>Details</summary>
Motivation: 现有MSR码通常需要指数级大的子分组大小，导致显著的磁盘I/O开销。虽然已有研究探索了子分组大小与修复带宽之间的权衡，但对于固定子分组大小的MDS码的最小修复带宽这一基本问题仍未解决。

Method: 针对n-k=2和ℓ=2的情况，推导修复带宽和I/O开销的紧下界，并提出两种显式的MDS阵列码构造来分别达到这些界限。

Result: 获得了单节点故障修复带宽和I/O开销的紧下界，并设计了实际可用的码构造，具有可证明的修复效率。

Conclusion: 解决了固定子分组大小下MDS码最小修复带宽这一开放问题，为分布式存储系统提供了更实用的编码方案。

Abstract: Maximum distance separable (MDS) codes are widely used in distributed storage
systems as they provide optimal fault tolerance for a given amount of storage
overhead.
  The seminal work of Dimakis~\emph{et al.} first established a lower bound on
the repair bandwidth for a single failed node of MDS codes, known as the
\emph{cut-set bound}. MDS codes that achieve this bound are called minimum
storage regenerating (MSR) codes. Numerous constructions and theoretical
analyses of MSR codes reveal that they typically require exponentially large
sub-packetization levels, leading to significant disk I/O overhead. To mitigate
this issue, many studies explore the trade-offs between the sub-packetization
level and repair bandwidth, achieving reduced sub-packetization at the cost of
suboptimal repair bandwidth. Despite these advances, the fundamental question
of determining the minimum repair bandwidth for a single failure of MDS codes
with fixed sub-packetization remains open.
  In this paper, we address this challenge for the case of two parity nodes
($n-k=2$) and sub-packetization $\ell=2$. We derive tight lower bounds on both
the minimum repair bandwidth and the minimum I/O overhead. Furthermore, we
present two explicit MDS array code constructions that achieve these bounds,
respectively, offering practical code designs with provable repair efficiency.

</details>


### [48] [Task-Oriented Computation Offloading for Edge Inference: An Integrated Bayesian Optimization and Deep Reinforcement Learning Framework](https://arxiv.org/abs/2509.21090)
*Xian Li,Suzhi Bi,Ying-Jun Angela Zhang*

Main category: cs.IT

TL;DR: 论文提出LAB框架，通过结合深度强化学习和贝叶斯优化来解决边缘智能中数据传输精度与延迟的权衡问题，在真实自动驾驶数据集上实现了接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备将计算密集型AI任务卸载到边缘服务器时，传输高容量原始数据（如4K视频）在带宽受限的无线网络中会产生显著延迟。虽然可以通过降低数据质量（如分辨率）来减少传输延迟，但这会降低推理精度，形成了关键的精度-延迟权衡问题。

Method: 提出LAB学习框架，集成了深度强化学习（DRL）和贝叶斯优化（BO）：1）使用基于DNN的actor将系统状态映射到降级动作；2）使用基于BO的critic构建高斯过程代理模型来评估动作；3）通过凸优化高效推导最优带宽分配。

Result: 在真实世界自动驾驶数据集上的数值评估表明，LAB实现了接近最优的精度-延迟权衡，与穷举搜索相比仅产生1.22%的精度下降和0.07秒的额外延迟。

Conclusion: LAB框架有效解决了边缘智能中的黑盒混合整数非线性规划问题，通过智能学习机制在精度和延迟之间实现了良好的平衡，为边缘计算环境中的AI任务卸载提供了实用解决方案。

Abstract: Edge intelligence (EI) allows resource-constrained edge devices (EDs) to
offload computation-intensive AI tasks (e.g., visual object detection) to edge
servers (ESs) for fast execution. However, transmitting high-volume raw task
data (e.g., 4K video) over bandwidth-limited wireless networks incurs
significant latency. While EDs can reduce transmission latency by degrading
data before transmission (e.g., reducing resolution from 4K to 720p or 480p),
it often deteriorates inference accuracy, creating a critical accuracy-latency
tradeoff. The difficulty in balancing this tradeoff stems from the absence of
closed-form models capturing content-dependent accuracy-latency relationships.
Besides, under bandwidth sharing constraints, the discrete degradation
decisions among the EDs demonstrate inherent combinatorial complexity.
Mathematically, it requires solving a challenging \textit{black-box}
mixed-integer nonlinear programming (MINLP). To address this problem, we
propose LAB, a novel learning framework that seamlessly integrates deep
reinforcement learning (DRL) and Bayesian optimization (BO). Specifically, LAB
employs: (a) a DNN-based actor that maps input system state to degradation
actions, directly addressing the combinatorial complexity of the MINLP; and (b)
a BO-based critic with an explicit model built from fitting a Gaussian process
surrogate with historical observations, enabling model-based evaluation of
degradation actions. For each selected action, optimal bandwidth allocation is
then efficiently derived via convex optimization. Numerical evaluations on
real-world self-driving datasets demonstrate that LAB achieves near-optimal
accuracy-latency tradeoff, exhibiting only 1.22\% accuracy degradation and
0.07s added latency compared to exhaustive search...

</details>


### [49] [UAV-Enabled ISAC Systems with Fluid Antennas](https://arxiv.org/abs/2509.21105)
*Wenchao Liu,Xuhui Zhang,Jinke Ren,Weijie Yuan,Changsheng You,Shuangyang Li*

Main category: cs.IT

TL;DR: 提出了一种配备流体天线阵列的无人机集成感知通信框架，通过天线元件移动性引入额外空间自由度，同时提升通信和感知性能。


<details>
  <summary>Details</summary>
Motivation: 传统固定天线阵列限制了无人机充分发挥其潜力，需要克服这一限制来提升下一代无线系统的性能。

Method: 采用三时间尺度优化框架联合设计发射波束成形、流体天线位置和无人机轨迹，开发基于交替优化的算法求解非凸问题。

Result: 数值结果表明，所提方案显著优于各种基准方案，验证了将流体天线技术集成到无人机ISAC系统中的有效性。

Conclusion: 流体天线技术能够有效增强无人机集成感知通信系统的性能，为下一代无线系统提供了有前景的解决方案。

Abstract: Unmanned aerial vehicle (UAV)-enabled integrated sensing and communication
(ISAC) is regarded as a key enabler for next-generation wireless systems.
However, conventional fixed antenna arrays limit the ability of UAVs to fully
exploit their inherent potential. To overcome this limitation, we propose a
UAV-enabled ISAC framework equipped with fluid antenna (FA) arrays, where the
mobility of antenna elements introduces additional spatial degrees of freedom
to simultaneously enhance communication and sensing performance. A
multi-objective optimization problem is formulated to maximize the
communication rates of multiple users while minimizing the Cram\'er-Rao bound
(CRB) for single-target angle estimation. Due to excessively frequent updates
of FA positions may lead to response delays, a three-timescale optimization
framework is developed to jointly design transmit beamforming, FA positions,
and UAV trajectory based on their characteristics. To solve the non-convexity
of the problem, an alternating optimization-based algorithm is developed to
obtain a sub-optimal solution. Numerical results show that the proposed scheme
significantly outperforms various benchmark schemes, validating the
effectiveness of integrating the FA technology into the UAV-enabled ISAC
systems.

</details>


### [50] [Adapt or Regress: Rate-Memory-Compatible Spatially-Coupled Codes](https://arxiv.org/abs/2509.21112)
*Bade Aksoy,Doğukan Özbayrak,Ahmed Hareedy*

Main category: cs.IT

TL;DR: 本文提出了一种可重构的空间耦合码（RMC-SC码），通过增加码记忆实现速率兼容性，同时提升性能。采用概率设计和梯度下降算法优化短周期数，结合马尔可夫链蒙特卡洛方法进行有限长度优化。


<details>
  <summary>Details</summary>
Motivation: 在无线通信和存储系统中，需要支持多种信道条件和数据速率，自适应编码设计可以在保证可靠性的同时降低硬件成本。现有系统需要多种纠错编码方案，但缺乏高效的兼容性设计。

Method: 1. 概率设计RMC-SC码，通过增加SC码记忆实现速率兼容性；2. 使用梯度下降算法优化新组件的概率分布以最小化短周期数；3. 采用更新的马尔可夫链蒙特卡洛方法进行有限长度优化。

Result: 实验结果表明，与文献中的简单方案相比，RMC-SC码显著减少了周期数，并实现了显著的性能提升。

Conclusion: RMC-SC码提供了一种有效的自适应编码解决方案，能够递归设计任意数量的SC码，具有良好的兼容性和性能优势。

Abstract: Spatially-coupled (SC) codes are a class of low-density parity-check (LDPC)
codes that have excellent performance thanks to the degrees of freedom they
offer. An SC code is designed by partitioning a base matrix into components,
the number of which implies the code memory, then coupling and lifting them. In
the same system, various error-correction coding schemes are typically needed.
For example, in wireless communication standards, several channel conditions
and data rates should be supported. In storage and computing systems, stronger
codes should be adopted as the device ages. Adaptive code design enables
switching from one code to another when needed, ensuring reliability while
reducing hardware cost. In this paper, we introduce a class of reconfigurable
SC codes named rate-memory-compatible SC (RMC-SC) codes, which we design
probabilistically. In particular, rate compatibility in RMC-SC codes is
achieved via increasing the SC code memory, which also makes the codes
memory-compatible and improves performance. We express the expected number of
short cycles in the SC code protograph as a function of the fixed probability
distribution characterizing the already-designed SC code as well as the unknown
distribution characterizing the additional components. We use the
gradient-descent algorithm to find a locally-optimal distribution, in terms of
cycle count, for the new components. The method can be recursively used to
design any number of SC codes needed, and we show how to extend it to other
cases. Next, we perform the finite-length optimization using a Markov chain
Monte Carlo (MC$^2$) approach that we update to design the proposed RMC-SC
codes. Experimental results demonstrate significant reductions in cycle counts
and remarkable performance gains achieved by RMC-SC codes compared with a
literature-based straightforward scheme.

</details>


### [51] [Path-Controlled Secure Network Coding](https://arxiv.org/abs/2509.21115)
*Masahide Sasaki,Te Sun Han,Mikio Fujiwara,Kai Li,Oliver Hambrey,Atsushi Esumi*

Main category: cs.IT

TL;DR: 本文提出了一种名为PUSNEC的新型安全组播方法，通过多树路径查找与通用强斜坡安全网络编码相结合，实现了网络组播容量、信息论安全性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有安全组播方法（如量子密钥分发、物理层安全等）无法同时实现网络组播容量、长期安全性和可扩展性，特别是在大规模网络中，其假设条件（如可信节点、窃听阈值）逐渐失效。

Method: 开发了高效的多树组播路径查找方法，并与通用强斜坡安全网络编码（universal strongly ramp SNC）集成，构建了路径控制的通用强斜坡安全网络编码系统（PUSNEC）。

Result: 在概率窃听网络假设下推导了最大信息泄露量，通过数值模拟验证了多跳网络中的安全组播，定量分析了安全性与可靠性的权衡关系。

Conclusion: PUSNEC系统可叠加在现有QKD/PLS网络上，为实现全球范围内的安全可靠组播提供了一种实用方法。

Abstract: Multicast for securely sharing confidential data among many users is becoming
increasingly important. Currently, it relies on duplicate-and-forward routing
and cryptographic methods based on computational security. However, these
approaches neither attain multicast capacity of the network, nor ensure
long-term security against advances in computing (information-theoretic
security: ITS). Existing ITS solutions--quantum key distribution (QKD),
physical layer security (PLS), and secure network coding (SNC)--still fail to
enable scalable networks, as their underlying assumptions, such as trusted
nodes and wiretap thresholds, gradually become invalid as the network grows.
Here, we develop an efficient multi-tree multicast path-finding method to
address this issue, integrating it with universal strongly ramp SNC. This
system, path-controlled universal strongly ramp SNC (PUSNEC), can be overlaid
onto QKD/PLS networks, enabling multicast capacity, ITS, and scalability. We
derive the maximum leakage information to an eavesdropper under the
probabilistic wiretap network assumption and demonstrate secure multicast in
multi-hop networks through numerical simulations. Our quantitative analysis of
the secrecyreliability tradeoff highlights a practical approach to achieving
secure, reliable multicast on a global scale.

</details>


### [52] [A Converse For the Capacity of the Shotgun Sequencing Channel with Erasures](https://arxiv.org/abs/2509.21216)
*Mohammed Ihsan Ali,Hrishi Narayanan,Prasad Krishnan*

Main category: cs.IT

TL;DR: 本文针对带有擦除的鸟枪测序信道，提出了一个逆定理（converse），虽然该逆定理在一般情况下不紧，但在某些信道参数下渐近地达到了可达性结果。


<details>
  <summary>Details</summary>
Motivation: 现有文献已经研究了无噪声鸟枪测序的信息论容量，并考虑了实际测序器中可用的碱基质量分数，提出了带有擦除的鸟枪测序信道模型。本文旨在为该信道模型提供一个逆定理。

Method: 通过分析一个知道读段正确位置的精灵辅助解码器（genie-aided decoder），来推导逆定理。

Result: 得到了一个逆定理，该逆定理在一般情况下不紧，但在某些信道参数下渐近地达到了可达性结果。

Conclusion: 本文为带有擦除的鸟枪测序信道提供了一个逆定理，填补了该信道模型的理论分析空白，并在特定条件下达到了渐近紧性。

Abstract: The shotgun sequencing process involves fragmenting a long DNA sequence
(input string) into numerous shorter, unordered, and overlapping segments
(referred to as \emph{reads}). The reads are sequenced, and later aligned to
reconstruct the original string. Viewing the sequencing process as the
read-phase of a DNA storage system, the information-theoretic capacity of
noise-free shotgun sequencing has been characterized in literature. Motivated
by the base-wise quality scores available in practical sequencers, a recent
work considered the \emph{shotgun sequencing channel with erasures}, in which
the symbols in the reads are assumed to contain random erasures. Achievable
rates for this channel were identified. In the present work, we obtain a
converse for this channel. The arguments for the proof involve a careful
analysis of a genie-aided decoder, which knows the correct locations of the
reads. The converse is not tight in general. However, it meets the
achievability result asymptotically in some channel parameters.

</details>


### [53] [Fundamental Limits of Noncoherent Massive Random Access Networks](https://arxiv.org/abs/2509.21300)
*Grace Villacrés,Tobias Koch,Gonzalo Vazquez-Vilar*

Main category: cs.IT

TL;DR: 该论文研究了大规模随机接入蜂窝网络的容量，发现容量是否受限于发射功率取决于干扰信号的路径损耗衰减速度。当干扰衰减较慢时，容量有界；当衰减较快时，容量无界。


<details>
  <summary>Details</summary>
Motivation: 研究大规模随机接入蜂窝网络的容量极限，特别关注干扰对网络性能的影响，验证Lozano等人观察到的饱和现象是否可以通过随机用户活动或非高斯码本来避免。

Method: 采用MIMO衰落信道模型，假设无限个干扰小区，使用随机编码论证和相同码本分布假设，推导容量的严格上下界，分析不同路径损耗衰减情况下的容量行为。

Result: 当干扰信号的衰落系数按指数或更慢速度衰减时，容量受限于发射功率；当衰减速度快于双指数时，容量随功率增长无界。

Conclusion: 干扰限制网络中的饱和现象无法通过随机用户活动或扩展信道输入分布来避免，但通过突发信号和将干扰视为噪声的策略可以在特定条件下实现无界容量。

Abstract: This paper studies the capacity of massive random-access cellular networks,
modeled as a MIMO fading channel with an infinite number of interfering cells.
To characterize the symmetric sum rate of the network, a random-coding argument
is invoked together with the assumption that in all cells users draw their
codebooks according to the same distribution. This can be viewed as a
generalization of the assumption of Gaussian codebooks, often encountered in
the literature. The network is further assumed to be noncoherent: the
transmitters and receivers are cognizant of the statistics of the fading
coefficients, but are ignorant of their realizations. Finally, it is assumed
that the users access the network at random. For the considered channel model,
rigorous bounds on the capacity are derived. The behavior of these bounds
depends critically on the path loss from signals transmitted in interfering
cells to the intended cell. In particular, if the fading coefficients of the
interferers (ordered according to their distance to the receiver) decay
exponentially or more slowly, then the capacity is bounded in the transmit
power. This confirms that the saturation regime in interference-limited
networks -- observed by Lozano, Heath, and Andrews ("Fundamental limits of
cooperation", IEEE Trans. Inf. Theory, Sept. 2013) -- cannot be avoided by
random user activity or by using channel inputs beyond the scale family. In
contrast, if the fading coefficients decay faster than double-exponentially,
then the capacity is unbounded in the transmit power. Proving an unbounded
capacity is nontrivial even if the number of interfering cells is finite, since
the condition that the users' codebooks follow the same distribution prevents
interference-avoiding strategies such as time- or frequency-division multiple
access. We obtain this result by using bursty signaling together with treating
interference as noise.

</details>
