<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 18]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.IT](#cs.IT) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Fair Distribution of Digital Payments: Balancing Transaction Flows for Regulatory Compliance](https://arxiv.org/abs/2601.02369)
*Ashlesha Hota,Shashwat Kumar,Daman Deep Singh,Abolfazl Asudeh,Palash Dey,Abhijnan Chakraborty*

Main category: cs.NI

TL;DR: 论文提出了一种解决印度UPI支付应用交易量上限问题的算法，将用户交易重新分配到不同应用，同时最小化用户需要安装新应用的数量。


<details>
  <summary>Details</summary>
Motivation: 印度数字支付市场被PhonePe和Google Pay两家主导，形成双头垄断。印度国家支付公司(NPCI)规定单个UPI应用交易量不得超过30%，但如何在不造成用户不便的情况下重新分配交易流量是一个计算挑战。

Method: 将问题形式化为二分网络上的最小边激活流(MEAF)问题，提出名为解耦两阶段分配策略(DTAS)的可扩展启发式算法，利用流结构和容量重用。

Result: 实验表明DTAS能在几秒内找到接近最优整数线性规划(ILP)的解决方案，为公平高效执行交易上限提供了快速实用的方法。

Conclusion: 该研究为监管机构提供了有效的计算工具来执行交易上限政策，平衡市场竞争和用户便利性，同时证明了MEAF问题是NP完全的。

Abstract: The concentration of digital payment transactions in just two UPI apps like PhonePe and Google Pay has raised concerns of duopoly in India s digital financial ecosystem. To address this, the National Payments Corporation of India (NPCI) has mandated that no single UPI app should exceed 30 percent of total transaction volume. Enforcing this cap, however, poses a significant computational challenge: how to redistribute user transactions across apps without causing widespread user inconvenience while maintaining capacity limits? In this paper, we formalize this problem as the Minimum Edge Activation Flow (MEAF) problem on a bipartite network of users and apps, where activating an edge corresponds to a new app installation. The objective is to ensure a feasible flow respecting app capacities while minimizing additional activations. We further prove that Minimum Edge Activation Flow is NP-Complete. To address the computational challenge, we propose scalable heuristics, named Decoupled Two-Stage Allocation Strategy (DTAS), that exploit flow structure and capacity reuse. Experiments on large semi-synthetic transaction network data show that DTAS finds solutions close to the optimal ILP within seconds, offering a fast and practical way to enforce transaction caps fairly and efficiently.

</details>


### [2] [A Deep-SIC Channel Estimator Scheme in NOMA Network](https://arxiv.org/abs/2601.02373)
*Sumita Majhi,Kaushal Shelke,Pinaki Mitra*

Main category: cs.NI

TL;DR: Deep-SIC是一种基于Transformer的通道预测模型，利用NOMA中SIC的部分解码数据作为反馈信号，优化5G/下一代移动自组织网络中的切换决策，显著降低切换失败率和乒乓效应。


<details>
  <summary>Details</summary>
Motivation: 传统基于瞬时信道测量的切换触发机制容易因信道状态信息过时或不准确而导致切换失败和乒乓效应，特别是在高密度场景和移动用户中，需要更可靠的切换方案。

Method: 提出Deep-SIC模型，采用Transformer架构预测信道质量，利用NOMA中SIC的部分解码数据作为反馈信号持续改进预测，实现快速学习和稳定学习。

Result: 学习速度比现有最先进算法快68%，切换失败率降低达40%，在车辆速度下有效缓解乒乓效应，低信噪比下NRMSE降低20%，计算复杂度为线性O(K)。

Conclusion: Deep-SIC为动态无线网络中的鲁棒性和预测性移动性管理引入了新范式，通过利用SIC副产品改进信道预测，显著提升网络性能。

Abstract: In 5G and next-generation mobile ad-hoc networks, reliable handover is a key requirement, which guarantees continuity in connectivity, especially for mobile users and in high-density scenarios. However, conventional handover triggers based on instantaneous channel measurements are prone to failures and the ping-pong effect due to outdated or inaccurate channel state information. To address this, we introduce Deep-SIC, a knowledge-based channel prediction model that employs a Transformer-based approach to predict channel quality and optimise handover decisions. Deep-SIC is a unique model that utilises Partially Decoded Data (PDD), a byproduct of successive interference cancellation (SIC) in NOMA, as a feedback signal to improve its predictions continually. This special purpose enables learners to learn quickly and stabilise their learning. Our model learns 68\% faster than existing state-of-the-art algorithms, such as Graph-NOMA, while offering verifiable guarantees of stability and resilience to user mobility (Theorem~2). When simulated at the system level, it can be shown that our strategy can substantially enhance network performance: the handover failure rate can be reduced by up to 40\%, and the ping-pong effect can be mitigated, especially at vehicular speeds (e.g., 60 km/h). Moreover, Deep-SIC has a 20\% smaller normalised root mean square error (NRMSE) in low-SNR situations than state-of-the-art algorithms with linear computational complexity, $O(K)$. This work has introduced a new paradigm for robust and predictive mobility management in dynamic wireless networks.

</details>


### [3] [A Secure Edge Gateway Architecture for Wi-Fi-Enabled IoT](https://arxiv.org/abs/2601.02376)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.NI

TL;DR: 提出一种用于Wi-Fi物联网的安全边缘网关架构，在不改变现有基础设施的情况下增强本地网络保护，通过中间控制点监控流量、隔离不可信设备、防止常见无线攻击，在真实办公环境中部署验证了有效性。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi物联网设备面临多种无线攻击威胁（如欺骗、解除认证、未授权访问），现有基础设施难以在不改变网络架构的情况下提供足够保护，需要一种轻量级、适应性强的边缘安全解决方案。

Method: 设计安全边缘网关作为Wi-Fi接入点与核心网络之间的中间控制点，采用自适应流量过滤和轻量级策略执行，而非复杂分析模型，监控流量、隔离不可信设备、防止常见无线攻击。

Result: 在约70台设备（含28个物联网单元）的真实办公环境中部署10天，成功欺骗事件减少87%，解除认证后恢复时间改善42%，网络延迟仅增加3.1%，吞吐量降低不到4%，相比WPA3基线配置。

Conclusion: 在边缘层实施安全功能可显著提高Wi-Fi物联网环境的弹性，而不会引入明显开销或需要专用硬件，为中等规模网络环境提供了有效的安全增强方案。

Abstract: This paper presents a Secure Edge Gateway Architecture for Wi-Fi-Enabled IoT designed to strengthen local network protection without altering existing infrastructure. The proposed gateway acts as an intermediate control point between Wi-Fi access points and the core network, monitoring traffic, isolating untrusted devices, and preventing common wireless attacks such as spoofing, deauthentication, and unauthorized access. The design focuses on adaptive traffic filtering and lightweight policy enforcement instead of complex analytical models, making it suitable for medium-sized network environments. The prototype gateway was deployed in a real office with around 70 total devices, including 28 IoT units such as sensors, cameras, and smart controllers. Over ten days of continuous operation, the system reduced successful spoofing incidents by 87% and improved recovery time after deauthentication by 42%, while increasing network latency by only 3.1% and reducing throughput by less than 4% compared to a baseline WPA3 configuration. These results confirm that implementing security functions at the edge layer can significantly improve the resilience of Wi-Fi-enabled IoT environments without introducing noticeable overhead or requiring specialized hardware.

</details>


### [4] [How to Discover Knowledge for FutureG: Contextual RAG and LLM Prompting for O-RAN](https://arxiv.org/abs/2601.02382)
*Nathan Conger,Nathan Scollar,Kemal Davaslioglu,Yalin E. Sagduyu,Sastry Kompella*

Main category: cs.NI

TL;DR: 提出基于上下文检索增强生成（Contextual RAG）的问答框架，用于5G/6G网络中的O-RAN标准文档理解，相比传统RAG能实现更精准的文档检索和问答准确性。


<details>
  <summary>Details</summary>
Motivation: O-RAN（开放无线接入网）作为5G/6G网络中的关键架构，其快速变化的技术规范和复杂接口给研究人员和从业者带来了巨大挑战。手动查阅这些文档耗时且易错，严重影响了系统设计、集成和部署效率。

Method: 采用Contextual RAG方法，通过候选答案选项指导文档检索，并结合特定文本块的上下文信息来提升大语言模型性能。该方法在动态领域特别有效，无需对LLM进行微调即可适应快速变化的数据。在ORANBenchmark-13K数据集上评估了Llama3.2、Qwen2.5-7B和Qwen3.0-4B三种模型，并比较了直接问答和思维链两种提示策略。

Result: Contextual RAG在准确性上持续优于标准RAG和基础提示方法，同时在运行时间和CO2排放方面保持竞争力。该方法显著提升了文档检索的相关性，特别是在查询本身缺乏足够上下文的情况下。

Conclusion: Contextual RAG为O-RAN及更广泛的5G/6G环境提供了一个可扩展且有效的领域特定问答解决方案，能够在保持效率和可持续性的同时，更准确地解读不断演进的技术标准。

Abstract: We present a retrieval-augmented question answering framework for 5G/6G networks, where the Open Radio Access Network (O-RAN) has become central to disaggregated, virtualized, and AI-driven wireless systems. While O-RAN enables multi-vendor interoperability and cloud-native deployments, its fast-changing specifications and interfaces pose major challenges for researchers and practitioners. Manual navigation of these complex documents is labor-intensive and error-prone, slowing system design, integration, and deployment. To address this challenge, we adopt Contextual Retrieval-Augmented Generation (Contextual RAG), a strategy in which candidate answer choices guide document retrieval and chunk-specific context to improve large language model (LLM) performance. This improvement over traditional RAG achieves more targeted and context-aware retrieval, which improves the relevance of documents passed to the LLM, particularly when the query alone lacks sufficient context for accurate grounding. Our framework is designed for dynamic domains where data evolves rapidly and models must be continuously updated or redeployed, all without requiring LLM fine-tuning. We evaluate this framework using the ORANBenchmark-13K dataset, and compare three LLMs, namely, Llama3.2, Qwen2.5-7B, and Qwen3.0-4B, across both Direct Question Answering (Direct Q&A) and Chain-of-Thought (CoT) prompting strategies. We show that Contextual RAG consistently improves accuracy over standard RAG and base prompting, while maintaining competitive runtime and CO2 emissions. These results highlight the potential of Contextual RAG to serve as a scalable and effective solution for domain-specific Q&A in ORAN and broader 5G/6G environments, enabling more accurate interpretation of evolving standards while preserving efficiency and sustainability.

</details>


### [5] [Base Station Deployment under EMF constrain by Deep Reinforcement learning](https://arxiv.org/abs/2601.02385)
*Mohammed Mallik,Guillaume Villemaud*

Main category: cs.NI

TL;DR: 提出cGAN预测RSS和EMF暴露，结合DQN优化基站部署，实现从小时级到秒级的快速网络设计


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络密集部署、毫米波通信和动态波束成形需要可扩展的仿真工具，以评估覆盖范围和射频电磁场暴露等关键性能指标，指导网络设计并确保符合安全法规

Method: 1. 提出条件生成对抗网络(cGAN)从网络拓扑预测位置特定的接收信号强度(RSS)和EMF暴露；2. 提出深度Q网络(DQN)框架，利用训练好的cGAN进行最优基站部署

Result: 相比传统射线追踪仿真，cGAN将推理和部署时间从数小时减少到秒级；GAN-DQN框架支持覆盖和暴露约束下的序列决策，学习有效的部署策略

Conclusion: 该框架适用于动态场景中的实时设计和适应，能够满足预定义的网络特定异构性能目标，解决了基站部署问题

Abstract: As 5G networks rapidly expand and 6G technologies emerge, characterized by dense deployments, millimeter-wave communications, and dynamic beamforming, the need for scalable simulation tools becomes increasingly critical. These tools must support efficient evaluation of key performance metrics such as coverage and radio-frequency electromagnetic field (RF-EMF) exposure, inform network design decisions, and ensure compliance with safety regulations. Moreover, base station (BS) placement is a crucial task in the network design, where satisfying coverage requirements is essential. To address these, based on our previous work, we first propose a conditional generative adversarial network (cGAN) that predicts location specific received signal strength (RSS), and EMF exposure simultaneously from the network topology, as images. As a network designing application, we propose a Deep Q Network (DQN) framework, using the trained cGAN, for optimal base station (BS) deployment in the network. Compared to conventional ray tracing simulations, the proposed cGAN reduces inference and deployment time from several hours to seconds. Unlike a standalone cGAN, which provides static performance maps, the proposed GAN-DQN framework enables sequential decision making under coverage and exposure constraints, learning effective deployment strategies that directly solve the BS placement problem. Thus making it well suited for real time design and adaptation in dynamic scenarios in order to satisfy pre defined network specific heterogeneous performance goals.

</details>


### [6] [Regional Resource Management for Service Provisioning in LEO Satellite Networks: A Topology Feature-Based DRL Approach](https://arxiv.org/abs/2601.02387)
*Chenxi Bao,Di Zhou,Min Sheng,Yan Shi,Jiandong Li,Zhili Sun*

Main category: cs.NI

TL;DR: 提出基于区域资源管理(RRM)模式和深度强化学习的卫星网络资源管理算法，解决网络规模变化下的端到端服务资源链动态规划问题


<details>
  <summary>Details</summary>
Motivation: 低轨卫星网络固有的拓扑动态性和不确定的网络规模，使得端到端服务资源链需要高效重规划，实现高度自适应的资源管理在实际部署中具有重要意义

Method: 首先设计区域资源管理(RRM)模式，构建与网络规模无关的统一决策空间；然后基于RRM模式和深度强化学习框架，开发基于拓扑特征的动态自适应资源管理算法，考虑神经网络固定输出维度和变化的资源链需求

Result: 提出的算法在收敛性能和收敛速度方面表现最佳，对于变化的网络规模显著提升服务性能，相比对比算法分别获得超过2.7%、11.9%和10.2%的性能增益

Conclusion: 通过RRM模式和深度强化学习的结合，成功解决了卫星网络中网络规模变化带来的资源管理挑战，服务导向信息和分阶段奖励函数的匹配设计有效提升了算法在RRM模式下的服务性能

Abstract: Satellite networks with wide coverage are considered natural extensions to terrestrial networks for their long-distance end-to-end (E2E) service provisioning. However, the inherent topology dynamics of low earth orbit satellite networks and the uncertain network scales bring an inevitable requirement that resource chains for E2E service provisioning must be efficiently re-planned. Therefore, achieving highly adaptive resource management is of great significance in practical deployment applications. This paper first designs a regional resource management (RRM) mode and further formulates the RRM problem that can provide a unified decision space independent of the network scale. Subsequently, leveraging the RRM mode and deep reinforcement learning framework, we develop a topology feature-based dynamic and adaptive resource management algorithm to combat the varying network scales. The proposed algorithm successfully takes into account the fixed output dimension of the neural network and the changing resource chains for E2E service provisioning. The matched design of the service orientation information and phased reward function effectively improves the service performance of the algorithm under the RRM mode. The numerical results demonstrate that the proposed algorithm with the best convergence performance and fastest convergence rate significantly improves service performance for varying network scales, with gains over compared algorithms of more than 2.7%, 11.9%, and 10.2%, respectively.

</details>


### [7] [Generative AI for Networking](https://arxiv.org/abs/2601.02389)
*Faisal Zaman,Ouns Bouachir,Moayad Aloqaily,Ismaeel Al Ridhawi*

Main category: cs.NI

TL;DR: 本文探讨了生成式AI和大语言模型在电信网络管理中的革命性作用，通过自然语言处理和自注意力机制实现网络自主优化和流量预测，推动自适应网络发展。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大语言模型正在彻底改变网络管理系统，为实现完全自主和自优化的通信系统铺平道路。这些技术能够帮助网络处理从短期运营到长期战略规划的复杂决策任务，提升电信服务的性能和用户体验。

Method: 利用大语言模型的自然语言理解能力分析客户查询、预测网络拥塞模式和自动化故障排除；使用生成式AI优化内容交付，生成个性化推荐并动态调整网络资源；特别提出利用Transformer的自注意力机制进行长期流量预测的用例。

Result: 这些技术能够实现更高效的客户支持和网络维护，提高用户参与度，根据实时需求动态调整网络资源，最终增强电信服务的整体性能和用户体验，将网络弹性和适应性提升到前所未有的水平。

Conclusion: 生成式AI和大语言模型在推进网络性能和实现自适应网络目标方面发挥着关键作用，展示了这些前沿技术在彻底改变电信网络方面的变革力量，为构建更具弹性和适应性的通信系统提供了技术基础。

Abstract: Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are revolutionizing network management systems, paving the way towards fully autonomous and self-optimizing communication systems. These models enable networks to address complex decision-making tasks across both short-term operational scenarios and long-term strategic planning. Through natural language understanding, LLMs can analyze customer inquiries, predict network congestion patterns, and automate troubleshooting processes, leading to more efficient customer support and network maintenance. GenAI can optimize content delivery by generating personalized recommendations, improving user engagement, and dynamically adjusting network resources based on real-time demands, ultimately enhancing overall performance and user experience in telecommunication services. In this paper, we discuss the pivotal role of GenAI in advancing network performance and achieving the ultimate objective of self-adaptive networks. Moreover, we present a use case that leverages the self-attention mechanism of transformers to perform long-term traffic prediction. Harnessing these cutting-edge technologies demonstrates the transformative power of LLM and GenAI in revolutionizing telecommunication networks, elevating resilience and adaptability to unprecedented levels.

</details>


### [8] [SLASh: Simulation of LISLs Aboard LEO Satellite Shells](https://arxiv.org/abs/2601.02396)
*Davy Romine,Andrew Kingery,Guanqun Song,Ting Zhu*

Main category: cs.NI

TL;DR: SLASh是一个高度可定制的LEO卫星网络仿真器，用于模拟真实世界条件并生成抽象遥测数据，帮助识别网络瓶颈和资源浪费问题。


<details>
  <summary>Details</summary>
Motivation: 当前LEO卫星网络技术缺乏强大的开源仿真模型，无法有效识别潜在瓶颈和资源浪费问题，导致地面用户和网络提供商浪费时间和金钱。

Method: 开发SLASh仿真器，允许用户设计具有特定特征的模拟网络，并按照真实世界条件构建网络模型，同时生成可在网络中模拟传输的抽象遥测数据。

Result: SLASh能够模拟真实世界条件下的卫星网络，生成抽象遥测数据，使用户能够在多种框架下比较网络性能。

Conclusion: SLASh为LEO卫星网络提供了一个强大的开源仿真工具，有助于识别网络瓶颈、优化资源分配，提高网络效率和成本效益。

Abstract: Recent advances in satellite technology have introduced a new frontier of wireless networking by establishing Low Earth Orbit (LEO) Satellite networks that work to connect difficult to reach areas and improve global connectivity. These novel advancements lack robust open-source simulation models that can highlight potential bottlenecks or potential wasted resources, wasting terrestrial users and the companies that provide these networks time and money. To that end, we propose SLASh, a highly-customizable satellite network simulation which allows users to design a simulated network with specific characteristics, and constructs them analog to real-world conditions. Additionally, SLASh can generate abstract telemetry that can be simulated moving throughout the network, allowing users to compare network capabilities across a variety of frameworks.

</details>


### [9] [AI-Native Integrated Sensing and Communications for Self-Organizing Wireless Networks: Architectures, Learning Paradigms, and System-Level Design](https://arxiv.org/abs/2601.02398)
*S. Zhang,M. Feizarefi,A. F. Mirzaei*

Main category: cs.NI

TL;DR: 本文综述了AI原生的集成感知与通信（ISAC）自组织无线网络，为6G及未来网络提供系统级框架，涵盖信号模型、网络感知、学习驱动机制和跨层架构。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需要同时支持数据传输和环境感知，ISAC作为基础范式能够实现这一目标。同时，未来无线系统的规模、异构性和动态性需要自组织网络智能来自主管理资源、拓扑和服务。AI技术是实现这一愿景的关键使能器。

Method: 提出统一分类法，涵盖：1) ISAC信号模型和感知模态；2) 从感知感知无线电数据的网络状态抽象和感知；3) 用于资源分配、拓扑控制和移动管理的学习驱动自组织机制；4) 集成感知、通信和网络智能的跨层架构。同时探讨深度强化学习、图学习、多智能体协调和联邦智能等新兴学习范式。

Result: 提供了AI原生ISAC自组织无线网络的全面系统级综述，建立了统一的理论框架，分析了感知-通信权衡、可扩展性、延迟、可靠性和安全性等实际问题，并提出了代表性的评估方法和性能指标。

Conclusion: AI原生ISAC系统是6G及未来网络的关键发展方向，但仍面临可部署性、可信性和可扩展性等开放挑战，需要进一步研究以实现实际部署。

Abstract: Integrated Sensing and Communications (ISAC) is emerging as a foundational paradigm for next-generation wireless networks, enabling communication infrastructures to simultaneously support data transmission and environment sensing. By tightly coupling radio sensing with communication functions, ISAC unlocks new capabilities for situational awareness, localization, tracking, and network adaptation. At the same time, the increasing scale, heterogeneity, and dynamics of future wireless systems demand self-organizing network intelligence capable of autonomously managing resources, topology, and services. Artificial intelligence (AI), particularly learning-driven and data-centric methods, has become a key enabler for realizing this vision. This survey provides a comprehensive and system-level review of AI-native ISAC-enabled self-organizing wireless networks. We develop a unified taxonomy that spans: (i) ISAC signal models and sensing modalities, (ii) network state abstraction and perception from sensing-aware radio data, (iii) learning-driven self-organization mechanisms for resource allocation, topology control, and mobility management, and (iv) cross-layer architectures integrating sensing, communication, and network intelligence. We further examine emerging learning paradigms, including deep reinforcement learning, graph-based learning, multi-agent coordination, and federated intelligence that enable autonomous adaptation under uncertainty, mobility, and partial observability. Practical considerations such as sensing-communication trade-offs, scalability, latency, reliability, and security are discussed alongside representative evaluation methodologies and performance metrics. Finally, we identify key open challenges and future research directions toward deployable, trustworthy, and scalable AI-native ISAC systems for 6G and beyond.

</details>


### [10] [Auction-Driven Spectrum Allocation With AutoEncoder-Based Compression in Rural Wireless Networks: A Novel Framework for Reliable Telemedicine](https://arxiv.org/abs/2601.02402)
*Nadjemat El Houda Issaad,Ismail Lotfi,Mohamed Senouci,Zekri Lougmiri*

Main category: cs.NI

TL;DR: 本文提出了一种结合自编码器数据压缩和拍卖理论频谱分配的混合框架，用于改善农村无线网络中的医疗数据传输效率和频谱利用率。


<details>
  <summary>Details</summary>
Motivation: 农村医疗面临专业医疗服务有限、诊断设备不足等问题，医疗图像和数据传输到城市医院存在带宽限制、网络不可靠、数据安全隐私担忧等挑战，同时医疗数据量大、物联网设备电池寿命有限，频谱分配效率低下。

Method: 提出一种新颖的混合框架，集成自编码器（AE）数据压缩技术和拍卖理论频谱分配机制。AE用于减少通信开销而不牺牲图像质量，拍卖理论用于动态优化频谱分配。

Result: 通过大量仿真验证，该框架能够提高频谱利用率、传输效率和整体连接性，为改善农村远程医疗基础设施提供了实用解决方案。

Conclusion: 该混合框架有效解决了农村无线网络中医疗数据传输的通信效率和频谱利用问题，为农村远程医疗基础设施的增强提供了可行的技术方案。

Abstract: Rural healthcare faces numerous challenges, including limited access to specialized medical services and diagnostic equipment, which delays patient care. Enhancing the ability to transmit medical images and data from rural areas to urban hospitals via wireless networks is critical. However, bandwidth limitations, unreliable networks, and concerns over data security and privacy hinder efficient transmission. Additionally, the high data volume of medical content and the limited battery life of IoT devices pose further challenges. To address these challenges, data compression techniques such as Autoencoders (AEs) offer promising solutions by significantly reducing the communication overhead without sacrificing essential image quality or details. Additionally, spectrum allocation mechanisms in rural areas are often inefficient, leading to poor resource utilization. Auction theory presents a dynamic and adaptive approach to optimize spectrum allocation. This paper proposes a novel hybrid framework that integrates AE-based data compression with auction-based spectrum allocation, addressing both communication efficiency and spectrum utilization in rural wireless networks. Extensive simulations validate the framework's ability to improve spectrum utilization, transmission efficiency, and overall connectivity, offering a practical solution for enhancing rural telemedicine infrastructure.

</details>


### [11] [Optimal Oblivious Load-Balancing for Sparse Traffic in Large-Scale Satellite Networks](https://arxiv.org/abs/2601.02537)
*Rudrapatna Vallabh Ramakanth,Eytan Modiano*

Main category: cs.NI

TL;DR: 本文研究环形网络中的无感知负载均衡问题，针对稀疏流量场景（最多k个活跃源-目的对），建立了理论下界并构造了最优无感知路由方案。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于大规模低轨卫星网络的负载均衡问题，该网络可建模为环形拓扑，且流量具有稀疏性和局部热点特性。

Method: 将问题建模为线性规划，分析无感知路由的理论下界，比较Valiant负载均衡方案，并构造最优无感知负载均衡方案。

Result: 证明了无感知路由的最坏情况负载下界：当1<k≤N²/2时约为√(2k)/4，当N²/2≤k≤N²时为N/4。发现Valiant方案在稀疏流量下非最优，构造了达到下界的最优方案，并揭示了无感知与非无感知路由间存在√2倍性能差距。

Conclusion: 在环形网络的稀疏流量场景下，存在最优无感知负载均衡方案可达到理论下界，且无感知路由相比非无感知路由存在固有性能差距，结果可推广到一般环形网络。

Abstract: Oblivious load-balancing in networks involves routing traffic from sources to destinations using predetermined routes independent of the traffic, so that the maximum load on any link in the network is minimized. We investigate oblivious load-balancing schemes for a $N\times N$ torus network under sparse traffic where there are at most $k$ active source-destination pairs. We are motivated by the problem of load-balancing in large-scale LEO satellite networks, which can be modelled as a torus, where the traffic is known to be sparse and localized to certain hotspot areas. We formulate the problem as a linear program and show that no oblivious routing scheme can achieve a worst-case load lower than approximately $\frac{\sqrt{2k}}{4}$ when $1<k \leq N^2/2$ and $\frac{N}{4}$ when $N^2/2\leq k\leq N^2$. Moreover, we demonstrate that the celebrated Valiant Load Balancing scheme is suboptimal under sparse traffic and construct an optimal oblivious load-balancing scheme that achieves the lower bound. Further, we discover a $\sqrt{2}$ multiplicative gap between the worst-case load of a non-oblivious routing and the worst-case load of any oblivious routing. The results can also be extended to general $N\times M$ tori with unequal link capacities along the vertical and horizontal directions.

</details>


### [12] [Which Deep Learner? A Systematic Evaluation of Advanced Deep Forecasting Models Accuracy and Efficiency for Network Traffic Prediction](https://arxiv.org/abs/2601.02694)
*Eilaf MA Babai,Aalaa MA Babai,Koji Okamura*

Main category: cs.NI

TL;DR: 该研究系统评估了12种先进时间序列预测模型在4个真实网络流量数据集上的表现，包括transformer和传统深度学习模型，评估了性能、鲁棒性、数据效率和资源效率，为网络流量预测提供了部署指导和建模方向。


<details>
  <summary>Details</summary>
Motivation: 网络流量预测对自动化网络管理至关重要，但由于网络环境多样性和流量时间尺度变化，需要识别有效的部署选择和建模方向。现有深度学习模型在时间序列预测方面有进展，但缺乏针对网络流量预测的系统评估。

Method: 系统识别并评估12种先进时间序列预测模型（包括transformer和传统DL方法）和3个统计基线模型，在4个真实流量数据集上进行多时间尺度和预测范围的测试，评估性能、异常鲁棒性、数据缺口处理、外部因素影响、数据效率和资源效率（时间、内存、能耗）。

Result: 研究结果突出了性能区间、效率阈值，以及能够平衡准确性和效率的有前景的架构，展示了模型对流量挑战的鲁棒性，并提出了超越传统RNN的新方向。

Conclusion: 该研究为网络流量预测提供了系统评估框架，识别了在不同场景下表现优异的模型架构，为实际部署提供了指导，并指出了超越传统RNN的未来研究方向。

Abstract: Network traffic prediction is essential for automating modern network management. It is a difficult time series forecasting (TSF) problem that has been addressed by Deep Learning (DL) models due to their ability to capture complex patterns. Advances in forecasting, from sophisticated transformer architectures to simple linear models, have improved performance across diverse prediction tasks. However, given the variability of network traffic across network environments and traffic series timescales, it is essential to identify effective deployment choices and modeling directions for network traffic prediction. This study systematically identify and evaluates twelve advanced TSF models -- including transformer-based and traditional DL approaches, each with unique advantages for network traffic prediction -- against three statistical baselines on four real traffic datasets, across multiple time scales and horizons, assessing performance, robustness to anomalies, data gaps, external factors, data efficiency, and resource efficiency in terms of time, memory, and energy. Results highlight performance regimes, efficiency thresholds, and promising architectures that balance accuracy and efficiency, demonstrating robustness to traffic challenges and suggesting new directions beyond traditional RNNs.

</details>


### [13] [Probabilistic Time Slot Leasing in TDMA-Based IoT Networks for Enhanced Channel Utilization](https://arxiv.org/abs/2601.02930)
*Hicham Lakhlef,Mohamed Ali Zormati,Khaled Abid,Toufik Ahmed*

Main category: cs.NI

TL;DR: 提出一种完全分布式的TDMA调度协议，通过智能重新分配未充分利用的时隙来最大化通信资源利用率，特别适用于资源受限的物联网网络。


<details>
  <summary>Details</summary>
Motivation: 在大规模资源受限的无线网络（如物联网）中，高效通信调度仍然是一个关键挑战。现有TDMA协议在动态环境中性能不佳，特别是当节点活动水平随时间波动时。

Method: 提出一种完全分布式的TDMA调度协议，能够自适应地重新分配暂时不活跃节点的未充分利用时隙给通信需求更高的节点。采用轻量级概率机制来管理未使用时隙的临时租赁，平衡时隙可用性和传输可靠性之间的权衡。

Result: 在各种网络场景下的仿真表明，该协议显著提高了资源受限环境中的吞吐量、延迟和可靠性。

Conclusion: 该协议作为下一代物联网网络中自适应和节能调度的稳健可扩展解决方案具有巨大潜力。

Abstract: In large-scale resource-constrained wireless networks, such as those prevalent in the Internet of Things (IoT), efficient communication scheduling remains a critical challenge. Among the various approaches, Time Division Multiple Access (TDMA) protocols have been widely adopted for their structured and collision-free communication capabilities. Nevertheless, despite extensive research in this area, current solutions often exhibit suboptimal performance, particularly in dynamic environments where node activity levels fluctuate over time.
  This paper introduces a novel fully distributed TDMA-based scheduling protocol that intelligently maximizes the utilization of communication resources. The proposed approach adaptively reallocates underutilized time slots, originally assigned to temporarily inactive nodes, to those experiencing higher communication demands. This dynamic reallocation not only improves channel utilization but also reduces idle periods, thereby enhancing overall network efficiency. To further enhance performance, we incorporate a lightweight probabilistic mechanism that governs the temporal leasing of unused slots. This mechanism balances the trade-off between slot availability and transmission reliability, minimizing packet loss while preserving fairness and stability within the network.
  Simulations across a range of network scenarios demonstrate that our protocol significantly improves throughput, latency, and reliability in resource-constrained environments. These results highlight the protocol's potential as a robust and scalable solution for adaptive and energy-efficient scheduling in next-generation IoT networks.

</details>


### [14] [Eco-WakeLoc: An Energy-Neutral and Cooperative UWB Real-Time Locating System](https://arxiv.org/abs/2601.03171)
*Silvano Cortesi,Lukas Schulthess,Davide Plozza,Christian Vogt,Michele Magno*

Main category: cs.NI

TL;DR: Eco-WakeLoc：结合超低功耗唤醒无线电与太阳能收集的厘米级UWB室内定位系统，通过按需激活锚节点和协作定位实现能量中性运行


<details>
  <summary>Details</summary>
Motivation: 室内定位系统面临效率与响应性的基本权衡，传统RTLS要么需要持续供电的基础设施（限制可扩展性），要么响应性有限。移动机器人在GPS拒止环境中的新兴应用需要高精度定位同时保持能量中性。

Method: 结合超低功耗唤醒无线电与太阳能收集；按需激活锚节点消除持续能耗；采用协作定位：主动标签发起测距交换（三边测量），被动标签机会性重用这些消息进行TDOA定位；基于AIMD的能量感知调度器根据收集的能量自适应调整定位速率。

Result: 主动标签每次定位能耗3.22mJ，被动标签951uJ，锚节点353uJ；四足机器人实际部署（9个锚节点）在动态室内环境中平均精度43cm；年度模拟显示标签每天平均2031次定位，一年后电池容量保持超过7%，实现持续能量中性运行。

Conclusion: Eco-WakeLoc证明高精度室内定位可以在不持续运行基础设施的情况下大规模实现，结合能量中性、协作定位和自适应调度，为移动机器人在GPS拒止环境中的定位提供了可持续解决方案。

Abstract: Indoor localization systems face a fundamental trade-off between efficiency and responsiveness, which is especially important for emerging use cases such as mobile robots operating in GPS-denied environments. Traditional RTLS either require continuously powered infrastructure, limiting their scalability, or are limited by their responsiveness. This work presents Eco-WakeLoc, designed to achieve centimeter-level UWB localization while remaining energy-neutral by combining ultra-low power wake-up radios (WuRs) with solar energy harvesting. By activating anchor nodes only on demand, the proposed system eliminates constant energy consumption while achieving centimeter-level positioning accuracy. To reduce coordination overhead and improve scalability, Eco-WakeLoc employs cooperative localization where active tags initiate ranging exchanges (trilateration), while passive tags opportunistically reuse these messages for TDOA positioning. An additive-increase/multiplicative-decrease (AIMD)-based energy-aware scheduler adapts localization rates according to the harvested energy, thereby maximizing the overall performance of the sensor network while ensuring long-term energy neutrality. The measured energy consumption is only 3.22mJ per localization for active tags, 951uJ for passive tags, and 353uJ for anchors. Real-world deployment on a quadruped robot with nine anchors confirms the practical feasibility, achieving an average accuracy of 43cm in dynamic indoor environments. Year-long simulations show that tags achieve an average of 2031 localizations per day, retaining over 7% battery capacity after one year -- demonstrating that the RTLS achieves sustained energy-neutral operation. Eco-WakeLoc demonstrates that high-accuracy indoor localization can be achieved at scale without continuous infrastructure operation, combining energy neutrality, cooperative positioning, and adaptive scheduling.

</details>


### [15] [Multi-Modal Data-Enhanced Foundation Models for Prediction and Control in Wireless Networks: A Survey](https://arxiv.org/abs/2601.03181)
*Han Zhang,Mohammad Farzanullah,Mohammad Ghassemi,Akram Bin Sediq,Ali Afana,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 该论文探讨了基础模型在无线网络中的应用，重点关注多模态基础模型如何支持网络管理中的预测和控制任务，并讨论了开发无线专用基础模型的挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型被认为是人工智能领域的突破性进展，正在重塑学术界和工业界的AI未来。将基础模型集成到无线网络中，有望开发出能够处理多样化网络管理请求和复杂无线任务（涉及多模态数据）的通用AI代理。

Method: 论文首先讨论了基础模型支持的多模态上下文信息理解在无线网络中的应用，然后分别解释了基础模型如何应用于预测任务和控制任务。接着从两个角度介绍了无线专用基础模型的开发：可用的数据集和开发方法。

Result: 论文系统性地阐述了基础模型在无线网络管理中的应用框架，包括多模态信息理解、预测与控制任务的具体应用方式，以及开发无线专用基础模型所需的数据集和方法论。

Conclusion: 论文总结了基础模型增强无线网络面临的挑战和未来发展方向，强调了多模态基础模型在实现智能无线网络管理中的潜力和重要性。

Abstract: Foundation models (FMs) are recognized as a transformative breakthrough that has started to reshape the future of artificial intelligence (AI) across both academia and industry. The integration of FMs into wireless networks is expected to enable the development of general-purpose AI agents capable of handling diverse network management requests and highly complex wireless-related tasks involving multi-modal data. Inspired by these ideas, this work discusses the utilization of FMs, especially multi-modal FMs in wireless networks. We focus on two important types of tasks in wireless network management: prediction tasks and control tasks. In particular, we first discuss FMs-enabled multi-modal contextual information understanding in wireless networks. Then, we explain how FMs can be applied to prediction and control tasks, respectively. Following this, we introduce the development of wireless-specific FMs from two perspectives: available datasets for development and the methodologies used. Finally, we conclude with a discussion of the challenges and future directions for FM-enhanced wireless networks.

</details>


### [16] [TaNG: Modeling Packet Classification with TSS-assisted Neural Networks on GPUs](https://arxiv.org/abs/2601.03187)
*Zhengyu Liao,Shiyou Qian*

Main category: cs.NI

TL;DR: TaNG是一种基于学习的包分类方法，通过单一神经网络处理多维特征确保完整覆盖，结合元组空间降低模型复杂度，并采用CPU-GPU混合流框架提升吞吐量，在512k规则集上相比现有方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的包分类方法在处理重叠规则时存在覆盖不全或规则复制过多的问题，且GPU集成有限，难以应对大规模规则集的性能需求。

Method: 提出TaNG方法：1) 使用单一神经网络训练多维特征确保完整覆盖；2) 采用半结构化设计结合神经网络和元组空间降低复杂度；3) 基于半结构设计规则更新机制；4) 实现针对学习方法的CPU-GPU混合流框架。

Result: 在GPU分类框架上测试512k规则集，TaNG相比NuevoMatch和NeuTree分别实现12.19倍和9.37倍吞吐量提升，以及98.84倍和156.98倍性能稳定性提升。

Conclusion: TaNG通过创新的半结构化设计和CPU-GPU混合流框架，有效解决了现有学习方法在重叠规则处理和大规模规则集性能方面的局限性，显著提升了包分类的吞吐量和稳定性。

Abstract: Packet classification is a core function in software-defined networks, and learning-based methods have recently shown significant throughput gains on large-scale rulesets. However, existing learning-based approaches struggle with overlapping rules, leading to incomplete model coverage or excessive rule replication. Their limited GPU integration also hampers performance with large-scale rulesets. To address these issues, we propose TaNG, which utilizes a single neural network trained on multi-dimensional features to ensure complete coverage without duplicating rules. TaNG employs a semi-structured design that combines the neural network model with a tuple space, reducing model complexity. Furthermore, we develop a mechanism based on the semi-structure for rule updates. Finally, we implement a CPU-GPU hybrid streaming framework tailored for learning-based methods, further enhancing throughput. On our GPU-based classification framework with 512k rulesets, TaNG achieves 12.19x and 9.37x higher throughput and 98.84x and 156.98x higher performance stability compared to two state-of-the-art learning methods NuevoMatch and NeuTree, respectively.

</details>


### [17] [oneTwin: Online Digital Network Twin via Neural Radio Radiance Field](https://arxiv.org/abs/2601.03216)
*Yuru Zhang,Ming Zhao,Qiang Liu,Nakjung Choi*

Main category: cs.NI

TL;DR: oneTwin是首个在线数字孪生系统，通过增强模拟器和神经无线电辐射场技术，实时预测物理层指标，显著降低孪生与真实网络之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有数字网络孪生方法（如基于模拟器和基于神经网络的方法）在保真度、同步性和可处理性方面存在不足，无法有效实现实时数字网络孪生。

Method: 系统包含两个主要组件：1）增强模拟器，通过材料调优算法逐步优化建筑材料以最小化孪生与真实差距；2）神经无线电辐射场，通过基于在线和模拟数据的神经网络学习算法持续更新DNN。

Result: oneTwin实现实时更新（0.98秒），在分布内和分布外测试数据集上分别将孪生与真实差距降低了36.39%和57.50%，优于现有解决方案。

Conclusion: oneTwin是首个在线数字孪生系统，通过结合增强模拟器和神经学习方法，有效解决了数字网络孪生在保真度、同步性和可处理性方面的挑战。

Abstract: Digital network twin is a promising technology that replicates real-world networks in real-time and assists with the design, operation, and management of next-generation networks. However, existing approaches (e.g., simulator-based and neural-based) cannot effectively realize the digital network twin, in terms of fidelity, synchronicity, and tractability. In this paper, we propose oneTwin, the first online digital twin system, for the prediction of physical layer metrics. We architect the oneTwin system with two primary components: an enhanced simulator and a neural radio radiance field (NRRF). On the one hand, we achieve the enhanced simulator by designing a material tuning algorithm that incrementally optimizes the building materials to minimize the twin-to-real gap. On the other hand, we achieve the NRRF by designing a neural learning algorithm that continually updates its DNNs based on both online and simulated data from the enhanced simulator. We implement oneTwin system using Sionna RT as the simulator and developing new DNNs as the NRRF, under a public cellular network. Extensive experimental results show that, compared to state-of-the-art solutions, oneTwin achieves real-time updating (0.98s), with 36.39% and 57.50% reductions of twin-to-real gap under in-distribution and out-of-distribution test datasets, respectively.

</details>


### [18] [inRAN: Interpretable Online Bayesian Learning for Network Automation in Open Radio Access Networks](https://arxiv.org/abs/2601.03219)
*Ming Zhao,Yuru Zhang,Qiang Liu,Ahan Kak,Nakjung Choi*

Main category: cs.NI

TL;DR: inRAN：一种用于Open RAN网络自动化的可解释在线贝叶斯学习框架，通过集成可解释的替代模型和安全优化求解器，在保证约束满足的同时适应非平稳网络动态。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度神经网络的AI/ML网络控制方法缺乏可解释性、可解释性和透明度，这在实际网络部署中造成重大障碍。需要开发既高效又可解释的网络自动化解决方案。

Method: 提出inRAN框架，包含三个关键组件：1) 通过集成Kolmogorov-Arnold Networks (KANs)构建可解释替代模型；2) 集成遗传搜索和信任域下降法的安全优化求解器；3) 通过持续模型学习和自适应阈值偏移的在线动态跟踪器。

Result: 在O-RAN兼容网络测试床中实现inRAN，通过空中实验验证网络切片用例。结果显示，inRAN显著优于现有方法，在不可预见的时间演化网络动态下，保证基于机会的约束满足率达到92.67%，同时保持可比较的资源使用。

Conclusion: inRAN为Open RAN网络自动化提供了一种可解释、安全且高效的解决方案，解决了现有黑盒方法在实际部署中的障碍，在保证约束满足的同时适应非平稳网络动态。

Abstract: Emerging AI/ML techniques have been showing great potential in automating network control in open radio access networks (Open RAN). However, existing approaches heavily rely on blackbox policies parameterized by deep neural networks, which inherently lack interpretability, explainability, and transparency, and create substantial obstacles in practical network deployment. In this paper, we propose inRAN, a novel interpretable online Bayesian learning framework for network automation in Open RAN. The core idea is to integrate interpretable surrogate models and safe optimization solvers to continually optimize control actions, while adapting to non-stationary dynamics in real-world networks. We achieve the inRAN framework with three key components: 1) an interpretable surrogate model via ensembling Kolmogorov-Arnold Networks (KANs); 2) safe optimization solvers via integrating genetic search and trust-region descent method; 3) an online dynamics tracker via continual model learning and adaptive threshold offset. We implement inRAN in an end-to-end O-RAN-compliant network testbed, and conduct extensive over-the-air experiments with the focused use case of network slicing. The results show that, inRAN substantially outperforms state-of-the-art works, by guaranteeing the chance-based constraint with a 92.67% assurance ratio with comparative resource usage throughout the online network control, under unforeseeable time-evolving network dynamics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [Textual Explanations and Their Evaluations for Reinforcement Learning Policy](https://arxiv.org/abs/2601.02514)
*Ahmad Terra,Mohit Ahmed,Rafia Inam,Elena Fersman,Martin Törngren*

Main category: cs.AI

TL;DR: 提出一個XRL框架，使用LLM生成文本解釋並轉換為透明規則，透過聚類技術識別頻繁條件，並使用兩種精煉技術提升解釋品質和減少衝突。


<details>
  <summary>Details</summary>
Motivation: 現有的可解釋強化學習(XRL)技術中，文本解釋雖然易於人類理解，但確保其正確性仍是挑戰，且現有評估方法有限。需要一個能生成正確文本解釋並進行系統評估的框架。

Method: 使用大型語言模型(LLM)生成文本解釋，透過聚類技術識別頻繁條件，將這些條件轉換為透明規則。提出兩種精煉技術改善解釋品質和減少衝突，並包含專家知識整合和自動謂詞生成器。

Result: 在三個開源環境和一個電信用例中進行實驗，框架能解決現有方法(自主策略解釋)的限制，生成的透明規則在某些任務上能達到滿意性能，並實現對文本解釋的系統性量化評估。

Conclusion: 該XRL框架為強化學習策略提供系統化的文本解釋生成和評估方法，能生成透明規則並改善解釋品質，為XRL領域提供有價值的見解和工業應用潛力。

Abstract: Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.

</details>


### [20] [SimpleMem: Efficient Lifelong Memory for LLM Agents](https://arxiv.org/abs/2601.02553)
*Jiaqi Liu,Yaofeng Su,Peng Xia,Siwei Han,Zeyu Zheng,Cihang Xie,Mingyu Ding,Huaxiu Yao*

Main category: cs.AI

TL;DR: SimpleMem是一个基于语义无损压缩的高效记忆框架，通过三阶段流水线（语义结构化压缩、递归记忆巩固、自适应查询感知检索）来管理LLM代理的历史经验，在保持性能的同时大幅降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理记忆系统存在两个问题：要么通过被动上下文扩展保留完整交互历史导致大量冗余，要么依赖迭代推理过滤噪声带来高token成本。需要一种既能高效管理历史经验又能减少冗余和成本的记忆框架。

Method: 提出三阶段流水线：1) 语义结构化压缩：应用熵感知过滤将非结构化交互蒸馏为紧凑的多视图索引记忆单元；2) 递归记忆巩固：异步过程将相关单元整合为更高级的抽象表示以减少冗余；3) 自适应查询感知检索：根据查询复杂度动态调整检索范围，高效构建精确上下文。

Result: 在基准数据集上的实验表明，该方法在准确性、检索效率和推理成本方面均优于基线方法，平均F1提升26.4%，推理时token消耗减少高达30倍，实现了性能与效率的优越平衡。

Conclusion: SimpleMem通过语义无损压缩和智能记忆管理，为LLM代理在复杂环境中实现可靠长期交互提供了高效解决方案，在保持高性能的同时显著降低了计算成本。

Abstract: To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.

</details>


### [21] [Orchestral AI: A Framework for Agent Orchestration](https://arxiv.org/abs/2601.02577)
*Alexander Roman,Jacob Roman*

Main category: cs.AI

TL;DR: Orchestral是一个轻量级Python框架，提供统一的类型安全接口，用于构建跨多个LLM提供商的AI代理，解决了供应商锁定和API碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理框架面临两大问题：一是供应商锁定（使用特定提供商的SDK），二是复杂的多包生态系统导致控制流不透明、可复现性差。跨提供商集成工具调用存在API碎片化、消息格式不兼容、流式处理和工具调用行为不一致等核心工程挑战，难以构建可移植、可靠的代理系统。

Method: Orchestral采用统一的消息、工具和LLM使用表示，跨提供商无缝操作，消除手动格式转换。通过Python类型提示自动生成工具模式，保持类型安全。采用支持流式的同步执行模型，实现确定性行为。模块化架构清晰分离提供商集成、工具执行、对话编排和用户界面。

Result: 框架提供轻量级解决方案，支持高级代理功能，包括丰富的工具调用、上下文压缩、工作区沙盒、用户审批流程、子代理、内存管理和MCP集成，同时保持科学计算和生产部署所需的简单性。

Conclusion: Orchestral解决了LLM代理开发中的供应商锁定和复杂性挑战，通过统一接口和类型安全设计，使开发者能够构建可移植、可靠的代理系统，同时保持简单性和可扩展性。

Abstract: The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration.

</details>


### [22] [An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices](https://arxiv.org/abs/2601.02641)
*Jeiyoon Park,Daehwan Lee,Changmin Yeo,Yongshin Han,Minseop Kim*

Main category: cs.AI

TL;DR: 该论文研究了设备端AI模型的实际部署问题，包括CPU利用率和热条件，通过构建LiveChatBench基准测试，在5台移动设备上验证了设备端模型在直播聊天翻译任务上能达到与GPT-5.1相当的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管设备端AI模型效率高，但缺乏对实际部署方面的研究，如设备CPU利用率和热条件。需要解决设备端模型选择和资源消耗，以及领域适应能力等问题，以支持真实世界服务部署。

Method: 构建LiveChatBench基准测试（包含1000个韩英平行句对），在5台移动设备上进行广泛实验，研究设备端模型选择和资源消耗，以及领域适应能力。

Result: 实验表明，虽然服务大规模异构用户需要考虑高度受限的部署设置和模型选择，但所提出的方法在针对性任务上能达到与GPT-5.1等商业模型相当的性能。

Conclusion: 设备端AI模型在实际部署中需要仔细考虑资源约束和模型选择，但通过适当的优化，在特定任务上能达到与云端商业模型相当的性能，为设备端AI社区提供有价值的见解。

Abstract: Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community.

</details>


### [23] [AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents](https://arxiv.org/abs/2601.02643)
*Mehmet Kurmaz*

Main category: cs.AI

TL;DR: 论文提出将不可行查询处理视为偏好感知查询修复问题，通过LLM推断约束重要性，并引入AWARE-US基准测试


<details>
  <summary>Details</summary>
Motivation: 现有工具调用对话代理在处理结构化数据库查询时面临两个关联问题：欠规范（缺少精确查询所需约束）和不可行性（完全指定的查询返回空集）。现有方法要么返回"无结果"，要么使用临时规则放松约束，这可能违反用户意图，丢弃用户最关心的需求。

Method: 提出三种基于LLM的方法从对话中推断相对约束重要性：1) 局部加权，2) 全局一次性加权，3) 成对排序。将不可行性处理框架化为偏好感知查询修复问题：当查询不可满足时，代理应放松对用户最不重要的约束。

Result: 实验表明局部加权在偏好对齐方面表现最佳，而全局加权在正确约束放松方面表现最好。局部加权实现了最佳的用户偏好对齐。

Conclusion: 论文提出了偏好感知查询修复框架，通过LLM方法有效处理不可行查询，并引入AWARE-US基准测试来评估代理在基于角色的查询中的表现，要求代理通过对话澄清请求并以符合角色隐含偏好的方式解决不可行性。

Abstract: Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with "no results" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences.

</details>


### [24] [Inferring Causal Graph Temporal Logic Formulas to Expedite Reinforcement Learning in Temporally Extended Tasks](https://arxiv.org/abs/2601.02666)
*Hadi Partovi Aria,Zhe Xu*

Main category: cs.AI

TL;DR: GTL-CIRL：一个同时学习策略和挖掘因果图时序逻辑规范的闭环框架，通过基于高斯过程的贝叶斯优化提升样本效率和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统黑盒强化学习在具有时空动态的图结构决策任务中，往往忽视局部变化如何通过网络结构传播，这限制了样本效率和可解释性

Method: 提出GTL-CIRL框架：1）使用鲁棒性塑造奖励；2）当效应失败时收集反例；3）使用高斯过程驱动的贝叶斯优化来精化参数化的因果模板；4）GP模型捕获系统动态中的空间和时间相关性

Result: 在基因网络和电力网络的案例研究中，相比标准RL基线，该方法展现出更快的学习速度和更清晰、可验证的行为

Conclusion: GTL-CIRL框架能够同时学习策略和挖掘因果图时序逻辑规范，通过利用空间和时间相关性实现高效探索，在复杂决策任务中提供更好的样本效率和可解释性

Abstract: Decision-making tasks often unfold on graphs with spatial-temporal dynamics. Black-box reinforcement learning often overlooks how local changes spread through network structure, limiting sample efficiency and interpretability. We present GTL-CIRL, a closed-loop framework that simultaneously learns policies and mines Causal Graph Temporal Logic (Causal GTL) specifications. The method shapes rewards with robustness, collects counterexamples when effects fail, and uses Gaussian Process (GP) driven Bayesian optimization to refine parameterized cause templates. The GP models capture spatial and temporal correlations in the system dynamics, enabling efficient exploration of complex parameter spaces. Case studies in gene and power networks show faster learning and clearer, verifiable behavior compared to standard RL baselines.

</details>


### [25] [Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization](https://arxiv.org/abs/2601.02683)
*Dongyu Chen,Jian Ma,Xianpeng Zhang,Lei Zhang,Haonan Lu,Chen Chen,Chuangchuang Wang,Kai Tang*

Main category: cs.AI

TL;DR: HAPO框架通过动态归因机制、语义单元优化和多模态友好流程，解决提示优化中的提示漂移和可解释性问题，在图像QA和复杂任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前提示优化方法存在两个主要问题：1) 提示漂移 - 新提示修复先前失败但损害先前成功任务的性能；2) 从头生成提示会损害可解释性。需要一种既能自动优化又能保持可解释性的方法。

Method: 提出分层归因提示优化(HAPO)框架，包含三个创新：1) 动态归因机制，针对训练数据和提示历史中的错误模式；2) 语义单元优化，编辑功能性提示片段；3) 多模态友好流程，支持端到端LLM和LLM-MLLM工作流。

Result: 在单/多图像QA(如OCRV2)和复杂任务分析(如BBH)等场景中，HAPO展示了增强的优化效率，优于可比较的自动提示优化方法。

Conclusion: HAPO建立了一个可扩展的提示工程范式，解决了提示漂移和可解释性问题，为大规模提示优化提供了有效框架。

Abstract: Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.

</details>


### [26] [Learning User Preferences Through Interaction for Long-Term Collaboration](https://arxiv.org/abs/2601.02702)
*Shuhaib Mehri,Priyanka Kargupta,Tal August,Dilek Hakkani-Tür*

Main category: cs.AI

TL;DR: MultiSessionCollab基准测试评估智能体在多轮会话中学习用户偏好并提升协作质量的能力，提出具有持久记忆的长期协作智能体，通过用户模拟器行为训练智能体生成更全面的反思并更有效地更新记忆。


<details>
  <summary>Details</summary>
Motivation: 随着对话智能体与用户协作经验的积累，适应用户偏好对于建立长期关系和提升协作质量至关重要。当前需要评估智能体在多轮会话中学习用户偏好并利用这些偏好改善协作的能力。

Method: 引入MultiSessionCollab基准测试，开发具有持久记忆的长期协作智能体，该记忆随交互经验积累而持续更新和优化。利用MultiSessionCollab中的用户模拟器行为作为学习信号，训练智能体生成更全面的反思并更有效地更新记忆。

Result: 实验表明，配备记忆的智能体显著改善了长期协作，实现了更高的任务成功率、更高效的交互和更少的用户努力。人类用户研究进一步证实记忆在实际场景中提升了用户体验。

Conclusion: 持久记忆对于智能体在多轮会话中学习用户偏好和提升协作质量至关重要。MultiSessionCollab基准测试为评估长期协作能力提供了有效框架，基于用户模拟器行为的训练方法能有效提升智能体的反思和记忆更新能力。

Abstract: As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.

</details>


### [27] [Time-Scaling Is What Agents Need Now](https://arxiv.org/abs/2601.02714)
*Zhi Liu,Guangzhi Wang*

Main category: cs.AI

TL;DR: 该论文提出"时间缩放"概念，旨在通过扩展和优化智能体在时间维度上的推理能力，实现更深层次的问题空间探索和动态策略调整，而不需要成比例增加静态模型参数。


<details>
  <summary>Details</summary>
Motivation: 早期人工智能范式存在认知功能分离问题，而当前大型语言模型虽然能生成流畅文本，但缺乏稳健的语义推理能力。现有的提示技术如思维链和思维树在搜索完整性和效率方面存在局限，需要系统性地扩展智能体在时间维度上的推理能力。

Method: 提出"时间缩放"架构设计，利用扩展的时间路径，实现更深层次的问题空间探索、动态策略调整和增强的元认知控制。这种方法强调显式的时间推理管理，使智能体能够展开更长的推理轨迹。

Result: 论文认为时间缩放是增强深度推理和问题解决能力的关键前沿，能够在不增加静态模型参数的情况下，通过优化时间维度的推理能力来提升智能体性能。

Conclusion: 推进智能体能力需要将时间缩放原则置于前沿，将显式时间推理管理作为基础，这是实现更强大认知代理的关键方向。

Abstract: Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on "perception-representation," Reinforcement Learning on "decision-making-behavior," and Symbolic AI on "knowledge-reasoning." With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop "perception-decision-action" capabilities.
  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.
  This highlights the need for "Time-Scaling"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.

</details>


### [28] [The Path Ahead for Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2601.02749)
*Nadia Sibai,Yara Ahmed,Serry Sibaee,Sawsan AlHalawani,Adel Ammar,Wadii Boulila*

Main category: cs.AI

TL;DR: 该章节探讨了LLM从被动文本生成器向自主智能体系统的演进，分析了实现自主行为所需的核心组件（感知、记忆、规划、工具执行），并评估了应用挑战和研究重点。


<details>
  <summary>Details</summary>
Motivation: 研究LLM从语言理解系统向自主行动系统的架构转变，识别实现真正自主智能体所需解决的技术差距，为负责任地开发和应用自主AI系统提供框架。

Method: 通过分析LLM架构的演进历程，从统计模型到基于Transformer的系统，识别实现自主行为的关键能力（长程推理、上下文感知、自适应决策），并构建集成框架。

Result: 提出了三个主要贡献：1）LLM能力通过推理-行动-反思循环向自主性扩展的合成；2）连接LLM与自主行为的核心组件集成框架；3）应用评估和持续性挑战分析。

Conclusion: 负责任地推进自主AI系统需要同时解决技术稳健性、可解释性和伦理保障问题，重点关注可验证规划、可扩展多智能体协调、持久记忆架构和治理框架等研究重点。

Abstract: The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.

</details>


### [29] [LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery](https://arxiv.org/abs/2601.02757)
*Zixuan Xiao,Jun Ma*

Main category: cs.AI

TL;DR: ChangeGPT：一个结合大语言模型与视觉基础模型的通用代理框架，用于遥感变化检测，通过分层结构减少幻觉，在多样化查询中表现出色


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法缺乏处理多样化真实世界查询的通用性和进行综合分析的能力，需要更智能、适应性更强的解决方案

Method: 提出ChangeGPT框架，集成大语言模型（LLM）与视觉基础模型，采用分层结构来减轻幻觉问题，构建包含140个问题的数据集进行评估

Result: ChangeGPT（特别是使用GPT-4-turbo后端）在工具选择能力和整体查询准确率上表现优异，达到90.71%的匹配率，在处理需要多步推理的变化查询方面特别强大

Conclusion: ChangeGPT通过提供智能性、适应性和多类型变化分析能力，为遥感应用中的决策制定提供了强大的解决方案，并在深圳前海湾的实际城市变化监测案例中验证了其实用性

Abstract: Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.

</details>


### [30] [HAL: Inducing Human-likeness in LLMs with Alignment](https://arxiv.org/abs/2601.02813)
*Masum Hasan,Junjie Zhao,Ehsan Hoque*

Main category: cs.AI

TL;DR: HAL框架通过可解释的数据驱动奖励，将语言模型对齐到对话人类相似性，使用对比对话数据提取明确对话特征，组合成标量分数作为对齐信号。


<details>
  <summary>Details</summary>
Motivation: 对话人类相似性在人机交互中至关重要，但难以定义、测量和优化，现有改进主要依赖规模或广泛监督训练，而非针对性对齐。

Method: 从对比对话数据中提取明确的对话特征，组合成紧凑的标量分数作为透明奖励信号，使用标准偏好优化方法进行对齐。

Result: 对齐后的模型在人类评估中更频繁被视为人类相似对话，且不影响整体性能；HAL支持对齐行为检查和意外效应诊断。

Conclusion: HAL展示了如何将语言中软性、定性属性（先前超出对齐范围）变得可测量、可对齐，并以可解释和可解释的方式实现。

Abstract: Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way.

</details>


### [31] [Causal-Enhanced AI Agents for Medical Research Screening](https://arxiv.org/abs/2601.02814)
*Duc Ngo,Arya Rahgoza*

Main category: cs.AI

TL;DR: 提出因果图增强的检索增强生成系统，在系统综述任务中实现95%准确率、100%检索成功率和零幻觉，显著优于基线AI


<details>
  <summary>Details</summary>
Motivation: 系统综述对循证医学至关重要，但每年150万+出版物无法人工处理。现有AI方法存在幻觉问题（2-15%），这在影响患者护理时不可接受

Method: 因果图增强的检索增强生成系统，整合显式因果推理与双层知识图谱，采用证据优先协议，每个因果声明都追溯到检索文献，自动生成干预-结果路径的有向无环图

Result: 在234个痴呆运动摘要评估中，CausalAgent达到95%准确率、100%检索成功率和零幻觉，而基线AI仅34%准确率和10%幻觉。自动因果图支持显式机制建模、可视化合成和增强可解释性

Conclusion: 虽然概念验证评估仅针对痴呆运动研究的十个问题，但该架构方法展示了可信医疗AI和因果推理在高风险医疗中的可转移原则和潜力

Abstract: Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.
  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.
  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.

</details>


### [32] [Quantum-enhanced long short-term memory with attention for spatial permeability prediction in oilfield reservoirs](https://arxiv.org/abs/2601.02818)
*Muzhen Zhang,Yujie Cheng,Zhanxiang Lei*

Main category: cs.AI

TL;DR: 提出量子增强的长短期记忆注意力模型（QLSTMA），首次将变分量子电路集成到循环单元中，显著提升储层渗透率预测精度


<details>
  <summary>Details</summary>
Motivation: 储层参数（特别是渗透率）的空间预测对油气勘探开发至关重要，但渗透率变化范围大、变异性高，现有方法难以提供可靠预测

Method: 设计QLSTMA模型，将变分量子电路（VQCs）集成到循环单元中，利用量子纠缠和叠加原理；提出两种量子化结构：共享门（QLSTMA-SG）和独立门（QLSTMA-IG）

Result: 8-qubit QLSTMA-IG模型显著优于传统LSTMA，MAE降低19%，RMSE降低20%，在复杂测井数据区域表现尤为突出

Conclusion: 验证了量子-经典混合神经网络在储层预测中的潜力，增加量子比特数可进一步提升精度，为未来在真实量子硬件上部署此类模型奠定基础

Abstract: Spatial prediction of reservoir parameters, especially permeability, is crucial for oil and gas exploration and development. However, the wide range and high variability of permeability prevent existing methods from providing reliable predictions. For the first time in subsurface spatial prediction, this study presents a quantum-enhanced long short-term memory with attention (QLSTMA) model that incorporates variational quantum circuits (VQCs) into the recurrent cell. Using quantum entanglement and superposition principles, the QLSTMA significantly improves the ability to predict complex geological parameters such as permeability. Two quantization structures, QLSTMA with Shared Gates (QLSTMA-SG) and with Independent Gates (QLSTMA-IG), are designed to investigate and evaluate the effects of quantum structure configurations and the number of qubits on model performance. Experimental results demonstrate that the 8-qubit QLSTMA-IG model significantly outperforms the traditional long short-term memory with attention (LSTMA), reducing Mean Absolute Error (MAE) by 19% and Root Mean Squared Error (RMSE) by 20%, with particularly strong performance in regions featuring complex well-logging data. These findings validate the potential of quantum-classical hybrid neural networks for reservoir prediction, indicating that increasing the number of qubits yields further accuracy gains despite the reliance on classical simulations. This study establishes a foundational framework for the eventual deployment of such models on real quantum hardware and their extension to broader applications in petroleum engineering and geoscience.

</details>


### [33] [Sample-Efficient Neurosymbolic Deep Reinforcement Learning](https://arxiv.org/abs/2601.02850)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 提出一种神经符号深度强化学习方法，通过整合符号知识提高样本效率和泛化能力，将简单任务的部分策略作为先验知识加速复杂任务学习。


<details>
  <summary>Details</summary>
Motivation: 当前深度强化学习算法需要大量训练数据，且难以泛化到小规模训练场景之外。需要提高样本效率和泛化能力，特别是在稀疏奖励环境和长规划任务中。

Method: 提出神经符号DRL方法，将部分策略表示为逻辑规则，通过在线推理指导训练过程：1）在探索阶段偏置动作分布；2）在利用阶段重新缩放Q值。将简单任务的部分策略作为先验知识迁移到复杂任务。

Result: 在具有挑战性的gridworld环境变体（完全可观察和部分可观察设置）上验证方法，相比最先进的奖励机制基线表现出更好的性能，加速收敛并提高可解释性和可信度。

Conclusion: 神经符号DRL方法通过整合符号知识有效提高了样本效率和泛化能力，特别是在稀疏奖励和长规划任务中，同时增强了可解释性和可信度。

Abstract: Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.

</details>


### [34] [M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?](https://arxiv.org/abs/2601.02854)
*Ao Li,Jinghui Zhang,Luyu Li,Yuxiang Duan,Lang Gao,Mingcai Chen,Weijun Qin,Shaopeng Li,Fengxian Ji,Ning Liu,Lizhen Cui,Xiuying Chen,Yuntao Du*

Main category: cs.AI

TL;DR: M3MAD-Bench是一个统一可扩展的多智能体辩论基准，涵盖多领域任务、多模态输入和多维度指标，解决了现有评估方法碎片化和单模态限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论研究存在两个根本性局限：评估在碎片化且不一致的设置下进行，阻碍了公平比较；主要局限于依赖纯文本输入的单模态场景。

Method: 建立M3MAD-Bench基准，在五个核心任务领域（知识、数学、医学、自然科学、复杂推理）建立标准化协议，系统覆盖纯文本和视觉语言数据集，支持跨模态比较。评估了9个不同架构、规模和模态能力的基础模型。

Result: 广泛实验提供了关于多智能体辩论在纯文本和多模态场景下有效性、鲁棒性和效率的系统性见解。基准不仅包含准确性指标，还整合了面向效率的指标（如token消耗和推理时间），提供了性能-成本权衡的整体视图。

Conclusion: M3MAD-Bench为未来标准化多智能体辩论评估研究提供了可靠基础，促进了多模态多智能体辩论系统的公平比较和深入分析。

Abstract: As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench.

</details>


### [35] [SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection](https://arxiv.org/abs/2601.02871)
*Zhiyong Cao,Dunqiang Liu,Qi Dai,Haojun Xu,Huaiyan Xu,Huan He,Yafei Liu,Siyuan Liu,XiaoLin Lin,Ke Ma,Ruqian Shi,Sijia Yao,Hao Wang,Sicheng Zhou*

Main category: cs.AI

TL;DR: SimRPD：一个三阶段框架，通过用户模拟器生成大规模对话数据，结合多维度评估筛选高质量数据，用于训练招聘场景的主动对话代理


<details>
  <summary>Details</summary>
Motivation: 招聘场景中任务导向的主动对话代理需要高质量领域特定训练数据，但现实中这类数据稀缺，限制了监督微调和强化学习的效果

Method: 1. 开发高保真用户模拟器生成大规模多轮在线对话数据；2. 基于意图链(CoI)的多维度评估框架，结合全局和实例级指标筛选高质量数据；3. 在筛选后的数据集上训练招聘主动对话代理

Result: 在真实招聘场景实验中，SimRPD优于现有的基于模拟器的数据选择策略，展示了工业部署的实用价值和其他业务导向对话场景的潜在适用性

Conclusion: SimRPD框架有效解决了招聘主动对话代理训练数据稀缺问题，通过模拟生成和智能筛选机制提升了代理性能，具有实际应用价值

Abstract: Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios.

</details>


### [36] [ReTreVal: Reasoning Tree with Validation -- A Hybrid Framework for Enhanced LLM Multi-Step Reasoning](https://arxiv.org/abs/2601.02880)
*Abhishek HS,Pavan C Shekar,Arpit Jain,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: ReTreVal是一个结合树状思维探索、自我精炼、LLM批判评分和反思记忆的混合框架，用于解决LLM多步推理挑战，在数学和创意写作任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在多步推理（如数学和创意写作）方面仍有挑战。现有方法如ReAct、Reflexion和Self-Refine虽然通过迭代精炼和反思改进推理，但缺乏对替代解决方案路径的结构化探索和跨问题的持续学习能力。

Method: 提出ReTreVal框架：1）构建基于问题复杂度的自适应深度推理树；2）每个节点进行迭代自我批判和精炼，由LLM生成的显式反馈指导；3）双重验证机制评估每个节点的推理质量、连贯性和正确性；4）将成功推理路径和失败模式存储在反思记忆缓冲区中实现跨问题学习；5）基于批判的剪枝保留每层最高分节点，控制计算成本。

Result: 使用Qwen 2.5 7B作为基础LLM，在500个数学问题和创意写作任务上评估ReTreVal，结果显示ReTreVal在结构化探索、批判驱动精炼和跨问题记忆的结合下，持续优于ReAct、Reflexion和Self-Refine等方法。

Conclusion: ReTreVal通过结构化探索、批判驱动精炼和跨问题记忆的有效结合，特别适用于需要探索性推理、严格验证和知识迁移的任务，为LLM多步推理提供了有效的解决方案。

Abstract: Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.

</details>


### [37] [Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning](https://arxiv.org/abs/2601.02902)
*Xinglang Zhang,Yunyao Zhang,ZeLiang Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.AI

TL;DR: 本文发现大语言模型在逻辑推理中存在"逻辑相变"现象：推理性能在特定逻辑深度内保持稳定，但超过临界点后会突然崩溃，类似物理相变。作者提出神经符号课程调优框架来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 符号逻辑推理是LLMs的关键但未被充分探索的能力，在数学推理和法律判断等高风险领域提供可靠且可验证的决策。目前缺乏对逻辑推理在复杂度增加时性能变化的系统分析。

Method: 提出神经符号课程调优框架：1) 自适应对齐自然语言与逻辑符号以建立共享表示；2) 围绕相变边界重塑训练动态，逐步增强在增加逻辑深度下的推理能力。

Result: 在五个基准测试上的实验表明，该方法有效缓解了高复杂度下的逻辑推理崩溃，在朴素提示和思维链方法中分别获得平均+1.26和+3.95的准确率提升，并提高了对未见逻辑组合的泛化能力。

Conclusion: 发现了LLMs逻辑推理中的相变现象，并提出神经符号课程调优框架来增强模型在复杂逻辑推理任务中的鲁棒性和泛化能力，为提升LLMs的逻辑推理能力提供了新方向。

Abstract: Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.

</details>


### [38] [Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning](https://arxiv.org/abs/2601.02950)
*Xuan Yang,Furong Jia,Roy Xie,Xiong Xi,Hengwei Bian,Jian Li,Monica Agrawal*

Main category: cs.AI

TL;DR: Batch-of-Thought (BoT) 通过批量处理相关查询实现跨实例学习，利用共享推理模式和一致性约束提升LLM推理性能，结合多智能体反射架构(BoT-R)进一步优化，在多个基准测试中显著提高准确率和置信度校准，同时降低推理成本达61%。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型推理系统独立处理查询，忽略了跨实例信号如共享推理模式和一致性约束，导致信息利用不足和计算效率低下。

Method: 提出Batch-of-Thought (BoT)方法，通过批量处理相关查询进行跨实例学习：1) 通过比较分析识别高质量推理模板；2) 通过一致性检查检测错误；3) 分摊计算成本。进一步在多智能体反射架构(BoT-R)中实现，其中Reflector进行联合评估以获取孤立处理无法获得的互信息增益。

Result: 在三个模型系列和六个基准测试上的实验表明，BoT-R持续提高准确率和置信度校准，同时将推理成本降低高达61%。理论和实验分析揭示了批量感知推理何时以及为何有益于LLM系统。

Conclusion: 批量处理相关查询能够有效利用跨实例信号，通过共享推理模式、一致性检查和计算分摊显著提升LLM推理系统的性能、可靠性和效率，为LLM推理系统设计提供了新方向。

Abstract: Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.

</details>


### [39] [Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2601.02968)
*Qingxiang Liu,Zhiqing Cui,Xiaoliang Luo,Yuqian Wu,Zhuoyang Jiang,Huaiyu Wan,Sheng Sun,Lvchun Wang,Wei Yu,Yuxuan Liang*

Main category: cs.AI

TL;DR: RationaleTS方法通过引入基于原理的上下文学习，将时间序列观察与下游结果连接起来，解决了现有多模态大语言模型在时间序列推理中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在时间序列推理中表现不佳，主要是因为缺乏连接时间观察与下游结果的原理先验，导致模型依赖表面模式匹配而非原则性推理。

Method: 提出RationaleTS方法：1) 诱导标签条件化原理，构建从可观察证据到潜在结果的推理路径；2) 设计混合检索机制，平衡时间模式和语义上下文，为新的样本检索相关原理先验进行上下文推理。

Result: 在三个领域的时间序列推理任务上进行了广泛实验，证明了RationaleTS方法的有效性和效率。

Conclusion: RationaleTS通过引入原理作为指导推理单元而非事后解释，显著提升了时间序列推理性能，并将开源代码供复现使用。

Abstract: The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.

</details>


### [40] [Explainable Fuzzy GNNs for Leak Detection in Water Distribution Networks](https://arxiv.org/abs/2601.03062)
*Qusai Khaled,Pasquale De Marinis,Moez Louati,David Ferras,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 提出一个可解释的图神经网络框架，结合互信息和模糊逻辑，用于水管网泄漏检测与定位，在保持良好性能的同时提供直观的规则解释。


<details>
  <summary>Details</summary>
Motivation: 水管网泄漏检测对资源保护和运营效率至关重要。虽然图神经网络擅长捕捉传感器数据的时空依赖关系，但其黑盒特性以及针对水管网的图解释模型研究不足，阻碍了实际应用。

Method: 提出可解释GNN框架：1) 集成互信息识别关键网络区域；2) 使用模糊逻辑为节点分类任务提供清晰的基于规则的解释；3) 在多种GNN架构中选择了广义图卷积网络(GENConv)，并开发了模糊增强变体(FGENConv)。

Result: FGENConv在检测和定位任务上分别获得0.889和0.814的Graph F1分数，略低于原始GENConv的0.938和0.858，但能够提供空间局部化的模糊规则解释。

Conclusion: 通过在精度和可解释性之间取得平衡，提出的模糊网络能够帮助水利工程师验证预测的泄漏位置、节约人力资源并优化维护策略。

Abstract: Timely leak detection in water distribution networks is critical for conserving resources and maintaining operational efficiency. Although Graph Neural Networks (GNNs) excel at capturing spatial-temporal dependencies in sensor data, their black-box nature and the limited work on graph-based explainable models for water networks hinder practical adoption. We propose an explainable GNN framework that integrates mutual information to identify critical network regions and fuzzy logic to provide clear, rule-based explanations for node classification tasks. After benchmarking several GNN architectures, we selected the generalized graph convolution network (GENConv) for its superior performance and developed a fuzzy-enhanced variant that offers intuitive explanations for classified leak locations. Our fuzzy graph neural network (FGENConv) achieved Graph F1 scores of 0.889 for detection and 0.814 for localization, slightly below the crisp GENConv 0.938 and 0.858, respectively. Yet it compensates by providing spatially localized, fuzzy rule-based explanations. By striking the right balance between precision and explainability, the proposed fuzzy network could enable hydraulic engineers to validate predicted leak locations, conserve human resources, and optimize maintenance strategies. The code is available at github.com/pasqualedem/GNNLeakDetection.

</details>


### [41] [A framework for assuring the accuracy and fidelity of an AI-enabled Digital Twin of en route UK airspace](https://arxiv.org/abs/2601.03120)
*Adam Keane,Nick Pepper,Chris Burr,Amy Hodgkin,Dewi Gould,John Korna,Marc Thomas*

Main category: cs.AI

TL;DR: 本文提出了一个用于航空数字孪生的保证框架，结合了可信和伦理保证方法，为AI空中交通控制代理的训练和测试提供结构化评估方法。


<details>
  <summary>Details</summary>
Motivation: 数字孪生结合仿真、操作数据和AI，在航空业具有巨大潜力，但面临新兴的监管环境。需要为数字孪生的开发和AI/ML在空管中的应用提供保证框架，以满足监管要求并支持利益相关者参与。

Method: 采用可信和伦理保证方法开发保证案例，构建结构化论证框架。该框架定义了可操作的目标和所需证据，以证明数字孪生准确代表其物理对应物并在目标用例中提供足够功能。

Result: 提出了一个全面的保证框架，包含结构化论证和深入分析，详细阐述了特定论证所需的证据、假设和理由。该框架为研究人员提供了评估、理解和记录数字孪生优势和局限性的结构化方法。

Conclusion: 该框架不仅支持数字孪生的技术评估，还为与利益相关者和监管机构的互动奠定了基础，有助于推动未来应用的监管需求讨论，并通过具体工作示例为新兴指导做出贡献。

Abstract: Digital Twins combine simulation, operational data and Artificial Intelligence (AI), and have the potential to bring significant benefits across the aviation industry. Project Bluebird, an industry-academic collaboration, has developed a probabilistic Digital Twin of en route UK airspace as an environment for training and testing AI Air Traffic Control (ATC) agents. There is a developing regulatory landscape for this kind of novel technology. Regulatory requirements are expected to be application specific, and may need to be tailored to each specific use case.
  We draw on emerging guidance for both Digital Twin development and the use of Artificial Intelligence/Machine Learning (AI/ML) in Air Traffic Management (ATM) to present an assurance framework. This framework defines actionable goals and the evidence required to demonstrate that a Digital Twin accurately represents its physical counterpart and also provides sufficient functionality across target use cases. It provides a structured approach for researchers to assess, understand and document the strengths and limitations of the Digital Twin, whilst also identifying areas where fidelity could be improved. Furthermore, it serves as a foundation for engagement with stakeholders and regulators, supporting discussions around the regulatory needs for future applications, and contributing to the emerging guidance through a concrete, working example of a Digital Twin.
  The framework leverages a methodology known as Trustworthy and Ethical Assurance (TEA) to develop an assurance case. An assurance case is a nested set of structured arguments that provides justified evidence for how a top-level goal has been realised. In this paper we provide an overview of each structured argument and a number of deep dives which elaborate in more detail upon particular arguments, including the required evidence, assumptions and justifications.

</details>


### [42] [Automatic Prompt Engineering with No Task Cues and No Tuning](https://arxiv.org/abs/2601.03130)
*Faisal Chowdhury,Nandana Mihindukulasooriya,Niharika S D'Souza,Horst Samulowitz,Neeru Gupta,Tomasz Hanusiak,Michal Kapitonow*

Main category: cs.AI

TL;DR: 提出一种更简单但同样有效的自动提示工程系统，无需调参和任务线索，首次应用于数据库列名扩展任务，并支持英语和德语


<details>
  <summary>Details</summary>
Motivation: 现有自动提示工程方法复杂且需要调参，而数据库列名扩展任务对表格数据搜索、访问和理解至关重要，但现有研究很少

Method: 设计了一个简单且无需调参的自动提示工程系统，不需要任务的具体线索，应用于数据库列名扩展任务

Result: 在英语和德语数据集上评估了该方法，这是首次将自动提示工程应用于列名扩展任务，也是首次在英语以外的语言上应用自动提示工程

Conclusion: 提出的自动提示工程系统设计简单、无需调参，在数据库列名扩展任务上表现有效，并成功扩展到多语言应用

Abstract: This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.

</details>


### [43] [InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents](https://arxiv.org/abs/2601.03204)
*Chenglin Yu,Yuchen Wang,Songmiao Wang,Hongxia Yang,Ming Li*

Main category: cs.AI

TL;DR: InfiAgent框架通过将持久状态外部化到文件中心的状态抽象中，使LLM智能体在长时程任务中保持有界上下文，无需任务特定微调即可与大型专有系统竞争


<details>
  <summary>Details</summary>
Motivation: LLM智能体在长时程任务中常因上下文无限增长和错误累积而失效，现有解决方案（如上下文压缩或检索增强提示）在信息保真度和推理稳定性之间存在权衡

Method: 提出InfiAgent框架，通过文件中心的状态抽象将智能体的持久状态外部化，使推理上下文严格有界；每个步骤中，智能体从工作空间状态快照加上固定窗口的最近动作重建上下文

Result: 在DeepResearch和80篇文献综述任务上的实验表明，无需任务特定微调，InfiAgent使用20B开源模型即可与大型专有系统竞争，并在长时程覆盖度上显著优于基于上下文的基线方法

Conclusion: 显式状态外部化是构建稳定长时程智能体的实用基础，InfiAgent框架为此提供了有效解决方案

Abstract: LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent

</details>


### [44] [MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents](https://arxiv.org/abs/2601.03236)
*Dongming Jiang,Yi Li,Guanpeng Li,Bingzhe Li*

Main category: cs.AI

TL;DR: MAGMA提出多图记忆架构，将记忆项分解到语义、时序、因果和实体四个正交图中，通过策略引导的图遍历实现查询自适应的检索，提升长程推理的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义相似度的记忆增强方法将时间、因果和实体信息混在一起，导致可解释性差、查询意图与检索证据对齐不佳，限制了长上下文推理的准确性。

Method: MAGMA采用多图代理记忆架构，将每个记忆项表示为四个正交图：语义图、时序图、因果图和实体图。检索过程通过策略引导的图遍历实现，根据查询自适应选择相关图并构建结构化上下文。

Result: 在LoCoMo和LongMemEval基准测试中，MAGMA在长程推理任务上持续优于最先进的代理记忆系统。

Conclusion: 通过解耦记忆表示与检索逻辑，MAGMA提供了透明的推理路径和细粒度的检索控制，显著提升了长上下文推理的准确性和可解释性。

Abstract: Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [45] [Breaking Rank -- A Novel Unscented Kalman Filter for Parameter Estimations of a Lumped-Parameter Cardiovascular Model](https://arxiv.org/abs/2601.02390)
*Alex Thornton,Ian Halliday,Harry Saxton,Xu Xu*

Main category: cs.IT

TL;DR: 论文提出改进的无迹卡尔曼滤波器(UKF)，解决了心血管模型参数估计中的秩不足问题，实现了10个参数的高精度估计，准确率超过98%。


<details>
  <summary>Details</summary>
Motivation: 传统UKF在参数估计中存在秩不足问题，通常只能估计一小部分参数，需要固定非关键参数。对于具有10个参数和4个观测值的非线性心血管模型，这是一个临床重要但极具挑战性的问题。

Method: 对标准UKF进行改进，克服参数估计中的秩不足问题。修改后的算法不再受限于传统UKF用于状态跟踪时的约束，能够同时估计所有参数，包括影响较小的参数。

Result: 改进后的UKF在50个10参数样本的挑战性数据集上，能够以超过90%的概率恢复几乎所有参数，准确率超过98%。相比原始UKF有显著提升，且在严重噪声、病理生理条件和无先验参数分布知识下表现出优异鲁棒性。

Conclusion: 改进的UKF算法显著提升了参数估计能力，为非线性心血管模型的临床应用提供了有效的参数识别工具，解决了传统方法中的秩不足限制。

Abstract: We make modifications to the unscented Kalman filter (UKF) which bestow almost complete practical identifiability upon a lumped-parameter cardiovascular model with 10 parameters and 4 output observables - a highly non-linear, stiff problem of clinical significance. The modifications overcome the challenging problems of rank deficiency when applying the UKF to parameter estimation. Rank deficiency usually means only a small subset of parameters can be estimated. Traditionally, pragmatic compromises are made, such as selecting an optimal subset of parameters for estimation and fixing non-influential parameters. Kalman filters are typically used for dynamical state tracking, to facilitate the control u at every time step. However, for the purpose of parameter estimation, this constraint no longer applies. Our modification has transformed the utility of UKF for the parameter estimation purpose, including minimally influential parameters, with excellent robustness (i.e., under severe noise corruption, challenging patho-physiology, and no prior knowledge of parameter distributions). The modified UKF algorithm is robust in recovering almost all parameters to over 98% accuracy, over 90% of the time, with a challenging target data set of 50, 10-parameter samples. We compare this to the original implementation of the UKF algorithm for parameter estimation and demonstrate a significant improvement.

</details>


### [46] [Weights on finite fields and failures of the MacWilliams identities](https://arxiv.org/abs/2601.02608)
*Jay A. Wood*

Main category: cs.IT

TL;DR: 论文探讨了线性码重量枚举器的对偶性质：Hamming重量下对偶码有相同枚举器，但其他重量下可能存在相同枚举器但不同对偶枚举器的情况。


<details>
  <summary>Details</summary>
Motivation: 研究线性码重量枚举器与其对偶码重量枚举器之间的关系，特别是探讨在Hamming重量之外的其他重量度量下，这种对偶关系是否仍然成立。

Method: 通过理论分析和构造反例，研究不同重量度量下线性码重量枚举器与其对偶码重量枚举器的关系。

Result: 发现存在一类重量度量，使得两个线性码可以有相同的重量枚举器，但它们的对偶码却有不同的重量枚举器，这与Hamming重量的情况形成鲜明对比。

Conclusion: Hamming重量的对偶性质是特殊的，对于更广泛的重量度量，线性码与其对偶码的重量枚举器关系可能更加复杂，相同重量枚举器不能保证对偶码有相同枚举器。

Abstract: In the 1960s, MacWilliams proved that the Hamming weight enumerator of a linear code over a finite field completely determines, and is determined by, the Hamming weight enumerator of its dual code. In particular, if two linear codes have the same Hamming weight enumerator, then their dual codes have the same Hamming weight enumerator.
  In contrast, there is a wide class of weights on finite fields whose weight enumerators have the opposite behavior: there exist two linear codes having the same weight enumerator, but their dual codes have different weight enumerators.

</details>


### [47] [State-Dependent Fading Gaussian Channel with Common Reconstruction Constraints](https://arxiv.org/abs/2601.02802)
*Viswanathan Ramachandran*

Main category: cs.IT

TL;DR: 研究高斯衰落信道中同时传输消息和重建信道状态的联合通信问题，在发射机已知非因果状态信息、收发双方已知衰落系数的条件下，完全刻画了最优的率失真折衷区域。


<details>
  <summary>Details</summary>
Motivation: 研究在衰落高斯信道中，如何同时实现消息传输和信道状态重建的联合优化问题。发射机非因果地知道独立同分布的高斯状态干扰，收发双方都完美知道瞬时衰落系数，接收机需要解码消息并重建状态，且重建估计要与发射机保持一致。

Method: 采用信息论方法分析高斯衰落信道模型，考虑发射机非因果状态信息和完美信道状态信息，建立联合通信和状态重建的数学模型，推导最优的率失真折衷区域。

Result: 完全刻画了该场景下的最优率失真折衷区域，这是本文的主要理论贡献。通过数值例子验证了理论结果，展示了率失真和功率失真的折衷关系。

Conclusion: 成功解决了衰落高斯信道中联合消息传输和状态重建的优化问题，给出了完整的率失真折衷区域表征，为相关通信系统的设计提供了理论基础。

Abstract: The task of jointly communicating a message and reconstructing a common estimate of the channel state is examined for a fading Gaussian model with additive state interference. The state is an independent and identically distributed Gaussian sequence known noncausally at the transmitter, and the instantaneous fading coefficient is perfectly known at both the transmitter and the receiver. The receiver is required to decode the transmitted message and, in addition, reconstruct the state under a common reconstruction constraint ensuring that its estimate coincides with that at the transmitter. A complete characterization of the optimal rate distortion tradeoff region for this setting is the main result of our work. The analytical results are also validated through numerical examples illustrating the rate distortion and power distortion tradeoffs.

</details>


### [48] [DeepFP: Deep-Unfolded Fractional Programming for MIMO Beamforming](https://arxiv.org/abs/2601.02822)
*Jianhang Zhu,Tsung-Hui Chang,Liyao Xiang,Kaiming Shen*

Main category: cs.IT

TL;DR: 提出一种混合学习和优化的方法来解决MIMO无线网络中的加权和速率波束成形问题，通过将深度展开网络集成到FastFP算法中进行步长优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法如FP和WMMSE算法计算复杂度高，需要大量矩阵求逆和拉格朗日乘子调优。FastFP方法虽然解决了这些问题，但步长选择困难。因此需要一种更高效的方法。

Method: 将深度展开网络集成到FastFP算法中，利用神经网络优化步长选择，形成混合学习和优化的方法。

Result: 数值实验表明，所提出的方法比基于WMMSE算法的学习方法效率更高。

Conclusion: 通过深度展开网络优化FastFP算法的步长选择，能够有效解决MIMO无线网络中加权和速率波束成形问题，提高计算效率。

Abstract: This work proposes a mixed learning-based and optimization-based approach to the weighted-sum-rates beamforming problem in a multiple-input multiple-output (MIMO) wireless network. The conventional methods, i.e., the fractional programming (FP) method and the weighted minimum mean square error (WMMSE) algorithm, can be computationally demanding for two reasons: (i) they require inverting a sequence of matrices whose sizes are proportional to the number of antennas; (ii) they require tuning a set of Lagrange multipliers to account for the power constraints. The recently proposed method called the reduced WMMSE addresses the above two issues for a single cell. In contrast, for the multicell case, another recent method called the FastFP eliminates the large matrix inversion and the Lagrange multipliers by using an improved FP technique, but the update stepsize in the FastFP can be difficult to decide. As such, we propose integrating the deep unfolding network into the FastFP for the stepsize optimization. Numerical experiments show that the proposed method is much more efficient than the learning method based on the WMMSE algorithm.

</details>


### [49] [Context-aware Privacy Bounds for Linear Queries](https://arxiv.org/abs/2601.02855)
*Heng Zhao,Sara Saeidian,Tobias J. Oechtering*

Main category: cs.IT

TL;DR: 本文通过点态最大泄漏(PML)视角重新分析拉普拉斯机制的隐私保护，发现传统差分隐私(DP)框架因忽略数据生成分布而添加过多噪声，提出利用先验分布知识降低噪声规模的方法。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私(DP)采用与数据生成分布无关的上下文无关定义，这通常导致添加过多噪声。作者希望通过结合先验分布知识，在保持隐私保护的同时减少噪声添加。

Method: 使用点态最大泄漏(PML)框架重新分析拉普拉斯机制，引入关于先验分布的假设（限定单个记录属于特定类别的概率下界），推导出针对一般线性查询的紧致、上下文感知的泄漏边界。

Result: 证明推导的边界严格比标准DP保证更紧致，当概率下界趋近于零时收敛到DP保证。数值评估表明，通过利用先验知识，可以在保持隐私保证的同时显著降低所需噪声规模。

Conclusion: 结合先验分布知识的上下文感知隐私分析能够比传统DP框架更有效地平衡隐私保护和数据效用，为实际应用中的隐私机制设计提供了更优的方案。

Abstract: Linear queries, as the basis of broad analysis tasks, are often released through privacy mechanisms based on differential privacy (DP), the most popular framework for privacy protection. However, DP adopts a context-free definition that operates independently of the data-generating distribution. In this paper, we revisit the privacy analysis of the Laplace mechanism through the lens of pointwise maximal leakage (PML). We demonstrate that the distribution-agnostic definition of the DP framework often mandates excessive noise. To address this, we incorporate an assumption about the prior distribution by lower-bounding the probability of any single record belonging to any specific class. With this assumption, we derive a tight, context-aware leakage bound for general linear queries, and prove that our derived bound is strictly tighter than the standard DP guarantee and converges to the DP guarantee as this probability lower bound approaches zero. Numerical evaluations demonstrate that by exploiting this prior knowledge, the required noise scale can be reduced while maintaining privacy guarantees.

</details>


### [50] [Dualities for finite abelian groups and applications to coding theory](https://arxiv.org/abs/2601.03126)
*Jay A. Wood*

Main category: cs.IT

TL;DR: 研究有限阿贝尔群与其特征群之间的对偶性，用于定义加法码的对偶码，延续Delsarte和Dougherty等人的工作。


<details>
  <summary>Details</summary>
Motivation: 探讨有限阿贝尔群与其特征群之间的同构（对偶性）如何用于定义加法码的对偶码，延续1973年Delsarte的工作以及Dougherty等人近期的研究。

Method: 研究对偶性的性质，分析加法码及其对偶码之间的关系，基于有限阿贝尔群与其特征群之间的同构理论。

Result: 建立了对偶性和对偶码的性质理论，为加法码的对偶性提供了系统的数学框架。

Conclusion: 有限阿贝尔群与其特征群之间的对偶性为定义和研究加法码的对偶码提供了有效的数学工具，延续并扩展了前人的研究成果。

Abstract: The choice of an isomorphism, a duality, between a finite abelian group $A$ and its character group allows one to define dual codes of additive codes over $A$. Properties of dualities and dual codes are studied, continuing work of Delsarte from 1973 and more recent work of Dougherty and his collaborators.

</details>


### [51] [On the Euclidean duals of the cyclic codes generated by cyclotomic polynomials](https://arxiv.org/abs/2601.03165)
*Anuj Kumar Bhagat,Ritumoni Sarma*

Main category: cs.IT

TL;DR: 证明了与q互质的正整数n的循环码C_n的欧几里得对偶码的最小距离为2^ω(n)，其中ω(n)是n的不同素因子的个数


<details>
  <summary>Details</summary>
Motivation: 研究由第n个分圆多项式生成的循环码的欧几里得对偶码的最小距离，验证之前提出的猜想

Method: 使用数论和编码理论的方法，分析由分圆多项式生成的循环码的对偶码结构，证明最小距离与n的不同素因子个数之间的关系

Result: 证明了对于所有与q互质的正整数n，循环码C_n的欧几里得对偶码的最小距离为2^ω(n)，其中ω(n)是n的不同素因子的个数

Conclusion: 成功验证了先前提出的猜想，确定了由分圆多项式生成的循环码的对偶码的最小距离公式，为这类码的纠错能力提供了精确的数学描述

Abstract: In this article, we determine the minimum distance of the Euclidean dual of the cyclic code $\mathcal{C}_n$ generated by the $n$th cyclotomic polynomial $Q_n(x)$ over $\mathbb{F}_q$, for every positive integer $n$ co-prime to $q$. In particular, we prove that the minimum distance of $\mathcal{C}_{n}^{\perp}$ is a function of $n$, namely $2^{ω(n)}$. This was precisely the conjecture posed by us in \cite{BHAGAT2025}.

</details>


### [52] [On the Capacity Region of Individual Key Rates in Vector Linear Secure Aggregation](https://arxiv.org/abs/2601.03241)
*Lei Hu,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文解决了向量线性安全聚合问题中最小个体密钥速率问题，提出了新的可达区域，揭示了并非每个用户都需要持有密钥，并证明了在最小化持有密钥用户数量时的最优性。


<details>
  <summary>Details</summary>
Motivation: 解决Yuan-Sun在ISIT 2025提出的开放问题：确定向量线性安全聚合问题中每个用户所需的最小个体密钥速率。现有研究对密钥分配要求较高，需要探索更高效的密钥分配方案。

Method: 通过分析秩增量条件，提出了一个多面体可达区域。该区域的顶点由二进制速率分配(R₁,...,Rₖ) = (𝟙(1∈ℐ),...,𝟙(K∈ℐ))表征，其中ℐ⊆[K]满足秩增量条件：rank([F_ℐ;G_ℐ]) = rank(F_ℐ)+N。

Result: 1. 揭示了并非每个用户都需要持有密钥的新事实，严格扩大了文献中已知的最佳可达区域；2. 通过反证分析证明了在最小化持有密钥用户数量时的最优性；3. 为向量线性安全聚合问题提供了更高效的密钥分配方案。

Conclusion: 本文解决了Yuan-Sun提出的开放问题，通过秩增量条件表征了新的可达区域，证明了并非所有用户都需要密钥，为分布式安全计算提供了更高效的密钥分配策略，并在最小化密钥持有用户数量方面达到了最优。

Abstract: We provide new insights into an open problem recently posed by Yuan-Sun [ISIT 2025], concerning the minimum individual key rate required in the vector linear secure aggregation problem. Consider a distributed system with $K$ users, where each user $k\in [K]$ holds a data stream $W_k$ and an individual key $Z_k$. A server aims to compute a linear function $\mathbf{F}[W_1;\ldots;W_K]$ without learning any information about another linear function $\mathbf{G}[W_1;\ldots;W_K]$, where $[W_1;\ldots;W_K]$ denotes the row stack of $W_1,\ldots,W_K$. The open problem is to determine the minimum required length of $Z_k$, denoted as $R_k$, $k\in [K]$. In this paper, we characterize a new achievable region for the rate tuple $(R_1,\ldots,R_K)$. The region is polyhedral, with vertices characterized by a binary rate assignment $(R_1,\ldots,R_K) = (\mathbf{1}(1 \in \mathcal{I}),\ldots,\mathbf{1}(K\in \mathcal{I}))$, where $\mathcal{I}\subseteq [K]$ satisfies the \textit{rank-increment condition}: $\mathrm{rank}\left(\bigl[\mathbf{F}_{\mathcal{I}};\mathbf{G}_{\mathcal{I}}\bigr]\right) =\mathrm{rank}\bigl(\mathbf{F}_{\mathcal{I}}\bigr)+N$. Here, $\mathbf{F}_\mathcal{I}$ and $\mathbf{G}_\mathcal{I}$ are the submatrices formed by the columns indexed by $\mathcal{I}$. Our results uncover the novel fact that it is not necessary for every user to hold a key, thereby strictly enlarging the best-known achievable region in the literature. Furthermore, we provide a converse analysis to demonstrate its optimality when minimizing the number of users that hold keys.

</details>
