<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 4]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.IT](#cs.IT) [Total: 2]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in Distributed Tracing for Microservices](https://arxiv.org/abs/2508.11342)
*Linh-An Phan,MingXue Wang,Guangyu Wu,Wang Dawei,Chen Liqun,Li Jin*

Main category: cs.NI

TL;DR: CrossTrace是一种无需修改源代码的分布式追踪解决方案，通过贪婪算法和eBPF技术高效关联跨服务调用，适用于微服务调试。


<details>
  <summary>Details</summary>
Motivation: 分布式追踪在大规模应用中实现困难，现有零代码方案在跨度关联上存在性能、安全或计算开销问题。

Method: 使用贪婪算法推断服务内跨度关系，通过eBPF在TCP包头嵌入跨度标识符实现跨服务关联。

Result: CrossTrace能在数秒内以超过90%的准确率关联数千个跨度。

Conclusion: CrossTrace是一种实用且高效的分布式追踪方案，适合生产环境部署。

Abstract: Distributed tracing has become an essential technique for debugging and
troubleshooting modern microservice-based applications, enabling software
engineers to detect performance bottlenecks, identify failures, and gain
insights into system behavior. However, implementing distributed tracing in
large-scale applications remains challenging due to the need for extensive
instrumentation. To reduce this burden, zero-code instrumentation solutions,
such as those based on eBPF, have emerged, allowing span data to be collected
without modifying application code. Despite this promise, span correlation, the
process of establishing causal relationships between spans, remains a critical
challenge in zero-code approaches. Existing solutions often rely on thread
affinity, compromise system security by requiring the kernel integrity mode to
be disabled, or incur significant computational overhead due to complex
inference algorithms. This paper presents CrossTrace, a practical and efficient
distributed tracing solution designed to support the debugging of microservice
applications without requiring source code modifications. CrossTrace employs a
greedy algorithm to infer intra-service span relationships from delay patterns,
eliminating reliance on thread identifiers. For inter-service correlation,
CrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling
secure and efficient correlation compromising system security policies.
Evaluation results show that CrossTrace can correlate thousands of spans within
seconds with over 90% accuracy, making it suitable for production deployment
and valuable for microservice observability and diagnosis.

</details>


### [2] [Optimizing ROS 2 Communication for Wireless Robotic Systems](https://arxiv.org/abs/2508.11366)
*Sanghoon Lee,Taehun Kim,Jiyeong Chae,Kyung-Joon Park*

Main category: cs.NI

TL;DR: 论文分析了ROS 2中DDS通信栈在无线传输大负载时的性能问题，提出了轻量级优化框架，显著提升了传输效率。


<details>
  <summary>Details</summary>
Motivation: ROS 2中DDS通信栈在无线传输大负载时性能下降，但原因尚未被深入研究。

Method: 通过网络层分析识别问题，提出基于链路和负载特性的轻量级DDS优化框架。

Result: 优化框架在多种无线场景下成功传输大负载，保持低延迟。

Conclusion: 提出的解决方案无需修改协议或额外组件，通过简单配置即可显著提升性能。

Abstract: Wireless transmission of large payloads, such as high-resolution images and
LiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source
robotics middleware. The default Data Distribution Service (DDS) communication
stack in ROS 2 exhibits significant performance degradation over lossy wireless
links. Despite the widespread use of ROS 2, the underlying causes of these
wireless communication challenges remain unexplored. In this paper, we present
the first in-depth network-layer analysis of ROS 2's DDS stack under wireless
conditions with large payloads. We identify the following three key issues:
excessive IP fragmentation, inefficient retransmission timing, and congestive
buffer bursts. To address these issues, we propose a lightweight and fully
compatible DDS optimization framework that tunes communication parameters based
on link and payload characteristics. Our solution can be seamlessly applied
through the standard ROS 2 application interface via simple XML-based QoS
configuration, requiring no protocol modifications, no additional components,
and virtually no integration efforts. Extensive experiments across various
wireless scenarios demonstrate that our framework successfully delivers large
payloads in conditions where existing DDS modes fail, while maintaining low
end-to-end latency.

</details>


### [3] [D2Q Synchronizer: Distributed SDN Synchronization for Time Sensitive Applications](https://arxiv.org/abs/2508.11475)
*Ioannis Panitsas,Akrit Mudvari,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 提出了一种基于强化学习的D2Q Synchronizer算法，用于在分布式SDN中优化网络和用户性能，显著降低网络成本。


<details>
  <summary>Details</summary>
Motivation: 现有分布式SDN控制器同步策略未综合考虑网络和用户性能的联合优化。

Method: 采用强化学习算法D2Q Synchronizer，策略性地将时间敏感任务卸载到成本效益高的边缘服务器。

Result: 相比启发式和其他学习策略，网络成本分别降低至少45%和10%，同时满足所有任务的延迟要求。

Conclusion: D2Q Synchronizer在多域动态SDN网络中表现优越，确保服务质量的同时显著降低成本。

Abstract: In distributed Software-Defined Networking (SDN), distributed SDN controllers
require synchronization to maintain a global network state. Despite the
availability of synchronization policies for distributed SDN architectures,
most policies do not consider joint optimization of network and user
performance. In this work, we propose a reinforcement learning-based algorithm
called D2Q Synchronizer, to minimize long-term network costs by strategically
offloading time-sensitive tasks to cost-effective edge servers while satisfying
the latency requirements for all tasks. Evaluation results demonstrate the
superiority of our synchronizer compared to heuristic and other learning
policies in literature, by reducing network costs by at least 45% and 10%,
respectively, while ensuring the QoS requirements for all user tasks across
dynamic and multi-domain SDN networks.

</details>


### [4] [Intelligent Edge Resource Provisioning for Scalable Digital Twins of Autonomous Vehicles](https://arxiv.org/abs/2508.11574)
*Mohammad Sajid Shahriar,Suresh Subramaniam,Motoharu Matsuura,Hiroshi Hasegawa,Shih-Chun Lin*

Main category: cs.NI

TL;DR: 本文提出了一种结合数字孪生和移动边缘计算的分布式架构，用于智能交通系统，显著提升了资源利用率和同步性能。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在智能交通系统中潜力巨大，但如何高效管理计算资源以保障其持续运行仍是一个挑战。

Method: 开发了一种网络感知的可扩展协作任务分配算法，并在仿真环境中训练自主代理。

Result: 框架将同步错误降至5%，边缘计算资源利用率高达99.5%。

Conclusion: 该架构显著提升了数字孪生操作的鲁棒性和可扩展性。

Abstract: The next generation networks offers significant potential to advance
Intelligent Transportation Systems (ITS), particularly through the integration
of Digital Twins (DTs). However, ensuring the uninterrupted operation of DTs
through efficient computing resource management remains an open challenge. This
paper introduces a distributed computing archi tecture that integrates DTs and
Mobile Edge Computing (MEC) within a software-defined vehicular networking
framework to enable intelligent, low-latency transportation services. A network
aware scalable collaborative task provisioning algorithm is de veloped to train
an autonomous agent, which is evaluated using a realistic connected autonomous
vehicle (CAV) traffic simulation. The proposed framework significantly enhances
the robustness and scalability of DT operations by reducing synchronization
errors to as low as 5% while achieving up to 99.5% utilization of edge
computing resources.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Grounding Rule-Based Argumentation Using Datalog](https://arxiv.org/abs/2508.10976)
*Martin Diller,Sarah Alice Gaggl,Philipp Hanisch,Giuseppina Monterosso,Fritz Rauschenbach*

Main category: cs.AI

TL;DR: 本文提出了一种智能基础化方法，用于处理ASPIC+中的一阶规则，通过Datalog转换和查询优化，避免无效规则的基础化，并验证了其可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有ASPIC+方法仅支持命题规则，而一阶规则的基础化可能导致输入理论规模指数增长，缺乏专门解决方案。

Method: 将一阶ASPIC+实例转换为Datalog程序，利用Datalog引擎获取基础替换，并提出特定简化以避免无效规则的基础化。

Result: 原型实现的实证评估表明该方法具有可扩展性。

Conclusion: 提出的智能基础化方法有效管理基础化规模，同时保持推理正确性。

Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for
AI. Although first-order rules are commonly used in ASPIC+ examples, most
existing approaches to reason over rule-based argumentation only support
propositional rules. To enable reasoning over first-order instances, a
preliminary grounding step is required. As groundings can lead to an
exponential increase in the size of the input theories, intelligent procedures
are needed. However, there is a lack of dedicated solutions for ASPIC+.
Therefore, we propose an intelligent grounding procedure that keeps the size of
the grounding manageable while preserving the correctness of the reasoning
process. To this end, we translate the first-order ASPIC+ instance into a
Datalog program and query a Datalog engine to obtain ground substitutions to
perform the grounding of rules and contraries. Additionally, we propose
simplifications specific to the ASPIC+ formalism to avoid grounding of rules
that have no influence on the reasoning process. Finally, we performed an
empirical evaluation of a prototypical implementation to show scalability.

</details>


### [6] [From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching](https://arxiv.org/abs/2508.11070)
*Zahra Khotanlou,Kate Larson,Amir-Hossein Karimi*

Main category: cs.AI

TL;DR: 论文提出了一种多主体算法追索框架，解决现实世界中多个追索寻求者和提供者之间的互动问题，通过优化社会福祉和资源分配。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单主体和单模型场景，忽视了多主体互动和资源竞争的复杂性，因此需要一种新框架来应对这些挑战。

Method: 采用加权二分图匹配问题建模多对多互动，提出三层优化框架：基础容量匹配、最优容量再分配和成本感知优化。

Result: 实验验证表明，该框架能在系统设置最小修改下实现接近最优的社会福祉。

Conclusion: 该研究将算法追索从个体推荐扩展到系统设计，为社会福祉和个体可操作性提供了可行路径。

Abstract: Decision makers are increasingly relying on machine learning in sensitive
situations. In such settings, algorithmic recourse aims to provide individuals
with actionable and minimally costly steps to reverse unfavorable AI-driven
decisions. While existing research predominantly focuses on single-individual
(i.e., seeker) and single-model (i.e., provider) scenarios, real-world
applications often involve multiple interacting stakeholders. Optimizing
outcomes for seekers under an individual welfare approach overlooks the
inherently multi-agent nature of real-world systems, where individuals interact
and compete for limited resources. To address this, we introduce a novel
framework for multi-agent algorithmic recourse that accounts for multiple
recourse seekers and recourse providers. We model this many-to-many interaction
as a capacitated weighted bipartite matching problem, where matches are guided
by both recourse cost and provider capacity. Edge weights, reflecting recourse
costs, are optimized for social welfare while quantifying the welfare gap
between individual welfare and this collectively feasible outcome. We propose a
three-layer optimization framework: (1) basic capacitated matching, (2) optimal
capacity redistribution to minimize the welfare gap, and (3) cost-aware
optimization balancing welfare maximization with capacity adjustment costs.
Experimental validation on synthetic and real-world datasets demonstrates that
our framework enables the many-to-many algorithmic recourse to achieve
near-optimal welfare with minimum modification in system settings. This work
extends algorithmic recourse from individual recommendations to system-level
design, providing a tractable path toward higher social welfare while
maintaining individual actionability.

</details>


### [7] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: 提出了一种基于数据驱动的逆优化器和PPO框架的自动治疗计划方法，显著提高了头颈部癌症质子PBS治疗计划的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 头颈部癌症质子PBS治疗计划涉及多个冲突目标，传统方法依赖人工调整参数和计算密集的逆优化，耗时且效率低。

Method: 结合基于PPO的虚拟计划器和L2O逆优化器，利用Transformer处理长上下文数据，自动生成高质量治疗计划。

Result: 相比传统方法，L2O逆优化器在效率和效果上分别提升36.41%和22.97%，平均2.55小时内生成优于或媲美人工计划的方案。

Conclusion: 该方法显著提升了治疗计划的自动化水平和质量，适用于多种复杂临床场景。

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [8] [On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation](https://arxiv.org/abs/2508.11182)
*Matti Berthold,Lydia Blümel,Anna Rapberger*

Main category: cs.AI

TL;DR: 本文扩展了基于假设的论证（ABA）中的可接受性概念研究，引入了强和弱可接受性及其语义，并探讨了它们在非平坦ABA中的性质。


<details>
  <summary>Details</summary>
Motivation: 研究ABA中标准可接受性概念的替代方案，即强和弱可接受性，以丰富论证框架的理论基础。

Method: 使用抽象双极集基论证框架（BSAFs）作为形式工具，研究非平坦ABA中的强和弱可接受性及其语义。

Result: 证明了强和弱可接受性在非平坦ABA中保持了模块化性质，但也存在一些与标准可接受性相似的缺陷。

Conclusion: 强和弱可接受性在非平坦ABA中具有理论和实际意义，但需进一步研究以解决其缺陷。

Abstract: In this work, we broaden the investigation of admissibility notions in the
context of assumption-based argumentation (ABA). More specifically, we study
two prominent alternatives to the standard notion of admissibility from
abstract argumentation, namely strong and weak admissibility, and introduce the
respective preferred, complete and grounded semantics for general (sometimes
called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation
frameworks (BSAFs) as formal playground since they concisely capture the
relations between assumptions and are expressive enough to represent general
non-flat ABA frameworks, as recently shown. While weak admissibility has been
recently investigated for a restricted fragment of ABA in which assumptions
cannot be derived (flat ABA), strong admissibility has not been investigated
for ABA so far. We introduce strong admissibility for ABA and investigate
desirable properties. We furthermore extend the recent investigations of weak
admissibility in the flat ABA fragment to the non-flat case. We show that the
central modularization property is maintained under classical, strong, and weak
admissibility. We also show that strong and weakly admissible semantics in
non-flat ABA share some of the shortcomings of standard admissible semantics
and discuss ways to address these.

</details>


### [9] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: 论文提出了一个评估大型推理模型（LRMs）的新数据集，揭示其在处理不完整问题时的不足，并探讨了监督微调的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有评估仅关注定义明确的问题，忽视了智能体应具备主动询问信息的能力，因此需要填补这一评估空白。

Method: 构建包含两类不完整问题的数据集，并系统评估LRMs的表现，分析其行为（如过度思考和幻觉）。

Result: LRMs在处理不完整问题时无法主动询问信息，且表现出过度思考和幻觉行为。

Conclusion: 研究为开发真正智能的LRMs提供了新视角，强调需超越单纯解题能力的提升。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [10] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: SAGE是一种针对动态知识图谱嵌入的尺度感知渐进演化框架，通过动态调整嵌入维度和动态蒸馏机制，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱嵌入方法主要针对静态图谱，无法适应动态更新的需求。SAGE旨在解决动态知识图谱嵌入中更新尺度和知识保留的问题。

Method: SAGE根据更新尺度确定嵌入维度并扩展嵌入空间，采用动态蒸馏机制平衡新旧知识的整合。

Result: 在七个基准测试中，SAGE在MRR、H@1和H@10上分别提升了1.38%、1.25%和1.6%，且在所有快照中表现最优。

Conclusion: SAGE通过自适应嵌入维度和动态蒸馏机制，显著提升了动态知识图谱嵌入的性能，证明了其有效性。

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


### [11] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: CRAFT-GUI提出了一种基于GRPO的课程学习框架，解决了GUI任务中难度差异和奖励信号单一的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在GUI环境中忽视了任务难度差异和奖励信号的单一性，导致学习效率低下。

Method: 采用GRPO框架设计课程学习，结合规则和模型评估的奖励函数。

Result: 在公开和内部基准测试中分别提升了5.6%和10.3%。

Conclusion: 结合RL和课程学习在GUI任务中具有显著优势。

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [12] [AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager](https://arxiv.org/abs/2508.11416)
*Xuhua Zhao,Yuxuan Xie,Caihua Chen,Yuxiang Sun*

Main category: cs.AI

TL;DR: 论文介绍了AIM-Bench基准，用于评估LLM代理在不确定供应链管理中的决策行为，发现其存在类似人类的决策偏见，并探讨了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理在不确定库存决策中的能力和潜在偏见，填补现有研究空白。

Method: 通过AIM-Bench基准进行多样化的库存补充实验，评估LLM代理的决策行为。

Result: 不同LLM表现出类似人类的决策偏见，并提出了缓解策略（认知反思和信息共享）。

Conclusion: 需谨慎考虑LLM在库存决策中的偏见，为开发以人为本的供应链决策支持系统提供方向。

Abstract: Recent advances in mathematical reasoning and the long-term planning
capabilities of large language models (LLMs) have precipitated the development
of agents, which are being increasingly leveraged in business operations
processes. Decision models to optimize inventory levels are one of the core
elements of operations management. However, the capabilities of the LLM agent
in making inventory decisions in uncertain contexts, as well as the
decision-making biases (e.g. framing effect, etc.) of the agent, remain largely
unexplored. This prompts concerns regarding the capacity of LLM agents to
effectively address real-world problems, as well as the potential implications
of biases that may be present. To address this gap, we introduce AIM-Bench, a
novel benchmark designed to assess the decision-making behaviour of LLM agents
in uncertain supply chain management scenarios through a diverse series of
inventory replenishment experiments. Our results reveal that different LLMs
typically exhibit varying degrees of decision bias that are similar to those
observed in human beings. In addition, we explored strategies to mitigate the
pull-to-centre effect and the bullwhip effect, namely cognitive reflection and
implementation of information sharing. These findings underscore the need for
careful consideration of the potential biases in deploying LLMs in Inventory
decision-making scenarios. We hope that these insights will pave the way for
mitigating human decision bias and developing human-centred decision support
systems for supply chains.

</details>


### [13] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: Inclusion Arena是一个实时排行榜，通过从AI应用中收集的人类反馈对模型进行排名，弥补了静态数据集和通用领域提示的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准和排行榜依赖静态数据集或通用领域提示，难以反映模型在真实应用中的性能，因此需要一种更贴近实际使用场景的评估方法。

Method: 平台通过自然用户交互中的成对模型比较收集反馈，并采用改进的Bradley-Terry模型（包括冷启动机制和智能比较策略）进行排名。

Result: Inclusion Arena提供了可靠且稳定的排名，数据传递性更高，并显著降低了恶意操纵的风险。

Conclusion: 该平台通过连接基础模型和实际应用，加速了面向用户部署的LLMs和MLLMs的开发。

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


### [14] [Landmark-Assisted Monte Carlo Planning](https://arxiv.org/abs/2508.11493)
*David H. Chan,Mark Roberts,Dana S. Nau*

Main category: cs.AI

TL;DR: 论文提出了一种概率性地标的概念，并将其应用于UCT算法以分解MDP问题，实验表明地标能显著提升在线概率规划的性能。


<details>
  <summary>Details</summary>
Motivation: 地标在经典规划中已有重要应用，但在随机领域很少使用，研究旨在探索其在概率规划中的潜力。

Method: 形式化概率地标，并改进UCT算法以利用地标作为子目标分解MDP，核心是平衡贪婪地标达成与最终目标达成。

Result: 基准测试显示，合适的地标能显著提升UCT性能，但贪婪与长期目标的平衡因问题而异。

Conclusion: 地标可为解决MDP的随时算法提供有效指导。

Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in
every solution plan$\unicode{x2013}$have contributed to major advancements in
classical planning, but they have seldom been used in stochastic domains. We
formalize probabilistic landmarks and adapt the UCT algorithm to leverage them
as subgoals to decompose MDPs; core to the adaptation is balancing between
greedy landmark achievement and final goal achievement. Our results in
benchmark domains show that well-chosen landmarks can significantly improve the
performance of UCT in online probabilistic planning, while the best balance of
greedy versus long-term goal achievement is problem-dependent. The results
suggest that landmarks can provide helpful guidance for anytime algorithms
solving MDPs.

</details>


### [15] [Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models](https://arxiv.org/abs/2508.11524)
*Wenkai Yu,Jianhang Tang,Yang Zhang,Shanjiang Tang,Kebing Jin,Hankz Hankui Zhuo*

Main category: cs.AI

TL;DR: 论文提出了一种结合LLM和问题分解的新型规划器，通过LLM4Inspire和LLM4Predict两种范式辅助分解大规模规划问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模规划问题中的状态空间爆炸问题，并探索如何结合LLM与领域知识以确保规划有效性。

Method: 提出LLM辅助规划器，先分解问题为子任务，再通过LLM4Inspire（启发式引导）和LLM4Predict（领域知识推断）辅助分解。

Result: 实验验证了规划器在多领域的有效性，LLM4Predict因结合领域知识表现优于LLM4Inspire。

Conclusion: 结合领域知识的LLM（如LLM4Predict）在修剪搜索空间时更具潜力。

Abstract: Addressing large-scale planning problems has become one of the central
challenges in the planning community, deriving from the state-space explosion
caused by growing objects and actions. Recently, researchers have explored the
effectiveness of leveraging Large Language Models (LLMs) to generate helpful
actions and states to prune the search space. However, prior works have largely
overlooked integrating LLMs with domain-specific knowledge to ensure valid
plans. In this paper, we propose a novel LLM-assisted planner integrated with
problem decomposition, which first decomposes large planning problems into
multiple simpler sub-tasks. Then we explore two novel paradigms to utilize
LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where
LLM4Inspire provides heuristic guidance according to general knowledge and
LLM4Predict employs domain-specific knowledge to infer intermediate conditions.
We empirically validate the effectiveness of our planner across multiple
domains, demonstrating the ability of search space partition when solving
large-scale planning problems. The experimental results show that LLMs
effectively locate feasible solutions when pruning the search space, where
infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds
particular promise compared with LLM4Inspire, which offers general knowledge
within LLMs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [16] [CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems](https://arxiv.org/abs/2508.11287)
*Xuran Liu,Nan Xue,Rui Bao,Yaping Sun,Zhiyong Chen,Meixia Tao,Xiaodong Xu,Shuguang Cui*

Main category: cs.IT

TL;DR: 提出了一种延迟感知调度框架，通过重叠模型加载与计算和通信，减少边缘设备上大型语言模型推理的总延迟。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，现有流水线并行方法忽略了按需模型加载导致的冷启动延迟。

Method: 设计了一个动态调整层分区和分配的框架，将问题建模为混合整数非线性规划，并采用动态规划算法优化。

Result: 实验表明，该方法显著降低了冷启动延迟。

Conclusion: 提出的框架有效隐藏了加载时间，减少了空闲期，提升了推理效率。

Abstract: While deploying large language models on edge devices promises low-latency
and privacy-preserving AI services, it is hindered by limited device resources.
Although pipeline parallelism facilitates distributed inference, existing
approaches often ignore the cold-start latency caused by on-demand model
loading. In this paper, we propose a latency-aware scheduling framework that
overlaps model loading with computation and communication to minimize total
inference latency. Based on device and model parameters, the framework
dynamically adjusts layer partitioning and allocation to effectively hide
loading time, thereby eliminating as many idle periods as possible. We
formulate the problem as a Mixed-Integer Non-Linear Program and design an
efficient dynamic programming algorithm to optimize model partitioning and
device assignment. Experimental results show that the proposed method
significantly reduces cold-start latency compared to baseline strategies.

</details>


### [17] [Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks](https://arxiv.org/abs/2508.11291)
*Rui Bao,Nan Xue,Yaping Sun,Zhiyong Chen*

Main category: cs.IT

TL;DR: 论文提出了一种动态、质量-延迟感知的路由框架，用于在移动设备和边缘服务器之间协调推理任务，以平衡推理质量和延迟。


<details>
  <summary>Details</summary>
Motivation: 无线通信与大型语言模型（LLMs）的结合有望实现无处不在的智能服务，但在无线边缘设备协作环境中部署时，推理质量和端到端延迟之间存在关键权衡。

Method: 提出了一种动态路由框架，结合轻量级移动设备模型和强大的边缘服务器模型，针对单轮查询和多轮对话设计了不同的成本模型。

Result: 实验表明，该框架在保持推理质量的同时，平均响应延迟降低了5-15%，并减少了10-20%的大型模型调用。

Conclusion: 该框架有效解决了任务复杂性与资源分配之间的不匹配问题，为无线边缘设备协作环境中的智能服务提供了实用解决方案。

Abstract: The integration of wireless communications and Large Language Models (LLMs)
is poised to unlock ubiquitous intelligent services, yet deploying them in
wireless edge-device collaborative environments presents a critical trade-off
between inference quality and end-to-end latency. A fundamental mismatch exists
between task complexity and resource allocation: offloading simple queries
invites prohibitive latency, while on-device models lack the capacity for
demanding computations. To address this challenge, we propose a dynamic,
quality-latency aware routing framework that orchestrates inference between a
lightweight model on the mobile device and a powerful model on the edge server.
Our framework employs two distinct cost models: for single-turn queries, it
fuses a BERT-predicted semantic score with communication and computation
overheads; for multi-turn dialogues, it further quantifies context-aware costs
arising from model switching and KV-cache management. While maintaining full
inference quality, extensive experiments demonstrate that our framework cuts
average response latency by 5-15% and reduces large model invocations by 10-20%
against competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.

</details>
