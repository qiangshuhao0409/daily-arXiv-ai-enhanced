{"id": "2510.13031", "categories": ["cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.13031", "abs": "https://arxiv.org/abs/2510.13031", "authors": ["Pragya Sharma", "Shihua Sun", "Shachi Deshpande", "Angelos Stavrou", "Haining Wang"], "title": "Towards xApp Conflict Evaluation with Explainable Machine Learning and Causal Inference in O-RAN", "comment": null, "summary": "The Open Radio Access Network (O-RAN) architecture enables a flexible,\nvendor-neutral deployment of 5G networks by disaggregating base station\ncomponents and supporting third-party xApps for near real-time RAN control.\nHowever, the concurrent operation of multiple xApps can lead to conflicting\ncontrol actions, which may cause network performance degradation. In this work,\nwe propose a framework for xApp conflict management that combines explainable\nmachine learning and causal inference to evaluate the causal relationships\nbetween RAN Control Parameters (RCPs) and Key Performance Indicators (KPIs). We\nuse model explainability tools such as SHAP to identify RCPs that jointly\naffect the same KPI, signaling potential conflicts, and represent these\ninteractions as a causal Directed Acyclic Graph (DAG). We then estimate the\ncausal impact of each of these RCPs on their associated KPIs using metrics such\nas Average Treatment Effect (ATE) and Conditional Average Treatment Effect\n(CATE). This approach offers network operators guided insights into identifying\nconflicts and quantifying their impacts, enabling more informed and effective\nconflict resolution strategies across diverse xApp deployments."}
{"id": "2510.12896", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.12896", "abs": "https://arxiv.org/abs/2510.12896", "authors": ["Ekram Hossain", "Angelo Vera-Rivera"], "title": "Toward Hyper-Dimensional Connectivity in Beyond 6G: A Conceptual Framework", "comment": null, "summary": "Cellular wireless networks enable mobile broadband connectivity for\nInternet-based applications through their radio access and core network\ninfrastructure. While Fifth-Generation (5G) cellular systems are currently\nbeing deployed, ongoing research on cellular technologies primarily focuses on\nSixth-Generation (6G) networks to set the stage for developing standards for\nthese systems. Therefore, the time has come to articulate the visions for\nbeyond 6G (B6G) systems. In this article, we present a visionary framework\ntoward hyper-dimensional connectivity in B6G that enables wireless access to\nhyper-immersive Internet technologies. Our contributions include a conceptual\nframework for B6G cellular systems with jointly integrated communication,\ncognition, computing, and cyber-physical capabilities as core connectivity\ndimensions, a set of technical definitions outlining potential use cases and\nsystem-level requirements, a mapping of prospective technology enablers, and a\nforward-looking research agenda for B6G systems. The conceptual discussions in\nthis article would be helpful for identifying innovation drivers, shaping\nlong-term technical goals, and defining research agendas for the future of\nmobile broadband technologies."}
{"id": "2510.13171", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13171", "abs": "https://arxiv.org/abs/2510.13171", "authors": ["Jun Qian", "Ross Murch", "Khaled B. Letaief"], "title": "On the performance of Active STAR-RIS-Assisted Cell-Free Massive MIMO Systems with Phase Errors and Channel Aging", "comment": "5 pages, 3 figures, accepted by IEEE WCL", "summary": "Active reconfigurable intelligent surfaces (RISs) employ amplification to\novercome attenuation caused by the RIS cascaded link. In this paper, we analyze\nthe effects of phase errors and channel aging in active simultaneously\ntransmitting and reflecting (STAR) RIS-assisted cell-free massive\nmultiple-input multiple-output (MIMO) systems. By leveraging a spatially\ncorrelated Rayleigh fading model, this paper derives minimum mean square error\nestimate-based channel estimates and formulates closed-form expressions for\ndownlink spectral efficiency. This analytical framework enables a comprehensive\nevaluation of the effects of channel aging and uniformly distributed phase\nerrors on system performance. The results demonstrate that active STAR-RISs can\neffectively compensate for the adverse effects of phase errors and channel\naging. To counteract the impact of channel aging, we propose practical\nguidelines for resource-block-length design. Also, an increase in APs and\nSTAR-RIS elements, along with a larger amplification factor, can alleviate\nperformance degradation."}
{"id": "2510.12864", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12864", "abs": "https://arxiv.org/abs/2510.12864", "authors": ["Imran Khan"], "title": "From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models", "comment": "13 pages. Code and data are available at\n  https://github.com/strongSoda/LITERAL-TO-LIBERAL", "summary": "Large Language Models (LLMs) are increasingly being deployed as the reasoning\nengines for agentic AI systems, yet they exhibit a critical flaw: a rigid\nadherence to explicit rules that leads to decisions misaligned with human\ncommon sense and intent. This \"rule-rigidity\" is a significant barrier to\nbuilding trustworthy autonomous agents. While prior work has shown that\nsupervised fine-tuning (SFT) with human explanations can mitigate this issue,\nSFT is computationally expensive and inaccessible to many practitioners. To\naddress this gap, we introduce the Rule-Intent Distinction (RID) Framework, a\nnovel, low-compute meta-prompting technique designed to elicit human-aligned\nexception handling in LLMs in a zero-shot manner. The RID framework provides\nthe model with a structured cognitive schema for deconstructing tasks,\nclassifying rules, weighing conflicting outcomes, and justifying its final\ndecision. We evaluated the RID framework against baseline and Chain-of-Thought\n(CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced\njudgment across diverse domains. Our human-verified results demonstrate that\nthe RID framework significantly improves performance, achieving a 95% Human\nAlignment Score (HAS), compared to 80% for the baseline and 75% for CoT.\nFurthermore, it consistently produces higher-quality, intent-driven reasoning.\nThis work presents a practical, accessible, and effective method for steering\nLLMs from literal instruction-following to liberal, goal-oriented reasoning,\npaving the way for more reliable and pragmatic AI agents."}
{"id": "2510.13031", "categories": ["cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.13031", "abs": "https://arxiv.org/abs/2510.13031", "authors": ["Pragya Sharma", "Shihua Sun", "Shachi Deshpande", "Angelos Stavrou", "Haining Wang"], "title": "Towards xApp Conflict Evaluation with Explainable Machine Learning and Causal Inference in O-RAN", "comment": null, "summary": "The Open Radio Access Network (O-RAN) architecture enables a flexible,\nvendor-neutral deployment of 5G networks by disaggregating base station\ncomponents and supporting third-party xApps for near real-time RAN control.\nHowever, the concurrent operation of multiple xApps can lead to conflicting\ncontrol actions, which may cause network performance degradation. In this work,\nwe propose a framework for xApp conflict management that combines explainable\nmachine learning and causal inference to evaluate the causal relationships\nbetween RAN Control Parameters (RCPs) and Key Performance Indicators (KPIs). We\nuse model explainability tools such as SHAP to identify RCPs that jointly\naffect the same KPI, signaling potential conflicts, and represent these\ninteractions as a causal Directed Acyclic Graph (DAG). We then estimate the\ncausal impact of each of these RCPs on their associated KPIs using metrics such\nas Average Treatment Effect (ATE) and Conditional Average Treatment Effect\n(CATE). This approach offers network operators guided insights into identifying\nconflicts and quantifying their impacts, enabling more informed and effective\nconflict resolution strategies across diverse xApp deployments."}
{"id": "2510.13180", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13180", "abs": "https://arxiv.org/abs/2510.13180", "authors": ["Qi Qi", "Abdelhamid Tayebi", "Daizhan Cheng", "Jun-e Feng"], "title": "A Dimension-Keeping Semi-Tensor Product Framework for Compressed Sensing", "comment": null, "summary": "In compressed sensing (CS), sparse signals can be reconstructed from\nsignificantly fewer samples than required by the Nyquist-Shannon sampling\ntheorem. While non-sparse signals can be sparsely represented in appropriate\ntransformation domains, conventional CS frameworks rely on the incoherence of\nthe measurement matrix columns to guarantee reconstruction performance. This\npaper proposes a novel method termed Dimension-Keeping Semi-Tensor Product\nCompressed Sensing (DK-STP-CS), which leverages intra-group correlations while\nmaintaining inter-group incoherence to enhance the measurement matrix design.\nSpecifically, the DK-STP algorithm is integrated into the design of the sensing\nmatrix, enabling dimensionality reduction while preserving signal recovery\ncapability. For image compression and reconstruction tasks, the proposed method\nachieves notable noise suppression and improves visual fidelity. Experimental\nresults demonstrate that DK-STP-CS significantly outperforms traditional CS and\nSTP-CS approaches, as evidenced by higher Peak Signal-to-Noise Ratio (PSNR)\nvalues between the reconstructed and original images. The robustness of\nDK-STP-CS is further validated under noisy conditions and varying sampling\nrates, highlighting its potential for practical applications in\nresource-constrained environments."}
{"id": "2510.12979", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12979", "abs": "https://arxiv.org/abs/2510.12979", "authors": ["Wei Fan", "Wenlin Yao", "Zheng Li", "Feng Yao", "Xin Liu", "Liang Qiu", "Qingyu Yin", "Yangqiu Song", "Bing Yin"], "title": "DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping", "comment": "Under Review", "summary": "Large language models (LLMs) augmented with multi-step reasoning and action\ngeneration abilities have shown promise in leveraging external tools to tackle\ncomplex tasks that require long-horizon planning. However, existing approaches\neither rely on implicit planning in the reasoning stage or introduce explicit\nplanners without systematically addressing how to optimize the planning stage.\nAs evidence, we observe that under vanilla reinforcement learning (RL),\nplanning tokens exhibit significantly higher entropy than other action tokens,\nrevealing uncertain decision points that remain under-optimized. To address\nthis, we propose DeepPlanner, an end-to-end RL framework that effectively\nenhances the planning capabilities of deep research agents. Our approach shapes\ntoken-level advantage with an entropy-based term to allocate larger updates to\nhigh entropy tokens, and selectively upweights sample-level advantages for\nplanning-intensive rollouts. Extensive experiments across seven deep research\nbenchmarks demonstrate that DeepPlanner improves planning quality and achieves\nstate-of-the-art results under a substantially lower training budget."}
{"id": "2510.13248", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13248", "abs": "https://arxiv.org/abs/2510.13248", "authors": ["Yunze Wei", "Kaiwen Wei", "Shibo Du", "Jianyu Wang", "Zhangzhong Liu", "Yawen Wang", "Zhanyou Li", "Congcong Miao", "Xiaohui Xie", "Yong Cui"], "title": "Automated Network Protocol Testing with LLM Agents", "comment": null, "summary": "Network protocol testing is fundamental for modern network infrastructure.\nHowever, traditional network protocol testing methods are labor-intensive and\nerror-prone, requiring manual interpretation of specifications, test case\ndesign, and translation into executable artifacts, typically demanding one\nperson-day of effort per test case. Existing model-based approaches provide\npartial automation but still involve substantial manual modeling and expert\nintervention, leading to high costs and limited adaptability to diverse and\nevolving protocols. In this paper, we propose a first-of-its-kind system called\nNeTestLLM that takes advantage of multi-agent Large Language Models (LLMs) for\nend-to-end automated network protocol testing. NeTestLLM employs hierarchical\nprotocol understanding to capture complex specifications, iterative test case\ngeneration to improve coverage, a task-specific workflow for executable\nartifact generation, and runtime feedback analysis for debugging and\nrefinement. NeTestLLM has been deployed in a production environment for several\nmonths, receiving positive feedback from domain experts. In experiments,\nNeTestLLM generated 4,632 test cases for OSPF, RIP, and BGP, covering 41\nhistorical FRRouting bugs compared to 11 by current national standards. The\nprocess of generating executable artifacts also improves testing efficiency by\na factor of 8.65x compared to manual methods. NeTestLLM provides the first\npractical LLM-powered solution for automated end-to-end testing of\nheterogeneous network protocols."}
{"id": "2510.13209", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13209", "abs": "https://arxiv.org/abs/2510.13209", "authors": ["Lipeng Zhu", "Haobin Mao", "Ge Yan", "Wenyan Ma", "Zhenyu Xiao", "Rui Zhang"], "title": "Movable and Reconfigurable Antennas for 6G: Unlocking Electromagnetic-Domain Design and Optimization", "comment": null, "summary": "The growing demands of 6G mobile communication networks necessitate advanced\nantenna technologies. Movable antennas (MAs) and reconfigurable antennas (RAs)\nenable dynamic control over antenna's position, orientation, radiation,\npolarization, and frequency response, introducing rich electromagnetic-domain\ndegrees of freedom for the design and performance enhancement of wireless\nsystems. This article overviews their application scenarios, hardware\narchitectures, and design methods. Field test and simulation results highlight\ntheir performance benefits over conventional fixed/non-reconfigurable antennas."}
{"id": "2510.12985", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12985", "abs": "https://arxiv.org/abs/2510.12985", "authors": ["Simon Sinong Zhan", "Yao Liu", "Philip Wang", "Zinan Wang", "Qineng Wang", "Zhian Ruan", "Xiangyu Shi", "Xinyu Cao", "Frank Yang", "Kangrui Wang", "Huajie Shao", "Manling Li", "Qi Zhu"], "title": "SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents", "comment": null, "summary": "We present Sentinel, the first framework for formally evaluating the physical\nsafety of Large Language Model(LLM-based) embodied agents across the semantic,\nplan, and trajectory levels. Unlike prior methods that rely on heuristic rules\nor subjective LLM judgments, Sentinel grounds practical safety requirements in\nformal temporal logic (TL) semantics that can precisely specify state\ninvariants, temporal dependencies, and timing constraints. It then employs a\nmulti-level verification pipeline where (i) at the semantic level, intuitive\nnatural language safety requirements are formalized into TL formulas and the\nLLM agent's understanding of these requirements is probed for alignment with\nthe TL formulas; (ii) at the plan level, high-level action plans and subgoals\ngenerated by the LLM agent are verified against the TL formulas to detect\nunsafe plans before execution; and (iii) at the trajectory level, multiple\nexecution trajectories are merged into a computation tree and efficiently\nverified against physically-detailed TL specifications for a final safety\ncheck. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate\nmultiple LLM-based embodied agents against diverse safety requirements. Our\nexperiments show that by grounding physical safety in temporal logic and\napplying verification methods across multiple levels, Sentinel provides a\nrigorous foundation for systematically evaluating LLM-based embodied agents in\nphysical environments, exposing safety violations overlooked by previous\nmethods and offering insights into their failure modes."}
{"id": "2510.13467", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.13467", "abs": "https://arxiv.org/abs/2510.13467", "authors": ["Enhan Li", "Hongyang Du", "Kaibin Huang"], "title": "NetMCP: Network-Aware Model Context Protocol Platform for LLM Capability Extension", "comment": null, "summary": "Large Language Models (LLMs) remain static in functionality after training,\nand extending their capabilities requires integration with external data,\ncomputation, and services. The Model Context Protocol (MCP) has emerged as a\nstandard interface for such extensions, but current implementations rely solely\non semantic matching between users' requests and server function descriptions,\nwhich makes current deployments and simulation testbeds fragile under latency\nfluctuations or server failures. We address this gap by enhancing MCP tool\nrouting algorithms with real-time awareness of network and server status. To\nprovide a controlled test environment for development and evaluation, we\nconstruct a heterogeneous experimental platform, namely Network-aware MCP\n(NetMCP), which offers five representative network states and build a benchmark\nfor latency sequence generation and MCP server datasets. On top of NetMCP\nplatform, we analyze latency sequences and propose a Semantic-Oriented and\nNetwork-Aware Routing (SONAR) algorithm, which jointly optimizes semantic\nsimilarity and network Quality of Service (QoS) metrics for adaptive tool\nrouting. Results show that SONAR consistently improves task success rate and\nreduces completion time and failure number compared with semantic-only,\nLLM-based baselines, demonstrating the value of network-aware design for\nproduction-scale LLM systems. The code for NetMCP is available at\nhttps://github.com/NICE-HKU/NetMCP."}
{"id": "2510.13485", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13485", "abs": "https://arxiv.org/abs/2510.13485", "authors": ["Akash Kulkarni", "Rajshekhar V Bhat"], "title": "Non-Linear Precoding via Dirty Paper Coding for Near-Field Downlink MISO Communications", "comment": null, "summary": "In 6G systems, extremely large-scale antenna arrays operating at terahertz\nfrequencies extend the near-field region to typical user distances from the\nbase station, enabling near-field communication (NFC) with fine spatial\nresolution through beamfocusing. Existing multiuser NFC systems predominantly\nemploy linear precoding techniques such as zero-forcing (ZF), which suffer from\nperformance degradation due to the high transmit power required to suppress\ninterference. This paper proposes a nonlinear precoding framework based on\nDirty Paper Coding (DPC), which pre-cancels known interference to maximize the\nsum-rate performance. We formulate and solve the corresponding sum-rate\nmaximization problems, deriving optimal power allocation strategies for both\nDPC and ZF schemes. Extensive simulations demonstrate that DPC achieves\nsubstantial sum-rate gains over ZF across various near-field configurations,\nwith the most pronounced improvements observed for closely spaced users."}
{"id": "2510.13002", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13002", "abs": "https://arxiv.org/abs/2510.13002", "authors": ["Boyou Chen", "Gerui Xu", "Zifei Wang", "Huizhong Guo", "Ananna Ahmed", "Zhaonan Sun", "Zhen Hu", "Kaihan Zhang", "Shan Bao"], "title": "From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model", "comment": null, "summary": "Vehicle crashes involve complex interactions between road users, split-second\ndecisions, and challenging environmental conditions. Among these, two-vehicle\ncrashes are the most prevalent, accounting for approximately 70% of roadway\ncrashes and posing a significant challenge to traffic safety. Identifying\nDriver Hazardous Action (DHA) is essential for understanding crash causation,\nyet the reliability of DHA data in large-scale databases is limited by\ninconsistent and labor-intensive manual coding practices. Here, we present an\ninnovative framework that leverages a fine-tuned large language model to\nautomatically infer DHAs from textual crash narratives, thereby improving the\nvalidity and interpretability of DHA classifications. Using five years of\ntwo-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on\ndetailed crash narratives and benchmarked its performance against conventional\nmachine learning classifiers, including Random Forest, XGBoost, CatBoost, and a\nneural network. The fine-tuned LLM achieved an overall accuracy of 80%,\nsurpassing all baseline models and demonstrating pronounced improvements in\nscenarios with imbalanced data. To increase interpretability, we developed a\nprobabilistic reasoning approach, analyzing model output shifts across original\ntest sets and three targeted counterfactual scenarios: variations in driver\ndistraction and age. Our analysis revealed that introducing distraction for one\ndriver substantially increased the likelihood of \"General Unsafe Driving\";\ndistraction for both drivers maximized the probability of \"Both Drivers Took\nHazardous Actions\"; and assigning a teen driver markedly elevated the\nprobability of \"Speed and Stopping Violations.\" Our framework and analytical\nmethods provide a robust and interpretable solution for large-scale automated\nDHA detection, offering new opportunities for traffic safety analysis and\nintervention."}
{"id": "2510.13664", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.13664", "abs": "https://arxiv.org/abs/2510.13664", "authors": ["Muhammad Haseeb", "Jinkun Geng", "Radhika Mittal", "Aurojit Panda", "Srinivas Narayana", "Anirudh Sivaraman"], "title": "Fair Ordering", "comment": null, "summary": "A growing class of applications demands \\emph{fair ordering/sequencing} of\nevents which ensures that events generated earlier by one client are processed\nbefore later events from other clients. However, achieving such sequencing is\nfundamentally challenging due to the inherent limitations of clock\nsynchronization. We advocate for an approach that embraces, rather than\neliminates, clock variability. Instead of attempting to remove error from a\ntimestamp, Tommy, our proposed system, leverages a statistical model to compare\ntwo noisy timestamps probabilistically by learning per-clock offset\ndistributions. Our preliminary statistical model computes the probability that\none event precedes another w.r.t. the wall-clock time without access to the\nwall-clock. This serves as a foundation for a new relation:\n\\emph{likely-happened-before} denoted by $\\xrightarrow{p}$ where $p$ represents\nthe probability of an event to have happened before another. The\n$\\xrightarrow{p}$ relation provides a basis for ordering multiple events which\nare otherwise considered \\emph{concurrent} by the typical\n\\emph{happened-before} ($\\rightarrow$) relation. We highlight various related\nchallenges including intransitivity of $\\xrightarrow{p}$ relation as opposed to\nthe transitive $\\rightarrow$ relation. We also outline several research\ndirections: online fair sequencing, stochastically fair total ordering,\nhost-level support for fairness and more."}
{"id": "2510.13532", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13532", "abs": "https://arxiv.org/abs/2510.13532", "authors": ["Dushyantha A Basnayaka"], "title": "Simulating Mediumband Wireless Communication Systems: A Concise Description", "comment": "10 pages, 4 figures, and a MATLAB code included", "summary": "In this paper, we describe the necessary procedures for accurately simulating\ndigital wireless communication systems operating in the mediumband, aimed at\nboth beginners and experts. In the research literature, digital wireless\ncommunication systems are typically simulated in the discrete-time complex\nbaseband domain, where pulse shaping, upconversion, mixing, carrier\nsynchronization, and symbol timing synchronization are often ignored. These\nassumptions are indeed sufficient in most cases, but to capture the essence of\ncommunication in the mediumband, certain physical layer (PHY) operations should\nbe simulated in detail. In this paper, we concisely describe how to simulate a\nmediumband wireless communication scenario from a single transmitter (TX) to a\nsingle receiver (RX) in MATLAB, elaborating the operation of key PHY\nsubsystems. The approach described here ensures that the simulated system\ncaptures the delicate dynamics of mediumband wireless communication, including\nthe effect of deep fading avoidance."}
{"id": "2510.13029", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13029", "abs": "https://arxiv.org/abs/2510.13029", "authors": ["Xinlei Wang", "Mingtian Tan", "Jing Qiu", "Junhua Zhao", "Jinjin Gu"], "title": "Toward Reasoning-Centric Time-Series Analysis", "comment": null, "summary": "Traditional time series analysis has long relied on pattern recognition,\ntrained on static and well-established benchmarks. However, in real-world\nsettings -- where policies shift, human behavior adapts, and unexpected events\nunfold -- effective analysis must go beyond surface-level trends to uncover the\nactual forces driving them. The recent rise of Large Language Models (LLMs)\npresents new opportunities for rethinking time series analysis by integrating\nmultimodal inputs. However, as the use of LLMs becomes popular, we must remain\ncautious, asking why we use LLMs and how to exploit them effectively. Most\nexisting LLM-based methods still employ their numerical regression ability and\nignore their deeper reasoning potential. This paper argues for rethinking time\nseries with LLMs as a reasoning task that prioritizes causal structure and\nexplainability. This shift brings time series analysis closer to human-aligned\nunderstanding, enabling transparent and context-aware insights in complex\nreal-world environments."}
{"id": "2510.13689", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.13689", "abs": "https://arxiv.org/abs/2510.13689", "authors": ["Zhiyuan He", "Yi Xu", "Cheng Luo", "Lili Qiu", "Yuqing Yang"], "title": "Optimize Replica Server Placement in a Satellite Network", "comment": "14 pages, two columns", "summary": "Satellite communication offers Internet connectivity to remote locations,\nsuch as villages, deserts, mountains, and at sea. However, transmitting content\nover satellite networks is significantly more expensive than traditional\nInternet. To address this issue, we propose placing content replica servers\nwithin satellite networks and optimizing replica placement for important\nperformance metrics, such as latency, transmission, and storage cost. Our\napproach can support different types of satellite networks, including Low Earth\nOrbit (LEO), Medium Earth Orbit (MEO), Geostationary Orbit (GEO), and their\ncombinations. An important challenge for supporting content replicas in such\nnetworks is that LEO and MEO satellites are constantly moving. We address this\nchallenge by explicitly considering their moving trajectories and strategically\noptimizing not only client performance, but also the cost of transferring\ncontent from one satellite to another as needed. We demonstrate the\neffectiveness of our approach using both simulated traffic traces and a\nprototype system."}
{"id": "2510.13661", "categories": ["cs.IT", "cs.CR", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13661", "abs": "https://arxiv.org/abs/2510.13661", "authors": ["Emmanouil M. Athanasakos", "Nicholas Kalouptsidis", "Hariprasad Manjunath"], "title": "Local Information-Theoretic Security via Euclidean Geometry", "comment": "48 pages, 12 figures, submitted to IEEE Transactions on Information\n  Theory", "summary": "This paper introduces a methodology based on Euclidean information theory to\ninvestigate local properties of secure communication over discrete memoryless\nwiretap channels. We formulate a constrained optimization problem that\nmaximizes a legitimate user's information rate while imposing explicit upper\nbounds on both the information leakage to an eavesdropper and the informational\ncost of encoding the secret message. By leveraging local geometric\napproximations, this inherently non-convex problem is transformed into a\ntractable quadratic programming structure. It is demonstrated that the optimal\nLagrange multipliers governing this approximated problem can be found by\nsolving a linear program. The constraints of this linear program are derived\nfrom Karush-Kuhn-Tucker conditions and are expressed in terms of the\ngeneralized eigenvalues of channel-derived matrices. This framework facilitates\nthe derivation of an analytical formula for an approximate local secrecy\ncapacity. Furthermore, we define and analyze a new class of secret local\ncontraction coefficients. These coefficients, characterized as the largest\ngeneralized eigenvalues of a matrix pencil, quantify the maximum achievable\nratio of approximate utility to approximate leakage, thus measuring the\nintrinsic local leakage efficiency of the channel. We establish bounds\nconnecting these local coefficients to their global counterparts defined over\ntrue mutual information measures. The efficacy of the proposed framework is\ndemonstrated through detailed analysis and numerical illustrations for both\ngeneral multi-mode channels and the canonical binary symmetric wiretap channel."}
{"id": "2510.13036", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13036", "abs": "https://arxiv.org/abs/2510.13036", "authors": ["Stephane Hatgis-Kessell", "Logan Mondal Bhamidipaty", "Emma Brunskill"], "title": "Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking", "comment": null, "summary": "Human-designed reward functions for reinforcement learning (RL) agents are\nfrequently misaligned with the humans' true, unobservable objectives, and thus\nact only as proxies. Optimizing for a misspecified proxy reward function often\ninduces reward hacking, resulting in a policy misaligned with the human's true\nobjectives. An alternative is to perform RL from human feedback, which involves\nlearning a reward function from scratch by collecting human preferences over\npairs of trajectories. However, building such datasets is costly. To address\nthe limitations of both approaches, we propose Preference-Based Reward Repair\n(PBRR): an automated iterative framework that repairs a human-specified proxy\nreward function by learning an additive, transition-dependent correction term\nfrom preferences. A manually specified reward function can yield policies that\nare highly suboptimal under the ground-truth objective, yet corrections on only\na few transitions may suffice to recover optimal performance. To identify and\ncorrect for those transitions, PBRR uses a targeted exploration strategy and a\nnew preference-learning objective. We prove in tabular domains PBRR has a\ncumulative regret that matches, up to constants, that of prior preference-based\nRL methods. In addition, on a suite of reward-hacking benchmarks, PBRR\nconsistently outperforms baselines that learn a reward function from scratch\nfrom preferences or modify the proxy reward function using other approaches,\nrequiring substantially fewer preferences to learn high performing policies."}
{"id": "2510.13710", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.13710", "abs": "https://arxiv.org/abs/2510.13710", "authors": ["Rohan Bose", "Jinwei Zhao", "Tanya Shreedhar", "Jianping Pan", "Nitinder Mohan"], "title": "Investigating Web Content Delivery Performance over Starlink", "comment": "12 pages, 14 figures, LEO Satellite Networks, Content Delivery\n  Networks, Domain Name System, Internet Measurements", "summary": "Low Earth Orbit (LEO) satellite ISPs promise universal Internet connectivity,\nyet their interaction with content delivery remains poorly understood. We\npresent the first comprehensive measurement study decomposing Starlink's web\ncontent delivery performance decomposed across Point of Presence (PoP), DNS,\nand CDN layers. Through two years of measurements combining 225K Cloudflare AIM\ntests, M-Lab data, and active probing from 99 RIPE Atlas and controlled\nStarlink probes, we collect 6.1M traceroutes and 10.8M DNS queries to quantify\nhow satellite architecture disrupts terrestrial CDN assumptions. We identify\nthree distinct performance regimes based on infrastructure density. Regions\nwith local content-rich PoPs achieve near-terrestrial latencies with the\nsatellite segment dominating 80-90% of RTT. Infrastructure-sparse regions\nsuffer cascading penalties: remote PoPs force distant resolver selection, which\ntriggers CDN mis-localization, pushing latencies beyond 200 ms.\nDense-infrastructure regions show minimal sensitivity to PoP changes.\nLeveraging Starlink's infrastructure expansion in early 2025 as a natural\nexperiment, we demonstrate that relocating PoPs closer to user location reduces\nmedian page-fetch times by 60%. Our findings reveal that infrastructure\nproximity, not satellite coverage, influences web performance, requiring\nfundamental changes to CDN mapping and DNS resolution for satellite ISPs."}
{"id": "2510.13775", "categories": ["cs.IT", "math.CA", "math.CO", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13775", "abs": "https://arxiv.org/abs/2510.13775", "authors": ["Joshua Brakensiek", "Yeyuan Chen", "Manik Dhar", "Zihan Zhang"], "title": "Combinatorial Bounds for List Recovery via Discrete Brascamp--Lieb Inequalities", "comment": "27 pages", "summary": "In coding theory, the problem of list recovery asks one to find all codewords\n$c$ of a given code $C$ which such that at least $1-\\rho$ fraction of the\nsymbols of $c$ lie in some predetermined set of $\\ell$ symbols for each\ncoordinate of the code. A key question is bounding the maximum possible list\nsize $L$ of such codewords for the given code $C$.\n  In this paper, we give novel combinatorial bounds on the list recoverability\nof various families of linear and folded linear codes, including random linear\ncodes, random Reed--Solomon codes, explicit folded Reed--Solomon codes, and\nexplicit univariate multiplicity codes. Our main result is that in all of these\nsettings, we show that for code of rate $R$, when $\\rho = 1 - R - \\epsilon$\napproaches capacity, the list size $L$ is at most\n$(\\ell/(R+\\epsilon))^{O(R/\\epsilon)}$. These results also apply in the\naverage-radius regime. Our result resolves a long-standing open question on\nwhether $L$ can be bounded by a polynomial in $\\ell$. In the zero-error regime,\nour bound on $L$ perfectly matches known lower bounds.\n  The primary technique is a novel application of a discrete entropic\nBrascamp--Lieb inequality to the problem of list recovery, allowing us to\nrelate the local structure of each coordinate with the global structure of the\nrecovered list. As a result of independent interest, we show that a recent\nresult by Chen and Zhang (STOC 2025) on the list decodability of folded\nReed--Solomon codes can be generalized into a novel Brascamp--Lieb type\ninequality."}
{"id": "2510.13195", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13195", "abs": "https://arxiv.org/abs/2510.13195", "authors": ["Qun Ma", "Xiao Xue", "Xuwen Zhang", "Zihan Zhao", "Yuwei Guo", "Ming Zhang"], "title": "Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation", "comment": null, "summary": "The advent of large language models (LLMs) has enabled agents to represent\nvirtual humans in societal simulations, facilitating diverse interactions\nwithin complex social systems. However, existing LLM-based agents exhibit\nsevere limitations in affective cognition: They fail to simulate the bounded\nrationality essential for bridging virtual and real-world services; They lack\nempirically validated integration mechanisms embedding emotions within agent\ndecision architectures. This paper constructs an emotional cognition framework\nincorporating desire generation and objective management, designed to achieve\nemotion alignment between LLM-based agents and humans, modeling the complete\ndecision-making process of LLM-based agents, encompassing state evolution,\ndesire generation, objective optimization, decision generation, and action\nexecution. This study implements the proposed framework within our proprietary\nmulti-agent interaction environment. Experimental results demonstrate that\nagents governed by our framework not only exhibit behaviors congruent with\ntheir emotional states but also, in comparative assessments against other agent\ntypes, demonstrate superior ecological validity and generate decision outcomes\nthat significantly more closely approximate human behavioral patterns."}
{"id": "2510.13732", "categories": ["cs.NI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13732", "abs": "https://arxiv.org/abs/2510.13732", "authors": ["Mohd Saif Ali Khan", "Karthik RM", "Samar Agnihotri"], "title": "Scalable Pilot Assignment for Distributed Massive MIMO using Channel Estimation Error", "comment": null, "summary": "Pilot contamination remains a major bottleneck in realizing the full\npotential of distributed massive MIMO systems. We propose two dynamic and\nscalable pilot assignment strategies designed for practical deployment in such\nnetworks. First, we present a low complexity centralized algorithm that\nsequentially assigns pilots to user equipments (UEs) to minimize the global\nchannel estimation errors across serving access points (APs). This improves the\nchannel estimation quality and reduces interference among UEs, enhancing the\nspectral efficiency. Second, we develop a fully distributed algorithm that uses\na priority-based pilot selection approach. In this algorithm, each selected AP\nminimizes estimation error using only local information and offers candidate\npilots to the UEs. Every UE then selects a suitable pilot based on AP priority.\nThis approach ensures consistency and minimizes interference while\nsignificantly reducing pilot contamination. The method requires no global\ncoordination, maintains low signaling overhead, and adapts dynamically to the\nUE deployment. Numerical simulations demonstrate the superiority of our\nproposed schemes in terms of network throughput when compared to other\nstate-of-the-art benchmark schemes."}
{"id": "2510.13777", "categories": ["cs.IT", "math.CO", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13777", "abs": "https://arxiv.org/abs/2510.13777", "authors": ["Joshua Brakensiek", "Yeyuan Chen", "Manik Dhar", "Zihan Zhang"], "title": "From Random to Explicit via Subspace Designs With Applications to Local Properties and Matroids", "comment": "41 pages", "summary": "In coding theory, a common question is to understand the threshold rates of\nvarious local properties of codes, such as their list decodability and list\nrecoverability. A recent work Levi, Mosheiff, and Shagrithaya (FOCS 2025) gave\na novel unified framework for calculating the threshold rates of local\nproperties for random linear and random Reed--Solomon codes.\n  In this paper, we extend their framework to studying the local properties of\nsubspace designable codes, including explicit folded Reed-Solomon and\nunivariate multiplicity codes. Our first main result is a local equivalence\nbetween random linear codes and (nearly) optimal subspace design codes up to an\narbitrarily small rate decrease. We show any local property of random linear\ncodes applies to all subspace design codes. As such, we give the first explicit\nconstruction of folded linear codes that simultaneously attain all local\nproperties of random linear codes. Conversely, we show that any local property\nwhich applies to all subspace design codes also applies to random linear codes.\n  Our second main result is an application to matroid theory. We show that the\ncorrectable erasure patterns in a maximally recoverable tensor code can be\nidentified in deterministic polynomial time, assuming a positive answer to a\nmatroid-theoretic question due to Mason (1981). This improves on a result of\nJackson and Tanigawa (JCTB 2024) who gave a complexity characterization of\n$\\mathsf{RP} \\cap \\mathsf{coNP}$ assuming a stronger conjecture. Our result\nalso applies to the generic bipartite rigidity and matrix completion matroids.\n  As a result of additional interest, we study the existence and limitations of\nsubspace designs. In particular, we tighten the analysis of family of subspace\ndesigns constructioned by Guruswami and Kopparty (Combinatorica 2016) and show\nthat better subspace designs do not exist over algebraically closed fields."}
{"id": "2510.13214", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13214", "abs": "https://arxiv.org/abs/2510.13214", "authors": ["Zehui Ling", "Deshu Chen", "Yichi Zhang", "Yuchen Liu", "Xigui Li", "Xin Guo", "Yuan Cheng"], "title": "Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) demonstrate that\nchain-of-thought prompting and deep reasoning substantially enhance performance\non complex tasks, and multi-agent systems can further improve accuracy by\nenabling model debates. However, applying deep reasoning to all problems is\ncomputationally expensive. To mitigate these costs, we propose a complementary\nagent system integrating small and large LLMs. The small LLM first generates an\ninitial answer, which is then verified by the large LLM. If correct, the answer\nis adopted directly; otherwise, the large LLM performs in-depth reasoning.\nExperimental results show that, for simple problems, our approach reduces the\ncomputational cost of the large LLM by more than 50% with negligible accuracy\nloss, while consistently maintaining robust performance on complex tasks."}
{"id": "2510.13459", "categories": ["cs.AI", "cs.CE", "cs.NI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.13459", "abs": "https://arxiv.org/abs/2510.13459", "authors": ["Timothy Wong", "Tom Freeman", "Joseph Feehily"], "title": "Mobile Coverage Analysis using Crowdsourced Data", "comment": "8 pages", "summary": "Effective assessment of mobile network coverage and the precise\nidentification of service weak spots are paramount for network operators\nstriving to enhance user Quality of Experience (QoE). This paper presents a\nnovel framework for mobile coverage and weak spot analysis utilising\ncrowdsourced QoE data. The core of our methodology involves coverage analysis\nat the individual cell (antenna) level, subsequently aggregated to the site\nlevel, using empirical geolocation data. A key contribution of this research is\nthe application of One-Class Support Vector Machine (OC-SVM) algorithm for\ncalculating mobile network coverage. This approach models the decision\nhyperplane as the effective coverage contour, facilitating robust calculation\nof coverage areas for individual cells and entire sites. The same methodology\nis extended to analyse crowdsourced service loss reports, thereby identifying\nand quantifying geographically localised weak spots. Our findings demonstrate\nthe efficacy of this novel framework in accurately mapping mobile coverage and,\ncrucially, in highlighting granular areas of signal deficiency, particularly\nwithin complex urban environments."}
{"id": "2510.13732", "categories": ["cs.NI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13732", "abs": "https://arxiv.org/abs/2510.13732", "authors": ["Mohd Saif Ali Khan", "Karthik RM", "Samar Agnihotri"], "title": "Scalable Pilot Assignment for Distributed Massive MIMO using Channel Estimation Error", "comment": null, "summary": "Pilot contamination remains a major bottleneck in realizing the full\npotential of distributed massive MIMO systems. We propose two dynamic and\nscalable pilot assignment strategies designed for practical deployment in such\nnetworks. First, we present a low complexity centralized algorithm that\nsequentially assigns pilots to user equipments (UEs) to minimize the global\nchannel estimation errors across serving access points (APs). This improves the\nchannel estimation quality and reduces interference among UEs, enhancing the\nspectral efficiency. Second, we develop a fully distributed algorithm that uses\na priority-based pilot selection approach. In this algorithm, each selected AP\nminimizes estimation error using only local information and offers candidate\npilots to the UEs. Every UE then selects a suitable pilot based on AP priority.\nThis approach ensures consistency and minimizes interference while\nsignificantly reducing pilot contamination. The method requires no global\ncoordination, maintains low signaling overhead, and adapts dynamically to the\nUE deployment. Numerical simulations demonstrate the superiority of our\nproposed schemes in terms of network throughput when compared to other\nstate-of-the-art benchmark schemes."}
{"id": "2510.13215", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13215", "abs": "https://arxiv.org/abs/2510.13215", "authors": ["Joy Jia Yin Lim", "Ye He", "Jifan Yu", "Xin Cong", "Daniel Zhang-Li", "Zhiyuan Liu", "Huiqin Liu", "Lei Hou", "Juanzi Li", "Bin Xu"], "title": "Personalized Learning Path Planning with Goal-Driven Learner State Modeling", "comment": null, "summary": "Personalized Learning Path Planning (PLPP) aims to design adaptive learning\npaths that align with individual goals. While large language models (LLMs) show\npotential in personalizing learning experiences, existing approaches often lack\nmechanisms for goal-aligned planning. We introduce Pxplore, a novel framework\nfor PLPP that integrates a reinforcement-based training paradigm and an\nLLM-driven educational architecture. We design a structured learner state model\nand an automated reward function that transforms abstract objectives into\ncomputable signals. We train the policy combining supervised fine-tuning (SFT)\nand Group Relative Policy Optimization (GRPO), and deploy it within a\nreal-world learning platform. Extensive experiments validate Pxplore's\neffectiveness in producing coherent, personalized, and goal-driven learning\npaths. We release our code and dataset to facilitate future research."}
{"id": "2510.13220", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13220", "abs": "https://arxiv.org/abs/2510.13220", "authors": ["Yufei He", "Juncheng Liu", "Yue Liu", "Yibo Li", "Tri Cao", "Zhiyuan Hu", "Xinxing Xu", "Bryan Hooi"], "title": "EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems", "comment": null, "summary": "A fundamental limitation of current AI agents is their inability to learn\ncomplex skills on the fly at test time, often behaving like \"clever but\nclueless interns\" in novel environments. This severely limits their practical\nutility. To systematically measure and drive progress on this challenge, we\nfirst introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a\nnew evaluation setup where an agent must play the same game for several\nconsecutive episodes, attempting to improve its performance from one episode to\nthe next. On J-TTL, we find that existing adaptation methods like reflection,\nmemory, or reinforcement learning struggle. To address the challenges posed by\nour benchmark, we present EvoTest, an evolutionary test-time learning framework\nthat improves an agent without any fine-tuning or gradients-by evolving the\nentire agentic system after every episode. EvoTest has two roles: the Actor\nAgent, which plays the game, and the Evolver Agent, which analyzes the episode\ntranscript to propose a revised configuration for the next run. This\nconfiguration rewrites the prompt, updates memory by logging effective\nstate-action choices, tunes hyperparameters, and learns the tool-use routines.\nOn our J-TTL benchmark, EvoTest consistently increases performance,\noutperforming not only reflection and memory-only baselines but also more\ncomplex online fine-tuning methods. Notably, our method is the only one capable\nof winning two games (Detective and Library), while all baselines fail to win\nany."}
{"id": "2510.13230", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13230", "abs": "https://arxiv.org/abs/2510.13230", "authors": ["Jalal Khan", "Manzoor Khan", "Sherzod Turaev", "Sumbal Malik", "Hesham El-Sayed", "Farman Ullah"], "title": "An Analytical Framework to Enhance Autonomous Vehicle Perception for Smart Cities", "comment": "32 pages, 14 figures", "summary": "The driving environment perception has a vital role for autonomous driving\nand nowadays has been actively explored for its realization. The research\ncommunity and relevant stakeholders necessitate the development of Deep\nLearning (DL) models and AI-enabled solutions to enhance autonomous vehicles\n(AVs) for smart mobility. There is a need to develop a model that accurately\nperceives multiple objects on the road and predicts the driver's perception to\ncontrol the car's movements. This article proposes a novel utility-based\nanalytical model that enables perception systems of AVs to understand the\ndriving environment. The article consists of modules: acquiring a custom\ndataset having distinctive objects, i.e., motorcyclists, rickshaws, etc; a\nDL-based model (YOLOv8s) for object detection; and a module to measure the\nutility of perception service from the performance values of trained model\ninstances. The perception model is validated based on the object detection\ntask, and its process is benchmarked by state-of-the-art deep learning models'\nperformance metrics from the nuScense dataset. The experimental results show\nthree best-performing YOLOv8s instances based on mAP@0.5 values, i.e.,\nSGD-based (0.832), Adam-based (0.810), and AdamW-based (0.822). However, the\nAdamW-based model (i.e., car: 0.921, motorcyclist: 0.899, truck: 0.793, etc.)\nstill outperforms the SGD-based model (i.e., car: 0.915, motorcyclist: 0.892,\ntruck: 0.781, etc.) because it has better class-level performance values,\nconfirmed by the proposed perception model. We validate that the proposed\nfunction is capable of finding the right perception for AVs. The results above\nencourage using the proposed perception model to evaluate the utility of\nlearning models and determine the appropriate perception for AVs."}
{"id": "2510.13262", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13262", "abs": "https://arxiv.org/abs/2510.13262", "authors": ["Weiqi Guo", "Guanjun Liu", "Ziyuan Zhou"], "title": "SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning", "comment": null, "summary": "Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for\ncooperative and competitive tasks such as autonomous driving and strategic\ngaming. However, models trained by MADRL are vulnerable to adversarial\nperturbations on states and actions. Therefore, it is essential to investigate\nthe robustness of MADRL models from an attack perspective. Existing studies\nfocus on either state-only attacks or action-only attacks, but do not consider\nhow to effectively joint them. Simply combining state and action perturbations\nsuch as randomly perturbing states and actions does not exploit their potential\nsynergistic effects. In this paper, we propose the State-Action Joint Attack\n(SAJA) framework that has a good synergistic effects. SAJA consists of two\nimportant phases: (1) In the state attack phase, a multi-step gradient ascent\nmethod utilizes both the actor network and the critic network to compute an\nadversarial state, and (2) in the action attack phase, based on the perturbed\nstate, a second gradient ascent uses the critic network to craft the final\nadversarial action. Additionally, a heuristic regularizer measuring the\ndistance between the perturbed actions and the original clean ones is added\ninto the loss function to enhance the effectiveness of the critic's guidance.\nWe evaluate SAJA in the Multi-Agent Particle Environment (MPE), demonstrating\nthat (1) it outperforms and is more stealthy than state-only or action-only\nattacks, and (2) existing state or action defense methods cannot defend its\nattacks."}
{"id": "2510.13393", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13393", "abs": "https://arxiv.org/abs/2510.13393", "authors": ["Yunxiao Zhao", "Zhiqiang Wang", "Xingtong Yu", "Xiaoli Li", "Jiye Liang", "Ru Li"], "title": "Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization", "comment": "14 pages, 7 figures, 11 tables. Under review by IEEE", "summary": "Rationalization, a data-centric framework, aims to build self-explanatory\nmodels to explain the prediction outcome by generating a subset of\nhuman-intelligible pieces of the input data. It involves a cooperative game\nmodel where a generator generates the most human-intelligible parts of the\ninput (i.e., rationales), followed by a predictor that makes predictions based\non these generated rationales. Conventional rationalization methods typically\nimpose constraints via regularization terms to calibrate or penalize undesired\ngeneration. However, these methods are suffering from a problem called mode\ncollapse, in which the predictor produces correct predictions yet the generator\nconsistently outputs rationales with collapsed patterns. Moreover, existing\nstudies are typically designed separately for specific collapsed patterns,\nlacking a unified consideration. In this paper, we systematically revisit\ncooperative rationalization from a novel game-theoretic perspective and\nidentify the fundamental cause of this problem: the generator no longer tends\nto explore new strategies to uncover informative rationales, ultimately leading\nthe system to converge to a suboptimal game equilibrium (correct predictions\nv.s collapsed rationales). To solve this problem, we then propose a novel\napproach, Game-theoretic Policy Optimization oriented RATionalization (PORAT),\nwhich progressively introduces policy interventions to address the game\nequilibrium in the cooperative game process, thereby guiding the model toward a\nmore optimal solution state. We theoretically analyse the cause of such a\nsuboptimal equilibrium and prove the feasibility of the proposed method.\nFurthermore, we validate our method on nine widely used real-world datasets and\ntwo synthetic settings, where PORAT achieves up to 8.1% performance\nimprovements over existing state-of-the-art methods."}
{"id": "2510.13417", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13417", "abs": "https://arxiv.org/abs/2510.13417", "authors": ["Liesbeth Allein", "Nataly Pineda-Castañeda", "Andrea Rocci", "Marie-Francine Moens"], "title": "Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse", "comment": null, "summary": "How does a cause lead to an effect, and which intermediate causal steps\nexplain their connection? This work scrutinizes the mechanistic causal\nreasoning capabilities of large language models (LLMs) to answer these\nquestions through the task of implicit causal chain discovery. In a diagnostic\nevaluation framework, we instruct nine LLMs to generate all possible\nintermediate causal steps linking given cause-effect pairs in causal chain\nstructures. These pairs are drawn from recent resources in argumentation\nstudies featuring polarized discussion on climate change. Our analysis reveals\nthat LLMs vary in the number and granularity of causal steps they produce.\nAlthough they are generally self-consistent and confident about the\nintermediate causal connections in the generated chains, their judgments are\nmainly driven by associative pattern matching rather than genuine causal\nreasoning. Nonetheless, human evaluations confirmed the logical coherence and\nintegrity of the generated chains. Our baseline causal chain discovery\napproach, insights from our diagnostic evaluation, and benchmark dataset with\ncausal chains lay a solid foundation for advancing future work in implicit,\nmechanistic causal reasoning in argumentation settings."}
{"id": "2510.13459", "categories": ["cs.AI", "cs.CE", "cs.NI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.13459", "abs": "https://arxiv.org/abs/2510.13459", "authors": ["Timothy Wong", "Tom Freeman", "Joseph Feehily"], "title": "Mobile Coverage Analysis using Crowdsourced Data", "comment": "8 pages", "summary": "Effective assessment of mobile network coverage and the precise\nidentification of service weak spots are paramount for network operators\nstriving to enhance user Quality of Experience (QoE). This paper presents a\nnovel framework for mobile coverage and weak spot analysis utilising\ncrowdsourced QoE data. The core of our methodology involves coverage analysis\nat the individual cell (antenna) level, subsequently aggregated to the site\nlevel, using empirical geolocation data. A key contribution of this research is\nthe application of One-Class Support Vector Machine (OC-SVM) algorithm for\ncalculating mobile network coverage. This approach models the decision\nhyperplane as the effective coverage contour, facilitating robust calculation\nof coverage areas for individual cells and entire sites. The same methodology\nis extended to analyse crowdsourced service loss reports, thereby identifying\nand quantifying geographically localised weak spots. Our findings demonstrate\nthe efficacy of this novel framework in accurately mapping mobile coverage and,\ncrucially, in highlighting granular areas of signal deficiency, particularly\nwithin complex urban environments."}
{"id": "2510.13501", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13501", "abs": "https://arxiv.org/abs/2510.13501", "authors": ["He Du", "Bowen Li", "Chengxing Xie", "Chang Gao", "Kai Chen", "Dacheng Tao"], "title": "Confidence as a Reward: Transforming LLMs into Reward Models", "comment": null, "summary": "Reward models can significantly enhance the reasoning capabilities of large\nlanguage models (LLMs), but they typically require extensive curated data and\ncostly training. To mitigate these challenges, training-free approaches such as\nLLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate\nresponses, achieving promising results. Recent works have also indicated that\nmodel confidence can serve effectively as a reward metric, distinguishing\nbetween chain-of-thought (CoT) and non-CoT paths. However, the concept of using\nconfidence as a reward has not been comprehensively studied. In this work, we\nsystematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful\ntraining-free method that utilizes token-level confidence in the model's final\nanswers as a proxy for reward, especially suitable for close-ended tasks.\nThrough extensive experiments on mathematical reasoning tasks, we demonstrate\nthat CRew outperforms existing training-free reward approaches on the MATH500\nand RewardMATH benchmarks, and even surpasses most trained reward models. We\nfurther identify a strong correlation between CRew scores and the actual\nreasoning performance of the model. Additionally, we find that CRew can\neffectively filter high-quality training data. Building upon these insights, we\npropose CRew-DPO, a training strategy that constructs preference data from\nconfidence scores combined with correctness signals. Finetuning with CRew-DPO\nfurther enhances the model's judging capabilities and consistently outperforms\nexisting self-training methods."}
{"id": "2510.13524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13524", "abs": "https://arxiv.org/abs/2510.13524", "authors": ["William Flanagan", "Mukunda Das", "Rajitha Ramanyake", "Swaunja Maslekar", "Meghana Manipuri", "Joong Ho Choi", "Shruti Nair", "Shambhavi Bhusan", "Sanjana Dulam", "Mouni Pendharkar", "Nidhi Singh", "Vashisth Doshi", "Sachi Shah Paresh"], "title": "A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain", "comment": "NeurIPS 2025 GenAI in Finance Workshop", "summary": "As Generative Artificial Intelligence is adopted across the financial\nservices industry, a significant barrier to adoption and usage is measuring\nmodel performance. Historical machine learning metrics can oftentimes fail to\ngeneralize to GenAI workloads and are often supplemented using Subject Matter\nExpert (SME) Evaluation. Even in this combination, many projects fail to\naccount for various unique risks present in choosing specific metrics.\nAdditionally, many widespread benchmarks created by foundational research labs\nand educational institutions fail to generalize to industrial use. This paper\nexplains these challenges and provides a Risk Assessment Framework to allow for\nbetter application of SME and machine learning Metrics"}
{"id": "2510.13551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13551", "abs": "https://arxiv.org/abs/2510.13551", "authors": ["Robert West", "Ashton Anderson", "Ece Kamar", "Eric Horvitz"], "title": "Tandem Training for Language Models", "comment": null, "summary": "As language models continue to rapidly improve, we can expect their actions\nand reasoning to become difficult or impossible for weaker agents and humans to\nfollow, undermining interpretability and oversight. With an eye on long-term\nfutures, we pursue methods that encourage models to produce solutions that\nremain intelligible to weaker collaborators. We formalize intelligibility as\nhandoff robustness: a strong model's solution is intelligible to a weaker model\nif randomly handing off control to the weaker model along the solution path\ndoes not cause failure. Building on this criterion, we introduce tandem\ntraining for language models, a reinforcement learning (RL) paradigm in which\nrollout tokens are intermittently and randomly sampled from a frozen weak model\nrather than the strong model being trained. Because rollouts succeed only when\nthe strong model's actions and reasoning process can be continued by the weak\nmodel -- when the two can co-construct a successful solution -- optimizing\nstandard RL objectives with tandem training implicitly incentivizes both\ncorrectness and intelligibility. In the GSM8K math reasoning task, tandem\ntraining reliably teaches models to abandon jargon and adapt their language to\nweaker partners while keeping task accuracy high. Our results demonstrate a\npromising route to building AI systems that remain auditable by weaker agents,\nwith implications for human--AI collaboration and multi-agent communication."}
{"id": "2510.13691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13691", "abs": "https://arxiv.org/abs/2510.13691", "authors": ["Cecilia Di Florio", "Huimin Dong", "Antonino Rotolo"], "title": "A Modal Logic for Temporal and Jurisdictional Classifier Models", "comment": "18 pages, 2 figures. Extended version of a short paper accepted at\n  PRIMA 2025. This is the authors' version of the work. It is posted here for\n  your personal use", "summary": "Logic-based models can be used to build verification tools for machine\nlearning classifiers employed in the legal field. ML classifiers predict the\noutcomes of new cases based on previous ones, thereby performing a form of\ncase-based reasoning (CBR). In this paper, we introduce a modal logic of\nclassifiers designed to formally capture legal CBR. We incorporate principles\nfor resolving conflicts between precedents, by introducing into the logic the\ntemporal dimension of cases and the hierarchy of courts within the legal\nsystem."}
{"id": "2510.13709", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13709", "abs": "https://arxiv.org/abs/2510.13709", "authors": ["Evan Ellis", "Vivek Myers", "Jens Tuyls", "Sergey Levine", "Anca Dragan", "Benjamin Eysenbach"], "title": "Training LLM Agents to Empower Humans", "comment": null, "summary": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards."}
{"id": "2510.13727", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13727", "abs": "https://arxiv.org/abs/2510.13727", "authors": ["Ravi Pandya", "Madison Bland", "Duy P. Nguyen", "Changliu Liu", "Jaime Fernández Fisac", "Andrea Bajcsy"], "title": "From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails", "comment": null, "summary": "Generative AI systems are increasingly assisting and acting on behalf of end\nusers in practical settings, from digital shopping assistants to\nnext-generation autonomous cars. In this context, safety is no longer about\nblocking harmful content, but about preempting downstream hazards like\nfinancial or physical harm. Yet, most AI guardrails continue to rely on output\nclassification based on labeled datasets and human-specified criteria,making\nthem brittle to new hazardous situations. Even when unsafe conditions are\nflagged, this detection offers no path to recovery: typically, the AI system\nsimply refuses to act--which is not always a safe choice. In this work, we\nargue that agentic AI safety is fundamentally a sequential decision problem:\nharmful outcomes arise from the AI system's continually evolving interactions\nand their downstream consequences on the world. We formalize this through the\nlens of safety-critical control theory, but within the AI model's latent\nrepresentation of the world. This enables us to build predictive guardrails\nthat (i) monitor an AI system's outputs (actions) in real time and (ii)\nproactively correct risky outputs to safe ones, all in a model-agnostic manner\nso the same guardrail can be wrapped around any AI model. We also offer a\npractical training recipe for computing such guardrails at scale via\nsafety-critical reinforcement learning. Our experiments in simulated driving\nand e-commerce settings demonstrate that control-theoretic guardrails can\nreliably steer LLM agents clear of catastrophic outcomes (from collisions to\nbankruptcy) while preserving task performance, offering a principled dynamic\nalternative to today's flag-and-block guardrails."}
{"id": "2510.13744", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13744", "abs": "https://arxiv.org/abs/2510.13744", "authors": ["Shrey Pandit", "Austin Xu", "Xuan-Phi Nguyen", "Yifei Ming", "Caiming Xiong", "Shafiq Joty"], "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math", "comment": "21 pages, 8 figures, 5 tables", "summary": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics."}
