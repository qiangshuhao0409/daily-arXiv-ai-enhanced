<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.IT](#cs.IT) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [UAV-enabled Computing Power Networks: Task Completion Probability Analysis](https://arxiv.org/abs/2512.15173)
*Yiqin Deng,Zhengru Fang,Senkang Hu,Yanan Ma,Haixia Zhang,Yuguang Fang*

Main category: cs.NI

TL;DR: 提出无人机赋能计算能力网络(UAV-CPNs)框架，通过无人机动态定位增强计算性能，分析任务完成概率作为关键性能指标，强调通信与计算能力平衡的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统计算能力网络存在通信瓶颈和多接入边缘计算的"孤岛效应"，需要解决复杂时空动态下的网络性能量化挑战。

Method: 建立UAV-CPNs框架，无人机作为动态空中中继，将请求区域任务外包到扩展服务区域；引入任务完成概率作为性能指标，运用随机过程和随机几何理论推导分析表达式。

Result: 数值结果表明通信与计算能力之间的平衡对提升UAV-CPNs性能至关重要，计算节点广泛分布带来显著性能增益。

Conclusion: UAV-CPNs框架能有效缓解传统网络瓶颈，任务完成概率是评估网络性能的有效指标，通信与计算能力的优化平衡是未来研究方向。

Abstract: This paper presents an innovative framework that synergistically enhances computing performance through ubiquitous computing power distribution and dynamic computing node accessibility control via adaptive unmanned aerial vehicle (UAV) positioning, establishing UAV-enabled Computing Power Networks (UAV-CPNs). In UAV-CPNs, UAVs function as dynamic aerial relays, outsourcing tasks generated in the request zone to an expanded service zone, consisting of a diverse range of computing devices, from vehicles with onboard computational capabilities and edge servers to dedicated computing nodes. This approach has the potential to alleviate communication bottlenecks in traditional computing power networks and overcome the "island effect" observed in multi-access edge computing. However, how to quantify the network performance under the complex spatio-temporal dynamics of both communication and computing power is a significant challenge, which introduces intricacies beyond those found in conventional networks. To address this, in this paper, we introduce task completion probability as the primary performance metric for evaluating the ability of UAV-CPNs to complete ground users' tasks within specified end-to-end latency requirements. Utilizing theories from stochastic processes and stochastic geometry, we derive analytical expressions that facilitate the assessment of this metric. Our numerical results emphasize that striking a delicate balance between communication and computational capabilities is essential for enhancing the performance of UAV-CPNs. Moreover, our findings show significant performance gains from the widespread distribution of computing nodes.

</details>


### [2] [More Capacity from Less Spectrum: Tapping into Optical-layer Intelligence in Optical Computing-Communication Integrated Network](https://arxiv.org/abs/2512.15190)
*Dao Thanh Hai,Shuo Li,Isaac Woungang*

Main category: cs.NI

TL;DR: 提出光计算-通信融合网络新架构，在光层同时提供计算和通信服务，利用光计算提升频谱和计算效率


<details>
  <summary>Details</summary>
Motivation: 随着光计算和全光信号处理技术的重大进展，需要探索下一代光传输网络的新架构，以充分利用光计算潜力，在光层同时提供计算和通信服务

Method: 提出光计算-通信融合网络架构，引入光层智能概念，通过光聚合操作案例研究展示与传统光旁路网络的区别，提供数学优化模型并在NSFNET拓扑上进行性能比较

Result: 在NSFNET拓扑上与传统光旁路模型进行性能比较，展示了光计算-通信融合网络在频谱和计算效率方面的优势

Conclusion: 光计算-通信融合网络是下一代光传输网络的有前景架构，能够在光层同时提供计算和通信服务，提升网络效率和性能

Abstract: Driven by massive investments and consequently significant progresses in optical computing and all-optical signal processing technologies lately, this paper presents a new architectural paradigm for next-generation optical transport network, entitled \textit{optical computing-communication integrated network}, which is capable of providing dual services at the optical layer, namely, computing and communication. This approach seeks to exploit the potential for performing optical computing operations among lightpaths that traverse the same intermediate node. \textit{Optical-layer intelligence concept} is thus introduced as the capability to perform computing / processing at the lightpath scale to achieve greater spectral and/or computing efficiency. A case study focusing on optical aggregation operation is introduced, highlighting the key differences between optical computing-communication integrated network and its current counterpart, optical-bypass ones. A mathematical formulation for optimal designs of optical-aggregation-enabled network is then provided and performance comparison with traditional optical-bypass model is drawn on the realistic NSFNET topology.

</details>


### [3] [DNS-based dynamic context resolution for SCHC](https://arxiv.org/abs/2512.15217)
*Antoine Bernard,Sandoche Balakrichenan,Michel Marot,Benoit Ampeau*

Main category: cs.NI

TL;DR: 提出基于DNS的机制来动态查找和下载LoRaWAN中SCHC压缩规则，减少存储开销并评估通信延迟


<details>
  <summary>Details</summary>
Motivation: LPWAN网络资源稀缺且载荷有限，LoRaWAN提供开放的长距离网络解决方案。SCHC用于IPv6头部压缩，但不同终端设备需要不同的上下文规则，本地存储所有规则不现实，需要远程动态获取机制。

Method: 提出基于DNS的机制来查找终端设备关联的SCHC上下文规则，允许从HTTP服务器动态下载规则。通过真实测试平台进行实验测量评估通信延迟。

Result: 通过实验测量评估了基于DNS的SCHC规则查找机制对通信延迟的影响，提供了实际性能数据。

Conclusion: 基于DNS的动态SCHC规则获取机制可行，能够有效解决LPWAN中多种终端设备规则存储问题，但需要评估引入的额外延迟。

Abstract: LPWANs are networks characterised by the scarcity of their radio resources and their limited payload size. LoRaWAN offers an open, easy-to-deploy and efficient solution to operate a long-range network. To efficiently communicate using IPv6, the LPWAN working group from the IETF developed a solution called Static Context Header Compression (SCHC). It uses context rules, which are linked to a given End Device, to compress the IPv6 and UDP header. Since there may be a huge variety of End Devices profile, it makes sense to store the rules remotely and use a system to retrieve the profiles dynamically. In this paper we propose a mechanism based on DNS to find the context rules associated with an End Device, allowing it to be downloaded from an HTTP Server. We evaluate the corresponding delay added to the communications using experimental measurements from a real testbed.

</details>


### [4] [Packet-Level Traffic Modeling with Heavy-Tailed Payload and Inter-Arrival Distributions for Digital Twins](https://arxiv.org/abs/2512.15432)
*Enes Koktas,Peter Rost*

Main category: cs.NI

TL;DR: 提出一种用于数字孪生无线接入网络的混合流量生成器，结合小规模隐马尔可夫模型和混合密度网络，以少量参数精确复现分组大小和时序特征


<details>
  <summary>Details</summary>
Motivation: 数字孪生无线接入网络需要能够复现分组大小和时序特征的流量生成器，同时要求模型紧凑且易于重新校准以适应流量变化

Method: 采用混合生成器：小型隐马尔可夫模型捕捉缓冲、流媒体和空闲状态；混合密度网络使用Student-t混合分布建模每个状态中载荷长度和到达间隔时间的联合分布；通过显式空闲状态处理重尾分布

Result: 在Web、智能家居和加密媒体流量数据集上评估，相比最近的神经网络、Transformer和隐马尔可夫基线，在多数指标（包括累积分布函数、自相关时间结构和Wasserstein距离）上最接近真实流量，参数数量少几个数量级

Conclusion: 该生成器仅占用约0.2MB内存，在数字孪生部署中具有低内存占用和低开销适应优势，能够以极少的参数精确复现实时流量特征

Abstract: Digital twins of radio access networks require packet-level traffic generators that reproduce the size and timing of packets while remaining compact and easy to recalibrate as traffic changes. We address this need with a hybrid generator that combines a small hidden Markov model, which captures buffering, streaming, and idle states, with a mixture density network that models the joint distribution of payload length and inter-arrival time (IAT) in each state using Student-t mixtures. The state space and emission family are designed to handle heavy-tailed IAT by anchoring an explicit idle state in the tail and allowing each component to adapt its tail thickness. We evaluate the model on public traces of web, smart home, and encrypted media traffic and compare it with recent neural network and transformer based generators as well as hidden Markov baselines. Across most datasets and metrics, including average per-flow cumulative distribution functions, autocorrelation based measures of temporal structure, and Wasserstein distances between flow descriptors, the proposed generator matches the real traffic most closely in the majority of cases while using orders of magnitude fewer parameters. The full model occupies around 0.2 MB in our experiments, which makes it suitable for deployment inside digital twins where memory footprint and low-overhead adaptation are critical.

</details>


### [5] [GenAI-enabled Residual Motion Estimation for Energy-Efficient Semantic Video Communication](https://arxiv.org/abs/2512.15481)
*Shavbo Salehi,Pedro Enrique Iturria-Rivera,Medhat Elsayed,Majid Bavand,Yigit Ozcan,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 提出PENME方法，通过可预测性感知和熵自适应神经运动估计，结合选择性扩散细化，显著降低视频语义通信的延迟、传输数据和功耗，同时提高质量指标。


<details>
  <summary>Details</summary>
Motivation: 传统香农范式在视频传输中存在高延迟、高比特率和功耗问题。语义通信通过传输含义而非精确表示来减少资源消耗，特别适合占据网络流量主导且对带宽和功率要求高的视频传输。

Method: 提出PENME方法：1) 基于运动强度、全局运动一致性、峰值锐度、异质性和残差误差的五步策略，逐帧选择残差运动提取模型；2) 在接收端通过运动补偿更新重建帧；3) 对低可预测性或大残差帧应用选择性扩散细化（LCM-4）；4) 结合残差运动和信道状态感知分配无线资源块。

Result: 在Vimeo90K数据集上的仿真显示：相比传统通信、混合和自适应比特率语义通信技术，PENME实现40%延迟降低、90%传输数据减少、35%吞吐量提高。语义指标方面：PSNR提升约40%、MS-SSIM提高约19%、LPIPS降低近35%。

Conclusion: PENME方法有效解决了视频语义通信中的延迟、比特率和功耗挑战，通过智能模型选择和选择性细化机制，在显著降低资源消耗的同时保持高质量传输，适用于各种视频类型。

Abstract: Semantic communication addresses the limitations of the Shannon paradigm by focusing on transmitting meaning rather than exact representations, thereby reducing unnecessary resource consumption. This is particularly beneficial for video, which dominates network traffic and demands high bandwidth and power, making semantic approaches ideal for conserving resources while maintaining quality. In this paper, we propose a Predictability-aware and Entropy-adaptive Neural Motion Estimation (PENME) method to address challenges related to high latency, high bitrate, and power consumption in video transmission. PENME makes per-frame decisions to select a residual motion extraction model, convolutional neural network, vision transformer, or optical flow, using a five-step policy based on motion strength, global motion consistency, peak sharpness, heterogeneity, and residual error. The residual motions are then transmitted to the receiver, where the frames are reconstructed via motion-compensated updates. Next, a selective diffusion-based refinement, the Latent Consistency Model (LCM-4), is applied on frames that trigger refinement due to low predictability or large residuals, while predictable frames skip refinement. PENME also allocates radio resource blocks with awareness of residual motion and channel state, reducing power consumption and bandwidth usage while maintaining high semantic similarity. Our simulation results on the Vimeo90K dataset demonstrate that the proposed PENME method handles various types of video, outperforming traditional communication, hybrid, and adaptive bitrate semantic communication techniques, achieving 40% lower latency, 90% less transmitted data, and 35% higher throughput. For semantic communication metrics, PENME improves PSNR by about 40%, increases MS-SSIM by roughly 19%, and reduces LPIPS by nearly 35%, compared with the baseline methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning](https://arxiv.org/abs/2512.14709)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: 该论文提出将Transformer的自注意力机制解释为近似的向量符号架构(VSA)，将查询/键视为角色空间，值视为填充物，注意力权重执行软解绑定，残差连接实现多绑定结构的叠加，为理解Transformer的推理行为和失败模式提供了代数视角。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型展现出类似推理的行为，但在需要稳定符号操作的任务上仍然脆弱。为了理解这些现象并改进模型的逻辑可靠性，需要建立统一的理论框架来解释Transformer的内部工作机制。

Method: 1. 将自注意力和残差流解释为近似的向量符号架构(VSA)；2. 提出VSA启发的架构偏置，包括显式的绑定/解绑定头和超维内存层；3. 设计促进角色-填充物分离和鲁棒叠加的训练目标；4. 提出衡量"VSA相似度"和逻辑组合性的度量标准。

Result: 该框架能够解释Transformer的推理行为（如思维链、基于程序的推理、内存增强工具使用）和典型失败模式（如变量混淆、逻辑相关提示间的不一致性），并为构建更可解释和逻辑可靠的推理系统提供了理论指导。

Conclusion: 将注意力视为软向量符号计算为理解Transformer的推理能力提供了统一的代数视角，指出了构建更可解释和逻辑可靠推理系统的原则性路径，并提出了理论和架构上的开放性问题。

Abstract: Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring "VSA-likeness" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.

</details>


### [7] [GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge](https://arxiv.org/abs/2512.14766)
*Dongzhuoran Zhou,Yuqicheng Zhu,Xiaxia Wang,Hongkuan Zhou,Jiaoyan Chen,Steffen Staab,Yuan He,Evgeny Kharlamov*

Main category: cs.AI

TL;DR: 论文提出了一种在知识图谱不完整情况下构建基准测试的方法，并开发了自适应图推理智能体（GR-Agent）来解决不完整知识图谱上的问答问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答（KGQA）基准测试通常假设知识图谱是完整的，包含直接支持答案的三元组。然而现实中知识图谱往往不完整，许多事实缺失，需要从现有事实进行推理。这种假设导致评估过于简化，忽略了实际应用中的推理需求。

Method: 1. 提出构建不完整知识图谱基准测试的方法论：移除直接支持的三元组，同时保留推理答案所需的替代推理路径。2. 开发自适应图推理智能体（GR-Agent）：首先从知识图谱构建交互环境，然后将KGQA形式化为智能体与环境在该环境中的交互。GR-Agent在包含图推理工具的动作空间上操作，并维护潜在支持推理证据的记忆，包括相关关系和推理路径。

Result: 实验表明：1. 使用该方法构建的基准测试显示，现有方法在不完整性下性能持续下降，突显其推理能力有限。2. GR-Agent在完整和不完整设置下都优于非训练基线方法，与基于训练的方法表现相当。

Conclusion: 论文填补了知识图谱不完整性评估的空白，提出的基准构建方法能更真实地评估KGQA系统的推理能力。GR-Agent通过将KGQA形式化为智能体环境交互，有效处理不完整知识图谱，展示了强大的推理能力。

Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.

</details>


### [8] [IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection](https://arxiv.org/abs/2512.14792)
*Roman Nekrasov,Stefano Fossati,Indika Kumara,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.AI

TL;DR: 该研究通过系统注入结构化配置知识，显著提升了LLM生成Terraform IaC代码的成功率，从27.1%提高到62.6%，但发现LLM在技术正确性和意图对齐之间存在差距。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在生成正确且意图对齐的基础设施即代码（IaC）方面成功率较低，特别是在Terraform领域。研究旨在探索如何通过结构化配置知识注入来改善LLM的IaC生成能力。

Method: 1. 增强IaC-Eval基准测试，加入云仿真和自动化错误分析；2. 开发LLM辅助IaC代码生成的错误分类法；3. 实施一系列知识注入技术，从朴素检索增强生成（RAG）到更复杂的图RAG方法，包括图组件的语义增强和资源间依赖关系建模。

Result: 基线LLM性能较差（总体成功率27.1%），注入结构化配置知识后，技术验证成功率提高到75.3%，总体成功率提高到62.6%。但意图对齐方面出现平台期，揭示了"正确性-一致性差距"。

Conclusion: 虽然结构化知识注入显著提升了LLM生成IaC代码的技术正确性，但LLM在理解复杂用户意图方面仍有局限，表明它们可以成为熟练的"编码者"，但在作为"架构师"满足细微用户意图方面仍有不足。

Abstract: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a "Correctness-Congruence Gap" where LLMs can become proficient "coders" but remain limited "architects" in fulfilling nuanced user intent.

</details>


### [9] [AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally](https://arxiv.org/abs/2512.14910)
*Nadine Angela Cantonjos,Arpita Biswas*

Main category: cs.AI

TL;DR: AgroAskAI是一个用于农业气候适应决策支持的多智能体推理系统，采用模块化架构协调专业智能体，集成实时工具和数据，提供本地化、可操作的气候适应策略。


<details>
  <summary>Details</summary>
Motivation: 农村农业地区面临干旱、强降雨和天气模式变化等气候相关风险的损害。现有系统多为单智能体模型或仅用于静态功能的多智能体框架，缺乏支持动态协作推理和情境感知输出的架构。需要为脆弱农村社区提供适应性风险管理解决方案和决策支持策略。

Method: 提出AgroAskAI多智能体推理系统，采用模块化、角色专业化的架构，使用责任链方法协调自主智能体，集成实时工具和数据集。系统内置治理机制减少幻觉，支持内部反馈，提供连贯的本地相关策略，并支持多语言交互。

Result: 实验表明，通过额外工具和提示优化，AgroAskAI在常见农业气候适应查询中能提供更可操作、更接地气、更具包容性的输出。结果突显了智能体AI在农业气候适应中可持续和负责任决策支持的潜力。

Conclusion: AgroAskAI展示了智能体AI在农业气候适应决策支持中的前景，通过多智能体协作推理、实时数据集成和治理机制，为脆弱农村社区提供本地化、可操作的气候适应策略。

Abstract: Agricultural regions in rural areas face damage from climate-related risks, including droughts, heavy rainfall, and shifting weather patterns. Prior research calls for adaptive risk-management solutions and decision-making strategies. To this end, artificial intelligence (AI), particularly agentic AI, offers a promising path forward. Agentic AI systems consist of autonomous, specialized agents capable of solving complex, dynamic tasks. While past systems have relied on single-agent models or have used multi-agent frameworks only for static functions, there is a growing need for architectures that support dynamic collaborative reasoning and context-aware outputs. To bridge this gap, we present AgroAskAI, a multi-agent reasoning system for climate adaptation decision support in agriculture, with a focus on vulnerable rural communities. AgroAskAI features a modular, role-specialized architecture that uses a chain-of-responsibility approach to coordinate autonomous agents, integrating real-time tools and datasets. The system has built-in governance mechanisms that mitigate hallucination and enable internal feedback for coherent, locally relevant strategies. The system also supports multilingual interactions, making it accessible to non-English-speaking farmers. Experiments on common agricultural queries related to climate adaptation show that, with additional tools and prompt refinement, AgroAskAI delivers more actionable, grounded, and inclusive outputs. Our experimental results highlight the potential of agentic AI for sustainable and accountable decision support in climate adaptation for agriculture.

</details>


### [10] [Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation](https://arxiv.org/abs/2512.15033)
*Xidan Song,Weiqi Wang,Ruifeng Cao,Qingya Hu*

Main category: cs.AI

TL;DR: 论文提出几何稳定性框架，发现LLMs在象棋推理中存在准确率-稳定性悖论：高准确率模型在几何变换下表现崩溃，揭示其依赖模式匹配而非抽象空间逻辑。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在复杂推理领域的评估主要依赖与基准模型的准确率对齐，但这种标量准确率无法区分真正的几何推理与对典型棋盘状态的表面记忆。需要新的评估方法来测试模型在几何变换下的稳定性。

Method: 提出几何稳定性框架，通过不变变换（棋盘旋转、镜像对称、颜色反转、格式转换）来严格测试模型一致性。对6个最先进的LLMs（包括GPT-5.1、Claude Sonnet 4.5、Kimi K2 Turbo等）使用约3000个位置的数据集进行对比分析。

Result: 发现显著的准确率-稳定性悖论：GPT-5.1在标准位置上接近最优准确率，但在几何扰动下表现灾难性退化，旋转任务中错误率飙升超过600%。Claude Sonnet 4.5和Kimi K2 Turbo表现出卓越的双重鲁棒性。Gemini 2.5 Flash在非法状态拒绝方面领先（96.0%）。

Conclusion: 几何稳定性为AI评估提供了正交且必要的度量标准，能够有效区分推理能力与数据污染和过拟合，是评估大规模模型真实理解能力的重要代理指标。

Abstract: The evaluation of Large Language Models (LLMs) in complex reasoning domains typically relies on performance alignment with ground-truth oracles. In the domain of chess, this standard manifests as accuracy benchmarks against strong engines like Stockfish. However, high scalar accuracy does not necessarily imply robust conceptual understanding. This paper argues that standard accuracy metrics fail to distinguish between genuine geometric reasoning and the superficial memorization of canonical board states. To address this gap, we propose a Geometric Stability Framework, a novel evaluation methodology that rigorously tests model consistency under invariant transformations-including board rotation, mirror symmetry, color inversion, and format conversion. We applied this framework to a comparative analysis of six state-of-the-art LLMs including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, utilizing a dataset of approximately 3,000 positions. Our results reveal a significant Accuracy-Stability Paradox. While models such as GPT-5.1 achieve near-optimal accuracy on standard positions, they exhibit catastrophic degradation under geometric perturbation, specifically in rotation tasks where error rates surge by over 600%. This disparity suggests a reliance on pattern matching over abstract spatial logic. Conversely, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, we analyze the trade-off between helpfulness and safety, identifying Gemini 2.5 Flash as the leader in illegal state rejection (96.0%). We conclude that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.

</details>


### [11] [Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study](https://arxiv.org/abs/2512.15044)
*Wenwen Xie,Geng Sun,Ruichen Zhang,Xuejie Liu,Yinqiu Liu,Jiacheng Wang,Dusit Niyato,Ping Zhang*

Main category: cs.AI

TL;DR: 该论文探讨了智能体人工智能在集成感知与通信系统中的应用，提出了一个基于智能体AI的ISAC框架，并通过案例研究验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 随着无线环境日益动态复杂，ISAC系统需要更智能的处理和更自主的操作来保持效率和适应性，而智能体AI通过感知-推理-行动的连续循环为解决这些挑战提供了可行方案。

Method: 首先全面回顾智能体AI和ISAC系统的关键特性，展示ISAC系统的常见优化方法，突出基于生成式AI的智能体AI的优势，然后提出新颖的智能体ISAC框架并通过案例研究验证其性能。

Result: 提出了一个基于智能体AI的ISAC框架，并通过案例研究验证了其在优化ISAC性能方面的优越性。

Conclusion: 智能体AI为ISAC系统提供了智能、自主和高效操作的解决方案，论文明确了基于智能体AI的ISAC系统未来的研究方向。

Abstract: Integrated sensing and communication (ISAC) has emerged as a key development direction in the sixth-generation (6G) era, which provides essential support for the collaborative sensing and communication of future intelligent networks. However, as wireless environments become increasingly dynamic and complex, ISAC systems require more intelligent processing and more autonomous operation to maintain efficiency and adaptability. Meanwhile, agentic artificial intelligence (AI) offers a feasible solution to address these challenges by enabling continuous perception-reasoning-action loops in dynamic environments to support intelligent, autonomous, and efficient operation for ISAC systems. As such, we delve into the application value and prospects of agentic AI in ISAC systems in this work. Firstly, we provide a comprehensive review of agentic AI and ISAC systems to demonstrate their key characteristics. Secondly, we show several common optimization approaches for ISAC systems and highlight the significant advantages of generative artificial intelligence (GenAI)-based agentic AI. Thirdly, we propose a novel agentic ISAC framework and prensent a case study to verify its superiority in optimizing ISAC performance. Finally, we clarify future research directions for agentic AI-based ISAC systems.

</details>


### [12] [LADY: Linear Attention for Autonomous Driving Efficiency without Transformers](https://arxiv.org/abs/2512.15038)
*Jihao Huang,Xi Xia,Zhiyuan Li,Tianle Liu,Jingke Wang,Junbo Chen,Tengju Ye*

Main category: cs.AI

TL;DR: LADY是首个完全基于线性注意力的端到端自动驾驶生成模型，通过恒定计算和内存成本实现长程时空建模，在边缘设备上验证了实用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的端到端自动驾驶方法存在二次注意力成本问题，限制了长时空序列建模能力，特别是在资源受限的边缘平台上。线性注意力机制虽然计算效率高，但现有架构仅限于自注意力，缺乏对自动驾驶至关重要的跨模态和跨时间交互支持。

Method: 提出LADY模型：1）采用完全线性注意力架构，实现恒定计算和内存成本的长程时间上下文融合；2）引入轻量级线性交叉注意力机制，支持有效的跨模态信息交换；3）在推理时无论相机和LiDAR特征历史长度如何，都能保持恒定复杂度。

Result: 在NAVSIM和Bench2Drive基准测试中达到最先进性能，同时保持恒定时间和内存复杂度，显著降低了计算成本。模型已成功部署到边缘设备，在资源受限场景中验证了实用性。

Conclusion: LADY通过完全线性注意力架构解决了自动驾驶中长时空建模的计算效率问题，实现了高性能与低计算成本的平衡，为边缘设备上的实时自动驾驶部署提供了可行方案。

Abstract: End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.

</details>


### [13] [Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models](https://arxiv.org/abs/2512.15089)
*Jinwu Hu,Dongjin Yang,Langyu Bian,Zhiquan Wen,Yufeng Wang,Yaofo Chen,Bin Xiao,Yuanqing Li,Mingkui Tan*

Main category: cs.AI

TL;DR: CogER是一个受人类分层推理启发的弹性推理框架，通过动态选择最适合每个查询的推理策略来平衡推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理策略主要依赖模型自身的快慢模式（如o1思考），难以在不同难度查询间平衡推理效率和准确性。

Method: 1) 评估查询复杂度并分配到预定义级别；2) 将策略选择建模为马尔可夫决策过程，使用强化学习训练CogER-Agent；3) 引入认知工具辅助推理，允许LLM在思维链中自主调用外部工具。

Result: CogER在领域内任务上相对改进至少13%的平均精确匹配，在领域外任务上相对增益8%，优于最先进的测试时缩放方法。

Conclusion: CogER通过动态策略选择和工具集成，有效解决了LLM推理中效率与准确性的平衡问题，为复杂推理任务提供了更优解决方案。

Abstract: Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.

</details>


### [14] [A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem](https://arxiv.org/abs/2512.15198)
*Mohsen Nafar,Michael Römer,Lin Xie*

Main category: cs.AI

TL;DR: 提出基于聚类的变量排序框架，通过将变量分组来缩小动态排序的搜索空间，在MWISP问题上显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 松弛决策图(DDs)在离散优化中提供对偶界，但其质量严重依赖变量排序和合并决策。动态变量排序虽然能收紧界，但在全局评估时计算开销大，需要在界质量和计算效率之间权衡。

Method: 提出聚类框架：先将变量分区成簇，再基于结构分解指导排序。研究两种策略：1) Cluster-to-Cluster：按问题特定聚合准则（如MWISP中的累计顶点权重）顺序处理簇；2) Pick-and-Sort：从每个簇迭代选择和排序代表性变量以平衡局部多样性和启发式指导。针对MWISP推导DD大小增长的理论结果，提出两种设置簇数量的策略。

Result: 将策略嵌入基于DD的分支定界算法，在MWISP基准实例上评估。相比标准动态变量排序基线，提出的方法持续降低计算成本。

Conclusion: 基于聚类的变量排序框架有效解决了动态排序的计算开销问题，通过缩小搜索空间在MWISP问题上实现了更好的计算效率。

Abstract: Efficient exact algorithms for Discrete Optimization (DO) rely heavily on strong primal and dual bounds. Relaxed Decision Diagrams (DDs) provide a versatile mechanism for deriving such dual bounds by compactly over-approximating the solution space through node merging. However, the quality of these relaxed diagrams, i.e. the tightness of the resulting dual bounds, depends critically on the variable ordering and the merging decisions executed during compilation. While dynamic variable ordering heuristics effectively tighten bounds, they often incur computational overhead when evaluated globally across the entire variable set. To mitigate this trade-off, this work introduces a novel clustering-based framework for variable ordering. Instead of applying dynamic ordering heuristics to the full set of unfixed variables, we first partition variables into clusters. We then leverage this structural decomposition to guide the ordering process, significantly reducing the heuristic's search space. Within this framework, we investigate two distinct strategies: Cluster-to-Cluster, which processes clusters sequentially using problem-specific aggregate criteria (such as cumulative vertex weights in the Maximum Weighted Independent Set Problem (MWISP)), and Pick-and-Sort, which iteratively selects and sorts representative variables from each cluster to balance local diversity with heuristic guidance. Later on, developing some theoretical results on the growth of the size of DDs for MWISP we propose two different policies for setting the number of clusters within the proposed framework. We embed these strategies into a DD-based branch-and-bound algorithm and evaluate them on the MWISP. Across benchmark instances, the proposed methodology consistently reduces computational costs compared to standard dynamic variable ordering baseline.

</details>


### [15] [CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications](https://arxiv.org/abs/2512.15231)
*Zhengchao Chen,Haoran Wang,Jing Yao,Pedram Ghamisi,Jun Zhou,Peter M. Atkinson,Bing Zhang*

Main category: cs.AI

TL;DR: CangLing-KnowFlow是一个统一智能代理框架，用于自动化处理遥感数据，通过知识库、动态工作流调整和进化记忆模块，在复杂地球观测任务中实现可靠的自适应学习。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据处理系统通常针对特定任务，缺乏统一框架来管理从数据预处理到高级解释的端到端工作流。需要一种能够适应多样化遥感应用、减少幻觉并可靠运行的自动化解决方案。

Method: 提出CangLing-KnowFlow框架，包含三个核心组件：1）程序知识库（PKB）- 包含1,008个专家验证的工作流案例，覆盖162个实际遥感任务；2）动态工作流调整 - 运行时故障时自主诊断和重新规划恢复策略；3）进化记忆模块 - 从事件中持续学习，迭代增强代理知识和性能。

Result: 在KnowFlow-Bench基准测试（包含324个工作流）上评估，使用13个大型语言模型作为后端。在所有复杂任务中，CangLing-KnowFlow的任务成功率比Reflexion基线至少高出4%，展示了其作为稳健、高效、可扩展自动化解决方案的潜力。

Conclusion: CangLing-KnowFlow通过将专家知识转化为自适应且可验证的程序，展示了作为解决复杂地球观测挑战的可靠自动化解决方案的巨大潜力，是该新兴领域首次最全面的验证。

Abstract: The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).

</details>


### [16] [Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis](https://arxiv.org/abs/2512.15295)
*Toshihide Ubukata,Enhong Mu,Takuto Yamauchi,Mingyue Zhang,Jialong Li,Kenji Tei*

Main category: cs.AI

TL;DR: 提出GCRL方法，通过图神经网络增强强化学习在控制器合成中的探索策略，相比现有方法在多数基准域中表现出更好的学习效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 控制器合成中现有探索策略通常依赖固定规则或仅考虑有限当前特征的强化学习方法，缺乏对历史探索信息的充分利用，限制了合成效率。

Method: 提出GCRL方法，将LTS探索历史编码为图结构，利用图神经网络捕捉更广泛的非当前上下文信息，增强RL-based方法的探索策略。

Result: 在五个基准域中的四个，GCRL相比最先进方法表现出更优的学习效率和泛化能力，仅在一个具有高对称性和严格局部交互特性的特定域中表现不佳。

Conclusion: GCRL通过图神经网络整合历史探索信息，有效提升了控制器合成的效率，为基于强化学习的控制器合成提供了新的改进方向。

Abstract: Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or strategies learned through reinforcement learning (RL) that consider only a limited set of current features. To address this limitation, this paper introduces GCRL, an approach that enhances RL-based methods by integrating Graph Neural Networks (GNNs). GCRL encodes the history of LTS exploration into a graph structure, allowing it to capture a broader, non-current-based context. In a comparative experiment against state-of-the-art methods, GCRL exhibited superior learning efficiency and generalization across four out of five benchmark domains, except one particular domain characterized by high symmetry and strictly local interactions.

</details>


### [17] [ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I](https://arxiv.org/abs/2512.15298)
*Seok-Hyun Ga,Chun-Yen Chang*

Main category: cs.AI

TL;DR: 研究分析GPT-4o、Gemini 2.5等大型语言模型在韩国高考地球科学试题中的表现，发现模型存在感知-认知鸿沟、计算-概念化差异等认知局限，为设计"AI抗性"试题提供依据。


<details>
  <summary>Details</summary>
Motivation: 随着学生使用AI完成作业的现象日益普遍，学术诚信和评估有效性受到威胁。本研究旨在深入分析先进LLM在多模态科学推理方面的能力和认知局限，为应对AI在学术评估中的挑战提供实证基础。

Method: 使用2025年韩国高考地球科学I试题，测试GPT-4o、Gemini 2.5 Flash和Gemini 2.5 Pro三种模型。设计三种实验条件：整页输入、单项输入和优化多模态输入，评估不同数据结构下的模型表现。进行定量和定性分析，识别模型的具体认知缺陷。

Result: 非结构化输入导致性能显著下降（由于分割和OCR失败）。即使在优化条件下，模型仍表现出基本推理缺陷。定性分析发现"感知错误"占主导，存在"感知-认知鸿沟"（模型能识别视觉数据但无法解释示意图的符号意义）、"计算-概念化差异"（能执行计算但无法应用科学概念）和"过程幻觉"（跳过视觉验证而依赖背景知识）。

Conclusion: 通过利用AI的认知弱点（特别是感知与认知之间的鸿沟），教育工作者可以设计针对性的"AI抗性"问题，有效区分真实学生能力与AI生成答案，确保评估的公平性。这为解决课程作业中未经授权使用AI的挑战提供了可行方案。

Abstract: The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that "Perception Errors" were dominant, highlighting a "Perception-Cognition Gap" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a "Calculation-Conceptualization Discrepancy," successfully performing calculations while failing to apply the underlying scientific concepts, and "Process Hallucination," where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing "AI-resistant questions" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.

</details>


### [18] [SCOPE: Prompt Evolution for Enhancing Agent Effectiveness](https://arxiv.org/abs/2512.15374)
*Zehua Pei,Hui-Ling Zhen,Shixiong Kai,Sinno Jialin Pan,Yunhe Wang,Mingxuan Yuan,Bei Yu*

Main category: cs.AI

TL;DR: SCOPE通过自动化的提示演化机制解决LLM代理在动态上下文环境中的管理瓶颈，将上下文管理视为在线优化问题，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: LLM代理在动态环境中面临上下文管理瓶颈，静态提示无法有效处理大规模动态上下文，导致频繁的纠正和增强失败。

Method: 提出SCOPE框架，将上下文管理视为在线优化问题，通过双流机制平衡战术特异性（解决即时错误）和战略通用性（演化长期原则），并引入视角驱动探索最大化策略覆盖。

Result: 在HLE基准测试中，SCOPE将任务成功率从14.23%提升到38.64%，无需人工干预。

Conclusion: SCOPE通过自动化的提示演化有效解决了LLM代理在动态上下文环境中的管理问题，显著提升了代理的性能和适应性。

Abstract: Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\% to 38.64\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.

</details>


### [19] [Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations](https://arxiv.org/abs/2512.15388)
*Reinhard Moratz,Niklas Daute,James Ondieki,Markus Kattenbeck,Mario Krajina,Ioannis Giannopoulos*

Main category: cs.AI

TL;DR: 该论文通过定性空间关系增强大语言模型为行人提供路线指引的能力


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在提供行人路线指引时，缺乏对定性空间关系的理解和运用，这限制了其在实际导航场景中的实用性和准确性

Method: 通过整合定性空间关系（如方位、距离、拓扑关系等）到LLM中，改进模型生成路线指引的能力

Result: 改进后的模型能够生成更准确、更符合人类认知的路线指引，提高了导航指令的质量和实用性

Conclusion: 将定性空间关系整合到大语言模型中能显著提升其行人导航能力，为智能导航系统的发展提供了新方向

Abstract: This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.

</details>


### [20] [Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat](https://arxiv.org/abs/2512.15435)
*Stefan Edelkamp*

Main category: cs.AI

TL;DR: 提出一个基于自学习框架的纸牌游戏AI系统，通过扩展人类专家游戏数据库并融合AI自对弈数据来提升决策预测准确性，在Skat游戏中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 在多玩家纸牌游戏（如Skat或桥牌）中，早期阶段（如叫牌、游戏选择和初始出牌）对游戏成功至关重要，但当前计算限制下这些决策主要依赖人类专家游戏的统计信息，存在局限性。

Method: 提出通用的自学习框架，通过扩展人类游戏数据库，加入数百万局AI自对弈游戏来生成和融合统计信息；使用完美特征哈希函数处理压缩表，实现持续自我改进的游戏引擎。

Result: 在Skat游戏中的案例研究表明，该自动化方法能够有效支持游戏中的各种决策，提升了预测准确性。

Conclusion: 通过自学习框架扩展游戏数据库并融合AI自对弈数据，可以显著提升纸牌游戏早期决策的准确性，为多玩家纸牌游戏AI提供了有效的改进方案。

Abstract: In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.

</details>


### [21] [Intent-Driven UAM Rescheduling](https://arxiv.org/abs/2512.15462)
*Jeongseok Kim,Kangjin Kim*

Main category: cs.AI

TL;DR: 本文提出一个结合ASP和MILP的集成框架，用于处理UAM垂直起降场动态调度需求，支持模糊的人类重调度请求，提供可解释的自适应调度方案。


<details>
  <summary>Details</summary>
Motivation: 城市空中交通(UAM)垂直起降场资源有限，需要高效调度。现有调度方法难以同时处理动态操作需求和人类模糊的重调度请求，需要更灵活、可解释的调度系统。

Method: 使用混合整数线性规划(MILP)建模资源受限项目调度问题(RCPSP)，结合三值逻辑解释模糊用户意图和决策树，提出集成答案集编程(ASP)和MILP的新系统框架。

Result: 开发了一个集成框架，能够优化调度方案并透明支持人类输入，为UAM调度提供了鲁棒、可解释、自适应的结构。

Conclusion: 提出的ASP-MILP集成系统有效解决了UAM垂直起降场调度中的动态需求和模糊请求问题，实现了可解释的自适应调度，提升了调度系统的灵活性和透明度。

Abstract: Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.

</details>


### [22] [Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision](https://arxiv.org/abs/2512.15489)
*Wei Du,Shubham Toshniwal,Branislav Kisacanin,Sadegh Mahdavi,Ivan Moshkov,George Armstrong,Stephen Ge,Edgar Minasyan,Feng Chen,Igor Gitman*

Main category: cs.AI

TL;DR: Nemotron-Math是一个包含750万条数学推理轨迹的大规模数据集，整合了AoPS竞赛题和StackExchange数学问题，支持多种推理模式和Python工具集成，通过顺序分桶策略加速长上下文训练，在数学竞赛基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据集在推理风格多样性、长形式轨迹和工具集成方面存在局限，需要更高质量、更大规模的监督数据来提升数学推理模型的性能。

Method: 利用GPT-OSS-120B的多模式生成能力，创建包含高、中、低三种推理模式的7.5M解决方案轨迹，整合85K AoPS问题和262K StackExchange数学问题，开发顺序分桶策略加速128K上下文长度的微调。

Result: Nemotron-Math在匹配的AoPS问题上持续优于原始OpenMathReasoning，StackExchange-Math的加入显著提升了鲁棒性和泛化能力，在AIME 2024和2025上使用Python TIR实现了100% maj@16准确率。

Conclusion: Nemotron-Math通过大规模、多样化的数学推理数据集和高效的训练策略，实现了数学推理任务的SOTA性能，为高质量数学推理监督提供了重要资源。

Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).
  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.
  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.
  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.

</details>


### [23] [Evaluating Large Language Models in Scientific Discovery](https://arxiv.org/abs/2512.15567)
*Zhangde Song,Jieyu Lu,Yuanqi Du,Botao Yu,Thomas M. Pruyn,Yue Huang,Kehan Guo,Xiuzhe Luo,Yuanhao Qu,Yi Qu,Yinkai Wang,Haorui Wang,Jeff Guo,Jingru Gan,Parshin Shojaee,Di Luo,Andres M Bran,Gen Li,Qiyuan Zhao,Shao-Xiong Lennon Luo,Yuxuan Zhang,Xiang Zou,Wanru Zhao,Yifan F. Zhang,Wucheng Zhang,Shunan Zheng,Saiyang Zhang,Sartaaj Takrim Khan,Mahyar Rajabi-Kochi,Samantha Paradi-Maropakis,Tony Baltoiu,Fengyu Xie,Tianyang Chen,Kexin Huang,Weiliang Luo,Meijing Fang,Xin Yang,Lixue Cheng,Jiajun He,Soha Hassoun,Xiangliang Zhang,Wei Wang,Chandan K. Reddy,Chao Zhang,Zhiling Zheng,Mengdi Wang,Le Cong,Carla P. Gomes,Chang-Yu Hsieh,Aditya Nandy,Philippe Schwaller,Heather J. Kulik,Haojun Jia,Huan Sun,Seyed Mohamad Moosavi,Chenru Duan*

Main category: cs.AI

TL;DR: 该论文提出了一个科学发现评估框架，通过领域专家定义的真实研究项目和模块化研究场景来评估大语言模型在科学发现中的能力，发现现有模型在科学发现方面存在系统性弱点。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准测试主要评估去语境化的知识，而忽视了科学发现所需的迭代推理、假设生成和观察解释等关键能力，因此需要开发更贴近真实科学发现过程的评估框架。

Method: 提出了一个两阶段科学发现评估框架：1）在问题层面评估模型在具体研究场景中的准确性；2）在项目层面评估模型提出可测试假设、设计实验/模拟以及解释结果的能力。该框架涵盖生物学、化学、材料和物理等领域，由领域专家定义真实研究项目并分解为模块化研究场景。

Result: 评估显示：1）与通用科学基准相比，大语言模型在科学发现任务上存在一致的性能差距；2）模型规模和推理能力的扩展带来的收益递减；3）不同提供商的最佳模型存在系统性弱点；4）研究场景间的性能差异导致不同科学发现项目的最佳模型选择会变化。

Conclusion: 当前所有大语言模型距离通用科学"超级智能"还很遥远，但它们在许多科学发现项目中已显示出潜力，即使构成场景得分较低。该评估框架为发现相关的模型评估提供了可复现的基准，并为推动大语言模型在科学发现方面的发展指明了实用路径。

Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.

</details>


### [24] [A Decision-Theoretic Approach for Managing Misalignment](https://arxiv.org/abs/2512.15584)
*Daniel A. Herrmann,Abinav Chari,Isabelle Qian,Sree Sharvesh,B. A. Levinstein*

Main category: cs.AI

TL;DR: 论文提出了一个决策理论框架，分析在价值对齐不完全的情况下何时应该将决策委托给AI系统，区分了通用委托和上下文特定委托的不同要求。


<details>
  <summary>Details</summary>
Motivation: 当前价值对齐文献主要关注如何塑造AI的价值观，但较少研究在不确定性下，如何确定不完全对齐何时足够好以证明委托决策的合理性。需要一种原则性方法来评估委托的风险和收益。

Method: 引入了一个正式的决策理论框架，精确分析价值（不对）齐、认知准确性和可达性（可用行为）之间的权衡。开发了一个新颖的评分框架来量化这种事前决策。

Result: 分析揭示了两种委托情景的明显区别：通用委托需要近乎完美的价值对齐和完全的认知信任，而上下文特定委托即使在显著不对齐的情况下也可能是最优的。AI的更高准确性或更广泛的可达性可以提供更好的整体决策问题。

Conclusion: 该工作提供了确定AI在特定上下文中是否足够对齐的原则性方法，将重点从实现完美对齐转向在不确定性下管理委托的风险和回报。

Abstract: When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.

</details>


### [25] [Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning](https://arxiv.org/abs/2512.15662)
*Jiaqi Xu,Cuiling Lan,Xuejin Chen,Yan LU*

Main category: cs.AI

TL;DR: STC框架将推理与自我批判在每一步交织，通过混合强化学习目标优化推理质量和自我评估，在数学推理基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型将推理与验证分离：要么生成推理而不进行自我检查，要么依赖外部验证器事后检测错误。前者缺乏即时反馈，后者增加系统复杂性并阻碍同步学习。受人类批判性思维启发，需要统一框架。

Method: 提出Stepwise Think-Critique (STC)框架，在单个模型内每一步交织推理和自我批判。使用混合强化学习目标训练，结合推理奖励和批判一致性奖励，共同优化推理质量和自我评估。

Result: 在数学推理基准测试中，STC展现出强大的批判性思维能力，产生更可解释的推理轨迹，朝着具有内置批判性思维的大语言模型迈进一步。

Conclusion: STC框架通过将推理与自我批判交织在每一步，实现了更接近人类批判性思维的大语言模型能力，提高了推理质量和可解释性。

Abstract: Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.

</details>


### [26] [Explaining the Reasoning of Large Language Models Using Attribution Graphs](https://arxiv.org/abs/2512.15663)
*Chase Walker,Rickard Ewetz*

Main category: cs.AI

TL;DR: CAGE框架通过构建有向归因图，量化每个生成token如何受提示和先前生成的影响，相比现有方法提升归因忠实度达40%


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然能力强大但推理过程不透明，存在安全和信任问题。现有的上下文归因方法只关注生成token与提示的直接关系，忽略了生成token之间的相互影响，导致解释不完整。

Method: 提出CAGE框架，构建有向归因图来量化每个生成token如何受提示和所有先前生成的影响。该图保持因果性和行随机性两个属性，通过沿图中路径边缘化中间贡献来计算上下文归因。

Result: 在多个模型、数据集、指标和方法上，CAGE显著提高了上下文归因的忠实度，平均提升高达40%。

Conclusion: CAGE通过考虑生成token间的相互影响，提供了更完整和忠实的LLM行为解释框架，解决了现有上下文归因方法的局限性。

Abstract: Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.

</details>


### [27] [Artism: AI-Driven Dual-Engine System for Art Generation and Critique](https://arxiv.org/abs/2512.15710)
*Shuai Liu,Yiqing Tian,Yang Chen,Mar Canet Sola*

Main category: cs.AI

TL;DR: 提出双引擎AI架构方法，通过AIDA（人工艺术家社交网络）和Ismism Machine（批判分析系统）探索艺术演化的潜在轨迹，利用深度学习与多智能体协作进行艺术史发展与概念创新模式的多维模拟。


<details>
  <summary>Details</summary>
Motivation: 传统单向批判模式在分析复杂艺术演化轨迹时存在局限，需要更智能、交互式的反思实践方法来探索艺术发展的潜在可能性与创新模式。

Method: 采用双引擎架构：AIDA作为人工艺术家社交网络，Ismism Machine作为批判分析系统，结合深度学习与多智能体协作技术，实现艺术史发展与概念创新模式的多维模拟。

Result: 目前正在当代艺术概念上进行实验研究，该方法实现了从传统单向批判向智能交互式反思实践的转变，为艺术计算分析提供了新可能性。

Conclusion: 基于AI驱动批判循环的通用方法论为艺术演化轨迹探索提供了新框架，通过双引擎架构实现了艺术发展的多维模拟与概念创新分析。

Abstract: This paper proposes a dual-engine AI architectural method designed to address the complex problem of exploring potential trajectories in the evolution of art. We present two interconnected components: AIDA (an artificial artist social network) and the Ismism Machine, a system for critical analysis. The core innovation lies in leveraging deep learning and multi-agent collaboration to enable multidimensional simulations of art historical developments and conceptual innovation patterns. The framework explores a shift from traditional unidirectional critique toward an intelligent, interactive mode of reflexive practice. We are currently applying this method in experimental studies on contemporary art concepts. This study introduces a general methodology based on AI-driven critical loops, offering new possibilities for computational analysis of art.

</details>


### [28] [Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants](https://arxiv.org/abs/2512.15712)
*Vincent Huang,Dami Choi,Daniel D. Johnson,Sarah Schwettmann,Jacob Steinhardt*

Main category: cs.AI

TL;DR: 提出Predictive Concept Decoder（PCD），通过端到端训练让解释性助手从神经网络激活中预测模型行为，使用通信瓶颈压缩激活为稀疏概念列表，在大型无监督数据上预训练后微调回答自然语言问题。


<details>
  <summary>Details</summary>
Motivation: 现有可扩展解释性方法依赖手工设计的代理来假设和测试内部激活与外部行为的关系，这种方法不够高效。需要一种能够自动从激活中提取有意义概念并预测模型行为的端到端方法。

Method: 1. 编码器将神经网络激活压缩为稀疏概念列表；2. 解码器读取概念列表并回答关于模型的自然语言问题；3. 在大规模无监督数据上预训练；4. 微调以回答特定问题；5. 形成Predictive Concept Decoder（PCD）架构。

Result: PCD具有良好的扩展性：瓶颈概念的自解释评分随数据增加而提高，下游应用性能也相应提升。能够检测越狱攻击、秘密提示和植入的潜在概念，并能准确揭示潜在用户属性。

Conclusion: 将解释性任务转化为端到端训练目标是可行的，Predictive Concept Decoder通过通信瓶颈和预训练-微调策略，实现了从神经网络激活中提取有意义概念并预测模型行为的可扩展解决方案。

Abstract: Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [29] [Low-Complexity Channel Estimation for Internet of Vehicles AFDM Communications With Sparse Bayesian Learning](https://arxiv.org/abs/2512.14776)
*Xiangxiang Li,Haiyan Wang,Yao Ge,Xiaohong Shen,Miaowen Wen,Shun Zhang,Yong Liang Guan*

Main category: cs.IT

TL;DR: 提出基于稀疏贝叶斯学习的AFDM信道估计框架，包含网格细化SBL和网格演化SBL两种离网估计方法，并开发分布式计算方案降低复杂度。


<details>
  <summary>Details</summary>
Motivation: AFDM在车联网中具有前景，但在双弥散信道中需要精确的信道估计来达到预期性能，现有方法面临挑战。

Method: 提出SBL框架，开发GR-SBL（局部网格细化）和GE-SBL（一阶线性近似网格演化）两种离网信道估计方法，并设计分布式计算方案D-GR-SBL和D-GE-SBL降低复杂度。

Result: 提出的信道估计器优于现有方案：GR-SBL以高复杂度实现高精度估计，GE-SBL在性能和复杂度间提供更好权衡，分布式版本有效降低复杂度并保持可比性能。

Conclusion: 提出的SBL框架和分布式计算方案为AFDM系统提供了高效的信道估计解决方案，在双弥散信道中实现了高可靠连接。

Abstract: Affine frequency division multiplexing (AFDM) has been considered as a promising waveform to enable high-reliable connectivity in the internet of vehicles. However, accurate channel estimation is critical and challenging to achieve the expected performance of the AFDM systems in doubly-dispersive channels. In this paper, we propose a sparse Bayesian learning (SBL) framework for AFDM systems and develop a dynamic grid update strategy with two off-grid channel estimation methods, i.e., grid-refinement SBL (GR-SBL) and grid-evolution SBL (GE-SBL) estimators. Specifically, the GR-SBL employs a localized grid refinement method and dynamically updates grid for a high-precision estimation. The GE-SBL estimator approximates the off-grid components via first-order linear approximation and enables gradual grid evolution for estimation accuracy enhancement. Furthermore, we develop a distributed computing scheme to decompose the large-dimensional channel estimation model into multiple manageable small-dimensional sub-models for complexity reduction of GR-SBL and GE-SBL, denoted as distributed GR-SBL (D-GR-SBL) and distributed GE-SBL (D-GE-SBL) estimators, which also support parallel processing to reduce the computational latency. Finally, simulation results demonstrate that the proposed channel estimators outperform existing competitive schemes. The GR-SBL estimator achieves high-precision estimation with fine step sizes at the cost of high complexity, while the GE-SBL estimator provides a better trade-off between performance and complexity. The proposed D-GR-SBL and D-GE-SBL estimators effectively reduce complexity and maintain comparable performance to GR-SBL and GE-SBL estimators, respectively.

</details>


### [30] [On the Stochastic Analysis of Random Linear Streaming Codes in Multi-Hop Relay Networks](https://arxiv.org/abs/2512.15049)
*Kai Huang,Xinyu Xie,Chunpeng Chen,Wenjie Guan,Xiaoran Wang,Jinbei Zhang*

Main category: cs.IT

TL;DR: 本文研究了多跳中继网络中大规模随机线性流码的随机性能极限，针对随机信道进行误差概率分析，将信息债务概念扩展到两跳网络，并推广到任意跳数网络。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注确定性对抗信道，仅考虑有限类型的擦除模式，旨在设计最优容量实现编码。本文聚焦于随机信道，其中每跳都受到独立同分布的包擦除，对多跳随机线性流码的误差概率进行随机分析。

Method: 1. 在两跳中继网络中，通过量化网络中每个节点的信息流，使用新颖框架描述大规模随机线性流码的误差事件；2. 基于误差事件，通过精心分析期望项，推导两跳网络中的平均误差概率表达式；3. 将两跳网络的结果推广到任意跳数的中继网络；4. 进行仿真验证分析准确性并与现有对抗信道流码比较。

Result: 1. 将点对点网络中的"信息债务"概念扩展到两跳中继网络；2. 通过构建具有嵌套结构的"带状"转移矩阵，处理两跳网络所有可能擦除模式的期望；3. 推导出两跳网络的平均误差概率表达式；4. 将结果推广到任意跳数的中继网络；5. 仿真验证了随机分析的准确性。

Conclusion: 本文成功分析了多跳中继网络中大规模随机线性流码的随机性能极限，将信息债务概念扩展到多跳网络，提供了随机信道下的误差概率分析方法，为随机信道下的流码设计提供了理论基础。

Abstract: In this paper, we aim to explore the stochastic performance limit of large-field-size Random Linear Streaming Codes (RLSCs) in multi-hop relay networks. In our model, a source transmits a sequence of streaming messages to a destination through multiple relays subject to a delay constraint. Most previous research focused on deterministic adversarial channel which introduces only restricted types of erasure patterns, and aimed to design the optimal capacity-achieving codes. In this paper, we focus on stochastic channel where each hop is subject to i.i.d. packet erasures, and carry out stochastic analysis on the error probability of multi-hop RLSCs. Our contributions are three-folds. Firstly, the error event of large-field-size RLSCs is characterized in two-hop relay network with a novel framework, which features quantification of information flowing through each node in the network. Due to the erasures in different hops, some source symbols can be "detained" at the source or relay while others have arrived at the destination. By iteratively computing the number of detained symbols at each node, this framework extends the concept "information debt" from point-to-point network [Pinwen Su et al. 2022] into two-hop relay networks. Secondly, based on the error event, the expression of average error probability in two-hop network is derived by carefully analyzing the expectation terms. To handle the expectation over all possible erasure patterns along two hops of the network, the transition matrices of the detained symbols are novelly constructed in a "band fashion" with nested structure. Thirdly, the derived results in two-hop network are further generalized into relay networks with arbitrary number of hops. Furthermore, simulations are conducted to verify the accuracy of our stochastic analysis, and compare with some existing streaming codes for the adversarial channels.

</details>


### [31] [Rotatable IRS-Assisted 6DMA Communications: A Two-timescale Design](https://arxiv.org/abs/2512.15092)
*Chao Zhou,Changsheng You,Cong Zhou,Liujia Yao,Weijie Yuan,Beixiong Zheng,Nan Wu*

Main category: cs.IT

TL;DR: 提出了一种结合可旋转智能反射面(R-IRS)和六维可移动天线(6DMA)的多功能天线/表面系统，采用双时间尺度协议优化天线配置和波束成形，显著提升多用户下行通信性能。


<details>
  <summary>Details</summary>
Motivation: 智能反射面(IRS)和可移动天线(MA)技术在实际应用中存在性能限制，需要结合两者的互补优势来增强无线通信能力。

Method: 部署可旋转IRS(R-IRS)配合配备6DMA的基站，采用双时间尺度传输协议：基于统计CSI优化天线配置和IRS旋转反射，基于瞬时CSI设计波束成形。针对多用户非凸优化问题，提出结合WMMSE和SSCA的高效算法。

Result: 单用户情况下，6DMA应形成稀疏阵列进行多波束传输，IRS旋转实现有效多径对齐。多用户情况下，所提算法验证了系统性能，通过联合利用6DMA-BS和R-IRS的空间自由度获得显著性能增益。

Conclusion: 提出的多功能天线/表面系统通过结合6DMA和R-IRS的互补优势，在双时间尺度协议下实现了显著的性能提升，为未来无线通信系统设计提供了有效解决方案。

Abstract: Intelligent reflecting surface (IRS) and movable antenna (MA) are promising technologies to enhance wireless communication by reconfiguring channels at the environment and transceiver sides. However, their performance is constrained by practical limitations. To address this, we propose a multi-functional antenna/surface system that leverages their complementary advantages. A rotatable IRS (R-IRS) is deployed to enhance downlink communications from a six-dimensional MA (6DMA)-equipped base station (BS) to multiple single-antenna users. To reduce the complexity of real-time channel estimation and beamforming, we formulate an optimization problem to maximize the average sum-rate using a two-timescale (TTS) transmission protocol. Specifically, the BS antenna configuration (including position and rotation) and IRS rotation and reflection are optimized based on statistical channel state information (S-CSI), while BS transmit beamforming is designed using instantaneous CSI (I-CSI) in the short timescale. We first consider a single-user case and show that the 6DMA at the BS should form a sparse array for multi-beam transmission towards both the IRS and the user, allowing efficient coordination of direct and reflected channels, while the IRS rotation achieves effective multi-path alignment. For the general multi-user case, the optimization problem is non-convex and challenging to solve. To tackle this, we propose an efficient algorithm combining weighted minimum mean-square error (WMMSE) and stochastic successive convex approximation (SSCA) techniques. A low-complexity algorithm is also proposed to reduce computational complexity. Numerical results validate the proposed system, showing significant performance gains by jointly exploiting the spatial degrees of freedom of the 6DMA-BS and R-IRS under the TTS protocol.

</details>


### [32] [Sparse Principal Component Analysis with Energy Profile Dependent Sample Complexity](https://arxiv.org/abs/2512.15191)
*Mengchu Xu,Jian Wang,Yonina C. Eldar*

Main category: cs.IT

TL;DR: SEP是一种自适应稀疏主成分分析方法，通过迭代筛选和重选坐标，样本复杂度能适应信号能量分布，在能量分布不均匀时优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏主成分分析方法主要针对能量均匀分布的"平坦尖峰"信号，当信号能量在支撑集上分布不均匀时缺乏有效指导，需要能适应不同能量分布的方法。

Method: 提出谱能量追踪(SEP)方法，通过迭代的坐标筛选和重选机制，利用结构函数s(p)量化尖峰能量在前p个条目上的累积，最后通过截断幂迭代进行后处理。

Result: SEP的样本复杂度为max_{1≤p≤k} p·s²(p)·log n，在平坦尖峰时退化为经典的k²log n，在能量集中时接近klog n。实验验证了SEP在平坦、幂律和指数信号上的优越性能。

Conclusion: SEP方法能自适应不同能量分布结构，无需调参，在样本有限的高维稀疏主成分分析中优于现有算法，为能量分布不均匀的信号提供了有效的恢复方案。

Abstract: We study sparse principal component analysis in the high-dimensional, sample-limited regime, aiming to recover a leading component supported on a few coordinates. Despite extensive progress, most methods and analyses are tailored to the flat-spike case, offering little guidance when spike energy is unevenly distributed across the support. Motivated by this, we propose Spectral Energy Pursuit (SEP), an effective iterative scheme that repeatedly screens and reselects coordinates, with a sample complexity that adapts to the energy profile. We develop our framework around a structure function \(s(p)\) that quantifies how spike energy accumulates over its top \(p\) entries. We establish that SEP succeeds with a sample size of order \(\max_{1\le p\le k} p\,s^2(p)\,\log n\), which matches the classical \(k^2\log n\) sample complexity for flat spikes and improves toward the \(k\log n\) regime as the profile becomes more concentrated. As a lightweight post-processing, a single truncated power iteration is proven to enable the final estimator to attain a uniform statistical error bound. Empirical simulations across flat, power-law, and exponential signals validate that SEP adapts to profile structure without tuning and outperforms existing algorithms.

</details>


### [33] [Symbol Detection in Ambient Backscatter Communications Under Residual Time Synchronization Errors](https://arxiv.org/abs/2512.15241)
*Yinghui Ye,Ying Li,Xiaoli Chu,Gan Zheng,Sumei Sun*

Main category: cs.IT

TL;DR: 提出一个考虑残留时间同步误差的AmBC符号检测框架，推导了BER表达式和近最优检测阈值


<details>
  <summary>Details</summary>
Motivation: 现有AmBC符号检测假设完美时间同步不切实际，残留时间同步误差导致部分样本失配，降低检测性能

Method: 提出包含当前和相邻符号以及信道系数的AmBC符号检测框架，以能量检测器为例推导BER表达式，提出利用接收信号样本特性的参数估计方法

Result: 残留时间同步误差显著降低能量检测器BER性能，推导出最小化BER的近最优检测阈值闭式表达式

Conclusion: 提出的框架能有效处理残留时间同步误差，通过近最优检测阈值和参数估计方法提升AmBC符号检测性能

Abstract: Ambient backscatter communications (AmBC), where a backscatter transmitter (BT) modulates and reflects ambient signals to a backscatter receiver (BR), have been deemed a low-power communication technology for the Internet of Things. Previous work on symbol detection in AmBC assumed perfect time synchronization (TS), which is unrealistic in practice. The residual TS errors (RTSE) cause \emph{partial sample mismatch}, degrading symbol detection performance. To address this, we propose a new AmBC symbol detection framework that incorporates the BT's current and adjacent symbols, as well as channel coefficients. Using energy detector (ED) as a case study, we derive both exact and approximate bit error rate (BER) expressions. Our results show that the ED's BER performance degrades significantly under RTSE, with the symbol detection threshold optimized under the assumption of perfect TS. We then derive a closed-form expression for a near-optimal symbol detection threshold that minimizes BER under RTSE. To estimate the required parameters for the detection threshold, we propose a novel method exploiting the attributes of the BR's received signal samples. The analytical results are verified by simulation results.

</details>


### [34] [Joint Activity Detection and Channel Estimation For Fluid Antenna System Exploiting Geographical and Angular Information](https://arxiv.org/abs/2512.15342)
*Zhentian Zhang,Jian Dang,David Morales-Jimenez,Hao Jiang,Zaichen Zhang,Christos Masouros,Chan-Byoung Chae*

Main category: cs.IT

TL;DR: 本文提出基于近似消息传递（AMP）与自适应期望最大化（EM）结合的EM-AMP框架，用于解决流体天线系统（FAS）中的信道估计问题，克服现有方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 流体天线系统（FAS）为未来通信网络提供物理层自由度，但信道状态信息（CSI）获取面临挑战。现有贪婪算法性能受信号假设影响，无模型方法计算复杂度过高，需要灵活、可行且低复杂度的解决方案来支持FAS的大规模连接。

Method: 提出基于近似消息传递（AMP）与自适应期望最大化（EM）结合的EM-AMP框架。该框架能够高效处理大规模矩阵计算并具有自适应学习能力，无需先验知识。引入两种利用FAS网络地理和角度特征的EM-AMP变体算法。

Result: 提出的算法在大型活动区域表现出更高的估计精度、快速收敛和低计算复杂度。数值结果验证了算法设计的有效性，并阐明了贪婪方法固有性能平台的原因以及角度信息在算法设计中的关键作用。

Conclusion: EM-AMP框架为FAS网络提供了灵活、可行且低复杂度的信道估计解决方案，克服了现有方法的局限性，为支持大规模连接提供了有效途径。

Abstract: The fluid antenna system (FAS) refers to a family of reconfigurable antenna technologies that provide substantial spatial gains within a compact, predefined small space, thereby offering extensive degrees of freedom in the physical layer for future communication networks. The acquisition of channel state information (CSI) is critical, as it determines the placement of ports/antennas, which directly impacts FAS-based optimization. Although various channel estimation methods have been developed, significant flaws persist. For instance, the performance of greedy-based algorithms is heavily influenced by signal assumptions, and current model-free methods are infeasible due to prohibitively high computational complexity issue. Consequently, there is a pressing need for a well-balanced solution that exhibits flexibility, feasibility, and low complexity to support massive connectivity in FAS. In this work, we propose methods based on approximate message passing (AMP) integrated with adaptive expectation maximization (EM). The EM-AMP framework uniquely enables efficient large matrix computations with adaptive learning capabilities, independent of prior knowledge of the model or parameters within potential distributions, making it a robust candidate for FAS networks. We introduce two variants of the EM-AMP framework that leverage geographical and angular features in a FAS network. These proposed algorithms demonstrate improved estimation precision, fast convergence, and low computational complexity in large activity regions. Additionally, we analytically elucidate the reasons behind the inherent performance floor of greedy-based methods and highlight the critical role of angular information in algorithm design. Extensive numerical results validate the promising efficacy of the proposed algorithm designs and the derived analytical findings.

</details>


### [35] [Three-Dimensional Radio Localization: A Channel Charting-Based Approach](https://arxiv.org/abs/2512.15399)
*Phillip Stephan,Florian Euchner,Stephan ten Brink*

Main category: cs.IT

TL;DR: 本文研究三维室内定位场景下的信道图表技术，提出两种方法：针对三维数据分布的增强信道图表，以及针对多层建筑的楼层分类+专家网络两阶段方法，并引入新的稀疏特征提取技术。


<details>
  <summary>Details</summary>
Motivation: 现有信道图表研究主要关注二维场景，但真实环境本质上是三维的。需要解决三维室内定位问题，包括工厂大厅的三维数据分布场景和多层建筑的楼层间定位挑战。

Method: 1) 针对三维数据分布场景：应用增强信道图表概念，结合传统定位和信道图表；2) 针对多层建筑：提出多楼层信道图表，采用两阶段方法：先通过聚类进行楼层分类，然后为每个楼层训练专门的专家神经网络；3) 提出新的特征工程方法，从波束空间信道状态信息中提取适合定位的稀疏特征。

Result: 在模拟但真实的射线追踪数据集上验证了两种三维室内定位场景的有效性。增强信道图表适用于工厂大厅等三维数据分布环境，而多楼层信道图表通过两阶段方法提升了多层建筑中的信道图表性能。

Conclusion: 信道图表技术可以扩展到三维室内定位场景。针对不同三维环境特点需要采用不同的方法：三维数据分布场景适合增强信道图表，多层建筑场景适合两阶段的多楼层信道图表方法。提出的稀疏特征提取技术有助于提升定位性能。

Abstract: Channel charting creates a low-dimensional representation of the radio environment in a self-supervised manner using manifold learning. Preserving relative spatial distances in the latent space, channel charting is well suited to support user localization. While prior work on channel charting has mainly focused on two-dimensional scenarios, real-world environments are inherently three-dimensional. In this work, we investigate two distinct three-dimensional indoor localization scenarios using simulated, but realistic ray tracing-based datasets: a factory hall with a three-dimensional spatial distribution of datapoints, and a multistory building where each floor exhibits a two-dimensional datapoint distribution. For the first scenario, we apply the concept of augmented channel charting, which combines classical localization and channel charting, to a three-dimensional setting. For the second scenario, we introduce multistory channel charting, a two-stage approach consisting of floor classification via clustering followed by the training of a dedicated expert neural network for channel charting on each individual floor, thereby enhancing the channel charting performance. In addition, we propose a novel feature engineering method designed to extract sparse features from the beamspace channel state information that are suitable for localization.

</details>


### [36] [Variational Robust Kalman Filters: A Unified Framework](https://arxiv.org/abs/2512.15419)
*Shilei Li,Dawei Shi,Hao Yu,Ling Shi*

Main category: cs.IT

TL;DR: 提出一种基于学生t分布损失函数和变分推理的统一变分鲁棒卡尔曼滤波器，通过切换规则将鲁棒性和自适应性融合，能在复杂噪声环境中同时抑制过程和测量噪声。


<details>
  <summary>Details</summary>
Motivation: 鲁棒性和自适应性是卡尔曼滤波中两个相互竞争的目标。实际应用中，过程和测量噪声可能受异常值影响、时变或两者兼有。现有方法难以有效处理这些复杂噪声场景，因为鲁棒滤波器和自适应滤波器之间存在内在不兼容性。

Method: 基于学生t分布诱导的损失函数和变分推理构建统一变分鲁棒卡尔曼滤波器，通过定点迭代以计算高效的方式求解。通过切换规则将鲁棒性和自适应性融合到单一框架中。

Result: 提出的滤波器能够通过调整参数恢复传统卡尔曼滤波器、鲁棒卡尔曼滤波器和自适应卡尔曼滤波器，并能同时抑制不完美的过程和测量噪声，在复杂噪声环境中表现优异。仿真验证了方法的有效性。

Conclusion: 鲁棒性可以被理解为自适应性的先决条件，使得通过切换规则将这两个竞争目标融合到单一框架成为可能。所提出的滤波器为复杂噪声环境下的状态估计问题提供了有效的解决方案。

Abstract: Robustness and adaptivity are two competing objectives in Kalman filters (KF). Robustness involves temporarily inflating prior estimates of noise covariances, while adaptivity updates prior beliefs using real-time information. In practical applications, both process and measurement noise can be influenced by outliers, be time-varying, or both. Existing works may not effectively address the above complex noise scenarios, as there is an intrinsic incompatibility between robust filters and adaptive filters. In this work, we propose a unified variational robust Kalman filter, built on a Student's t-distribution induced loss function and variational inference, and solved through fixed-point iteration in a computationally efficient manner. We demonstrate that robustness can be understood as a prerequisite for adaptivity, making it possible to merge the above two competing goals into a single framework through switching rules. Additionally, our proposed filter can recover conventional KF, robust KF, and adaptive KF by adjusting parameters, and can suppress both the imperfect process and measurement noise, enabling it to perform superiorly in complex noise environments. Simulations verify the effectiveness of the proposed method.

</details>


### [37] [An Anti-Interference AFDM System: Interference Impacts Analyses and Parameter Optimization](https://arxiv.org/abs/2512.15425)
*Peng Yuan,Zulin Wang,Tao Luo,Yuanhan Ni*

Main category: cs.IT

TL;DR: 提出抗干扰仿射频分复用系统，用于高移动性场景下对抗恶意高功率干扰，通过参数优化算法最大化分组吞吐量，并设计线性复杂度相关检测器实现全分集增益。


<details>
  <summary>Details</summary>
Motivation: 在高移动性场景中，对抗性设备产生的恶意高功率干扰会严重影响通信系统的可靠性和资源效率，需要设计有效的抗干扰方案。

Method: 1) 推导离散仿射傅里叶变换域中干扰的闭式表达式；2) 基于干扰分类设计参数优化算法最大化分组吞吐量；3) 设计线性复杂度相关检测器，利用扩频序列自相关函数和AFDM输入输出关系的循环移位特性。

Result: 数值结果验证了闭式表达式的准确性，证明所提出的抗干扰AFDM系统在高移动性干扰场景下能够实现高分组吞吐量。

Conclusion: 提出的抗干扰AFDM系统通过参数优化和高效检测器设计，能够在高移动性干扰环境中保证通信可靠性和资源效率，为对抗恶意干扰提供了有效解决方案。

Abstract: This paper proposes an anti-interference affine frequency division multiplexing (AFDM) system to ensure reliability and resource efficiency under malicious high-power interference originating from adversarial devices in high-mobility scenarios. Closed-form expressions of interferences in the discrete affine Fourier transform (DAFT) domain are derived by utilizing the stationary phase principle and the Affine Fourier transform convolution theorem, which indicates that interference impacts can be classified into stationary and non-stationary categories. On this basis, we reveal the analytical relationship between packet throughput and the paramerters of spread spectrum and error correction coding in our proposed anti-interference system, which enables the design of a parameter optimization algorithm that maximizes packet throughput. For reception, by jointly utilizing the autocorrelation function of spreading sequence and the cyclic-shift property of AFDM input-output relation, we design a linear-complexity correlation-based DAFT domain detector (CDD) capable of achieving full diversity gain, which performs correlation-based equalization to avoid matrix inversion. Numerical results validate the accuracy of the derived closed-form expressions and verify that the proposed anti-interference AFDM system could achieve high packet throughput under interference in high-mobility scenarios.

</details>


### [38] [Reducing Pilots in Channel Estimation With Predictive Foundation Models](https://arxiv.org/abs/2512.15562)
*Xingyu Zhou,Le Liang,Hao Ye,Jing Zhang,Chao-Kai Wen,Shi Jin*

Main category: cs.IT

TL;DR: 提出基于预测基础模型的信道估计框架，通过大规模跨域CSI数据训练提取通用信道表示，结合ViT架构的导频处理网络，实现准确、低开销、可泛化的CSI获取。


<details>
  <summary>Details</summary>
Motivation: 现代无线系统在大规模天线阵列、严格导频开销约束和多样化部署环境下，准确获取信道状态信息(CSI)变得越来越困难。现有基于人工智能的解决方案通常缺乏鲁棒性，无法跨场景泛化。

Method: 1) 使用大规模跨域CSI数据训练预测基础模型，提取通用信道表示并提供具有强跨场景可迁移性的预测先验；2) 设计基于视觉Transformer架构的导频处理网络，从导频观测中捕获空间、时间和频率相关性；3) 提出高效融合机制，将预测先验与实时测量相结合，即使在稀疏或噪声条件下也能实现可靠的CSI重建。

Result: 在多种配置下的广泛评估表明，所提出的估计器在准确性、鲁棒性和泛化能力方面显著优于经典方法和数据驱动基线方法。

Conclusion: 提出的预测基础模型信道估计框架解决了现有AI方法在鲁棒性和泛化性方面的不足，实现了准确、低开销、可泛化的CSI获取，为现代无线系统提供了有效的解决方案。

Abstract: Accurate channel state information (CSI) acquisition is essential for modern wireless systems, which becomes increasingly difficult under large antenna arrays, strict pilot overhead constraints, and diverse deployment environments. Existing artificial intelligence-based solutions often lack robustness and fail to generalize across scenarios. To address this limitation, this paper introduces a predictive-foundation-model-based channel estimation framework that enables accurate, low-overhead, and generalizable CSI acquisition. The proposed framework employs a predictive foundation model trained on large-scale cross-domain CSI data to extract universal channel representations and provide predictive priors with strong cross-scenario transferability. A pilot processing network based on a vision transformer architecture is further designed to capture spatial, temporal, and frequency correlations from pilot observations. An efficient fusion mechanism integrates predictive priors with real-time measurements, enabling reliable CSI reconstruction even under sparse or noisy conditions. Extensive evaluations across diverse configurations demonstrate that the proposed estimator significantly outperforms both classical and data-driven baselines in accuracy, robustness, and generalization capability.

</details>
