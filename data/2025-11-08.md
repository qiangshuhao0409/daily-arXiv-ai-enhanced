<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 2]
- [cs.AI](#cs.AI) [Total: 32]
- [cs.IT](#cs.IT) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Hybrid Quantum-Classical Detection for RIS-Assisted SC-FDE via Grover Adaptive Search](https://arxiv.org/abs/2511.04173)
*Maryam Tariq,Omar Alhussein,Raneem Abdelraheem,Abdullah Quran,Georges Kaddoum,Sami Muhaidat*

Main category: cs.NI

TL;DR: 提出了一种混合量子-经典检测框架，用于RIS辅助的SC-FDE系统，通过GAS算法实现接近ML性能的低复杂度检测。


<details>
  <summary>Details</summary>
Motivation: 6G网络对宽带和低延迟的需求要求检测器在接近最大似然性能的同时避免指数复杂度。

Method: 将ML检测目标重新表述为QUBO问题，通过GAS求解，并引入频域MMSE阈值进行低复杂度初始化。

Result: 在理想情况下实现接近最优性能，噪声环境下性能接近MMSE基准，验证了量子增强检测在RIS辅助宽带通信中的可行性。

Conclusion: 该框架在6G网络中展现出算法可扩展性和实际鲁棒性，为量子增强检测在宽带通信中的应用奠定了基础。

Abstract: Wideband and low-latency requirements in sixth-generation (6G) networks
demand detectors that approach maximum-likelihood (ML) performance without
incurring exponential complexity. This work develops a hybrid quantum-classical
detection framework for reconfigurable intelligent surface (RIS)-assisted
single-carrier (SC) frequency-domain equalization (FDE) over
frequency-selective channels. The ML detection objective is reformulated as a
quadratic unconstrained binary optimization (QUBO) problem and solved via
Grover adaptive search (GAS). To accelerate convergence, we introduce a
frequency-domain MMSE threshold that exploits the circulant structure of SC-FDE
channels, yielding low-complexity initialization. The framework is evaluated
across varying channel lengths and RIS sizes, confirming robustness and
scalability. In addition, GAS requirements are quantified through register
widths and gate counts, and its query complexity is analyzed to characterize
the algorithm's cost for block transmission in frequency-selective channels.
Quantum circuit simulations are conducted in Qiskit under both ideal and noisy
conditions. In the ideal case, the detector achieves near-optimal performance
while benefiting from Grover's quadratic speedup, reducing the search cost from
from O(M^N) exhaustive evaluations to O(SQRT(M^N)) oracle queries. Under noise,
the shallow depth of the GAS circuits, aided by MMSE initialization, makes
depolarizing errors negligible, while readout errors introduce moderate
degradation yet still preserve performance close to the MMSE baseline. These
results establish the feasibility of quantum-enhanced detection for
RIS-assisted broadband communications, highlighting both algorithmic
scalability and practical robustness for 6G networks.

</details>


### [2] [Improving dynamic congestion isolation in data-center networks](https://arxiv.org/abs/2511.04639)
*Alberto Merino,Jesus Escudero-Sahuquillo,Pedro Javier Garcia,Francisco J. Quiles*

Main category: cs.NI

TL;DR: 提出了改进的拥塞隔离机制ICI，通过协调CI和DCQCN来减少误报拥塞检测，提高网络资源利用率


<details>
  <summary>Details</summary>
Motivation: 分布式AI和大规模应用导致数据中心网络拥塞，现有DCQCN和CI机制结合使用时会产生误报拥塞、过度限流和资源利用低效的问题

Method: ICI机制将隔离的拥塞流信息用于指导DCQCN的ECN标记，避免受害流被错误标记，减少误报拥塞检测

Result: 在多种流量模式下，ICI将生成的BECN数量减少最多32倍，尾部延迟改善最多31%，同时保持高吞吐量和可扩展性

Conclusion: ICI通过协调CI和DCQCN有效解决了误报拥塞问题，提高了网络性能和响应能力

Abstract: The rise of distributed AI and large-scale applications has impacted the
communication operations of data-center and Supercomputer interconnection
networks, leading to dramatic incast or in-network congestion scenarios and
challenging existing congestion control mechanisms, such as injection
throttling (e.g., DCQCN) or congestion isolation (CI). While DCQCN provides a
scalable traffic rate adjustment for congesting flows at end nodes (which is
slow) and CI effectively isolates these flows in special network resources
(which requires extra logic in the switches), their combined use, although it
diminishes their particular drawbacks, leads to false congestion scenarios
identification and signaling, excessive throttling, and inefficient network
resource utilization. In this paper, we propose a new CI mechanism, called
Improved Congestion Isolation (ICI), which efficiently combines CI and DCQCN so
that the information of the isolated congesting flows is used to guide the ECN
marking performed by DCQCN in a way that victim flows do not end up being
marked. This coordination reduces false-positive congestion detection,
suppresses unnecessary closed-loop feedback (i.e., wrong congestion
notifications), and improves responsiveness to communication microbursts.
Evaluated under diverse traffic patterns, including incast and Data-center
workloads, ICI reduces the number of generated BECNs by up to 32x and improves
tail latency by up to 31%, while maintaining high throughput and scalability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)
*Zhaorun Chen,Zhuokai Zhao,Kai Zhang,Bo Liu,Qi Qi,Yifan Wu,Tarun Kalluri,Sara Cao,Yuanhao Xiong,Haibo Tong,Huaxiu Yao,Hengduo Li,Jiacheng Zhu,Xian Li,Dawn Song,Bo Li,Jason Weston,Dat Huynh*

Main category: cs.AI

TL;DR: DreamGym是一个统一的强化学习框架，通过基于推理的经验模型合成多样化经验数据，解决RL训练中昂贵环境交互、任务多样性不足、奖励信号不可靠等问题。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在大语言模型代理中实际应用的挑战：昂贵的环境交互、有限的任务多样性、不可靠的奖励信号和基础设施复杂性，这些阻碍了可扩展经验数据的收集。

Method: 1) 将环境动态提炼为基于推理的经验模型，通过逐步推理获得一致的状态转换和反馈信号；2) 使用离线真实数据初始化经验回放缓冲区并持续丰富；3) 自适应生成挑战当前代理策略的新任务，实现在线课程学习。

Result: 在多样环境和代理骨干上的实验表明，DreamGym显著改善了RL训练。在WebArena等非RL就绪任务上超越所有基线30%以上；在RL就绪但成本高昂的设置中，仅使用合成交互就能匹配GRPO和PPO性能；在将纯合成经验训练的策略迁移到真实环境RL时，DreamGym在需要更少真实交互的同时获得显著性能提升。

Conclusion: DreamGym为通用强化学习提供了一个可扩展的预热策略，通过合成经验数据有效解决了RL训练中的可扩展性问题。

Abstract: While reinforcement learning (RL) can empower large language model (LLM)
agents by enabling self-improvement through interaction, its practical adoption
remains challenging due to costly rollouts, limited task diversity, unreliable
reward signals, and infrastructure complexity, all of which obstruct the
collection of scalable experience data. To address these challenges, we
introduce DreamGym, the first unified framework designed to synthesize diverse
experiences with scalability in mind to enable effective online RL training for
autonomous agents. Rather than relying on expensive real-environment rollouts,
DreamGym distills environment dynamics into a reasoning-based experience model
that derives consistent state transitions and feedback signals through
step-by-step reasoning, enabling scalable agent rollout collection for RL. To
improve the stability and quality of transitions, DreamGym leverages an
experience replay buffer initialized with offline real-world data and
continuously enriched with fresh interactions to actively support agent
training. To improve knowledge acquisition, DreamGym adaptively generates new
tasks that challenge the current agent policy, enabling more effective online
curriculum learning. Experiments across diverse environments and agent
backbones demonstrate that DreamGym substantially improves RL training, both in
fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready
tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in
RL-ready but costly settings, it matches GRPO and PPO performance using only
synthetic interactions. When transferring a policy trained purely on synthetic
experiences to real-environment RL, DreamGym yields significant additional
performance gains while requiring far fewer real-world interactions, providing
a scalable warm-start strategy for general-purpose RL.

</details>


### [4] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 本研究评估了NLP分词模型在汇编代码分析中的内在特性，包括词汇量大小、语义覆盖等，并探讨了预处理定制选项和预分词规则对下游任务的影响。


<details>
  <summary>Details</summary>
Motivation: 汇编代码分析中的分词问题尚未得到充分研究，但其对下游任务性能有重要影响，本研究旨在填补这一空白。

Method: 系统研究多种分词模型，通过内在评估比较分词效率、词汇压缩和表示保真度，使用Llama 3.2、BERT和BART等预训练模型评估性能。

Result: 分词器选择显著影响下游性能，内在指标只能部分预测外在评估结果，揭示了内在分词器特性与实际应用之间的复杂权衡。

Conclusion: 本研究为低级代码分析中的分词模型优化提供了宝贵见解，有助于提高基于自然语言模型的二进制分析工作流程的鲁棒性和可扩展性。

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [5] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong,Luyi Ma,Varun Vasudevan,Jason Cho,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: BehaviorLens框架系统评估了用户行为数据的文本与图像表示对多模态大语言模型性能的影响，发现图像表示能显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 探索用户行为数据的文本与图像表示哪种更能最大化多模态大语言模型的性能，这一问题尚未得到充分研究。

Method: 开发BehaviorLens基准框架，在六个MLLMs上测试交易数据的三种表示方式：文本段落、散点图和流程图，使用真实购买序列数据集。

Result: 当数据表示为图像时，MLLMs的下一次购买预测准确率比等效文本表示提高了87.5%，且无需额外计算成本。

Conclusion: 图像表示用户行为数据能显著提升MLLMs的推理性能，为优化智能代理系统提供了重要指导。

Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.

</details>


### [6] [KnowThyself: An Agentic Assistant for LLM Interpretability](https://arxiv.org/abs/2511.03878)
*Suraj Prasai,Mengnan Du,Ying Zhang,Fan Yang*

Main category: cs.AI

TL;DR: KnowThyself是一个基于聊天的LLM可解释性工具，通过整合现有功能、降低技术门槛，提供交互式可视化和引导解释。


<details>
  <summary>Details</summary>
Motivation: 现有LLM可解释性工具存在碎片化和代码密集的问题，需要开发更易用、整合的工具来降低技术门槛。

Method: 使用编排器LLM重新表述用户查询，通过代理路由器分发到专门模块，最后将输出情境化为连贯解释。

Result: 开发了一个基于聊天的可扩展平台，提供交互式可视化和引导解释，增强了LLM可解释性的可访问性。

Conclusion: KnowThyself通过对话式工作流为LLM可解释性提供了稳健基础，使模型检查更加易于访问。

Abstract: We develop KnowThyself, an agentic assistant that advances large language
model (LLM) interpretability. Existing tools provide useful insights but remain
fragmented and code-intensive. KnowThyself consolidates these capabilities into
a chat-based interface, where users can upload models, pose natural language
questions, and obtain interactive visualizations with guided explanations. At
its core, an orchestrator LLM first reformulates user queries, an agent router
further directs them to specialized modules, and the outputs are finally
contextualized into coherent explanations. This design lowers technical
barriers and provides an extensible platform for LLM inspection. By embedding
the whole process into a conversational workflow, KnowThyself offers a robust
foundation for accessible LLM interpretability.

</details>


### [7] [Extracting Causal Relations in Deep Knowledge Tracing](https://arxiv.org/abs/2511.03948)
*Kevin Hong,Kia Karbasi,Gregory Pottie*

Main category: cs.AI

TL;DR: 本文挑战了关于深度知识追踪(DKT)性能优势的流行解释，证明DKT的优势在于其隐式建模先决条件关系作为因果结构的能力，而非双向关系。


<details>
  <summary>Details</summary>
Motivation: 长期以来计算教育研究的目标是开发可解释的知识追踪模型。DKT被认为是传统方法的重大进步，但对其性能来源的解释存在争议。

Method: 通过将练习关系图修剪为有向无环图(DAGs)，在Assistments数据集的因果子集上训练DKT，并使用DKT学习表示提取练习关系DAGs的替代方法。

Result: 研究表明DKT的预测能力与这些因果结构高度一致，其有效性主要源于近似知识组件间因果依赖关系的能力。

Conclusion: DKT的有效性主要受其建模知识组件间因果依赖关系的能力驱动，而非简单的双向关系映射。

Abstract: A longstanding goal in computational educational research is to develop
explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which
leverages a Recurrent Neural Network (RNN) to predict student knowledge and
performance on exercises, has been proposed as a major advancement over
traditional KT methods. Several studies suggest that its performance gains stem
from its ability to model bidirectional relationships between different
knowledge components (KCs) within a course, enabling the inference of a
student's understanding of one KC from their performance on others. In this
paper, we challenge this prevailing explanation and demonstrate that DKT's
strength lies in its implicit ability to model prerequisite relationships as a
causal structure, rather than bidirectional relationships. By pruning exercise
relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal
subsets of the Assistments dataset, we show that DKT's predictive capabilities
align strongly with these causal structures. Furthermore, we propose an
alternative method for extracting exercise relation DAGs using DKT's learned
representations and provide empirical evidence supporting our claim. Our
findings suggest that DKT's effectiveness is largely driven by its capacity to
approximate causal dependencies between KCs rather than simple relational
mappings.

</details>


### [8] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: LLMs对提示语言和文化框架敏感，但存在系统性文化偏见，偏向荷兰、德国、美国和日本的价值观，无法充分代表全球文化多样性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能代表其广泛用户群体的文化多样性，因为存在训练数据和优化目标不平衡的问题。

Method: 使用霍夫斯泰德价值观调查模块和世界价值观调查的63个项目，翻译成11种语言，测试10个LLMs在不同文化视角下的响应。

Result: 提示语言和文化视角都会影响LLM输出，但模型系统性偏向特定国家的价值观。文化框架比针对性提示语言更能改善与人类价值观的匹配度。

Conclusion: LLMs处于尴尬的中间地带：对提示变化敏感，但过于固守特定文化默认值，无法充分代表文化多样性。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [9] [ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering](https://arxiv.org/abs/2511.03985)
*Zhuowen Yuan,Tao Liu,Yang Yang,Yang Wang,Feng Qi,Kaushik Rangadurai,Bo Li,Shuang Yang*

Main category: cs.AI

TL;DR: ArchPilot是一个多代理系统，通过架构生成、基于代理的评估和自适应搜索来解决LLM代理在自动机器学习工程中的计算开销大、可扩展性差和迭代周期慢的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在自动机器学习工程中严重依赖重复的完整训练来评估候选方案，导致计算开销大、在大搜索空间中可扩展性差、迭代周期慢。

Method: 采用多代理协作框架，包含编排代理（使用MCTS启发算法协调搜索）、生成代理（迭代生成和改进架构）和评估代理（执行代理训练并生成保真度感知性能指标）。

Result: 在MLE-Bench上的实验表明，ArchPilot优于AIDE和ML-Master等最先进基线方法。

Conclusion: ArchPilot通过多代理协作，能够在有限预算下优先考虑高潜力候选方案，减少对昂贵完整训练的依赖，实现高效的机器学习工程。

Abstract: Recent LLM-based agents have demonstrated strong capabilities in automated ML
engineering. However, they heavily rely on repeated full training runs to
evaluate candidate solutions, resulting in significant computational overhead,
limited scalability to large search spaces, and slow iteration cycles. To
address these challenges, we introduce ArchPilot, a multi-agent system that
integrates architecture generation, proxy-based evaluation, and adaptive search
into a unified framework. ArchPilot consists of three specialized agents: an
orchestration agent that coordinates the search process using a Monte Carlo
Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and
manages memory of previous candidates; a generation agent that iteratively
generates, improves, and debugs candidate architectures; and an evaluation
agent that executes proxy training runs, generates and optimizes proxy
functions, and aggregates the proxy scores into a fidelity-aware performance
metric. This multi-agent collaboration allows ArchPilot to prioritize
high-potential candidates with minimal reliance on expensive full training
runs, facilitating efficient ML engineering under limited budgets. Experiments
on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE
and ML-Master, validating the effectiveness of our multi-agent system.

</details>


### [10] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: 本文提出了多智能体AI系统中的异常检测任务，通过构建包含4,275和894条轨迹的数据集，评估了监督和半监督方法在检测漂移、循环和输出缺失等无声故障方面的性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统具有非确定性且容易出现难以检测的无声故障，如漂移、循环和输出细节缺失，需要系统化的异常检测方法。

Method: 提出了一个数据集构建流程，捕捉用户行为、智能体非确定性和LLM变异，并创建了两个基准数据集。使用XGBoost（监督）和SVDD（半监督）方法进行异常检测。

Result: 监督方法（XGBoost）和半监督方法（SVDD）表现相当，准确率分别达到98%和96%。

Conclusion: 这是对多智能体AI系统中异常检测的首次系统研究，提供了数据集、基准测试和见解，为未来研究提供指导。

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [11] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: LLMs在数值推理中存在系统错误，主要原因是数值属性在共享潜在子空间中编码时会放大真实世界相关性，且无关上下文会干扰数值表示，影响决策质量。


<details>
  <summary>Details</summary>
Motivation: 尽管行为研究发现LLMs存在数值推理错误，但其内部表示机制尚不明确。研究旨在揭示LLMs如何整合单个实体的多个数值属性，以及无关数值上下文如何干扰这些表示。

Method: 结合线性探测与偏相关分析，在不同规模模型上进行基于提示的脆弱性测试，研究数值属性的内部表示机制。

Result: LLMs编码了真实世界的数值相关性但会系统性地放大它们；无关上下文会引起数值表示的一致偏移，且下游影响随模型规模变化。

Conclusion: 这些发现揭示了LLM决策中的脆弱性，为在多属性纠缠下实现更公平、表示感知的控制奠定了基础。

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [12] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: 提出了一个名为Agentmandering的框架，将选区重划重新构想为两个代表对立政治利益的智能体之间的回合制谈判，通过LLM智能体将战略互动嵌入到选区重划过程中，显著减少了党派偏见和不公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法主要生成大量法律上有效的选区重划方案，但忽视了选择过程中的战略动态，这为党派行为者挑选技术上合规但政治上有利的地图创造了机会。

Method: 采用基于博弈论思想的"选择并冻结"协议，让两个代表对立政治利益的LLM智能体轮流从一小部分候选地图中选择和冻结选区，通过受限且可解释的选择逐步划分州。

Result: 在2020年美国人口普查数据上的评估显示，Agentmandering显著减少了党派偏见和不公平性，同时实现了比标准基线低2到3个数量级的方差，在摇摆州场景中表现出公平性和稳定性。

Conclusion: Agentmandering框架通过将战略互动嵌入选区重划过程，能够有效减少党派偏见，提高公平性和稳定性，特别是在竞争激烈的政治环境中。

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


### [13] [KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04093)
*Yuanning Cui,Zequn Sun,Wei Hu,Zhangjie Fu*

Main category: cs.AI

TL;DR: 提出LLM-KGFR框架，通过LLM与知识图谱检索器协作，解决大语言模型在知识密集型问题上的局限性，实现零样本泛化和高效处理大规模图谱。


<details>
  <summary>Details</summary>
Motivation: 大语言模型擅长推理但在知识密集型问题上表现不佳，现有方法受限于数据集特定调优和大规模图谱的可扩展性问题。

Method: 使用LLM生成关系描述的知识图谱检索器KGFR，通过非对称渐进传播算法高效处理大规模图谱，提供节点、边和路径级接口支持迭代推理。

Result: 实验表明LLM-KGFR在保持可扩展性和泛化能力的同时实现了强劲性能。

Conclusion: 该框架为知识图谱增强推理提供了实用解决方案，平衡了性能与可扩展性。

Abstract: Large language models (LLMs) excel at reasoning but struggle with
knowledge-intensive questions due to limited context and parametric knowledge.
However, existing methods that rely on finetuned LLMs or GNN retrievers are
limited by dataset-specific tuning and scalability on large or unseen graphs.
We propose the LLM-KGFR collaborative framework, where an LLM works with a
structured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR
encodes relations using LLM-generated descriptions and initializes entities
based on their roles in the question, enabling zero-shot generalization to
unseen KGs. To handle large graphs efficiently, it employs Asymmetric
Progressive Propagation (APP)- a stepwise expansion that selectively limits
high-degree nodes while retaining informative paths. Through node-, edge-, and
path-level interfaces, the LLM iteratively requests candidate answers,
supporting facts, and reasoning paths, forming a controllable reasoning loop.
Experiments demonstrate that LLM-KGFR achieves strong performance while
maintaining scalability and generalization, providing a practical solution for
KG-augmented reasoning.

</details>


### [14] [Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms](https://arxiv.org/abs/2511.04133)
*Miguel E. Andres,Vadim Fedorov,Rida Sadek,Enric Spagnolo-Arrizabalaga,Nadescha Trudel*

Main category: cs.AI

TL;DR: 提出了首个系统化框架，通过以人为中心的基准测试来评估语音AI测试质量，解决了测试平台生成真实对话和准确评估响应的双重挑战。


<details>
  <summary>Details</summary>
Motivation: 随着语音AI代理快速进入生产部署，确保测试可靠性的系统方法仍然不足。组织无法客观评估其测试方法是否真正有效，在语音AI扩展到数十亿日常交互时造成了关键的测量差距。

Method: 结合成熟的心理测量技术（成对比较产生Elo评分、自举置信区间和置换测试）与严格的统计验证，提供适用于任何测试方法的可重复指标。

Result: 对三个领先商业平台进行了全面实证评估，结果显示存在显著的性能差异。表现最佳的平台Evalion在评估质量上达到0.92 F1分数，而其他平台为0.73；在模拟质量上达到0.61，而其他平台为0.43。

Conclusion: 该框架使研究人员和组织能够实证验证任何平台的测试能力，为大规模自信部署语音AI提供了必要的测量基础。

Abstract: Voice AI agents are rapidly transitioning to production deployments, yet
systematic methods for ensuring testing reliability remain underdeveloped.
Organizations cannot objectively assess whether their testing approaches
(internal tools or external platforms) actually work, creating a critical
measurement gap as voice AI scales to billions of daily interactions.
  We present the first systematic framework for evaluating voice AI testing
quality through human-centered benchmarking. Our methodology addresses the
fundamental dual challenge of testing platforms: generating realistic test
conversations (simulation quality) and accurately evaluating agent responses
(evaluation quality). The framework combines established psychometric
techniques (pairwise comparisons yielding Elo ratings, bootstrap confidence
intervals, and permutation tests) with rigorous statistical validation to
provide reproducible metrics applicable to any testing approach.
  To validate the framework and demonstrate its utility, we conducted
comprehensive empirical evaluation of three leading commercial platforms
focused on Voice AI Testing using 21,600 human judgments across 45 simulations
and ground truth validation on 60 conversations. Results reveal statistically
significant performance differences with the proposed framework, with the
top-performing platform, Evalion, achieving 0.92 evaluation quality measured as
f1-score versus 0.73 for others, and 0.61 simulation quality using a league
based scoring system (including ties) vs 0.43 for other platforms.
  This framework enables researchers and organizations to empirically validate
the testing capabilities of any platform, providing essential measurement
foundations for confident voice AI deployment at scale. Supporting materials
are made available to facilitate reproducibility and adoption.

</details>


### [15] [When Empowerment Disempowers](https://arxiv.org/abs/2511.04177)
*Claire Yang,Maya Cakmak,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Empowerment, a measure of an agent's ability to control its environment, has
been proposed as a universal goal-agnostic objective for motivating assistive
behavior in AI agents. While multi-human settings like homes and hospitals are
promising for AI assistance, prior work on empowerment-based assistance assumes
that the agent assists one human in isolation. We introduce an open source
multi-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we
empirically show that assistive RL agents optimizing for one human's
empowerment can significantly reduce another human's environmental influence
and rewards - a phenomenon we formalize as disempowerment. We characterize when
disempowerment occurs in these environments and show that joint empowerment
mitigates disempowerment at the cost of the user's reward. Our work reveals a
broader challenge for the AI alignment community: goal-agnostic objectives that
seem aligned in single-agent settings can become misaligned in multi-agent
contexts.

</details>


### [16] [Opus: A Quantitative Framework for Workflow Evaluation](https://arxiv.org/abs/2511.04220)
*Alan Seroul,Théo Fagnoni,Inès Adnani,Dana O. Mohamed,Phillip Kingston*

Main category: cs.AI

TL;DR: Opus工作流评估框架：一个结合概率奖励和规范性惩罚的数学模型，用于量化工作流质量、效率和结构特性，支持自动化评估、排序和优化。


<details>
  <summary>Details</summary>
Motivation: 需要一种统一的数学框架来量化工作流的质量和效率，结合正确性、可靠性和成本等因素，实现工作流的直接比较、评分和优化。

Method: 结合Opus工作流奖励（概率函数估计预期性能）和Opus工作流规范性惩罚（衡量结构信息质量的可测量函数），形成统一的优化公式。

Result: 提出了一个支持自动化工作流评估、排序和优化的框架，可集成到强化学习循环中指导工作流发现和优化。

Conclusion: 该框架为工作流质量评估和优化提供了数学基础，能够识别和排序在奖励-惩罚权衡下的最优工作流。

Abstract: This paper introduces the Opus Workflow Evaluation Framework, a
probabilistic-normative formulation for quantifying Workflow quality and
efficiency. It integrates notions of correctness, reliability, and cost into a
coherent mathematical model that enables direct comparison, scoring, and
optimization of Workflows. The framework combines the Opus Workflow Reward, a
probabilistic function estimating expected performance through success
likelihood, resource usage, and output gain, with the Opus Workflow Normative
Penalties, a set of measurable functions capturing structural and informational
quality across Cohesion, Coupling, Observability, and Information Hygiene. It
supports automated Workflow assessment, ranking, and optimization within modern
automation systems such as Opus and can be integrated into Reinforcement
Learning loops to guide Workflow discovery and refinement. In this paper, we
introduce the Opus Workflow Reward model that formalizes Workflow success as a
probabilistic expectation over costs and outcomes. We define measurable Opus
Workflow Normative Penalties capturing structural, semantic, and signal-related
properties of Workflows. Finally, we propose a unified optimization formulation
for identifying and ranking optimal Workflows under joint Reward-Penalty
trade-offs.

</details>


### [17] [Shared Spatial Memory Through Predictive Coding](https://arxiv.org/abs/2511.04235)
*Zhengru Fang,Yu Guo,Jingjing Wang,Yuang Zhang,Haonan An,Yinhai Wang,Yuguang Fang*

Main category: cs.AI

TL;DR: 提出了一个多智能体预测编码框架，通过最小化智能体间的相互不确定性来解决协调问题，在带宽受限条件下表现出色。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中，部分可观测性和有限带宽常常导致协调失败，需要开发能够共享和重建一致空间记忆的方法。

Method: 采用信息瓶颈目标，让智能体学习何时、与谁、以及如何通信；基于网格细胞状度量的内部空间编码；分层强化学习策略主动探索以减少联合不确定性。

Result: 在Memory-Maze基准测试中，方法在带宽从128位/步降至4位/步时，成功率从73.5%优雅降至64.4%，而全广播基线从67.6%崩溃至28.6%。

Conclusion: 建立了一个理论上有原则且生物学上合理的框架，展示了复杂社会表征如何从统一的预测驱动中涌现，实现社会集体智能。

Abstract: Sharing and reconstructing a consistent spatial memory is a critical
challenge in multi-agent systems, where partial observability and limited
bandwidth often lead to catastrophic failures in coordination. We introduce a
multi-agent predictive coding framework that formulate coordination as the
minimization of mutual uncertainty among agents. Instantiated as an information
bottleneck objective, it prompts agents to learn not only who and what to
communicate but also when. At the foundation of this framework lies a
grid-cell-like metric as internal spatial coding for self-localization,
emerging spontaneously from self-supervised motion prediction. Building upon
this internal spatial code, agents gradually develop a bandwidth-efficient
communication mechanism and specialized neural populations that encode
partners' locations: an artificial analogue of hippocampal social place cells
(SPCs). These social representations are further enacted by a hierarchical
reinforcement learning policy that actively explores to reduce joint
uncertainty. On the Memory-Maze benchmark, our approach shows exceptional
resilience to bandwidth constraints: success degrades gracefully from 73.5% to
64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast
baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically
principled and biologically plausible basis for how complex social
representations emerge from a unified predictive drive, leading to social
collective intelligence.

</details>


### [18] [RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization](https://arxiv.org/abs/2511.04285)
*Zeng Zhiyuan,Jiashuo Liu,Zhangyue Yin,Ge Zhang,Wenhao Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: RLVR训练中存在RL过拟合问题，模型获得训练奖励但失去泛化能力。RLoop框架通过迭代策略初始化，将训练过程转化为探索和利用的良性循环，有效缓解遗忘并提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR训练中的RL过拟合问题，即模型在训练中获得奖励但失去泛化能力，这由策略过度专门化和灾难性遗忘驱动。

Method: RLoop框架：通过迭代策略初始化，先使用RL从给定策略探索解空间，然后过滤成功轨迹创建专家数据集，通过拒绝采样微调(RFT)来优化初始策略，为下一次迭代创建更好的起点。

Result: RLoop显著缓解遗忘并提升泛化能力，相比原始RL平均准确率提升9%，pass@32提升超过15%。

Conclusion: RLoop通过将瞬态策略变化转化为稳健性能增益，有效解决了RL训练中的过拟合和遗忘问题。

Abstract: While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for
training large reasoning models, its training dynamics harbor a critical
challenge: RL overfitting, where models gain training rewards but lose
generalization. Our analysis reveals this is driven by policy
over-specialization and catastrophic forgetting of diverse solutions generated
during training. Standard optimization discards this valuable inter-step policy
diversity. To address this, we introduce RLoop, a self-improving framework
built on iterative policy initialization. RLoop transforms the standard
training process into a virtuous cycle: it first uses RL to explore the
solution space from a given policy, then filters the successful trajectories to
create an expert dataset. This dataset is used via Rejection-sampling
Fine-Tuning (RFT) to refine the initial policy, creating a superior starting
point for the next iteration. This loop of exploration and exploitation via
iterative re-initialization effectively converts transient policy variations
into robust performance gains. Our experiments show RLoop mitigates forgetting
and substantially improves generalization, boosting average accuracy by 9% and
pass@32 by over 15% compared to vanilla RL.

</details>


### [19] [GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents](https://arxiv.org/abs/2511.04307)
*Jian Mu,Chaoyun Zhang,Chiming Ni,Lu Wang,Bo Qiao,Kartik Mathur,Qianhui Wu,Yuhang Xie,Xiaojun Ma,Mengyu Zhou,Si Qin,Liqun Li,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: GUI-360°是一个大规模数据集和基准套件，用于推进计算机使用代理（CUAs）的研究，包含超过120万执行动作步骤，支持GUI定位、屏幕解析和动作预测三个核心任务。


<details>
  <summary>Details</summary>
Motivation: 解决计算机使用代理研究中的三个关键问题：真实CUA任务的稀缺性、多模态轨迹自动收集标注流程的缺乏，以及统一评估GUI定位、屏幕解析和动作预测的基准的缺失。

Method: 采用LLM增强的自动化流程，包括查询来源、环境模板构建、任务实例化、批量执行和LLM驱动的质量过滤，在Windows办公应用中收集数千条轨迹数据。

Result: 基准测试显示最先进的视觉-语言模型在定位和动作预测方面存在显著不足，监督微调和强化学习虽有改进但未达到人类水平可靠性。

Conclusion: GUI-360°的发布旨在促进可重复研究并加速稳健桌面CUAs的进展，数据集已在Hugging Face公开。

Abstract: We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and
benchmark suite designed to advance computer-using agents (CUAs). CUAs present
unique challenges and is constrained by three persistent gaps: a scarcity of
real-world CUA tasks, the lack of automated collection-and-annotation pipelines
for multi-modal trajectories, and the absence of a unified benchmark that
jointly evaluates GUI grounding, screen parsing, and action prediction.
  GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated
pipeline for query sourcing, environment-template construction, task
instantiation, batched execution, and LLM-driven quality filtering. The
released corpus contains over 1.2M executed action steps across thousands of
trajectories in popular Windows office applications, and includes
full-resolution screenshots, accessibility metadata when available,
instantiated goals, intermediate reasoning traces, and both successful and
failed action trajectories. The dataset supports three canonical tasks, GUI
grounding, screen parsing, and action prediction, and a hybrid GUI+API action
space that reflects modern agent designs. Benchmarking state-of-the-art
vision--language models on GUI-360$^\circ$ reveals substantial out-of-the-box
shortcomings in grounding and action prediction; supervised fine-tuning and
reinforcement learning yield significant gains but do not close the gap to
human-level reliability. We release GUI-360$^\circ$ and accompanying code to
facilitate reproducible research and accelerate progress on robust desktop
CUAs.
  The full dataset has been made public on
https://huggingface.co/datasets/vyokky/GUI-360.

</details>


### [20] [Probing the Probes: Methods and Metrics for Concept Alignment](https://arxiv.org/abs/2511.04312)
*Jacob Lysnæs-Larsen,Marte Eggen,Inga Strümke*

Main category: cs.AI

TL;DR: 本文揭示了在可解释AI中，仅凭线性分类器探针的准确率无法可靠评估概念激活向量(CAV)与目标概念的对齐程度，因为探针容易捕捉虚假相关性而非真正概念。作者提出了基于空间线性归因的新概念定位方法，并引入三类量化指标来评估概念对齐。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI领域普遍假设高探针准确率意味着CAV能忠实表示目标概念，但作者发现这种假设不可靠，探针更容易捕捉虚假相关性而非真正概念，这严重影响了概念解释的可信度。

Method: 提出了基于空间线性归因的概念定位方法，并与现有特征可视化技术进行比较；引入了三类概念对齐评估指标：硬准确率、分割得分和增强鲁棒性；构建了故意错位的探针来验证问题。

Result: 研究表明，故意错位的探针利用虚假相关性也能达到接近标准探针的准确率；具有平移不变性和空间对齐的探针能持续提高概念对齐度；传统探针准确率无法可靠评估概念对齐。

Conclusion: 需要基于对齐的评估指标而非探针准确率来评估概念表示，探针设计应同时考虑模型架构和目标概念的性质，平移不变性和空间对齐对提高概念对齐至关重要。

Abstract: In explainable AI, Concept Activation Vectors (CAVs) are typically obtained
by training linear classifier probes to detect human-understandable concepts as
directions in the activation space of deep neural networks. It is widely
assumed that a high probe accuracy indicates a CAV faithfully representing its
target concept. However, we show that the probe's classification accuracy alone
is an unreliable measure of concept alignment, i.e., the degree to which a CAV
captures the intended concept. In fact, we argue that probes are more likely to
capture spurious correlations than they are to represent only the intended
concept. As part of our analysis, we demonstrate that deliberately misaligned
probes constructed to exploit spurious correlations, achieve an accuracy close
to that of standard probes. To address this severe problem, we introduce a
novel concept localization method based on spatial linear attribution, and
provide a comprehensive comparison of it to existing feature visualization
techniques for detecting and mitigating concept misalignment. We further
propose three classes of metrics for quantitatively assessing concept
alignment: hard accuracy, segmentation scores, and augmentation robustness. Our
analysis shows that probes with translation invariance and spatial alignment
consistently increase concept alignment. These findings highlight the need for
alignment-based evaluation metrics rather than probe accuracy, and the
importance of tailoring probes to both the model architecture and the nature of
the target concept.

</details>


### [21] [AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research](https://arxiv.org/abs/2511.04316)
*Tim Beyer,Jonas Dornbusch,Jakob Steimle,Moritz Ladenburger,Leo Schwinn,Stephan Günnemann*

Main category: cs.AI

TL;DR: AdversariaLLM是一个用于大语言模型越狱鲁棒性研究的工具箱，旨在解决当前LLM安全研究生态系统碎片化、难以复现和比较的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全和鲁棒性研究生态系统存在碎片化、实现错误等问题，导致研究难以复现和比较，阻碍了有意义的进展。

Method: 设计了一个以可复现性、正确性和可扩展性为核心的工具箱，实现了12种对抗攻击算法，集成了7个基准数据集，并通过Hugging Face提供对各种开源LLM的访问。

Result: 该框架提供了计算资源跟踪、确定性结果和分布评估技术等高级功能，确保可比性和可复现性。

Conclusion: AdversariaLLM为LLM安全研究建立了透明、可比和可复现的坚实基础。

Abstract: The rapid expansion of research on Large Language Model (LLM) safety and
robustness has produced a fragmented and oftentimes buggy ecosystem of
implementations, datasets, and evaluation methods. This fragmentation makes
reproducibility and comparability across studies challenging, hindering
meaningful progress. To address these issues, we introduce AdversariaLLM, a
toolbox for conducting LLM jailbreak robustness research. Its design centers on
reproducibility, correctness, and extensibility. The framework implements
twelve adversarial attack algorithms, integrates seven benchmark datasets
spanning harmfulness, over-refusal, and utility evaluation, and provides access
to a wide range of open-weight LLMs via Hugging Face. The implementation
includes advanced features for comparability and reproducibility such as
compute-resource tracking, deterministic results, and distributional evaluation
techniques. \name also integrates judging through the companion package
JudgeZoo, which can also be used independently. Together, these components aim
to establish a robust foundation for transparent, comparable, and reproducible
research in LLM safety.

</details>


### [22] [RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation](https://arxiv.org/abs/2511.04328)
*Jiahao Zhao,Luxin Xu,Minghuan Tan,Lichao Zhang,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: 提出了RxSafeBench框架，通过模拟临床咨询评估LLMs的药物安全能力，包含6,725种禁忌症、28,781种药物相互作用和14,906个适应症-药物对，构建了2,443个高质量咨询场景的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLMs的医疗系统在药物安全方面的研究有限，缺乏真实世界数据集，且在现实临床咨询环境中的评估不足。

Method: 开发了模拟临床咨询的框架，生成包含药物风险的问诊对话，采用两阶段过滤策略确保临床真实性和专业质量，构建RxRisk DB药物安全数据库和RxSafeBench基准。

Result: 评估显示当前LLMs难以整合禁忌症和相互作用知识，特别是当风险是隐含而非明确时表现不佳。

Conclusion: 研究揭示了LLM系统在确保药物安全方面的关键挑战，RxSafeBench为评估LLMs药物安全能力提供了首个全面基准，推动更安全可靠的AI临床决策支持。

Abstract: Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.

</details>


### [23] [Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning](https://arxiv.org/abs/2511.04341)
*Nick Oh,Fernand Gobet*

Main category: cs.AI

TL;DR: 该论文提出了Monitor-Generate-Verify (MGV)框架，通过在Generate-Verify范式前添加显式监控机制来解决前缀主导陷阱问题，将元认知理论转化为计算规范。


<details>
  <summary>Details</summary>
Motivation: 现有测试时推理架构缺乏监控过程，导致模型过早陷入次优推理路径而难以恢复（约20%准确率损失），即前缀主导陷阱问题。

Method: 将Flavell和Nelson-Narens的元认知理论形式化为计算规范，在Generate-Verify范式前添加显式监控模块，捕获生成前的元认知体验并通过验证反馈优化未来监控。

Result: 虽然未提供实证验证，但首次系统地将基础元认知理论转化为计算框架，为理解推理系统失败提供了原则性词汇。

Conclusion: MGV框架为未来测试时推理设计提供了具体的架构干预建议，填补了现有架构中监控过程的缺失。

Abstract: Test-time reasoning architectures such as those following the Generate-Verify
paradigm -- where a model iteratively refines or verifies its own generated
outputs -- prioritise generation and verification but exclude the monitoring
processes that determine when and how reasoning should begin. This omission may
contribute to the prefix dominance trap, in which models commit early to
suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy
loss. We address this architectural gap by formalising Flavell's and Nelson and
Narens' metacognitive theories into computational specifications, proposing the
Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify
paradigm by adding explicit monitoring that captures metacognitive experiences
(from difficulty assessments to confidence judgements) before generation begins
and refines future monitoring through verification feedback. Though we present
no empirical validation, this work provides the first systematic computational
translation of foundational metacognitive theories, offering a principled
vocabulary for understanding reasoning system failures and suggesting specific
architectural interventions for future test-time reasoning designs.

</details>


### [24] [Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach](https://arxiv.org/abs/2511.04393)
*Chanwoo Park,Ziyang Chen,Asuman Ozdaglar,Kaiqing Zhang*

Main category: cs.AI

TL;DR: 提出Iterative RMFT方法，通过迭代蒸馏低后悔决策轨迹来增强LLM的决策能力，无需依赖已知算法或人工模板


<details>
  <summary>Details</summary>
Motivation: LLM作为决策代理时在基础在线决策问题中表现不佳，难以实现低后悔或有效的探索-利用权衡

Method: 迭代后悔最小化微调：模型多次生成决策轨迹，选择k个最低后悔的轨迹，然后在这些轨迹上微调自身

Result: Iterative RMFT提升了多种模型（从Transformer到GPT-4o mini）的决策性能，并能泛化到不同任务设置

Conclusion: Iterative RMFT为增强LLM决策能力提供了一个原则性且通用的后训练框架

Abstract: Large language models (LLMs) are increasingly deployed as "agents" for
decision-making (DM) in interactive and dynamic environments. Yet, since they
were not originally designed for DM, recent studies show that LLMs can struggle
even in basic online DM problems, failing to achieve low regret or an effective
exploration-exploitation tradeoff. To address this, we introduce Iterative
Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure
that repeatedly distills low-regret decision trajectories back into the base
model. At each iteration, the model rolls out multiple decision trajectories,
selects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior
methods that (a) distill action sequences from known DM algorithms or (b) rely
on manually crafted chain-of-thought templates, our approach leverages the
regret metric to elicit the model's own DM ability and reasoning rationales.
This reliance on model-generated reasoning avoids rigid output engineering and
provides more flexible, natural-language training signals. Empirical results
show that Iterative RMFT improves LLMs' DM performance across diverse models -
from Transformers with numerical input/output, to open-weight LLMs, and
advanced closed-weight models like GPT-4o mini. Its flexibility in output and
reasoning formats enables generalization across tasks with varying horizons,
action spaces, reward processes, and natural-language contexts. Finally, we
provide theoretical insight showing that a single-layer Transformer under this
paradigm can act as a no-regret learner in a simplified setting. Overall,
Iterative RMFT offers a principled and general post-training framework for
enhancing LLMs' decision-making capabilities.

</details>


### [25] [The Peril of Preference: Why GRPO fails on Ordinal Rewards](https://arxiv.org/abs/2511.04439)
*Anisha Garg,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: CoRPO解决了GRPO在强化学习中处理序数奖励时的缺陷，通过自适应基线确保失败轨迹不被正向强化，并在达到质量阈值后转向相对偏好模式以寻找最优解。


<details>
  <summary>Details</summary>
Motivation: GRPO在处理序数奖励时存在缺陷，其组平均基线会给失败轨迹分配正优势值，从而强化错误行为。需要一种新方法来处理更丰富的非二元反馈。

Method: 提出CoRPO方法，使用自适应基线强制执行最低质量阈值，确保失败解决方案永远不会被正向强化。当策略持续满足阈值后，基线自动过渡到相对偏好模式。

Result: 在代码验证任务上实证验证，CoRPO表现出更稳定的收敛性和更好的域外泛化能力。

Conclusion: CoRPO是使LLM通过强化学习学习真正新能力的关键步骤，能够处理从二元到序数奖励的丰富多维反馈。

Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly
desirable for adapting LLMs to become experts at specific tasks. But this
simplicity also makes it ill-specified as we seek to enhance RL training with
richer, non-binary feedback. When using ordinal rewards to give partial credit,
GRPO's simplicity starts to hurt, as its group-average baseline often assigns a
positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new
formulation that solves this flaw. CoRPO uses an adaptive baseline that
enforces a minimum quality threshold, ensuring failed solutions are never
positively reinforced. Once the policy consistently meets this threshold, the
baseline automatically transitions to a relative preference mode, pushing the
model to find optimal solutions rather than just "acceptable" ones. We
empirically validate CoRPO on a code verification task, where it demonstrates
more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to
enable LLMs to learn genuinely new capabilities through reinforcement learning.
We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback
- progressing from binary to ordinal rewards in this work, and onward to
denser, per-step supervision.

</details>


### [26] [Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context](https://arxiv.org/abs/2511.04464)
*Carnot Braun,Rafael O. Jarczewski,Gabriel U. Talasso,Leandro A. Villas,Allan M. de Souza*

Main category: cs.AI

TL;DR: PAVe系统结合传统路径规划算法与LLM语义推理，通过多目标Dijkstra算法生成候选路线，再由LLM代理根据用户任务、偏好和规避规则进行语义评估，实现个性化车辆路线规划。


<details>
  <summary>Details</summary>
Motivation: 传统车辆路线系统只能优化单一指标，缺乏对驾驶员复杂语义和动态上下文的理解能力，无法处理多步骤任务、情境约束或紧急需求。

Method: 采用混合代理方法：多目标（时间、CO2）Dijkstra算法生成候选路线，LLM代理基于预处理的地理空间POI缓存，评估路线是否符合用户任务、偏好和规避规则。

Result: 在真实城市场景基准测试中，PAVe成功将复杂用户意图转化为适当路线修改，使用本地模型时初始路线选择准确率超过88%。

Conclusion: 将经典路由算法与基于LLM的语义推理层相结合，是创建个性化、自适应和可扩展城市移动优化解决方案的稳健有效方法。

Abstract: Traditional vehicle routing systems efficiently optimize singular metrics
like time or distance, and when considering multiple metrics, they need more
processes to optimize . However, they lack the capability to interpret and
integrate the complex, semantic, and dynamic contexts of human drivers, such as
multi-step tasks, situational constraints, or urgent needs. This paper
introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a
hybrid agentic assistant designed to augment classical pathfinding algorithms
with contextual reasoning. Our approach employs a Large Language Model (LLM)
agent that operates on a candidate set of routes generated by a multi-objective
(time, CO2) Dijkstra algorithm. The agent evaluates these options against
user-provided tasks, preferences, and avoidance rules by leveraging a
pre-processed geospatial cache of urban Points of Interest (POIs). In a
benchmark of realistic urban scenarios, PAVe successfully used complex user
intent into appropriate route modifications, achieving over 88% accuracy in its
initial route selections with a local model. We conclude that combining
classical routing algorithms with an LLM-based semantic reasoning layer is a
robust and effective approach for creating personalized, adaptive, and scalable
solutions for urban mobility optimization.

</details>


### [27] [Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis](https://arxiv.org/abs/2511.04481)
*Lars Krupp,Daniel Geißler,Vishal Banwari,Paul Lukowicz,Jakob Karolus*

Main category: cs.AI

TL;DR: 本文首次探讨了网络代理的能源消耗和碳排放问题，通过理论和实证分析揭示了不同网络代理设计理念对能源消耗的显著影响，并呼吁在评估网络代理时考虑能源效率指标。


<details>
  <summary>Details</summary>
Motivation: 尽管网络代理研究蓬勃发展，但其引发的可持续性问题仍未得到充分探索。本文旨在揭示网络代理的能源和碳排放成本，强调这一问题的紧迫性。

Method: 采用理论估计和实证基准测试相结合的方法，从理论和实践两个角度分析网络代理的能源消耗。

Result: 研究发现不同网络代理设计理念会严重影响能源消耗，且更多能源消耗并不一定带来更好结果。同时发现某些网络代理在披露模型参数和流程方面缺乏透明度，限制了能源消耗的准确估计。

Conclusion: 本文呼吁改变网络代理的评估方式，建议在基准测试中引入专门的能源消耗度量指标，以促进更可持续的网络代理发展。

Abstract: Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful
agentic systems pushing the boundaries of Large Language Models (LLM). They can
autonomously interact with the internet at the user's behest, such as
navigating websites, filling search masks, and comparing price lists. Though
web agent research is thriving, induced sustainability issues remain largely
unexplored. To highlight the urgency of this issue, we provide an initial
exploration of the energy and $CO_2$ cost associated with web agents from both
a theoretical -via estimation- and an empirical perspective -by benchmarking.
Our results show how different philosophies in web agent creation can severely
impact the associated expended energy, and that more energy consumed does not
necessarily equate to better results. We highlight a lack of transparency
regarding disclosing model parameters and processes used for some web agents as
a limiting factor when estimating energy consumption. Our work contributes
towards a change in thinking of how we evaluate web agents, advocating for
dedicated metrics measuring energy consumption in benchmarks.

</details>


### [28] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: 研究发现LLMs能够复制人类合作行为模式，Llama模型能高保真重现人类偏离理性选择理论的行为，而Qwen模型更接近纳什均衡预测，无需基于角色的提示即可实现群体行为复制。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs与人类决策的相似性至关重要，因为不匹配可能导致实际应用中的有害结果，而无法复制人类行为会使LLMs在社会模拟中无效。

Method: 开发游戏理论实验的数字孪生，引入系统性的提示和探测框架进行机器行为评估，测试三个开源模型（Llama、Mistral和Qwen）。

Result: Llama能高保真重现人类合作模式，捕捉人类偏离理性选择理论的行为；Qwen与纳什均衡预测高度一致；无需角色提示即可实现群体行为复制；生成并预注册了原始参数网格外新游戏配置的可测试假设。

Conclusion: 适当校准的LLMs可以复制聚合人类行为模式，并能够系统探索未开发的实验空间，为社会科学研究提供补充方法，生成关于人类社交决策的新经验预测。

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [29] [Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach](https://arxiv.org/abs/2511.04556)
*Zihang Ding,Kun Zhang*

Main category: cs.AI

TL;DR: 提出数据驱动的稀疏传感框架，结合EPA-SWMM模型，优化传感器布设并重建雨水系统峰值流量，在资源受限条件下实现高效洪水监测。


<details>
  <summary>Details</summary>
Motivation: 城市地表洪水因强降雨超出排水系统能力而日益频发，但高时空分辨率洪水预测和监测面临时间、预算和技术限制。如何在资源受限条件下监测城市排水网络并预测流量状况是主要挑战。

Method: 使用SWMM模型生成训练数据集，应用数据驱动稀疏传感框架，利用奇异值分解降维和QR分解进行传感器分配，基于模拟训练数据集识别最优监测节点。

Result: 在77个节点中仅需3个优化布设的传感器即可实现满意重建效果，纳什-萨克利夫效率值为0.92-0.95。模型对测量不确定性具有良好鲁棒性，对传感器故障的鲁棒性随部署传感器数量增加而改善。

Conclusion: 该框架平衡计算效率和物理可解释性，能以最少传感器实现高精度流量重建，可进一步与预测模型集成，在有限传感和监测资源下实现洪水预警和实时控制。

Abstract: Urban surface water flooding, triggered by intense rainfall overwhelming
drainage systems, is increasingly frequent and widespread. While flood
prediction and monitoring in high spatial-temporal resolution are desired,
practical constraints in time, budget, and technology hinder its full
implementation. How to monitor urban drainage networks and predict flow
conditions under constrained resource is a major challenge. This study presents
a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to
optimize sensor placement and reconstruct peak flowrates in a stormwater
system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case
study. We utilized a SWMM model to generate a training dataset of peak flowrate
profiles across the stormwater network. Furthermore, we applied DSS -
leveraging singular value decomposition for dimensionality reduction and QR
factorization for sensor allocation - to identify the optimal monitoring nodes
based on the simulated training dataset. We then validated the
representativeness of these identified monitoring nodes by comparing the
DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three
optimally placed sensors among 77 nodes achieved satisfactory reconstruction
performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to
75th percentiles). In addition, the model showed good robustness to uncertainty
in measurements. Its robustness to sensor failures is location-dependent and
improves with the number of sensors deployed. The framework balances
computational efficiency and physical interpretability, enabling high-accuracy
flow reconstruction with minimal sensors. This DSS framework can be further
integrated with predictive models to realize flood early warning and real-time
control under limited sensing and monitoring resource.

</details>


### [30] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: 开发了Jr. AI Scientist系统，这是一个模拟学生研究流程的自主AI科学家系统，能够分析论文局限性、提出假设、实验验证并撰写论文，在评估中表现优于现有全自动系统，但仍存在重要局限性。


<details>
  <summary>Details</summary>
Motivation: 理解当前AI科学家系统的能力和风险对于确保可信赖和可持续的AI驱动科学进步至关重要，同时保护学术生态系统的完整性。

Method: 开发Jr. AI Scientist系统，模拟新手学生研究者的核心研究流程：分析基线论文局限性、制定改进假设、通过严格实验验证、撰写结果论文，利用现代编码代理处理复杂多文件实现。

Result: 评估显示Jr. AI Scientist生成的论文获得比现有全自动系统更高的评审分数，但作者评估和Agents4Science评审都发现了重要局限性。

Conclusion: 当前AI科学家系统存在直接应用的风险和关键挑战，开发过程中识别了各种风险，这些见解有助于加深对AI科学家发展现状和风险的理解。

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


### [31] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 本文提出将自然语言查询中的歧义视为合作交互特征而非缺陷的框架，分析了15个流行数据集中的查询类型，发现现有评估方法混淆了不同类型查询，无法准确评估系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将自然语言查询中的歧义视为需要修复的缺陷，但作者认为这实际上是合作交互的特征，用户和系统应共同承担查询规范的责任。

Method: 开发了一个原则性框架来区分合作查询（可解析）和非合作查询（无法解析），并将该框架应用于15个流行表格问答和分析数据集的查询分析。

Result: 发现现有数据集中的查询类型混淆不清，既不适合评估系统执行准确性，也不适合评估解释能力。大多数数据集未能适当区分不同类型的查询。

Conclusion: 该框架将视角从修复歧义转向在解析查询中拥抱合作，为表格数据自然语言接口的设计和评估提供了更明智的方法，并指出了未来研究方向。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [32] [Question the Questions: Auditing Representation in Online Deliberative Processes](https://arxiv.org/abs/2511.04588)
*Soham De,Lodewijk Gelauff,Ashish Goel,Smitha Milli,Ariel Procaccia,Alice Siu*

Main category: cs.AI

TL;DR: 提出了一个基于正当代表性概念的审计框架，用于衡量专家问答环节中问题选择的代表性水平，并开发了高效的审计算法。


<details>
  <summary>Details</summary>
Motivation: 在公民大会等审议过程中，参与者只能选择有限数量的问题向专家提问，如何确保所选问题能代表所有参与者的利益是一个重要挑战。

Method: 基于社会选择理论中的正当代表性概念建立审计框架，开发了O(mn log n)时间复杂度的算法，并应用于历史审议数据，比较了主持人选择、整数线性规划选择和LLM生成问题三种方法的代表性。

Result: 研究结果揭示了LLM在支持审议过程中的潜力和当前局限性，并将方法集成到已在50多个国家使用的在线审议平台中。

Conclusion: 该框架为实践者提供了审计和改进审议过程中代表性的工具，有助于提升审议过程的公平性和包容性。

Abstract: A central feature of many deliberative processes, such as citizens'
assemblies and deliberative polls, is the opportunity for participants to
engage directly with experts. While participants are typically invited to
propose questions for expert panels, only a limited number can be selected due
to time constraints. This raises the challenge of how to choose a small set of
questions that best represent the interests of all participants. We introduce
an auditing framework for measuring the level of representation provided by a
slate of questions, based on the social choice concept known as justified
representation (JR). We present the first algorithms for auditing JR in the
general utility setting, with our most efficient algorithm achieving a runtime
of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number
of proposed questions. We apply our auditing methods to historical
deliberations, comparing the representativeness of (a) the actual questions
posed to the expert panel (chosen by a moderator), (b) participants' questions
chosen via integer linear programming, (c) summary questions generated by large
language models (LLMs). Our results highlight both the promise and current
limitations of LLMs in supporting deliberative processes. By integrating our
methods into an online deliberation platform that has been used for over
hundreds of deliberations across more than 50 countries, we make it easy for
practitioners to audit and improve representation in future deliberations.

</details>


### [33] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: DR.WELL是一个去中心化的神经符号框架，用于协作多智能体规划，通过两阶段协商协议和符号规划避免轨迹级协调的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体在部分信息和有限通信下进行联合决策时，轨迹级协调容易因微小偏差导致冲突的问题。

Method: 采用两阶段协商协议：智能体先提出候选角色及推理，然后在共识和环境约束下承诺联合分配；承诺后各自独立生成和执行符号计划，通过共享世界模型进行执行结果落地。

Result: 在协作推块任务实验中，智能体能够跨回合适应，动态世界模型捕获可重用模式，提高了任务完成率和效率。

Conclusion: 通过符号规划而非原始轨迹推理，DR.WELL避免了脆弱的步级对齐，实现了更高级、可重用、可同步和可解释的操作，以时间开销换取演化出更高效的协作策略。

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [34] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: VeriCoT是一种神经符号方法，从CoT推理中提取并验证形式逻辑论证，通过一阶逻辑形式化推理步骤，使用自动求解器验证逻辑有效性，提高推理可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然能通过CoT进行多步推理，但无法可靠验证自身逻辑，即使在得出正确答案时底层推理也可能存在缺陷，这在高风险场景中会削弱信任。

Method: VeriCoT将每个CoT推理步骤形式化为一级逻辑，识别基于源上下文、常识知识或先前推理步骤的前提，使用符号表示和自动求解器验证逻辑有效性。

Result: 在ProofWriter、LegalBench和BioASQ数据集上的实验表明，VeriCoT能有效识别有缺陷的推理，并作为最终答案正确性的强预测指标。

Conclusion: VeriCoT的验证信号可用于推理时自我反思、监督微调和偏好微调，进一步提高了推理的有效性和准确性。

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [35] [Age of Job Completion Minimization with Stable Queues](https://arxiv.org/abs/2511.04630)
*Stavros Mitrolaris,Subhankar Banerjee,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究了一个具有马尔可夫状态机器的作业分配系统，提出了最小化作业完成年龄和采样成本的策略，并分析了队列稳定性条件。


<details>
  <summary>Details</summary>
Motivation: 在具有随机作业到达和马尔可夫状态变化的机器系统中，需要设计有效的调度策略来最大化单位时间内完成的作业数量，同时考虑采样成本。

Method: 提出了两种策略来最小化作业完成年龄和采样成本，通过分析系统状态信息来调度用户作业。

Result: 数值评估了所提策略的性能，并找到了保证作业队列稳定的充分条件。

Conclusion: 所提出的策略能够有效管理具有马尔可夫状态机器的作业分配系统，在满足稳定性条件下优化系统性能。

Abstract: We consider a time-slotted job-assignment system with a central server, N
users and a machine which changes its state according to a Markov chain (hence
called a Markov machine). The users submit their jobs to the central server
according to a stochastic job arrival process. For each user, the server has a
dedicated job queue. Upon receiving a job from a user, the server stores that
job in the corresponding queue. When the machine is not working on a job
assigned by the server, the machine can be either in internally busy or in free
state, and the dynamics of these states follow a binary symmetric Markov chain.
Upon sampling the state information of the machine, if the server identifies
that the machine is in the free state, it schedules a user and submits a job to
the machine from the job queue of the scheduled user. To maximize the number of
jobs completed per unit time, we introduce a new metric, referred to as the age
of job completion. To minimize the age of job completion and the sampling cost,
we propose two policies and numerically evaluate their performance. For both of
these policies, we find sufficient conditions under which the job queues will
remain stable.

</details>


### [36] [Environment Division Multiple Access (EDMA): A Feasibility Study via Pinching Antennas](https://arxiv.org/abs/2511.03820)
*Zhiguo Ding,Robert Schober,H. V. Poor*

Main category: cs.IT

TL;DR: 本文提出了一种新的多址技术——环境分割多址(EDMA)，利用无线传播环境的动态特性，通过捏合天线智能重构多用户传播环境，在不需复杂信号处理的情况下增强目标接收器信号强度并抑制多址干扰。


<details>
  <summary>Details</summary>
Motivation: 传统多址技术需要复杂的信号处理如预编码、波束成形或多用户检测，而EDMA旨在通过物理环境重构来简化系统复杂度并提升性能。

Method: 采用捏合天线重新配置视距链路，通过精心放置天线位置来有意阻断干扰链路。开发了两种低复杂度算法分别用于上行和下行传输。

Result: 推导了EDMA相比传统多址技术的遍历和速率增益闭式表达式，仿真结果表明所提算法在性能上接近穷举搜索的最优解。

Conclusion: EDMA在支持多用户通信方面具有显著潜力，通过物理环境重构实现了性能提升和复杂度降低。

Abstract: This paper exploits the dynamic features of wireless propagation environments
as the basis for a new multiple access technique, termed environment division
multiple access (EDMA). In particular, with the proposed
pinching-antenna-assisted EDMA, the multi-user propagation environment is
intelligently reconfigured to improve signal strength at intended receivers and
simultaneously suppress multiple-access interference, without requiring complex
signal processing, e.g., precoding, beamforming, or multi-user detection. The
key to creating a favorable propagation environment is to utilize the
capability of pinching antennas to reconfigure line-of-sight (LoS) links, e.g.,
pinching antennas are placed at specific locations, such that interference
links are blocked on purpose. Based on a straightforward choice of
pinching-antenna locations, the ergodic sum-rate gain of EDMA over conventional
multiple access and the probability that EDMA achieves a larger instantaneous
sum rate than the considered benchmarking scheme are derived in closed form.
The obtained analytical results demonstrate the significant potential of EDMA
for supporting multi-user communications. Furthermore, pinching antenna
location optimization is also investigated, since the locations of pinching
antennas are critical for reconfiguring LoS links and large-scale path losses.
Two low-complexity algorithms are developed for uplink and downlink
transmission, respectively, and simulation results are provided to show their
optimality in comparison to exhaustive searches.

</details>


### [37] [Which Similarity-Sensitive Entropy?](https://arxiv.org/abs/2511.03849)
*Phuc Nguyen,Josiah Couch,Rahul Bansal,Alexandra Morgan,Chris Tam,Miao Li,Rima Arnaout,Ramy Arnaout*

Main category: cs.IT

TL;DR: 本文比较了两种相似性敏感熵度量方法：Leinster-Cobbold-Reeve (LCR) 和 Vendi score (VS)，通过概念、分析和实验验证，发现两者在量级和捕获信息方面存在显著差异，并确定了各自适用的场景。


<details>
  <summary>Details</summary>
Motivation: 传统熵度量仅捕获系统元素频率信息，而LCR和VS方法能够捕获元素间相似性和差异性的丰富信息。本文旨在比较这两种方法的优劣，确定哪种方法更适合不同场景。

Method: 使用53个机器学习数据集进行概念分析、理论分析和实验验证，引入"半距离"概念来参数化相似性缩放的影响，并证明VS对LCR的边界关系。

Result: LCR和VS在量级上可能相差数个数量级，且捕获系统互补信息（除极限情况外）。VS为LCR提供了多个Rényi-Hill阶参数值的上界，并推测该边界对所有值都成立。

Conclusion: VS仅在将元素解释为更基本"原始元素"的线性组合或系统具有量子力学特性时更优；在更广泛情况下，若仅需捕获相似性编码的丰富信息，LCR更受青睐；但在某些半距离下，两种方法可以互补使用。

Abstract: A canonical step in quantifying a system is to measure its entropy. Shannon
entropy and other traditional entropy measures capture only the information
encoded in the frequencies of a system's elements. Recently, Leinster, Cobbold,
and Reeve (LCR) introduced a method that also captures the rich information
encoded in the similarities and differences among elements, yielding
similarity-sensitive entropy. More recently, the Vendi score (VS) was
introduced as an alternative, raising the question of how LCR and VS compare,
and which is preferable. Here we address these questions conceptually,
analytically, and experimentally, using 53 machine-learning datasets. We show
that LCR and VS can differ by orders of magnitude and can capture complementary
information about a system, except in limiting cases. We demonstrate that both
LCR and VS depend on how similarities are scaled and introduce the concept of
``half distance'' to parameterize this dependence. We prove that VS provides an
upper bound on LCR for several values of the R\'enyi-Hill order parameter and
conjecture that this bound holds for all values. We conclude that VS is
preferable only when interpreting elements as linear combinations of a more
fundamental set of ``ur-elements'' or when the system or dataset possesses a
quantum-mechanical character. In the broader circumstance where one seeks
simply to capture the rich information encoded by similarity, LCR is favored;
nevertheless, for certain half-distances the two methods can complement each
other.

</details>


### [38] [Efficient and rate-optimal list-decoding in the presence of minimal feedback: Weldon and Slepian-Wolf in sheep's clothing](https://arxiv.org/abs/2511.04088)
*Pranav Joshi,Daniel McMorrow,Yihan Zhang,Amitalok J. Budkuley,Sidharth Jaggi*

Main category: cs.IT

TL;DR: 本文提出了第一个适用于任何q≥2的编码方案，在存在低速率反馈的情况下，能够以接近信息论最优的速率进行通信，同时确保接收方能够推断出发送方的消息来自一个"小"集合。


<details>
  <summary>Details</summary>
Motivation: 现有编码方案在大q值时具有计算可行的编码/解码过程，但对于任意q≥2的情况缺乏有效方案。本文旨在解决在任意q≥2和存在符号损坏的情况下，实现接近信息论最优速率的通信问题。

Method: 提出了一种最小反馈方案，利用渐近可忽略的反馈速率（相对于传输次数），通过精心设计的编码结构和参数选择，实现了高效的编码和解码过程。

Result: 该方案在足够小的ε>0和ρ∈(1-1/q-Θ(√ε))条件下，实现了速率1-H_q(ρ)-ε（接近信息论最优）、列表大小exp(O(ε^{-3/2}log²(1/ε)))、编码/解码计算复杂度n^{O(ε^{-1}log(1/ε))}、存储复杂度O(n^{η+1}log n)，错误概率为O(n^{-η})，反馈速率为O(1/log n)。

Conclusion: 本文首次证明了在任意q≥2和存在低速率反馈的情况下，可以实现接近信息论最优速率的通信，同时保持较小的列表大小和合理的计算复杂度，为对抗性信道下的可靠通信提供了新的解决方案。

Abstract: Given a channel with length-$n$ inputs and outputs over the alphabet
$\{0,1,\ldots,q-1\}$, and of which a fraction $\varrho \in (0,1-1/q)$ of
symbols can be arbitrarily corrupted by an adversary, a fundamental problem is
that of communicating at rates close to the information-theoretically optimal
values, while ensuring the receiver can infer that the transmitter's message is
from a ``small" set. While the existence of such codes is known, and
constructions with computationally tractable encoding/decoding procedures are
known for large $q$, we provide the first schemes that attain this performance
for any $q \geq 2$, as long as low-rate feedback (asymptotically negligible
relative to the number of transmissions) from the receiver to the transmitter
is available. For any sufficiently small $\varepsilon > 0$ and $\varrho \in
(1-1/q-\Theta(\sqrt{\varepsilon})$ our minimal feedback scheme has the
following parameters: Rate $1-H_q(\varrho) - \varepsilon$ (i.e.,
$\varepsilon$-close to information-theoretically optimal -- here $H_q(\varrho)$
is the $q$-ary entropy function), list-size
$\exp(\mathcal{O}(\varepsilon^{-3/2}\log^2(1/\varepsilon))$, computational
complexity of encoding/decoding
$n^{\mathcal{O}(\varepsilon^{-1}\log(1/\varepsilon))}$, storage complexity
$\mathcal{O}(n^{\eta+1}\log n)$ for a code design parameter $\eta>1$ that
trades off storage complexity with the probability of error. The error
probability is $\mathcal{O}(n^{-\eta})$, and the (vanishing) feedback rate is
$\mathcal{O}(1/ \log n)$.

</details>


### [39] [List Decoding of Folded Reed-Solomon Codes Over Galois Ring](https://arxiv.org/abs/2511.04135)
*Chen Yuan,Ruiqi Zhu*

Main category: cs.IT

TL;DR: 本文扩展了Guruswami-Sudan列表解码算法到伽罗瓦环上的Reed-Solomon码，证明了速率为r的RS码可以列表解码到半径1-√r，并研究了折叠RS码在伽罗瓦环上的列表解码，达到了Singleton界，同时将列表大小改进到O(1/ε²)。


<details>
  <summary>Details</summary>
Motivation: 由于零知识证明系统的进展，需要研究伽罗瓦环上码的邻近间隙问题。有限域上的RS码邻近间隙在列表解码下可改进到1-√r，但对伽罗瓦环上RS码的了解很少，这阻碍了基于环的算术电路的零知识证明系统发展。

Method: 首先将Guruswami-Sudan列表解码程序扩展到伽罗瓦环上的Reed-Solomon码，然后研究伽罗瓦环上折叠Reed-Solomon码的列表解码，最后将Shashank Srivastava(2025)的工作扩展到伽罗瓦环以改进列表大小。

Result: 证明了伽罗瓦环上速率为r的RS码可以列表解码到半径1-√r；折叠RS码的列表解码半径达到Singleton界；将折叠RS码的列表大小改进到O(1/ε²)。

Conclusion: 成功将列表解码技术扩展到伽罗瓦环上的RS码和折叠RS码，为基于环的零知识证明系统提供了重要的理论基础和解码能力保证。

Abstract: List decoding of codes can be seen as the generalization of unique decoding
of codes While list decoding over finite fields has been extensively studied,
extending these results to more general algebraic structures such as Galois
rings remains an important challenge. Due to recent progress in zero knowledge
systems, there is a growing demand to investigate the proximity gap of codes
over Galois rings in Yizhou Yao and coauthors(2025), Alexander Golovne and
coauthors(2023), Yuanju Wei and coauthors(2025). The proximity gap is closely
related to the decoding capability of codes. It was shown in Eli Ben-Sasson and
coauthors(2020) that the proximity gap for RS codes over finite field can be
improved to $1-\sqrt{r}$ if one consider list decoding instead of unique
decoding. However, we know very little about RS codes over Galois ring which
might hinder the development of zero knowledge proof system for ring-based
arithmetic circuit. In this work, we first extend the list decoding procedure
of Guruswami and Sudan to Reed-Solomon codes over Galois rings, which shows
that RS codes with rate $r$ can be list decoded up to radius $1-\sqrt{r}$.
Then, we investigate the list decoding of folded Reed-Solomon codes over Galois
rings. We show that the list decoding radius of folded Reed-Solomon codes can
reach the Singlton bound as its counterpart over finite field. Finally, we
improve the list size of our folded Reed-Solomon code to
$O(\frac{1}{\varepsilon^2})$ by extending recent work in Shashank
Srivastava(2025) to Galois Rings.

</details>


### [40] [Affine Frequency Division Multiplexing: From Communication to Sensing](https://arxiv.org/abs/2511.04471)
*Ali Bemani,Nassar Ksairi,Marios Kountouris*

Main category: cs.IT

TL;DR: AFDM波形在集成感知与通信系统中解决两个关键挑战：支持大带宽的同时保持低复杂度接收机，以及多雷达环境中的干扰抑制。


<details>
  <summary>Details</summary>
Motivation: 解决ISAC系统中大带宽需求与接收机复杂度/能耗的矛盾，以及多雷达环境中的干扰问题。

Method: 利用AFDM波形的特性：兼容低复杂度自干扰消除方案、支持模拟去啁啾降低采样率、离散仿射傅里叶变换的资源分配灵活性。

Result: 在单站感知中实现低复杂度自干扰消除和降低采样率；在双站感知中支持亚奈奎斯特采样且保持延迟分辨率；有效抑制多雷达干扰。

Conclusion: AFDM波形通过其独特特性有效解决了ISAC系统在大带宽和干扰抑制方面的关键挑战，为实际部署提供了可行方案。

Abstract: Affine Frequency Division Multiplexing (AFDM) has been proposed as an
effective waveform for achieving the full diversity of doubly-dispersive
(delay-Doppler) channels. While this property is closely related to range and
velocity estimation in sensing, this article focuses on other AFDM features
that are particularly relevant for addressing two challenges in integrated
sensing and communication (ISAC) systems: (1) maintaining receiver complexity
and energy consumption at acceptable levels while supporting the large
bandwidths required for high delay/range resolution, and (2) mitigating
interference in multiradar environments. In monostatic sensing, where direct
transmitter-receiver leakage is a major impairment, we show that AFDM-based
ISAC receivers can address the first challenge through their compatibility with
low-complexity self-interference cancellation (SIC) schemes and reduced
sampling rates via analog dechirping. In bistatic sensing, where such analog
solutions may not be feasible, we demonstrate that AFDM supports sub-Nyquist
sampling without requiring hardware modifications while preserving delay
resolution. Finally, we show that the second challenge can be addressed by
leveraging the resource-assignment flexibility of the discrete affine Fourier
transform (DAFT) underlying the AFDM waveform.

</details>
