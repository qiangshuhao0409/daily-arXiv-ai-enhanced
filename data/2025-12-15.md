<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 4]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.IT](#cs.IT) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [SHIFT: An RDMA Failure-Resilient Layer for Distributed Training](https://arxiv.org/abs/2512.11094)
*Shengkai Lin,Kairui Zhou,Yibo Wu,Hongtao Zhang,Qinwei Yang,Wei Zhang,Arvind Krishnamurthy,Shizhen Zhao*

Main category: cs.NI

TL;DR: SHIFT：在RDMA层实现容错，通过跨主机NIC重定向RDMA流量，减少大模型训练中网络异常导致的训练中断


<details>
  <summary>Details</summary>
Motivation: 大规模分布式大语言模型训练中，单个网络异常会通过gang scheduling传播导致整个任务失败。现有容错机制（如检查点）在应用层运行，仍会造成训练中断。

Method: 在RDMA层引入容错机制，设计SHIFT作为RDMA上的容错层，支持在不同主机内NIC间无缝重定向RDMA流量，通过精心设计的故障状态机和控制流实现。

Result: SHIFT引入的数据路径开销极小，能在网络故障下确保应用连续性，未修改的PyTorch+NCCL应用可直接运行并获得RDMA级容错能力。

Conclusion: SHIFT通过RDMA层容错与现有应用层技术结合，最小化训练进度损失，具有应用无关、对应用透明、低开销的特点。

Abstract: With gang scheduling in large-scale distributed Large Language Model training, a single network anomaly can propagate and cause complete task failure. The frequency of such anomalies increases with network scale. However, existing fault-tolerance mechanisms, such as checkpointing and runtime resilience methods, primarily operate at the application layer and inevitably cause disruptions in training progress.
  We propose to address this challenge by introducing fault tolerance at the Remote Direct Memory Access (RDMA) layer and integrating it with existing application-layer techniques. We present SHIFT, a fault-resilient layer over RDMA that enables seamless redirection of RDMA traffic across different intra-host NICs. By allowing applications to continue execution in the presence of network anomalies until the next checkpoint, SHIFT effectively minimizes training progress loss. SHIFT is designed to be application-agnostic, transparent to applications, and low-overhead.
  Through a carefully designed failure state machine and control flow, unmodified applications such as PyTorch with NCCL can run with RDMA-level fault tolerance. Experimental results demonstrate that SHIFT introduces minimal data path overhead while ensuring application continuity under network failures.

</details>


### [2] [BIER-Star: Stateless Geographic Multicast for Scalable Satellite-Terrestrial Integration](https://arxiv.org/abs/2512.11156)
*Mostafa Abdollahi,Wenjun Yang,Jianping Pan*

Main category: cs.NI

TL;DR: BIER-Star：一种用于天地一体化网络的无状态组播协议，采用地理空间网格编码目的地，减少头部开销并适应卫星动态拓扑


<details>
  <summary>Details</summary>
Motivation: 随着LEO卫星星座的快速发展，天地一体化网络需要可扩展、高效的组播协议来支持紧急警报、软件更新等关键应用。传统IP组播和软件定义组播在卫星动态拓扑中存在控制开销大、适应性差的问题。

Method: 提出BIER-Star无状态组播协议，采用两层地理空间网格方案（H3），将目的地编码为地球和空间单元标识符而非终端地址。这种基于单元的抽象缩短了头部比特串，简化了转发，消除了每流状态和复杂信令。

Result: 仿真表明，BIER-Star相比BIER减少了头部大小，避免了贪婪方法中的地理路径查找失败问题。

Conclusion: BIER-Star为天地一体化网络提供了一种高效、可扩展的组播解决方案，通过地理空间网格编码有效应对卫星网络的动态性和移动性挑战。

Abstract: The rapid expansion of LEO satellite constellations has enabled an integrated terrestrial network and non-terrestrial network (TN-NTN), connecting diverse users such as aircraft, ships, and remote communities. These networks increasingly need a scalable and efficient multicast protocol for critical applications like emergency alerts, large-scale software updates, and real-time broadcasting. However, traditional multicast protocols, such as IP-based multicast and software-defined multicast approaches, introduce significant control overhead and struggle to adapt to the dynamic and mobile nature of satellite topologies. This paper presents BIER-Star, a stateless multicast protocol designed for the integrated TN-NTN. BIER-Star uses a two-layer geospatial gridding scheme (i.e., H3) to encode destinations as Earth- and space-cell identifiers rather than per-terminal addresses. This cell-based abstraction shortens the header bitstring, simplifies forwarding, and eliminates per-flow state and complex signaling. Our simulations indicate that BIER-Star reduces header size versus BIER and avoids geographic path-finding failures seen in greedy methods.

</details>


### [3] [Distributed Resource Allocation and Application Deployment in Mesh Edge Networks](https://arxiv.org/abs/2512.11230)
*Antoine Bernard,Antoine Legrain,Maroua Ben Attia,Abdo Shabah*

Main category: cs.NI

TL;DR: 将虚拟网络嵌入扩展到移动边缘设备网络，解决拓扑快速变化和资源受限的挑战，提出三种分配策略并验证性能提升


<details>
  <summary>Details</summary>
Motivation: 传统VNE方法假设静态或缓慢变化的网络拓扑，但新兴移动应用需要在拓扑快速变化的移动环境中部署，传统方法已不适用

Method: 开发包含设备能力、连接性、移动性和能量约束的模型，提出三种分配策略：整数线性规划（最优分配）、贪心启发式（即时部署）和多目标遗传算法（平衡优化）

Result: 初步评估显示在应用接受率、资源利用率和延迟性能方面优于传统方法，为高度移动环境中的VNE部署提供了基础

Conclusion: 成功将VNE扩展到移动边缘网络，提出的方法能够有效处理移动网络的动态特性，在资源受限条件下实现性能改进

Abstract: Virtual Network Embedding (VNE) approaches typically assume static or slowly-changing network topologies, but emerging applications require deployment in mobile environments where traditional methods become insufficient. This work extends VNE to constrained mesh networks of mobile edge devices, addressing the unique challenges of rapid topology changes and limited resources. We develop models incorporating device capabilities, connectivity, mobility and energy constraints to evaluate optimal deployment strategies for mobile edge environments. Our approach handles the dynamic nature of mobile networks through three allocation strategies: an integer linear program for optimal allocation, a greedy heuristic for immediate deployment, and a multi-objective genetic algorithm for balanced optimization. Our initial evaluation analyzes application acceptance rates, resource utilization, and latency performance under resource limitations. Results demonstrate improvements over traditional approaches, providing a foundation for VNE deployment in highly mobile environments.

</details>


### [4] [Toward Scalable VR-Cloud Gaming: An Attention-aware Adaptive Resource Allocation Framework for 6G Networks](https://arxiv.org/abs/2512.11667)
*Gabriel Almeida,João Paulo Esper,Cleverson Nahum,Audebaro Klautau,Kleber Vieira Cardoso*

Main category: cs.NI

TL;DR: 提出一个面向6G网络的VR云游戏资源分配框架，通过三阶段优化显著提升用户体验质量，同时降低资源消耗和成本。


<details>
  <summary>Details</summary>
Motivation: VR云游戏对带宽、延迟和资源管理要求极高，现有方案难以同时满足这些需求，需要一种可扩展且QoE感知的优化框架。

Method: 将资源分配问题分解为三个相互依赖的阶段：用户关联与通信资源分配、VR游戏引擎放置与自适应多路径路由、基于运动到光子延迟的注意力感知调度与无线资源分配。为每个阶段设计专门的启发式算法，并引入基于视觉注意力的用户中心QoE模型。

Result: 相比现有方法，QoE提升高达50%，通信资源使用减少75%，成本节省达35%，平均最优性差距仅为5%。启发式算法能在0.1秒内解决大规模场景。

Conclusion: 该框架为6G网络中的VR云游戏提供了一种高效、可扩展的资源分配方案，具有实时部署潜力，能显著改善用户体验并降低运营成本。

Abstract: Virtual Reality Cloud Gaming (VR-CG) represents a demanding class of immersive applications, requiring high bandwidth, ultra-low latency, and intelligent resource management to ensure optimal user experience. In this paper, we propose a scalable and QoE-aware multi-stage optimization framework for resource allocation in VR-CG over 6G networks. Our solution decomposes the joint resource allocation problem into three interdependent stages: (i) user association and communication resource allocation; (ii) VR-CG game engine placement with adaptive multipath routing; and (iii) attention-aware scheduling and wireless resource allocation based on motion-to-photon latency. For each stage, we design specialized heuristic algorithms that achieve near-optimal performance while significantly reducing computational time. We introduce a novel user-centric QoE model based on visual attention to virtual objects, guiding adaptive resolution and frame rate selection. A dataset-driven evaluation demonstrates that, when compared against state-of-the-art approaches, our framework improves QoE by up to 50\%, reduces communication resource usage by 75\%, and achieves up to 35\% cost savings, while maintaining an average optimality gap of 5\%. Our proposed heuristics solve large-scale scenarios in under 0.1 seconds, highlighting their potential for real-time deployment in next-generation mobile networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound](https://arxiv.org/abs/2512.11169)
*Akhil S Anand,Elias Aarekol,Martin Mziray Dalseg,Magnus Stalhane,Sebastien Gros*

Main category: cs.AI

TL;DR: 提出CORL框架，使用强化学习端到端微调MILP方案，将分支定界求解的MILP转化为可微随机策略，以最大化实际操作性能而非精确建模。


<details>
  <summary>Details</summary>
Motivation: 传统MILP建模难以准确表示随机现实问题，导致实际性能不佳。现有机器学习方法依赖监督学习、假设能获取最优决策真值、使用MILP梯度的代理，存在局限性。

Method: 提出CORL框架，将分支定界算法求解的混合整数线性规划转化为可微随机策略，使其与强化学习兼容，在真实数据上端到端微调MILP方案。

Result: 在简单的组合序贯决策示例中验证了CORL方法的有效性，展示了框架的可行性。

Conclusion: CORL框架通过强化学习端到端优化MILP方案，避免了传统建模的局限性，能够直接最大化实际操作性能，为组合序贯决策问题提供了新思路。

Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.

</details>


### [6] [Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling](https://arxiv.org/abs/2512.11187)
*Haohui Zhang,Wouter van Heeswijk,Xinyu Hu,Neil Yorke-Smith,Martijn Mes*

Main category: cs.AI

TL;DR: 提出混合搜索框架，结合Transformer神经网络构造策略与多起点大邻域搜索，解决在线货运交易系统的组合捆绑问题，在亚秒延迟内实现高质量解。


<details>
  <summary>Details</summary>
Motivation: 在线货运交易系统需要实时匹配货主与承运商，但高效组合捆绑运输任务仍是瓶颈。现有方法难以在亚秒延迟内同时处理组合捆绑选择和取送货路径规划。

Method: 将问题建模为多商品一对一取送货选择性旅行商问题(m1-PDSTSP)。提出学习加速混合搜索管道：Transformer神经网络构造策略生成初始解，结合创新的多起点大邻域搜索元启发式，在滚动时域框架下重复冻结市场快照并求解。

Result: 在基准测试中优于现有神经组合优化和元启发式基线方法，在可比时间内获得更高质量解，相对于最佳精确基线方法的总收入最优性差距小于2%。

Conclusion: 首次证明基于深度神经网络的构造器能够为（多起点）改进启发式提供高质量种子解，该方法不仅适用于m1-PDSTSP，还可推广到更广泛的选择性旅行商问题和取送货问题。

Abstract: Online Freight Exchange Systems (OFEX) play a crucial role in modern freight logistics by facilitating real-time matching between shippers and carrier. However, efficient combinatorial bundling of transporation jobs remains a bottleneck. We model the OFEX combinatorial bundling problem as a multi-commodity one-to-one pickup-and-delivery selective traveling salesperson problem (m1-PDSTSP), which optimizes revenue-driven freight bundling under capacity, precedence, and route-length constraints. The key challenge is to couple combinatorial bundle selection with pickup-and-delivery routing under sub-second latency. We propose a learning--accelerated hybrid search pipeline that pairs a Transformer Neural Network-based constructive policy with an innovative Multi-Start Large Neighborhood Search (MSLNS) metaheuristic within a rolling-horizon scheme in which the platform repeatedly freezes the current marketplace into a static snapshot and solves it under a short time budget. This pairing leverages the low-latency, high-quality inference of the learning-based constructor alongside the robustness of improvement search; the multi-start design and plausible seeds help LNS to explore the solution space more efficiently. Across benchmarks, our method outperforms state-of-the-art neural combinatorial optimization and metaheuristic baselines in solution quality with comparable time, achieving an optimality gap of less than 2\% in total revenue relative to the best available exact baseline method. To our knowledge, this is the first work to establish that a Deep Neural Network-based constructor can reliably provide high-quality seeds for (multi-start) improvement heuristics, with applicability beyond the \textit{m1-PDSTSP} to a broad class of selective traveling salesperson problems and pickup and delivery problems.

</details>


### [7] [FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration](https://arxiv.org/abs/2512.11213)
*Dongwon Jung,Peng Shi,Yi Zhang*

Main category: cs.AI

TL;DR: FutureWeaver：一个在固定预算下规划和优化多智能体系统中测试时计算分配的框架，通过模块化协作和双级规划提升多智能体协作性能


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展技术（如重复采样、自我验证、自我反思）在单智能体场景中能显著提升性能，但难以应用于多智能体系统。缺乏原则性机制来分配计算以促进智能体间协作，将测试时计算扩展扩展到协作交互，或在明确预算约束下跨智能体分配计算。

Method: 1. 引入模块化协作，定义为封装可重用多智能体工作流的可调用函数；2. 通过自我反思从过去轨迹中抽象出重复交互模式，自动推导这些模块；3. 基于这些模块，采用双级规划架构，在推理当前任务状态的同时推测未来步骤，优化计算分配。

Result: 在复杂智能体基准测试上的实验表明，FutureWeaver在不同预算设置下始终优于基线方法，验证了其在推理时优化中多智能体协作的有效性。

Conclusion: FutureWeaver填补了多智能体系统中测试时计算分配的原则性机制空白，通过模块化协作和前瞻性规划，在固定预算约束下有效提升多智能体协作性能，为推理时优化提供了新框架。

Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.

</details>


### [8] [A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation](https://arxiv.org/abs/2512.11270)
*Hong Je-Gal,Chan-Bin Yi,Hyun-Suk Lee*

Main category: cs.AI

TL;DR: A-LAMP是一个基于大语言模型的自动化框架，能够将自然语言任务描述自动转换为MDP建模和训练好的策略，通过分解建模、编码和训练为可验证阶段来确保语义对齐。


<details>
  <summary>Details</summary>
Motivation: 将强化学习应用于现实任务需要将非正式描述转换为正式的MDP、实现可执行环境并训练策略代理。自动化这一过程面临建模错误、脆弱代码和目标不对齐等挑战，这些因素常常阻碍策略训练。

Method: 引入基于大语言模型的自动化MDP建模和策略生成框架(A-LAMP)，将自由形式的自然语言任务描述自动转换为MDP公式和训练好的策略。该框架将建模、编码和训练分解为可验证阶段，确保整个流程的语义对齐。

Result: 在经典控制和自定义RL领域中，A-LAMP始终比单个最先进的大语言模型获得更高的策略生成能力。值得注意的是，即使是基于较小语言模型的轻量级变体，也能接近更大模型的性能。失败分析揭示了这些改进的原因。

Conclusion: A-LAMP框架能够自动生成环境和策略，同时保持任务的最优性，证实了其正确性和可靠性，为自动化强化学习应用提供了有效的解决方案。

Abstract: Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.

</details>


### [9] [TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning](https://arxiv.org/abs/2512.11271)
*Yuxing Chen,Basem Suleiman,Qifan Chen*

Main category: cs.AI

TL;DR: TriFlow是一个用于真实世界行程规划的多智能体框架，通过检索-规划-治理三阶段流水线，结合结构化推理和语言灵活性，显著提升了约束满足能力和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在行程规划中存在约束满足困难、工具协调不足和效率低下问题，经常产生不可行或成本过高的计划，需要更有效的解决方案。

Method: 提出TriFlow渐进式多智能体框架，采用三阶段流水线：检索阶段缩小搜索空间，规划阶段通过规则-LLM协作组装约束一致的行程，治理阶段进行有界迭代优化确保全局可行性和个性化。

Result: 在TravelPlanner和TripTailor基准测试中达到最先进水平，分别获得91.1%和97%的最终通过率，相比当前SOTA实现了超过10倍的运行时效率提升。

Conclusion: TriFlow通过结合结构化推理和语言灵活性，有效解决了真实世界行程规划中的约束满足、工具协调和效率问题，为开放端用户请求到可执行行程的转换提供了实用解决方案。

Abstract: Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.

</details>


### [10] [CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving](https://arxiv.org/abs/2512.11323)
*Jianyi Zhang,Ziyin Zhou,Xu Ji,Shizhao Liu,Zhangchi Zhao*

Main category: cs.AI

TL;DR: 首次为大型视觉语言模型（LVLMs）设计的CAPTCHA基准测试，涵盖4种主要类型和25种子类型，来自31个供应商，用于全面评估LVLMs解决验证码的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉验证码的基准测试存在局限性，无法全面覆盖所有验证码类型，且缺乏专门针对LVLMs的基准测试，需要更全面的评估工具。

Method: 创建名为CAPTURE（CAPTCHA for Testing Under Real-world Experiments）的新基准测试，包含4种主要验证码类型和25种子类型，来自31个供应商，具有广泛的类别多样性、大规模数据和专门为LVLMs定制的标签。

Result: 使用该基准测试评估当前LVLMs时，发现它们在解决验证码方面表现不佳，显示出LVLMs在实际验证码场景中的局限性。

Conclusion: CAPTURE基准测试填补了先前研究在数据全面性和标签针对性方面的空白，为LVLMs提供了多维度的全面评估工具，揭示了当前LVLMs在验证码解决能力上的不足。

Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.

</details>


### [11] [Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance](https://arxiv.org/abs/2512.11421)
*Gonca Gürsun*

Main category: cs.AI

TL;DR: 提出一个LLM智能体任务完成框架，通过强化学习形式化描述环境，结合任务分析器、推理模块和生成模块，确保行为可靠可验证


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮任务中行为缺乏可靠性和可验证性，需要一种框架使LLM智能体能在明确定义的环境中表现出可信行为

Method: 1) 轻量级任务分析器选择推理和生成策略；2) 推理模块学习可验证的观察-动作映射；3) 生成模块通过验证或确定性合成确保约束合规输出。三个组件在智能体与环境交互中协同演化

Result: 框架使LLM智能体能够在强化学习形式化描述的环境中表现出可信行为，各组件协同演化产生可靠行为

Conclusion: 该框架通过整合任务分析、可验证推理和约束合规生成，解决了LLM在多轮任务中的可靠性和可验证性问题，实现了可信的智能体行为

Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.

</details>


### [12] [AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints](https://arxiv.org/abs/2512.11426)
*Shuowei Cai,Yansong Ning,Hao Liu*

Main category: cs.AI

TL;DR: AgentBalance是一个在明确token成本和延迟预算下构建成本效益多智能体系统的框架，采用"先骨干后拓扑"设计，相比现有方法在相同预算下性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体系统在web规模应用中越来越重要，但成本效益成为大规模部署的主要约束。现有工作很少在明确的token成本和延迟预算下进行建模和优化，导致预算约束时成本效益不佳。

Method: 采用"先骨干后拓扑"设计：1) 骨干导向的智能体生成：通过LLM池构建、池选择和角色-骨干匹配构建异构骨干智能体；2) 自适应MAS拓扑生成：通过智能体表示学习、门控和延迟感知拓扑合成指导智能体间通信。

Result: 在包含14个候选LLM骨干的基准测试中，AgentBalance在匹配的token成本预算下实现高达10%的性能提升，在延迟预算下实现高达22%的性能提升，并在性能-预算曲线上表现出强大的AUC。

Conclusion: AgentBalance是一个有效的框架，可在明确预算约束下构建成本效益的多智能体系统，可作为现有MAS的插件提升性能，并能很好地泛化到未见过的LLM，实现实用的预算感知部署。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance

</details>


### [13] [Back to the Baseline: Examining Baseline Effects on Explainability Metrics](https://arxiv.org/abs/2512.11433)
*Agustin Martin Picard,Thibaut Boissin,Varshini Subhash,Rémi Cadène,Thomas Fel*

Main category: cs.AI

TL;DR: 该论文指出当前XAI中基于基准线的保真度评估指标存在问题，不同基准线会偏向不同的归因方法，甚至线性模型也会得出矛盾结果。作者提出理想基准线应具备移除信息且不产生过度分布外图像的特性，并引入一种新的模型依赖基准线来改进这一权衡。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能中广泛使用的归因方法通常通过插入和删除等保真度指标进行评估。然而，这些指标依赖于基准线函数来修改输入图像像素，而基准线的选择会不可避免地偏向某些归因方法，甚至导致矛盾结果。这引发了一个关键问题：应该使用哪个基准线？

Method: 作者首先提出理想基准线应满足两个属性：(1) 能够移除信息，(2) 不产生过度分布外图像。通过测试现有基准线发现它们无法同时满足这两个标准，存在权衡取舍。然后，作者利用特征可视化技术的最新进展，提出了一种新的模型依赖基准线，能够移除信息而不产生过度分布外图像。

Result: 研究发现现有基准线都无法同时满足移除信息和不产生过度分布外图像的要求，存在明显的权衡关系。作者提出的新基准线在权衡方面优于现有基准线，能够更好地移除信息同时避免产生过度分布外图像。

Conclusion: 基准线选择对XAI归因方法的评估有重大影响，现有基准线存在固有缺陷。通过特征可视化技术构建的模型依赖基准线能够改进这一权衡，为更公平的归因方法评估提供了更好的基准线选择。

Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline

</details>


### [14] [Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes](https://arxiv.org/abs/2512.11463)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Minsu Ha,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.AI

TL;DR: Motif-2-12.7B-Reasoning是一个12.7B参数的语言模型，通过创新的训练方法在复杂推理和长上下文理解方面达到接近前沿专有模型的性能，同时提供了可复现的训练方案。


<details>
  <summary>Details</summary>
Motivation: 解决开源模型与专有前沿模型在复杂推理和长上下文理解方面的性能差距，同时应对推理适应过程中常见的模型崩溃和训练不稳定性问题。

Method: 采用系统、数据和算法优化的综合训练方案：1）使用混合并行和内核级优化的内存高效基础设施支持64K令牌上下文；2）两阶段监督微调课程，通过验证对齐的合成数据缓解分布不匹配；3）强化学习微调管道，通过难度感知数据过滤和混合策略轨迹重用稳定训练。

Result: Motif-2-12.7B-Reasoning在数学、编码和智能体基准测试中达到了与参数数量显著更大的模型相当的性能，在现实计算约束下提供了具有竞争力的开源模型。

Conclusion: 该研究不仅提供了一个性能优异的开源推理模型，更重要的是提供了一个可复现的训练蓝图，展示了如何在有限计算资源下扩展模型的推理能力，为社区提供了实用的技术方案。

Abstract: We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.

</details>


### [15] [Three methods, one problem: Classical and AI approaches to no-three-in-line](https://arxiv.org/abs/2512.11469)
*Pranav Ramanathan,Thomas Prellberg,Matthew Lewis,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: 本文首次系统比较了经典优化方法与AI方法在No-Three-In-Line问题上的表现，发现ILP在19×19网格内可获最优解，PatternBoost在14×14网格内匹配最优性能，PPO在10×10网格表现完美但在11×11失败。


<details>
  <summary>Details</summary>
Motivation: No-Three-In-Line是组合几何中的著名问题，经典方法如整数线性规划(ILP)能保证最优解但面临指数级扩展问题，而机器学习方法为模式近似提供了有前景的替代方案。

Method: 首次将PatternBoost变压器学习和强化学习(PPO)应用于该问题，并与传统ILP算法进行对比。ILP提供可证明的最优解，PatternBoost通过变压器学习模式，PPO通过强化学习探索解空间。

Result: ILP在19×19网格内获得可证明的最优解；PatternBoost在14×14网格内匹配最优性能，测试损失减少96%；PPO在10×10网格获得完美解但在11×11网格失败（约束违反）。

Conclusion: 经典优化方法对于精确解仍然至关重要，而AI方法在较小实例上提供有竞争力的性能，混合方法为扩展到更大问题规模提供了最有前景的方向。

Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

</details>


### [16] [General-purpose AI models can generate actionable knowledge on agroecological crop protection](https://arxiv.org/abs/2512.11474)
*Kris A. G. Wyckhuys*

Main category: cs.AI

TL;DR: 研究评估了DeepSeek和ChatGPT在农业生态作物保护领域的科学知识生成能力，发现DeepSeek在文献覆盖、解决方案数量和效果评估方面表现更好，但两者都存在幻觉、命名混淆等问题，需结合人工监督才能有效支持农场决策。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在民主化科学知识和转化为可操作信息方面具有潜力，但在农业食品科学领域的应用尚未探索。研究旨在验证大型语言模型在农业生态作物保护领域的科学知识生成能力。

Method: 针对9种全球限制性病虫害和杂草，评估DeepSeek（基于网络）和ChatGPT免费版（非基于网络）的事实准确性、数据一致性和知识广度。比较两者的文献覆盖范围、生物控制剂/管理方案数量、效果评估等指标。

Result: DeepSeek在各方面表现更优：文献覆盖范围是ChatGPT的4.8-49.7倍，报告的生物控制剂/管理方案多1.6-2.4倍，效果评估高21.6%，实验室到田间数据一致性更好，对病虫害身份和管理策略的影响评估更现实。但两者都存在幻觉、虚构参考文献、混淆新旧科学命名、遗漏关键信息等问题。

Conclusion: 尽管存在局限性，两种LLM都能正确报告低分辨率的效果趋势。结合严格的人工监督，LLM可能成为支持农场层面决策和释放科学创造力的强大工具。

Abstract: Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.

</details>


### [17] [BAID: A Benchmark for Bias Assessment of AI Detectors](https://arxiv.org/abs/2512.11505)
*Priyam Basu,Yunfeng Zhang,Vipul Raheja*

Main category: cs.AI

TL;DR: BAID是一个用于评估AI文本检测器偏见的综合框架，包含超过20万样本覆盖7个社会语言学类别，发现现有检测器对少数群体文本存在系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在教育和工作场景中被广泛采用，但之前的研究只发现了零散的偏见案例（特别是针对英语学习者），缺乏对社会语言学因素的系统性评估。

Method: 提出BAID评估框架，包含7个主要类别（人口统计、年龄、教育水平、方言、正式程度、政治倾向、主题）的20多万个样本，并为每个样本生成保留原始内容但反映特定群体写作风格的合成版本，然后评估4个开源的最先进AI文本检测器。

Result: 发现检测性能存在一致的差异，特别是对来自代表性不足群体的文本召回率较低，表明现有AI检测器存在系统性偏见。

Conclusion: BAID提供了一个可扩展、透明的AI检测器审计方法，强调在这些工具部署到公共使用之前需要进行偏见感知的评估。

Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

</details>


### [18] [EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection](https://arxiv.org/abs/2512.11506)
*Georgios Kaoukis,Ioannis Aris Koufopoulos,Psaroudaki Eleni,Danae Pla Karidi,Evaggelia Pitoura,George Papastefanatos,Panayiotis Tsaparas*

Main category: cs.AI

TL;DR: EmeraldMind是一个基于事实的框架，通过整合领域特定知识图谱和检索增强生成技术，自动检测企业绿色洗白行为。


<details>
  <summary>Details</summary>
Motivation: 随着AI和网络代理在决策中日益普及，设计既能支持可持续发展又能防范错误信息的智能系统至关重要。绿色洗白（误导性的企业可持续发展声明）对环境进步构成重大挑战。

Method: 引入EmeraldMind框架，构建EmeraldGraph知识图谱（从多样化的企业ESG报告中提取），结合检索增强生成技术，提供基于证据的声明评估和透明解释。

Result: 在新绿色洗白声明数据集上的实验表明，EmeraldMind在准确性、覆盖范围和解释质量方面优于通用大语言模型，且无需微调或重新训练。

Conclusion: EmeraldMind框架通过整合领域特定知识图谱和检索增强生成，有效解决了绿色洗白检测问题，提供了透明、基于证据的评估方法。

Abstract: As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.

</details>


### [19] [AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives](https://arxiv.org/abs/2512.11544)
*Yuan Shen,Xiaojun Wu,Linghua Yu*

Main category: cs.AI

TL;DR: 研究通过模拟真实临床场景，发现主流大语言模型在处理含噪声冗余的患者主诉时出现功能缺陷，提出"AI-MASLD"概念，警告AI医疗应用需人类专家监督。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型从含噪声和冗余的患者主诉中提取核心医疗信息的能力，验证其是否表现出类似代谢功能障碍相关脂肪性肝病的功能衰退。

Method: 采用基于标准化医疗探针的横断面分析设计，选取GPT-4o、Gemini 2.5、DeepSeek 3.1和Qwen3-Max四种主流LLM，使用包含20个医疗探针的评估系统模拟真实临床沟通环境，由两位独立临床医生进行双盲反向评分。

Result: 所有测试模型均表现出不同程度功能缺陷，Qwen3-Max整体表现最佳，Gemini 2.5最差；极端噪声下多数模型功能崩溃；GPT-4o在深静脉血栓继发肺栓塞风险评估中出现严重误判。

Conclusion: 首次实证确认LLM处理临床信息时表现出类似代谢功能障碍的特征，提出"AI-MASLD"创新概念，强调当前LLM必须作为人类专家监督下的辅助工具使用。

Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.

</details>


### [20] [AI Benchmark Democratization and Carpentry](https://arxiv.org/abs/2512.11588)
*Gregor von Laszewski,Wesley Brewer,Jeyan Thiyagalingam,Juri Papay,Armstrong Foundjem,Piotr Luszczek,Murali Emani,Shirley V. Moore,Vijay Janapa Reddi,Matthew D. Sinclair,Sebastian Lobentanzer,Sujata Goswami,Benjamin Hawks,Marco Colombo,Nhan Tran,Christine R. Kirkpatrick,Abdulkareem Alsudais,Gregg Barrett,Tianhao Li,Kirsten Morehouse,Shivaram Venkataraman,Rutwik Jain,Kartik Mathur,Victor Lu,Tejinder Singh,Khojasteh Z. Mirza,Kongtao Chen,Sasidhar Kunapuli,Gavin Farrell,Renato Umeton,Geoffrey C. Fox*

Main category: cs.AI

TL;DR: 论文提出AI基准测试需要从静态转向动态自适应框架，以应对AI快速发展和实际部署需求，并倡导建立"AI基准测试工艺"的教育体系。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试面临多重挑战：模型架构快速演进、规模扩大、数据集更新和部署环境多样化，使得评估成为移动目标。静态基准容易被大语言模型记忆，导致基准结果与实际性能脱节。需要新的基准测试方法来解决资源需求高、专业硬件访问有限、设计专业知识缺乏等问题。

Method: 提出动态自适应基准测试框架，包含持续演进的模型、更新数据和异构平台，同时保持透明度、可复现性和可解释性。倡导建立"AI基准测试工艺"教育体系，通过技术创新和系统化教育相结合，培养基准设计和使用的专业知识。

Result: 识别了当前基准测试的关键障碍：高资源需求、专业硬件访问限制、基准设计专业知识缺乏、结果与应用领域关联不确定性。强调基准测试需要支持应用相关的比较，为不同场景提供有意义的指导，而不仅仅是顶级硬件上的峰值性能。

Conclusion: 动态包容的基准测试对于确保评估跟上AI发展步伐至关重要，能够支持负责任、可复现和可访问的AI部署。社区努力可以为"AI基准测试工艺"提供基础，通过技术革新和教育体系相结合，实现基准测试的民主化。

Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.

</details>


### [21] [Causal Inference in Energy Demand Prediction](https://arxiv.org/abs/2512.11653)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 提出基于结构因果模型的能源需求预测方法，利用因果洞察作为先验知识构建贝叶斯模型，在测试集上达到3.84% MAPE的先进性能


<details>
  <summary>Details</summary>
Motivation: 能源需求预测对电网运营商、工业能源消费者和服务提供商至关重要。能源需求受天气条件（温度、湿度、风速、太阳辐射）和日历信息（小时、月份）等多因素影响，这些因素因果相互依赖，比简单的基于相关性的学习技术更复杂

Method: 提出结构因果模型解释变量间的因果关系，通过完整分析验证因果信念。然后构建贝叶斯模型，将学到的因果洞察作为先验知识，在未见数据上进行训练和测试

Result: 模型在测试集上达到3.84% MAPE的先进性能，跨两年数据的交叉验证平均MAPE为3.88%，表现出强鲁棒性。因果分析发现：1）能源需求对温度波动的响应具有季节依赖性敏感性；2）冬季能源需求方差较低，因为温度变化与日常活动模式解耦

Conclusion: 结构因果模型能有效捕捉能源需求预测中的复杂因果关系，将因果洞察作为先验知识构建的贝叶斯模型实现了先进预测性能，为能源需求预测提供了新的因果分析方法

Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.

</details>


### [22] [MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition](https://arxiv.org/abs/2512.11682)
*Tim Cofala,Christian Kalfar,Jingge Xiao,Johanna Schrader,Michelle Tang,Wolfgang Nejdl*

Main category: cs.AI

TL;DR: TxAgent是一个用于临床治疗决策的AI代理系统，通过迭代检索增强生成(RAG)整合多种生物医学工具，在CURE-Bench挑战赛中因改进工具检索策略获得性能提升，荣获开放科学卓越奖。


<details>
  <summary>Details</summary>
Motivation: 临床治疗决策是高风险领域，需要AI系统在患者特征、疾病过程和药物之间进行复杂交互的稳健多步推理。医疗应用具有严格的安全约束，要求推理轨迹和工具调用序列的准确性，这促使需要将token级推理和工具使用行为作为显式监督信号进行评估。

Method: TxAgent采用微调的Llama-3.1-8B模型，通过迭代检索增强生成(RAG)动态生成和执行函数调用，访问统一的生物医学工具套件(ToolUniverse)，整合FDA Drug API、OpenTargets和Monarch资源以确保获取最新的治疗信息。

Result: 该工作分析了函数(工具)调用的检索质量如何影响整体模型性能，展示了通过改进工具检索策略实现的性能提升。在CURE-Bench NeurIPS 2025挑战赛中，该工作因评估治疗推理系统的正确性、工具利用和推理质量而获得开放科学卓越奖。

Conclusion: TxAgent通过迭代RAG和统一生物医学工具套件，为临床治疗决策提供了稳健的AI指导系统。改进的工具检索策略对提升性能至关重要，该工作在CURE-Bench挑战赛中的表现验证了其在治疗推理任务中的有效性。

Abstract: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [23] [Sphere Decoding Revisited](https://arxiv.org/abs/2512.11195)
*Zheng Wang,Cong Ling,Shi Jin,Yongming Huang,Feifei Gao*

Main category: cs.IT

TL;DR: 本文重新审视球解码范式，提出等效球解码(ESD)，通过引入初始搜索大小K和偏差因子σ两个参数，首次精确指定球解码复杂度，实现可控的解码权衡。


<details>
  <summary>Details</summary>
Motivation: 传统球解码(Fincke-Pohst算法)的复杂度难以精确控制和分析，需要一种能够明确指定复杂度并提供灵活解码权衡的新方法。

Method: 提出等效球解码(ESD)，引入初始搜索大小K和偏差因子σ两个参数，将球半径D定义为σ√(2lnK)。通过归一化加权和候选保护两种增强机制提升算法性能，实现从次优到最优解码性能的灵活调节。

Result: ESD的复杂度上界为|S|<nK，首次精确指定了球解码复杂度。通过调节K可以实现可控的解码权衡，在大规模MIMO检测中验证了算法的灵活性和有效性。

Conclusion: ESD算法首次实现了球解码复杂度的精确控制，通过参数K提供灵活的解码权衡，为大规模MIMO检测等应用提供了可扩展且性能可控的解码方案。

Abstract: In this paper, the paradigm of sphere decoding (SD) for solving the integer least square problem (ILS) is revisited, where extra degrees of freedom are introduced to exploit the decoding potential. Firstly, the equivalent sphere decoding (ESD) is proposed, which is essentially the same with the classic Fincke-Pohst sphere decoding but characterizes the sphere radius $D>0$ with two new parameters named as initial searching size $K>1$ and deviation factor $σ>0$. By fixing $σ$ properly, we show that given the sphere radius $D\triangleqσ\sqrt{2\ln K}$, the complexity of ESD in terms of the number of visited nodes is upper bounded by $|S|<nK$, thus resulting in an explicit and tractable decoding trade-off solely controlled by $K$. To the best of our knowledge, this is the first time that the complexity of sphere decoding is exactly specified, where considerable decoding potential can be explored from it. After that, two enhancement mechanisms named as normalized weighting and candidate protection are proposed to further upgrade the ESD algorithm. On one hand, given the same setups of $K$ and $σ$, a larger sphere radius is achieved, indicating a better decoding trade-off. On the other hand, the proposed ESD algorithm is generalized, which bridges suboptimal and optimal decoding performance through the flexible choice of $K$. Finally, further performance optimization and complexity reduction with respect to ESD are also derived, and the introduced tractable and flexible decoding trade-off is verified through large-scale MIMO detection.

</details>


### [24] [Information-Theoretic Equivalences Across Rate-Distortion, Quantization, and Decoding](https://arxiv.org/abs/2512.11279)
*Bruno Macchiavello*

Main category: cs.IT

TL;DR: 该论文提出了一个统一的数学框架，将率失真理论、格点量化和现代纠错码联系起来，强调它们的变分和凸分析结构，展示了压缩、量化和解码作为连续信息到离散流形的凸投影的统一性。


<details>
  <summary>Details</summary>
Motivation: 建立率失真理论、格点量化和现代纠错码之间的统一数学框架，揭示它们共有的变分和凸分析结构，为信息理论的不同分支提供统一的视角。

Method: 1. 建立率失真函数的Gibbs型变分公式，证明最优测试信道形成指数族；2. 将反向注水隐喻扩展到分布式格点量化；3. 将LDPC码的置信传播和极化码的极化过程形式化为递归变分推理。

Result: 1. 建立了率失真函数的变分公式，揭示了最优测试信道的指数族结构；2. 推导了条件协方差矩阵特征模上的失真分配界限；3. 展示了置信传播和极化过程作为变分推理的统一解释；4. 将压缩、量化和解码统一为凸投影问题。

Conclusion: 该框架统一了信息理论中的压缩、量化和解码问题，揭示了它们作为连续信息到离散流形凸投影的共同本质，并可以扩展到神经压缩和量子信息领域，展示了框架的普适性。

Abstract: We propose a unified mathematical framework for rate-distortion theory, lattice quantization, and modern error-correcting codes by emphasizing their variational and convex-analytic structure. First, we establish a Gibbs-type variational formulation of the rate-distortion function and show that optimal test channels form an exponential family, with Fullback-Leibler divergence acting as a Bregman divergence. This yields a generalized Pythagorean theorem for projections and a Legendre duality that couples distortion constraints with inverse temperature parameters. Second, the reverse water-filling metaphor is extended to distributed lattice quantization, deriving distortion allocation bounds across eigenmodes of conditional covariance matrices. Third, inference is formalized as decoding by showing that belief propagation in LDPC ensembles and polarization in polar codes can be interpreted as recursive variational inference procedures. These results unify compression, quantization, and decoding as convex projections of continuous information onto discrete manifolds. Extensions to neural compression and quantum information are sketched as corollaries, illustrating the universality of the framework. Illustrative connections to other scientific fields are also presented. Finally, complementary numerical examples and scripts are located in the appendix

</details>


### [25] [Refinements and Generalizations of the Shannon Lower Bound via Extensions of the Kraft Inequality](https://arxiv.org/abs/2512.11322)
*Neri Merhav*

Main category: cs.IT

TL;DR: 该论文推导了无损压缩中Kraft不等式的扩展版本，并以此为基础改进了香农下界，包括一对一编码、D-半忠实编码、滑动窗口失真度量和个体序列的香农下界。


<details>
  <summary>Details</summary>
Motivation: 现有的香农下界在率失真编码中虽然重要，但在某些情况下可能不够紧致。论文旨在通过扩展Kraft不等式来获得更精确的率失真下界，以覆盖更广泛的编码场景。

Method: 首先推导了无损压缩中Kraft不等式的几个扩展版本，然后利用这些扩展版本系统地推导出香农下界的多种改进和扩展形式。

Result: 获得了多个率失真下界的改进版本：1）一对一编码的更紧致下界；2）D-半忠实编码的改进下界；3）基于滑动窗口函数的失真度量的香农下界；4）个体序列对应的香农下界。

Conclusion: 通过扩展Kraft不等式，论文成功推导出香农下界的多种改进和扩展形式，为率失真编码理论提供了更精确的理论下界工具。

Abstract: We derive a few extended versions of the Kraft inequality for lossy compression, which pave the way to the derivation of several refinements and extensions of the well known Shannon lower bound in a variety of instances of rate-distortion coding. These refinements and extensions include sharper bounds for one-to-one codes and $D$-semifaithful codes, a Shannon lower bound for distortion measures based on sliding-window functions, and an individual-sequence counterpart of the Shannon lower bound.

</details>


### [26] [AMBER: An Adaptive Multimodal Mask Transformer for Beam Prediction with Missing Modalities](https://arxiv.org/abs/2512.11331)
*Chenyiming Wen,Binpu Shi,Min Li,Ming-Min Zhao,Min-Jian Zhao,Jiangzhou Wang*

Main category: cs.IT

TL;DR: AMBER：一种用于毫米波大规模MIMO波束预测的自适应多模态掩码Transformer框架，能够处理图像、LiDAR、雷达和GPS数据，并在任意模态缺失情况下保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态感知的波束预测方法在现实场景中性能显著下降，因为它们对传感器遮挡、光照不良或GPS丢失等导致的模态缺失数据非常脆弱。

Method: 提出AMBER框架，包含可学习的模态token和缺失模态感知掩码以防止跨模态噪声传播，使用可学习的融合token和多头注意力实现鲁棒的模态特定信息蒸馏和特征级融合，并引入类former辅助的模态对齐模块和时间感知位置嵌入来保持时间一致性和跨模态语义对齐。

Result: 在真实世界DeepSense6G数据集上的广泛实验表明，AMBER显著优于现有的多模态学习基线方法，即使在严重模态缺失场景下也能保持高波束预测准确性和鲁棒性。

Conclusion: AMBER框架通过自适应处理任意模态缺失情况，实现了鲁棒的多模态波束预测，验证了其在现实车载网络中的有效性和实际适用性。

Abstract: With the widespread adoption of millimeter-wave (mmWave) massive multi-input-multi-output (MIMO) in vehicular networks, accurate beam prediction and alignment have become critical for high-speed data transmission and reliable access. While traditional beam prediction approaches primarily rely on in-band beam training, recent advances have started to explore multimodal sensing to extract environmental semantics for enhanced prediction. However, the performance of existing multimodal fusion methods degrades significantly in real-world settings because they are vulnerable to missing data caused by sensor blockage, poor lighting, or GPS dropouts. To address this challenge, we propose AMBER ({A}daptive multimodal {M}ask transformer for {BE}am p{R}ediction), a novel end-to-end framework that processes temporal sequences of image, LiDAR, radar, and GPS data, while adaptively handling arbitrary missing-modality cases. AMBER introduces learnable modality tokens and a missing-modality-aware mask to prevent cross-modal noise propagation, along with a learnable fusion token and multihead attention to achieve robust modality-specific information distillation and feature-level fusion. Furthermore, a class-former-aided modality alignment (CMA) module and temporal-aware positional embedding are incorporated to preserve temporal coherence and ensure semantic alignment across modalities, facilitating the learning of modality-invariant and temporally consistent representations for beam prediction. Extensive experiments on the real-world DeepSense6G dataset demonstrate that AMBER significantly outperforms existing multimodal learning baselines. In particular, it maintains high beam prediction accuracy and robustness even under severe missing-modality scenarios, validating its effectiveness and practical applicability.

</details>


### [27] [Capacity-Achieving Codes with Inverse-Ackermann-Depth Encoders](https://arxiv.org/abs/2512.11443)
*Yuan Li*

Main category: cs.IT

TL;DR: 该论文证明了对于任意对称离散无记忆信道，存在接近信道容量的纠错码，这些码可以通过大小为O(n)、深度为逆Ackermann函数的算术电路进行编码。


<details>
  <summary>Details</summary>
Motivation: 研究目标是构建既接近信道容量又具有高效编码电路的纠错码。传统容量逼近码通常编码复杂度较高，需要探索是否存在同时具有线性大小和极浅深度的编码电路。

Method: 方法结合了三个组件：1) 基于Gál等人和Drucker-Li构造的具有恒定码率和相对距离的线性码；2) 使用随机选择边权重的扩展图；3) 通过算术电路（带加权加法门）在有限域F_q上实现编码。

Result: 证明了对于输入输出字母表大小为素数幂q的对称离散无记忆信道，存在接近信道容量的纠错码，其编码电路大小为O(n)，深度为逆Ackermann函数α(n)。

Conclusion: 某些容量逼近码确实允许同时具有线性大小和逆Ackermann深度的极其高效的编码电路，这为高效编码器的实际实现提供了理论可能性。

Abstract: For any symmetric discrete memoryless channel with input and output alphabet of size $q$, where $q$ is a prime power, we prove that there exist error-correcting codes approaching channel capacity encodable by arithmetic circuits (with weighted addition gates) over $\mathbb{F}_q$ of size $O(n)$ and depth $α(n)$, where $α(n)$ is a version of the inverse Ackermann function. Our results suggest that certain capacity-achieving codes admit highly efficient encoding circuits that are both in linear size and of inverse-Ackermann depth. Our construction composes a linear code with constant rate and relative distance, based on the constructions of Gál, Hansen, Koucký, Pudlák, and Viola [IEEE Trans. Inform. Theory 59(10), 2013] and Drucker and Li [COCOON 2023], with an additional layer formed by a disperser graph whose edge weights are chosen uniformly at random.

</details>


### [28] [Stable low-rank matrix recovery from 3-designs](https://arxiv.org/abs/2512.11642)
*Timm Gilles*

Main category: cs.IT

TL;DR: 该论文研究了从复射影3-designs的秩一测量中恢复低秩厄米特矩阵的问题，使用核范数最小化方法，填补了3-designs恢复保证的理论空白。


<details>
  <summary>Details</summary>
Motivation: 虽然已知4-designs可以实现接近最优的恢复保证，且2-designs无法用亚二次测量实现恢复，但3-designs的情况一直未解决。由于4-designs的显式构造比3-designs困难得多，解决3-designs的恢复问题具有重要的实际意义。

Method: 使用核范数最小化方法，从复射影3-designs的均匀采样中获取秩一测量，研究低秩厄米特矩阵的恢复问题。该框架通过PhaseLift方法将相位恢复作为特例包含在内。

Result: 建立了与4-designs最佳已知结果平行的3-designs恢复保证，推导了通过核范数最小化实现稳定和鲁棒低秩恢复所需的测量数量界限。

Conclusion: 该工作填补了3-designs恢复的理论空白，证明了3-designs可以实现与4-designs相当的恢复性能，这在实践中特别重要，因为3-designs的显式构造比4-designs容易得多。

Abstract: We study the recovery of low-rank Hermitian matrices from rank-one measurements obtained by uniform sampling from complex projective 3-designs, using nuclear-norm minimization. This framework includes phase retrieval as a special case via the PhaseLift method. In general, complex projective $t$-designs provide a practical means of partially derandomizing Gaussian measurement models. While near-optimal recovery guarantees are known for $4$-designs, and it is known that $2$-designs do not permit recovery with a subquadratic number of measurements, the case of $3$-designs has remained open. In this work, we close this gap by establishing recovery guarantees for (exact and approximate) $3$-designs that parallel the best-known results for $4$-designs. In particular, we derive bounds on the number of measurements sufficient for stable and robust low-rank recovery via nuclear-norm minimization. Our results are especially relevant in practice, as explicit constructions of $4$-designs are significantly more challenging than those of $3$-designs.

</details>
