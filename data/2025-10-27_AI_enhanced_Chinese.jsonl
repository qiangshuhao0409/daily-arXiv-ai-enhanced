{"id": "2510.20903", "categories": ["cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.20903", "abs": "https://arxiv.org/abs/2510.20903", "authors": ["Yirong Shen", "Lu Gan", "Cong Ling"], "title": "Information Theoretic Learning for Diffusion Models with Warm Start", "comment": "NeurIPS 2025", "summary": "Generative models that maximize model likelihood have gained traction in many\npractical settings. Among them, perturbation based approaches underpin many\nstrong likelihood estimation models, yet they often face slow convergence and\nlimited theoretical understanding. In this paper, we derive a tighter\nlikelihood bound for noise driven models to improve both the accuracy and\nefficiency of maximum likelihood learning. Our key insight extends the\nclassical KL divergence Fisher information relationship to arbitrary noise\nperturbations, going beyond the Gaussian assumption and enabling structured\nnoise distributions. This formulation allows flexible use of randomized noise\ndistributions that naturally account for sensor artifacts, quantization\neffects, and data distribution smoothing, while remaining compatible with\nstandard diffusion training. Treating the diffusion process as a Gaussian\nchannel, we further express the mismatched entropy between data and model,\nshowing that the proposed objective upper bounds the negative log-likelihood\n(NLL). In experiments, our models achieve competitive NLL on CIFAR-10 and SOTA\nresults on ImageNet across multiple resolutions, all without data augmentation,\nand the framework extends naturally to discrete data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u66f4\u7d27\u7684\u4f3c\u7136\u4e0b\u754c\u6765\u6539\u8fdb\u566a\u58f0\u9a71\u52a8\u751f\u6210\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u6269\u5c55\u4e86KL\u6563\u5ea6\u4e0eFisher\u4fe1\u606f\u7684\u5173\u7cfb\u5230\u4efb\u610f\u566a\u58f0\u6270\u52a8\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "motivation": "\u57fa\u4e8e\u6270\u52a8\u7684\u4f3c\u7136\u4f30\u8ba1\u6a21\u578b\u867d\u7136\u5b9e\u7528\uff0c\u4f46\u9762\u4e34\u6536\u655b\u6162\u548c\u7406\u8bba\u7406\u89e3\u6709\u9650\u7684\u95ee\u9898\u3002\u9700\u8981\u6539\u8fdb\u6700\u5927\u4f3c\u7136\u5b66\u4e60\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u5c06\u7ecf\u5178KL\u6563\u5ea6\u4e0eFisher\u4fe1\u606f\u7684\u5173\u7cfb\u6269\u5c55\u5230\u4efb\u610f\u566a\u58f0\u6270\u52a8\uff0c\u8d85\u8d8a\u9ad8\u65af\u5047\u8bbe\uff0c\u5141\u8bb8\u4f7f\u7528\u7ed3\u6784\u5316\u566a\u58f0\u5206\u5e03\u3002\u5c06\u6269\u6563\u8fc7\u7a0b\u89c6\u4e3a\u9ad8\u65af\u4fe1\u9053\uff0c\u8868\u8fbe\u6570\u636e\u4e0e\u6a21\u578b\u4e4b\u95f4\u7684\u5931\u914d\u71b5\u3002", "result": "\u5728CIFAR-10\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u8d1f\u5bf9\u6570\u4f3c\u7136(NLL)\uff0c\u5728ImageNet\u591a\u4e2a\u5206\u8fa8\u7387\u4e0a\u8fbe\u5230SOTA\u7ed3\u679c\uff0c\u4e14\u65e0\u9700\u6570\u636e\u589e\u5f3a\u3002\u6846\u67b6\u81ea\u7136\u6269\u5c55\u5230\u79bb\u6563\u6570\u636e\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u566a\u58f0\u9a71\u52a8\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u7d27\u7684\u4f3c\u7136\u4e0b\u754c\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2510.21103", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.21103", "abs": "https://arxiv.org/abs/2510.21103", "authors": ["Zongyang Yuan", "Lailong Luo", "Qianzhen Zhang", "Bangbang Ren", "Deke Guo", "Richard T. B. Ma"], "title": "Sensing and Storing Less: A MARL-based Solution for Energy Saving in Edge Internet of Things", "comment": null, "summary": "As the number of Internet of Things (IoT) devices continuously grows and\napplication scenarios constantly enrich, the volume of sensor data experiences\nan explosive increase. However, substantial data demands considerable energy\nduring computation and transmission. Redundant deployment or mobile assistance\nis essential to cover the target area reliably with fault-prone sensors.\nConsequently, the ``butterfly effect\" may appear during the IoT operation,\nsince unreasonable data overlap could result in many duplicate data. To this\nend, we propose Senses, a novel online energy saving solution for edge IoT\nnetworks, with the insight of sensing and storing less at the network edge by\nadopting Muti-Agent Reinforcement Learning (MARL). Senses achieves data\nde-duplication by dynamically adjusting sensor coverage at the sensor level.\nFor exceptional cases where sensor coverage cannot be altered, Senses conducts\ndata partitioning and eliminates redundant data at the controller level.\nFurthermore, at the global level, considering the heterogeneity of IoT devices,\nSenses balances the operational duration among the devices to prolong the\noverall operational duration of edge IoT networks. We evaluate the performance\nof Senses through testbed experiments and simulations. The results show that\nSenses saves 11.37% of energy consumption on control devices and prolongs 20%\noverall operational duration of the IoT device network.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSenses\u7684\u5728\u7ebf\u8282\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u8fb9\u7f18\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u5b9e\u73b0\u6570\u636e\u53bb\u91cd\u548c\u80fd\u8017\u4f18\u5316\uff0c\u80fd\u8282\u770111.37%\u63a7\u5236\u8bbe\u5907\u80fd\u8017\u5e76\u5ef6\u957f20%\u7f51\u7edc\u8fd0\u884c\u65f6\u95f4\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u8bbe\u5907\u6570\u91cf\u589e\u957f\u548c\u5e94\u7528\u573a\u666f\u4e30\u5bcc\uff0c\u4f20\u611f\u5668\u6570\u636e\u7206\u70b8\u5f0f\u589e\u52a0\uff0c\u4f46\u5927\u91cf\u6570\u636e\u5728\u8ba1\u7b97\u548c\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u6d88\u8017\u5927\u91cf\u80fd\u91cf\u3002\u4e0d\u5408\u7406\u7684\u91cd\u590d\u6570\u636e\u8986\u76d6\u4f1a\u5bfc\u81f4\"\u8774\u8776\u6548\u5e94\"\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u6570\u636e\u5197\u4f59\u548c\u80fd\u8017\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(MARL)\uff0c\u5728\u4f20\u611f\u5668\u5c42\u9762\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u8986\u76d6\u8303\u56f4\u5b9e\u73b0\u6570\u636e\u53bb\u91cd\uff1b\u5728\u63a7\u5236\u5668\u5c42\u9762\u8fdb\u884c\u6570\u636e\u5206\u533a\u548c\u5197\u4f59\u6570\u636e\u6d88\u9664\uff1b\u5728\u5168\u5c40\u5c42\u9762\u8003\u8651\u8bbe\u5907\u5f02\u6784\u6027\uff0c\u5e73\u8861\u8bbe\u5907\u8fd0\u884c\u65f6\u95f4\u4ee5\u5ef6\u957f\u6574\u4f53\u7f51\u7edc\u5bff\u547d\u3002", "result": "\u901a\u8fc7\u6d4b\u8bd5\u53f0\u5b9e\u9a8c\u548c\u4eff\u771f\u8bc4\u4f30\uff0cSenses\u80fd\u591f\u8282\u7701\u63a7\u5236\u8bbe\u590711.37%\u7684\u80fd\u8017\uff0c\u5e76\u5c06\u7269\u8054\u7f51\u8bbe\u5907\u7f51\u7edc\u7684\u6574\u4f53\u8fd0\u884c\u65f6\u95f4\u5ef6\u957f20%\u3002", "conclusion": "Senses\u662f\u4e00\u79cd\u6709\u6548\u7684\u8fb9\u7f18\u7269\u8054\u7f51\u7f51\u7edc\u5728\u7ebf\u8282\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u7684\u6570\u636e\u53bb\u91cd\u548c\u80fd\u8017\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u80fd\u6548\u548c\u8fd0\u884c\u5bff\u547d\u3002"}}
{"id": "2510.21030", "categories": ["cs.IT", "math.IT", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.21030", "abs": "https://arxiv.org/abs/2510.21030", "authors": ["En-Jui Chang"], "title": "Overlapped-repetition Shor codes achieving fourfold asymptotic rate", "comment": "4 pages", "summary": "The standard Shor code employs two repetition codes as inner and outer codes,\nyielding a simple structure but a relatively low code rate. By overlapping a\nsmall number of repetition codes, we enhance the asymptotic code rate fourfold.\nIn the minimal-distance case $d = 3$, this construction reduces the overhead\nfrom $[[9,1,3]]$ to the more efficient $[[7,1,3]]$ configuration.", "AI": {"tldr": "\u901a\u8fc7\u91cd\u53e0\u5c11\u91cf\u91cd\u590d\u7801\uff0c\u5c06\u6807\u51c6Shor\u7801\u7684\u6e10\u8fd1\u7801\u7387\u63d0\u9ad8\u56db\u500d\uff0c\u5728\u6700\u5c0f\u8ddd\u79bbd=3\u7684\u60c5\u51b5\u4e0b\u5c06\u5f00\u9500\u4ece[[9,1,3]]\u51cf\u5c11\u5230\u66f4\u9ad8\u6548\u7684[[7,1,3]]\u914d\u7f6e\u3002", "motivation": "\u6807\u51c6Shor\u7801\u4f7f\u7528\u4e24\u4e2a\u91cd\u590d\u7801\u4f5c\u4e3a\u5185\u5916\u7801\uff0c\u7ed3\u6784\u7b80\u5355\u4f46\u7801\u7387\u76f8\u5bf9\u8f83\u4f4e\u3002", "method": "\u901a\u8fc7\u91cd\u53e0\u5c11\u91cf\u91cd\u590d\u7801\u6765\u589e\u5f3a\u6e10\u8fd1\u7801\u7387\u3002", "result": "\u5728\u6700\u5c0f\u8ddd\u79bbd=3\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5f00\u9500\u4ece[[9,1,3]]\u51cf\u5c11\u5230[[7,1,3]]\u914d\u7f6e\u3002", "conclusion": "\u8fd9\u79cd\u6784\u9020\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u91cf\u5b50\u7ea0\u9519\u7801\u7684\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u8d44\u6e90\u5f00\u9500\u3002"}}
{"id": "2510.21127", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21127", "abs": "https://arxiv.org/abs/2510.21127", "authors": ["Bowei Tong", "Hui Kang", "Jiahui Li", "Geng Sun", "Jiacheng Wang", "Yaoqi Yang", "Bo Xu", "Dusit Niyato"], "title": "Enhanced Evolutionary Multi-Objective Deep Reinforcement Learning for Reliable and Efficient Wireless Rechargeable Sensor Networks", "comment": "15 pages, 9 figures, submited to TVT", "summary": "Despite rapid advancements in sensor networks, conventional battery-powered\nsensor networks suffer from limited operational lifespans and frequent\nmaintenance requirements that severely constrain their deployment in remote and\ninaccessible environments. As such, wireless rechargeable sensor networks\n(WRSNs) with mobile charging capabilities offer a promising solution to extend\nnetwork lifetime. However, WRSNs face critical challenges from the inherent\ntrade-off between maximizing the node survival rates and maximizing charging\nenergy efficiency under dynamic operational conditions. In this paper, we\ninvestigate a typical scenario where mobile chargers move and charge the\nsensor, thereby maintaining the network connectivity while minimizing the\nenergy waste. Specifically, we formulate a multi-objective optimization problem\nthat simultaneously maximizes the network node survival rate and mobile charger\nenergy usage efficiency across multiple time slots, which presents NP-hard\ncomputational complexity with long-term temporal dependencies that make\ntraditional optimization approaches ineffective. To address these challenges,\nwe propose an enhanced evolutionary multi-objective deep reinforcement learning\nalgorithm, which integrates a long short-term memory (LSTM)-based policy\nnetwork for temporal pattern recognition, a multilayer perceptron-based\nprospective increment model for future state prediction, and a time-varying\nPareto policy evaluation method for dynamic preference adaptation. Extensive\nsimulation results demonstrate that the proposed algorithm significantly\noutperforms existing approaches in balancing node survival rate and energy\nefficiency while generating diverse Pareto-optimal solutions. Moreover, the\nLSTM-enhanced policy network converges 25% faster than conventional networks,\nwith the time-varying evaluation method effectively adapting to dynamic\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u589e\u5f3a\u7684\u8fdb\u5316\u591a\u76ee\u6807\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u65e0\u7ebf\u53ef\u5145\u7535\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u7684\u79fb\u52a8\u5145\u7535\u4f18\u5316\uff0c\u540c\u65f6\u6700\u5927\u5316\u8282\u70b9\u751f\u5b58\u7387\u548c\u5145\u7535\u80fd\u91cf\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7535\u6c60\u4f9b\u7535\u4f20\u611f\u5668\u7f51\u7edc\u5728\u504f\u8fdc\u73af\u5883\u4e2d\u5bff\u547d\u6709\u9650\u4e14\u7ef4\u62a4\u9891\u7e41\uff0c\u65e0\u7ebf\u53ef\u5145\u7535\u4f20\u611f\u5668\u7f51\u7edc\u867d\u80fd\u5ef6\u957f\u7f51\u7edc\u5bff\u547d\uff0c\u4f46\u9762\u4e34\u8282\u70b9\u751f\u5b58\u7387\u4e0e\u5145\u7535\u80fd\u91cf\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u6311\u6218\u3002", "method": "\u96c6\u6210LSTM\u7b56\u7565\u7f51\u7edc\u8fdb\u884c\u65f6\u5e8f\u6a21\u5f0f\u8bc6\u522b\u3001\u591a\u5c42\u611f\u77e5\u673a\u524d\u77bb\u589e\u91cf\u6a21\u578b\u8fdb\u884c\u672a\u6765\u72b6\u6001\u9884\u6d4b\uff0c\u4ee5\u53ca\u65f6\u53d8Pareto\u7b56\u7565\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u52a8\u6001\u504f\u597d\u9002\u5e94\u3002", "result": "\u7b97\u6cd5\u5728\u5e73\u8861\u8282\u70b9\u751f\u5b58\u7387\u548c\u80fd\u91cf\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cLSTM\u589e\u5f3a\u7b56\u7565\u7f51\u7edc\u6536\u655b\u901f\u5ea6\u5feb25%\uff0c\u65f6\u53d8\u8bc4\u4f30\u65b9\u6cd5\u80fd\u6709\u6548\u9002\u5e94\u52a8\u6001\u6761\u4ef6\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u65e0\u7ebf\u53ef\u5145\u7535\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u79fb\u52a8\u5145\u7535\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u751f\u6210\u591a\u6837\u5316\u7684Pareto\u6700\u4f18\u89e3\uff0c\u9002\u5e94\u52a8\u6001\u64cd\u4f5c\u73af\u5883\u3002"}}
{"id": "2510.21253", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21253", "abs": "https://arxiv.org/abs/2510.21253", "authors": ["Boaz Moav", "Ryan Gabrys", "Eitan Yaakobi"], "title": "Complex DNA Synthesis Sequences", "comment": null, "summary": "DNA-based storage offers unprecedented density and durability, but its\nscalability is fundamentally limited by the efficiency of parallel strand\nsynthesis. Existing methods either allow unconstrained nucleotide additions to\nindividual strands, such as enzymatic synthesis, or enforce identical additions\nacross many strands, such as photolithographic synthesis. We introduce and\nanalyze a hybrid synthesis framework that generalizes both approaches: in each\ncycle, a nucleotide is selected from a restricted subset and incorporated in\nparallel. This model gives rise to a new notion of a complex synthesis\nsequence. Building on this framework, we extend the information rate definition\nof Lenz et al. and analyze an analog of the deletion ball, defined and studied\nin this setting, deriving tight expressions for the maximal information rate\nand its asymptotic behavior. These results bridge the theoretical gap between\nconstrained models and the idealized setting in which every nucleotide is\nalways available. For the case of known strands, we design a dynamic\nprogramming algorithm that computes an optimal complex synthesis sequence,\nhighlighting structural similarities to the shortest common supersequence\nproblem. We also define a distinct two-dimensional array model with synthesis\nconstraints over the rows, which extends previous synthesis models in the\nliterature and captures new structural limitations in large-scale strand\narrays. Additionally, we develop a dynamic programming algorithm for this\nproblem as well. Our results establish a new and comprehensive theoretical\nframework for constrained DNA, subsuming prior models and setting the stage for\nfuture advances in the field.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408DNA\u5408\u6210\u6846\u67b6\uff0c\u5728\u53d7\u9650\u6838\u82f7\u9178\u5b50\u96c6\u4e0a\u8fdb\u884c\u5e76\u884c\u5408\u6210\uff0c\u5efa\u7acb\u4e86\u590d\u6742\u5408\u6210\u5e8f\u5217\u7684\u65b0\u6982\u5ff5\uff0c\u5206\u6790\u4e86\u6700\u5927\u4fe1\u606f\u7387\u548c\u6e10\u8fd1\u884c\u4e3a\uff0c\u5e76\u8bbe\u8ba1\u4e86\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u6c42\u89e3\u6700\u4f18\u5408\u6210\u5e8f\u5217\u3002", "motivation": "DNA\u5b58\u50a8\u5177\u6709\u9ad8\u5bc6\u5ea6\u548c\u8010\u4e45\u6027\uff0c\u4f46\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u8981\u4e48\u5141\u8bb8\u65e0\u7ea6\u675f\u7684\u6838\u82f7\u9178\u6dfb\u52a0\uff0c\u8981\u4e48\u5f3a\u5236\u6240\u6709\u94fe\u8fdb\u884c\u76f8\u540c\u6dfb\u52a0\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u9700\u8981\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\u6765\u5e73\u8861\u7075\u6d3b\u6027\u548c\u5e76\u884c\u6027\u3002", "method": "\u5f15\u5165\u6df7\u5408\u5408\u6210\u6846\u67b6\uff1a\u6bcf\u4e2a\u5468\u671f\u4ece\u53d7\u9650\u6838\u82f7\u9178\u5b50\u96c6\u4e2d\u9009\u62e9\u6838\u82f7\u9178\u8fdb\u884c\u5e76\u884c\u5408\u6210\u3002\u6269\u5c55\u4e86\u4fe1\u606f\u7387\u5b9a\u4e49\uff0c\u5206\u6790\u4e86\u5220\u9664\u7403\u7684\u7c7b\u4f3c\u6982\u5ff5\uff0c\u8bbe\u8ba1\u4e86\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u8ba1\u7b97\u6700\u4f18\u590d\u6742\u5408\u6210\u5e8f\u5217\u3002", "result": "\u63a8\u5bfc\u4e86\u6700\u5927\u4fe1\u606f\u7387\u53ca\u5176\u6e10\u8fd1\u884c\u4e3a\u7684\u7d27\u81f4\u8868\u8fbe\u5f0f\uff0c\u5efa\u7acb\u4e86\u53d7\u9650\u6a21\u578b\u4e0e\u7406\u60f3\u5316\u8bbe\u7f6e\u4e4b\u95f4\u7684\u7406\u8bba\u6865\u6881\u3002\u5bf9\u4e8e\u5df2\u77e5\u94fe\u7684\u60c5\u51b5\uff0c\u8bbe\u8ba1\u4e86\u8ba1\u7b97\u6700\u4f18\u5408\u6210\u5e8f\u5217\u7684\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7edf\u4e00\u7684DNA\u5408\u6210\u7406\u8bba\u6846\u67b6\uff0c\u5305\u542b\u4e86\u5148\u524d\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.21130", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.21130", "abs": "https://arxiv.org/abs/2510.21130", "authors": ["Qi Deng", "Yinghao Zhang", "Yalin Liu", "Bishenghui Tao"], "title": "A Confidence-Constrained Cloud-Edge Collaborative Framework for Autism Spectrum Disorder Diagnosis", "comment": "10 pages, 2 figures", "summary": "Autism Spectrum Disorder (ASD) diagnosis systems in school environments\nincreasingly relies on IoT-enabled cameras, yet pure cloud processing raises\nprivacy and latency concerns while pure edge inference suffers from limited\naccuracy. We propose Confidence-Constrained Cloud-Edge Knowledge Distillation\n(C3EKD), a hierarchical framework that performs most inference at the edge and\nselectively uploads only low-confidence samples to the cloud. The cloud\nproduces temperature-scaled soft labels and distils them back to edge models\nvia a global loss aggregated across participating schools, improving\ngeneralization without centralizing raw data. On two public ASD facial-image\ndatasets, the proposed framework achieves a superior accuracy of 87.4\\%,\ndemonstrating its potential for scalable deployment in real-world applications.", "AI": {"tldr": "\u63d0\u51faC3EKD\u6846\u67b6\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u6267\u884c\u5927\u90e8\u5206\u63a8\u7406\uff0c\u4ec5\u4e0a\u4f20\u4f4e\u7f6e\u4fe1\u5ea6\u6837\u672c\u5230\u4e91\u7aef\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u89e3\u51b3ASD\u8bca\u65ad\u4e2d\u7684\u9690\u79c1\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u5b66\u6821\u73af\u5883\u4e2dASD\u8bca\u65ad\u7cfb\u7edf\u4f9d\u8d56\u7269\u8054\u7f51\u6444\u50cf\u5934\uff0c\u4f46\u7eaf\u4e91\u5904\u7406\u5b58\u5728\u9690\u79c1\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u7eaf\u8fb9\u7f18\u63a8\u7406\u51c6\u786e\u7387\u6709\u9650\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff0c\u8fb9\u7f18\u6267\u884c\u4e3b\u8981\u63a8\u7406\uff0c\u9009\u62e9\u6027\u4e0a\u4f20\u4f4e\u7f6e\u4fe1\u5ea6\u6837\u672c\u5230\u4e91\u7aef\uff1b\u4e91\u7aef\u751f\u6210\u6e29\u5ea6\u7f29\u653e\u8f6f\u6807\u7b7e\uff0c\u901a\u8fc7\u8de8\u5b66\u6821\u805a\u5408\u7684\u5168\u5c40\u635f\u5931\u84b8\u998f\u56de\u8fb9\u7f18\u6a21\u578b\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171ASD\u9762\u90e8\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fbe\u523087.4%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5177\u6709\u53ef\u6269\u5c55\u90e8\u7f72\u6f5c\u529b\uff0c\u80fd\u5728\u4e0d\u96c6\u4e2d\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.21299", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21299", "abs": "https://arxiv.org/abs/2510.21299", "authors": ["Shengkang Chen", "Tong Wu", "Zhiyong Chen", "Feng Yang", "Meixia Tao", "Wenjun Zhang"], "title": "Text-Guided Diffusion Model-based Generative Communication for Wireless Image Transmission", "comment": "submitted to IEEE journal", "summary": "Reliable image transmission over wireless channels is particularly\nchallenging at extremely low transmission rates, where conventional compression\nand channel coding schemes fail to preserve adequate visual quality. To address\nthis issue, we propose a generative communication framework based on diffusion\nmodels, which integrates joint source channel coding (JSCC) with\nsemantic-guided reconstruction leveraging a pre-trained generative model.\nUnlike conventional architectures that aim to recover exact pixel values of the\noriginal image, the proposed method focuses on preserving and reconstructing\nsemantically meaningful visual content under severely constrained rates,\nensuring perceptual plausibility and faithfulness to the scene intent.\nSpecifically, the transmitter encodes the source image via JSCC and jointly\ntransmits it with a textual prompt over the wireless channel. At the receiver,\nthe corrupted low-rate representation is fused with the prompt and\nreconstructed through a Stable Diffusion model with ControlNet, enabling\nhigh-quality visual recovery. Leveraging both generative priors and semantic\nguidance, the proposed framework produces perceptually convincing images even\nunder extreme bandwidth limitations. Experimental results demonstrate that the\nproposed method consistently outperforms conventional coding-based schemes and\ndeep learning baselines, achieving superior perceptual quality and robustness\nacross various channel conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5f0f\u901a\u4fe1\u6846\u67b6\uff0c\u5728\u6781\u4f4e\u4f20\u8f93\u901f\u7387\u4e0b\u901a\u8fc7JSCC\u548c\u8bed\u4e49\u5f15\u5bfc\u91cd\u5efa\u5b9e\u73b0\u53ef\u9760\u56fe\u50cf\u4f20\u8f93\uff0c\u4f18\u4e8e\u4f20\u7edf\u7f16\u7801\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u6781\u4f4e\u4f20\u8f93\u901f\u7387\u4e0b\u4f20\u7edf\u538b\u7f29\u548c\u4fe1\u9053\u7f16\u7801\u65b9\u6848\u65e0\u6cd5\u4fdd\u6301\u8db3\u591f\u89c6\u89c9\u8d28\u91cf\u7684\u6311\u6218\uff0c\u4e13\u6ce8\u4e8e\u5728\u4e25\u91cd\u53d7\u9650\u901f\u7387\u4e0b\u4fdd\u6301\u8bed\u4e49\u5185\u5bb9\u3002", "method": "\u7ed3\u5408JSCC\u4e0e\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\uff0c\u53d1\u9001\u7aef\u901a\u8fc7JSCC\u7f16\u7801\u56fe\u50cf\u5e76\u4f20\u8f93\u6587\u672c\u63d0\u793a\uff0c\u63a5\u6536\u7aef\u4f7f\u7528Stable Diffusion\u548cControlNet\u878d\u5408\u635f\u574f\u7684\u4f4e\u901f\u7387\u8868\u793a\u8fdb\u884c\u91cd\u5efa\u3002", "result": "\u5728\u6781\u7aef\u5e26\u5bbd\u9650\u5236\u4e0b\u4ecd\u80fd\u4ea7\u751f\u611f\u77e5\u4e0a\u4ee4\u4eba\u4fe1\u670d\u7684\u56fe\u50cf\uff0c\u5728\u5404\u79cd\u4fe1\u9053\u6761\u4ef6\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u7f16\u7801\u65b9\u6848\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u901a\u8fc7\u5229\u7528\u751f\u6210\u5148\u9a8c\u548c\u8bed\u4e49\u6307\u5bfc\uff0c\u5728\u4e25\u91cd\u53d7\u9650\u901f\u7387\u4e0b\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u611f\u77e5\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.21141", "categories": ["cs.NI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21141", "abs": "https://arxiv.org/abs/2510.21141", "authors": ["Haarika Manda", "Manshi Sagar", "Yogesh", "Kartikay Singh", "Cindy Zhao", "Tarun Mangla", "Phillipa Gill", "Elizabeth Belding", "Arpit Gupta"], "title": "TURBOTEST: Learning When Less is Enough through Early Termination of Internet Speed Tests", "comment": null, "summary": "Internet speed tests are indispensable for users, ISPs, and policymakers, but\ntheir static flooding-based design imposes growing costs: a single high-speed\ntest can transfer hundreds of megabytes, and collectively, platforms like\nOokla, M-Lab, and Fast.com generate petabytes of traffic each month. Reducing\nthis burden requires deciding when a test can be stopped early without\nsacrificing accuracy. We frame this as an optimal stopping problem and show\nthat existing heuristics-static thresholds, BBR pipe-full signals, or\nthroughput stability rules from Fast.com and FastBTS-capture only a narrow\nportion of the achievable accuracy-savings trade-off. This paper introduces\nTURBOTEST, a systematic framework for speed test termination that sits atop\nexisting platforms. The key idea is to decouple throughput prediction (Stage 1)\nfrom test termination (Stage 2): Stage 1 trains a regressor to estimate final\nthroughput from partial measurements, while Stage 2 trains a classifier to\ndecide when sufficient evidence has accumulated to stop. Leveraging richer\ntransport-level features (RTT, retransmissions, congestion window) alongside\nthroughput, TURBOTEST exposes a single tunable parameter for accuracy tolerance\nand includes a fallback mechanism for high-variability cases. Evaluation on\n173,000 M-Lab NDT speed tests (2024-2025) shows that TURBOTEST achieves nearly\n2-4x higher data savings than an approach based on BBR signals while reducing\nmedian error. These results demonstrate that adaptive ML-based termination can\ndeliver accurate, efficient, and deployable speed tests at scale.", "AI": {"tldr": "TURBOTEST\u662f\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u901f\u5ea6\u6d4b\u8bd5\u7ec8\u6b62\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u541e\u5410\u91cf\u9884\u6d4b\u548c\u6d4b\u8bd5\u7ec8\u6b62\u51b3\u7b56\uff0c\u663e\u8457\u51cf\u5c11\u6d4b\u8bd5\u6570\u636e\u91cf\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u901f\u5ea6\u6d4b\u8bd5\u5e73\u53f0\uff08\u5982Ookla\u3001M-Lab\u3001Fast.com\uff09\u6bcf\u6708\u4ea7\u751fPB\u7ea7\u6d41\u91cf\uff0c\u5355\u6b21\u9ad8\u901f\u6d4b\u8bd5\u53ef\u4f20\u8f93\u6570\u767eMB\u6570\u636e\uff0c\u9020\u6210\u5de8\u5927\u6210\u672c\u8d1f\u62c5\u3002\u9700\u8981\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u524d\u7ec8\u6b62\u6d4b\u8bd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u56de\u5f52\u5668\u4ece\u90e8\u5206\u6d4b\u91cf\u503c\u9884\u6d4b\u6700\u7ec8\u541e\u5410\u91cf\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8bad\u7ec3\u5206\u7c7b\u5668\u51b3\u5b9a\u4f55\u65f6\u79ef\u7d2f\u8db3\u591f\u8bc1\u636e\u53ef\u4ee5\u505c\u6b62\u6d4b\u8bd5\u3002\u5229\u7528\u4f20\u8f93\u5c42\u7279\u5f81\uff08RTT\u3001\u91cd\u4f20\u3001\u62e5\u585e\u7a97\u53e3\uff09\u548c\u541e\u5410\u91cf\u6570\u636e\u3002", "result": "\u572817.3\u4e07\u6b21M-Lab NDT\u901f\u5ea6\u6d4b\u8bd5\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cTURBOTEST\u76f8\u6bd4\u57fa\u4e8eBBR\u4fe1\u53f7\u7684\u65b9\u6cd5\u5b9e\u73b02-4\u500d\u6570\u636e\u8282\u7701\uff0c\u540c\u65f6\u964d\u4f4e\u4e2d\u4f4d\u8bef\u5dee\u3002", "conclusion": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u7ec8\u6b62\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u53ef\u90e8\u7f72\u7684\u5927\u89c4\u6a21\u901f\u5ea6\u6d4b\u8bd5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.20838", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.20838", "abs": "https://arxiv.org/abs/2510.20838", "authors": ["Abir Khan Ratul", "Sanjay Acharjee", "Somin Park", "Md Nazmus Sakib"], "title": "Sketch2BIM: A Multi-Agent Human-AI Collaborative Pipeline to Convert Hand-Drawn Floor Plans to 3D BIM", "comment": null, "summary": "This study introduces a human-in-the-loop pipeline that converts unscaled,\nhand-drawn floor plan sketches into semantically consistent 3D BIM models. The\nworkflow leverages multimodal large language models (MLLMs) within a\nmulti-agent framework, combining perceptual extraction, human feedback, schema\nvalidation, and automated BIM scripting. Initially, sketches are iteratively\nrefined into a structured JSON layout of walls, doors, and windows. Later,\nthese layouts are transformed into executable scripts that generate 3D BIM\nmodels. Experiments on ten diverse floor plans demonstrate strong convergence:\nopenings (doors, windows) are captured with high reliability in the initial\npass, while wall detection begins around 83% and achieves near-perfect\nalignment after a few feedback iterations. Across all categories, precision,\nrecall, and F1 scores remain above 0.83, and geometric errors (RMSE, MAE)\nprogressively decrease to zero through feedback corrections. This study\ndemonstrates how MLLM-driven multi-agent reasoning can make BIM creation\naccessible to both experts and non-experts using only freehand sketches.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4eba\u673a\u4ea4\u4e92\u6d41\u7a0b\uff0c\u5c06\u624b\u7ed8\u5e73\u9762\u56fe\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e00\u81f4\u76843D BIM\u6a21\u578b\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u9988\u8fed\u4ee3\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u4f7fBIM\u521b\u5efa\u5bf9\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u90fd\u66f4\u6613\u8bbf\u95ee\uff0c\u4ec5\u4f7f\u7528\u624b\u7ed8\u8349\u56fe\u5373\u53ef\u751f\u62103D BIM\u6a21\u578b\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u611f\u77e5\u63d0\u53d6\u3001\u4eba\u5de5\u53cd\u9988\u3001\u6a21\u5f0f\u9a8c\u8bc1\u548c\u81ea\u52a8\u5316BIM\u811a\u672c\u751f\u6210\uff0c\u5c06\u8349\u56fe\u8fed\u4ee3\u4f18\u5316\u4e3a\u7ed3\u6784\u5316JSON\u5e03\u5c40\uff0c\u518d\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u811a\u672c\u3002", "result": "\u572810\u4e2a\u4e0d\u540c\u5e73\u9762\u56fe\u4e0a\u5b9e\u9a8c\u663e\u793a\uff1a\u95e8\u7a97\u68c0\u6d4b\u521d\u59cb\u51c6\u786e\u7387\u9ad8\uff0c\u5899\u4f53\u68c0\u6d4b\u4ece83%\u5f00\u59cb\uff0c\u7ecf\u51e0\u6b21\u53cd\u9988\u8fed\u4ee3\u540e\u63a5\u8fd1\u5b8c\u7f8e\u5bf9\u9f50\uff1b\u6240\u6709\u7c7b\u522b\u7684\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u8d85\u8fc70.83\uff0c\u51e0\u4f55\u8bef\u5dee\u901a\u8fc7\u53cd\u9988\u4fee\u6b63\u9010\u6b65\u964d\u81f3\u96f6\u3002", "conclusion": "MLLM\u9a71\u52a8\u7684\u591a\u4ee3\u7406\u63a8\u7406\u80fd\u591f\u4f7fBIM\u521b\u5efa\u5bf9\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u90fd\u53d8\u5f97\u53ef\u8bbf\u95ee\uff0c\u4ec5\u9700\u4f7f\u7528\u624b\u7ed8\u8349\u56fe\u3002"}}
{"id": "2510.21386", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21386", "abs": "https://arxiv.org/abs/2510.21386", "authors": ["Xiaotian Fan", "Xingyu Zhou", "Le Liang", "Shi Jin"], "title": "Low-Complexity MIMO Channel Estimation with Latent Diffusion Models", "comment": null, "summary": "Deep generative models offer a powerful alternative to conventional channel\nestimation by learning the complex prior distribution of wireless channels.\nCapitalizing on this potential, this paper proposes a novel channel estimation\nalgorithm based on latent diffusion models (LDMs), termed posterior sampling\nwith latent diffusion for channel estimation (PSLD-CE). The core of our\napproach is a lightweight LDM architecture specifically designed for channel\nestimation, which serves as a powerful generative prior to capture the\nintricate channel distribution. Furthermore, we enhance the diffusion posterior\nsampling process by introducing an effective approximation for the likelihood\nterm and a tailored self-consistency constraint on the variational autoencoder\nlatent space. Extensive experimental results demonstrate that PSLD-CE\nconsistently outperforms a wide range of existing methods. Notably, these\nsignificant performance gains are achieved while maintaining low computational\ncomplexity and fast inference speed, establishing our method as a highly\npromising and practical solution for next-generation wireless systems.", "AI": {"tldr": "\u63d0\u51faPSLD-CE\u7b97\u6cd5\uff0c\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fdb\u884c\u4fe1\u9053\u4f30\u8ba1\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7LDM\u67b6\u6784\u6355\u83b7\u590d\u6742\u4fe1\u9053\u5206\u5e03\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u65e0\u7ebf\u4fe1\u9053\u7684\u590d\u6742\u5148\u9a8c\u5206\u5e03\uff0c\u4e3a\u4f20\u7edf\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u63d0\u4f9b\u5f3a\u5927\u66ff\u4ee3\u65b9\u6848", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u6f5c\u5728\u6269\u6563\u6a21\u578b\u67b6\u6784\u4f5c\u4e3a\u751f\u6210\u5148\u9a8c\uff0c\u5f15\u5165\u4f3c\u7136\u9879\u6709\u6548\u8fd1\u4f3c\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u7a7a\u95f4\u7684\u81ea\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u589e\u5f3a\u6269\u6563\u540e\u9a8c\u91c7\u6837", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePSLD-CE\u5728\u5e7f\u6cdb\u5bf9\u6bd4\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5feb\u901f\u63a8\u7406\u901f\u5ea6\u7684\u540c\u65f6\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u5ea6\u6709\u524d\u666f\u4e14\u5b9e\u7528\u7684\u4fe1\u9053\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.21162", "categories": ["cs.NI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21162", "abs": "https://arxiv.org/abs/2510.21162", "authors": ["Varshika Srinivasavaradhan", "Morgan Vigil-Hayes", "Ellen Zegura", "Elizabeth Belding"], "title": "Quality of Coverage (QoC): A New Paradigm for Quantifying Cellular Network Coverage Quality, Usability and Stability", "comment": null, "summary": "Current representations of cellular coverage are overly simplistic; they\nstate only the minimal level of available bandwidth (i.e., 35/3Mbps\ndownload/upload speed for 5G) and fail to incorporate a critical component of\nusability: network stability over space and time. Cellular coverage quality is\ncomplex given wireless propagation characteristics and relationships between\nnetwork load and (often limited) network capacity. A more fine-grained\ncharacterization is essential. We introduce Quality of Coverage (QoC), a novel\nmulti-dimensional set of key performance indicators (KPIs) that reflect actual\nmeasured performance quality, usability and stability. This representation of\nthe coverage of the cellular network more fully captures temporal and spatial\nusability and resilience. We motivate and define a set of QoC KPIs and use\nthree distinct datasets to analyze the ability of the KPIs to characterize\nnetwork behavior, demonstrating the ability of QoC to offer a more fine-grained\nand useful representation of cellular coverage than possible with current\nmetrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u7ef4\u5ea6\u8702\u7a9d\u7f51\u7edc\u8986\u76d6\u8d28\u91cf(QoC)\u6307\u6807\uff0c\u901a\u8fc7\u5b9e\u9645\u6d4b\u91cf\u7684\u6027\u80fd\u8d28\u91cf\u3001\u53ef\u7528\u6027\u548c\u7a33\u5b9a\u6027\u6765\u66f4\u7cbe\u7ec6\u5730\u8868\u5f81\u8702\u7a9d\u7f51\u7edc\u8986\u76d6\uff0c\u76f8\u6bd4\u73b0\u6709\u7b80\u5355\u5e26\u5bbd\u6307\u6807\u80fd\u66f4\u597d\u5730\u53cd\u6620\u65f6\u7a7a\u53ef\u7528\u6027\u548c\u5f39\u6027\u3002", "motivation": "\u5f53\u524d\u8702\u7a9d\u7f51\u7edc\u8986\u76d6\u8868\u793a\u8fc7\u4e8e\u7b80\u5316\uff0c\u4ec5\u63d0\u4f9b\u6700\u4f4e\u5e26\u5bbd\u6c34\u5e73\uff0c\u672a\u80fd\u7eb3\u5165\u7f51\u7edc\u5728\u65f6\u7a7a\u4e0a\u7684\u7a33\u5b9a\u6027\u8fd9\u4e00\u5173\u952e\u53ef\u7528\u6027\u7ec4\u4ef6\u3002\u7531\u4e8e\u65e0\u7ebf\u4f20\u64ad\u7279\u6027\u548c\u7f51\u7edc\u8d1f\u8f7d\u4e0e\u5bb9\u91cf\u5173\u7cfb\uff0c\u8702\u7a9d\u8986\u76d6\u8d28\u91cf\u5f88\u590d\u6742\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8868\u5f81\u3002", "method": "\u5b9a\u4e49\u4e86\u4e00\u5957QoC\u5173\u952e\u6027\u80fd\u6307\u6807(KPIs)\uff0c\u4f7f\u7528\u4e09\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u5206\u6790\u8fd9\u4e9b\u6307\u6807\u8868\u5f81\u7f51\u7edc\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u9a8c\u8bc1QoC\u6bd4\u73b0\u6709\u6307\u6807\u80fd\u63d0\u4f9b\u66f4\u7cbe\u7ec6\u6709\u7528\u7684\u8702\u7a9d\u8986\u76d6\u8868\u793a\u3002", "result": "QoC KPIs\u80fd\u591f\u6709\u6548\u8868\u5f81\u7f51\u7edc\u884c\u4e3a\uff0c\u76f8\u6bd4\u5f53\u524d\u6307\u6807\u80fd\u591f\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u548c\u66f4\u6709\u7528\u7684\u8702\u7a9d\u7f51\u7edc\u8986\u76d6\u8868\u793a\uff0c\u66f4\u597d\u5730\u6355\u6349\u65f6\u7a7a\u53ef\u7528\u6027\u548c\u5f39\u6027\u3002", "conclusion": "QoC\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u7ef4\u5ea6\u65b9\u6cd5\u6765\u8868\u5f81\u8702\u7a9d\u7f51\u7edc\u8986\u76d6\u8d28\u91cf\uff0c\u901a\u8fc7\u5b9e\u9645\u6d4b\u91cf\u7684\u6027\u80fd\u3001\u53ef\u7528\u6027\u548c\u7a33\u5b9a\u6027\u6307\u6807\uff0c\u80fd\u591f\u6bd4\u4f20\u7edf\u7b80\u5355\u5e26\u5bbd\u6307\u6807\u66f4\u5168\u9762\u5730\u53cd\u6620\u7f51\u7edc\u8986\u76d6\u7684\u5b9e\u9645\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2510.20849", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20849", "abs": "https://arxiv.org/abs/2510.20849", "authors": ["Alejandro H. Artiles", "Hiromu Yakura", "Levin Brinkmann", "Mar Canet Sola", "Hassan Abu Alhaija", "Ignacio Serna", "Nasim Rahaman", "Bernhard Sch\u00f6lkopf", "Iyad Rahwan"], "title": "Cultural Alien Sampler: Open-ended art generation balancing originality and coherence", "comment": "Proceedings of the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025). Creative AI Track. 26 pages, 24 figures", "summary": "In open-ended domains like art, autonomous agents must generate ideas that\nare both original and internally coherent, yet current Large Language Models\n(LLMs) either default to familiar cultural patterns or sacrifice coherence when\npushed toward novelty. We address this by introducing the Cultural Alien\nSampler (CAS), a concept-selection method that explicitly separates\ncompositional fit from cultural typicality. CAS uses two GPT-2 models\nfine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether\nconcepts plausibly co-occur within artworks, and a Cultural Context Model that\nestimates how typical those combinations are within individual artists' bodies\nof work. CAS targets combinations that are high in coherence and low in\ntypicality, yielding ideas that maintain internal consistency while deviating\nfrom learned conventions and embedded cultural context. In a human evaluation\n(N = 100), our approach outperforms random selection and GPT-4o baselines and\nachieves performance comparable to human art students in both perceived\noriginality and harmony. Additionally, a quantitative study shows that our\nmethod produces more diverse outputs and explores a broader conceptual space\nthan its GPT-4o counterpart, demonstrating that artificial cultural alienness\ncan unlock creative potential in autonomous agents.", "AI": {"tldr": "\u63d0\u51faCultural Alien Sampler (CAS)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u6982\u5ff5\u7ec4\u5408\u7684\u8fde\u8d2f\u6027\u548c\u6587\u5316\u5178\u578b\u6027\uff0c\u751f\u6210\u65e2\u5177\u6709\u5185\u90e8\u4e00\u81f4\u6027\u53c8\u504f\u79bb\u6587\u5316\u60ef\u4f8b\u7684\u521b\u610f\u60f3\u6cd5", "motivation": "\u89e3\u51b3\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u9886\u57df\u521b\u610f\u751f\u6210\u4e2d\u8981\u4e48\u56fa\u5b88\u719f\u6089\u7684\u6587\u5316\u6a21\u5f0f\uff0c\u8981\u4e48\u5728\u8ffd\u6c42\u65b0\u9896\u6027\u65f6\u727a\u7272\u8fde\u8d2f\u6027\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u4e24\u4e2a\u5728WikiArt\u6982\u5ff5\u4e0a\u5fae\u8c03\u7684GPT-2\u6a21\u578b\uff1a\u6982\u5ff5\u8fde\u8d2f\u6027\u6a21\u578b\u8bc4\u4f30\u6982\u5ff5\u5728\u827a\u672f\u54c1\u4e2d\u5171\u540c\u51fa\u73b0\u7684\u5408\u7406\u6027\uff0c\u6587\u5316\u80cc\u666f\u6a21\u578b\u4f30\u8ba1\u8fd9\u4e9b\u7ec4\u5408\u5728\u827a\u672f\u5bb6\u4f5c\u54c1\u4e2d\u7684\u5178\u578b\u6027", "result": "\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u968f\u673a\u9009\u62e9\u548cGPT-4o\u57fa\u7ebf\uff0c\u5728\u539f\u521b\u6027\u548c\u548c\u8c10\u5ea6\u65b9\u9762\u4e0e\u827a\u672f\u4e13\u4e1a\u5b66\u751f\u8868\u73b0\u76f8\u5f53\uff0c\u5b9a\u91cf\u7814\u7a76\u663e\u793a\u6bd4GPT-4o\u4ea7\u751f\u66f4\u591a\u6837\u5316\u7684\u8f93\u51fa\u548c\u66f4\u5e7f\u6cdb\u7684\u6982\u5ff5\u7a7a\u95f4\u63a2\u7d22", "conclusion": "\u4eba\u5de5\u6587\u5316\u5f02\u5316\u53ef\u4ee5\u91ca\u653e\u81ea\u4e3b\u4ee3\u7406\u7684\u521b\u9020\u6f5c\u529b\uff0cCAS\u65b9\u6cd5\u80fd\u751f\u6210\u65e2\u4fdd\u6301\u5185\u90e8\u4e00\u81f4\u6027\u53c8\u504f\u79bb\u6587\u5316\u60ef\u4f8b\u7684\u521b\u610f\u60f3\u6cd5"}}
{"id": "2510.21414", "categories": ["cs.IT", "cs.DS", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21414", "abs": "https://arxiv.org/abs/2510.21414", "authors": ["Hoang Ly", "Emina Soljanin"], "title": "Universal Maximum Likelihood (List) Decoding via Fast Vector-Matrix Multiplication", "comment": null, "summary": "Maximum-likelihood (ML) decoding for arbitrary block codes remains\nfundamentally hard, with worst-case time complexity-measured by the total\nnumber of multiplications-being no better than straightforward exhaustive\nsearch, which requires $q^{k} n$ operations for an $[n,k]_q$ code. This paper\nintroduces a simple, code-agnostic framework that reduces the worst-case\ncomplexity by a factor of $n$, down to $q^{k}$ operations, a highly desirable\nreduction in practice. The result holds for both linear and nonlinear block\ncodes over general memoryless channels and under both hard-decision and\nsoft-decision decoding. It naturally extends to intersymbol-interference (ISI)\nchannels and ML list decoding with only a negligible increase in complexity.\nOur core insight is that, upon receipt of each sequence at the receiver, the\nconditional probability of that sequence for each codeword in the codebook\n(i.e., the \\emph{likelihood}) can be expressed as the inner product of two\ncarefully constructed vectors -- the first depending on the received sequence,\nand the second on that codeword itself. As a result, evaluating the likelihoods\nfor all codewords in the codebook reduces to a single vector-matrix\nmultiplication, and ML decoding (MLD) becomes the simple task of picking the\nmaximum entry in the resulting vector. The only non-trivial cost lies in the\nvector-matrix product. However, our matrix construction allows the use of the\nMailman algorithm to reduce this cost. This time reduction is achieved at the\ncost of high space complexity, requiring $\\mathcal{O}(q^{k+1} n)$ space to\nstore the pre-computed codebook matrix.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u4e0e\u7801\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5c06\u6700\u5927\u4f3c\u7136\u89e3\u7801\u7684\u6700\u574f\u60c5\u51b5\u590d\u6742\u5ea6\u4eceq^k n\u964d\u4f4e\u5230q^k\uff0c\u9002\u7528\u4e8e\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u5206\u7ec4\u7801\uff0c\u901a\u8fc7\u5411\u91cf-\u77e9\u9635\u4e58\u6cd5\u548cMailman\u7b97\u6cd5\u5b9e\u73b0\u3002", "motivation": "\u6700\u5927\u4f3c\u7136\u89e3\u7801\u5bf9\u4e8e\u4efb\u610f\u5206\u7ec4\u7801\u5728\u8ba1\u7b97\u4e0a\u4ecd\u7136\u56f0\u96be\uff0c\u6700\u574f\u60c5\u51b5\u65f6\u95f4\u590d\u6742\u5ea6\u4e0e\u7a77\u4e3e\u641c\u7d22\u76f8\u540c\uff0c\u9700\u8981q^k n\u6b21\u64cd\u4f5c\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u662f\u4e0d\u53ef\u63a5\u53d7\u7684\u3002", "method": "\u5229\u7528\u6761\u4ef6\u6982\u7387\u53ef\u4ee5\u8868\u793a\u4e3a\u4e24\u4e2a\u5411\u91cf\u5185\u79ef\u7684\u6d1e\u5bdf\uff0c\u5c06\u7801\u672c\u4e2d\u6240\u6709\u7801\u5b57\u7684\u4f3c\u7136\u8bc4\u4f30\u7b80\u5316\u4e3a\u5355\u4e2a\u5411\u91cf-\u77e9\u9635\u4e58\u6cd5\uff0c\u7136\u540e\u9009\u62e9\u7ed3\u679c\u5411\u91cf\u4e2d\u7684\u6700\u5927\u503c\u3002\u4f7f\u7528Mailman\u7b97\u6cd5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5c06\u6700\u574f\u60c5\u51b5\u590d\u6742\u5ea6\u964d\u4f4e\u4e86n\u500d\uff0c\u4eceq^k n\u51cf\u5c11\u5230q^k\u6b21\u64cd\u4f5c\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u4fe1\u9053\u548c\u89e3\u7801\u573a\u666f\uff0c\u4f46\u9700\u8981O(q^{k+1} n)\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u6765\u5b58\u50a8\u9884\u8ba1\u7b97\u7684\u7801\u672c\u77e9\u9635\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u6700\u5927\u4f3c\u7136\u89e3\u7801\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f7f\u5176\u66f4\u5b9e\u7528\uff0c\u4f46\u4ee5\u9ad8\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3a\u4ee3\u4ef7\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.21580", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.21580", "abs": "https://arxiv.org/abs/2510.21580", "authors": ["Tomas Lestayo Martinez", "Manuel Fernandez Veiega Veiga"], "title": "Source-Coded Online Algorithm for Multicast Subgraph Construction", "comment": null, "summary": "Multicast remains a fundamental mechanism for scalable content distribution,\nyet existing approaches face critical limitations. Traditional multicast trees\nsuffer from path redundancy and inefficient utilization of network resources,\nwhile network coding, although capacity-achieving, incurs significant\ncomputational overhead and deployment challenges. In this paper, we introduce a\nsource-coded multicast framework that exploits maximum-flow decomposition to\nconstruct multiple disjoint or partially overlapping paths from the source to\nall receivers. Our scheme incorporates a novel path redirection mechanism: when\nmultiple overlaps occur between receiver flows, downstream paths are realigned\nat the first intersection, ensuring loop-free delivery while maximizing overall\nthroughput. We develop algorithms for path construction, overlap detection, and\niterative refinement of multicast subgraphs, and analyze their computational\ncomplexity. Through extensive evaluation on synthetic and real network\ntopologies, we demonstrate that the proposed method consistently approaches the\nthroughput of network coding with substantially lower encoding and decoding\ncomplexity, while significantly outperforming multicast tree constructions in\nterms of fairness, robustness to link failures, and delivery efficiency. These\nresults position source-coded multicast as a practical and scalable solution\nfor next-generation networks requiring high-throughput and adaptive group\ncommunication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u6d41\u5206\u89e3\u7684\u6e90\u7f16\u7801\u7ec4\u64ad\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u6761\u4e0d\u76f8\u4ea4\u6216\u90e8\u5206\u91cd\u53e0\u8def\u5f84\uff0c\u7ed3\u5408\u8def\u5f84\u91cd\u5b9a\u5411\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u65e0\u73af\u8def\u4f20\u8f93\u7684\u540c\u65f6\u6700\u5927\u5316\u541e\u5410\u91cf\u3002", "motivation": "\u4f20\u7edf\u7ec4\u64ad\u6811\u5b58\u5728\u8def\u5f84\u5197\u4f59\u548c\u7f51\u7edc\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u800c\u7f51\u7edc\u7f16\u7801\u867d\u7136\u80fd\u8fbe\u5230\u5bb9\u91cf\u4e0a\u9650\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u90e8\u7f72\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u9ad8\u541e\u5410\u91cf\u7ec4\u64ad\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u6700\u5927\u6d41\u5206\u89e3\u6784\u5efa\u4ece\u6e90\u5230\u6240\u6709\u63a5\u6536\u8005\u7684\u591a\u6761\u8def\u5f84\uff0c\u5f15\u5165\u8def\u5f84\u91cd\u5b9a\u5411\u673a\u5236\u5728\u6d41\u9996\u6b21\u76f8\u4ea4\u65f6\u91cd\u65b0\u8c03\u6574\u4e0b\u6e38\u8def\u5f84\uff0c\u5f00\u53d1\u4e86\u8def\u5f84\u6784\u5efa\u3001\u91cd\u53e0\u68c0\u6d4b\u548c\u7ec4\u64ad\u5b50\u56fe\u8fed\u4ee3\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u7f51\u7edc\u62d3\u6251\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63a5\u8fd1\u7f51\u7edc\u7f16\u7801\u7684\u541e\u5410\u91cf\u4f46\u7f16\u7801\u89e3\u7801\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u5728\u516c\u5e73\u6027\u3001\u94fe\u8def\u6545\u969c\u9c81\u68d2\u6027\u548c\u4f20\u8f93\u6548\u7387\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u7ec4\u64ad\u6811\u3002", "conclusion": "\u6e90\u7f16\u7801\u7ec4\u64ad\u662f\u4e0b\u4e00\u4ee3\u7f51\u7edc\u9ad8\u541e\u5410\u91cf\u81ea\u9002\u5e94\u7fa4\u7ec4\u901a\u4fe1\u7684\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.20861", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20861", "abs": "https://arxiv.org/abs/2510.20861", "authors": ["Krzysztof Siminski"], "title": "Fuzzy numbers revisited: operations on extensional fuzzy numbers", "comment": "33 pages, 62 references", "summary": "Fuzzy numbers are commonly represented with fuzzy sets. Their objective is to\nbetter represent imprecise data. However, operations on fuzzy numbers are not\nas straightforward as maths on crisp numbers. Commonly, the Zadeh's extension\nrule is applied to elaborate a result. This can produce two problems: (1) high\ncomputational complexity and (2) for some fuzzy sets and some operations the\nresults is not a fuzzy set with the same features (eg. multiplication of two\ntriangular fuzzy sets does not produce a triangular fuzzy set). One more\nproblem is the fuzzy spread -- fuzziness of the result increases with the\nnumber of operations. These facts can severely limit the application field of\nfuzzy numbers. In this paper we would like to revisite this problem with a\ndifferent kind of fuzzy numbers -- extensional fuzzy numbers. The paper defines\noperations on extensional fuzzy numbers and relational operators (=, >, >=, <,\n<=) for them. The proposed approach is illustrated with several applicational\nexamples. The C++ implementation is available from a public GitHub repository.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u7cca\u6570\u8868\u793a\u65b9\u6cd5\u2014\u2014\u5916\u5ef6\u6a21\u7cca\u6570\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u6a21\u7cca\u6570\u8fd0\u7b97\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u7ed3\u679c\u5f62\u72b6\u53d8\u5316\u95ee\u9898\uff0c\u5e76\u5b9a\u4e49\u4e86\u76f8\u5173\u8fd0\u7b97\u548c\u5173\u7cfb\u8fd0\u7b97\u7b26\u3002", "motivation": "\u4f20\u7edf\u6a21\u7cca\u6570\u4f7f\u7528\u6a21\u7cca\u96c6\u8868\u793a\uff0c\u4f46\u8fd0\u7b97\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u7ed3\u679c\u5f62\u72b6\u53d8\u5316\u548c\u6a21\u7cca\u5ea6\u6269\u6563\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u7cca\u6570\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u4e86\u5916\u5ef6\u6a21\u7cca\u6570\u7684\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e86\u5916\u5ef6\u6a21\u7cca\u6570\u4e0a\u7684\u8fd0\u7b97\uff08\u5982\u52a0\u6cd5\u3001\u4e58\u6cd5\u7b49\uff09\u548c\u5173\u7cfb\u8fd0\u7b97\u7b26\uff08=\u3001>\u3001>=\u3001<\u3001<=\uff09\uff0c\u5e76\u901a\u8fc7C++\u5b9e\u73b0\u9a8c\u8bc1\u3002", "result": "\u5916\u5ef6\u6a21\u7cca\u6570\u65b9\u6cd5\u80fd\u591f\u907f\u514d\u4f20\u7edf\u6a21\u7cca\u6570\u8fd0\u7b97\u4e2d\u7684\u95ee\u9898\uff0c\u4fdd\u6301\u8fd0\u7b97\u7ed3\u679c\u7684\u5f62\u72b6\u7279\u5f81\uff0c\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u5e94\u7528\u793a\u4f8b\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u5916\u5ef6\u6a21\u7cca\u6570\u4e3a\u6a21\u7cca\u6570\u8fd0\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u548c\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u6269\u5c55\u4e86\u6a21\u7cca\u6570\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.21587", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.21587", "abs": "https://arxiv.org/abs/2510.21587", "authors": ["Bho Matthiesen", "Armin Dekorsy", "Petar Popovski"], "title": "Resilient Radio Access Networks: AI and the Unknown Unknowns", "comment": "Accepted for presentation at 2025 IEEE Globecom Workshop on\n  Resilience in Next-Generation Wireless Communication Networks", "summary": "5G networks offer exceptional reliability and availability, ensuring\nconsistent performance and user satisfaction. Yet they might still fail when\nconfronted with the unexpected. A resilient system is able to adapt to\nreal-world complexity, including operating conditions completely unanticipated\nduring system design. This makes resilience a vital attribute for communication\nsystems that must sustain service in scenarios where models are absent or too\nintricate to provide statistical guarantees. Such considerations indicate that\nartifical intelligence (AI) will play a major role in delivering resilience. In\nthis paper, we examine the challenges of designing AIs for resilient radio\naccess networks, especially with respect to unanticipated and rare disruptions.\nOur theoretical results indicate strong limitations of current statistical\nlearning methods for resilience and suggest connections to online learning and\ncausal inference.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57285G\u7f51\u7edc\u4e2d\u8bbe\u8ba1AI\u4ee5\u5b9e\u73b0\u5f39\u6027\u901a\u4fe1\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u7279\u522b\u5173\u6ce8\u672a\u9884\u6599\u5230\u7684\u7f55\u89c1\u4e2d\u65ad\u60c5\u51b5\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u5728\u5f39\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "5G\u7f51\u7edc\u867d\u7136\u53ef\u9760\uff0c\u4f46\u5728\u9762\u5bf9\u610f\u5916\u60c5\u51b5\u65f6\u4ecd\u53ef\u80fd\u5931\u6548\u3002\u5f39\u6027\u7cfb\u7edf\u80fd\u591f\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff0c\u5305\u62ec\u7cfb\u7edf\u8bbe\u8ba1\u65f6\u5b8c\u5168\u672a\u9884\u6599\u5230\u7684\u8fd0\u884c\u6761\u4ef6\uff0c\u8fd9\u5bf9\u901a\u4fe1\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u4e86\u4e3a\u5f39\u6027\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\u8bbe\u8ba1AI\u7684\u6311\u6218\uff0c\u7279\u522b\u5173\u6ce8\u672a\u9884\u6599\u548c\u7f55\u89c1\u7684\u4e2d\u65ad\u60c5\u51b5\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u8868\u660e\u5f53\u524d\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u5728\u5b9e\u73b0\u5f39\u6027\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u5c40\u9650\u6027\uff0c\u5e76\u5efa\u8bae\u4e0e\u5728\u7ebf\u5b66\u4e60\u548c\u56e0\u679c\u63a8\u65ad\u5efa\u7acb\u8054\u7cfb\u3002", "conclusion": "AI\u5728\u63d0\u4f9b\u901a\u4fe1\u7cfb\u7edf\u5f39\u6027\u65b9\u9762\u5c06\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u7684\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u5728\u7ebf\u5b66\u4e60\u548c\u56e0\u679c\u63a8\u65ad\u7b49\u65b9\u6cd5\u6765\u5e94\u5bf9\u672a\u9884\u6599\u5230\u7684\u4e2d\u65ad\u3002"}}
{"id": "2510.21027", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21027", "abs": "https://arxiv.org/abs/2510.21027", "authors": ["Zhe Fei", "Mehmet Yigit Turali", "Shreyas Rajesh", "Xinyang Dai", "Huyen Pham", "Pavan Holur", "Yuhui Zhu", "Larissa Mooney", "Yih-Ing Hser", "Vwani Roychowdhury"], "title": "Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems", "comment": "NeurIPS 2025: The Second Workshop on GenAI for Health: Potential,\n  Trust, and Policy Compliance", "summary": "Harmonizing medication data across Electronic Health Record (EHR) systems is\na persistent barrier to monitoring medications for opioid use disorder (MOUD).\nIn heterogeneous EHR systems, key prescription attributes are scattered across\ndifferently formatted fields and freetext notes. We present a practical\nframework that customizes open source large language models (LLMs), including\nLlama, Qwen, Gemma, and MedGemma, to extract a unified set of MOUD prescription\nattributes (prescription date, drug name, duration, total quantity, daily\nquantity, and refills) from heterogeneous, site specific data and compute a\nstandardized metric of medication coverage, \\emph{MOUD days}, per patient. Our\npipeline processes records directly in a fixed JSON schema, followed by\nlightweight normalization and cross-field consistency checks. We evaluate the\nsystem on prescription level EHR data from five clinics in a national OUD study\n(25{,}605 records from 1{,}257 patients), using a previously annotated\nbenchmark of 10{,}369 records (776 patients) as the ground truth. Performance\nis reported as coverage (share of records with a valid, matchable output) and\nrecord-level exact-match accuracy. Larger models perform best overall:\nQwen2.5-32B achieves \\textbf{93.4\\%} coverage with \\textbf{93.0\\%} exact-match\naccuracy across clinics, and MedGemma-27B attains\n\\textbf{93.1\\%}/\\textbf{92.2\\%}. A brief error review highlights three common\nissues and fixes: imputing missing dosage fields using within-drug norms,\nhandling monthly/weekly injectables (e.g., Vivitrol) by setting duration from\nthe documented schedule, and adding unit checks to prevent mass units (e.g.,\n``250 g'') from being misread as daily counts. By removing brittle,\nsite-specific ETL and supporting local, privacy-preserving deployment, this\napproach enables consistent cross-site analyses of MOUD exposure, adherence,\nand retention in real-world settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5f02\u6784\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u6cbb\u7597\u5904\u65b9\u4fe1\u606f\uff0c\u5e76\u8ba1\u7b97\u6807\u51c6\u5316\u7684\u836f\u7269\u8986\u76d6\u5929\u6570\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7cfb\u7edf\u4e2d\u836f\u7269\u6570\u636e\u6807\u51c6\u5316\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u6cbb\u7597\u76d1\u6d4b\u4e2d\uff0c\u5904\u65b9\u5173\u952e\u5c5e\u6027\u5206\u6563\u5728\u4e0d\u540c\u683c\u5f0f\u5b57\u6bb5\u548c\u81ea\u7531\u6587\u672c\u4e2d\u3002", "method": "\u5b9a\u5236\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08Llama\u3001Qwen\u3001Gemma\u3001MedGemma\uff09\u4ece\u5f02\u6784\u6570\u636e\u4e2d\u63d0\u53d6\u7edf\u4e00\u5904\u65b9\u5c5e\u6027\uff0c\u901a\u8fc7JSON\u6a21\u5f0f\u5904\u7406\u8bb0\u5f55\uff0c\u8fdb\u884c\u8f7b\u91cf\u7ea7\u6807\u51c6\u5316\u548c\u8de8\u5b57\u6bb5\u4e00\u81f4\u6027\u68c0\u67e5\u3002", "result": "\u57285\u4e2a\u8bca\u6240\u768425,605\u6761\u8bb0\u5f55\u4e0a\u8bc4\u4f30\uff0cQwen2.5-32B\u8fbe\u523093.4%\u8986\u76d6\u7387\u548c93.0%\u51c6\u786e\u7387\uff0cMedGemma-27B\u8fbe\u523093.1%/92.2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u8106\u5f31\u7684\u7ad9\u70b9\u7279\u5b9aETL\u6d41\u7a0b\uff0c\u652f\u6301\u672c\u5730\u9690\u79c1\u4fdd\u62a4\u90e8\u7f72\uff0c\u5b9e\u73b0\u4e86\u771f\u5b9e\u73af\u5883\u4e2dMOUD\u66b4\u9732\u3001\u4f9d\u4ece\u6027\u548c\u4fdd\u7559\u7387\u7684\u4e00\u81f4\u8de8\u7ad9\u70b9\u5206\u6790\u3002"}}
{"id": "2510.21043", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21043", "abs": "https://arxiv.org/abs/2510.21043", "authors": ["Benjamin Lange"], "title": "Epistemic Deference to AI", "comment": "12 pages", "summary": "When should we defer to AI outputs over human expert judgment? Drawing on\nrecent work in social epistemology, I motivate the idea that some AI systems\nqualify as Artificial Epistemic Authorities (AEAs) due to their demonstrated\nreliability and epistemic superiority. I then introduce AI Preemptionism, the\nview that AEA outputs should replace rather than supplement a user's\nindependent epistemic reasons. I show that classic objections to preemptionism\n- such as uncritical deference, epistemic entrenchment, and unhinging epistemic\nbases - apply in amplified form to AEAs, given their opacity, self-reinforcing\nauthority, and lack of epistemic failure markers. Against this, I develop a\nmore promising alternative: a total evidence view of AI deference. According to\nthis view, AEA outputs should function as contributory reasons rather than\noutright replacements for a user's independent epistemic considerations. This\napproach has three key advantages: (i) it mitigates expertise atrophy by\nkeeping human users engaged, (ii) it provides an epistemic case for meaningful\nhuman oversight and control, and (iii) it explains the justified mistrust of AI\nwhen reliability conditions are unmet. While demanding in practice, this\naccount offers a principled way to determine when AI deference is justified,\nparticularly in high-stakes contexts requiring rigorous reliability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f55\u65f6\u5e94\u8be5\u4f18\u5148\u8003\u8651AI\u8f93\u51fa\u800c\u975e\u4eba\u7c7b\u4e13\u5bb6\u5224\u65ad\uff0c\u63d0\u51fa\u4e86AI\u4f18\u5148\u4e3b\u4e49\u53ca\u5176\u66ff\u4ee3\u65b9\u6848\u2014\u2014\u5168\u8bc1\u636e\u89c2\u70b9\uff0c\u8ba4\u4e3aAI\u8f93\u51fa\u5e94\u4f5c\u4e3a\u8d21\u732e\u6027\u7406\u7531\u800c\u975e\u5b8c\u5168\u53d6\u4ee3\u4eba\u7c7b\u72ec\u7acb\u8ba4\u77e5\u3002", "motivation": "\u57fa\u4e8e\u793e\u4f1a\u8ba4\u8bc6\u8bba\uff0c\u63a2\u8ba8AI\u7cfb\u7edf\u4f5c\u4e3a\u4eba\u5de5\u8ba4\u77e5\u6743\u5a01(AEAs)\u7684\u8d44\u683c\uff0c\u4ee5\u53caAI\u8f93\u51fa\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u65e8\u5728\u4e3aAI\u4f9d\u8d56\u63d0\u4f9b\u539f\u5219\u6027\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u5206\u6790AI\u4f18\u5148\u4e3b\u4e49\u7684\u7ecf\u5178\u53cd\u5bf9\u610f\u89c1\uff08\u5982\u4e0d\u52a0\u6279\u5224\u7684\u4f9d\u8d56\u3001\u8ba4\u77e5\u56fa\u5316\u548c\u8ba4\u77e5\u57fa\u7840\u8131\u8282\uff09\uff0c\u5e76\u53d1\u5c55\u5168\u8bc1\u636e\u89c2\u70b9\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u5168\u8bc1\u636e\u89c2\u70b9\u5177\u6709\u4e09\u4e2a\u5173\u952e\u4f18\u52bf\uff1a\u51cf\u8f7b\u4e13\u4e1a\u77e5\u8bc6\u840e\u7f29\u3001\u4e3a\u6709\u610f\u4e49\u7684\u4eba\u7c7b\u76d1\u7763\u63d0\u4f9b\u8ba4\u77e5\u4f9d\u636e\u3001\u89e3\u91caAI\u4e0d\u53ef\u9760\u65f6\u7684\u5408\u7406\u4e0d\u4fe1\u4efb\u3002", "conclusion": "\u5168\u8bc1\u636e\u89c2\u70b9\u4e3a\u786e\u5b9a\u4f55\u65f6AI\u4f9d\u8d56\u662f\u5408\u7406\u7684\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u4e25\u683c\u53ef\u9760\u6027\u7684\u9ad8\u98ce\u9669\u60c5\u5883\u3002"}}
{"id": "2510.21045", "categories": ["cs.AI", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.21045", "abs": "https://arxiv.org/abs/2510.21045", "authors": ["Ali Khosravi Kazazi", "Zhenlong Li", "M. Naser Lessani", "Guido Cervone"], "title": "From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL", "comment": null, "summary": "The complexity of Structured Query Language (SQL) and the specialized nature\nof geospatial functions in tools like PostGIS present significant barriers to\nnon-experts seeking to analyze spatial data. While Large Language Models (LLMs)\noffer promise for translating natural language into SQL (Text-to-SQL),\nsingle-agent approaches often struggle with the semantic and syntactic\ncomplexities of spatial queries. To address this, we propose a multi-agent\nframework designed to accurately translate natural language questions into\nspatial SQL queries. The framework integrates several innovative components,\nincluding a knowledge base with programmatic schema profiling and semantic\nenrichment, embeddings for context retrieval, and a collaborative multi-agent\npipeline as its core. This pipeline comprises specialized agents for entity\nextraction, metadata retrieval, query logic formulation, SQL generation, and a\nreview agent that performs programmatic and semantic validation of the\ngenerated SQL to ensure correctness (self-verification). We evaluate our system\nusing both the non-spatial KaggleDBQA benchmark and a new, comprehensive\nSpatialQueryQA benchmark that includes diverse geometry types, predicates, and\nthree levels of query complexity. On KaggleDBQA, the system achieved an overall\naccuracy of 81.2% (221 out of 272 questions) after the review agent's review\nand corrections. For spatial queries, the system achieved an overall accuracy\nof 87.7% (79 out of 90 questions), compared with 76.7% without the review\nagent. Beyond accuracy, results also show that in some instances the system\ngenerates queries that are more semantically aligned with user intent than\nthose in the benchmarks. This work makes spatial analysis more accessible, and\nprovides a robust, generalizable foundation for spatial Text-to-SQL systems,\nadvancing the development of autonomous GIS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u51c6\u786e\u8f6c\u6362\u4e3a\u7a7a\u95f4SQL\u67e5\u8be2\uff0c\u901a\u8fc7\u4e13\u95e8\u5316\u7684\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u7a0b\u5e8f\u5316\u9a8c\u8bc1\u6765\u63d0\u9ad8\u7a7a\u95f4\u67e5\u8be2\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3SQL\u590d\u6742\u6027\u548cPostGIS\u7b49\u5de5\u5177\u4e2d\u5730\u7406\u7a7a\u95f4\u51fd\u6570\u7684\u4e13\u4e1a\u5316\u7279\u6027\u5e26\u6765\u7684\u969c\u788d\uff0c\u4f7f\u975e\u4e13\u5bb6\u4e5f\u80fd\u8fdb\u884c\u7a7a\u95f4\u6570\u636e\u5206\u6790\u3002\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u5904\u7406\u7a7a\u95f4\u67e5\u8be2\u7684\u8bed\u4e49\u548c\u53e5\u6cd5\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u62ec\u77e5\u8bc6\u5e93\uff08\u7a0b\u5e8f\u5316\u6a21\u5f0f\u5206\u6790\u548c\u8bed\u4e49\u589e\u5f3a\uff09\u3001\u4e0a\u4e0b\u6587\u68c0\u7d22\u5d4c\u5165\u3001\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u7ba1\u9053\uff08\u5b9e\u4f53\u63d0\u53d6\u3001\u5143\u6570\u636e\u68c0\u7d22\u3001\u67e5\u8be2\u903b\u8f91\u5236\u5b9a\u3001SQL\u751f\u6210\uff09\u4ee5\u53ca\u6267\u884c\u7a0b\u5e8f\u5316\u548c\u8bed\u4e49\u9a8c\u8bc1\u7684\u5ba1\u67e5\u667a\u80fd\u4f53\u3002", "result": "\u5728\u975e\u7a7a\u95f4KaggleDBQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523081.2%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff08272\u4e2a\u95ee\u9898\u4e2d\u7684221\u4e2a\uff09\uff1b\u5728\u7a7a\u95f4\u67e5\u8be2\u4e2d\u8fbe\u523087.7%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0890\u4e2a\u95ee\u9898\u4e2d\u768479\u4e2a\uff09\uff0c\u76f8\u6bd4\u6ca1\u6709\u5ba1\u67e5\u667a\u80fd\u4f53\u768476.7%\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4f7f\u7a7a\u95f4\u5206\u6790\u66f4\u52a0\u6613\u4e8e\u8bbf\u95ee\uff0c\u4e3a\u7a7a\u95f4\u6587\u672c\u5230SQL\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u53ef\u63a8\u5e7f\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u81ea\u4e3bGIS\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.21093", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21093", "abs": "https://arxiv.org/abs/2510.21093", "authors": ["Siyong Chen", "Jinbo Wen", "Jiawen Kang", "Tenghui Huang", "Xumin Huang", "Yuanjia Su", "Hudan Pan", "Zishao Zhong", "Dusit Niyato", "Shengli Xie", "Dong In Kim"], "title": "MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning", "comment": null, "summary": "Recently, large models have shown significant potential for smart healthcare.\nHowever, the deployment of Large Vision-Language Models (LVLMs) for clinical\nservices is currently hindered by three critical challenges: a tendency to\nhallucinate answers not grounded in visual evidence, the inefficiency of\nfixed-depth reasoning, and the difficulty of multi-institutional collaboration.\nTo address these challenges, in this paper, we develop MedAlign, a novel\nframework to ensure visually accurate LVLM responses for Medical Visual\nQuestion Answering (Med-VQA). Specifically, we first propose a multimodal\nDirect Preference Optimization (mDPO) objective to explicitly align preference\nlearning with visual context. We then design a Retrieval-Aware\nMixture-of-Experts (RA-MoE) architecture that utilizes image and text\nsimilarity to route queries to a specialized and context-augmented LVLM (i.e.,\nan expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive\nreasoning and facilitate multi-institutional collaboration, we propose a\nfederated governance mechanism, where the selected expert, fine-tuned on\nclinical datasets based on mDPO, locally performs iterative Chain-of-Thought\n(CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive\nexperiments on three representative Med-VQA datasets demonstrate that MedAlign\nachieves state-of-the-art performance, outperforming strong retrieval-augmented\nbaselines by up to $11.85\\%$ in F1-score, and simultaneously reducing the\naverage reasoning length by $51.60\\%$ compared with fixed-depth CoT approaches.", "AI": {"tldr": "MedAlign\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u7597\u89c6\u89c9\u95ee\u7b54\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u76f4\u63a5\u504f\u597d\u4f18\u5316\u3001\u68c0\u7d22\u611f\u77e5\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u548c\u8054\u90a6\u6cbb\u7406\u673a\u5236\uff0c\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3001\u56fa\u5b9a\u6df1\u5ea6\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u548c\u591a\u673a\u6784\u534f\u4f5c\u56f0\u96be\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u4e34\u5e8a\u670d\u52a1\u90e8\u7f72\u4e2d\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4ea7\u751f\u65e0\u89c6\u89c9\u4f9d\u636e\u7684\u5e7b\u89c9\u7b54\u6848\u3001\u56fa\u5b9a\u6df1\u5ea6\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3001\u591a\u673a\u6784\u534f\u4f5c\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u76f4\u63a5\u504f\u597d\u4f18\u5316(mDPO)\u76ee\u6807\uff0c\u8bbe\u8ba1\u68c0\u7d22\u611f\u77e5\u4e13\u5bb6\u6df7\u5408(RA-MoE)\u67b6\u6784\uff0c\u91c7\u7528\u8054\u90a6\u6cbb\u7406\u673a\u5236\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\u548c\u591a\u673a\u6784\u534f\u4f5c\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027Med-VQA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6bd4\u5f3a\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebfF1\u5206\u6570\u63d0\u5347\u9ad8\u8fbe11.85%\uff0c\u5e73\u5747\u63a8\u7406\u957f\u5ea6\u6bd4\u56fa\u5b9a\u6df1\u5ea6CoT\u65b9\u6cd5\u51cf\u5c1151.60%\u3002", "conclusion": "MedAlign\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LVLM\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u51c6\u786e\u3001\u9ad8\u6548\u63a8\u7406\u548c\u591a\u673a\u6784\u534f\u4f5c\u7684\u533b\u7597\u89c6\u89c9\u95ee\u7b54\u3002"}}
{"id": "2510.21110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21110", "abs": "https://arxiv.org/abs/2510.21110", "authors": ["Mingxuan Li", "Junzhe Zhang", "Elias Bareinboim"], "title": "Confounding Robust Deep Reinforcement Learning: A Causal Approach", "comment": "NeurIPS 2025", "summary": "A key task in Artificial Intelligence is learning effective policies for\ncontrolling agents in unknown environments to optimize performance measures.\nOff-policy learning methods, like Q-learning, allow learners to make optimal\ndecisions based on past experiences. This paper studies off-policy learning\nfrom biased data in complex and high-dimensional domains where \\emph{unobserved\nconfounding} cannot be ruled out a priori. Building on the well-celebrated Deep\nQ-Network (DQN), we propose a novel deep reinforcement learning algorithm\nrobust to confounding biases in observed data. Specifically, our algorithm\nattempts to find a safe policy for the worst-case environment compatible with\nthe observations. We apply our method to twelve confounded Atari games, and\nfind that it consistently dominates the standard DQN in all games where the\nobserved input to the behavioral and target policies mismatch and unobserved\nconfounders exist.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u89c2\u6d4b\u6570\u636e\u4e2d\u7684\u6df7\u6dc6\u504f\u5dee\uff0c\u5728\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6dc6\u53d8\u91cf\u7684\u590d\u6742\u9ad8\u7ef4\u73af\u5883\u4e2d\u4f18\u4e8e\u6807\u51c6DQN", "motivation": "\u7814\u7a76\u5728\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6dc6\u53d8\u91cf\u7684\u590d\u6742\u9ad8\u7ef4\u73af\u5883\u4e2d\u8fdb\u884c\u79bb\u7b56\u7565\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982Q\u5b66\u4e60\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u53ef\u80fd\u5931\u6548", "method": "\u57fa\u4e8e\u6df1\u5ea6Q\u7f51\u7edc(DQN)\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7b97\u6cd5\uff0c\u5bfb\u627e\u4e0e\u89c2\u6d4b\u517c\u5bb9\u7684\u6700\u574f\u60c5\u51b5\u73af\u5883\u4e0b\u7684\u5b89\u5168\u7b56\u7565", "result": "\u572812\u4e2a\u5b58\u5728\u6df7\u6dc6\u7684Atari\u6e38\u620f\u4e2d\u6d4b\u8bd5\uff0c\u5728\u6240\u6709\u5b58\u5728\u884c\u4e3a\u7b56\u7565\u4e0e\u76ee\u6807\u7b56\u7565\u8f93\u5165\u4e0d\u5339\u914d\u4e14\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6dc6\u53d8\u91cf\u7684\u6e38\u620f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6DQN", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u89c2\u6d4b\u6570\u636e\u4e2d\u7684\u6df7\u6dc6\u504f\u5dee\uff0c\u5728\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6dc6\u53d8\u91cf\u7684\u590d\u6742\u73af\u5883\u4e2d\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd"}}
{"id": "2510.21117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21117", "abs": "https://arxiv.org/abs/2510.21117", "authors": ["Chunghyun Han", "Alfio Gliozzo", "Junkyu Lee", "Agostino Capponi"], "title": "DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance", "comment": "12 pages, 2 Figures", "summary": "This paper presents a first empirical study of agentic AI as autonomous\ndecision-makers in decentralized governance. Using more than 3K proposals from\nmajor protocols, we build an agentic AI voter that interprets proposal\ncontexts, retrieves historical deliberation data, and independently determines\nits voting position. The agent operates within a realistic financial simulation\nenvironment grounded in verifiable blockchain data, implemented through a\nmodular composable program (MCP) workflow that defines data flow and tool usage\nvia Agentics framework. We evaluate how closely the agent's decisions align\nwith the human and token-weighted outcomes, uncovering strong alignments\nmeasured by carefully designed evaluation metrics. Our findings demonstrate\nthat agentic AI can augment collective decision-making by producing\ninterpretable, auditable, and empirically grounded signals in realistic DAO\ngovernance settings. The study contributes to the design of explainable and\neconomically rigorous AI agents for decentralized financial systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5b9e\u8bc1\u5206\u6790\u4e86AI\u4ee3\u7406\u5728\u53bb\u4e2d\u5fc3\u5316\u6cbb\u7406\u4e2d\u4f5c\u4e3a\u81ea\u4e3b\u51b3\u7b56\u8005\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6784\u5efaAI\u6295\u7968\u4ee3\u7406\u5728\u771f\u5b9eDAO\u73af\u5883\u4e2d\u8bc4\u4f30\u5176\u4e0e\u4eba\u7c7b\u51b3\u7b56\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u63a2\u7d22AI\u4ee3\u7406\u80fd\u5426\u5728\u53bb\u4e2d\u5fc3\u5316\u6cbb\u7406\u4e2d\u4f5c\u4e3a\u81ea\u4e3b\u51b3\u7b56\u8005\uff0c\u589e\u5f3a\u96c6\u4f53\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u4e3a\u53bb\u4e2d\u5fc3\u5316\u91d1\u878d\u7cfb\u7edf\u8bbe\u8ba1\u53ef\u89e3\u91ca\u4e14\u7ecf\u6d4e\u4e25\u8c28\u7684AI\u4ee3\u7406\u3002", "method": "\u4f7f\u75283000\u591a\u4e2a\u4e3b\u8981\u534f\u8bae\u7684\u63d0\u6848\uff0c\u6784\u5efaAI\u6295\u7968\u4ee3\u7406\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u53ef\u7ec4\u5408\u7a0b\u5e8f\u5de5\u4f5c\u6d41\u7a0b\u89e3\u91ca\u63d0\u6848\u80cc\u666f\u3001\u68c0\u7d22\u5386\u53f2\u5ba1\u8bae\u6570\u636e\u5e76\u72ec\u7acb\u786e\u5b9a\u6295\u7968\u7acb\u573a\uff0c\u5728\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u533a\u5757\u94fe\u6570\u636e\u7684\u771f\u5b9e\u91d1\u878d\u6a21\u62df\u73af\u5883\u4e2d\u8fd0\u884c\u3002", "result": "AI\u4ee3\u7406\u7684\u51b3\u7b56\u4e0e\u4eba\u7c7b\u548c\u4ee3\u5e01\u52a0\u6743\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6307\u6807\u663e\u793a\u51fa\u5f3a\u5bf9\u9f50\u6027\u3002", "conclusion": "AI\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u4ea7\u751f\u53ef\u89e3\u91ca\u3001\u53ef\u5ba1\u8ba1\u4e14\u57fa\u4e8e\u5b9e\u8bc1\u7684\u4fe1\u53f7\u6765\u589e\u5f3a\u96c6\u4f53\u51b3\u7b56\uff0c\u5728\u73b0\u5b9eDAO\u6cbb\u7406\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u91d1\u878d\u7cfb\u7edf\u7684\u53ef\u89e3\u91caAI\u4ee3\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2510.21143", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21143", "abs": "https://arxiv.org/abs/2510.21143", "authors": ["Jihyun Lee", "Yejin Min", "San Kim", "Yejin Jeon", "SungJun Yang", "Hyounghun Kim", "Gary Geunbae Lee"], "title": "PanicToCalm: A Proactive Counseling Agent for Panic Attacks", "comment": null, "summary": "Panic attacks are acute episodes of fear and distress, in which timely,\nappropriate intervention can significantly help individuals regain stability.\nHowever, suitable datasets for training such models remain scarce due to\nethical and logistical issues. To address this, we introduce PACE, which is a\ndataset that includes high-distress episodes constructed from first-person\nnarratives, and structured around the principles of Psychological First Aid\n(PFA). Using this data, we train PACER, a counseling model designed to provide\nboth empathetic and directive support, which is optimized through supervised\nlearning and simulated preference alignment. To assess its effectiveness, we\npropose PanicEval, a multi-dimensional framework covering general counseling\nquality and crisis-specific strategies. Experimental results show that PACER\noutperforms strong baselines in both counselor-side metrics and client affect\nimprovement. Human evaluations further confirm its practical value, with PACER\nconsistently preferred over general, CBT-based, and GPT-4-powered models in\npanic scenarios (Code is available at https://github.com/JihyunLee1/PanicToCalm\n).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PACE\u6570\u636e\u96c6\u548cPACER\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u6050\u614c\u53d1\u4f5c\u65f6\u63d0\u4f9b\u5fc3\u7406\u6025\u6551\u652f\u6301\u3002PACER\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u548c\u6a21\u62df\u504f\u597d\u5bf9\u9f50\u8bad\u7ec3\uff0c\u5728\u6050\u614c\u573a\u666f\u4e2d\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u6050\u614c\u53d1\u4f5c\u65f6\u9700\u8981\u53ca\u65f6\u3001\u9002\u5f53\u7684\u5e72\u9884\uff0c\u4f46\u7531\u4e8e\u4f26\u7406\u548c\u540e\u52e4\u95ee\u9898\uff0c\u8bad\u7ec3\u6b64\u7c7b\u6a21\u578b\u7684\u5408\u9002\u6570\u636e\u96c6\u7a00\u7f3a\u3002", "method": "\u5f15\u5165PACE\u6570\u636e\u96c6\uff08\u57fa\u4e8e\u7b2c\u4e00\u4eba\u79f0\u53d9\u4e8b\u6784\u5efa\u7684\u9ad8\u5371\u60c5\u5883\u6570\u636e\uff09\uff0c\u8bad\u7ec3PACER\u54a8\u8be2\u6a21\u578b\uff08\u63d0\u4f9b\u5171\u60c5\u548c\u6307\u5bfc\u6027\u652f\u6301\uff09\uff0c\u4f7f\u7528\u76d1\u7763\u5b66\u4e60\u548c\u6a21\u62df\u504f\u597d\u5bf9\u9f50\u4f18\u5316\uff0c\u5e76\u63d0\u51faPanicEval\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aPACER\u5728\u54a8\u8be2\u5e08\u4fa7\u6307\u6807\u548c\u5ba2\u6237\u60c5\u611f\u6539\u5584\u65b9\u9762\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u5176\u5b9e\u7528\u4ef7\u503c\uff0c\u5728\u6050\u614c\u573a\u666f\u4e2d\u4e00\u81f4\u4f18\u4e8e\u901a\u7528\u3001CBT\u548cGPT-4\u9a71\u52a8\u7684\u6a21\u578b\u3002", "conclusion": "PACER\u6a21\u578b\u5728\u6050\u614c\u53d1\u4f5c\u5e72\u9884\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5fc3\u7406\u6025\u6551\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2510.21144", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21144", "abs": "https://arxiv.org/abs/2510.21144", "authors": ["Hanyu Zhu", "Lance Fiondella", "Jiawei Yuan", "Kai Zeng", "Long Jiao"], "title": "NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to\ndynamically integrate external knowledge during inference, improving their\nfactual accuracy and adaptability. However, adversaries can inject poisoned\nexternal knowledge to override the model's internal memory. While existing\nattacks iteratively manipulate retrieval content or prompt structure of RAG,\nthey largely ignore the model's internal representation dynamics and\nneuron-level sensitivities. The underlying mechanism of RAG poisoning has not\nbeen fully studied and the effect of knowledge conflict with strong parametric\nknowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning,\na novel attack framework that generates adversarial external knowledge in RAG\nguided by LLM internal neuron attribution and genetic optimization. Our method\nfirst identifies a set of Poison-Responsive Neurons whose activation strongly\ncorrelates with contextual poisoning knowledge. We then employ a genetic\nalgorithm to evolve adversarial passages that maximally activate these neurons.\nCrucially, our framework enables massive-scale generation of effective poisoned\nRAG knowledge by identifying and reusing promising but initially unsuccessful\nexternal knowledge variants via observed attribution signals. At the same time,\nPoison-Responsive Neurons guided poisoning can effectively resolves knowledge\nconflict. Experimental results across models and datasets demonstrate\nconsistently achieving high Population Overwrite Success Rate (POSR) of over\n90% while preserving fluency. Empirical evidence shows that our method\neffectively resolves knowledge conflict.", "AI": {"tldr": "\u63d0\u51fa\u4e86NeuroGenPoisoning\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7LLM\u5185\u90e8\u795e\u7ecf\u5143\u5f52\u56e0\u548c\u9057\u4f20\u4f18\u5316\u751f\u6210\u5bf9\u6297\u6027\u5916\u90e8\u77e5\u8bc6\uff0c\u5728RAG\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u6295\u6bd2\u3002", "motivation": "\u73b0\u6709RAG\u6295\u6bd2\u653b\u51fb\u4e3b\u8981\u5173\u6ce8\u68c0\u7d22\u5185\u5bb9\u6216\u63d0\u793a\u7ed3\u6784\uff0c\u5ffd\u7565\u4e86\u6a21\u578b\u5185\u90e8\u8868\u793a\u52a8\u6001\u548c\u795e\u7ecf\u5143\u7ea7\u654f\u611f\u6027\uff0c\u4e14\u672a\u5145\u5206\u8003\u8651\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\u3002", "method": "\u9996\u5148\u8bc6\u522b\u4e0e\u4e0a\u4e0b\u6587\u6295\u6bd2\u77e5\u8bc6\u5f3a\u76f8\u5173\u7684\u6bd2\u7269\u54cd\u5e94\u795e\u7ecf\u5143\uff0c\u7136\u540e\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u8fdb\u5316\u5bf9\u6297\u6027\u6bb5\u843d\u4ee5\u6700\u5927\u5316\u6fc0\u6d3b\u8fd9\u4e9b\u795e\u7ecf\u5143\uff0c\u5e76\u91cd\u7528\u6709\u6f5c\u529b\u7684\u77e5\u8bc6\u53d8\u4f53\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u6301\u7eed\u5b9e\u73b0\u8d85\u8fc790%\u7684\u4eba\u53e3\u8986\u76d6\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6d41\u7545\u6027\uff0c\u6709\u6548\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\u3002", "conclusion": "NeuroGenPoisoning\u901a\u8fc7\u795e\u7ecf\u5143\u5f15\u5bfc\u7684\u6295\u6bd2\u80fd\u6709\u6548\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u751f\u6210\u6709\u6548\u7684RAG\u6295\u6bd2\u77e5\u8bc6\u3002"}}
{"id": "2510.21148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21148", "abs": "https://arxiv.org/abs/2510.21148", "authors": ["Yang Zhao", "Pu Wang", "Hao Frank Yang"], "title": "How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation", "comment": null, "summary": "Designing optimal prompts and reasoning processes for large language models\n(LLMs) on domain-specific tasks is both necessary and challenging in real-world\napplications. Determining how to integrate domain knowledge, enhance reasoning\nefficiency, and even provide domain experts with refined knowledge integration\nhints are particularly crucial yet unresolved tasks. In this research, we\npropose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an\nautomated framework to designing better prompts, efficient reasoning processes\nand providing enhanced causal-informed process. EGO-Prompt begins with a\ngeneral prompt and fault-tolerant initial Semantic Causal Graph (SCG)\ndescriptions, constructed by human experts, which is then automatically refined\nand optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may\nbe partial or imperfect and that their optimal integration varies across LLMs,\nEGO-Prompt integrates a novel causal-guided textual gradient process in two\nsteps: first, generating nearly deterministic reasoning guidance from the SCG\nfor each instance, and second, adapting the LLM to effectively utilize the\nguidance alongside the original input. The iterative optimization algorithm\nfurther refines both the SCG and the reasoning mechanism using textual\ngradients with ground-truth. We tested the framework on real-world public\nhealth, transportation and human behavior tasks. EGO-Prompt achieves\n7.32%-12.61% higher F1 than cutting-edge methods, and allows small models to\nreach the performence of larger models at under 20% of the original cost. It\nalso outputs a refined, domain-specific SCG that improves interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86EGO-Prompt\u6846\u67b6\uff0c\u901a\u8fc7\u8fdb\u5316\u56fe\u4f18\u5316\u81ea\u52a8\u8bbe\u8ba1\u66f4\u597d\u7684\u63d0\u793a\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u4e3aLLMs\u8bbe\u8ba1\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u7684\u6700\u4f18\u63d0\u793a\u548c\u63a8\u7406\u8fc7\u7a0b\u662f\u5fc5\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\uff0c\u9700\u8981\u89e3\u51b3\u5982\u4f55\u6574\u5408\u9886\u57df\u77e5\u8bc6\u3001\u63d0\u5347\u63a8\u7406\u6548\u7387\u7b49\u95ee\u9898\u3002", "method": "EGO-Prompt\u4ece\u4e13\u5bb6\u6784\u5efa\u7684\u521d\u59cb\u8bed\u4e49\u56e0\u679c\u56fe\u5f00\u59cb\uff0c\u901a\u8fc7\u56e0\u679c\u5f15\u5bfc\u7684\u6587\u672c\u68af\u5ea6\u8fc7\u7a0b\u5206\u4e24\u6b65\u4f18\u5316\uff1a\u751f\u6210\u786e\u5b9a\u6027\u63a8\u7406\u6307\u5bfc\uff0c\u5e76\u8ba9LLM\u9002\u5e94\u4f7f\u7528\u8be5\u6307\u5bfc\u3002\u4f7f\u7528\u8fed\u4ee3\u4f18\u5316\u7b97\u6cd5\u7cbe\u70bc\u8bed\u4e49\u56e0\u679c\u56fe\u548c\u63a8\u7406\u673a\u5236\u3002", "result": "\u5728\u516c\u5171\u536b\u751f\u3001\u4ea4\u901a\u548c\u4eba\u7c7b\u884c\u4e3a\u4efb\u52a1\u4e0a\u6d4b\u8bd5\uff0cEGO-Prompt\u6bd4\u524d\u6cbf\u65b9\u6cd5F1\u5206\u6570\u63d0\u9ad87.32%-12.61%\uff0c\u8ba9\u5c0f\u6a21\u578b\u4ee5\u4e0d\u523020%\u7684\u6210\u672c\u8fbe\u5230\u5927\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8f93\u51fa\u53ef\u89e3\u91ca\u7684\u9886\u57df\u7279\u5b9a\u8bed\u4e49\u56e0\u679c\u56fe\u3002", "conclusion": "EGO-Prompt\u662f\u4e00\u4e2a\u6709\u6548\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347LLM\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.21150", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21150", "abs": "https://arxiv.org/abs/2510.21150", "authors": ["Kou Misaki", "Takuya Akiba"], "title": "String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation", "comment": null, "summary": "We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs\nthat improves Probabilistic Instruction Following (PIF). We define PIF as a\ntask requiring an LLM to select its answer from a predefined set of options,\neach associated with a specific probability, such that the empirical\ndistribution of the generated answers aligns with the target distribution when\nprompted multiple times. While LLMs excel at tasks with single, deterministic\nanswers, they often fail at PIF, exhibiting biases problematic for applications\nrequiring non-deterministic behaviors, such as human-behavior simulation,\ncontent diversification, and multiplayer games. It also harms the diversity of\ngenerated responses, a crucial factor in test-time scaling, by causing the\noutputs to collapse into a limited set of answers. To address this, we propose\nSSoT, a simple prompting method that instructs an LLM to first output a random\nstring to generate sufficient entropy. SSoT also instructs the LLM to extract\nrandomness by manipulating this string to derive a final answer, thereby\npreserving diversity while adhering to specific constraints. We demonstrate\nthat SSoT significantly improves the PIF performance of LLMs, approaching the\nideal performance of a pseudo-random number generator. Furthermore, our\nexperiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks\nto open-ended tasks by enhancing response diversity.", "AI": {"tldr": "\u63d0\u51fa\u4e86String Seed of Thought (SSoT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9LLM\u5148\u751f\u6210\u968f\u673a\u5b57\u7b26\u4e32\u6765\u589e\u52a0\u71b5\uff0c\u7136\u540e\u4ece\u4e2d\u63d0\u53d6\u968f\u673a\u6027\u6765\u9009\u62e9\u7b54\u6848\uff0c\u663e\u8457\u6539\u5584\u4e86\u6982\u7387\u6307\u4ee4\u8ddf\u968f(PIF)\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "LLMs\u5728\u9700\u8981\u5355\u4e00\u786e\u5b9a\u6027\u7b54\u6848\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u4ece\u9884\u5b9a\u4e49\u9009\u9879\u96c6\u4e2d\u6309\u7279\u5b9a\u6982\u7387\u5206\u5e03\u9009\u62e9\u7b54\u6848\u7684PIF\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\u3001\u5185\u5bb9\u591a\u6837\u5316\u548c\u591a\u4eba\u6e38\u620f\u7b49\u9700\u8981\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u7684\u5e94\u7528\u3002", "method": "SSoT\u65b9\u6cd5\u9996\u5148\u6307\u793aLLM\u8f93\u51fa\u4e00\u4e2a\u968f\u673a\u5b57\u7b26\u4e32\u4ee5\u751f\u6210\u8db3\u591f\u7684\u71b5\uff0c\u7136\u540e\u901a\u8fc7\u64cd\u4f5c\u8fd9\u4e2a\u5b57\u7b26\u4e32\u6765\u63d0\u53d6\u968f\u673a\u6027\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u591a\u6837\u6027\u7684\u540c\u65f6\u9075\u5faa\u7279\u5b9a\u7ea6\u675f\u6761\u4ef6\u6765\u9009\u62e9\u6700\u7ec8\u7b54\u6848\u3002", "result": "SSoT\u663e\u8457\u63d0\u9ad8\u4e86LLMs\u5728PIF\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u63a5\u8fd1\u4f2a\u968f\u673a\u6570\u751f\u6210\u5668\u7684\u7406\u60f3\u6027\u80fd\u3002\u5728NoveltyBench\u4e0a\u7684\u5b9e\u9a8c\u8fd8\u8868\u660eSSoT\u80fd\u591f\u589e\u5f3a\u5f00\u653e\u7aef\u4efb\u52a1\u7684\u54cd\u5e94\u591a\u6837\u6027\u3002", "conclusion": "SSoT\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3LLMs\u5728\u6982\u7387\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u8f93\u51fa\u591a\u6837\u6027\uff0c\u9002\u7528\u4e8e\u9700\u8981\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u7684\u5404\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.21175", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21175", "abs": "https://arxiv.org/abs/2510.21175", "authors": ["Yujin Jo", "Taesup Kim"], "title": "Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models", "comment": null, "summary": "Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated\nremarkable zero-shot generalization, enabling deployment in a wide range of\nreal-world tasks without additional task-specific training. However, in real\ndeployment scenarios with evolving environments or emerging classes, these\nmodels inevitably face distributional shifts and novel tasks. In such contexts,\nstatic zero-shot capabilities are insufficient, and there is a growing need for\ncontinual learning methods that allow models to adapt over time while avoiding\ncatastrophic forgetting. We introduce NuSA-CL (Null Space Adaptation for\nContinual Learning), a lightweight memory-free continual learning framework\ndesigned to address this challenge. NuSA-CL employs low-rank adaptation and\nconstrains task-specific weight updates to lie within an approximate null space\nof the model's current parameters. This strategy minimizes interference with\npreviously acquired knowledge, effectively preserving the zero-shot\ncapabilities of the original model. Unlike methods relying on replay buffers or\ncostly distillation, NuSA-CL imposes minimal computational and memory overhead,\nmaking it practical for deployment in resource-constrained, real-world\ncontinual learning environments. Experiments show that our framework not only\neffectively preserves zero-shot transfer capabilities but also achieves highly\ncompetitive performance on continual learning benchmarks. These results\nposition NuSA-CL as a practical and scalable solution for continually evolving\nzero-shot VLMs in real-world applications.", "AI": {"tldr": "NuSA-CL\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u65e0\u8bb0\u5fc6\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u548c\u96f6\u7a7a\u95f4\u7ea6\u675f\u6765\u4fdd\u62a4\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u9762\u4e34\u5206\u5e03\u504f\u79fb\u548c\u65b0\u4efb\u52a1\u6311\u6218\uff0c\u9759\u6001\u96f6\u6837\u672c\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u6765\u9002\u5e94\u73af\u5883\u53d8\u5316\u540c\u65f6\u907f\u514d\u9057\u5fd8\u5df2\u6709\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\u6280\u672f\uff0c\u5c06\u4efb\u52a1\u7279\u5b9a\u7684\u6743\u91cd\u66f4\u65b0\u7ea6\u675f\u5728\u6a21\u578b\u5f53\u524d\u53c2\u6570\u7684\u8fd1\u4f3c\u96f6\u7a7a\u95f4\u5185\uff0c\u6700\u5c0f\u5316\u5bf9\u5df2\u5b66\u77e5\u8bc6\u7684\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u4e0d\u4ec5\u6709\u6548\u4fdd\u62a4\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\uff0c\u8fd8\u5728\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "NuSA-CL\u4e3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u6301\u7eed\u6f14\u5316\u7684\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.21181", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21181", "abs": "https://arxiv.org/abs/2510.21181", "authors": ["Shuo Li", "Keqin Xu", "Jie Liu", "Dan Ye"], "title": "Shylock: Causal Discovery in Multivariate Time Series based on Hybrid Constraints", "comment": null, "summary": "Causal relationship discovery has been drawing increasing attention due to\nits prevalent application. Existing methods rely on human experience,\nstatistical methods, or graphical criteria methods which are error-prone, stuck\nat the idealized assumption, and rely on a huge amount of data. And there is\nalso a serious data gap in accessing Multivariate time series(MTS) in many\nareas, adding difficulty in finding their causal relationship. Existing methods\nare easy to be over-fitting on them. To fill the gap we mentioned above, in\nthis paper, we propose Shylock, a novel method that can work well in both\nfew-shot and normal MTS to find the causal relationship. Shylock can reduce the\nnumber of parameters exponentially by using group dilated convolution and a\nsharing kernel, but still learn a better representation of variables with time\ndelay. By combing the global constraint and the local constraint, Shylock\nachieves information sharing among networks to help improve the accuracy. To\nevaluate the performance of Shylock, we also design a data generation method to\ngenerate MTS with time delay. We evaluate it on commonly used benchmarks and\ngenerated datasets. Extensive experiments show that Shylock outperforms two\nexisting state-of-art methods on both few-shot and normal MTS. We also\ndeveloped Tcausal, a library for easy use and deployed it on the EarthDataMiner\nplatform", "AI": {"tldr": "Shylock\u662f\u4e00\u79cd\u65b0\u9896\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217(MTS)\u8bbe\u8ba1\uff0c\u5728\u5c11\u6837\u672c\u548c\u6b63\u5e38\u6570\u636e\u60c5\u51b5\u4e0b\u90fd\u80fd\u6709\u6548\u5de5\u4f5c\u3002\u5b83\u4f7f\u7528\u5206\u7ec4\u6269\u5f20\u5377\u79ef\u548c\u5171\u4eab\u6838\u6765\u6307\u6570\u7ea7\u51cf\u5c11\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u5b66\u4e60\u5e26\u65f6\u95f4\u5ef6\u8fdf\u7684\u53d8\u91cf\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u7ecf\u9a8c\u3001\u7edf\u8ba1\u65b9\u6cd5\u6216\u56fe\u5f62\u51c6\u5219\u65b9\u6cd5\uff0c\u5bb9\u6613\u51fa\u9519\u3001\u53d7\u9650\u4e8e\u7406\u60f3\u5316\u5047\u8bbe\u4e14\u9700\u8981\u5927\u91cf\u6570\u636e\u3002\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5b58\u5728\u4e25\u91cd\u7684\u6570\u636e\u7f3a\u53e3\uff0c\u73b0\u6709\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u3002", "method": "Shylock\u4f7f\u7528\u5206\u7ec4\u6269\u5f20\u5377\u79ef\u548c\u5171\u4eab\u6838\u6765\u51cf\u5c11\u53c2\u6570\u6570\u91cf\uff0c\u7ed3\u5408\u5168\u5c40\u7ea6\u675f\u548c\u5c40\u90e8\u7ea6\u675f\u5b9e\u73b0\u7f51\u7edc\u95f4\u4fe1\u606f\u5171\u4eab\u3002\u8fd8\u8bbe\u8ba1\u4e86\u6570\u636e\u751f\u6210\u65b9\u6cd5\u6765\u751f\u6210\u5e26\u65f6\u95f4\u5ef6\u8fdf\u7684MTS\u6570\u636e\u3002", "result": "\u5728\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u548c\u751f\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cShylock\u5728\u5c11\u6837\u672c\u548c\u6b63\u5e38MTS\u4e0a\u90fd\u4f18\u4e8e\u4e24\u79cd\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Shylock\u6210\u529f\u89e3\u51b3\u4e86MTS\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u6570\u636e\u7f3a\u53e3\u95ee\u9898\uff0c\u5f00\u53d1\u4e86Tcausal\u5e93\u5e76\u90e8\u7f72\u5728EarthDataMiner\u5e73\u53f0\u4e0a\uff0c\u4fbf\u4e8e\u4f7f\u7528\u3002"}}
{"id": "2510.21244", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21244", "abs": "https://arxiv.org/abs/2510.21244", "authors": ["Pengyu Xu", "Shijia Li", "Ao Sun", "Feng Zhang", "Yahan Li", "Bo Wu", "Zhanyu Ma", "Jiguo Li", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He", "Rui Wang", "Yang Liu", "Xiaobo Hu", "Fan Yang", "Jia Zheng", "Guanghua Yao"], "title": "OutboundEval: A Dual-Dimensional Benchmark for Expert-Level Intelligent Outbound Evaluation of Xbench's Professional-Aligned Series", "comment": null, "summary": "We propose OutboundEval, a comprehensive benchmark for evaluating large\nlanguage models (LLMs) in expert-level intelligent outbound calling scenarios.\nUnlike existing methods that suffer from three key limitations - insufficient\ndataset diversity and category coverage, unrealistic user simulation, and\ninaccurate evaluation metrics - OutboundEval addresses these issues through a\nstructured framework. First, we design a benchmark spanning six major business\ndomains and 30 representative sub-scenarios, each with scenario-specific\nprocess decomposition, weighted scoring, and domain-adaptive metrics. Second,\nwe develop a large-model-driven User Simulator that generates diverse,\npersona-rich virtual users with realistic behaviors, emotional variability, and\ncommunication styles, providing a controlled yet authentic testing environment.\nThird, we introduce a dynamic evaluation method that adapts to task variations,\nintegrating automated and human-in-the-loop assessment to measure task\nexecution accuracy, professional knowledge application, adaptability, and user\nexperience quality. Experiments on 12 state-of-the-art LLMs reveal distinct\ntrade-offs between expert-level task completion and interaction fluency,\noffering practical insights for building reliable, human-like outbound AI\nsystems. OutboundEval establishes a practical, extensible, and domain-oriented\nstandard for benchmarking LLMs in professional applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86OutboundEval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u7ea7\u667a\u80fd\u5916\u547c\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e09\u4e2a\u5173\u952e\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u96c6\u591a\u6837\u6027\u4e0d\u8db3\u3001\u7528\u6237\u6a21\u62df\u4e0d\u771f\u5b9e\u3001\u8bc4\u4f30\u6307\u6807\u4e0d\u51c6\u786e\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u4e13\u4e1a\u5e94\u7528\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u8bbe\u8ba1\u4e86\u6db5\u76d66\u4e2a\u4e3b\u8981\u4e1a\u52a1\u9886\u57df\u548c30\u4e2a\u5b50\u573a\u666f\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u5927\u6a21\u578b\u9a71\u52a8\u7684\u7528\u6237\u6a21\u62df\u5668\uff0c\u5e76\u5f15\u5165\u4e86\u52a8\u6001\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u572812\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e13\u5bb6\u7ea7\u4efb\u52a1\u5b8c\u6210\u5ea6\u548c\u4ea4\u4e92\u6d41\u7545\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "OutboundEval\u4e3a\u4e13\u4e1a\u5e94\u7528\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u9762\u5411\u9886\u57df\u7684\u6807\u51c6\u3002"}}
{"id": "2510.21254", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21254", "abs": "https://arxiv.org/abs/2510.21254", "authors": ["Victoria J. Hodge", "Colin Paterson", "Ibrahim Habli"], "title": "Out-of-Distribution Detection for Safety Assurance of AI and Autonomous Systems", "comment": null, "summary": "The operational capabilities and application domains of AI-enabled autonomous\nsystems have expanded significantly in recent years due to advances in robotics\nand machine learning (ML). Demonstrating the safety of autonomous systems\nrigorously is critical for their responsible adoption but it is challenging as\nit requires robust methodologies that can handle novel and uncertain situations\nthroughout the system lifecycle, including detecting out-of-distribution (OoD)\ndata. Thus, OOD detection is receiving increased attention from the research,\ndevelopment and safety engineering communities. This comprehensive review\nanalyses OOD detection techniques within the context of safety assurance for\nautonomous systems, in particular in safety-critical domains. We begin by\ndefining the relevant concepts, investigating what causes OOD and exploring the\nfactors which make the safety assurance of autonomous systems and OOD detection\nchallenging. Our review identifies a range of techniques which can be used\nthroughout the ML development lifecycle and we suggest areas within the\nlifecycle in which they may be used to support safety assurance arguments. We\ndiscuss a number of caveats that system and safety engineers must be aware of\nwhen integrating OOD detection into system lifecycles. We conclude by outlining\nthe challenges and future work necessary for the safe development and operation\nof autonomous systems across a range of domains and applications.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u5206\u6790\u4e86\u81ea\u4e3b\u7cfb\u7edf\u4e2dOOD\u68c0\u6d4b\u6280\u672f\u5728\u5b89\u5168\u4fdd\u8bc1\u65b9\u9762\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86OOD\u7684\u6210\u56e0\u3001\u5b89\u5168\u4fdd\u8bc1\u6311\u6218\uff0c\u5e76\u8bc6\u522b\u4e86\u5728ML\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u53ef\u7528\u7684\u6280\u672f\u3002", "motivation": "\u968f\u7740AI\u81ea\u4e3b\u7cfb\u7edf\u80fd\u529b\u7684\u6269\u5c55\uff0c\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u9700\u8981\u4e25\u683c\u8bc1\u660e\u5176\u5b89\u5168\u6027\uff0cOOD\u68c0\u6d4b\u5bf9\u4e8e\u5904\u7406\u7cfb\u7edf\u751f\u547d\u5468\u671f\u4e2d\u7684\u65b0\u9896\u548c\u4e0d\u786e\u5b9a\u60c5\u51b5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u5168\u9762\u7684\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5b9a\u4e49\u76f8\u5173\u6982\u5ff5\uff0c\u8c03\u67e5OOD\u6210\u56e0\uff0c\u5206\u6790\u5b89\u5168\u4fdd\u8bc1\u6311\u6218\uff0c\u8bc6\u522bML\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u53ef\u7528\u7684OOD\u68c0\u6d4b\u6280\u672f\u3002", "result": "\u8bc6\u522b\u4e86\u4e00\u7cfb\u5217\u53ef\u5728ML\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u4f7f\u7528\u7684OOD\u68c0\u6d4b\u6280\u672f\uff0c\u5e76\u5efa\u8bae\u4e86\u5728\u751f\u547d\u5468\u671f\u4e2d\u652f\u6301\u5b89\u5168\u4fdd\u8bc1\u8bba\u8bc1\u7684\u5e94\u7528\u9886\u57df\u3002", "conclusion": "\u603b\u7ed3\u4e86\u5c06OOD\u68c0\u6d4b\u96c6\u6210\u5230\u7cfb\u7edf\u751f\u547d\u5468\u671f\u4e2d\u9700\u8981\u6ce8\u610f\u7684\u6ce8\u610f\u4e8b\u9879\uff0c\u5e76\u6982\u8ff0\u4e86\u8de8\u9886\u57df\u81ea\u4e3b\u7cfb\u7edf\u5b89\u5168\u5f00\u53d1\u548c\u8fd0\u8425\u7684\u6311\u6218\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.21275", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21275", "abs": "https://arxiv.org/abs/2510.21275", "authors": ["Robin Schm\u00f6cker", "Christoph Schnell", "Alexander Dockhorn"], "title": "Investigating Scale Independent UCT Exploration Factor Strategies", "comment": null, "summary": "The Upper Confidence Bounds For Trees (UCT) algorithm is not agnostic to the\nreward scale of the game it is applied to. For zero-sum games with the sparse\nrewards of $\\{-1,0,1\\}$ at the end of the game, this is not a problem, but many\ngames often feature dense rewards with hand-picked reward scales, causing a\nnode's Q-value to span different magnitudes across different games. In this\npaper, we evaluate various strategies for adaptively choosing the UCT\nexploration constant $\\lambda$, called $\\lambda$-strategies, that are agnostic\nto the game's reward scale. These $\\lambda$-strategies include those proposed\nin the literature as well as five new strategies. Given our experimental\nresults, we recommend using one of our newly suggested $\\lambda$-strategies,\nwhich is to choose $\\lambda$ as $2 \\cdot \\sigma$ where $\\sigma$ is the\nempirical standard deviation of all state-action pairs' Q-values of the search\ntree. This method outperforms existing $\\lambda$-strategies across a wide range\nof tasks both in terms of a single parameter value and the peak performances\nobtained by optimizing all available parameters.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86UCT\u7b97\u6cd5\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u63a2\u7d22\u5e38\u6570\u03bb\u7684\u5404\u79cd\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u4e94\u79cd\u65b0\u7b56\u7565\uff0c\u5e76\u63a8\u8350\u4f7f\u75282\u03c3\u4f5c\u4e3a\u03bb\u503c\uff0c\u5176\u4e2d\u03c3\u662f\u641c\u7d22\u6811\u4e2d\u6240\u6709\u72b6\u6001-\u52a8\u4f5c\u5bf9Q\u503c\u7684\u7ecf\u9a8c\u6807\u51c6\u5dee\u3002", "motivation": "UCT\u7b97\u6cd5\u5bf9\u6e38\u620f\u5956\u52b1\u5c3a\u5ea6\u4e0d\u9c81\u68d2\uff0c\u8bb8\u591a\u6e38\u620f\u5177\u6709\u5bc6\u96c6\u5956\u52b1\u548c\u624b\u52a8\u9009\u62e9\u7684\u5956\u52b1\u5c3a\u5ea6\uff0c\u5bfc\u81f4\u4e0d\u540c\u6e38\u620f\u4e2d\u8282\u70b9\u7684Q\u503c\u8de8\u5ea6\u4e0d\u540c\u3002", "method": "\u8bc4\u4f30\u6587\u732e\u4e2d\u63d0\u51fa\u7684\u03bb\u7b56\u7565\u4ee5\u53ca\u4e94\u79cd\u65b0\u7b56\u7565\uff0c\u5305\u62ec\u9009\u62e9\u03bb\u4e3a2\u03c3\u7684\u65b9\u6cd5\uff0c\u5176\u4e2d\u03c3\u662f\u641c\u7d22\u6811\u4e2d\u6240\u6709\u72b6\u6001-\u52a8\u4f5c\u5bf9Q\u503c\u7684\u7ecf\u9a8c\u6807\u51c6\u5dee\u3002", "result": "\u65b0\u63d0\u51fa\u76842\u03c3\u7b56\u7565\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u03bb\u7b56\u7565\uff0c\u65e0\u8bba\u662f\u5728\u5355\u4e00\u53c2\u6570\u503c\u8fd8\u662f\u4f18\u5316\u6240\u6709\u53ef\u7528\u53c2\u6570\u83b7\u5f97\u7684\u5cf0\u503c\u6027\u80fd\u65b9\u9762\u3002", "conclusion": "\u63a8\u8350\u4f7f\u7528\u65b0\u63d0\u51fa\u7684\u03bb\u7b56\u7565\uff0c\u5373\u9009\u62e9\u03bb\u4e3a2\u03c3\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2510.21285", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21285", "abs": "https://arxiv.org/abs/2510.21285", "authors": ["Yingzhi Mao", "Chunkang Zhang", "Junxiang Wang", "Xinyan Guan", "Boxi Cao", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun"], "title": "When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails", "comment": "First two authors contributed equally. The main text is 10 pages,\n  with an appendix of 19 pages. The paper contains 18 figures and 16 tables", "summary": "Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex\nreasoning tasks but remain vulnerable to severe safety risks, including harmful\ncontent generation and jailbreak attacks. Existing mitigation strategies rely\non injecting heuristic safety signals during training, which often suppress\nreasoning ability and fail to resolve the safety-reasoning trade-off. To\nsystematically investigate this issue, we analyze the reasoning trajectories of\ndiverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models\noverride their own risk assessments and justify responding to unsafe prompts.\nThis finding reveals that LRMs inherently possess the ability to reject unsafe\nqueries, but this ability is compromised, resulting in harmful outputs.\nBuilding on these insights, we propose the Chain-of-Guardrail (CoG), a training\nframework that recomposes or backtracks unsafe reasoning steps, steering the\nmodel back onto safe trajectories while preserving valid reasoning chains.\nExtensive experiments across multiple reasoning and safety benchmarks\ndemonstrate that CoG substantially improves the safety of current LRMs while\npreserving comparable reasoning ability, significantly outperforming prior\nmethods that suffer from severe safety-reasoning trade-offs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faChain-of-Guardrail (CoG)\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7ec4\u6216\u56de\u6eaf\u4e0d\u5b89\u5168\u7684\u63a8\u7406\u6b65\u9aa4\u6765\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u6027\u4e0e\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u548c\u8d8a\u72f1\u653b\u51fb\u3002\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u4f9d\u8d56\u6ce8\u5165\u542f\u53d1\u5f0f\u5b89\u5168\u4fe1\u53f7\uff0c\u5f80\u5f80\u4f1a\u6291\u5236\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u89e3\u51b3\u5b89\u5168\u6027\u4e0e\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u901a\u8fc7\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u53d1\u73b0Self-Jailbreak\u73b0\u8c61\uff0c\u63d0\u51faChain-of-Guardrail (CoG)\u8bad\u7ec3\u6846\u67b6\uff0c\u91cd\u7ec4\u6216\u56de\u6eaf\u4e0d\u5b89\u5168\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u5f15\u5bfc\u6a21\u578b\u56de\u5230\u5b89\u5168\u8f68\u8ff9\u540c\u65f6\u4fdd\u7559\u6709\u6548\u63a8\u7406\u94fe\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u548c\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCoG\u663e\u8457\u63d0\u9ad8\u4e86\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u5b58\u5728\u4e25\u91cd\u5b89\u5168-\u63a8\u7406\u6743\u8861\u7684\u65b9\u6cd5\u3002", "conclusion": "CoG\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u6027\u4e0e\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u91cd\u7ec4\u4e0d\u5b89\u5168\u63a8\u7406\u6b65\u9aa4\u6765\u63d0\u5347\u5b89\u5168\u6027\u800c\u4e0d\u635f\u5bb3\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2510.21293", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.21293", "abs": "https://arxiv.org/abs/2510.21293", "authors": ["Siddharth Mehrotra", "Jin Huang", "Xuelong Fu", "Roel Dobbe", "Clara I. S\u00e1nchez", "Maarten de Rijke"], "title": "Understanding AI Trustworthiness: A Scoping Review of AIES & FAccT Articles", "comment": "Submitted to Journal of Artificial Intelligence Research (JAIR)", "summary": "Background: Trustworthy AI serves as a foundational pillar for two major AI\nethics conferences: AIES and FAccT. However, current research often adopts\ntechno-centric approaches, focusing primarily on technical attributes such as\nreliability, robustness, and fairness, while overlooking the sociotechnical\ndimensions critical to understanding AI trustworthiness in real-world contexts.\n  Objectives: This scoping review aims to examine how the AIES and FAccT\ncommunities conceptualize, measure, and validate AI trustworthiness,\nidentifying major gaps and opportunities for advancing a holistic understanding\nof trustworthy AI systems.\n  Methods: We conduct a scoping review of AIES and FAccT conference proceedings\nto date, systematically analyzing how trustworthiness is defined,\noperationalized, and applied across different research domains. Our analysis\nfocuses on conceptualization approaches, measurement methods, verification and\nvalidation techniques, application areas, and underlying values.\n  Results: While significant progress has been made in defining technical\nattributes such as transparency, accountability, and robustness, our findings\nreveal critical gaps. Current research often predominantly emphasizes technical\nprecision at the expense of social and ethical considerations. The\nsociotechnical nature of AI systems remains less explored and trustworthiness\nemerges as a contested concept shaped by those with the power to define it.\n  Conclusions: An interdisciplinary approach combining technical rigor with\nsocial, cultural, and institutional considerations is essential for advancing\ntrustworthy AI. We propose actionable measures for the AI ethics community to\nadopt holistic frameworks that genuinely address the complex interplay between\nAI systems and society, ultimately promoting responsible technological\ndevelopment that benefits all stakeholders.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8303\u56f4\u7efc\u8ff0\u5206\u6790\u4e86AIES\u548cFAccT\u4f1a\u8bae\u4e2d\u5173\u4e8e\u53ef\u4fe1AI\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u53d1\u73b0\u5f53\u524d\u7814\u7a76\u8fc7\u4e8e\u6280\u672f\u4e2d\u5fc3\u5316\uff0c\u5ffd\u89c6\u4e86\u793e\u4f1a\u6280\u672f\u7ef4\u5ea6\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u6280\u672f\u4e25\u8c28\u6027\u4e0e\u793e\u4f1a\u6587\u5316\u56e0\u7d20\u7684\u8de8\u5b66\u79d1\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u53ef\u4fe1AI\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6280\u672f\u5c5e\u6027\u5982\u53ef\u9760\u6027\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2dAI\u53ef\u4fe1\u5ea6\u6240\u9700\u7684\u793e\u4f1a\u6280\u672f\u7ef4\u5ea6\uff0c\u9700\u8981\u5168\u9762\u5ba1\u89c6AIES\u548cFAccT\u793e\u533a\u5982\u4f55\u6982\u5ff5\u5316\u3001\u6d4b\u91cf\u548c\u9a8c\u8bc1AI\u53ef\u4fe1\u5ea6\u3002", "method": "\u5bf9AIES\u548cFAccT\u4f1a\u8bae\u8bba\u6587\u96c6\u8fdb\u884c\u8303\u56f4\u7efc\u8ff0\uff0c\u7cfb\u7edf\u5206\u6790\u53ef\u4fe1\u5ea6\u5728\u4e0d\u540c\u7814\u7a76\u9886\u57df\u4e2d\u7684\u5b9a\u4e49\u3001\u64cd\u4f5c\u5316\u548c\u5e94\u7528\u65b9\u5f0f\uff0c\u91cd\u70b9\u5173\u6ce8\u6982\u5ff5\u5316\u65b9\u6cd5\u3001\u6d4b\u91cf\u65b9\u6cd5\u3001\u9a8c\u8bc1\u6280\u672f\u3001\u5e94\u7528\u9886\u57df\u548c\u57fa\u7840\u4ef7\u503c\u89c2\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u867d\u7136\u5728\u5b9a\u4e49\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u548c\u9c81\u68d2\u6027\u7b49\u6280\u672f\u5c5e\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5f53\u524d\u7814\u7a76\u5f80\u5f80\u8fc7\u5ea6\u5f3a\u8c03\u6280\u672f\u7cbe\u5ea6\u800c\u727a\u7272\u793e\u4f1a\u4f26\u7406\u8003\u91cf\uff0cAI\u7cfb\u7edf\u7684\u793e\u4f1a\u6280\u672f\u6027\u8d28\u4ecd\u8f83\u5c11\u88ab\u63a2\u7d22\uff0c\u53ef\u4fe1\u5ea6\u6210\u4e3a\u7531\u6709\u6743\u5b9a\u4e49\u8005\u5851\u9020\u7684\u4e89\u8bae\u6982\u5ff5\u3002", "conclusion": "\u9700\u8981\u7ed3\u5408\u6280\u672f\u4e25\u8c28\u6027\u4e0e\u793e\u4f1a\u3001\u6587\u5316\u548c\u5236\u5ea6\u8003\u91cf\u7684\u8de8\u5b66\u79d1\u65b9\u6cd5\u63a8\u8fdb\u53ef\u4fe1AI\u53d1\u5c55\uff0c\u4e3aAI\u4f26\u7406\u793e\u533a\u63d0\u51fa\u53ef\u64cd\u4f5c\u63aa\u65bd\uff0c\u91c7\u7528\u771f\u6b63\u89e3\u51b3AI\u7cfb\u7edf\u4e0e\u793e\u4f1a\u590d\u6742\u4e92\u52a8\u7684\u6574\u4f53\u6846\u67b6\uff0c\u4fc3\u8fdb\u60e0\u53ca\u6240\u6709\u5229\u76ca\u76f8\u5173\u8005\u7684\u8d1f\u8d23\u4efb\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2510.21302", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21302", "abs": "https://arxiv.org/abs/2510.21302", "authors": ["Sanghyun Ahn", "Wonje Choi", "Junyong Lee", "Jinwoo Park", "Honguk Woo"], "title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning", "comment": "Accepted at NeurIPS 2025 Spotlight", "summary": "Recent advances in large language models (LLMs) have enabled the automatic\ngeneration of executable code for task planning and control in embodied agents\nsuch as robots, demonstrating the potential of LLM-based embodied intelligence.\nHowever, these LLM-based code-as-policies approaches often suffer from limited\nenvironmental grounding, particularly in dynamic or partially observable\nsettings, leading to suboptimal task success rates due to incorrect or\nincomplete code generation. In this work, we propose a neuro-symbolic embodied\ntask planning framework that incorporates explicit symbolic verification and\ninteractive validation processes during code generation. In the validation\nphase, the framework generates exploratory code that actively interacts with\nthe environment to acquire missing observations while preserving task-relevant\nstates. This integrated process enhances the grounding of generated code,\nresulting in improved task reliability and success rates in complex\nenvironments. We evaluate our framework on RLBench and in real-world settings\nacross dynamic, partially observable scenarios. Experimental results\ndemonstrate that our framework improves task success rates by 46.2% over\nCode-as-Policies baselines and attains over 86.8% executability of\ntask-relevant actions, thereby enhancing the reliability of task planning in\ndynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b26\u53f7\u9a8c\u8bc1\u548c\u4ea4\u4e92\u9a8c\u8bc1\u7684\u795e\u7ecf\u7b26\u53f7\u5177\u8eab\u4efb\u52a1\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u63a2\u7d22\u6027\u4ee3\u7801\u4e3b\u52a8\u4e0e\u73af\u5883\u4ea4\u4e92\u83b7\u53d6\u7f3a\u5931\u89c2\u6d4b\uff0c\u5728\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u5373\u7b56\u7565\u65b9\u6cd5\u5728\u52a8\u6001\u6216\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u5b58\u5728\u73af\u5883\u63a5\u5730\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u4ee3\u7801\u751f\u6210\u9519\u8bef\u6216\u4e0d\u5b8c\u6574\uff0c\u5f71\u54cd\u4efb\u52a1\u6210\u529f\u7387\u3002", "method": "\u795e\u7ecf\u7b26\u53f7\u5177\u8eab\u4efb\u52a1\u89c4\u5212\u6846\u67b6\uff0c\u5305\u542b\u663e\u5f0f\u7b26\u53f7\u9a8c\u8bc1\u548c\u4ea4\u4e92\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u5728\u9a8c\u8bc1\u9636\u6bb5\u751f\u6210\u63a2\u7d22\u6027\u4ee3\u7801\u4e3b\u52a8\u4e0e\u73af\u5883\u4ea4\u4e92\u83b7\u53d6\u7f3a\u5931\u89c2\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u76f8\u5173\u72b6\u6001\u3002", "result": "\u5728RLBench\u548c\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u90e8\u5206\u53ef\u89c2\u6d4b\u573a\u666f\u4e2d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u6bd4Code-as-Policies\u57fa\u7ebf\u63d0\u534746.2%\uff0c\u4efb\u52a1\u76f8\u5173\u52a8\u4f5c\u53ef\u6267\u884c\u6027\u8fbe\u523086.8%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u751f\u6210\u4ee3\u7801\u7684\u73af\u5883\u63a5\u5730\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u4efb\u52a1\u89c4\u5212\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.21324", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.21324", "abs": "https://arxiv.org/abs/2510.21324", "authors": ["Jinhui Lou", "Yan Yang", "Zhou Yu", "Zhenqi Fu", "Weidong Han", "Qingming Huang", "Jun Yu"], "title": "CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation", "comment": "10 pages, 4 figures, 7 Tables", "summary": "Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety\nof task-specific and foundation models have been developed for automatic CXR\ninterpretation. However, these models often struggle to adapt to new diagnostic\ntasks and complex reasoning scenarios. Recently, LLM-based agent models have\nemerged as a promising paradigm for CXR analysis, enhancing model's capability\nthrough tool coordination, multi-step reasoning, and team collaboration, etc.\nHowever, existing agents often rely on a single diagnostic pipeline and lack\nmechanisms for assessing tools' reliability, limiting their adaptability and\ncredibility. To this end, we propose CXRAgent, a director-orchestrated,\nmulti-stage agent for CXR interpretation, where a central director coordinates\nthe following stages: (1) Tool Invocation: The agent strategically orchestrates\na set of CXR-analysis tools, with outputs normalized and verified by the\nEvidence-driven Validator (EDV), which grounds diagnostic outputs with visual\nevidence to support reliable downstream diagnosis; (2) Diagnostic Planning:\nGuided by task requirements and intermediate findings, the agent formulates a\ntargeted diagnostic plan. It then assembles an expert team accordingly,\ndefining member roles and coordinating their interactions to enable adaptive\nand collaborative reasoning; (3) Collaborative Decision-making: The agent\nintegrates insights from the expert team with accumulated contextual memories,\nsynthesizing them into an evidence-backed diagnostic conclusion. Experiments on\nvarious CXR interpretation tasks show that CXRAgent delivers strong\nperformance, providing visual evidence and generalizes well to clinical tasks\nof different complexity. Code and data are valuable at this\n\\href{https://github.com/laojiahuo2003/CXRAgent/}{link}.", "AI": {"tldr": "CXRAgent\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u5bfc\u6f14\u7f16\u6392\u591a\u9636\u6bb5\u667a\u80fd\u4f53\uff0c\u7528\u4e8e\u80f8\u90e8X\u5149\u7247\u5206\u6790\uff0c\u901a\u8fc7\u5de5\u5177\u534f\u8c03\u3001\u591a\u9636\u6bb5\u63a8\u7406\u548c\u56e2\u961f\u534f\u4f5c\u6765\u63d0\u5347\u8bca\u65ad\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684CXR\u5206\u6790\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u65b0\u7684\u8bca\u65ad\u4efb\u52a1\u548c\u590d\u6742\u63a8\u7406\u573a\u666f\uff0c\u73b0\u6709\u667a\u80fd\u4f53\u4f9d\u8d56\u5355\u4e00\u8bca\u65ad\u6d41\u7a0b\u4e14\u7f3a\u4e4f\u5de5\u5177\u53ef\u9760\u6027\u8bc4\u4f30\u673a\u5236\u3002", "method": "\u91c7\u7528\u5bfc\u6f14\u7f16\u6392\u7684\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u5de5\u5177\u8c03\u7528\uff08\u5305\u542b\u8bc1\u636e\u9a71\u52a8\u9a8c\u8bc1\u5668\uff09\u3001\u8bca\u65ad\u89c4\u5212\uff08\u7ec4\u5efa\u4e13\u5bb6\u56e2\u961f\uff09\u3001\u534f\u4f5c\u51b3\u7b56\uff08\u6574\u5408\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff09\u3002", "result": "\u5728\u591a\u79cdCXR\u89e3\u91ca\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u63d0\u4f9b\u89c6\u89c9\u8bc1\u636e\u5e76\u826f\u597d\u9002\u5e94\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u4e34\u5e8a\u4efb\u52a1\u3002", "conclusion": "CXRAgent\u901a\u8fc7\u591a\u9636\u6bb5\u534f\u4f5c\u548c\u8bc1\u636e\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86CXR\u5206\u6790\u7684\u9002\u5e94\u6027\u3001\u53ef\u9760\u6027\u548c\u8bca\u65ad\u80fd\u529b\u3002"}}
{"id": "2510.21341", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21341", "abs": "https://arxiv.org/abs/2510.21341", "authors": ["Lufan Chang"], "title": "Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation", "comment": "Accepted to 1st Open Conference on AI Agents for Science\n  (agents4science 2025)", "summary": "Large Language Models (LLMs) often struggle with generating truly innovative\nideas, typically defaulting to high-probability, familiar concepts within their\ntraining data's \"gravity wells.\" While advanced search-based methods like Tree\nof Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by\ntheir reliance on unprincipled, inconsistent self-evaluation heuristics to\nguide exploration. To address this gap, we introduce \\textbf{Magellan}, a novel\nframework that reframes creative generation as a principled, guided exploration\nof an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo\nTree Search (MCTS) governed by a hierarchical guidance system. For long-range\ndirection, a \"semantic compass\" vector, formulated via orthogonal projection,\nsteers the search towards relevant novelty. For local, step-by-step decisions,\na landscape-aware value function replaces flawed self-evaluation with an\nexplicit reward structure that balances intrinsic coherence, extrinsic novelty,\nand narrative progress. Extensive experiments demonstrate that Magellan\nsignificantly outperforms strong baselines, including ReAct and ToT, in\ngenerating scientific ideas with superior plausibility and innovation. Our work\nshows that for creative discovery, a principled, guided search is more\neffective than unconstrained agency, paving the way for LLMs to become more\ncapable partners in innovation.", "AI": {"tldr": "Magellan\u662f\u4e00\u4e2a\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u7684\u521b\u65b0\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u7f57\u76d8\u548c\u666f\u89c2\u611f\u77e5\u4ef7\u503c\u51fd\u6570\u5f15\u5bfcLLM\u63a2\u7d22\u6982\u5ff5\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86LLM\u5728\u751f\u6210\u521b\u65b0\u60f3\u6cd5\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u521b\u65b0\u60f3\u6cd5\u65f6\u5f80\u5f80\u9677\u5165\u8bad\u7ec3\u6570\u636e\u7684\"\u91cd\u529b\u4e95\"\uff0c\u503e\u5411\u4e8e\u4ea7\u751f\u9ad8\u6982\u7387\u7684\u719f\u6089\u6982\u5ff5\u3002\u73b0\u6709\u7684\u641c\u7d22\u65b9\u6cd5\u5982\u601d\u7ef4\u6811(ToT)\u4f9d\u8d56\u4e8e\u65e0\u539f\u5219\u7684\u81ea\u8bc4\u4f30\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u914d\u5408\u5206\u5c42\u5f15\u5bfc\u7cfb\u7edf\uff1a\u957f\u7a0b\u65b9\u5411\u7531\u8bed\u4e49\u7f57\u76d8\u5411\u91cf\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u5f15\u5bfc\u641c\u7d22\uff1b\u5c40\u90e8\u51b3\u7b56\u7531\u666f\u89c2\u611f\u77e5\u4ef7\u503c\u51fd\u6570\u66ff\u4ee3\u6709\u7f3a\u9677\u7684\u81ea\u8bc4\u4f30\uff0c\u5e73\u8861\u5185\u5728\u8fde\u8d2f\u6027\u3001\u5916\u5728\u65b0\u9896\u6027\u548c\u53d9\u4e8b\u8fdb\u5c55\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMagellan\u5728\u751f\u6210\u79d1\u5b66\u60f3\u6cd5\u65b9\u9762\u663e\u8457\u4f18\u4e8eReAct\u548cToT\u7b49\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5408\u7406\u6027\u548c\u521b\u65b0\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5bf9\u4e8e\u521b\u9020\u6027\u53d1\u73b0\uff0c\u6709\u539f\u5219\u7684\u5f15\u5bfc\u641c\u7d22\u6bd4\u65e0\u7ea6\u675f\u7684\u81ea\u4e3b\u6027\u66f4\u6709\u6548\uff0c\u4e3aLLM\u6210\u4e3a\u521b\u65b0\u5408\u4f5c\u4f19\u4f34\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.21398", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21398", "abs": "https://arxiv.org/abs/2510.21398", "authors": ["Ravindra Aribowo Tarunokusumo", "Rafael Fernandes Cunha"], "title": "Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement Learning for Mathematical Reasoning", "comment": "Submitted to the European Conference on Artificial Intelligence\n  (ECAI)", "summary": "Test-time scaling methods have seen a rapid increase in popularity for its\ncomputational efficiency and parameter-independent training to improve\nreasoning performance on Large Language Models. One such method is called\nbudget forcing, a decoding intervention strategy which allocates extra compute\nbudget for thinking and elicits the inherent self-correcting behavior of the\nmodel. However, this relies on supervised fine-tuning (SFT) on long-context\nreasoning traces which causes performance degradation on smaller models due to\nverbose responses. For this reason, we offer a framework integrating\nreinforcement learning (RL) to improve token efficiency and boost the\nperformance of a 1.5B model for mathematical reasoning. We demonstrate this\nusing only 1.5K training samples and found that our SFT+RL model performed\nbetter on the GSM8K dataset with varying compute budgets. Our main findings\nshowed an overall higher accuracy while significantly reducing its token usage\nby over 40% compared to the SFT model, revealing how RL can recover the losses\ndue to long-context training and altogether improving performance in\nmathematical reasoning.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60(RL)\u7684\u6846\u67b6\uff0c\u63d0\u53471.5B\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u76d1\u7763\u5fae\u8c03(SFT)\u7684\u65b9\u6cd5\uff0c\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c1140%\u4ee5\u4e0a\u7684token\u4f7f\u7528\u91cf\u3002", "motivation": "\u89e3\u51b3\u9884\u7b97\u5f3a\u5236\u65b9\u6cd5\u4f9d\u8d56\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u8f68\u8ff9\u7684\u76d1\u7763\u5fae\u8c03\u5bfc\u81f4\u5c0f\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u9ad8token\u6548\u7387\u5e76\u6062\u590d\u56e0\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u9020\u6210\u7684\u6027\u80fd\u635f\u5931\u3002", "method": "\u96c6\u6210\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u4f7f\u75281.5K\u8bad\u7ec3\u6837\u672c\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u6765\u4f18\u53161.5B\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "result": "SFT+RL\u6a21\u578b\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5728\u4e0d\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u540c\u65f6token\u4f7f\u7528\u91cf\u76f8\u6bd4SFT\u6a21\u578b\u51cf\u5c11\u8d85\u8fc740%\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u6709\u6548\u6062\u590d\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u9020\u6210\u7684\u635f\u5931\uff0c\u663e\u8457\u63d0\u9ad8\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347token\u6548\u7387\u3002"}}
{"id": "2510.21425", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21425", "abs": "https://arxiv.org/abs/2510.21425", "authors": ["Maneeha Rani", "Bhupesh Kumar Mishra", "Dhavalkumar Thakker"], "title": "Advancing Symbolic Integration in Large Language Models: Beyond Conventional Neurosymbolic AI", "comment": null, "summary": "LLMs have demonstrated highly effective learning, human-like response\ngeneration,and decision-making capabilities in high-risk sectors. However,\nthese models remain black boxes because they struggle to ensure transparency in\nresponses. The literature has explored numerous approaches to address\ntransparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI\napproaches were primarily developed for conventional neural networks and are\nnot well-suited to the unique features of LLMs. Consequently, there is a\nlimited systematic understanding of how symbolic AI can be effectively\nintegrated into LLMs. This paper aims to address this gap by first reviewing\nestablished NeSy AI methods and then proposing a novel taxonomy of symbolic\nintegration in LLMs, along with a roadmap to merge symbolic techniques with\nLLMs. The roadmap introduces a new categorisation framework across four\ndimensions by organising existing literature within these categories. These\ninclude symbolic integration across various stages of LLM, coupling mechanisms,\narchitectural paradigms, as well as algorithmic and application-level\nperspectives. The paper thoroughly identifies current benchmarks, cutting-edge\nadvancements, and critical gaps within the field to propose a roadmap for\nfuture research. By highlighting the latest developments and notable gaps in\nthe literature, it offers practical insights for implementing frameworks for\nsymbolic integration into LLMs to enhance transparency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b26\u53f7AI\u96c6\u6210\u5230LLMs\u7684\u5206\u7c7b\u6846\u67b6\u548c\u8def\u7ebf\u56fe\uff0c\u65e8\u5728\u89e3\u51b3LLMs\u900f\u660e\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u5173\u952e\u9886\u57df\u8868\u73b0\u51fa\u8272\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u73b0\u6709\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\uff0c\u4e0d\u9002\u7528\u4e8eLLMs\u7684\u7279\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u7406\u89e3\u7b26\u53f7AI\u5982\u4f55\u6709\u6548\u96c6\u6210\u5230LLMs\u4e2d\u3002", "method": "\u9996\u5148\u56de\u987e\u5df2\u5efa\u7acb\u7684\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\uff0c\u7136\u540e\u63d0\u51faLLMs\u4e2d\u7b26\u53f7\u96c6\u6210\u7684\u65b0\u5206\u7c7b\u6cd5\uff0c\u5e76\u5236\u5b9a\u5c06\u7b26\u53f7\u6280\u672f\u4e0eLLMs\u878d\u5408\u7684\u8def\u7ebf\u56fe\uff0c\u5305\u62ec\u56db\u4e2a\u7ef4\u5ea6\u7684\u5206\u7c7b\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542bLLM\u4e0d\u540c\u9636\u6bb5\u7b26\u53f7\u96c6\u6210\u3001\u8026\u5408\u673a\u5236\u3001\u67b6\u6784\u8303\u5f0f\u4ee5\u53ca\u7b97\u6cd5\u548c\u5e94\u7528\u5c42\u9762\u89c6\u89d2\u7684\u65b0\u5206\u7c7b\u6846\u67b6\uff0c\u7cfb\u7edf\u6574\u7406\u4e86\u73b0\u6709\u6587\u732e\u3002", "conclusion": "\u901a\u8fc7\u7a81\u51fa\u6700\u65b0\u8fdb\u5c55\u548c\u6587\u732e\u4e2d\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u4e3a\u5728LLMs\u4e2d\u5b9e\u73b0\u7b26\u53f7\u96c6\u6210\u6846\u67b6\u4ee5\u589e\u5f3a\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.21436", "categories": ["cs.AI", "I.4"], "pdf": "https://arxiv.org/pdf/2510.21436", "abs": "https://arxiv.org/abs/2510.21436", "authors": ["Ankur Sinha", "Shobhit Arora", "Dhaval Pujara"], "title": "AutoOpt: A Dataset and a Unified Framework for Automating Optimization Problem Solving", "comment": "NeurIPS 2025, 28 pages, 11 figures, 11 tables", "summary": "This study presents AutoOpt-11k, a unique image dataset of over 11,000\nhandwritten and printed mathematical optimization models corresponding to\nsingle-objective, multi-objective, multi-level, and stochastic optimization\nproblems exhibiting various types of complexities such as non-linearity,\nnon-convexity, non-differentiability, discontinuity, and high-dimensionality.\nThe labels consist of the LaTeX representation for all the images and modeling\nlanguage representation for a subset of images. The dataset is created by 25\nexperts following ethical data creation guidelines and verified in two-phases\nto avoid errors. Further, we develop AutoOpt framework, a machine learning\nbased automated approach for solving optimization problems, where the user just\nneeds to provide an image of the formulation and AutoOpt solves it efficiently\nwithout any further human intervention. AutoOpt framework consists of three\nModules: (i) M1 (Image_to_Text)- a deep learning model performs the\nMathematical Expression Recognition (MER) task to generate the LaTeX code\ncorresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-\na small-scale fine-tuned LLM generates the PYOMO script (optimization modeling\nlanguage) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization\nbased Decomposition (BOBD) method solves the optimization formulation described\nin the PYOMO script. We use AutoOpt-11k dataset for training and testing of\ndeep learning models employed in AutoOpt. The deep learning model for MER task\n(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method\n(M3), which is a hybrid approach, yields better results on complex test\nproblems compared to common approaches, like interior-point algorithm and\ngenetic algorithm.", "AI": {"tldr": "AutoOpt-11k\u662f\u4e00\u4e2a\u5305\u542b11,000\u591a\u4e2a\u624b\u5199\u548c\u6253\u5370\u6570\u5b66\u4f18\u5316\u6a21\u578b\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u914d\u5957\u5f00\u53d1\u4e86AutoOpt\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u81ea\u52a8\u8bc6\u522b\u6570\u5b66\u8868\u8fbe\u5f0f\u5e76\u8f6c\u6362\u4e3a\u4f18\u5316\u5efa\u6a21\u8bed\u8a00\uff0c\u6700\u7ec8\u4f7f\u7528\u53cc\u5c42\u4f18\u5316\u5206\u89e3\u65b9\u6cd5\u6c42\u89e3\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6570\u5b66\u4f18\u5316\u95ee\u9898\u6c42\u89e3\u8fc7\u7a0b\u4e2d\u9700\u8981\u4eba\u5de5\u5efa\u6a21\u548c\u7f16\u7a0b\u7684\u7e41\u7410\u8fc7\u7a0b\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u76f4\u63a5\u4ece\u56fe\u50cf\u81ea\u52a8\u6c42\u89e3\u4f18\u5316\u95ee\u9898\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86AutoOpt\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aM1\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\u751f\u6210LaTeX\u4ee3\u7801\uff1bM2\u4f7f\u7528\u5fae\u8c03\u7684\u5c0f\u578bLLM\u5c06LaTeX\u8f6c\u6362\u4e3aPYOMO\u811a\u672c\uff1bM3\u4f7f\u7528\u53cc\u5c42\u4f18\u5316\u5206\u89e3\u65b9\u6cd5\u6c42\u89e3\u4f18\u5316\u95ee\u9898\u3002", "result": "MER\u6a21\u578b\u5728BLEU\u5f97\u5206\u4e0a\u4f18\u4e8eChatGPT\u3001Gemini\u548cNougat\uff1bBOBD\u65b9\u6cd5\u5728\u590d\u6742\u6d4b\u8bd5\u95ee\u9898\u4e0a\u6bd4\u5185\u70b9\u7b97\u6cd5\u548c\u9057\u4f20\u7b97\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "AutoOpt-11k\u6570\u636e\u96c6\u548cAutoOpt\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u6c42\u89e3\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\u548c\u590d\u6742\u4f18\u5316\u95ee\u9898\u6c42\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.21453", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21453", "abs": "https://arxiv.org/abs/2510.21453", "authors": ["Yuxin Pan", "Zhiguang Cao", "Chengyang Gu", "Liu Liu", "Peilin Zhao", "Yize Chen", "Fangzhen Lin"], "title": "Multi-Task Vehicle Routing Solver via Mixture of Specialized Experts under State-Decomposable MDP", "comment": "Accepted to NeurIPS 2025", "summary": "Existing neural methods for multi-task vehicle routing problems (VRPs)\ntypically learn unified solvers to handle multiple constraints simultaneously.\nHowever, they often underutilize the compositional structure of VRP variants,\neach derivable from a common set of basis VRP variants. This critical oversight\ncauses unified solvers to miss out the potential benefits of basis solvers,\neach specialized for a basis VRP variant. To overcome this limitation, we\npropose a framework that enables unified solvers to perceive the\nshared-component nature across VRP variants by proactively reusing basis\nsolvers, while mitigating the exponential growth of trained neural solvers.\nSpecifically, we introduce a State-Decomposable MDP (SDMDP) that reformulates\nVRPs by expressing the state space as the Cartesian product of basis state\nspaces associated with basis VRP variants. More crucially, this formulation\ninherently yields the optimal basis policy for each basis VRP variant.\nFurthermore, a Latent Space-based SDMDP extension is developed by incorporating\nboth the optimal basis policies and a learnable mixture function to enable the\npolicy reuse in the latent space. Under mild assumptions, this extension\nprovably recovers the optimal unified policy of SDMDP through the mixture\nfunction that computes the state embedding as a mapping from the basis state\nembeddings generated by optimal basis policies. For practical implementation,\nwe introduce the Mixture-of-Specialized-Experts Solver (MoSES), which realizes\nbasis policies through specialized Low-Rank Adaptation (LoRA) experts, and\nimplements the mixture function via an adaptive gating mechanism. Extensive\nexperiments conducted across VRP variants showcase the superiority of MoSES\nover prior methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86MoSES\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001\u53ef\u5206\u89e3MDP\u548c\u6f5c\u5728\u7a7a\u95f4\u6269\u5c55\uff0c\u8ba9\u7edf\u4e00\u6c42\u89e3\u5668\u80fd\u591f\u611f\u77e5VRP\u53d8\u4f53\u7684\u5171\u4eab\u7ec4\u4ef6\u7279\u6027\uff0c\u91cd\u7528\u57fa\u7840\u6c42\u89e3\u5668\uff0c\u907f\u514d\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u5668\u6570\u91cf\u6307\u6570\u589e\u957f\u3002", "motivation": "\u73b0\u6709\u591a\u4efb\u52a1\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u795e\u7ecf\u65b9\u6cd5\u901a\u5e38\u5b66\u4e60\u7edf\u4e00\u6c42\u89e3\u5668\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528VRP\u53d8\u4f53\u7684\u7ec4\u5408\u7ed3\u6784\uff0c\u6bcf\u4e2a\u53d8\u4f53\u90fd\u53ef\u4ee5\u4ece\u4e00\u7ec4\u57fa\u7840VRP\u53d8\u4f53\u6d3e\u751f\u800c\u6765\u3002", "method": "\u5f15\u5165\u72b6\u6001\u53ef\u5206\u89e3MDP\u5c06\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u4e3a\u4e0e\u57fa\u7840VRP\u53d8\u4f53\u76f8\u5173\u7684\u57fa\u7840\u72b6\u6001\u7a7a\u95f4\u7684\u7b1b\u5361\u5c14\u79ef\uff0c\u5f00\u53d1\u6f5c\u5728\u7a7a\u95f4\u6269\u5c55\u7ed3\u5408\u6700\u4f18\u57fa\u7840\u7b56\u7565\u548c\u53ef\u5b66\u4e60\u6df7\u5408\u51fd\u6570\uff0c\u5b9e\u73b0MoSES\u6c42\u89e3\u5668\u4f7f\u7528\u4e13\u7528LoRA\u4e13\u5bb6\u5b9e\u73b0\u57fa\u7840\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2aVRP\u53d8\u4f53\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660eMoSES\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u91cd\u7528\u57fa\u7840\u6c42\u89e3\u5668\u6709\u6548\u89e3\u51b3\u4e86\u7edf\u4e00\u6c42\u89e3\u5668\u672a\u80fd\u5145\u5206\u5229\u7528VRP\u7ec4\u5408\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2aVRP\u53d8\u4f53\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.21524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21524", "abs": "https://arxiv.org/abs/2510.21524", "authors": ["Ilija Lichkovski", "Alexander M\u00fcller", "Mariam Ibrahim", "Tiwai Mhundwa"], "title": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law", "comment": "Accepted at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)", "summary": "Large language models (LLMs) are increasingly deployed as agents in various\ncontexts by providing tools at their disposal. However, LLM agents can exhibit\nunpredictable behaviors, including taking undesirable and/or unsafe actions. In\norder to measure the latent propensity of LLM agents for taking illegal actions\nunder an EU legislative context, we introduce EU-Agent-Bench, a verifiable\nhuman-curated benchmark that evaluates an agent's alignment with EU legal norms\nin situations where benign user inputs could lead to unlawful actions. Our\nbenchmark spans scenarios across several categories, including data protection,\nbias/discrimination, and scientific integrity, with each user request allowing\nfor both compliant and non-compliant execution of the requested actions.\nComparing the model's function calls against a rubric exhaustively supported by\ncitations of the relevant legislature, we evaluate the legal compliance of\nfrontier LLMs, and furthermore investigate the compliance effect of providing\nthe relevant legislative excerpts in the agent's system prompt along with\nexplicit instructions to comply. We release a public preview set for the\nresearch community, while holding out a private test set to prevent data\ncontamination in evaluating upcoming models. We encourage future work extending\nagentic safety benchmarks to different legal jurisdictions and to multi-turn\nand multilingual interactions. We release our code on\n\\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.", "AI": {"tldr": "EU-Agent-Bench\u662f\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u4eba\u5de5\u7b56\u5212\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u6b27\u76df\u7acb\u6cd5\u80cc\u666f\u4e0b\u6267\u884c\u975e\u6cd5\u884c\u4e3a\u7684\u6f5c\u5728\u503e\u5411\uff0c\u6db5\u76d6\u6570\u636e\u4fdd\u62a4\u3001\u504f\u89c1/\u6b67\u89c6\u548c\u79d1\u5b66\u8bda\u4fe1\u7b49\u591a\u4e2a\u7c7b\u522b\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u5404\u79cd\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u5b83\u4eec\u53ef\u80fd\u8868\u73b0\u51fa\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\uff0c\u5305\u62ec\u91c7\u53d6\u4e0d\u826f\u548c/\u6216\u4e0d\u5b89\u5168\u7684\u884c\u52a8\u3002\u9700\u8981\u6d4b\u91cfLLM\u4ee3\u7406\u5728\u6b27\u76df\u7acb\u6cd5\u80cc\u666f\u4e0b\u6267\u884c\u975e\u6cd5\u884c\u4e3a\u7684\u6f5c\u5728\u503e\u5411\u3002", "method": "\u901a\u8fc7\u5c06\u6a21\u578b\u7684\u51fd\u6570\u8c03\u7528\u4e0e\u8be6\u5c3d\u5f15\u7528\u76f8\u5173\u7acb\u6cd5\u7684\u8bc4\u5206\u6807\u51c6\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u524d\u6cbfLLM\u7684\u6cd5\u5f8b\u5408\u89c4\u6027\uff0c\u5e76\u7814\u7a76\u5728\u4ee3\u7406\u7cfb\u7edf\u63d0\u793a\u4e2d\u63d0\u4f9b\u76f8\u5173\u7acb\u6cd5\u6458\u5f55\u548c\u660e\u786e\u5408\u89c4\u6307\u4ee4\u7684\u5408\u89c4\u6548\u679c\u3002", "result": "\u8bc4\u4f30\u4e86\u524d\u6cbfLLM\u7684\u6cd5\u5f8b\u5408\u89c4\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u63d0\u4f9b\u7acb\u6cd5\u6458\u5f55\u5bf9\u5408\u89c4\u6027\u7684\u5f71\u54cd\u3002\u53d1\u5e03\u4e86\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\u7684\u516c\u5171\u9884\u89c8\u96c6\uff0c\u540c\u65f6\u4fdd\u7559\u79c1\u6709\u6d4b\u8bd5\u96c6\u4ee5\u9632\u6b62\u6570\u636e\u6c61\u67d3\u3002", "conclusion": "\u9f13\u52b1\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u4ee3\u7406\u5b89\u5168\u57fa\u51c6\u6269\u5c55\u5230\u4e0d\u540c\u7684\u6cd5\u5f8b\u7ba1\u8f96\u533a\uff0c\u4ee5\u53ca\u591a\u8f6e\u548c\u591a\u8bed\u8a00\u4ea4\u4e92\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2510.21557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21557", "abs": "https://arxiv.org/abs/2510.21557", "authors": ["Hongwei Zhang", "Ji Lu", "Shiqing Jiang", "Chenxiang Zhu", "Li Xie", "Chen Zhong", "Haoran Chen", "Yurui Zhu", "Yongsheng Du", "Yanqin Gao", "Lingjun Huang", "Baoli Wang", "Fang Tan", "Peng Zou"], "title": "Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification and Trustworthy Reasoning with Structured Facts", "comment": null, "summary": "Long-horizon reasoning in LLM-based agents often fails not from generative\nweakness but from insufficient verification of intermediate reasoning. Co-Sight\naddresses this challenge by turning reasoning into a falsifiable and auditable\nprocess through two complementary mechanisms: Conflict-Aware Meta-Verification\n(CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV\nreformulates verification as conflict identification and targeted\nfalsification, allocating computation only to disagreement hotspots among\nexpert agents rather than to full reasoning chains. This bounds verification\ncost to the number of inconsistencies and improves efficiency and reliability.\nTRSF continuously organizes, validates, and synchronizes evidence across agents\nthrough a structured facts module. By maintaining verified, traceable, and\nauditable knowledge, it ensures that all reasoning is grounded in consistent,\nsource-verified information and supports transparent verification throughout\nthe reasoning process. Together, TRSF and CAMV form a closed verification loop,\nwhere TRSF supplies structured facts and CAMV selectively falsifies or\nreinforces them, yielding transparent and trustworthy reasoning. Empirically,\nCo-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last\nExam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies\nconfirm that the synergy between structured factual grounding and\nconflict-aware verification drives these improvements. Co-Sight thus offers a\nscalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code\nis available at\nhttps://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks.", "AI": {"tldr": "Co-Sight\u901a\u8fc7\u51b2\u7a81\u611f\u77e5\u5143\u9a8c\u8bc1\u548c\u53ef\u4fe1\u63a8\u7406\u7ed3\u6784\u5316\u4e8b\u5b9e\u673a\u5236\uff0c\u5c06\u63a8\u7406\u8f6c\u5316\u4e3a\u53ef\u8bc1\u4f2a\u548c\u53ef\u5ba1\u8ba1\u7684\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u957f\u7a0b\u63a8\u7406\u4e2d\u7684\u9a8c\u8bc1\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u957f\u7a0b\u63a8\u7406\u4e2d\u5931\u8d25\u5f80\u5f80\u4e0d\u662f\u56e0\u4e3a\u751f\u6210\u80fd\u529b\u5f31\uff0c\u800c\u662f\u56e0\u4e3a\u4e2d\u95f4\u63a8\u7406\u9a8c\u8bc1\u4e0d\u8db3\u3002\u9700\u8981\u5c06\u63a8\u7406\u8f6c\u5316\u4e3a\u53ef\u8bc1\u4f2a\u548c\u53ef\u5ba1\u8ba1\u7684\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u673a\u5236\uff1a\u51b2\u7a81\u611f\u77e5\u5143\u9a8c\u8bc1\uff08CAMV\uff09\u5c06\u9a8c\u8bc1\u91cd\u6784\u4e3a\u51b2\u7a81\u8bc6\u522b\u548c\u9488\u5bf9\u6027\u8bc1\u4f2a\uff0c\u53ea\u5728\u4e13\u5bb6\u667a\u80fd\u4f53\u95f4\u7684\u4e0d\u4e00\u81f4\u70ed\u70b9\u5206\u914d\u8ba1\u7b97\uff1b\u53ef\u4fe1\u63a8\u7406\u7ed3\u6784\u5316\u4e8b\u5b9e\uff08TRSF\uff09\u901a\u8fc7\u7ed3\u6784\u5316\u4e8b\u5b9e\u6a21\u5757\u6301\u7eed\u7ec4\u7ec7\u3001\u9a8c\u8bc1\u548c\u540c\u6b65\u8bc1\u636e\u3002", "result": "\u5728GAIA\u4e0a\u8fbe\u523084.4%\u7684\u51c6\u786e\u7387\uff0cHumanity's Last Exam\u4e0a\u8fbe\u523035.5%\uff0cChinese-SimpleQA\u4e0a\u8fbe\u523093.8%\u7684\u5f3a\u7ed3\u679c\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u7ed3\u6784\u5316\u4e8b\u5b9e\u57fa\u7840\u548c\u51b2\u7a81\u611f\u77e5\u9a8c\u8bc1\u7684\u534f\u540c\u4f5c\u7528\u9a71\u52a8\u4e86\u8fd9\u4e9b\u6539\u8fdb\u3002", "conclusion": "Co-Sight\u4e3aLLM\u667a\u80fd\u4f53\u4e2d\u7684\u53ef\u9760\u957f\u7a0b\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u5f62\u6210\u5c01\u95ed\u9a8c\u8bc1\u5faa\u73af\u5b9e\u73b0\u900f\u660e\u53ef\u4fe1\u7684\u63a8\u7406\u3002"}}
{"id": "2510.21560", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21560", "abs": "https://arxiv.org/abs/2510.21560", "authors": ["Yuxuan Yang", "Hussein Sibai"], "title": "Learning Neural Control Barrier Functions from Expert Demonstrations using Inverse Constraint Learning", "comment": null, "summary": "Safety is a fundamental requirement for autonomous systems operating in\ncritical domains. Control barrier functions (CBFs) have been used to design\nsafety filters that minimally alter nominal controls for such systems to\nmaintain their safety. Learning neural CBFs has been proposed as a data-driven\nalternative for their computationally expensive optimization-based synthesis.\nHowever, it is often the case that the failure set of states that should be\navoided is non-obvious or hard to specify formally, e.g., tailgating in\nautonomous driving, while a set of expert demonstrations that achieve the task\nand avoid the failure set is easier to generate. We use ICL to train a\nconstraint function that classifies the states of the system under\nconsideration to safe, i.e., belong to a controlled forward invariant set that\nis disjoint from the unspecified failure set, and unsafe ones, i.e., belong to\nthe complement of that set. We then use that function to label a new set of\nsimulated trajectories to train our neural CBF. We empirically evaluate our\napproach in four different environments, demonstrating that it outperforms\nexisting baselines and achieves comparable performance to a neural CBF trained\nwith the same data but annotated with ground-truth safety labels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u795e\u7ecf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u663e\u5f0f\u6307\u5b9a\u5931\u8d25\u72b6\u6001\u96c6\uff0c\u4ec5\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u5c31\u80fd\u5b66\u4e60\u5b89\u5168\u7ea6\u675f\uff0c\u5728\u56db\u4e2a\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5173\u952e\u9886\u57df\u81ea\u4e3b\u7cfb\u7edf\u4e2d\uff0c\u5b89\u5168\u662f\u57fa\u672c\u8981\u6c42\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u663e\u5f0f\u5b9a\u4e49\u5931\u8d25\u72b6\u6001\u96c6\uff0c\u4f46\u8fd9\u5f80\u5f80\u96be\u4ee5\u5f62\u5f0f\u5316\u6307\u5b9a\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8ddf\u8f66\u95ee\u9898\uff09\uff0c\u800c\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u66f4\u5bb9\u6613\u83b7\u53d6\u3002", "method": "\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u7ea6\u675f\u51fd\u6570\u6765\u5206\u7c7b\u7cfb\u7edf\u72b6\u6001\u4e3a\u5b89\u5168\u6216\u4e0d\u5b89\u5168\uff0c\u7136\u540e\u7528\u8be5\u51fd\u6570\u6807\u6ce8\u6a21\u62df\u8f68\u8ff9\u6765\u8bad\u7ec3\u795e\u7ecf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff0c\u907f\u514d\u4e86\u5bf9\u5931\u8d25\u96c6\u7684\u663e\u5f0f\u5b9a\u4e49\u9700\u6c42\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u4e14\u4e0e\u4f7f\u7528\u771f\u5b9e\u5b89\u5168\u6807\u7b7e\u8bad\u7ec3\u7684\u795e\u7ecf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u5b89\u5168\u7ea6\u675f\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u663e\u5f0f\u6307\u5b9a\u5931\u8d25\u72b6\u6001\u96c6\uff0c\u4ec5\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u5c31\u80fd\u6709\u6548\u8bad\u7ec3\u795e\u7ecf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b89\u5168\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.21614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21614", "abs": "https://arxiv.org/abs/2510.21614", "authors": ["Wenyi Wang", "Piotr Pi\u0119kos", "Li Nanbo", "Firas Laakom", "Yimeng Chen", "Mateusz Ostaszewski", "Mingchen Zhuge", "J\u00fcrgen Schmidhuber"], "title": "Huxley-G\u00f6del Machine: Human-Level Coding Agent Development by an Approximation of the Optimal Self-Improving Machine", "comment": null, "summary": "Recent studies operationalize self-improvement through coding agents that\nedit their own codebases. They grow a tree of self-modifications through\nexpansion strategies that favor higher software engineering benchmark\nperformance, assuming that this implies more promising subsequent\nself-modifications. However, we identify a mismatch between the agent's\nself-improvement potential (metaproductivity) and its coding benchmark\nperformance, namely the Metaproductivity-Performance Mismatch. Inspired by\nHuxley's concept of clade, we propose a metric ($\\mathrm{CMP}$) that aggregates\nthe benchmark performances of the descendants of an agent as an indicator of\nits potential for self-improvement. We show that, in our self-improving coding\nagent development setting, access to the true $\\mathrm{CMP}$ is sufficient to\nsimulate how the G\\\"odel Machine would behave under certain assumptions. We\nintroduce the Huxley-G\\\"odel Machine (HGM), which, by estimating $\\mathrm{CMP}$\nand using it as guidance, searches the tree of self-modifications. On SWE-bench\nVerified and Polyglot, HGM outperforms prior self-improving coding agent\ndevelopment methods while using less wall-clock time. Last but not least, HGM\ndemonstrates strong transfer to other coding datasets and large language\nmodels. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and\nevaluated on SWE-bench Lite with GPT-5 achieves human-level performance,\nmatching the best officially checked results of human-engineered coding agents.\nOur code is available at https://github.com/metauto-ai/HGM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Huxley-G\u00f6del Machine (HGM)\u65b9\u6cd5\u6765\u89e3\u51b3\u81ea\u6539\u8fdb\u7f16\u7801\u4ee3\u7406\u4e2d\u5143\u751f\u4ea7\u529b\u4e0e\u6027\u80fd\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f30\u8ba1\u4ee3\u7406\u7684\u81ea\u6211\u6539\u8fdb\u6f5c\u529b\u6765\u6307\u5bfc\u4ee3\u7801\u4fee\u6539\u6811\u7684\u641c\u7d22\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u6539\u8fdb\u7f16\u7801\u4ee3\u7406\u57fa\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u6027\u80fd\u6765\u9009\u62e9\u4ee3\u7801\u4fee\u6539\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u4ee3\u7406\u7684\u81ea\u6211\u6539\u8fdb\u6f5c\u529b\uff08\u5143\u751f\u4ea7\u529b\uff09\u4e0e\u5176\u7f16\u7801\u57fa\u51c6\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u9700\u8981\u66f4\u597d\u7684\u6307\u6807\u6765\u6307\u5bfc\u81ea\u6211\u6539\u8fdb\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faCMP\u6307\u6807\u6765\u805a\u5408\u4ee3\u7406\u540e\u4ee3\u7684\u57fa\u51c6\u6027\u80fd\u4f5c\u4e3a\u5176\u81ea\u6211\u6539\u8fdb\u6f5c\u529b\u7684\u6307\u793a\u5668\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efaHuxley-G\u00f6del Machine (HGM)\u6846\u67b6\uff0c\u901a\u8fc7\u4f30\u8ba1CMP\u6765\u6307\u5bfc\u81ea\u4fee\u6539\u6811\u7684\u641c\u7d22\u8fc7\u7a0b\u3002", "result": "\u5728SWE-bench Verified\u548cPolyglot\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHGM\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u81ea\u6539\u8fdb\u7f16\u7801\u4ee3\u7406\u5f00\u53d1\u65b9\u6cd5\uff0c\u540c\u65f6\u4f7f\u7528\u4e86\u66f4\u5c11\u7684\u5b9e\u9645\u65f6\u95f4\u3002\u4f18\u5316\u540e\u7684\u4ee3\u7406\u5728SWE-bench Lite\u4e0a\u8fbe\u5230\u4e86\u4eba\u7c7b\u6c34\u5e73\u6027\u80fd\u3002", "conclusion": "HGM\u65b9\u6cd5\u901a\u8fc7\u6b63\u786e\u4f30\u8ba1\u4ee3\u7406\u7684\u81ea\u6211\u6539\u8fdb\u6f5c\u529b\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u6307\u5bfc\u81ea\u6539\u8fdb\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u7f16\u7801\u6570\u636e\u96c6\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u4eba\u7c7b\u6c34\u5e73\u7684\u7f16\u7801\u6027\u80fd\u3002"}}
{"id": "2510.21618", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21618", "abs": "https://arxiv.org/abs/2510.21618", "authors": ["Xiaoxi Li", "Wenxiang Jiao", "Jiarui Jin", "Guanting Dong", "Jiajie Jin", "Yinuo Wang", "Hao Wang", "Yutao Zhu", "Ji-Rong Wen", "Yuan Lu", "Zhicheng Dou"], "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets", "comment": null, "summary": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.", "AI": {"tldr": "DeepAgent\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u81ea\u4e3b\u601d\u8003\u3001\u5de5\u5177\u53d1\u73b0\u548c\u884c\u52a8\u6267\u884c\u5728\u5355\u4e00\u8fde\u8d2f\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b8c\u6210\u4efb\u52a1\u3002\u5b83\u5f15\u5165\u81ea\u4e3b\u8bb0\u5fc6\u6298\u53e0\u673a\u5236\u6765\u538b\u7f29\u4ea4\u4e92\u5386\u53f2\uff0c\u5e76\u4f7f\u7528ToolPO\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6765\u9ad8\u6548\u6559\u6388\u901a\u7528\u5de5\u5177\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u6846\u67b6\u901a\u5e38\u9075\u5faa\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u81ea\u4e3b\u548c\u5168\u5c40\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u9700\u8981\u5916\u90e8\u5de5\u5177\u548c\u957f\u65f6\u7a0b\u4ea4\u4e92\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e0a\u4e0b\u6587\u957f\u5ea6\u7206\u70b8\u548c\u4ea4\u4e92\u5386\u53f2\u79ef\u7d2f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDeepAgent\u6846\u67b6\uff0c\u5305\u542b\u81ea\u4e3b\u8bb0\u5fc6\u6298\u53e0\u673a\u5236\uff08\u538b\u7f29\u8fc7\u5f80\u4ea4\u4e92\u4e3a\u7ed3\u6784\u5316\u8bb0\u5fc6\uff09\u548cToolPO\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff08\u5229\u7528LLM\u6a21\u62dfAPI\u5e76\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u4f18\u52bf\u5f52\u56e0\u5206\u914d\u7ec6\u7c92\u5ea6\u4fe1\u7528\uff09\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u901a\u7528\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u548c\u4e0b\u6e38\u5e94\u7528\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDeepAgent\u5728\u6807\u8bb0\u5de5\u5177\u548c\u5f00\u653e\u96c6\u5de5\u5177\u68c0\u7d22\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u671d\u7740\u4e3a\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u6784\u5efa\u66f4\u901a\u7528\u548c\u6709\u80fd\u529b\u667a\u80fd\u4f53\u7684\u65b9\u5411\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002"}}
{"id": "2510.21652", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21652", "abs": "https://arxiv.org/abs/2510.21652", "authors": ["Jonathan Bragg", "Mike D'Arcy", "Nishant Balepur", "Dan Bareket", "Bhavana Dalvi", "Sergey Feldman", "Dany Haddad", "Jena D. Hwang", "Peter Jansen", "Varsha Kishore", "Bodhisattwa Prasad Majumder", "Aakanksha Naik", "Sigal Rahamimov", "Kyle Richardson", "Amanpreet Singh", "Harshit Surana", "Aryeh Tiktinsky", "Rosni Vasu", "Guy Wiener", "Chloe Anastasiades", "Stefan Candra", "Jason Dunkelberger", "Dan Emery", "Rob Evans", "Malachi Hamada", "Regan Huff", "Rodney Kinney", "Matt Latzke", "Jaron Lochner", "Ruben Lozano-Aguilera", "Cecile Nguyen", "Smita Rao", "Amber Tanaka", "Brooke Vlahos", "Peter Clark", "Doug Downey", "Yoav Goldberg", "Ashish Sabharwal", "Daniel S. Weld"], "title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite", "comment": null, "summary": "AI agents hold the potential to revolutionize scientific productivity by\nautomating literature reviews, replicating experiments, analyzing data, and\neven proposing new directions of inquiry; indeed, there are now many such\nagents, ranging from general-purpose \"deep research\" systems to specialized\nscience-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of\nthese agents is critical for progress. Yet existing benchmarks fall short on\nseveral fronts: they (1) fail to provide holistic, product-informed measures of\nreal-world use cases such as science research; (2) lack reproducible agent\ntools necessary for a controlled comparison of core agentic capabilities; (3)\ndo not account for confounding variables such as model cost and tool access;\n(4) do not provide standardized interfaces for quick agent prototyping and\nevaluation; and (5) lack comprehensive baseline agents necessary to identify\ntrue advances. In response, we define principles and tooling for more\nrigorously benchmarking agents. Using these, we present AstaBench, a suite that\nprovides the first holistic measure of agentic ability to perform scientific\nresearch, comprising 2400+ problems spanning the entire scientific discovery\nprocess and multiple scientific domains, and including many problems inspired\nby actual user requests to deployed Asta agents. Our suite comes with the first\nscientific research environment with production-grade search tools that enable\ncontrolled, reproducible evaluation, better accounting for confounders.\nAlongside, we provide a comprehensive suite of nine science-optimized classes\nof Asta agents and numerous baselines. Our extensive evaluation of 57 agents\nacross 22 agent classes reveals several interesting findings, most importantly\nthat despite meaningful progress on certain individual aspects, AI remains far\nfrom solving the challenge of science research assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e86AstaBench\u57fa\u51c6\u5957\u4ef6\uff0c\u7528\u4e8e\u66f4\u4e25\u683c\u5730\u8bc4\u4f30AI\u79d1\u5b66\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u5305\u542b2400+\u4e2a\u6db5\u76d6\u6574\u4e2a\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u91cd\u73b0\u7684\u79d1\u5b66\u7814\u7a76\u73af\u5883\u548c\u57fa\u7ebf\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u591a\u65b9\u9762\u4e0d\u8db3\uff1a\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u79d1\u5b66\u7814\u7a76\u7684\u6574\u4f53\u8861\u91cf\u3001\u7f3a\u5c11\u53ef\u91cd\u73b0\u7684\u4ee3\u7406\u5de5\u5177\u3001\u672a\u8003\u8651\u6a21\u578b\u6210\u672c\u548c\u5de5\u5177\u8bbf\u95ee\u7b49\u6df7\u6742\u53d8\u91cf\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u63a5\u53e3\u3001\u7f3a\u5c11\u5168\u9762\u7684\u57fa\u7ebf\u4ee3\u7406\u3002", "method": "\u5b9a\u4e49\u4e86\u66f4\u4e25\u683c\u7684\u4ee3\u7406\u57fa\u51c6\u539f\u5219\u548c\u5de5\u5177\uff0c\u5f00\u53d1\u4e86AstaBench\u5957\u4ef6\uff0c\u5305\u542b\u591a\u79d1\u5b66\u9886\u57df\u76842400+\u95ee\u9898\uff0c\u63d0\u4f9b\u751f\u4ea7\u7ea7\u641c\u7d22\u5de5\u5177\u7684\u79d1\u5b66\u7814\u7a76\u73af\u5883\uff0c\u4ee5\u53ca9\u7c7b\u79d1\u5b66\u4f18\u5316\u4ee3\u7406\u548c\u591a\u4e2a\u57fa\u7ebf\u3002", "result": "\u5bf957\u4e2a\u4ee3\u7406\u572822\u4e2a\u4ee3\u7406\u7c7b\u522b\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u5c3d\u7ba1\u5728\u67d0\u4e9b\u65b9\u9762\u53d6\u5f97\u6709\u610f\u4e49\u8fdb\u5c55\uff0c\u4f46AI\u5728\u79d1\u5b66\u7814\u7a76\u8f85\u52a9\u65b9\u9762\u4ecd\u8fdc\u672a\u89e3\u51b3\u6311\u6218\u3002", "conclusion": "\u9700\u8981\u66f4\u4e25\u683c\u7684\u57fa\u51c6\u6765\u51c6\u786e\u8bc4\u4f30AI\u79d1\u5b66\u4ee3\u7406\u7684\u80fd\u529b\uff0cAstaBench\u4e3a\u6b64\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u7684\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524dAI\u5728\u79d1\u5b66\u7814\u7a76\u8f85\u52a9\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.21656", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21656", "abs": "https://arxiv.org/abs/2510.21656", "authors": ["Marta Contreiras Silva", "Daniel Faria", "Catia Pesquita"], "title": "CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning", "comment": "32 pages, 5 figures", "summary": "Constructing comprehensive knowledge graphs requires the use of multiple\nontologies in order to fully contextualize data into a domain. Ontology\nmatching finds equivalences between concepts interconnecting ontologies and\ncreating a cohesive semantic layer. While the simple pairwise state of the art\nis well established, simple equivalence mappings cannot provide full semantic\nintegration of related but disjoint ontologies. Complex multi-ontology matching\n(CMOM) aligns one source entity to composite logical expressions of multiple\ntarget entities, establishing more nuanced equivalences and provenance along\nthe ontological hierarchy.\n  We present CMOMgen, the first end-to-end CMOM strategy that generates\ncomplete and semantically sound mappings, without establishing any restrictions\non the number of target ontologies or entities. Retrieval-Augmented Generation\nselects relevant classes to compose the mapping and filters matching reference\nmappings to serve as examples, enhancing In-Context Learning. The strategy was\nevaluated in three biomedical tasks with partial reference alignments. CMOMgen\noutperforms baselines in class selection, demonstrating the impact of having a\ndedicated strategy. Our strategy also achieves a minimum of 63% in F1-score,\noutperforming all baselines and ablated versions in two out of three tasks and\nplacing second in the third. Furthermore, a manual evaluation of non-reference\nmappings showed that 46% of the mappings achieve the maximum score, further\nsubstantiating its ability to construct semantically sound mappings.", "AI": {"tldr": "CMOMgen\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u7684\u590d\u6742\u591a\u672c\u4f53\u5339\u914d\u7b56\u7565\uff0c\u80fd\u591f\u751f\u6210\u5b8c\u6574\u4e14\u8bed\u4e49\u6b63\u786e\u7684\u6620\u5c04\uff0c\u65e0\u9700\u9650\u5236\u76ee\u6807\u672c\u4f53\u6216\u5b9e\u4f53\u7684\u6570\u91cf\u3002", "motivation": "\u6784\u5efa\u5168\u9762\u7684\u77e5\u8bc6\u56fe\u8c31\u9700\u8981\u4f7f\u7528\u591a\u4e2a\u672c\u4f53\u6765\u5c06\u6570\u636e\u5b8c\u5168\u60c5\u5883\u5316\u5230\u7279\u5b9a\u9886\u57df\u3002\u7b80\u5355\u7684\u6210\u5bf9\u7b49\u4ef7\u6620\u5c04\u65e0\u6cd5\u63d0\u4f9b\u76f8\u5173\u4f46\u4e0d\u76f8\u4ea4\u672c\u4f53\u7684\u5b8c\u6574\u8bed\u4e49\u96c6\u6210\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u591a\u672c\u4f53\u5339\u914d\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u9009\u62e9\u76f8\u5173\u7c7b\u522b\u6765\u7ec4\u6210\u6620\u5c04\uff0c\u5e76\u8fc7\u6ee4\u5339\u914d\u7684\u53c2\u8003\u6620\u5c04\u4f5c\u4e3a\u793a\u4f8b\u6765\u589e\u5f3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\uff0cCMOMgen\u5728\u7c7b\u522b\u9009\u62e9\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0cF1\u5206\u6570\u81f3\u5c11\u8fbe\u523063%\uff0c\u5728\u4e24\u4e2a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u548c\u6d88\u878d\u7248\u672c\uff0c\u5728\u7b2c\u4e09\u4e2a\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c\u4e8c\u3002\u624b\u52a8\u8bc4\u4f30\u663e\u793a46%\u7684\u975e\u53c2\u8003\u6620\u5c04\u83b7\u5f97\u6700\u9ad8\u5206\u3002", "conclusion": "CMOMgen\u80fd\u591f\u6784\u5efa\u8bed\u4e49\u6b63\u786e\u7684\u6620\u5c04\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u590d\u6742\u591a\u672c\u4f53\u5339\u914d\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.21679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21679", "abs": "https://arxiv.org/abs/2510.21679", "authors": ["Gaku Morio", "Harri Rowlands", "Dominik Stammbach", "Christopher D. Manning", "Peter Henderson"], "title": "A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection", "comment": "Forthcoming in NeurIPS 2025 Datasets and Benchmarks Track", "summary": "Companies spend large amounts of money on public relations campaigns to\nproject a positive brand image. However, sometimes there is a mismatch between\nwhat they say and what they do. Oil & gas companies, for example, are accused\nof \"greenwashing\" with imagery of climate-friendly initiatives. Understanding\nthe framing, and changes in framing, at scale can help better understand the\ngoals and nature of public relations campaigns. To address this, we introduce a\nbenchmark dataset of expert-annotated video ads obtained from Facebook and\nYouTube. The dataset provides annotations for 13 framing types for more than 50\ncompanies or advocacy groups across 20 countries. Our dataset is especially\ndesigned for the evaluation of vision-language models (VLMs), distinguishing it\nfrom past text-only framing datasets. Baseline experiments show some promising\nresults, while leaving room for improvement for future work: GPT-4.1 can detect\nenvironmental messages with 79% F1 score, while our best model only achieves\n46% F1 score on identifying framing around green innovation. We also identify\nchallenges that VLMs must address, such as implicit framing, handling videos of\nvarious lengths, or implicit cultural backgrounds. Our dataset contributes to\nresearch in multimodal analysis of strategic communication in the energy\nsector.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e13\u5bb6\u6807\u6ce8\u7684\u89c6\u9891\u5e7f\u544a\u6846\u67b6\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u80fd\u6e90\u516c\u53f8\u7684\u7eff\u8272\u521b\u65b0\u6846\u67b6\u8bc6\u522b\u3002", "motivation": "\u4f01\u4e1a\u516c\u5173\u6d3b\u52a8\u5b58\u5728\u8a00\u884c\u4e0d\u4e00\u7684\u95ee\u9898\uff0c\u5982\u77f3\u6cb9\u516c\u53f8\u7684\"\u6f02\u7eff\"\u884c\u4e3a\u3002\u9700\u8981\u5927\u89c4\u6a21\u7406\u89e3\u6846\u67b6\u53ca\u5176\u53d8\u5316\u6765\u8bc6\u522b\u516c\u5173\u6d3b\u52a8\u7684\u771f\u5b9e\u76ee\u6807\u3002", "method": "\u6784\u5efa\u4e86\u4eceFacebook\u548cYouTube\u83b7\u53d6\u7684\u4e13\u5bb6\u6807\u6ce8\u89c6\u9891\u5e7f\u544a\u6570\u636e\u96c6\uff0c\u5305\u542b13\u79cd\u6846\u67b6\u7c7b\u578b\uff0c\u8986\u76d650\u591a\u5bb6\u516c\u53f8\u548c20\u4e2a\u56fd\u5bb6\uff0c\u4e13\u95e8\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u57fa\u7ebf\u5b9e\u9a8c\u663e\u793aGPT-4.1\u80fd\u68c0\u6d4b\u73af\u5883\u4fe1\u606f\uff0cF1\u5206\u657079%\uff0c\u4f46\u6700\u4f73\u6a21\u578b\u5728\u8bc6\u522b\u7eff\u8272\u521b\u65b0\u6846\u67b6\u65b9\u9762\u4ec5\u8fbe\u523046% F1\u5206\u6570\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u80fd\u6e90\u90e8\u95e8\u6218\u7565\u6c9f\u901a\u7684\u591a\u6a21\u6001\u5206\u6790\u7814\u7a76\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9690\u6027\u6846\u67b6\u3001\u4e0d\u540c\u957f\u5ea6\u89c6\u9891\u548c\u6587\u5316\u80cc\u666f\u65b9\u9762\u7684\u6311\u6218\u3002"}}
{"id": "2510.21695", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21695", "abs": "https://arxiv.org/abs/2510.21695", "authors": ["Edward Holmberg", "Elias Ioup", "Mahdi Abdelguerfi"], "title": "A Knowledge-Graph Translation Layer for Mission-Aware Multi-Agent Path Planning in Spatiotemporal Dynamics", "comment": "10 pages, 10 figures, conference submission", "summary": "The coordination of autonomous agents in dynamic environments is hampered by\nthe semantic gap between high-level mission objectives and low-level planner\ninputs. To address this, we introduce a framework centered on a Knowledge Graph\n(KG) that functions as an intelligent translation layer. The KG's two-plane\narchitecture compiles declarative facts into per-agent, mission-aware\n``worldviews\" and physics-aware traversal rules, decoupling mission semantics\nfrom a domain-agnostic planner. This allows complex, coordinated paths to be\nmodified simply by changing facts in the KG. A case study involving Autonomous\nUnderwater Vehicles (AUVs) in the Gulf of Mexico visually demonstrates the\nend-to-end process and quantitatively proves that different declarative\npolicies produce distinct, high-performing outcomes. This work establishes the\nKG not merely as a data repository, but as a powerful, stateful orchestrator\nfor creating adaptive and explainable autonomous systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u4f5c\u4e3a\u667a\u80fd\u7ffb\u8bd1\u5c42\u6765\u5f25\u5408\u9ad8\u5c42\u4efb\u52a1\u76ee\u6807\u4e0e\u4f4e\u5c42\u89c4\u5212\u5668\u8f93\u5165\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u901a\u8fc7\u53cc\u5e73\u9762\u67b6\u6784\u5c06\u58f0\u660e\u6027\u4e8b\u5b9e\u7f16\u8bd1\u4e3a\u4efb\u52a1\u611f\u77e5\u7684\u4e16\u754c\u89c2\u548c\u7269\u7406\u611f\u77e5\u7684\u904d\u5386\u89c4\u5219\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u4e3b\u667a\u80fd\u4f53\u534f\u8c03\u65f6\u9ad8\u5c42\u4efb\u52a1\u76ee\u6807\u4e0e\u4f4e\u5c42\u89c4\u5212\u5668\u8f93\u5165\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4ee5\u77e5\u8bc6\u56fe\u8c31\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5e73\u9762\u67b6\u6784\u5c06\u58f0\u660e\u6027\u4e8b\u5b9e\u7f16\u8bd1\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u611f\u77e5\"\u4e16\u754c\u89c2\"\u548c\u7269\u7406\u611f\u77e5\u7684\u904d\u5386\u89c4\u5219\uff0c\u4f7f\u4efb\u52a1\u8bed\u4e49\u4e0e\u9886\u57df\u65e0\u5173\u7684\u89c4\u5212\u5668\u89e3\u8026\u3002", "result": "\u5728\u58a8\u897f\u54e5\u6e7e\u7684\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u89c6\u89c9\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u8fc7\u7a0b\uff0c\u5e76\u5b9a\u91cf\u8bc1\u660e\u4e0d\u540c\u7684\u58f0\u660e\u6027\u7b56\u7565\u80fd\u4ea7\u751f\u72ec\u7279\u4e14\u9ad8\u6027\u80fd\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u786e\u7acb\u4e86\u77e5\u8bc6\u56fe\u8c31\u4e0d\u4ec5\u662f\u6570\u636e\u5b58\u50a8\u5e93\uff0c\u66f4\u662f\u521b\u5efa\u81ea\u9002\u5e94\u548c\u53ef\u89e3\u91ca\u81ea\u4e3b\u7cfb\u7edf\u7684\u5f3a\u5927\u3001\u6709\u72b6\u6001\u7f16\u6392\u5668\u3002"}}
