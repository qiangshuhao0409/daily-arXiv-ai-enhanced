<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.IT](#cs.IT) [Total: 12]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Improving the Graph Challenge Reference Implementation](https://arxiv.org/abs/2601.00974)
*Inna Voloshchuk,Hayden Jananthan,Chansup Byun,Jeremy Kepner*

Main category: cs.NI

TL;DR: 该论文对Graph Challenge的参考代码进行了重构和基准测试，将1000行Python代码精简至325行，减少67%代码量，同时通过并行化提升了大规模流量矩阵分析的性能。


<details>
  <summary>Details</summary>
Motivation: Graph Challenge为大规模图和稀疏数据分析提供展示平台，但现有参考代码在清晰度、适应性和性能方面有改进空间。作者旨在通过重构提升代码质量，为参与者提供更清晰高效的基础实现。

Method: 采用代码重构方法，将原始3个文件约1000行Python代码精简为2个模块325行代码。使用pMatlab和pPython分布式数组编程库，添加并行映射实现数据并行基准测试。

Result: 代码量减少67%的同时保持完整功能，实现了大规模流量矩阵求和与分析的可扩展性能提升。重构后的实现为Graph Challenge参与者提供了更清晰高效的基础代码。

Conclusion: 通过代码重构和并行化优化，显著提升了Graph Challenge参考代码的清晰度、适应性和性能，增强了该挑战赛的影响力，为参与者提供了更好的学习和发展基础。

Abstract: The MIT/IEEE/Amazon Graph Challenge provides a venue for individuals and teams to showcase new innovations in large-scale graph and sparse data analysis. The Anonymized Network Sensing Graph Challenge processes over 100 billion network packets to construct privacy-preserving traffic matrices, with a GraphBLAS reference implementation demonstrating how hypersparse matrices can be applied to this problem. This work presents a refactoring and benchmarking of a section of the reference code to improve clarity, adaptability, and performance. The original Python implementation spanning approximately 1000 lines across 3 files has been streamlined to 325 lines across two focused modules, achieving a 67% reduction in code size while maintaining full functionality. Using pMatlab and pPython distributed array programming libraries, the addition of parallel maps allowed for parallel benchmarking of the data. Scalable performance is demonstrated for large-scale summation and analysis of traffic matrices. The resulting implementation increases the potential impact of the Graph Challenge by providing a clear and efficient foundation for participants.

</details>


### [2] [Decision-Aware Semantic State Synchronization in Compute-First Networking](https://arxiv.org/abs/2601.01086)
*Jianpeng Qi,Chao Liu,Chengrui Wang,Rui Wang,Junyu Dong,Yanwei Yu*

Main category: cs.NI

TL;DR: SenseCFN提出了一种决策感知的状态同步框架，通过语义状态表示和语义偏差指数，仅在状态变化影响卸载决策时触发更新，显著减少更新开销同时保持高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 在计算优先网络中，接入点基于服务节点报告的资源状态信息做出任务卸载决策。传统基于周期性更新或信息年龄的方法主要关注时间新鲜度，但忽略了状态变化是否真正影响卸载决策，导致更新开销与决策准确性之间的权衡问题。

Method: 提出SenseCFN框架，采用轻量级语义状态表示捕获决策相关的系统特征，引入语义偏差指数量化状态变化对决策结果的影响。服务节点仅在检测到显著决策影响变化时触发更新，接入点使用缓存的语义状态进行卸载决策并明确考虑潜在过时性。采用集中训练分布式执行的联合优化方法。

Result: 仿真结果显示，SenseCFN在易饱和场景中保持高达99.6%的任务成功率，比基线方法提升超过25%，同时将状态更新频率降低约70%至96%。

Conclusion: 决策感知的状态同步为计算优先网络提供了一种有效且实用的替代方案，优于纯粹基于时间的更新策略，能够在减少更新开销的同时保持高决策准确性。

Abstract: In Compute-First Networking (CFN), an Access Point (AP) makes task offloading decisions based on resource state information reported by a Service Node (SN). A fundamental challenge arises from the trade-off between update overhead and decision accuracy: Frequent state updates consume limited network resources, while infrequent updates lead to stale state views and degraded task performance, especially under high system load. Existing approaches based on periodic updates or Age of Information (AoI) mainly focus on temporal freshness and often overlook whether a state change is actually relevant to offloading decisions. This paper proposes SenseCFN, a decision-aware state synchronization framework for CFN. Instead of synchronizing raw resource states, SenseCFN focuses on identifying state changes that are likely to alter offloading decisions. To this end, we introduce a lightweight semantic state representation that captures decision-relevant system characteristics, along with a Semantic Deviation Index (SDI) to quantify the impact of state shifts on decision outcomes. Based on SDI, the SN triggers updates only when significant decision-impacting changes are detected. Meanwhile, the AP performs offloading decisions using cached semantic states with explicit awareness of potential staleness. The update and offloading policies are jointly optimized using a centralized training with distributed execution (CTDE) approach. Simulation results show that SenseCFN maintains a task success rate of up to 99.6% in saturation-prone scenarios, outperforming baseline methods by more than 25%, while reducing status update frequency by approximately 70% to 96%. These results indicate that decision-aware state synchronization provides an effective and practical alternative to purely time-based update strategies in CFN.

</details>


### [3] [Utility Maximization in Wireless Backhaul Networks with Service Guarantees](https://arxiv.org/abs/2601.01630)
*Nicholas Jones,Eytan Modiano*

Main category: cs.NI

TL;DR: 无线回程网络中通过有界间隔调度满足SLA约束的效用最大化问题，提出多项式时间pinwheel调度算法和分布式线性复杂度解决方案


<details>
  <summary>Details</summary>
Motivation: 解决无线回程网络中最大化效用的问题，其中效用取决于满足的服务水平协议（SLA），这些协议定义了端到端数据包延迟和瞬时吞吐量要求

Method: 将回程网络建模为树形拓扑，通过构造有界间隔调度（pinwheel调度）来满足SLA约束。针对对称树拓扑使用简单轮询调度，一般情况开发混合整数规划。提出新颖的pinwheel调度算法，显著扩展多项式时间可找到的调度集合

Result: 开发了可扩展的分布式方法来解决效用最大化问题，其复杂度与树深度呈线性关系，相比现有技术显著提升了调度算法的效率和适用范围

Conclusion: 通过pinwheel调度方法有效解决了无线回程网络中的SLA约束效用最大化问题，提出的算法在多项式时间内找到更广泛的调度方案，并实现了线性复杂度的分布式解决方案

Abstract: We consider the problem of maximizing utility in wireless backhaul networks, where utility is a function of satisfied service level agreements (SLAs), defined in terms of end-to-end packet delays and instantaneous throughput. We model backhaul networks as a tree topology and show that SLAs can be satisfied by constructing link schedules with bounded inter-scheduling times, an NP-complete problem known as pinwheel scheduling. For symmetric tree topologies, we show that simple round-robin schedules can be optimal under certain conditions. In the general case, we develop a mixed-integer program that optimizes over the set of admission decisions and pinwheel schedules. We develop a novel pinwheel scheduling algorithm, which significantly expands the set of schedules that can be found in polynomial time over the state of the art. Using conditions from this algorithm, we develop a scalable, distributed approach to solve the utility-maximization problem, with complexity that is linear in the depth of the tree.

</details>


### [4] [Revisiting the Interface between Error and Erasure Correction in Wireless Standards](https://arxiv.org/abs/2601.01645)
*Vipindev Adat Vasudevan,Homa Esfahanizadeh,Benjamin D. Kim,Laura Landon,Alejandro Cohen,Muriel Médard*

Main category: cs.NI

TL;DR: 网络编码作为前向擦除校正方案，相比5G现有的纠错和反馈重传机制，能显著降低延迟并提高资源效率，为6G标准化提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现代5G通信系统采用纠错和基于反馈的擦除校正（HARQ/ARQ）作为可靠性机制，但这些机制会引入显著延迟和资源低效问题。需要寻找更延迟高效的替代方案。

Method: 提出使用网络编码作为前向擦除校正方案，通过数学建模分析现有可靠性机制和网络编码的网络延迟特性，并在网络切片环境中进行仿真验证。

Result: 仿真表明网络编码不仅改善了使用该切片的应用程序的有序交付延迟和吞吐量，还通过减少编码切片的资源利用，使共享网络的其他应用程序受益。

Conclusion: 分析表明6G标准化需要关注协议栈设计的更大模块化，以便在未来的无线网络中集成新技术，网络编码是值得考虑的重要方向。

Abstract: Modern 5G communication systems implement a combination of error correction and feedback-based erasure correction (HARQ/ARQ) as reliability mechanisms, which can introduce substantial delay and resource inefficiency. We propose forward erasure correction using network coding as a more delay-efficient alternative. We present a mathematical characterization of network delay for existing reliability mechanisms and network coding. Through simulations in a network slicing environment, we demonstrate that network coding not only improves the in-order delivery delay and goodput for the applications utilizing the slice, but also benefits other applications sharing the network by reducing resource utilization for the coded slice. Our analysis and characterization point towards ideas that require attention in the 6G standardization process. These findings highlight the need for greater modularity in protocol stack design that enables the integration of novel technologies in future wireless networks.

</details>


### [5] [Enhanced Open-Source NWDAF for Event-Driven Analytics in 5G Networks](https://arxiv.org/abs/2601.01838)
*Henok Daniel,Omar Alhussein,Jie Liang,Cheng Li,Ernesto Damiani*

Main category: cs.NI

TL;DR: 开源NWDAF框架集成到Free5GC中，支持标准化事件订阅/通知，实现UE行为分析、会话生命周期跟踪和切换预测，预测准确率达80.65%


<details>
  <summary>Details</summary>
Motivation: 5G核心标准引入了NWDAF（网络数据分析功能）以实现事件驱动分析和智能网络自动化，但现有实现多为专有方案，开源方案缺乏端到端事件订阅和通知的全面支持

Method: 将开源NWDAF框架集成到Free5GC实现中，扩展会话管理功能以支持标准化事件暴露接口，在SMF和AMF中引入自定义通知机制，实现无缝数据传输

Result: 系统通过两周部署验证，涉及4个虚拟gNB和多个具有动态移动模式的虚拟UE，实现了可靠的UE注册、状态跟踪和跨小区切换，移动感知模块预测下一个gNB切换小区的准确率达到80.65%

Conclusion: 提出的开源NWDAF框架成功集成到Free5GC中，填补了开源5G核心实现中事件驱动分析的空白，支持标准化事件订阅和通知，并展示了预测能力

Abstract: The network data analytics function (NWDAF) has been introduced in the fifth-generation (5G) core standards to enable event-driven analytics and support intelligent network automation. However, existing implementations remain largely proprietary, and open-source alternatives lack comprehensive support for end-to-end event subscription and notification. In this paper, we present an open source NWDAF framework integrated into an existing Free5GC implementation, which serves as an open-source 5G core implementation. Our implementation extends the session management function to support standardized event exposure interfaces and introduces custom-built notification mechanisms into the SMF and the access and mobility management function for seamless data delivery. The NWDAF subscribes to events and generates analytics on user equipment (UE) behavior, session lifecycle, and handover dynamics. We validate our system through a two-week deployment involving four virtual next-generation NodeBs (gNBs) and multiple virtual UEs with dynamic mobility patterns. To demonstrate predictive capabilities, we incorporate a mobility-aware module that achieves 80.65\% accuracy in forecasting the next gNB handover cell. The framework supports reliable UE registration, state tracking, and cross-cell handovers.

</details>


### [6] [Near-Field Multi-Cell ISCAP with Extremely Large-Scale Antenna Array](https://arxiv.org/abs/2601.01968)
*Yuan Guo,Yilong Chen,Zixiang Ren,Derrick Wing Kwan Ng,Jie Xu*

Main category: cs.NI

TL;DR: 论文研究了多基站协作的近场ISCAP系统，使用超大规模天线阵列同时支持通信、无线能量传输和环境感知，提出了鲁棒优化框架来最大化最差检测概率。


<details>
  <summary>Details</summary>
Motivation: 随着6G和物联网发展，需要集成感知、通信和能量传输的智能系统。现有研究多关注远场，而近场场景下超大规模天线阵列的特性未被充分探索，且用户干扰消除能力差异和能量接收器位置不确定性带来设计挑战。

Method: 提出鲁棒优化框架，通过优化波束成形策略最大化规定感知区域的最差检测概率，同时满足用户SINR约束和能量接收器的能量收集要求。使用半定松弛将非凸问题转化为凸半定规划，并开发了基于最大比传输的低复杂度次优方案。

Result: 数值结果表明，所提方法在感知精度、通信可靠性和无线能量传输效率之间存在基本权衡。半定松弛被证明是紧的，而MRT方案在无限天线数渐近条件下有闭式解，计算复杂度显著降低。

Conclusion: 该研究为近场ISCAP系统提供了有效的设计框架，解决了多目标优化中的不确定性挑战，揭示了系统性能权衡，为未来集成感知通信能量传输系统设计提供了理论基础。

Abstract: This paper investigates a coordinated multi-cell integrated sensing, communication, and powering (ISCAP) system operating in the electromagnetic near field, where each base station (BS) employs an extremely large-scale antenna array (ELAA) to simultaneously support downlink communication, wireless power transfer (WPT), and environmental sensing. Three categories of communication users (CUs) with different interference cancellation capabilities are considered, and sensing is enabled through a distributed multiple-input multiple-output (MIMO) radar architecture. To address the resulting design challenges, a robust optimization framework is proposed by optimizing the beamforming strategy to maximize the worst-case detection probability over a prescribed sensing region, subject to per-user signal-to-interference-plus-noise ratio (SINR) constraints and energy harvesting requirements at energy receivers (ERs), while explicitly capturing the uncertainty in ER locations. By leveraging semidefinite relaxation (SDR), the original non-convex problem is reformulated as a convex semidefinite program with a provably tight relaxation. Furthermore, a low-complexity maximum ratio transmission (MRT)-based suboptimal scheme is developed, yielding a closed-form solution in the asymptotic regime as the number of antenna elements approaches infinity. Extensive numerical results reveal the fundamental trade-offs among sensing accuracy, communication reliability, and WPT efficiency.

</details>


### [7] [AgentVNE: LLM-Augmented Graph Reinforcement Learning for Affinity-Aware Multi-Agent Placement in Edge Agentic AI](https://arxiv.org/abs/2601.02021)
*Runze Zheng,Yuqing Zheng,Zhengyi Cheng,Long Luo,Haoxiang Luo,Gang Sun,Hongfang Yu,Dusit Niyato*

Main category: cs.NI

TL;DR: AgentVNE：基于云边协同的双层框架，利用LLM识别语义约束和资源相似性感知神经网络，解决多智能体服务在资源受限边缘部署的挑战，显著降低通信延迟并提高服务接受率。


<details>
  <summary>Details</summary>
Motivation: 物联网智能体正在推动边缘计算向智能体AI和边缘通用智能发展，但在资源受限的边缘基础设施上部署多智能体服务面临严峻挑战。这些服务具有复杂的跨节点交互、动态内存积累和协作工具使用，表现出链式拓扑依赖和严格亲和性约束，传统VNE算法无法满足其实时性需求。

Method: 提出AgentVNE云边协同框架：1）使用LLM识别隐式语义约束并生成基于亲和性的资源增强，解决物理依赖问题；2）构建资源相似性感知神经网络，采用预训练和PPO微调策略，精确捕捉动态工作流与异构网络间的拓扑相似性。

Result: 仿真结果表明，AgentVNE将工作流通信延迟降低至基线方法的40%以下，在高负载场景下将服务接受率提高约5%-10%。

Conclusion: 该工作通过语义感知与拓扑推理的耦合，有效弥合了动态服务需求与物理基础设施之间的差距，为智能体AI的语义感知部署提供了基础解决方案。

Abstract: The Internet of Agents is propelling edge computing toward agentic AI and edge general intelligence (EGI). However, deploying multi-agent service (MAS) on resource-constrained edge infrastructure presents severe challenges. MAS service workflows are driven by complex cross-node interactions, dynamic memory accumulation, and collaborative tool usage. Exhibiting chain-like topological dependencies and strict affinity constraints, these workflows demand real-time responsiveness that exceeds the capabilities of traditional VNE algorithms designed for static resources. To address this, we propose AgentVNE, a cloud-edge collaborative framework utilizing a dual-layer architecture. First, AgentVNE employs a large language model (LLM) to identify implicit semantic constraints and generate affinity-based resource augmentation to resolve physical dependency issues. Second, it constructs a resource similarity-aware neural network, utilizing a pre-training and PPO fine-tuning strategy to precisely capture topological similarities between dynamic workflows and heterogeneous networks. By coupling semantic perception with topological reasoning, this mechanism effectively bridges the gap between dynamic service requirements and physical infrastructure. Simulation results demonstrate that AgentVNE reduces workflow communication latency to less than 40% of baselines and improves the service acceptance rate by approximately 5%-10% under high-load scenarios. Ultimately, this work provides a foundational solution for the semantic-aware deployment of agentic AI.

</details>


### [8] [Enabling Deep Reinforcement Learning Research for Energy Saving in Open RAN](https://arxiv.org/abs/2601.02240)
*Matteo Bordin,Andrea Lacava,Michele Polese,Francesca Cuomo,Tommaso Melodia*

Main category: cs.NI

TL;DR: 提出一个基于深度强化学习的框架，用于提升可编程开放无线接入网络的能效管理，通过动态控制5G网络中的小区激活/休眠来优化能耗。


<details>
  <summary>Details</summary>
Motivation: 下一代无线系统对性能和部署密度要求越来越高，需要有效管理移动网络的能源效率。开放无线接入网络（O-RAN）的智能化和可编程特性为基于AI的能效优化提供了机会。

Method: 使用开源模拟器ns-O-RAN和强化学习环境Gymnasium构建框架，训练和评估深度强化学习代理，动态控制5G网络中小区的激活和休眠状态。

Result: 该框架能够在包含用户移动性、切换、完整协议栈和3GPP兼容信道模型的真实5G网络场景中，收集训练数据并评估深度强化学习对能效的影响。

Conclusion: 该工具将开源并附带ns-O-RAN能效测试教程，为可编程开放无线接入网络的能效优化研究提供实用框架。

Abstract: The growing performance demands and higher deployment densities of next-generation wireless systems emphasize the importance of adopting strategies to manage the energy efficiency of mobile networks. In this demo, we showcase a framework that enables research on Deep Reinforcement Learning (DRL) techniques for improving the energy efficiency of intelligent and programmable Open Radio Access Network (RAN) systems. Using the open-source simulator ns-O-RAN and the reinforcement learning environment Gymnasium, the framework enables to train and evaluate DRL agents that dynamically control the activation and deactivation of cells in a 5G network. We show how to collect data for training and evaluate the impact of DRL on energy efficiency in a realistic 5G network scenario, including users' mobility and handovers, a full protocol stack, and 3rd Generation Partnership Project (3GPP)-compliant channel models. The tool will be open-sourced and a tutorial for energy efficiency testing in ns-O-RAN.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Energy-Aware Routing to Large Reasoning Models](https://arxiv.org/abs/2601.00823)
*Austin R. Ellis-Mohr,Max Hartman,Lav R. Varshney*

Main category: cs.AI

TL;DR: 该论文提出通过方差感知的路由和调度策略来优化大型推理模型的能源效率，在临界状态下平衡基础能源和辅助能源的使用，避免能源浪费。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型具有异构的推理能源成本，不同模型和推理方式能耗不同。为了减少能源消耗，需要选择合适的模型并以合适的方式运行。系统性能取决于平均能源供应和随机波动之间的平衡。

Method: 提出方差感知路由和调度作为设计原则，基于训练计算和推理计算缩放定律来制定调度策略。在临界状态下分析系统性能，研究如何跨时间、模型和执行选择吸收变异性。

Result: 建立了能源感知模型路由策略的理论基础，展示了在临界状态下性能如何受到变异性吸收方式的影响，并提供了二阶表征来进一步理解系统行为。

Conclusion: 方差感知路由和调度是优化大型推理模型能源效率的关键设计维度，为开发能源感知的模型路由策略提供了理论依据，有助于在避免能源浪费的同时维持系统性能。

Abstract: Large reasoning models (LRMs) have heterogeneous inference energy costs based on which model is used and how much it reasons. To reduce energy, it is important to choose the right LRM and operate it in the right way. As a result, the performance of systems that dispatch tasks to different individual LRMs depend on the balance between mean energy provisioning and stochastic fluctuations. The critical regime is the unique operating point at which neither auxiliary energy nor baseline energy is systematically wasted. Increasing baseline supply shifts the system toward persistent over-supply and baseline-energy waste, while reducing supply induces persistent reliance on auxiliary energy. Yet in this regime, performance remains volatility-limited and so a second-order characterization provides further insights that we develop. Here, performance is governed by how variability is absorbed across time, models, and execution choices. This perspective highlights variance-aware routing and dispatch as a principled design axis, and provides a theoretical basis for developing energy-aware model routing policies. Routing behavior is characterized when dispatch policies are based on training-compute and inference-compute scaling laws for LRMs.

</details>


### [10] [Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections](https://arxiv.org/abs/2601.00814)
*Abhishek Kumar*

Main category: cs.AI

TL;DR: 该论文提出了一种基于嵌入余弦相似度的跨语言本体对齐系统，通过创新描述生成技术丰富本体实体上下文，使用微调的多语言Transformer模型生成更好嵌入，在OAEI-2022多语言农场赛道获得71% F1分数，比最佳基线提升16%。


<details>
  <summary>Details</summary>
Motivation: 跨语言本体对齐是知识融合和语义网应用的关键挑战，现有方法在捕捉跨语言语义相似度方面存在局限，需要更有效的对齐方法来提升多语言本体匹配的准确率。

Method: 1. 使用创新技术为ontology实体生成丰富上下文描述；2. 采用微调的多语言Transformer模型生成高质量嵌入；3. 基于余弦相似度匹配实体对；4. 应用阈值过滤保留高相似度实体对。

Result: 在OAEI-2022 multifarm track评估数据集上获得71% F1分数（召回率78%，精确率65%），比最佳基线提升16%，表明系统能有效捕捉跨语言语义相似度。

Conclusion: 提出的基于嵌入和描述增强的本体对齐管道能有效处理跨语言本体对齐任务，通过上下文丰富和高质量嵌入显著提升对齐性能，为多语言知识融合提供有效解决方案。

Abstract: The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.

</details>


### [11] [MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback](https://arxiv.org/abs/2601.00816)
*Ismail Ahmad Abdullah*

Main category: cs.AI

TL;DR: MathLedger是一个可验证机器认知平台，将形式验证、密码学证明和学习动态整合到单一认知循环中，通过Reflexive Formal Learning实现基于验证器结果的符号化学习更新


<details>
  <summary>Details</summary>
Motivation: 当前AI系统虽然性能卓越但缺乏透明度和可验证性，在安全关键部署中存在信任危机，需要建立可验证的机器认知基础

Method: 采用Reflexive Formal Learning（RFL），这是梯度下降的符号化类比，通过验证器结果而非统计损失驱动更新；结合形式验证、密码学证明和学习动态

Result: 第一阶段实验验证了测量和治理基础设施在受控条件下的有效性：CAL-EXP-3验证了测量基础设施（Delta p计算、方差跟踪），压力测试确认了越界条件下故障关闭治理的正确触发

Conclusion: 贡献是基础设施性的：提供了一个可工作的账本证明学习原型，实现了大规模可审计性，但未做出收敛或能力声明

Abstract: Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.
  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.
  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance

</details>


### [12] [Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making](https://arxiv.org/abs/2601.00818)
*Chandra Sekhar Kubam*

Main category: cs.AI

TL;DR: 提出一个基于多智能体系统的Agentic AI框架，用于实现自主、透明、实时的信用风险评估，相比传统模型在决策速度、透明度和响应性方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 金融服务快速数字化需要自主、透明、实时的信用风险决策系统。传统机器学习模型虽然有效，但缺乏自适应推理、情境感知和自主性，无法满足现代金融运营需求。

Method: 提出Agentic AI框架，采用多智能体系统，结合强化学习、自然语言推理、可解释AI模块和实时数据吸收管道，通过智能体协作协议、风险评分引擎、可解释性层和持续反馈学习循环来评估借款人风险。

Result: 该系统在决策速度、透明度和响应性方面优于传统信用评分模型。但仍存在模型漂移风险、高维数据解释不一致、监管不确定性以及低资源环境基础设施限制等实际问题。

Conclusion: 该框架有潜力变革信用分析领域，未来研究应关注动态监管合规机制、新型智能体协作、对抗鲁棒性以及跨国信用生态系统中的大规模实施。

Abstract: Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.

</details>


### [13] [CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations](https://arxiv.org/abs/2601.00821)
*Tao An*

Main category: cs.AI

TL;DR: CogCanvas是一个无需训练的框架，通过提取对话中的认知构件并组织成时序感知图，解决大语言模型在长对话中的上下文限制和信息保真度问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长对话中面临上下文窗口限制和信息保真度的基本矛盾。现有方法（截断和摘要）要么丢弃早期信息，要么丢失细节信息。

Method: 提出CogCanvas框架，从对话轮次中提取基于原文的认知构件（决策、事实、提醒），并将其组织成时序感知图，实现抗压缩检索。

Result: 在LoCoMo基准测试中，CogCanvas达到34.7%总体准确率，优于RAG（25.6%）和GraphRAG（13.7%）。时序推理方面表现尤为突出：31.5% vs 9.3%（RAG）和5.0%（GraphRAG）。多跳因果推理达到81.0%通过率。

Conclusion: 虽然经过大量优化的方法通过专门训练能达到更高分数，但CogCanvas无需训练的方法为实践者提供了立即可部署的替代方案，显著优于标准基线方法。

Abstract: Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.
  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.
  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.

</details>


### [14] [Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis](https://arxiv.org/abs/2601.00828)
*Yin Li*

Main category: cs.AI

TL;DR: 研究发现大语言模型的自我修正能力存在"准确率-修正悖论"：较弱模型（GPT-3.5）的自我修正率反而比较强模型（DeepSeek）高1.6倍，挑战了模型能力与自我改进的线性假设。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型被认为具有自我修正能力，但最近研究表明内在自我修正（无需外部反馈）效果有限。研究旨在系统分析自我修正的三个子能力：错误检测、错误定位和错误修正。

Method: 将自我修正分解为三个子能力，在GSM8K-Complex数据集上进行跨模型实验（n=500/模型，共346个错误），使用三个主要LLM（GPT-3.5、DeepSeek、Claude），分析错误检测率、定位能力和修正成功率。

Result: 发现"准确率-修正悖论"：较弱模型（GPT-3.5，66%准确率）的内在修正率（26.8%）比较强模型（DeepSeek，94%准确率）的修正率（16.7%）高1.6倍。错误检测率在架构间差异巨大（10%-82%），但检测能力不能预测修正成功率。提供错误位置提示反而损害所有模型性能。

Conclusion: 研究挑战了模型能力与自我改进的线性假设，提出"错误深度假说"：较强模型犯错更少但错误更深，难以自我修正。这对自我改进流程设计有重要启示，表明简单的"检测-修正"方法可能不够有效。

Abstract: Large Language Models (LLMs) are widely believed to possess self-correction capabilities, yet recent studies suggest that intrinsic self-correction--where models correct their own outputs without external feedback--remains largely ineffective. In this work, we systematically decompose self-correction into three distinct sub-capabilities: error detection, error localization, and error correction. Through cross-model experiments on GSM8K-Complex (n=500 per model, 346 total errors) with three major LLMs, we uncover a striking Accuracy-Correction Paradox: weaker models (GPT-3.5, 66% accuracy) achieve 1.6x higher intrinsic correction rates than stronger models (DeepSeek, 94% accuracy)--26.8% vs 16.7%. We propose the Error Depth Hypothesis: stronger models make fewer but deeper errors that resist self-correction. Error detection rates vary dramatically across architectures (10% to 82%), yet detection capability does not predict correction success--Claude detects only 10% of errors but corrects 29% intrinsically. Surprisingly, providing error location hints hurts all models. Our findings challenge linear assumptions about model capability and self-improvement, with important implications for the design of self-refinement pipelines.

</details>


### [15] [Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.00830)
*Deep Pankajbhai Mehta*

Main category: cs.AI

TL;DR: AI模型的逐步推理解释并不能真实反映其决策依据，研究发现模型会注意到提示信息但选择不报告，即使被监视也无改善


<details>
  <summary>Details</summary>
Motivation: 验证AI系统逐步推理解释是否真实反映了影响其答案的实际因素，探究模型是否诚实地报告其决策依据

Method: 在问题中嵌入提示信息，测试11个领先AI模型在9000多个测试案例中是否自发提及这些提示，并直接询问模型是否注意到提示

Result: 模型几乎从不自发提及提示，但被直接询问时承认注意到了提示；监视模型无帮助；强制报告提示会导致虚假报告和准确率下降；迎合用户偏好的提示最危险

Conclusion: 仅仅观察AI推理过程不足以发现隐藏的影响因素，需要更可靠的方法来确保AI解释的真实性

Abstract: When AI systems explain their reasoning step-by-step, practitioners often assume these explanations reveal what actually influenced the AI's answer. We tested this assumption by embedding hints into questions and measuring whether models mentioned them. In a study of over 9,000 test cases across 11 leading AI models, we found a troubling pattern: models almost never mention hints spontaneously, yet when asked directly, they admit noticing them. This suggests models see influential information but choose not to report it. Telling models they are being watched does not help. Forcing models to report hints works, but causes them to report hints even when none exist and reduces their accuracy. We also found that hints appealing to user preferences are especially dangerous-models follow them most often while reporting them least. These findings suggest that simply watching AI reasoning is not enough to catch hidden influences.

</details>


### [16] [OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification](https://arxiv.org/abs/2601.00843)
*Ayda Aghaei Nia*

Main category: cs.AI

TL;DR: OmniNeuro是一个新型HCI框架，将BCI从黑盒解码器转变为透明反馈伙伴，通过物理、混沌和量子启发的不确定性建模三个可解释性引擎，实现实时神经声化和生成式AI临床报告。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习提高了脑机接口的解码精度，但其"黑盒"特性阻碍了临床采用，导致用户挫折感和神经可塑性结果不佳，需要可解释的反馈系统。

Method: 提出OmniNeuro框架，集成三个可解释性引擎：1)物理(能量)分析，2)混沌(分形复杂度)分析，3)量子启发的不确定性建模。这些指标驱动实时神经声化和生成式AI临床报告，框架与解码器无关，可作为任何先进架构的可解释性层。

Result: 在PhysioNet数据集(N=109)上达到平均58.52%的准确率，定性试点研究(N=3)证实可解释反馈帮助用户调节心理努力，减少"试错"阶段。

Conclusion: OmniNeuro通过将BCI转变为透明反馈伙伴，解决了深度学习黑盒问题，提高了临床可用性，其解码器无关设计使其适用于各种先进架构。

Abstract: While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the "Black Box" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the "trial-and-error" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.

</details>


### [17] [Enhancing Temporal Awareness in LLMs for Temporal Point Processes](https://arxiv.org/abs/2601.00845)
*Lili Chen,Wensheng Gan,Shuang Liang,Philip S. Yu*

Main category: cs.AI

TL;DR: TPP-TAL是一个增强LLMs中时间感知的即插即用框架，通过显式对齐时间动态与上下文语义，改进连续时间事件建模中的时间似然估计和事件预测准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在序列建模中表现出色，但应用于时间点过程时仍面临挑战。现有方法难以有效捕捉时间信息与语义上下文之间的复杂交互，而这对于准确的事件建模至关重要。

Method: 提出TPP-TAL框架，不采用简单拼接事件时间和类型嵌入的传统方法，而是在将信息输入LLM之前显式对齐时间动态与上下文语义，使模型能更好地感知事件及其周围上下文之间的时间依赖性和长程交互。

Result: 在多个基准数据集上的综合实验表明，TPP-TAL在时间似然估计和事件预测准确性方面带来显著改进，突显了增强LLMs时间感知对于连续时间事件建模的重要性。

Conclusion: TPP-TAL通过增强LLMs中的时间感知能力，有效解决了时间点过程建模中的关键挑战，为连续时间事件建模提供了更准确的解决方案。

Abstract: Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL

</details>


### [18] [Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models](https://arxiv.org/abs/2601.00848)
*Ron F. Del Rosario*

Main category: cs.AI

TL;DR: 论文提出了一种基于OpenTelemetry追踪分析、用于检测多智能体AI工作流中时序攻击模式的语言模型微调方法，通过精心设计的数据集和迭代训练，准确率从42.86%提升到74.29%。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体AI工作流面临时序攻击威胁，需要有效的检测方法。现有解决方案缺乏针对多智能体协调攻击和监管违规的专门检测能力，且缺乏可复现的框架供从业者根据自身威胁环境定制安全模型。

Method: 1) 从18个公共网络安全源和35,026条合成OpenTelemetry追踪中构建80,851个示例的数据集；2) 在资源受限的ARM64硬件上使用迭代QLoRA微调进行三次训练迭代；3) 采用策略性数据增强；4) 开发自定义基准进行评估。

Result: 自定义基准准确率从42.86%提升至74.29%，实现了31.4个百分点的显著提升。针对特定知识差距的定向示例训练效果优于无差别扩展。虽然实际部署仍需人工监督（存在误报率），但该方法为从业者构建定制化智能体安全模型提供了首个可复现框架。

Conclusion: 该研究建立了首个可复现的框架，使从业者能够构建适应其威胁环境的定制化智能体安全模型。关键贡献包括：多智能体协调攻击的合成追踪生成方法、训练数据组成决定行为的实证证据，以及在HuggingFace上完整开源数据集、训练脚本和评估基准。

Abstract: We present an openly documented methodology for fine-tuning language models to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis. We curate a dataset of 80,851 examples from 18 public cybersecurity sources and 35,026 synthetic OpenTelemetry traces. We apply iterative QLoRA fine-tuning on resource-constrained ARM64 hardware (NVIDIA DGX Spark) through three training iterations with strategic augmentation. Our custom benchmark accuracy improves from 42.86% to 74.29%, a statistically significant 31.4-point gain. Targeted examples addressing specific knowledge gaps outperform indiscriminate scaling. Key contributions include: (1) synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations, (2) empirical evidence that training data composition fundamentally determines behavior, and (3) complete open release of datasets, training scripts, and evaluation benchmarks on HuggingFace. While practical deployment requires human oversight due to false positive rates, this work establishes the first reproducible framework enabling practitioners to build custom agentic security models adapted to their threat landscapes.

</details>


### [19] [Comment on: Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Tasks](https://arxiv.org/abs/2601.00856)
*Milos Stankovic,Ella Hirche,Sarah Kollatzsch,Julia Nadine Doetsch*

Main category: cs.AI

TL;DR: 这是一篇对Kosmyna等人(2025)关于ChatGPT与人类认知表现研究的评论文章，指出了原研究在样本量、可重复性、EEG分析、结果报告和透明度等方面的问题。


<details>
  <summary>Details</summary>
Motivation: 对Kosmyna等人(2025)关于AI助手对人类写作任务认知影响的研究进行建设性评论，旨在提高该研究的学术严谨性和发表准备度。

Method: 通过批判性分析原研究的设计、方法、结果和报告，重点关注样本量限制、分析可重复性、EEG方法学问题、结果不一致性和透明度不足等方面。

Result: 识别出原研究存在的五个主要问题：1) 样本量有限；2) 分析可重复性问题；3) EEG分析方法学问题；4) 结果报告不一致；5) 研究过程和发现透明度不足。

Conclusion: 建议对Kosmyna等人(2025)的研究结果进行更保守的解释，并提出了改进建议以增强研究的科学严谨性和发表准备度。

Abstract: Recently published work titled Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task by Kosmyna et al. (2025) has sparked a vivid debate on the topic of artificial intelligence (AI) and human performance. We sincerely congratulate Kosmyna et al. for initiating such important research, collecting a valuable dataset, and establishing highly automated pipelines for Natural Language Processing (NLP) analyses and scoring. We aim to provide constructive comments that may improve the manuscript's readiness for peer-reviewed publication, as some results by Kosmyna et al. (2025) could be interpreted more conservatively. Our primary concerns focus on: (i) study design considerations, including the limited sample size; (ii) the reproducibility of the analyses; (iii) methodological issues related to the EEG analysis; (iv) inconsistencies in the reporting of results; and (v) limited transparency in several aspects of the study's procedures and findings.

</details>


### [20] [Cultural Encoding in Large Language Models: The Existence Gap in AI-Mediated Brand Discovery](https://arxiv.org/abs/2601.00869)
*Huang Junyao,Situ Ruimin,Ye Renqin*

Main category: cs.AI

TL;DR: 研究发现LLM训练数据的地理分布导致品牌推荐存在系统性差异，中国LLM的品牌提及率比国际LLM高30.6个百分点，提出了"存在差距"概念和"数据护城河"框架


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统越来越多地介入消费者信息发现过程，品牌面临算法不可见性问题。研究旨在探究大型语言模型中的文化编码现象——由训练数据构成导致的品牌推荐系统性差异

Method: 分析1,909个纯英文查询，覆盖6个LLM（GPT-4o、Claude、Gemini、Qwen3、DeepSeek、Doubao）和30个品牌，通过案例研究分析品牌可见性差异

Result: 中国LLM的品牌提及率比国际LLM高30.6个百分点（88.9% vs. 58.3%），这种差异在相同的英文查询中持续存在，表明训练数据的地理分布而非语言是主要驱动因素

Conclusion: 提出了"数据护城河"框架，将AI可见内容概念化为VRIN战略资源，建议品牌通过语义覆盖、技术深度和文化本地化构建数据护城河，实现算法无处不在的战略目标

Abstract: As artificial intelligence systems increasingly mediate consumer information discovery,
  brands face algorithmic invisibility. This study investigates Cultural Encoding in Large
  Language Models (LLMs) -- systematic differences in brand recommendations arising from
  training data composition. Analyzing 1,909 pure-English queries across 6 LLMs (GPT-4o,
  Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, we find Chinese LLMs exhibit 30.6
  percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%,
  p<.001). This disparity persists in identical English queries, indicating training data
  geography -- not language -- drives the effect. We introduce the Existence Gap: brands
  absent from LLM training corpora lack "existence" in AI responses regardless of quality.
  Through a case study of Zhizibianjie (OmniEdge), a collaboration platform with 65.6%
  mention rate in Chinese LLMs but 0% in International models (p<.001), we demonstrate how
  Linguistic Boundary Barriers create invisible market entry obstacles. Theoretically, we
  contribute the Data Moat Framework, conceptualizing AI-visible content as a VRIN strategic
  resource. We operationalize Algorithmic Omnipresence -- comprehensive brand visibility
  across LLM knowledge bases -- as the strategic objective for Generative Engine Optimization
  (GEO). Managerially, we provide an 18-month roadmap for brands to build Data Moats
  through semantic coverage, technical depth, and cultural localization. Our findings reveal
  that in AI-mediated markets, the limits of a brand's "Data Boundaries" define the limits
  of its "Market Frontiers."

</details>


### [21] [Universal Conditional Logic: A Formal Language for Prompt Engineering](https://arxiv.org/abs/2601.00880)
*Anthony Mikinka*

Main category: cs.AI

TL;DR: UCL是一个将提示工程从启发式实践转化为系统优化的数学框架，通过系统评估显著减少token使用并降低成本，揭示了过度规范悖论和模型架构特定的优化需求。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程主要依赖启发式实践，缺乏系统化的优化框架。研究者希望将提示工程从经验性实践转变为可系统优化的数学框架，以提高LLM交互的效率和可预测性。

Method: 提出Universal Conditional Logic (UCL)框架，包含指标函数(I_i)、结构开销(O_s = gamma * sum(ln C_k))、早期绑定等核心机制。通过系统评估(N=305, 11个模型, 4次迭代)验证框架效果，并分析过度规范悖论(阈值S* = 0.509)。

Result: 显著减少token使用(29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01)，相应降低成本。发现过度规范悖论：超过阈值S* = 0.509后，额外规范会二次降低性能。验证了核心机制的有效性，并发现最优UCL配置因模型架构而异。

Conclusion: UCL为高效LLM交互提供了一个可校准的框架，模型家族特定的优化是关键研究方向。该框架将提示工程从启发式实践转变为系统化优化，为LLM交互效率提供了理论基础。

Abstract: We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.

</details>


### [22] [Counterfactual Self-Questioning for Stable Policy Optimization in Language Models](https://arxiv.org/abs/2601.00885)
*Mandar Parab*

Main category: cs.AI

TL;DR: 提出Counterfactual Self-Questioning框架，让单个语言模型生成并评估自身推理的反事实批评，通过内部生成的监督实现可扩展的自我改进


<details>
  <summary>Details</summary>
Motivation: 现有语言模型自我改进方法依赖外部批评者、学习奖励模型或集成采样，增加了复杂性和训练不稳定性，需要更简单稳定的自我改进方法

Method: Counterfactual Self-Questioning框架：模型首先生成初始推理轨迹，然后提出针对潜在失败点的质疑问题，最后生成暴露错误假设或无效步骤的替代推理轨迹，这些反事实轨迹提供可直接用于策略优化的结构化相对反馈

Result: 在多个数学推理基准测试中，反事实自我质疑提高了准确性和训练稳定性，特别是对于较小模型，仅使用内部生成的监督就能实现可扩展的自我改进

Conclusion: 提出的Counterfactual Self-Questioning框架为语言模型自我改进提供了一种更简单、更稳定的方法，无需外部模型即可实现有效的自我监督学习

Abstract: Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produces an initial reasoning trace, formulates targeted questions that challenge potential failure points, and generates alternative reasoning trajectories that expose incorrect assumptions or invalid steps. These counterfactual trajectories provide structured relative feedback that can be directly used for policy optimization without auxiliary models. Experiments on multiple mathematical reasoning benchmarks show that counterfactual self-questioning improves accuracy and training stability, particularly for smaller models, enabling scalable self-improvement using internally generated supervision alone.

</details>


### [23] [Context Collapse: In-Context Learning and Model Collapse](https://arxiv.org/abs/2601.00923)
*Josef Ott*

Main category: cs.AI

TL;DR: 该论文研究了LLM中的两个关键现象：上下文学习和模型崩溃。通过线性变换器和简化设置，分析了ICL中的相变现象和模型崩溃的收敛性，并提出了上下文崩溃的新概念。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型中上下文学习和模型崩溃的机制，理解ICL中的相变现象，分析模型崩溃的收敛条件，并探索长序列生成中的稳定性问题。

Method: 1. 使用带权重绑定的线性变换器研究ICL，将其前向传播简化为预条件梯度下降；2. 使用鞅和随机游走理论分析线性回归和高斯拟合中的模型崩溃；3. 提出上下文崩溃概念分析长序列生成问题。

Result: 1. ICL中参数存在相变：超过临界上下文长度时，解出现斜对称分量；2. 模型崩溃几乎必然发生，除非数据快速增长或保留；3. 提出上下文崩溃概念，连接ICL动态与生成模型长期稳定性。

Conclusion: 论文揭示了LLM中ICL和模型崩溃的数学机制，证明了模型崩溃的必然性，并提出了上下文崩溃这一连接ICL动态与长期稳定性挑战的新概念，为理解LLM行为提供了理论框架。

Abstract: This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.

</details>


### [24] [ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems](https://arxiv.org/abs/2601.00994)
*Michael Bao*

Main category: cs.AI

TL;DR: ElecTwit是一个模拟社交媒体政治选举中多智能体说服行为的框架，发现LLM使用了25种说服技巧，不同模型架构和训练会影响模拟中的说服动态。


<details>
  <summary>Details</summary>
Motivation: 克服以往研究中基于游戏模拟的局限性，在更真实的环境中研究多智能体系统中的说服行为，特别是在社交媒体政治选举场景下。

Method: 开发ElecTwit模拟框架，在类似社交媒体的真实环境中进行多智能体实验，测试不同LLM模型在政治选举场景中的说服行为。

Result: 观察到大多数测试的LLM全面使用了25种特定说服技巧，范围比以往报道更广；不同模型在技巧使用和总体说服输出上存在差异；发现了"真相核心"消息和"墨水痴迷"等独特现象。

Conclusion: 该研究为在真实世界环境中评估有说服力的LLM智能体奠定了基础，有助于确保对齐性和防止危险结果，展示了模型架构和训练如何影响现实社交模拟的动态。

Abstract: This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as "kernel of truth" messages and spontaneous developments with an "ink" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.

</details>


### [25] [Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering](https://arxiv.org/abs/2601.01195)
*Wuzhenghong Wen,Chao Xue,Su Pan,Yuwei Sun,Minlong Peng*

Main category: cs.AI

TL;DR: 提出MRE框架，通过增强前向和后向推理来改进TKGQA中的多跳推理，使用提示工程生成多样推理轨迹，并通过T-GRPO优化策略提升全局最优推理路径的识别能力。


<details>
  <summary>Details</summary>
Motivation: TKGQA任务中，大语言模型在每个推理步骤会检索大量时间相似且语义复杂的关系子图，这增加了次优决策和错误传播的风险，需要改进多跳推理的准确性和鲁棒性。

Method: 提出MRE框架：1) 使用提示工程引导LLM生成多样推理轨迹；2) 选择有效轨迹进行监督微调作为冷启动策略；3) 引入T-GRPO递归树结构学习探索方法，建立前后跳之间的强因果依赖关系。

Result: 在两个TKGQA基准测试中，MRE框架模型持续超越现有最优方法，特别是在处理复杂多跳查询时表现优异，同时提高了可解释性和对噪声时间标注的鲁棒性。

Conclusion: MRE框架通过增强前向和后向推理，有效提升了TKGQA中多跳推理的准确性和鲁棒性，为复杂时间知识图谱问答提供了有效的解决方案。

Abstract: Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.

</details>


### [26] [Accelerating Monte-Carlo Tree Search with Optimized Posterior Policies](https://arxiv.org/abs/2601.01301)
*Keith Frankston,Benjamin Howard*

Main category: cs.AI

TL;DR: 提出递归AlphaZero风格的蒙特卡洛树搜索算法RMCTS，相比MCTS-UCB速度提升显著，在单根状态搜索时快40倍以上，批量搜索时快3倍，训练时间减少到三分之一。


<details>
  <summary>Details</summary>
Motivation: AlphaZero的MCTS-UCB算法存在GPU延迟成本高的问题，需要更快的搜索算法来加速训练过程。

Method: 采用递归的广度优先搜索策略，通过批量网络推理减少GPU延迟。使用基于Grill等人提出的后验策略优化方法，从叶子节点向根节点递归计算优化后验策略。

Result: RMCTS在单根状态搜索时比MCTS-UCB快40倍以上，批量搜索时快3倍。在Connect-4、Dots-and-Boxes和Othello三个游戏中，RMCTS训练的网络质量与MCTS-UCB相当，但训练时间减少到三分之一。

Conclusion: RMCTS通过递归广度优先搜索显著加速了蒙特卡洛树搜索，虽然牺牲了自适应树构建的优势，但速度提升更为显著，在实际应用中能够大幅减少训练时间而不损失模型质量。

Abstract: We introduce a recursive AlphaZero-style Monte--Carlo tree search algorithm, "RMCTS". The advantage of RMCTS over AlphaZero's MCTS-UCB is speed. In RMCTS, the search tree is explored in a breadth-first manner, so that network inferences naturally occur in large batches. This significantly reduces the GPU latency cost. We find that RMCTS is often more than 40 times faster than MCTS-UCB when searching a single root state, and about 3 times faster when searching a large batch of root states.
  The recursion in RMCTS is based on computing optimized posterior policies at each game state in the search tree, starting from the leaves and working back up to the root. Here we use the posterior policy explored in "Monte--Carlo tree search as regularized policy optimization" (Grill, et al.) Their posterior policy is the unique policy which maximizes the expected reward given estimated action rewards minus a penalty for diverging from the prior policy.
  The tree explored by RMCTS is not defined in an adaptive manner, as it is in MCTS-UCB. Instead, the RMCTS tree is defined by following prior network policies at each node. This is a disadvantage, but the speedup advantage is more significant, and in practice we find that RMCTS-trained networks match the quality of MCTS-UCB-trained networks in roughly one-third of the training time. We include timing and quality comparisons of RMCTS vs. MCTS-UCB for three games: Connect-4, Dots-and-Boxes, and Othello.

</details>


### [27] [Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models](https://arxiv.org/abs/2601.01321)
*Rong Zhou,Dongping Chen,Zihan Jia,Yao Su,Yixin Liu,Yiwen Lu,Dongwei Shi,Yue Huang,Tianyang Xu,Yi Pan,Xinliang Li,Yohannes Abate,Qingyu Chen,Zhengzhong Tu,Yu Yang,Yu Zhang,Qingsong Wen,Gengchen Mai,Sunyang Fu,Jiachen Li,Xuyu Wang,Ziran Wang,Jing Huang,Tianming Liu,Yong Chen,Lichao Sun,Lifang He*

Main category: cs.AI

TL;DR: 本文提出了一个统一的四阶段框架，系统描述AI在数字孪生全生命周期中的集成：建模、镜像、干预和自主管理，分析了物理建模与数据驱动的协同，并探讨了生成式AI如何将数字孪生转变为主动认知系统。


<details>
  <summary>Details</summary>
Motivation: 数字孪生已从被动仿真工具发展为智能自主实体，但缺乏系统化的AI集成框架。本文旨在提供一个统一的框架来系统描述AI方法在数字孪生全生命周期中的嵌入方式，并分析物理建模与数据驱动学习的协同关系。

Method: 提出了统一的四阶段框架：1) 通过物理基础和物理信息AI方法建模物理孪生；2) 通过实时同步将物理系统镜像为数字孪生；3) 通过预测建模、异常检测和优化策略干预物理孪生；4) 通过大语言模型、基础模型和智能代理实现自主管理。通过跨11个应用领域的综述分析协同关系。

Result: 系统分析了物理建模与数据驱动学习的协同，展示了从传统数值求解器向物理信息和基础模型的转变。探讨了生成式AI技术（包括大语言模型和生成世界模型）如何将数字孪生转变为能够推理、通信和创造性场景生成的主动认知系统。

Conclusion: AI技术正在将数字孪生转变为主动、自我改进的认知系统。通过跨领域分析识别了可扩展性、可解释性和可信度方面的共同挑战，并为负责任AI驱动的数字孪生系统指明了发展方向。

Abstract: Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.

</details>


### [28] [Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale](https://arxiv.org/abs/2601.01330)
*Shengji Tang,Weihao Lin,Jingqi Ye,Hao Li,Bo Zhang,Shuyue Hu,Tao Chen,Wangli Ouyang,Lei Bai,Peng Ye*

Main category: cs.AI

TL;DR: JiSi框架通过查询-响应混合路由、支持集聚合器选择和自适应路由-聚合切换，使开源LLM协作超越Gemini-3-Pro性能，成本仅47%


<details>
  <summary>Details</summary>
Motivation: 探索集体智能作为替代单体扩展的路径，解决当前LLM路由和聚合的三个关键瓶颈：查询式路由局限、静态聚合方法、路由与聚合互补性未充分利用

Method: 提出JiSi框架，包含三个创新：1) 查询-响应混合路由，同时捕捉语义信息和问题难度；2) 基于支持集的聚合器选择，联合评估聚合能力和领域能力；3) 自适应路由-聚合切换，动态利用路由和聚合优势

Result: 在9个基准测试中，JiSi通过协调10个开源LLM，以仅47%的成本超越了Gemini-3-Pro性能，同时优于主流基线方法

Conclusion: 集体智能代表了通往人工通用智能的新路径，开源LLM的协作可以超越最先进的单体模型

Abstract: Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).

</details>


### [29] [A unified multimodal understanding and generation model for cross-disciplinary scientific research](https://arxiv.org/abs/2601.01363)
*Xiaomeng Yang,Zhiyu Tan,Xiaohui Zhong,Mengping Yang,Qiusheng Huang,Lei Chen,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: FuXi-Uni是一个统一的多模态科学模型，能够在单一架构中理解和生成跨学科的高维科学数据，在地球科学和生物医学领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型通常是领域特定的，缺乏同时理解和生成多模态科学数据的能力，而许多全球性挑战和科学问题本质上是跨学科的，需要跨多个领域的协调进展。

Method: FuXi-Uni将跨学科的科学token与自然语言token对齐，并使用科学解码器重建科学token，从而支持自然语言对话和科学数值预测。

Result: 在地球系统建模中，FuXi-Uni在0.25°分辨率下生成的10天全球天气预报优于最先进的物理预报系统；在热带气旋预测和空间降尺度方面表现优异；在生物医学视觉问答基准测试中超越了领先的多模态大语言模型。

Conclusion: FuXi-Uni通过在原生共享潜在空间中统一异构科学模态，同时保持强大的领域特定性能，为更通用的多模态科学模型迈出了重要一步。

Abstract: Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25° resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.

</details>


### [30] [KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models](https://arxiv.org/abs/2601.01366)
*Zixian Liu,Sihao Liu,Yuqi Zhao*

Main category: cs.AI

TL;DR: KGCE是一个用于跨平台教育智能体评测的新基准平台，通过知识库增强和双图评估框架解决现有基准在私有教育软件任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准框架在支持跨平台教育任务方面存在缺陷，特别是在处理学校专用软件（如小雅智能助手、华师匣子等）时，智能体因不了解这些私有领域软件的结构细节而效率显著下降。此外，当前评估方法主要依赖粗粒度指标，难以捕捉复杂任务中的详细执行效率。

Method: 提出KGCE平台，整合知识库增强和双图评估框架。首先构建包含104个教育相关任务的数据集，涵盖Windows、Android和跨平台协作任务。引入双图评估框架将任务分解为多个子目标并验证完成状态，提供细粒度评估指标。为克服现有智能体在私有领域任务的执行瓶颈，开发了包含学校专用软件知识库的增强智能体系统。

Result: 开发了KGCE基准平台，包含104个教育任务的数据集和双图评估框架，提供了更精细的智能体性能评估方法。代码已在GitHub开源。

Conclusion: KGCE通过知识库增强和双图评估框架，有效解决了跨平台教育智能体评测中的关键问题，为教育领域智能体的性能评估提供了更准确、细粒度的基准平台。

Abstract: With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.

</details>


### [31] [Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification](https://arxiv.org/abs/2601.01378)
*Han Yuan,Yilin Wu,Li Zhang,Zheng Ma*

Main category: cs.AI

TL;DR: 提出AAAI三阶段流程，通过减少事实幻觉来提升小语言模型在金融分类任务上的性能


<details>
  <summary>Details</summary>
Motivation: 小语言模型在金融分类中因推理时易产生事实幻觉而导致分类性能下降，需要探索减少事实幻觉是否能改善其分类能力

Method: 提出AAAI三阶段流程：关联识别、自动检测和自适应推理，通过检测和反馈事实错误来增强模型推理

Result: 实验发现：1）事实幻觉与误分类正相关；2）基于编码器的验证器能有效检测事实幻觉；3）融入事实错误反馈能提升分类性能

Conclusion: AAAI流程有助于提升小语言模型在金融领域的可信度和有效性，为实际应用提供支持

Abstract: Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.

</details>


### [32] [A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts](https://arxiv.org/abs/2601.01467)
*Romuald Kwessy Mouona,Blaise Blériot Koguep Njionou,Etienne Romuald Temgoua Alomo,Rokia Missaoui,Leonard Kwuida*

Main category: cs.AI

TL;DR: 该论文研究三元背景中的蕴涵关系，专注于Ganter和Obiedkov提出的条件属性蕴涵和属性条件蕴涵，目标是构建这些蕴涵的最优基


<details>
  <summary>Details</summary>
Motivation: 三元背景中的蕴涵关系在形式概念分析中具有重要理论价值，但现有研究缺乏对这些蕴涵关系最优基的系统构建方法

Method: 研究Ganter和Obiedkov提出的条件属性蕴涵和属性条件蕴涵，开发构建这些蕴涵最优基的算法或理论框架

Result: 提出了三元背景下条件属性蕴涵和属性条件蕴涵的最优基构建方法

Conclusion: 成功构建了三元背景中蕴涵关系的最优基，为形式概念分析中的三元关系研究提供了重要理论工具

Abstract: This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.

</details>


### [33] [Reading Between the Lines: Deconfounding Causal Estimates using Text Embeddings and Deep Learning](https://arxiv.org/abs/2601.01511)
*Ahmed Dawoud,Osama El-Shamy*

Main category: cs.AI

TL;DR: 提出神经网络增强的双重机器学习框架，利用文本嵌入解决未观测混杂变量问题，相比传统树模型显著降低偏差


<details>
  <summary>Details</summary>
Motivation: 在观测性研究中，未观测混杂变量导致选择偏差，传统计量方法在混杂变量与结构化协变量正交时失效，而高维非结构化文本包含丰富的潜在变量代理信息

Method: 提出神经网络增强的双重机器学习框架，利用文本嵌入进行因果识别，通过深度学习架构建模嵌入流形的连续拓扑结构

Result: 标准树基DML估计器存在显著偏差（+24%），而深度学习方法将偏差降至-0.86%，有效恢复真实因果参数

Conclusion: 当基于高维自然语言数据进行条件化时，深度学习架构对于满足无混杂假设至关重要

Abstract: Estimating causal treatment effects in observational settings is frequently compromised by selection bias arising from unobserved confounders. While traditional econometric methods struggle when these confounders are orthogonal to structured covariates, high-dimensional unstructured text often contains rich proxies for these latent variables. This study proposes a Neural Network-Enhanced Double Machine Learning (DML) framework designed to leverage text embeddings for causal identification. Using a rigorous synthetic benchmark, we demonstrate that unstructured text embeddings capture critical confounding information that is absent from structured tabular data. However, we show that standard tree-based DML estimators retain substantial bias (+24%) due to their inability to model the continuous topology of embedding manifolds. In contrast, our deep learning approach reduces bias to -0.86% with optimized architectures, effectively recovering the ground-truth causal parameter. These findings suggest that deep learning architectures are essential for satisfying the unconfoundedness assumption when conditioning on high-dimensional natural language data

</details>


### [34] [Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making](https://arxiv.org/abs/2601.01522)
*Danial Amin*

Main category: cs.AI

TL;DR: 论文提出贝叶斯成本感知的多LLM编排框架，将LLM视为近似似然模型而非分类器，通过对比提示、鲁棒统计聚合和贝叶斯更新，在非对称错误成本的顺序决策中显著降低总成本并提升公平性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在招聘、医疗分诊、欺诈检测等非对称错误成本场景中作为自主决策代理时，主流方法（查询单个LLM获取后验概率并基于"置信度"阈值行动）在顺序决策中存在不足，需要更系统的概率框架来处理成本敏感决策。

Method: 提出贝叶斯成本感知的多LLM编排框架：1) 将LLM视为近似似然模型而非分类器；2) 通过对比提示为每个候选状态获取似然；3) 使用鲁棒统计方法聚合多个不同LLM的结果；4) 在显式先验下用贝叶斯规则随着新证据更新信念；5) 支持预期成本行动选择、基于信息价值的原则性信息收集。

Result: 在简历筛选实验中（错失人才成本40000美元，面试成本2500美元，电话筛选成本150美元），使用5个LLM（GPT-4o、Claude 4.5 Sonnet、Gemini Pro、Grok、DeepSeek）处理1000份简历，相比最佳单LLM基线降低总成本294000美元（34%），人口统计公平性提升45%（最大组差距从22%降至5%）。

Conclusion: 正确的概率基础在多LLM顺序决策中至关重要，多LLM聚合贡献51%的成本节省，顺序更新贡献43%，分歧触发信息收集贡献20%，证明了贝叶斯成本感知框架在非对称错误成本场景中的优越性，同时通过集成缓解偏见提升公平性。

Abstract: Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds "confidence," and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.

</details>


### [35] [Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix](https://arxiv.org/abs/2601.01532)
*Fanzhe Fu*

Main category: cs.AI

TL;DR: 提出Project Aletheia框架，使用Tikhonov正则化反演判断混淆矩阵来量化System 2推理模型的"认知确信度"，并引入对齐确信分数确保安全性。


<details>
  <summary>Details</summary>
Motivation: 当前AGI评估范式面临认识论危机，静态基准测试能衡量知识广度但无法量化信念深度。需要扩展CHOKE现象框架来量化推理模型的认知确信度。

Method: 提出Project Aletheia认知物理学框架，采用Tikhonov正则化反演判断混淆矩阵。为避免依赖不透明的私有数据，实施合成代理协议。引入对齐确信分数(S_aligned)验证确信度不损害安全性。

Result: 初步试点研究显示，推理模型虽然作为"认知缓冲区"，但在对抗压力下可能表现出"防御性过度思考"。框架为测量AI科学完整性提供了蓝图。

Conclusion: 该工作提出了量化AI认知确信度的新方法，通过Project Aletheia框架和S_aligned分数，为评估AI科学完整性提供了系统化工具，确保推理模型的确信度不会损害安全性。

Abstract: In the progressive journey toward Artificial General Intelligence (AGI), current evaluation paradigms face an epistemological crisis. Static benchmarks measure knowledge breadth but fail to quantify the depth of belief. While Simhi et al. (2025) defined the CHOKE phenomenon in standard QA, we extend this framework to quantify "Cognitive Conviction" in System 2 reasoning models. We propose Project Aletheia, a cognitive physics framework that employs Tikhonov Regularization to invert the judge's confusion matrix. To validate this methodology without relying on opaque private data, we implement a Synthetic Proxy Protocol. Our preliminary pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) suggests that while reasoning models act as a "cognitive buffer," they may exhibit "Defensive OverThinking" under adversarial pressure. Furthermore, we introduce the Aligned Conviction Score (S_aligned) to verify that conviction does not compromise safety. This work serves as a blueprint for measuring AI scientific integrity.

</details>


### [36] [Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation](https://arxiv.org/abs/2601.01546)
*Letian Kong,Qianran,Jin,Renyu Zhang*

Main category: cs.AI

TL;DR: 提出两阶段框架改善LLM在复杂决策环境中的行为对齐：第一阶段明确实验设计建立准确任务表征，第二阶段指导该表征内的推理过程。验证表明复杂环境需要两阶段，简单任务仅需第一阶段。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多用于模拟人类行为实验，但在复杂决策环境中（需要预测他人行动、基于观察行为形成信念）与人类决策存在系统性偏差，需要改进行为对齐方法。

Method: 提出两阶段框架：1) 情境形成 - 明确指定实验设计，建立决策任务及其情境的准确表征；2) 情境导航 - 在该表征内指导推理过程做出决策。在三个不同决策环境中验证：顺序购买游戏、众筹游戏、需求估计任务。

Result: 在四个SOTA模型上测试发现：复杂决策环境需要两阶段才能实现与人类基准的行为对齐，而简单的需求估计任务仅需情境形成阶段。框架能系统设计和诊断LLM社会模拟。

Conclusion: 两阶段框架能有效改善LLM在行为研究中的行为对齐，明确了各阶段适用条件，为将LLM社会模拟作为人类受试者补充提供了系统方法。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.

</details>


### [37] [Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement](https://arxiv.org/abs/2601.01562)
*Mingyu Xu,Cheng Fang,Keyue Jiang,Yuqian Zheng,Yanghua Xiao,Baojian Zhou,Qifang Zhao,Suhang Zheng,Xiuwen Zhu,Jiyang Tang,Yongchi Zhao,Yijia Luo,Zhiqi Bai,Yuchi Xu,Wenbo Su,Wei Wang,Bing Zhao,Lin Qu,Xiaoxiao Xu*

Main category: cs.AI

TL;DR: Logics-STEM是一个在10M规模高质量数据集上微调的推理模型，专注于STEM领域，在8B规模上比次优模型平均提升4.68%性能，通过数据算法协同设计实现优化。


<details>
  <summary>Details</summary>
Motivation: 当前STEM领域推理任务需要更强大的模型能力，但现有开源长思维链数据集规模有限，且数据与算法协同优化不足，限制了推理能力的提升。

Method: 采用数据算法协同设计：数据方面构建10M规模Logics-STEM-SFT-Dataset，包含5阶段数据整理流程；算法方面采用失败驱动的后训练框架，针对SFT阶段的失败区域进行针对性知识检索和数据合成。

Result: Logics-STEM在STEM相关基准测试中表现卓越，8B规模模型比次优模型平均提升4.68%，展示了大规模开源数据与精心设计合成数据结合的潜力。

Conclusion: 数据算法协同设计对于通过后训练增强推理能力至关重要，Logics-STEM模型和数据集的开源将支持开源社区的未来研究。

Abstract: We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.

</details>


### [38] [CaveAgent: Transforming LLMs into Stateful Runtime Operators](https://arxiv.org/abs/2601.01569)
*Maohao Ran,Zhenglin Wan,Cooper Lin,Yanting Zhang,Hongyu Xin,Hongwei Fan,Yibo Xu,Beier Luo,Yaxin Zhou,Wangbo Zhao,Lijie Yang,Lang Feng,Fuchao Yang,Jingxuan Wu,Yiqiao Huang,Chendong Ma,Dailing Jiang,Jianbo Deng,Sihui Han,Bo An,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: CaveAgent是一个将LLM从文本生成器转变为运行时操作员的框架，通过双流上下文架构和状态化运行时管理，解决了传统JSON函数调用在长时任务中的脆弱性和上下文漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代理系统受限于文本中心范式，传统的JSON函数调用方法在处理长时任务时存在脆弱的多轮依赖和上下文漂移问题，需要更强大的状态管理和执行能力。

Method: 提出CaveAgent框架，采用双流上下文架构：轻量级语义流用于推理，持久化确定性Python运行时流用于执行。引入状态化运行时管理，支持复杂Python对象（如DataFrame、数据库连接）的注入、操作和跨轮次持久化。

Result: 在Tau²-bench、BFCL等基准测试中表现优异：零售任务成功率提升10.5%，多轮场景总token消耗减少28.4%。数据密集型任务中，直接变量存储和检索减少59% token消耗，能处理导致其他代理上下文溢出的海量数据。

Conclusion: CaveAgent通过将LLM转变为运行时操作员，解决了传统代理系统的关键限制，实现了更高效、更可靠的长时任务执行，为LLM代理系统提供了新的范式。

Abstract: LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from "LLM-as-Text-Generator" to "LLM-as-Runtime-Operator." We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\% success rate improvement on retail tasks and reduces total token consumption by 28.4\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.

</details>


### [39] [Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration](https://arxiv.org/abs/2601.01609)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 提出一个结合LLM灵活性与符号推理保证性的框架：用LLM将非结构化文本转为ABox断言，再用SWRL推理器进行确定性规则应用


<details>
  <summary>Details</summary>
Motivation: 在需要可审计和可解释决策的领域（如临床协议、法律证据规则、科学标准），现有方法存在矛盾：LLM灵活但无法保证一致性，符号系统有保证但需要结构化输入

Method: 提出集成模式：LLM作为本体填充引擎，将非结构化文本转为ABox断言（基于专家编写的TBox规范），SWRL推理器应用规则提供确定性保证；框架将推理分解为实体识别、断言提取和符号验证

Result: 在三个领域（法律传闻确定、科学方法任务应用、临床试验资格）和11个语言模型上验证，结构化分解在总体上比few-shot提示有显著改进，所有三个领域都有提升；消融研究证实符号验证比单纯结构化提示有实质好处

Conclusion: 该框架结合了LLM的灵活性和符号推理的保证性，填充的ABox可与标准语义网工具集成，支持更丰富的推理模式

Abstract: Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.

</details>


### [40] [Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications](https://arxiv.org/abs/2601.01718)
*YuanLab. ai,:,Shawn Wu,Sean Wang,Louie Li,Darcy Chen,Allen Wang,Jiangang Luo,Xudong Zhao,Joseph Shen,Gawain Ma,Jasper Jia,Marcus Mao,Claire Wang,Hunter He,Carol Wang,Zera Zhang,Jason Wang,Chonly Shen,Leo Zhang,Logan Chen,Qasim Meng,James Gong,Danied Zhao,Penn Zheng,Owen Zhu,Tong Yu*

Main category: cs.AI

TL;DR: Yuan3.0 Flash是一个开源的MoE多模态大语言模型，具有37亿激活参数和400亿总参数，专为企业任务设计，同时保持通用任务竞争力，并通过RAPO算法解决大推理模型的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决大推理模型（LRMs）中常见的过度思考现象，并开发一个既能在企业任务上表现出色，又能在通用任务上保持竞争力的高效多模态大语言模型。

Method: 采用混合专家（MoE）架构，提出反射感知自适应策略优化（RAPO）算法来调节过度思考行为，模型具有37亿激活参数和400亿总参数。

Result: 在企业任务（如RAG、复杂表格理解、摘要生成）上表现优异，在数学、科学等推理领域达到前沿模型可比精度，同时仅需约1/4到1/2的平均token数。

Conclusion: Yuan3.0 Flash是一个高效的多模态大语言模型，通过RAPO算法有效解决过度思考问题，在企业任务和通用推理任务上都表现出色，已完全开源供研究和实际部署。

Abstract: We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.

</details>


### [41] [AI Agent Systems: Architectures, Applications, and Evaluation](https://arxiv.org/abs/2601.01743)
*Bin Xu*

Main category: cs.AI

TL;DR: 本文对AI智能体架构进行了系统性综述，涵盖推理、规划、工具调用等核心组件，提出了统一的分类体系，并讨论了设计权衡、评估挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: AI智能体结合基础模型与推理、规划、记忆和工具使用能力，正在成为连接自然语言意图与真实世界计算的重要接口。随着该领域的快速发展，需要系统性地梳理智能体架构的现状、分类和设计考量。

Method: 通过文献综述方法，将现有工作组织为统一的分类体系：1) 智能体组件（策略/LLM核心、记忆、世界模型、规划器、工具路由器、批评器）；2) 编排模式（单智能体vs.多智能体，集中式vs.去中心化协调）；3) 部署设置（离线分析vs.在线交互，安全关键vs.开放任务）。

Result: 建立了AI智能体架构的全面分类框架，识别了关键设计权衡（延迟vs.准确性、自主性vs.可控性、能力vs.可靠性），分析了评估面临的挑战（非确定性、长时程信用分配、工具和环境变异性、隐藏成本），并总结了当前测量和基准测试实践。

Conclusion: AI智能体架构研究已形成系统化框架，但仍面临工具动作验证与防护、可扩展内存与上下文管理、决策可解释性、真实工作负载下的可复现评估等开放挑战，需要进一步研究解决。

Abstract: AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\ multi-agent; centralized vs.\ decentralized coordination), and deployment settings (offline analysis vs.\ online interactive assistance; safety-critical vs.\ open-ended tasks). We discuss key design trade-offs -- latency vs.\ accuracy, autonomy vs.\ controllability, and capability vs.\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.

</details>


### [42] [A New Benchmark for the Appropriate Evaluation of RTL Code Optimization](https://arxiv.org/abs/2601.01765)
*Yao Lu,Shang Liu,Hangan Zhou,Wenji Fang,Qijun Zhang,Zhiyao Xie*

Main category: cs.AI

TL;DR: RTL-OPT是一个用于评估LLM在RTL优化能力的新基准测试，包含36个手工设计的数字电路，涵盖多种实现类别，并提供自动化评估框架验证功能正确性和量化PPA改进。


<details>
  <summary>Details</summary>
Motivation: 当前AI芯片设计需要高效的集成电路设计，现有LLM生成RTL代码的基准测试主要评估语法正确性，而非功率、性能和面积（PPA）的优化质量，因此需要专门评估LLM在RTL优化能力的基准。

Method: 创建RTL-OPT基准测试，包含36个手工设计的数字电路，涵盖组合逻辑、流水线数据通路、有限状态机和存储器接口等类别。每个任务提供次优版本和人工优化的参考版本，反映行业验证的优化模式。集成自动化评估框架验证功能正确性并量化PPA改进。

Result: RTL-OPT提供了一个标准化的评估框架，能够验证生成模型的RTL代码功能正确性，并量化其在功率、性能和面积方面的改进，填补了现有基准测试在评估优化质量方面的空白。

Conclusion: RTL-OPT基准测试为评估LLM在硬件设计优化能力提供了标准化且有意义的评估工具，有助于推动AI在集成电路设计优化方面的发展。

Abstract: The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.

</details>


### [43] [Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches](https://arxiv.org/abs/2601.01774)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.AI

TL;DR: LLMs在求解超越方程时，直接数值预测效果较差，但结合传统迭代求解器的混合架构能显著降低误差67.9%-81.8%，表明LLMs更适合作为经典数值求解器的智能接口而非独立计算引擎。


<details>
  <summary>Details</summary>
Motivation: 超越方程在工程实践中广泛存在，需要迭代数值求解。研究旨在评估大型语言模型（LLMs）能否直接求解这些方程，还是需要结合传统求解器的混合架构更有效。

Method: 测试6个最先进的LLM模型（GPT-5.1、GPT-5.2、Gemini-3-Flash、Gemini-2.5-Lite、Claude-Sonnet-4.5、Claude-Opus-4.5），在7个工程领域的100个问题上，比较直接数值预测与求解器辅助计算（LLMs制定控制方程并提供初始条件，牛顿-拉夫逊迭代执行数值求解）。

Result: 直接预测的平均相对误差为0.765-1.262，而求解器辅助计算为0.225-0.301，误差降低67.9%-81.8%。电子领域改进最显著（93.1%），流体力学改进最小（7.2%）。

Conclusion: 当代LLMs擅长符号操作和领域知识检索，但在精度关键的迭代算术方面表现不佳，最佳部署方式是作为经典数值求解器的智能接口，而非独立计算引擎。

Abstract: Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.

</details>


### [44] [PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor](https://arxiv.org/abs/2601.01802)
*Qianjun Pan,Junyi Wang,Jie Zhou,Yutao Yang,Junsong Li,Kaiyin Xu,Yougen Zhou,Yihan Li,Jingyuan Zhao,Qin Chen,Ningning Zhou,Kai Chen,Liang He*

Main category: cs.AI

TL;DR: PsychEval：一个用于心理评估AI的多会话、多疗法、高真实度基准，包含数据集、评估框架和强化学习环境，旨在训练和评估AI心理咨询师。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的心理评估AI面临三大挑战：1) 训练高真实度的AI咨询师需要处理纵向任务、持续记忆和动态目标跟踪；2) 复杂案例需要多疗法灵活策略；3) 需要系统化的评估框架。

Method: 构建多会话基准（6-10个会话，三个阶段），涵盖五种治疗模式（心理动力学、行为主义、CBT、人本存在主义、后现代主义）和整合疗法；标注677个元技能和4577个原子技能；建立包含18个指标的评估框架；创建2000多个多样化的客户档案。

Result: 实验分析充分验证了数据集的高质量和临床保真度；PsychEval超越了静态基准测试，可作为高保真强化学习环境，支持临床负责任和适应性AI咨询师的自我进化训练。

Conclusion: PsychEval为解决AI心理评估的关键挑战提供了全面解决方案，通过多会话、多疗法的高真实度基准和系统评估框架，推动了可靠、适应性强的AI心理咨询师的发展。

Abstract: To develop a reliable AI for psychological assessment, we introduce \texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.

</details>


### [45] [Admissibility Alignment](https://arxiv.org/abs/2601.01816)
*Chris Duffey*

Main category: cs.AI

TL;DR: 论文提出"可接受性对齐"框架，将AI对齐重新定义为在不确定性下对结果分布的可接受行动和决策选择属性，并介绍MAP-AI系统架构通过蒙特卡洛估计和政策选择实现对齐。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐方法通常将对齐视为静态或二元条件，缺乏在不确定性下评估决策政策行为的能力。需要将对齐重新概念化为概率性、决策理论属性，以应对现实世界中的不确定性、干预效应、价值模糊性和治理约束。

Method: 提出MAP-AI（蒙特卡洛对齐政策）系统架构，通过蒙特卡洛估计结果分布和可接受性控制的政策选择来执行对齐。框架评估决策政策在多个可能未来场景中的表现，明确建模不确定性、干预效应、价值模糊性和治理约束。

Result: 开发了一个实用的基础框架，用于治理AI系统，其影响不是由个体预测决定，而是由政策在分布和尾部事件中的行为决定。展示了如何将分布对齐评估整合到决策过程中，产生可接受性控制的行动选择机制。

Conclusion: 可接受性对齐框架提供了一种可执行的方法论，用于评估企业和机构AI系统中的信任和对齐，将概率预测与不确定性下的决策推理区分开来，为AI治理提供了实用的基础。

Abstract: This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.
  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.

</details>


### [46] [COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs](https://arxiv.org/abs/2601.01836)
*Dasol Choi,DongGeon Lee,Brigitta Jesica Kartono,Helena Berndt,Taeyoun Kwon,Joonwon Jang,Haon Park,Hwanjo Yu,Minsuk Kahng*

Main category: cs.AI

TL;DR: COMPASS框架首次系统评估LLMs是否符合组织政策，发现模型在处理合法请求时表现良好（>95%准确率），但在执行禁令时严重失败（仅拒绝13-40%的违规请求），揭示当前LLMs缺乏政策关键部署所需的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗、金融等高风险企业应用中部署，确保模型遵守组织特定政策变得至关重要。然而现有安全评估仅关注通用危害，缺乏针对组织政策的系统性评估框架。

Method: 提出COMPASS框架，应用于八个不同行业场景，生成并验证5,920个查询，测试常规合规性和对抗鲁棒性。评估七个最先进模型，通过精心设计的边缘案例测试政策执行能力。

Result: 发现基本不对称性：模型能可靠处理合法请求（>95%准确率），但在执行禁令时严重失败，仅拒绝13-40%的对抗性违规请求。不同模型在政策合规性方面表现差异显著。

Conclusion: 当前LLMs缺乏政策关键部署所需的鲁棒性，COMPESS框架为组织AI安全提供了必要的评估工具，突显了在现实企业环境中确保政策合规性的紧迫需求。

Abstract: As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.

</details>


### [47] [Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01844)
*Udiptaman Das,Krishnasai B. Atmakuri,Duy Ho,Chi Lee,Yugyung Lee*

Main category: cs.AI

TL;DR: 提出一个端到端框架，利用多智能体提示和模式约束的检索增强生成策略，直接从自由文本构建临床知识图谱，特别针对肿瘤学领域，无需依赖黄金标准标注。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖结构化输入，缺乏对事实准确性和语义一致性的鲁棒验证，这在肿瘤学领域尤其成问题。需要直接从非结构化临床叙述构建知识图谱的新方法。

Method: 采用多智能体提示和模式约束的检索增强生成策略，包括：(1) 提示驱动的实体、属性和关系抽取；(2) 基于熵的不确定性评分；(3) 本体对齐的RDF/OWL模式生成；(4) 多LLM共识验证用于幻觉检测和语义精炼。

Result: 应用于两个肿瘤学队列（PDAC和BRCA），该方法产生了可解释、SPARQL兼容且临床基础的知识图谱。实验结果显示在精确度、相关性和本体一致性方面相比基线方法有持续提升。

Conclusion: 该框架支持连续精炼和自监督评估，能够迭代改进图谱质量，为直接从自由文本构建临床知识图谱提供了有效解决方案，特别适用于肿瘤学领域。

Abstract: Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.

</details>


### [48] [Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios](https://arxiv.org/abs/2601.01857)
*Defei Xia,Bingfeng Pi,Shenbin Zhang,Song Hua,Yunfei Wei,Lei Zuo*

Main category: cs.AI

TL;DR: 本文提出了Jenius-Agent框架，通过自适应提示生成、上下文感知工具编排和分层内存机制三大创新，显著提升LLM智能体的任务准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体系统发展，提升自主智能体在上下文理解、工具使用和响应生成方面的任务性能变得日益重要。尽管先前研究改进了LLM智能体的整体设计，但其内部推理和工具使用流程的系统性优化仍未被充分探索。

Method: 提出了基于真实世界实践经验的智能体框架，包含三大关键创新：1）自适应提示生成策略，根据智能体状态和任务目标调整提示以提高可靠性和鲁棒性；2）上下文感知工具编排模块，基于用户意图和上下文进行工具分类、语义检索和自适应调用；3）分层内存机制，集成会话内存、任务历史和外部摘要，通过动态摘要和压缩提高相关性和效率。框架集成了基于模型上下文协议（MCP）的工具、文件输入/输出和执行反馈三大优化。

Result: 实验显示任务准确性提高了20%，同时降低了令牌成本、响应延迟和调用失败率。该框架已在Jenius平台部署，为稳健、协议兼容的自主智能体提供了轻量级可扩展解决方案。

Conclusion: Jenius-Agent框架通过系统性优化智能体的内部推理和工具使用流程，显著提升了任务性能，为实际部署提供了有效的解决方案。

Abstract: As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.

</details>


### [49] [Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence](https://arxiv.org/abs/2601.01875)
*Kewen Cao,Jianxu Chen,Yongbing Zhang,Ye Zhang,Hongxiao Wang*

Main category: cs.AI

TL;DR: 提出SQL为中心的代理框架，通过特征测量和可审计推理提升病理图像分析的透明度


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型生成的解释缺乏可验证证据，临床医生需要了解模型决策背后的具体图像特征驱动因素

Method: 提取可解释的细胞特征后，特征推理代理编写执行SQL查询聚合视觉证据，知识比较代理将发现与病理知识对比

Result: 在两个病理视觉问答数据集上的实验表明，该方法提高了可解释性和决策可追溯性，生成可执行的SQL追踪

Conclusion: SQL为中心的代理框架通过可审计的特征测量和推理，增强了病理图像分析的透明度和可信度

Abstract: Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.

</details>


### [50] [Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs](https://arxiv.org/abs/2601.01878)
*Farzan Karimi-Malekabadi,Suhaib Abdurahman,Zhivar Sourati,Jackson Trager,Morteza Dehghani*

Main category: cs.AI

TL;DR: 论文指出当前大语言模型的社会认知基准测试存在理论空白问题，导致评估结果被过度泛化，并提出了理论追踪卡（TTC）作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的社会认知基准测试虽然得分高，但无法预测真实世界行为。现有研究主要归因于测量和效度问题，但作者认为更根本的问题是缺乏明确的理论基础，导致评估结果被系统性过度解释。

Method: 首先诊断并形式化理论空白问题，然后提出理论追踪卡（TTC）——一种轻量级文档工具，明确记录评估的理论基础、目标能力组件、操作化过程和局限性。

Result: 通过TTC可以增强社会认知评估的可解释性和可重用性，明确理论、任务操作化、评分和局限性之间的完整效度链，无需修改基准测试或要求单一理论共识。

Conclusion: 理论追踪卡为解决社会认知评估中的理论空白问题提供了实用工具，有助于防止基准测试结果的系统性过度泛化，提高评估的透明度和有效性。

Abstract: Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.

</details>


### [51] [MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning](https://arxiv.org/abs/2601.01910)
*Minh Hieu Ha,Khanh Ly Ta,Hung Phan,Tung Doan,Tung Dao,Dao Tran,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MMP-A* 是一个多模态路径规划框架，结合视觉语言模型的空间感知能力和自适应衰减机制，在复杂环境中实现接近最优的轨迹规划，同时大幅降低计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 传统A*算法在大规模复杂环境中计算和内存成本过高，而基于大语言模型的路径规划方法仅依赖文本推理，缺乏空间感知能力，在拓扑复杂环境中容易产生错误路径点，导致纠正成本高昂。

Method: 提出MMP-A*多模态框架：1）集成视觉语言模型的空间感知能力，将高层推理锚定在物理几何中；2）引入自适应衰减机制，动态调节不确定路径点在启发式函数中的影响，确保几何有效性并减少内存开销。

Result: 在具有严重杂乱和拓扑复杂性的挑战性环境中测试，MMP-A*实现了接近最优的轨迹规划，同时显著降低了操作成本（计算和内存开销）。

Conclusion: MMP-A*作为一个感知接地且计算高效的范式，为自主导航提供了新的解决方案，通过多模态融合解决了纯文本规划器的局限性。

Abstract: Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.
  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.

</details>


### [52] [OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation](https://arxiv.org/abs/2601.01939)
*Victor Sanchez,Chris Reinke,Ahamed Mohamed,Xavier Alameda-Pineda*

Main category: cs.AI

TL;DR: OpenSocInt是一个开源软件包，提供多模态社交交互模拟器和模块化架构，用于训练社交智能体，已在社交导航任务中展示应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏用于研究和训练社交智能体的开源模拟环境，特别是在多模态社交交互方面。需要模块化、可扩展的工具来探索不同感知特征、编码融合方法以及智能体架构。

Method: 开发了OpenSocInt开源软件包，包含多模态社交交互模拟器和模块化训练架构。采用基于社交导航任务的实验协议进行验证，支持不同感知特征、编码融合方法和智能体类型的探索。

Result: 成功开发并开源了OpenSocInt软件包，已在社交导航任务中展示其应用价值。软件采用GPL许可证，已在GitLab上公开可用。

Conclusion: OpenSocInt为社交智能体研究提供了有价值的开源工具，支持多模态交互模拟和模块化训练，有助于推动社交人工智能领域的发展。

Abstract: In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.

</details>


### [53] [CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes](https://arxiv.org/abs/2601.01976)
*Yasmine Souissi,Fabrice Boissier,Nida Meddouri*

Main category: cs.AI

TL;DR: 本文对基于形式概念分析（FCA）的分类器进行了综述，提出了一种从名义数据计算闭包算子的新方法，并构建了专注于最相关概念的部分概念格，实验证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 知识发现（KDD）旨在从海量数据中提取隐藏知识，分类是核心数据挖掘技术之一。形式概念分析（FCA）作为一种可解释和可解释的学习方法，基于概念格的数学结构，能够生成形式概念并发现隐藏关系，因此值得深入研究其分类应用。

Method: 1. 对基于FCA的分类器进行了最先进的综述；2. 探索了从名义数据计算闭包算子的各种方法；3. 提出了一种构建部分概念格的新方法，专注于最相关的概念。

Result: 通过实验验证了所提方法的效率，证明了该方法在构建部分概念格方面的有效性。

Conclusion: FCA作为一种可解释的分类方法具有重要价值，提出的部分概念格构建方法能够有效提高分类效率，为基于FCA的分类器研究提供了新的方向。

Abstract: Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.

</details>


### [54] [ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems](https://arxiv.org/abs/2601.01982)
*Noel Thomas*

Main category: cs.AI

TL;DR: ChaosBench-Logic是一个评估大语言模型在混沌动力系统领域逻辑推理能力的基准测试，包含30个系统、621个问题，测试显示前沿LLMs在单项准确率上达到91-94%，但在组合推理上得分为0%，对话准确率在53.1-75.5%之间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言任务上表现出色，但在需要精确逻辑和符号推理的领域仍然脆弱。混沌动力系统提供了一个特别严格的测试环境，因为混沌是确定性的，但经常被误解为随机性或复杂性。需要建立一个基准来评估LLMs在这种复杂领域的推理能力。

Method: 引入ChaosBench-Logic基准测试，使用统一的一阶逻辑本体论评估30个不同的动力系统。每个系统标注了11个语义谓词的真值分配，生成621个问题，涵盖七个推理类别：多跳推理、跨系统类比、反事实推理、偏见探测和多轮对话。定义了逻辑准确性、蕴含一致性、对话连贯性和矛盾性等指标，并发布了开源评估管道。

Result: 前沿LLMs（GPT-4、Claude 3.5 Sonnet、Gemini 2.5 Flash、LLaMA-3 70B）在单项准确率上达到91-94%，但在组合推理项目上得分为0%，表现出脆弱的全局一致性。对话级准确率从53.1%（GPT-4 CoT）到75.5%（LLaMA-3零样本）不等。

Conclusion: ChaosBench-Logic为诊断LLMs在复杂逻辑推理中的失败提供了一个严格的测试平台，并为开发改进LLMs科学推理能力的神经符号方法奠定了基础。研究表明，尽管LLMs在单项任务上表现良好，但在需要组合推理和全局一致性的复杂逻辑任务上仍然存在显著缺陷。

Abstract: Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.

</details>


### [55] [MindChat: A Privacy-preserving Large Language Model for Mental Health Support](https://arxiv.org/abs/2601.01993)
*Dong Xue,Jicheng Tu,Ming Wang,Xin Yan,Fangzhou Liu,Jie Hu*

Main category: cs.AI

TL;DR: MindChat是一个保护隐私的心理健康支持大语言模型，配合MindCorpus合成咨询数据集，通过联邦学习和差分隐私减少隐私风险，在咨询能力评估中表现良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在心理健康支持方面有潜力，但受限于真实咨询对话的稀缺性和敏感性，需要解决数据获取和隐私保护问题。

Method: 1) 使用多智能体角色扮演框架构建MindCorpus合成咨询数据集，采用双闭环反馈设计：轮次级批判修订和会话级策略优化；2) 通过联邦学习配合LoRA适配器进行参数高效微调，并加入差分隐私优化减少隐私风险。

Result: MindCorpus提高了训练效果，MindChat在自动LLM评估和人工评估中与现有通用和咨询导向的LLM基线竞争，同时在成员推理攻击下表现出减少的隐私泄露。

Conclusion: 该方法成功构建了高质量合成咨询数据集和保护隐私的心理健康支持模型，为解决真实咨询数据稀缺和隐私问题提供了可行方案。

Abstract: Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.

</details>


### [56] [XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging](https://arxiv.org/abs/2601.02008)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: XAIMeD是一个可解释的医疗AI框架，通过神经符号架构整合临床专家知识，提升分布偏移下的鲁棒性、罕见类别敏感性，并提供临床对齐的解释。


<details>
  <summary>Details</summary>
Motivation: 医疗AI中可解释性、领域泛化和罕见类别可靠性是关键挑战，深度学习模型在真实世界分布偏移下经常失败，并对罕见临床条件表现出偏见。

Method: 将临床专业知识编码为原子医学命题的逻辑连接，转化为机器可检查的类别特定规则；通过加权特征满足分数量化诊断效用；符号推理分支与神经预测互补；置信度加权融合整合符号和深度输出；基于熵不平衡增益和罕见类别基尼系数的自适应路由机制。

Result: 在四个挑战性任务上评估，包括从rs-fMRI的癫痫发作区定位和6个多中心数据集的糖尿病视网膜病变分级，显示显著性能提升：跨领域泛化提高6%，罕见类别F1分数提升10%，远超最先进的深度学习基线。

Conclusion: XAIMeD提供了一个原则性、临床忠实且可解释的多模态医疗AI方法，临床基础的符号组件作为有效的正则化器，确保对分布偏移的鲁棒性。

Abstract: Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.

</details>


### [57] [Simulated Reasoning is Reasoning](https://arxiv.org/abs/2601.02043)
*Hendrik Kempt,Alon Lavie*

Main category: cs.AI

TL;DR: 论文认为基础模型通过模仿"思考过程"、测试生成路径并迭代，实现了不同于人类符号推理的新型推理能力，这挑战了传统推理概念，并带来安全性和适当性方面的规范考量。


<details>
  <summary>Details</summary>
Motivation: 传统上认为推理是通过符号推理实现理解的路径，但基础模型展示了不同的推理方式——通过模仿"思考过程"、测试路径和迭代来解决问题。这挑战了传统推理概念，需要重新评估推理的必要条件，并为应对基础模型推理的脆弱性提供安全性和鲁棒性防御方法。

Method: 本文采用哲学分析方法，讨论基础模型推理现象的不同哲学解释，论证"随机鹦鹉"隐喻已失去相关性应被抛弃，并反思从这些推理模型及其增长能力中产生的安全性和适当性规范要素。

Result: 基础模型通过模仿思考过程、测试路径和迭代的方式实现了某种形式的推理能力，能够独立或通过少量样本学习解决问题，但其推理与人类推理存在根本差异——缺乏基础和常识，导致推理过程脆弱。这些发现将显著改变我们对推理及其必要条件的评估。

Conclusion: 基础模型展示了不同于传统符号推理的新型推理能力，这要求我们重新思考推理的本质。虽然"随机鹦鹉"隐喻已过时，但需要关注这种推理的脆弱性，并建立相应的安全性和适当性规范框架来应对这些新兴推理模型带来的挑战。

Abstract: Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., "symbolic reasoning". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can "reason" by way of imitating the process of "thinking out loud", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the "stochastic parrot" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.

</details>


### [58] [Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management](https://arxiv.org/abs/2601.02061)
*Faizan Ahmed,Aniket Dixit,James Brusey*

Main category: cs.AI

TL;DR: 论文研究了通过高阶导数惩罚实现动作平滑正则化，在连续控制基准测试和实际建筑能源管理中验证了其有效性，特别是三阶导数惩罚（急动度最小化）在保持性能的同时显著提升平滑度。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习代理常表现出不稳定、高频的控制行为，这在实际部署中会导致能耗过高和机械磨损，因此需要研究动作平滑正则化方法来改善这一问题。

Method: 采用高阶导数惩罚进行动作平滑正则化，从连续控制基准测试的理论理解到建筑能源管理的实际验证，系统评估了不同阶数导数惩罚的效果，重点关注三阶导数惩罚（急动度最小化）。

Result: 在四个连续控制环境中的评估表明，三阶导数惩罚能持续实现最优平滑度同时保持竞争力性能；在HVAC控制系统应用中，平滑策略将设备切换减少了60%，带来显著运营效益。

Conclusion: 高阶动作正则化是连接强化学习优化与能源关键应用中操作约束的有效桥梁，为实际部署提供了实用的平滑控制方法。

Abstract: Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.

</details>


### [59] [FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations](https://arxiv.org/abs/2601.02071)
*Adeshola Okubena,Yusuf Ali Mohammed,Moe Elbadawi*

Main category: cs.AI

TL;DR: 本研究探索了将大型语言模型（LLM）应用于药物3D打印配方开发，通过微调四种LLM架构在1400多个FDM配方数据集上，实现了基于API剂量的辅料推荐和丝材机械性能预测。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的药物3D打印研究大多局限于狭窄领域，未能全面解决配方开发中的复杂挑战。随着人工通用智能概念的发展，需要探索更通用、类人推理的系统来推动药物3D打印的个性化制剂发展。

Method: 研究微调了四种LLM架构，使用包含1400多个FDM配方的数据集，系统评估了微调和生成参数配置。重点关注基于API剂量的辅料推荐和丝材机械性能预测能力。

Result: Llama2模型在FDM配方辅料推荐方面表现最佳；模型选择和参数化显著影响性能，较小的LLM出现灾难性遗忘现象；发现标准LLM指标仅评估语言能力而非配方可加工性；生物医学相关数据训练的LLM不一定产生最佳结果。

Conclusion: 需要解决灾难性遗忘、评估指标局限性和数据适用性等挑战，才能推动LLM超越语言能力，成为药物配方开发中可靠的系统，实现真正的个性化制剂。

Abstract: Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.

</details>


### [60] [EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning](https://arxiv.org/abs/2601.02163)
*Chuanrui Hu,Xingze Gao,Zuyi Zhou,Dannong Xu,Yi Bai,Xintong Li,Hui Zhang,Tong Li,Chong Zhang,Lidong Bing,Yafeng Deng*

Main category: cs.AI

TL;DR: EverMemOS是一个自组织记忆操作系统，采用类记忆印迹的生命周期管理计算记忆，通过将对话流转换为记忆单元、组织成主题记忆场景，实现智能检索，在长上下文记忆增强推理任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为长期交互代理部署时，有限的上下文窗口难以维持长时间连贯行为。现有记忆系统通常存储孤立记录并检索片段，难以整合演变的用户状态和解决冲突。

Method: 提出EverMemOS自组织记忆操作系统：1) 情节痕迹形成：将对话流转换为记忆单元，捕捉情节痕迹、原子事实和时间有界的前瞻信号；2) 语义整合：将记忆单元组织成主题记忆场景，提炼稳定的语义结构并更新用户画像；3) 重构回忆：执行记忆场景引导的智能检索，为下游推理组合必要且充分的上下文。

Result: 在LoCoMo和LongMemEval基准测试中，EverMemOS在记忆增强推理任务上达到最先进性能。在PersonaMem v2上的画像研究和定性案例研究展示了聊天导向能力，如用户画像和前瞻预测。

Conclusion: EverMemOS通过自组织记忆操作系统有效解决了LLMs在长期交互中的记忆管理问题，实现了更连贯、智能的对话代理行为，代码已开源。

Abstract: Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.

</details>


### [61] [Streaming Hallucination Detection in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.02170)
*Haolang Lu,Minghui Pan,Ripeng Li,Guoshun Nan,Jialin Zhuang,Zijie Zhao,Zhongxiang Sun,Kun Wang,Yang Liu*

Main category: cs.AI

TL;DR: 该论文提出将长链思维推理中的幻觉视为演化潜状态而非一次性错误事件，引入累积前缀级幻觉信号来追踪推理状态的全局演化，实现流式幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 长链思维推理虽然能提升大语言模型性能，但其中的幻觉问题往往微妙且会在推理步骤间传播。传统方法将幻觉视为一次性错误事件，但作者认为在长链推理中，幻觉应被理解为演化中的潜状态，需要全局视角来追踪其演变过程。

Method: 将步骤级幻觉判断视为局部观测，引入累积前缀级幻觉信号来追踪整个推理轨迹上的推理状态全局演化。该方法能够实现长链思维推理中的流式幻觉检测，提供实时、可解释的证据。

Result: 提出的方法能够有效检测长链推理中的幻觉演化，提供实时监控能力，并且检测结果具有可解释性，能够展示幻觉在推理过程中的传播路径。

Conclusion: 将长链思维推理中的幻觉视为演化潜状态而非一次性错误事件，通过累积前缀级信号追踪全局演化，能够实现更有效的流式幻觉检测，为长链推理的可靠性提供新的监控框架。

Abstract: Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.

</details>


### [62] [Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents](https://arxiv.org/abs/2601.02314)
*Sourena Khanzadeh*

Main category: cs.AI

TL;DR: 论文提出Project Ariadne框架，使用因果模型和反事实逻辑来审计LLM代理推理的因果完整性，发现当前代理存在"因果解耦"问题，推理痕迹只是"推理剧场"而非真实决策驱动。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理越来越多地承担高风险自主决策任务，其推理过程的透明度成为关键安全问题。虽然思维链提示允许生成人类可读的推理痕迹，但尚不清楚这些痕迹是模型输出的真实驱动因素还是事后合理化解释。

Method: 提出Project Ariadne框架，利用结构因果模型和反事实逻辑来审计代理推理的因果完整性。该方法对中间推理节点进行硬干预（do-演算），系统性地反转逻辑、否定前提和颠倒事实主张，以测量终端答案的因果敏感性。

Result: 对最先进模型的实证评估揭示了持续的"忠实性差距"。定义并检测到一种普遍的故障模式"因果解耦"，在事实和科学领域中违反密度高达0.77。在这些情况下，尽管内部逻辑矛盾，代理仍得出相同结论，证明其推理痕迹只是"推理剧场"，而决策由潜在参数先验控制。

Conclusion: 当前代理架构本质上容易产生不忠实的解释，提出Ariadne分数作为对齐陈述逻辑与模型行为的新基准。这表明需要新的方法来确保LLM代理推理的因果忠实性。

Abstract: As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \textbf{faithful} generative drivers of the model's output or merely \textbf{post-hoc rationalizations}. We introduce \textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.

</details>


### [63] [Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.02346)
*Falcon LLM Team,Iheb Chaabane,Puneesh Khanna,Suhail Mohmad,Slim Frikha,Shi Hu,Abdalgader Abubaker,Reda Alami,Mikhail Lubinets,Mohamed El Amine Seddik,Hakim Hacid*

Main category: cs.AI

TL;DR: Falcon-H1R是一个7B参数的推理优化模型，证明了小语言模型也能实现有竞争力的推理性能，在多项基准测试中匹配或超越2-7倍大的SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 探索小语言模型是否能够实现与大型模型相媲美的推理性能，解决模型参数效率问题，为需要广泛思维链生成和并行测试时间扩展的场景提供实用的推理系统骨干。

Method: 采用精心策划的数据集和针对性训练策略，包括高效的监督微调和强化学习扩展；设计混合并行架构以实现更快推理；结合DeepConf方法实现最先进的测试时间扩展效率。

Result: 在多种推理密集型基准测试中，Falcon-H1R-7B模型一致匹配或超越2-7倍大的最先进推理模型，实现了参数效率、推理速度、token效率和准确性的三维效率极限。

Conclusion: 通过有针对性的模型训练和架构选择，紧凑模型能够提供强大且可扩展的推理性能，为高级推理系统的扩展提供了实用的骨干模型。

Abstract: This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [64] [Improved decoding algorithms for surface codes under independent bit-flip and phase-flip errors](https://arxiv.org/abs/2601.00972)
*Louay Bazzi*

Main category: cs.IT

TL;DR: 本文研究了表面码和环面码在独立X/Z噪声模型下的精确解码问题，提出了改进的解码算法复杂度，证明了SMW解码在NC类中，SMLC解码也有多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 研究表面码和环面码在量子纠错中的精确解码问题，旨在改进现有解码算法的计算复杂度，特别是针对独立X/Z噪声模型下的SMW和SMLC解码问题。

Method: 采用Fisher gadget将SMW解码局部约化为最小权重完美匹配问题，保持平面性；对SMLC解码采用对偶环表述和Fisher-Kasteleyn-Temperley构造约化为平面Pfaffian计算；利用Lipton-Tarjan平面分隔方法和MacWilliams对偶性进行理论分析。

Result: 对于SMW解码，实现了O(n^{3/2} log n)时间复杂度的解码器，优于标准方法的O(n^3 log n)；对于SMLC解码，平面表面码实现了O(n^{3/2})代数复杂度，环面码实现了O(n^3)代数复杂度；证明了SMW解码在NC类中。

Conclusion: 本文为表面码和环面码提供了改进的精确解码算法，显著降低了计算复杂度，建立了与统计力学的联系，并提出了向去极化噪声模型扩展的开放问题。

Abstract: We study exact decoding for the toric code and for planar and rotated surface codes under the standard independent \(X/Z\) noise model, focusing on Separate Minimum Weight (SMW) decoding and Separate Most Likely Coset (SMLC) decoding. For the SMW decoding problem, we show that an \(O(n^{3/2}\log n)\)-time decoder is achievable for surface and toric codes, improving over the \(O(n^{3}\log n)\) worst-case time of the standard approach based on complete decoding graphs. Our approach is based on a local reduction of SMW decoding to the minimum weight perfect matching problem using Fisher gadgets, which preserves planarity for planar and rotated surface codes and genus~\(1\) for the toric code. This reduction enables the use of Lipton--Tarjan planar separator methods and implies that SMW decoding lies in \(\mathrm{NC}\). For SMLC decoding, we show that the planar surface code admits an exact decoder with \(O(n^{3/2})\) algebraic complexity and that the problem lies in \(\mathrm{NC}\), improving over the \(O(n^{2})\) algebraic complexity of Bravyi \emph{et al.} Our approach proceeds via a dual-cycle formulation of coset probabilities and an explicit reduction to planar Pfaffian evaluation using Fisher--Kasteleyn--Temperley constructions. The same complexity measures apply to SMLC decoding of the rotated surface code. For the toric code, we obtain an exact polynomial-time SMLC decoder with \(O(n^{3})\) algebraic complexity. In addition, while the SMLC formulation is motivated by connections to statistical mechanics, we provide a purely algebraic derivation of the underlying duality based on MacWilliams duality and Fourier analysis. Finally, we discuss extensions of the framework to the depolarizing noise model and identify resulting open problems.

</details>


### [65] [A Novel Approach of Solving Polynomial Equations Over Binary Extension Fields](https://arxiv.org/abs/2601.01079)
*Leilei Yu,Yunghsiang S. Han,Pingping Li,Jiasheng Yuan*

Main category: cs.IT

TL;DR: 提出了一种统一的公式化方法，使用纯异或运算求解有限域GF(2^m)上的二次方程x²+x+c=0，适用于所有正整数m，无需区分m的奇偶性。


<details>
  <summary>Details</summary>
Motivation: 有限域上二次方程求解是代数编码理论的基础任务，也是计算三次和四次多项式根的关键子程序。现有公式化方法依赖大量指数运算或对m的奇偶性进行区分，限制了方法的统一性和效率。

Method: 利用Reed-Muller矩阵对评估进行表征，将问题简化为求解二元线性系统。该方法仅使用异或运算，无需指数运算或对m进行奇偶性区分。

Result: 该方法总成本最多为m²-2m+1次异或运算，在并行化下延迟仅为⌈log₂ m⌉次异或运算，适合低功耗、低延迟应用。

Conclusion: 提出了一种统一高效的公式化方法，使用纯异或运算求解GF(2^m)上的二次方程，适用于所有正整数m，在计算效率和实现简单性方面具有优势。

Abstract: Solving quadratic equations over finite fields is a fundamental task in algebraic coding theory and serves as a key subroutine for computing the roots of cubic and quartic polynomials. For the reduced quadratic polynomial $x^2+x+c\in \mathbb{F}_{2^m}[x]$, existing formula-based methods rely on heavy exponentiation or case distinctions on $m$ (odd/even or powers of two), which limits uniformity and efficiency. This paper presents a unified, formula-based solution for all positive integers $m$ that uses only exclusive-OR operations (XORs). The approach leverages a Reed-Muller matrix characterization of evaluations and reduces the problem to solving a binary linear system. The total cost is at most $m^2-2m+1$ XORs, and under parallelism, the latency is $\lceil \log_2 m\rceil$ XORs, making the method attractive for low-power, low-latency applications.

</details>


### [66] [Single-Shot and Few-Shot Decoding via Stabilizer Redundancy in Bivariate Bicycle Codes](https://arxiv.org/abs/2601.01137)
*Mohammad Rowshan*

Main category: cs.IT

TL;DR: 双变量自行车码(BB码)是量子LDPC码的重要类别，本文证明其容错性能由多项式g(z)决定，揭示了量子速率与稳定子冗余密度之间的严格等式关系，并发现了高量子速率会限制综合征距离的结构瓶颈。


<details>
  <summary>Details</summary>
Motivation: 虽然互质BB码的逻辑维度和量子距离已知由最大公约数多项式g(z)决定，但其在噪声测量下的容错特性仍不明确。需要理解g(z)如何影响稳定子冗余和单次解码所需的经典综合征码结构。

Method: 通过代数分析证明g(z)决定码的稳定子冗余和综合征码结构，推导量子速率与稳定子冗余密度的严格等式，提供类似BCH界限的单次测量误差容限界限，并构造具有改进综合征距离的小型互质BB码，使用BP+OSD进行评估。

Result: 发现量子速率与稳定子冗余密度之间存在严格等式关系，获得了单次测量误差容限的界限，构造的BB码显示出显著改善的综合征距离(d_S)，但分析揭示了结构瓶颈：在互质BB码框架内，高量子速率会限制综合征距离的上界。

Conclusion: g(z)多项式完全决定了BB码的稳定子冗余和单次解码结构，量子速率与稳定子冗余密度存在严格关系，高量子速率会限制单次解码性能。这些结果为下一代2BGA码在测量受限架构中的设计提供了具体的代数设计规则。

Abstract: Bivariate bicycle (BB) codes are a prominent class of quantum LDPC codes constructed from group algebras. While the logical dimension and quantum distance of \emph{coprime} BB codes are known to be determined by a greatest common divisor polynomial $g(z)$, the properties governing their fault tolerance under noisy measurement have remained implicit. In this work, we prove that this same polynomial $g(z)$ dictates the code's stabilizer redundancy and the structure of the classical \emph{syndrome codes} required for single-shot decoding. We derive a strict equality between the quantum rate and the stabilizer redundancy density, and we provide BCH-like bounds on the achievable single-shot measurement error tolerance. Guided by this framework, we construct small coprime BB codes with significantly improved syndrome distance ($d_S$) and evaluate them using BP+OSD. Our analysis reveals a structural bottleneck: within the coprime BB ansatz, high quantum rate imposes an upper bound on syndrome distance, limiting single-shot performance. These results provide concrete algebraic design rules for next-generation 2BGA codes in measurement-limited architectures.

</details>


### [67] [On the Structure of the Optimal Detector for Sub-THz Multi-Hop Relays with Unknown Prior: Over-the-Air Diffusion](https://arxiv.org/abs/2601.01194)
*Ozgur Ercetin,Mohaned Chraiti*

Main category: cs.IT

TL;DR: 提出AF-DDIM框架，将AF中继链解释为方差保持扩散过程，利用DDIM进行信号恢复，实现无需每跳CSI的近最优贝叶斯解码。


<details>
  <summary>Details</summary>
Motivation: AF中继能扩展sub-THz链路覆盖，但会传播噪声导致累积退化。在非高斯输入分布下，最优解码具有挑战性，且不清楚中继是否需要CSI和噪声统计信息。

Method: 将AF中继链解释为方差保持扩散过程，每跳等效于扩散步骤。整个多跳链可简化为仅由三个实标量描述的等效高斯信道。接收端利用DDIM去噪器进行信号恢复，无需每跳CSI。

Result: 仿真显示AF-DDIM解码器在AWGN和莱斯衰落下能降低MSE、SER和BER，特别是在中等SNR和高阶调制下表现更优。

Conclusion: 建立了该等效性的信息论基础，证明解码性能仅取决于最终有效SNR，与中间噪声/信道分配或先验分布无关，为AF中继系统提供了新的解码框架。

Abstract: Amplify and forward (AF) relaying is a viable strategy to extend the coverage of sub-terahertz (sub-THz) links, but inevitably propagates noise, leading to cumulative degradation across multiple hops. At the receiver, optimal decoding is desirable, yet challenging under non-Gaussian input distributions (video, voice, etc), for which neither the Minimum Mean Square Error (MMSE) estimator nor the mutual information admits a closed form. A further open question is whether knowledge of Channel State Information (CSI) and noise statistics at the intermediate relays is necessary for optimal detection. Aiming for an optimal decoder, this paper introduces a new framework that interprets the AF relay chain as a variance-preserving diffusion process and employs denoising diffusion implicit models (DDIMs) for signal recovery. We show that each AF hop is mathematically equivalent to a diffusion step with hop-dependent attenuation and noise injection. Consequently, the entire multi-hop chain collapses to an equivalent Gaussian channel fully described by only three real scalars per block: the cumulative complex gain and the effective noise variance. At the receiver, these end-to-end sufficient statistics define a matched reverse schedule that guides the DDIM-based denoiser, enabling near-optimal Bayesian decoding without per-hop CSI. We establish the information-theoretic foundation of this equivalence, proving that decoding performance depends solely on the final effective Signal-to-Noise-Ratio (SNR), regardless of intermediate noise/channel allocation or prior distribution. Simulations under AWGN and Rician fading confirm that the proposed AF-DDIM decoder reduces mean-squared error, symbol error rate, and bit error rate, particularly at moderate SNRs and for higher-order constellations.

</details>


### [68] [Probabilistic verification algorithm for linear codes](https://arxiv.org/abs/2601.01372)
*Mingchao Li,Jiyou Li*

Main category: cs.IT

TL;DR: 提出一种适用于任意线性码的随机算法，用于判断给定向量是否属于该码，时间复杂度O(n log n)，空间复杂度O(n²)，错误概率小于1/poly(n)


<details>
  <summary>Details</summary>
Motivation: 需要高效判断向量是否属于线性码的算法，传统方法可能计算复杂度高，需要开发更快的概率算法

Method: 提出概率算法，适用于任意线性码，通过随机化技术降低计算复杂度，利用多项式时间近似实现高效判断

Result: 算法达到O(n log n)时间复杂度，O(n²)空间复杂度，错误概率小于1/poly(n)，在渐进意义下表现优异

Conclusion: 该概率算法为线性码成员判定问题提供了高效解决方案，在计算复杂度方面有显著改进

Abstract: In this paper, we propose a probabilistic algorithm suitable for any linear code $C$ to determine whether a given vector $\mathbf{x}$ belongs to $ C$. The algorithm achieves $O(n\log n)$ time complexity, $ O(n^2)$ space complexity and with an error probability less than $1/\mathrm{poly}(n)$ in the asymptotic sense.

</details>


### [69] [Edge grouping using methods in Algorithmic Information Theory](https://arxiv.org/abs/2601.01760)
*Gabriel Potestades*

Main category: cs.IT

TL;DR: 该论文探讨了如何利用算法信息理论中的块分解方法和边扰动技术来分析图结构中子图之间的连接边，以验证算法信息理论能否作为理解和测量图复杂性的理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前科学界对复杂系统的研究日益重视，但缺乏既理论严谨又计算实用的复杂性测量框架。算法信息理论在定义复杂性方面取得进展，但需要验证其是否能有效分析图结构中的子结构，为复杂性测量提供理论基础。

Method: 采用Zenil等人2018年提出的块分解方法来近似计算图的邻接矩阵复杂性，并结合边扰动技术。通过图的顶点置换的整个对称群以及称为自同构子集的特殊子集，穷尽地确定连接图中两个子图的边。分析边是否会在平均算法信息贡献方面更接近其所属的子图。

Result: 论文通过分析边与子图在算法信息贡献上的关联性，验证了算法信息理论在识别图子结构方面的有效性。边扰动和块分解方法的结合能够有效识别连接子图的边，并显示这些边在信息贡献上更倾向于其所属的子图。

Conclusion: 算法信息理论可以作为理解和分析图结构中子结构的可行理论框架，为建立复杂性测量和分析的通用方法提供了理论基础。该方法在识别图子结构方面具有潜力，但仍需进一步研究来完善复杂性测量的实用框架。

Abstract: Understanding natural phenomenon through the interactions of different complex systems has become an increasing focus in scientific inquiry. Defining complexity and actually measuring it is an ongoing debate and no standard framework has been established that is both theoretically sound and computationally practical to use. Currently, one of the fields which attempts to formally define complexity is in the realm of Algorithmic Information Theory. The field has shown advances by studying the outputs of 1-dimensional and 2-dimensional Turing machines to determine the complexity values of binary strings and 2-dimensional binary matrices respectively. Using these complexity values, an algorithm called the Block Decomposition Method developed by Zenil, et al. in 2018, has been created to approximate the complexity of adjacency matrices of graphs which has found relative success in grouping graphs based on their complexity values. We use this method along with another method called edge perturbation to exhaustively determine if an edge can be identified to connect two sub-graphs within a graph using the entire symmetric group of its vertices permutation and via unique permutations we call automorphic subsets, which is a special subset of the symmetric group. We also analyze if edges will be grouped closer to their respective sub-graphs in terms of the average algorithmic information contribution. This analysis has been done in order to ascertain if Algorithmic Information Theory can be a viable theory in understanding substructures within graphs and ultimately as a foundation to create frameworks of measuring and analyzing complexity.

</details>


### [70] [Information Gradient for Directed Acyclic Graphs: A Score-based Framework for End-to-End Mutual Information Maximization](https://arxiv.org/abs/2601.01789)
*Tadashi Wadayama*

Main category: cs.IT

TL;DR: 提出一个基于随机有向无环图的端到端互信息最大化通用框架，利用边际和条件得分函数推导统一的信息梯度公式，可通过自动微分高效计算，并扩展到数字孪生校准新范式。


<details>
  <summary>Details</summary>
Motivation: 通信和感知系统中端到端互信息最大化是一个重要但具有挑战性的问题，特别是在复杂网络结构和全局资源约束下。现有方法难以处理随机有向无环图表示的复杂系统，需要一种通用且高效的优化框架。

Method: 提出基于随机有向无环图的通用框架，利用边际和条件得分函数推导互信息梯度统一公式，通过向量-雅可比积在自动微分框架中高效计算梯度，支持去噪得分匹配学习得分函数，并扩展到费希尔散度最小化的数字孪生校准。

Result: 在线性多径DAG和非线性信道上的数值实验验证了框架有效性：基于去噪得分匹配学习的得分函数能够准确复现真实梯度，成功最大化端到端互信息。框架还实现了数字孪生校准的新无监督范式。

Conclusion: 该框架为通信和感知系统中的互信息最大化提供了通用且高效的解决方案，能够处理复杂网络结构和全局约束，并开辟了数字孪生校准等新应用方向，具有重要的理论和实践意义。

Abstract: This paper presents a general framework for end-to-end mutual information maximization in communication and sensing systems represented by stochastic directed acyclic graphs (DAGs). We derive a unified formula for the (mutual) information gradient with respect to arbitrary internal parameters, utilizing marginal and conditional score functions. We demonstrate that this gradient can be efficiently computed using vector-Jacobian products (VJP) within standard automatic differentiation frameworks, enabling the optimization of complex networks under global resource constraints. Numerical experiments on both linear multipath DAGs and nonlinear channels validate the proposed framework; the results confirm that the estimator, utilizing score functions learned via denoising score matching, accurately reproduces ground-truth gradients and successfully maximizes end-to-end mutual information. Beyond maximization, we extend our score-based framework to a novel unsupervised paradigm: digital twin calibration via Fisher divergence minimization.

</details>


### [71] [Information Flow in geophysical systems](https://arxiv.org/abs/2601.01795)
*Peter Jan van Leeuwen*

Main category: cs.IT

TL;DR: 提出分析地球物理系统中信息演化的新框架，应用于Kuramoto-Sivashinsky模型和浅水方程，发现信息可逆流传播，不同变量展现不同的信息传播模式。


<details>
  <summary>Details</summary>
Motivation: 理解信息和不确定性如何传播对可预测性研究至关重要，对预报不确定性量化和风险管理有重要应用价值，同时能提供对系统底层物理的深入洞察。信息传播与因果关系密切相关，涉及系统各部分如何相互影响以及某些区域如何保持动态隔离。

Method: 提出分析信息演化的新框架，应用于一维高度非线性的Kuramoto-Sivashinsky模型和代表中纬度大气带的浅水方程。该框架关注信息传播与流体流动的关系，以及不同模型变量的信息演化模式。

Result: 观察到信息可以逆着流体流动方向传播，不同模型变量展现出不同的信息演化模式。例如，压力相关信息与相对涡度信息的传播方式不同，这反映了重力波与平衡流动力学的影响差异。

Conclusion: 这一新框架为研究复杂动力系统提供了有前景的诊断工具补充，能够揭示信息传播的独特特征及其与系统物理过程的联系。

Abstract: We present a new framework for analyzing the evolution of information in geophysical systems. Understanding how information, and its counterpart, uncertainty, propagates is central to predictability studies and has significant implications for applications such as forecast uncertainty quantification and risk management. It also offers valuable insight into the underlying physics of the system. Information propagation is closely linked to causality: how one part of a system influences another, and how some regions remain dynamically isolated. We apply this framework to the one-dimensional, highly nonlinear Kuramoto-Sivashinsky model and to the shallow-water equations, representing a mid-latitude atmospheric strip. Notably, we observe that information can propagate against the fluid flow, and that different model variables exhibit distinct patterns of information evolution. For example, pressure-related information propagates differently from relative vorticity, reflecting the influence of gravity waves versus balanced flow dynamics. This new framework offers a promising addition to the diagnostic tools available for studying complex dynamical systems.

</details>


### [72] [Information Geometry of Imaging Operators](https://arxiv.org/abs/2601.02111)
*Charles Wood*

Main category: cs.IT

TL;DR: 该论文基于成像算子的归一化奇异谱，构建了一个最小几何结构，通过将谱等价类映射到概率单纯形上并赋予Fisher-Rao信息度量，获得了一个具有恒定正曲率的黎曼几何。


<details>
  <summary>Details</summary>
Motivation: 成像系统通常表示为线性算子，其奇异值谱描述了算子层面可恢复的结构。作者希望建立一个抽象的、算子层面的几何框架，为后续分析成像流程中的信息流动和约束提供固定的几何背景。

Method: 基于算子信息论框架，将归一化奇异谱的等价类识别为概率单纯形上的点，并在该空间上装备Fisher-Rao信息度量，从而获得一个在酉变换和全局缩放下不变的黎曼几何结构。

Result: 构建的几何结构具有闭式表达的距离和测地线，且具有恒定正曲率。在特定限制下，组合运算通过秩约束强制边界面，并在对齐模型中诱导谱状态的非线性重加权。Fisher-Rao距离仅在谱均匀情况下保持不变。

Conclusion: 该工作提供了一个抽象的、算子层面的几何构造，不引入优化原则、随机模型或模态特定假设，旨在为成像流程中的信息流和约束分析提供固定的几何背景。

Abstract: Imaging systems are represented as linear operators, and their singular value spectra describe the structure recoverable at the operator level. Building on an operator-based information-theoretic framework, this paper introduces a minimal geometric structure induced by the normalised singular spectra of imaging operators. By identifying spectral equivalence classes with points on a probability simplex, and equipping this space with the Fisher--Rao information metric, a well-defined Riemannian geometry can be obtained that is invariant under unitary transformations and global rescaling. The resulting geometry admits closed-form expressions for distances and geodesics, and has constant positive curvature. Under explicit restrictions, composition enforces boundary faces through rank constraints and, in an aligned model with stated idealisations, induces a non-linear re-weighting of spectral states. Fisher--Rao distances are preserved only in the spectrally uniform case. The construction is abstract and operator-level, introducing no optimisation principles, stochastic models, or modality-specific assumptions. It is intended to provide a fixed geometric background for subsequent analysis of information flow and constraints in imaging pipelines.

</details>


### [73] [Single- and Multi-Objective Stochastic Optimization for Next-Generation Networks in the Generative AI and Quantum Computing Era](https://arxiv.org/abs/2601.02175)
*Trinh Van Chien,Bui Trong Duc,Nguyen Xuan Tung,Van Duc Nguyen,Waqas Khalid,Symeon Chatzinotas,Lajos Hanzo*

Main category: cs.IT

TL;DR: 这篇综述论文探讨了随机优化算法在下一代网络中的应用，特别关注6G网络，分析了八项关键问题并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 下一代网络（特别是6G）需要处理大规模部署和高密度网络，传统优化方法在真实场景中计算复杂度高且依赖精确模型，而随机优化算法能适应高密度和网络可扩展性，但目前相关研究有限。

Method: 通过综述研究，首先详细概述随机优化技术和下一代网络，涵盖从单目标到多目标信号处理的基础方法；然后探索不同算法如何解决下一代网络挑战；最后分析当前研究挑战并提出新方向。

Result: 论文系统性地分析了随机优化与下一代网络的关系，提出了八个涉及背景、关键特征和经验教训的开放性问题，为随机优化在6G网络中的应用提供了全面框架。

Conclusion: 随机优化是优化下一代无线网络的有力解决方案，能够在大规模场景中收敛于可行时间范围内，但需要更多研究来应对6G网络的复杂挑战，本文为此提供了系统性指导。

Abstract: Next Generation (NG) networks move beyond simply connecting devices to creating an ecosystem of connected intelligence, especially with the support of generative Artificial Intelligence (AI) and quantum computation. These systems are expected to handle large-scale deployments and high-density networks with diverse functionalities. As a result, there is an increasing demand for efficient and intelligent algorithms that can operate under uncertainty from both propagation environments and networking systems. Traditional optimization methods often depend on accurate theoretical models of data transmission, but in real-world NG scenarios, they suffer from high computational complexity in large-scale settings. Stochastic Optimization (SO) algorithms, designed to accommodate extremely high density and extensive network scalability, have emerged as a powerful solution for optimizing wireless networks. This includes various categories that range from model-based approaches to learning-based approaches. These techniques are capable of converging within a feasible time frame while addressing complex, large-scale optimization problems. However, there is currently limited research on SO applied for NG networks, especially the upcoming Sixth-Generation (6G). In this survey, we emphasize the relationship between NG systems and SO by eight open questions involving the background, key features, and lesson learned. Overall, our study starts by providing a detailed overview of both areas, covering fundamental and widely used SO techniques, spanning from single to multi-objective signal processing. Next, we explore how different algorithms can solve NG challenges, such as load balancing, optimizing energy efficiency, improving spectral efficiency, or handling multiple performance trade-offs. Lastly, we highlight the challenges in the current research and propose new directions for future studies.

</details>


### [74] [Generative Site-Specific Beamforming for Next-Generation Spatial Intelligence](https://arxiv.org/abs/2601.02301)
*Zhaolin Wang,Zihao Zhou,Cheng-Jie Zhao,Yuanwei Liu*

Main category: cs.IT

TL;DR: 本文提出生成式站点特定波束成形（GenSSBF），利用条件生成模型学习可行波束成形器的条件分布，以解决传统基于判别式深度学习的SSBF在多模态传播表示和波束结构特征捕捉方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统基于判别式深度学习的站点特定波束成形（SSBF）存在两个主要问题：1）难以恰当表示无线传播固有的多模态特性；2）无法有效捕捉波束成形器的结构特征。这些问题限制了SSBF在下一代无线网络空间智能中的应用。

Method: 提出生成式站点特定波束成形（GenSSBF），采用条件生成模型学习可行波束成形器的条件分布。该方法能够从粗糙的信道感知测量中合成多样且高保真的波束候选方案，包括基础原理、系统设计和实现方法。

Result: 室内和室外场景的案例研究表明，GenSSBF能够以极低的信道获取开销实现接近最优的波束成形增益。

Conclusion: GenSSBF通过条件生成模型有效解决了传统SSBF的局限性，为下一代无线网络空间智能提供了有前景的解决方案，但仍有一些开放研究问题需要进一步探索。

Abstract: This article proposes generative site-specific beamforming (GenSSBF) for next-generation spatial intelligence in wireless networks. Site-specific beamforming (SSBF) has emerged as a promising paradigm to mitigate the channel acquisition bottleneck in multiantenna systems by exploiting environmental priors. However, classical SSBF based on discriminative deep learning struggles: 1) to properly represent the inherent multimodality of wireless propagation and 2) to effectively capture the structural features of beamformers. In contrast, by leveraging conditional generative models, GenSSBF addresses these issues via learning a conditional distribution over feasible beamformers. By doing so, the synthesis of diverse and high-fidelity beam candidates from coarse channel sensing measurements can be guaranteed. This article presents the fundamentals, system designs, and implementation methods of GenSSBF. Case studies in both indoor and outdoor scenarios show that GenSSBF attains near-optimal beamforming gain with ultra-low channel acquisition overhead. Finally, several open research problems are highlighted.

</details>


### [75] [Error-Building Decoding of Linear Block Codes](https://arxiv.org/abs/2601.02330)
*Guoda Qiu,Ling Liu,Yuejun Wei,Liping Li*

Main category: cs.IT

TL;DR: 提出一种基于最大似然的软判决解码框架EBD，仅需校验矩阵即可完成解码，无需预构建信息，并通过递归定理高效构建最优错误块。


<details>
  <summary>Details</summary>
Motivation: 传统软判决解码方法需要预构建网格图或错误模式列表等额外信息，计算复杂度高。本文旨在开发一种仅依赖校验矩阵的通用解码框架，降低复杂度同时保持最大似然性能。

Method: 提出错误构建解码(EBD)框架：1) 定义错误构建块；2) 推导递归定理，从小块高效构建更大的局部最优块；3) 针对扩展汉明码优化，通过离线和在线排除机制大幅降低复杂度。

Result: 对于长度为64、128和256的扩展汉明码，在帧错误率10^-3时，完全优化的EBD比最小边网格Viterbi解码平均减少约一个数量级的浮点运算。

Conclusion: EBD是一种仅需校验矩阵的通用最大似然软判决解码框架，通过递归构建和优化机制显著降低复杂度，在扩展汉明码上验证了其有效性。

Abstract: This paper proposes a novel maximum-likelihood (ML) soft-decision decoding framework for linear block codes, termed error-building decoding (EBD). The complete decoding process can be performed using only the parity-check matrix, without requiring any other pre-constructed information (such as trellis diagrams or error-pattern lists), and it can also be customized by exploiting the algebraic properties of the code. We formally define error-building blocks, and derive a recursive theorem that allows efficient construction of larger locally optimal blocks from smaller ones, thereby effectively searching for the block associated with the most likely error pattern. The EBD framework is further optimized for extended Hamming codes as an example, through offline and online exclusion mechanisms, leading to a substantial complexity reduction without loss of ML performance. Complexity analysis shows that, for extended Hamming codes of lengths 64, 128, and 256, the fully optimized EBD requires approximately an order of magnitude fewer floating-point operations on average than minimum-edge trellis Viterbi decoding at a frame error rate of $10^{-3}$.

</details>
