<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 11]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.IT](#cs.IT) [Total: 6]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Task Offloading and Resource Allocation for MEC-assisted Consumer Internet of Vehicle Systems](https://arxiv.org/abs/2508.15795)
*Yanheng Liu,Dalin Li,Hao Wu,Zemin Sun,Weihong Qin,Jun Li,Hongyang Du,Geng Sun*

Main category: cs.NI

TL;DR: 本文研究MEC辅助车联网中的AI任务出置和资源分配问题，通过MADDPG算法设计了联合优化方案JTOCRA，以实现系统成本最小化。


<details>
  <summary>Details</summary>
Motivation: 车联网中计算敏感和计算密集型需求导致了资源供给与严格计算要求间的差异，动态环境下的实时处理和高效资源管理靠战。

Method: 首先提出多MEC辅助的车联网架构，然后构建以服务延迟和能消耗为目标的系统成本最小化优化问题，最后采用多自然人深度确定性策略梯度算法(MADDPG)设计联合任务出置和计算资源分配方案JTOCRA。

Result: 模拟结果表明，所提出的JTOCRA方案能够获得更优异的系统性能，并且与其他替代方案相比具有更好的可扩展性。

Conclusion: 该研究通过AI技术有效解决了MEC辅助车联网中的任务出置和资源分配挑战，为动态环境下的实时计算服务提供了高效解决方案。

Abstract: Mobile edge computing (MEC)-assisted internet of vehicle (IoV) is emerging as
a promising paradigm to provide computing services for vehicles. However,
meeting the computing-sensitive and computation-intensive demands of vehicles
poses several challenges, including the discrepancy between the limited
resource provision and stringent computing requirement, the difficulty in
capturing and integrating the intricate features of the MEC-assisted IoV system
into the problem formulation, and the need for real-time processing and
efficient resource management in the dynamic environment. In this work, we
explore the AI-enabled task offloading and resource allocation for MEC-assisted
consumer IoV systems. Specifically, we first present a multi-MEC-assisted
consumer IoV architecture that leverages the computational resources of MEC
servers to provide offloading services close to vehicles. Subsequently, we
formulate a system cost minimization optimization problem (SCMOP) by
integrating the service delay and energy consumption. To efficiently solve this
problem, we design a joint task offloading and computing resource allocation
approach (JTOCRA) by applying the multi-agent deep deterministic policy
gradient (MADDPG) algorithm. Finally, simulation results demonstrate that the
proposed JTOCRA can achieve superior system performances and exhibits better
scalability compared to other alternative approaches.

</details>


### [2] [Better Together: Leveraging Multiple Digital Twins for Deployment Optimization of Airborne Base Stations](https://arxiv.org/abs/2508.15816)
*Mauro Belgiovine,Chris Dick,Kaushik Chowdhury*

Main category: cs.NI

TL;DR: 这篇论文提出了一种基于数字双生的方法，通过结合NVIDIA Sionna和Aerial Omniverse两个平台来优化空中基站置位，提高网络覆盖效率和弹性。


<details>
  <summary>Details</summary>
Motivation: 空中基站(ABS)可以灵活分配网络资源和在灾难时提供连接，但无人机飞行时间有限，需要一种高效的位置优化方法避免测试浪费。

Method: 实现了两个开源数字双生平台的软件桥接，设计了基于反向传播的算法来优化无人机位置、天线方向和发射功率，并在大规模网络场景中进行数值评估。

Result: 在50个用户设备和10个空中基站的大规模场景下，识别了环境条件对两个数字双生平台性能结果一致性的影响，并提出了一种可以为关键设备提供一致覆盖的弹性机制。

Conclusion: 该方法通过数字双生技术有效地优化了空中基站的置位配置，提高了网络覆盖效率和可靠性，为灾难恢复和动态负荷分配提供了重要技术支撑。

Abstract: Airborne Base Stations (ABSs) allow for flexible geographical allocation of
network resources with dynamically changing load as well as rapid deployment of
alternate connectivity solutions during natural disasters. Since the radio
infrastructure is carried by unmanned aerial vehicles (UAVs) with limited
flight time, it is important to establish the best location for the ABS without
exhaustive field trials. This paper proposes a digital twin (DT)-guided
approach to achieve this through the following key contributions: (i)
Implementation of an interactive software bridge between two open-source DTs
such that the same scene is evaluated with high fidelity across NVIDIA's Sionna
and Aerial Omniverse Digital Twin (AODT), highlighting the unique features of
each of these platforms for this allocation problem, (ii) Design of a
back-propagation-based algorithm in Sionna for rapidly converging on the
physical location of the UAVs, orientation of the antennas and transmit power
to ensure efficient coverage across the swarm of the UAVs, and (iii) numerical
evaluation in AODT for large network scenarios (50 UEs, 10 ABS) that identifies
the environmental conditions in which there is agreement or divergence of
performance results between these twins. Finally, (iv) we propose a resilience
mechanism to provide consistent coverage to mission-critical devices and
demonstrate a use case for bi-directional flow of information between the two
DTs.

</details>


### [3] [Agent Communications toward Agentic AI at Edge -- A Case Study of the Agent2Agent Protocol](https://arxiv.org/abs/2508.15819)
*Qiang Duan,Zhihui Lu*

Main category: cs.NI

TL;DR: 本文评估了当前代理通信协议（特别是A2A协议）在边缘计算环境中的适用性，分析了边缘计算带来的特殊挑战，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着边缘智能的兴起，需要在网络边缘部署代理AI，但现有的代理通信协议设计时未充分考虑边缘计算的特殊挑战，其有效性在边缘环境中尚未得到充分验证。

Method: 首先讨论代理通信的核心功能，展示代理通信协议的概况，识别边缘计算带来的主要挑战；然后以A2A协议为代表性案例进行研究，分析协议中采用的关键技术对满足边缘计算需求的效力。

Result: 通过评估发现了当前代理通信技术在应对边缘计算挑战方面存在的问题和局限性。

Conclusion: 基于评估获得的见解，识别了当前代理通信技术的开放性问题，并讨论了未来研究解决这些问题的方向。

Abstract: The current evolution of artificial intelligence introduces a paradigm shift
toward agentic AI built upon multi-agent systems (MAS). Agent communications
serve as a key to effective agent interactions in MAS and thus have a
significant impact on the performance of agentic AI applications. The recent
research on agent communications has made exciting rapid progress that leads to
a variety of protocol designs, among which the Agent2Agent (A2A) protocol is
considered the most representative one. Simultaneously, the rise of edge
intelligence is expected to enable agentic AI at the network edge. However, the
current agent communication protocols are designed without sufficient
consideration of the special challenges of edge computing, and their
effectiveness in the edge environment is largely unexamined. In this paper, we
attempt to assess the abilities of agent communication technologies to face the
challenges of edge computing using the A2A protocol as a representative case.
We first discuss the core functionalities of agent communications, present a
landscape of agent communication protocols, and identify the main challenges
introduced by edge computing. Then, we conduct a case study on the A2A protocol
to examine the key technologies leveraged in the protocol for their
effectiveness in meeting the requirements of agent communications in edge
computing. Based on the insights obtained from this assessment, we identify
open issues in the current agent communication technologies and discuss
directions for future research to address these issues.

</details>


### [4] [Towards Integrated Energy-Communication-Transportation Hub: A Base-Station-Centric Design in 5G and Beyond](https://arxiv.org/abs/2508.15833)
*Linfeng Shen,Guanzhen Wu,Cong Zhang,Xiaoyi Fan,Jiangchuan Liu*

Main category: cs.NI

TL;DR: 基站维护成本高，通过5G基站组网与电动汽车充电基础设施协同，利用深度强化学习调度销余电力，降低运营成本并通过EV充电获取收益。


<details>
  <summary>Details</summary>
Motivation: 5G基站部署带来巨大能耗，业界希望通过能源存储系统在谷底时存储多余电力，降低成本并参与需求响应。电池技术发展为基站资源利用开启新可能性。

Method: 探索基站分布与EV充电基础设施的重合区，构建ECT-Hub模型。采用激励机制设置充电价格，并使用深度强化学习进行电池调度。考虑基站流量条件、天气和EV充电行为等因素。

Result: 实验结果证明ECT-Hub能够有效优化多余能源利用，显著降低运营成本，尤其是通过产生收益的EV充电服务。

Conclusion: 基站在智能化集成能源-通信-交通基础设施中可以发挥关键作用。通过基站电池为EV提供充电服务的模式可行且效果显著，为下一代智能基础设施的发展提供了新思路。

Abstract: The rise of 5G communication has transformed the telecom industry for
critical applications. With the widespread deployment of 5G base stations comes
a significant concern about energy consumption. Key industrial players have
recently shown strong interest in incorporating energy storage systems to store
excess energy during off-peak hours, reducing costs and participating in demand
response. The fast development of batteries opens up new possibilities, such as
the transportation area. An effective method is needed to maximize base station
battery utilization and reduce operating costs. In this trend towards
next-generation smart and integrated energy-communication-transportation (ECT)
infrastructure, base stations are believed to play a key role as service hubs.
By exploring the overlap between base station distribution and electric vehicle
charging infrastructure, we demonstrate the feasibility of efficiently charging
EVs using base station batteries and renewable power plants at the Hub. Our
model considers various factors, including base station traffic conditions,
weather, and EV charging behavior. This paper introduces an incentive mechanism
for setting charging prices and employs a deep reinforcement learning-based
method for battery scheduling. Experimental results demonstrate the
effectiveness of our proposed ECT-Hub in optimizing surplus energy utilization
and reducing operating costs, particularly through revenue-generating EV
charging.

</details>


### [5] [Safeguarding ISAC Performance in Low-Altitude Wireless Networks Under Channel Access Attack](https://arxiv.org/abs/2508.15838)
*Jiacheng Wang,Jialing He,Geng Sun,Zehui Xiong,Dusit Niyato,Shiwen Mao,Dong In Kim,Tao Xiang*

Main category: cs.NI

TL;DR: 本文针对低空无线网络中的恶意信道接入攻击问题，提出了基于Stackelberg博弈的框架来优化集成感知与通信性能，通过逆向归纳算法实现均衡并证明了均衡的存在性和唯一性。


<details>
  <summary>Details</summary>
Motivation: 随着地面资源日益饱和，低空应用如空中出租车得到发展，但低空无线网络的开放性使其易受恶意信道接入攻击，影响集成感知与通信性能。

Method: 首先推导受攻击条件下通信数据的信干噪比和感知数据的信息年龄作为服务质量指标，然后将ISAC性能优化问题建模为Stackelberg博弈，设计逆向归纳算法实现Stackelberg均衡。

Result: 仿真结果表明，所提算法优于现有基准和静态纳什均衡基准，能够确保低空无线网络为低空应用提供可靠服务。

Conclusion: 提出的博弈框架和算法有效缓解了攻击对低空无线网络ISAC性能的退化影响，证明了均衡的存在性和唯一性，为低空应用提供了可靠的服务保障。

Abstract: The increasing saturation of terrestrial resources has driven the exploration
of low-altitude applications such as air taxis. Low altitude wireless networks
(LAWNs) serve as the foundation for these applications, and integrated sensing
and communication (ISAC) constitutes one of the core technologies within LAWNs.
However, the openness nature of low-altitude airspace makes LAWNs vulnerable to
malicious channel access attacks, which degrade the ISAC performance.
Therefore, this paper develops a game-based framework to mitigate the influence
of the attacks on LAWNs. Concretely, we first derive expressions of
communication data's signal-to-interference-plus-noise ratio and the age of
information of sensing data under attack conditions, which serve as quality of
service metrics. Then, we formulate the ISAC performance optimization problem
as a Stackelberg game, where the attacker acts as the leader, and the
legitimate drone and the ground ISAC base station act as second and first
followers, respectively. On this basis, we design a backward induction
algorithm that achieves the Stackelberg equilibrium while maximizing the
utilities of all participants, thereby mitigating the attack-induced
degradation of ISAC performance in LAWNs. We further prove the existence and
uniqueness of the equilibrium. Simulation results show that the proposed
algorithm outperforms existing baselines and a static Nash equilibrium
benchmark, ensuring that LAWNs can provide reliable service for low-altitude
applications.

</details>


### [6] [xDiff: Online Diffusion Model for Collaborative Inter-Cell Interference Management in 5G O-RAN](https://arxiv.org/abs/2508.15843)
*Peihao Yan,Huacheng Zeng,Y. Thomas Hou*

Main category: cs.NI

TL;DR: xDiff是一个基于扩散模型的强化学习框架，用于O-RAN网络中的小区间干扰管理，通过扩散模型实现近实时策略生成，在5G测试床上验证了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: O-RAN是5G及未来蜂窝网络的关键架构范式，需要智能高效的资源管理解决方案。扩散模型在图像和视频生成方面表现出色，有潜力用于网络优化任务。

Method: 将小区间干扰管理建模为资源分配优化问题，通过将扩散模型集成到强化学习框架中实现在线学习，引入偏好值作为策略表示，在O-RAN分布式单元中实现策略引导的资源分配。

Result: 在由三个小区和一组智能手机组成的5G测试床上进行实验，xDiff在两种小小区场景下均优于最先进的小区间干扰管理方法。

Conclusion: xDiff展示了扩散模型在O-RAN在线优化中的潜力，为网络资源管理提供了新的解决方案。

Abstract: Open Radio Access Network (O-RAN) is a key architectural paradigm for 5G and
beyond cellular networks, enabling the adoption of intelligent and efficient
resource management solutions. Meanwhile, diffusion models have demonstrated
remarkable capabilities in image and video generation, making them attractive
for network optimization tasks. In this paper, we propose xDiff, a
diffusion-based reinforcement learning(RL) framework for inter-cell
interference management (ICIM) in O-RAN. We first formulate ICIM as a resource
allocation optimization problem aimed at maximizing a user-defined reward
function and then develop an online learning solution by integrating a
diffusion model into an RL framework for near-real-time policy generation.
Particularly, we introduce a novel metric, preference values, as the policy
representation to enable efficient policy-guided resource allocation within
O-RAN distributed units (DUs). We implement xDiff on a 5G testbed consisting of
three cells and a set of smartphones in two small-cell scenarios. Experimental
results demonstrate that xDiff outperforms state-of-the-art ICIM approaches,
highlighting the potential of diffusion models for online optimization of
O-RAN. Source code is available on GitHub [1].

</details>


### [7] [Time Series Based Network Intrusion Detection using MTF-Aided Transformer](https://arxiv.org/abs/2508.16035)
*Poorvi Joshi,Mohan Gurusamy*

Main category: cs.NI

TL;DR: 提出了一种基于马尔可夫转移场(MTF)辅助Transformer的时间序列分类新方法，专门用于软件定义网络(SDN)，在数据受限环境下表现优异


<details>
  <summary>Details</summary>
Motivation: 解决SDN中时间序列分类的挑战，特别是在数据稀疏环境下传统方法性能受限的问题

Method: 将MTF的时间依赖性建模能力与Transformer的复杂模式识别能力相结合，构建MTF辅助的Transformer模型

Result: 在InSDN数据集上超越基线分类模型，在数据受限环境中表现尤其突出，同时保持竞争力的训练和推理时间

Conclusion: MTF辅助Transformer为解决SDN中时间序列分类问题提供了有前景的解决方案，特别是在数据稀疏场景下具有可靠和可扩展的分析能力

Abstract: This paper introduces a novel approach to time series classification using a
Markov Transition Field (MTF)-aided Transformer model, specifically designed
for Software-Defined Networks (SDNs). The proposed model integrates the
temporal dependency modeling strengths of MTFs with the sophisticated pattern
recognition capabilities of Transformer architectures. We evaluate the model's
performance using the InSDN dataset, demonstrating that our model outperforms
baseline classification models, particularly in data-constrained environments
commonly encountered in SDN applications. We also highlight the relationship
between the MTF and Transformer components, which leads to better performance,
even with limited data. Furthermore, our approach achieves competitive training
and inference times, making it an efficient solution for real-world SDN
applications. These findings establish the potential of MTF-aided Transformers
to address the challenges of time series classification in SDNs, offering a
promising path for reliable and scalable analysis in scenarios with sparse
data.

</details>


### [8] [Congestion Control System Optimization with Large Language Models](https://arxiv.org/abs/2508.16074)
*Zhiyuan He,Aashish Gottipati,Lili Qiu,Yuqing Yang,Francis Y. Yan*

Main category: cs.NI

TL;DR: 使用大语言模型自动优化拥塞控制算法，在QUIC实现中比原始BBR算法性能提升高达27%


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量研究，现有拥塞控制算法在不同网络环境中仍表现不佳，需要新的优化方法

Method: 基于大语言模型的算法生成框架，包含结构化算法生成过程、覆盖广泛网络条件的仿真评估管道，以及统计指导的评估时间缩减方法

Result: 四个不同大语言模型的实证结果验证了方法的有效性，在QUIC实现中识别出比原始BBR算法性能提升高达27%的算法

Conclusion: 这项工作展示了大语言模型在加速高性能网络算法设计方面的潜力，为网络系统中更广泛的应用铺平了道路

Abstract: Congestion control is a fundamental component of Internet infrastructure, and
researchers have dedicated considerable effort to developing improved
congestion control algorithms. However, despite extensive study, existing
algorithms continue to exhibit suboptimal performance across diverse network
environments. In this paper, we introduce a novel approach that automatically
optimizes congestion control algorithms using large language models (LLMs). Our
framework consists of a structured algorithm generation process, an
emulation-based evaluation pipeline covering a broad range of network
conditions, and a statistically guided method to substantially reduce
evaluation time. Empirical results from four distinct LLMs validate the
effectiveness of our approach. We successfully identify algorithms that achieve
up to 27% performance improvements over the original BBR algorithm in a
production QUIC implementation. Our work demonstrates the potential of LLMs to
accelerate the design of high-performance network algorithms and paves the way
for broader applications in networking systems.

</details>


### [9] [ANSC: Probabilistic Capacity Health Scoring for Datacenter-Scale Reliability](https://arxiv.org/abs/2508.16119)
*Madhava Gaikwad,Abhishek Gandhi*

Main category: cs.NI

TL;DR: ANSC是一个概率容量健康评分框架，用于超大规模数据中心网络，通过颜色编码系统评估容量违规的紧迫性概率，而不仅仅是当前影响。


<details>
  <summary>Details</summary>
Motivation: 现有告警系统只能检测单个设备或链路故障，无法捕捉级联容量短缺的聚合风险，需要一种能评估即将发生容量违规概率的系统。

Method: ANSC综合考虑当前剩余容量和额外故障概率，在数据中心和区域级别进行归一化处理，提供颜色编码的评分系统。

Result: ANSC使操作人员能够在400多个数据中心和60个区域中优先处理修复工作，减少噪音并使SRE专注于最关键的风险。

Conclusion: ANSC框架有效解决了超大规模数据中心网络中容量风险的概率评估问题，提升了运维效率和风险管理的精准性。

Abstract: We present ANSC, a probabilistic capacity health scoring framework for
hyperscale datacenter fabrics. While existing alerting systems detect
individual device or link failures, they do not capture the aggregate risk of
cascading capacity shortfalls. ANSC provides a color-coded scoring system that
indicates the urgency of issues \emph{not solely by current impact, but by the
probability of imminent capacity violations}. Our system accounts for both
current residual capacity and the probability of additional failures,
normalized at datacenter and regional level. We demonstrate that ANSC enables
operators to prioritize remediation across more than 400 datacenters and 60
regions, reducing noise and aligning SRE focus on the most critical risks.

</details>


### [10] [Joint Cache Placement and Routing in Satellite-Terrestrial Edge Computing Network: A GNN-Enabled DRL Approach](https://arxiv.org/abs/2508.16184)
*Yuhao Zheng,Ting You,Kejia Peng,Chang Liu*

Main category: cs.NI

TL;DR: 基于图神经网络和深度强化学习的卡车服务优化方案，在卫星地面边缘计算网络中实现了更高的内容传输效率


<details>
  <summary>Details</summary>
Motivation: 解决动态低赨道卫星网络拓扑结构和异构内容需求带来的挑战，提升地理分布用户的缓存服务质量

Method: 使用图神经网络(GNN)和深度强化学习(DRL)相结合的学习框架，将卫星网络表示为动态图，通过Markov决策过程(MDP)和软执行策-评估者(SAC)算法优化缓存策略

Result: 模拟结果显示该方法显著提高了内容传输成功率，同时降低了通信流量成本

Conclusion: 该学习框架能够有效处理卫星网络的动态性和复杂性，为地理分布用户提供更好的缓存服务

Abstract: In this letter, we investigate the problem of joint content caching and
routing in satellite-terrestrial edge computing networks (STECNs) to improve
caching service for geographically distributed users. To handle the challenges
arising from dynamic low Earth orbit (LEO) satellite topologies and
heterogeneous content demands, we propose a learning-based framework that
integrates graph neural networks (GNNs) with deep reinforcement learning (DRL).
The satellite network is represented as a dynamic graph, where GNNs are
embedded within the DRL agent to capture spatial and topological dependencies
and support routing-aware decision-making. The caching strategy is optimized by
formulating the problem as a Markov decision process (MDP) and applying soft
actor-critic (SAC) algorithm. Simulation results demonstrate that our approach
significantly improves the delivery success rate and reduces communication
traffic cost.

</details>


### [11] [Self-Healing Network of Interconnected Edge Devices Empowered by Infrastructure-as-Code and LoRa Communication](https://arxiv.org/abs/2508.16268)
*Rob Carson,Mohamed Chahine Ghanem,Feriel Bouakkaz*

Main category: cs.NI

TL;DR: 这篇论文提出了一种基于Raspberry Pi和LoRa协议的自恢复自动化网络系统，通过容器化架构和IaC原则解决传统IaC工具在LoRa网络中的兼容性问题，并集成了自动故障转移机制。


<details>
  <summary>Details</summary>
Motivation: 解决在传统网络不可用场景下，使用LoRa协议的低功耗长距离网络部署的挑战，包括带宽限制、数据碰撞和节点故障等问题。

Method: 采用容器化架构在Raspberry Pi集群中部署，适配IaC原则以满足LoRa协议的包基础结构。通过数据包分片和重传机制缓解通信限制，并集成自动故障转移机制。

Result: 实验评估显示，数据包分片和重传机制可有效缓解LoRa的吞吐量和包大小限制。自动故障转移机制能在1秒内将服务重新部署到其他节点，显示了系统的弹性。但碰撞和视线干扰问题仍存在。

Conclusion: 该研究成功展示了在LoRa网络中实现IaC原则的可行性。未来工作应探索网格网络集成、更先进的调度算法以及新兴的低功耗广域网技术来提升系统性能。

Abstract: This Paper proposes a self-healing, automated network of Raspberry Pi devices
designed for deployment in scenarios where traditional networking is
unavailable. Leveraging the low-power, long-range capabilities of the LoRa
(Long Range) protocol alongside Infrastructure as Code (IaC) methodologies, the
research addresses challenges such as limited bandwidth, data collisions, and
node failures. Given that LoRa's packet-based system is incompatible with
conventional IaC tools like Ansible and Terraform, which rely on TCP/IP
networking, the research adapts IaC principles within a containerised
architecture deployed across a Raspberry Pi cluster. Evaluation experiments
indicate that fragmenting data packets and retransmitting any missed fragments
can mitigate LoRa's inherent throughput and packet size limitations, although
issues such as collisions and line-of-sight interference persist. An automated
failover mechanism was integrated into the architecture, enabling unresponsive
services to be redeployed to alternative nodes within one second, demonstrating
the system's resilience in maintaining operational continuity despite node or
service failures. The paper also identifies practical challenges, including the
necessity for time-slotting transmissions to prevent data packet overlap and
collisions. Future research should explore the integration of mesh networking
to enhance range, develop more advanced scheduling algorithms, and adopt
cutting-edge low-power wide-area network (LPWAN) techniques.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [T-ILR: a Neurosymbolic Integration for LTLf](https://arxiv.org/abs/2508.15943)
*Riccardo Andreoni,Andrei Buliga,Alessandro Daniele,Chiara Ghidini,Marco Montali,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的神经符号框架T-ILR，用于在深度学习中集成时态逻辑规范，在图像序列分类任务中实现了更高的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前的神经符号方法主要处理静态领域，对时态逻辑规范的处理方法较少。唯一的现有方法需要显式表示有限状态自动机，效率低下。

Method: 扩展了迭代局部精炼(ILR)算法，利用模糊LTLf解释，提出T-ILR方法直接将有限迹LTLf表达的时态逻辑规范集成到深度学习架构中。

Result: 在时态神经符号架构标准测试集上，T-ILR方法在图像序列分类任务中实现了比现有最佳方法更高的准确性和计算效率。

Conclusion: T-ILR为处理时态逻辑规范提供了一种高效的神经符号解决方案，充实了深度学习与符号知识集成在序列任务中的应用。

Abstract: State-of-the-art approaches for integrating symbolic knowledge with deep
learning architectures have demonstrated promising results in static domains.
However, methods to handle temporal logic specifications remain underexplored.
The only existing approach relies on an explicit representation of a
finite-state automaton corresponding to the temporal specification. Instead, we
aim at proposing a neurosymbolic framework designed to incorporate temporal
logic specifications, expressed in Linear Temporal Logic over finite traces
(LTLf), directly into deep learning architectures for sequence-based tasks. We
extend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging
the recent introduction of fuzzy LTLf interpretations. We name this proposed
method Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an
existing benchmark for temporal neurosymbolic architectures, consisting of the
classification of image sequences in the presence of temporal knowledge. The
results demonstrate improved accuracy and computational efficiency compared to
the state-of-the-art method.

</details>


### [13] [CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics](https://arxiv.org/abs/2508.16033)
*Jong-Hwan Jang,Junho Song,Yong-Yeon Jo*

Main category: cs.AI

TL;DR: 这篇论文提出了一种可解释性AI框架CoFE，通过生成反事实ECG信号来解释AI-ECG模型的预测决策，提高临床应用的可信过程。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI基础ECG预测模型在临床实践中的可解释性需求，使得医生能够理解模型的决策过程和依据。

Method: 开发了反事实ECG生成框架(CoFE)，通过改变ECG信号的特定特征(如振幅和间隔)来演示对模型预测的影响，并在房颤分类和钾水平回归模型中进行实验验证。

Result: CoFE框架生成的反事实ECG显示的特征变化与已知临床知识相符合，能够清晰地呈现有效特征出现的位置以及它们如何影响模型预测。

Conclusion: 该框架有助于提高AI-ECG模型的可解释性，支持更有效的临床决策过程，促进AI在医疗预测中的实际应用。

Abstract: Recognizing the need for explainable AI (XAI) approaches to enable the
successful integration of AI-based ECG prediction models (AI-ECG) into clinical
practice, we introduce a framework generating \textbf{Co}unter\textbf{F}actual
\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as
amplitudes and intervals, influence the model's predictive decisions. To
demonstrate the applicability of the CoFE, we present two case studies: atrial
fibrillation classification and potassium level regression models. The CoFE
reveals feature changes in ECG signals that align with the established clinical
knowledge. By clarifying both \textbf{where valid features appear} in the ECG
and \textbf{how they influence the model's predictions}, we anticipate that our
framework will enhance the interpretability of AI-ECG models and support more
effective clinical decision-making. Our demonstration video is available at:
https://www.youtube.com/watch?v=YoW0bNBPglQ.

</details>


### [14] [MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs](https://arxiv.org/abs/2508.16051)
*Yiheng Hu,Xiaoyang Wang,Qing Liu,Xiwei Xu,Qian Fu,Wenjie Zhang,Liming Zhu*

Main category: cs.AI

TL;DR: 提出了一种基于自适应规划图的无训练多模态多跳问答框架，通过动态规划、检索和推理模块实现灵活的多路径推理，避免了传统方法的中间错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态多跳问答方法依赖顺序检索和推理，容易因中间步骤错误而失败，且训练成本高昂。需要一种无需训练、能动态探索多路径的解决方案。

Method: 采用自适应规划图框架，包含规划模块（分析当前状态决定下一步行动）、检索模块（模态特定策略处理不同数据类型）和推理模块，实现动态灵活的推理路径探索。

Result: 在MultimodalQA和WebQA数据集上的实验表明，该方法无需训练即可达到或超越依赖训练的现有模型性能。

Conclusion: 提出的无训练自适应规划图框架有效解决了多模态多跳问答中的错误累积和训练成本问题，实现了与最新模型的无缝集成和优异性能。

Abstract: Multimodal Multi-hop question answering requires integrating information from
diverse sources, such as images and texts, to derive answers. Existing methods
typically rely on sequential retrieval and reasoning, where each step builds on
the previous output. However, this single-path paradigm makes them vulnerable
to errors due to misleading intermediate steps. Moreover, developing multimodal
models can be computationally expensive, often requiring extensive training. To
address these limitations, we propose a training-free framework guided by an
Adaptive Planning Graph, which consists of planning, retrieval and reasoning
modules. The planning module analyzes the current state of the Adaptive
Planning Graph, determines the next action and where to expand the graph, which
enables dynamic and flexible exploration of reasoning paths. To handle
retrieval of text to unspecified target modalities, we devise modality-specific
strategies that dynamically adapt to distinct data types. Our approach
preserves the characteristics of multimodal information without costly
task-specific training, enabling seamless integration with up-to-date models.
Finally, the experiments on MultimodalQA and WebQA show that our approach
matches or outperforms existing models that rely on training.

</details>


### [15] [Generative Foundation Model for Structured and Unstructured Electronic Health Records](https://arxiv.org/abs/2508.16054)
*Sonish Sivarajkumar,Hang Zhang,Yuelyu Ji,Maneesh Bilalpur,Xizhi Wu,Chenyu Li,Min Gu Kwak,Shyam Visweswaran,Yanshan Wang*

Main category: cs.AI

TL;DR: GDP是一个多模态基础模型，通过CNN-Transformer编码器处理结构化EHR时间序列数据，并与非结构化临床文本融合，能够同时进行临床预测和高质量临床叙述生成。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHR)包含丰富的多模态临床数据，但现有方法通常将数值数据序列化为文本，会丢失时间和定量细节信息。需要开发能够原生处理多模态EHR数据的基础模型。

Method: 采用两阶段训练：1)生成式预训练，学习从原始患者时间线生成临床叙述，同时进行掩码特征预测和下一时间步预测；2)多任务微调，用于临床预测任务。使用CNN-Transformer编码器处理结构化时间序列，通过跨模态注意力与LLaMA解码器融合。

Result: 在MIMIC-IV数据集上表现优异：心衰预测AUROC=0.923，2型糖尿病AUROC=0.817，30天再入院AUROC=0.627。叙述生成达到ROUGE-L=0.135和BERTScore-F1=0.545。人工评估显示在忠实性、流畅性和临床实用性方面得分最高。

Conclusion: 单一多模态基础模型能够同时预测临床可操作事件和生成高质量临床叙述，减少医院文档工作负担而不牺牲准确性。GDP的灵活架构可扩展到其他模态。

Abstract: Electronic health records (EHRs) are rich clinical data sources but complex
repositories of patient data, spanning structured elements (demographics,
vitals, lab results, codes), unstructured clinical notes and other modalities
of data. Harnessing this heterogeneity is critical for improving patient
outcomes. Recent advances in large language models (LLMs) have enabled
foundation models that can learn from multiple data modalities and support
clinical tasks. However, most current approaches simply serialize numeric EHR
data into text, which risks losing temporal and quantitative detail. We
introduce Generative Deep Patient (GDP), a multimodal foundation model that
natively encodes structured EHR time-series via a CNN-Transformer encoder and
fuses it with unstructured EHRs through cross-modal attention into a
LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,
where it learns to produce clinical narratives from raw patient timelines while
also performing masked feature prediction (MFP) and next time-step prediction
(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for
clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day
readmission). In clinical prediction, GDP demonstrated superior performance on
MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and
30-day readmission AUROC = 0.627. For narrative generation, GDP achieved
ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,
GDP-Instruct scored highest on faithfulness, fluency, and overall clinical
utility, suggesting reduced hospital documentation workload without sacrificing
accuracy. Our results demonstrate that a single multimodal foundation model can
both predict clinically actionable events and generate high-quality clinical
narratives. Furthermore, GDP's flexible architecture can be extended to
additional modalities.

</details>


### [16] [Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework](https://arxiv.org/abs/2508.16057)
*Sijie Yang,Binyu Lei,Filip Biljecki*

Main category: cs.AI

TL;DR: 本文探讨数字规划中城市舒适度的理论解释和评估方法，重点关注多维分析、数据支持和AI辅助三个关键维度


<details>
  <summary>Details</summary>
Motivation: 确保宜居性和舒适度是城市规划的基本目标，但目前缺乏对城市舒适度的明确定义和综合评估框架

Method: 通过多维分析、数据支持和AI辅助的方法来评估城市舒适度，包括绿化覆盖率、热舒适度和步行性等因素

Result: 提出了城市舒适度评估的理论框架和方法论，为数字规划提供支持

Conclusion: 研究为城市舒适度的综合评估提供了理论基础和方法指导，有助于提升城市规划的宜居性和舒适度

Abstract: Ensuring liveability and comfort is one of the fundamental objectives of
urban planning. Numerous studies have employed computational methods to assess
and quantify factors related to urban comfort such as greenery coverage,
thermal comfort, and walkability. However, a clear definition of urban comfort
and its comprehensive evaluation framework remain elusive. Our research
explores the theoretical interpretations and methodologies for assessing urban
comfort within digital planning, emphasising three key dimensions:
multidimensional analysis, data support, and AI assistance.

</details>


### [17] [Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting](https://arxiv.org/abs/2508.16059)
*Zhuomin Chen,Dan Li,Jiahui Zhou,Shunyu Wu,Haozheng Ye,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: MSEF框架通过多层可引导嵌入融合，解决LLM在时间序列预测中TS信息在深层逐渐消失的问题，在7个基准测试中平均MSE降低31.8%


<details>
  <summary>Details</summary>
Motivation: 现有方法将时间序列信息浅层集成到LLM中，导致TS表示在深层逐渐消失，文本嵌入与TS表示之间适应效果不佳

Method: 利用现成的时间序列基础模型提取语义丰富的嵌入，通过层特定引导向量在LLM各层中间文本表示进行融合，持续优化时间序列与文本模态的对齐

Result: 在7个基准测试中相比基线方法表现显著提升，平均MSE降低31.8%

Conclusion: MSEF框架通过多层融合机制有效缓解了TS信息在LLM深层丢失的问题，实现了高效的小样本学习能力

Abstract: Time series (TS) data are ubiquitous across various application areas,
rendering time series forecasting (TSF) a fundamental task. With the astounding
advances in large language models (LLMs), a variety of methods have been
developed to adapt LLMs for time series forecasting. Despite unlocking the
potential of LLMs in comprehending TS data, existing methods are inherently
constrained by their shallow integration of TS information, wherein LLMs
typically access TS representations at shallow layers, primarily at the input
layer. This causes the influence of TS representations to progressively fade in
deeper layers and eventually leads to ineffective adaptation between textual
embeddings and TS representations. In this paper, we propose the Multi-layer
Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to
directly access time series patterns at all depths, thereby mitigating the
progressive loss of TS information in deeper layers. Specifically, MSEF
leverages off-the-shelf time series foundation models to extract semantically
rich embeddings, which are fused with intermediate text representations across
LLM layers via layer-specific steering vectors. These steering vectors are
designed to continuously optimize the alignment between time series and textual
modalities and facilitate a layer-specific adaptation mechanism that ensures
efficient few-shot learning capabilities. Experimental results on seven
benchmarks demonstrate significant performance improvements by MSEF compared
with baselines, with an average reduction of 31.8% in terms of MSE. The code is
available at https://github.com/One1sAll/MSEF.

</details>


### [18] [InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles](https://arxiv.org/abs/2508.16072)
*Zizhen Li,Chuanhao Li,Yibin Wang,Qi Chen,Diping Song,Yukang Feng,Jianwen Sun,Jiaxin Ai,Fanrui Zhang,Mingzhu Sun,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 本文提出了InMind评估框架，用于测试大语言模型在社交推理游戏中捕捉和应用个性化推理风格的能力，发现当前LLMs在个体化适应性推理方面存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 现有评估往往忽视个体化推理风格对社交情境理解的影响，需要开发能够评估LLMs是否能够理解和应用个性化推理策略的框架。

Method: 引入InMind框架，通过增强结构化游戏数据（回合级策略追踪和赛后反思），在观察者和参与者两种模式下收集数据，支持4个认知驱动的评估任务。

Result: 通用LLMs（包括GPT-4o）过度依赖词汇线索，难以在时间游戏中锚定反思或适应演化策略；而推理增强型LLMs（如DeepSeek-R1）显示出风格敏感推理的早期迹象。

Conclusion: 当前LLMs在个体化适应性推理能力上存在关键局限，InMind框架为认知对齐的人机交互迈出了重要一步。

Abstract: LLMs have shown strong performance on human-centric reasoning tasks. While
previous evaluations have explored whether LLMs can infer intentions or detect
deception, they often overlook the individualized reasoning styles that
influence how people interpret and act in social contexts. Social deduction
games (SDGs) provide a natural testbed for evaluating individualized reasoning
styles, where different players may adopt diverse but contextually valid
reasoning strategies under identical conditions. To address this, we introduce
InMind, a cognitively grounded evaluation framework designed to assess whether
LLMs can capture and apply personalized reasoning styles in SDGs. InMind
enhances structured gameplay data with round-level strategy traces and
post-game reflections, collected under both Observer and Participant modes. It
supports four cognitively motivated tasks that jointly evaluate both static
alignment and dynamic adaptation. As a case study, we apply InMind to the game
Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o
frequently rely on lexical cues, struggling to anchor reflections in temporal
gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs
like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These
findings reveal key limitations in current LLMs' capacity for individualized,
adaptive reasoning, and position InMind as a step toward cognitively aligned
human-AI interaction.

</details>


### [19] [IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra](https://arxiv.org/abs/2508.16112)
*Heewoong Noh,Namkyeong Lee,Gyoung S. Na,Kibum Kim,Chanyoung Park*

Main category: cs.AI

TL;DR: IR-Agent是一个新颖的多代理框架，用于从红外光谱中解析分子结构，模拟专家分析流程并具有良好扩展性


<details>
  <summary>Details</summary>
Motivation: 现有红外光谱分析方法无法充分反映专家分析过程，缺乏整合多种化学知识的灵活性，而这对实际分析场景至关重要

Method: 提出多代理框架，每个代理专门负责红外光谱解释的特定方面，通过互补角色实现集成推理

Result: 实验表明IR-Agent不仅提高了实验红外光谱的基线性能，还展现出对各种化学信息形式的强适应性

Conclusion: 该框架能够有效模拟专家驱动的红外分析流程，提高结构解析的整体准确性，并具有固有的可扩展性

Abstract: Spectral analysis provides crucial clues for the elucidation of unknown
materials. Among various techniques, infrared spectroscopy (IR) plays an
important role in laboratory settings due to its high accessibility and low
cost. However, existing approaches often fail to reflect expert analytical
processes and lack flexibility in incorporating diverse types of chemical
knowledge, which is essential in real-world analytical scenarios. In this
paper, we propose IR-Agent, a novel multi-agent framework for molecular
structure elucidation from IR spectra. The framework is designed to emulate
expert-driven IR analysis procedures and is inherently extensible. Each agent
specializes in a specific aspect of IR interpretation, and their complementary
roles enable integrated reasoning, thereby improving the overall accuracy of
structure elucidation. Through extensive experiments, we demonstrate that
IR-Agent not only improves baseline performance on experimental IR spectra but
also shows strong adaptability to various forms of chemical information.

</details>


### [20] [Extending FKG.in: Towards a Food Claim Traceability Network](https://arxiv.org/abs/2508.16117)
*Saransh Kumar Gupta,Rizwan Gulzar Mir,Lipika Dey,Partha Pratim Das,Anirban Sen,Ramesh Jain*

Main category: cs.AI

TL;DR: 这篇论文提出了食物声明可追溯网络(FCN)，通过知识图谱和半自动化知识组织方法，建立结构化的食物相关声明验证体系，以应对食品预防健康声明的混乱现象。


<details>
  <summary>Details</summary>
Motivation: 当前全球食品预防健康声明混乱不清，包含科学、文化和商业各种声明，但缺乏统一的追踪、验证和上下文化基础设施。

Method: 在印度食物知识图FKG.in基础上扩展构建FCN，使用本体论设计和半自动化知识组织流程，利用Reddit数据和大语言模型实现原型系统。

Result: 开发了集成精选数据输入、结构化模式和来源可追溯流程的FCN系统，方法论具有应用无关性，可适配不同地理、炖饪或监管环境。

Conclusion: 通过结构化、可验证和可解释的方式建模食物声明可追溯性，有助于构建更透明负责的食物知识生态系统，支持研究人员、政策制定者和普通消费者应对食物声明沉淀的世界。

Abstract: The global food landscape is rife with scientific, cultural, and commercial
claims about what foods are, what they do, what they should not do, or should
not do. These range from rigorously studied health benefits (probiotics improve
gut health) and misrepresentations (soaked almonds make one smarter) to vague
promises (superfoods boost immunity) and culturally rooted beliefs (cold foods
cause coughs). Despite their widespread influence, the infrastructure for
tracing, verifying, and contextualizing these claims remains fragmented and
underdeveloped. In this paper, we propose a Food Claim-Traceability Network
(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have
been incrementally building. We also present the ontology design and the
semi-automated knowledge curation workflow that we used to develop a proof of
concept of FKG.in-FCN using Reddit data and Large Language Models. FCN
integrates curated data inputs, structured schemas, and provenance-aware
pipelines for food-related claim extraction and validation. While directly
linked to the Indian food knowledge graph as an application, our methodology
remains application-agnostic and adaptable to other geographic, culinary, or
regulatory settings. By modeling food claims and their traceability in a
structured, verifiable, and explainable way, we aim to contribute to more
transparent and accountable food knowledge ecosystems, supporting researchers,
policymakers, and most importantly, everyday consumers in navigating a world
saturated with dietary assertions.

</details>


### [21] [Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning](https://arxiv.org/abs/2508.16129)
*Ruiqi Wu,Yuang Yao,Tengfei Ma,Chenran Zhang,Na Su,Tao Zhou,Geng Chen,Wen Fan,Yi Zhou*

Main category: cs.AI

TL;DR: 这篇论文提出了OphthaReason模型和UADT方法，构建了第一个眼科多模态数据集MM-Retinal-Reason，在眼科医学诊断中实现了基础和复杂推理任务的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理模型仅聚焦基础推理，而真实临床诊断需要整合异构临床信息和多模态医学影像数据的复杂推理过程。

Method: 构建MM-Retinal-Reason数据集，提出OphthaReason模型和不确定性感知动态思维（UADT）方法，通过熳提估计样本不确定性并动态调节模型探索深度。

Result: 模型在基础和复杂推理任务上都达到最佳性能，超过通用MLLM、医学MLLM、RL基础医学MLLM和眼科MLLM至少24.92%、15.00%、21.20%和17.66%。

Conclusion: 该研究填补了眼科多模态推理的空白，通过UADT方法实现了精准的临床思维模拟，为医学AI推理提供了新方向。

Abstract: Multimodal large language models (MLLMs) have recently demonstrated
remarkable reasoning abilities with reinforcement learning paradigm. Although
several multimodal reasoning models have been explored in the medical domain,
most of them focus exclusively on basic reasoning, which refers to shallow
inference based on visual feature matching. However, real-world clinical
diagnosis extends beyond basic reasoning, demanding reasoning processes that
integrate heterogeneous clinical information (such as chief complaints and
medical history) with multimodal medical imaging data. To bridge this gap, we
introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the
full spectrum of perception and reasoning. It encompasses both basic reasoning
tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental
reasoning capabilities and emulate realistic clinical thinking patterns.
Building upon MM-Retinal-Reason, we propose OphthaReason, the first
ophthalmology-specific multimodal reasoning model with step-by-step reasoning
traces. To enable flexible adaptation to both basic and complex reasoning
tasks, we specifically design a novel method called Uncertainty-Aware Dynamic
Thinking (UADT), which estimates sample-level uncertainty via entropy and
dynamically modulates the model's exploration depth using a shaped advantage
mechanism. Comprehensive experiments demonstrate that our model achieves
state-of-the-art performance on both basic and complex reasoning tasks,
outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and
ophthalmic MLLMs by at least 24.92\%, 15.00\%, 21.20\%, and 17.66\%. Project
Page: \href{https://github.com/lxirich/OphthaReason}{link}.

</details>


### [22] [Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain](https://arxiv.org/abs/2508.16172)
*Kai Hu,Parfait Atchade-Adelomou,Carlo Adornetto,Adrian Mora-Carrero,Luis Alonso-Pastor,Ariel Noyman,Yubo Liu,Kent Larson*

Main category: cs.AI

TL;DR: 本文提出Preference Chain方法，结合图检索增强生成和LLM，提升交通系统中人类行为的上下文感知模拟效果，在Replica数据集上表现优于标准LLM。


<details>
  <summary>Details</summary>
Motivation: 城市环境中人类行为理解很重要，但准确数据收集困难，特别是在新兴区域。现有生成代理方法在产生一致、上下文敏感和现实行为输出方面存在不足。

Method: 提出Preference Chain方法，整合图检索增强生成（RAG）与大型语言模型（LLM），增强交通系统中人类行为的上下文感知模拟能力。

Result: 在Replica数据集上的实验表明，Preference Chain在符合真实世界交通方式选择方面优于标准LLM方法。

Conclusion: 该方法为数据稀缺环境中模拟复杂人类行为提供了有前景的框架，可应用于新兴城市移动性建模、个性化出行行为分析和动态交通预测。

Abstract: Understanding human behavior in urban environments is a crucial field within
city sciences. However, collecting accurate behavioral data, particularly in
newly developed areas, poses significant challenges. Recent advances in
generative agents, powered by Large Language Models (LLMs), have shown promise
in simulating human behaviors without relying on extensive datasets.
Nevertheless, these methods often struggle with generating consistent,
context-sensitive, and realistic behavioral outputs. To address these
limitations, this paper introduces the Preference Chain, a novel method that
integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance
context-aware simulation of human behavior in transportation systems.
Experiments conducted on the Replica dataset demonstrate that the Preference
Chain outperforms standard LLM in aligning with real-world transportation mode
choices. The development of the Mobility Agent highlights potential
applications of proposed method in urban mobility modeling for emerging cities,
personalized travel behavior analysis, and dynamic traffic forecasting. Despite
limitations such as slow inference and the risk of hallucination, the method
offers a promising framework for simulating complex human behavior in
data-scarce environments, where traditional data-driven models struggle due to
limited data availability.

</details>


### [23] [Competition and Attraction Improve Model Fusion](https://arxiv.org/abs/2508.16204)
*João Abrantes,Robert Tjarko Lange,Yujin Tang*

Main category: cs.AI

TL;DR: M2N2是一种基于进化算法的模型融合方法，通过动态调整融合边界、多样性保持机制和启发式吸引力度量，实现了从零开始进化模型，在MNIST分类和语言/图像生成任务中达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法需要手动将参数划分为固定组进行融合，限制了潜在组合的探索和性能提升。

Method: 提出M2N2进化算法，包含三个关键特征：动态调整融合边界、基于自然竞争的资源多样性保持机制、启发式吸引力度量来识别最有前景的模型对进行融合。

Result: 首次证明模型融合可以从零开始进化模型，在MNIST分类上达到与CMA-ES相当的性能但计算更高效，在语言和图像生成模型融合中达到最先进性能。

Conclusion: M2N2展示了强大的鲁棒性和多功能性，能够保持超出适应度函数明确优化的关键模型能力，为模型融合提供了新的进化途径。

Abstract: Model merging is a powerful technique for integrating the specialized
knowledge of multiple machine learning models into a single model. However,
existing methods require manually partitioning model parameters into fixed
groups for merging, which restricts the exploration of potential combinations
and limits performance. To overcome these limitations, we propose Model Merging
of Natural Niches (M2N2), an evolutionary algorithm with three key features:
(1) dynamic adjustment of merging boundaries to progressively explore a broader
range of parameter combinations; (2) a diversity preservation mechanism
inspired by the competition for resources in nature, to maintain a population
of diverse, high-performing models that are particularly well-suited for
merging; and (3) a heuristicbased attraction metric to identify the most
promising pairs of models for fusion. Our experimental results demonstrate, for
the first time, that model merging can be used to evolve models entirely from
scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch
and achieve performance comparable to CMA-ES, while being computationally more
efficient. Furthermore, M2N2 scales to merge specialized language and image
generation models, achieving state-of-the-art performance. Notably, it
preserves crucial model capabilities beyond those explicitly optimized by the
fitness function, highlighting its robustness and versatility. Our code is
available at https://github.com/SakanaAI/natural_niches

</details>


### [24] [The next question after Turing's question: Introducing the Grow-AI test](https://arxiv.org/abs/2508.16277)
*Alexandru Tugui*

Main category: cs.AI

TL;DR: GROW-AI框架扩展了人工智能评估方法，通过六个标准游戏化测试来评估AI的"成长"水平，计算成长指数并确定成熟度阈值


<details>
  <summary>Details</summary>
Motivation: 解决"机器能否成长"的问题，作为图灵测试的自然延伸，将人类成长过程概念化地移植到人工智能领域

Method: 基于六个主要标准(C1-C6)的游戏化评估系统，分为四个竞技场，使用标准化AI日志记录所有决策和行动，采用专家先验方法确定权重，计算算术平均的成长指数

Result: 该方法能够对不同类型AI实体(机器人、软件代理、大语言模型)进行一致且可比较的"成长"水平评估，多游戏结构能突出优势领域和薄弱环节

Conclusion: GROW-AI框架通过结合心理学、机器人学、计算机科学和伦理学的综合测试格式，不仅测量性能，还能捕捉AI实体向成熟度进化的路径，保证了评估的可追溯性和可复现性

Abstract: This study aims to extend the framework for assessing artificial
intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),
designed to answer the question "Can machines grow up?" -- a natural successor
to the Turing Test. The methodology applied is based on a system of six primary
criteria (C1-C6), each assessed through a specific "game", divided into four
arenas that explore both the human dimension and its transposition into AI. All
decisions and actions of the entity are recorded in a standardized AI Journal,
the primary source for calculating composite scores. The assessment uses the
prior expert method to establish initial weights, and the global score -- Grow
Up Index -- is calculated as the arithmetic mean of the six scores, with
interpretation on maturity thresholds. The results show that the methodology
allows for a coherent and comparable assessment of the level of "growth" of AI
entities, regardless of their type (robots, software agents, LLMs). The
multi-game structure highlights strengths and vulnerable areas, and the use of
a unified journal guarantees traceability and replicability in the evaluation.
The originality of the work lies in the conceptual transposition of the process
of "growing" from the human world to that of artificial intelligence, in an
integrated testing format that combines perspectives from psychology, robotics,
computer science, and ethics. Through this approach, GROW-AI not only measures
performance but also captures the evolutionary path of an AI entity towards
maturity.

</details>


### [25] [AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications](https://arxiv.org/abs/2508.16279)
*Dawei Gao,Zitao Li,Yuexiang Xie,Weirui Kuang,Liuyi Yao,Bingchen Qian,Zhijian Ma,Yue Cui,Haohao Luo,Shen Li,Lu Yi,Yi Yu,Shiqi He,Zhiling Luo,Wenmeng Zhou,Zhicheng Zhang,Xuguang He,Ziqian Chen,Weikai Liao,Farruh Isakulovich Kushnazarov,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: AgentScope 1.0是一个支持工具驱动智能体交互的框架，提供统一接口、异步设计、内置智能体和工程支持，用于构建可扩展的智能体应用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，智能体需要结合内在知识和动态工具使用来增强处理现实世界任务的能力，需要一个支持灵活高效工具交互的框架。

Method: 抽象基础组件并提供统一接口和可扩展模块，基于ReAct范式设计系统级异步架构，集成内置智能体，提供可视化评估模块和运行时沙盒。

Result: 构建了一个支持人类-智能体和智能体-智能体交互模式的框架，提高了执行效率，使长轨迹智能体应用开发更易管理和追踪。

Conclusion: AgentScope 1.0为构建可扩展、自适应和有效的智能体应用提供了实用基础，支持安全执行和快速部署。

Abstract: Driven by rapid advancements of Large Language Models (LLMs), agents are
empowered to combine intrinsic knowledge with dynamic tool use, greatly
enhancing their capacity to address real-world tasks. In line with such an
evolution, AgentScope introduces major improvements in a new version (1.0),
towards comprehensively supporting flexible and efficient tool-based
agent-environment interactions for building agentic applications. Specifically,
we abstract foundational components essential for agentic applications and
provide unified interfaces and extensible modules, enabling developers to
easily leverage the latest progress, such as new models and MCPs. Furthermore,
we ground agent behaviors in the ReAct paradigm and offer advanced agent-level
infrastructure based on a systematic asynchronous design, which enriches both
human-agent and agent-agent interaction patterns while improving execution
efficiency. Building on this foundation, we integrate several built-in agents
tailored to specific practical scenarios. AgentScope also includes robust
engineering support for developer-friendly experiences. We provide a scalable
evaluation module with a visual studio interface, making the development of
long-trajectory agentic applications more manageable and easier to trace. In
addition, AgentScope offers a runtime sandbox to ensure safe agent execution
and facilitates rapid deployment in production environments. With these
enhancements, AgentScope provides a practical foundation for building scalable,
adaptive, and effective agentic applications.

</details>


### [26] [Do What? Teaching Vision-Language-Action Models to Reject the Impossible](https://arxiv.org/abs/2508.16292)
*Wen-Han Hsieh,Elvis Hsieh,Dantong Niu,Trevor Darrell,Roei Herzig,David M. Chan*

Main category: cs.AI

TL;DR: 提出了IVA框架，使视觉-语言-动作模型能够检测错误前提指令、进行语言澄清并执行合理替代方案


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在处理错误前提指令（引用环境中不存在对象或条件的指令）时存在局限，需要提升模型识别和响应这类指令的能力

Method: 构建大规模指令调优设置，使用结构化语言提示训练VLA模型，利用上下文增强的半合成数据集包含配对的正例和错误前提指令

Result: IVA在错误前提检测准确率上比基线提高97.56%，在错误前提场景中的成功响应率提高50.78%

Conclusion: IVA框架有效提升了VLA模型处理错误前提指令的能力，实现了检测、澄清和执行的统一解决方案

Abstract: Recently, Vision-Language-Action (VLA) models have demonstrated strong
performance on a range of robotic tasks. These models rely on multimodal
inputs, with language instructions playing a crucial role -- not only in
predicting actions, but also in robustly interpreting user intent, even when
the requests are impossible to fulfill. In this work, we investigate how VLAs
can recognize, interpret, and respond to false-premise instructions: natural
language commands that reference objects or conditions absent from the
environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that
(i) detects when an instruction cannot be executed due to a false premise, (ii)
engages in language-based clarification or correction, and (iii) grounds
plausible alternatives in perception and action. Towards this end, we construct
a large-scale instruction tuning setup with structured language prompts and
train a VLA model capable of handling both accurate and erroneous requests. Our
approach leverages a contextually augmented, semi-synthetic dataset containing
paired positive and false-premise instructions, enabling robust detection and
natural language correction. Our experiments show that IVA improves false
premise detection accuracy by 97.56% over baselines, while increasing
successful responses in false-premise scenarios by 50.78%.

</details>


### [27] [Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management](https://arxiv.org/abs/2508.16352)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.AI

TL;DR: 提出了一种基于因果发现的深度学习框架，用于毫米波MIMO系统中的波束对齐，通过因果特征选择显著减少输入选择时间和波束扫描开销


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的波束对齐方法往往忽略输入输出之间的因果关系，导致可解释性差、泛化能力有限和不必要的波束扫描开销

Method: 提出两阶段因果波束选择算法：首先通过因果发现学习接收功率输入与最优波束之间的贝叶斯图，然后利用该图指导基于DL的分类器的因果特征选择

Result: 仿真结果显示，所提出的因果波束选择方法在性能上与常规方法相当，同时将输入选择时间减少94.4%，波束扫描开销减少59.4%

Conclusion: 因果感知的深度学习框架能够有效识别因果相关特征，在保持性能的同时显著降低系统开销，为6G及以后的毫米波MIMO系统提供高效可靠的波束对齐解决方案

Abstract: Efficient and reliable beam alignment is a critical requirement for mmWave
multiple-input multiple-output (MIMO) systems, especially in 6G and beyond,
where communication must be fast, adaptive, and resilient to real-world
uncertainties. Existing deep learning (DL)-based beam alignment methods often
neglect the underlying causal relationships between inputs and outputs, leading
to limited interpretability, poor generalization, and unnecessary beam sweeping
overhead. In this work, we propose a causally-aware DL framework that
integrates causal discovery into beam management pipeline. Particularly, we
propose a novel two-stage causal beam selection algorithm to identify a minimal
set of relevant inputs for beam prediction. First, causal discovery learns a
Bayesian graph capturing dependencies between received power inputs and the
optimal beam. Then, this graph guides causal feature selection for the DL-based
classifier. Simulation results reveal that the proposed causal beam selection
matches the performance of conventional methods while drastically reducing
input selection time by 94.4% and beam sweeping overhead by 59.4% by focusing
only on causally relevant features.

</details>


### [28] [GLARE: Agentic Reasoning for Legal Judgment Prediction](https://arxiv.org/abs/2508.16383)
*Xinyu Yang,Chenlong Deng,Zhicheng Dou*

Main category: cs.AI

TL;DR: GLARE是一个基于代理的法律推理框架，通过动态获取关键法律知识来解决大型语言模型在法律判决预测中推理不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在法律领域因缺乏法律知识而导致推理能力不足，需要改进法律判决预测的准确性和深度

Method: 开发GLARE代理法律推理框架，通过调用不同模块动态获取关键法律知识，提升推理的广度和深度

Result: 在真实数据集上的实验验证了该方法的有效性，生成的推理链提高了可解释性

Conclusion: GLARE框架能够有效提升法律判决预测的性能，并为实际应用提供了可能性

Abstract: Legal judgment prediction (LJP) has become increasingly important in the
legal field. In this paper, we identify that existing large language models
(LLMs) have significant problems of insufficient reasoning due to a lack of
legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning
framework that dynamically acquires key legal knowledge by invoking different
modules, thereby improving the breadth and depth of reasoning. Experiments
conducted on the real-world dataset verify the effectiveness of our method.
Furthermore, the reasoning chain generated during the analysis process can
increase interpretability and provide the possibility for practical
applications.

</details>


### [29] [Modular Embedding Recomposition for Incremental Learning](https://arxiv.org/abs/2508.16463)
*Aniello Panariello,Emanuele Frascaroli,Pietro Buzzega,Lorenzo Bonicelli,Angelo Porrello,Simone Calderara*

Main category: cs.AI

TL;DR: 提出MoDER方法，通过模块化专家训练和组合来增强视觉语言模型的零样本能力，在持续学习中实现零样本分类性能的提升


<details>
  <summary>Details</summary>
Motivation: 现有的持续学习方法主要关注保持视觉语言模型的零样本能力，但本文希望进一步将这些能力从保持转变为增强，特别是在下游任务与预训练领域差异较大时

Method: MoDER方法训练多个文本专家（每个专家专注于一个已见类别），存储在基础中心中。推理时，为每个未见类别查询中心并组合检索到的专家，合成改进的分类原型

Result: 在包含14个数据集的两种零样本增量协议（Class-IL和MTIL）上验证了方法的有效性

Conclusion: MoDER方法成功地将零样本能力从保持转变为增强，为视觉语言模型在持续学习中的实际应用提供了有效解决方案

Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly
transformed Continual Learning (CL), mainly due to their zero-shot
classification abilities. Such proficiency makes VLMs well-suited for
real-world applications, enabling robust performance on novel unseen classes
without requiring adaptation. However, fine-tuning remains essential when
downstream tasks deviate significantly from the pre-training domain. Prior CL
approaches primarily focus on preserving the zero-shot capabilities of VLMs
during incremental fine-tuning on a downstream task. We take a step further by
devising an approach that transforms preservation into enhancement of the
zero-shot capabilities of VLMs. Our approach, named MoDular Embedding
Recomposition (MoDER), introduces a modular framework that trains multiple
textual experts, each specialized in a single seen class, and stores them in a
foundational hub. At inference time, for each unseen class, we query the hub
and compose the retrieved experts to synthesize a refined prototype that
improves classification. We show the effectiveness of our method across two
popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total
of 14 datasets. The codebase is available at
https://github.com/aimagelab/mammoth.

</details>


### [30] [Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning](https://arxiv.org/abs/2508.16524)
*Xuan Zhang,Zhijian Zhou,Weidi Xu,Yanting Miao,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: 基于扩散模型的神经符号学习方法，通过两阶段训练策略和强化学习优化，在数独、迷宫等逻辑推理任务上实现高精度和逻辑一致性


<details>
  <summary>Details</summary>
Motivation: 解决神经网络学习复杂逻辑约束和符号推理的挑战，弥合神经网络输出分布与符号约束之间的差距

Method: 采用扩散模型架构，两阶段训练策略：第一阶段培养基础推理能力，第二阶段通过马尔可夫决策过程建模，使用改进的近端策略优化算法进行微调，基于逻辑一致性的规则奖励信号

Result: 在数独、迷宫、路径规划和偏好学习等经典符号推理基准测试中取得了优异的准确性和逻辑一致性

Conclusion: 扩散模型架构结合强化学习优化能够有效实现神经符号学习，为神经网络解决逻辑推理问题提供了新的有效途径

Abstract: Enabling neural networks to learn complex logical constraints and fulfill
symbolic reasoning is a critical challenge. Bridging this gap often requires
guiding the neural network's output distribution to move closer to the symbolic
constraints. While diffusion models have shown remarkable generative capability
across various domains, we employ the powerful architecture to perform
neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline
adopts a two-stage training strategy: the first stage focuses on cultivating
basic reasoning abilities, while the second emphasizes systematic learning of
logical constraints. To impose hard constraints on neural outputs in the second
stage, we formulate the diffusion reasoner as a Markov decision process and
innovatively fine-tune it with an improved proximal policy optimization
algorithm. We utilize a rule-based reward signal derived from the logical
consistency of neural outputs and adopt a flexible strategy to optimize the
diffusion reasoner's policy. We evaluate our methodology on some classical
symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and
preference learning. Experimental results demonstrate that our approach
achieves outstanding accuracy and logical consistency among neural networks.

</details>


### [31] [LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence](https://arxiv.org/abs/2508.16571)
*Alisa Vinogradova,Vlad Vinogradov,Dmitrii Radkevich,Ilya Yasny,Dmitry Kobyzev,Ivan Izmailov,Katsiaryna Yanchanka,Andrey Doronichev*

Main category: cs.AI

TL;DR: 本文介绍了一个用于药物资产快速尽职调查的竞争对手发现AI系统，通过LLM代理将多模态非结构化数据转化为结构化评估语料库，在基准测试中达到83%的召回率，显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的AI系统无法可靠检索所有竞争药物名称，且缺乏公认的公共基准测试。药物竞争数据具有投资者特定、付费墙限制、跨注册表碎片化、本体论不匹配、别名繁多、多模态和快速变化等特点。

Method: 使用LLM代理将5年多模态非结构化尽职调查备忘录转化为结构化评估语料库，建立指示到竞争药物的映射，并引入竞争对手验证LLM作为评判代理来过滤误报。

Result: 竞争对手发现代理达到83%的召回率，显著超过OpenAI Deep Research（65%）和Perplexity Labs（60%）。在生物技术VC基金案例中，分析师周转时间从2.5天降至约3小时（约20倍提升）。

Conclusion: 该系统成功解决了药物竞争发现的关键挑战，在生产和实际应用中证明了其有效性，大幅提升了竞争分析的效率和准确性。

Abstract: In this paper, we describe and benchmark a competitor-discovery component
used within an agentic AI system for fast drug asset due diligence. A
competitor-discovery AI agent, given an indication, retrieves all drugs
comprising the competitive landscape of that indication and extracts canonical
attributes for these drugs. The competitor definition is investor-specific, and
data is paywalled/licensed, fragmented across registries, ontology-mismatched
by indication, alias-heavy for drug names, multimodal, and rapidly changing.
Although considered the best tool for this problem, the current LLM-based AI
systems aren't capable of reliably retrieving all competing drug names, and
there is no accepted public benchmark for this task. To address the lack of
evaluation, we use LLM-based agents to transform five years of multi-modal,
unstructured diligence memos from a private biotech VC fund into a structured
evaluation corpus mapping indications to competitor drugs with normalized
attributes. We also introduce a competitor validating LLM-as-a-judge agent that
filters out false positives from the list of predicted competitors to maximize
precision and suppress hallucinations. On this benchmark, our
competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research
(65%) and Perplexity Labs (60%). The system is deployed in production with
enterprise users; in a case study with a biotech VC investment fund, analyst
turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the
competitive analysis.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [32] [Straggler-Resilient Federated Learning over A Hybrid Conventional and Pinching Antenna Network](https://arxiv.org/abs/2508.15821)
*Bibo Wu,Fang Fang,Ming Zeng,Xianbin Wang*

Main category: cs.IT

TL;DR: 该论文提出了一种混合传统和捏合天线网络(HCPAN)，通过捏合天线技术优化联邦学习系统的通信效率，使用模糊逻辑分类和深度强化学习算法解决资源分配问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中的"拖后腿"问题，通过动态建立强视距链接来提高通信效率，优化非正交多址接入联邦学习系统的性能。

Method: 提出混合传统和捏合天线网络框架，采用模糊逻辑客户端分类方案平衡数据贡献和通信条件，使用深度强化学习算法联合优化捏合天线部署和资源分配。

Result: 仿真结果验证了所提方案通过优化捏合天线部署来增强联邦学习性能的优越性。

Conclusion: 捏合天线技术能有效改善联邦学习通信效率，提出的混合网络框架和优化算法为解决系统复杂性问题提供了有效解决方案。

Abstract: Leveraging pinching antennas in wireless network enabled federated learning
(FL) can effectively mitigate the common "straggler" issue in FL by dynamically
establishing strong line-of-sight (LoS) links on demand. This letter proposes a
hybrid conventional and pinching antenna network (HCPAN) to significantly
improve communication efficiency in the non-orthogonal multiple access
(NOMA)-enabled FL system. Within this framework, a fuzzy logic-based client
classification scheme is first proposed to effectively balance clients' data
contributions and communication conditions. Given this classification, we
formulate a total time minimization problem to jointly optimize pinching
antenna placement and resource allocation. Due to the complexity of variable
coupling and non-convexity, a deep reinforcement learning (DRL)-based algorithm
is developed to effectively address this problem. Simulation results validate
the superiority of the proposed scheme in enhancing FL performance via the
optimized deployment of pinching antenna.

</details>


### [33] [Tri-Hybrid Beamforming for Radiation-Center Reconfigurable Antenna Array: Spectral Efficiency and Energy Efficiency](https://arxiv.org/abs/2508.15924)
*Yinchen Li,Chenhao Qi,Shiwen Mao,Octavia A. Dobre*

Main category: cs.IT

TL;DR: 基于辐射中心可重配天线数组的三混合放形成复合架构，包含数字、模拟和电磁放形成，通过三循环交替优化算法实现频谱效率和能量效率的最大化。


<details>
  <summary>Details</summary>
Motivation: 为了在硬件和功耗约束下提升天线系统的频谱效率和能量效率，探索基于辐射中心可重配天线数组的新型混合放形成方案。

Method: 提出三混合放形成架构，包含数字、模拟和电磁放形成。使用三循环交替优化算法：内循环和中循环用惩罚对偶分解优化数字和模拟放形成，外循环用坐标递减法确定辐射中心选择。对于能量效率最大化，提出双二次变换基于的分数规划算法。

Result: 模拟结果表明辐射中心可重配天线数组在提高频谱效率和能量效率方面具有巨大潜力。拉格朗日对偶变换基于的分数规划算法在仅有轻微性能损失的情况下显著降低了计算复杂度。

Conclusion: 该研究提出的三混合放形成架构和相应优化算法能够有效提升天线系统性能，并提供了计算复杂度优化的方案，为未来高效通信系统设计提供了有价值的技术路径。

Abstract: In this paper, we propose a tri-hybrid beamforming (THBF) architecture based
on the radiation-center (RC) reconfigurable antenna array (RCRAA), including
the digital beamforming, analog beamforming, and electromagnetic (EM)
beamforming, where the EM beamformer design is modeled as RC selection. Aiming
at spectral efficiency (SE) maximization subject to the hardware and power
consumption constraints, we propose a tri-loop alternating optimization (TLAO)
scheme for the THBF design, where the digital and analog beamformers are
optimized based on the penalty dual decomposition in the inner and middle
loops, and the RC selection is determined through the coordinate descent method
in the outer loop. Aiming at energy-efficiency (EE) maximization, we develop a
dual quadratic transform-based fractional programming (DQTFP) scheme, where the
TLAO scheme is readily used for the THBF design. To reduce the computational
complexity, we propose the Lagrange dual transform-based fractional programming
(LDTFP) scheme, where each iteration has a closed-form solution. Simulation
results demonstrate the great potential of the RCRAA in improving both SE and
EE. Compared to the DQTFP scheme, the LDTFP scheme significantly reduces the
computational complexity with only minor performance loss.

</details>


### [34] [Multi-User SLNR-Based Precoding With Gold Nanoparticles in Vehicular VLC Systems](https://arxiv.org/abs/2508.16075)
*Geonho Han,Hyuckjin Choi,Hyesang Cho,Jeong Hyeon Han,Ki Tae Nam,Junil Choi*

Main category: cs.IT

TL;DR: 本文提出一种基于金纳米粒子(GNPs)和SLNR预编码的方法，通过降低LED相关性和优化RGB比例来提高车辆可见光通信系统的多用户性能和安全性


<details>
  <summary>Details</summary>
Motivation: 车辆可见光通信(VVLC)系统中LED灯之间存在高相关性，导致空间复用困难，限制了数据速率的提升，需要新技术来解决这一挑战

Method: 利用金纳米粒子(GNPs)的旋光特性降低LED相关性，采用SLNR基础的预编码支持多用户，通过普通雷达啉啉比和逐步凸近优化法解决RGB比例优化问题

Result: 模拟结果显示，优化后的SLNR预编码方案在多用户车辆环境中显著提高了总速率，在监听场景中也提升了保密速率

Conclusion: 证明了降低LED相关性和优化RGB比例对提升VVLC系统性能的关键作用，为车辆可见光通信提供了有效的性能改善方案

Abstract: Visible spectrum is an emerging frontier in wireless communications for
enhancing connectivity and safety in vehicular environments. The vehicular
visible light communication (VVLC) system is a key feature in leveraging
existing infrastructures, but it still has several critical challenges.
Especially, VVLC channels are highly correlated due to the small gap between
light emitting diodes (LEDs) in each headlight, making it difficult to increase
data rates by spatial multiplexing. In this paper, we exploit recently
synthesized gold nanoparticles (GNPs) to reduce the correlation between LEDs,
i.e., the chiroptical properties of GNPs for differential absorption depending
on the azimuth angle of incident light are used to mitigate the LED
correlation. In addition, we adopt a signal-to-leakage-plus-noise ratio
(SLNR)-based precoder to support multiple users. The ratio of RGB light sources
in each LED also needs to be optimized to maximize the sum SLNR satisfying a
white light constraint for illumination since the GNPs can vary the color of
transmitted light by the differential absorption across wavelength. The
nonconvex optimization problems for precoders and RGB ratios can be solved by
the generalized Rayleigh quotient with the approximated shot noise and
successive convex approximation (SCA). The simulation results show that the
SLNR-based precoder with the optimized RGB ratios significantly improves the
sum rate in a multi-user vehicular environment and the secrecy rate in a
wiretapping scenario. The proposed SLNR-based precoding verifies that the
decorrelation between LEDs and the RGB ratio optimization are essential to
enhance the VVLC performance.

</details>


### [35] [Implicit and Explicit Formulas of the Joint RDF for a Tuple of Multivariate Gaussian Sources with Individual Square-Error Distortions](https://arxiv.org/abs/2508.16301)
*Evagoras Stylianou,Charalambos D. Charalambous,Themistoklis Charalambous*

Main category: cs.IT

TL;DR: 本文分析了相关多元高斯源的联合率失真函数，提出了闭式解和对称失真情况下的显式表达式


<details>
  <summary>Details</summary>
Motivation: 研究相关多元高斯源在个体平方误差失真下的联合率失真函数，以改进对该问题的理解和闭式解的发展

Method: 利用Hotelling的典型变量形式，推导出涉及非线性方程组的闭式特征，并在对称失真情况下用两个注水变量显式表达

Result: 获得了联合率失真函数的闭式特征和对称失真情况下的显式解，大大提高了对多元高斯源率失真问题的理解

Conclusion: 该研究显著推进了多元高斯源在个体平方误差失真下联合率失真函数的闭式解发展，为相关领域提供了重要理论进展

Abstract: This paper analyzes the joint Rate Distortion Function (RDF) of correlated
multivariate Gaussian sources with individual square-error distortions.
Leveraging Hotelling's canonical variable form, presented is a closed-form
characterization of the joint RDF, that involves {a system of nonlinear
equations. Furthermore, for the special case of symmetric distortions (i.e.,
equal distortions), the joint RDF is explicitly expressed in terms of} two
water-filling variables. The results greatly improve our understanding and
advance the development of closed-form solutions of the joint RDF for
multivariate Gaussian sources with individual square-error distortions.

</details>


### [36] [Agentic AI Empowered Multi-UAV Trajectory Optimization in Low-Altitude Economy Networks](https://arxiv.org/abs/2508.16379)
*Feibo Jiang,Li Dong,Xitao Pan,Kezhi Wang,Cunhua Pan*

Main category: cs.IT

TL;DR: ARMAIT框架结合Agentic RAG和Mamba-Attention集成Transformer，用于多无人机轨迹优化，通过自主解释任务需求并实现离散和连续轨迹空间的统一策略优化。


<details>
  <summary>Details</summary>
Motivation: 解决多无人机轨迹优化中高维状态空间、复杂约束和不同系统规模下的高效建模挑战，需要能够自主理解任务需求并有效处理长序列依赖的智能框架。

Method: 1) 基于LLM的Agentic RAG自主解析任务需求；2) 提出Mamba-Attention集成Transformer(MAIT)混合架构；3) 设计Trajectory-Group Relative Policy Optimization(T-GRPO)方法进行统一策略优化

Result: 大量实验结果验证了ARMAIT框架的可行性和有效性，在多无人机轨迹优化任务中表现出色。

Conclusion: ARMAIT框架成功整合了Agentic AI、RAG和混合神经网络架构，为多无人机轨迹优化提供了有效的解决方案，实现了自主任务理解和高效策略优化。

Abstract: This paper proposes a novel Agentic Retrieval-augmented generation with
Mamba-Attention Integrated Transformer (ARMAIT) framework for multi-Unmanned
Aerial Vehicle (UAV) trajectory optimization. The framework is built upon Large
Language Models (LLMs), incorporating Retrieval-Augmented Generation (RAG)
empowered by Agentic AI and integrated with a UAV-specific knowledge base.
Through the Agentic RAG, the LLM autonomously interprets high-level task
requirements and identifies the key components necessary for trajectory
optimization, including model inputs and outputs, network architecture, reward
functions, and task constraints. To support efficient modeling across different
system scales, we introduce the Mamba-Attention Integrated Transformer (MAIT),
a hybrid neural architecture that combines the long-range dependency modeling
capability of attention mechanisms with the efficient temporal dynamic
representation of Mamba. Furthermore, a Trajectory-Group Relative Policy
Optimization (T-GRPO) method is proposed to achieve unified policy gradient
optimization in both discrete and continuous trajectory spaces for MAIT
training. Extensive experimental results validate the feasibility and
effectiveness of the proposed ARMAIT framework.

</details>


### [37] [Enhanced Successive Cancellation List Decoder for Long Polar Codes Targeting 6G Air Interface](https://arxiv.org/abs/2508.16498)
*Jiajie Li,Sihui Shen,Warren J. Gross*

Main category: cs.IT

TL;DR: 提出了一系列算法改进技术，包括PE SCL、BE SCL、BE GPSCL和IDA解码方案，在保持解码性能的同时大幅降低长极码的内存使用和计算复杂度


<details>
  <summary>Details</summary>
Motivation: 为满足6G通信标准对能源和面积成本降低的要求，需要解决长极码（8K比特）在SCL解码中的内存使用和计算复杂度挑战

Method: 提出四种算法技术：1、污波增强(PE)SCL解码器；2、偏置增强(BE)SCL解码器；3、BE广义分区(GPSCL)解码器；4、输入分布感知(IDA)解码

Result: PE SCL解码器用L列表大小达到2L的解码性能；BE GPSCL解码器内存使用降低67%；IDA解码计算复杂度降低5.4倍，性能损失仅0.05dB以内

Conclusion: 该方法为6G通信标准中长极码的实际应用提供了可行的解决方案，在保持高解码性能的同时大幅优化了硬件资源消耗

Abstract: The 6th generation communication standard's air interface requires innovation
in channel coding to fulfill anticipated energy and area cost reduction
requirements. In this paper, we propose algorithmic techniques to enable the
implementation of long polar codes (e.g., length 8K bits) in next-generation
communications standards by addressing key challenges in memory usage and
computational complexity presented by successive decoding list (SCL) polar
decoding. Perturbation-enhanced (PE) successive cancelation list (SCL) decoders
with a list size of $L$ reach the decoding performance of the SCL decoder with
a list size of $2L$. The proposed bias-enhanced (BE) SCL decoders, which
simplifies the PE SCL decoder based on insights gained by an ablation study,
returns similar decoding performance to PE SCL decoders. Also, proposed BE
generalized partitioned SCL (GPSCL) decoders with a list size of $8$ have a
$67\%$ reduction in the memory usage and similar decoding performance compared
to SCL decoders with a list size of $16$. Furthermore, input-distribution-aware
(IDA) decoding is applied to BE GPSCL decoders. Up to $5.4\times$ reduction in
the computational complexity is achieved compared to SCL decoders with a list
size of $16$. The degraded decoding performance is at most $0.05\text{ dB}$
compared to BE GPSCL decoders without IDA decoding.

</details>
