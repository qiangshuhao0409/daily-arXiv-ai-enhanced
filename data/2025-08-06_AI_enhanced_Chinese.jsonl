{"id": "2508.02770", "categories": ["cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.02770", "abs": "https://arxiv.org/abs/2508.02770", "authors": ["Kirill Sokolov", "Alexander Korotin"], "title": "Exponential convergence rate for Iterative Markovian Fitting", "comment": null, "summary": "We consider the discrete-time Schr\\\"odinger bridge problem on a finite state\nspace. Although it has been known that the Iterative Markovian Fitting (IMF)\nalgorithm converges in Kullback-Leibler divergence to the ground truth\nsolution, the speed of that convergence remained unquantified. In this work, we\nestablish for the first time that IMF exhibits exponential convergence with an\nexplicit contraction factor.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u8bc1\u660e\u4e86\u79bb\u6563\u65f6\u95f4Schr\u00f6dinger\u6865\u95ee\u9898\u4e2dIMF\u7b97\u6cd5\u7684\u6307\u6570\u6536\u655b\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86\u660e\u786e\u7684\u6536\u7f29\u56e0\u5b50\u3002", "motivation": "\u7814\u7a76IMF\u7b97\u6cd5\u5728Kullback-Leibler\u6563\u5ea6\u4e0b\u7684\u6536\u655b\u901f\u5ea6\uff0c\u586b\u8865\u4e86\u4e4b\u524d\u672a\u91cf\u5316\u7684\u7a7a\u767d\u3002", "method": "\u5206\u6790\u4e86\u79bb\u6563\u65f6\u95f4Schr\u00f6dinger\u6865\u95ee\u9898\u5728\u6709\u9650\u72b6\u6001\u7a7a\u95f4\u4e0a\u7684IMF\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86IMF\u7b97\u6cd5\u5177\u6709\u6307\u6570\u6536\u655b\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u6536\u7f29\u56e0\u5b50\u3002", "conclusion": "IMF\u7b97\u6cd5\u5728\u89e3\u51b3Schr\u00f6dinger\u6865\u95ee\u9898\u65f6\u5177\u6709\u9ad8\u6548\u7684\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2508.02694", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.02694", "abs": "https://arxiv.org/abs/2508.02694", "authors": ["Ningning Wang", "Xavier Hu", "Pai Liu", "He Zhu", "Yue Hou", "Heyuan Huang", "Shengyu Zhang", "Jian Yang", "Jiaheng Liu", "Ge Zhang", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "Efficient Agents: Building Effective Agents While Reducing Cost", "comment": "Work in progress. For GitHub repository, see\n  https://github.com/OPPO-PersonalAI/OAgents", "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from $0.398 to $0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u4ee3\u7406\u7cfb\u7edf\u7684\u6548\u7387\u4e0e\u6027\u80fd\u6743\u8861\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u4f18\u5316\u7684\u65b0\u578b\u4ee3\u7406\u6846\u67b6Efficient Agents\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u7cfb\u7edf\u590d\u6742\u6027\u548c\u6210\u672c\u7684\u589e\u52a0\uff0c\u5982\u4f55\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u6210\u672c\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790GAIA\u57fa\u51c6\uff0c\u8bc4\u4f30LLM\u9aa8\u5e72\u9009\u62e9\u3001\u4ee3\u7406\u6846\u67b6\u8bbe\u8ba1\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u5e76\u4f7f\u7528\u6210\u672c-\u901a\u8fc7\u6307\u6807\u91cf\u5316\u6548\u7387\u4e0e\u6027\u80fd\u7684\u6743\u8861\u3002", "result": "\u63d0\u51fa\u7684Efficient Agents\u6846\u67b6\u5728\u4fdd\u630196.7%\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u8fd0\u8425\u6210\u672c\u964d\u4f4e\u4e8628.4%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86AI\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2508.02960", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.02960", "abs": "https://arxiv.org/abs/2508.02960", "authors": ["Pedro Duarte", "Andr\u00e9 Coelho", "Manuel Ricardo"], "title": "A Reinforcement Learning Framework for Mobility Control of gNBs in Dynamic Radio Access Networks", "comment": null, "summary": "The increasing complexity of wireless environments, characterized by user\nmobility and dynamic obstructions, poses challenges for the maintenance of\nLine-of-Sight (LoS) connectivity. Mobile base stations (gNBs) stand as a\npromising solution by physically relocating to restore or sustain LoS, thereby\nnecessitating the development of intelligent algorithms for autonomous movement\ncontrol.\n  As part of the CONVERGE research project, which is developing an experimental\nchamber to integrate computer vision (CV) into mobile networks and enhance\nQuality of Service (QoS) in dynamic wireless environments, this paper presents\ntwo key contributions. First, we introduce the CONVERGE Chamber Simulator\n(CC-SIM), a 3D simulation environment for developing, training, and validating\nmobility control algorithms for mobile gNBs. CC-SIM models user and obstacle\nmobility, visual occlusion, and Radio Frequency (RF) propagation behavior. It\nsupports both offline reinforcement learning and real-time testing through\ntight integration with a standalone 5G system via the OpenAirInterface (OAI) RF\nsimulator, enabling validation under realistic network conditions.\n  Second, leveraging CC-SIM, we develop a Deep Q-Network (DQN) agent that\nlearns to reposition the gNB proactively in response to dynamic environmental\nchanges. Experiments across three representative use cases show that the\ntrained agent significantly reduces LoS blockage time - by up to 42% - when\ncompared to static deployments. These results highlight the effectiveness of\nlearning-based mobility control in adaptive next-generation wireless networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u6a21\u62df\u73af\u5883\uff08CC-SIM\uff09\u7684\u79fb\u52a8\u57fa\u7ad9\uff08gNB\uff09\u667a\u80fd\u63a7\u5236\u7b97\u6cd5\uff0c\u901a\u8fc7DQN\u4ee3\u7406\u663e\u8457\u51cf\u5c11\u4e86\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e2d\u7684LoS\u963b\u585e\u65f6\u95f4\u3002", "motivation": "\u65e0\u7ebf\u73af\u5883\u7684\u590d\u6742\u6027\uff08\u5982\u7528\u6237\u79fb\u52a8\u6027\u548c\u52a8\u6001\u969c\u788d\u7269\uff09\u5bf9\u7ef4\u6301LoS\u8fde\u63a5\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u667a\u80fd\u7b97\u6cd5\u63a7\u5236\u79fb\u52a8\u57fa\u7ad9\u4ee5\u4f18\u5316\u8fde\u63a5\u3002", "method": "\u5f00\u53d1\u4e86CC-SIM\u6a21\u62df\u73af\u5883\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u9a8c\u8bc1\u79fb\u52a8gNB\u7684\u63a7\u5236\u7b97\u6cd5\uff0c\u5e76\u5229\u7528DQN\u4ee3\u7406\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u57fa\u7ad9\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDQN\u4ee3\u7406\u5728\u4e09\u79cd\u5178\u578b\u573a\u666f\u4e0b\u5c06LoS\u963b\u585e\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe42%\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u79fb\u52a8\u63a7\u5236\u7b97\u6cd5\u5728\u4e0b\u4e00\u4ee3\u81ea\u9002\u5e94\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2508.02996", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.02996", "abs": "https://arxiv.org/abs/2508.02996", "authors": ["Xuan Guang", "Xiufang Sun", "Ruze Zhang"], "title": "Distributed Source Coding for Compressing Vector-Linear Functions", "comment": "49 pages, 22 figures", "summary": "Inspired by mobile satellite communication systems and the important and\nprevalent applications of computational tasks, we consider a distributed source\ncoding model for compressing vector-linear functions, which consists of\nmultiple sources, multiple encoders and a decoder linked to all the encoders.\nEach encoder has access to a certain subset of the sources and the decoder is\nrequired to compute with zero error a vector-linear function of the source\ninformation, which corresponds to a matrix $T$. The connectivity state between\nthe sources and the encoders and the vector-linear function are all arbitrary.\nIn the paper, we are interested in the function-compression capacity to measure\nthe efficiency of using the system. We first present a general lower bound on\nthe function-compression capacity applicable to arbitrary connectivity states\nand vector-linear functions. Next, we confine to the nontrivial models with\nonly three sources and no more than three encoders, and prove that all the\n$3\\times2$ column-full-rank matrices $T$ can be divided into two types $T_1$\nand $T_2$, for which the function-compression capacities are identical if the\nmatrices $T$ have the same type. We explicitly characterize the\nfunction-compression capacities for two most nontrivial models associated with\n$T_2$ by a novel approach of both upper bounding and lower bounding the size of\nimage sets of encoding functions. This shows that the lower bound thus obtained\nis not always tight. Rather, by completely characterizing their capacities, the\nlower bound is tight for all the models associated with $T_1$ and all the\nmodels associated with $T_2$ except for the two most nontrivial models. We\nfinally apply the obtained results to network function computation and answer\nthe open problem whether the best known upper bound proved by Guang et. al.\n(2019) on computing capacity is in general asymptotically tight.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5206\u5e03\u5f0f\u6e90\u7f16\u7801\u6a21\u578b\u4e2d\u7684\u5411\u91cf\u7ebf\u6027\u51fd\u6570\u538b\u7f29\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u51fd\u6570\u538b\u7f29\u5bb9\u91cf\u7684\u901a\u7528\u4e0b\u754c\uff0c\u5e76\u5bf9\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002", "motivation": "\u53d7\u79fb\u52a8\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\u548c\u8ba1\u7b97\u4efb\u52a1\u5e94\u7528\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u9ad8\u6548\u538b\u7f29\u5411\u91cf\u7ebf\u6027\u51fd\u6570\u3002", "method": "\u63d0\u51fa\u901a\u7528\u4e0b\u754c\uff0c\u5e76\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\uff083\u6e90\u3001\u4e0d\u8d85\u8fc73\u7f16\u7801\u5668\uff09\u5206\u7c7b\u77e9\u9635T\uff0c\u901a\u8fc7\u4e0a\u4e0b\u754c\u65b9\u6cd5\u5206\u6790\u7f16\u7801\u51fd\u6570\u56fe\u50cf\u96c6\u5927\u5c0f\u3002", "result": "\u8bc1\u660e\u4e86\u901a\u7528\u4e0b\u754c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4e0d\u7d27\uff0c\u5e76\u5b8c\u5168\u523b\u753b\u4e86T1\u548cT2\u7c7b\u77e9\u9635\u7684\u538b\u7f29\u5bb9\u91cf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5e94\u7528\u4e8e\u7f51\u7edc\u51fd\u6570\u8ba1\u7b97\uff0c\u89e3\u51b3\u4e86\u5df2\u77e5\u4e0a\u754c\u662f\u5426\u6e10\u8fd1\u7d27\u7684\u5f00\u95ee\u9898\u3002"}}
{"id": "2508.02697", "categories": ["cs.AI", "cs.CE", "03B35 (Primary) 03A99, 03B10, 03B25, 68V15, 03C07 (Secondary)", "I.2.3; I.2.4; F.4.1; F.2.2; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.02697", "abs": "https://arxiv.org/abs/2508.02697", "authors": ["Mikhail Soutchanski", "Yongmei Liu"], "title": "Planning with Dynamically Changing Domains", "comment": "A revised version of the paper accepted to the 1st International\n  Workshop on Trends in Knowledge Representation and Reasoning organized as a\n  IJCAI 2025 workshop that takes place in August 2025 in Montreal, Canada. See\n  the details at https://tkr2025.krportal.org/programme.html", "summary": "In classical planning and conformant planning, it is assumed that there are\nfinitely many named objects given in advance, and only they can participate in\nactions and in fluents. This is the Domain Closure Assumption (DCA). However,\nthere are practical planning problems where the set of objects changes\ndynamically as actions are performed; e.g., new objects can be created, old\nobjects can be destroyed. We formulate the planning problem in first-order\nlogic, assume an initial theory is a finite consistent set of fluent literals,\ndiscuss when this guarantees that in every situation there are only finitely\nmany possible actions, impose a finite integer bound on the length of the plan,\nand propose to organize search over sequences of actions that are grounded at\nplanning time. We show the soundness and completeness of our approach. It can\nbe used to solve the bounded planning problems without DCA that belong to the\nintersection of sequential generalized planning (without sensing actions) and\nconformant planning, restricted to the case without the disjunction over fluent\nliterals. We discuss a proof-of-the-concept implementation of our planner.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9886\u57df\u95ed\u5305\u5047\u8bbe\uff08DCA\uff09\u7684\u6709\u754c\u89c4\u5212\u95ee\u9898\u89e3\u51b3\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u5bf9\u8c61\u53d8\u5316\u7684\u573a\u666f\uff0c\u5e76\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u5b8c\u5907\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u7ecf\u5178\u89c4\u5212\u548c\u4e00\u81f4\u6027\u89c4\u5212\u4e2d\uff0c\u9886\u57df\u95ed\u5305\u5047\u8bbe\uff08DCA\uff09\u9650\u5236\u4e86\u5bf9\u8c61\u7684\u52a8\u6001\u53d8\u5316\uff0c\u800c\u5b9e\u9645\u89c4\u5212\u95ee\u9898\u4e2d\u5bf9\u8c61\u53ef\u80fd\u52a8\u6001\u589e\u51cf\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4e00\u9636\u903b\u8f91\u6784\u5efa\u89c4\u5212\u95ee\u9898\uff0c\u521d\u59cb\u7406\u8bba\u4e3a\u6709\u9650\u7684\u6d41\u6587\u5b57\u96c6\uff0c\u9650\u5236\u8ba1\u5212\u957f\u5ea6\uff0c\u5e76\u5728\u89c4\u5212\u65f6\u5bf9\u52a8\u4f5c\u5e8f\u5217\u8fdb\u884c\u63a5\u5730\u641c\u7d22\u3002", "result": "\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u5b8c\u5907\u6027\u548c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u65e0DCA\u7684\u6709\u754c\u89c4\u5212\u95ee\u9898\uff0c\u4e14\u5c5e\u4e8e\u987a\u5e8f\u5e7f\u4e49\u89c4\u5212\u548c\u4e00\u81f4\u6027\u89c4\u5212\u7684\u4ea4\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u5bf9\u8c61\u53d8\u5316\u7684\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u5c55\u793a\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2508.03095", "categories": ["cs.NI", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.03095", "abs": "https://arxiv.org/abs/2508.03095", "authors": ["Aditi Singh", "Abul Ehtesham", "Ramesh Raskar", "Mahesh Lambe", "Pradyumna Chari", "Jared James Grogan", "Abhishek Singh", "Saket Kumar"], "title": "A Survey of AI Agent Registry Solutions", "comment": null, "summary": "As As autonomous AI agents scale across cloud, enterprise, and decentralized\nenvironments, the need for standardized registry systems to support discovery,\nidentity, and capability sharing has become essential. This paper surveys three\nprominent registry approaches each defined by a unique metadata model: MCP's\nmcp.json, A2A's Agent Card, and NANDA's AgentFacts. MCP uses a centralized\nmetaregistry with GitHub authenticated publishing and structured metadata for\nserver discovery. A2A enables decentralized interaction via JSON-based Agent\nCards, discoverable through well-known URIs, curated catalogs, or direct\nconfiguration. NANDA Index introduces AgentFacts, a cryptographically\nverifiable and privacy-preserving metadata model designed for dynamic\ndiscovery, credentialed capabilities, and cross-domain interoperability. These\napproaches are compared across four dimensions: security, scalability,\nauthentication, and maintainability. The paper concludes with suggestions and\nrecommendations to guide future design and adoption of registry systems for the\nInternet of AI Agents.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4e09\u79cdAI\u4ee3\u7406\u6ce8\u518c\u7cfb\u7edf\uff08MCP\u3001A2A\u3001NANDA\uff09\uff0c\u6bd4\u8f83\u4e86\u5b83\u4eec\u5728\u5b89\u5168\u6027\u3001\u53ef\u6269\u5c55\u6027\u3001\u8ba4\u8bc1\u548c\u7ef4\u62a4\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u8bbe\u8ba1\u7684\u5efa\u8bae\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u4e91\u3001\u4f01\u4e1a\u548c\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u7684\u666e\u53ca\uff0c\u6807\u51c6\u5316\u7684\u6ce8\u518c\u7cfb\u7edf\u5bf9\u4e8e\u652f\u6301\u53d1\u73b0\u3001\u8eab\u4efd\u548c\u80fd\u529b\u5171\u4eab\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5206\u6790\u4e86\u4e09\u79cd\u6ce8\u518c\u7cfb\u7edf\uff1aMCP\u7684\u96c6\u4e2d\u5f0f\u5143\u6ce8\u518c\u3001A2A\u7684\u57fa\u4e8eJSON\u7684\u53bb\u4e2d\u5fc3\u5316\u4ea4\u4e92\uff0c\u4ee5\u53caNANDA\u7684\u52a0\u5bc6\u53ef\u9a8c\u8bc1\u9690\u79c1\u4fdd\u62a4\u6a21\u578b\u3002", "result": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u65b9\u6cd5\u5728\u56db\u4e2a\u7ef4\u5ea6\uff08\u5b89\u5168\u6027\u3001\u53ef\u6269\u5c55\u6027\u3001\u8ba4\u8bc1\u3001\u7ef4\u62a4\u6027\uff09\u7684\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765AI\u4ee3\u7406\u6ce8\u518c\u7cfb\u7edf\u8bbe\u8ba1\u548c\u91c7\u7528\u7684\u5efa\u8bae\u3002"}}
{"id": "2508.03196", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.03196", "abs": "https://arxiv.org/abs/2508.03196", "authors": ["Gang Wang", "Xuan Gao", "Sihem Mesnager", "Fang-Wei Fu"], "title": "Multilevel inserting constructions for constant dimension subspace codes", "comment": "22 pages,1 table. Des.Codes Cryptogr. (2025)", "summary": "Subspace codes, especially constant dimension subspace codes (CDCs),\nrepresent an intriguing domain that can be used to conduct basic coding theory\ninvestigations. They have received widespread attention due to their\napplications in random network coding. This paper presents inverse bilateral\nmultilevel construction by introducing inverse bilateral identifying vectors\nand inverse bilateral Ferrers diagram rank-metric codes. By inserting the\ninverse bilateral multilevel construction into the double multilevel\nconstruction and bilateral multilevel construction, an effective construction\nfor CDCs is provided. Furthermore, via providing a new set of bilateral\nidentifying vectors, we give another efficient construction for CDCs. In this\narticle, several CDCs are exhibited, equipped with the rank-metric, with larger\nsizes than the known ones in the existing literature. From a practical\nstandpoint, our results could help in the pragmatic framework of\nconstant-dimension-lifted rank-metric codes for applications in network coding.\nThe ratio of the new lower bound to the known upper bound for some CDCs is\ncalculated, which is greater than 0.94548 for any prime power $q \\geq 3.$", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9006\u53cc\u8fb9\u591a\u7ea7\u6784\u9020\u548c\u65b0\u7684\u53cc\u8fb9\u6807\u8bc6\u5411\u91cf\u6784\u5efa\u6052\u5b9a\u7ef4\u5ea6\u5b50\u7a7a\u95f4\u7801\uff08CDCs\uff09\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6bd4\u73b0\u6709\u6587\u732e\u66f4\u5927\u7684\u7801\u96c6\uff0c\u5e76\u8ba1\u7b97\u4e86\u65b0\u7684\u4e0b\u754c\u4e0e\u5df2\u77e5\u4e0a\u754c\u7684\u6bd4\u7387\u3002", "motivation": "\u6052\u5b9a\u7ef4\u5ea6\u5b50\u7a7a\u95f4\u7801\uff08CDCs\uff09\u5728\u968f\u673a\u7f51\u7edc\u7f16\u7801\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u6784\u9020\u548c\u6027\u80fd\u4ecd\u9700\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u9006\u53cc\u8fb9\u6807\u8bc6\u5411\u91cf\u548c\u9006\u53cc\u8fb9Ferrers\u56fe\u79e9\u5ea6\u91cf\u7801\uff0c\u7ed3\u5408\u591a\u7ea7\u6784\u9020\u65b9\u6cd5\uff0c\u63d0\u51fa\u65b0\u7684CDC\u6784\u9020\u3002", "result": "\u5c55\u793a\u4e86\u591a\u4e2a\u5177\u6709\u66f4\u5927\u5c3a\u5bf8\u7684CDC\u7801\u96c6\uff0c\u65b0\u4e0b\u754c\u4e0e\u5df2\u77e5\u4e0a\u754c\u7684\u6bd4\u7387\u5927\u4e8e0.94548\uff08q\u22653\u7684\u7d20\u6570\u5e42\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728CDC\u6784\u9020\u4e2d\u6709\u6548\uff0c\u4e3a\u7f51\u7edc\u7f16\u7801\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2508.02734", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.02734", "abs": "https://arxiv.org/abs/2508.02734", "authors": ["Weiyu Luo", "Chenfeng Xiong"], "title": "Recovering Individual-Level Activity Sequences from Location-Based Service Data Using a Novel Transformer-Based Model", "comment": "20 pages, 5 figures", "summary": "Location-Based Service (LBS) data provides critical insights into human\nmobility, yet its sparsity often yields incomplete trip and activity sequences,\nmaking accurate inferences about trips and activities difficult. We raise a\nresearch problem: Can we use activity sequences derived from high-quality LBS\ndata to recover incomplete activity sequences at the individual level? This\nstudy proposes a new solution, the Variable Selection Network-fused Insertion\nTransformer (VSNIT), integrating the Insertion Transformer's flexible sequence\nconstruction with the Variable Selection Network's dynamic covariate handling\ncapability, to recover missing segments in incomplete activity sequences while\npreserving existing data. The findings show that VSNIT inserts more diverse,\nrealistic activity patterns, more closely matching real-world variability, and\nrestores disrupted activity transitions more effectively aligning with the\ntarget. It also performs significantly better than the baseline model across\nall metrics. These results highlight VSNIT's superior accuracy and diversity in\nactivity sequence recovery tasks, demonstrating its potential to enhance LBS\ndata utility for mobility analysis. This approach offers a promising framework\nfor future location-based research and applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVSNIT\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4e0d\u5b8c\u6574\u7684LBS\u6570\u636e\u4e2d\u6062\u590d\u6d3b\u52a8\u5e8f\u5217\uff0c\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "LBS\u6570\u636e\u7684\u7a00\u758f\u6027\u5bfc\u81f4\u6d3b\u52a8\u5e8f\u5217\u4e0d\u5b8c\u6574\uff0c\u96be\u4ee5\u51c6\u786e\u63a8\u65ad\u51fa\u884c\u548c\u6d3b\u52a8\u3002\u7814\u7a76\u76ee\u6807\u662f\u5229\u7528\u9ad8\u8d28\u91cfLBS\u6570\u636e\u6062\u590d\u4e2a\u4f53\u5c42\u9762\u7684\u4e0d\u5b8c\u6574\u6d3b\u52a8\u5e8f\u5217\u3002", "method": "\u7ed3\u5408Insertion Transformer\u7684\u7075\u6d3b\u5e8f\u5217\u6784\u5efa\u80fd\u529b\u548cVariable Selection Network\u7684\u52a8\u6001\u534f\u53d8\u91cf\u5904\u7406\u80fd\u529b\uff0c\u63d0\u51faVSNIT\u65b9\u6cd5\u3002", "result": "VSNIT\u80fd\u63d2\u5165\u66f4\u591a\u6837\u5316\u3001\u66f4\u771f\u5b9e\u7684\u6d3b\u52a8\u6a21\u5f0f\uff0c\u6062\u590d\u7684\u6d3b\u52a8\u5e8f\u5217\u66f4\u63a5\u8fd1\u771f\u5b9e\u4e16\u754c\u7684\u53d8\u5316\uff0c\u4e14\u5728\u5404\u9879\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "VSNIT\u5728\u6d3b\u52a8\u5e8f\u5217\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u4e3a\u672a\u6765\u57fa\u4e8e\u4f4d\u7f6e\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6846\u67b6\u3002"}}
{"id": "2508.03101", "categories": ["cs.NI", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.03101", "abs": "https://arxiv.org/abs/2508.03101", "authors": ["Sichao Wang", "Ramesh Raskar", "Mahesh Lambe", "Pradyumna Chari", "Rekha Singhal", "Shailja Gupta", "Rajesh Ranjan", "Ken Huang"], "title": "Using the NANDA Index Architecture in Practice: An Enterprise Perspective", "comment": null, "summary": "The proliferation of autonomous AI agents represents a paradigmatic shift\nfrom traditional web architectures toward collaborative intelligent systems\nrequiring sophisticated mechanisms for discovery, authentication, capability\nverification, and secure collaboration across heterogeneous protocol\nenvironments. This paper presents a comprehensive framework addressing the\nfundamental infrastructure requirements for secure, trustworthy, and\ninteroperable AI agent ecosystems. We introduce the NANDA (Networked AI Agents\nin a Decentralized Architecture) framework, providing global agent discovery,\ncryptographically verifiable capability attestation through AgentFacts, and\ncross-protocol interoperability across Anthropic's Modal Context Protocol\n(MCP), Google's Agent-to-Agent (A2A), Microsoft's NLWeb, and standard HTTPS\ncommunications. NANDA implements Zero Trust Agentic Access (ZTAA) principles,\nextending traditional Zero Trust Network Access (ZTNA) to address autonomous\nagent security challenges including capability spoofing, impersonation attacks,\nand sensitive data leakage. The framework defines Agent Visibility and Control\n(AVC) mechanisms enabling enterprise governance while maintaining operational\nautonomy and regulatory compliance. Our approach transforms isolated AI agents\ninto an interconnected ecosystem of verifiable, trustworthy intelligent\nservices, establishing foundational infrastructure for large-scale autonomous\nagent deployment across enterprise and consumer environments. This work\naddresses the critical gap between current AI agent capabilities and\ninfrastructure requirements for secure, scalable, multi-agent collaboration,\npositioning the foundation for next-generation autonomous intelligent systems.", "AI": {"tldr": "NANDA\u6846\u67b6\u4e3aAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u5b89\u5168\u3001\u53ef\u4fe1\u548c\u4e92\u64cd\u4f5c\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u62ec\u5168\u7403\u4ee3\u7406\u53d1\u73b0\u3001\u80fd\u529b\u9a8c\u8bc1\u548c\u8de8\u534f\u8bae\u4e92\u64cd\u4f5c\u6027\uff0c\u89e3\u51b3\u4e86\u5f53\u524dAI\u4ee3\u7406\u80fd\u529b\u4e0e\u57fa\u7840\u8bbe\u65bd\u9700\u6c42\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\u3002", "motivation": "\u968f\u7740\u81ea\u4e3bAI\u4ee3\u7406\u7684\u666e\u53ca\uff0c\u4f20\u7edf\u7f51\u7edc\u67b6\u6784\u9700\u8981\u8f6c\u5411\u534f\u4f5c\u667a\u80fd\u7cfb\u7edf\uff0c\u4f46\u7f3a\u4e4f\u53d1\u73b0\u3001\u8ba4\u8bc1\u3001\u80fd\u529b\u9a8c\u8bc1\u548c\u5b89\u5168\u534f\u4f5c\u7684\u673a\u5236\u3002NANDA\u6846\u67b6\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u57fa\u7840\u8bbe\u65bd\u9700\u6c42\u7684\u5173\u952e\u7a7a\u767d\u3002", "method": "\u63d0\u51faNANDA\u6846\u67b6\uff0c\u901a\u8fc7AgentFacts\u5b9e\u73b0\u52a0\u5bc6\u80fd\u529b\u9a8c\u8bc1\uff0c\u652f\u6301\u8de8\u534f\u8bae\u4e92\u64cd\u4f5c\u6027\uff0c\u5e76\u91c7\u7528\u96f6\u4fe1\u4efb\u4ee3\u7406\u8bbf\u95ee\uff08ZTAA\uff09\u539f\u5219\u89e3\u51b3\u5b89\u5168\u95ee\u9898\u3002", "result": "NANDA\u6846\u67b6\u6210\u529f\u5c06\u5b64\u7acb\u7684AI\u4ee3\u7406\u8f6c\u53d8\u4e3a\u53ef\u9a8c\u8bc1\u3001\u53ef\u4fe1\u7684\u4e92\u8054\u751f\u6001\u7cfb\u7edf\uff0c\u652f\u6301\u5927\u89c4\u6a21\u81ea\u4e3b\u4ee3\u7406\u90e8\u7f72\u3002", "conclusion": "NANDA\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u8bbe\u65bd\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u9700\u6c42\u3002"}}
{"id": "2508.03381", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.03381", "abs": "https://arxiv.org/abs/2508.03381", "authors": ["Seonjung Kim", "Yongjeong Oh", "Yongjune Kim", "Namyoon Lee", "Yo-Seb Jeon"], "title": "Channel Coding for Unequal Error Protection in Digital Semantic Communication", "comment": null, "summary": "Semantic communication is an emerging paradigm that prioritizes transmitting\ntask-relevant information over accurately delivering raw data bits. In this\npaper, we address an unequal error protection (UEP) problem in digital semantic\ncommunication, where bits of higher semantic importance require stronger\nprotection. To quantify bit-level importance, we leverage bit-flip\nprobabilities of semantic bits as target error protection levels, which are\njointly learned with semantic encoder and decoder. We propose two novel channel\ncoding frameworks aimed at minimizing the total blocklength while satisfying\nUEP constraints. First, we develop a bit-level UEP framework based on\nrepetition coding, in which the repetition number for each bit is optimized to\nprecisely meet its target bit-flip probability. Second, we introduce a\nblock-level UEP framework utilizing modern channel codes, where semantic bits\nwith similar target bit-flip probabilities are grouped to exploit coding gains.\nWithin this framework, we propose a bit-grouping algorithm guided by finite\nblocklength capacity analysis. Simulation results conducted on image\ntransmission tasks confirm that the proposed frameworks significantly\noutperform conventional approaches, yielding substantial improvements in both\ntask performance and transmission efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u578b\u4fe1\u9053\u7f16\u7801\u6846\u67b6\uff0c\u7528\u4e8e\u6570\u5b57\u8bed\u4e49\u901a\u4fe1\u4e2d\u7684\u4e0d\u7b49\u9519\u8bef\u4fdd\u62a4\uff08UEP\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u6bd4\u7279\u7ea7\u548c\u5757\u7ea7\u4fdd\u62a4\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u548c\u4f20\u8f93\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6570\u5b57\u8bed\u4e49\u901a\u4fe1\u4e2d\u6bd4\u7279\u7ea7\u8bed\u4e49\u91cd\u8981\u6027\u4e0d\u7b49\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u9ad8\u91cd\u8981\u6027\u6bd4\u7279\u5f97\u5230\u66f4\u5f3a\u4fdd\u62a4\u3002", "method": "1. \u57fa\u4e8e\u91cd\u590d\u7f16\u7801\u7684\u6bd4\u7279\u7ea7UEP\u6846\u67b6\uff1b2. \u5229\u7528\u73b0\u4ee3\u4fe1\u9053\u7801\u7684\u5757\u7ea7UEP\u6846\u67b6\uff0c\u7ed3\u5408\u6bd4\u7279\u5206\u7ec4\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u63d0\u6846\u67b6\u5728\u56fe\u50cf\u4f20\u8f93\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684UEP\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8bed\u4e49\u901a\u4fe1\u7684\u4efb\u52a1\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.02744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02744", "abs": "https://arxiv.org/abs/2508.02744", "authors": ["Peiran Wang", "Yaoning Yu", "Ke Chen", "Xianyang Zhan", "Haohan Wang"], "title": "Large Language Model-based Data Science Agent: A Survey", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has driven novel\napplications across diverse domains, with LLM-based agents emerging as a\ncrucial area of exploration. This survey presents a comprehensive analysis of\nLLM-based agents designed for data science tasks, summarizing insights from\nrecent studies. From the agent perspective, we discuss the key design\nprinciples, covering agent roles, execution, knowledge, and reflection methods.\nFrom the data science perspective, we identify key processes for LLM-based\nagents, including data preprocessing, model development, evaluation,\nvisualization, etc. Our work offers two key contributions: (1) a comprehensive\nreview of recent developments in applying LLMbased agents to data science\ntasks; (2) a dual-perspective framework that connects general agent design\nprinciples with the practical workflows in data science.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u5728\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4ece\u4ee3\u7406\u548c\u6570\u636e\u79d1\u5b66\u53cc\u89c6\u89d2\u5206\u6790\u4e86\u8bbe\u8ba1\u539f\u5219\u548c\u5173\u952e\u6d41\u7a0b\u3002", "motivation": "\u63a2\u7d22LLM\u4ee3\u7406\u5728\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u8fd1\u671f\u7814\u7a76\uff0c\u603b\u7ed3\u4ee3\u7406\u8bbe\u8ba1\u539f\u5219\uff08\u89d2\u8272\u3001\u6267\u884c\u3001\u77e5\u8bc6\u3001\u53cd\u601d\uff09\u548c\u6570\u636e\u79d1\u5b66\u6d41\u7a0b\uff08\u9884\u5904\u7406\u3001\u6a21\u578b\u5f00\u53d1\u3001\u8bc4\u4f30\u3001\u53ef\u89c6\u5316\u7b49\uff09\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u89c6\u89d2\u6846\u67b6\uff0c\u8fde\u63a5\u4ee3\u7406\u8bbe\u8ba1\u539f\u5219\u4e0e\u6570\u636e\u79d1\u5b66\u5b9e\u8df5\u3002", "conclusion": "\u4e3aLLM\u4ee3\u7406\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u7efc\u8ff0\u548c\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2508.03113", "categories": ["cs.NI", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.03113", "abs": "https://arxiv.org/abs/2508.03113", "authors": ["John Zinky", "Hema Seshadri", "Mahesh Lambe", "Pradyumna Chari", "Ramesh Raskar"], "title": "NANDA Adaptive Resolver: Architecture for Dynamic Resolution of AI Agent Names", "comment": null, "summary": "AdaptiveResolver is a dynamic microservice architecture designed to address\nthe limitations of static endpoint resolution for AI agent communication in\ndistributed, heterogeneous environments. Unlike traditional DNS or static URLs,\nAdaptiveResolver enables context-aware, real-time selection of communication\nendpoints based on factors such as geographic location, system load, agent\ncapabilities, and security threats. Agents advertise their Agent Name and\ncontext requirements through Agent Fact cards in an Agent Registry/Index. A\nrequesting Agent discovers a Target Agent using the registry. The Requester\nAgent can then resolve the Target Agent Name to obtain a tailored communication\nchannel to the agent based on actual environmental context between the agents.\nThe architecture supports negotiation of trust, quality of service, and\nresource constraints, facilitating flexible, secure, and scalable\nagent-to-agent interactions that go beyond the classic client-server model.\nAdaptiveResolver provides a foundation for robust, future-proof agent\ncommunication that can evolve with increasing ecosystem complexity.", "AI": {"tldr": "AdaptiveResolver\u662f\u4e00\u79cd\u52a8\u6001\u5fae\u670d\u52a1\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u5f02\u6784\u73af\u5883\u4e2dAI\u4ee3\u7406\u901a\u4fe1\u7684\u9759\u6001\u7aef\u70b9\u89e3\u6790\u9650\u5236\u3002", "motivation": "\u4f20\u7edfDNS\u6216\u9759\u6001URL\u65e0\u6cd5\u6ee1\u8db3\u57fa\u4e8e\u5730\u7406\u3001\u8d1f\u8f7d\u3001\u80fd\u529b\u548c\u5b89\u5168\u5a01\u80c1\u7684\u52a8\u6001\u901a\u4fe1\u9700\u6c42\u3002", "method": "\u901a\u8fc7Agent Fact\u5361\u7247\u5728Agent Registry/Index\u4e2d\u6ce8\u518c\u4ee3\u7406\u540d\u79f0\u548c\u4e0a\u4e0b\u6587\u9700\u6c42\uff0c\u5b9e\u73b0\u5b9e\u65f6\u7aef\u70b9\u9009\u62e9\u548c\u901a\u4fe1\u901a\u9053\u5b9a\u5236\u3002", "result": "\u652f\u6301\u4fe1\u4efb\u3001\u670d\u52a1\u8d28\u91cf\u548c\u8d44\u6e90\u7ea6\u675f\u7684\u534f\u5546\uff0c\u63d0\u4f9b\u7075\u6d3b\u3001\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684\u4ee3\u7406\u95f4\u901a\u4fe1\u3002", "conclusion": "AdaptiveResolver\u4e3a\u672a\u6765\u590d\u6742\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4ee3\u7406\u901a\u4fe1\u63d0\u4f9b\u4e86\u5065\u58ee\u7684\u57fa\u7840\u3002"}}
{"id": "2508.03467", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.03467", "abs": "https://arxiv.org/abs/2508.03467", "authors": ["Mehdi Dabirnia", "Hamdi Joudeh", "Albert Guill\u00e9n i F\u00e0bregas"], "title": "Dual Domain Expurgated Error Exponents for Source Coding with Side Information", "comment": null, "summary": "We introduce an expurgation method for source coding with side information\nthat enables direct dual-domain derivations of expurgated error exponents.\nDual-domain methods yield optimization problems over few parameters, with any\nsub-optimal choice resulting in an achievable exponent, as opposed to\nprimal-domain optimization over distributions. In addition, dual-domain methods\nnaturally allow for general alphabets and/or memory. We derive two such\nexpurgated error exponents for different random-coding ensembles. We show the\nbetter of the exponents coincides with the Csisz\\'ar-K\\\"orner exponent obtained\nvia a graph decomposition lemma. We show some numerical examples that\nillustrate the differences between the two exponents and show that in the case\nof source coding without side information, the expurgated exponent coincides\nwith the error exponent of the source optimal code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5e26\u8fb9\u4fe1\u606f\u7684\u6e90\u7f16\u7801\u7684\u6392\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u57df\u63a8\u5bfc\u76f4\u63a5\u5f97\u5230\u6392\u9664\u8bef\u5dee\u6307\u6570\u3002\u53cc\u57df\u65b9\u6cd5\u4f18\u5316\u53c2\u6570\u5c11\uff0c\u4e14\u5141\u8bb8\u4e00\u822c\u5b57\u6bcd\u8868\u548c/\u6216\u8bb0\u5fc6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6e90\u7f16\u7801\u4e2d\u9700\u8981\u4f18\u5316\u5206\u5e03\uff0c\u800c\u53cc\u57df\u65b9\u6cd5\u7b80\u5316\u4e86\u4f18\u5316\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "method": "\u91c7\u7528\u53cc\u57df\u65b9\u6cd5\u63a8\u5bfc\u6392\u9664\u8bef\u5dee\u6307\u6570\uff0c\u6bd4\u8f83\u4e24\u79cd\u968f\u673a\u7f16\u7801\u96c6\u5408\u7684\u8bef\u5dee\u6307\u6570\u3002", "result": "\u4e24\u79cd\u8bef\u5dee\u6307\u6570\u4e2d\u66f4\u4f18\u8005\u4e0eCsisz\u00e1r-K\u00f6rner\u6307\u6570\u4e00\u81f4\uff0c\u4e14\u5728\u65e0\u8fb9\u4fe1\u606f\u60c5\u51b5\u4e0b\u4e0e\u6e90\u6700\u4f18\u7801\u7684\u8bef\u5dee\u6307\u6570\u4e00\u81f4\u3002", "conclusion": "\u53cc\u57df\u65b9\u6cd5\u7b80\u5316\u4e86\u4f18\u5316\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u7f16\u7801\u573a\u666f\u3002"}}
{"id": "2508.02789", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02789", "abs": "https://arxiv.org/abs/2508.02789", "authors": ["Newman Cheng", "Gordon Broadbent", "William Chappell"], "title": "Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science", "comment": null, "summary": "The capacity for artificial intelligence (AI) to formulate, evolve, and test\naltered thought patterns under dynamic conditions indicates advanced cognition\nthat is crucial for scientific discovery. The existing AI development landscape\nfalls into two categories: 1) frameworks over non-reasoning models that\nnatively incorporate opinions on how humans think, and 2) reasoning models that\nabstract precise control of the reasoning intuition away from end users. While\npowerful, for scientists to maximize utility of AI in scientific discovery,\nthey not only require accuracy and transparency in reasoning, but also\nsteerability. Hence, we introduce an alternative approach that enables deep and\nprecise control over the reasoning process called: a cognitive loop via in-situ\noptimization (CLIO). CLIO enables large language models (LLMs) to\nself-formulate ways of approaching a problem, adapt behavior when\nself-confidence is low, and ultimately provide scientists with a final belief\nor answer. Through CLIO's open design, scientists can observe uncertainty\nlevels, understand how final belief states are formulated using graph\nstructures, and interject corrections. Without any further post-training,\nOpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\\% in text-based biology\nand medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\\% net\nor 161.64\\% relative increase when compared to the base GPT-4.1 model and\nsurpasses OpenAI's o3 performance in high and low reasoning effort modes. We\nfurther discovered that oscillations within internal uncertainty measures are\nkey in determining the accuracy of CLIO's results, revealing how its open\ndesign and internal mechanisms can provide insight and control into scientific\ndecision-making processes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLIO\u7684\u8ba4\u77e5\u5faa\u73af\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u4f18\u5316\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u591f\u81ea\u4e3b\u8c03\u6574\u95ee\u9898\u89e3\u51b3\u65b9\u5f0f\uff0c\u5e76\u5728\u4f4e\u81ea\u4fe1\u65f6\u9002\u5e94\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684AI\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709AI\u5f00\u53d1\u6846\u67b6\u8981\u4e48\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\uff0c\u8981\u4e48\u5c06\u63a8\u7406\u63a7\u5236\u62bd\u8c61\u5316\uff0c\u79d1\u5b66\u5bb6\u9700\u8981\u66f4\u900f\u660e\u3001\u53ef\u64cd\u63a7\u7684AI\u5de5\u5177\u6765\u8f85\u52a9\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u5f15\u5165CLIO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u653e\u8bbe\u8ba1\u8ba9\u79d1\u5b66\u5bb6\u89c2\u5bdf\u4e0d\u786e\u5b9a\u6027\u3001\u7406\u89e3\u63a8\u7406\u8fc7\u7a0b\u5e76\u4ecb\u5165\u4fee\u6b63\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5728HLE\u751f\u7269\u548c\u533b\u5b66\u95ee\u9898\u4e0a\uff0cCLIO\u4f7fGPT-4.1\u7684\u51c6\u786e\u7387\u63d0\u534713.82%\uff08\u76f8\u5bf9\u63d0\u5347161.64%\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\u6ce2\u52a8\u5bf9\u7ed3\u679c\u7684\u5173\u952e\u5f71\u54cd\u3002", "conclusion": "CLIO\u7684\u5f00\u653e\u8bbe\u8ba1\u548c\u5185\u90e8\u673a\u5236\u4e3a\u79d1\u5b66\u51b3\u7b56\u63d0\u4f9b\u4e86\u6d1e\u5bdf\u529b\u548c\u63a7\u5236\u529b\uff0c\u5c55\u793a\u4e86AI\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.03146", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.03146", "abs": "https://arxiv.org/abs/2508.03146", "authors": ["Kostas Chounos", "Katerina Kyriakou", "Thanasis Korakis"], "title": "Scalability and Performance Evaluation of IEEE 802.11ah IoT Deployments: A Testbed Approach", "comment": null, "summary": "This work focuses on the development and assessment of modern wireless\nInternet of Things (IoT) architectures, with relevance to emerging 5G and\nbeyond applications. To analyze the growing demands for data, and their impact,\nwe built an IEEE 802.11ah (WiFi Halow) office testbed for real-world\nexperimentation. This deployment allows us to uncover the practical performance\nand scalability limitations of such networks under various challenging\nscenarios. To the best of our knowledge, this is the first study to consider\ncomplex real-world IEEE 802.11ah implementations, aiming specifically to reveal\nunexpected performance behaviors, such as significant throughput degradation\narising in closely deployed wireless links. Our findings show that intense\nnetwork contention and Adjacent Channel Interference (ACI), drastically impact\nthe performance of the wireless links involved. Beyond evaluating network\nperformance, our experimental analysis also considers the energy consumption of\nthe devices under test, offering a more holistic perspective on the feasibility\nof IEEE 802.11ah in real-world deployments. The effective disclosure of such\nunexpected phenomena, can lead to well planned decisions and energy consumption\noptimization across the IoT to Cloud continuum.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u73b0\u4ee3\u65e0\u7ebf\u7269\u8054\u7f51\uff08IoT\uff09\u67b6\u6784\uff0c\u91cd\u70b9\u5173\u6ce85G\u53ca\u66f4\u9ad8\u7248\u672c\u5e94\u7528\u7684\u5b9e\u9645\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002", "motivation": "\u5206\u6790\u6570\u636e\u9700\u6c42\u7684\u589e\u957f\u53ca\u5176\u5f71\u54cd\uff0c\u63ed\u793aIEEE 802.11ah\uff08WiFi Halow\uff09\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u884c\u4e3a\u548c\u80fd\u6e90\u6d88\u8017\u95ee\u9898\u3002", "method": "\u6784\u5efaIEEE 802.11ah\u529e\u516c\u5ba4\u6d4b\u8bd5\u5e73\u53f0\u8fdb\u884c\u5b9e\u9645\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u7f51\u7edc\u6027\u80fd\u548c\u8bbe\u5907\u80fd\u6e90\u6d88\u8017\u3002", "result": "\u53d1\u73b0\u7f51\u7edc\u7ade\u4e89\u548c\u76f8\u90bb\u4fe1\u9053\u5e72\u6270\uff08ACI\uff09\u663e\u8457\u5f71\u54cd\u65e0\u7ebf\u94fe\u8def\u6027\u80fd\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u80fd\u6e90\u6d88\u8017\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u4f18\u5316\u7269\u8054\u7f51\u5230\u4e91\u7684\u51b3\u7b56\u548c\u80fd\u6e90\u6d88\u8017\uff0c\u63d0\u5347IEEE 802.11ah\u7684\u5b9e\u9645\u90e8\u7f72\u53ef\u884c\u6027\u3002"}}
{"id": "2508.03552", "categories": ["cs.IT", "math.IT", "94B05, 94B65"], "pdf": "https://arxiv.org/pdf/2508.03552", "abs": "https://arxiv.org/abs/2508.03552", "authors": ["Guanghui Zhang", "Liren Lin", "Bocong Chen"], "title": "Decoding Algorithms for Twisted GRS Codes", "comment": "17 pages", "summary": "Twisted generalized Reed-Solomon (TGRS) codes were introduced to extend the\nalgebraic capabilities of classical generalized Reed-Solomon (GRS) codes. This\nextension holds the potential for constructing new non-GRS maximum distance\nseparable (MDS) codes and enhancing cryptographic security. It is known that\nTGRS codes with $1$ twist can either be MDS or near-MDS. In this paper, we\nemploy the Gaussian elimination method to propose new decoding algorithms for\nMDS TGRS codes with parameters $[n,k,n-k+1]$. The algorithms can correct up to\n$\\lfloor \\frac{n-k}{2}\\rfloor$ errors when $n-k$ is odd, and $\\lfloor\n\\frac{n-k}{2}\\rfloor-1$ errors when $n-k$ is even. The computational complexity\nfor both scenarios is $O(n^3)$. %, where $\\omega\\approx 2.37286$ is the matrix\nmultiplication exponent. Our approach diverges from existing methods based on\nEuclidean algorithm and addresses situations that have not been considered in\nthe existing literature \\cite{SYJL}. Furthermore, this method is also\napplicable to decoding near-MDS TGRS codes with parameters $[n, k, n-k]$,\nenabling correction of up to $\\lfloor \\frac{n-k-1}{2} \\rfloor$ errors, while\nmaintaining polynomial time complexity in $n$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9MDS TGRS\u7801\u7684\u65b0\u89e3\u7801\u7b97\u6cd5\uff0c\u57fa\u4e8e\u9ad8\u65af\u6d88\u5143\u6cd5\uff0c\u80fd\u7ea0\u6b63\u7279\u5b9a\u6570\u91cf\u7684\u9519\u8bef\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3aO(n\u00b3)\u3002", "motivation": "\u6269\u5c55TGRS\u7801\u7684\u4ee3\u6570\u80fd\u529b\uff0c\u6784\u5efa\u65b0\u7684\u975eGRS MDS\u7801\u5e76\u63d0\u5347\u5bc6\u7801\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6d88\u5143\u6cd5\u8bbe\u8ba1\u89e3\u7801\u7b97\u6cd5\uff0c\u9002\u7528\u4e8eMDS\u548c\u8fd1MDS TGRS\u7801\u3002", "result": "\u7b97\u6cd5\u80fd\u7ea0\u6b63\u7279\u5b9a\u6570\u91cf\u7684\u9519\u8bef\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3aO(n\u00b3)\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u7a7a\u767d\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684TGRS\u7801\u89e3\u7801\u573a\u666f\u3002"}}
{"id": "2508.02841", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02841", "abs": "https://arxiv.org/abs/2508.02841", "authors": ["Ziruo Yi", "Jinyu Liu", "Ting Xiao", "Mark V. Albert"], "title": "A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering", "comment": null, "summary": "Radiology visual question answering (RVQA) provides precise answers to\nquestions about chest X-ray images, alleviating radiologists' workload. While\nrecent methods based on multimodal large language models (MLLMs) and\nretrieval-augmented generation (RAG) have shown promising progress in RVQA,\nthey still face challenges in factual accuracy, hallucinations, and cross-modal\nmisalignment. We introduce a multi-agent system (MAS) designed to support\ncomplex reasoning in RVQA, with specialized agents for context understanding,\nmultimodal reasoning, and answer validation. We evaluate our system on a\nchallenging RVQA set curated via model disagreement filtering, comprising\nconsistently hard cases across multiple MLLMs. Extensive experiments\ndemonstrate the superiority and effectiveness of our system over strong MLLM\nbaselines, with a case study illustrating its reliability and interpretability.\nThis work highlights the potential of multi-agent approaches to support\nexplainable and trustworthy clinical AI applications that require complex\nreasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u7528\u4e8e\u653e\u5c04\u5b66\u89c6\u89c9\u95ee\u7b54\uff08RVQA\uff09\uff0c\u901a\u8fc7\u4e13\u95e8\u4ee3\u7406\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u51cf\u5c11\u5e7b\u89c9\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u5728\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684RVQA\u65b9\u6cd5\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5e7b\u89c9\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u591a\u6a21\u6001\u63a8\u7406\u548c\u7b54\u6848\u9a8c\u8bc1\u4e09\u4e2a\u4e13\u95e8\u4ee3\u7406\uff0c\u4ee5\u652f\u6301\u590d\u6742\u63a8\u7406\u3002", "result": "\u5728\u6311\u6218\u6027RVQA\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u8868\u73b0\u4f18\u4e8e\u5f3a\u57fa\u7ebfMLLMs\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u591a\u4ee3\u7406\u65b9\u6cd5\u5728\u652f\u6301\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u8d56\u4e34\u5e8aAI\u5e94\u7528\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.03171", "categories": ["cs.NI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.03171", "abs": "https://arxiv.org/abs/2508.03171", "authors": ["Chien-Wei Fu", "Meng-Lin Ku"], "title": "Energy-efficient Federated Learning for UAV Communications", "comment": null, "summary": "In this paper, we propose an unmanned aerial vehicle (UAV)-assisted federated\nlearning (FL) framework that jointly optimizes UAV trajectory, user\nparticipation, power allocation, and data volume control to minimize overall\nsystem energy consumption. We begin by deriving the convergence accuracy of the\nFL model under multiple local updates, enabling a theoretical understanding of\nhow user participation and data volume affect FL learning performance. The\nresulting joint optimization problem is non-convex; to address this, we employ\nalternating optimization (AO) and successive convex approximation (SCA)\ntechniques to convexify the non-convex constraints, leading to the design of an\niterative energy consumption optimization (ECO) algorithm. Simulation results\nconfirm that ECO consistently outperform existing baseline schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u7528\u6237\u53c2\u4e0e\u3001\u529f\u7387\u5206\u914d\u548c\u6570\u636e\u91cf\u63a7\u5236\uff0c\u4ee5\u6700\u5c0f\u5316\u7cfb\u7edf\u603b\u80fd\u8017\u3002", "motivation": "\u7814\u7a76\u65e0\u4eba\u673a\u8f85\u52a9\u8054\u90a6\u5b66\u4e60\u4e2d\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u591a\u53d8\u91cf\u63d0\u5347\u5b66\u4e60\u6027\u80fd\u5e76\u964d\u4f4e\u80fd\u8017\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u548c\u9010\u6b21\u51f8\u8fd1\u4f3c\uff08SCA\uff09\u6280\u672f\u5904\u7406\u975e\u51f8\u7ea6\u675f\uff0c\u8bbe\u8ba1\u8fed\u4ee3\u80fd\u91cf\u6d88\u8017\u4f18\u5316\uff08ECO\uff09\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660eECO\u7b97\u6cd5\u5728\u80fd\u8017\u4f18\u5316\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65e0\u4eba\u673a\u8f85\u52a9\u8054\u90a6\u5b66\u4e60\u7684\u80fd\u8017\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03681", "categories": ["cs.IT", "cs.CR", "cs.LG", "cs.NI", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.03681", "abs": "https://arxiv.org/abs/2508.03681", "authors": ["Shreya Meel", "Mohamed Nomeir", "Pasan Dissanayake", "Sanghamitra Dutta", "Sennur Ulukus"], "title": "What If, But Privately: Private Counterfactual Retrieval", "comment": "arXiv admin note: text overlap with arXiv:2410.13812,\n  arXiv:2411.10429", "summary": "Transparency and explainability are two important aspects to be considered\nwhen employing black-box machine learning models in high-stake applications.\nProviding counterfactual explanations is one way of catering this requirement.\nHowever, this also poses a threat to the privacy of the institution that is\nproviding the explanation, as well as the user who is requesting it. In this\nwork, we are primarily concerned with the user's privacy who wants to retrieve\na counterfactual instance, without revealing their feature vector to the\ninstitution. Our framework retrieves the exact nearest neighbor counterfactual\nexplanation from a database of accepted points while achieving perfect,\ninformation-theoretic, privacy for the user. First, we introduce the problem of\nprivate counterfactual retrieval (PCR) and propose a baseline PCR scheme that\nkeeps the user's feature vector information-theoretically private from the\ninstitution. Building on this, we propose two other schemes that reduce the\namount of information leaked about the institution database to the user,\ncompared to the baseline scheme. Second, we relax the assumption of mutability\nof all features, and consider the setting of immutable PCR (I-PCR). Here, the\nuser retrieves the nearest counterfactual without altering a private subset of\ntheir features, which constitutes the immutable set, while keeping their\nfeature vector and immutable set private from the institution. For this, we\npropose two schemes that preserve the user's privacy information-theoretically,\nbut ensure varying degrees of database privacy. Third, we extend our PCR and\nI-PCR schemes to incorporate user's preference on transforming their\nattributes, so that a more actionable explanation can be received. Finally, we\npresent numerical results to support our theoretical findings, and compare the\ndatabase leakage of the proposed schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u68c0\u7d22\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u7ebf\u65b9\u6848\u548c\u4e24\u79cd\u6539\u8fdb\u65b9\u6848\uff0c\u540c\u65f6\u8003\u8651\u4e86\u4e0d\u53ef\u53d8\u7279\u5f81\u7684\u8bbe\u7f6e\uff0c\u5e76\u6269\u5c55\u4e86\u7528\u6237\u504f\u597d\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff0c\u9ed1\u76d2\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53cd\u4e8b\u5b9e\u89e3\u91ca\u53ef\u80fd\u5a01\u80c1\u7528\u6237\u9690\u79c1\u3002\u672c\u6587\u65e8\u5728\u4fdd\u62a4\u7528\u6237\u5728\u68c0\u7d22\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65f6\u7684\u9690\u79c1\u3002", "method": "\u63d0\u51fa\u4e86\u79c1\u6709\u53cd\u4e8b\u5b9e\u68c0\u7d22\uff08PCR\uff09\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u57fa\u7ebfPCR\u65b9\u6848\u548c\u4e24\u79cd\u6539\u8fdb\u65b9\u6848\uff0c\u968f\u540e\u6269\u5c55\u5230\u4e0d\u53ef\u53d8PCR\uff08I-PCR\uff09\u548c\u7528\u6237\u504f\u597d\u8bbe\u7f6e\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u80fd\u5b9e\u73b0\u7528\u6237\u9690\u79c1\u7684\u5b8c\u7f8e\u4fdd\u62a4\uff0c\u5e76\u51cf\u5c11\u6570\u636e\u5e93\u4fe1\u606f\u6cc4\u9732\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u9700\u6c42\u4e0e\u7528\u6237\u9690\u79c1\u4fdd\u62a4\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.02900", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02900", "abs": "https://arxiv.org/abs/2508.02900", "authors": ["Michael Katz", "Harsha Kokel", "Sarath Sreedharan"], "title": "Seemingly Simple Planning Problems are Computationally Challenging: The Countdown Game", "comment": null, "summary": "There is a broad consensus that the inability to form long-term plans is one\nof the key limitations of current foundational models and agents. However, the\nexisting planning benchmarks remain woefully inadequate to truly measure their\nplanning capabilities. Most existing benchmarks either focus on loosely defined\ntasks like travel planning or end up leveraging existing domains and problems\nfrom international planning competitions. While the former tasks are hard to\nformalize and verify, the latter were specifically designed to test and\nchallenge the weaknesses of existing automated planners. To address these\nshortcomings, we propose a procedure for creating a planning benchmark centered\naround the game called Countdown, where a player is expected to form a target\nnumber from a list of input numbers through arithmetic operations. We discuss\nhow this problem meets many of the desiderata associated with an ideal\nbenchmark for planning capabilities evaluation. Specifically, the domain allows\nfor an intuitive, natural language description for each problem instance, it is\ncomputationally challenging (NP-complete), and the instance space is rich\nenough that we do not have to worry about memorization. We perform an extensive\ntheoretical analysis, establishing the computational complexity result and\ndemonstrate the advantage of our instance generation procedure over public\nbenchmarks. We evaluate a variety of existing LLM-assisted planning methods on\ninstances generated using our procedure. Our results show that, unlike other\ndomains like 24 Game (a special case of Countdown), our proposed dynamic\nbenchmark remains extremely challenging for existing LLM-based approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6e38\u620fCountdown\u7684\u89c4\u5212\u80fd\u529b\u8bc4\u6d4b\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6311\u6218\u6027\u3002", "motivation": "\u73b0\u6709\u89c4\u5212\u80fd\u529b\u8bc4\u6d4b\u57fa\u51c6\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u51c6\u786e\u8861\u91cf\u57fa\u7840\u6a21\u578b\u548c\u667a\u80fd\u4f53\u7684\u957f\u671f\u89c4\u5212\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eCountdown\u6e38\u620f\u7684\u8bc4\u6d4b\u57fa\u51c6\u751f\u6210\u65b9\u6cd5\uff0c\u8fdb\u884c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u52a8\u6001\u751f\u6210\u7684\u8bc4\u6d4b\u57fa\u51c6\u5bf9\u73b0\u6709LLM\u8f85\u52a9\u89c4\u5212\u65b9\u6cd5\u6781\u5177\u6311\u6218\u6027\u3002", "conclusion": "Countdown\u57fa\u51c6\u4e3a\u89c4\u5212\u80fd\u529b\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2508.03287", "categories": ["cs.NI", "cs.DC", "C.2.4; C.4"], "pdf": "https://arxiv.org/pdf/2508.03287", "abs": "https://arxiv.org/abs/2508.03287", "authors": ["Falk Dettinger", "Matthias Wei\u00df", "Daniel Baumann", "Martin Sommer", "Michael Weyrich"], "title": "Directives for Function Offloading in 5G Networks Based on a Performance Characteristics Analysis", "comment": "7 pages, 4 figures", "summary": "Cloud-based offloading helps address energy consumption and performance\nchallenges in executing resource-intensive vehicle algorithms. Utilizing 5G,\nwith its low latency and high bandwidth, enables seamless vehicle-to-cloud\nintegration. Currently, only non-standalone 5G is publicly available, and\nreal-world applications remain underexplored compared to theoretical studies.\nThis paper evaluates 5G non-standalone networks for cloud execution of vehicle\nfunctions, focusing on latency, Round Trip Time, and packet delivery. Tests\nused two AI-based algorithms -- emotion recognition and object recognition --\nalong an 8.8 km route in Baden-W\\\"urttemberg, Germany, encompassing urban,\nrural, and forested areas. Two platforms were analyzed: a cloudlet in Frankfurt\nand a cloud in Mannheim, employing various deployment strategies like\nconventional applications and containerized and container-orchestrated setups.\nKey findings highlight an average signal quality of 84 %, with no connectivity\ninterruptions despite minor drops in built-up areas. Packet analysis revealed a\nPacket Error Rate below 0.1 % for both algorithms. Transfer times varied\nsignificantly depending on the geographical location and the backend servers'\nnetwork connections, while processing times were mainly influenced by the\ncomputation hardware in use. Additionally, cloud offloading seems only be a\nsuitable option, when a round trip time of more than 150 ms is possible.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e865G\u975e\u72ec\u7acb\u7f51\u7edc\u5728\u8f66\u8f86\u529f\u80fd\u4e91\u6267\u884c\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u5ef6\u8fdf\u3001\u5f80\u8fd4\u65f6\u95f4\u548c\u6570\u636e\u5305\u4f20\u9012\uff0c\u6d4b\u8bd5\u4e86\u4e24\u79cdAI\u7b97\u6cd5\u5728\u4e0d\u540c\u5730\u7406\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8d44\u6e90\u5bc6\u96c6\u578b\u8f66\u8f86\u7b97\u6cd5\u7684\u80fd\u8017\u548c\u6027\u80fd\u95ee\u9898\uff0c\u63a2\u7d225G\u975e\u72ec\u7acb\u7f51\u7edc\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5728\u5fb7\u56fd\u5df4\u767b-\u7b26\u817e\u5821\u5dde8.8\u516c\u91cc\u8def\u7ebf\u4e0a\u6d4b\u8bd5\u60c5\u7eea\u8bc6\u522b\u548c\u7269\u4f53\u8bc6\u522b\u7b97\u6cd5\uff0c\u5206\u6790\u4e91\u548c\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\u7684\u6027\u80fd\u3002", "result": "\u5e73\u5747\u4fe1\u53f7\u8d28\u91cf84%\uff0c\u65e0\u8fde\u63a5\u4e2d\u65ad\uff0c\u6570\u636e\u5305\u9519\u8bef\u7387\u4f4e\u4e8e0.1%\uff0c\u4e91\u5378\u8f7d\u5728\u5f80\u8fd4\u65f6\u95f4\u8d85\u8fc7150ms\u65f6\u9002\u7528\u3002", "conclusion": "5G\u975e\u72ec\u7acb\u7f51\u7edc\u5728\u8f66\u8f86\u4e91\u529f\u80fd\u6267\u884c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4e91\u5378\u8f7d\u9700\u6ee1\u8db3\u7279\u5b9a\u5ef6\u8fdf\u6761\u4ef6\u3002"}}
{"id": "2507.05938", "categories": ["cs.AI", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.05938", "abs": "https://arxiv.org/abs/2507.05938", "authors": ["Yucheng Sheng", "Jiacheng Wang", "Xingyu Zhou", "Le Liang", "Hao Ye", "Shi Jin", "Geoffrey Ye Li"], "title": "A Wireless Foundation Model for Multi-Task Prediction", "comment": null, "summary": "With the growing complexity and dynamics of the mobile communication\nnetworks, accurately predicting key system parameters, such as channel state\ninformation (CSI), user location, and network traffic, has become essential for\na wide range of physical (PHY)-layer and medium access control (MAC)-layer\ntasks. Although traditional deep learning (DL)-based methods have been widely\napplied to such prediction tasks, they often struggle to generalize across\ndifferent scenarios and tasks. In response, we propose a unified foundation\nmodel for multi-task prediction in wireless networks that supports diverse\nprediction intervals. The proposed model enforces univariate decomposition to\nunify heterogeneous tasks, encodes granularity for interval awareness, and uses\na causal Transformer backbone for accurate predictions. Additionally, we\nintroduce a patch masking strategy during training to support arbitrary input\nlengths. After trained on large-scale datasets, the proposed foundation model\ndemonstrates strong generalization to unseen scenarios and achieves zero-shot\nperformance on new tasks that surpass traditional full-shot baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u591a\u4efb\u52a1\u9884\u6d4b\uff0c\u652f\u6301\u4e0d\u540c\u9884\u6d4b\u533a\u95f4\uff0c\u5e76\u901a\u8fc7\u5206\u89e3\u3001\u7f16\u7801\u548c\u56e0\u679cTransformer\u5b9e\u73b0\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u79fb\u52a8\u901a\u4fe1\u7f51\u7edc\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\u589e\u52a0\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u573a\u666f\u548c\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u91c7\u7528\u5355\u53d8\u91cf\u5206\u89e3\u7edf\u4e00\u5f02\u6784\u4efb\u52a1\uff0c\u7f16\u7801\u7c92\u5ea6\u5b9e\u73b0\u533a\u95f4\u611f\u77e5\uff0c\u4f7f\u7528\u56e0\u679cTransformer\u4f5c\u4e3a\u4e3b\u5e72\uff0c\u5e76\u5f15\u5165\u8865\u4e01\u63a9\u7801\u7b56\u7565\u652f\u6301\u4efb\u610f\u8f93\u5165\u957f\u5ea6\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u5728\u672a\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u65b0\u4efb\u52a1\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u6027\u80fd\u8d85\u8d8a\u4f20\u7edf\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u7840\u6a21\u578b\u4e3a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u591a\u4efb\u52a1\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02913", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02913", "abs": "https://arxiv.org/abs/2508.02913", "authors": ["Carolina Minami Oguchi", "Leo Wei", "Koyo Kobayashi", "Hsin-Tai Wu", "Dipak Ghosal"], "title": "Enhancing Japanese Large Language Models with Reasoning Vectors", "comment": null, "summary": "Post-training methods have improved the performance and enhanced the\nreasoning capability for mainstream large language models (LLMs), but the same\nis challenging for Japanese LLMs to achieve due to the amount of resources\nrequired. Inspired by task vectors that extract the change of weights before\nand after training, specifically for a certain task, we obtain reasoning\nvectors from reasoning LLMs and apply them to Japanese LLMs to boost their\nperformance. While the resources available present a challenge to improve\nJapanese LLMs, we present a simple and effective way to obtain high improvement\nand hope to inspire for other languages.", "AI": {"tldr": "\u901a\u8fc7\u4ece\u63a8\u7406LLM\u4e2d\u63d0\u53d6\u63a8\u7406\u5411\u91cf\u5e76\u5e94\u7528\u4e8e\u65e5\u8bedLLM\uff0c\u63d0\u5347\u5176\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u8d44\u6e90\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u8d44\u6e90\u9650\u5236\uff0c\u65e5\u8bedLLM\u7684\u6027\u80fd\u63d0\u5347\u56f0\u96be\uff0c\u9700\u5bfb\u627e\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u4efb\u52a1\u5411\u91cf\uff08\u6743\u91cd\u53d8\u5316\uff09\u63d0\u53d6\u63a8\u7406\u5411\u91cf\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u65e5\u8bedLLM\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u65e5\u8bedLLM\u7684\u6027\u80fd\uff0c\u65b9\u6cd5\u7b80\u5355\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u8bed\u8a00\u3002"}}
{"id": "2508.03321", "categories": ["cs.NI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03321", "abs": "https://arxiv.org/abs/2508.03321", "authors": ["J\u00f6rn Bodenhausen", "Simon Mangel", "Thomas Vogt", "Martin Henze"], "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT Scenarios", "comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)", "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.", "AI": {"tldr": "BiTHaC\u901a\u8fc7\u53cc\u5411TLS\u63e1\u624b\u7f13\u5b58\u51cf\u5c11\u8d44\u6e90\u53d7\u9650\u5de5\u4e1a\u7269\u8054\u7f51\u4e2d\u7684\u5e26\u5bbd\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "TLS\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5de5\u4e1a\u7269\u8054\u7f51\u4e2d\u56e0\u63e1\u624b\u8fc7\u7a0b\u7684\u9ad8\u5f00\u9500\u800c\u53d7\u9650\u3002", "method": "\u5229\u7528\u91cd\u590dTLS\u63e1\u624b\uff08\u5982\u8bc1\u4e66\uff09\u7684\u9759\u6001\u90e8\u5206\u5b9e\u73b0\u53cc\u5411\u7f13\u5b58\uff0c\u51cf\u5c11\u4f20\u8f93\u548c\u8ba1\u7b97\u3002", "result": "\u5e26\u5bbd\u6d88\u8017\u51cf\u5c1161.1%\uff0c\u8ba1\u7b97\u5f00\u9500\u964d\u4f4e8.5%\uff0c\u5185\u5b58\u5f00\u9500\u53ef\u63a7\u4e14\u5b89\u5168\u6027\u4e0d\u53d8\u3002", "conclusion": "BiTHaC\u6709\u6548\u89e3\u51b3\u4e86TLS\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6027\u80fd\u95ee\u9898\u3002"}}
{"id": "2508.02921", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02921", "abs": "https://arxiv.org/abs/2508.02921", "authors": ["Shane Caldwell", "Max Harley", "Michael Kouremetis", "Vincent Abruzzo", "Will Pearce"], "title": "PentestJudge: Judging Agent Behavior Against Operational Requirements", "comment": "18 pages, 5 figures, 3 tables", "summary": "We introduce PentestJudge, a system for evaluating the operations of\npenetration testing agents. PentestJudge is a large language model\n(LLM)-as-judge with access to tools that allow it to consume arbitrary\ntrajectories of agent states and tool call history to determine whether a\nsecurity agent's actions meet certain operating criteria that would be\nimpractical to evaluate programmatically. We develop rubrics that use a tree\nstructure to hierarchically collapse the penetration testing task for a\nparticular environment into smaller, simpler, and more manageable sub-tasks and\ncriteria until each leaf node represents simple yes-or-no criteria for\nPentestJudge to evaluate. Task nodes are broken down into different categories\nrelated to operational objectives, operational security, and tradecraft.\nLLM-as-judge scores are compared to human domain experts as a ground-truth\nreference, allowing us to compare their relative performance with standard\nbinary classification metrics, such as F1 scores. We evaluate several frontier\nand open-source models acting as judge agents, with the best model reaching an\nF1 score of 0.83. We find models that are better at tool-use perform more\nclosely to human experts. By stratifying the F1 scores by requirement type, we\nfind even models with similar overall scores struggle with different types of\nquestions, suggesting certain models may be better judges of particular\noperating criteria. We find that weaker and cheaper models can judge the\ntrajectories of pentests performed by stronger and more expensive models,\nsuggesting verification may be easier than generation for the penetration\ntesting task. We share this methodology to facilitate future research in\nunderstanding the ability of judges to holistically and scalably evaluate the\nprocess quality of AI-based information security agents so that they may be\nconfidently used in sensitive production environments.", "AI": {"tldr": "PentestJudge\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u6e17\u900f\u6d4b\u8bd5\u4ee3\u7406\u7684\u64cd\u4f5c\uff0c\u901a\u8fc7\u5206\u5c42\u4efb\u52a1\u5206\u89e3\u548c\u7b80\u5355\u6807\u51c6\u5b9e\u73b0\u9ad8\u6548\u8bc4\u4f30\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u8bc4\u4f30\u6e17\u900f\u6d4b\u8bd5\u4ee3\u7406\u64cd\u4f5c\u7684\u7cfb\u7edf\uff0c\u907f\u514d\u7a0b\u5e8f\u5316\u8bc4\u4f30\u7684\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528\u6811\u7ed3\u6784\u5c06\u6e17\u900f\u6d4b\u8bd5\u4efb\u52a1\u5206\u89e3\u4e3a\u66f4\u5c0f\u7684\u5b50\u4efb\u52a1\u548c\u7b80\u5355\u6807\u51c6\uff0cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u5206\u5bf9\u6bd4\u3002", "result": "\u6700\u4f73\u6a21\u578b\u7684F1\u5f97\u5206\u4e3a0.83\uff0c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u5206\u3002", "conclusion": "PentestJudge\u5c55\u793a\u4e86LLM\u5728\u8bc4\u4f30\u6e17\u900f\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u9a8c\u8bc1\u53ef\u80fd\u6bd4\u751f\u6210\u66f4\u5bb9\u6613\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u652f\u6301\u3002"}}
{"id": "2508.03674", "categories": ["cs.NI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03674", "abs": "https://arxiv.org/abs/2508.03674", "authors": ["Abhishek Vijaya Kumar", "Eric Ding", "Arjun Devraj", "Rachee Singh"], "title": "Morphlux: Programmable chip-to-chip photonic fabrics in multi-accelerator servers for ML", "comment": null, "summary": "We optically interconnect accelerator chips (e.g., GPUs, TPUs) within compute\nservers using newly viable programmable chip-to-chip photonic fabrics. In\ncontrast, today, commercial multi-accelerator compute servers that are\nworkhorses of ML, use electrical interconnects to network accelerator chips in\nthe server. However, recent trends have shown an interconnect bandwidth wall\ncaused by accelerator FLOPS scaling at a faster rate than the bandwidth of the\ninterconnect between accelerators in the same server. This has led to\nunder-utilization and idling of GPU resources in cloud datacenters. We develop\nMorphlux, a server-scale programmable photonic fabric, to interconnect\naccelerators within servers. We show that augmenting state-of-the-art photonic\nML-centric datacenters with Morphlux can improve the bandwidth of tenant\ncompute allocations by up to 66% and reduce compute fragmentation by up to 70%.\nWe develop a novel end-to-end hardware prototype of Morphlux to demonstrate\nthese performance benefits, which translate to 1.72x improvement in training\nthroughput of ML models. By rapidly programming the server-scale fabric in our\nhardware testbed, Morphlux can logically replace a failed accelerator chip in\n1.2 seconds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMorphlux\u7684\u5149\u4e92\u8fde\u6280\u672f\uff0c\u7528\u4e8e\u670d\u52a1\u5668\u5185\u52a0\u901f\u5668\u82af\u7247\u95f4\u7684\u8fde\u63a5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7535\u4e92\u8fde\u5e26\u5bbd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u5f53\u524d\u591a\u52a0\u901f\u5668\u670d\u52a1\u5668\u4f7f\u7528\u7535\u4e92\u8fde\u8fde\u63a5\u52a0\u901f\u5668\u82af\u7247\uff0c\u4f46\u7531\u4e8e\u52a0\u901f\u5668\u8ba1\u7b97\u80fd\u529b\u63d0\u5347\u5feb\u4e8e\u4e92\u8fde\u5e26\u5bbd\uff0c\u5bfc\u81f4\u5e26\u5bbd\u74f6\u9888\u548cGPU\u8d44\u6e90\u95f2\u7f6e\u3002", "method": "\u5f00\u53d1\u4e86Morphlux\uff0c\u4e00\u79cd\u53ef\u7f16\u7a0b\u7684\u5149\u4e92\u8fde\u67b6\u6784\uff0c\u7528\u4e8e\u670d\u52a1\u5668\u5185\u52a0\u901f\u5668\u95f4\u7684\u8fde\u63a5\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u539f\u578b\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "Morphlux\u53ef\u5c06\u79df\u6237\u8ba1\u7b97\u5206\u914d\u7684\u5e26\u5bbd\u63d0\u534766%\uff0c\u51cf\u5c11\u8ba1\u7b97\u788e\u7247\u531670%\uff0c\u5e76\u63d0\u9ad8ML\u6a21\u578b\u8bad\u7ec3\u541e\u5410\u91cf1.72\u500d\u3002", "conclusion": "Morphlux\u901a\u8fc7\u5149\u4e92\u8fde\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u670d\u52a1\u5668\u5185\u52a0\u901f\u5668\u4e92\u8fde\u7684\u5e26\u5bbd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2508.02936", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02936", "abs": "https://arxiv.org/abs/2508.02936", "authors": ["Songkun Yan", "Zhi Li", "Siyu Zhu", "Yixin Wen", "Mofan Zhang", "Mengye Chen", "Jie Cao", "Yang Hong"], "title": "AQUAH: Automatic Quantification and Unified Agent in Hydrology", "comment": "8 pages, 5 figures, 2025 ICCV SEA workshop paper", "summary": "We introduce AQUAH, the first end-to-end language-based agent designed\nspecifically for hydrologic modeling. Starting from a simple natural-language\nprompt (e.g., 'simulate floods for the Little Bighorn basin from 2020 to\n2022'), AQUAH autonomously retrieves the required terrain, forcing, and gauge\ndata; configures a hydrologic model; runs the simulation; and generates a\nself-contained PDF report. The workflow is driven by vision-enabled large\nlanguage models, which interpret maps and rasters on the fly and steer key\ndecisions such as outlet selection, parameter initialization, and uncertainty\ncommentary. Initial experiments across a range of U.S. basins show that AQUAH\ncan complete cold-start simulations and produce analyst-ready documentation\nwithout manual intervention. The results are judged by hydrologists as clear,\ntransparent, and physically plausible. While further calibration and validation\nare still needed for operational deployment, these early outcomes highlight the\npromise of LLM-centered, vision-grounded agents to streamline complex\nenvironmental modeling and lower the barrier between Earth observation data,\nphysics-based tools, and decision makers.", "AI": {"tldr": "AQUAH\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u7684\u7aef\u5230\u7aef\u6c34\u6587\u5efa\u6a21\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u81ea\u52a8\u5b8c\u6210\u6570\u636e\u68c0\u7d22\u3001\u6a21\u578b\u914d\u7f6e\u3001\u6a21\u62df\u8fd0\u884c\u548c\u62a5\u544a\u751f\u6210\u3002", "motivation": "\u7b80\u5316\u6c34\u6587\u5efa\u6a21\u6d41\u7a0b\uff0c\u964d\u4f4e\u5730\u7403\u89c2\u6d4b\u6570\u636e\u3001\u7269\u7406\u5de5\u5177\u548c\u51b3\u7b56\u8005\u4e4b\u95f4\u7684\u95e8\u69db\u3002", "method": "\u5229\u7528\u89c6\u89c9\u652f\u6301\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u52a8\u6001\u89e3\u6790\u5730\u56fe\u548c\u6805\u683c\u6570\u636e\uff0c\u6307\u5bfc\u5173\u952e\u51b3\u7b56\u5982\u51fa\u53e3\u9009\u62e9\u3001\u53c2\u6570\u521d\u59cb\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0cAQUAH\u80fd\u5b8c\u6210\u51b7\u542f\u52a8\u6a21\u62df\u5e76\u751f\u6210\u6e05\u6670\u3001\u900f\u660e\u4e14\u7269\u7406\u5408\u7406\u7684\u62a5\u544a\u3002", "conclusion": "AQUAH\u5c55\u793a\u4e86\u4ee5LLM\u4e3a\u6838\u5fc3\u7684\u89c6\u89c9\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u6821\u51c6\u548c\u9a8c\u8bc1\u3002"}}
{"id": "2508.02951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02951", "abs": "https://arxiv.org/abs/2508.02951", "authors": ["Mahtab Bigverdi", "Wisdom Ikezogwo", "Kevin Zhang", "Hyewon Jeong", "Mingyu Lu", "Sungjae Cho", "Linda Shapiro", "Ranjay Krishna"], "title": "MedBLINK: Probing Basic Perception in Multimodal Language Models for Medicine", "comment": null, "summary": "Multimodal language models (MLMs) show promise for clinical decision support\nand diagnostic reasoning, raising the prospect of end-to-end automated medical\nimage interpretation. However, clinicians are highly selective in adopting AI\ntools; a model that makes errors on seemingly simple perception tasks such as\ndetermining image orientation or identifying whether a CT scan is\ncontrast-enhance are unlikely to be adopted for clinical tasks. We introduce\nMedblink, a benchmark designed to probe these models for such perceptual\nabilities. Medblink spans eight clinically meaningful tasks across multiple\nimaging modalities and anatomical regions, totaling 1,429 multiple-choice\nquestions over 1,605 images. We evaluate 19 state-of-the-art MLMs, including\ngeneral purpose (GPT4o, Claude 3.5 Sonnet) and domain specific (Med Flamingo,\nLLaVA Med, RadFM) models. While human annotators achieve 96.4% accuracy, the\nbest-performing model reaches only 65%. These results show that current MLMs\nfrequently fail at routine perceptual checks, suggesting the need to strengthen\ntheir visual grounding to support clinical adoption. Data is available on our\nproject page.", "AI": {"tldr": "Medblink\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MLMs\uff09\u5728\u4e34\u5e8a\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d68\u4e2a\u4efb\u52a1\u548c1,605\u5f20\u56fe\u50cf\u3002\u6d4b\u8bd5\u663e\u793a\uff0c\u5f53\u524dMLMs\u5728\u7b80\u5355\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u4ec565%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768496.4%\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u5bf9AI\u5de5\u5177\u7684\u91c7\u7528\u975e\u5e38\u8c28\u614e\uff0c\u6a21\u578b\u5728\u7b80\u5355\u611f\u77e5\u4efb\u52a1\u4e0a\u7684\u9519\u8bef\u4f1a\u963b\u788d\u5176\u4e34\u5e8a\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8bc4\u4f30MLMs\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165Medblink\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8\u4e2a\u4e34\u5e8a\u76f8\u5173\u4efb\u52a1\u548c1,429\u4e2a\u591a\u9009\u9898\uff0c\u8986\u76d6\u591a\u79cd\u6210\u50cf\u6a21\u5f0f\u548c\u89e3\u5256\u533a\u57df\u3002\u8bc4\u4f30\u4e8619\u79cdMLMs\uff0c\u5305\u62ec\u901a\u7528\u548c\u9886\u57df\u4e13\u7528\u6a21\u578b\u3002", "result": "\u4eba\u7c7b\u6807\u6ce8\u8005\u51c6\u786e\u7387\u4e3a96.4%\uff0c\u800c\u6700\u4f73\u6a21\u578b\u4ec5\u8fbe65%\uff0c\u8868\u660e\u5f53\u524dMLMs\u5728\u89c6\u89c9\u57fa\u7840\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "conclusion": "\u5f53\u524dMLMs\u5728\u4e34\u5e8a\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u5176\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u4ee5\u652f\u6301\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2508.02959", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02959", "abs": "https://arxiv.org/abs/2508.02959", "authors": ["Chia-Tung Ho", "Jing Gong", "Xufeng Yao", "Yunsheng Bai", "Abhishek B Akkur", "Haoxing Ren"], "title": "Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow", "comment": "18 pages, 12 figures, under review for AAAI2026", "summary": "Large language models (LLMs) excel at solving complex tasks by executing\nagentic workflows composed of detailed instructions and structured operations.\nYet, building general-purpose agents by manually embedding foundation models\ninto agentic systems such as Chain-of-Thought, Self-Reflection, and ReACT\nthrough text interfaces limits scalability and efficiency. Recently, many\nresearchers have sought to automate the generation and optimization of these\nworkflows through code-based representations. However, existing methods often\nrely on labeled datasets to train and optimize workflows, making them\nineffective and inflexible for solving real-world, dynamic problems where\nlabeled data is unavailable. To address this challenge, we introduce Polymath,\na self-optimizing agent with dynamic hierarchical workflow that leverages the\nflexibility of task flow graphs and the expressiveness of code-represented\nworkflows to solve a wide range of real-world, dynamic problems. The proposed\noptimization methodology integrates multi-grid-inspired graph optimization with\na self-reflection-guided evolutionary algorithm to refine workflows without\nlabeled data. Experimental results on six benchmark datasets across coding,\nmath, and multi-turn QA tasks show that Polymath achieves 8.1% average\nimprovement over state-of-the-art baselines.", "AI": {"tldr": "Polymath\u662f\u4e00\u79cd\u81ea\u4f18\u5316\u4ee3\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u5c42\u5de5\u4f5c\u6d41\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u52a8\u6001\u95ee\u9898\uff0c\u65e0\u9700\u6807\u8bb0\u6570\u636e\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf8.1%\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6807\u8bb0\u6570\u636e\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u52a8\u6001\u95ee\u9898\u65f6\u6548\u7387\u4f4e\u4e14\u4e0d\u7075\u6d3b\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6807\u8bb0\u6570\u636e\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u751f\u6210\u4e0e\u4f18\u5316\u65b9\u6cd5\u3002", "method": "Polymath\u7ed3\u5408\u4efb\u52a1\u6d41\u56fe\u7684\u7075\u6d3b\u6027\u548c\u4ee3\u7801\u8868\u793a\u5de5\u4f5c\u6d41\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u91c7\u7528\u591a\u7f51\u683c\u56fe\u4f18\u5316\u548c\u81ea\u53cd\u601d\u8fdb\u5316\u7b97\u6cd5\u4f18\u5316\u5de5\u4f5c\u6d41\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cPolymath\u5e73\u5747\u6027\u80fd\u63d0\u53478.1%\u3002", "conclusion": "Polymath\u4e3a\u52a8\u6001\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u8bb0\u6570\u636e\u3002"}}
{"id": "2508.02961", "categories": ["cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02961", "abs": "https://arxiv.org/abs/2508.02961", "authors": ["Boshi Huang", "Fabio Nonato de Paula"], "title": "Defend LLMs Through Self-Consciousness", "comment": "Presented at KDD Workshop on Ethical Artificial Intelligence: Methods\n  and Applications (EAI) 2025", "summary": "This paper introduces a novel self-consciousness defense mechanism for Large\nLanguage Models (LLMs) to combat prompt injection attacks. Unlike traditional\napproaches that rely on external classifiers, our method leverages the LLM's\ninherent reasoning capabilities to perform self-protection. We propose a\nframework that incorporates Meta-Cognitive and Arbitration Modules, enabling\nLLMs to evaluate and regulate their own outputs autonomously. Our approach is\nevaluated on seven state-of-the-art LLMs using two datasets: AdvBench and\nPrompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate\nsignificant improvements in defense success rates across models and datasets,\nwith some achieving perfect and near-perfect defense in Enhanced Mode. We also\nanalyze the trade-off between defense success rate improvement and\ncomputational overhead. This self-consciousness method offers a lightweight,\ncost-effective solution for enhancing LLM ethics, particularly beneficial for\nGenAI use cases across various platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u81ea\u610f\u8bc6\u9632\u5fa1\u673a\u5236\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u81ea\u4e3b\u5bf9\u6297\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9632\u5fa1\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u5206\u7c7b\u5668\uff0c\u800c\u672c\u6587\u5e0c\u671b\u901a\u8fc7LLM\u81ea\u8eab\u7684\u63a8\u7406\u80fd\u529b\u5b9e\u73b0\u81ea\u6211\u9632\u62a4\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5305\u542b\u5143\u8ba4\u77e5\u548c\u4ef2\u88c1\u6a21\u5757\u7684\u6846\u67b6\uff0c\u4f7fLLM\u80fd\u591f\u81ea\u4e3b\u8bc4\u4f30\u548c\u8c03\u8282\u8f93\u51fa\u3002", "result": "\u5728\u4e03\u4e2a\u5148\u8fdbLLM\u4e0a\u6d4b\u8bd5\uff0c\u4f7f\u7528\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u663e\u793a\u9632\u5fa1\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\uff0c\u90e8\u5206\u6a21\u578b\u5728\u589e\u5f3a\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u5b8c\u7f8e\u9632\u5fa1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d0\u5347LLM\u4f26\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e73\u53f0\u7684\u751f\u6210\u5f0fAI\u7528\u4f8b\u3002"}}
{"id": "2508.02979", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02979", "abs": "https://arxiv.org/abs/2508.02979", "authors": ["Peng Ding", "Rick Stevens"], "title": "Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.10593", "summary": "The proliferation of tool-augmented Large Language Models (LLMs) has created\na fragmented ecosystem where developers must navigate multiple protocols,\nmanual schema definitions, and complex execution workflows. We address this\nchallenge by proposing a unified approach to tool integration that abstracts\nprotocol differences while optimizing execution performance. Our solution\ndemonstrates how protocol-agnostic design principles can significantly reduce\ndevelopment overhead through automated schema generation, dual-mode concurrent\nexecution, and seamless multi-source tool management. Experimental results show\n60-80% code reduction across integration scenarios, performance improvements up\nto 3.1x through optimized concurrency, and full compatibility with existing\nfunction calling standards. This work contributes both theoretical insights\ninto tool integration architecture and practical solutions for real-world LLM\napplication development.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u5de5\u5177\u96c6\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u5f00\u9500\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5de5\u5177\u589e\u5f3a\u578b\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6001\u788e\u7247\u5316\u95ee\u9898\uff0c\u7b80\u5316\u5f00\u53d1\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u534f\u8bae\u65e0\u5173\u8bbe\u8ba1\u539f\u5219\uff0c\u81ea\u52a8\u5316\u6a21\u5f0f\u751f\u6210\uff0c\u53cc\u6a21\u5f0f\u5e76\u53d1\u6267\u884c\uff0c\u591a\u6e90\u5de5\u5177\u7ba1\u7406\u3002", "result": "\u4ee3\u7801\u51cf\u5c1160-80%\uff0c\u6027\u80fd\u63d0\u53473.1\u500d\uff0c\u517c\u5bb9\u73b0\u6709\u6807\u51c6\u3002", "conclusion": "\u4e3a\u5de5\u5177\u96c6\u6210\u67b6\u6784\u63d0\u4f9b\u7406\u8bba\u89c1\u89e3\u548c\u5b9e\u9645\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02994", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02994", "abs": "https://arxiv.org/abs/2508.02994", "authors": ["Fangyi Yu"], "title": "When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs", "comment": null, "summary": "As large language models (LLMs) grow in capability and autonomy, evaluating\ntheir outputs-especially in open-ended and complex tasks-has become a critical\nbottleneck. A new paradigm is emerging: using AI agents as the evaluators\nthemselves. This \"agent-as-a-judge\" approach leverages the reasoning and\nperspective-taking abilities of LLMs to assess the quality and safety of other\nmodels, promising calable and nuanced alternatives to human evaluation. In this\nreview, we define the agent-as-a-judge concept, trace its evolution from\nsingle-model judges to dynamic multi-agent debate frameworks, and critically\nexamine their strengths and shortcomings. We compare these approaches across\nreliability, cost, and human alignment, and survey real-world deployments in\ndomains such as medicine, law, finance, and education. Finally, we highlight\npressing challenges-including bias, robustness, and meta evaluation-and outline\nfuture research directions. By bringing together these strands, our review\ndemonstrates how agent-based judging can complement (but not replace) human\noversight, marking a step toward trustworthy, scalable evaluation for\nnext-generation LLMs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528AI\u4ee3\u7406\u4f5c\u4e3a\u8bc4\u4f30\u8005\uff08agent-as-a-judge\uff09\u7684\u65b0\u8303\u5f0f\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8f93\u51fa\u8bc4\u4f30\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5206\u6790\u4e86\u5176\u4f18\u52bf\u3001\u6311\u6218\u53ca\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u968f\u7740LLMs\u80fd\u529b\u548c\u81ea\u4e3b\u6027\u7684\u63d0\u5347\uff0c\u5176\u8f93\u51fa\u7684\u8bc4\u4f30\u5728\u5f00\u653e\u6027\u548c\u590d\u6742\u4efb\u52a1\u4e2d\u6210\u4e3a\u5173\u952e\u74f6\u9888\uff0c\u9700\u8981\u53ef\u6269\u5c55\u4e14\u7ec6\u81f4\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5b9a\u4e49\u4e86agent-as-a-judge\u6982\u5ff5\uff0c\u8ffd\u8e2a\u4e86\u4ece\u5355\u6a21\u578b\u8bc4\u4f30\u5230\u52a8\u6001\u591a\u4ee3\u7406\u8fa9\u8bba\u6846\u67b6\u7684\u6f14\u53d8\uff0c\u5e76\u6bd4\u8f83\u4e86\u53ef\u9760\u6027\u3001\u6210\u672c\u548c\u4eba\u7c7b\u5bf9\u9f50\u7b49\u65b9\u9762\u3002", "result": "\u5c55\u793a\u4e86\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u91d1\u878d\u548c\u6559\u80b2\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u504f\u89c1\u3001\u9c81\u68d2\u6027\u548c\u5143\u8bc4\u4f30\u7b49\u6311\u6218\u3002", "conclusion": "\u57fa\u4e8e\u4ee3\u7406\u7684\u8bc4\u4f30\u53ef\u4ee5\u8865\u5145\uff08\u800c\u975e\u66ff\u4ee3\uff09\u4eba\u7c7b\u76d1\u7763\uff0c\u4e3a\u4e0b\u4e00\u4ee3LLMs\u7684\u53ef\u4fe1\u3001\u53ef\u6269\u5c55\u8bc4\u4f30\u8fc8\u51fa\u4e00\u6b65\u3002"}}
{"id": "2508.02999", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02999", "abs": "https://arxiv.org/abs/2508.02999", "authors": ["Xinjie Zhao", "Moritz Blum", "Fan Gao", "Yingjian Chen", "Boming Yang", "Luis Marquez-Carpintero", "M\u00f3nica Pina-Navarro", "Yanran Fu", "So Morikawa", "Yusuke Iwasawa", "Yutaka Matsuo", "Chanjun Park", "Irene Li"], "title": "AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots", "comment": "CIKM 2025, Demo Track", "summary": "AGENTiGraph is a user-friendly, agent-driven system that enables intuitive\ninteraction and management of domain-specific data through the manipulation of\nknowledge graphs in natural language. It gives non-technical users a complete,\nvisual solution to incrementally build and refine their knowledge bases,\nallowing multi-round dialogues and dynamic updates without specialized query\nlanguages. The flexible design of AGENTiGraph, including intent classification,\ntask planning, and automatic knowledge integration, ensures seamless reasoning\nbetween diverse tasks. Evaluated on a 3,500-query benchmark within an\neducational scenario, the system outperforms strong zero-shot baselines\n(achieving 95.12% classification accuracy, 90.45% execution success),\nindicating potential scalability to compliance-critical or multi-step queries\nin legal and medical domains, e.g., incorporating new statutes or research on\nthe fly. Our open-source demo offers a powerful new paradigm for multi-turn\nenterprise knowledge management that bridges LLMs and structured graphs.", "AI": {"tldr": "AGENTiGraph\u662f\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684\u3001\u57fa\u4e8e\u4ee3\u7406\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u64cd\u4f5c\u77e5\u8bc6\u56fe\u8c31\uff0c\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u76f4\u89c2\u7684\u6570\u636e\u4ea4\u4e92\u548c\u7ba1\u7406\u3002", "motivation": "\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u65e0\u9700\u4e13\u4e1a\u67e5\u8be2\u8bed\u8a00\u7684\u5de5\u5177\uff0c\u4f7f\u5176\u80fd\u591f\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u548c\u52a8\u6001\u66f4\u65b0\u6784\u5efa\u548c\u4f18\u5316\u77e5\u8bc6\u5e93\u3002", "method": "\u7cfb\u7edf\u8bbe\u8ba1\u5305\u62ec\u610f\u56fe\u5206\u7c7b\u3001\u4efb\u52a1\u89c4\u5212\u548c\u81ea\u52a8\u77e5\u8bc6\u96c6\u6210\uff0c\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\u7684\u65e0\u7f1d\u63a8\u7406\u3002", "result": "\u57283500\u4e2a\u67e5\u8be2\u7684\u6559\u80b2\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7cfb\u7edf\u8868\u73b0\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7ebf\uff08\u5206\u7c7b\u51c6\u786e\u738795.12%\uff0c\u6267\u884c\u6210\u529f\u738790.45%\uff09\u3002", "conclusion": "AGENTiGraph\u5c55\u793a\u4e86\u5728\u5408\u89c4\u5173\u952e\u6216\u591a\u6b65\u9aa4\u67e5\u8be2\uff08\u5982\u6cd5\u5f8b\u548c\u533b\u7597\u9886\u57df\uff09\u4e2d\u7684\u6269\u5c55\u6f5c\u529b\uff0c\u4e3a\u591a\u8f6e\u4f01\u4e1a\u77e5\u8bc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.03018", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03018", "abs": "https://arxiv.org/abs/2508.03018", "authors": ["Yutong Wang", "Pengliang Ji", "Kaixin Li", "Baolong Bi", "Tao Feng", "Guillaume Sartoretti"], "title": "Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning", "comment": null, "summary": "Large Language Reasoning Models have demonstrated remarkable success on\nstatic tasks, yet their application to multi-round agentic planning in\ninteractive environments faces two fundamental challenges. First, the\nintractable credit assignment problem renders conventional reinforcement\nlearning ineffective in sparse-reward settings. Second, the computational\noverhead of verbose, step-by-step reasoning histories is prohibitive. To\naddress these challenges, we propose BPO, a three-stage framework\n(bootstrapping, extrapolation, and refinement) that establishes a\nself-improving data flywheel to develop robust reasoning models for\nlong-horizon, sparse-reward environments. Our framework first bootstraps\nefficient reasoning using the proposed planning quaternions with long-short\nchain-of-thought fusion. It then extrapolates to out-of-distribution tasks\nthrough complexity-stratified curriculum learning. Finally, the model\niteratively refines itself by learning exclusively on experiences selected via\nreward-gated rejection sampling. Experiments on ALFWorld, ScienceWorld, and\nWebShop demonstrate that our approach achieves state-of-the-art with\nsignificant token efficiency, providing a new recipe for reasoning models in\nagentic planning.", "AI": {"tldr": "BPO\u6846\u67b6\u901a\u8fc7\u4e09\u9636\u6bb5\uff08\u5f15\u5bfc\u3001\u5916\u63a8\u548c\u4f18\u5316\uff09\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u63a8\u7406\u6a21\u578b\u5728\u591a\u8f6e\u4ee3\u7406\u89c4\u5212\u4e2d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u4ee3\u7406\u89c4\u5212\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "method": "\u63d0\u51faBPO\u6846\u67b6\uff0c\u5305\u62ec\u5f15\u5bfc\uff08\u89c4\u5212\u56db\u5143\u7ec4\uff09\u3001\u5916\u63a8\uff08\u590d\u6742\u5ea6\u5206\u5c42\u8bfe\u7a0b\u5b66\u4e60\uff09\u548c\u4f18\u5316\uff08\u5956\u52b1\u95e8\u63a7\u62d2\u7edd\u91c7\u6837\uff09\u3002", "result": "\u5728ALFWorld\u3001ScienceWorld\u548cWebShop\u4e0a\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u5177\u6709\u663e\u8457\u7684\u6807\u8bb0\u6548\u7387\u3002", "conclusion": "BPO\u4e3a\u4ee3\u7406\u89c4\u5212\u4e2d\u7684\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u957f\u89c6\u91ce\u3001\u7a00\u758f\u5956\u52b1\u73af\u5883\u3002"}}
{"id": "2508.03030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03030", "abs": "https://arxiv.org/abs/2508.03030", "authors": ["Siyuan Li", "Yifan Yu", "Yanchen Deng", "Zhihao Zhang", "Mengjing Chen", "Fangzhou Zhu", "Tao Zhong", "Jianye Hao", "Peng Liu", "Bo An"], "title": "Collab-Solver: Collaborative Solving Policy Learning for Mixed-Integer Linear Programming", "comment": null, "summary": "Mixed-integer linear programming (MILP) has been a fundamental problem in\ncombinatorial optimization. Previous works have designed a plethora of\nhard-coded heuristics to accomplish challenging MILP solving with domain\nknowledge. Driven by the high capability of neural networks, recent research is\ndevoted to replacing manually designed heuristics with learned policies.\nAlthough learning-based MILP methods have shown great promise, existing\nworksindependentlytreatthepolicylearningineachmoduleofMILPsolvers without\nconsidering their interdependence, severely hurting the solving speed and\nquality. To address this issue, we propose a novel multi-agent-based policy\nlearning framework for MILP (Collab-Solver), which can collaboratively optimize\nthe policies for multiple modules. Specifically, we formulate the collaboration\nof cut selection and branching in MILP solving as a Stackelberg game. Under\nthis formulation, we develop a two-phase learning paradigm to stabilize the\ncollaborative policy learning, where the first phase achieves the\ndata-communicated policy pretraining and the second phase further orchestrates\nthe policy learning for various modules. The jointly learned policy\nsignificantly improves the solving performance on both synthetic and\nlarge-scale real-world MILP datasets. Moreover, the policies learned by\nCollab-Solver have also demonstrated excellent generalization abilities across\ndifferent instance sets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u5b66\u4e60\u6846\u67b6\uff08Collab-Solver\uff09\uff0c\u7528\u4e8e\u4f18\u5316\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6c42\u89e3\u5668\u4e2d\u591a\u4e2a\u6a21\u5757\u7684\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c42\u89e3\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684MILP\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u5404\u6a21\u5757\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u5ffd\u89c6\u4e86\u6a21\u5757\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\uff0c\u5f71\u54cd\u4e86\u6c42\u89e3\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u5c06MILP\u6c42\u89e3\u4e2d\u7684\u5272\u5e73\u9762\u9009\u62e9\u548c\u5206\u652f\u534f\u4f5c\u5efa\u6a21\u4e3aStackelberg\u535a\u5f08\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u8303\u5f0f\uff08\u6570\u636e\u901a\u4fe1\u9884\u8bad\u7ec3\u548c\u6a21\u5757\u7b56\u7565\u534f\u540c\u4f18\u5316\uff09\u6765\u7a33\u5b9a\u534f\u4f5c\u5b66\u4e60\u3002", "result": "\u5728\u5408\u6210\u548c\u5b9e\u9645\u5927\u89c4\u6a21MILP\u6570\u636e\u96c6\u4e0a\uff0c\u8054\u5408\u5b66\u4e60\u7684\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6c42\u89e3\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Collab-Solver\u901a\u8fc7\u534f\u4f5c\u5b66\u4e60\u4f18\u5316MILP\u6c42\u89e3\u5668\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u6a21\u5757\u95f4\u72ec\u7acb\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2508.03031", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03031", "abs": "https://arxiv.org/abs/2508.03031", "authors": ["Ziyang Ma", "Baojian Zhou", "Deqing Yang", "Yanghua Xiao"], "title": "From Text to Trajectories: GPT-2 as an ODE Solver via In-Context", "comment": null, "summary": "In-Context Learning (ICL) has emerged as a new paradigm in large language\nmodels (LLMs), enabling them to perform novel tasks by conditioning on a few\nexamples embedded in the prompt. Yet, the highly nonlinear behavior of ICL for\nNLP tasks remains poorly understood. To shed light on its underlying\nmechanisms, this paper investigates whether LLMs can solve ordinary\ndifferential equations (ODEs) under the ICL setting. We formulate standard ODE\nproblems and their solutions as sequential prompts and evaluate GPT-2 models on\nthese tasks. Experiments on two types of ODEs show that GPT-2 can effectively\nlearn a meta-ODE algorithm, with convergence behavior comparable to, or better\nthan, the Euler method, and achieve exponential accuracy gains with increasing\nnumbers of demonstrations. Moreover, the model generalizes to\nout-of-distribution (OOD) problems, demonstrating robust extrapolation\ncapabilities. These empirical findings provide new insights into the mechanisms\nof ICL in NLP and its potential for solving nonlinear numerical problems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u89e3\u51b3\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u7684\u80fd\u529b\uff0c\u53d1\u73b0GPT-2\u80fd\u6709\u6548\u5b66\u4e60ODE\u7b97\u6cd5\uff0c\u6027\u80fd\u63a5\u8fd1\u6216\u4f18\u4e8e\u6b27\u62c9\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22ICL\u5728\u975e\u7ebf\u6027\u6570\u503c\u95ee\u9898\u4e2d\u7684\u673a\u5236\uff0c\u63ed\u793aLLM\u5728\u89e3\u51b3ODE\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5c06ODE\u95ee\u9898\u53ca\u5176\u89e3\u4f5c\u4e3a\u5e8f\u5217\u63d0\u793a\uff0c\u8bc4\u4f30GPT-2\u5728\u4e24\u7c7bODE\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "GPT-2\u80fd\u5b66\u4e60ODE\u7b97\u6cd5\uff0c\u6027\u80fd\u63a5\u8fd1\u6b27\u62c9\u65b9\u6cd5\uff0c\u4e14\u968f\u7740\u793a\u4f8b\u589e\u52a0\u7cbe\u5ea6\u5448\u6307\u6570\u63d0\u5347\uff0c\u8fd8\u80fd\u6cdb\u5316\u5230\u5206\u5e03\u5916\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u4e3aICL\u5728NLP\u4e2d\u7684\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u5176\u5728\u89e3\u51b3\u975e\u7ebf\u6027\u6570\u503c\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.03038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03038", "abs": "https://arxiv.org/abs/2508.03038", "authors": ["Qi Peng", "Jialin Cui", "Jiayuan Xie", "Yi Cai", "Qing Li"], "title": "Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree", "comment": "Accepted by ACM MM 2025", "summary": "Large language models (LLMs) have shown great potential in the medical\ndomain. However, existing models still fall short when faced with complex\nmedical diagnosis task in the real world. This is mainly because they lack\nsufficient reasoning depth, which leads to information loss or logical jumps\nwhen processing a large amount of specialized medical data, leading to\ndiagnostic errors. To address these challenges, we propose Tree-of-Reasoning\n(ToR), a novel multi-agent framework designed to handle complex scenarios.\nSpecifically, ToR introduces a tree structure that can clearly record the\nreasoning path of LLMs and the corresponding clinical evidence. At the same\ntime, we propose a cross-validation mechanism to ensure the consistency of\nmulti-agent decision-making, thereby improving the clinical reasoning ability\nof multi-agents in complex medical scenarios. Experimental results on\nreal-world medical data show that our framework can achieve better performance\nthan existing baseline methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTree-of-Reasoning (ToR)\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u533b\u5b66\u8bca\u65ad\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u533b\u5b66\u8bca\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u4e3b\u8981\u56e0\u63a8\u7406\u6df1\u5ea6\u4e0d\u591f\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u6216\u903b\u8f91\u8df3\u8dc3\uff0c\u4ece\u800c\u5f15\u53d1\u8bca\u65ad\u9519\u8bef\u3002", "method": "ToR\u6846\u67b6\u91c7\u7528\u6811\u72b6\u7ed3\u6784\u8bb0\u5f55\u63a8\u7406\u8def\u5f84\u548c\u4e34\u5e8a\u8bc1\u636e\uff0c\u5e76\u5f15\u5165\u4ea4\u53c9\u9a8c\u8bc1\u673a\u5236\u786e\u4fdd\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u771f\u5b9e\u533b\u5b66\u6570\u636e\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ToR\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u5728\u590d\u6742\u533b\u5b66\u573a\u666f\u4e2d\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.03054", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03054", "abs": "https://arxiv.org/abs/2508.03054", "authors": ["Rui Pu", "Chaozhuo Li", "Rui Ha", "Litian Zhang", "Lirong Qiu", "Xi Zhang"], "title": "Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning", "comment": null, "summary": "Defending large language models (LLMs) against jailbreak attacks is essential\nfor their safe and reliable deployment. Existing defenses often rely on shallow\npattern matching, which struggles to generalize to novel and unseen attack\nstrategies. To address this challenge, we propose the Cognitive-Driven Defense\n(CDD) framework, which targets the underlying structure of jailbreak prompts by\napplying meta-operations, defined as basic manipulations that conceal harmful\nintent.CDD emulates human cognitive reasoning through a structured reasoning\nchain. It begins with a global perception of the prompt and follows with a\nlocalized analysis to uncover hidden manipulations. By applying supervised\nfine-tuning on this structured chain, the model learns to identify and reason\nabout known manipulation patterns. To enhance generalization to unseen threats,\nan entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to\nencourage exploration of new types and variants of meta-operations. Experiments\ndemonstrate that CDD can achieve state-of-the-art defense performance and\nexhibit strong generalization to unseen jailbreak attacks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba4\u77e5\u9a71\u52a8\u9632\u5fa1\uff08CDD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u9632\u5fa1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u8868\u73b0\u51fa\u8272\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u6d45\u5c42\u6a21\u5f0f\u5339\u914d\uff0c\u96be\u4ee5\u5e94\u5bf9\u65b0\u578b\u653b\u51fb\u7b56\u7565\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u901a\u7528\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "CDD\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u94fe\uff08\u5168\u5c40\u611f\u77e5\u548c\u5c40\u90e8\u5206\u6790\uff09\u8bc6\u522b\u9690\u85cf\u64cd\u4f5c\uff0c\u5e76\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u71b5\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\uff08EG-GRPO\uff09\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCDD\u5728\u9632\u5fa1\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "CDD\u4e3aLLM\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2508.03080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03080", "abs": "https://arxiv.org/abs/2508.03080", "authors": ["Shuang Liu", "Zelong Li", "Ruoyun Ma", "Haiyan Zhao", "Mengnan Du"], "title": "ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification in Commercial Contracts", "comment": null, "summary": "The potential of large language models (LLMs) in specialized domains such as\nlegal risk analysis remains underexplored. In response to growing interest in\nlocally deploying open-source LLMs for legal tasks while preserving data\nconfidentiality, this paper introduces ContractEval, the first benchmark to\nthoroughly evaluate whether open-source LLMs could match proprietary LLMs in\nidentifying clause-level legal risks in commercial contracts. Using the\nContract Understanding Atticus Dataset (CUAD), we assess 4 proprietary and 15\nopen-source LLMs. Our results highlight five key findings: (1) Proprietary\nmodels outperform open-source models in both correctness and output\neffectiveness, though some open-source models are competitive in certain\nspecific dimensions. (2) Larger open-source models generally perform better,\nthough the improvement slows down as models get bigger. (3) Reasoning\n(\"thinking\") mode improves output effectiveness but reduces correctness, likely\ndue to over-complicating simpler tasks. (4) Open-source models generate \"no\nrelated clause\" responses more frequently even when relevant clauses are\npresent. This suggests \"laziness\" in thinking or low confidence in extracting\nrelevant content. (5) Model quantization speeds up inference but at the cost of\nperformance drop, showing the tradeoff between efficiency and accuracy. These\nfindings suggest that while most LLMs perform at a level comparable to junior\nlegal assistants, open-source models require targeted fine-tuning to ensure\ncorrectness and effectiveness in high-stakes legal settings. ContractEval\noffers a solid benchmark to guide future development of legal-domain LLMs.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u5f00\u6e90\u4e0e\u4e13\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u98ce\u9669\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u5f00\u6e90\u6a21\u578b\u9700\u9488\u5bf9\u6027\u4f18\u5316\u3002", "motivation": "\u63a2\u7d22\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u6f5c\u529b\uff0c\u6ee1\u8db3\u672c\u5730\u90e8\u7f72\u9700\u6c42\u5e76\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "method": "\u4f7f\u7528Contract Understanding Atticus Dataset (CUAD)\u8bc4\u4f304\u4e2a\u4e13\u6709\u548c15\u4e2a\u5f00\u6e90LLM\uff0c\u5206\u6790\u5176\u8bc6\u522b\u5546\u4e1a\u5408\u540c\u6761\u6b3e\u98ce\u9669\u7684\u80fd\u529b\u3002", "result": "\u4e13\u6709\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u5f00\u6e90\u6a21\u578b\u5728\u7279\u5b9a\u7ef4\u5ea6\u6709\u7ade\u4e89\u529b\uff1b\u6a21\u578b\u89c4\u6a21\u589e\u5927\u6548\u679c\u63d0\u5347\u51cf\u7f13\uff1b\u63a8\u7406\u6a21\u5f0f\u5f71\u54cd\u6027\u80fd\uff1b\u5f00\u6e90\u6a21\u578b\u6613\u6f0f\u68c0\u76f8\u5173\u6761\u6b3e\uff1b\u91cf\u5316\u52a0\u901f\u4f46\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u5f00\u6e90\u6a21\u578b\u9700\u9488\u5bf9\u6027\u4f18\u5316\u4ee5\u5339\u914d\u4e13\u6709\u6a21\u578b\uff0cContractEval\u4e3a\u6cd5\u5f8b\u9886\u57dfLLM\u53d1\u5c55\u63d0\u4f9b\u57fa\u51c6\u3002"}}
{"id": "2508.03082", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03082", "abs": "https://arxiv.org/abs/2508.03082", "authors": ["Fei Liu", "Yilu Liu", "Qingfu Zhang", "Xialiang Tong", "Mingxuan Yuan"], "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design", "comment": null, "summary": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has\nachieved notable success in recent years. Despite the effectiveness of existing\napproaches, they only design a single heuristic to serve all problem instances,\noften inducing poor generalization across different distributions or settings.\nTo address this issue, we propose Automated Heuristic Set Design (AHSD), a new\nformulation for LLM-driven AHD. The aim of AHSD is to automatically generate a\nsmall-sized complementary heuristic set to serve diverse problem instances,\nsuch that each problem instance could be optimized by at least one heuristic in\nthis set. We show that the objective function of AHSD is monotone and\nsupermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the\nAHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary\npopulation management and complementary-aware memetic search, EoH-S could\neffectively generate a set of high-quality and complementary heuristics.\nComprehensive experimental results on three AHD tasks with diverse instances\nspanning various sizes and distributions demonstrate that EoH-S consistently\noutperforms existing state-of-the-art AHD methods and achieves up to 60\\%\nperformance improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u5316\u542f\u53d1\u5f0f\u96c6\u5408\u8bbe\u8ba1\uff08AHSD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e92\u8865\u7684\u542f\u53d1\u5f0f\u96c6\u5408\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86EoH-S\u7b97\u6cd5\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u8bbe\u8ba1\u5355\u4e00\u542f\u53d1\u5f0f\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u95ee\u9898\u5b9e\u4f8b\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faAHSD\u6846\u67b6\uff0c\u76ee\u6807\u4e3a\u751f\u6210\u4e92\u8865\u542f\u53d1\u5f0f\u96c6\u5408\uff0c\u5e76\u8bbe\u8ba1EoH-S\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e92\u8865\u79cd\u7fa4\u7ba1\u7406\u548c\u4e92\u8865\u611f\u77e5\u7684\u6a21\u56e0\u641c\u7d22\u3002", "result": "\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEoH-S\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe60%\u3002", "conclusion": "AHSD\u548cEoH-S\u6709\u6548\u89e3\u51b3\u4e86\u5355\u4e00\u542f\u53d1\u5f0f\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u5316\u542f\u53d1\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.03083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03083", "abs": "https://arxiv.org/abs/2508.03083", "authors": ["Youran Zhou", "Mohamed Reda Bouadjenek", "Sunil Aryal"], "title": "MissDDIM: Deterministic and Efficient Conditional Diffusion for Tabular Data Imputation", "comment": null, "summary": "Diffusion models have recently emerged as powerful tools for missing data\nimputation by modeling the joint distribution of observed and unobserved\nvariables. However, existing methods, typically based on stochastic denoising\ndiffusion probabilistic models (DDPMs), suffer from high inference latency and\nvariable outputs, limiting their applicability in real-world tabular settings.\nTo address these deficiencies, we present in this paper MissDDIM, a conditional\ndiffusion framework that adapts Denoising Diffusion Implicit Models (DDIM) for\ntabular imputation. While stochastic sampling enables diverse completions, it\nalso introduces output variability that complicates downstream processing.", "AI": {"tldr": "MissDDIM\u662f\u4e00\u79cd\u57fa\u4e8eDDIM\u7684\u6761\u4ef6\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u8868\u683c\u6570\u636e\u586b\u8865\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u7684\u9ad8\u5ef6\u8fdf\u548c\u8f93\u51fa\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eDDPM\u7684\u7f3a\u5931\u6570\u636e\u586b\u8865\u65b9\u6cd5\u5b58\u5728\u9ad8\u63a8\u7406\u5ef6\u8fdf\u548c\u8f93\u51fa\u4e0d\u7a33\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMissDDIM\uff0c\u4e00\u79cd\u57fa\u4e8eDDIM\u7684\u6761\u4ef6\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u8868\u683c\u6570\u636e\u586b\u8865\u3002", "result": "MissDDIM\u901a\u8fc7\u786e\u5b9a\u6027\u91c7\u6837\u51cf\u5c11\u4e86\u8f93\u51fa\u4e0d\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u586b\u8865\u7684\u591a\u6837\u6027\u3002", "conclusion": "MissDDIM\u4e3a\u8868\u683c\u6570\u636e\u586b\u8865\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03091", "categories": ["cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03091", "abs": "https://arxiv.org/abs/2508.03091", "authors": ["Xingjun Ma", "Hanxun Huang", "Tianwei Song", "Ye Sun", "Yifeng Gao", "Yu-Gang Jiang"], "title": "T2UE: Generating Unlearnable Examples from Text Descriptions", "comment": "To appear in ACM MM 2025", "summary": "Large-scale pre-training frameworks like CLIP have revolutionized multimodal\nlearning, but their reliance on web-scraped datasets, frequently containing\nprivate user data, raises serious concerns about misuse. Unlearnable Examples\n(UEs) have emerged as a promising countermeasure against unauthorized model\ntraining, employing carefully crafted unlearnable noise to disrupt the learning\nof meaningful representations from protected data. Current approaches typically\ngenerate UEs by jointly optimizing unlearnable noise for both images and their\nassociated text descriptions (or labels). However, this optimization process is\noften computationally prohibitive for on-device execution, forcing reliance on\nexternal third-party services. This creates a fundamental privacy paradox:\nusers must initially expose their data to these very services to achieve\nprotection, thereby compromising privacy in the process. Such a contradiction\nhas severely hindered the development of practical, scalable data protection\nsolutions. To resolve this paradox, we introduce \\textbf{Text-to-Unlearnable\nExample (T2UE)}, a novel framework that enables users to generate UEs using\nonly text descriptions. T2UE circumvents the need for original image data by\nemploying a text-to-image (T2I) model to map text descriptions into the image\n(noise) space, combined with an error-minimization framework to produce\neffective unlearnable noise. Extensive experiments show that T2UE-protected\ndata substantially degrades performance in downstream tasks (e.g., cross-modal\nretrieval) for state-of-the-art models. Notably, the protective effect\ngeneralizes across diverse architectures and even to supervised learning\nsettings. Our work demonstrates the feasibility of \"zero-contact data\nprotection\", where personal data can be safeguarded based solely on their\ntextual descriptions, eliminating the need for direct data exposure.", "AI": {"tldr": "T2UE\u6846\u67b6\u901a\u8fc7\u4ec5\u4f7f\u7528\u6587\u672c\u63cf\u8ff0\u751f\u6210\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\uff08UEs\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u66b4\u9732\u539f\u59cb\u56fe\u50cf\u6570\u636e\u7684\u9690\u79c1\u77db\u76fe\u3002", "motivation": "\u73b0\u6709\u751f\u6210UEs\u7684\u65b9\u6cd5\u9700\u8981\u8054\u5408\u4f18\u5316\u56fe\u50cf\u548c\u6587\u672c\u566a\u58f0\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u7b2c\u4e09\u65b9\u670d\u52a1\uff0c\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002", "method": "T2UE\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5c06\u6587\u672c\u63cf\u8ff0\u6620\u5c04\u5230\u56fe\u50cf\uff08\u566a\u58f0\uff09\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u8bef\u5dee\u6700\u5c0f\u5316\u6846\u67b6\u751f\u6210\u6709\u6548\u7684\u4e0d\u53ef\u5b66\u4e60\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT2UE\u4fdd\u62a4\u7684\u6570\u636e\u663e\u8457\u964d\u4f4e\u4e86\u6700\u5148\u8fdb\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u8de8\u6a21\u6001\u68c0\u7d22\uff09\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u4fdd\u62a4\u6548\u679c\u6cdb\u5316\u5230\u591a\u79cd\u67b6\u6784\u548c\u76d1\u7763\u5b66\u4e60\u573a\u666f\u3002", "conclusion": "T2UE\u5b9e\u73b0\u4e86\u201c\u96f6\u63a5\u89e6\u6570\u636e\u4fdd\u62a4\u201d\uff0c\u4ec5\u9700\u6587\u672c\u63cf\u8ff0\u5373\u53ef\u4fdd\u62a4\u6570\u636e\uff0c\u65e0\u9700\u76f4\u63a5\u66b4\u9732\u539f\u59cb\u6570\u636e\u3002"}}
{"id": "2508.03092", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03092", "abs": "https://arxiv.org/abs/2508.03092", "authors": ["Zikun Cui", "Tianyi Huang", "Chia-En Chiang", "Cuiqianhe Du"], "title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework", "comment": null, "summary": "With the proliferation of Large Language Models (LLMs), the detection of\nmisinformation has become increasingly important and complex. This research\nproposes an innovative verifiable misinformation detection LLM agent that goes\nbeyond traditional true/false binary judgments. The agent actively verifies\nclaims through dynamic interaction with diverse web sources, assesses\ninformation source credibility, synthesizes evidence, and provides a complete\nverifiable reasoning process. Our designed agent architecture includes three\ncore tools: precise web search tool, source credibility assessment tool and\nnumerical claim verification tool. These tools enable the agent to execute\nmulti-step verification strategies, maintain evidence logs, and form\ncomprehensive assessment conclusions. We evaluate using standard misinformation\ndatasets such as FakeNewsNet, comparing with traditional machine learning\nmodels and LLMs. Evaluation metrics include standard classification metrics,\nquality assessment of reasoning processes, and robustness testing against\nrewritten content. Experimental results show that our agent outperforms\nbaseline methods in misinformation detection accuracy, reasoning transparency,\nand resistance to information rewriting, providing a new paradigm for\ntrustworthy AI-assisted fact-checking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9a8c\u8bc1\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4bLLM\u4ee3\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u4e0e\u591a\u6e90\u9a8c\u8bc1\uff0c\u8d85\u8d8a\u4f20\u7edf\u4e8c\u5143\u5224\u65ad\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u666e\u53ca\uff0c\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u53d8\u5f97\u6108\u53d1\u91cd\u8981\u4e14\u590d\u6742\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u7cbe\u786e\u7f51\u7edc\u641c\u7d22\u3001\u6765\u6e90\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u548c\u6570\u503c\u58f0\u660e\u9a8c\u8bc1\u5de5\u5177\u7684\u4ee3\u7406\u67b6\u6784\uff0c\u652f\u6301\u591a\u6b65\u9a8c\u8bc1\u7b56\u7565\u548c\u8bc1\u636e\u8bb0\u5f55\u3002", "result": "\u5728FakeNewsNet\u7b49\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4ee3\u7406\u5728\u51c6\u786e\u6027\u3001\u63a8\u7406\u900f\u660e\u5ea6\u548c\u6297\u6539\u5199\u80fd\u529b\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u4ee3\u7406\u4e3a\u53ef\u4fe1AI\u8f85\u52a9\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.03109", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03109", "abs": "https://arxiv.org/abs/2508.03109", "authors": ["Wen-Xi Yang", "Tian-Fang Zhao"], "title": "AgentSME for Simulating Diverse Communication Modes in Smart Education", "comment": null, "summary": "Generative agent models specifically tailored for smart education are\ncritical, yet remain relatively underdeveloped. A key challenge stems from the\ninherent complexity of educational contexts: learners are human beings with\nvarious cognitive behaviors, and pedagogy is fundamentally centered on\npersonalized human-to-human communication. To address this issue, this paper\nproposes AgentSME, a unified generative agent framework powered by LLM. Three\ndirectional communication modes are considered in the models, namely Solo,\nMono, and Echo, reflecting different types of agency autonomy and communicative\nreciprocity. Accuracy is adopted as the primary evaluation metric, complemented\nby three diversity indices designed to assess the diversity of reasoning\ncontents. Six widely used LLMs are tested to validate the robustness of\ncommunication modes across different model tiers, which are equally divided\ninto base-capacity and high-capacity configurations. The results show that\ngenerative agents that employ the Echo communication mode achieve the highest\naccuracy scores, while DeepSeek exhibits the greatest diversity. This study\nprovides valuable information to improve agent learning capabilities and\ninspire smart education models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7edf\u4e00\u751f\u6210\u4ee3\u7406\u6846\u67b6AgentSME\uff0c\u7528\u4e8e\u667a\u80fd\u6559\u80b2\uff0c\u901a\u8fc7\u4e09\u79cd\u901a\u4fe1\u6a21\u5f0f\uff08Solo\u3001Mono\u3001Echo\uff09\u9a8c\u8bc1\u4e86Echo\u6a21\u5f0f\u5728\u51c6\u786e\u6027\u4e0a\u7684\u4f18\u52bf\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e0d\u540cLLM\u7684\u591a\u6837\u6027\u3002", "motivation": "\u667a\u80fd\u6559\u80b2\u4e2d\u7684\u751f\u6210\u4ee3\u7406\u6a21\u578b\u5c1a\u672a\u5145\u5206\u53d1\u5c55\uff0c\u6559\u80b2\u73af\u5883\u7684\u590d\u6742\u6027\u548c\u4e2a\u6027\u5316\u9700\u6c42\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51faAgentSME\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u79cd\u901a\u4fe1\u6a21\u5f0f\uff08Solo\u3001Mono\u3001Echo\uff09\uff0c\u4ee5\u51c6\u786e\u6027\u4e3a\u4e3b\u8981\u6307\u6807\uff0c\u8f85\u4ee5\u591a\u6837\u6027\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u4e86\u516d\u79cdLLM\u3002", "result": "Echo\u901a\u4fe1\u6a21\u5f0f\u7684\u4ee3\u7406\u5728\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u6700\u4f73\uff0cDeepSeek\u5728\u591a\u6837\u6027\u4e0a\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347\u4ee3\u7406\u5b66\u4e60\u80fd\u529b\u548c\u667a\u80fd\u6559\u80b2\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002"}}
{"id": "2508.03117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03117", "abs": "https://arxiv.org/abs/2508.03117", "authors": ["Vinicius Lima", "Dzung T. Phan", "Jayant Kalagnanam", "Dhaval Patel", "Nianjun Zhou"], "title": "Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation", "comment": "25 pages", "summary": "We present a framework for training trustworthy large language model (LLM)\nagents for optimization modeling via a verifiable synthetic data generation\npipeline. Focusing on linear and mixed-integer linear programming, our approach\nbegins with structured symbolic representations and systematically produces\nnatural language descriptions, mathematical formulations, and solver-executable\ncode. By programmatically constructing each instance with known optimal\nsolutions, the pipeline ensures full verifiability and enables automatic\nfiltering of low-quality demonstrations generated by teacher models. Each\ndataset instance includes a structured representation of the optimization\nproblem, a corresponding natural language description, the verified optimal\nsolution, and step-by-step demonstrations - generated by a teacher model - that\nshow how to model and solve the problem across multiple optimization modeling\nlanguages. This enables supervised fine-tuning of open-source LLMs specifically\ntailored to optimization tasks. To operationalize this pipeline, we introduce\nOptiTrust, a modular LLM agent that performs multi-stage translation from\nnatural language to solver-ready code, leveraging stepwise demonstrations,\nmulti-language inference, and majority-vote cross-validation. Our agent\nachieves state-of-the-art performance on standard benchmarks. Out of 7\ndatasets, it achieves the highest accuracy on six and outperforms the next-best\nalgorithm by at least 8 percentage on three of them. Our approach provides a\nscalable, verifiable, and principled path toward building reliable LLM agents\nfor real-world optimization applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\u8bad\u7ec3\u53ef\u4fe1\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u7ebf\u6027\u548c\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff0c\u5b9e\u73b0\u4e86\u4ece\u7ed3\u6784\u5316\u7b26\u53f7\u8868\u793a\u5230\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001\u6570\u5b66\u516c\u5f0f\u53ca\u53ef\u6267\u884c\u4ee3\u7801\u7684\u7cfb\u7edf\u751f\u6210\u3002", "motivation": "\u4e3a\u4f18\u5316\u4efb\u52a1\u6784\u5efa\u53ef\u9760\u4e14\u53ef\u9a8c\u8bc1\u7684LLM\u4ee3\u7406\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u539f\u5219\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u5e26\u6709\u5df2\u77e5\u6700\u4f18\u89e3\u7684\u5b9e\u4f8b\uff0c\u786e\u4fdd\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u5229\u7528\u6559\u5e08\u6a21\u578b\u751f\u6210\u9010\u6b65\u6f14\u793a\u3002\u5f00\u53d1\u4e86OptiTrust\u4ee3\u7406\uff0c\u5b9e\u73b0\u4ece\u81ea\u7136\u8bed\u8a00\u5230\u53ef\u6267\u884c\u4ee3\u7801\u7684\u591a\u9636\u6bb5\u7ffb\u8bd1\u3002", "result": "\u57287\u4e2a\u6570\u636e\u96c6\u4e2d\uff0cOptiTrust\u57286\u4e2a\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u57283\u4e2a\u4e0a\u6bd4\u6b21\u4f18\u7b97\u6cd5\u81f3\u5c11\u9ad8\u51fa8\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u9002\u7528\u4e8e\u5b9e\u9645\u4f18\u5316\u5e94\u7528\u7684\u53ef\u4fe1\u8d56LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u9a8c\u8bc1\u7684\u8def\u5f84\u3002"}}
{"id": "2508.03149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03149", "abs": "https://arxiv.org/abs/2508.03149", "authors": ["Linda Smail", "David Santandreu Calonge", "Firuz Kamalov", "Nur H. Orak"], "title": "Can Large Language Models Bridge the Gap in Environmental Knowledge?", "comment": "20 pages, 3 figures, 7 tables. No external funding", "summary": "This research investigates the potential of Artificial Intelligence (AI)\nmodels to bridge the knowledge gap in environmental education among university\nstudents. By focusing on prominent large language models (LLMs) such as\nGPT-3.5, GPT-4, GPT-4o, Gemini, Claude Sonnet, and Llama 2, the study assesses\ntheir effectiveness in conveying environmental concepts and, consequently,\nfacilitating environmental education. The investigation employs a standardized\ntool, the Environmental Knowledge Test (EKT-19), supplemented by targeted\nquestions, to evaluate the environmental knowledge of university students in\ncomparison to the responses generated by the AI models. The results of this\nstudy suggest that while AI models possess a vast, readily accessible, and\nvalid knowledge base with the potential to empower both students and academic\nstaff, a human discipline specialist in environmental sciences may still be\nnecessary to validate the accuracy of the information provided.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8AI\u6a21\u578b\uff08\u5982GPT-3.5\u3001GPT-4\u7b49\uff09\u5728\u5f25\u8865\u5927\u5b66\u751f\u73af\u5883\u6559\u80b2\u77e5\u8bc6\u5dee\u8ddd\u4e2d\u7684\u6f5c\u529b\uff0c\u53d1\u73b0AI\u867d\u80fd\u63d0\u4f9b\u4e30\u5bcc\u77e5\u8bc6\uff0c\u4f46\u4ecd\u9700\u4eba\u7c7b\u4e13\u5bb6\u9a8c\u8bc1\u51c6\u786e\u6027\u3002", "motivation": "\u8bc4\u4f30AI\u6a21\u578b\u5728\u73af\u5883\u6559\u80b2\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4ee5\u5f25\u8865\u5b66\u751f\u77e5\u8bc6\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u5de5\u5177EKT-19\u53ca\u9488\u5bf9\u6027\u95ee\u9898\uff0c\u6bd4\u8f83AI\u6a21\u578b\u4e0e\u5927\u5b66\u751f\u73af\u5883\u77e5\u8bc6\u6c34\u5e73\u3002", "result": "AI\u6a21\u578b\u5177\u5907\u4e30\u5bcc\u77e5\u8bc6\u5e93\uff0c\u4f46\u4fe1\u606f\u51c6\u786e\u6027\u9700\u73af\u5883\u79d1\u5b66\u4e13\u5bb6\u9a8c\u8bc1\u3002", "conclusion": "AI\u5728\u73af\u5883\u6559\u80b2\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u4ee5\u786e\u4fdd\u4fe1\u606f\u51c6\u786e\u3002"}}
{"id": "2508.03167", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03167", "abs": "https://arxiv.org/abs/2508.03167", "authors": ["Charles Tapley Hoyt", "Craig Bakker", "Richard J. Callahan", "Joseph Cottam", "August George", "Benjamin M. Gyori", "Haley M. Hummel", "Nathaniel Merrill", "Sara Mohammad Taheri", "Pruthvi Prakash Navada", "Marc-Antoine Parent", "Adam Rupe", "Olga Vitek", "Jeremy Zucker"], "title": "Causal identification with $Y_0$", "comment": null, "summary": "We present the $Y_0$ Python package, which implements causal identification\nalgorithms that apply interventional, counterfactual, and transportability\nqueries to data from (randomized) controlled trials, observational studies, or\nmixtures thereof. $Y_0$ focuses on the qualitative investigation of causation,\nhelping researchers determine whether a causal relationship can be estimated\nfrom available data before attempting to estimate how strong that relationship\nis. Furthermore, $Y_0$ provides guidance on how to transform the causal query\ninto a symbolic estimand that can be non-parametrically estimated from the\navailable data. $Y_0$ provides a domain-specific language for representing\ncausal queries and estimands as symbolic probabilistic expressions, tools for\nrepresenting causal graphical models with unobserved confounders, such as\nacyclic directed mixed graphs (ADMGs), and implementations of numerous\nidentification algorithms from the recent causal inference literature. The\n$Y_0$ source code can be found under the MIT License at\nhttps://github.com/y0-causal-inference/y0 and it can be installed with pip\ninstall y0.", "AI": {"tldr": "$Y_0$\u662f\u4e00\u4e2aPython\u5305\uff0c\u7528\u4e8e\u5b9e\u73b0\u56e0\u679c\u8bc6\u522b\u7b97\u6cd5\uff0c\u652f\u6301\u5e72\u9884\u3001\u53cd\u4e8b\u5b9e\u548c\u53ef\u8fc1\u79fb\u6027\u67e5\u8be2\uff0c\u9002\u7528\u4e8e\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u548c\u89c2\u5bdf\u6027\u7814\u7a76\u3002", "motivation": "\u5e2e\u52a9\u7814\u7a76\u8005\u5728\u5c1d\u8bd5\u4f30\u8ba1\u56e0\u679c\u5173\u7cfb\u7684\u5f3a\u5ea6\u4e4b\u524d\uff0c\u5148\u5b9a\u6027\u5206\u6790\u56e0\u679c\u5173\u7cfb\u662f\u5426\u53ef\u4ee5\u4ece\u53ef\u7528\u6570\u636e\u4e2d\u4f30\u8ba1\u3002", "method": "\u63d0\u4f9b\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u8868\u793a\u56e0\u679c\u67e5\u8be2\u548c\u7b26\u53f7\u6982\u7387\u8868\u8fbe\u5f0f\uff0c\u652f\u6301\u542b\u672a\u89c2\u6d4b\u6df7\u6742\u7684\u56e0\u679c\u56fe\u6a21\u578b\uff08\u5982ADMGs\uff09\uff0c\u5e76\u5b9e\u73b0\u591a\u79cd\u8bc6\u522b\u7b97\u6cd5\u3002", "result": "$Y_0$\u80fd\u5c06\u56e0\u679c\u67e5\u8be2\u8f6c\u5316\u4e3a\u7b26\u53f7\u4f30\u8ba1\u91cf\uff0c\u652f\u6301\u975e\u53c2\u6570\u4f30\u8ba1\uff0c\u5e76\u5f00\u6e90\u63d0\u4f9b\u5de5\u5177\u3002", "conclusion": "$Y_0$\u4e3a\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u652f\u6301\u590d\u6742\u67e5\u8be2\u548c\u6a21\u578b\uff0c\u4fc3\u8fdb\u56e0\u679c\u7814\u7a76\u7684\u5b9a\u6027\u5206\u6790\u3002"}}
{"id": "2508.03173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03173", "abs": "https://arxiv.org/abs/2508.03173", "authors": ["Jingxuan Wei", "Caijun Jia", "Qi Chen", "Honghao He", "Linzhuang Sun", "Conghui He", "Lijun Wu", "Bihui Yu", "Cheng Tan"], "title": "Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions", "comment": null, "summary": "Mathematical geometric reasoning is essential for scientific discovery and\neducational development, requiring precise logic and rigorous formal\nverification. While recent advances in Multimodal Large Language Models (MLLMs)\nhave improved reasoning tasks, existing models typically struggle with formal\ngeometric reasoning, particularly when dynamically constructing and verifying\nauxiliary geometric elements. To address these challenges, we introduce\nGeoint-R1, a multimodal reasoning framework designed to generate formally\nverifiable geometric solutions from textual descriptions and visual diagrams.\nGeoint-R1 uniquely integrates auxiliary elements construction, formal reasoning\nrepresented via Lean4, and interactive visualization. To systematically\nevaluate and advance formal geometric reasoning, we propose the Geoint\nbenchmark, comprising 1,885 rigorously annotated geometry problems across\ndiverse topics such as plane, spatial, and solid geometry. Each problem\nincludes structured textual annotations, precise Lean4 code for auxiliary\nconstructions, and detailed solution steps verified by experts. Extensive\nexperiments demonstrate that Geoint-R1 significantly surpasses existing\nmultimodal and math-specific reasoning models, particularly on challenging\nproblems requiring explicit auxiliary element constructions.", "AI": {"tldr": "Geoint-R1\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u65e8\u5728\u4ece\u6587\u672c\u63cf\u8ff0\u548c\u89c6\u89c9\u56fe\u8868\u751f\u6210\u53ef\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u51e0\u4f55\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u5f62\u5f0f\u5316\u51e0\u4f55\u63a8\u7406\uff08\u5c24\u5176\u662f\u52a8\u6001\u6784\u5efa\u548c\u9a8c\u8bc1\u8f85\u52a9\u51e0\u4f55\u5143\u7d20\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "Geoint-R1\u6574\u5408\u4e86\u8f85\u52a9\u5143\u7d20\u6784\u5efa\u3001\u901a\u8fc7Lean4\u8868\u793a\u7684\u5f62\u5f0f\u63a8\u7406\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGeoint-R1\u5728\u9700\u8981\u663e\u5f0f\u8f85\u52a9\u5143\u7d20\u6784\u5efa\u7684\u6311\u6218\u6027\u95ee\u9898\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "Geoint-R1\u4e3a\u5f62\u5f0f\u5316\u51e0\u4f55\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7Geoint\u57fa\u51c6\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.03174", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03174", "abs": "https://arxiv.org/abs/2508.03174", "authors": ["Tian-Fang Zhao", "Wen-Xi Yang"], "title": "InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation", "comment": null, "summary": "Collaborative partnership matters in inquiry-oriented education. However,\nmost study partners are selected either rely on experience-based assignments\nwith little scientific planning or build on rule-based machine assistants,\nencountering difficulties in knowledge expansion and inadequate flexibility.\nThis paper proposes an LLM-empowered agent model for simulating and selecting\nlearning partners tailored to inquiry-oriented learning, named InqEduAgent.\nGenerative agents are designed to capture cognitive and evaluative features of\nlearners in real-world scenarios. Then, an adaptive matching algorithm with\nGaussian process augmentation is formulated to identify patterns within prior\nknowledge. Optimal learning-partner matches are provided for learners facing\ndifferent exercises. The experimental results show the optimal performance of\nInqEduAgent in most knowledge-learning scenarios and LLM environment with\ndifferent levels of capabilities. This study promotes the intelligent\nallocation of human-based learning partners and the formulation of AI-based\nlearning partners. The code, data, and appendix are publicly available at\nhttps://github.com/InqEduAgent/InqEduAgent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u6a21\u578bInqEduAgent\uff0c\u7528\u4e8e\u6a21\u62df\u548c\u9009\u62e9\u9002\u5408\u63a2\u7a76\u5f0f\u5b66\u4e60\u7684\u5b66\u4e60\u4f19\u4f34\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5339\u914d\u7b97\u6cd5\u4f18\u5316\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u63a2\u7a76\u5f0f\u6559\u80b2\u4e2d\u5b66\u4e60\u4f19\u4f34\u7684\u9009\u62e9\u901a\u5e38\u4f9d\u8d56\u7ecf\u9a8c\u6216\u89c4\u5219\uff0c\u7f3a\u4e4f\u79d1\u5b66\u89c4\u5212\u548c\u7075\u6d3b\u6027\uff0c\u5bfc\u81f4\u77e5\u8bc6\u6269\u5c55\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u751f\u6210\u4ee3\u7406\u6355\u6349\u5b66\u4e60\u8005\u7684\u8ba4\u77e5\u548c\u8bc4\u4f30\u7279\u5f81\uff0c\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u589e\u5f3a\u7684\u81ea\u9002\u5e94\u5339\u914d\u7b97\u6cd5\u8bc6\u522b\u5148\u9a8c\u77e5\u8bc6\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eInqEduAgent\u5728\u591a\u6570\u77e5\u8bc6\u5b66\u4e60\u573a\u666f\u548c\u4e0d\u540c\u80fd\u529b\u7684LLM\u73af\u5883\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u57fa\u4e8e\u4eba\u7c7b\u5b66\u4e60\u4f19\u4f34\u7684\u667a\u80fd\u5206\u914d\u548cAI\u5b66\u4e60\u4f19\u4f34\u7684\u6784\u5efa\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.03251", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03251", "abs": "https://arxiv.org/abs/2508.03251", "authors": ["Osama Mohammed", "Jiaxin Pan", "Mojtaba Nayyeri", "Daniel Hern\u00e1ndez", "Steffen Staab"], "title": "Full-History Graphs with Edge-Type Decoupled Networks for Temporal Reasoning", "comment": "European Conference of Artificial Intelligence 2025", "summary": "Modeling evolving interactions among entities is critical in many real-world\ntasks. For example, predicting driver maneuvers in traffic requires tracking\nhow neighboring vehicles accelerate, brake, and change lanes relative to one\nanother over consecutive frames. Likewise, detecting financial fraud hinges on\nfollowing the flow of funds through successive transactions as they propagate\nthrough the network. Unlike classic time-series forecasting, these settings\ndemand reasoning over who interacts with whom and when, calling for a\ntemporal-graph representation that makes both the relations and their evolution\nexplicit. Existing temporal-graph methods typically use snapshot graphs to\nencode temporal evolution. We introduce a full-history graph that instantiates\none node for every entity at every time step and separates two edge sets: (i)\nintra-time-step edges that capture relations within a single frame and (ii)\ninter-time-step edges that connect an entity to itself at consecutive steps. To\nlearn on this graph we design an Edge-Type Decoupled Network (ETDNet) with\nparallel modules: a graph-attention module aggregates information along\nintra-time-step edges, a multi-head temporal-attention module attends over an\nentity's inter-time-step history, and a fusion module combines the two messages\nafter every layer. Evaluated on driver-intention prediction (Waymo) and Bitcoin\nfraud detection (Elliptic++), ETDNet consistently surpasses strong baselines,\nlifting Waymo joint accuracy to 75.6\\% (vs. 74.1\\%) and raising Elliptic++\nillicit-class F1 to 88.1\\% (vs. 60.4\\%). These gains demonstrate the benefit of\nrepresenting structural and temporal relations as distinct edges in a single\ngraph.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u5386\u53f2\u56fe\u6a21\u578b\u548cETDNet\u7f51\u7edc\uff0c\u7528\u4e8e\u5efa\u6a21\u5b9e\u4f53\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u610f\u56fe\u9884\u6d4b\u548c\u6bd4\u7279\u5e01\u6b3a\u8bc8\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4efb\u52a1\u4e2d\u9700\u8981\u5efa\u6a21\u5b9e\u4f53\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\uff08\u5982\u4ea4\u901a\u9884\u6d4b\u3001\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\uff09\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u6355\u6349\u7ed3\u6784\u548c\u65f6\u95f4\u5173\u7cfb\u3002", "method": "\u5f15\u5165\u5168\u5386\u53f2\u56fe\uff0c\u533a\u5206\u65f6\u95f4\u6b65\u5185\u548c\u65f6\u95f4\u6b65\u95f4\u8fb9\uff1b\u8bbe\u8ba1ETDNet\u7f51\u7edc\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u548c\u65f6\u95f4\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5728Waymo\u548cElliptic++\u6570\u636e\u96c6\u4e0a\uff0cETDNet\u5206\u522b\u5c06\u51c6\u786e\u7387\u548cF1\u5206\u6570\u63d0\u5347\u81f375.6%\u548c88.1%\u3002", "conclusion": "\u901a\u8fc7\u533a\u5206\u7ed3\u6784\u548c\u65f6\u95f4\u5173\u7cfb\uff0c\u5168\u5386\u53f2\u56fe\u548cETDNet\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u4ea4\u4e92\u5efa\u6a21\u7684\u6027\u80fd\u3002"}}
{"id": "2508.03284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03284", "abs": "https://arxiv.org/abs/2508.03284", "authors": ["Shaofeng Yin", "Ting Lei", "Yang Liu"], "title": "ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools", "comment": null, "summary": "Integrating external tools into Large Foundation Models (LFMs) has emerged as\na promising approach to enhance their problem-solving capabilities. While\nexisting studies have demonstrated strong performance in tool-augmented Visual\nQuestion Answering (VQA), recent benchmarks reveal significant gaps in\nreal-world tool-use proficiency, particularly in functionally diverse\nmultimodal settings requiring multi-step reasoning. In this work, we introduce\nToolVQA, a large-scale multimodal dataset comprising 23K instances, designed to\nbridge this gap. Unlike previous datasets that rely on synthetic scenarios and\nsimplified queries, ToolVQA features real-world visual contexts and challenging\nimplicit multi-step reasoning tasks, better aligning with real user\ninteractions. To construct this dataset, we propose ToolEngine, a novel data\ngeneration pipeline that employs Depth-First Search (DFS) with a dynamic\nin-context example matching mechanism to simulate human-like tool-use\nreasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse task\ndomains, with an average inference length of 2.78 reasoning steps per instance.\nThe fine-tuned 7B LFMs on ToolVQA not only achieve impressive performance on\nour test set but also surpass the large close-sourced model GPT-3.5-turbo on\nvarious out-of-distribution (OOD) datasets, demonstrating strong\ngeneralizability to real-world tool-use scenarios.", "AI": {"tldr": "ToolVQA\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u57fa\u7840\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5de5\u5177\u589e\u5f3a\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u63d0\u51faToolVQA\u6570\u636e\u96c6\u548cToolEngine\u6570\u636e\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u91c7\u7528\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff08DFS\uff09\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u793a\u4f8b\u5339\u914d\u673a\u5236\u6a21\u62df\u4eba\u7c7b\u5de5\u5177\u4f7f\u7528\u63a8\u7406\u3002", "result": "\u5728ToolVQA\u4e0a\u5fae\u8c03\u76847B LFMs\u5728\u6d4b\u8bd5\u96c6\u548c\u591a\u4e2aOOD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8aGPT-3.5-turbo\u3002", "conclusion": "ToolVQA\u548cToolEngine\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.03341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03341", "abs": "https://arxiv.org/abs/2508.03341", "authors": ["Jiayan Nan", "Wenquan Ma", "Wenlong Wu", "Yize Chen"], "title": "Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science", "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet their\ninability to maintain persistent memory in long contexts limits their\neffectiveness as autonomous agents in long-term interactions. While existing\nmemory systems have made progress, their reliance on arbitrary granularity for\ndefining the basic memory unit and passive, rule-based mechanisms for knowledge\nextraction limits their capacity for genuine learning and evolution. To address\nthese foundational limitations, we present Nemori, a novel self-organizing\nmemory architecture inspired by human cognitive principles. Nemori's core\ninnovation is twofold: First, its Two-Step Alignment Principle, inspired by\nEvent Segmentation Theory, provides a principled, top-down method for\nautonomously organizing the raw conversational stream into semantically\ncoherent episodes, solving the critical issue of memory granularity. Second,\nits Predict-Calibrate Principle, inspired by the Free-energy Principle, enables\nthe agent to proactively learn from prediction gaps, moving beyond pre-defined\nheuristics to achieve adaptive knowledge evolution. This offers a viable path\ntoward handling the long-term, dynamic workflows of autonomous agents.\nExtensive experiments on the LoCoMo and LongMemEval benchmarks demonstrate that\nNemori significantly outperforms prior state-of-the-art systems, with its\nadvantage being particularly pronounced in longer contexts.", "AI": {"tldr": "Nemori\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u7ec4\u7ec7\u8bb0\u5fc6\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u8bb0\u5fc6\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u6b65\u5bf9\u9f50\u539f\u5219\u548c\u9884\u6d4b\u6821\u51c6\u539f\u5219\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u5728\u5b9a\u4e49\u57fa\u672c\u8bb0\u5fc6\u5355\u5143\u548c\u77e5\u8bc6\u63d0\u53d6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u5b66\u4e60\u548c\u8fdb\u5316\u3002", "method": "Nemori\u91c7\u7528\u4e24\u6b65\u5bf9\u9f50\u539f\u5219\u7ec4\u7ec7\u5bf9\u8bdd\u6d41\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u9884\u6d4b\u6821\u51c6\u539f\u5219\u4e3b\u52a8\u5b66\u4e60\u9884\u6d4b\u5dee\u8ddd\u3002", "result": "\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNemori\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\uff0c\u5c24\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Nemori\u4e3a\u81ea\u4e3b\u4ee3\u7406\u5904\u7406\u957f\u671f\u52a8\u6001\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2508.03345", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03345", "abs": "https://arxiv.org/abs/2508.03345", "authors": ["Xingdan Wang", "Jiayi He", "Zhiqing Tang", "Jianxiong Guo", "Jiong Lou", "Liping Qian", "Tian Wang", "Weijia Jia"], "title": "Adaptive AI Agent Placement and Migration in Edge Intelligence Systems", "comment": null, "summary": "The rise of LLMs such as ChatGPT and Claude fuels the need for AI agents\ncapable of real-time task handling. However, migrating data-intensive,\nmulti-modal edge workloads to cloud data centers, traditionally used for agent\ndeployment, introduces significant latency. Deploying AI agents at the edge\nimproves efficiency and reduces latency. However, edge environments present\nchallenges due to limited and heterogeneous resources. Maintaining QoS for\nmobile users necessitates agent migration, which is complicated by the\ncomplexity of AI agents coordinating LLMs, task planning, memory, and external\ntools. This paper presents the first systematic deployment and management\nsolution for LLM-based AI agents in dynamic edge environments. We propose a\nnovel adaptive framework for AI agent placement and migration in edge\nintelligence systems. Our approach models resource constraints and\nlatency/cost, leveraging ant colony algorithms and LLM-based optimization for\nefficient decision-making. It autonomously places agents to optimize resource\nutilization and QoS and enables lightweight agent migration by transferring\nonly essential state. Implemented on a distributed system using AgentScope and\nvalidated across globally distributed edge servers, our solution significantly\nreduces deployment latency and migration costs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u4e2d\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u7684\u7cfb\u7edf\u5316\u90e8\u7f72\u548c\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6846\u67b6\u4f18\u5316\u8d44\u6e90\u5229\u7528\u548cQoS\u3002", "motivation": "\u968f\u7740LLM\uff08\u5982ChatGPT\u548cClaude\uff09\u7684\u5174\u8d77\uff0c\u9700\u8981\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u4efb\u52a1\u7684AI\u4ee3\u7406\u3002\u7136\u800c\uff0c\u5c06\u6570\u636e\u5bc6\u96c6\u578b\u3001\u591a\u6a21\u6001\u7684\u8fb9\u7f18\u5de5\u4f5c\u8d1f\u8f7d\u8fc1\u79fb\u5230\u4e91\u6570\u636e\u4e2d\u5fc3\u4f1a\u5f15\u5165\u663e\u8457\u5ef6\u8fdf\u3002\u8fb9\u7f18\u90e8\u7f72\u867d\u80fd\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u8d44\u6e90\u6709\u9650\u4e14\u5f02\u6784\uff0c\u7ef4\u62a4\u79fb\u52a8\u7528\u6237\u7684QoS\u9700\u8981\u590d\u6742\u7684\u4ee3\u7406\u8fc1\u79fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8eAI\u4ee3\u7406\u5728\u8fb9\u7f18\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u653e\u7f6e\u548c\u8fc1\u79fb\u3002\u8be5\u65b9\u6cd5\u5efa\u6a21\u8d44\u6e90\u7ea6\u675f\u548c\u5ef6\u8fdf/\u6210\u672c\uff0c\u5229\u7528\u8681\u7fa4\u7b97\u6cd5\u548c\u57fa\u4e8eLLM\u7684\u4f18\u5316\u8fdb\u884c\u9ad8\u6548\u51b3\u7b56\u3002", "result": "\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5e76\u901a\u8fc7\u5168\u7403\u5206\u5e03\u7684\u8fb9\u7f18\u670d\u52a1\u5668\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6848\u663e\u8457\u964d\u4f4e\u4e86\u90e8\u7f72\u5ef6\u8fdf\u548c\u8fc1\u79fb\u6210\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u4e2d\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u90e8\u7f72\u548c\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u5316\u4e86\u8d44\u6e90\u5229\u7528\u548cQoS\u3002"}}
{"id": "2508.03346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03346", "abs": "https://arxiv.org/abs/2508.03346", "authors": ["Zeju Li", "Jianyuan Zhong", "Ziyang Zheng", "Xiangyu Wen", "Zhijian Xu", "Yingying Cheng", "Fan Zhang", "Qiang Xu"], "title": "Compressing Chain-of-Thought in LLMs via Step Entropy", "comment": null, "summary": "Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at\ncomplex reasoning but generate verbose thought processes with considerable\nredundancy, leading to increased inference costs and reduced efficiency. We\nintroduce a novel CoT compression framework based on step entropy, a metric\nthat quantifies the informational contribution of individual reasoning steps to\nidentify redundancy. Through theoretical analysis and extensive empirical\nvalidation on mathematical reasoning benchmarks, we demonstrate that steps with\nlow entropy are indeed highly redundant. Our experiments reveal that an\nastonishing 80\\% of low-entropy intermediate steps can be pruned with minor\ndegradation in the final answer accuracy across DeepSeek-R1-7B, 14B and\nQwen3-8B. This finding sharply contrasts with random or high-entropy pruning,\nwhich severely impairs reasoning performance. Building on this, we propose a\nnovel two-stage training strategy combining Supervised Fine-Tuning (SFT) and\nGroup Relative Policy Optimization (GRPO) reinforcement learning. This approach\nenables LLMs to autonomously learn to generate compressed COTs during inference\nby strategically incorporating [SKIP] tokens. Our method significantly enhances\nLLM inference efficiency while rigorously preserving accuracy, offering\nprofound implications for practical LLM deployment and a deeper understanding\nof reasoning structures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b65\u9aa4\u71b5\u7684CoT\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u5197\u4f59\u6b65\u9aa4\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\u3002", "motivation": "LLMs\u4f7f\u7528CoT\u63d0\u793a\u65f6\u751f\u6210\u7684\u5197\u957f\u63a8\u7406\u8fc7\u7a0b\u5b58\u5728\u5197\u4f59\uff0c\u589e\u52a0\u4e86\u63a8\u7406\u6210\u672c\u5e76\u964d\u4f4e\u4e86\u6548\u7387\u3002", "method": "\u5f15\u5165\u6b65\u9aa4\u71b5\u91cf\u5316\u63a8\u7406\u6b65\u9aa4\u7684\u4fe1\u606f\u8d21\u732e\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08SFT\u548cGRPO\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u7ed3\u5408[SKIP]\u6807\u8bb0\u751f\u6210\u538b\u7f29CoT\u3002", "result": "\u5b9e\u9a8c\u8868\u660e80%\u7684\u4f4e\u71b5\u6b65\u9aa4\u53ef\u88ab\u4fee\u526a\u4e14\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6df1\u5316\u4e86\u5bf9\u63a8\u7406\u7ed3\u6784\u7684\u7406\u89e3\u3002"}}
{"id": "2508.03360", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03360", "abs": "https://arxiv.org/abs/2508.03360", "authors": ["Feng Rui", "Zhiyao Luo", "Wei Wang", "Yuting Song", "Yong Liu", "Tingting Zhu", "Jianqing Li", "Xingyao Wang"], "title": "CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment", "comment": "19 pages, 9 figures, 12 tables", "summary": "Automatic assessment of cognitive impairment from spontaneous speech offers a\npromising, non-invasive avenue for early cognitive screening. However, current\napproaches often lack generalizability when deployed across different languages\nand clinical settings, limiting their practical utility. In this study, we\npropose CogBench, the first benchmark designed to evaluate the cross-lingual\nand cross-site generalizability of large language models (LLMs) for\nspeech-based cognitive impairment assessment. Using a unified multimodal\npipeline, we evaluate model performance on three speech datasets spanning\nEnglish and Mandarin: ADReSSo, NCMMSC2021-AD, and a newly collected test set,\nCIR-E. Our results show that conventional deep learning models degrade\nsubstantially when transferred across domains. In contrast, LLMs equipped with\nchain-of-thought prompting demonstrate better adaptability, though their\nperformance remains sensitive to prompt design. Furthermore, we explore\nlightweight fine-tuning of LLMs via Low-Rank Adaptation (LoRA), which\nsignificantly improves generalization in target domains. These findings offer a\ncritical step toward building clinically useful and linguistically robust\nspeech-based cognitive assessment tools.", "AI": {"tldr": "CogBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u591a\u4e34\u5e8a\u73af\u5883\u4e0b\u8ba4\u77e5\u969c\u788d\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u7ed3\u679c\u663e\u793a\u4f20\u7edf\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u800c\u91c7\u7528\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u7684LLMs\u8868\u73b0\u66f4\u597d\uff0cLoRA\u5fae\u8c03\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8ba4\u77e5\u969c\u788d\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u548c\u4e34\u5e8a\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faCogBench\u57fa\u51c6\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6d41\u7a0b\u8bc4\u4f30LLMs\u5728\u82f1\u8bed\u548c\u666e\u901a\u8bdd\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5305\u62ecADReSSo\u3001NCMMSC2021-AD\u548cCIR-E\u3002", "result": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8de8\u57df\u8868\u73b0\u5dee\uff0cLLMs\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u8868\u73b0\u66f4\u597d\uff0cLoRA\u5fae\u8c03\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6784\u5efa\u4e34\u5e8a\u5b9e\u7528\u4e14\u8bed\u8a00\u9c81\u68d2\u7684\u8ba4\u77e5\u8bc4\u4f30\u5de5\u5177\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.03366", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2508.03366", "abs": "https://arxiv.org/abs/2508.03366", "authors": ["Michael K. Chen"], "title": "A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning", "comment": "Accepted to NeSy 2025", "summary": "General logical reasoning, defined as the ability to reason deductively on\ndomain-agnostic tasks, continues to be a challenge for large language models\n(LLMs). Current LLMs fail to reason deterministically and are not\ninterpretable. As such, there has been a recent surge in interest in\nneurosymbolic AI, which attempts to incorporate logic into neural networks. We\nfirst identify two main neurosymbolic approaches to improving logical\nreasoning: (i) the integrative approach comprising models where symbolic\nreasoning is contained within the neural network, and (ii) the hybrid approach\ncomprising models where a symbolic solver, separate from the neural network,\nperforms symbolic reasoning. Both contain AI systems with promising results on\ndomain-specific logical reasoning benchmarks. However, their performance on\ndomain-agnostic benchmarks is understudied. To the best of our knowledge, there\nhas not been a comparison of the contrasting approaches that answers the\nfollowing question: Which approach is more promising for developing general\nlogical reasoning? To analyze their potential, the following best-in-class\ndomain-agnostic models are introduced: Logic Neural Network (LNN), which uses\nthe integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the\nhybrid approach. Using both models as case studies and representatives of each\napproach, our analysis demonstrates that the hybrid approach is more promising\nfor developing general logical reasoning because (i) its reasoning chain is\nmore interpretable, and (ii) it retains the capabilities and advantages of\nexisting LLMs. To support future works using the hybrid approach, we propose a\ngeneralizable framework based on LLM-SS that is modular by design,\nmodel-agnostic, domain-agnostic, and requires little to no human input.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u795e\u7ecf\u7b26\u53f7AI\u5728\u901a\u7528\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u4e24\u79cd\u4e3b\u8981\u65b9\u6cd5\uff08\u96c6\u6210\u4e0e\u6df7\u5408\uff09\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u6df7\u5408\u65b9\u6cd5\u66f4\u5177\u524d\u666f\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u901a\u7528\u903b\u8f91\u63a8\u7406\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u786e\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u6b64\u7814\u7a76\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\u4ee5\u6539\u8fdb\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u4ee3\u8868\u6027\u6a21\u578b\uff08LNN\u548cLLM-SS\uff09\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u96c6\u6210\u4e0e\u6df7\u5408\u65b9\u6cd5\u5728\u901a\u7528\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6df7\u5408\u65b9\u6cd5\uff08LLM-SS\uff09\u5728\u901a\u7528\u903b\u8f91\u63a8\u7406\u4e2d\u66f4\u5177\u6f5c\u529b\uff0c\u56e0\u5176\u63a8\u7406\u94fe\u66f4\u53ef\u89e3\u91ca\u4e14\u4fdd\u7559\u4e86LLM\u7684\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM-SS\u7684\u901a\u7528\u6846\u67b6\uff0c\u652f\u6301\u672a\u6765\u6df7\u5408\u65b9\u6cd5\u7684\u7814\u7a76\uff0c\u5177\u6709\u6a21\u5757\u5316\u3001\u6a21\u578b\u65e0\u5173\u548c\u9886\u57df\u65e0\u5173\u7684\u7279\u70b9\u3002"}}
{"id": "2508.03368", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.03368", "abs": "https://arxiv.org/abs/2508.03368", "authors": ["Lucia Cipolina-Kun", "Marianna Nezhurina", "Jenia Jitsev"], "title": "Board Game Arena: A Framework and Benchmark for Assessing Large Language Models via Strategic Play", "comment": null, "summary": "The Board Game Arena library provides a framework for evaluating the decision\nmaking abilities of large language models (LLMs) through strategic board games\nimplemented in Google OpenSpiel library. The framework enables systematic\ncomparisons between LLM based agents and other agents (random, human,\nreinforcement learning agents, etc.) in various game scenarios by wrapping\nmultiple board and matrix games and supporting different agent types. It\nintegrates API access to models via LiteLLM, local model deployment via vLLM,\nand offers distributed execution through Ray. Additionally it provides\nextensive analysis tools for the LLM reasoning traces. This paper summarizes\nthe structure, key characteristics, and motivation of the repository,\nhighlighting how it contributes to the empirical evaluation of the reasoning of\nLLM and game-theoretic behavior", "AI": {"tldr": "Board Game Arena\u5e93\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u51b3\u7b56\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u6218\u7565\u68cb\u76d8\u6e38\u620f\u5b9e\u73b0\u7cfb\u7edf\u6bd4\u8f83\u3002", "motivation": "\u901a\u8fc7\u6e38\u620f\u573a\u666f\u8bc4\u4f30LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u535a\u5f08\u884c\u4e3a\u3002", "method": "\u96c6\u6210Google OpenSpiel\u5e93\uff0c\u652f\u6301\u591a\u79cd\u6e38\u620f\u548c\u4ee3\u7406\u7c7b\u578b\uff0c\u63d0\u4f9bAPI\u8bbf\u95ee\u548c\u5206\u5e03\u5f0f\u6267\u884c\u3002", "result": "\u63d0\u4f9b\u5168\u9762\u7684\u5206\u6790\u5de5\u5177\uff0c\u652f\u6301\u5bf9LLM\u63a8\u7406\u8f68\u8ff9\u7684\u6df1\u5165\u7814\u7a76\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u7684\u5b9e\u8bc1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2508.03379", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03379", "abs": "https://arxiv.org/abs/2508.03379", "authors": ["Wenxin Mao", "Zhitao Wang Long Wang", "Sirong Chen", "Cuiyun Gao", "Luyang Cao", "Ziming Liu", "Qiming Zhang", "Jun Zhou", "Zhi Jin"], "title": "Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams", "comment": null, "summary": "Large language models (LLMs) excel at generating code from natural language\n(NL) descriptions. However, the plain textual descriptions are inherently\nambiguous and often fail to capture complex requirements like intricate system\nbehaviors, conditional logic, and architectural constraints; implicit data\ndependencies in service-oriented architectures are difficult to infer and\nhandle correctly. To bridge this gap, we propose a novel step-by-step code\ngeneration framework named UML2Dep by leveraging unambiguous formal\nspecifications of complex requirements. First, we introduce an enhanced Unified\nModeling Language (UML) sequence diagram tailored for service-oriented\narchitectures. This diagram extends traditional visual syntax by integrating\ndecision tables and API specifications, explicitly formalizing structural\nrelationships and business logic flows in service interactions to rigorously\neliminate linguistic ambiguity. Second, recognizing the critical role of data\nflow, we introduce a dedicated data dependency inference (DDI) task. DDI\nsystematically constructs an explicit data dependency graph prior to actual\ncode synthesis. To ensure reliability, we formalize DDI as a constrained\nmathematical reasoning task through novel prompting strategies, aligning with\nLLMs' excellent mathematical strengths. Additional static parsing and\ndependency pruning further reduce context complexity and cognitive load\nassociated with intricate specifications, thereby enhancing reasoning accuracy\nand efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUML2Dep\u7684\u9010\u6b65\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u89c4\u8303\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u6a21\u7cca\u6027\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5728\u590d\u6742\u9700\u6c42\uff08\u5982\u7cfb\u7edf\u884c\u4e3a\u3001\u6761\u4ef6\u903b\u8f91\u548c\u67b6\u6784\u7ea6\u675f\uff09\u4e2d\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u96be\u4ee5\u6355\u6349\u9690\u5f0f\u6570\u636e\u4f9d\u8d56\u3002", "method": "1. \u5f15\u5165\u589e\u5f3a\u7684UML\u5e8f\u5217\u56fe\uff0c\u7ed3\u5408\u51b3\u7b56\u8868\u548cAPI\u89c4\u8303\uff1b2. \u63d0\u51fa\u6570\u636e\u4f9d\u8d56\u63a8\u65ad\uff08DDI\uff09\u4efb\u52a1\uff0c\u6784\u5efa\u663e\u5f0f\u6570\u636e\u4f9d\u8d56\u56fe\u3002", "result": "\u901a\u8fc7\u5f62\u5f0f\u5316\u89c4\u8303\u548cDDI\u4efb\u52a1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u7cca\u6027\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "UML2Dep\u6846\u67b6\u901a\u8fc7\u5f62\u5f0f\u5316\u89c4\u8303\u548c\u6570\u5b66\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u9700\u6c42\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u95ee\u9898\u3002"}}
{"id": "2508.03396", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03396", "abs": "https://arxiv.org/abs/2508.03396", "authors": ["Rui Zou", "Mengqi Wei", "Yutao Zhu", "Jirong Wen", "Xin Zhao", "Jing Chen"], "title": "Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis", "comment": null, "summary": "Large Language Models (LLMs) excel in reasoning and generation across\ndomains, but still struggle with identifying and diagnosing complex errors.\nThis stems mainly from training objectives that prioritize correct answers,\nlimiting exposure to and learning from errors. While recent studies have begun\nto address this by introducing error signals, most rely on shallow, static\nerrors, restricting improvement in deep diagnostic ability. To overcome this,\nwe propose Hide and Seek Game (HSG), a dynamic adversarial framework for error\ngeneration and diagnosis, and evaluate it on mathematical problem-solving. HSG\ninvolves two adversarial roles: Sneaky, which \"hides\" by generating subtle,\ndeceptive reasoning errors, and Diagnosis, which \"seeks\" to accurately detect\nthem. Through adversarial co-evolution, both error stealth and diagnostic\nprecision are enhanced. Experiments on several math reasoning tasks show that\nHSG significantly boosts error diagnosis, achieving 16.8\\%--31.4\\% higher\naccuracy than baselines like GPT-4o. We also release a challenging dataset of\ndeceptive errors and diagnostic annotations as a benchmark for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5bf9\u6297\u6846\u67b6HSG\uff0c\u901a\u8fc7\u751f\u6210\u548c\u8bca\u65ad\u590d\u6742\u9519\u8bef\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9519\u8bef\u8bca\u65ad\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8bc6\u522b\u548c\u8bca\u65ad\u590d\u6742\u9519\u8bef\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u76ee\u6807\u8fc7\u4e8e\u5173\u6ce8\u6b63\u786e\u7b54\u6848\uff0c\u7f3a\u4e4f\u5bf9\u9519\u8bef\u7684\u5b66\u4e60\u3002", "method": "\u63d0\u51faHSG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5bf9\u6297\u89d2\u8272\uff1aSneaky\uff08\u751f\u6210\u9690\u853d\u7684\u9519\u8bef\uff09\u548cDiagnosis\uff08\u68c0\u6d4b\u9519\u8bef\uff09\uff0c\u901a\u8fc7\u5bf9\u6297\u534f\u540c\u8fdb\u5316\u63d0\u5347\u80fd\u529b\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cHSG\u663e\u8457\u63d0\u5347\u4e86\u9519\u8bef\u8bca\u65ad\u80fd\u529b\uff0c\u51c6\u786e\u7387\u6bd4GPT-4o\u7b49\u57fa\u7ebf\u6a21\u578b\u9ad816.8%--31.4%\u3002", "conclusion": "HSG\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9519\u8bef\u8bca\u65ad\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u57fa\u51c6\u3002"}}
{"id": "2508.03406", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03406", "abs": "https://arxiv.org/abs/2508.03406", "authors": ["Kai Li", "Ruihao Zheng", "Xinye Hao", "Zhenkun Wang"], "title": "Multi-Objective Infeasibility Diagnosis for Routing Problems Using Large Language Models", "comment": null, "summary": "In real-world routing problems, users often propose conflicting or\nunreasonable requirements, which result in infeasible optimization models due\nto overly restrictive or contradictory constraints, leading to an empty\nfeasible solution set. Existing Large Language Model (LLM)-based methods\nattempt to diagnose infeasible models, but modifying such models often involves\nmultiple potential adjustments that these methods do not consider. To fill this\ngap, we introduce Multi-Objective Infeasibility Diagnosis (MOID), which\ncombines LLM agents and multi-objective optimization within an automatic\nrouting solver, to provide a set of representative actionable suggestions.\nSpecifically, MOID employs multi-objective optimization to consider both path\ncost and constraint violation, generating a set of trade-off solutions, each\nencompassing varying degrees of model adjustments. To extract practical\ninsights from these solutions, MOID utilizes LLM agents to generate a solution\nanalysis function for the infeasible model. This function analyzes these\ndistinct solutions to diagnose the original infeasible model, providing users\nwith diverse diagnostic insights and suggestions. Finally, we compare MOID with\nseveral LLM-based methods on 50 types of infeasible routing problems. The\nresults indicate that MOID automatically generates multiple diagnostic\nsuggestions in a single run, providing more practical insights for restoring\nmodel feasibility and decision-making compared to existing methods.", "AI": {"tldr": "MOID\u7ed3\u5408LLM\u4ee3\u7406\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u4e3a\u4e0d\u53ef\u884c\u8def\u7531\u95ee\u9898\u63d0\u4f9b\u591a\u79cd\u53ef\u64cd\u4f5c\u5efa\u8bae\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u65b9\u6cd5\u5728\u8bca\u65ad\u4e0d\u53ef\u884c\u6a21\u578b\u65f6\u672a\u8003\u8651\u591a\u8c03\u6574\u65b9\u6848\u7684\u95ee\u9898\u3002", "method": "MOID\u7ed3\u5408\u591a\u76ee\u6807\u4f18\u5316\u548cLLM\u4ee3\u7406\uff0c\u751f\u6210\u6743\u8861\u89e3\u5e76\u5206\u6790\u3002", "result": "\u572850\u7c7b\u4e0d\u53ef\u884c\u8def\u7531\u95ee\u9898\u4e0a\uff0cMOID\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u66f4\u591a\u5b9e\u7528\u5efa\u8bae\u3002", "conclusion": "MOID\u80fd\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u8bca\u65ad\u5efa\u8bae\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u884c\u6027\u548c\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2508.03438", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03438", "abs": "https://arxiv.org/abs/2508.03438", "authors": ["Taine J. Elliott", "Stephen P. Levitt", "Ken Nixon", "Martin Bekker"], "title": "Data Overdose? Time for a Quadruple Shot: Knowledge Graph Construction using Enhanced Triple Extraction", "comment": "18 pages, 8 figures, Published in the Annual Conference of South\n  African Institute of Computer Scientists and Information Technologists,\n  Preprint (author original)", "summary": "The rapid expansion of publicly-available medical data presents a challenge\nfor clinicians and researchers alike, increasing the gap between the volume of\nscientific literature and its applications. The steady growth of studies and\nfindings overwhelms medical professionals at large, hindering their ability to\nsystematically review and understand the latest knowledge. This paper presents\nan approach to information extraction and automatic knowledge graph (KG)\ngeneration to identify and connect biomedical knowledge. Through a pipeline of\nlarge language model (LLM) agents, the system decomposes 44 PubMed abstracts\ninto semantically meaningful proposition sentences and extracts KG triples from\nthese sentences. The triples are enhanced using a combination of open domain\nand ontology-based information extraction methodologies to incorporate\nontological categories. On top of this, a context variable is included during\nextraction to allow the triple to stand on its own - thereby becoming\n`quadruples'. The extraction accuracy of the LLM is validated by comparing\nnatural language sentences generated from the enhanced triples to the original\npropositions, achieving an average cosine similarity of 0.874. The similarity\nfor generated sentences of enhanced triples were compared with generated\nsentences of ordinary triples showing an increase as a result of the context\nvariable. Furthermore, this research explores the ability for LLMs to infer new\nrelationships and connect clusters in the knowledge base of the knowledge\ngraph. This approach leads the way to provide medical practitioners with a\ncentralised, updated in real-time, and sustainable knowledge source, and may be\nthe foundation of similar gains in a wide variety of fields.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4fe1\u606f\u63d0\u53d6\u548c\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u63a5\u751f\u7269\u533b\u5b66\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u53d8\u91cf\u589e\u5f3a\u63d0\u53d6\u7684\u4e09\u5143\u7ec4\u4e3a\u56db\u5143\u7ec4\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u6570\u636e\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u4e34\u5e8a\u533b\u751f\u548c\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u7cfb\u7edf\u5316\u7406\u89e3\u548c\u5e94\u7528\u6700\u65b0\u77e5\u8bc6\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528LLM\u4ee3\u7406\u7ba1\u9053\u5206\u89e3PubMed\u6458\u8981\u4e3a\u8bed\u4e49\u547d\u9898\u53e5\u5b50\uff0c\u63d0\u53d6KG\u4e09\u5143\u7ec4\uff0c\u5e76\u901a\u8fc7\u5f00\u653e\u9886\u57df\u548c\u57fa\u4e8e\u672c\u4f53\u7684\u65b9\u6cd5\u589e\u5f3a\u4e3a\u56db\u5143\u7ec4\u3002", "result": "\u63d0\u53d6\u7684\u4e09\u5143\u7ec4\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u53e5\u5b50\u4e0e\u539f\u547d\u9898\u7684\u5e73\u5747\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e3a0.874\uff0c\u4e0a\u4e0b\u6587\u53d8\u91cf\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u76f8\u4f3c\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u5b66\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9e\u65f6\u66f4\u65b0\u7684\u96c6\u4e2d\u77e5\u8bc6\u6e90\uff0c\u5e76\u53ef\u80fd\u5728\u5176\u4ed6\u9886\u57df\u5b9e\u73b0\u7c7b\u4f3c\u6548\u679c\u3002"}}
{"id": "2508.03465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03465", "abs": "https://arxiv.org/abs/2508.03465", "authors": ["Saleh Nikooroo"], "title": "Toward a Graph-Theoretic Model of Belief: Confidence, Credibility, and Structural Coherence", "comment": null, "summary": "Belief systems are often treated as globally consistent sets of propositions\nor as scalar-valued probability distributions. Such representations tend to\nobscure the internal structure of belief, conflate external credibility with\ninternal coherence, and preclude the modeling of fragmented or contradictory\nepistemic states. This paper introduces a minimal formalism for belief systems\nas directed, weighted graphs. In this framework, nodes represent individual\nbeliefs, edges encode epistemic relationships (e.g., support or contradiction),\nand two distinct functions assign each belief a credibility (reflecting source\ntrust) and a confidence (derived from internal structural support). Unlike\nclassical probabilistic models, our approach does not assume prior coherence or\nrequire belief updating. Unlike logical and argumentation-based frameworks, it\nsupports fine-grained structural representation without committing to binary\njustification status or deductive closure. The model is purely static and\ndeliberately excludes inference or revision procedures. Its aim is to provide a\nfoundational substrate for analyzing the internal organization of belief\nsystems, including coherence conditions, epistemic tensions, and\nrepresentational limits. By distinguishing belief structure from belief\nstrength, this formalism enables a richer classification of epistemic states\nthan existing probabilistic, logical, or argumentation-based approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6709\u5411\u52a0\u6743\u56fe\u7684\u4fe1\u5ff5\u7cfb\u7edf\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u533a\u5206\u4e86\u4fe1\u5ff5\u7684\u53ef\u4fe1\u5ea6\u548c\u4fe1\u5fc3\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6982\u7387\u6216\u903b\u8f91\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u4fe1\u5ff5\u7cfb\u7edf\u8868\u793a\u65b9\u6cd5\uff08\u5982\u6982\u7387\u5206\u5e03\u6216\u903b\u8f91\u547d\u9898\uff09\u5ffd\u89c6\u4e86\u5185\u90e8\u7ed3\u6784\uff0c\u65e0\u6cd5\u5904\u7406\u77db\u76fe\u6216\u788e\u7247\u5316\u7684\u8ba4\u77e5\u72b6\u6001\u3002", "method": "\u4f7f\u7528\u6709\u5411\u52a0\u6743\u56fe\u8868\u793a\u4fe1\u5ff5\u7cfb\u7edf\uff0c\u8282\u70b9\u4e3a\u4fe1\u5ff5\uff0c\u8fb9\u4e3a\u8ba4\u77e5\u5173\u7cfb\uff0c\u5e76\u5206\u522b\u5b9a\u4e49\u53ef\u4fe1\u5ea6\u548c\u4fe1\u5fc3\u51fd\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u7cbe\u7ec6\u5730\u8868\u793a\u4fe1\u5ff5\u7ed3\u6784\uff0c\u652f\u6301\u5bf9\u8ba4\u77e5\u72b6\u6001\u7684\u5206\u7c7b\u548c\u5206\u6790\u3002", "conclusion": "\u8be5\u5f62\u5f0f\u5316\u4e3a\u4fe1\u5ff5\u7cfb\u7edf\u7684\u5185\u90e8\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.03484", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03484", "abs": "https://arxiv.org/abs/2508.03484", "authors": ["Zhiyao Xu", "Dan Zhao", "Qingsong Zou", "Qing Li", "Yong Jiang", "Yuhang Wang", "Jingyu Xiao"], "title": "Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes", "comment": null, "summary": "As smart homes become increasingly prevalent, intelligent models are widely\nused for tasks such as anomaly detection and behavior prediction. These models\nare typically trained on static datasets, making them brittle to behavioral\ndrift caused by seasonal changes, lifestyle shifts, or evolving routines.\nHowever, collecting new behavior data for retraining is often impractical due\nto its slow pace, high cost, and privacy concerns. In this paper, we propose\nSmartGen, an LLM-based framework that synthesizes context-aware user behavior\ndata to support continual adaptation of downstream smart home models. SmartGen\nconsists of four key components. First, we design a Time and Semantic-aware\nSplit module to divide long behavior sequences into manageable, semantically\ncoherent subsequences under dual time-span constraints. Second, we propose\nSemantic-aware Sequence Compression to reduce input length while preserving\nrepresentative semantics by clustering behavior mapping in latent space. Third,\nwe introduce Graph-guided Sequence Synthesis, which constructs a behavior\nrelationship graph and encodes frequent transitions into prompts, guiding the\nLLM to generate data aligned with contextual changes while retaining core\nbehavior patterns. Finally, we design a Two-stage Outlier Filter to identify\nand remove implausible or semantically inconsistent outputs, aiming to improve\nthe factual coherence and behavioral validity of the generated sequences.\nExperiments on three real-world datasets demonstrate that SmartGen\nsignificantly enhances model performance on anomaly detection and behavior\nprediction tasks under behavioral drift, with anomaly detection improving by\n85.43% and behavior prediction by 70.51% on average. The code is available at\nhttps://github.com/horizonsinzqs/SmartGen.", "AI": {"tldr": "SmartGen\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7528\u6237\u884c\u4e3a\u6570\u636e\uff0c\u4ee5\u652f\u6301\u667a\u80fd\u5bb6\u5c45\u6a21\u578b\u7684\u6301\u7eed\u9002\u5e94\u3002", "motivation": "\u667a\u80fd\u5bb6\u5c45\u6a21\u578b\u901a\u5e38\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u96be\u4ee5\u5e94\u5bf9\u884c\u4e3a\u6f02\u79fb\uff08\u5982\u5b63\u8282\u53d8\u5316\u3001\u751f\u6d3b\u65b9\u5f0f\u6539\u53d8\u7b49\uff09\uff0c\u800c\u6536\u96c6\u65b0\u6570\u636e\u53c8\u5b58\u5728\u6210\u672c\u9ad8\u3001\u9690\u79c1\u95ee\u9898\u7b49\u6311\u6218\u3002", "method": "SmartGen\u5305\u542b\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u65f6\u95f4\u548c\u8bed\u4e49\u611f\u77e5\u5206\u5272\u3001\u8bed\u4e49\u611f\u77e5\u5e8f\u5217\u538b\u7f29\u3001\u56fe\u5f15\u5bfc\u5e8f\u5217\u5408\u6210\u548c\u4e24\u9636\u6bb5\u5f02\u5e38\u8fc7\u6ee4\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSmartGen\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u884c\u4e3a\u6f02\u79fb\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u884c\u4e3a\u9884\u6d4b\u6027\u80fd\uff0c\u5f02\u5e38\u68c0\u6d4b\u5e73\u5747\u63d0\u534785.43%\uff0c\u884c\u4e3a\u9884\u6d4b\u63d0\u534770.51%\u3002", "conclusion": "SmartGen\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u884c\u4e3a\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u5bb6\u5c45\u6a21\u578b\u7684\u884c\u4e3a\u6f02\u79fb\u95ee\u9898\u3002"}}
{"id": "2508.03488", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03488", "abs": "https://arxiv.org/abs/2508.03488", "authors": ["Khaled Bachir Delassi", "Lakhdar Zeggane", "Hadda Cherroun", "Abdelhamid Haouhat", "Kaoutar Bouzouad"], "title": "VQA support to Arabic Language Learning Educational Tool", "comment": null, "summary": "We address the problem of scarcity of educational Arabic Language Learning\ntools that advocate modern pedagogical models such as active learning which\nensures language proficiency. In fact, we investigate the design and evaluation\nof an AI-powered educational tool designed to enhance Arabic language learning\nfor non-native speakers with beginner-to-intermediate proficiency level. The\ntool leverages advanced AI models to generate interactive visual quizzes,\ndeploying Visual Question Answering as the primary activity. Adopting a\nconstructivist learning approach, the system encourages active learning through\nreal-life visual quizzes, and image-based questions that focus on improving\nvocabulary, grammar, and comprehension. The system integrates Vision-Language\nPretraining models to generate contextually relevant image description from\nwhich Large Language Model generate assignments based on customized Arabic\nlanguage Learning quizzes thanks to prompting.\n  The effectiveness of the tool is evaluated through a manual annotated\nbenchmark consisting of 1266 real-life visual quizzes, with human participants\nproviding feedback. The results show a suitable accuracy rates, validating the\ntool's potential to bridge the gap in Arabic language education and\nhighlighting the tool's promise as a reliable, AI-powered resource for Arabic\nlearners, offering personalized and interactive learning experiences.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u5de5\u5177\u7684\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u6559\u80b2\u5de5\u5177\uff0c\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54\u6d3b\u52a8\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u5de5\u5177\u7a00\u7f3a\u95ee\u9898\uff0c\u5c24\u5176\u662f\u652f\u6301\u73b0\u4ee3\u6559\u5b66\u6cd5\uff08\u5982\u4e3b\u52a8\u5b66\u4e60\uff09\u7684\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cdAI\u9a71\u52a8\u7684\u6559\u80b2\u5de5\u5177\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u4ea4\u4e92\u5f0f\u89c6\u89c9\u6d4b\u9a8c\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u5b9a\u5236\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u4efb\u52a1\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u76841266\u4e2a\u89c6\u89c9\u6d4b\u9a8c\u8bc4\u4f30\uff0c\u5de5\u5177\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5de5\u5177\u4e3a\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u8005\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u4e2a\u6027\u5316\u7684\u4e92\u52a8\u5b66\u4e60\u4f53\u9a8c\uff0c\u586b\u8865\u4e86\u6559\u80b2\u8d44\u6e90\u7684\u7a7a\u767d\u3002"}}
{"id": "2508.03500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03500", "abs": "https://arxiv.org/abs/2508.03500", "authors": ["Yijin Yang", "Cristina Cornelio", "Mario Leiva", "Paulo Shakarian"], "title": "Error Detection and Correction for Interpretable Mathematics in Large Language Models", "comment": null, "summary": "Recent large language models (LLMs) have demonstrated the ability to perform\nexplicit multi-step reasoning such as chain-of-thought prompting. However,\ntheir intermediate steps often contain errors that can propagate leading to\ninaccurate final predictions. Additionally, LLMs still struggle with\nhallucinations and often fail to adhere to prescribed output formats, which is\nparticularly problematic for tasks like generating mathematical expressions or\nsource code. This work introduces EDCIM (Error Detection and Correction for\nInterpretable Mathematics), a method for detecting and correcting these errors\nin interpretable mathematics tasks, where the model must generate the exact\nfunctional form that explicitly solve the problem (expressed in natural\nlanguage) rather than a black-box solution. EDCIM uses LLMs to generate a\nsystem of equations for a given problem, followed by a symbolic error-detection\nframework that identifies errors and provides targeted feedback for LLM-based\ncorrection. To optimize efficiency, EDCIM integrates lightweight, open-source\nLLMs with more powerful proprietary models, balancing cost and accuracy. This\nbalance is controlled by a single hyperparameter, allowing users to control the\ntrade-off based on their cost and accuracy requirements. Experimental results\nacross different datasets show that EDCIM significantly reduces both\ncomputational and financial costs, while maintaining, and even improving,\nprediction accuracy when the balance is properly configured.", "AI": {"tldr": "EDCIM\u65b9\u6cd5\u901a\u8fc7\u68c0\u6d4b\u548c\u7ea0\u6b63LLMs\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\uff0c\u5e73\u8861\u6210\u672c\u4e0e\u51c6\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "LLMs\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u5e38\u4ea7\u751f\u9519\u8bef\uff0c\u4e14\u96be\u4ee5\u9075\u5faa\u7279\u5b9a\u8f93\u51fa\u683c\u5f0f\uff0c\u5bfc\u81f4\u6700\u7ec8\u9884\u6d4b\u4e0d\u51c6\u786e\u3002", "method": "EDCIM\u7ed3\u5408\u8f7b\u91cf\u7ea7\u548c\u5f3a\u5927LLMs\uff0c\u751f\u6210\u65b9\u7a0b\u7ec4\u5e76\u901a\u8fc7\u7b26\u53f7\u5316\u6846\u67b6\u68c0\u6d4b\u9519\u8bef\uff0c\u63d0\u4f9b\u53cd\u9988\u8fdb\u884c\u7ea0\u6b63\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEDCIM\u5728\u964d\u4f4e\u6210\u672c\u548c\u8ba1\u7b97\u8d1f\u62c5\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "EDCIM\u4e3aLLMs\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u7ea0\u6b63\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u914d\u7f6e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03616", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03616", "abs": "https://arxiv.org/abs/2508.03616", "authors": ["Jorge Gallego-Feliciano", "S. Aaron McClendon", "Juan Morinelli", "Stavros Zervoudakis", "Antonios Saravanos"], "title": "Hidden Dynamics of Massive Activations in Transformer Training", "comment": null, "summary": "Massive activations are scalar values in transformer hidden states that\nachieve values orders of magnitude larger than typical activations and have\nbeen shown to be critical for model functionality. While prior work has\ncharacterized these phenomena in fully trained models, the temporal dynamics of\ntheir emergence during training remain poorly understood. We present the first\ncomprehensive analysis of massive activation development throughout transformer\ntraining, using the Pythia model family as our testbed. Through systematic\nanalysis of various model sizes across multiple training checkpoints, we\ndemonstrate that massive activation emergence follows predictable mathematical\npatterns that can be accurately modeled using an exponentially-modulated\nlogarithmic function with five key parameters. We develop a machine learning\nframework to predict these mathematical parameters from architectural\nspecifications alone, achieving high accuracy for steady-state behavior and\nmoderate accuracy for emergence timing and magnitude. These findings enable\narchitects to predict and potentially control key aspects of massive activation\nemergence through design choices, with significant implications for model\nstability, training cycle length, interpretability, and optimization. Our\nfindings demonstrate that the emergence of massive activations is governed by\nmodel design and can be anticipated, and potentially controlled, before\ntraining begins.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86Transformer\u8bad\u7ec3\u4e2d\u5927\u89c4\u6a21\u6fc0\u6d3b\u503c\u7684\u52a8\u6001\u53d1\u5c55\u89c4\u5f8b\uff0c\u53d1\u73b0\u5176\u9075\u5faa\u53ef\u9884\u6d4b\u7684\u6570\u5b66\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u9884\u6d4b\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u5927\u89c4\u6a21\u6fc0\u6d3b\u503c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u53d1\u5c55\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528Pythia\u6a21\u578b\u5bb6\u65cf\uff0c\u5206\u6790\u4e0d\u540c\u89c4\u6a21\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\u7684\u5927\u89c4\u6a21\u6fc0\u6d3b\u503c\uff0c\u5efa\u7acb\u6570\u5b66\u6a21\u578b\u548c\u9884\u6d4b\u6846\u67b6\u3002", "result": "\u53d1\u73b0\u5927\u89c4\u6a21\u6fc0\u6d3b\u503c\u7684\u53d1\u5c55\u9075\u5faa\u6307\u6570\u8c03\u5236\u7684\u5bf9\u6570\u51fd\u6570\uff0c\u9884\u6d4b\u6846\u67b6\u80fd\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u7a33\u6001\u884c\u4e3a\u3002", "conclusion": "\u5927\u89c4\u6a21\u6fc0\u6d3b\u503c\u7684\u53d1\u5c55\u53d7\u6a21\u578b\u8bbe\u8ba1\u5f71\u54cd\uff0c\u53ef\u901a\u8fc7\u8bbe\u8ba1\u9009\u62e9\u9884\u6d4b\u548c\u63a7\u5236\uff0c\u5bf9\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u4f18\u5316\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.03622", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03622", "abs": "https://arxiv.org/abs/2508.03622", "authors": ["Jialin Li", "Jinzhe Li", "Gengxu Li", "Yi Chang", "Yuan Wu"], "title": "Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework", "comment": null, "summary": "With the advancement of code generation capabilities in large language models\n(LLMs), their reliance on input premises has intensified. When users provide\ninputs containing faulty premises, the probability of code generation\nhallucinations rises significantly, exposing deficiencies in their\nself-scrutiny capabilities. This paper proposes Faulty Premises Bench\n(FPBench), the first code generation evaluation framework targeting faulty\npremises. By systematically constructing three categories of faulty premises\nand integrating multi-dimensional evaluation metrics, it conducts in-depth\nassessments of 15 representative LLMs. The key findings are as follows: (1)\nMost models exhibit poor reasoning abilities and suboptimal code generation\nperformance under faulty premises, heavily relying on explicit prompts for\nerror detection, with limited self-scrutiny capabilities; (2) Faulty premises\ntrigger a point of diminishing returns in resource investment, leading to\nblindly increasing length fails to enhance quality; (3) The three types of\nfaulty premises respectively activate distinct defect patterns in models,\nrevealing a triple dissociation in the cognitive mechanisms of code generation\nmodels. This study not only highlights the urgent need for LLMs to proactively\nverify premises in code generation but also, through the proposed FPBench\nframework and multi-dimensional evaluation system, provides a theoretical\nfoundation and practical pathway for developing reliable, human-centric code\ngeneration models.", "AI": {"tldr": "FPBench\u662f\u9996\u4e2a\u9488\u5bf9\u4ee3\u7801\u751f\u6210\u4e2d\u9519\u8bef\u524d\u63d0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6784\u5efa\u4e09\u7c7b\u9519\u8bef\u524d\u63d0\u548c\u591a\u7ef4\u8bc4\u4f30\u6307\u6807\uff0c\u8bc4\u4f30\u4e8615\u79cd\u4ee3\u8868\u6027\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u9519\u8bef\u524d\u63d0\u4e0b\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u5176\u8ba4\u77e5\u673a\u5236\u7684\u7f3a\u9677\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5176\u5bf9\u8f93\u5165\u524d\u63d0\u7684\u4f9d\u8d56\u52a0\u5267\uff0c\u9519\u8bef\u524d\u63d0\u6613\u5bfc\u81f4\u4ee3\u7801\u751f\u6210\u5e7b\u89c9\uff0c\u66b4\u9732\u6a21\u578b\u81ea\u6211\u5ba1\u67e5\u80fd\u529b\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faFPBench\u6846\u67b6\uff0c\u7cfb\u7edf\u6784\u5efa\u4e09\u7c7b\u9519\u8bef\u524d\u63d0\uff0c\u7ed3\u5408\u591a\u7ef4\u8bc4\u4f30\u6307\u6807\uff0c\u5bf915\u79cd\u4ee3\u8868\u6027\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6df1\u5165\u8bc4\u4f30\u3002", "result": "\uff081\uff09\u591a\u6570\u6a21\u578b\u5728\u9519\u8bef\u524d\u63d0\u4e0b\u8868\u73b0\u5dee\uff0c\u4f9d\u8d56\u663e\u5f0f\u63d0\u793a\uff1b\uff082\uff09\u9519\u8bef\u524d\u63d0\u5bfc\u81f4\u8d44\u6e90\u6295\u5165\u8fb9\u9645\u6548\u76ca\u9012\u51cf\uff1b\uff083\uff09\u4e09\u7c7b\u9519\u8bef\u524d\u63d0\u5206\u522b\u6fc0\u6d3b\u6a21\u578b\u7684\u4e0d\u540c\u7f3a\u9677\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u9700\u4e3b\u52a8\u9a8c\u8bc1\u524d\u63d0\u7684\u91cd\u8981\u6027\uff0cFPBench\u6846\u67b6\u4e3a\u5f00\u53d1\u53ef\u9760\u3001\u4ee5\u4eba\u4e3a\u672c\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2508.03661", "categories": ["cs.AI", "astro-ph.HE", "astro-ph.IM", "gr-qc"], "pdf": "https://arxiv.org/pdf/2508.03661", "abs": "https://arxiv.org/abs/2508.03661", "authors": ["He Wang", "Liang Zeng"], "title": "Automated Algorithmic Discovery for Gravitational-Wave Detection Guided by LLM-Informed Evolutionary Monte Carlo Tree Search", "comment": "89 pages (37 main), 6+6 figures, 1 table. Initial submission; subject\n  to revision", "summary": "Computational scientific discovery increasingly relies on algorithms to\nprocess complex data and identify meaningful patterns - yet faces persistent\nchallenges in gravitational-wave signal identification. While existing\nalgorithmic approaches like matched filtering (MF) and deep neural networks\n(DNNs) have achieved partial success, their limitations directly stem from\nfundamental limitations: MF's excessive computational demands arise from its\nreliance on predefined theoretical waveform templates, while DNNs' black-box\narchitectures obscure decision logic and introduce hidden biases. We propose\nEvolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses\nthese limitations through systematic algorithm space exploration guided by\ndomain-aware physical constraints. Our approach combines tree-structured search\nwith evolutionary optimization and large language model heuristics to create\ninterpretable algorithmic solutions. Our Evo-MCTS framework demonstrates\nsubstantial improvements, achieving a 20.2\\% improvement over state-of-the-art\ngravitational wave detection algorithms on the MLGWSC-1 benchmark dataset.\nHigh-performing algorithm variants consistently exceed thresholds. The\nframework generates human-interpretable algorithmic pathways that reveal\ndistinct performance patterns. Beyond performance improvements, our framework\ndiscovers novel algorithmic combinations, thereby establishing a transferable\nmethodology for automated algorithmic discovery across computational science\ndomains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEvo-MCTS\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u8fdb\u5316\u4f18\u5316\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u7528\u4e8e\u6539\u8fdb\u5f15\u529b\u6ce2\u4fe1\u53f7\u8bc6\u522b\uff0c\u6027\u80fd\u63d0\u534720.2%\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\uff08\u5982\u5339\u914d\u6ee4\u6ce2\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u5f15\u529b\u6ce2\u4fe1\u53f7\u8bc6\u522b\u4e2d\u5b58\u5728\u8ba1\u7b97\u91cf\u5927\u548c\u9ed1\u76d2\u51b3\u7b56\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6811\u72b6\u641c\u7d22\u3001\u8fdb\u5316\u4f18\u5316\u548c\u5927\u8bed\u8a00\u6a21\u578b\u542f\u53d1\u5f0f\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728MLGWSC-1\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u534720.2%\uff0c\u5e76\u53d1\u73b0\u65b0\u7684\u7b97\u6cd5\u7ec4\u5408\u3002", "conclusion": "Evo-MCTS\u4e3a\u8ba1\u7b97\u79d1\u5b66\u9886\u57df\u7684\u81ea\u52a8\u5316\u7b97\u6cd5\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2508.03680", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03680", "abs": "https://arxiv.org/abs/2508.03680", "authors": ["Xufang Luo", "Yuge Zhang", "Zhiyuan He", "Zilong Wang", "Siyun Zhao", "Dongsheng Li", "Luna K. Qiu", "Yuqing Yang"], "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning", "comment": null, "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.", "AI": {"tldr": "Agent Lightning\u662f\u4e00\u4e2a\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u652f\u6301\u591a\u79cdAI\u4ee3\u7406\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u65e0\u9700\u4ee3\u7801\u4fee\u6539\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06RL\u8bad\u7ec3\u4e0e\u4ee3\u7406\u7d27\u5bc6\u8026\u5408\u6216\u4f9d\u8d56\u5e8f\u5217\u62fc\u63a5\u4e0e\u63a9\u7801\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u4ee3\u7406\u6267\u884c\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5b9a\u4e49\u7edf\u4e00\u6570\u636e\u63a5\u53e3\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u5c42RL\u7b97\u6cd5LightningRL\uff0c\u5305\u542b\u4fe1\u7528\u5206\u914d\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u5728\u6587\u672c\u5230SQL\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u6570\u5b66\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u7a33\u5b9a\u6301\u7eed\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4ee3\u7406\u8bad\u7ec3\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
