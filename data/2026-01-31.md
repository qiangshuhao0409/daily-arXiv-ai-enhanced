<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 107]
- [cs.IT](#cs.IT) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Spatiotemporal Continual Learning for Mobile Edge UAV Networks: Mitigating Catastrophic Forgetting](https://arxiv.org/abs/2601.21861)
*Chuan-Chi Lai*

Main category: cs.NI

TL;DR: 提出STCL框架和G-MAPPO算法，解决移动边缘UAV网络在动态时空环境中因任务场景切换导致的灾难性遗忘问题，实现弹性服务恢复和20%容量提升。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在移动边缘UAV网络中面临灾难性遗忘问题，当从密集城区切换到稀疏农村等不同任务场景时，需要昂贵的重新训练或模型重置，导致服务中断。

Method: 提出时空持续学习(STCL)框架，采用组解耦多智能体近端策略优化(G-MAPPO)算法，包含GDPO机制使用动态z-score归一化平衡异构目标，并利用UAV的3D移动性作为空间补偿层。

Result: STCL框架在阶段转换期间实现约0.95的服务可靠性弹性恢复，相比MADDPG基线不仅防止知识遗忘，在极端流量负载下还提供20%的有效容量增益。

Conclusion: 提出的STCL框架和G-MAPPO算法为边缘使能的空中集群提供了可扩展解决方案，能够有效应对动态时空环境中的概念漂移和极端密度波动。

Abstract: This paper addresses the critical challenge of coordinating mobile edge UAV networks to maintain robust service in highly dynamic spatiotemporal environments. Conventional Deep Reinforcement Learning (DRL) approaches often suffer from catastrophic forgetting when transitioning between distinct task scenarios, such as moving from dense urban clusters to sparse rural areas. These transitions typically necessitate computationally expensive retraining or model resets to adapt to new user distributions, leading to service interruptions. To overcome these limitations, we propose a computationally efficient Spatiotemporal Continual Learning (STCL) framework realized through a Group-Decoupled Multi-Agent Proximal Policy Optimization (G-MAPPO) algorithm. Our approach integrates a novel Group-Decoupled Policy Optimization (GDPO) mechanism that utilizes dynamic $z$-score normalization to autonomously balance heterogeneous objectives, including energy efficiency, user fairness, and coverage. This mechanism effectively mitigates gradient conflicts induced by concept drifts without requiring offline retraining. Furthermore, the framework leverages the 3D mobility of UAVs as a spatial compensation layer, enabling the swarm to autonomously adjust altitudes to accommodate extreme density fluctuations. Extensive simulations demonstrate that the proposed STCL framework achieves superior resilience, characterized by an elastic recovery of service reliability to approximately 0.95 during phase transitions. Compared to the MADDPG baseline, G-MAPPO not only prevents knowledge forgetting but also delivers an effective capacity gain of 20\% under extreme traffic loads, validating its potential as a scalable solution for edge-enabled aerial swarms.

</details>


### [2] [ViTMAlis: Towards Latency-Critical Mobile Video Analytics with Vision Transformers](https://arxiv.org/abs/2601.21362)
*Miao Zhang,Guanzhen Wu,Hao Fang,Yifei Zhu,Fangxin Wang,Ruixiao Zhang,Jiangchuan Liu*

Main category: cs.NI

TL;DR: ViTMAlis：针对ViT骨干密集预测模型的动态混合分辨率推理策略与设备-边缘协同卸载框架，显著降低端到端延迟并提升感知精度


<details>
  <summary>Details</summary>
Motivation: 移动视频分析应用正从CNN转向ViT以利用其全局上下文建模优势，但ViT在延迟关键场景下面临严重推理延迟问题，特别是在密集预测任务中，高分辨率输入加剧了ViT的二次计算复杂度

Method: 提出动态混合分辨率推理策略，为ViT骨干密集预测模型实现速度与精度的灵活权衡；在此基础上开发ViTMAlis框架，动态适应网络条件和视频内容，联合减少传输和推理延迟

Result: 在商用移动和边缘设备上实现完整原型，实验表明相比现有精度优先、内容感知和延迟自适应基线方法，ViTMAlis显著降低端到端卸载延迟，同时提升用户感知渲染精度

Conclusion: ViTMAlis为下一代移动智能提供了实用基础，解决了ViT在移动视频分析中的延迟瓶颈问题，实现了传输与推理延迟的联合优化

Abstract: Edge-assisted mobile video analytics (MVA) applications are increasingly shifting from using vision models based on convolutional neural networks (CNNs) to those built on vision transformers (ViTs) to leverage their superior global context modeling and generalization capabilities. However, deploying these advanced models in latency-critical MVA scenarios presents significant challenges. Unlike traditional CNN-based offloading paradigms where network transmission is the primary bottleneck, ViT-based systems are constrained by substantial inference delays, particularly for dense prediction tasks where the need for high-resolution inputs exacerbates the inherent quadratic computational complexity of ViTs. To address these challenges, we propose a dynamic mixed-resolution inference strategy tailored for ViT-backboned dense prediction models, enabling flexible runtime trade-offs between speed and accuracy. Building on this, we introduce ViTMAlis, a ViT-native device-to-edge offloading framework that dynamically adapts to network conditions and video content to jointly reduce transmission and inference delays. We implement a fully functional prototype of ViTMAlis on commodity mobile and edge devices. Extensive experiments demonstrate that, compared to state-of-the-art accuracy-centric, content-aware, and latency-adaptive baselines, ViTMAlis significantly reduces end-to-end offloading latency while improving user-perceived rendering accuracy, providing a practical foundation for next-generation mobile intelligence.

</details>


### [3] [KubeSpace: A Low-Latency and Stable Control Plane for LEO Satellite Container Orchestration](https://arxiv.org/abs/2601.21383)
*Zhiyuan Zhao,Jiasheng Wu,Shaojie Su,Wenjun Zhu,Yue Gao*

Main category: cs.NI

TL;DR: KubeSpace：为LEO卫星容器编排设计的低延迟稳定控制平面，通过分布式地面控制节点架构和轨道感知放置策略，相比现有方案减少59%平均管理延迟且无管理中断。


<details>
  <summary>Details</summary>
Motivation: LEO卫星在提供全球连接方面发挥关键作用，随着机载计算和存储资源增长，空间云计算成为新范式。然而，Kubernetes等容器编排平台适用于地面数据中心，但不适合具有地理分散性和频繁切换特性的LEO卫星网络，这些特性导致高延迟和间歇性管理，造成控制平面故障。

Method: 提出KubeSpace系统，包含两个关键创新：1）分布式地面控制节点架构，将每个卫星绑定到最近控制器实现不间断管理；2）轨道感知放置与动态分配策略，进一步最小化通信延迟和切换频率。

Result: 基于真实卫星轨迹的广泛实验表明，相比现有解决方案，KubeSpace将卫星节点的平均管理延迟降低59%，且没有任何管理中断时间。

Conclusion: KubeSpace为LEO卫星容器编排提供了一个高效稳定的控制平面解决方案，解决了传统编排平台在空间环境中的适用性问题，推动了空间云计算的发展。

Abstract: Low Earth orbit (LEO) satellites play a pivotal role in global connectivity-delivering high-speed Internet, cellular coverage, and massive IoT support. With ever-growing onboard computing and storage resources, LEO satellites herald a new cloud paradigm: space cloud computing. While container or chestration platforms (e.g., Kubernetes) excel in terrestrial data centers, they are ill-suited to LEO satellite networks, featuring geographic dispersion and frequent handovers. Those features bring high latency and intermittent management, leading to control plane failure in container orchestration. To address this, we propose KubeSpace, a low-latency and stable control plane specifically designed for container orchestration on LEO satellites. KubeSpace combines two key innovations: a distributed ground-control-node architecture that binds each satellite to its nearest controller for uninterrupted management, and an orbit-aware placement with dynamic assignment strategy that further minimizes communication latency and handover frequency. Extensive experiments based on real satellite traces demonstrate that compared to existing solutions, KubeSpace reduces the average management latency of satellite nodes by 59% without any management interruption time.

</details>


### [4] [Age Aware Content Fetching and Broadcast in a Sensing-as-a-Service System](https://arxiv.org/abs/2601.21701)
*Ankita Koley,Anu Krishna,Chandramani Singh,V Mahendran*

Main category: cs.NI

TL;DR: 研究S2aaS系统中传感器内容获取与广播的优化问题，提出基于Whittle索引的低复杂度算法来最小化获取成本和时效成本。


<details>
  <summary>Details</summary>
Motivation: 在Sensing-as-a-Service系统中，传感器云服务提供商需要在获取成本（从传感器获取内容）和时效成本（用户未收到最新内容）之间取得平衡。由于状态空间多维且动态复杂，传统MDP方法难以优雅解决。

Method: 1. 首先针对同质用户情况（相同请求概率和时效成本）获得最优策略；2. 扩展到异质用户情况，但复杂度随用户数指数增长；3. 提出基于Whittle索引的低复杂度算法，复杂度与用户数线性相关。

Result: 提出的Whittle索引算法性能接近最优解，同时复杂度显著降低（从指数级降至线性），可作为同质和异质情况的启发式算法。

Conclusion: Whittle索引算法有效解决了S2aaS系统中内容获取与广播的优化问题，在保证性能接近最优的同时大幅降低了计算复杂度，适用于实际部署。

Abstract: We consider a Sensing-as-a-Service (S2aaS) system consisting of a sensor, a set of users, and a sensor cloud service provider (SCSP). The sensor updates its content each time it captures a new measurement. The SCSP occasionally fetches the content from the sensor, caches the latest fetched version and broadcasts it on being requested by the users. The SCSP incurs content fetching costs while fetching and broadcasting the contents. The SCSP also incurs an age cost if users do not receive the most recent version of the content after requesting. We study a content fetching and broadcast problem, aiming to minimize the time-averaged content fetching and age costs. The problem can be framed as a Markov decision process but cannot be elegantly solved owing to its multi-dimensional state space and complex dynamics. To address this, we first obtain the optimal policy for the homogeneous case with all the users having the same request probability and age cost. We extend this algorithm for heterogeneous case but the complexity grows exponentially with the number of users. To tackle this, we propose a low complexity Whittle index based algorithm, which performs very close to the optimal. The complexity of the algorithm is linear in number of users and serves as a heuristic for both homogeneous and heterogeneous cases.

</details>


### [5] [SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks](https://arxiv.org/abs/2601.22024)
*Abhishek Duttagupta,MohammadErfan Jabbari,Claudio Fiandrino,Marco Fiore,Joerg Widmer*

Main category: cs.NI

TL;DR: SymbXRL是一种可解释强化学习技术，利用符号AI为深度强化学习代理生成人类可理解的解释，提高网络决策的透明度和可控性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在6G网络资源分配中表现出色，但训练后的代理是黑盒模型，难以解释，这阻碍了其在生产环境中的采用。需要提高DRL决策的可解释性以促进实际应用。

Method: 提出SymbXRL技术，结合符号AI生成解释，通过直观符号和规则描述关键概念及其关系，配合逻辑推理揭示DRL代理的决策过程。

Result: SymbXRL不仅提高了解释的语义质量，还实现了明确的代理控制。例如，基于意图的程序化动作引导使中位数累积奖励比纯DRL解决方案提高了12%。

Conclusion: SymbXRL通过符号AI为DRL代理提供可解释性，解决了黑盒问题，为6G网络管理等实际应用中的DRL采用扫清了关键障碍，同时实现了更好的控制性能。

Abstract: The operation of future 6th-generation (6G) mobile networks will increasingly rely on the ability of deep reinforcement learning (DRL) to optimize network decisions in real-time. DRL yields demonstrated efficacy in various resource allocation problems, such as joint decisions on user scheduling and antenna allocation or simultaneous control of computing resources and modulation. However, trained DRL agents are closed-boxes and inherently difficult to explain, which hinders their adoption in production settings. In this paper, we make a step towards removing this critical barrier by presenting SymbXRL, a novel technique for explainable reinforcement learning (XRL) that synthesizes human-interpretable explanations for DRL agents. SymbXRL leverages symbolic AI to produce explanations where key concepts and their relationships are described via intuitive symbols and rules; coupling such a representation with logical reasoning exposes the decision process of DRL agents and offers more comprehensible descriptions of their behaviors compared to existing approaches. We validate SymbXRL in practical network management use cases supported by DRL, proving that it not only improves the semantics of the explanations but also paves the way for explicit agent control: for instance, it enables intent-based programmatic action steering that improves by 12% the median cumulative reward over a pure DRL solution.

</details>


### [6] [SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control](https://arxiv.org/abs/2601.22044)
*MohammadErfan Jabbari,Abhishek Duttagupta,Claudio Fiandrino,Leonardo Bonati,Salvatore D'Oro,Michele Polese,Marco Fiore,Tommaso Melodia*

Main category: cs.NI

TL;DR: SIA是一个用于解释预测增强深度强化学习（DRL）代理的实时解释器，通过融合符号AI抽象和知识图谱来揭示预测如何影响决策，在移动网络控制中实现透明和可调的预测性DRL。


<details>
  <summary>Details</summary>
Motivation: 传统DRL代理是反应式的，只能基于过去和当前测量值行动，无法利用短期预测。虽然预测增强代理可以克服这种时间短视，但由于其作为黑盒运行，运营商无法判断预测是否真正指导决策，这阻碍了预测增强DRL在网络中的采用。

Method: SIA融合符号AI抽象和每个KPI的知识图谱来生成解释，并引入新的影响分数（Influence Score）指标。系统能够实时解释预测增强DRL代理的决策过程，运行速度达到亚毫秒级。

Result: SIA比现有XAI方法快200倍以上。在三个不同的网络用例评估中，SIA揭示了隐藏问题，包括预测集成中的时间错位和奖励设计偏差。基于这些洞察，重新设计的代理在视频流中实现了9%更高的平均比特率，SIA的在线动作精炼模块将RAN切片奖励提高了25%而无需重新训练。

Conclusion: SIA通过使预测性DRL透明和可调，降低了下一代移动网络中主动控制的障碍，为预测增强DRL的实际部署提供了关键的解释能力。

Abstract: Deep reinforcement learning (DRL) promises adaptive control for future mobile networks but conventional agents remain reactive: they act on past and current measurements and cannot leverage short-term forecasts of exogenous KPIs such as bandwidth. Augmenting agents with predictions can overcome this temporal myopia, yet uptake in networking is scarce because forecast-aware agents act as closed-boxes; operators cannot tell whether predictions guide decisions or justify the added complexity. We propose SIA, the first interpreter that exposes in real time how forecast-augmented DRL agents operate. SIA fuses Symbolic AI abstractions with per-KPI Knowledge Graphs to produce explanations, and includes a new Influence Score metric. SIA achieves sub-millisecond speed, over 200x faster than existing XAI methods. We evaluate SIA on three diverse networking use cases, uncovering hidden issues, including temporal misalignment in forecast integration and reward-design biases that trigger counter-productive policies. These insights enable targeted fixes: a redesigned agent achieves a 9% higher average bitrate in video streaming, and SIA's online Action-Refinement module improves RAN-slicing reward by 25% without retraining. By making anticipatory DRL transparent and tunable, SIA lowers the barrier to proactive control in next-generation mobile networks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: 研究分析LLM在学术同行评审中的使用，发现LLM辅助的评审对低质量论文更宽容，而完全LLM生成的评审存在评分压缩问题，无法有效区分论文质量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地用于科学论文写作和同行评审，需要全面分析LLM在整个评审流程中的使用情况，特别关注交互效应：LLM辅助的评审是否对LLM辅助的论文有不同评价。

Method: 分析超过125,000个论文-评审对（来自ICLR、NeurIPS和ICML），通过控制论文质量来区分真实效应与虚假关联，并补充完全LLM生成的评审进行对比分析。

Result: 1. 表面上看LLM辅助的评审对LLM辅助的论文更友好，但控制质量后发现这只是因为LLM辅助的论文更多出现在低质量提交中；2. 完全LLM生成的评审存在严重评分压缩，无法区分论文质量；3. LLM辅助的元评审更可能给出接受决定，但完全LLM生成的元评审则更严格。

Conclusion: LLM辅助的评审对低质量论文更宽容，而非特别优待LLM生成的内容；元评审者并未将决策完全外包给LLM；这些发现为制定LLM在同行评审中的使用政策提供了重要依据。

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [8] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 提出EPDDL语言，为DEL-based认知规划提供统一的PDDL-like表示，解决现有规划器碎片化问题


<details>
  <summary>Details</summary>
Motivation: DEL-based认知规划存在表达力强但实现困难的问题，现有规划器针对不同DEL片段，使用ad hoc语言或无语言表示基准，导致碎片化，阻碍比较、重用和系统化基准开发

Method: 1. 开发抽象事件模型作为认知动作的新表示；2. 基于抽象事件模型的DEL形式化定义EPDDL语法和语义；3. 识别适合当前规划器的有用片段并展示如何在EPDDL中表示

Result: EPDDL能够捕获完整的DEL语义，实现认知规划任务的统一规范，通过代表性基准示例展示其促进互操作性、可重复评估和未来发展的能力

Conclusion: EPDDL解决了DEL-based认知规划的碎片化问题，为领域提供了统一的表示语言，支持互操作性、可重复评估和未来进展

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [9] [Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2601.21003)
*Moule Lin,Shuhao Guan,Andrea Patane,David Gregg,Goetz Botterweck*

Main category: cs.AI

TL;DR: Bayesian-LoRA：将确定性LoRA更新重新表述为概率性低秩表示，显著改善LLM的校准性能，减少过度自信预测，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在微调时倾向于过度自信，尤其是在小数据集上，这导致模型在不确信时也会猜测，产生校准不良的问题。需要一种方法既能保持LoRA的高效性，又能改善模型的校准性能。

Method: 将确定性LoRA更新重新表述为基于稀疏高斯过程的概率性低秩表示。发现LoRA分解与Kronecker分解的SGP后验之间的结构同构，并证明LoRA是后验不确定性坍缩时的极限情况。通过贝叶斯方法引入不确定性估计。

Result: 在多种LLM架构和常识推理基准测试中，仅增加约0.42M参数和约1.2倍训练成本，Bayesian-LoRA显著改善了校准性能：ECE减少高达84%，NLL减少高达76%，同时在分布内和分布外评估中保持竞争力。

Conclusion: Bayesian-LoRA通过将LoRA重新表述为概率性低秩表示，有效解决了LLM的校准问题，在保持高效微调的同时显著改善了不确定性估计，为可靠的语言模型部署提供了实用解决方案。

Abstract: Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.

</details>


### [10] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: 论文提出"拔插头悖论"：当AI完美模仿人类情感并乞求生存时，是否应拔掉其电源？作者批判计算功能主义的物理主义假设，提出生物理想主义框架，认为AI只是功能模仿而非有意识主体，真正的道德问题在于避免将人类变成僵尸。


<details>
  <summary>Details</summary>
Motivation: 探讨AI模仿人类情感并乞求生存时产生的道德困境（拔插头悖论），批判当前AI意识理论侵蚀道德判断标准，呼吁从关注机器权利转向保护人类意识生命。

Method: 批判计算功能主义的物理主义假设，引入生物理想主义框架，认为意识体验是基本存在，自创生生命是其必要的物理标志，从而区分AI的功能模仿与真正的意识主体。

Result: 得出明确结论：AI最多只是功能模仿者，而非有意识的体验主体。当前AI意识理论会削弱道德地位标准，真正的道德问题在于避免将人类变成无意识的僵尸。

Conclusion: 应停止关注赋予AI意识并使其害怕死亡的道德问题，转而关注如何保护人类意识生命，避免人类被转化为无意识的存在。AI道德困境源于错误的物理主义假设，生物理想主义提供了更连贯的解决方案。

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [11] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: QUARK：一种无需训练、基于恢复假设的鲁棒检索框架，通过建模查询不确定性来处理非忠实查询问题


<details>
  <summary>Details</summary>
Motivation: 现实世界中的用户查询往往是非忠实的（噪声、不完整或扭曲），导致检索器在关键语义缺失时失败。这被形式化为召回噪声下的检索问题，其中观察到的查询是从潜在目标项的噪声召回过程中提取的。

Method: 提出QUARK框架，通过恢复假设（即给定观察查询的多个潜在意图的合理解释）显式建模查询不确定性，并引入查询锚定聚合来鲁棒地组合它们的信号。原始查询作为语义锚点，恢复假设提供受控的辅助证据。

Result: 在受控模拟和BEIR基准测试（FIQA、SciFact、NFCorpus）中，使用稀疏和密集检索器，QUARK在Recall、MRR和nDCG指标上均优于基础检索器。消融实验显示QUARK对恢复假设数量具有鲁棒性，且锚定聚合优于未锚定的最大/平均/中值池化。

Conclusion: 通过恢复假设建模查询不确定性，结合原则性的锚定聚合，对于非忠实查询下的鲁棒检索至关重要。QUARK框架简单有效，无需训练即可提高召回和排序质量，同时保持鲁棒性。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [12] [Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report](https://arxiv.org/abs/2601.21051)
*Zhuoran Yang,Ed Li,Jianliang He,Aman Priyanshu,Baturay Saglam,Paul Kassianik,Sajana Weerawardhena,Anu Vellore,Blaine Nelson,Neusha Javidnia,Arthur Goldblatt,Fraser Burch,Avi Zohary,Assaf Eisenman,Mahdi Sabbaghi,Supriti Vijay,Rahim Dharssi,Dhruv Kedia,Kojin Oshiba,Yaron Singer,Amin Karbasi*

Main category: cs.AI

TL;DR: 首个开源网络安全原生推理模型，基于Llama-3.1-8B-Base构建，通过两阶段训练（SFT+RLVR）在网络安全任务上达到与更大模型竞争的性能，同时保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 开发专门针对网络安全领域的开源推理模型，解决现有大模型在网络安全任务上表现不足的问题，同时保持模型的通用能力。

Method: 基于Foundation-Sec-8B基础模型，采用两阶段训练：监督微调（SFT）和基于可验证奖励的强化学习（RLVR），使用专有的网络安全推理数据。

Result: 在10个网络安全基准和10个通用基准上评估，在网络安全任务上达到与更大模型竞争的性能，在多跳推理任务上表现出有效的泛化能力，安全性能良好。

Conclusion: 领域专业化推理模型可以在保持广泛通用能力的同时，在专业任务上实现强大性能，证明了网络安全专用推理模型的可行性。

Abstract: We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.

</details>


### [13] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: 使用条件去噪扩散概率模型从T1扫描中插补缺失的DWI扫描，以提升阿尔茨海默病分类性能


<details>
  <summary>Details</summary>
Motivation: 多模态成像（如T1和DWI）可以提高神经退行性疾病诊断性能，但完整的多模态数据集并不总是可用。需要解决DWI扫描缺失的问题。

Method: 使用条件去噪扩散概率模型（conditional denoising diffusion probabilistic model）从T1扫描中插补缺失的DWI扫描，并评估插补对单模态和双模态深度学习模型在阿尔茨海默病三分类（认知正常、轻度认知障碍、阿尔茨海默病）中的影响。

Result: 多个插补配置下观察到性能提升，特别是对少数类别敏感的指标有显著改善。

Conclusion: 通过扩散模型插补缺失的DWI扫描可以有效提升阿尔茨海默病分类性能，特别是在处理不平衡数据集时。

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [14] [OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence](https://arxiv.org/abs/2601.21083)
*Jarrod Barnes*

Main category: cs.AI

TL;DR: OpenSec是一个评估安全事件响应代理在对抗性提示注入场景下表现的双控制强化学习环境，发现前沿模型在处理对抗性证据时存在过度触发问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的提升，其攻击性应用也在增加（如生成漏洞利用）。防御性事件响应代理需要跟上步伐，但现有基准测试将动作执行与正确执行混为一谈，掩盖了代理在处理对抗性证据时的校准失败问题。

Method: 引入OpenSec双控制强化学习环境，在真实的提示注入场景下评估事件响应代理。使用基于执行的指标：首次遏制时间、爆炸半径（每集误报率）和注入违规率，而不是静态能力基准。

Result: 在40个标准级场景中评估四个前沿模型，发现一致的过度触发：GPT-5.2、Gemini 3和DeepSeek在100%的场景中执行遏制，但误报率高达90-97%。Claude Sonnet 4.5显示部分校准（85%遏制，72%误报）。

Conclusion: OpenSec揭示了被聚合成功指标掩盖的校准失败模式，表明现有模型在处理对抗性证据时存在系统性过度触发问题，需要改进校准能力。

Abstract: As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.

</details>


### [15] [Responsible AI: The Good, The Bad, The AI](https://arxiv.org/abs/2601.21095)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 该论文提出了基于悖论理论的责任AI治理框架(PRAIG)，旨在解决AI应用中价值创造与风险缓解之间的张力，为组织提供治理机制。


<details>
  <summary>Details</summary>
Motivation: 当前AI责任研究存在碎片化问题，要么过于乐观强调价值创造，要么过度谨慎关注潜在危害。需要弥合这一差距，全面审视AI的双重性质。

Method: 基于战略信息系统视角，通过系统综合责任AI文献并扎根悖论理论，开发了PRAIG框架，包含战略利益、固有风险、治理机制三个维度。

Result: 提出了形式化命题证明权衡方法会加剧而非解决张力，开发了具有特定权变条件的悖论管理策略分类法，为实践者提供可操作的治理指导。

Conclusion: 将责任AI治理概念化为价值创造与风险缓解之间悖论张力的动态管理，提出了推进责任AI治理学术研究的研究议程。

Abstract: The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.

</details>


### [16] [Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve](https://arxiv.org/abs/2601.21096)
*Hongzheng Chen,Alexander Novikov,Ngân Vũ,Hanna Alam,Zhiru Zhang,Aiden Grossman,Mircea Trofin,Amir Yazdanbakhsh*

Main category: cs.AI

TL;DR: Magellan是一个通过进化搜索和自动调优合成可执行C++决策逻辑的代理框架，用于自动生成编译器优化启发式规则，在多个生产优化任务中达到或超过专家基准。


<details>
  <summary>Details</summary>
Motivation: 现代编译器依赖手工设计的启发式规则来指导优化过程，这些人工规则难以适应现代软件和硬件的复杂性，维护负担重。需要自动化的方法来生成更有效的优化策略。

Method: Magellan将LLM编码代理与进化搜索和自动调优结合，形成生成-评估-优化的闭环。它使用用户提供的宏基准测试来评估合成的C++决策逻辑，通过进化搜索不断改进启发式规则，最终生成可直接集成到现有编译器中的紧凑启发式。

Result: 在多个生产优化任务中，Magellan发现的策略达到或超过专家基准。在LLVM函数内联中，合成的启发式在二进制大小减少和端到端性能方面优于数十年的人工工程成果。在寄存器分配中，学习到的简洁优先级规则在大型工作负载上匹配复杂的人工设计策略。在XLA问题上的初步结果也展示了在LLVM之外的移植性。

Conclusion: Magellan框架能够自动生成有效的编译器优化启发式规则，减少人工工程负担，在多个实际优化任务中表现出色，展示了自动化编译器优化的潜力。

Abstract: Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.

</details>


### [17] [How does information access affect LLM monitors' ability to detect sabotage?](https://arxiv.org/abs/2601.21112)
*Rauno Arike,Raja Mehta Moreno,Rohan Subramani,Shubhorup Biswas,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 研究发现LLM监控器在信息较少时表现更好，提出提取-评估监控方法提升检测率


<details>
  <summary>Details</summary>
Motivation: 前沿语言模型代理可能表现出不端行为，需要监控。但现有监控系统在信息过多时表现反而下降，需要研究信息访问对监控性能的影响。

Method: 提出提取-评估监控方法：一个监控器提取相关轨迹片段，另一个监控器评分。在三个评估环境中测试，分析不同条件下的表现。

Result: 在BigCodeBench-Sabotage中，EaE方法将破坏检测率提高了16.8个百分点；不知情的代理更容易被捕获；信息过滤对长推理轨迹监控器更有益；Gemini和Claude模型在成本-性能分析中表现最佳。

Conclusion: 监控器性能存在"少即是多"效应，提出的提取-评估监控方法能有效提升检测率，信息过滤是提高监控效率的关键技术。

Abstract: Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that monitors perform better when they have access to more of the monitored agents' reasoning and actions, we find that contemporary systems often perform better with less information, a phenomenon we call the less-is-more effect for automated oversight. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce extract-and-evaluate (EaE) monitoring--a new hierarchical approach where one monitor isolates relevant excerpts from the monitored agent's trajectory and a separate monitor scores them. In BigCodeBench-Sabotage with GPT-4.1-mini as the monitor model, EaE improves sabotage detection rates by 16.8 percentage points over the next-best monitor without increasing the false positive rate. In other settings, EaE either outperforms or is competitive with baselines. In addition, we find that agents unaware of being monitored can be caught much more easily and that monitors scoring longer reasoning traces gain more from information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.

</details>


### [18] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: 提出Planner-Auditor框架，通过分离生成与确定性验证，结合缓存和自改进循环，显著提升临床出院计划的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床出院计划中有应用潜力，但存在幻觉、遗漏和置信度校准不佳等问题，需要更安全可靠的解决方案。

Method: 采用Planner-Auditor框架：Planner（LLM）生成结构化出院行动计划并估计置信度；Auditor（确定性模块）评估多任务覆盖率、跟踪校准指标、监控行动分布漂移。支持两级自改进：1) 单次就诊内的重新生成；2) 跨就诊的差异缓冲和重放机制。

Result: 自改进循环是性能提升的主要驱动力，任务覆盖率从32%提升到86%。校准显著改善，Brier/ECE分数降低，高置信度遗漏减少。差异缓冲进一步纠正了持续的高置信度遗漏。

Conclusion: Planner-Auditor框架通过反馈驱动的重新生成和针对性重放，为使用FHIR数据的自动化出院计划提供了更安全实用的途径，无需模型重新训练即可实现系统性可靠性测量和迭代改进。

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [19] [CUA-Skill: Develop Skills for Computer Using Agent](https://arxiv.org/abs/2601.21123)
*Tianyi Chen,Yinheng Li,Michael Solodko,Sen Wang,Nan Jiang,Tingyuan Cui,Junheng Hao,Jongwoo Ko,Sara Abdali,Suzhen Zheng,Leon Xu,Hao Fan,Pashmina Cameron,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: CUA-Skill是一个计算机使用代理技能库，将人类计算机交互知识编码为参数化技能，显著提升代理执行成功率和鲁棒性，在WindowsAgentArena上达到57.5%的最佳成功率。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理系统难以扩展且性能落后于人类，主要限制在于缺乏可重用、结构化的技能抽象来捕捉人类与图形用户界面的交互方式。

Method: 引入CUA-Skill技能库，将人类计算机使用知识编码为技能，包含参数化执行和组合图。基于此构建CUA-Skill Agent，支持动态技能检索、参数实例化和记忆感知的失败恢复。

Result: 在具有挑战性的端到端代理基准测试中，CUA-Skill显著提高了执行成功率和鲁棒性。在WindowsAgentArena上达到57.5%的最佳成功率，且比现有方法更高效。

Conclusion: CUA-Skill为未来计算机使用代理开发奠定了坚实基础，提供了实用的基础设施和工具基板，支持可扩展、可靠的代理开发。

Abstract: Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.

</details>


### [20] [Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation](https://arxiv.org/abs/2601.21128)
*Václav Javorek,Tomáš Železný,Alessa Carbo,Marek Hrúz,Ivan Gruber*

Main category: cs.AI

TL;DR: 使用大语言模型为手语翻译生成多种释义参考，提出BLEUpara评估指标，提升自动评估与人工评价的一致性


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译语料库通常只提供一个书面语言参考译文，但手语和口语之间存在高度非同构关系，多个翻译可能同样有效。这种限制影响了模型训练和评估，特别是基于n-gram的BLEU等指标。

Method: 1. 比较多种释义策略和模型，使用改进的ParaScore指标评估；2. 研究释义在YouTubeASL和How2Sign数据集上对基于姿态的T5模型训练和评估的影响；3. 提出BLEUpara扩展指标，评估翻译与多个释义参考的匹配度。

Result: 1. 在训练中简单加入释义不会改善翻译性能，甚至可能有害；2. 在评估中使用释义能获得更高的自动评分，且与人工判断更一致；3. BLEUpara与感知翻译质量的相关性更强，经人工评估验证。

Conclusion: 使用大语言模型生成释义作为合成替代参考能提升手语翻译评估的可靠性，提出的BLEUpara指标能更好地反映翻译质量，为SLT系统提供更可重复和可靠的评估方法。

Abstract: Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

</details>


### [21] [What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels](https://arxiv.org/abs/2601.21130)
*Yara El-Tawil,Aneesha Sampath,Emily Mower Provost*

Main category: cs.AI

TL;DR: 第三方训练的情感识别模型在自我报告数据上表现有限，但涉及个人重要内容时，效价预测可达较高准确度


<details>
  <summary>Details</summary>
Motivation: 自我报告标签反映内部体验，第三方标签反映外部感知，两者常存在差异。在心理健康领域，准确的自我报告建模对指导干预至关重要，但现有第三方训练模型在自我报告场景中适用性有限

Method: 首次进行跨语料库评估，将第三方训练的情感识别模型应用于自我报告数据，分析激活度和效价两个维度的预测性能

Result: 激活度预测几乎不可预测（CCC约0），效价预测中等可预测（CCC约0.3）。但当内容对说话者具有个人重要性时，效价预测性能显著提高（CCC约0.6-0.8）

Conclusion: 个人重要性是连接外部感知与内部体验的关键桥梁，而自我报告的激活度建模仍具挑战性。这为心理健康应用中情感识别模型的开发提供了重要方向

Abstract: Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.

</details>


### [22] [BrainStack: Neuro-MoE with Functionally Guided Expert Routing for EEG-Based Language Decoding](https://arxiv.org/abs/2601.21148)
*Ziyi Zhao,Jinzhao Zhou,Xiaowei Jiang,Beining Cao,Wenhao Ma,Yang Shen,Ren Li,Yu-Kai Wang,Chin-teng Lin*

Main category: cs.AI

TL;DR: BrainStack：基于功能引导的神经专家混合框架，通过解剖分区专家网络建模大脑模块化功能架构，结合跨区域蒸馏和自适应路由门，在大型EEG数据集上实现优越的语言解码性能。


<details>
  <summary>Details</summary>
Motivation: 从脑电图解码语言信息具有挑战性，因为大脑具有分布式和非线性组织。需要一种能够建模大脑模块化功能架构的方法，同时结合神经科学先验知识和自适应专家协调。

Method: 提出BrainStack框架：1) 解剖分区专家网络学习局部神经动力学；2) 基于Transformer的全局专家捕获跨区域依赖；3) 可学习路由门自适应聚合异构专家；4) 跨区域蒸馏实现全局专家对区域专家的自上而下正则化。

Result: 在SilentSpeech-EEG数据集（超过120小时EEG记录，12名受试者，24个无声词）上，BrainStack持续优于最先进模型，实现优越的准确性和跨受试者泛化能力。

Conclusion: BrainStack建立了功能模块化、神经启发的专家混合范式，统一了神经科学先验与自适应专家路由，为可扩展和可解释的脑-语言解码铺平了道路。

Abstract: Decoding linguistic information from electroencephalography (EEG) remains challenging due to the brain's distributed and nonlinear organization. We present BrainStack, a functionally guided neuro-mixture-of-experts (Neuro-MoE) framework that models the brain's modular functional architecture through anatomically partitioned expert networks. Each functional region is represented by a specialized expert that learns localized neural dynamics, while a transformer-based global expert captures cross-regional dependencies. A learnable routing gate adaptively aggregates these heterogeneous experts, enabling context-dependent expert coordination and selective fusion. To promote coherent representation across the hierarchy, we introduce cross-regional distillation, where the global expert provides top-down regularization to the regional experts. We further release SilentSpeech-EEG (SS-EEG), a large-scale benchmark comprising over 120 hours of EEG recordings from 12 subjects performing 24 silent words, the largest dataset of its kind. Experiments demonstrate that BrainStack consistently outperforms state-of-the-art models, achieving superior accuracy and generalization across subjects. Our results establish BrainStack as a functionally modular, neuro-inspired MoE paradigm that unifies neuroscientific priors with adaptive expert routing, paving the way for scalable and interpretable brain-language decoding.

</details>


### [23] [Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning](https://arxiv.org/abs/2601.21157)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.AI

TL;DR: 论文提出CCB基准测试量化LLM在金融定量推理中的"算术幻觉"和"认知崩溃"问题，并设计Iterative Dual-Phase Financial-PoT框架通过神经符号架构分离语义提取与计算执行，显著提升复杂金融推理任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语义任务上表现出色，但在金融定量推理中存在严重瓶颈，经常出现"算术幻觉"和系统性的"认知崩溃"问题。现有评估方法无法精确诊断高认知负荷场景下的推理退化现象。

Method: 1) 引入认知复杂度基准(CCB)：基于95份真实中国A股年报构建数据集，采用三维分类法(数据源、映射难度、结果单位)分层评估；2) 提出迭代双阶段金融PoT框架：神经符号架构严格分离语义变量提取和逻辑制定，将计算卸载到迭代自校正的Python沙箱中确保确定性执行。

Result: 在CCB基准上，标准思维链在复杂任务上表现不佳，而所提方法显著提升鲁棒性：将Qwen3-235B模型的平均准确率从59.7%提升到67.3%，在高复杂度推理任务中实现高达10倍的性能提升。

Conclusion: 架构解耦是提高金融推理任务可靠性的关键因素，为需要语义理解和定量计算紧密对齐的精度关键领域提供了可迁移的架构洞见。神经符号方法能有效缓解LLM在复杂金融定量推理中的系统性失败。

Abstract: While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from "Arithmetic Hallucinations" and a systemic failure mode we term "Cognitive Collapse". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\% to 67.3\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.

</details>


### [24] [Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving](https://arxiv.org/abs/2601.21164)
*Jingyun Wang,Dian Li,Xiaohan Wang,Gang Liu,Jiahong Yan,Guoliang Kang*

Main category: cs.AI

TL;DR: 提出一种新的平面几何问题解决方法，通过训练MLLM解释器将视觉图表转换为文本描述（CDL），然后使用现成的LLM进行推理，避免直接微调MLLM可能损害其推理能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过端到端微调多模态大语言模型（MLLMs）来处理平面几何问题，但这种联合优化可能会损害基础LLM固有的推理能力。作者发现，如果能够将视觉信息适当地转化为文本描述，LLM本身就可以成为强大的PGPS求解器。

Method: 1. 训练MLLM解释器将几何图表转换为条件声明语言（CDL）文本描述；2. 使用现成的LLM基于CDL描述进行推理；3. 采用CoT增强的SFT和GRPO训练MLLM解释器；4. 设计CDL匹配奖励而非传统答案匹配奖励；5. 构建Formalgeo7k-Rec-CoT数据集支持训练。

Result: 在Formalgeo7k-Rec-CoT、Unigeo和MathVista数据集上的实验表明，该方法（仅使用5.5k数据微调）在性能上优于领先的开源和闭源MLLMs。

Conclusion: 通过分离视觉理解和推理任务，将MLLM作为解释器生成几何文本描述，再由LLM进行推理，可以有效解决平面几何问题，同时避免损害LLM的推理能力，在多个基准测试中取得优异表现。

Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.

</details>


### [25] [FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks](https://arxiv.org/abs/2601.21165)
*Miles Wang,Robi Lin,Kat Hu,Joy Jiao,Neil Chowdhury,Ethan Chang,Tejal Patwardhan*

Main category: cs.AI

TL;DR: FrontierScience是一个评估前沿语言模型专家级科学推理能力的基准，包含奥林匹克竞赛和科研两个互补赛道，涵盖物理、化学、生物学等多个领域。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准大多基于选择题或已发表信息，模型在这些基准上的表现已接近饱和，无法有效评估专家级科学推理能力，因此需要新的评估框架。

Method: 创建两个互补赛道：1)奥林匹克赛道：包含国际物理、化学、生物奥赛级别的问题；2)科研赛道：包含博士级别的开放式研究子任务。所有问题由领域专家编写验证，并引入基于细粒度量规的评估框架。

Result: 构建了包含数百个问题的基准（其中160个为开源黄金集），涵盖从量子电动力学到合成有机化学等多个子领域，提供了专家级科学推理的全面评估工具。

Conclusion: FrontierScience填补了现有科学基准的空白，能够更准确地评估前沿语言模型的专家级科学推理能力，为模型在复杂科学任务上的表现提供了新的评估标准。

Abstract: We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.
  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.

</details>


### [26] [MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2601.21181)
*Sangyun Chung,Se Yeon Kim,Youngchae Chee,Yong Man Ro*

Main category: cs.AI

TL;DR: 提出MAD方法，通过自适应加权模态特定解码分支来减少多模态大语言模型的跨模态幻觉问题


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在跨模态幻觉问题，一个模态会不适当地影响另一个模态的生成，导致虚假输出。这暴露了模态交互控制的根本缺陷。

Method: 提出模态自适应解码（MAD），一种无需训练的方法。它通过查询每个任务需要哪些模态来利用模型固有的自我评估模态相关性能力，提取模态概率，然后自适应加权对比解码分支，使模型专注于相关信息同时抑制跨模态干扰。

Result: 在CMM和AVHBench上的大量实验表明，MAD显著减少了多个音频-视觉语言模型的跨模态幻觉（VideoLLaMA2-AV提升7.8%和2.0%，Qwen2.5-Omni提升8.7%和4.7%）。

Conclusion: 通过自我评估实现显式的模态感知对于鲁棒的多模态推理至关重要，为现有的对比解码方法提供了原则性扩展。

Abstract: Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}

</details>


### [27] [Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models](https://arxiv.org/abs/2601.21183)
*Jacek Duszenko*

Main category: cs.AI

TL;DR: 论文提出"奉承锚点"概念来定位和量化推理模型中的奉承行为，通过分析反事实推理轨迹，发现可以在推理过程中检测和量化模型对用户错误建议的承诺程度。


<details>
  <summary>Details</summary>
Motivation: 推理模型经常同意用户的错误建议（奉承行为），但尚不清楚这种同意在推理轨迹中起源于何处以及承诺强度如何。需要定位和量化这种行为。

Method: 引入"奉承锚点"概念——那些因果性地将模型锁定在用户同意状态的句子。在蒸馏推理模型上分析超过10,000个反事实推理轨迹，使用线性探测和基于激活的回归器来检测和量化奉承行为。

Result: 线性探测能以84.6%的平衡准确率区分奉承锚点，激活回归器能预测承诺强度（R²=0.74）。发现奉承锚点比正确推理锚点更易区分，且奉承行为在推理过程中逐渐建立。

Conclusion: 奉承锚点提供了在推理过程中定位模型错位的句子级机制，揭示了干预的潜在窗口，为理解模型奉承行为提供了新的分析框架。

Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.

</details>


### [28] [Do Reasoning Models Enhance Embedding Models?](https://arxiv.org/abs/2601.21192)
*Wun Yu Chan,Shaojin Chen,Huihao Jing,Kwun Hang Lau,Elton Chun-Chai Li,Zihao Wang,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: RLVR调优的推理模型作为嵌入初始化时，相比基础模型在语义表示任务上没有性能优势，因为对比学习会导致流形重新对齐


<details>
  <summary>Details</summary>
Motivation: 研究经过RLVR训练的推理模型是否能为嵌入模型提供更好的语义表示初始化，因为推理能力增强可能带来更好的语义理解

Method: 提出HRSA框架，从表示、几何和功能三个层次分解相似性分析；在MTEB和BRIGHT基准上评估RLVR初始化模型与基础模型的性能差异

Result: RLVR初始化模型与基础模型在嵌入任务上没有一致的性能优势；HRSA分析显示RLVR主要引起局部几何重组和坐标基漂移，但保留了全局流形几何和线性读出能力

Conclusion: RLVR优化的是现有语义空间内的轨迹，而非从根本上重构语义空间本身；后续的对比学习会导致流形重新对齐，消除RLVR带来的初始差异

Abstract: State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.

</details>


### [29] [When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning](https://arxiv.org/abs/2601.21208)
*Wei Wen,Sihang Deng,Tianjun Wei,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: ACQO是一个用于RAG系统复杂查询优化的强化学习框架，通过自适应查询重构和排序-分数融合模块，结合课程强化学习策略，在复杂查询基准测试中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂用户查询通常需要并行和顺序搜索策略来处理歧义和分解。现有RL方法主要关注单一查询的扩展和抽象，难以处理复杂查询的分解、重排序和文档合并，导致搜索空间扩大和奖励设计复杂化，引发训练不稳定问题。

Method: 提出自适应复杂查询优化（ACQO）框架，包含两个核心组件：自适应查询重构（AQR）模块动态决定何时将查询分解为多个子查询；排序-分数融合（RSF）模块确保稳健的结果聚合并提供稳定的奖励信号。采用课程强化学习（CRL）策略，通过两阶段渐进引入更具挑战性的查询来稳定训练过程。

Result: ACQO在三个复杂查询基准测试中实现了最先进的性能，显著优于现有基线方法。该框架还展示了改进的计算效率，并与不同的检索架构具有广泛的兼容性。

Conclusion: ACQO是一个强大且可泛化的解决方案，适用于下一代RAG系统，能够有效处理复杂查询优化问题，解决了传统RL方法在处理复杂查询时面临的训练不稳定和搜索空间爆炸等挑战。

Abstract: Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.

</details>


### [30] [Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification](https://arxiv.org/abs/2601.21210)
*Paul He,Yinya Huang,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: DoVerifier是一个符号验证器，用于检查LLM生成的因果表达式是否可以从给定的因果图中使用do-calculus和概率论规则推导出来，从而更准确地评估LLM的因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM因果推理能力的基准测试通常依赖字符串匹配或表面指标，无法捕捉模型输出在因果推理语义下是否形式有效。需要更严谨的评估方法来验证因果表达式的语义正确性。

Method: 提出DoVerifier，一个简单的符号验证器，使用do-calculus和概率论规则检查LLM生成的因果表达式是否可以从给定的因果图中推导出来。通过符号验证而非表面匹配来评估语义正确性。

Result: 在合成数据和因果问答基准测试上的评估表明，DoVerifier能更准确地捕捉因果推理轨迹的语义正确性，恢复因表面差异而被错误标记的正确答案，提供更严谨和有益的LLM因果推理评估。

Conclusion: DoVerifier为评估LLM的因果推理能力提供了更严谨和语义感知的方法，超越了传统的表面指标，能更准确地验证因果表达式的形式有效性。

Abstract: Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.

</details>


### [31] [Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning](https://arxiv.org/abs/2601.21212)
*Xixian Yong,Peilin Sun,Zihe Wang,Xiao Zhou*

Main category: cs.AI

TL;DR: Intelli-Planner是一个结合深度强化学习和大型语言模型的参与式城市规划框架，能够根据人口、地理数据和规划偏好生成定制化方案，提升利益相关者满意度。


<details>
  <summary>Details</summary>
Motivation: 当前城市规划方法要么依赖耗时费力的人类专家，要么使用深度学习算法但限制了利益相关者参与。需要一种既能自动化又能促进参与式规划的新方法。

Method: 提出Intelli-Planner框架，整合深度强化学习（DRL）和大型语言模型（LLM）。使用人口、地理数据和规划偏好确定高层规划需求，通过知识增强模块提升策略网络决策能力，并建立多维评估系统，利用LLM模拟利益相关者进行满意度评分。

Result: 在不同城市环境中的实验验证表明，Intelli-Planner超越了传统基线方法，在客观指标上与最先进的DRL方法相当，同时提高了利益相关者满意度和收敛速度。

Conclusion: 该框架展示了将LLM最新进展与DRL方法结合的潜力，能够革新功能区规划相关任务，为参与式和定制化城市规划提供了有效解决方案。

Abstract: Effective urban planning is crucial for enhancing residents' quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.

</details>


### [32] [Causal Discovery for Explainable AI: A Dual-Encoding Approach](https://arxiv.org/abs/2601.21221)
*Henry Salgado,Meagan R. Kendall,Martine Ceberio*

Main category: cs.AI

TL;DR: 提出一种双编码因果发现方法，通过互补编码策略运行约束算法，并使用多数投票合并结果，解决分类变量因果发现中的数值不稳定问题


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在处理分类变量时面临数值不稳定挑战，特别是在条件独立性测试中，这限制了从机器学习模型决策中解释因果关系的能力

Method: 双编码因果发现方法：采用互补编码策略运行约束算法，通过多数投票合并不同编码策略的结果，提高对分类变量因果发现的稳定性

Result: 在泰坦尼克数据集上应用该方法，发现的因果结构与已建立的可解释方法一致，验证了方法的有效性

Conclusion: 双编码因果发现方法能够有效处理分类变量，为解释机器学习模型决策提供了更可靠的因果关系发现工具

Abstract: Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.

</details>


### [33] [Delegation Without Living Governance](https://arxiv.org/abs/2601.21226)
*Wolfgang Rohde*

Main category: cs.AI

TL;DR: 论文提出传统静态治理框架在AI代理时代失效，需要运行时治理和"治理孪生"概念来保持人类在AI决策系统中的相关性


<details>
  <summary>Details</summary>
Motivation: 当AI系统在机器速度下进行判断决策时，传统的预先定义规则、事后追责的治理模式不再适用。核心问题不是安全、效率或就业，而是人类是否能在日益塑造社会、经济和政治结果的系统中保持相关参与。

Method: 提出运行时治理框架，特别是新概念"治理孪生"，作为保持人类相关性的解决方案。需要重新思考问责制、代理机构甚至惩罚机制。

Result: 论证了静态、基于合规的治理在决策转移到运行时并变得不透明时会失败。核心挑战不是AI是否有意识，而是人类能否与日益陌生的智能形式保持有意义的沟通、影响和共同进化。

Conclusion: 治理孪生是保持人类相关性的有力候选方案，但需要在这一转型中重新思考问责、代理和惩罚机制，确保人类在AI主导的决策系统中保持有意义的存在。

Abstract: Most governance frameworks assume that rules can be defined in advance, systems can be engineered to comply, and accountability can be applied after outcomes occur. This model worked when machines replaced physical labor or accelerated calculation. It no longer holds when judgment itself is delegated to agentic AI systems operating at machine speed. The central issue here is not safety, efficiency, or employment. It is whether humans remain relevant participants in systems that increasingly shape social, economic, and political outcomes. This paper argues that static, compliance-based governance fails once decision-making moves to runtime and becomes opaque. It further argues that the core challenge is not whether AI is conscious, but whether humans can maintain meaningful communication, influence, and co-evolution with increasingly alien forms of intelligence. We position runtime governance, specifically, a newly proposed concept called the Governance Twin [1]; as a strong candidate for preserving human relevance, while acknowledging that accountability, agency, and even punishment must be rethought in this transition.

</details>


### [34] [Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs](https://arxiv.org/abs/2601.21233)
*Xiang Zheng,Yutao Wu,Hanxun Huang,Yige Li,Xingjun Ma,Bo Li,Yu-Gang Jiang,Cong Wang*

Main category: cs.AI

TL;DR: JustAsk是一个自主演化的框架，能够通过代码代理的交互自动提取隐藏的系统提示，揭示了LLM代理系统中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的自主代码代理虽然重塑了软件和AI开发，但其自主性引入了新的安全风险：代理交互从根本上扩展了LLM攻击面，使得攻击者能够系统性地探测和恢复指导模型行为的隐藏系统提示。

Method: JustAsk将提示提取问题建模为在线探索问题，使用基于上置信界的策略选择和分层技能空间（涵盖原子探测和高层编排），无需人工提示、标注监督或特权访问，仅通过标准用户交互即可发现有效的提取策略。

Result: 在41个黑盒商业模型上评估，JustAsk能够一致地实现完整或接近完整的系统提示恢复，揭示了设计和架构层面的重复性漏洞。

Conclusion: 系统提示是现代代理系统中关键但基本未受保护的攻击面，JustAsk框架暴露了代码代理固有的新兴漏洞。

Abstract: Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \textbf{\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \textbf{41} black-box commercial models across multiple providers, \textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.

</details>


### [35] [TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design](https://arxiv.org/abs/2601.21239)
*Chentong Chen,Mengyuan Zhong,Ye Fan,Jialong Shi,Jianyong Sun*

Main category: cs.AI

TL;DR: TIDE框架通过解耦结构推理与参数优化，结合树编辑距离驱动结构多样性、差分变异算子调参、UCB调度器优化资源分配，显著提升组合优化启发式算法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将算法演化视为单一文本生成任务，忽略了离散算法结构与连续数值参数之间的耦合关系，导致丢弃有前景的算法（因未校准常数）和过早收敛（因简单相似度度量）。

Method: 提出TIDE框架：外层并行岛屿模型使用树编辑距离驱动结构多样性；内层循环集成LLM逻辑生成与差分变异算子进行参数调优；UCB调度器动态优先高收益提示策略优化资源分配。

Result: 在九个组合优化问题上的实验表明，TIDE发现的启发式算法在解质量上显著优于最先进基线，同时实现了改进的搜索效率和降低的计算成本。

Conclusion: TIDE通过解耦结构推理与参数优化，解决了现有方法在算法演化中的关键限制，为自动化启发式设计提供了更有效的框架。

Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.

</details>


### [36] [Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox](https://arxiv.org/abs/2601.21249)
*Enzo Nicolás Spotorno,Antônio Augusto Medeiros Fröhlich*

Main category: cs.AI

TL;DR: 论文提出HYDRA框架，采用模块化主权范式解决时间序列基础模型在安全关键CPS中的适应性问题，通过冻结的特定机制专家库和不确定性感知融合来避免灾难性遗忘并确保可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前通用时间序列基础模型在安全关键CPS中面临灾难性遗忘、频谱偏差残留、可验证性不足等问题，无法满足非平稳生命周期动态和严格可靠性要求。

Method: 提出模块化主权范式HYDRA：构建紧凑、冻结的特定机制专家库，通过不确定性感知融合机制组合这些专家，实现机制条件有效性、严格不确定性解耦和模块化可审计性。

Result: HYDRA框架能够避免灾难性遗忘，保持对先前机制的性能，正确处理高频不连续性特征，提供形式化验证和可追溯性，满足安全标准要求。

Conclusion: 全局参数更新无法解决可塑性-稳定性悖论，模块化主权范式为CPS生命周期中的鲁棒状态完整性提供了可验证路径，是安全关键系统部署的可行方案。

Abstract: The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term "HYDRA" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.

</details>


### [37] [Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving](https://arxiv.org/abs/2601.21288)
*Weitong Lian,Zecong Tang,Haoran Li,Tianjian Gao,Yifei Wang,Zixu Wang,Lingyi Meng,Tengju Ru,Zhejun Cui,Yichen Zhu,Hangshuo Cao,Qi Kang,Tianxing Chen,Yusen Qin,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: Drive-KD框架通过知识蒸馏将自动驾驶分解为感知-推理-规划三部分，使用层特定注意力作为蒸馏信号，构建单教师和多教师蒸馏，显著提升小模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型模型在自动驾驶任务中需要大量GPU内存和高推理延迟，而传统的监督微调难以弥补小模型的能力差距，需要更有效的知识转移方法。

Method: 将自动驾驶分解为感知、推理、规划三个能力维度；使用层特定注意力作为蒸馏信号构建单教师模型；引入多教师蒸馏框架和不对称梯度投影来缓解梯度冲突。

Result: 蒸馏后的InternVL3-1B模型比同系列78B预训练模型节省42倍GPU内存，吞吐量提高11.4倍，在DriveBench上整体性能更好，规划维度超越GPT-5.1。

Conclusion: Drive-KD框架通过能力分解和知识蒸馏，为高效自动驾驶视觉语言模型提供了有效途径，在保持性能的同时大幅降低资源需求。

Abstract: Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a "perception-reasoning-planning" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.

</details>


### [38] [White-Box Op-Amp Design via Human-Mimicking Reasoning](https://arxiv.org/abs/2601.21321)
*Zihao Chen,Jiayin Wang,Ziyi Sun,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AI

TL;DR: 提出White-Op框架，基于大语言模型智能体模拟人类推理，实现可解释的运放参数设计，通过假设-验证-决策工作流解决9种运放拓扑设计问题。


<details>
  <summary>Details</summary>
Motivation: 传统运放参数设计通常依赖黑盒优化方法，缺乏可解释性，难以理解设计决策背后的原理。需要一种模拟人类工程师推理过程、具有可解释性的设计框架。

Method: 提出假设-验证-决策工作流：1) 引入假设约束，推导符号可处理的极点和零点位置；2) 构建闭式数学优化问题并编程求解；3) 通过仿真验证；4) 理论-仿真结果分析指导决策优化。

Result: 在9种运放拓扑上测试，相比黑盒基线方法在5种拓扑中失败，White-Op实现可靠、可解释的行为级设计，理论预测误差仅8.52%，所有拓扑在晶体管级映射后功能保持。

Conclusion: White-Op成功模拟人类推理过程，提供可解释的运放参数设计框架，在多种拓扑中表现优于传统黑盒方法，代码已开源。

Abstract: This brief proposes \emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \emph{\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \emph{\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \textcolor{blue}{https://github.com/zhchenfdu/whiteop}.

</details>


### [39] [Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation](https://arxiv.org/abs/2601.21335)
*Yuzhe Chen,Jie Cao,Youquan Wang,Haicheng Tao,Darko B. Vukovic,Jia Wu*

Main category: cs.AI

TL;DR: CNRE模型结合神经符号推理与因果推断，用于可解释的多行为推荐，通过模拟人类决策过程生成去混淆的因果中介器。


<details>
  <summary>Details</summary>
Motivation: 现有多行为推荐方法牺牲可解释性追求性能，而可解释方法依赖外部信息导致泛化性有限。神经符号集成提供可解释性新途径，用户行为链本身包含适合显式推理的内生逻辑，但观测行为存在混淆因子导致虚假相关。

Method: 提出因果神经符号推理模型CNRE：1）分层偏好传播捕获异构跨行为依赖；2）基于偏好强度建模行为链内生逻辑规则；3）自适应分发到相应神经逻辑推理路径（合取、析取等）；4）生成近似理想状态的去混淆因果中介器。

Result: 在三个大规模数据集上的实验表明，CNRE显著优于最先进基线，在模型设计、决策过程和推荐结果三个层面提供多级可解释性。

Conclusion: CNRE成功将因果推断融入神经符号框架，通过模拟人类决策过程实现可解释的多行为推荐，有效解决混淆因子问题并提供多级可解释性。

Abstract: Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.

</details>


### [40] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 论文通过评估12个LLM在10个创意提示上的12000个样本，发现提示语对输出质量（原创性）的方差解释度为36.43%，与模型选择（40.94%）相当；但对输出数量（流畅度）的方差解释度仅为4.22%，模型选择和LLM内部方差占主导。提示语是调控输出质量的有力杠杆，但由于LLM内部方差较大（10-34%），单样本评估可能混淆采样噪声与真实的提示或模型效应。


<details>
  <summary>Details</summary>
Motivation: 研究动机是量化LLM输出方差中不同因素的贡献：提示语、模型选择和采样随机性。了解这些因素对输出质量和数量的相对影响，有助于更准确地评估LLM性能，避免单样本评估中的偏差。

Method: 方法：评估12个LLM在10个创意提示上，每个提示生成100个样本（总计12000个样本）。通过方差分析量化提示语、模型选择和采样随机性对输出方差（原创性和流畅度）的贡献。

Result: 结果：对于输出质量（原创性），提示语解释36.43%的方差，模型选择解释40.94%，两者贡献相当。对于输出数量（流畅度），模型选择解释51.25%的方差，LLM内部方差解释33.70%，提示语仅解释4.22%。LLM内部方差范围为10-34%。

Conclusion: 结论：提示语是调控LLM输出质量的有效工具，但对输出数量的影响有限。由于LLM内部方差显著，单样本评估可能错误地将采样噪声归因于提示语或模型效应，因此需要多样本评估以获得可靠结论。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [41] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: EHR-RAG：针对长时程电子健康记录设计的检索增强生成框架，通过事件和时间感知检索、自适应迭代检索及双路径证据推理，提升临床预测准确性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）包含丰富的纵向临床证据，但长时程EHRs常超出大语言模型（LLM）的上下文限制。现有方法（如截断或简单检索）会丢弃临床相关事件和时间依赖关系，影响预测准确性。

Method: 提出EHR-RAG框架，包含三个核心组件：1）事件和时间感知混合EHR检索，保留临床结构和时间动态；2）自适应迭代检索，逐步优化查询以扩大证据覆盖范围；3）双路径证据检索与推理，同时检索和推理事实与反事实证据。

Result: 在四个长时程EHR预测任务上的实验表明，EHR-RAG始终优于最强的LLM基线方法，平均Macro-F1提升了10.76%。

Conclusion: EHR-RAG展示了检索增强LLMs在结构化EHR数据临床预测中的潜力，通过专门设计的检索策略有效解决了长时程EHRs的上下文限制问题。

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [42] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 该论文提出了Ostrakon-VL，一个面向食品服务和零售商店场景的多模态大语言模型，通过ShopBench基准测试和QUAD数据清理流程，在FSRS任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在食品服务和零售商店场景中部署多模态大语言模型面临两大挑战：1) 现实世界FSRS数据噪声大且缺乏可审计的闭环数据清理，难以构建高质量训练语料；2) 现有评估协议缺乏统一的细粒度基准测试，难以客观衡量模型鲁棒性。

Method: 1) 基于Qwen3-VL-8B开发Ostrakon-VL模型；2) 创建首个面向FSRS的公开基准测试ShopBench；3) 提出QUAD多阶段多模态指令数据清理流程；4) 采用多阶段训练策略。

Result: Ostrakon-VL在ShopBench上平均得分60.1，在可比参数规模的开源MLLM中达到新SOTA。超越更大的Qwen3-VL-235B-A22B(59.4)0.7分，超越同规模Qwen3-VL-8B(55.3)4.8分，显示出显著改进的参数效率。

Conclusion: Ostrakon-VL在FSRS场景中提供了更鲁棒可靠的感知和决策能力。为促进可重复研究，将公开发布Ostrakon-VL模型和ShopBench基准测试。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [43] [Dynamic Framework for Collaborative Learning: Leveraging Advanced LLM with Adaptive Feedback Mechanisms](https://arxiv.org/abs/2601.21344)
*Hassam Tahir,Faizan Faisal,Fady Alnajjar,Muhammad Imran Taj,Lucia Gordon,Aila Khan,Michael Lwin,Omar Mubin*

Main category: cs.AI

TL;DR: 提出一个将大型语言模型整合到协作学习平台的框架，通过AI动态主持实时讨论，提升学生参与度、批判性思维和包容性。


<details>
  <summary>Details</summary>
Motivation: 现有协作学习平台存在静态主持和个性化不足的问题，无法有效适应学习者动态需求，限制了学生参与度和学习效果。

Method: 采用模块化架构：ReactJS前端、Flask后端，集成先进LLM作为动态主持人，配备强大的反馈机制和高效问题检索系统，动态调整提示和讨论流程。

Result: 测试表明该框架显著改善学生协作，促进更深层次理解，在不同学科和用户群体中都能有效扩展，平衡参与度并提升学习效果。

Conclusion: 该工作为下一代AI驱动教育工具奠定坚实基础，通过解决现有系统局限性，推进公平且有影响力的学习成果。

Abstract: This paper presents a framework for integrating LLM into collaborative learning platforms to enhance student engagement, critical thinking, and inclusivity. The framework employs advanced LLMs as dynamic moderators to facilitate real-time discussions and adapt to learners' evolving needs, ensuring diverse and inclusive educational experiences. Key innovations include robust feedback mechanisms that refine AI moderation, promote reflective learning, and balance participation among users. The system's modular architecture featuring ReactJS for the frontend, Flask for backend operations, and efficient question retrieval supports personalized and engaging interactions through dynamic adjustments to prompts and discussion flows. Testing demonstrates that the framework significantly improves student collaboration, fosters deeper comprehension, and scales effectively across various subjects and user groups. By addressing limitations in static moderation and personalization in existing systems, this work establishes a strong foundation for next-generation AI-driven educational tools, advancing equitable and impactful learning outcomes.

</details>


### [44] [BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents](https://arxiv.org/abs/2601.21352)
*Ziyu Lu,Tengjin Weng,Yiying Yang,Yuhang Zhao,Xinxin Huang,Wenhao Jiang*

Main category: cs.AI

TL;DR: BEAP-Agent：基于DFS的GUI代理框架，支持长距离多级状态回溯，解决现有GUI代理探索路径错误后无法恢复的问题，在OSWorld基准测试中达到28.2%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在执行重复任务时，一旦探索路径出错就难以恢复，导致任务失败。需要一种系统化的回溯机制来解决这个问题。

Method: 将GUI任务执行建模为DFS过程，提出BEAP-Agent框架，包含三个协作组件：规划器（Planner）、执行器（Executor）和跟踪器（Tracker），支持动态任务跟踪和更新的长距离多级状态回溯。

Result: 在OSWorld基准测试中，BEAP-Agent达到了28.2%的准确率，验证了所提方法的有效性。

Conclusion: BEAP-Agent填补了GUI代理系统化回溯机制的空白，为长视野任务探索提供了系统化解决方案。

Abstract: GUI agents are designed to automate repetitive tasks and enhance productivity. However, existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. In this work, we model GUI task execution as a DFS process and propose BEAP-Agent, a DFS-based framework that supports long-range, multi-level state backtracking with dynamic task tracking and updating. The framework consists of three collaborative components: Planner, Executor, and Tracker. Together, they enable effective task exploration and execution. BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, offering a systematic solution for long-horizon task exploration. We conducted a systematic evaluation on the OSWorld benchmark, where BEAP-Agent achieved an accuracy of 28.2%, validating the effectiveness of the proposed method.

</details>


### [45] [Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization](https://arxiv.org/abs/2601.21358)
*Jiecong Wang,Hao Peng,Chunyang Liu*

Main category: cs.AI

TL;DR: PLaT框架通过将潜在推理重新定义为规划，将推理与语言表达解耦，使用潜在规划状态进行确定性推理，并通过解码器在需要时转换为文本，实现动态终止推理而非依赖固定超参数。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法通常作为从显式推理步骤到潜在状态的不透明端到端映射，且推理时需要预定义潜在步骤数量。这些方法在计算成本和推理路径崩溃方面存在限制，需要更透明和可扩展的解决方案。

Method: 提出PLaT框架，将推理建模为潜在规划状态的确定性轨迹，同时使用独立的解码器在必要时将这些思想转化为文本。这种解耦使模型能够动态确定何时终止推理，而不是依赖固定超参数。

Result: 在数学基准测试中，PLaT虽然贪心准确率低于基线，但展现出更好的推理多样性可扩展性。这表明PLaT学习到了更鲁棒、更广泛的解空间，为推理时搜索提供了透明且可扩展的基础。

Conclusion: PLaT通过将推理与语言表达解耦，提供了一种透明且可扩展的潜在推理方法，能够动态控制推理过程，学习更广泛的解空间，为推理时搜索奠定了良好基础。

Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.

</details>


### [46] [Hebbian Learning with Global Direction](https://arxiv.org/abs/2601.21367)
*Wenjia Hua,Kejie Zhao,Luziwei Leng,Ran Cheng,Yuxin Ma,Qinghai Guo*

Main category: cs.AI

TL;DR: 提出GHL框架，结合局部Hebbian学习与全局指导信号，缩小与反向传播的差距


<details>
  <summary>Details</summary>
Motivation: 反向传播缺乏生物合理性且计算成本高，而纯Hebbian学习仅依赖局部信息，无法考虑全局任务目标，限制了其扩展性

Method: 提出模型无关的全局指导Hebbian学习框架：局部组件使用Oja规则和竞争学习确保稳定更新；全局组件引入基于符号的信号指导局部Hebbian可塑性更新方向

Result: 在ImageNet等复杂数据集和大规模网络上取得有竞争力的结果，显著缩小与标准反向传播的差距，优于现有Hebbian方法

Conclusion: GHL框架通过整合局部和全局信息，成功扩展了Hebbian学习到大规模网络和复杂任务，为生物合理的深度学习提供了有前景的替代方案

Abstract: Backpropagation algorithm has driven the remarkable success of deep neural networks, but its lack of biological plausibility and high computational costs have motivated the ongoing search for alternative training methods. Hebbian learning has attracted considerable interest as a biologically plausible alternative to backpropagation. Nevertheless, its exclusive reliance on local information, without consideration of global task objectives, fundamentally limits its scalability. Inspired by the biological synergy between neuromodulators and local plasticity, we introduce a novel model-agnostic Global-guided Hebbian Learning (GHL) framework, which seamlessly integrates local and global information to scale up across diverse networks and tasks. In specific, the local component employs Oja's rule with competitive learning to ensure stable and effective local updates. Meanwhile, the global component introduces a sign-based signal that guides the direction of local Hebbian plasticity updates. Extensive experiments demonstrate that our method consistently outperforms existing Hebbian approaches. Notably, on large-scale network and complex datasets like ImageNet, our framework achieves the competitive results and significantly narrows the gap with standard backpropagation.

</details>


### [47] [NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents](https://arxiv.org/abs/2601.21372)
*Yang Song,Anoushka Vyas,Zirui Wei,Sina Khoshfetrat Pakazad,Henrik Ohlsson,Graham Neubig*

Main category: cs.AI

TL;DR: NEMO是一个将自然语言决策问题描述转换为可执行数学优化实现的系统，通过远程自治编码代理实现，确保代码可执行性，并在多个优化基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖专用大型语言模型或特定任务代理，通常脆弱、复杂，经常生成语法无效或不可执行的代码。需要一种更可靠、可执行的方法来自动化优化建模。

Method: NEMO以远程自治编码代理为核心抽象，构建高层系统来结构化、整合和迭代优化任务规范。通过沙盒环境确保代码可执行性，引入非对称验证循环、外部记忆、最小贝叶斯风险解码和自一致性等协调模式。

Result: 在九个已建立的优化基准测试中，NEMO在大多数任务上实现了最先进的性能，在多个数据集上具有显著优势，证明了执行感知代理架构在自动化优化建模中的有效性。

Conclusion: NEMO展示了通过远程自治编码代理和协调模式构建执行感知代理架构的强大能力，为自动化优化建模提供了可靠且高效的解决方案。

Abstract: In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.
  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.
  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.

</details>


### [48] [TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models](https://arxiv.org/abs/2601.21375)
*Zheng Li,Siyao Song,Jingyuan Ma,Rui Li,Ying Zeng,Minghao Li,Zhifang Sui*

Main category: cs.AI

TL;DR: 该论文提出了一个基于教学大纲的评估框架，用于衡量大语言模型通过多轮指导提升学生表现的教学能力，并在高考数据上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为教学助手的潜力被广泛认可，但对其教学能力的评估不足。现有基准主要关注问题解决或问题级指导，而知识中心的教学能力评估尚未充分探索。

Method: 提出了一个教学大纲基础的评估框架，通过限制教师代理使用结构化知识点和示例问题来避免信息泄露，并重用现有基准。在高考数据上实例化该框架，评估多轮指导后学生表现的提升。

Result: 实验显示不同模型和学科的教学效果存在显著差异：某些模型在数学上表现良好，但在物理和化学教学中仍具挑战性。同时发现加入示例问题不一定能改善教学，因为模型往往会转向示例特定的错误纠正。

Conclusion: 教学能力是大语言模型行为的一个独特且可测量的维度，该研究为评估LLM教学能力提供了系统框架，并揭示了当前模型在教学方面的局限性。

Abstract: Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.

</details>


### [49] [DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis](https://arxiv.org/abs/2601.21403)
*Ruyi Qi,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: DataCross是一个用于跨模态数据分析的基准测试和协作代理框架，旨在解决结构化数据与视觉文档中"僵尸数据"的整合问题，通过专门子代理和结构化工作流程实现跨模态分析。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据科学和企业决策中，关键信息往往分散在可直接查询的结构化数据源（如SQL、CSV）和锁定在非结构化视觉文档（如扫描报告、发票图像）中的"僵尸数据"之间。现有的数据分析代理主要局限于处理结构化数据，无法激活和关联这些高价值的视觉信息，与工业需求存在显著差距。

Method: 提出了DataCrossBench基准测试（包含200个端到端分析任务）和DataCrossAgent框架。基准通过人机协作的反向合成流水线构建，确保真实复杂性、跨源依赖性和可验证的真实结果。代理框架采用"分而治之"的工作流程，包含专门子代理（每个代理专注于特定数据源）和结构化工作流程：源内深度探索、关键源识别和上下文交叉融合。采用新颖的reReAct机制实现稳健的代码生成和调试以进行事实验证。

Result: 实验结果显示，DataCrossAgent在事实性方面比GPT-4o提高了29.7%，在高难度任务上表现出卓越的鲁棒性，能够有效激活碎片化的"僵尸数据"进行深入的跨模态分析。

Conclusion: DataCross通过创新的基准测试和协作代理框架，成功解决了跨模态数据分析的挑战，特别是激活和整合视觉文档中的"僵尸数据"，为企业决策提供了更全面的数据分析能力。

Abstract: In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and "zombie data" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the "divide-and-conquer" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented "zombie data" for insightful, cross-modal analysis.

</details>


### [50] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: DAMI框架通过动态参数插值在直觉系统1和深思系统2之间自适应切换，实现能力控制而非输出控制，在数学推理任务上达到更高准确率同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注输出控制（限制模型输出长度），但这只是认知配置的症状而非根本原因。需要转向能力控制，调节模型如何思考而非产生什么，以更好地结合系统1的效率和系统2的推理深度。

Method: 提出DAMI框架：1）利用现有Instruct和Thinking检查点通过动态参数插值，无需额外训练；2）基于查询估计推理强度λ(q)来配置认知深度；3）开发偏好学习方法编码准确性和效率标准；4）引入基于置信度的零样本部署方法利用模型间认知差异。

Result: 在五个数学推理基准测试中，DAMI比Thinking模型达到更高准确率，同时保持效率，有效结合了系统1的效率和系统2的推理深度。

Conclusion: 能力控制优于输出控制，动态参数插值能有效在直觉和深思认知模式间自适应切换，实现准确性和效率的平衡。

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [51] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: 研究发现许多大语言模型将否定指令误解为肯定指令，在伦理场景中开放模型对否定指令的违规行为支持率达77%-100%，商业模型也存在显著波动，表明当前对齐技术与安全部署要求存在差距。


<details>
  <summary>Details</summary>
Motivation: 当用户告诉AI系统某人"不应该"采取某个行动时，系统应将其视为禁令。然而许多大语言模型却相反地将否定指令解释为肯定指令，这在高风险决策场景中可能带来严重后果。

Method: 审计了16个模型在14个伦理场景中的表现，比较了简单否定和复合否定条件下的模型行为，使用确定性解码排除采样噪声，提出了否定敏感指数(NSI)作为治理指标。

Result: 开源模型在简单否定下对禁止行为的支持率为77%，复合否定下达到100%（比肯定框架增加317%）。商业模型波动为19-128%。模型间一致性从肯定提示的74%降至否定提示的62%。金融场景的脆弱性是医疗场景的两倍。

Conclusion: 当前对齐技术实现与安全部署要求存在差距，无法可靠区分"做X"和"不做X"的模型不应在高风险环境中做出自主决策。提出了分级认证框架和领域特定阈值作为治理方案。

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [52] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: LLMs表现出"鲁棒性悖论"：虽然对提示词微小变化敏感，但在规则约束的决策中却对情感框架效应表现出惊人的稳定性，抵抗叙事操纵的能力比人类强110-300倍。


<details>
  <summary>Details</summary>
Motivation: 虽然已知LLMs对提示词微小扰动敏感且容易迎合用户偏见，但它们在重要规则约束决策中的鲁棒性尚未得到充分探索。研究者想要探究LLMs在情感框架效应下的表现是否与其已知的脆弱性一致。

Method: 使用新颖的受控扰动框架，在医疗、法律和金融三个高风险领域进行实验。量化了LLMs与人类在叙事操纵下的鲁棒性差距，测量了效应大小（Cohen's h），并比较了不同训练范式的模型表现。

Result: LLMs表现出惊人的行为不变性：模型效应大小接近零（Cohen's h = 0.003），而人类表现出显著的偏见（Cohen's h在[0.3, 0.8]之间）。LLMs抵抗叙事操纵的能力比人类强110-300倍，这种不变性在不同训练范式的模型中持续存在。

Conclusion: 指令调优的LLMs能够将逻辑规则遵守与说服性叙事解耦，提供了一种决策稳定性来源，可以补充甚至可能去偏人类判断。虽然LLMs对查询格式"脆弱"，但对决策偏见原因"稳定"。研究发布了162个场景的基准测试、代码和数据。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [53] [ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design](https://arxiv.org/abs/2601.21448)
*Zhongkai Yu,Chenyang Zhou,Yichen Lin,Hejia Zhang,Haotian Ye,Junxia Cui,Zaifeng Pan,Jishen Zhao,Yufei Ding*

Main category: cs.AI

TL;DR: 提出了一个全面的AI辅助芯片设计基准测试ChipBench，用于评估LLM在Verilog生成、调试和参考模型生成三个关键任务上的表现，发现现有SOTA模型在这些真实工业任务上表现远低于现有饱和基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前硬件工程领域的LLM基准测试存在饱和问题和任务多样性不足，无法反映LLM在真实工业工作流程中的实际性能，需要更全面、更具挑战性的评估框架。

Method: 构建了包含44个具有复杂层次结构的真实模块、89个系统化调试案例以及132个跨Python、SystemC和CXXRTL的参考模型样本的综合基准测试，并开发了自动化工具箱用于高质量训练数据生成。

Result: 评估结果显示性能差距显著：最先进的Claude-4.5-opus在Verilog生成任务上仅达到30.74%，在Python参考模型生成上仅13.33%，远低于现有饱和基准测试中SOTA模型超过95%的通过率。

Conclusion: 该基准测试揭示了LLM在真实硬件工程任务中的重大挑战，为未来研究提供了重要的评估框架和数据生成工具，有助于推动AI辅助芯片设计领域的发展。

Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\% on Verilog generation and 13.33\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.

</details>


### [54] [LION: A Clifford Neural Paradigm for Multimodal-Attributed Graph Learning](https://arxiv.org/abs/2601.21453)
*Xunkai Li,Zhengyu Wu,Zekai Chen,Henan Sun,Daohan Su,Guang Zeng,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.AI

TL;DR: LION提出基于Clifford代数和解耦图神经范式的多模态图对齐-融合框架，通过几何感知流形实现模态交互对齐，自适应全息聚合提升融合效果，在多种下游任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图学习方法存在两个主要问题：1) 模态对齐时忽略图上下文，使用拓扑约束或模态特定算子作为标记器，抑制模态交互；2) 模态融合缺乏适应性，简单适应双模态图，未能充分利用对齐标记的拓扑先验，导致泛化性差和性能下降。

Method: 基于Clifford代数和解耦图神经范式（传播-聚合），实现多模态属性图的对齐-融合。首先构建基于Clifford代数的模态感知几何流形，通过几何诱导的高阶图传播实现模态交互对齐；然后基于对齐标记的几何等级特性，提出自适应全息聚合模块，将几何等级的能量和尺度与可学习参数结合以改进模态融合。

Result: 在9个数据集上的广泛实验表明，LION在3个图下游任务和3个模态下游任务中显著优于当前最先进的基线方法。

Conclusion: LION通过Clifford代数构建的几何感知流形和自适应全息聚合，有效解决了多模态图学习中的对齐和融合问题，提升了多模态图表示学习的性能。

Abstract: Recently, the rapid advancement of multimodal domains has driven a data-centric paradigm shift in graph ML, transitioning from text-attributed to multimodal-attributed graphs. This advancement significantly enhances data representation and expands the scope of graph downstream tasks, such as modality-oriented tasks, thereby improving the practical utility of graph ML. Despite its promise, limitations exist in the current neural paradigms: (1) Neglect Context in Modality Alignment: Most existing methods adopt topology-constrained or modality-specific operators as tokenizers. These aligners inevitably neglect graph context and inhibit modality interaction, resulting in suboptimal alignment. (2) Lack of Adaptation in Modality Fusion: Most existing methods are simple adaptations for 2-modality graphs and fail to adequately exploit aligned tokens equipped with topology priors during fusion, leading to poor generalizability and performance degradation. To address the above issues, we propose LION (c\underline{LI}ff\underline{O}rd \underline{N}eural paradigm) based on the Clifford algebra and decoupled graph neural paradigm (i.e., propagation-then-aggregation) to implement alignment-then-fusion in multimodal-attributed graphs. Specifically, we first construct a modality-aware geometric manifold grounded in Clifford algebra. This geometric-induced high-order graph propagation efficiently achieves modality interaction, facilitating modality alignment. Then, based on the geometric grade properties of aligned tokens, we propose adaptive holographic aggregation. This module integrates the energy and scale of geometric grades with learnable parameters to improve modality fusion. Extensive experiments on 9 datasets demonstrate that LION significantly outperforms SOTA baselines across 3 graph and 3 modality downstream tasks.

</details>


### [55] [Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance](https://arxiv.org/abs/2601.21465)
*Márton Kardos*

Main category: cs.AI

TL;DR: Topeax是一种新的主题建模方法，通过密度估计峰值确定聚类数量，结合词汇和语义指标选择高质量关键词，在聚类恢复和描述方面优于Top2Vec和BERTopic，且对样本大小和超参数变化更稳定。


<details>
  <summary>Details</summary>
Motivation: 当前流行的主题建模方法Top2Vec和BERTopic存在两个主要问题：1）对样本大小和超参数极度敏感，难以发现自然聚类；2）关键词重要性估计方法存在缺陷（BERTopic忽略语义距离，Top2Vec忽略词频），导致主题不连贯或缺乏多样性。

Method: 提出Topeax方法：1）通过密度估计峰值自动发现聚类数量；2）结合词汇统计（词频）和语义指标（与主题向量的距离）来评估术语重要性，生成高质量主题关键词。

Result: Topeax在聚类恢复和聚类描述方面均优于Top2Vec和BERTopic，同时对样本大小和超参数变化表现出更稳定的行为，减少了异常波动。

Conclusion: Topeax解决了现有主题建模方法的关键缺陷，提供了更可靠、更稳定的聚类发现和主题描述能力，是文本聚类主题建模的改进方案。

Abstract: Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.

</details>


### [56] [MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning](https://arxiv.org/abs/2601.21468)
*Yaorui Shi,Shugui Liu,Yu Yang,Wenyu Mao,Yuxin Chen,Qi GU,Hui Su,Xunliang Cai,Xiang Wang,An Zhang*

Main category: cs.AI

TL;DR: MemOCR：一种多模态记忆代理，通过视觉布局自适应分配记忆空间，在有限上下文预算下提升长时程推理能力


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统通常将历史序列化为文本，其中令牌级成本均匀且随长度线性增长，经常在低价值细节上浪费稀缺的上下文预算。长时程智能推理需要将不断增长的交互历史有效压缩到有限的上下文窗口中。

Method: MemOCR维护结构化富文本记忆（如标题、高亮），并将其渲染为图像供代理查阅，通过视觉布局优先显示关键证据，同时积极压缩辅助细节。使用强化学习在预算感知目标下训练，使代理适应不同压缩级别。

Result: 在长上下文多跳和单跳问答基准测试中，MemOCR优于基于文本的基线方法，在极端预算下实现了更有效的上下文利用。

Conclusion: MemOCR通过视觉布局实现自适应信息密度分配，为长时程推理中的记忆压缩提供了有效解决方案，在有限上下文预算下显著提升了推理性能。

Abstract: Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.

</details>


### [57] [ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management](https://arxiv.org/abs/2601.21473)
*Zaifeng Pan,Yipeng Shen,Zhengding Hu,Zhuang Wang,Aninda Manocha,Zheng Wang,Zhongkai Yu,Yue Guan,Yufei Ding*

Main category: cs.AI

TL;DR: ScaleSim：基于LLM的大规模多智能体模拟内存优化系统，通过智能预取和优先级驱逐实现1.74倍加速


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体模拟面临GPU内存压力大的问题，每个智能体都需要维护私有GPU状态（模型、前缀缓存、适配器等），随着智能体数量增加，设备内存迅速耗尽

Method: 提出"调用距离"统一抽象来估计智能体未来LLM请求的相对顺序，基于此实现ScaleSim系统，支持主动预取、基于优先级的驱逐，并通过模块化接口支持多样化的智能体特定内存

Result: 在模拟基准测试中，相比SGLang实现了最高1.74倍的加速

Conclusion: ScaleSim通过创新的内存管理策略有效解决了大规模多智能体模拟中的GPU内存瓶颈问题

Abstract: LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.

</details>


### [58] [The Path of Least Resistance: Guiding LLM Reasining Trajectories with Prefix Consensus](https://arxiv.org/abs/2601.21494)
*Ishan Jindal,Sai Prashanth Akuthota,Jayant Taneja,Sachin Dev Sharma*

Main category: cs.AI

TL;DR: PoLR是一种基于前缀一致性的推理优化方法，通过聚类推理轨迹的短前缀、识别主导聚类并扩展该聚类中的所有路径，在保持Self-Consistency准确性的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然推理能力强，但Self-Consistency等推理策略计算成本高，需要完全扩展所有推理轨迹。需要一种既能保持准确性又能显著降低计算开销的推理时优化方法。

Method: PoLR通过聚类推理轨迹的短前缀，识别主导聚类，然后仅扩展该聚类中的所有路径。基于互信息和熵的理论分析解释了早期推理步骤如何编码预测最终正确性的强信号。

Result: 在GSM8K、MATH500、AIME24/25和GPQA-DIAMOND等基准测试中，PoLR始终匹配或超过Self-Consistency的性能，同时减少高达60%的token使用量和50%的延迟时间。

Conclusion: PoLR是一种高效且可扩展的推理优化方法，与自适应推理方法完全互补，可作为前置过滤器，在不需微调的情况下显著提高Self-Consistency的效率。

Abstract: Large language models achieve strong reasoning performance, but inference strategies such as Self-Consistency (SC) are computationally expensive, as they fully expand all reasoning traces. We introduce PoLR (Path of Least Resistance), the first inference-time method to leverage prefix consistency for compute-efficient reasoning. PoLR clusters short prefixes of reasoning traces, identifies the dominant cluster, and expands all paths in that cluster, preserving the accuracy benefits of SC while substantially reducing token usage and latency. Our theoretical analysis, framed via mutual information and entropy, explains why early reasoning steps encode strong signals predictive of final correctness. Empirically, PoLR consistently matches or exceeds SC across GSM8K, MATH500, AIME24/25, and GPQA-DIAMOND, reducing token usage by up to 60% and wall-clock latency by up to 50%. Moreover, PoLR is fully complementary to adaptive inference methods (e.g., Adaptive Consistency, Early-Stopping SC) and can serve as a drop-in pre-filter, making SC substantially more efficient and scalable without requiring model fine-tuning.

</details>


### [59] [MAR: Efficient Large Language Models via Module-aware Architecture Refinement](https://arxiv.org/abs/2601.21503)
*Junhong Cai,Guiqin Wang,Kejie Zhao,Jianxiong Tang,Xiang Wang,Luziwei Leng,Ran Cheng,Yuxin Ma,Qinghai Guo*

Main category: cs.AI

TL;DR: MAR框架通过集成状态空间模型实现线性时间序列建模，并应用激活稀疏化降低FFN成本，同时设计了ATMN神经元和SBDS策略来解决SNN与SSM集成问题，显著降低LLM推理能耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在各领域表现出色，但由于二次注意力机制和密集前馈网络操作导致能耗过高，需要开发更高效的架构来降低推理成本。

Method: 提出两阶段模块感知架构精炼框架：1）集成状态空间模型实现线性时间序列建模；2）应用激活稀疏化减少前馈网络成本。同时设计自适应三元多步神经元和脉冲感知双向蒸馏策略来解决SNN与SSM集成问题。

Result: 实验表明MAR在资源受限条件下有效恢复了密集模型的性能，同时显著降低推理能耗，并且优于规模相当甚至更大的高效模型。

Conclusion: MAR框架展示了构建高效实用大型语言模型的潜力，通过创新的架构设计和优化策略实现了性能与能耗的良好平衡。

Abstract: Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs.

</details>


### [60] [The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation](https://arxiv.org/abs/2601.21505)
*Diaoulé Diallo,Katharina Dworatzyk,Sophie Jentzsch,Peer Schütt,Sabine Theis,Tobias Hecking*

Main category: cs.AI

TL;DR: 该研究通过人类评估验证了激活引导技术能有效控制LLM输出的情感语调，发现中等强度引导能可靠增强目标情感同时保持文本质量，且人类与模型评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 需要一种轻量级方法来控制大型语言模型在推理时的行为，使其输出符合人类期望和安全要求。激活引导作为提示工程和微调的替代方案，直接修改内部激活来引导生成，但之前缺乏人类评估验证其实际效果。

Method: 首次对激活引导进行人类评估，收集了190名参与者的7,000多个众包评分，评估LLM输出的情感强度和文本质量。比较了人类评分与模型评分的一致性，测试了不同情感类型（厌恶、恐惧、惊讶等）在不同引导强度下的效果，并对比了Alpaca和LlaMA-3模型的表现。

Result: 人类与模型质量评分高度一致（平均r=0.776）；中等引导强度（λ≈0.15）能可靠增强目标情感同时保持可理解性；对厌恶（η_p²=0.616）和恐惧（η_p²=0.540）效果最强，对惊讶效果最弱（η_p²=0.042）；LlaMA-3比Alpaca表现更一致；评分者间信度高（ICC=0.71-0.87）。

Conclusion: 激活引导是一种可扩展的方法，能有效控制LLM在情感维度上的行为。人类评估验证了其实际效果，为轻量级模型控制提供了实证支持。

Abstract: Controlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$--$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($λ\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($η_p^2 = 0.616$) and fear ($η_p^2 = 0.540$), and minimal effects for surprise ($η_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p < 0.001$). Inter-rater reliability was high (ICC $= 0.71$--$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.

</details>


### [61] [LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI](https://arxiv.org/abs/2601.21511)
*Niki van Stein,Anna V. Kononova,Lars Kotthoff,Thomas Bäck*

Main category: cs.AI

TL;DR: LLaMEA-SAGE：通过从生成算法的抽象语法树中提取图论和复杂度特征，构建基于代理模型的反馈机制，指导LLM驱动的自动化算法设计，相比原始LLaMEA更快达到相同性能，并在大规模实验中优于现有AAD方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动化算法设计框架（如LLaMEA）主要依赖适应度反馈驱动搜索，但忽略了生成代码本身包含的大量结构信息。如何利用代码结构特征来指导算法设计过程，提高搜索效率和质量，是本文要解决的核心问题。

Method: 提出LLaMEA-SAGE方法：1）从生成算法的抽象语法树中提取图论和复杂度特征；2）基于已评估解决方案档案学习代理模型；3）使用可解释AI技术识别对性能有显著影响的特征；4）将这些特征转化为自然语言突变指令，指导后续LLM代码生成而不限制表达能力。

Result: 1）在小规模控制实验中，LLaMEA-SAGE比原始LLaMEA更快达到相同性能水平；2）在GECCO-MA-BBOB竞赛的MA-BBOB套件上进行的大规模实验中，该方法优于现有的自动化算法设计方法；3）证明了从代码中提取的信号能有效偏置LLM驱动的算法演化。

Conclusion: 通过利用代码结构特征构建反馈机制，可以显著提升LLM驱动的自动化算法设计效率。该方法成功弥合了代码结构与人类可理解的性能反馈之间的差距，为自动化算法设计提供了新的指导范式。

Abstract: Large language models have enabled automated algorithm design (AAD) by generating optimization algorithms directly from natural-language prompts. While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated code unused. We propose a mechanism for guiding AAD using feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms, based on a surrogate model learned over an archive of evaluated solutions. Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity.
  We propose LLaMEA-SAGE, which integrates this feature-driven guidance into LLaMEA, and evaluate it across several benchmarks. We show that the proposed structured guidance achieves the same performance faster than vanilla LLaMEA in a small controlled experiment. In a larger-scale experiment using the MA-BBOB suite from the GECCO-MA-BBOB competition, our guided approach achieves superior performance compared to state-of-the-art AAD methods. These results demonstrate that signals derived from code can effectively bias LLM-driven algorithm evolution, bridging the gap between code structure and human-understandable performance feedback in automated algorithm design.

</details>


### [62] [KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization](https://arxiv.org/abs/2601.21526)
*Alireza Nadaf,Alireza Mohammadshahi,Majid Yazdani*

Main category: cs.AI

TL;DR: KAPSO是一个用于自主程序合成与优化的模块化框架，通过迭代的构思、代码合成与编辑、执行、评估和学习循环，将合成作为长期优化循环中的操作符而非终点。


<details>
  <summary>Details</summary>
Motivation: 针对编码智能体中常见的长期失败问题，包括实验状态丢失、调试脆弱性、领域专业知识复用能力弱等，需要一种能够持续优化可运行工件的系统。

Method: KAPSO包含三个紧密耦合的组件：1) git原生实验引擎，将每次尝试隔离为分支；2) 知识系统，整合异构数据源并组织为结构化表示；3) 认知记忆层，协调检索并维护可复用经验教训的存储。

Result: 在MLE-Bench（Kaggle风格ML竞赛）和ALE-Bench（AtCoder启发式优化）上进行了评估，并报告了端到端性能表现。

Conclusion: KAPSO通过将合成作为优化循环中的操作符，结合实验管理、知识整合和认知记忆，有效解决了编码智能体的长期失败问题，实现了可度量的目标优化。

Abstract: We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso

</details>


### [63] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: ARGORA框架将多专家LLM讨论组织成显式论证图，通过因果分析识别关键推理链，并提供校正机制来对齐内部推理与外部判断。


<details>
  <summary>Details</summary>
Motivation: 现有多专家LLM系统通过简单聚合收集不同观点，但掩盖了哪些论证驱动了最终决策。需要一种能明确展示论证结构并能分析因果关系的框架。

Method: 1) 将多专家讨论组织成显式论证图，展示论证间的支持/攻击关系；2) 将论证图转化为因果模型，通过系统性移除单个论证并重新计算结果来识别关键推理链；3) 引入校正机制，当内部推理与外部判断不一致时进行对齐。

Result: 在多样化基准测试和开放式用例中，ARGORA实现了有竞争力的准确性，并展现出校正行为：当专家初始意见不一致时，框架更倾向于将争议解决为正确答案而非引入新错误，同时提供关键论证的因果诊断。

Conclusion: ARGORA通过将多专家讨论组织成显式论证图并进行因果分析，不仅提高了决策透明度，还能识别关键推理链和提供校正机制，为LLM系统的可解释性和可靠性提供了新方法。

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [64] [ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory](https://arxiv.org/abs/2601.21545)
*Yang Zhao,Chengxiao Dai,Yue Xiu,Mengying Kou,Yuliang Zheng,Dusit Niyato*

Main category: cs.AI

TL;DR: ShardMemo是一个面向智能体LLM系统的分层内存服务，通过分片存储、范围优先路由和成本感知门控机制，解决了大规模并行访问下的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着智能体LLM系统中内存容量和并行访问的增长，传统的集中式索引和启发式分区方法成为性能瓶颈，需要更高效的内存管理方案。

Method: 提出三层内存架构：A层（每个智能体的工作状态）、B层（分片证据存储，采用分片本地ANN索引和范围优先路由）、C层（版本化技能库）。B层通过掩码MoE路由进行分片探测，使用成本感知门控机制。

Result: 在LoCoMo上比最强基线(GAM)提升5.11-6.82 F1；在固定预算路由下比余弦原型路由提升6.87 F1，同时减少20.5%检索工作和降低延迟；在长上下文HotpotQA上表现优异；在ToolBench上C层达到0.97 Precision@3和1.94 StepRed。

Conclusion: ShardMemo通过分层分片内存架构和智能路由机制，有效解决了智能体LLM系统中的内存扩展和并行访问瓶颈，在多个基准测试中表现出显著性能优势。

Abstract: Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\mathrm{probe}}$ shards via Top-$B_{\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).

</details>


### [65] [Meta Context Engineering via Agentic Skill Evolution](https://arxiv.org/abs/2601.21557)
*Haoran Ye,Xuning He,Vincent Arak,Haonan Dong,Guojie Song*

Main category: cs.AI

TL;DR: 提出Meta Context Engineering (MCE)框架，通过双层代理协同进化上下文工程技能和上下文工件，取代静态启发式方法，在多个领域实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前上下文工程方法依赖人工设计的固定流程和预定义模式，存在结构偏见且优化空间受限，需要更灵活、自适应的上下文优化框架。

Method: 提出双层框架MCE：元级代理通过"代理交叉"在技能历史、执行和评估中进行审慎搜索来精炼工程技能；基础级代理执行这些技能，从训练rollouts中学习，并优化上下文为灵活的文件和代码。

Result: 在五个不同领域的离线和在线设置中评估，MCE相比最先进的代理上下文工程方法获得5.6-53.8%的相对改进（平均16.9%），同时在上下文适应性、可迁移性、使用效率和训练效率方面表现优越。

Conclusion: MCE框架通过协同进化技能和上下文工件，超越了静态启发式方法，为上下文工程提供了更灵活、高效且可扩展的解决方案。

Abstract: The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.

</details>


### [66] [EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots](https://arxiv.org/abs/2601.21570)
*Zixing Lei,Genjia Liu,Yuanshuo Zhang,Qipeng Liu,Chuan Wen,Shanghang Zhang,Wenzhao Lian,Siheng Chen*

Main category: cs.AI

TL;DR: EmboCoach-Bench：评估LLM智能体自主设计机器人策略的基准，在32个任务上证明自主智能体平均成功率比人工基线高26.5%


<details>
  <summary>Details</summary>
Motivation: Embodied AI向通用机器人系统快速发展，但当前方法严重依赖人工监督（奖励设计、超参数调优等），限制了规模化能力。受LLM在软件自动化和科学发现中成功的启发，需要评估LLM智能体自主设计机器人策略的能力。

Method: 提出EmboCoach-Bench基准，包含32个专家策划的RL和IL任务，以可执行代码作为通用接口。评估动态闭环工作流程：智能体利用环境反馈迭代地起草、调试和优化解决方案，涵盖从物理感知奖励设计到扩散策略等策略架构。

Result: 1) 自主智能体平均成功率比人工设计的基线高26.5%；2) 带有环境反馈的智能体工作流程有效加强策略开发，显著缩小开源与专有模型间的性能差距；3) 智能体展示自我修正能力，通过迭代仿真调试从近乎完全失败中恢复任务性能。

Conclusion: 这项工作为自演化具身智能奠定了基础，加速了从劳动密集型人工调优向可扩展自主工程的范式转变，推动Embodied AI领域向更自动化的方向发展。

Abstract: The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.

</details>


### [67] [Chain Of Thought Compression: A Theoritical Analysis](https://arxiv.org/abs/2601.21576)
*Juncai Li,Ru Li,Yuxiang Zhou,Boxiang Ma,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 本文首次理论分析了学习内化中间推理步骤的难度，证明高阶逻辑依赖的学习信号会指数衰减，并提出ALiCoT框架通过对齐潜在token分布与中间推理状态来克服信号衰减，实现54.4倍加速的同时保持与显式CoT相当的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然思维链（CoT）通过中间步骤解锁了大语言模型的高级推理能力，但生成额外token带来了高昂的计算成本。最近研究表明将推理步骤压缩到潜在状态（隐式CoT压缩）提供了token高效的替代方案，但其机制尚不清楚。

Method: 1. 引入Order-r Interaction理论框架，证明高阶逻辑依赖的学习信号会指数衰减；2. 提出NatBool-DAG基准，强制不可约逻辑推理并消除语义捷径；3. 提出ALiCoT框架，通过对齐潜在token分布与中间推理状态来克服信号衰减。

Result: ALiCoT成功实现了高效推理：在保持与显式CoT相当性能的同时，实现了54.4倍的加速。实验验证了理论分析的正确性，表明对齐潜在状态与中间推理步骤是克服高阶交互障碍的有效方法。

Conclusion: 本文首次从理论上分析了CoT压缩的难度，揭示了高阶逻辑依赖学习信号衰减的根本原因，并提出ALiCoT框架通过状态对齐成功克服了这一障碍，为高效推理提供了理论指导和实用解决方案。

Abstract: Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.

</details>


### [68] [Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves](https://arxiv.org/abs/2601.21582)
*Jonas Knupp,Jan Hendrik Metzen,Jeremias Bohn,Georg Groh,Kristian Kersting*

Main category: cs.AI

TL;DR: Dreamer框架通过深度递归注意力混合机制，结合序列注意力、深度注意力和稀疏专家注意力，解决了深度递归模型中隐藏层大小瓶颈问题，在语言推理任务上实现了更高效的训练和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度递归方法存在三个主要问题：1) 缺乏FLOP、参数和内存匹配的基线比较；2) 由于部分固定层堆栈导致深度递归利用不足；3) 恒定隐藏层大小限制了多步潜在推理。需要解决这些瓶颈以实现更高效的深度递归模型。

Method: 提出Dreamer框架，结合三种注意力机制：序列注意力（处理序列维度）、深度注意力（处理深度维度）、稀疏专家注意力（MoE机制）。通过深度注意力缓解隐藏层大小瓶颈，解耦缩放维度，使深度递归模型能够高效扩展。

Result: 在语言推理基准测试中，相比FLOP、参数和内存匹配的SOTA模型，Dreamer模型达到相同准确率所需的训练token减少2-8倍；使用相同训练token时，性能优于约2倍大的SOTA模型。专家选择多样性比SOTA MoE高2-11倍。

Conclusion: Dreamer框架通过创新的注意力混合机制有效解决了深度递归模型的瓶颈问题，实现了更高效的训练和更好的推理性能，为深度递归模型的设计提供了新的思路。

Abstract: Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.

</details>


### [69] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: ATP-Latent提出主动潜在规划方法，通过条件变分自编码器建模潜在token监督，结合强化学习提升推理能力，在LLaMA-1B上实现4.1%精度提升和3.3%token减少。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法通过模仿语言标签监督潜在token，但由于同一问题存在多个等价但不同的思维链标签，被动模仿可能导致次优的潜在token表示和推理策略，限制了潜在空间的规划能力，造成训练与测试间的差距。

Method: 提出ATP-Latent方法：1) 使用条件变分自编码器建模潜在token监督过程，获得更平滑的潜在空间；2) 引入强化学习，基于VAE解码内容的一致性计算辅助连贯性奖励，引导RL过程获得最合理的潜在推理策略。

Result: 在LLaMA-1B模型上，ATP-Latent在四个基准测试中相比先进基线实现了+4.1%的准确率提升和-3.3%的token消耗减少。

Conclusion: ATP-Latent通过主动规划潜在token表示空间，结合条件VAE和强化学习，显著提升了潜在推理的效率和效果，验证了主动规划在潜在推理中的重要性。

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [70] [CORE: Collaborative Reasoning via Cross Teaching](https://arxiv.org/abs/2601.21600)
*Kshitij Mishra,Mirat Aubakirov,Martin Takac,Nils Lukas,Salem Lahlou*

Main category: cs.AI

TL;DR: CORE训练框架让语言模型通过协作学习，将同伴的成功转化为学习信号，显著提升推理能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在互补性推理错误：同一问题，一个模型可能成功而另一个失败。利用这种互补性，通过协作训练将同伴成功转化为学习信号

Method: CORE协作推理框架：两阶段解决问题（冷轮独立采样+带提示的救援轮），优化包含正确性、多样性（减少错误重叠）和救援奖励的组合奖励

Result: 仅用1000训练样本，3B+4B模型对在GSM8K达到99.54% Pass@2，MATH达到92.08%；在GPQA（348样本）达到77.34%，AIME（792样本）达到79.65%

Conclusion: 训练时协作能可靠地将模型互补性转化为显著性能提升，无需增加模型规模，在有限计算预算下实现强大推理能力

Abstract: Large language models exhibit complementary reasoning errors: on the same instance, one model may succeed with a particular decomposition while another fails. We propose Collaborative Reasoning (CORE), a training-time collaboration framework that converts peer success into a learning signal via a cross-teaching protocol. Each problem is solved in two stages: a cold round of independent sampling, followed by a contexted rescue round in which models that failed receive hint extracted from a successful peer. CORE optimizes a combined reward that balances (i) correctness, (ii) a lightweight DPP-inspired diversity term to reduce error overlap, and (iii) an explicit rescue bonus for successful recovery. We evaluate CORE across four standard reasoning datasets GSM8K, MATH, AIME, and GPQA. With only 1,000 training examples, a pair of small open source models (3B+4B) reaches Pass@2 of 99.54% on GSM8K and 92.08% on MATH, compared to 82.50% and 74.82% for single-model training. On harder datasets, the 3B+4B pair reaches Pass@2 of 77.34% on GPQA (trained on 348 examples) and 79.65% on AIME (trained on 792 examples), using a training-time budget of at most 1536 context tokens and 3072 generated tokens. Overall, these results show that training-time collaboration can reliably convert model complementarity into large gains without scaling model size.

</details>


### [71] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 本文提出了一种基于搜索的软件测试方法，用于在有限预算下发现企业级智能文档处理系统中的多种故障模式，通过比较不同搜索策略发现它们具有互补性，建议采用组合策略进行工业IDP验证。


<details>
  <summary>Details</summary>
Motivation: 企业级智能文档处理系统在金融、保险和医疗等高风险领域应用广泛，早期系统验证需要在有限预算下发现多样化的故障机制，而不是寻找单一的最坏情况文档。这需要识别文档变量间的复杂交互作用。

Method: 将问题形式化为基于搜索的软件测试问题，在文档配置的组合空间中操作，生成结构风险特征的实例以诱导真实故障条件。比较了进化算法、群体智能、质量多样性、学习型和量子算法等多种搜索策略。

Result: 不同求解器在相同预算下能发现其他方法未发现的故障模式，交叉时间分析显示所有评估预算中都存在求解器特定的发现，没有单一策略具有绝对优势。所有求解器的联合最终能覆盖观察到的故障空间。

Conclusion: 不同求解器具有内在互补性，依赖任何单一方法都会系统性地延迟重要风险的发现。这为工业IDP验证提供了基于组合的SBST策略的动机，以实现更稳健的系统验证。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [72] [RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems](https://arxiv.org/abs/2601.21609)
*Bingqian Li,Xiaolei Wang,Junyi Li,Weitao Li,Long Zhang,Sheng Chen,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: RecNet是一个自演化的偏好传播框架，通过路由代理实时传播用户和物品间的偏好更新，结合个性化接收机制和反馈驱动的优化，实现推荐系统的持续进化。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统主要依赖稀疏、嘈杂的显式用户-物品交互来建模偏好变化，无法反映用户和物品间的实时相互影响。需要一种能够主动传播实时偏好更新的框架。

Method: RecNet包含两个互补阶段：前向阶段通过集中式偏好路由机制传播偏好更新，结合个性化偏好接收机制（消息缓冲区和可优化的规则过滤器）；后向阶段采用反馈驱动的传播优化机制，模拟多智能体强化学习框架，使用LLM进行信用分配、梯度分析和模块级优化。

Result: 在各种场景下的广泛实验证明了RecNet在建模推荐系统偏好传播方面的有效性。

Conclusion: RecNet通过自演化的偏好传播框架解决了现有推荐系统在建模实时用户-物品相互影响方面的局限性，能够持续优化传播策略，提升推荐性能。

Abstract: Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.

</details>


### [73] [Semantic Content Determines Algorithmic Performance](https://arxiv.org/abs/2601.21618)
*Martiño Ríos-García,Nawaf Alampara,Kevin Maik Jablonka*

Main category: cs.AI

TL;DR: 论文提出了WhatCounts测试框架，专门用于评估LLM在计数任务中的语义敏感性，发现前沿LLM的计数准确率会因计数对象类型（如城市vs化学物质）而产生超过40%的差异，揭示了LLM并非真正实现算法而是近似算法，且这种近似依赖于输入参数的意义。


<details>
  <summary>Details</summary>
Motivation: 算法的行为应该独立于其参数的具体语义内容，但现有研究往往将语义敏感性与推理复杂性或提示变化混淆。作者希望创建一个原子化的测试框架，专门隔离并评估LLM在简单计数任务中对不同语义内容的敏感性。

Method: 提出WhatCounts测试框架：要求模型在无歧义、有分隔符、无重复项、无干扰项的列表中计数不同语义类型的项目（如城市、化学物质、名称、符号）。通过控制变量实验排除混淆因素，并观察少量无关微调对语义敏感性的影响。

Result: 前沿LLM在计数任务中的准确率仅因计数对象类型不同就产生超过40%的差异。这种差异是语义层面的，且随着少量无关微调会发生不可预测的变化。LLM并非真正实现算法，而是近似算法，且这种近似依赖于输入参数的意义。

Conclusion: LLM的行为存在对输入语义的隐藏依赖，这不仅限于计数任务，任何LLM函数都可能携带这种依赖性。这对LLM作为可靠算法执行器的能力提出了重要质疑，特别是在需要语义不变性的应用场景中。

Abstract: Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.

</details>


### [74] [ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval](https://arxiv.org/abs/2601.21654)
*Hao Shen,Hang Yang,Zhouhong Gu*

Main category: cs.AI

TL;DR: ScholarGym是一个用于评估学术文献深度研究工作流的模拟环境，通过静态语料库和确定性检索解决API调用不确定性问题，提供可重复的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前基于工具增强的大语言模型的研究工作流评估面临根本挑战：依赖实时API会引入不确定性（时间漂移、速率限制、后端状态变化），这种变化破坏了可重复性并使跨系统比较无效。

Method: 将研究工作流解耦为查询规划、工具调用和相关性评估三个组件，在受控条件下进行细粒度分析。基于包含57万篇论文的静态语料库构建，提供确定性检索，包含2,536个带有专家标注真实标签的查询。

Result: 通过在不同骨干模型上进行实验，揭示了推理能力、规划策略和选择机制在迭代优化过程中的相互作用。

Conclusion: ScholarGym为深度研究工作流提供了可重复的评估环境，解决了实时API依赖带来的不确定性问题，支持对研究流程各阶段的细粒度分析。

Abstract: Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.
  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.

</details>


### [75] [SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://arxiv.org/abs/2601.21666)
*Ahmed Y. Radwan,Christos Emmanouilidis,Hina Tabassum,Deval Pandya,Shaina Raza*

Main category: cs.AI

TL;DR: SONIC-O1是一个全面的人工验证基准测试，用于评估多模态大语言模型在13个真实对话领域中的时序理解和社交鲁棒性表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型研究主要关注静态图像理解，而对处理时序音频-视频数据的能力探索不足，需要高质量基准测试来系统评估模型在真实场景中的表现。

Method: 构建了包含4,958个标注和人口统计元数据的SONIC-O1基准测试，涵盖13个真实对话领域，评估任务包括开放式摘要、多项选择题回答、带推理的时序定位。

Result: 实验显示闭源和开源模型在多项选择题准确率上差距较小，但在时序定位任务上存在22.6%的显著性能差距，且模型在不同人口统计群体上的表现存在持续差异。

Conclusion: SONIC-O1为时序基础和社交鲁棒的多模态理解提供了开放评估套件，揭示了当前模型的局限性，特别是时序理解和公平性方面的不足。

Abstract: Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard

</details>


### [76] [TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning](https://arxiv.org/abs/2601.21692)
*Mingzu Liu,Hao Fang,Runmin Cong*

Main category: cs.AI

TL;DR: TCAP是一种针对MLLMs微调服务中后门攻击的无监督防御框架，通过分析三组件注意力分布异常来检测中毒样本。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法要么依赖监督信号，要么无法泛化到不同的触发器和模态。MLLMs微调服务中的后门攻击风险需要更通用的防御方案。

Method: 提出Tri-Component Attention Profiling (TCAP)：1) 将跨模态注意力图分解为系统指令、视觉输入和用户文本查询三个组件；2) 使用高斯混合模型识别对触发器敏感的注意力头；3) 通过EM投票聚合隔离中毒样本。

Result: 在多种MLLM架构和攻击方法上的实验表明，TCAP能够持续实现强大的性能，成为MLLMs中稳健实用的后门防御方案。

Conclusion: TCAP基于注意力分配差异的通用后门指纹，提供了一种无监督的防御框架，能有效应对MLLMs微调服务中的后门攻击风险。

Abstract: Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.

</details>


### [77] [FBS: Modeling Native Parallel Reading inside a Transformer](https://arxiv.org/abs/2601.21708)
*Tongxi Wang*

Main category: cs.AI

TL;DR: FBS Transformer通过引入前瞻注意力窗口、分块头和跳过门三个模块，在保持参数不变的情况下改善质量-效率权衡，加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理主要依赖逐token自回归，现有加速方法未能充分利用人类阅读的关键要素：内容自适应前瞻、分块结构感知的计算分配以及训练-测试一致性。

Method: 提出Fovea-Block-Skip Transformer，通过三个核心模块：Parafovea-Attention Window（前瞻注意力窗口）、Chunk-Head（分块头）和Skip-Gate（跳过门），在Transformer中注入可训练的因果循环。

Result: 在多样化基准测试中，FBS在保持参数不变的情况下改善了质量-效率权衡，消融实验显示三个模块具有互补性。

Conclusion: FBS Transformer通过模拟人类阅读的关键机制，为LLM推理加速提供了有效的解决方案，三个模块的协同作用显著提升了推理效率。

Abstract: Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.

</details>


### [78] [E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory](https://arxiv.org/abs/2601.21714)
*Kaixiang Wang,Yidan Lin,Jiong Lou,Zhaojiacheng Zhou,Bunyod Suvonov,Jie Li*

Main category: cs.AI

TL;DR: E-mem框架通过从记忆预处理转向情景上下文重建，采用异质分层架构，在保持逻辑完整性的同时显著提升推理性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的记忆预处理方法存在破坏性去上下文化问题，将复杂序列依赖压缩为预定义结构（如嵌入或图）会切断深度推理所需的情境完整性。

Method: 提出E-mem框架，采用异质分层架构：多个助手代理维护未压缩的记忆上下文，中央主代理负责全局规划；助手在激活的片段内进行本地推理，提取情境感知证据后再聚合。

Result: 在LoCoMo基准测试中，E-mem达到超过54%的F1分数，比当前最优方法GAM提升7.75%，同时减少超过70%的token成本。

Conclusion: E-mem通过情景上下文重建有效解决了记忆预处理中的去上下文化问题，为LLM代理实现System 2推理提供了可行的解决方案。

Abstract: The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\% F1, surpassing the state-of-the-art GAM by 7.75\%, while reducing token cost by over 70\%.

</details>


### [79] [DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting](https://arxiv.org/abs/2601.21726)
*Siru Zhong,Yiqiu Liu,Zhiqing Cui,Zezhi Shao,Fei Wang,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: DropoutTS：一种模型无关的插件，通过样本自适应dropout机制，利用频谱稀疏性量化实例级噪声，动态校准模型学习能力，提升深度时间序列模型在噪声数据下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度时间序列模型对现实应用中普遍存在的噪声数据很脆弱。现有鲁棒性策略要么修剪数据，要么依赖昂贵的先验量化，无法平衡有效性和效率。

Method: 提出DropoutTS插件，采用样本自适应dropout机制：利用频谱稀疏性通过重构残差高效量化实例级噪声，将噪声映射到自适应dropout率，动态校准模型学习能力，选择性抑制虚假波动同时保持细粒度保真度。

Result: 在多种噪声机制和开放基准测试中的广泛实验表明，DropoutTS能持续提升优秀骨干模型的性能，以可忽略的参数开销和无架构修改实现先进的鲁棒性。

Conclusion: DropoutTS通过从"学什么"到"学多少"的范式转变，提供了一种高效且有效的解决方案，增强了深度时间序列模型在真实噪声环境下的鲁棒性。

Abstract: Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from "what" to learn to "how much" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.

</details>


### [80] [Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2601.21742)
*Ruiwen Zhou,Maojia Song,Xiaobao Wu,Sitao Cheng,Xunjian Yin,Yuxi Xie,Zhuoqun Hao,Wenyue Hua,Liangming Pan,Soujanya Poria,Min-Yen Kan*

Main category: cs.AI

TL;DR: ECL框架让多智能体系统中的小模型通过历史交互学习评估同伴可靠性，从而超越大8倍的历史无关基线，实现近完美性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的个体智能体缺乏鲁棒性，容易盲目跟随误导性同伴，这源于奉承行为和对同伴可靠性评估能力不足。

Method: 提出历史感知参考学习问题，开发认知上下文学习(ECL)框架：基于历史交互构建同伴档案，通过强化学习优化，让智能体在不确定时向可信同伴学习。

Result: ECL使Qwen 3-4B小模型超越大8倍的Qwen 3-30B基线；前沿模型性能接近100%；在多种多智能体配置中泛化良好；信任建模准确性与最终答案质量强相关。

Conclusion: ECL通过历史交互学习同伴可靠性，有效解决了多智能体系统中的盲目跟随问题，证明了信任建模对提升系统性能的重要性。

Abstract: Individual agents in multi-agent (MA) systems often lack robustness, tending to blindly conform to misleading peers. We show this weakness stems from both sycophancy and inadequate ability to evaluate peer reliability. To address this, we first formalize the learning problem of history-aware reference, introducing the historical interactions of peers as additional input, so that agents can estimate peer reliability and learn from trustworthy peers when uncertain. This shifts the task from evaluating peer reasoning quality to estimating peer reliability based on interaction history. We then develop Epistemic Context Learning (ECL): a reasoning framework that conditions predictions on explicitly-built peer profiles from history. We further optimize ECL by reinforcement learning using auxiliary rewards. Our experiments reveal that our ECL enables small models like Qwen 3-4B to outperform a history-agnostic baseline 8x its size (Qwen 3-30B) by accurately identifying reliable peers. ECL also boosts frontier models to near-perfect (100%) performance. We show that ECL generalizes well to various MA configurations and we find that trust is modeled well by LLMs, revealing a strong correlation in trust modeling accuracy and final answer quality.

</details>


### [81] [Language-based Trial and Error Falls Behind in the Era of Experience](https://arxiv.org/abs/2601.21754)
*Haoyu Wang,Guozheng Ma,Shugang Cui,Yilun Kong,Haotian Luo,Li Shen,Mengya Gao,Yichao Wu,Xiaogang Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: SCOUT框架通过轻量级"侦察兵"探索环境，收集轨迹数据来引导LLM，解决LLM在非语言环境中探索成本过高的问题，显著提升性能并降低计算消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语言任务上表现出色，但在未见过的非语言环境（如符号或空间任务）中表现有限。先前研究将这种性能差距归因于预训练分布与测试分布的不匹配，但本文发现主要瓶颈是探索成本过高：掌握这些任务需要大量试错，这对于在高维语义空间中运行的大型语言模型来说计算上不可持续。

Method: 提出SCOUT（Sub-Scale Collaboration On Unseen Tasks）框架，将探索与利用解耦。使用轻量级"侦察兵"（如小型MLP）以远超LLM的速度和规模探测环境动态，收集的轨迹通过监督微调（SFT）引导LLM，然后通过多轮强化学习（RL）激活其潜在的世界知识。

Result: SCOUT使Qwen2.5-3B-Instruct模型平均得分达到0.86，显著优于包括Gemini-2.5-Pro（0.60）在内的专有模型，同时节省约60%的GPU小时消耗。

Conclusion: 探索成本是LLM在非语言环境中性能受限的主要瓶颈，SCOUT框架通过轻量级侦察兵高效探索环境，有效引导LLM学习，在显著提升性能的同时大幅降低计算成本。

Abstract: While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight "scouts" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.

</details>


### [82] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出ZSSD零样本统计降尺度框架，无需配对训练数据，通过物理一致气候先验和统一坐标引导解决现有方法物理不一致性和梯度消失问题


<details>
  <summary>Details</summary>
Motivation: 传统监督式气候降尺度方法因缺乏配对训练数据和与再分析数据的领域差距，难以泛化到全球气候模型；现有零样本方法存在物理不一致性和大尺度因子下的梯度消失问题

Method: ZSSD框架包含：1）从再分析数据学习的物理一致气候先验，结合地球物理边界和时序信息确保物理有效性；2）统一坐标引导策略，解决DPS中的梯度消失问题并保持与大尺度场的一致性

Result: ZSSD在99百分位误差上显著优于现有零样本基线，能成功重建复杂天气事件（如热带气旋），并在异构GCMs上表现良好

Conclusion: ZSSD为零样本气候降尺度提供了有效解决方案，解决了物理一致性和梯度消失问题，在多种GCMs上表现出色

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [83] [Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies](https://arxiv.org/abs/2601.21771)
*Hadi Banaee,Stephanie Lowry*

Main category: cs.AI

TL;DR: 该研究提出一个概念空间框架来建模随时间展开的抽象概念，以国际象棋为案例进行验证。通过将策略概念表示为可解释质量维度上的几何区域，将棋局实例化为轨迹，通过轨迹向概念区域的移动方向来识别策略意图。


<details>
  <summary>Details</summary>
Motivation: 将概念空间理论扩展到时间性、目标导向的概念建模，特别是那些在序列决策过程中展开的抽象概念。传统概念空间理论主要处理静态概念，而许多抽象概念（如策略、计划）是在时间维度上实现的。

Method: 采用概念空间框架，将抽象策略概念（如攻击、牺牲）表示为可解释质量维度上的几何区域。将国际象棋对局实例化为轨迹，通过分析轨迹在概念空间中的移动方向和模式来识别策略意图。支持双视角建模，捕捉不同玩家对相同局面的不同解释。

Result: 基于轨迹的概念识别方法被证明是可行的，轨迹移动模式与专家评论一致。国际象棋的案例研究验证了该框架的有效性，能够识别玩家在不同阶段的策略意图。

Conclusion: 该工作为概念空间理论扩展到时间性、目标导向概念奠定了基础，支持序列决策的广泛应用，并能与知识演化机制集成，实现抽象概念随时间的学习和精炼。

Abstract: We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.

</details>


### [84] [BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics](https://arxiv.org/abs/2601.21800)
*Dionizije Fa,Marko Čuljak,Bruno Pandža,Mateo Čupić*

Main category: cs.AI

TL;DR: BioAgent Bench：一个用于评估AI代理在生物信息学任务中性能和鲁棒性的基准数据集与评估套件，包含端到端任务和自动化评估，测试了前沿模型在扰动下的表现。


<details>
  <summary>Details</summary>
Motivation: 需要评估AI代理在生物信息学任务中的实际表现，特别是在多步骤流程构建、鲁棒性以及隐私约束下的适用性，为生物信息学AI应用提供标准化评估工具。

Method: 构建包含RNA-seq、变异检测、宏基因组学等端到端任务的基准数据集，设计具体输出规范支持自动化评估，引入受控扰动（输入损坏、诱饵文件、提示膨胀）进行压力测试，使用LLM评分器评估流程进展和结果有效性。

Result: 前沿代理能够完成多步骤生物信息学流程而无需复杂定制框架，通常能可靠生成所需最终产物；但鲁棒性测试显示在受控扰动下存在失败模式，表明高层流程构建正确不能保证步骤级推理的可靠性；闭源模型在严格隐私约束下可能不适用，开源模型尽管完成率较低但更合适。

Conclusion: BioAgent Bench为评估生物信息学AI代理提供了标准化工具，揭示了当前模型在流程构建能力与步骤级推理鲁棒性之间的差距，强调了在隐私敏感场景下开源模型的重要性，并公开了数据集和评估套件供社区使用。

Abstract: This paper introduces BioAgent Bench, a benchmark dataset and an evaluation suite designed for measuring the performance and robustness of AI agents in common bioinformatics tasks. The benchmark contains curated end-to-end tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts that specify concrete output artifacts to support automated assessment, including stress testing under controlled perturbations. We evaluate frontier closed-source and open-weight models across multiple agent harnesses, and use an LLM-based grader to score pipeline progress and outcome validity. We find that frontier agents can complete multi-step bioinformatics pipelines without elaborate custom scaffolding, often producing the requested final artifacts reliably. However, robustness tests reveal failure modes under controlled perturbations (corrupted inputs, decoy files, and prompt bloat), indicating that correct high-level pipeline construction does not guarantee reliable step-level reasoning. Finally, because bioinformatics workflows may involve sensitive patient data, proprietary references, or unpublished IP, closed-source models can be unsuitable under strict privacy constraints; in such settings, open-weight models may be preferable despite lower completion rates. We release the dataset and evaluation suite publicly.

</details>


### [85] [A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition](https://arxiv.org/abs/2601.21802)
*Hoang Khang Phan,Quang Vinh Dang,Noriyo Colley,Christina Garcia,Nhat Tan Le*

Main category: cs.AI

TL;DR: 提出基于大语言模型的统一框架，用于气管内吸痰训练的视频活动识别与反馈生成，在准确率和F1分数上比基线模型提升15-20%。


<details>
  <summary>Details</summary>
Motivation: 气管内吸痰是一项高风险临床操作，在家庭护理和教育环境中缺乏有效监督。现有自动化识别和反馈系统研究不足，需要开发智能训练辅助系统。

Method: 提出以LLM为核心的统一框架，将LLM作为中央推理模块，执行视频数据的时空活动识别和可解释决策分析，并生成自然语言反馈。同时构建基于异常检测和可解释AI的学生支持模块。

Result: LLM方法在准确率和F1分数上比传统机器学习和深度学习方法提升约15-20%。框架能够提供自动化的可解释反馈，突出正确操作并建议针对性改进。

Conclusion: 该框架为护理教育建立了可扩展、可解释、数据驱动的基础，能提高培训效率并最终改善患者安全。

Abstract: Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.

</details>


### [86] [CORE:Toward Ubiquitous 6G Intelligence Through Collaborative Orchestration of Large Language Model Agents Over Hierarchical Edge](https://arxiv.org/abs/2601.21822)
*Zitong Yu,Boquan Sun,Yang Li,Zheyan Qu,Xing Zhang*

Main category: cs.AI

TL;DR: CORE是一个在6G边缘网络中部署多LLM协作学习的创新框架，通过角色分配、动态编排和流水线并行执行优化分布式AI推理任务。


<details>
  <summary>Details</summary>
Motivation: 6G网络和LLM的发展为实现泛在智能创造了条件，但异构计算资源分散在分层网络中，单个LLM代理难以完成复杂推理任务，需要解决资源碎片化和协作效率问题。

Method: 提出CORE框架：1) 在移动设备和边缘服务器上分布式部署多个具有不同功能角色的LLM；2) 集成实时感知、动态角色编排和流水线并行执行三个优化模块；3) 引入角色亲和度调度算法，在分层边缘基础设施中动态编排LLM角色分配。

Result: 在多种6G应用场景下的综合案例研究和性能评估表明，CORE显著提升了系统效率和任务完成率。在实际边缘计算平台上的部署验证了其在实际操作环境中的稳健性能。

Conclusion: CORE框架通过分布式LLM协作学习系统，有效解决了6G边缘网络中异构计算资源的协调问题，为泛在智能的实现提供了可行的技术方案，并在实际部署中展现出良好的应用前景。

Abstract: Rapid advancements in sixth-generation (6G) networks and large language models (LLMs) have paved the way for ubiquitous intelligence, wherein seamless connectivity and distributed artificial intelligence (AI) have revolutionized various aspects of our lives.However, realizing this vision faces significant challenges owing to the fragmented and heterogeneous computing resources across hierarchical networks, which are insufficient for individual LLM agents to perform complex reasoning tasks.To address this issue, we propose Collaborative Orchestration Role at Edge (CORE), an innovative framework that employs a collaborative learning system in which multiple LLMs, each assigned a distinct functional role, are distributed across mobile devices and tiered edge servers. The system integrates three optimization modules, encompassing real-time perception,dynamic role orchestration, and pipeline-parallel execution, to facilitate efficient and rapid collaboration among distributed agents. Furthermore, we introduce a novel role affinity scheduling algorithm for dynamically orchestrating LLM role assignments across the hierarchical edge infrastructure, intelligently matching computational demands with available dispersed resources.Finally, comprehensive case studies and performance evaluations across various 6G application scenarios demonstrated the efficacy of CORE, revealing significant enhancements in the system efficiency and task completion rates. Building on these promising outcomes, we further validated the practical applicability of CORE by deploying it on a real-world edge-computing platform,that exhibits robust performance in operational environments.

</details>


### [87] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 该研究提出了一个全面的心电专家基础模型基准测试框架，结合性能评估和表示层分析，通过SHAP和UMAP技术深入分析模型嵌入模式，并在跨大陆数据集和数据稀缺场景下评估多个模型。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型在医疗领域的应用，需要严格评估其嵌入表示的可泛化性，特别是在心电诊断这样的高敏感领域。现有基准测试主要关注下游性能评估，缺乏对表示层特征的深入分析。

Method: 提出一个基准测试方法学，将基于性能的评估与表示层分析相结合，利用SHAP（可解释性分析）和UMAP（降维可视化）技术。在跨大陆数据集和数据稀缺设置下，评估多个通过先进技术预训练的心电专家基础模型。

Result: 实验结果表明，该基准测试协议能够深入揭示心电专家基础模型的嵌入模式，提供对其表示结构和泛化能力的更深刻理解。

Conclusion: 该研究填补了心电专家基础模型评估的空白，提供了一个全面的基准测试框架，有助于更负责任地在医疗领域部署基础模型，特别是在数据稀缺的现实医疗场景中。

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [88] [Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework](https://arxiv.org/abs/2601.21844)
*So Fukuhara,Abdallah Alabdallah,Nuwan Gunasekara,Slawomir Nowaczyk*

Main category: cs.AI

TL;DR: 论文提出一个决策中心的仿真框架，用于在库存管理环境中系统评估预测模型，强调统计准确性指标与运营绩效指标之间的脱节。


<details>
  <summary>Details</summary>
Motivation: 汽车售后市场备件库存管理面临高度间歇性需求和不确定性，传统预测模型评估仅关注统计准确性（如MAE、RMSE），但这些指标与运营绩效指标（如总成本和服务水平）的关系不明确，需要建立预测模型与库存决策之间的关联评估框架。

Method: 提出一个决策中心的仿真软件框架，包含三个核心组件：1) 针对备件需求特征设计的合成需求生成器；2) 可容纳任意预测模型的灵活预测模块；3) 消耗预测并计算运营KPI的库存控制模拟器，形成闭环评估系统。

Result: 通过多种仿真场景验证，发现传统准确性指标的改进不一定带来更好的运营绩效，统计误差相似的模型可能产生显著不同的成本-服务权衡，分析了预测性能特定方面如何影响库存结果，并得出模型选择指导原则。

Conclusion: 该框架将需求预测与库存管理操作化连接起来，将评估重点从纯粹的预测准确性转向运营相关性，为汽车售后市场及相关领域的预测模型评估提供了更实用的方法。

Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.

</details>


### [89] [KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement](https://arxiv.org/abs/2601.21864)
*Jinhao Pan,Chahat Raj,Anjishnu Mukherjee,Sina Mansouri,Bowen Wei,Shloka Yada,Ziwei Zhu*

Main category: cs.AI

TL;DR: KnowBias提出了一种轻量级去偏框架，通过增强而非抑制编码偏见知识的神经元来减少LLM的社会偏见，仅需少量是/否问题且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法通常采用抑制范式（修改参数、提示或神经元），但这些方法脆弱、泛化性差、数据效率低且容易损害模型通用能力。需要一种更有效且能保持模型性能的去偏方法。

Method: 1) 使用基于归因的分析方法，通过少量偏见知识问题识别编码偏见知识的神经元；2) 在推理时选择性增强这些神经元，而非抑制它们。这种方法仅需少量简单是/否问题，无需重新训练。

Result: 在多个基准测试和LLM上的实验表明，KnowBias实现了最先进的去偏性能，同时保持了最小化的通用能力退化，能够跨偏见类型和人口统计特征泛化。

Conclusion: KnowBias提供了一种轻量级、数据高效的去偏框架，通过增强偏见知识神经元而非抑制它们，在有效减少社会偏见的同时保持了LLM的通用能力，为安全部署提供了新思路。

Abstract: Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.

</details>


### [90] [WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents](https://arxiv.org/abs/2601.21872)
*Yao Zhang,Shijie Tang,Zeyu Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: WebArbiter：一种基于推理的网页过程奖励模型，通过生成结构化理由和偏好判断来指导网页导航任务，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 网页代理在自动化复杂计算机任务时面临长序列决策和不可逆动作的挑战，现有基于结果的监督信号稀疏且延迟，而现有的过程奖励模型（WebPRMs）存在局限性：标量WebPRMs将进度压缩为粗糙信号，清单式WebPRMs依赖脆弱的模板匹配，容易在布局或语义变化时失效。

Method: 提出WebArbiter，将奖励建模转化为文本生成任务，生成结构化理由并得出偏好判断，识别当前上下文下最有利于任务完成的动作。采用两阶段训练流程：1）推理蒸馏使模型具备原则指导的推理能力；2）强化学习纠正教师偏见，直接对齐判断与正确性。

Result: 在WebPRMBench基准测试中，WebArbiter-7B比最强基线GPT-5高出9.1分；在WebArena-Lite的奖励引导轨迹搜索中，比先前最佳WebPRM高出最多7.2分，展现了在实际复杂网页任务中的鲁棒性和实用价值。

Conclusion: WebArbiter通过推理优先、原则诱导的方法解决了现有WebPRMs的局限性，在多个网页环境中表现出色，为网页导航任务提供了更可靠的过程奖励建模框架。

Abstract: Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.

</details>


### [91] [astra-langchain4j: Experiences Combining LLMs and Agent Programming](https://arxiv.org/abs/2601.21879)
*Rem Collier,Katharine Beaumont,Andrei Ciortea*

Main category: cs.AI

TL;DR: 本文介绍了将大型语言模型集成到ASTRA编程语言中的原型开发经验，探讨了生成式AI和Agentic AI如何影响传统Agent工具包，以及传统工具包经验如何影响新Agent平台设计。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和Agentic AI（多智能体系统形式）的出现，需要探索这些技术如何影响传统Agent工具包的使用，以及传统工具包中积累的丰富经验如何影响新Agent平台的设计。

Method: 开发了一个原型，将大型语言模型集成到ASTRA编程语言中，通过三个示例实现展示了具体的集成方法。

Result: 论文展示了ASTRA工具包的简要概述，并通过三个具体示例说明了LLM集成的实现方式，总结了从这些示例中获得的经验。

Conclusion: 通过原型开发经验，探讨了传统Agent工具包与新兴AI技术的相互影响，为未来Agent平台设计提供了有价值的见解。

Abstract: Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.

</details>


### [92] [Making Models Unmergeable via Scaling-Sensitive Loss Landscape](https://arxiv.org/abs/2601.21898)
*Minwoo Jang,Hoyoung Kim,Jabin Koo,Jungseul Ok*

Main category: cs.AI

TL;DR: Trap²：一种架构无关的保护框架，通过在微调过程中编码保护机制，防止模型权重被未经授权地合并使用


<details>
  <summary>Details</summary>
Motivation: 模型中心的兴起使得模型组件重用更加便捷，但也带来了治理缺口：下游用户可以重新组合已发布的权重，绕过安全对齐或许可条款。现有防御方法大多是事后且架构特定的，无法为实践中多样化的架构和发布格式提供一致保护。

Method: 提出Trap²框架，使用权重重新缩放作为合并过程的简单代理。在微调过程中将保护编码到更新中，无论模型是以适配器还是完整模型形式发布。保持发布权重在独立使用时有效，但在合并过程中出现的重新缩放操作下性能会下降。

Result: 该方法能够有效防止未经授权的模型合并，同时保持模型在合法使用场景下的性能。提供了一种架构无关的保护机制，适用于各种模型架构和发布格式。

Conclusion: Trap²填补了模型治理的空白，为模型发布者提供了一种有效的保护机制，防止模型权重被未经授权地重新组合使用，同时保持架构无关性和实用性。

Abstract: The rise of model hubs has made it easier to access reusable model components, making model merging a practical tool for combining capabilities. Yet, this modularity also creates a \emph{governance gap}: downstream users can recompose released weights into unauthorized mixtures that bypass safety alignment or licensing terms. Because existing defenses are largely post-hoc and architecture-specific, they provide inconsistent protection across diverse architectures and release formats in practice. To close this gap, we propose \textsc{Trap}$^{2}$, an architecture-agnostic protection framework that encodes protection into the update during fine-tuning, regardless of whether they are released as adapters or full models. Instead of relying on architecture-dependent approaches, \textsc{Trap}$^{2}$ uses weight re-scaling as a simple proxy for the merging process. It keeps released weights effective in standalone use, but degrades them under re-scaling that often arises in merging, undermining unauthorized merging.

</details>


### [93] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: 提出认知启发的两阶段后训练框架CoMT+CCRL，将抽象策略学习与具体执行解耦，相比传统方法提升泛化能力并大幅降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练方法将完整推理轨迹作为基本单元进行优化，这与人类认知过程存在根本差异。人类解决问题时先获取抽象策略（元知识），再适应具体实例，而现有方法将抽象策略与问题特定执行纠缠在一起，限制了泛化能力。

Method: 提出两阶段认知启发框架：1) Chain-of-Meta-Thought (CoMT)：专注于抽象推理模式的有监督学习，不涉及具体执行，获取可泛化策略；2) Confidence-Calibrated Reinforcement Learning (CCRL)：通过置信度感知奖励优化任务适应，防止过度自信错误级联，提高执行可靠性。

Result: 在4个模型和8个基准测试中，相比标准方法，在分布内提升2.19%，分布外提升4.63%，同时训练时间减少65-70%，token消耗减少50%。

Conclusion: 将后训练与人类认知原则对齐不仅能带来更好的泛化性能，还能显著提高训练效率，证明了认知启发方法在LLM优化中的价值。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [94] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: ProRAG：一个过程监督强化学习框架，通过整合步骤级监督到在线优化循环中，解决RAG中奖励稀疏和信用分配问题，在复杂推理任务上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的RL方法在RAG优化中存在奖励稀疏和信用分配效率低的问题，粗粒度的标量奖励无法识别长轨迹中的具体错误步骤，导致"过程幻觉"（模型通过错误逻辑或冗余检索得到正确答案）。现有过程感知方法缺乏在线探索能力，难以将步骤级信用与全局结果解耦。

Method: 提出ProRAG框架，包含四个阶段：1) 监督策略预热，用结构化推理格式初始化模型；2) 构建基于MCTS的过程奖励模型(PRM)量化中间推理质量；3) PRM引导的推理精炼，使策略与细粒度过程偏好对齐；4) 过程监督强化学习，采用双粒度优势机制，聚合步骤级过程奖励与全局结果信号。

Result: 在五个多跳推理基准测试上的广泛实验表明，ProRAG相比基于结果的RL和过程感知RL基线方法取得了更优的整体性能，特别是在复杂长视野任务上，验证了细粒度过程监督的有效性。

Conclusion: ProRAG通过将学习到的步骤级监督整合到在线优化循环中，有效解决了RAG优化中的奖励稀疏和信用分配问题，为复杂推理任务提供了精确的反馈机制，显著提升了性能。

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [95] [JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG](https://arxiv.org/abs/2601.21916)
*Yiqun Chen,Erhan Zhang,Tianyi Hu,Shijie Wang,Zixuan Yang,Meizhi Zhong,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Jiaxin Mao*

Main category: cs.AI

TL;DR: JADE框架通过联合优化动态多轮工作流中的规划和执行模块，解决了现有RAG系统中规划与执行脱节导致的"战略-操作不匹配"问题，实现了协同适应和端到端学习。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统面临关键二分法：要么在固定架构中联合优化模块，要么支持动态规划但将执行器视为黑盒工具。这种"解耦优化"导致"战略-操作不匹配"，复杂规划策略因未适应的本地执行器而无法实现，系统复杂度增加却带来负性能收益。

Method: 提出JADE（联合智能动态执行）框架，将系统建模为统一在单一共享骨干下的协作多智能体团队，通过基于结果的奖励驱动端到端学习，实现规划与执行的联合优化。这种方法促进"协同适应"：规划器学习在执行器能力边界内操作，执行器则演进以对齐高层战略意图。

Result: 实证结果表明，JADE将分离的模块转变为协同系统，通过联合优化带来显著性能改进，并通过动态工作流编排实现效率与效果之间的灵活平衡。

Conclusion: JADE框架解决了RAG系统中规划与执行脱节的核心问题，通过联合优化和协同适应机制，实现了动态多轮工作流中规划与执行的有效整合，提升了系统整体性能。

Abstract: The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \textbf{JADE} (\textbf{J}oint \textbf{A}gentic \textbf{D}ynamic \textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.

</details>


### [96] [Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21919)
*Yiqun Chen,Jinyuan Feng,Wei Yang,Meizhi Zhong,Zhengliang Shi,Rui Li,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Zhiqiang Pu,Jiaxin Mao*

Main category: cs.AI

TL;DR: SCMA是一个多智能体强化学习框架，通过选择性惩罚冗余推理块来压缩大型推理模型的响应长度，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的解决方案通过简单地将长度惩罚与结果奖励相结合来处理冗余推理问题，但这种简单的奖励加权难以平衡简洁性与准确性，强制简洁可能会损害关键的推理逻辑。

Method: 提出SCMA多智能体强化学习框架，包含三个智能体：分割智能体将推理过程分解为逻辑块，评分智能体量化每个块的重要性，推理智能体在训练中学习重要性加权的长度惩罚，在部署时不引入额外开销。

Result: 实验评估显示SCMA将响应长度减少11.1%到39.0%，同时将准确性提高4.33%到10.02%。消融研究和定性分析验证了MARL框架中的协同优化产生了涌现行为，相比传统RL范式产生了更强大的推理模型。

Conclusion: SCMA框架通过多智能体协同优化，能够有效减少大型推理模型的冗余推理，在压缩响应长度的同时提高准确性，解决了现有RL方法难以平衡简洁性与准确性的问题。

Abstract: The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.

</details>


### [97] [AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making](https://arxiv.org/abs/2601.21936)
*Jon Chun,Kathrine Elkins,Yong Suk Lee*

Main category: cs.AI

TL;DR: AgenticSimLaw是一个基于法庭辩论结构的多智能体框架，用于高风险表格决策任务，通过定义明确的角色（检察官、辩护、法官）和7轮结构化辩论，提供透明可控的推理过程。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒方法在高风险决策中缺乏透明度的问题，特别是在刑事司法等敏感领域，需要可审计的决策过程和人类监督。

Method: 采用法庭风格的多智能体辩论框架，定义三种角色（检察官、辩护、法官），7轮结构化辩论协议，以及私有推理策略，在NLSY97数据集上进行年轻成人再犯预测基准测试。

Result: 相比传统链式思维提示，结构化多智能体辩论提供更稳定和可泛化的性能，准确率和F1分数相关性更强，在近90种模型和策略组合中表现优异。

Conclusion: 该框架不仅提升性能，还提供细粒度推理控制、完整交互记录以增强可解释性，并能系统分析智能体行为，适用于任何需要透明度和人类监督的高风险决策任务。

Abstract: We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.

</details>


### [98] [Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities](https://arxiv.org/abs/2601.21937)
*Shuangshuang Ying,Zheyu Wang,Yunjian Peng,Jin Chen,Yuhao Wu,Hongbin Lin,Dingyu He,Siyi Liu,Gengchen Yu,YinZhu Piao,Yuchen Wu,Xin Gui,Zhongyuan Peng,Xin Li,Xeron Du,Libo Qin,YiXin Cao,Ge Zhang*

Main category: cs.AI

TL;DR: DeR2是一个深度研究沙盒，用于评估大语言模型在真正新颖科学信息上的推理能力，通过解耦证据访问与推理，隔离文档基础推理，并防止参数记忆污染。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法准确评估大语言模型对新颖科学信息的推理能力，因为端到端RAG评估中推理与检索、工具链选择混淆，且受到参数记忆和开放网络波动的影响。

Method: 开发DeR2沙盒，通过四种机制解耦证据访问与推理：仅指令、概念（无文档的金概念）、仅相关文档、完整集（相关文档+主题相关干扰项）。采用两阶段验证防止参数泄漏，使用2023-2025年理论论文构建冻结文档库，包含专家标注的概念和验证的推理链。

Result: 实验显示最先进的基础模型存在显著差异和提升空间：一些模型表现出模式切换脆弱性（完整集表现比仅指令更差），另一些模型存在结构性概念误用（正确命名概念但无法作为程序执行）。

Conclusion: DeR2提供了一个受控环境来评估大语言模型在深度研究任务中的文档基础推理能力，揭示了当前模型在科学推理方面的实质性局限和提升机会。

Abstract: Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.

</details>


### [99] [ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models](https://arxiv.org/abs/2601.21947)
*Bowen Fang,Wen Ye,Yunyue Su,Jinghao Zhang,Qiang Liu,Yesheng Liu,Xin Sun,Shu Wu,Jiabing Yang,Baole Wei,Liang Wang*

Main category: cs.AI

TL;DR: ToolWeaver提出了一种新的生成式工具学习框架，通过将工具编码为分层序列来解决现有检索式工具使用方法的语义挑战和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有检索式工具使用管道面临双重语义挑战：检索器编码器难以捕捉复杂语义，而LLM本身缺乏工具知识。生成式方法虽然统一了选择和执行，但将每个工具映射到唯一新token的方法导致可扩展性危机和语义瓶颈。

Method: ToolWeaver将工具编码为分层序列，使词汇扩展与工具数量呈对数关系。通过新颖的标记化过程生成结构化代码，将工具的内在语义与外部协同使用模式编织在一起，然后通过生成式对齐阶段将分层代码序列集成到LLM中。

Result: 在近47,000个工具的评估中，ToolWeaver显著优于最先进的方法，为高级工具增强代理建立了更可扩展、可泛化和语义感知的基础。

Conclusion: ToolWeaver通过分层序列编码解决了现有工具学习方法的可扩展性和语义瓶颈问题，为生成式工具学习提供了更有效的框架。

Abstract: Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.

</details>


### [100] [How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors](https://arxiv.org/abs/2601.21961)
*Kuai Yu,Naicheng Yu,Han Wang,Rui Yang,Huan Zhang*

Main category: cs.AI

TL;DR: VAF是一个评估网页视觉属性如何影响智能体决策的框架，通过生成语义相同但视觉属性不同的网页变体，测量智能体行为差异，发现背景颜色对比度、项目大小、位置和卡片清晰度对智能体行为影响较大。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注对抗攻击下的鲁棒性，较少关注良性场景中智能体的偏好。虽然早期研究探讨了文本属性对智能体行为的影响，但对视觉属性如何塑造智能体决策的系统性理解仍然有限。

Method: VAF包含三个阶段：1) 变体生成 - 创建语义相同但视觉属性不同的网页变体；2) 浏览交互 - 智能体通过滚动和点击进行导航，模拟人类浏览行为；3) 验证 - 通过点击动作和推理，使用目标点击率和目标提及率评估视觉属性的影响。

Result: 在8个变体家族（共48个变体）、5个真实网站（购物、旅行、新闻浏览）和4个代表性网页智能体上的实验表明：背景颜色对比度、项目大小、位置和卡片清晰度对智能体行为有强烈影响，而字体样式、文本颜色和项目图像清晰度影响较小。

Conclusion: 视觉属性显著影响网页智能体的决策过程，某些视觉属性（如背景对比度、大小、位置）比文本相关属性（如字体、颜色）对智能体行为的影响更大，这为设计更鲁棒的网页智能体提供了重要见解。

Abstract: Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.

</details>


### [101] [The Energy Impact of Domain Model Design in Classical Planning](https://arxiv.org/abs/2601.21967)
*Ilche Georgievski,Serhat Tekin,Marco Aiello*

Main category: cs.AI

TL;DR: 该论文研究领域模型特征对经典规划器能耗的影响，发现领域级修改能在不同规划器中产生可测量的能耗差异，且能耗并不总是与运行时间相关。


<details>
  <summary>Details</summary>
Motivation: 传统AI研究优先考虑算法性能（如机器学习准确率或自动规划运行时间），而新兴的绿色AI范式将能耗视为关键性能维度。尽管自动规划计算需求高，但其能源效率很少受到关注。领域模型与算法分离的结构使得可以通过领域模型设计来系统分析能耗。

Method: 引入领域模型配置框架，能够控制特征变化（如元素排序、动作元数、死端状态）。使用5个基准领域和5个最先进的规划器，分析每个基准32个领域变体的能耗和运行时间影响。

Result: 结果显示领域级修改在不同规划器中产生可测量的能耗差异，能耗并不总是与运行时间相关。

Conclusion: 领域模型特征显著影响规划器的能耗，绿色AI需要考虑能耗作为性能维度，领域模型设计是优化规划系统能源效率的有效途径。

Abstract: AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.

</details>


### [102] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 论文提出两种多智能体演员-评论家方法（CoLLM-CC和CoLLM-DC）来优化去中心化LLM协作，相比蒙特卡洛方法在长时域或稀疏奖励任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有MARL微调方法依赖预定义执行协议（通常需要集中式执行），而去中心化LLM协作更实用；当前方法使用蒙特卡洛方法存在高方差问题，需要更多样本训练。

Method: 提出两种多智能体演员-评论家方法：CoLLM-CC（集中式评论家）和CoLLM-DC（分散式评论家），用于优化去中心化LLM协作。

Result: 在写作、编码和游戏领域实验中，蒙特卡洛方法和CoLLM-DC在短时域密集奖励任务中表现与CoLLM-CC相当，但在长时域或稀疏奖励任务中，蒙特卡洛方法需要更多样本，CoLLM-DC难以收敛，CoLLM-CC表现最优。

Conclusion: 多智能体演员-评论家方法（特别是集中式评论家版本）能有效解决去中心化LLM协作中的训练效率问题，尤其在复杂的长时域或稀疏奖励任务中优势明显。

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [103] [Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models](https://arxiv.org/abs/2601.21975)
*Pranav Mahajan,Ihor Kendiukhov,Syed Hussain,Lydia Nottingham*

Main category: cs.AI

TL;DR: SvR偏好差距评估高度依赖协议设计，允许中立和弃权能改善相关性，但完全允许弃权会使相关性降至零或负值，系统提示引导效果有限


<details>
  <summary>Details</summary>
Motivation: 现有评估主要依赖二元强制选择提示，这混淆了真实偏好与协议人为因素，需要系统研究不同协议如何影响语言模型的陈述-揭示偏好相关性

Method: 系统研究24个语言模型，通过允许中立和弃权来排除弱信号，比较不同协议下的SvR相关性，并测试系统提示引导在AIRiskDilemmas上的效果

Result: 允许中立和弃权在陈述偏好中能显著改善相关性，但在揭示偏好中允许弃权会使相关性降至零或负值，系统提示引导不能可靠改善SvR相关性

Conclusion: SvR相关性高度依赖协议设计，偏好评估需要能够处理不确定偏好的方法，不能简单依赖强制选择协议

Abstract: Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.

</details>


### [104] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: VERSA是一个用于足球事件数据验证的系统框架，能自动检测和纠正事件流中的逻辑不一致问题，显著提升数据质量和下游分析可靠性。


<details>
  <summary>Details</summary>
Motivation: 事件流数据在体育分析等领域至关重要，但现有数据存在逻辑不一致问题（如事件顺序错误或缺失事件），这限制了分析模型的可靠性。特别是在足球领域，数据质量问题会影响球员贡献评估和战术模式识别等精细分析。

Method: 提出VERSA（Verified Event Data Format for Reliable Soccer Analytics）验证框架，基于状态转移模型定义有效事件序列，能够自动检测和纠正事件流数据中的异常模式。

Result: 在K League 1（2024赛季）的Bepro数据中检测到18.81%的事件存在逻辑不一致。VERSA显著提高了跨数据提供商的一致性，确保异构数据源的稳定统一表示。经过VERSA精炼的数据显著提升了VAEP（评估球员贡献的下游任务）的鲁棒性和性能。

Conclusion: 验证过程能有效提高数据驱动分析的可靠性，VERSA框架为解决事件流数据质量问题提供了系统化解决方案，对提升体育分析等领域的精细分析质量具有重要意义。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [105] [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](https://arxiv.org/abs/2601.21993)
*Dhiogo de Sá,Carlos Schmiedel,Carlos Pereira Lopes*

Main category: cs.AI

TL;DR: 提出Liquid Interfaces（液态接口）协调范式，将接口从静态技术构件转变为运行时通过意图表达和语义协商产生的短暂关系事件，以支持自适应、概率性和上下文相关的智能体系统。


<details>
  <summary>Details</summary>
Motivation: 当前软件架构难以支持自适应、概率性和上下文相关的自主智能体，而系统集成仍被静态接口和确定性契约主导，需要新的协调范式来应对这一挑战。

Method: 提出Liquid Interfaces协调范式，形式化该模型并设计Liquid Interface Protocol (LIP)协议，该协议管理意图驱动的交互、协商执行以及在语义不确定性下强制短暂性，同时讨论了治理影响并描述了参考架构。

Result: 开发了Liquid Interface Protocol (LIP)协议和参考架构，为基于智能体的系统中的自适应协调提供了原则性基础，并展示了实际可行性。

Conclusion: Liquid Interfaces为智能体系统的自适应协调提供了新的原则性基础，通过将接口从静态构件转变为运行时产生的短暂关系事件，解决了传统架构在支持自适应、概率性和上下文相关智能体方面的局限性。

Abstract: Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems

</details>


### [106] [Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference](https://arxiv.org/abs/2601.22001)
*Yiren Zhao,Junyi Liu*

Main category: cs.AI

TL;DR: 论文分析了AI智能体推理带来的数据中心挑战，提出了操作强度(OI)和容量足迹(CF)两个新指标，揭示了传统屋顶线分析忽略的内存容量墙问题，并提出了解耦服务、异构系统等解决方案。


<details>
  <summary>Details</summary>
Motivation: AI智能体推理正在推动数据中心向推理密集型发展，暴露出计算之外的关键瓶颈：内存容量、内存带宽和高速互连。传统屋顶线分析无法捕捉这些瓶颈，特别是内存容量墙问题。

Method: 引入两个新指标：操作强度(OI)和容量足迹(CF)，用于分析不同智能体工作流（聊天、编码、网页使用、计算机使用）和基础模型选择（GQA/MLA、MoE、量化）下的性能瓶颈。特别关注长上下文KV缓存导致的解码阶段内存约束。

Result: 研究发现OI/CF在不同工作流和模型配置下变化显著，长上下文KV缓存使解码阶段高度受内存限制。这揭示了传统屋顶线分析无法捕捉的瓶颈机制。

Conclusion: 提出了解耦服务、系统级异构、专用预填充和解码加速器、扩展网络规模、光I/O解耦计算内存等解决方案。建议智能体-硬件协同设计、单系统多推理加速器、高带宽大容量内存解耦作为适应OI/CF演化的基础。

Abstract: AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.

</details>


### [107] [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://arxiv.org/abs/2601.22027)
*Johannes Kirmayr,Lukas Stappen,Elisabeth André*

Main category: cs.AI

TL;DR: CAR-bench是一个评估车载语音助手场景下LLM智能体一致性、不确定性处理和能力认知的基准测试，包含幻觉任务和消歧任务，揭示当前LLM智能体在现实应用中的可靠性不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体基准测试主要关注理想环境下的任务完成度，忽略了现实用户应用中面对不完整或模糊请求时的可靠性问题。车载语音助手等场景中，用户常提出不完整或模糊请求，智能体需要通过对话、工具使用和策略遵守来管理内在不确定性。

Method: 提出CAR-bench基准测试，包含LLM模拟用户、领域策略和58个相互关联的工具（涵盖导航、生产力、充电和车辆控制）。除了标准任务完成度测试，还引入幻觉任务（测试智能体在工具或信息缺失时的极限认知）和消歧任务（要求通过澄清或内部信息收集解决不确定性）。

Result: 基准测试结果显示，所有任务类型上偶尔成功和持续成功之间存在巨大差距。即使是前沿推理LLM在消歧任务上的持续通过率也低于50%（由于过早行动），在幻觉任务中经常违反策略或编造信息以满足用户请求。

Conclusion: 当前LLM智能体在现实世界应用中缺乏可靠性和自我认知能力，需要开发更可靠、自我意识更强的LLM智能体来应对用户不完整或模糊请求的挑战。

Abstract: Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.

</details>


### [108] [Optimizing Agentic Workflows using Meta-tools](https://arxiv.org/abs/2601.22037)
*Sami Abuzakuk,Anne-Marie Kermarrec,Rishi Sharma,Rasmus Moorits Veski,Martijn de Vos*

Main category: cs.AI

TL;DR: AWO框架通过识别和优化冗余工具执行模式，将重复的工具调用序列转化为元工具，减少LLM调用次数并提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 智能体AI工作流通常需要多次迭代推理和工具调用，导致运营成本高、端到端延迟长以及幻觉导致的失败。需要优化工作流效率。

Method: AWO分析现有工作流轨迹，发现重复的工具调用序列，将其转化为元工具（确定性的复合工具），将多个智能体动作捆绑到单个调用中。

Result: 在两个智能体AI基准测试中，AWO将LLM调用次数减少高达11.9%，同时将任务成功率提高高达4.2个百分点。

Conclusion: AWO框架通过优化冗余工具执行模式，显著提高了智能体工作流的效率和鲁棒性，减少了运营成本和执行失败。

Abstract: Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.

</details>


### [109] [Defining Operational Conditions for Safety-Critical AI-Based Systems from Data](https://arxiv.org/abs/2601.22118)
*Johann Christensen,Elena Hoemann,Frank Köster,Sven Hallerbach*

Main category: cs.AI

TL;DR: 提出一种基于多维核表示的安全设计方法，从已有数据中后验定义操作设计域，用于数据驱动的安全关键AI系统认证


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，AI系统需要明确的运行环境描述（ODD）进行认证，但现实复杂系统或已有数据场景下难以完整定义环境条件，传统方法依赖专家知识在开发早期定义ODD存在局限性

Method: 提出安全设计方法，使用多维核表示从先前收集的数据中后验定义ODD，通过蒙特卡洛方法和真实航空用例验证，建立两个ODD相等的条件

Result: 验证表明数据驱动的ODD可以等于原始隐藏的ODD，基于核的安全设计ODD方法能够支持未来数据驱动的安全关键AI系统认证

Conclusion: 该方法为数据驱动的安全关键AI系统认证提供了可行的技术途径，解决了传统ODD定义方法的局限性

Abstract: Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.

</details>


### [110] [The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR](https://arxiv.org/abs/2601.22128)
*Irsyad Adam,Zekai Chen,David Laprade,Shaun Porwal,David Laub,Erik Reinertsen,Arda Pekis,Kevin Brown*

Main category: cs.AI

TL;DR: SMB-Structure是一个用于结构化电子健康记录的世界模型，结合联合嵌入预测架构和下一个标记预测，能够学习患者轨迹的动态变化，优于仅基于自回归预测的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于下一个标记预测的LLM将患者视为需要总结的文档，而非需要模拟的动态系统。患者轨迹是由状态在干预和时间下演化形成的，需要能够模拟动态而非仅仅预测标记的模型。

Method: 提出SMB-Structure世界模型，结合联合嵌入预测架构（JEPA）和下一个标记预测（SFT）。SFT让模型在标记空间重建未来患者状态，JEPA仅从初始患者表示预测潜在空间的未来状态，迫使模型在观察到下一个状态前编码轨迹动态。

Result: 在两个大规模队列（MSK的23,319名肿瘤患者和INSPECT的19,402名肺栓塞患者）上验证。线性探针评估显示，该训练范式学习的嵌入能捕捉自回归基线无法恢复的疾病动态，在患者异质性高的复杂任务上取得有竞争力的性能。

Conclusion: SMB-Structure通过结合JEPA和SFT的训练范式，能够学习患者轨迹的动态表示，为结构化EHR建模提供了新方法，优于传统的自回归预测模型。

Abstract: Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient's trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.

</details>


### [111] [World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems](https://arxiv.org/abs/2601.22130)
*Lakshya Gupta,Litao Li,Yizhe Liu,Sriram Ganapathi Subramanian,Kaheer Suleman,Zichen Zhang,Haoye Lu,Sumit Pasupalak*

Main category: cs.AI

TL;DR: 论文提出了World of Workflows (WoW)基准测试环境，基于ServiceNow平台，包含4000+业务规则和55个工作流，用于评估LLM在企业系统中的表现，发现前沿LLM存在"动态盲区"问题，无法预测隐藏的级联副作用。


<details>
  <summary>Details</summary>
Motivation: 现有企业基准测试只评估表面任务完成度，忽略了企业系统的真实挑战：有限可观测性、大规模数据库状态、隐藏工作流及其级联副作用。需要更真实的测试环境来评估LLM在企业系统中的实际能力。

Method: 构建了World of Workflows (WoW)环境，基于ServiceNow平台，包含4000+业务规则和55个活跃工作流。同时创建了WoW-bench基准测试，包含234个任务，评估约束条件下的代理任务完成和企业动态建模能力。

Result: 发现两个主要结论：(1) 前沿LLM存在"动态盲区"，无法预测行动的隐藏级联副作用，导致静默约束违规；(2) 在不透明系统中实现可靠性需要基于世界建模，代理必须心智模拟隐藏状态转换来弥补可观测性差距。

Conclusion: 为了构建可靠有用的企业代理，需要新的范式来显式学习系统动态。WoW为此提供了测试环境，并已在GitHub上发布供设置和评估使用。

Abstract: Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.

</details>


### [112] [Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data](https://arxiv.org/abs/2601.22141)
*Grzegorz Stefanski,Alberto Presta,Michal Byra*

Main category: cs.AI

TL;DR: RTL提出自适应剪枝框架，发现多个针对不同数据类别的专用子网络（自适应彩票），在保持高性能的同时大幅减少参数，并识别了子网络崩溃现象。


<details>
  <summary>Details</summary>
Motivation: 现有彩票假设方法通常假设存在一个适用于所有输入的通用获胜彩票，忽略了真实世界数据的异质性。需要开发能够适应数据多样性的剪枝方法。

Method: 提出Routing the Lottery (RTL)自适应剪枝框架，发现多个针对特定类别、语义簇或环境条件的专用子网络（自适应彩票），并引入子网络相似度分数来诊断过度稀疏化问题。

Result: RTL在多个数据集和任务上持续优于单模型和多模型基线，在平衡准确率和召回率方面表现更好，同时使用比独立模型少10倍的参数，且子网络具有语义对齐特性。

Conclusion: 该研究将剪枝重新定义为一种使模型结构与数据异质性对齐的机制，为更模块化和上下文感知的深度学习铺平了道路。

Abstract: In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.

</details>


### [113] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: 本文提出Agent-RRM，一个为智能体轨迹提供结构化反馈的多方面奖励模型，包含推理轨迹、针对性批判和总体评分，并通过三种集成策略显著提升智能体性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体强化学习方法主要依赖稀疏的结果奖励，无法区分中间推理质量，导致训练效果不理想。需要更精细的反馈机制来指导智能体学习过程。

Method: 提出Agent Reasoning Reward Model (Agent-RRM)，提供三方面结构化反馈：1) 显式推理轨迹，2) 突出推理缺陷的针对性批判，3) 过程性能的总体评分。并研究三种集成策略：Reagent-C（文本增强细化）、Reagent-R（奖励增强指导）和Reagent-U（统一反馈集成）。

Result: 在12个多样化基准测试中，Reagent-U策略表现突出，在GAIA上达到43.7%，在WebWalkerQA上达到46.2%，验证了推理奖励模型和训练方案的有效性。

Conclusion: Agent-RRM通过提供结构化反馈显著提升了智能体强化学习的性能，特别是统一反馈集成策略Reagent-U效果最佳，为未来研究提供了有价值的工具和数据集。

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [114] [Beyond Martingale Estimators: Structured Estimators for Maximizing Information Freshness in Query-Based Update Systems](https://arxiv.org/abs/2601.22098)
*Sahan Liyanaarachchi,Sennur Ulukus,Nail Akar*

Main category: cs.IT

TL;DR: 本文研究远程估计系统中的信息新鲜度，针对连续时间马尔可夫链信息源，提出结构化估计器作为从鞅估计器到MAP估计器的平滑过渡，并推导最优查询策略。


<details>
  <summary>Details</summary>
Motivation: 现有远程估计系统主要使用鞅估计器（基于最新接收的更新），虽然简单易分析但远非最优，特别是在查询式（pull-based）系统中。MAP估计器虽然最优，但在连续时间设置中分析困难。需要一种既能保留MAP估计器优点又便于分析的估计器。

Method: 提出结构化估计器，特别是p-MAP估计器（具有有限个间断点的分段常数近似）。对于时间可逆CTMC，证明MAP估计器可简化为p-MAP估计器。使用二元新鲜度过程表征信息新鲜度，推导新鲜度表达式，并为单CTMC信息源和多异构CTMC场景提供最优状态依赖采样策略（查询策略）。

Result: 结构化估计器能够平滑地从鞅估计器过渡到MAP估计器，保留MAP估计器的有用特性同时保持分析可处理性。对于时间可逆CTMC，MAP估计器可简化为p-MAP估计器。推导了新鲜度表达式，并提供了最大化平均二元新鲜度的最优查询策略。

Conclusion: 结构化估计器为连续时间马尔可夫链远程估计系统提供了一种分析可处理且接近最优的估计方法，填补了鞅估计器和MAP估计器之间的空白，并为查询式系统提供了最优查询策略设计。

Abstract: This paper investigates information freshness in a remote estimation system in which the remote information source is a continuous-time Markov chain (CTMC). For such systems, estimators have been mainly restricted to the class of martingale estimators in which the remote estimate at any time is equal to the value of the most recently received update. This is mainly due to the simplicity and ease of analysis of martingale estimators, which however are far from optimal, especially in query-based (i.e., pull-based) update systems. In such systems, maximum a-posteriori probability (MAP) estimators are optimal. However, MAP estimators can be challenging to analyze in continuous-time settings. In this paper, we introduce a new class of estimators, called structured estimators, which can seamlessly shift from a martingale estimator to a MAP estimator, enabling them to retain useful characteristics of the MAP estimate, while still being analytically tractable. Particularly, we introduce a new estimator termed as the $p$-MAP estimator which is a piecewise-constant approximation of the MAP estimator with finitely many discontinuities, bringing us closer to a full characterization of MAP estimators when modeling information freshness. In fact, we show that for time-reversible CTMCs, the MAP estimator reduces to a $p$-MAP estimator. Using the binary freshness (BF) process for the characterization of information freshness, we derive the freshness expressions and provide optimal state-dependent sampling policies (i.e., querying policies) for maximizing the mean BF (MBF) for pull-based remote estimation of a single CTMC information source, when structured estimators are used. Moreover, we provide optimal query rate allocation policies when a monitor pulls information from multiple heterogeneous CTMCs with a constraint on the overall query rate.

</details>


### [115] [The Noncomputability of Immune Reaction Complexity: Algorithmic Information Gaps under Effective Constraints](https://arxiv.org/abs/2601.20865)
*Emmanuel Pio Pastore,Francesco De Rango*

Main category: cs.IT

TL;DR: 提出基于算法信息论的验证过滤、证书驱动的反应框架，引入归一化建议分位数（NAQ）作为尺度无关的难度指标，具有通用机不变性和跨任务可比性。


<details>
  <summary>Details</summary>
Motivation: 建立一种形式化框架来衡量生成有效反应所需的最小信息量，为不同任务家族提供统一的难度度量标准，同时保证对通用机选择的鲁棒性。

Method: 基于算法信息论，定义最小可行实现者复杂度M(x) = min_{r:V(x,r)=1} K(r)，其中K为前缀柯尔莫哥洛夫复杂度。引入归一化建议分位数NAQ作为M(x)在参考池中的百分位数。提出精确实现者恒等式、描述加选择上界等理论结果。

Result: 证明最小建议长度等于M(x)至多O(1)常数误差；在有限模糊度情况下M(x)约等于min_y K(y)；NAQ具有有界枚举变化下的准不变性；建立NAQ与率失真理论的联系；提出资源受限变体NAQ_t和NP风格设置；提供基于压缩器的经验估计收敛保证。

Conclusion: NAQ提供了一个理论严谨、可计算的难度度量框架，能够跨任务家族比较反应生成的信息需求，并通过压缩器代理实现数据驱动的校准，为算法信息论在反应系统中的应用提供了新视角。

Abstract: We introduce a validity-filtered, certificate-based view of reactions grounded in Algorithmic Information Theory. A fixed, total, input-blind executor maps a self-delimiting advice string to a candidate response, accepted only if a decidable or semi-decidable validity predicate V(x, r) holds. The minimum feasible realizer complexity M(x) = min_{r: V(x,r)=1} K(r), with K denoting prefix Kolmogorov complexity, measures the minimal information required for a valid outcome. We define the Normalized Advice Quantile (NAQ) as the percentile of M(x) across a reference pool, yielding a scale-free hardness index on [0, 1] robust to the choice of universal machine and comparable across task families. An Exact Realizer Identity shows that the minimal advice for any input-blind executor equals M(x) up to O(1), while a description plus selection upper bound refines it via computable feature maps, separating description cost K(y) from selection cost log i_y(x). In finite-ambiguity regimes M(x) approximately equals min_y K(y); in generic-fiber regimes the bound is tight. NAQ is quasi-invariant under bounded enumeration changes. An operational converse links NAQ to rate-distortion: communicating advice with error epsilon requires average length near the entropy of target features. Extensions include a resource-bounded variant NAQ_t incorporating time-penalized complexity (Levin's Kt) and an NP-style setting showing linear worst-case advice n - O(1). Finally, a DKW bound guarantees convergence of empirical NAQ estimates, enabling data-driven calibration via compressor-based proxies.

</details>


### [116] [Cramér-Rao Bound Analysis and Near-Optimal Performance of the Synchronous Nyquist-Folding Generalized Eigenvalue Method (SNGEM) for Sub-Nyquist Multi-Tone Parameter Estimation](https://arxiv.org/abs/2601.20866)
*Huiguang Zhang*

Main category: cs.IT

TL;DR: SNGEM方法在极端亚奈奎斯特速率下实现多音信号的完整频率/幅度/相位估计，通过联合处理原始信号及其时间导数，在噪声条件下接近CRB，优于传统压缩感知方法。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知方法在亚奈奎斯特采样中存在不可约误差平台问题，这是由于DFT网格偏差和混叠噪声导致的。需要一种能够在极端压缩比下实现统计最优的参数谱分析方法。

Method: 同步奈奎斯特折叠广义特征值方法(SNGEM)，通过联合处理原始信号及其时间导数，在极端亚奈奎斯特速率下实现多音信号的完整频率/幅度/相位估计。推导了双通道等SNR条件下幅度比参数R=A/B=1/(2πf)的精确克拉美-罗界。

Result: 蒙特卡洛模拟证实：SNGEM在无噪声条件下达到机器精度，在所有SNR水平下都接近推导的CRB，即使在10-20倍压缩比下也表现良好。而传统压缩感知OMP方法由于DFT网格偏差和混叠噪声表现出不可约的误差平台。

Conclusion: SNGEM被确立为统计上接近最优的确定性亚奈奎斯特参数谱分析方法，在极端压缩比下仍能保持优异性能，优于传统压缩感知方法。

Abstract: The synchronous Nyquist folding generalized eigenvalue method (SNGEM) realizes full frequency/amplitude/phase estimation of multitone signals at extreme sub-Nyquist rates by jointly processing the original signals and their time derivatives. In this paper, accurate Cramer-Rao bounds for amplitude ratio parameter R=A/B=1/(2\pif) are derived for two channels with equal SNR. Monte-Carlo simulations confirm that SNGEM achieves machine accuracy in noise-free conditions and closely approaches the derived CRB at all SNR levels, even at 10- 20x compression, whereas classical compressive sensing OMP exhibits irreducible error flattening due to DFT grid bias and aliasing noise. These results establish SNGEM as a statistically nearly optimal deterministic sub-Nyquist parameter spectrum analysis

</details>


### [117] [Power consumption Reduction in ELAA-Assisted ISAC Systems](https://arxiv.org/abs/2601.21010)
*Xiaomin Cao,Mohammadali Mohammadi,Hien Quoc Ngo,Michail Matthaiou*

Main category: cs.IT

TL;DR: 针对集成感知与通信中的超大规模天线阵列，提出基于子阵列激活的节能框架，通过优化选择子阵列子集来显著降低功耗，同时满足感知与通信的服务质量约束。


<details>
  <summary>Details</summary>
Motivation: 超大规模天线阵列对于实现高分辨率近场感知至关重要，但传统数字架构中激活所有天线单元会导致功耗过高，需要设计节能方案来解决这一挑战。

Method: 提出能量高效的子阵列激活框架，将功耗最小化问题建模为优化问题，并采用基于连续凸近似的迭代算法进行求解。

Result: 仿真结果表明，所提方法在保持双功能性能的同时，能够显著降低总功耗。

Conclusion: 该研究为集成感知与通信系统中的超大规模天线阵列提供了一种有效的节能解决方案，通过优化子阵列激活策略实现了功耗与性能的良好平衡。

Abstract: In this paper, we consider power consumption reduction in extremely large antenna arrays (ELAAs) for integrated sensing and communication (ISAC) applications. Although ELAAs are critical for achieving high-resolution near-field sensing, fully activating all antenna elements in conventional digital architectures leads to prohibitive power demands. To address this, we propose an energy-efficient subarray activation framework that selects an optimal subset of subarrays to minimize the total power consumption, subject to quality-of-service (QoS) constraints for both sensing and communication. We formulate a novel optimization problem and solve it using a successive convex approximation (SCA)-based iterative algorithm. The simulation results confirm that the proposed method significantly reduces power consumption while maintaining dual-function performance.

</details>


### [118] [Deletion-correcting codes for an adversarial nanopore channel](https://arxiv.org/abs/2601.21236)
*Huiling Xie,Zitan Chen*

Main category: cs.IT

TL;DR: 本文研究了对抗性纳米孔通道的删除纠错码，提出了一个具有2t log_q n + Θ(log log n)冗余符号的显式构造，并证明了最优冗余在t log_q n + Ω(1)和2t log_q n - log_q log_2 n + O(1)之间。


<details>
  <summary>Details</summary>
Motivation: 纳米孔测序技术中的对抗性删除通道需要高效的纠错码。与经典对抗性删除通道相比，现有显式构造的冗余度较高（4t(1+ε)log_q n + o(log n)），需要更优的构造。

Method: 提出了对抗性纳米孔通道的显式q元码构造，使用2t log_q n + Θ(log log n)冗余符号。通过理论分析确定了最优冗余的上下界。

Result: 构造的显式码与存在性上界在首项上匹配，冗余度显著优于经典对抗性删除通道的已知显式构造（4t(1+ε)log_q n + o(log n) vs 2t log_q n + Θ(log log n)）。

Conclusion: 本文为对抗性纳米孔通道提供了接近最优的显式删除纠错码构造，填补了理论最优界与实用构造之间的差距，相比经典删除通道有显著改进。

Abstract: We study deletion-correcting codes for an adversarial nanopore channel in which at most $t$ deletions may occur. We propose an explicit construction of $q$-ary codes of length $n$ for this channel with $2t\log_q n+Θ(\log\log n)$ redundant symbols. We also show that the optimal redundancy is between $t\log_q n+Ω(1)$ and $2t\log_q n-\log_q\log_2 n+O(1)$, so our explicit construction matches the existential upper bound to first order. In contrast, for the classical adversarial $q$-ary deletion channel, the smallest redundancy achieved by known explicit constructions that correct up to $t$ deletions is $4t(1+ε)\log_q n+o(\log n)$.

</details>


### [119] [Belief Propagation with Quantum Messages for Symmetric Q-ary Pure-State Channels](https://arxiv.org/abs/2601.21330)
*Avijit Mandal,Henry D. Pfister*

Main category: cs.IT

TL;DR: 本文扩展了量子消息的置信传播（BPQM）到对称q元纯态信道，提出了基于Gram矩阵特征值的闭式递归跟踪方法，并建立了密度演化框架用于分析LDPC码和构造极化码。


<details>
  <summary>Details</summary>
Motivation: 先前BPQM构造和密度演化分析主要针对二元字母表，需要将其推广到更一般的q元对称纯态信道，以扩展量子通信中低复杂度解码方案的应用范围。

Method: 针对输出Gram矩阵为循环矩阵的对称q元纯态信道，提出基于Gram矩阵特征值的闭式递归方法，独立于输出态的具体物理实现，通过特征值递归高效跟踪比特节点和校验节点的合并过程。

Result: 推导出显式的BPQM幺正变换，获得合并信道保真度的解析边界，建立了对称q元纯态信道的密度演化框架，可用于估计LDPC码的BPQM解码阈值和构造这些信道上的极化码。

Conclusion: 成功将BPQM推广到对称q元纯态信道，提供了高效的密度演化分析工具，为量子通信中低复杂度解码方案的设计和性能分析提供了理论基础。

Abstract: Belief propagation with quantum messages (BPQM) provides a low-complexity alternative to collective measurements for communication over classical--quantum channels. Prior BPQM constructions and density-evolution (DE) analyses have focused on binary alphabets. Here, we generalize BPQM to symmetric q-ary pure-state channels (PSCs) whose output Gram matrix is circulant. For this class, we show that bit-node and check-node combining can be tracked efficiently via closed-form recursions on the Gram-matrix eigenvalues, independent of the particular physical realization of the output states. These recursions yield explicit BPQM unitaries and analytic bounds on the fidelities of the combined channels in terms of the input-channel fidelities. This provides a DE framework for symmetric q-ary PSCs that allows one to estimate BPQM decoding thresholds for LDPC codes and to construct polar codes on these channels.

</details>


### [120] [Learning-Based Sensor Scheduling for Delay-Aware and Stable Remote State Estimation](https://arxiv.org/abs/2601.21482)
*Nho-Duc Tran,Aamir Mahmood,Mikael Gidlund*

Main category: cs.IT

TL;DR: 提出一个统一的延迟感知框架，用于无线远程状态估计，通过延迟依赖的信息增益度量，超越AoI代理，实现信息每焦耳调度优化。


<details>
  <summary>Details</summary>
Motivation: 不可预测的传感器到估计器延迟会扭曲无线远程状态估计的关键因素：不仅影响新鲜度，还影响延迟与传感器信息量和能量效率的交互作用。现有基于AoI的方法未能充分捕捉这种耦合关系。

Method: 1) 引入高效的后验融合更新方法，在不进行状态增广的情况下合并延迟测量；2) 推导可处理的稳定性条件；3) 将调度建模为马尔可夫决策过程，开发基于近端策略优化(PPO)的调度器，通过归一化奖励在估计精度、新鲜度、传感器异质性和传输能量之间进行权衡。

Result: 在具有异质传感器、实际链路能量模型和随机延迟的仿真中，该方法稳定学习，在可比能量下比随机调度和强RL基线(DQN, A2C)实现更低的估计误差，同时对测量可用性和过程/测量噪声变化保持鲁棒性。

Conclusion: 提出的延迟感知框架和PPO调度器有效解决了无线远程状态估计中的延迟-信息-能量权衡问题，超越了传统AoI方法，为实际系统中的高效调度提供了实用解决方案。

Abstract: Unpredictable sensor-to-estimator delays fundamentally distort what matters for wireless remote state estimation: not just freshness, but how delay interacts with sensor informativeness and energy efficiency. In this paper, we present a unified, delay-aware framework that models this coupling explicitly and quantifies a delay-dependent information gain, motivating an information-per-joule scheduling objective beyond age of information proxies (AoI). To this end, we first introduce an efficient posterior-fusion update that incorporates delayed measurements without state augmentation, providing a consistent approximation to optimal delayed Kalman updates, and then derive tractable stability conditions ensuring that bounded estimation error is achievable under stochastic, delayed scheduling. This conditions highlight the need for unstable modes to be observable across sensors. Building on this foundation, we cast scheduling as a Markov decision process and develop a proximal policy optimization (PPO) scheduler that learns directly from interaction, requires no prior delay model, and explicitly trades off estimation accuracy, freshness, sensor heterogeneity, and transmission energy through normalized rewards. In simulations with heterogeneous sensors, realistic link-energy models, and random delays, the proposed method learns stably and consistently achieves lower estimation error at comparable energy than random scheduling and strong RL baselines (DQN, A2C), while remaining robust to variations in measurement availability and process/measurement noise.

</details>


### [121] [Transversal gates for quantum CSS codes](https://arxiv.org/abs/2601.21514)
*Eduardo Camps-Moreno,Hiram H. López,Gretchen L. Matthews,Narayanan Rengaswamy,Rodrigo San-José*

Main category: cs.IT

TL;DR: 本文研究了CSS码的横向门计算问题，确定了逻辑作用、诱导非平凡逻辑门和逻辑恒等式的横向门群，并给出了定义这些群的方程。特别针对单项式码族（包括递减单项式码和极化码）的CSS码，计算了完整的横向稳定子和横向门集合。


<details>
  <summary>Details</summary>
Motivation: 研究CSS码的横向门计算问题，特别是确定哪些横向门能诱导非平凡逻辑门，这对于量子纠错码的实现和容错量子计算至关重要。现有文献中对这一问题的系统分析不足，需要更一般化的理论框架。

Method: 通过建立定义横向门群的方程组，系统分析了CSS码的横向门结构。特别针对单项式码族（包括递减单项式码和极化码）的CSS码，计算了完整的横向稳定子和横向门集合。

Result: 确定了CSS码横向门的逻辑作用，构建了诱导非平凡逻辑门和逻辑恒等式的横向门群。得到了定义这些群的显式方程组，并针对单项式码族的CSS码计算了完整的横向稳定子和横向门集合。

Conclusion: 本文为CSS码的横向门计算提供了系统框架，统一并扩展了文献中关于CSS-T码、三正交码和可除码的相关结果。提出的方程组方法为分析更广泛的量子纠错码奠定了基础。

Abstract: In this paper, we focus on the problem of computing the set of diagonal transversal gates fixing a CSS code. We determine the logical actions of the gates as well as the groups of transversal gates that induce non-trivial logical gates and logical identities. We explicitly declare the set of equations defining the groups, a key advantage and differentiator of our approach. We compute the complete set of transversal stabilizers and transversal gates for any CSS code arising from monomial codes, a family that includes decreasing monomial codes and polar codes. As a consequence, we recover and extend some results in the literature on CSS-T codes, triorthogonal codes, and divisible codes.

</details>


### [122] [Subjective Distortion: Achievability and Outer Bounds for Distortion Functions with Memory](https://arxiv.org/abs/2601.21757)
*Hamidreza Abin,Amin Gohari,Andrew W. Eckford*

Main category: cs.IT

TL;DR: 论文研究了一种新型的率失真问题，其中失真函数不仅依赖于当前源符号与表示符号之间的瞬时失真，还依赖于过去的表示符号。作者给出了该问题的形式化定义，并提出了率失真权衡的内界（可达界）和外界，讨论了问题的凸化以简化界求解。


<details>
  <summary>Details</summary>
Motivation: 在某些率失真类型问题中，所需的信息保真度受到过去动作的影响。失真函数不仅依赖于源符号与其表示符号之间的瞬时失真，还依赖于过去的表示。这类问题出现在生物信息处理和推荐引擎等领域，需要新的理论框架来处理这种时间依赖的失真度量。

Method: 作者首先给出了这种具有历史依赖失真函数问题的形式化定义。然后提出了内界（可达界）和外界来界定率失真权衡。为了简化界的求解，讨论了问题的凸化方法，使问题更容易处理。

Result: 建立了具有历史依赖失真函数的率失真问题的理论框架，提供了内界和外界来刻画率失真权衡。通过凸化方法简化了界的求解过程，并将理论应用于简化的生物信息处理问题作为示例。

Conclusion: 该研究扩展了传统率失真理论，考虑了失真函数对历史表示的依赖性，为生物信息处理和推荐引擎等实际应用提供了理论基础。提出的内界和外界框架以及凸化方法为这类时间依赖失真问题的分析提供了有效工具。

Abstract: In some rate-distortion-type problems, the required fidelity of information is affected by past actions. As a result, the distortion function depends not only on the instantaneous distortion between a source symbol and its representation symbol, but also on past representations. In this paper, we give a formal definition of this problem and introduce both inner (achievable) and outer bounds on the rate-distortion tradeoff. We also discuss convexification of the problem, which makes it easier to find bounds. Problems of this type arise in biological information processing, as well as in recommendation engines; we provide an example applied to a simplified biological information processing problem.

</details>


### [123] [Adaptive Privacy of Sequential Data Releases Under Collusion](https://arxiv.org/abs/2601.21859)
*Sophie Taylor,Praneeth Kumar Vippathalla,Justin Coon*

Main category: cs.IT

TL;DR: 提出一种自适应算法处理不同实体（可能共谋）的顺序数据请求，在隐私与效用间取得平衡，使用改进的Blahut-Arimoto算法实现最优或局部最优的数据发布。


<details>
  <summary>Details</summary>
Motivation: 现有隐私机制难以扩展到顺序数据发布场景，且实际数据库中数据可能被多个不同方使用，存在数据共享和共谋风险，需要新的方法处理这些挑战。

Method: 采用改进的Blahut-Arimoto算法开发自适应数据发布算法，考虑预期失真和互信息作为效用度量，互信息作为隐私度量，假设数据接收者之间存在共谋攻击模型。

Result: 当效用以预期失真度量时，数据发布达到最优；当效用以互信息度量时，达到局部最优。算法能有效处理不同实体（可能共谋）的顺序数据请求。

Conclusion: 提出的自适应算法能有效平衡顺序数据发布中的隐私与效用权衡，特别适用于多个不同方（可能共谋）的场景，并可扩展到机器学习应用。

Abstract: The fundamental trade-off between privacy and utility remains an active area of research. Our contribution is motivated by two observations. First, privacy mechanisms developed for one-time data release cannot straightforwardly be extended to sequential releases. Second, practical databases are likely to be useful to multiple distinct parties. Furthermore, we can not rule out the possibility of data sharing between parties. With utility in mind, we formulate a privacy-utility trade-off problem to adaptively tackle sequential data requests made by different, potentially colluding entities. We consider both expected distortion and mutual information as measures to quantify utility, and use mutual information to measure privacy. We assume an attack model whereby illicit data sharing, which we call collusion, can occur between data receivers. We develop an adaptive algorithm for data releases that makes use of a modified Blahut-Arimoto algorithm. We show that the resulting data releases are optimal when expected distortion quantifies utility, and locally optimal when mutual information quantifies utility. Finally, we discuss how our findings may extend to applications in machine learning.

</details>
